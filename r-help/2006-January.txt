From john.maindonald at anu.edu.au  Sun Jan  1 07:23:24 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 1 Jan 2006 17:23:24 +1100
Subject: [R] lme X lmer results
In-Reply-To: <40e66e0b0512301051i2dc0f257r745c70e749c250f0@mail.gmail.com>
References: <mailman.10.1135854000.14037.r-help@stat.math.ethz.ch>
	<CB2F3CC4-F8F3-464D-8BBB-AC71EA03901B@anu.edu.au>
	<40e66e0b0512301051i2dc0f257r745c70e749c250f0@mail.gmail.com>
Message-ID: <49BD53F2-4C16-44AA-8CFD-E572DD257194@anu.edu.au>

Douglas -

As I understand Ronaldo's experiment, there are 4 plots, 8 subplots  
within
each of those 4 plots, and 20 subsubplots within each of the 8 subplots.
Within each subsbubplot there is an average of 1.375 observational
units.  We do not however need to know the distribution of observational
units per plot, for the discussion that follows.

A treatment Xvar, with 2 levels, has been applied at the plot level.
So 2 plots get the level 1 treatment, and 2 plots get the level 2  
treatment.
A simple-minded (and insightful) way to start thinking about the  
analysis
is to calculate averages for each of the 4 plots, and base the  
analysis on
those averages.

There are 220 observational units (SD 5.217), 32 plot 1 units (SD  
0.001965),
8 plot 2 units (SD 0.001587), together with an SD of 0.000741 at the  
plot 1,
that contribute to variation at the plot 1 level.  So the SD for an  
inidividual
plot 1 unit is
sd1 = sqrt(5.217^2/220+.001965^2/32+.001587^2/8+.000741^2) = 0.35173

Thus the estimated SED for comparing the two levels of Xvar is
sqrt(s^2/2+s^2/2) = s = 0.35173
Not accidentally, this is exactly the SED that is given by both lme  
and lmer.
Because the estimate is dominated by variation at the level of  
observational
units this is also, essentially, the value given by lm.

If we knew that the variance component at the plot 1 level was indeed  
very
small, giving the variance estimate 2df would be very conservative.  The
problem is that we do not know this.  We can either calculate sd1^2  
directly
from the plot 1 means, or we can go through the more convoluted process
of estimating the various variance components, then dividing by the  
relevant
numbers and adding.  Either way, both the estimate of sd1 and the  
estimate
of the component of variance at the plot 1 level is a 2df  
calculation, with a
CI that relies on assuming that sd1^2 has something like a chi-squared
distribution.  For moderately unbalanced designs the df calculation  
has to
be fudged a bit.

The following function has defaults that reproduce the essential  
features
or Ronaldo's analysis, though with a treatment effect that is noise.   
For
simplicity, there are just two levels of variation:

"mlsim" <-
   function(s1=0.00074, s2=5.22, n1=4, n2=220, nsamples=1000){
     ## Generate data with n1 plots, each with 220 subplots
     ## Data are such that estimated variance component at plot
     ## level is s1^2; at subplot level is s2^2
     ## A treatment with 2 levels is applied at the plot level;
     ## with n1/2 plots per treatment.
     ## n1 should be an even number
     nu1 <- n1-2       ## df for treatment comparison
     nu2 <- n1*(n2-1)  ## df for estimate of s2
     xp <- expand.grid(plot1=1:n1, plot2=1:n2)
     eff1 <- rnorm(n1)
     trt <- factor((xp$plot1+1) %% 2 + 1)
     sd1 <- sqrt(s1^2+s2^2/n2)
     eff1 <- residuals(lm(eff1~factor((2:(n1+1)) %% 2 + 1)))
     eff1 <- eff1*sd1/sd(eff1)
     eff2 <- rnorm(n1*n2)
     eff2 <- resid(lm(eff2~factor(xp$plot1)))
     eff2 <- eff2*s2/sqrt(sum(eff2^2)/nu2)
     y <- eff1[xp$plot1] + eff2
     df <- cbind(xp, trt=trt, y=y)
     df$plot1 <- factor(df$plot1)
     df$plot2 <- factor(df$plot2)
     asum <- summary(aov(y~trt +Error(plot1), data=df))
     bms2 <- asum[[1]][[1]][2,3]
     ws2 <- asum[[2]][[1]][1,3]
     eff1 <- eff1*sd1/sqrt(bms2/n2)
     df$y <- eff1[xp$plot1] + eff2
     if(nsamples>1){
       df.lmer <- lmer(y~trt + (1|plot1), data=df)
       print(summary(df.lmer))
       df.mc <- mcmcsamp(df.lmer, n=1000)
       par(mfrow=c(2,2), mar=c(3.1,3.1,2.1,1.1), mgp=c(2,.5,0))
       on.exit(par(oldpar))
       qqplot(qchisq(ppoints(1000),nu2)*s2^2/nu2, exp(df.mc[,3]))
       mtext(side=3,line=0.25, paste("s2 =", s2, " (nu2=", nu2, ")",  
sep=""))
       abline(0,1,col=2)
       qqplot(qchisq(ppoints(1000), nu1)*sd1^2/nu1,
              exp(df.mc[,3])/n2+exp(df.mc[,4]))
       abline(0,1,col=2)
       mtext(side=3,line=1.25, paste("s1=",s1, ";  Var of gp means=",
                      paste(round(sd1^2,2)), ";  df=", nu1, sep=""))
       mtext(side=3,line=.25, paste("(",signif(s1^2, 4), " (from s1); ",
                      signif(s2^2/n2,4)," ((from s2)", ")", sep=""))
       invisible(df.mc)
     }
     else invisible(df)
   }

Observe that the sample distribution of sd1^2 (the estimated variance
of the means at the plot 1 level) changes quite dramatically from one
run to the next, withe the direction of convexity in the qqplot changing
around the line y=x.  The distribution is not chi-squared 2, but it  
is not
consistently anything else either.  The distribution is very likely,  
I am
guessing, sensitive to the choice of prior.  Possibly a choice where
log(sigma2) is locally uniform, where sigma2^2 is the variance at
the plot 2 level, would yield a distribution that is closer to chi- 
squared 2.
The plots do however indicate that the chi-squared 2 distribution is in
the right ballpark, as judged by the results from multiple runs of
mcmcsamp().

With mlsim(s1=2.5, s2=5, n1=40, n2=4), the two components contribute
equally to sd1^2, and sd1^2 usually tracks quite closely to a
chi-squared distribution.

John Maindonald.


On 31 Dec 2005, at 5:51 AM, Douglas Bates wrote:

> On 12/29/05, John Maindonald <john.maindonald at anu.edu.au> wrote:
>> Surely there is a correct denominator degrees of freedom if the  
>> design
>> is balanced, as Ronaldo's design seems to be. Assuming that he has
>> specified the design correctly to lme() and that lme() is getting  
>> the df
>> right, the difference is between 2 df and 878 df.  If the t-statistic
>> for the
>> second level of Xvar had been 3.0 rather than 1.1, the difference
>> would be between a t-statistic equal to 0.095 and 1e-6.  In a design
>> where there are 10 observations on each experimental unit, and all
>> comparisons are at the level of experimental units or above, df for
>> all comparisons will be inflated by a factor of at least 9.
>
> I don't want to be obtuse and argumentative but I still am not
> convinced that there is a correct denominator degrees of freedom for
> _this_ F statistic.  I may be wrong about this but I think you are
> referring to an F statistic based on a denominator from a different
> error stratum, which is not what is being quoted.  (Those are not
> given because they don't generalize to unbalanced designs.)
>
> This is why I would like to see someone undertake a simulation study
> to compare various approaches to inference for the fixed effects terms
> in a mixed model, using realistic (i.e. unbalanced) examples.
>
> It seems peculiar to me that the F statistics are being created from
> the ratios of mean squares for different terms to the _same_ mean
> square (actually a penalized sum of squares divided by the degrees of
> freedom) and the adjustment suggested to take into account the
> presence of the random effects is to change the denominator degrees of
> freedom.  I think the rationale for this is an attempt to generalized
> another approach (the use of error strata) even though it is not being
> used here.
>
>> Rather than giving df that for the comparison(s) of interest may be
>> highly inflated, I'd prefer to give no degrees of freedom at all,  
>> & to
>> encourage users to work out df for themselves if at all possible.
>> If they are not able to do this, then mcmcsamp() is a good  
>> alternative,
>> and may be the way to go in any case.  This has the further advantage
>> of allowing assessments in cases where the relevant distribution is
>> hard to get at. I'd think a warning in order that the df are upper
>> bounds,
>> and may be grossly inflated.
>
> As I said, I am willing to change this if it is shown to be grossly
> inaccurate but please show me.
>
>> Incidentally, does mcmcsamp() do its calculations pretty well
>> independently of the lmer results?
>
> mcmcsamp starts from the parameter estimates when creating the chain
> but that is the extent to which it depends on the lmer results.
>
>> John Maindonald.
>>
>> On 29 Dec 2005, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:
>>
>>> From: Douglas Bates <dmbates at gmail.com>
>>> Date: 29 December 2005 5:59:07 AM
>>> To: "Ronaldo Reis-Jr." <chrysopa at gmail.com>
>>> Cc: R-Help <r-help at stat.math.ethz.ch>
>>> Subject: Re: [R] lme X lmer results
>>>
>>>
>>> On 12/26/05, Ronaldo Reis-Jr. <chrysopa at gmail.com> wrote:
>>>> Hi,
>>>>
>>>> this is not a new doubt, but is a doubt that I cant find a good
>>>> response.
>>>>
>>>> Look this output:
>>>>
>>>>> m.lme <- lme(Yvar~Xvar,random=~1|Plot1/Plot2/Plot3)
>>>>
>>>>> anova(m.lme)
>>>>             numDF denDF  F-value p-value
>>>> (Intercept)     1   860 210.2457  <.0001
>>>> Xvar            1     2   1.2352  0.3821
>>>>> summary(m.lme)
>>>> Linear mixed-effects model fit by REML
>>>>  Data: NULL
>>>>       AIC      BIC    logLik
>>>>   5416.59 5445.256 -2702.295
>>>>
>>>> Random effects:
>>>>  Formula: ~1 | Plot1
>>>>         (Intercept)
>>>> StdDev: 0.000745924
>>>>
>>>>  Formula: ~1 | Plot2 %in% Plot1
>>>>         (Intercept)
>>>> StdDev: 0.000158718
>>>>
>>>>  Formula: ~1 | Plot3 %in% Plot2 %in% Plot1
>>>>         (Intercept) Residual
>>>> StdDev: 0.000196583 5.216954
>>>>
>>>> Fixed effects: Yvar ~ Xvar
>>>>                    Value Std.Error  DF  t-value p-value
>>>> (Intercept)    2.3545454 0.2487091 860 9.467066  0.0000
>>>> XvarFactor2    0.3909091 0.3517278   2 1.111397  0.3821
>>>>
>>>> Number of Observations: 880
>>>> Number of Groups:
>>>>                          Plot1               Plot2 %in% Plot1
>>>>                              4                              8
>>>>    Plot3 %in% Plot2 %in% Plot1
>>>>                             20
>>>>
>>>> This is the correct result, de correct denDF for Xvar.
>>>>
>>>> I make this using lmer.
>>>>
>>>>> m.lmer <- lmer(Yvar~Xvar+(1|Plot1)+(1|Plot1:Plot2)+(1|Plot3))
>>>>> anova(m.lmer)
>>>> Analysis of Variance Table
>>>>            Df Sum Sq Mean Sq  Denom F value Pr(>F)
>>>> Xvar  1  33.62   33.62 878.00  1.2352 0.2667
>>>>> summary(m.lmer)
>>>> Linear mixed-effects model fit by REML
>>>> Formula: Yvar ~ Xvar + (1 | Plot1) + (1 | Plot1:Plot2) + (1 |  
>>>> Plot3)
>>>>      AIC     BIC    logLik MLdeviance REMLdeviance
>>>>  5416.59 5445.27 -2702.295   5402.698      5404.59
>>>> Random effects:
>>>>  Groups        Name        Variance   Std.Dev.
>>>>  Plot3         (Intercept) 1.3608e-08 0.00011665
>>>>  Plot1:Plot2   (Intercept) 1.3608e-08 0.00011665
>>>>  Plot1         (Intercept) 1.3608e-08 0.00011665
>>>>  Residual                  2.7217e+01 5.21695390
>>>> # of obs: 880, groups: Plot3, 20; Plot1:Plot2, 8; Plot1, 4
>>>>
>>>> Fixed effects:
>>>>                 Estimate Std. Error  DF t value Pr(>|t|)
>>>> (Intercept)      2.35455    0.24871 878  9.4671   <2e-16 ***
>>>> XvarFactor2      0.39091    0.35173 878  1.1114   0.2667
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Look the wrong P value, I know that it is wrong because the DF
>>>> used. But, In
>>>> this case, the result is not correct. Dont have any difference of
>>>> the result
>>>> using random effects with lmer and using a simple analyses with lm.
>>>
>>> You are assuming that there is a correct value of the denominator
>>> degrees of freedom.  I don't believe there is.  The statistic  
>>> that is
>>> quoted there doesn't have exactly an F distribution so there is no
>>> correct degrees of freedom.
>>>
>>> One thing you can do with lmer is to form a Markov Chain Monte Carlo
>>> sample from the posterior distribution of the parameters so you can
>>> check to see whether the value of zero is in the middle of the
>>> distribution of XvarFactor2 or not.
>>>
>>> It would be possible for me to recreate in lmer the rules used in  
>>> lme
>>> for calculating denominator degrees of freedom associated with terms
>>> of the random effects.  However, the class of models fit by lmer is
>>> larger than the class of models fit by lme (at least as far as the
>>> structure of the random-effects terms goes).  In particular lmer
>>> allows for random effects associated with crossed or partially  
>>> crossed
>>> grouping factors and the rules for denominator degrees of freedom in
>>> lme only apply cleanly to nested grouping factors.  I would  
>>> prefer to
>>> have a set of rules that would apply to the general case.
>>>
>>> Right now I would prefer to devote my time to other aspects of  
>>> lmer -
>>> in particular I am still working on code for generalized linear  
>>> mixed
>>> models using a supernodal Cholesky factorization.  I am willing  
>>> to put
>>> this aside and code up the rules for denominator degrees of freedom
>>> with nested grouping factors BUT first I want someone to show me an
>>> example demonstrating that there really is a problem.  The example
>>> must show that the p-value calculated in the anova table or the
>>> parameter estimates table for lmer is seriously wrong compared to an
>>> empirical p-value - obtained from simulation under the null
>>> distribution or through MCMC sampling or something like that.   
>>> Saying
>>> that "Software XYZ says there are n denominator d.f. and lmer says
>>> there are m" does NOT count as an example.  I will readily concede
>>> that the denominator degrees of freedom reported by lmer are  
>>> wrong but
>>> so are the degrees of freedom reported by Software XYZ because there
>>> is no right answer (in general - in a few simple balanced designs
>>> there may be a right answer).
>>>
>>>>
>>>>> m.lm <- lm(Yvar~Xvar)
>>>>>
>>>>> anova(m.lm)
>>>> Analysis of Variance Table
>>>>
>>>> Response: Nadultos
>>>>             Df  Sum Sq Mean Sq F value Pr(>F)
>>>> Xvar         1    33.6    33.6  1.2352 0.2667
>>>> Residuals  878 23896.2    27.2
>>>>>
>>>>> summary(m.lm)
>>>>
>>>> Call:
>>>> lm(formula = Yvar ~ Xvar)
>>>>
>>>> Residuals:
>>>>     Min      1Q  Median      3Q     Max
>>>> -2.7455 -2.3545 -1.7455  0.2545 69.6455
>>>>
>>>> Coefficients:
>>>>                Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept)      2.3545     0.2487   9.467   <2e-16 ***
>>>> XvarFactor2      0.3909     0.3517   1.111    0.267
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Residual standard error: 5.217 on 878 degrees of freedom
>>>> Multiple R-Squared: 0.001405,   Adjusted R-squared: 0.0002675
>>>> F-statistic: 1.235 on 1 and 878 DF,  p-value: 0.2667
>>>>
>>>> I read the rnews about this use of the full DF in lmer, but I dont
>>>> undestand
>>>> this use with a gaussian error, I undestand this with glm data.
>>>>
>>>> I need more explanations, please.
>>>>
>>>> Thanks
>>>> Ronaldo
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>



From dieter.menne at menne-biomed.de  Sun Jan  1 14:50:03 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 1 Jan 2006 13:50:03 +0000 (UTC)
Subject: [R] Forward reference in Sweave
References: <LPEJLJACLINDNMBMFAFIGEMBCBAA.dieter.menne@menne-biomed.de>
Message-ID: <loom.20060101T144753-632@post.gmane.org>

Dieter Menne <dieter.menne <at> menne-biomed.de> writes:

> 
> Dear Rweavers,
> 
> When generating reports with Sweave, I would like to quote some results in
> the abstract (Something like "The treatment effect is 10 mbar, see page
> 100).
> 
> Currently, I used verbatimwrite and friends to write to multiple files to be
> included,  but I wonder is there is a more elegant method for such
> accumulated forward references in one file similar to toc creation.
> 

It is not straightforward, but Markus Kohm (from KOMA package) provided a 
solution on 31.12.2005:

http://groups.google.com/group/de.comp.text.tex/browse_frm/thread/1be8d4f989690d
1c/b430a3e347f05414#b430a3e347f05414


Dieter



From kjetilbrinchmannhalvorsen at gmail.com  Sun Jan  1 15:36:32 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Sun, 1 Jan 2006 15:36:32 +0100
Subject: [R] A comment about R:
Message-ID: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060101/eb3571e6/attachment.pl

From baron at psych.upenn.edu  Sun Jan  1 16:10:43 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 1 Jan 2006 10:10:43 -0500
Subject: [R] A comment about R:
In-Reply-To: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
Message-ID: <20060101151043.GA11316@psych.upenn.edu>

On 01/01/06 15:36, Kjetil Halvorsen wrote:
> Readers of this list might be interested in the following commenta about R.
> 
> 
> In a recent report, by Michael N. Mitchell
> http://www.ats.ucla.edu/stat/technicalreports/
> says about R: ...

Just a warning to others.  If you go to the site, it asks for
comments, but if you then ask for the LaTeX style file that is
required for sending comments, you get a message saying that the
service does not deal with those outside of UCLA.

Of course I think this is wrong wrong wrong.  It makes some
assumptions about "statisticians" being the ones who use
statistics programs.  But there are some researchers who like to
think of themselves as empirical scientists and who do not have
the kinds of humongous grants required to hire people to do
everything except write grant proposals.  People in these fields
often even do their own data analysis!  Moreover, unlike
statisticians (who consult with a great variety of researchers),
they usually do the same few types of analysis over and over, so
the learning time becomes relatively small, and the other
advantages of R become more compelling.  But I will try
eventually to say this as a comment on the paper itself.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From spencer.graves at pdf.com  Sun Jan  1 19:21:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 01 Jan 2006 10:21:56 -0800
Subject: [R] glmmPQL and variance structure
In-Reply-To: <43B17D9B.3080107@univ-fcomte.fr>
References: <43B17D9B.3080107@univ-fcomte.fr>
Message-ID: <43B81DC4.9070806@pdf.com>

	  Have you received a reply to this post?  I haven't seen one.  I don't 
have an answer for you, but if you'd still like help from this list, I 
suggest you prepare the simplest possible toy example that you can 
conceive and send it to this list, restating your question in terms of 
that example.  You question indicates you've read the code for glmmPQL 
and seem to know enough to experiment with modifying the "glmmPQL" code 
or with extracting crucial pieces to make part of the simplified example 
illustrating your question (consistent with the posting guide, 
"www.R-project.org/posting-guide.html").

	  If you are not sure what "glmmPQL" does, you can say "debug(glmmPQL)" 
before executing a command that invokes "glmmPQL".  That will open a 
browser that will allow you to look at and change anything in the 
environment of "glmmPQL" before and after any command;  if a command 
commits a fatal error, you will be evicted from "glmmPQL" and will have 
to start over.  This is the quickest way I know to understand and debug 
R code.

	  hope this helps.
	  spencer graves

Patrick Giraudoux wrote:

> Dear listers,
> 
> glmmPQL (package MASS) is given to work by repeated call to lme. In the 
> classical outputs glmmPQL  the Variance Structure is given  as " fixed 
> weights,  Formula: ~invwt".  The script shows that the function 
> varFixed() is used, though the place where 'invwt' is defined remains 
> unclear to me.  I wonder if there is an easy way to specify another 
> variance structure (eg varPower, etc..), preferably using an lme object 
> of the varFunc classes ? Some trials show that the 'weights' argument of 
> glmmPQL is just the same as in glm (which is clearly stated in the help) 
> and I wonder actually, if not a nonsense, how to pass eg a 'weights' 
> arguments as used in lme (eg weights=varPower()) to specify a variance 
> function (in the same way as a correlation structure can be passed easy).
> 
> Thanks in advance for any hint,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From hodgess at gator.dt.uh.edu  Sun Jan  1 20:07:25 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sun, 1 Jan 2006 13:07:25 -0600
Subject: [R]  S3 vs. S4
Message-ID: <200601011907.k01J7PFB012052@gator.dt.uh.edu>

Dear R People: 

Could someone direct me to some documentation on the
difference between S3 and S4 classes, please?

For example, why would a person use one as opposed to another?
Maybe pros and cons of each?

Thanks in advance!

R Version 2.2.0 (I'm downloading the new one this afternoon!) Windows.

Happy New Year!


Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From MSchwartz at mn.rr.com  Sun Jan  1 21:59:49 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sun, 01 Jan 2006 14:59:49 -0600
Subject: [R] S3 vs. S4
In-Reply-To: <200601011907.k01J7PFB012052@gator.dt.uh.edu>
References: <200601011907.k01J7PFB012052@gator.dt.uh.edu>
Message-ID: <1136149189.13210.20.camel@localhost.localdomain>

On Sun, 2006-01-01 at 13:07 -0600, Erin Hodgess wrote:
> Dear R People: 
> 
> Could someone direct me to some documentation on the
> difference between S3 and S4 classes, please?
> 
> For example, why would a person use one as opposed to another?
> Maybe pros and cons of each?
> 
> Thanks in advance!
> 
> R Version 2.2.0 (I'm downloading the new one this afternoon!) Windows.
> 
> Happy New Year!


Some places to start:

1. Fritz Leisch's "S4 Classes and Methods" from the useR! 2004 meeting:

   http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Leisch.pdf


2. Doug Bates' "Converting Packages to S4" in R News 3/1, 2003:

   http://cran.r-project.org/doc/Rnews/Rnews_2003-1.pdf


3. Thomas Lumley's "Programmer's Niche: A Simple Class, in S3 and S4"
   in R News 4/1, 2004:

   http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf




Also, if you do a Google search with the following:

  http://www.google.com/search?q=site:stat.ethz.ch+s3+s4

that will bring up some discussions (248 hits) in the list archives,
mainly in r-devel, some of which will be relevant to your question.



In addition, you might want to look at:

1. John M. Chambers. Programming with Data. Springer, New York, 1998.
   ISBN 0-387-98503-4.  AKA "The Green Book"


2. William N. Venables and Brian D. Ripley. S Programming. Springer,
   2000. ISBN 0-387-98966-8.


HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Sun Jan  1 22:06:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 1 Jan 2006 16:06:58 -0500
Subject: [R] S3 vs. S4
In-Reply-To: <200601011907.k01J7PFB012052@gator.dt.uh.edu>
References: <200601011907.k01J7PFB012052@gator.dt.uh.edu>
Message-ID: <971536df0601011306o1fb1361fi33d649d6137bf421@mail.gmail.com>

On 1/1/06, Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> Dear R People:
>
> Could someone direct me to some documentation on the
> difference between S3 and S4 classes, please?

Check out:

http://www.maths.lth.se/help/R/S3toS4/

and references there.


>
> For example, why would a person use one as opposed to another?
> Maybe pros and cons of each?
>

S3 is simpler (but still quite powerful) and can have higher
performance. S4 has more functionality including multiple
inheritance and type checking of arguments.

stats4 in the R distribution is an example of an S4 package.
You could also look at the its package which is an S4 package
for irregular time series and zoo which is an S3 package for
the same purpose (although they differ in a number of ways
even apart from the different oo infrastructures).

Bioconductor is a large set of packages that use S4.

> Thanks in advance!
>
> R Version 2.2.0 (I'm downloading the new one this afternoon!) Windows.
>
> Happy New Year!
>
>
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu



From john.maindonald at anu.edu.au  Mon Jan  2 01:16:56 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 2 Jan 2006 11:16:56 +1100
Subject: [R] R] lme X lmer results
In-Reply-To: <mailman.8.1136113201.9604.r-help@stat.math.ethz.ch>
References: <mailman.8.1136113201.9604.r-help@stat.math.ethz.ch>
Message-ID: <E914E5B0-3C20-4A3A-B481-63D9E049173C@anu.edu.au>

 From a quick look at the paper in the SAS proceedings, the simulations
seem limited to nested designs.  The major problems are with repeated
measures designs where the error structure is not compound symmetric,
which lme4 does not at present handle (unless I have missed something).
Such imbalance as was investigated was not a serious issue, at least for
the Kenward and Roger degree of freedom calculations.

The paper ends by commenting that "research should continue".  What
may be even more important is to educate users to think carefully about
any df that they are presented with, and to be especially sceptical when
designs are not approximately balanced nested designs and/or there are
repeated measures error structures that are not compound symmetric.

It is also necessary to consider how well the analysis reflects matters
on which there may be existing good evidence. Suppose in Ronaldo's
case that he'd previously run a number of experiments with very similar
plots and observation al units, and with comparable treatments and
outcome measures. If the plot 1 SD estimate (i.e., at the level of
experimental units) had never been larger than 0.01, with the SD for
observational units always in a range of 2 to 20, I'd take this as  
licence
to ignore the variance at the plot 1 level.  It would be nice to be  
able to
build in such prior information more formally, probably via a modified
version of mcmcsamp().

[Some people are never satisfied, You've written a great piece of
software, and users reward you by complaining that they want even
more!]

John Maindonald.


On 1 Jan 2006, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Dave Atkins <datkins at u.washington.edu>
> Date: 1 January 2006 1:40:45 AM

> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] lme X lmer results
>
>
>
> Message: 18
> Date: Fri, 30 Dec 2005 12:51:59 -0600
> From: Douglas Bates <dmbates at gmail.com>
> Subject: Re: [R] lme X lmer results
> To: John Maindonald <john.maindonald at anu.edu.au>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<40e66e0b0512301051i2dc0f257r745c70e749c250f0 at mail.gmail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> On 12/29/05, John Maindonald <john.maindonald at anu.edu.au> wrote:
>
> >> Surely there is a correct denominator degrees of freedom if the  
> design
> >> is balanced, as Ronaldo's design seems to be. Assuming that he has
> >> specified the design correctly to lme() and that lme() is  
> getting the df
> >> right, the difference is between 2 df and 878 df.  If the t- 
> statistic
> >> for the
> >> second level of Xvar had been 3.0 rather than 1.1, the difference
> >> would be between a t-statistic equal to 0.095 and 1e-6.  In a  
> design
> >> where there are 10 observations on each experimental unit, and all
> >> comparisons are at the level of experimental units or above, df for
> >> all comparisons will be inflated by a factor of at least 9.
>
> Doug Bates commented:
>
> I don't want to be obtuse and argumentative but I still am not
> convinced that there is a correct denominator degrees of freedom for
> _this_ F statistic.  I may be wrong about this but I think you are
> referring to an F statistic based on a denominator from a different
> error stratum, which is not what is being quoted.  (Those are not
> given because they don't generalize to unbalanced designs.)
>
> This is why I would like to see someone undertake a simulation study
> to compare various approaches to inference for the fixed effects terms
> in a mixed model, using realistic (i.e. unbalanced) examples.
>
> Doug--
>
> Here is a paper that focused on the various alternatives to  
> denominator degrees of freedom in SAS and does report some  
> simulation results:
>
> http://www2.sas.com/proceedings/sugi26/p262-26.pdf
>
> Not sure whether it argues convincingly one way or the other in the  
> present discussion.
>
> cheers, Dave
>
> -- 
> Dave Atkins, PhD
> datkins at u.washington.edu
>



From darrenleeweber at gmail.com  Mon Jan  2 04:19:45 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Sun, 1 Jan 2006 19:19:45 -0800
Subject: [R] RODBC help
Message-ID: <d2095b8c0601011919y58eab8eld9e4881805c41558@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060101/08862a55/attachment.pl

From darrenleeweber at gmail.com  Mon Jan  2 04:30:28 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Sun, 1 Jan 2006 19:30:28 -0800
Subject: [R] RODBC help
In-Reply-To: <d2095b8c0601011919y58eab8eld9e4881805c41558@mail.gmail.com>
References: <d2095b8c0601011919y58eab8eld9e4881805c41558@mail.gmail.com>
Message-ID: <d2095b8c0601011930t48dd1032r8efabffc54dd2276@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060101/ba4f8bf3/attachment.pl

From Eric.Kort at vai.org  Mon Jan  2 06:29:41 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Mon, 2 Jan 2006 00:29:41 -0500
Subject: [R] A comment about R:
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F697262@VAIEXCH04.vai.org>



>Kjetil Halvorsen wrote...
>
>Readers of this list might be interested in the following commenta about R.
>
>
>In a recent report, by Michael N. Mitchell
>http://www.ats.ucla.edu/stat/technicalreports/
>says about R:
>"Perhaps the most notable exception to this discussion is R, a language for
>statistical computing and graphics.
>
-------8<-----------------------------------------

After reading this commentary a couple of times, I can't quite figure 
out if he is damning with faint praise, or praising with faint damnation.

(For example, after observing how many researchers around me approach
statistical analysis, I'd say discouraging "casual" use is a _feature_.)

-Eric
This email message, including any attachments, is for the so...{{dropped}}



From darrenleeweber at gmail.com  Mon Jan  2 07:18:17 2006
From: darrenleeweber at gmail.com (Darren Weber)
Date: Sun, 1 Jan 2006 22:18:17 -0800
Subject: [R] RODBC help
In-Reply-To: <d2095b8c0601011919y58eab8eld9e4881805c41558@mail.gmail.com>
References: <d2095b8c0601011919y58eab8eld9e4881805c41558@mail.gmail.com>
Message-ID: <d2095b8c0601012218k4084743fu63196e3ddb2c8f34@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060101/a88deb3c/attachment.pl

From hodgess at gator.dt.uh.edu  Mon Jan  2 07:45:58 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Mon, 2 Jan 2006 00:45:58 -0600
Subject: [R]  S3 vs. S4
Message-ID: <200601020645.k026jwCh012326@gator.dt.uh.edu>

Dear R People:

Here is an answer to my own question:

I looked on regular google and found the  following:

http://www.biostat.jhsph.edu/~rpeng/R-classes-scope.pdf

It's great!

Thanks to Prof. Peng!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From szlevine at nana.co.il  Mon Jan  2 07:51:56 2006
From: szlevine at nana.co.il (Stephen)
Date: Mon, 2 Jan 2006 08:51:56 +0200
Subject: [R] Alternative to ARIMA?
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD675@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/5fbc27fc/attachment.pl

From mi2kelgrum at yahoo.com  Mon Jan  2 08:27:00 2006
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 1 Jan 2006 23:27:00 -0800 (PST)
Subject: [R] "7:9, 12:14" in dataframe to c(7:9, 12:14)
Message-ID: <20060102072700.6413.qmail@web60212.mail.yahoo.com>

I want to do something like df[df$b %in% df2[i, 2], ] 
where df$b is a numeric vector and df2[i, 2] is a
factor with 
levels like "7:9, 12:14". For example:

a <- c(paste("A", 1:10, sep = ""), paste("B", 1:10,
sep = ""))
b <- 1:20
df <- as.data.frame(cbind(a, b))
df$b <- as.numeric(levels(df$b))[as.integer(df$b)]

f <- c("X", "Y", "Z")
g <- c("1:6", "7:9, 12:14", "18")
df2 <- as.data.frame(cbind(f, g))

i <- 2
df[df$b %in% df2[i, 2], ] # or
df[df$b %in% levels(df2)[as.integer(df2[i, 2])], ] #
this is the closest I've got

The results I want is given by 
> df[df$b %in% c(7:9, 12:14), ]
    a  b
7  A7  7
8  A8  8
9  A9  9
12 B2 12
13 B3 13
14 B4 14

Can it be done?

Mikkel Grum



From sumantab at ambaresearch.com  Mon Jan  2 08:51:18 2006
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Mon, 2 Jan 2006 13:21:18 +0530
Subject: [R] Use Of makeARIMA
Message-ID: <14850601FF012647A90A5DB31F96DB3731396D@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/edd1d21b/attachment.pl

From DrJones at alum.MIT.edu  Mon Jan  2 09:55:29 2006
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Mon, 2 Jan 2006 03:55:29 -0500
Subject: [R] An embarrassment of riches
Message-ID: <000301c60f7a$4876cd90$2f01a8c0@DrJones>

I have a dataset which I am trying to smooth, using locally weighted 
regression. The y values are count data, integers with Poisson 
distribution, and it is important for the regression function to know 
this, since assuming a Gaussian distribution will lead to substantial 
errors. It is a time series; the x values have equal five minute 
intervals.

Here is the problem: I have an embarrassment of riches. Unless I am 
mistaken, the following R packages will do this: locfit, aws, sm, gss, 
semipar, pgam, gregmisc, to name just a few. Questions: Which package 
should I use, and which function in the package should I use for the 
regression?

Here is the Google search result, restricted to r-project.org:

http://www.google.com/search?as_q=locally+weighted+poisson+regression&num=10&hl=en&btnG=Google+Search&as_epq=&as_oq=&as_eq=&lr=&as_ft=i&as_filetype=&as_qdr=all&as_occt=any&as_dt=i&as_sitesearch=r-project.org&as_rights=&safe=images

If the above hyperlink does not work, please try 
http://tinyurl.com/exw7e

Doc



From petr.pikal at precheza.cz  Mon Jan  2 10:29:10 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 02 Jan 2006 10:29:10 +0100
Subject: [R] "7:9, 12:14" in dataframe to c(7:9, 12:14)
In-Reply-To: <20060102072700.6413.qmail@web60212.mail.yahoo.com>
Message-ID: <43B90076.15481.E9598C@localhost>

Hi

I knew I encountered it somewhere. Looking into my archives gave me 
an answer of similar question by Prof.Ripley

So here you are

df[eval(parse(text=paste("c(",df2[2,"g"], ")"))),]

?eval
?parse

HTH
Petr



On 1 Jan 2006 at 23:27, Mikkel Grum wrote:

Date sent:      	Sun, 1 Jan 2006 23:27:00 -0800 (PST)
From:           	Mikkel Grum <mi2kelgrum at yahoo.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] "7:9, 12:14" in dataframe to c(7:9, 12:14)

> I want to do something like df[df$b %in% df2[i, 2], ] 
> where df$b is a numeric vector and df2[i, 2] is a
> factor with 
> levels like "7:9, 12:14". For example:
> 
> a <- c(paste("A", 1:10, sep = ""), paste("B", 1:10,
> sep = ""))
> b <- 1:20
> df <- as.data.frame(cbind(a, b))
> df$b <- as.numeric(levels(df$b))[as.integer(df$b)]
> 
> f <- c("X", "Y", "Z")
> g <- c("1:6", "7:9, 12:14", "18")
> df2 <- as.data.frame(cbind(f, g))
> 
> i <- 2
> df[df$b %in% df2[i, 2], ] # or
> df[df$b %in% levels(df2)[as.integer(df2[i, 2])], ] #
> this is the closest I've got
> 
> The results I want is given by 
> > df[df$b %in% c(7:9, 12:14), ]
>     a  b
> 7  A7  7
> 8  A8  8
> 9  A9  9
> 12 B2 12
> 13 B3 13
> 14 B4 14
> 
> Can it be done?
> 
> Mikkel Grum
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From phgrosjean at sciviews.org  Mon Jan  2 12:00:08 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 02 Jan 2006 12:00:08 +0100
Subject: [R] A comment about R:
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352F697262@VAIEXCH04.vai.org>
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
	<CEA39A213F7F2E44A0DED9210BCD352F697262@VAIEXCH04.vai.org>
Message-ID: <43B907B8.3020505@sciviews.org>

Kort, Eric wrote:
> 
>>Kjetil Halvorsen wrote...
>>
>>Readers of this list might be interested in the following commenta about R.
>>
>>
>>In a recent report, by Michael N. Mitchell
>>http://www.ats.ucla.edu/stat/technicalreports/
>>says about R:
>>"Perhaps the most notable exception to this discussion is R, a language for
>>statistical computing and graphics.
>>
> 
> -------8<-----------------------------------------
> 
> After reading this commentary a couple of times, I can't quite figure 
> out if he is damning with faint praise, or praising with faint damnation.
> 
> (For example, after observing how many researchers around me approach
> statistical analysis, I'd say discouraging "casual" use is a _feature_.)

There are numerous reasons why people tend to consider R as too 
complicate for them (or even worse, say peremptively to others that R is 
too complicate for them!). But one must decrypt the real reasons behind 
what they say. Mostly, it is because R imposes to think about the 
analysis we are doing. As Eric says, it is a _feature_ (well, not 
discouraging "casual" use, but forcing to think about what we do, which 
in turn forces to learn R a little deeper to get results... which in 
turn may discourage casual users, as an unwanted side-effect). According 
to my own experience with teaching to students and to advanced 
scientists in different environments (academic, industry, etc.), the 
main basic reason why people are reluctant to use R is lazyness. People 
are lazy by nature. They like course where they just sit and snooze. 
Unfortunatelly, this is not the right way to learn R: you have to dwell 
on the abondant litterature about R and experiment by yourself to become 
a good R user. This is the kind of thing people do not like at all! 
Someone named Dr Brian Ripley wrote once something like:
"`They' did write documentation that told you [...], but `they'
can't read it for you."

It is already many years that I write and use tools supposed to help 
beginners to master R: menu/dialog boxes approach, electronic reference 
cards, graphical object explorer, code tips, completion lists, etc... 
Everytime I got the same result: either these tools are badly designed 
because they hide the 'horrible code' those casual users don't want to 
see, and they make them *happy bad R users*, or they still force them to 
write code and think at what they exactly do (but just help them a bit), 
and they make them *good R users, but unhappy, poor, tortured 
beginners*! So, I tend to agree now: there is probably no way to instil 
R into lazy and reluctant minds.

That said, I think one should interpret Mitchell's paper in a different 
way. Obviously, he is an unconditional and happy Stata user (he even 
wrote a book about graphs programming in Stata). His claim in favor of 
Stata (versus SAS and SPSS, and also, indirectly, versus R) is to be 
interpreted the same way as unconditional lovers of Macintoshes or PCs 
would argue against the other clan. Both architectures are good and have 
strengths and weaknesses. Real arguments are more sentimental, and could 
resume in: "The more I use it, the more I like it,... and the aliens are 
bad, ugly and stupid!" Would this apply to Stata versus R? I don't know 
Stata at all, but I imagine it could be the case from what I read in 
Mitchell's paper...
Best,

Philippe

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................



From kjetilbrinchmannhalvorsen at gmail.com  Mon Jan  2 12:26:16 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Mon, 2 Jan 2006 12:26:16 +0100
Subject: [R] An embarrassment of riches
In-Reply-To: <000301c60f7a$4876cd90$2f01a8c0@DrJones>
References: <000301c60f7a$4876cd90$2f01a8c0@DrJones>
Message-ID: <556e90a80601020326v73d602e2y48bbc06f58555916@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/2d7637bc/attachment.pl

From eesteves at ualg.pt  Mon Jan  2 13:06:16 2006
From: eesteves at ualg.pt (Eduardo Esteves)
Date: Mon, 2 Jan 2006 12:06:16 -0000
Subject: [R] How to create "special" (source) file
Message-ID: <000601c60f94$ef3a9c50$3f8f0a0a@ualgestadea>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/6abb5525/attachment.pl

From ked at nilu.no  Mon Jan  2 13:53:32 2006
From: ked at nilu.no (Kare Edvardsen)
Date: Mon, 02 Jan 2006 13:53:32 +0100
Subject: [R] Plotting the mean of data
Message-ID: <43B9224C.6000805@nilu.no>

Hi all!

I've got a datstructure like this:

subject  week  value

1	1	4
2	1	8
3	1	3
4	1	5

1	2	5
2	2	6
3	2	2
4	2	6

1	3	3
2	3	7
3	3	3
4	3	7

I'd like to plot the mean of 'value' against week. Is there a direct way 
of doing this or do I have to make a new structure with the calculated 
values and then plot it?

All the best!

-- 
###########################################
Kare Edvardsen <kare.edvardsen at nilu.no>
Norwegian Institute for Air Research (NILU)
Polarmiljosenteret
NO-9296 Tromso       http://www.nilu.no
Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
###########################################



From ripley at stats.ox.ac.uk  Mon Jan  2 14:00:52 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jan 2006 13:00:52 +0000 (GMT)
Subject: [R] How to create "special" (source) file
In-Reply-To: <000601c60f94$ef3a9c50$3f8f0a0a@ualgestadea>
References: <000601c60f94$ef3a9c50$3f8f0a0a@ualgestadea>
Message-ID: <Pine.LNX.4.61.0601021254060.1707@gannet.stats>

On Mon, 2 Jan 2006, Eduardo Esteves wrote:

> Dear All,
>
> I'm a Marine Biologist using R (in a most informal and applied way) for 
> a couple of years. Actually, I've been using R to analyse the results 
> supporting my thesis. I would like to put together (in the same file) 
> the data (.txt files) and the scripts files (.R files) for each chapter. 
> How can I do this?

With some difficulty.  What I suggest would be a better idea is to read 
in your data files and dump() then, then edit the dumps into the scripts.
However, you can do things like

## preamble
galaxies <- scan()
  9172  9350  9483  9558  9775 10227 10406 16084 16170 18419 18552 18600
18927 19052 19070 19330 19343 19349 19440 19473 19529 19541 19547 19663
19846 19856 19863 19914 19918 19973 19989 20166 20175 20179 20196 20215
20221 20415 20629 20795 20821 20846 20875 20986 21137 21492 21701 21814
21921 21960 22185 22209 22242 22249 22314 22374 22495 22746 22747 22888
22914 23206 23241 23263 23484 23538 23542 23666 23706 23711 24129 24285
24289 24366 24717 24990 25633 26690 26995 32065 32789 34279

hist(galaxies)

but you can't use source() on such a script.

Perhaps an even better idea is to put together a small package which has 
your data in mypkg/data and the scripts in mypkg/inst/scripts.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jholtman at gmail.com  Mon Jan  2 14:27:29 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 2 Jan 2006 08:27:29 -0500
Subject: [R] Plotting the mean of data
In-Reply-To: <43B9224C.6000805@nilu.no>
References: <43B9224C.6000805@nilu.no>
Message-ID: <644e1f320601020527g776b58b3k154a74d3649049e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/0516070c/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jan  2 14:32:08 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Jan 2006 14:32:08 +0100
Subject: [R] Plotting the mean of data
In-Reply-To: <43B9224C.6000805@nilu.no>
References: <43B9224C.6000805@nilu.no>
Message-ID: <43B92B58.5000206@statistik.uni-dortmund.de>

Kare Edvardsen wrote:

> Hi all!
> 
> I've got a datstructure like this:
> 
> subject  week  value
> 
> 1	1	4
> 2	1	8
> 3	1	3
> 4	1	5
> 
> 1	2	5
> 2	2	6
> 3	2	2
> 4	2	6
> 
> 1	3	3
> 2	3	7
> 3	3	3
> 4	3	7
> 
> I'd like to plot the mean of 'value' against week. Is there a direct way 
> of doing this or do I have to make a new structure with the calculated 
> values and then plot it?

For your data.frame X, you want, e.g.:

   A <- aggregate(X, list(X$week), mean)
   plot(A$week, A$value)

Uwe Ligges

> All the best!
>



From Marjo.Pyy-Martikainen at stat.fi  Mon Jan  2 14:52:00 2006
From: Marjo.Pyy-Martikainen at stat.fi (Pyy-Martikainen Marjo)
Date: 2 Jan 2006 15:52:00 +0200
Subject: [R] Variance of expected survival based on a Cox model with frailty
Message-ID: <JA8AAAAAApZt0wABYQABU5AQ2thU@postila.stat.fi>


Dear R users,

I am interested in the expected survival probabilities of an "example person",
calculated on the basis of a Cox model with shared frailty. The example person
has covariate values equal to zero and a frailty term equal to its expected
value. I calculate the expected survival probabilities in the following way:

fit1=coxph(...)
base1=basehaz(fit1,centered=F)
surv1=exp(-base1$hazard).

Then I would like to calculate the variance of the expected survival
probability but
it seems to be a bit tricky.
Has anybody calculated the variance of expected survival probability and
willing to share the code?

Thank you in advance for any help.

Marjo Pyy-Martikainen



From ggrothendieck at gmail.com  Mon Jan  2 14:56:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jan 2006 08:56:54 -0500
Subject: [R] How to create "special" (source) file
In-Reply-To: <000601c60f94$ef3a9c50$3f8f0a0a@ualgestadea>
References: <000601c60f94$ef3a9c50$3f8f0a0a@ualgestadea>
Message-ID: <971536df0601020556i24decfe2k83e81739c49873ec@mail.gmail.com>

1. You can use dput.  Suppose x is a data frame.  Then dput(x)
will output x as R code.   e.g.

> dput(head(iris))
structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4),
    Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4,
    1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2,
    0.2, 0.2, 0.4), Species = structure(c(1, 1, 1, 1, 1, 1), .Label =
c("setosa",
    "versicolor", "virginica"), class = "factor")), .Names = c("Sepal.Length",
"Sepal.Width", "Petal.Length", "Petal.Width", "Species"), row.names = c("1",
"2", "3", "4", "5", "6"), class = "data.frame")

You can now copy and paste iris.head <- ...the output you just got...
into your script, i.e. you can now put this into your script:

iris.head <- structure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4),
    Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4,
    1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2,
    0.2, 0.2, 0.4), Species = structure(c(1, 1, 1, 1, 1, 1), .Label =
c("setosa",
    "versicolor", "virginica"), class = "factor")), .Names = c("Sepal.Length",
"Sepal.Width", "Petal.Length", "Petal.Width", "Species"), row.names = c("1",
"2", "3", "4", "5", "6"), class = "data.frame")


2. Another more readable way is to do this:

Lines <- "Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          5.1         3.5          1.4         0.2  setosa
          4.9         3.0          1.4         0.2  setosa
          4.7         3.2          1.3         0.2  setosa
          4.6         3.1          1.5         0.2  setosa
          5.0         3.6          1.4         0.2  setosa
          5.4         3.9          1.7         0.4  setosa"
iris.head <- read.table(textConnection(Lines), header = TRUE)


On 1/2/06, Eduardo Esteves <eesteves at ualg.pt> wrote:
> Dear All,
>
> I'm a Marine Biologist using R (in a most informal and applied way) for a couple of years. Actually, I've been using R to analyse the results supporting my thesis. I would like to put together (in the same file) the data (.txt files) and the scripts files (.R files) for each chapter. How can I do this?
>
> Thanks in advance, Eduardo Esteves



From antoniou at central.ntua.gr  Mon Jan  2 15:41:26 2006
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Mon, 2 Jan 2006 16:41:26 +0200
Subject: [R] mixed effects models - negative binomial family?
Message-ID: <F318F3CA-27B7-4400-9853-5BF65DD9677A@central.ntua.gr>

Hello all,

I would like to fit a mixed effects model, but my response is of the  
negative binomial (or overdispersed poisson) family. The only (?)  
package that looks like it can do this is glmm.ADMB (but it cannot  
run on Mac OS X - please correct me if I am wrong!) [1]

I think that glmmML {glmmML}, lmer {Matrix}, and glmmPQL {MASS} do  
not provide this "family" (i.e. nbinom, or overdispersed poisson). Is  
there any other package that offers this functionality?

Thanking you in advance,

Costas


[1] Yes, I know I can use this on another OS. But it is kind of a  
nuisance, as I have my whole workflow setup on a mac, including emacs 
+ess, the data etc etc. It will be non-trivial to start moving/ 
syncing files between >1 computers, in order to use this package...

--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From bolker at zoo.ufl.edu  Mon Jan  2 16:22:04 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 2 Jan 2006 15:22:04 +0000 (UTC)
Subject: [R] mixed effects models - negative binomial family?
References: <F318F3CA-27B7-4400-9853-5BF65DD9677A@central.ntua.gr>
Message-ID: <loom.20060102T161437-593@post.gmane.org>

 
 
Constantinos Antoniou <antoniou <at> central.ntua.gr> writes:

> 
> Hello all,
> 
> I would like to fit a mixed effects model, but my response is of the  
> negative binomial (or overdispersed poisson) family. The only (?)  
> package that looks like it can do this is glmm.ADMB (but it cannot  
> run on Mac OS X - please correct me if I am wrong!) [1]
> 
> I think that glmmML {glmmML}, lmer {Matrix}, and glmmPQL {MASS} do  
> not provide this "family" (i.e. nbinom, or overdispersed poisson). Is  
> there any other package that offers this functionality?

 You'll probably get more complete/informed information
shortly, but ... you may not be able to get a negative
binomial distribution per se, but other versions
of "overdispersed Poisson" are indeed possible.  glmmPQL
will let you use the quasipoisson family, which allows for
overdispersion in a phenomenological way; more mechanistically,
observation-level random effects on the scale of the
linear predictor (log for a GLMM with family=poisson)
lead to a lognormal-Poisson distribution, which has similar
properties to the NB.  I suspect you can do this in lmer
(lme4 package), which does GLMMs if you specify the family
argument.

See:

http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=11393830&dopt=Abstract
(analysis done in SAS but probably completely feasible in R at this point)

  Ben



From ggrothendieck at gmail.com  Mon Jan  2 16:59:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jan 2006 10:59:10 -0500
Subject: [R] A comment about R:
In-Reply-To: <43B907B8.3020505@sciviews.org>
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
	<CEA39A213F7F2E44A0DED9210BCD352F697262@VAIEXCH04.vai.org>
	<43B907B8.3020505@sciviews.org>
Message-ID: <971536df0601020759i4b9ec009l2d73fa6811113ddc@mail.gmail.com>

On 1/2/06, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> Kort, Eric wrote:
> >
> >>Kjetil Halvorsen wrote...
> >>
> >>Readers of this list might be interested in the following commenta about R.
> >>
> >>
> >>In a recent report, by Michael N. Mitchell
> >>http://www.ats.ucla.edu/stat/technicalreports/
> >>says about R:
> >>"Perhaps the most notable exception to this discussion is R, a language for
> >>statistical computing and graphics.
> >>
> >
> > -------8<-----------------------------------------
> >
> > After reading this commentary a couple of times, I can't quite figure
> > out if he is damning with faint praise, or praising with faint damnation.
> >
> > (For example, after observing how many researchers around me approach
> > statistical analysis, I'd say discouraging "casual" use is a _feature_.)
>
> There are numerous reasons why people tend to consider R as too
> complicate for them (or even worse, say peremptively to others that R is
> too complicate for them!). But one must decrypt the real reasons behind
> what they say. Mostly, it is because R imposes to think about the
> analysis we are doing. As Eric says, it is a _feature_ (well, not
> discouraging "casual" use, but forcing to think about what we do, which
> in turn forces to learn R a little deeper to get results... which in
> turn may discourage casual users, as an unwanted side-effect). According
> to my own experience with teaching to students and to advanced
> scientists in different environments (academic, industry, etc.), the
> main basic reason why people are reluctant to use R is lazyness. People
> are lazy by nature. They like course where they just sit and snooze.
> Unfortunatelly, this is not the right way to learn R: you have to dwell
> on the abondant litterature about R and experiment by yourself to become
> a good R user. This is the kind of thing people do not like at all!
> Someone named Dr Brian Ripley wrote once something like:
> "`They' did write documentation that told you [...], but `they'
> can't read it for you."
>
> It is already many years that I write and use tools supposed to help
> beginners to master R: menu/dialog boxes approach, electronic reference
> cards, graphical object explorer, code tips, completion lists, etc...
> Everytime I got the same result: either these tools are badly designed
> because they hide the 'horrible code' those casual users don't want to
> see, and they make them *happy bad R users*, or they still force them to
> write code and think at what they exactly do (but just help them a bit),
> and they make them *good R users, but unhappy, poor, tortured
> beginners*! So, I tend to agree now: there is probably no way to instil
> R into lazy and reluctant minds.
>
> That said, I think one should interpret Mitchell's paper in a different
> way. Obviously, he is an unconditional and happy Stata user (he even
> wrote a book about graphs programming in Stata). His claim in favor of
> Stata (versus SAS and SPSS, and also, indirectly, versus R) is to be
> interpreted the same way as unconditional lovers of Macintoshes or PCs
> would argue against the other clan. Both architectures are good and have
> strengths and weaknesses. Real arguments are more sentimental, and could
> resume in: "The more I use it, the more I like it,... and the aliens are
> bad, ugly and stupid!" Would this apply to Stata versus R? I don't know
> Stata at all, but I imagine it could be the case from what I read in
> Mitchell's paper...

Probably what is needed is for someone familiar with both Stata and R
to create a lexicon in the vein of the Octave to R lexicon

   http://cran.r-project.org/doc/contrib/R-and-octave-2.txt

to make it easier for Stata users to understand R.  Ditto for SAS and SPSS.



From szlevine at nana.co.il  Mon Jan  2 17:19:14 2006
From: szlevine at nana.co.il (Stephen)
Date: Mon, 2 Jan 2006 18:19:14 +0200
Subject: [R] ARIMA?
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD678@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/ecdc9817/attachment.pl

From 042045003 at fudan.edu.cn  Mon Jan  2 17:16:35 2006
From: 042045003 at fudan.edu.cn (ronggui)
Date: Tue, 03 Jan 2006 00:16:35 +0800
Subject: [R] A comment about R:
Message-ID: <0ISH00AE63DVAE@mail.fudan.edu.cn>

That's a good idea.
I will try to give a lexicon on Stata vs R.
	

======= 2006-01-02 23:59:10 =======

>On 1/2/06, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>> Kort, Eric wrote:
>> >
>> >>Kjetil Halvorsen wrote...
>> >>
>> >>Readers of this list might be interested in the following commenta about R.
>> >>
>> >>
>> >>In a recent report, by Michael N. Mitchell
>> >>http://www.ats.ucla.edu/stat/technicalreports/
>> >>says about R:
>> >>"Perhaps the most notable exception to this discussion is R, a language for
>> >>statistical computing and graphics.
>> >>
>> >
>> > -------8<-----------------------------------------
>> >
>> > After reading this commentary a couple of times, I can't quite figure
>> > out if he is damning with faint praise, or praising with faint damnation.
>> >
>> > (For example, after observing how many researchers around me approach
>> > statistical analysis, I'd say discouraging "casual" use is a _feature_.)
>>
>> There are numerous reasons why people tend to consider R as too
>> complicate for them (or even worse, say peremptively to others that R is
>> too complicate for them!). But one must decrypt the real reasons behind
>> what they say. Mostly, it is because R imposes to think about the
>> analysis we are doing. As Eric says, it is a _feature_ (well, not
>> discouraging "casual" use, but forcing to think about what we do, which
>> in turn forces to learn R a little deeper to get results... which in
>> turn may discourage casual users, as an unwanted side-effect). According
>> to my own experience with teaching to students and to advanced
>> scientists in different environments (academic, industry, etc.), the
>> main basic reason why people are reluctant to use R is lazyness. People
>> are lazy by nature. They like course where they just sit and snooze.
>> Unfortunatelly, this is not the right way to learn R: you have to dwell
>> on the abondant litterature about R and experiment by yourself to become
>> a good R user. This is the kind of thing people do not like at all!
>> Someone named Dr Brian Ripley wrote once something like:
>> "`They' did write documentation that told you [...], but `they'
>> can't read it for you."
>>
>> It is already many years that I write and use tools supposed to help
>> beginners to master R: menu/dialog boxes approach, electronic reference
>> cards, graphical object explorer, code tips, completion lists, etc...
>> Everytime I got the same result: either these tools are badly designed
>> because they hide the 'horrible code' those casual users don't want to
>> see, and they make them *happy bad R users*, or they still force them to
>> write code and think at what they exactly do (but just help them a bit),
>> and they make them *good R users, but unhappy, poor, tortured
>> beginners*! So, I tend to agree now: there is probably no way to instil
>> R into lazy and reluctant minds.
>>
>> That said, I think one should interpret Mitchell's paper in a different
>> way. Obviously, he is an unconditional and happy Stata user (he even
>> wrote a book about graphs programming in Stata). His claim in favor of
>> Stata (versus SAS and SPSS, and also, indirectly, versus R) is to be
>> interpreted the same way as unconditional lovers of Macintoshes or PCs
>> would argue against the other clan. Both architectures are good and have
>> strengths and weaknesses. Real arguments are more sentimental, and could
>> resume in: "The more I use it, the more I like it,... and the aliens are
>> bad, ugly and stupid!" Would this apply to Stata versus R? I don't know
>> Stata at all, but I imagine it could be the case from what I read in
>> Mitchell's paper...
>
>Probably what is needed is for someone familiar with both Stata and R
>to create a lexicon in the vein of the Octave to R lexicon
>
>   http://cran.r-project.org/doc/contrib/R-and-octave-2.txt
>
>to make it easier for Stata users to understand R.  Ditto for SAS and SPSS.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			


 

2006-01-03

------
Deparment of Sociology
Fudan University

My new mail addres is ronggui.huang at gmail.com
Blog:http://sociology.yculblog.com



From gruchti at pha.jhu.edu  Mon Jan  2 18:29:01 2006
From: gruchti at pha.jhu.edu (Gregory Ruchti)
Date: Mon, 2 Jan 2006 12:29:01 -0500 (EST)
Subject: [R] boostrap astronomy problem
Message-ID: <Pine.SOL.4.58.0601021222150.24581@eta.pha.jhu.edu>

Hi,

I am an astronomer and somewhat new to boostrap statistics.  I understand
the basic idea of bootstrap resampling, but am uncertain if it would be
useful in my case or not.  My problem consists of maximizing a likelihood
function based on the velocities of a number of stars.  My assumed
distribution of velocities of these stars is:
---------------------------------------------------------------------------
dmy=function(x,v,k,t)(k+1)/(v-t)^(k+1)*(v-x)^k
---------------------------------------------------------------------------
where x would be my stellar velocities.  (Essentially it is a beta
distribution.)
---------------------------------------------------------------------------
My likelihood function looks something like this:

lm<-function(x){
	e<-x[1]
	k<-x[2]
	log(e) - n*log(k+1) + (k+1)*n*log(e-t)-k*sum(log(e-vg))
}
--------------------------------------------------------------------------
The quantities n and t are known, and vg is my velocity data.  I am
minimizing this function using the function "optim" (BFGS option) to find
k and e that minimize this.  Also, my data set is small, only about 50
stars.  Therefore, I was thinking that I could use the boot function to
resample my data and solve the minimization for each resample.  This way I
believe I'll get better estimates for standard errors and confidence
intervals.  Is it safe to assume that the distributions for k and e are
approximately Normal, therefore making the bootstrap useful?  I have
actually used the boot function with this set up:
--------------------------------------------------------------------------
mystat=function(s,b){
	#Negative Log Likelihood Function
	lm<-function(x){
   		e<-x[1]
   		k<-x[2]
   		log(e) - n*log(k+1) + (k+1)*n*log(e-t) -
			k*sum(log(e-s[b]))
	}

	#Gradient of Negative Likelihood Function
	glm=function(x){
   		e<-x[1]
   		k<-x[2]
   		c(1/e + (k+1)*(n/(e-t)) - k*sum(1/(e-s[b])),-n/(k+1) +
			n*log(e-t) - sum(log(e-s[b])))
	}


optim(c(480.,2.),lm,glm,method="BFGS",control=list(maxit=10000000))$par
}

#Compute Bootstrap replicates of escape velocity and kr
m2B2=boot(vg,mystat,5000)
------------------------------------------------------------------------
Does this appear to be correct for what I'd like to achieve?  I have
looked at the distribution and it appears to be about Normal, but can I
say that this is true for the sampling distribution as well?  Also, the
bootstrap distribution is fairly biased, should I be using "bca" or tilted
bootstrap confidence intervals?  If so, I am having some trouble getting
the tilted bootstrap to work.  Specifically, it is having trouble finding
"multipliers".

Also, should I be in some way taking into account my velocity distribution
when resampling?  Any suggestions would be very helpful, thanks.

Thank you for your time.

Greg Ruchti

--
Gregory Ruchti
Bloomberg Center for Physics and Astronomy
Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218-1216

gruchti at pha.jhu.edu
Tel: (410)516-8520



From pmuhl1848 at gmail.com  Mon Jan  2 19:13:50 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Mon, 02 Jan 2006 13:13:50 -0500
Subject: [R] Bootstrap w/ Clustered Data
In-Reply-To: <mailman.9.1136113201.9604.r-help@stat.math.ethz.ch>
Message-ID: <BFDED78E.1276A%pmuhl1848@gmail.com>

Are there any functions in R for running bootstraps with clustered (as
opposed to stratified) data?  I can't seem to find anything obvious in boot
or Bootstrap, though I imagine boot can be manipulated to resample from
clusters.  Is that what people use?

I do see some cluster bootstrap resampling in glsD, but I need
non-parameteric resampling & the capacity to run a sem model.

Thanks in advance,

Peter



From ripley at stats.ox.ac.uk  Mon Jan  2 19:19:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jan 2006 18:19:19 +0000 (GMT)
Subject: [R] mixed effects models - negative binomial family?
In-Reply-To: <loom.20060102T161437-593@post.gmane.org>
References: <F318F3CA-27B7-4400-9853-5BF65DD9677A@central.ntua.gr>
	<loom.20060102T161437-593@post.gmane.org>
Message-ID: <Pine.LNX.4.61.0601021817520.19500@gannet.stats>

On Mon, 2 Jan 2006, Ben Bolker wrote:

>
>
> Constantinos Antoniou <antoniou <at> central.ntua.gr> writes:
>
>>
>> Hello all,
>>
>> I would like to fit a mixed effects model, but my response is of the
>> negative binomial (or overdispersed poisson) family. The only (?)
>> package that looks like it can do this is glmm.ADMB (but it cannot
>> run on Mac OS X - please correct me if I am wrong!) [1]
>>
>> I think that glmmML {glmmML}, lmer {Matrix}, and glmmPQL {MASS} do
>> not provide this "family" (i.e. nbinom, or overdispersed poisson). Is
>> there any other package that offers this functionality?
>
> You'll probably get more complete/informed information
> shortly, but ... you may not be able to get a negative
> binomial distribution per se, but other versions
> of "overdispersed Poisson" are indeed possible.  glmmPQL
> will let you use the quasipoisson family, which allows for
> overdispersion in a phenomenological way;

and has worked examples of this in the book it supports.

It also lets you use a negative binomial family, and MASS provides one.

> more mechanistically,
> observation-level random effects on the scale of the
> linear predictor (log for a GLMM with family=poisson)
> lead to a lognormal-Poisson distribution, which has similar
> properties to the NB.  I suspect you can do this in lmer
> (lme4 package), which does GLMMs if you specify the family
> argument.
>
> See:
>
> http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=11393830&dopt=Abstract
> (analysis done in SAS but probably completely feasible in R at this point)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rkoenker at uiuc.edu  Mon Jan  2 19:51:08 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 2 Jan 2006 12:51:08 -0600
Subject: [R] update?
Message-ID: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>

I'm having problems with environments and update() that
I expect have a simple explanation.  To illustrate, suppose
I wanted to make a very primitive Tukey one-degree-of-
freedom for nonadditivity test and naively wrote:

nonadd <- function(formula){
         f <- lm(formula)
         v <- f$fitted.values^2
         g <- update(f, . ~ . + v)
         anova(f,g)
         }

x <- rnorm(20)
y <- rnorm(20)
nonadd(y ~ x)

Evidently, update is looking in the environment producingf f and
doesn't find v, so I get:

Error in eval(expr, envir, enclos) : Object "v" not found

This may (or may not) be related to the discussion at:
http://bugs.r-project.org/cgi-bin/R/Models?id=1861;user=guest

but in any case I hope that someone can suggest how such
difficulties can be circumvented.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820



From billemont at cegetel.net  Mon Jan  2 20:12:45 2006
From: billemont at cegetel.net (billemont@cegetel.net)
Date: Mon, 2 Jan 2006 20:12:45 +0100
Subject: [R] cox model test heterogeinity
Message-ID: <4e623859f1739711804cda2acbf54fa8@cegetel.net>

  I'm a young physician and i work on the breast cancer. I made a cox
     model and i want to do a test of heterogeneity. Do you know how i 
can
     do this with R. I'm sorry for this question wich seems to be easy 
for
     you. I read the manual and the help and i don't find it.
     Thanks for your help
     Bertrand  Billemont



From ripley at stats.ox.ac.uk  Mon Jan  2 21:18:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jan 2006 20:18:59 +0000 (GMT)
Subject: [R] update?
In-Reply-To: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>
References: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>
Message-ID: <Pine.LNX.4.61.0601022011320.5586@gannet.stats>

R was changed 1.2.0 to look for variables in the environment of formula 
(see ?model.frame and ?formula). So either use data() or put v there as in

nonadd <- function(formula)
{
      f <- lm(formula)
      v <- f$fitted.values^2
      assign("v", v, envir=environment(formula))
      g <- update(f, . ~ . + v)
      anova(f,g)
}

or (I think preferable)

nonadd <- function(formula)
{
      f <- lm(formula)
      m <- model.frame(f)
      m$v <- f$fitted.values^2
      g <- update(f, . ~ . + v, data = m)
      anova(f,g)
}


On Mon, 2 Jan 2006, roger koenker wrote:

> I'm having problems with environments and update() that
> I expect have a simple explanation.  To illustrate, suppose
> I wanted to make a very primitive Tukey one-degree-of-
> freedom for nonadditivity test and naively wrote:
>
> nonadd <- function(formula){
>         f <- lm(formula)
>         v <- f$fitted.values^2
>         g <- update(f, . ~ . + v)
>         anova(f,g)
>         }
>
> x <- rnorm(20)
> y <- rnorm(20)
> nonadd(y ~ x)
>
> Evidently, update is looking in the environment producingf f and
> doesn't find v, so I get:
>
> Error in eval(expr, envir, enclos) : Object "v" not found
>
> This may (or may not) be related to the discussion at:
> http://bugs.r-project.org/cgi-bin/R/Models?id=1861;user=guest
>
> but in any case I hope that someone can suggest how such
> difficulties can be circumvented.
>
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pmilin at ff.ns.ac.yu  Mon Jan  2 21:56:18 2006
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Mon, 02 Jan 2006 21:56:18 +0100
Subject: [R] Problems with updating R-packages
Message-ID: <1136235378.11177.10.camel@localhost.localdomain>

Thanks to the detailed help with previous error message, I managed to
update Design package. However, when I tried to update Matrix and lme4 I
received an error message again:
...
> /usr/bin/ld: cannot find -lblas-3
> collect2: ld returned 1 exit status
> make: *** [Matrix.so] Error 1
> ERROR: compilation failed for package 'Matrix'
> ** Removing '/usr/lib/R/site-library/Matrix'
> ** Restoring previous '/usr/lib/R/site-library/Matrix'
...

When I tried to search for lblas-3, following instruction of professor
Jonathan Baron, Dirk Eddelbuettel and Martin Maechler, I could not find
it.
With:
> dpkg -S lblas-3
and
> dpkg -S *-lblas-3*
I received:
> dpkg: *-lblas-3* not found.

Without Matrix, it is not possible to have lme4 updated. Also, I would
like to have nlme and some other packages. The question is how should I
procees and not be tedious with questions to the Mailing list?

>>>>> "Jon" == Jonathan Baron <baron at psych.upenn.edu>
>>>>>     on Sat, 31 Dec 2005 08:08:49 -0500 writes:

    Jon> You may need to install glibc-devel or glibc-dev,
    Jon> depending on how Ubuntu works and depending on how you
    Jon> installed it.  (For Fedora, these are called "devel".)
    Jon> You may be missing a great many "devel" rpms, such as
    Jon> readline-devel, blas-devel, and so on, which you will
    Jon> need for other packages.

    Jon> I'm not sure how you are supposed to know this, if it
    Jon> is true, although you must of course start by looking
    Jon> at the error message itself, which tells you that the
    Jon> problem is not finding crti.o.

    Jon> Appendix C1 of the installation and administration
    Jon> guide does mention the need for "dev(el)," but does not
    Jon> mention glibc in particular.

    Jon> When these things happen to me, I search for the
    Jon> missing file, in this case crti.o, on Google or on some
    Jon> repository of rpms, such as http://rpm.pbone.net.  In
    Jon> this case, there is a lot of discussion on Google about
    Jon> the very error message you got, although it is
    Jon> probabaly all misleading.

    Jon> You can also see whether you actually have the file
    Jon> somewhere by saying

    Jon> slocate crti.o

    Jon> If you find it, you can discover which rpm it is part
    Jon> of by using its full path, e.g.,

    Jon> rpm -qf /usr/lib/crti.o

In Ubuntu (which is a Debian derivative), I directly search for
the file in the installed (debian) packages by

   dpkg -S crti.o

which returns

      libc6-dev: /usr/lib/crti.o

So with ubuntu/debian, you need the libc6-dev package, which you
install by e.g.,

	apt-get install libc6-dev

Martin



From ggrothendieck at gmail.com  Mon Jan  2 22:08:24 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jan 2006 16:08:24 -0500
Subject: [R] update?
In-Reply-To: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>
References: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>
Message-ID: <971536df0601021308h2a4437e0u585aa425715a5951@mail.gmail.com>

Here is another solution in addition to the ones already
provided.   This creates a proto object which is an
environment that contains v and whose parent is
the formula's environment.  update will look in the proto
object and find v.  It won't find x and y but will next
look to the parent of the proto object and will find them there.
The proto package makes it convenient to create an environment
with a specified parent and contents all in one line
but at the expensive of a few more lines you could also do
it without the proto package.

library(proto)
nonadd <- function(formula.) {
   f <- lm(formula.)
   g <- update(f, . ~ . + v, data = proto(environment(formula.), v =
f$fitted.values^2))
   anova(f, g)
}

x <- rnorm(20)
y <- rnorm(20)
nonadd(y ~ x)



On 1/2/06, roger koenker <rkoenker at uiuc.edu> wrote:
> I'm having problems with environments and update() that
> I expect have a simple explanation.  To illustrate, suppose
> I wanted to make a very primitive Tukey one-degree-of-
> freedom for nonadditivity test and naively wrote:
>
> nonadd <- function(formula){
>         f <- lm(formula)
>         v <- f$fitted.values^2
>         g <- update(f, . ~ . + v)
>         anova(f,g)
>         }
>
> x <- rnorm(20)
> y <- rnorm(20)
> nonadd(y ~ x)
>
> Evidently, update is looking in the environment producingf f and
> doesn't find v, so I get:
>
> Error in eval(expr, envir, enclos) : Object "v" not found
>
> This may (or may not) be related to the discussion at:
> http://bugs.r-project.org/cgi-bin/R/Models?id=1861;user=guest
>
> but in any case I hope that someone can suggest how such
> difficulties can be circumvented.
>
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From edd at debian.org  Mon Jan  2 22:14:19 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 2 Jan 2006 15:14:19 -0600
Subject: [R] Problems with updating R-packages
In-Reply-To: <1136235378.11177.10.camel@localhost.localdomain>
References: <1136235378.11177.10.camel@localhost.localdomain>
Message-ID: <17337.38827.431731.190984@basebud.nulle.part>


On 2 January 2006 at 21:56, Petar Milin wrote:
| Thanks to the detailed help with previous error message, I managed to
| update Design package. However, when I tried to update Matrix and lme4 I
| received an error message again:
| ...
| > /usr/bin/ld: cannot find -lblas-3
| > collect2: ld returned 1 exit status
| > make: *** [Matrix.so] Error 1
| > ERROR: compilation failed for package 'Matrix'
| > ** Removing '/usr/lib/R/site-library/Matrix'
| > ** Restoring previous '/usr/lib/R/site-library/Matrix'
| ...
| 
| When I tried to search for lblas-3, following instruction of professor
| Jonathan Baron, Dirk Eddelbuettel and Martin Maechler, I could not find
| it.
| With:
| > dpkg -S lblas-3
| and
| > dpkg -S *-lblas-3*
| I received:
| > dpkg: *-lblas-3* not found.
| 
| Without Matrix, it is not possible to have lme4 updated. Also, I would
| like to have nlme and some other packages. The question is how should I
| procees and not be tedious with questions to the Mailing list?

You could start on the r-sig-debian mailing list where we could have told you
that _all_ of these packages are available as native Debian (and hence native
Ubuntu) packages.  So get in the habit of doing 'apt-cache search $foo' for
various values of $foo -- there may be more than you think.  

Now, for your questions, the *blas-3 libs are tricky as they are 'virtual' --
actual libraries are provided by a variety of packages, including the
(non-optimised) reference blas and the various (optmised) atlas libraries of
which base, as well as one matching your cpu, are suitable.

You never told us if you installed r-base-dev which via it Depends: line
tries to ensure you have one of the *blas-3 libraries installed:

Source: r-base
Version: 2.2.1-1
Depends: r-base-core (= 2.2.1-1), refblas3-dev | atlas3-base-dev | 
  libblas-3.so,  ....

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From baron at psych.upenn.edu  Mon Jan  2 22:15:52 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 2 Jan 2006 16:15:52 -0500
Subject: [R] Problems with updating R-packages
In-Reply-To: <1136235378.11177.10.camel@localhost.localdomain>
References: <1136235378.11177.10.camel@localhost.localdomain>
Message-ID: <20060102211552.GA13415@psych.upenn.edu>

On 01/02/06 21:56, Petar Milin wrote:
> Thanks to the detailed help with previous error message, I managed to
> update Design package. However, when I tried to update Matrix and lme4 I
> received an error message again:
> ...
> > /usr/bin/ld: cannot find -lblas-3
> > collect2: ld returned 1 exit status
> > make: *** [Matrix.so] Error 1
> > ERROR: compilation failed for package 'Matrix'
> > ** Removing '/usr/lib/R/site-library/Matrix'
> > ** Restoring previous '/usr/lib/R/site-library/Matrix'
> ...
> 
> When I tried to search for lblas-3, following instruction of professor
> Jonathan Baron, Dirk Eddelbuettel and Martin Maechler, I could not find
> it.
> With:
> > dpkg -S lblas-3
> and
> > dpkg -S *-lblas-3*
> I received:
> > dpkg: *-lblas-3* not found.

Well, the other thing you can try, which I should have mentioned,
is RSiteSearch("cannot find lblas-3"), or something like that, or
go to http://finzi.psych.upenn.edu.

Apparently you aren't the only one to have this particular
problem, and the way to a solution is given in several replies to 
earlier messages in this list.  I cannot understand them because
they are all specific to Debian.  Do you have blas installed?  (I 
can't believe you got very far without it, though.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Daniel.Pick at biogenidec.com  Mon Jan  2 23:26:10 2006
From: Daniel.Pick at biogenidec.com (Daniel Pick)
Date: Mon, 2 Jan 2006 14:26:10 -0800
Subject: [R] Ordering a matrix by another variable
Message-ID: <OFF2878127.196BE6A0-ON882570EA.007A7FB3-882570EA.007B18AF@biogenidec.com>


Hello,
    Given the matrix M
    1 5     4
    2       4     6
    3       8             5
    4         2              7

and the vector V 4,2,1,3,   I would like to order the rows in M according
to the indices in V, that is, I want output

    4 2     7
    2 4     6
    1 5     4
    3 8     5

How do I do this?  This is not a standard ascending or descending sort.

Dan



From murdoch at stats.uwo.ca  Mon Jan  2 23:32:37 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 Jan 2006 17:32:37 -0500
Subject: [R] Ordering a matrix by another variable
In-Reply-To: <OFF2878127.196BE6A0-ON882570EA.007A7FB3-882570EA.007B18AF@biogenidec.com>
References: <OFF2878127.196BE6A0-ON882570EA.007A7FB3-882570EA.007B18AF@biogenidec.com>
Message-ID: <43B9AA05.2030008@stats.uwo.ca>

On 1/2/2006 5:26 PM, Daniel Pick wrote:
 > Hello,
 >     Given the matrix M
 >     1 5     4
 >     2       4     6
 >     3       8             5
 >     4         2              7
 >
 > and the vector V 4,2,1,3,   I would like to order the rows in M according
 > to the indices in V, that is, I want output
 >
 >     4 2     7
 >     2 4     6
 >     1 5     4
 >     3 8     5
 >
 > How do I do this?  This is not a standard ascending or descending sort.

I think M[V,] is all you need.

Duncan Murdoch



From DrJones at alum.MIT.edu  Mon Jan  2 23:33:03 2006
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Mon, 2 Jan 2006 17:33:03 -0500
Subject: [R] Bookmarking a page inside r-project.org
Message-ID: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>

By way of review, most large Web sites allow the user to create 
*bookmarks* which link to pages inside the Web site. However, here, 
the pages have one of just two URL's:

http://www.r-project.org and

http://cran.r-project.org

The reason is the way HTML *frames* are used in setup of the Web site. 
It would be very helpful if the Web site were revised so that many if 
not most pages had their own URL's, allowing the use of bookmarks.

Tom
Thomas L. Jones, Ph.D., Computer Science



From kjetilbrinchmannhalvorsen at gmail.com  Mon Jan  2 23:44:55 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Mon, 2 Jan 2006 23:44:55 +0100
Subject: [R] ARIMA?
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD678@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD678@NANAMAILBACK1.nanamail.co.il>
Message-ID: <556e90a80601021444n30d9a5c7nad2b5ba698a44ad5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060102/738a4467/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Jan  2 23:47:36 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jan 2006 23:47:36 +0100
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
Message-ID: <x2irt2nt6f.fsf@turmalin.kubism.ku.dk>

"Thomas L Jones" <DrJones at alum.MIT.edu> writes:

> By way of review, most large Web sites allow the user to create 
> *bookmarks* which link to pages inside the Web site. However, here, 
> the pages have one of just two URL's:
> 
> http://www.r-project.org and
> 
> http://cran.r-project.org
> 
> The reason is the way HTML *frames* are used in setup of the Web site. 
> It would be very helpful if the Web site were revised so that many if 
> not most pages had their own URL's, allowing the use of bookmarks.

I suspect that the webmasters (and -mistresses?) in Vienna are not
thrilled by the thought of a complete website redesign...

At least in Firefox, one thing you can do is to bring up the relevant
link in a new window or tab (just right-click it) and bookmark that.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From h.wickham at gmail.com  Tue Jan  3 00:14:34 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 2 Jan 2006 23:14:34 +0000
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <x2irt2nt6f.fsf@turmalin.kubism.ku.dk>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
	<x2irt2nt6f.fsf@turmalin.kubism.ku.dk>
Message-ID: <f8e6ff050601021514o1529af99n89ae0c7c6f0d62f3@mail.gmail.com>

> > The reason is the way HTML *frames* are used in setup of the Web site.
> > It would be very helpful if the Web site were revised so that many if
> > not most pages had their own URL's, allowing the use of bookmarks.
>
> I suspect that the webmasters (and -mistresses?) in Vienna are not
> thrilled by the thought of a complete website redesign...
>

Another problem with frames is that if you come in from a search
engine, you lose the navigation (and context) making it impossible to
move around the site.  For example, google for R foundation and click
on the second hit.

Although not a completely trivial process, converting the R website to
not use frames would not be a hideously difficult undertaking.  The
html files already have nice (ie. google friendly) names and it would
be largely a task of adding a common navigation bar to each page. 
This could be done nicely with some kind of server side include (eg. 
SSI, php) and some css.  This, however, would have repercussions for
the server software necessary to mirror the R website (although
generally it would be a matter of flipping a few switches in apache or
whatever software was been used)

Hadley



From murdoch at stats.uwo.ca  Tue Jan  3 00:21:23 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 Jan 2006 18:21:23 -0500
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
Message-ID: <43B9B573.8080403@stats.uwo.ca>

On 1/2/2006 5:33 PM, Thomas L Jones wrote:
> By way of review, most large Web sites allow the user to create 
> *bookmarks* which link to pages inside the Web site. However, here, 
> the pages have one of just two URL's:
> 
> http://www.r-project.org and
> 
> http://cran.r-project.org
> 
> The reason is the way HTML *frames* are used in setup of the Web site. 
> It would be very helpful if the Web site were revised so that many if 
> not most pages had their own URL's, allowing the use of bookmarks.

Unfortunately, the constraints mean that frames are hard to avoid:

- The pages need to be displayable on multiple mirrors and offline from 
a local copy; you can't count on server-side includes.

- Many of the pages are manually edited.

- We want some common border material.

A solution would be a content-management system that produced the HTML 
of the site from some other form of input.  Only the output HTML would 
need to be mirrored.  Care to put together such a thing, and import all 
the existing pages into it?

Duncan Murdoch



From murdoch at stats.uwo.ca  Tue Jan  3 00:40:47 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 Jan 2006 18:40:47 -0500
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <f8e6ff050601021514o1529af99n89ae0c7c6f0d62f3@mail.gmail.com>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>	<x2irt2nt6f.fsf@turmalin.kubism.ku.dk>
	<f8e6ff050601021514o1529af99n89ae0c7c6f0d62f3@mail.gmail.com>
Message-ID: <43B9B9FF.2000908@stats.uwo.ca>

On 1/2/2006 6:14 PM, hadley wickham wrote:
>>>The reason is the way HTML *frames* are used in setup of the Web site.
>>>It would be very helpful if the Web site were revised so that many if
>>>not most pages had their own URL's, allowing the use of bookmarks.
>>
>>I suspect that the webmasters (and -mistresses?) in Vienna are not
>>thrilled by the thought of a complete website redesign...
>>
> 
> 
> Another problem with frames is that if you come in from a search
> engine, you lose the navigation (and context) making it impossible to
> move around the site.  For example, google for R foundation and click
> on the second hit.
> 
> Although not a completely trivial process, converting the R website to
> not use frames would not be a hideously difficult undertaking.  The
> html files already have nice (ie. google friendly) names and it would
> be largely a task of adding a common navigation bar to each page. 
> This could be done nicely with some kind of server side include (eg. 
> SSI, php) and some css.  This, however, would have repercussions for
> the server software necessary to mirror the R website (although
> generally it would be a matter of flipping a few switches in apache or
> whatever software was been used)
> 

You can check out from 
https://svn.r-project.org/R-project-web/branches/DJM/ a version I 
started to put together using this approach.  It isn't hard to edit, but 
it did place unacceptable requirements on the server.

Duncan Murdoch



From ggrothendieck at gmail.com  Tue Jan  3 01:20:11 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jan 2006 19:20:11 -0500
Subject: [R] update?
In-Reply-To: <971536df0601021308h2a4437e0u585aa425715a5951@mail.gmail.com>
References: <092DD697-BD9F-463C-B4B7-FC19DA86065A@uiuc.edu>
	<971536df0601021308h2a4437e0u585aa425715a5951@mail.gmail.com>
Message-ID: <971536df0601021620l6fe8f27ag11f7567fcf19026a@mail.gmail.com>

Actually I tried it and a version of nonadd which
corresponds to my prior solution can be written
without using external packages in only 2
extra lines:

nonadd <- function(formula.) {
	f <- lm(formula.)
	e <- new.env(parent = environment(formula.))
	e$v <- f$fitted.values^2
	g <- update(f, . ~ . + v, data = e)
	anova(f, g)
}
x <- rnorm(20)
y <- rnorm(20)
nonadd(y ~ x)


On 1/2/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is another solution in addition to the ones already
> provided.   This creates a proto object which is an
> environment that contains v and whose parent is
> the formula's environment.  update will look in the proto
> object and find v.  It won't find x and y but will next
> look to the parent of the proto object and will find them there.
> The proto package makes it convenient to create an environment
> with a specified parent and contents all in one line
> but at the expensive of a few more lines you could also do
> it without the proto package.
>
> library(proto)
> nonadd <- function(formula.) {
>   f <- lm(formula.)
>   g <- update(f, . ~ . + v, data = proto(environment(formula.), v =
> f$fitted.values^2))
>   anova(f, g)
> }
>
> x <- rnorm(20)
> y <- rnorm(20)
> nonadd(y ~ x)
>
>
>
> On 1/2/06, roger koenker <rkoenker at uiuc.edu> wrote:
> > I'm having problems with environments and update() that
> > I expect have a simple explanation.  To illustrate, suppose
> > I wanted to make a very primitive Tukey one-degree-of-
> > freedom for nonadditivity test and naively wrote:
> >
> > nonadd <- function(formula){
> >         f <- lm(formula)
> >         v <- f$fitted.values^2
> >         g <- update(f, . ~ . + v)
> >         anova(f,g)
> >         }
> >
> > x <- rnorm(20)
> > y <- rnorm(20)
> > nonadd(y ~ x)
> >
> > Evidently, update is looking in the environment producingf f and
> > doesn't find v, so I get:
> >
> > Error in eval(expr, envir, enclos) : Object "v" not found
> >
> > This may (or may not) be related to the discussion at:
> > http://bugs.r-project.org/cgi-bin/R/Models?id=1861;user=guest
> >
> > but in any case I hope that someone can suggest how such
> > difficulties can be circumvented.
> >
> >
> > url:    www.econ.uiuc.edu/~roger            Roger Koenker
> > email    rkoenker at uiuc.edu            Department of Economics
> > vox:     217-333-4558                University of Illinois
> > fax:       217-244-6678                Champaign, IL 61820
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From baron at psych.upenn.edu  Tue Jan  3 01:44:40 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 2 Jan 2006 19:44:40 -0500
Subject: [R] bookmarking a page inside r-project.org
Message-ID: <20060103004440.GA20127@psych.upenn.edu>

I'm replying to:
https://stat.ethz.ch/pipermail/r-help/2006-January/083823.html

In Firefox (a browser), right click on the frame.  Then you get a
menu that has bookmark as one of the options.  Firefox is
available from http://www.mozilla.org.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Charles.Annis at StatisticalEngineering.com  Tue Jan  3 02:15:02 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 2 Jan 2006 20:15:02 -0500
Subject: [R] bookmarking a page inside r-project.org
In-Reply-To: <20060103004440.GA20127@psych.upenn.edu>
Message-ID: <200601030114.k031Ew7v003370@hypatia.math.ethz.ch>

You can do something similar with Microsoft's browser but it isn't quite as
easy as Foxfire:

Right-click on the frame and choose Properties.  Then highlight and copy the
URL and paste into the address window and click Go.

Then save the page.



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathan Baron
Sent: Monday, January 02, 2006 7:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] bookmarking a page inside r-project.org

I'm replying to:
https://stat.ethz.ch/pipermail/r-help/2006-January/083823.html

In Firefox (a browser), right click on the frame.  Then you get a
menu that has bookmark as one of the options.  Firefox is
available from http://www.mozilla.org.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From DrJones at alum.MIT.edu  Tue Jan  3 05:09:47 2006
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Mon, 2 Jan 2006 23:09:47 -0500
Subject: [R] More on bookmarking a page
Message-ID: <000301c6101b$897ed570$2f01a8c0@DrJones>

The subject is how to bookmark a page inside r-project.orgFrom Peter 
Dalgaard:At least in Firefox, one thing you can do is to bring up the 
relevant
link in a new window or tab (just right-click it) and bookmark 
that.----------------------------------------------------------------------------------------From 
Thomas Jones:Well, learn something every day! I stand corrected.



From sumantab at ambaresearch.com  Tue Jan  3 07:15:16 2006
From: sumantab at ambaresearch.com (Sumanta Basak)
Date: Tue, 3 Jan 2006 11:45:16 +0530
Subject: [R] KALMAN FILTER HELP
Message-ID: <14850601FF012647A90A5DB31F96DB37355732@INBLRDC01.BANG.irpvl.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/197014b1/attachment.pl

From h.wickham at gmail.com  Tue Jan  3 08:29:27 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 3 Jan 2006 07:29:27 +0000
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <43B9B573.8080403@stats.uwo.ca>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
	<43B9B573.8080403@stats.uwo.ca>
Message-ID: <f8e6ff050601022329x76ba4cdbhb55287b4e23b48be@mail.gmail.com>

> A solution would be a content-management system that produced the HTML
> of the site from some other form of input.  Only the output HTML would
> need to be mirrored.  Care to put together such a thing, and import all
> the existing pages into it?

One way to get around the offline problem is to have a dynamic copy
somewhere and then spider and save it (eg. with wget -r).  This would
(obviously) require a server somewhere - but with a post-commit svn
hook could be kept up to date easily.  However, it is still difficult
to view changes to the page immediately.

What assumptions can I make about what tools are available to the
editors?  Can I assume the standard unix tool chain?   What
assumptions can I make about the people doing the editing?  How many
people edit the pages?  How familiar with html are they?  You say many
of the pages are manually edited, which ones aren't?  How are they
generated?  Are all the pages under
https://svn.r-project.org/R-project-web/trunk/ ?

Hadley



From ripley at stats.ox.ac.uk  Tue Jan  3 09:09:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jan 2006 08:09:07 +0000 (GMT)
Subject: [R] problem with dse package (was KALMAN FILTER HELP)
In-Reply-To: <14850601FF012647A90A5DB31F96DB37355732@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB37355732@INBLRDC01.BANG.irpvl.com>
Message-ID: <Pine.LNX.4.61.0601030805060.12650@gannet.stats>

This has come up before: it needs a bug fix which Paul Gilbert has already 
implemented (but not yet released).

Please use an informative subject line, and don't SHOUT at us. (All caps 
is regarded as shouting, and BTW the package bundle is dse not DSE.)

On Tue, 3 Jan 2006, Sumanta Basak wrote:

> Currently I'm using DSE package for Kalman Filtering. I have a dataset
> of one dependent variable and seven other independent variables. I'm
> confused at one point. How to declare the input-output series using
> TSdata command. Because the given example at page 37 showing some error.
>
> rain <- matrix(rnorm(86*17), 86,17)
> radar <- matrix(rnorm(86*5), 86,5)
> mydata <- TSdata(input=radar, output=rain)
>
> input data:
>
> Error: evaluation nested too deeply: infinite recursion /
> options(expressions=)?
>
> Can anyone explain it to me what's going wrong in this? In my data set,
> I have "Change in Exchange Rate" as my dependent variable and seven
> other economic variables as independent variables. I'm trying to
> forecast "Change in Exchange Rate" using available dataset of 244
> points. How can declare the input and output dataset in this framework?
> I hope I'm right to explain in this way what ultimately I'm going to do.
> After having a TSdata object, I want to use toSS to convert the TS model
> into state space model, and then use l.SS. Am I right in my thinking?
> Please advice, and many thanks in advance.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetilbrinchmannhalvorsen at gmail.com  Tue Jan  3 10:14:46 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Tue, 3 Jan 2006 10:14:46 +0100
Subject: [R] GLARMA
In-Reply-To: <200512300257.jBU2vivg020234@hypatia.math.ethz.ch>
References: <200512300257.jBU2vivg020234@hypatia.math.ethz.ch>
Message-ID: <556e90a80601030114g398d3bcck1fa1f11fe8928085@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/c48da856/attachment.pl

From h.wickham at gmail.com  Tue Jan  3 10:45:28 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 3 Jan 2006 09:45:28 +0000
Subject: [R] Extending a data frame with S4
Message-ID: <f8e6ff050601030145p5f4f8980x3b04c75ae3c4644b@mail.gmail.com>

I'm trying to create an extension to data.frame with more complex row
and column names, and have run into some problems:

> setClass("new-data.frame", representation("data.frame"))
[1] "new-data.frame"
Warning message:
old-style ('S3') class "data.frame" supplied as a superclass of
"new-data.frame", but no automatic conversion will be peformed for S3
classes in: .validDataPartClass(clDef, name)

Do I need to be worried about this?

> new("new-data.frame", data.frame())
Error in initialize(value, ...) : initialize method returned an object
of class "data.frame" instead of the required class "new-data.frame"

I guess this is related to the warning above.  I presume I can fix
this with an initialize function, but I'm not sure how to go about
referring to the data frame that is the object.
Is there a way to extend a data.frame, or do I need to create an
object that contains the data frame in a slot?

Thanks for your help,

Hadley



From Rau at demogr.mpg.de  Tue Jan  3 11:08:00 2006
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 3 Jan 2006 11:08:00 +0100
Subject: [R] A comment about R:
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>

 > -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Monday, January 02, 2006 4:59 PM
> To: Philippe Grosjean
> Cc: Kort, Eric; Kjetil Halvorsen; R-help at stat.math.ethz.ch
> Subject: Re: [R] A comment about R:
>
> 
> Probably what is needed is for someone familiar with both Stata and R
> to create a lexicon in the vein of the Octave to R lexicon
> 
>    http://cran.r-project.org/doc/contrib/R-and-octave-2.txt
> 
> to make it easier for Stata users to understand R.  Ditto for 
> SAS and SPSS.
> 
>
IMO this is a very good proposal but I think that the main problem is
not the "translation" of one function in SPSS/Stata/SAS to the
equivalent in R.
Remembering my first contact with R after using SPSS for some years (and
having some experience with Stata and SAS) was that your mental
framework is different. You think in "SPSS-terms" (i.e. you expect that
data are automatically a rectangular matrix, functions operate on
columns of this matrix, you have always only one dataset available,
...). This is why "jumping" from SPSS to Stata is relatively easy. But
to jump from any of the three to R is much more difficult. 
This mental barrier is also the main obstacle for me now when I try to
encourage the use of R to other people who have a similar background as
I had.
What can be done about it? I guess the only answer is investing time
from the user which implies that R will probably never become the
language of choice for "casual users". But popularity is probably not
the main goal of the R-Project (it would be rather a nice side-effect).

Just a few thoughts ...

Best,
Roland

+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From phgrosjean at sciviews.org  Tue Jan  3 11:31:42 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 03 Jan 2006 11:31:42 +0100
Subject: [R] A comment about R:
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
Message-ID: <43BA528E.7020204@sciviews.org>

Roland,

Yes, indeed, you are perfectly right. The problem is that R richness 
means R complexity: many different data types, "sub-languages" like 
regexp or the formula interface, S3/S4 objects, classical versus lattice 
(versus RGL versus iplots) graphs, etc. During translation of R in 
French, I was thinking of a subset of one or two hundreds of functions 
that would be enough for beginners to start with, and to propose a 
translation of that small subset of the online help in French. This is 
still on my todo list, but I must admit it is not an easy task to decide 
which function should be kept in the subset and which should not!

In fact, that idea could be, perhaps, generalized into the whole online 
help. It would be sufficient to add a flag somewhere (perhaps a keyword) 
telling that page is fundamental and to allow filtering index and pages 
  ("fundamental only" or "full help"). Even for advanced users, it 
should be nice to have such a filter to display only the two or three 
most important functions in a new packages that proposes perhaps hundred 
online help pages...

Using R Commander is also an interesting experiment. R Commander 
simplifies the use of R down to the manipulation of a single data frame 
(the so-called "active dataset") + optionally one or two model objects. 
Just look at all you can do just with one active data frame with R 
Commander, and you will realize that it is perfectly manageable to learn 
R that way.

Best,

Philippe Grosjean


Rau, Roland wrote:
>  > -----Original Message-----
> 
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
>>Grothendieck
>>Sent: Monday, January 02, 2006 4:59 PM
>>To: Philippe Grosjean
>>Cc: Kort, Eric; Kjetil Halvorsen; R-help at stat.math.ethz.ch
>>Subject: Re: [R] A comment about R:
>>
>>
>>Probably what is needed is for someone familiar with both Stata and R
>>to create a lexicon in the vein of the Octave to R lexicon
>>
>>   http://cran.r-project.org/doc/contrib/R-and-octave-2.txt
>>
>>to make it easier for Stata users to understand R.  Ditto for 
>>SAS and SPSS.
>>
>>
> 
> IMO this is a very good proposal but I think that the main problem is
> not the "translation" of one function in SPSS/Stata/SAS to the
> equivalent in R.
> Remembering my first contact with R after using SPSS for some years (and
> having some experience with Stata and SAS) was that your mental
> framework is different. You think in "SPSS-terms" (i.e. you expect that
> data are automatically a rectangular matrix, functions operate on
> columns of this matrix, you have always only one dataset available,
> ...). This is why "jumping" from SPSS to Stata is relatively easy. But
> to jump from any of the three to R is much more difficult. 
> This mental barrier is also the main obstacle for me now when I try to
> encourage the use of R to other people who have a similar background as
> I had.
> What can be done about it? I guess the only answer is investing time
> from the user which implies that R will probably never become the
> language of choice for "casual users". But popularity is probably not
> the main goal of the R-Project (it would be rather a nice side-effect).
> 
> Just a few thoughts ...
> 
> Best,
> Roland
> 
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From abderrahim.oulhaj at pharmacology.oxford.ac.uk  Tue Jan  3 11:47:33 2006
From: abderrahim.oulhaj at pharmacology.oxford.ac.uk (Abderrahim Oulhaj)
Date: Tue, 3 Jan 2006 10:47:33 -0000
Subject: [R] lmer error message
Message-ID: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/47ecc8a2/attachment.pl

From flom at ndri.org  Tue Jan  3 12:27:39 2006
From: flom at ndri.org (Peter Flom)
Date: Tue, 03 Jan 2006 06:27:39 -0500
Subject: [R] A comment about R:
Message-ID: <43BA195C020000C90000177C@MAIL.NDRI.ORG>

>>> "Rau, Roland" <Rau at demogr.mpg.de>  >>> wrote
<<<
IMO this is a very good proposal but I think that the main problem is
not the "translation" of one function in SPSS/Stata/SAS to the
equivalent in R.
Remembering my first contact with R after using SPSS for some years (and
having some experience with Stata and SAS) was that your mental
framework is different. You think in "SPSS-terms" (i.e. you expect that
data are automatically a rectangular matrix, functions operate on
columns of this matrix, you have always only one dataset available,
...). This is why "jumping" from SPSS to Stata is relatively easy. But
to jump from any of the three to R is much more difficult. 
This mental barrier is also the main obstacle for me now when I try to
encourage the use of R to other people who have a similar background as
I had.
What can be done about it? I guess the only answer is investing time
from the user which implies that R will probably never become the
language of choice for "casual users". But popularity is probably not
the main goal of the R-Project (it would be rather a nice side-effect).
>>>>



As someone who uses SAS qutie a bit and R somewhat less, I think Roland 
makes some excellent points.  Going from SPSS to SAS (which I once did)
is like going from Spansih to French.  Going from SAS to R (which I am
trying to do) is like going from English to Chinese.

But it's more than that.  

Beyond the obvious differences in the languages is a difference in how
they are written about;
and how they are improved.  SAS documentation is much lengthier than
R's.  Some people like
the terseness of R's help.  Some like the verboseness of SAS's.  SOme of
this difference is doubtless
due to the fact that SAS is commercial, and pays people to write the
documentation.  I have tremednous
appreciation for the unpaid effort that goes into R, and nothing I say
here should be seen as detracting from that.

As to how they are improved, the fact that R is extended (in part) by
packages written by many many different
people is good, becuase it means that the latest techniques can be
written up, often by the people who
invent the techniques (and, again, I appreciate this tremendously), but
it does mean that a) It is hard to know what
is out there at any given time; b) the styles of pacakages difer
somewhat.

In addition, I think the distinction between 'casual user' and serious
user is something of a false dichotomy.
It's really a continuum, or, probably, several continua, that make R
harder or easier for people to learn.

I like R.  I like it a lot.  I like that it's free.  I like that it's
cutting edge.  I like that it can do amazing graphics.
I like that the code is open.  I like that I can write my own functions
in the same language.  And, again,
I am amazed at the amount of time and effort people put into it.

 But I do think that the link in the original post made some good
points, and the writer
of that post is not the only one who has found R difficult to learn.


Peter



From Matthias.Kohl at stamats.de  Tue Jan  3 12:36:06 2006
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Tue, 03 Jan 2006 12:36:06 +0100
Subject: [R] Extending a data frame with S4
In-Reply-To: <f8e6ff050601030145p5f4f8980x3b04c75ae3c4644b@mail.gmail.com>
References: <f8e6ff050601030145p5f4f8980x3b04c75ae3c4644b@mail.gmail.com>
Message-ID: <43BA61A6.3080107@stamats.de>

the help page on "setOldClass" might help you. In particular the section 
"Register or Convert?".

Matthias

hadley wickham schrieb:

>I'm trying to create an extension to data.frame with more complex row
>and column names, and have run into some problems:
>
>  
>
>>setClass("new-data.frame", representation("data.frame"))
>>    
>>
>[1] "new-data.frame"
>Warning message:
>old-style ('S3') class "data.frame" supplied as a superclass of
>"new-data.frame", but no automatic conversion will be peformed for S3
>classes in: .validDataPartClass(clDef, name)
>
>Do I need to be worried about this?
>
>  
>
>>new("new-data.frame", data.frame())
>>    
>>
>Error in initialize(value, ...) : initialize method returned an object
>of class "data.frame" instead of the required class "new-data.frame"
>
>I guess this is related to the warning above.  I presume I can fix
>this with an initialize function, but I'm not sure how to go about
>referring to the data frame that is the object.
>Is there a way to extend a data.frame, or do I need to create an
>object that contains the data frame in a slot?
>
>Thanks for your help,
>
>Hadley
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>


-- 
StaMatS - Statistik + Mathematik Service
Dipl.Math.(Univ.) Matthias Kohl
www.stamats.de



From ggrothendieck at gmail.com  Tue Jan  3 12:43:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jan 2006 06:43:26 -0500
Subject: [R] A comment about R:
In-Reply-To: <43BA195C020000C90000177C@MAIL.NDRI.ORG>
References: <43BA195C020000C90000177C@MAIL.NDRI.ORG>
Message-ID: <971536df0601030343x18e5be87n354dc8625e3d5c16@mail.gmail.com>

On 1/3/06, Peter Flom <flom at ndri.org> wrote:
> >>> "Rau, Roland" <Rau at demogr.mpg.de>  >>> wrote
> <<<
> IMO this is a very good proposal but I think that the main problem is
> not the "translation" of one function in SPSS/Stata/SAS to the
> equivalent in R.
> Remembering my first contact with R after using SPSS for some years (and
> having some experience with Stata and SAS) was that your mental
> framework is different. You think in "SPSS-terms" (i.e. you expect that
> data are automatically a rectangular matrix, functions operate on
> columns of this matrix, you have always only one dataset available,
> ...). This is why "jumping" from SPSS to Stata is relatively easy. But
> to jump from any of the three to R is much more difficult.
> This mental barrier is also the main obstacle for me now when I try to
> encourage the use of R to other people who have a similar background as
> I had.
> What can be done about it? I guess the only answer is investing time
> from the user which implies that R will probably never become the
> language of choice for "casual users". But popularity is probably not
> the main goal of the R-Project (it would be rather a nice side-effect).
> >>>>
>
>
>
> As someone who uses SAS qutie a bit and R somewhat less, I think Roland
> makes some excellent points.  Going from SPSS to SAS (which I once did)
> is like going from Spansih to French.  Going from SAS to R (which I am
> trying to do) is like going from English to Chinese.
>
> But it's more than that.
>
> Beyond the obvious differences in the languages is a difference in how
> they are written about;
> and how they are improved.  SAS documentation is much lengthier than
> R's.  Some people like
> the terseness of R's help.  Some like the verboseness of SAS's.  SOme of

Note that at least some packages do have vignettes which are lengthier
discussions of the package than the help files, e.g.

   library(zoo)
   vignette("zoo")

> this difference is doubtless
> due to the fact that SAS is commercial, and pays people to write the
> documentation.  I have tremednous
> appreciation for the unpaid effort that goes into R, and nothing I say
> here should be seen as detracting from that.
>
> As to how they are improved, the fact that R is extended (in part) by
> packages written by many many different
> people is good, becuase it means that the latest techniques can be
> written up, often by the people who
> invent the techniques (and, again, I appreciate this tremendously), but
> it does mean that a) It is hard to know what
> is out there at any given time; b) the styles of pacakages difer
> somewhat.


Regarding (a) note that for certain areas CRAN Task Views
addresses this, at least in part.  See:

     http://cran.r-project.org/src/contrib/Views/

and R-News has a section on changes in CRAN which lists all new
packages since the prior issue of CRAN.   See:

    http://cran.r-project.org/doc/Rnews



From masifulla at hcl.in  Tue Jan  3 12:46:23 2006
From: masifulla at hcl.in (Mohammed Asifulla - CTD , Chennai)
Date: Tue, 3 Jan 2006 17:16:23 +0530 
Subject: [R] need to know some basic functionality features of R-Proj
Message-ID: <A5D00D487D24C044BA9CDE5E41A649A60129179C@odcex01.ctd.hcltech.com>

Hi,

I am new-comer to statistics and R-Project. I would like to know if these
features can be attained in R-Project.Please help.

1)  beta 1 and Beta 2, or gamma one and gamma two for skewness and kurtosis,
respectively, including standard errors and tests for significance (relative
to values for a Gaussian distribution).
2)  linear correlation
3)  quadratic regression
4)  polynomial regression
5)  moving averages
6)  chi-square for a two-by two table and for an n by m contingency table
7)  moving averages - with various (e.g. exponential) weighting
8)  cubic splines (smoothing, not interpolating)
9)  other types of splines, e.g. 'linear' splines
10) erfc-1  inverse error function complement (i.e. tables of integrals of
the normal (Gaussian) curve, or mathematical approximations)
11) erfc    error function complement
12) Table of significant values for t test at P < 0.01 one sided or two
sided - or polynomial approximation
13) Table of significance levels for chi square test
14) Table of significance levels for F distribution  as arising in ANOVA
15) Confidence limits for binomial variables; possibly for multinomial
variables

Thanks and Regards
-Asif



From sdavis2 at mail.nih.gov  Tue Jan  3 12:52:12 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 03 Jan 2006 06:52:12 -0500
Subject: [R] S3 vs. S4
In-Reply-To: <200601011907.k01J7PFB012052@gator.dt.uh.edu>
Message-ID: <BFDFCF9C.2A71%sdavis2@mail.nih.gov>




On 1/1/06 2:07 PM, "Erin Hodgess" <hodgess at gator.dt.uh.edu> wrote:

> Dear R People: 
> 
> Could someone direct me to some documentation on the
> difference between S3 and S4 classes, please?
> 
> For example, why would a person use one as opposed to another?
> Maybe pros and cons of each?

The Bioconductor project has encouraged my use of S4 classes.  S4 allows
creation of data structures that have methods associated with them, so for
data-structure heavy programming, I think S4 might have some advantages, but
I am NOT an expert in the field.

Just one other link that I have found quite useful:

http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf

Sean



From sdavis2 at mail.nih.gov  Tue Jan  3 13:08:16 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 03 Jan 2006 07:08:16 -0500
Subject: [R] need to know some basic functionality features of R-Proj
In-Reply-To: <A5D00D487D24C044BA9CDE5E41A649A60129179C@odcex01.ctd.hcltech.com>
Message-ID: <BFDFD360.2A76%sdavis2@mail.nih.gov>




On 1/3/06 6:46 AM, "Mohammed Asifulla - CTD , Chennai" <masifulla at hcl.in>
wrote:

> Hi,
> 
> I am new-comer to statistics and R-Project. I would like to know if these
> features can be attained in R-Project.Please help.
> 
> 1)  beta 1 and Beta 2, or gamma one and gamma two for skewness and kurtosis,
> respectively, including standard errors and tests for significance (relative
> to values for a Gaussian distribution).
> 2)  linear correlation
> 3)  quadratic regression
> 4)  polynomial regression
> 5)  moving averages
> 6)  chi-square for a two-by two table and for an n by m contingency table
> 7)  moving averages - with various (e.g. exponential) weighting
> 8)  cubic splines (smoothing, not interpolating)
> 9)  other types of splines, e.g. 'linear' splines
> 10) erfc-1  inverse error function complement (i.e. tables of integrals of
> the normal (Gaussian) curve, or mathematical approximations)
> 11) erfc    error function complement
> 12) Table of significant values for t test at P < 0.01 one sided or two
> sided - or polynomial approximation
> 13) Table of significance levels for chi square test
> 14) Table of significance levels for F distribution  as arising in ANOVA
> 15) Confidence limits for binomial variables; possibly for multinomial
> variables

Asif,

It is highly likely that all these can be attained using R.  I think most
(if not all) of those on your list can be done with existing packages; for
those that can't, R is also a full-featured programming language, so you can
write functions to do what you like.  I would suggest starting with the
Introduction to R manual to learn what R can do.  It can be obtained via the
"Manuals" link at the left side of the R home page:

http://www.r-project.org

Also, if you are posting to the email list, it is quite helpful to read the
posting guide, available as a link at the bottom of all emails from this
list.

Sean



From Bernhard_Pfaff at fra.invesco.com  Tue Jan  3 14:33:44 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 3 Jan 2006 13:33:44 -0000 
Subject: [R] Q about RSQLite
Message-ID: <25D1C2585277D311B9A20000F6CCC71B077C0364@DEFRAEX02>

Hello Liu,

this might be caused by NA entries in your SQLite table. Have a look at the
following code:


(test <- data.frame(matrix(c(1:10, NA, NA), ncol=2, nrow=6)))
con <- dbConnect(SQLite(), dbname = "test.db")
dbWriteTable(con, "test", test, type="BLOB", overwrite=TRUE)
d1 <- dbReadTable(con, "test")
dbDisconnect(con)
d1


HTH,
Bernhard  

-----Urspr??ngliche Nachricht-----
Von: Wensui Liu [mailto:liuwensui at gmail.com] 
Gesendet: Samstag, 31. Dezember 2005 07:09
An: r-help at stat.math.ethz.ch
Betreff: [R] Q about RSQLite

Happy new year, dear listers,

I have a question about Rsqlite.

when I fetch the data out of sqlite database, there is something like '\r\n'
at the end of last column. Here is the example:
   Sepal_Length Sepal_Width Petal_Length Petal_Width    Species
1           5.1         3.5          1.4         0.2 setosa\r\n
2           4.9         3.0          1.4         0.2 setosa\r\n
3           4.7         3.2          1.3         0.2 setosa\r\n
4           4.6         3.1          1.5         0.2 setosa\r\n
5           5.0         3.6          1.4         0.2 setosa\r\n
6           5.4         3.9          1.7         0.4 setosa\r\n
7           4.6         3.4          1.4         0.3 setosa\r\n
8           5.0         3.4          1.5         0.2 setosa\r\n
9           4.4         2.9          1.4         0.2 setosa\r\n
10          4.9         3.1          1.5         0.1 setosa\r\n

Any idea?

Thank you so much

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}



From br44114 at gmail.com  Tue Jan  3 14:37:41 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 3 Jan 2006 08:37:41 -0500
Subject: [R] Q about RSQLite
Message-ID: <8d5a36350601030537p1b7e8d21sff4a0a5d49d19b43@mail.gmail.com>

Check the way you imported the data / the SQLite documentation. The
\r\n that you see (you're on Windows, right?) is used to indicate the
end of the data lines in the source file - \r is a carriage return,
and \n is a new line character.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
> Sent: Saturday, December 31, 2005 1:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Q about RSQLite
>
>
> Happy new year, dear listers,
>
> I have a question about Rsqlite.
>
> when I fetch the data out of sqlite database, there is
> something like '\r\n'
> at the end of last column. Here is the example:
>    Sepal_Length Sepal_Width Petal_Length Petal_Width    Species
> 1           5.1         3.5          1.4         0.2 setosa\r\n
> 2           4.9         3.0          1.4         0.2 setosa\r\n
> 3           4.7         3.2          1.3         0.2 setosa\r\n
> 4           4.6         3.1          1.5         0.2 setosa\r\n
> 5           5.0         3.6          1.4         0.2 setosa\r\n
> 6           5.4         3.9          1.7         0.4 setosa\r\n
> 7           4.6         3.4          1.4         0.3 setosa\r\n
> 8           5.0         3.4          1.5         0.2 setosa\r\n
> 9           4.4         2.9          1.4         0.2 setosa\r\n
> 10          4.9         3.1          1.5         0.1 setosa\r\n
>
> Any idea?
>
> Thank you so much
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Friedrich.Leisch at tuwien.ac.at  Tue Jan  3 14:44:27 2006
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Tue, 3 Jan 2006 14:44:27 +0100
Subject: [R] Bookmarking a page inside r-project.org
In-Reply-To: <f8e6ff050601022329x76ba4cdbhb55287b4e23b48be@mail.gmail.com>
References: <000301c60fec$7ed2f1d0$2f01a8c0@DrJones>
	<43B9B573.8080403@stats.uwo.ca>
	<f8e6ff050601022329x76ba4cdbhb55287b4e23b48be@mail.gmail.com>
Message-ID: <17338.32699.904527.541464@galadriel.ci.tuwien.ac.at>

>>>>> On Tue, 3 Jan 2006 07:29:27 +0000,
>>>>> hadley wickham (hw) wrote:

  >> A solution would be a content-management system that produced the HTML
  >> of the site from some other form of input.  Only the output HTML would
  >> need to be mirrored.  Care to put together such a thing, and import all
  >> the existing pages into it?

  > One way to get around the offline problem is to have a dynamic copy
  > somewhere and then spider and save it (eg. with wget -r).  This would
  > (obviously) require a server somewhere - but with a post-commit svn
  > hook could be kept up to date easily.  However, it is still difficult
  > to view changes to the page immediately.

  > What assumptions can I make about what tools are available to the
  > editors?  Can I assume the standard unix tool chain?

Yes.

  > What
  > assumptions can I make about the people doing the editing?  How many
  > people edit the pages?

For www.R-project.org all of R core have write access, but only a few
actually do it ;-)

  > How familiar with html are they?

Hard to tell, let's assume at least basic familiarity with HTML (but
very good familiarity to the concept of markup laguages per se).

  > You say many
  > of the pages are manually edited, which ones aren't?

Under www.r-project.org I think all are manual.

  > How are they
  > generated?

on CRAN all package listings are of course auto-generated (mostly
using perl scripts), the mirror list is created using R.

  > Are all the pages under
  > https://svn.r-project.org/R-project-web/trunk/ ?


No, CRAN is not, as it is pulled together from various sites where
maintainers of binary distributions etc. create their parts -> the
CRAN master itself is "mirror" for the pits and pieces (e.g., windows
R base binaries are mirrored from Duncan Murdoch, windows packages
from Uwe Ligges, etc. etc.).

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From roger.bos at gmail.com  Tue Jan  3 15:34:01 2006
From: roger.bos at gmail.com (roger bos)
Date: Tue, 3 Jan 2006 09:34:01 -0500
Subject: [R] r: RODBC QUESTION
In-Reply-To: <20051231193053.9268.qmail@web30203.mail.mud.yahoo.com>
References: <43B658BA.A84B05CF@STATS.uct.ac.za>
	<20051231193053.9268.qmail@web30203.mail.mud.yahoo.com>
Message-ID: <1db726800601030634l100b9ea9rfd9c3e12acecb194@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/6fb93c61/attachment.pl

From ked at nilu.no  Tue Jan  3 15:48:17 2006
From: ked at nilu.no (Kare Edvardsen)
Date: Tue, 03 Jan 2006 15:48:17 +0100
Subject: [R] Labels exceed the plot area
Message-ID: <43BA8EB1.80701@nilu.no>

If I use cex.lab = 2 and cex.axis = 2 the yaxis label in a plot exceed 
the plot area. How do I get the plot itself smaller to get space for the 
label so I still use cex.lab = 2 and cex.axis = 2?

Kare

-- 
###########################################
Kare Edvardsen <kare.edvardsen at nilu.no>
Norwegian Institute for Air Research (NILU)
Polarmiljosenteret
NO-9296 Tromso       http://www.nilu.no
Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
###########################################



From beperron at wustl.edu  Tue Jan  3 15:51:25 2006
From: beperron at wustl.edu (Brian Perron)
Date: Tue, 3 Jan 2006 08:51:25 -0600
Subject: [R] Package for multiple membership model?
Message-ID: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/805b5786/attachment.pl

From pgilbert at bank-banque-canada.ca  Tue Jan  3 16:16:33 2006
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 03 Jan 2006 10:16:33 -0500
Subject: [R] KALMAN FILTER HELP
In-Reply-To: <14850601FF012647A90A5DB31F96DB37355732@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB37355732@INBLRDC01.BANG.irpvl.com>
Message-ID: <43BA9551.9040602@bank-banque-canada.ca>

Is this happening with the example as you show it, or are you trying to 
print mydata?

There is a bug in the print method for TSdata objects, which I have 
fixed and was intending to put on CRAN in a few days. This bug does give 
the infinite recursion error, but would only happen when you print the 
data by typing

mydata
or
print(mydata)

I don't think the assignment you show would produce this problem, but 
please send me more details if it does. The problem, which will be fixed 
in the next release, is only with the print method. Other things are 
working and you should be able to do model estimation, conversion, and 
plot the data, just not print it.

Paul Gilbert

Sumanta Basak wrote:

> Hi All,
>
> Currently I?m using DSE package for Kalman Filtering. I have a dataset 
> of one dependent variable and seven other independent variables. I?m 
> confused at one point. How to declare the input-output series using 
> TSdata command. Because the given example at page 37 showing some error.
>
> rain <- matrix(rnorm(86*17), 86,17)
>
> radar <- matrix(rnorm(86*5), 86,5)
>
> mydata <- TSdata(input=radar, output=rain)
>
> *input data:*
>
> *Error: evaluation nested too deeply: infinite recursion / 
> options(expressions=)?*
>
> Can anyone explain it to me what?s going wrong in this? In my data 
> set, I have ?Change in Exchange Rate? as my dependent variable and 
> seven other economic variables as independent variables. I?m trying to 
> forecast ?Change in Exchange Rate? using available dataset of 244 
> points. How can declare the input and output dataset in this 
> framework? I hope I?m right to explain in this way what ultimately I?m 
> going to do. After having a TSdata object, I want to use toSS to 
> convert the TS model into state space model, and then use l.SS. Am I 
> right in my thinking? Please advice, and many thanks in advance.
>
> ------------------------------------------
>
> SUMANTA BASAK.
>
> Analyst.
>
> Phone No. - 080 - 41989937 (O)
>
> 09886047620 (M)
>
> Amba Research (India) Pvt Ltd.
>
> G02 Prestige Loka.
>
> 7/1, Brunton Road.
>
> Bangalore - 560025.
>
> India.
>
> ------------------------------------------
>
> -------------------------------------------------------------------------------------------------------------------
> This e-mail may contain confidential and/or privileged inf...{{dropped}}



From costas.magnuse at gmail.com  Tue Jan  3 16:24:17 2006
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Tue, 3 Jan 2006 17:24:17 +0200
Subject: [R] how to work on multiple R objects?...
Message-ID: <30ddfdae0601030724o26c9c6a3p2db2eb3d546d97a6@mail.gmail.com>

Hello, Happy New Year!...

I am encountering a problem trying to work on the data that I load in R.

I have loaded to R a series of stock data using
(csv files are named e.g. IBM.R)

length.R <- length(list.files(".", pattern=".R")) # the number of
files with one             #column in the directory "./" ending to
".R"
for (i in 1:length.R) {
assign(read.csv(list.files(".", pattern=".R")[i],
read.csv(list.files(".", pattern=".R")[i])))
}

I would like to perform various tasks on all these objects, but I cannot because
> ls(pattern=".R")[1]
is not a list, but a character string!!!:
	> typeof(ls(pa=".R")[1])
	[1] "character"

Exempli gratia:
> typeof(ls(pattern=".R")[45]) 	
[1] "character"
> ls(pattern=".R")[45] 			 I do not want that:
[1] "wmd.txt.R"
> typeof(wmd.txt.R)			I want that:
[1] "list"

so that I can find the mean of the series on all of these files/loaded
objects with a loop that uses the command
mean(wmd.txt.R) instead of mean("wmd.txt.R") that does not work...

Could you help me, please or propose another way to achieve the same result?

Thank you very much for your assistance,

Tsardounis Constantine



From sdavis2 at mail.nih.gov  Tue Jan  3 16:28:26 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 03 Jan 2006 10:28:26 -0500
Subject: [R] how to work on multiple R objects?...
In-Reply-To: <30ddfdae0601030724o26c9c6a3p2db2eb3d546d97a6@mail.gmail.com>
Message-ID: <BFE0024A.2BEA%sdavis2@mail.nih.gov>




On 1/3/06 10:24 AM, "Constantine Tsardounis" <costas.magnuse at gmail.com>
wrote:

> Hello, Happy New Year!...
> 
> I am encountering a problem trying to work on the data that I load in R.
> 
> I have loaded to R a series of stock data using
> (csv files are named e.g. IBM.R)
> 
> length.R <- length(list.files(".", pattern=".R")) # the number of
> files with one             #column in the directory "./" ending to
> ".R"
> for (i in 1:length.R) {
> assign(read.csv(list.files(".", pattern=".R")[i],
> read.csv(list.files(".", pattern=".R")[i])))
> }

 mylist <- list()
 for (i in list.files('.',pattern='.R')) {
   mylist[[i]] <- read.csv(i)
 }

Sean



From begert at ipb-halle.de  Tue Jan  3 16:30:04 2006
From: begert at ipb-halle.de (begert)
Date: Tue, 03 Jan 2006 16:30:04 +0100
Subject: [R] How to set the size of a rgl window, par3d() ?
Message-ID: <43BA987C.2000607@ipb-halle.de>

Dear R- Users,

   is there a way to determine the size of
   an rgl window (rgl.open()) either in advance or
   afterwards, (without using the mouse, of course) ?

   Intuitively, one would assume to set the size by:

   library("rgl");
   par3d(viewport=c(0,0,500,500));
   #rgl.open();

   for example. As the parameter 'viewport' is 'readonly'
   this results in an error message:
   Error in par3d(viewport = c(0, 0, 500, 500)) :
   invalid value specified for rgl parameter "viewport"
   In addition: Warning message:
   parameter "viewport" cannot be set.

   Any possible workarounds ?

Thanks
Bjoern



From pburns at pburns.seanet.com  Tue Jan  3 16:31:09 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 03 Jan 2006 15:31:09 +0000
Subject: [R] A comment about R:
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
Message-ID: <43BA98BD.7060707@pburns.seanet.com>

I have had an email conversation with the author of the
technical report from which the quote was taken.  I am
formulating a comment to the report that will be posted
with the technical report.

I would be pleased if this thread continued, so I will know
better what I want to say.  Plus I should be able to reference
this thread in the comment.

Regards,

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Rau, Roland wrote:

> > -----Original Message-----
>  
>
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
>>Grothendieck
>>Sent: Monday, January 02, 2006 4:59 PM
>>To: Philippe Grosjean
>>Cc: Kort, Eric; Kjetil Halvorsen; R-help at stat.math.ethz.ch
>>Subject: Re: [R] A comment about R:
>>
>>
>>Probably what is needed is for someone familiar with both Stata and R
>>to create a lexicon in the vein of the Octave to R lexicon
>>
>>   http://cran.r-project.org/doc/contrib/R-and-octave-2.txt
>>
>>to make it easier for Stata users to understand R.  Ditto for 
>>SAS and SPSS.
>>
>>
>>    
>>
>IMO this is a very good proposal but I think that the main problem is
>not the "translation" of one function in SPSS/Stata/SAS to the
>equivalent in R.
>Remembering my first contact with R after using SPSS for some years (and
>having some experience with Stata and SAS) was that your mental
>framework is different. You think in "SPSS-terms" (i.e. you expect that
>data are automatically a rectangular matrix, functions operate on
>columns of this matrix, you have always only one dataset available,
>...). This is why "jumping" from SPSS to Stata is relatively easy. But
>to jump from any of the three to R is much more difficult. 
>This mental barrier is also the main obstacle for me now when I try to
>encourage the use of R to other people who have a similar background as
>I had.
>What can be done about it? I guess the only answer is investing time
>from the user which implies that R will probably never become the
>language of choice for "casual users". But popularity is probably not
>the main goal of the R-Project (it would be rather a nice side-effect).
>
>Just a few thoughts ...
>
>Best,
>Roland
>
>+++++
>This mail has been sent through the MPI for Demographic Rese...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From tlumley at u.washington.edu  Tue Jan  3 16:35:28 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jan 2006 07:35:28 -0800 (PST)
Subject: [R] A comment about R:
In-Reply-To: <43B907B8.3020505@sciviews.org>
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
	<CEA39A213F7F2E44A0DED9210BCD352F697262@VAIEXCH04.vai.org>
	<43B907B8.3020505@sciviews.org>
Message-ID: <Pine.LNX.4.64.0601030722560.25469@homer24.u.washington.edu>

On Mon, 2 Jan 2006, Philippe Grosjean wrote:
>
> That said, I think one should interpret Mitchell's paper in a different
> way. Obviously, he is an unconditional and happy Stata user (he even
> wrote a book about graphs programming in Stata). His claim in favor of
> Stata (versus SAS and SPSS, and also, indirectly, versus R) is to be
> interpreted the same way as unconditional lovers of Macintoshes or PCs
> would argue against the other clan. Both architectures are good and have
> strengths and weaknesses. Real arguments are more sentimental, and could
> resume in: "The more I use it, the more I like it,... and the aliens are
> bad, ugly and stupid!" Would this apply to Stata versus R? I don't know
> Stata at all, but I imagine it could be the case from what I read in
> Mitchell's paper...


I think there are good reasons why Stata is becoming much more popular in 
epidemiology and biostatistics [and I'm not particularly prejudiced 
against R]. In my experience people who like R also like Stata, though 
clearly the reverse is not necessarily true.

Stata, like R, is readily programmable.  Users can -- and do -- write 
and distribute programs that look just like the built-in routines.  There 
is an active and helpful mailing list. However, Stata programming is very 
different from R programming, since it is macro-based (think Tcl/Tk) 
rather than function-based.

Stata is also easier to learn: it has a very consistent syntax and even 
better documentation than R.  We use Stata for all our service course 
teaching, and despite the fact that it is command-line based rather than 
GUI the students were no more unhappy than when SPSS was used for the 
lowest-level courses and Egret for the higher-level service courses. 
[Stata now has a GUI but it is awful and quite a lot of students prefer 
the command-line]


 	-thomas



From br44114 at gmail.com  Tue Jan  3 16:35:38 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 3 Jan 2006 10:35:38 -0500
Subject: [R] bookmarking a page inside r-project.org
Message-ID: <8d5a36350601030735y55bfcd42m6a6f43fba5bba028@mail.gmail.com>

In fact it's just as easy in Internet Explorer: right-click + Open in
New Window, or Shift-Click, followed by Ctrl+D. Or, right-click + Add
to Favorites.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Charles Annis, P.E.
> Sent: Monday, January 02, 2006 8:15 PM
> To: 'Jonathan Baron'; r-help at stat.math.ethz.ch
> Subject: Re: [R] bookmarking a page inside r-project.org
>
>
> You can do something similar with Microsoft's browser but it
> isn't quite as
> easy as Foxfire:
>
> Right-click on the frame and choose Properties.  Then
> highlight and copy the
> URL and paste into the address window and click Go.
>
> Then save the page.
>
>
>
> Charles Annis, P.E.
>
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathan Baron
> Sent: Monday, January 02, 2006 7:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] bookmarking a page inside r-project.org
>
> I'm replying to:
> https://stat.ethz.ch/pipermail/r-help/2006-January/083823.html
>
> In Firefox (a browser), right click on the frame.  Then you get a
> menu that has bookmark as one of the options.  Firefox is
> available from http://www.mozilla.org.
>
> Jon
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From shigesong at gmail.com  Tue Jan  3 16:39:27 2006
From: shigesong at gmail.com (Shige Song)
Date: Tue, 3 Jan 2006 23:39:27 +0800
Subject: [R] Package for multiple membership model?
In-Reply-To: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
References: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
Message-ID: <5abc11d80601030739x4567b053h886c00db557672dc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/62cef679/attachment.pl

From tlumley at u.washington.edu  Tue Jan  3 16:41:19 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jan 2006 07:41:19 -0800 (PST)
Subject: [R] Package for multiple membership model?
In-Reply-To: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
References: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
Message-ID: <Pine.LNX.4.64.0601030740400.25469@homer24.u.washington.edu>


On Tue, 3 Jan 2006, Brian Perron wrote:

> Hello all:
>
> I am interested in computing what the multilevel modeling literature 
> calls a multiple membership model.  More specifically, I am working with 
> a data set involving clients and providers.  The clients are the 
> lower-level units who are nested within providers (higher-level). 
> However, this is not nesting in the usual sense, as clients can belong 
> to multple providers, which I understand makes this a "multiple 
> membership model."  Right now, I would like to keep this simple, using 
> only a continuous dependent variable, but would like to also extend this 
> to a repeated measures design.  This doesn't seem to be possible with 
> the lme package.  Is there something else I could consider? Thanks,

I think you want lmer() in the lme4 & Matrix packages. It allows crossed 
random effects.

 	-thomas



From p.dalgaard at biostat.ku.dk  Tue Jan  3 16:46:52 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jan 2006 16:46:52 +0100
Subject: [R] Package for multiple membership model?
In-Reply-To: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
References: <84C59624B1B0204BBCB3B7DDF981AE63010AD031@GWB-PO.gwb.wustl.edu>
Message-ID: <x2u0clfh5f.fsf@viggo.kubism.ku.dk>

"Brian Perron" <beperron at wustl.edu> writes:

> Hello all:  
>  
> I am interested in computing what the multilevel modeling literature
> calls a multiple membership model. More specifically, I am working
> with a data set involving clients and providers. The clients are the
> lower-level units who are nested within providers (higher-level).
> However, this is not nesting in the usual sense, as clients can
> belong to multple providers, which I understand makes this a
> "multiple membership model." Right now, I would like to keep this
> simple, using only a continuous dependent variable, but would like
> to also extend this to a repeated measures design. This doesn't seem
> to be possible with the lme package. Is there something else I could
> consider? Thanks, Brian

You could take a look at the lmer() function in the lme4/Matrix
packages - see the Rnews 2005/1 article. One potential problem is that
for repeated measurements, it is not (currently?) as strong on
correlation structure as lme().

You can actually deal with crossed random effects in lme() too, it
just gets a little more complicated, involving things like

library(nlme)
data(Assay)
as1 <- lme(logDens~sample*dilut, data=Assay,
           random=pdBlocked(list(
                     pdIdent(~1),
                     pdIdent(~sample-1),
                     pdIdent(~dilut-1))))

as2 <- lme(logDens~sample*dilut, data=Assay,
           random=list(Block=pdBlocked(list(
                     pdIdent(~1),
                     pdIdent(~sample-1))),dilut=~1))

as3 <- lme(logDens~sample*dilut, data=Assay,
           random=list(Block=~1,
                     Block=pdIdent(~sample-1),
                     dilut=~1))

which all fit the same model (but get the DF wrong in three different
ways...)

This is slightly different from your example because the crossed
factors are nested in "Block", but you can always fake a nesting using

one <- rep(1, length(logDens)) #or whatever 
lme(...., random=list(one=~....))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jsorkin at grecc.umaryland.edu  Tue Jan  3 16:46:26 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 03 Jan 2006 10:46:26 -0500
Subject: [R] under (and over) dispersion in Poisson regression
Message-ID: <s3ba5620.091@medicine.umaryland.edu>

I am trying to use Poisson regression to model count data. My results are suggestive of under dispersion (0.79). How close to one does one want the measure of dispersion to be before one accepts the results of the analysis?

I know that there is no definitive answer to my question, but I would like to get some sense of general practice.
Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From jfox at mcmaster.ca  Tue Jan  3 15:35:54 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 3 Jan 2006 09:35:54 -0500
Subject: [R] A comment about R:
In-Reply-To: <43BA195C020000C90000177C@MAIL.NDRI.ORG>
Message-ID: <20060103143551.ZJJO8316.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter et al.,

It's not reasonable to argue with someone's experience -- that is, if people
tell me that they found R harder to learn than SAS, say, then I believe them
-- but that's not my experience in teaching relatively inexperienced
students to use statistical software. A few points:

(1) Casual and initial  use of statistical software is easier through a GUI,
so it's not reasonable, for example, to compare learning to use SPSS via its
GUI to learning R via commands.

(2) I don't believe that it's hard to teach a useful initial subset of R
commands. Which commands are in the subset will depend somewhat on what one
is trying to do. I believe that there are several examples of this approach,
including my R and S-PLUS Companion to Applied Regression. Likewise,
starting with a simple modus operandi, such as working with a single
attached data frame, can cut through a lot of the complexity. Once someone
is comfortable with basic use of R, expanding knowledge of functions,
packages, and other ways of handling data comes naturally. 

(3) I don't find R less uniform than SAS or SPSS, particularly in the way
that statistical models are handled. Moreover, trying to do something
innovative or non-standard in SAS is relatively difficult (in my
experience), and even harder in SPSS. I'm less familiar with Stata, but
uniformity seems one of its strengths. (The Stata scripting language puts me
off, however.)

(4) Not everyone has the same experience and thinks in the same way. I've
used many different statistical packages and computing environments, and
have learned quite a few programming languages (most of which I can no
longer use). Of these, I found APL and R the easiest to learn, and Lisp
(Lisp-Stat) the hardest. Sometimes, though, it's worth expending the effort
to learn something that's difficult -- I feel that I got a lot out of
learning to program in Lisp, for example.

(5) The essential point is that how hard one finds it to learn something is
a function of the intrinsic difficulty of the thing, the person's previous
experience, preferred modes of thinking, etc., and how learning is
approached.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Flom
> Sent: Tuesday, January 03, 2006 6:28 AM
> To: Rau at demogr.mpg.de; ggrothendieck at gmail.com
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] A comment about R:
> 
> >>> "Rau, Roland" <Rau at demogr.mpg.de>  >>> wrote
> <<<
> IMO this is a very good proposal but I think that the main 
> problem is not the "translation" of one function in 
> SPSS/Stata/SAS to the equivalent in R.
> Remembering my first contact with R after using SPSS for some 
> years (and having some experience with Stata and SAS) was 
> that your mental framework is different. You think in 
> "SPSS-terms" (i.e. you expect that data are automatically a 
> rectangular matrix, functions operate on columns of this 
> matrix, you have always only one dataset available, ...). 
> This is why "jumping" from SPSS to Stata is relatively easy. 
> But to jump from any of the three to R is much more difficult. 
> This mental barrier is also the main obstacle for me now when 
> I try to encourage the use of R to other people who have a 
> similar background as I had.
> What can be done about it? I guess the only answer is 
> investing time from the user which implies that R will 
> probably never become the language of choice for "casual 
> users". But popularity is probably not the main goal of the 
> R-Project (it would be rather a nice side-effect).
> >>>>
> 
> 
> 
> As someone who uses SAS qutie a bit and R somewhat less, I 
> think Roland makes some excellent points.  Going from SPSS to 
> SAS (which I once did) is like going from Spansih to French.  
> Going from SAS to R (which I am trying to do) is like going 
> from English to Chinese.
> 
> But it's more than that.  
> 
> Beyond the obvious differences in the languages is a 
> difference in how they are written about; and how they are 
> improved.  SAS documentation is much lengthier than R's.  
> Some people like the terseness of R's help.  Some like the 
> verboseness of SAS's.  SOme of this difference is doubtless 
> due to the fact that SAS is commercial, and pays people to 
> write the documentation.  I have tremednous appreciation for 
> the unpaid effort that goes into R, and nothing I say here 
> should be seen as detracting from that.
> 
> As to how they are improved, the fact that R is extended (in 
> part) by packages written by many many different people is 
> good, becuase it means that the latest techniques can be 
> written up, often by the people who invent the techniques 
> (and, again, I appreciate this tremendously), but it does 
> mean that a) It is hard to know what is out there at any 
> given time; b) the styles of pacakages difer somewhat.
> 
> In addition, I think the distinction between 'casual user' 
> and serious user is something of a false dichotomy.
> It's really a continuum, or, probably, several continua, that 
> make R harder or easier for people to learn.
> 
> I like R.  I like it a lot.  I like that it's free.  I like 
> that it's cutting edge.  I like that it can do amazing graphics.
> I like that the code is open.  I like that I can write my own 
> functions in the same language.  And, again, I am amazed at 
> the amount of time and effort people put into it.
> 
>  But I do think that the link in the original post made some 
> good points, and the writer of that post is not the only one 
> who has found R difficult to learn.
> 
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From flom at ndri.org  Tue Jan  3 15:49:47 2006
From: flom at ndri.org (Peter Flom)
Date: Tue, 03 Jan 2006 09:49:47 -0500
Subject: [R] A comment about R:
In-Reply-To: <20060103143551.ZJJO8316.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <43BA195C020000C90000177C@MAIL.NDRI.ORG>
	<20060103143551.ZJJO8316.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <43BA48BA.B875.00C9.0@ndri.org>

>>> "John Fox" <jfox at mcmaster.ca> 1/3/2006 9:35 am >>> as always,
raises some excellent points.  I have some responses, interspersed

<<<<
It's not reasonable to argue with someone's experience -- that is, if
people
tell me that they found R harder to learn than SAS, say, then I believe
them
-- but that's not my experience in teaching relatively inexperienced
students to use statistical software. A few points:
>>>

A lot of this probably has to do with what you learned first.  I
learned SAS long
before I learned R.  Had it been reversed, I would probably find SAS
hard.  

<<<
(1) Casual and initial  use of statistical software is easier through a
GUI,
so it's not reasonable, for example, to compare learning to use SPSS
via its
GUI to learning R via commands.
>>>

True, but I was comparing SAS and R, and this originally started
with STATA and R, and all 3 of those are command driven.

<<<<
(4) Not everyone has the same experience and thinks in the same way.
I've
used many different statistical packages and computing environments,
and
have learned quite a few programming languages (most of which I can no
longer use). Of these, I found APL and R the easiest to learn, and
Lisp
(Lisp-Stat) the hardest. Sometimes, though, it's worth expending the
effort
to learn something that's difficult -- I feel that I got a lot out of
learning to program in Lisp, for example.
>>>>

This is, I think, a big part of it.  I think that R would be a lot
easier to learn for
someone who has learned some other computer language.  I have not.  

I agree that learning something difficult can often be worth it.


Peter



From billemont at cegetel.net  Tue Jan  3 17:07:46 2006
From: billemont at cegetel.net (billemont@cegetel.net)
Date: Tue, 3 Jan 2006 17:07:46 +0100 (CET)
Subject: [R] cox model
Message-ID: <11118.164.2.255.244.1136304466.squirrel@monmail.cegetel.net>

I'm a french medicine student and i work on oncology. I work about
treatment oh breast cancer. I have 3 sub group of patient. I made some
kaplan meyer survival curve, and i made a cox model.

On the survival curve, on the last observations there is a crossmatch of
the different survival curve at 150 month .
I study the validity of my model by study of residual by  study the
proportional risk. So i used Coxzph formula, but the global test is p<
0.05, so my model is not a  proportional risk.

Do you know how i can cut the cox model analysis before the 150 month,
wich are the time where the curves are  crossing?

Thank you for your Help

Dr Billemont



From murdoch at stats.uwo.ca  Tue Jan  3 17:13:17 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Jan 2006 11:13:17 -0500
Subject: [R] How to set the size of a rgl window, par3d() ?
In-Reply-To: <43BA987C.2000607@ipb-halle.de>
References: <43BA987C.2000607@ipb-halle.de>
Message-ID: <43BAA29D.8060003@stats.uwo.ca>

On 1/3/2006 10:30 AM, begert wrote:
> Dear R- Users,
> 
>    is there a way to determine the size of
>    an rgl window (rgl.open()) either in advance or
>    afterwards, (without using the mouse, of course) ?
> 
>    Intuitively, one would assume to set the size by:
> 
>    library("rgl");
>    par3d(viewport=c(0,0,500,500));
>    #rgl.open();
> 
>    for example. As the parameter 'viewport' is 'readonly'
>    this results in an error message:
>    Error in par3d(viewport = c(0, 0, 500, 500)) :
>    invalid value specified for rgl parameter "viewport"
>    In addition: Warning message:
>    parameter "viewport" cannot be set.
> 
>    Any possible workarounds ?

Not that I know of.  This is handled by OpenGL and the windowing system; 
rgl just queries OpenGL to give the par3d("viewport") response.

It would take a bit of time to add this, because it would need to be 
added for all 3 output devices (Windows, X11, OSX).

Duncan Murdoch



From mbock at Environcorp.com  Tue Jan  3 17:25:53 2006
From: mbock at Environcorp.com (Mike Bock)
Date: Tue, 3 Jan 2006 10:25:53 -0600
Subject: [R] Summary functions to dataframe
Message-ID: <EB693868E54B314483E8E28E8BAAC6EF039D45@chisrv01.environchicago.environ.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/2deefad8/attachment.pl

From loesljrg at verizon.net  Tue Jan  3 17:35:56 2006
From: loesljrg at verizon.net (JRG)
Date: Tue, 03 Jan 2006 11:35:56 -0500
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.64.0601030722560.25469@homer24.u.washington.edu>
References: <43B907B8.3020505@sciviews.org>
Message-ID: <43BA619C.12032.9F21E9@localhost>

On 3 Jan 2006 at 7:35, Thomas Lumley wrote:

> On Mon, 2 Jan 2006, Philippe Grosjean wrote:
> >
> > That said, I think one should interpret Mitchell's paper in a different
> > way. Obviously, he is an unconditional and happy Stata user (he even
> > wrote a book about graphs programming in Stata). His claim in favor of
> > Stata (versus SAS and SPSS, and also, indirectly, versus R) is to be
> > interpreted the same way as unconditional lovers of Macintoshes or PCs
> > would argue against the other clan. Both architectures are good and have
> > strengths and weaknesses. Real arguments are more sentimental, and could
> > resume in: "The more I use it, the more I like it,... and the aliens are
> > bad, ugly and stupid!" Would this apply to Stata versus R? I don't know
> > Stata at all, but I imagine it could be the case from what I read in
> > Mitchell's paper...
> 
> 
> I think there are good reasons why Stata is becoming much more popular in 
> epidemiology and biostatistics [and I'm not particularly prejudiced 
> against R]. In my experience people who like R also like Stata, though 
> clearly the reverse is not necessarily true.
> 
> Stata, like R, is readily programmable.  Users can -- and do -- write 
> and distribute programs that look just like the built-in routines.  There 
> is an active and helpful mailing list. However, Stata programming is very 
> different from R programming, since it is macro-based (think Tcl/Tk) 
> rather than function-based.
> 
> Stata is also easier to learn: it has a very consistent syntax and even 
> better documentation than R.  We use Stata for all our service course 
> teaching, and despite the fact that it is command-line based rather than 
> GUI the students were no more unhappy than when SPSS was used for the 
> lowest-level courses and Egret for the higher-level service courses. 
> [Stata now has a GUI but it is awful and quite a lot of students prefer 
> the command-line]
> 
> 
>  	-thomas
> 

I'll offer a Second to Thomas's motion.

I like R but I find Stata much easier to teach in service courses.  For most of my students, the Stata learning curve is much more 
tolerable than that of R (at a reduction in capability, of course).  I state on Day 1 that I think R is the world's best package, 
and that Stata is my choice for a very acceptable compromise --- for most purposes.  A few students go on to write their own Stata 
programs, and a few go on to learn R and love it.  

But the vast majority of my students learn enough Stata to get through the courses, and afterward they do whatever their advisor 
wants them to do (the First Law of Graduate School).  For a sizable fraction (maybe 25%), that also proves to be Stata, as there is 
a solid core of Stata users among the faculty here.

I'l also agree that Stata's GUI is ghastly; most of my students (both during courses and any later use) quickly adapt to using 
Stata's command line, and they use it quite effectively.

---JRG

John R. Gleason
Associate Professor

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers



From p.dalgaard at biostat.ku.dk  Tue Jan  3 17:53:40 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jan 2006 17:53:40 +0100
Subject: [R] A comment about R:
In-Reply-To: <43BA98BD.7060707@pburns.seanet.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
Message-ID: <x2psn9fe23.fsf@viggo.kubism.ku.dk>

Patrick Burns <pburns at pburns.seanet.com> writes:

> I have had an email conversation with the author of the
> technical report from which the quote was taken.  I am
> formulating a comment to the report that will be posted
> with the technical report.
> 
> I would be pleased if this thread continued, so I will know
> better what I want to say.  Plus I should be able to reference
> this thread in the comment.

One thing that is often overlooked, and hasn't yet been mentioned in
the thread, is how much *simpler* R can be for certain completely
basic tasks of practical or pedagogical relevance: Calculate a simple
derived statistic, confidence intervals from estimate and SE,
percentage points of the binomial distribution - using dbinom or from
the formula, take the sum of each of 10 random samples from a set of
numbers, etc. This is where other packages get stuck in the
procedure+dataset mindset.

For much the same reason, those packages make you tend to treat
practical data analysis as something distinct from theoretical
understanding of the methods: You just don't use SAS or SPSS or Stata
to illustrate the concept of a random sample by setting up a small
simulation study as the first thing you do in a statistics class,
whereas you could quite conceivably do it in R. (What *is* the
equivalent of rnorm(25) in those languages, actually?)

Even when using SAS in teaching, I sometimes fire up R just to
calculate simple things like

  pbar <- (p1+p2)/2
  sqrt(pbar*(1-pbar))

which you need to cheat SAS Analyst's sample size calculator to work
with proportions rather than means. SAS leaves you no way to do this
short of setting up a new data set. The Windows calculator will do it,
of course, but the students can't see what you are doing then.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ruser2006 at yahoo.com  Tue Jan  3 17:59:19 2006
From: ruser2006 at yahoo.com (r user)
Date: Tue, 3 Jan 2006 08:59:19 -0800 (PST)
Subject: [R] For loop gets exponentially slower as dataset gets larger...
Message-ID: <20060103165919.49819.qmail@web37003.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/28ae0b48/attachment.pl

From ripley at stats.ox.ac.uk  Tue Jan  3 18:03:46 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jan 2006 17:03:46 +0000 (GMT)
Subject: [R] under (and over) dispersion in Poisson regression
In-Reply-To: <s3ba5620.091@medicine.umaryland.edu>
References: <s3ba5620.091@medicine.umaryland.edu>
Message-ID: <Pine.LNX.4.61.0601031700080.30732@gannet.stats>

This most often indicates a problem with dispersion estimate.  See the 
cautionary tale in MASS4 chapter 7.  If you have a reliable dispersion
estimate that low for genuine counts, they are either not independent or 
not Poisson (for example, limited), and one would want to find out what is 
going on.

On Tue, 3 Jan 2006, John Sorkin wrote:

> I am trying to use Poisson regression to model count data. My results 
> are suggestive of under dispersion (0.79). How close to one does one 
> want the measure of dispersion to be before one accepts the results of 
> the analysis?
>
> I know that there is no definitive answer to my question, but I would 
> like to get some sense of general practice.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pmuhl1848 at gmail.com  Tue Jan  3 18:14:38 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Tue, 03 Jan 2006 12:14:38 -0500
Subject: [R]  Bootstrap w/ Clustered Data
Message-ID: <BFE01B2E.12774%pmuhl1848@gmail.com>

Looks like I may have found a function that addresses my needs.  Bootcov in
Design handles bootstrapping from clustered data and will save the
coefficients.  I'm not entirely sure it handles clusters the way I'd like,
but I'm going through the code.  If it doesn't, it looks easily
re-writeable.  As far as I can tell, boot in package boot would do clusters
only if the estimation function passed to it pastes together data based on
the clusters boot selects.



From ggrothendieck at gmail.com  Tue Jan  3 18:23:59 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jan 2006 12:23:59 -0500
Subject: [R] Summary functions to dataframe
In-Reply-To: <EB693868E54B314483E8E28E8BAAC6EF039D45@chisrv01.environchicago.environ.local>
References: <EB693868E54B314483E8E28E8BAAC6EF039D45@chisrv01.environchicago.environ.local>
Message-ID: <971536df0601030923r46f8c41ag52b4037fd8a99f03@mail.gmail.com>

Try this:

Pstats <- function(x) c(Max = max(x),
   Min = min(x),
   AMean = mean(x),
   AStdev = sd(x),
   Samples = length(x),
   quantile(x, 1:9/10, na.rm = TRUE))

res <- with(areas, by(AdRes, N_Type, Pstats))
do.call("rbind", res)

Also, check out summaryBy in the doBy package at
http://genetics.agrsci.dk/~sorenh/misc/index.html



On 1/3/06, Mike Bock <mbock at environcorp.com> wrote:
> I have written a few different summary functions. I want to calculate
> the statistics by groups and I am having trouble getting the output as a
> dataframe. I have attached one example with a small dataset that
> calculates summary stats and percentiles, I have others that calculate
> upper confidence limits etc. I would like the output to be converted to
> a dataframe with one of the columns as the grouping variable. This seems
> simple but my attempts with do.call("cbind") and rbind have not worked
> so I have concluded I a missing something obvious. Any help is
> appreciated.
>
> Thanks,
> Mike
>
>
>
> areas <- structure (list(N_Type = structure(c(4, 1, 4, 1, 1, 4, 1, 4, 4,
> 1, 4, 1, 4, 1, 4, 1, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 4, 1,
> 4, 1, 4, 1, 4, 1, 4, 1, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1,
> 4, 1, 4, 4, 1, 4, 1, 2, 1, 2, 1, 4, 1, 4, 1, 4, 1, 4, 1, 1, 4,
> 1, 4, 1, 4, 1, 4, 4, 1, 4, 1, 2, 1, 2, 1, 1, 4, 1, 4, 4, 1, 4,
> 1, 4, 1, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 4, 1,
> 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,
> 1, 4, 1, 4, 1, 1, 4, 1, 4, 2, 1, 2, 1, 1, 4, 1, 4, 1, 4, 4, 1,
> 4, 1), .Label = c("All", "Inside 370", "Not Applicable", "Outside 370"
> ), class = "factor"), AdRes = c(23.7, 23.7, 42.4, 42.4, 630,
> 630, 990, 990, 72.85, 72.85, 70.6, 70.6, 10, 10, 21.7, 21.7,
> 171.66, 171.66, 306, 306, 62.1, 62.1, 53.25, 53.25, 208, 208,
> 64.8, 64.8, 87.3, 87.3, 356, 356, 25.8, 25.8, 156, 156, 166,
> 166, 135.5, 135.5, 170.5, 170.5, 203, 203, 227.5, 227.5, 224,
> 224, 123, 123, 140.66, 140.66, 142.5, 142.5, 44.65, 44.65, 50.3,
> 50.3, 1320, 1320, 577, 577, 71.1, 71.1, 411, 411, 104, 104, 122,
> 122, 201, 201, 230, 230, 192, 192, 304, 304, 184.5, 184.5, 350,
> 350, 536, 536, 470.5, 470.5, 172, 172, 166, 166, 205, 205, 595,
> 595, 227.5, 227.5, 9.1, 9.1, 14.6, 14.6, 10.9, 10.9, 11.1, 11.1,
> 313.5, 313.5, 53.8, 53.8, 29.8, 29.8, 29.5, 29.5, 34.05, 34.05,
> 21.8, 21.8, 385.5, 385.5, 541, 541, 168, 168, 119, 119, 376,
> 376, 91.9, 91.9, 97.76, 97.76, 164, 164, 244, 244, 303.5, 303.5,
> 388, 388, 59.8, 59.8, 227.5, 227.5, 165, 165, 19.15, 19.15, 651,
> 651, 195, 195, 190, 190, 164, 164, 190, 190, 334, 334)), .Names =
> c("N_Type",
> "AdRes"), row.names = c("8956", "8957", "8972", "8973", "8974",
> "8975", "8976", "8977", "8978", "8979", "8980", "8981", "8982",
> "8983", "8984", "8985", "9159", "9160", "9175", "9176", "9177",
> "9178", "9185", "9186", "9201", "9202", "9203", "9204", "9205",
> "9206", "9207", "9208", "9209", "9210", "9217", "9218", "9233",
> "9234", "9241", "9242", "9261", "9262", "9277", "9278", "9285",
> "9286", "9301", "9302", "9309", "9310", "9329", "9330", "9345",
> "9346", "9353", "9354", "9369", "9370", "9371", "9372", "9373",
> "9374", "9410", "9411", "9412", "9413", "9414", "9415", "9422",
> "9423", "9424", "9425", "9426", "9427", "9428", "9429", "9430",
> "9431", "9432", "9433", "9434", "9435", "9436", "9437", "9444",
> "9445", "9452", "9453", "9454", "9455", "9456", "9457", "9458",
> "9459", "9460", "9461", "9468", "9469", "9470", "9471", "9472",
> "9473", "9474", "9475", "9476", "9477", "9478", "9479", "9480",
> "9481", "9488", "9489", "9496", "9497", "9498", "9499", "9720",
> "9721", "9722", "9723", "9724", "9725", "9726", "9727", "9728",
> "9729", "9730", "9731", "9732", "9733", "9734", "9735", "9736",
> "9737", "9738", "9739", "9740", "9741", "9742", "9743", "9744",
> "9745", "9746", "9747", "9748", "9749", "9750", "9751", "9752",
> "9753", "9754", "9755", "9756", "9757", "9758", "9759", "9760",
> "9761"), class = "data.frame")
>
>
> Pstats <- function(x)
>                {
>    Max = max(x)
>    Min = min(x)
>                AMean = mean(x)
>                AStdev = sd(x)
>                Samples <- length(x)
>                p10 <- quantile(x,0.1,na.rm = TRUE, names = FALSE)
>                p20 <- quantile(x,0.2,na.rm = TRUE, names = FALSE)
>                p30 <- quantile(x,0.3,na.rm = TRUE, names = FALSE)
>                p40 <- quantile(x,0.4,na.rm = TRUE, names = FALSE)
>                p50 <- quantile(x,0.5,na.rm = TRUE, names = FALSE)
>                p60 <- quantile(x,0.6,na.rm = TRUE, names = FALSE)
>                p70 <- quantile(x,0.7,na.rm = TRUE, names = FALSE)
>                p80 <- quantile(x,0.8,na.rm = TRUE, names = FALSE)
>                p90 <- quantile(x,0.9,na.rm = TRUE, names = FALSE)
>    Result <- data.frame(Samples,AMean,AStdev,
> Min,Max,p10,p20,p30,p40,p50,p60,p70,p80,p90)
>    return(Result)
>    #write.table(Result, file = "Results.csv", sep = ",",row.names =
> FALSE)
>        }
>
> attach(areas)
> res <- by(areas, N_Type, function (x)
>  (Pstats(AdRes)))
>
> #need to convert res to a dataframe
>
>
>
> Michael Bock, PhD
> ENVIRON International Corporation
> 136 Commercial Street, Suite 402
> Portland, ME 04101
> phone: 207.347.4413
> fax: 207.347.4384
>
>
>
>
> This message contains information that may be confidential, ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From liuwensui at gmail.com  Tue Jan  3 18:40:54 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 3 Jan 2006 12:40:54 -0500
Subject: [R] A comment about R:
In-Reply-To: <x2psn9fe23.fsf@viggo.kubism.ku.dk>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
Message-ID: <1115a2b00601030940nc6488e7m193b62fb05517e39@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/76628bda/attachment.pl

From BEN at SSANET.COM  Tue Jan  3 18:42:31 2006
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 3 Jan 2006 11:42:31 -0600
Subject: [R] A comment about R:
Message-ID: <CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>

One implicit point in Kjetil's message is the difficulty of learning
enough of R to make its use a natural and desired "first choice
alternative," which I see as the point at which real progress and
learning commence with any new language.  I agree that the long learning
curve is a serious problem, and in the past I have discussed, off list,
with one of the very senior contributors to this list the possibility of
splitting the list into sections for newcomers and for advanced users.
He gave some very cogent reasons for not splitting, such as the
possibility of newcomers' getting bad advice from others only slightly
more advanced than themselves.  And yet I suspect that a newcomers'
section would encourage the kind of mutually helpful collegiality among
newcomers that now characterizes the exchanges of the more experienced
users on this list.  I know that I have occasionally been reluctant to
post issues that seem too elementary or trivial to vex the others on the
list with and so have stumbled around for an hour or so seeking the
solution to a simple problem.  Had I the counsel of others similarly
situated progress might have been far faster.  Have other newcomers or
occasional users had the same experience?

Is it time to reconsider splitting this list into two sections?
Certainly the volume of traffic could justify it.

Ben Fairbank

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kjetil Halvorsen
Sent: Sunday, January 01, 2006 8:37 AM
To: R-help at stat.math.ethz.ch
Subject: [R] A comment about R:


Readers of this list might be interested in the following commenta about
R.


In a recent report, by Michael N. Mitchell
http://www.ats.ucla.edu/stat/technicalreports/
says about R:


"Perhaps the most notable exception to this discussion is R, a language
for statistical computing and graphics. R is free to download under the
terms of the GNU General Public License (see http://www.r-project.
org/). Our web site has resources on R and I have tried, sometimes in
great earnest, to learn and understand R. I have learned and used a
number of statistical packages (well over 10) and a number of
programming languages (over 5), and I regret to say that I have had
enormous diffculties learning and using R. I know that R has a great fan
base composed of skilled and excellent statisticians, and that includes
many people from the UCLA statistics department. However, I feel like R
is not so much of a statistical package as much as it is a statistical
programming environment that has many new and cutting edge features. For
me learning R has been very diffcult and I have had a very hard time
finding answers to many questions about using it. Since the R community
tends to be composed of experts deeply enmeshed in R, I often felt that
I was missing half of the pieces of the puzzle when reading information
about the use of R { it often feels like there is an assumption that
readers are also experts in R. I often found the documentation for R
quite sparse and many essential terms or constructs were used but not
defined or cross-referenced. While there are mailing lists regarding R
where people can ask questions, there is no offcial "technical support".
Because R is free and is based on the contributions of the R community,
it is extremely extensible and programmable and I have been told that it
has many cutting edge features, some not available anywhere else.
Although R is free, it may be more costly in terms of your time to
learn, use, and obtain support for it. My feeling is that R is much more
suited to the sort of statistician who is oriented towards working very
deeply with it. I think R is the kind of package that you really need to
become immersed in (like a foreign language) and then need to use on a
regular basis. I think that it is much more diffcult to use it casually
as compared to SAS, Stata or SPSS. But by devoting time and effort to it
would give you access to a programming environment where you can write R
programs and collaborate with others who are also using R. Those who are
able to access its power, even at an applied level, would be able to
access tools that may not be found in other packages, but this might
come with a serious investment of time to suffciently use R and maintain
your skills with R."


Kjetil

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From flom at ndri.org  Tue Jan  3 19:11:53 2006
From: flom at ndri.org (Peter Flom)
Date: Tue, 03 Jan 2006 13:11:53 -0500
Subject: [R] A comment about R:
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>
Message-ID: <43BA7818.B875.00C9.0@ndri.org>

>>> "Ben Fairbank" <BEN at SSANET.COM> 1/3/2006 12:42 pm >>> wrote
<<<
One implicit point in Kjetil's message is the difficulty of learning
enough of R to make its use a natural and desired "first choice
alternative," which I see as the point at which real progress and
learning commence with any new language.  I agree that the long
learning
curve is a serious problem, and in the past I have discussed, off
list,
with one of the very senior contributors to this list the possibility
of
splitting the list into sections for newcomers and for advanced users.
He gave some very cogent reasons for not splitting, such as the
possibility of newcomers' getting bad advice from others only slightly
more advanced than themselves.  And yet I suspect that a newcomers'
section would encourage the kind of mutually helpful collegiality
among
newcomers that now characterizes the exchanges of the more experienced
users on this list.  I know that I have occasionally been reluctant to
post issues that seem too elementary or trivial to vex the others on
the
list with and so have stumbled around for an hour or so seeking the
solution to a simple problem.  Had I the counsel of others similarly
situated progress might have been far faster.  Have other newcomers or
occasional users had the same experience?
>>>

I, for one, have had this experience.  I am usually hesitant to post
elementary questions here.

However, I think that the 'cogent reasons' given by 'one of the very
senior contributors' are valid.
I think that  a 'newcomers list' would only really be useful if it
included some experts who could respond,
out of generosity.  I don't think the R community lacks generosity -
obviously not, given all the thousands of 
hours people have spent writing the language and all the packages and
so on.  

But these generous people have different abilities and get pleasure in
different ways.  Some people get a thrill
out of answering complex questions that require them to come up with
novel solutions involving complex code.
Some people get a thrill out of helping newbies over the humps. 
Dividing the lists might help the experts, as much as it helps the
beginners. 


Peter



From gunter.berton at gene.com  Tue Jan  3 19:23:17 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 3 Jan 2006 10:23:17 -0800
Subject: [R] A comment about R:
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>
Message-ID: <200601031823.k03IN7Hc026878@volta.gene.com>

Ummmm....

I cannot say how easy or hard R is to learn, but in response to the UCLA
commentary:

> However, I 
> feel like R
> is not so much of a statistical package as much as it is a statistical
> programming environment that has many new and cutting edge 
> features. 

Please note: the first sentence of the Preface of THE Green Book
(PROGRAMMING WITH DATA: A GUIDE TO THE S LANGUAGE) by John Chambers, the
inventor of the S Language, explicitly states:
 
"S is a programming language and environment for all kinds of computing
involving data."

I think this says that R is **not** meant to be a statistical package in the
conventional sense and should not be considered one. As computing involving
data is a complex and frequently messy business on both technical
(statistics), practical (messy data), and aesthetic (graphics, tables)
levels, it is perhaps to be expected that "a programming language and
environment for all kinds of computing involving data"  is complex.
Personally, I find that (Chambers's next sentence) R's ability "To turn
ideas into software, quickly and faithfully," to be a boon. But, then again,
I'm a statistical professional and not a "casual user."

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box



From pburns at pburns.seanet.com  Tue Jan  3 19:28:36 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 03 Jan 2006 18:28:36 +0000
Subject: [R] A comment about R:
In-Reply-To: <1115a2b00601030940nc6488e7m193b62fb05517e39@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>	<43BA98BD.7060707@pburns.seanet.com>	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<1115a2b00601030940nc6488e7m193b62fb05517e39@mail.gmail.com>
Message-ID: <43BAC254.50105@pburns.seanet.com>

Wensui Liu wrote:

>Another big difference between R and other computing language such as
>SPSS/SAS/STATA.
>You can easily get a job using SPSS/SAS/STATA. But it is extremely difficult
>to find a job using R. ^_^.
>  
>

Actually in finance it is getting easier all the time for
knowledge of R to be a significant benefit.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From Eric.Kort at vai.org  Tue Jan  3 19:37:42 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Tue, 3 Jan 2006 13:37:42 -0500
Subject: [R] A comment about R:
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC16D@VAIEXCH04.vai.org>



Berton Gunter writes....
> Ummmm....
> 
> I cannot say how easy or hard R is to learn, but in response to the
UCLA
> commentary:
> 
> > However, I
> > feel like R
> > is not so much of a statistical package as much as it is a
statistical
> > programming environment that has many new and cutting edge
> > features.
> 
> Please note: the first sentence of the Preface of THE Green Book
> (PROGRAMMING WITH DATA: A GUIDE TO THE S LANGUAGE) by John Chambers,
the
> inventor of the S Language, explicitly states:
> 
> "S is a programming language and environment for all kinds of
computing
> involving data."
> 
> I think this says that R is **not** meant to be a statistical package
in
> the
> conventional sense and should not be considered one. As computing
> involving
> data is a complex and frequently messy business on both technical
> (statistics), practical (messy data), and aesthetic (graphics, tables)
> levels, it is perhaps to be expected that "a programming language and
> environment for all kinds of computing involving data"  is complex.
> Personally, I find that (Chambers's next sentence) R's ability "To
turn
> ideas into software, quickly and faithfully," to be a boon. <snip>

Right.  

So in 2 months I will finish my MD program here in the U.S.  I also have
a master's degree in Epidemiology (in which we used SAS)--but that
hardly qualifies me as statistics expert.  Nonetheless, I have learned
to use R out of necessity without undue difficulty.  So have multiple of
my colleagues around me with MDs, PhDs, and Master's degrees.  We do
mainly microarray analysis, so the availability of a rapidly developing
and customizable toolset (BioC packages) is essential to our work.

And, in the same vein of others' comments, R's "nuts and bolts"
characteristics make me think, learn, and improve.  And the fear of
getting Ripleyed on the mailing list also makes me think, read, and
improve before submitting half baked questions to the list.

So in sum, I use R because it encourages thoughtful analysis, it is
flexible and extensible, and it is free.  I feel that these are
strengths of the environment, not weaknesses.  So if an individual finds
another tool better suited for their work that is obviously just fine,
but I hardly think these characteristics of R are grounds for criticism,
excellent proposals for evolution of documentation and mailing lists
notwithstanding.

-Eric
This email message, including any attachments, is for the so...{{dropped}}



From br44114 at gmail.com  Tue Jan  3 19:49:56 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 3 Jan 2006 13:49:56 -0500
Subject: [R] For loop gets exponentially slower as dataset gets larger...
Message-ID: <8d5a36350601031049r7934b259y75836ba8ddb1f2f7@mail.gmail.com>

Your 2-million loop is overkill, because apparently in the (vast)
majority of cases you don't need to loop at all. You could try
something like this:
1. Split the price by id, e.g.
price.list <- split(price,id)
For each id,
2a. When price is not NA, assign it to next price _without_ using a
for loop - e.g.
next.price[!is.na(price)] <- price[!is.na(price)]
2b. Use a for loop only when price is NA, but even then work with
vectors as much as you can, for example (untested)
for (i in setdiff(which(is.na(price)),length(price))) {
	remaining.prices <- price[(i+1):length(price)]
	of.interest <- head(remaining.prices[!is.na(remaining.prices)],1)
	if (class(of.interest) == "logical") next.price[i] <- NA else
next.price[i] <- of.interest
	}
To run (2a) and (2b) you could use lapply(); to paste the bits
together try do.call("rbind",price.list). You might also want to take
a look at ?Rprof and check the archives for efficiency suggestions.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of r user
> Sent: Tuesday, January 03, 2006 11:59 AM
> To: rhelp
> Subject: [R] For loop gets exponentially slower as dataset
> gets larger...
>
>
> I am running R 2.1.1 in a Microsoft Windows XP environment.
>
>   I have a matrix with three vectors ("columns") and ~2
> million "rows".  The three vectors are date_, id, and price.
> The data is ordered (sorted) by code and date_.
>
>   (The matrix contains daily prices for several thousand
> stocks, and has ~2 million "rows". If a stock did not trade
> on a particular date, its price is set to "NA")
>
>   I wish to add a fourth vector that is "next_price". ("Next
> price" is the current price as long as the current price is
> not "NA".  If the current price is NA, the "next_price" is
> the next price that the security with this same ID trades.
> If the stock does not trade again,  "next_price" is set to NA.)
>
>   I wrote the following loop to calculate next_price.  It
> works as intended, but I have one problem.  When I have only
> 10,000 rows of data, the calculations are very fast.
> However, when I run the loop on the full 2 million rows, it
> seems to take ~ 1 second per row.
>
>   Why is this happening?  What can I do to speed the
> calculations when running the loop on the full 2 million rows?
>
>   (I am not running low on memory, but I am maxing out my CPU at 100%)
>
>   Here is my code and some sample data:
>
>   data<- data[order(data$code,data$date_),]
>   l<-dim(data)[1]
>   w<-3
>   data[l,w+1]<-NA
>
>   for (i in (l-1):(1)){
>
> data[i,w+1]<-ifelse(is.na(data[i,w])==F,data[i,w],ifelse(data[
> i,2]==data[i+1,2],data[i+1,w+1],NA))
>   }
>
>
>   date      id         price     next_price
>   6/24/2005        1635    444.7838         444.7838
>   6/27/2005        1635    448.4756         448.4756
>   6/28/2005        1635    455.4161         455.4161
>   6/29/2005        1635    454.6658         454.6658
>   6/30/2005        1635    453.9155         453.9155
>   7/1/2005          1635    453.3153         453.3153
>   7/4/2005          1635    NA      453.9155
>   7/5/2005          1635    453.9155         453.9155
>   7/6/2005          1635    453.0152         453.0152
>   7/7/2005          1635    452.8651         452.8651
>   7/8/2005          1635    456.0163         456.0163
>   12/19/2005      1635    442.6982         442.6982
>   12/20/2005      1635    446.5159         446.5159
>   12/21/2005      1635    452.4714         452.4714
>   12/22/2005      1635    451.074           451.074
>   12/23/2005      1635    454.6453         454.6453
>   12/27/2005      1635    NA      NA
>   12/28/2005      1635    NA      NA
>   12/1/2003        1881    66.1562           66.1562
>   12/2/2003        1881    64.9192           64.9192
>   12/3/2003        1881    66.0078           66.0078
>   12/4/2003        1881    65.8098           65.8098
>   12/5/2003        1881    64.1275           64.1275
>   12/8/2003        1881    64.8697           64.8697
>   12/9/2003        1881    63.5337           63.5337
>   12/10/2003      1881    62.9399           62.9399
>
> 		
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
>



From tlumley at u.washington.edu  Tue Jan  3 20:23:18 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jan 2006 11:23:18 -0800 (PST)
Subject: [R] A comment about R:
In-Reply-To: <x2psn9fe23.fsf@viggo.kubism.ku.dk>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
Message-ID: <Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>

On Tue, 3 Jan 2006, Peter Dalgaard wrote:
> One thing that is often overlooked, and hasn't yet been mentioned in
> the thread, is how much *simpler* R can be for certain completely
> basic tasks of practical or pedagogical relevance: Calculate a simple
> derived statistic, confidence intervals from estimate and SE,
> percentage points of the binomial distribution - using dbinom or from
> the formula, take the sum of each of 10 random samples from a set of
> numbers, etc. This is where other packages get stuck in the
> procedure+dataset mindset.

Some of these things are actually fairly straightforward in Stata. For 
example, Stata will give confidence intervals and tests for linear 
combinations of coefficients and even (using symbolic differentiation and 
the delta method) for nonlinear combinations.  The first is available in 
packages in R, the second is in "S Programming" but doesn't seem to be 
packaged.

. di Binomial(10,4,0.2)
.12087388

Taking the sum of each of ten random samples, or other things of that 
sort, obviously requires creating a new data set, but again there are 
facilities to automate this.  I have, for example, computed bootstrap 
confidence intervals for ratio or difference of medians in a service 
course using Stata.  It would be easier in R, but not that much easier.


> For much the same reason, those packages make you tend to treat
> practical data analysis as something distinct from theoretical
> understanding of the methods: You just don't use SAS or SPSS or Stata
> to illustrate the concept of a random sample by setting up a small
> simulation study as the first thing you do in a statistics class,
> whereas you could quite conceivably do it in R. (What *is* the
> equivalent of rnorm(25) in those languages, actually?)

set obs 25
gen x = invnorm(uniform())

[This does create a new data set, of course]

> Even when using SAS in teaching, I sometimes fire up R just to
> calculate simple things like
>
>  pbar <- (p1+p2)/2
>  sqrt(pbar*(1-pbar))

local pbar=(0.3+0.5)/2
display sqrt(`pbar'*(1-`pbar'))

Now, I still prefer R both for data analysis and (even more so) for 
programming. There are some things that it is genuinely difficult to 
program in Stata -- and as evidence that this isn't just my ignorance of 
the best approaches, the language was substantially reworked in both 
versions 8 and 9 to allow the vendor to implement better graphics and
linear mixed models.

On the question of which system really is easier to learn I can only 
comment that this isn't the only question where education, as a field, 
would benefit from some good randomized controlled trials.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From gunter.berton at gene.com  Tue Jan  3 20:26:05 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 3 Jan 2006 11:26:05 -0800
Subject: [R] R fortunes candidate? (was  "A comment about R")
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC16D@VAIEXCH04.vai.org>
Message-ID: <200601031925.k03JPt2W020532@meitner.gene.com>


A candidate for the fortunes package? 

(Perhaps the highest honor one can receive: being "verbified" :-) )

>  And the fear of
> getting Ripleyed on the mailing list also makes me think, read, and
> improve before submitting half baked questions to the list.
> 

>	-- Eric Kort



Cheers,
Bert



From ggrothendieck at gmail.com  Tue Jan  3 20:30:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jan 2006 14:30:34 -0500
Subject: [R] For loop gets exponentially slower as dataset gets larger...
In-Reply-To: <20060103165919.49819.qmail@web37003.mail.mud.yahoo.com>
References: <20060103165919.49819.qmail@web37003.mail.mud.yahoo.com>
Message-ID: <971536df0601031130r438ca8cbk93c1b7f21c2470da@mail.gmail.com>

Accepting this stacked representation for the
moment try this.  When reordering the dates do it
in reverse order.  Then loop over all codes
applying the zoo function na.locf to the the
prices for that code.  locf stands for last
observation carried forward.  Since our dates
are reversed it will bring the next one
backwards. Finally sort back into ascending order.

library(zoo)     # needed for na.locf which also works for non-zoo objects
data <- data[order(data$code, - as.numeric(data$date_)),]
attach(data)
next_price <- price
for(i in unique(code)) next_price[code==i] <- na.locf(price[code==i], na.rm=F)
data$next_price <- next_price
data <- data[order(data$code, data$date_),]
detach()

Here it is again but this time we represent it as
a list of zoo objects with one component per code.
In the code below we split the data on code and
apply f to do that.  Note that na.locf replaces
NAs with the last observation carried forward so
by reversing the data, using na.locf and reversing
the data again we get the effect.

library(zoo)
f <- function(x) {
	z <- zoo(x$price, x$date_)
	next_price <- rev(na.locf(rev(coredata(z)), na.rm = FALSE))
	merge(z, next_price)
}
z <- lapply(split(data, data$code), f)


On 1/3/06, r user <ruser2006 at yahoo.com> wrote:
> I am running R 2.1.1 in a Microsoft Windows XP environment.
>
>  I have a matrix with three vectors ("columns") and ~2 million "rows".  The three vectors are date_, id, and price.  The data is ordered (sorted) by code and date_.
>
>  (The matrix contains daily prices for several thousand stocks, and has ~2 million "rows". If a stock did not trade on a particular date, its price is set to "NA")
>
>  I wish to add a fourth vector that is "next_price". ("Next price" is the current price as long as the current price is not "NA".  If the current price is NA, the "next_price" is the next price that the security with this same ID trades.  If the stock does not trade again,  "next_price" is set to NA.)
>
>  I wrote the following loop to calculate next_price.  It works as intended, but I have one problem.  When I have only 10,000 rows of data, the calculations are very fast.  However, when I run the loop on the full 2 million rows, it seems to take ~ 1 second per row.
>
>  Why is this happening?  What can I do to speed the calculations when running the loop on the full 2 million rows?
>
>  (I am not running low on memory, but I am maxing out my CPU at 100%)
>
>  Here is my code and some sample data:
>
>  data<- data[order(data$code,data$date_),]
>  l<-dim(data)[1]
>  w<-3
>  data[l,w+1]<-NA
>
>  for (i in (l-1):(1)){
>  data[i,w+1]<-ifelse(is.na(data[i,w])==F,data[i,w],ifelse(data[i,2]==data[i+1,2],data[i+1,w+1],NA))
>  }
>
>
>  date      id         price     next_price
>  6/24/2005        1635    444.7838         444.7838
>  6/27/2005        1635    448.4756         448.4756
>  6/28/2005        1635    455.4161         455.4161
>  6/29/2005        1635    454.6658         454.6658
>  6/30/2005        1635    453.9155         453.9155
>  7/1/2005          1635    453.3153         453.3153
>  7/4/2005          1635    NA      453.9155
>  7/5/2005          1635    453.9155         453.9155
>  7/6/2005          1635    453.0152         453.0152
>  7/7/2005          1635    452.8651         452.8651
>  7/8/2005          1635    456.0163         456.0163
>  12/19/2005      1635    442.6982         442.6982
>  12/20/2005      1635    446.5159         446.5159
>  12/21/2005      1635    452.4714         452.4714
>  12/22/2005      1635    451.074           451.074
>  12/23/2005      1635    454.6453         454.6453
>  12/27/2005      1635    NA      NA
>  12/28/2005      1635    NA      NA
>  12/1/2003        1881    66.1562           66.1562
>  12/2/2003        1881    64.9192           64.9192
>  12/3/2003        1881    66.0078           66.0078
>  12/4/2003        1881    65.8098           65.8098
>  12/5/2003        1881    64.1275           64.1275
>  12/8/2003        1881    64.8697           64.8697
>  12/9/2003        1881    63.5337           63.5337
>  12/10/2003      1881    62.9399           62.9399
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From roger.bos at gmail.com  Tue Jan  3 20:31:33 2006
From: roger.bos at gmail.com (roger bos)
Date: Tue, 3 Jan 2006 14:31:33 -0500
Subject: [R] A comment about R:
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC16D@VAIEXCH04.vai.org>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC16D@VAIEXCH04.vai.org>
Message-ID: <1db726800601031131p75387bccy78d92bb560ced5a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/6047914f/attachment.pl

From ggrothendieck at gmail.com  Tue Jan  3 20:37:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jan 2006 14:37:36 -0500
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
Message-ID: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>

On 1/3/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Tue, 3 Jan 2006, Peter Dalgaard wrote:
> > One thing that is often overlooked, and hasn't yet been mentioned in
> > the thread, is how much *simpler* R can be for certain completely
> > basic tasks of practical or pedagogical relevance: Calculate a simple
> > derived statistic, confidence intervals from estimate and SE,
> > percentage points of the binomial distribution - using dbinom or from
> > the formula, take the sum of each of 10 random samples from a set of
> > numbers, etc. This is where other packages get stuck in the
> > procedure+dataset mindset.
>
> Some of these things are actually fairly straightforward in Stata. For

In fact there are some things that are very easy
to do in Stata and can be done in R but only with more difficulty.
For example, consider this introductory session in Stata:

http://www.stata.com/capabilities/session.html

Looking at the first few queries,
see how easy it is to take the top few in Stata whereas in R one would
have a complex use of order.  Its not hard in R to write a function
that would make it just as easy but its not available off the top
of one's head though RSiteSearch("sort.data.frame") will find one
if one knew what to search for.



From mail at bymouth.com  Tue Jan  3 21:22:41 2006
From: mail at bymouth.com (Stephen Choularton)
Date: Wed, 4 Jan 2006 07:22:41 +1100
Subject: [R] randomForest - classifier switch
Message-ID: <002d01c610a3$84e21300$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/6b349759/attachment.pl

From andy_liaw at merck.com  Tue Jan  3 21:28:25 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 Jan 2006 15:28:25 -0500
Subject: [R] randomForest - classifier switch
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6B0@usctmx1106.merck.com>

From: Stephen Choularton
> 
> Hi
>  
> I am trying to use randomForest for classification. I am using this
> code:
>  
> > set.seed(71)
> > rf.model <- randomForest(similarity ~ ., data=set1[1:100,],
> importance=TRUE, proximity=TRUE)
> Warning message: 
> The response has five or fewer unique values.  Are you sure 
> you want to
> do regression? in: randomForest.default(m, y, ...) 
> > rf.model
>  
> Call:
>  randomForest(x = similarity ~ ., data = set1[1:100, ], importance =
> TRUE,      proximity = TRUE) 
>                Type of random forest: regression
>                      Number of trees: 500
> No. of variables tried at each split: 10
>  
>           Mean of squared residuals: 0.1159130
>                     % Var explained: 50.8
> >
>  
> As you can see I get a regression model.  How can I make sure I get a
> classification model?

By making sure your response variable is a factor, e.g.,

  set1$similarity <- as.factor(set1$similarity)

Andy

  
> Thanks .
>  
> Stephen
> 
> -- 
> 
> 
> 
> 2/01/2006
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Mleeds at kellogggroup.com  Tue Jan  3 22:07:05 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Tue, 3 Jan 2006 16:07:05 -0500
Subject: [R] newbie R question
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/11fbe875/attachment.pl

From verena.s.hoffmann at web.de  Tue Jan  3 22:03:04 2006
From: verena.s.hoffmann at web.de (Verena Hoffmann)
Date: Tue, 03 Jan 2006 22:03:04 +0100
Subject: [R] p-value of Logrank-Test
Message-ID: <43BAE688.3020809@web.de>

Hello!

I want to compare two Kaplan-Meier-Curves by using the Logrank-Test:

logrank(Surv(time[b], status[b]) ~ group[b])

This way I only get the value of the test-statistic, but not the p-value.

Does anybody know how I can get the p-value?

Thanks in advance!

Verena Hoffmann



From p.dalgaard at biostat.ku.dk  Tue Jan  3 22:11:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jan 2006 22:11:45 +0100
Subject: [R] newbie R question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
Message-ID: <x2d5j9vwxa.fsf@turmalin.kubism.ku.dk>

"Mark Leeds" <Mleeds at kellogggroup.com> writes:

> I'm sorry to bother everyone with a stupid
> question but, when I am at an R prompt in Windows,
> is there a way to see what packages
> you already have installed from the R site so
> that you can just do library(name_of_package)
> and it will work.
>  
> I've looked at help etc but I can't find
> a command like this. Maybe there
> isn't one which is fine.

Just library() (w/no arguments)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From eric.kort at vai.org  Tue Jan  3 22:11:46 2006
From: eric.kort at vai.org (Eric Kort)
Date: Tue, 03 Jan 2006 16:11:46 -0500
Subject: [R] newbie R question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
Message-ID: <1136322706.13748.1.camel@localhost.localdomain>

On Tue, 2006-01-03 at 16:07 -0500, Mark Leeds wrote:
> I'm sorry to bother everyone with a stupid
> question but, when I am at an R prompt in Windows,
> is there a way to see what packages
> you already have installed from the R site so
> that you can just do library(name_of_package)
> and it will work.
>  
> I've looked at help etc but I can't find
> a command like this. Maybe there
> isn't one which is fine.

library()


HTH,
Eric

This email message, including any attachments, is for the so...{{dropped}}



From Roger.Bivand at nhh.no  Tue Jan  3 22:12:35 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 3 Jan 2006 22:12:35 +0100 (CET)
Subject: [R] newbie R question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
Message-ID: <Pine.LNX.4.44.0601032212240.8559-100000@reclus.nhh.no>

On Tue, 3 Jan 2006, Mark Leeds wrote:

> I'm sorry to bother everyone with a stupid
> question but, when I am at an R prompt in Windows,
> is there a way to see what packages
> you already have installed from the R site so
> that you can just do library(name_of_package)
> and it will work.
>  
> I've looked at help etc but I can't find
> a command like this. Maybe there
> isn't one which is fine.

library()

>  
>                                              Mark 
> 
> 
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Tue Jan  3 22:15:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jan 2006 21:15:35 +0000 (GMT)
Subject: [R] newbie R question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466261@exchange.psg.com>
Message-ID: <Pine.LNX.4.61.0601032114160.7322@gannet.stats>

On Tue, 3 Jan 2006, Mark Leeds wrote:

> I'm sorry to bother everyone with a stupid
> question but, when I am at an R prompt in Windows,
> is there a way to see what packages
> you already have installed from the R site so
> that you can just do library(name_of_package)
> and it will work.
>
> I've looked at help etc but I can't find
> a command like this. Maybe there
> isn't one which is fine.

library() (no arguments) lists all the installed packages (by library).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Mleeds at kellogggroup.com  Tue Jan  3 22:24:18 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Tue, 3 Jan 2006 16:24:18 -0500
Subject: [R] newbie R question
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466266@exchange.psg.com>

Thanks to all. I didn't
Realize that you
Got so many packages
Automatically.
 
I've used S+ for roughly
10 years on and off and
I am starting to switch over
( was finally forced to because my new company preferred
me to use R for cost. I am the only user ) 
and it's unbelievable what has been done
in R by everyone. Truly
amazing. You should
all be quite proud
about what you have created.

               Mark

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Tuesday, January 03, 2006 4:16 PM
To: Mark Leeds
Cc: R-Stat Help
Subject: Re: [R] newbie R question

On Tue, 3 Jan 2006, Mark Leeds wrote:

> I'm sorry to bother everyone with a stupid
> question but, when I am at an R prompt in Windows,
> is there a way to see what packages
> you already have installed from the R site so
> that you can just do library(name_of_package)
> and it will work.
>
> I've looked at help etc but I can't find
> a command like this. Maybe there
> isn't one which is fine.

library() (no arguments) lists all the installed packages (by library).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From tlumley at u.washington.edu  Tue Jan  3 22:46:31 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jan 2006 13:46:31 -0800 (PST)
Subject: [R] p-value of Logrank-Test
In-Reply-To: <43BAE688.3020809@web.de>
References: <43BAE688.3020809@web.de>
Message-ID: <Pine.LNX.4.64.0601031338550.22515@homer22.u.washington.edu>

On Tue, 3 Jan 2006, Verena Hoffmann wrote:

> Hello!
>
> I want to compare two Kaplan-Meier-Curves by using the Logrank-Test:
>
> logrank(Surv(time[b], status[b]) ~ group[b])
>
> This way I only get the value of the test-statistic, but not the p-value.
>
> Does anybody know how I can get the p-value?
>

You don't say where you found the logrank() function, but

a)  The survdiff() function in the survival package gives p-values as well 
as test statistic for the logrank test

b) The test statistic presumably has a  chisquare null distribution, so 
pchisq() would turn it into a p-value.

 	-thomas



From lizzylaws at yahoo.com  Tue Jan  3 22:54:43 2006
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Tue, 3 Jan 2006 13:54:43 -0800 (PST)
Subject: [R] mixed effects models - negative binomial family?
In-Reply-To: <F318F3CA-27B7-4400-9853-5BF65DD9677A@central.ntua.gr>
Message-ID: <20060103215443.28920.qmail@web32101.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/598a7d40/attachment.pl

From mwilliam at warnercnr.colostate.edu  Tue Jan  3 23:58:40 2006
From: mwilliam at warnercnr.colostate.edu (Matt Williamson)
Date: Tue, 3 Jan 2006 15:58:40 -0700
Subject: [R] All possible subsets model selection using AICc
Message-ID: <000901c610b9$549409e0$09125281@Willis>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/ad089934/attachment.pl

From tlumley at u.washington.edu  Wed Jan  4 00:12:17 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jan 2006 15:12:17 -0800 (PST)
Subject: [R] All possible subsets model selection using AICc
In-Reply-To: <000901c610b9$549409e0$09125281@Willis>
References: <000901c610b9$549409e0$09125281@Willis>
Message-ID: <Pine.LNX.4.64.0601031505060.22515@homer22.u.washington.edu>

On Tue, 3 Jan 2006, Matt Williamson wrote:

> Hello List,
> I was wondering if a package or piece of code exists that will allow all
> possible subsets regression model selection within program R.  I have
> already looked at step(AIC) which does not test differing combinations
> of variables within a model as far as I can tell.  In addition I tried
> to use the leaps command, but that does not use the criterion I am
> looking for.

leaps() or regsubsets() in the leaps package almost certainly do use the 
criterion you are looking for (even though you don't tell us what that 
criterion is).

These functions produce one or more best models of each size, and for 
models of the same size all the commonly-used criteria reduce to ranking 
by residual sum of squares, which is what leaps() and regsubsets() do.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jmarshal at ualberta.net  Tue Jan  3 14:10:10 2006
From: jmarshal at ualberta.net (Jason Marshal)
Date: Tue, 03 Jan 2006 10:10:10 -0300
Subject: [R] Including random effects in logistic regression.
Message-ID: <6.2.3.4.0.20060103100641.01cf3fb0@mail.island.net>

I'm trying to analyse some data using logistic regression in R, but I 
want to include random effects in the model.  The glm function 
appears not to have options for including random effects, and the lme 
and nlme documentation indicates that these functions are for 
continuous, not dichotomous, response variables.  Are there options 
in R for this type of analysis?

Jason Marshal
Bariloche, Argentina



From rossibarra at gmail.com  Wed Jan  4 01:10:48 2006
From: rossibarra at gmail.com (Jeffrey Ross-Ibarra)
Date: Tue, 3 Jan 2006 19:10:48 -0500
Subject: [R] abline in log-log plot
Message-ID: <23f82cc70601031610nd6f6526qc03e42a4e6a79da8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/0220727b/attachment.pl

From Scott.Waichler at pnl.gov  Wed Jan  4 01:18:05 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 03 Jan 2006 16:18:05 -0800
Subject: [R] Connectivity across a grid above a variable surface
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A033DB4AB@pnlmse35.pnl.gov>

Hi, 
I'm looking for ideas or packages with relevant algorithms for
calculating the connectivity across a grid, where connectivity is
defined as the minimum amount of cross-sectional area along a continuous
path.  The upper boundary of the cross-sectional area is a fixed
elevation, and the lower boundary is a gridded surface of variable
elevation.  My variable elevation surface represents the top of an
impermeable geologic layer.  I would like to represent the degree to
which a fluid could flow from one end of my grid to another, above the
surface and below the fixed level.  I don't need to derive information
about path lengths and hydraulic gradient, but if I could, that would be
a plus.  A groundwater flow model would provide the exact answer, but
I'm looking for something more approximate and faster.  

My grids are such that there are many "dead-end" flow paths, where the
bottom boundary rises to meet the top boundary and the cross-sectional
area available for flow pinches out.  In plan view, fluid can enter all
along one boundary and leave all along the opposite boundary, but flow
connectivity across the grid varies between bottom boundary scenarios.

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov



From emorgenr at uiuc.edu  Wed Jan  4 01:57:17 2006
From: emorgenr at uiuc.edu (Eberhard F Morgenroth)
Date: Tue, 3 Jan 2006 18:57:17 -0600
Subject: [R] all possible combinations of list elements
Message-ID: <B0DA9A795E5AC54F9C08914554AA4787054BAE9D@DSBEXCLUSTER.ad.uiuc.edu>

I have a list as follows

P <- list(A  = c("CS", "CX"),
          B  = 1:4,
          Y  = c(4, 9))

I now would like to prepare a new list where the rows of the new list
provide all possible combinations of the elements in the orginal list.
Thus, the result should be the following

CS	1	4
CS	1	9
CS	2	4
CS	2	9
CS	3	4
CS	3	9
CS	4	4
CS	4	9
CX	1	4
CX	1	9
CX	2	4
CX	2	9
CX	3	4
CX	3	9
CX	4	4
CX	4	9

Is there a simple routine in R to create this list of all possible
combinations? The routine will be part of a function with the list "P"
as an input. "P" will not always have the same number of elements and
each element in the list "P" may have different numbers of values.

Thanks,
Eberhard Morgenroth
____________________________________________________________________ 
Eberhard Morgenroth, Assistant Professor of Environmental Engineering 
University of Illinois at Urbana-Champaign 
3219 Newmark Civil Engineering Laboratory, MC-250 
205 North Mathews Avenue, Urbana, IL 61801, USA 
Email: emorgenr at uiuc.edu 
http://cee.uiuc.edu/research/morgenroth



From MSchwartz at mn.rr.com  Wed Jan  4 02:04:14 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 03 Jan 2006 19:04:14 -0600
Subject: [R] all possible combinations of list elements
In-Reply-To: <B0DA9A795E5AC54F9C08914554AA4787054BAE9D@DSBEXCLUSTER.ad.uiuc.edu>
References: <B0DA9A795E5AC54F9C08914554AA4787054BAE9D@DSBEXCLUSTER.ad.uiuc.edu>
Message-ID: <1136336655.9497.3.camel@localhost.localdomain>

On Tue, 2006-01-03 at 18:57 -0600, Eberhard F Morgenroth wrote:
> I have a list as follows
> 
> P <- list(A  = c("CS", "CX"),
>           B  = 1:4,
>           Y  = c(4, 9))
> 
> I now would like to prepare a new list where the rows of the new list
> provide all possible combinations of the elements in the orginal list.
> Thus, the result should be the following
> 
> CS	1	4
> CS	1	9
> CS	2	4
> CS	2	9
> CS	3	4
> CS	3	9
> CS	4	4
> CS	4	9
> CX	1	4
> CX	1	9
> CX	2	4
> CX	2	9
> CX	3	4
> CX	3	9
> CX	4	4
> CX	4	9
> 
> Is there a simple routine in R to create this list of all possible
> combinations? The routine will be part of a function with the list "P"
> as an input. "P" will not always have the same number of elements and
> each element in the list "P" may have different numbers of values.

See ?expand.grid

> expand.grid(P)
    A B Y
1  CS 1 4
2  CX 1 4
3  CS 2 4
4  CX 2 4
5  CS 3 4
6  CX 3 4
7  CS 4 4
8  CX 4 4
9  CS 1 9
10 CX 1 9
11 CS 2 9
12 CX 2 9
13 CS 3 9
14 CX 3 9
15 CS 4 9
16 CX 4 9

HTH,

Marc Schwartz



From bgreen at dyson.brisnet.org.au  Wed Jan  4 02:36:46 2006
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Wed, 04 Jan 2006 11:36:46 +1000
Subject: [R] A comment about R:
In-Reply-To: <mailman.11.1136286001.18726.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20060104091630.023aa008@pop3.brisnet.org.au>


>Hello,


>Unlike most posts on the R mailing list I feel qualified to comment on 
>this one.  For about 3 months I have been trying to learn use R,  after 
>having  used various versions of SPSS for about  10 years.


I think it is far too simplistic to ascribe non-use of R to laziness.  This 
may well be the case for some, however, I have read 5-6 books on R, waded 
through on-line resources,  read the documentation and asked multiple 
questions via e-mails - and still find even some of the basics very difficult.

There are several reasons for this:

1. For some tasks R is extremely user-unfriendly.  Some comparative examples:

(a) In running a chi-square analysis in SPSS the following syntax is included

/STATISTIC=CHISQ
   /CELLS= COUNT EXPECTED ROW COLUMN TOTAL RESID .

this produces expected and observed counts, row & column percentages, 
residuals, chi-square & Fisher's exact  test + other output.

In R, it is a herculean task to produce similar output . It certainly, 
can't be produced in 2 lines as far as I can tell.

(b)  in SPSS if I want to compare multiple variables by a single dependent 
variable this is readily performed

CROSSTABS
   /TABLES=baserdis  baserenh  basersoc baseradd socbest disbest entbest 
addbest worsdis worsphy by group

I used the chi-square example again, but the same applies for a t-test. I 
started looking into how  to do something similar in R, with the t-test 
command but gave up. R does force the user to take a more considered 
approach to analysis.

(c) To obtain a correlation matrix in R with the correlation & p-value is 
no simple task -

In SPSS this is obtained via:

GET
   FILE='D:\a study\data\dat\key data\master data.sav'.
NONPAR CORR
   /VARIABLES= goodnum badnum good5 bad5 avfreq avdayamt
   /PRINT=KENDALL TWOTAIL
   /MISSING=PAIRWISE .

In R something like this is required -

 > by(mydat, mydat$group, function(x) {
+ nm <- names(x)
+ rho <- matrix(, 6, 2)
+ rho.nm <- matrix(, 6, 2)
+ k <- 1
+ for(i in 2:4) {
+ for(j in (i + 1):5) {
+ x.i <- x[, i]
+ x.j <- x[, j]
+ ct <- cor.test(x.i, x.j, method=c("kendall") , alternative =c("two-sided"))
+ rho[k, 1] <- ct$estimate
+ rho[k, 2] <- round(ct$p-value, 3)
+ rho.nm[k, ] <- c(nm[i], nm[j])
+ k <- k + 1
+ }
+ }
+ rho <- cbind(as.data.frame(rho.nm), as.data.frame(rho))
+ names(rho) <- c("freq.i", "freq.j", "cor", "p-value")
+ rho
+ })

2) It is not always clear what the output produced by R, is. The 
Mann-Whitney U-test is a good example. In R, it seems a standardised value 
is obtained. I was advised that it is easy enough to check this as R is 
open-source, but at least for me, I don't believe I would understand this 
code anyway. It is confusing when comparative programs such as R and SPSS 
produce dis-similar results. For the user it is important to be able to 
fairly easily reconcile such differences, to engender confidence in results.

3) I find the help files in R quite difficult to understand.  For example, 
see help(t.test).  It is almost assumed by the examples that you know what 
to do. Personally, I would find some form of simple decision tree easier 
-e.g. If you want to perform a t-test with the dependent variable in one 
column and the dependent use in another use t.test(AVFREQ~GROUP) . If you 
want to perform a t-test with the dependent variable in separate columns 
(each column representing a different group) use - t.test(AVFREQ1, AVFREQ2) .

4) My initial approach to using R, was to run commands I had used commonly 
in SPSS and compare the results. I have only got as far  as basic ANOVA. 
This has been time-consuming and at times it has been difficult to obtain 
advice. Some people on the R list have been extremely generous with their 
time and knowledge, and I have much appreciated this assistance. At other 
times I see responses met  with something like arrogance. With the 
sophistication of R, there is also an elitism.  This is a barrier to R 
being more widely accepted and used.

5) differences in terminology - this is just part of the learning process, 
but I still found it took quite some time to work out simple commands and 
what different analyses were called.

6) system administrators may be wary of freeware.

No doubt for the sophisticated user, my comments may seem trite and easily 
resolved, however I believe my comments have some relevance as to why R is 
not more readily used or accepted.


Bob Green



From HDoran at air.org  Wed Jan  4 02:43:48 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 3 Jan 2006 20:43:48 -0500
Subject: [R] Including random effects in logistic regression.
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC99139@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/608b16b4/attachment.pl

From spencer.graves at pdf.com  Wed Jan  4 02:55:38 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 Jan 2006 17:55:38 -0800
Subject: [R] Glimmix  and glm
In-Reply-To: <OFFAAF2B86.606EB6BB-ON862570E6.0062623B-862570E6.0062B1FF@aphis.usda.gov>
References: <OFFAAF2B86.606EB6BB-ON862570E6.0062623B-862570E6.0062B1FF@aphis.usda.gov>
Message-ID: <43BB2B1A.5090009@pdf.com>

	  I'm not certain what you are asking.  I just got 10 hits for 
'RSiteSearch("Glimmix")'.  Seven of them mentioned SAS PROC GLIMMIX:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65945.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65954.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/53310.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/53311.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65935.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65949.html
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/57731.html

	  If you'd like more help from this group, PLEASE do read the posting 
guide! "www.R-project.org/posting-guide.html".  Anecdotal evidence 
suggests that posts that conform more closely to the suggestions there 
tend to get quicker more useful replies.

	  Best Wishes,
	  Spencer Graves

Antonio_Paredes at aphis.usda.gov wrote:

> Hello.
> 
> Some months age an e-mail was posted in which a comparison between Glimmix 
> and glm was discussed. I have not been able to find that e-mail on the R 
> archive. Does anyone recall the date of the above e-mail?
> 
> Thank you very much.
> 
> *******************************************
> Antonio Paredes
> USDA- Center for Veterinary Biologics
> Biometrics Unit
> 510 South 17th Street, Suite 104
> Ames, IA 50010
> (515) 232-5785
> 
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Mike.Prager at noaa.gov  Wed Jan  4 03:29:40 2006
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Tue, 03 Jan 2006 21:29:40 -0500
Subject: [R] A comment about R: (sort.data.frame)
In-Reply-To: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
Message-ID: <43BB3314.9010804@noaa.gov>


Gabor Grothendieck wrote on 1/3/2006 2:37 PM:

>Looking at the first few queries,
>see how easy it is to take the top few in Stata whereas in R one would
>have a complex use of order.  Its not hard in R to write a function
>that would make it just as easy but its not available off the top
>of one's head though RSiteSearch("sort.data.frame") will find one
>if one knew what to search for.
>  
>
Yes, R has a few peculiar gaps.  As to sort.data.frame(), it should be 
added to R base, in my opinion.  It is silly to make people download 
code for such a basic operation.

MHP



From fhduan at gmail.com  Wed Jan  4 04:23:46 2006
From: fhduan at gmail.com (Frank Duan)
Date: Tue, 3 Jan 2006 21:23:46 -0600
Subject: [R] Looking for packages to do Feature Selection and Classification
Message-ID: <3b9172310601031923pc5847bdrbc011707916d1c7f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/4736e6d9/attachment.pl

From Mleeds at kellogggroup.com  Wed Jan  4 04:56:17 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Tue, 3 Jan 2006 22:56:17 -0500
Subject: [R] newbie where to look question
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466276@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/961835da/attachment.pl

From edd at debian.org  Wed Jan  4 05:21:45 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 3 Jan 2006 22:21:45 -0600
Subject: [R] newbie where to look question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466276@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466276@exchange.psg.com>
Message-ID: <17339.19801.310966.694426@basebud.nulle.part>


On 3 January 2006 at 22:56, Mark Leeds wrote:
| out there but could
| someone just tell me the
| best placed to look/read
| for learning about ( for R-2-2.1 in Windows )
|  
| .Rprofile
| .REnviron.
| .Rdata

?Startup

| .First function ( analogous to the one in Splus ).

?.First
  
| Analog of Splus Chapter 

Not sure. Running

	RSiteSearch("S-Plus Chapter")

leads to the R Data Import/Export manual, and

	RSiteSearch("SPlus Chapter")

has some hits too.

| Basically, I want to learn how to start R so that
| my own source code and various
| packages are already available when
| I start up and how to make separate .Data ( I used to
| do this in Splus with Splus Chapter ) directories etc.

That's done a little differently here but I do not know of a good migration
guide for users with prior S-Plus experience.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From spencer.graves at pdf.com  Wed Jan  4 05:40:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 Jan 2006 20:40:56 -0800
Subject: [R] unexpected "false convergence"
In-Reply-To: <BAY102-F6ABF1C3BCCB8ADD2AADECCA280@phx.gbl>
References: <BAY102-F6ABF1C3BCCB8ADD2AADECCA280@phx.gbl>
Message-ID: <43BB51D8.9040407@pdf.com>

	  I replicated your 'false convergence' using R 2.2.0:
 > sessionInfo()
R version 2.2.0, 2005-10-06, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
     nlme     MASS
"3.1-66" "7.2-23"

	  Since the error message said, "Error in lme.formula", I listed the 
code for "lme.formula" and traced it using "debug(lme.formula)",  The 
function "glmmPQL" calls "lme.formula" repeatedly.  The function 
"lme.formula" in turn calls "nlminb" when it's available, though it used 
to call "optim".  The fifth time "lme.formula" was called, "nlminb" 
returned the error message "false convergence (8)".

	  Under R 2.2, "nlminb" is part of the "base" package.  I'm not 
certain, but I don't think it was available in "base" under R 2.1.1.

	  I think this explains the problem, but not how to fix it.  I tried 
modifying the code fo "lme.formula" to force it to call "optim", but 
this generated a different error.  I am therefore copying Professors 
Bates & Ripley in case one of them might want to look at this.

	  hope this helps.
	  spencer graves

Jack Tanner wrote:
> I've come into some code that produces different results under R 2.1.1 and R 
> 2.2.1. I'm really unfamiliar with the libraries in question (MASS and nlme), 
> so I don't know if this is a bug in my code, or a regression in R. If it's a 
> bug on my end, I'd appreciate any advice on potential causes and relevant 
> documentation.
> 
> The code:
> 
> score<-c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
> id<-rep(1:13,rep(4,13))
> test<-gl(2,1,52,labels=c("pre","post"))
> coder<-gl(2,2,52,labels=c("two","three"))
> il<-data.frame(id,score,test,coder)
> attach(il)
> cs1<-corSymm(value=c(.396,.786,.718,.639,.665,.849),form=~1|id)
> cs1<-Initialize(cs1,data=il)
> run<-glmmPQL(score~test+coder, 
> random=~1|id,family=poisson,data=il,correlation=cs1)
> 
> The output under R 2.2.1, which leaves the run object (last line of the 
> code) undefined:
> 
> iteration 1
> iteration 2
> iteration 3
> iteration 4
> Error in lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data = 
> list( :
>         false convergence (8)
> 
> Under R 2.1.1, I get exactly 4 iterations as well, but no "false 
> convergence" message, and run is defined.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Wed Jan  4 06:08:27 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 03 Jan 2006 23:08:27 -0600
Subject: [R] newbie where to look question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466276@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466276@exchange.psg.com>
Message-ID: <1136351307.9497.50.camel@localhost.localdomain>

On Tue, 2006-01-03 at 22:56 -0500, Mark Leeds wrote: 
> I don't want to bother anyone
> with specific questions
> because I am a R newbie and
> I see that there is TON ( emphasis on
> Ton ) of documentation
> out there but could
> someone just tell me the
> best placed to look/read
> for learning about ( for R-2-2.1 in Windows )
>  
> .Rprofile
> .REnviron.
> .Rdata
>  
> .First function ( analogous to the one in Splus ).
>  
> Analog of Splus Chapter 
>  
> Basically, I want to learn how to start R so that
> my own source code and various
> packages are already available when
> I start up and how to make separate .Data ( I used to
> do this in Splus with Splus Chapter ) directories etc.
>  
> I am willing to fight through it and try to figure it out
> myself but there's so much stuff on the net in terms
> of threads etc that I might be helped by knowing the best
> place to start to learn. Thanks.


Mark,

One of the best places to start looking is actually the R e-mail list
Posting Guide, for which there is a link at the bottom of every e-mail
that comes through the list:

  http://www.r-project.org/posting-guide.html

Much of what you want to cover is in An Introduction to R, which is
available from the menus in the Windows version or online at the main R
web site under Manuals. 

Additional information on your specific questions are available
using ?Startup and ?.First from within an R session.

For Chapters, see ?save and ?load, which I believe will provide for
parallel functionality in a fashion.


The main R FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html

and the R Windows FAQ:

http://cran.r-project.org/bin/windows/base/rw-FAQ.html

are good resources as well. If you are transitioning from S-PLUS, you
might want to pay particular attention to section 3.3 of the main R FAQ
on the differences between R and S-PLUS.

Finally, thanks to Andy Liaw and Jon Baron, there is there RSiteSearch()
function, which will enable you to search the e-mail list archives and
documentation online from within an R session. See ?RSiteSearch.

HTH,

Marc Schwartz



From bioflash at gmail.com  Wed Jan  4 06:28:42 2006
From: bioflash at gmail.com (Vincent Deng)
Date: Wed, 4 Jan 2006 13:28:42 +0800
Subject: [R] Questions about cbind
Message-ID: <455343d90601032128v7b06748avd8fb03c0eb170573@mail.gmail.com>

Dear R-helpers

I have a stupid question about cbind function. Suppose I have a
dataframe like this
Frame:
A 10
C 20
B 40

and a numeric matrix like this
Matrix:
A 1
B 2
C 3

cbind(Frame[,2],Matrix[,1]) simply binds these two columns without
checking the order, I mean, the result will be
A 10 1
B 20 2
C 30 3

rather than
A 10 1
B 30 2
C 20 3

So my problem is: Is there any solution for R to bind columns with
correct order?

Many thanks



From MSchwartz at mn.rr.com  Wed Jan  4 06:49:36 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 03 Jan 2006 23:49:36 -0600
Subject: [R] Questions about cbind
In-Reply-To: <455343d90601032128v7b06748avd8fb03c0eb170573@mail.gmail.com>
References: <455343d90601032128v7b06748avd8fb03c0eb170573@mail.gmail.com>
Message-ID: <1136353776.9497.57.camel@localhost.localdomain>

On Wed, 2006-01-04 at 13:28 +0800, Vincent Deng wrote:
> Dear R-helpers
> 
> I have a stupid question about cbind function. Suppose I have a
> dataframe like this
> Frame:
> A 10
> C 20
> B 40
> 
> and a numeric matrix like this
> Matrix:
> A 1
> B 2
> C 3
> 
> cbind(Frame[,2],Matrix[,1]) simply binds these two columns without
> checking the order, I mean, the result will be
> A 10 1
> B 20 2
> C 30 3
> 
> rather than
> A 10 1
> B 30 2
> C 20 3
> 
> So my problem is: Is there any solution for R to bind columns with
> correct order?
> 
> Many thanks

I presume that either the '40' in the first expression of Frame or the
'30's in the second and third outputs are typos?

See ?merge, which will perform SQL-like 'join' operations using a
primary key:

> Frame
  V1 V2
1  A 10
2  C 20
3  B 40


Note that despite the name, this is not a matrix, but also a data frame.
A matrix can only have one data type, while a data frame can have more
than one.

> Matrix
  V1 V2
1  A  1
2  B  2
3  C  3


> merge(Frame, Matrix, by = "V1")
  V1 V2.x V2.y
1  A   10    1
2  B   40    2
3  C   20    3


HTH,

Marc Schwartz



From ahimsa at camposarceiz.com  Wed Jan  4 07:10:35 2006
From: ahimsa at camposarceiz.com (ahimsa campos arceiz)
Date: Wed,  4 Jan 2006 07:10:35 +0100
Subject: [R] silly, extracting the value of "C" from the results of somers2
Message-ID: <1136355035.43bb66dbadf9f@correo.electronico.info>

Sorry I have a very simple question:

I used somers2 function from Design package:

> z<- somers2(x,y, weights=w)

results are:

>z
 C    Dxy    n    Missing
 0.88  0.76  500    0.00

Now I want to call only the value of C to be used in further analyses, but I 
fail to do it. I have tried:

> z$C
NULL
> z[,C]
Error in z[,C]: incorrect number of dimensions

and some other silly things. If I do 
>list(z)
[[1]]
  C    Dxy    n    Missing
 0.88  0.76  500    0.00

Can somebody tell me how can I obtain just the value of c?

Thank you useRs,

very gratefull

Ahimsa

-- 
Ahimsa Campos Arceiz
The University Museum,
The University of Tokyo
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033
phone +81-(0)3-5841-2824



From ehlers at math.ucalgary.ca  Wed Jan  4 07:30:17 2006
From: ehlers at math.ucalgary.ca (P Ehlers)
Date: Tue, 03 Jan 2006 23:30:17 -0700
Subject: [R] silly,
	extracting the value of "C" from the results of somers2
In-Reply-To: <1136355035.43bb66dbadf9f@correo.electronico.info>
References: <1136355035.43bb66dbadf9f@correo.electronico.info>
Message-ID: <43BB6B79.4020903@math.ucalgary.ca>



ahimsa campos arceiz wrote:
> Sorry I have a very simple question:
> 
> I used somers2 function from Design package:
> 
> 
>>z<- somers2(x,y, weights=w)
> 
> 
> results are:
> 
> 
>>z
> 
>  C    Dxy    n    Missing
>  0.88  0.76  500    0.00
> 
> Now I want to call only the value of C to be used in further analyses, but I 
> fail to do it. I have tried:
> 
> 
>>z$C
> 
> NULL
> 
>>z[,C]
> 
> Error in z[,C]: incorrect number of dimensions
> 
> and some other silly things. If I do 
> 
>>list(z)
> 
> [[1]]
>   C    Dxy    n    Missing
>  0.88  0.76  500    0.00
> 
> Can somebody tell me how can I obtain just the value of c?

(I think that somers2() is in package:Hmisc.)
The help page clearly says that somers2 returns a vector and
there's an example on the help page that does _exactly_ what you ask!

z["C"]  or  z[1]

Peter Ehlers

> 
> Thank you useRs,
> 
> very gratefull
> 
> Ahimsa
>



From antoniou at central.ntua.gr  Wed Jan  4 09:20:59 2006
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Wed, 4 Jan 2006 10:20:59 +0200
Subject: [R] mixed effects models - negative binomial family?
In-Reply-To: <20060103215443.28920.qmail@web32101.mail.mud.yahoo.com>
References: <20060103215443.28920.qmail@web32101.mail.mud.yahoo.com>
Message-ID: <02E88B2F-8A2D-4ED9-BE0A-0A5782798F65@central.ntua.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/ae8280da/attachment.pl

From rdiaz at cnio.es  Wed Jan  4 09:30:05 2006
From: rdiaz at cnio.es (Diaz.Ramon)
Date: Wed, 4 Jan 2006 09:30:05 +0100
Subject: [R] Looking for packages to do Feature Selection and
	Classification
Message-ID: <BE216486E4154040BF783AE5DAAA3ED401C2D1D9@SRVEXCH1.cnio.es>

Dear Frank,
I expect you'll get many different answers since a wide variety of approaches have been suggested. So I'll stick to self-advertisment: I've written an R package, varSelRF (available from R), that uses random forest together with a simple variable selection approach, and provides also bootstrap estimates of the error rate of the procedure. Andy Liaw and collaborators previously developed and published a somewhat similar procedure. You probably also want to take a look at several packages available from BioConductor.

Best,

R.


-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of Frank Duan
Sent:	Wed 1/4/2006 4:23 AM
To:	r-help
Cc:	
Subject:	[R] Looking for packages to do Feature Selection and Classification

Hi All,

Sorry if this is a repost (a quick browse didn't give me the answer).

I wonder if there are packages that can do the feature selection and
classification at the same time. For instance, I am using SVM to classify my
samples, but it's easy to get overfitted if using all of the features. Thus,
it is necessary to select "good" features to build an optimum hyperplane
(?). Here is a simple example: Suppose I have 100 "useful" features and 100
"useless" features (or noise features), I want the SVM to give me the
same results when 1) using only 100 useful features or 2) using all 200
features.

Any suggestions or point me to a reference?

Thanks in advance!

Frank

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}



From Matthias.Templ at statistik.gv.at  Wed Jan  4 09:47:59 2006
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 4 Jan 2006 09:47:59 +0100
Subject: [R] A comment about R:
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BADAC@xchg1.statistik.local>

Hello,

One additional example how easy are simple calculations in R.

Calculate the mean of data htinches, multiply it with 2.54 and round the result:

In R:
round( 2.54 * mean( htinches ) )

In SAS could this be done in 2 data steps and 2 proc steps:
DATA new; SET old;
htcm = htinches * 2.54;
PROC means; VAR htcm;
output out=new2 mean=htcm;
DATA new2; set new2;
htcm=round(htcm);
PROC fsview; run;

(you can do this also in one data step, but the code would be longer and more(!) cryptic (or say horrible).
And, of course, you can do this with the help of SAS??s SQL approach, but note that the syntax
is different (!) (comma??s,...) as the "normal" syntax in a data step.)

--> useR!

Matthias



> Patrick Burns <pburns at pburns.seanet.com> writes:
> 
> > I have had an email conversation with the author of the technical 
> > report from which the quote was taken.  I am formulating a 
> comment to 
> > the report that will be posted with the technical report.
> > 
> > I would be pleased if this thread continued, so I will know better 
> > what I want to say.  Plus I should be able to reference 
> this thread in 
> > the comment.
> 
> One thing that is often overlooked, and hasn't yet been 
> mentioned in the thread, is how much *simpler* R can be for 
> certain completely basic tasks of practical or pedagogical 
> relevance: Calculate a simple derived statistic, confidence 
> intervals from estimate and SE, percentage points of the 
> binomial distribution - using dbinom or from the formula, 
> take the sum of each of 10 random samples from a set of 
> numbers, etc. This is where other packages get stuck in the
> procedure+dataset mindset.
> 
> For much the same reason, those packages make you tend to 
> treat practical data analysis as something distinct from 
> theoretical understanding of the methods: You just don't use 
> SAS or SPSS or Stata to illustrate the concept of a random 
> sample by setting up a small simulation study as the first 
> thing you do in a statistics class, whereas you could quite 
> conceivably do it in R. (What *is* the equivalent of 
> rnorm(25) in those languages, actually?)
> 
> Even when using SAS in teaching, I sometimes fire up R just 
> to calculate simple things like
> 
>   pbar <- (p1+p2)/2
>   sqrt(pbar*(1-pbar))
> 
> which you need to cheat SAS Analyst's sample size calculator 
> to work with proportions rather than means. SAS leaves you no 
> way to do this short of setting up a new data set. The 
> Windows calculator will do it, of course, but the students 
> can't see what you are doing then.
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From rossiter at itc.nl  Wed Jan  4 09:58:51 2006
From: rossiter at itc.nl (David Rossiter)
Date: Wed, 4 Jan 2006 09:58:51 +0100
Subject: [R] Discrepency between confidence intervals from t.test and
	computed manually -- why?
Message-ID: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>

I am sure there is something simple here I am missing, so please bear
with  me.

It concerns the computation of the confidence interval for a population
mean.

The data are 125 measurements of Cs137 radation, a sample data set from
Davis "Statistics and Data Analysis in Geology" 3rd ed. (CROATRAD.TXT)
------------------
method 1: using textbook definitions: mean \pm se_mean * t-value

mu <- mean(Cs137); n <- length(Cs137)
se.mean <- sqrt(var(Cs137)/n)
# two-tail alphas
alpha <- c(1, 5, 10, 20)/100
# t-values for each tail
t.vals <- qt(1-(alpha/2), n-1)
# name them for the respective alpha
names(t.vals) <- alpha
# low and high ends of the confidence interval
round(ci.low <- mu - se.mean * t.vals, 2)
round(ci.hi <- mu + se.mean * t.vals, 2)

Output:
0.01 0.05  0.1  0.2 
5.66 5.81 5.90 5.99 

0.01 0.05  0.1  0.2 
6.69 6.54 6.46 6.36 

-----------------

So for the 95% confidence level I seem to get a CI of 5.81 .. 6.54

------------------
method 2: using t.test.  I am not really testing for any specific mean,
I just want the confidence interval of the mean, which t.test seems to
give to me:

Input:
t.test(Cs137)

Output:

        One Sample t-test

data:  Cs137 
t = 11.5122, df = 124, p-value < 2.2e-16              <-- not relevant
alternative hypothesis: true mean is not equal to 0   <-- not relevant
95 percent confidence interval:
 5.115488 7.239712 
sample estimates:
mean of x 
   6.1776 
------------------------------

So with t.test I seem to get a CI of 5.12 .. 7.24 which is considerably
wider than the directly computed interval 5.81 .. 6.54.  Perhaps I am
mis-understanding the CI which t.test is reporting?

Any help would be appreciated.

Thank you.

D G Rossiter
Senior University Lecturer
Department of Earth Systems Analysis (DESA)
International Institute for Geo-Information Science and Earth
Observation (ITC)
Hengelosestraat 99
PO Box 6, 7500 AA Enschede, The Netherlands
mailto:rossiter at itc.nl,  Internet: http://www.itc.nl/personal/rossiter



From Roger.Bivand at nhh.no  Wed Jan  4 10:05:01 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 4 Jan 2006 10:05:01 +0100 (CET)
Subject: [R] A comment about R:
In-Reply-To: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>

On Tue, 3 Jan 2006, Gabor Grothendieck wrote:

> On 1/3/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Tue, 3 Jan 2006, Peter Dalgaard wrote:
> > > One thing that is often overlooked, and hasn't yet been mentioned in
> > > the thread, is how much *simpler* R can be for certain completely
> > > basic tasks of practical or pedagogical relevance: Calculate a simple
> > > derived statistic, confidence intervals from estimate and SE,
> > > percentage points of the binomial distribution - using dbinom or from
> > > the formula, take the sum of each of 10 random samples from a set of
> > > numbers, etc. This is where other packages get stuck in the
> > > procedure+dataset mindset.
> >
> > Some of these things are actually fairly straightforward in Stata. For
> 
> In fact there are some things that are very easy
> to do in Stata and can be done in R but only with more difficulty.
> For example, consider this introductory session in Stata:
> 
> http://www.stata.com/capabilities/session.html
> 
> Looking at the first few queries,
> see how easy it is to take the top few in Stata whereas in R one would
> have a complex use of order.  Its not hard in R to write a function
> that would make it just as easy but its not available off the top
> of one's head though RSiteSearch("sort.data.frame") will find one
> if one knew what to search for.

Could I ask for comments on:

source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)

as a reproduction of the Stata capabilities session? Both the t test and
the chi-square from our side point up oddities. I didn't succeed on
putting fit lines on a grouped xyplot, so backed out to base graphics.
This could be Swoven, possibly using the RweaveHTML driver.

The three obvious world-view differences are functions returning objects
(but difficult to "see" when the default print method for the object shows
the object), nested function calls (on-the-fly objects), and multiple
data-sets (objects, typically data frames) simultaneously present in the 
workspace. Inserting objects() into the script would show the workspace 
growing, perhaps.

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From DrJones at alum.MIT.edu  Wed Jan  4 10:11:16 2006
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Wed, 4 Jan 2006 04:11:16 -0500
Subject: [R] Newbie question--locally weighted regression
Message-ID: <000301c6110e$d1a98540$2f01a8c0@DrJones>


I have a dataset, a time series comprising count data at five minute
intervals. These are the number of people who voted at a particular
voting place during a recent election. The next step is to smooth the
data and estimate a demand vs time-of-day function; the problem is of
interest in preventing long lines at voting places. I am using the R
Project software.

However, I am not a statistician, and I am somewhat baffled by how to
do the smoothing. These are integers with roughly Poisson
distribution, and the use of a least-squares regression would create
large errors. Apparently something called a "link function" factors
into the equation somehow.

Question: Do I want a link function? If so, do I want a logarithmic
link function? Unless I change my mind, I will use lowess or loess for
the smoothing; how do I tell it to use a link function?

Doc



From gynmeerut at indiatimes.com  Wed Jan  4 10:22:30 2006
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Wed, 04 Jan 2006 14:52:30 +0530
Subject: [R] removal of an element from a vector
Message-ID: <200601040857.OAA31306@WS0005.indiatimes.com>


Dear All,
  I have some problem in R which I'm explaining using an example:
x<-(120,235,172,95,175,200,233,142)
i want to remove the elements which are lesser than 100 and as a result i want two vectors

y<-(containing elements <100)
z<-(remaining elements)


Moreover if I wish to use two different programs for vectors y and z.
which command shall I use(will IF-ELSE  work ?)



Thanks and regards,

GS



From dimitris.rizopoulos at med.kuleuven.be  Wed Jan  4 10:21:07 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 4 Jan 2006 10:21:07 +0100
Subject: [R] Discrepency between confidence intervals from t.test
	andcomputed manually -- why?
References: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>
Message-ID: <014a01c61110$31a0ab80$0540210a@www.domain>

for me your code works correctly with simulated data, e.g.,

Cs137 <- rexp(100, 1/6)

mu <- mean(Cs137)
n <- length(Cs137)
se.mean <- sqrt(var(Cs137)/n)
alpha <- c(1, 5, 10, 20)/100
t.vals <- qt(1 -(alpha/2), n-1)
names(t.vals) <- alpha
ci.low <- mu - se.mean * t.vals
ci.hi <- mu + se.mean * t.vals
######################
rbind(ci.low, ci.hi)

t.test(Cs137)

maybe you overwrite somewhere the value of the vector Cs137.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "David Rossiter" <rossiter at itc.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 04, 2006 9:58 AM
Subject: [R] Discrepency between confidence intervals from t.test 
andcomputed manually -- why?


>I am sure there is something simple here I am missing, so please bear
> with  me.
>
> It concerns the computation of the confidence interval for a 
> population
> mean.
>
> The data are 125 measurements of Cs137 radation, a sample data set 
> from
> Davis "Statistics and Data Analysis in Geology" 3rd ed. 
> (CROATRAD.TXT)
> ------------------
> method 1: using textbook definitions: mean \pm se_mean * t-value
>
> mu <- mean(Cs137); n <- length(Cs137)
> se.mean <- sqrt(var(Cs137)/n)
> # two-tail alphas
> alpha <- c(1, 5, 10, 20)/100
> # t-values for each tail
> t.vals <- qt(1-(alpha/2), n-1)
> # name them for the respective alpha
> names(t.vals) <- alpha
> # low and high ends of the confidence interval
> round(ci.low <- mu - se.mean * t.vals, 2)
> round(ci.hi <- mu + se.mean * t.vals, 2)
>
> Output:
> 0.01 0.05  0.1  0.2
> 5.66 5.81 5.90 5.99
>
> 0.01 0.05  0.1  0.2
> 6.69 6.54 6.46 6.36
>
> -----------------
>
> So for the 95% confidence level I seem to get a CI of 5.81 .. 6.54
>
> ------------------
> method 2: using t.test.  I am not really testing for any specific 
> mean,
> I just want the confidence interval of the mean, which t.test seems 
> to
> give to me:
>
> Input:
> t.test(Cs137)
>
> Output:
>
>        One Sample t-test
>
> data:  Cs137
> t = 11.5122, df = 124, p-value < 2.2e-16              <-- not 
> relevant
> alternative hypothesis: true mean is not equal to 0   <-- not 
> relevant
> 95 percent confidence interval:
> 5.115488 7.239712
> sample estimates:
> mean of x
>   6.1776
> ------------------------------
>
> So with t.test I seem to get a CI of 5.12 .. 7.24 which is 
> considerably
> wider than the directly computed interval 5.81 .. 6.54.  Perhaps I 
> am
> mis-understanding the CI which t.test is reporting?
>
> Any help would be appreciated.
>
> Thank you.
>
> D G Rossiter
> Senior University Lecturer
> Department of Earth Systems Analysis (DESA)
> International Institute for Geo-Information Science and Earth
> Observation (ITC)
> Hengelosestraat 99
> PO Box 6, 7500 AA Enschede, The Netherlands
> mailto:rossiter at itc.nl,  Internet: 
> http://www.itc.nl/personal/rossiter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From dimitris.rizopoulos at med.kuleuven.be  Wed Jan  4 10:38:03 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 4 Jan 2006 10:38:03 +0100
Subject: [R] removal of an element from a vector
References: <200601040857.OAA31306@WS0005.indiatimes.com>
Message-ID: <017f01c61112$8f6a7500$0540210a@www.domain>

try the following:

x <- c(120, 235, 172, 95, 175, 200, 233, 142)
y <- x[x < 100]
z <- x[x >= 100]

x
y
z

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "gynmeerut" <gynmeerut at indiatimes.com>
To: "R-Stat Help" <R-help at stat.math.ethz.ch>
Sent: Wednesday, January 04, 2006 10:22 AM
Subject: [R] removal of an element from a vector


>
> Dear All,
>  I have some problem in R which I'm explaining using an example:
> x<-(120,235,172,95,175,200,233,142)
> i want to remove the elements which are lesser than 100 and as a 
> result i want two vectors
>
> y<-(containing elements <100)
> z<-(remaining elements)
>
>
> Moreover if I wish to use two different programs for vectors y and 
> z.
> which command shall I use(will IF-ELSE  work ?)
>
>
>
> Thanks and regards,
>
> GS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From rossiter at itc.nl  Wed Jan  4 10:33:37 2006
From: rossiter at itc.nl (David Rossiter)
Date: Wed, 4 Jan 2006 10:33:37 +0100
Subject: [R] Discrepency between confidence intervals from t.test and
	computed manually -- why?
Message-ID: <36141C39871E4D4DAE92F79D18385436017F5FB9@itcnt14.itc.nl>

Problem solved thanks to Dimitris Rizopoulos. A classic beginner's
mistake -- though I've been using R for four years -- I had a local
variable named Cs137 and then one in an active data frame in the R
Commander (I am testing Rcmdr for possible classroom use); in Rcmdr the
active frame is explicitly named, e.g. t.test(hrrad$Cs137); at the
command prompt I used t.test(Cs137), with frame hrrad attached, but of
course the local variable took precendence. Dumb of me and sorry to
bother you all.

David Rossiter


> -----Original Message-----
> From: David Rossiter 
> Sent: Wednesday, January 04, 2006 9:59
> To: 'r-help at lists.R-project.org'
> Subject: Discrepency between confidence intervals from t.test 
> and computed manually -- why?
> 
> I am sure there is something simple here I am missing, so 
> please bear with  me.
> 
> It concerns the computation of the confidence interval for a 
> population mean.
> 
> The data are 125 measurements of Cs137 radation, a sample 
> data set from Davis "Statistics and Data Analysis in Geology" 
> 3rd ed. (CROATRAD.TXT)
> ------------------
> method 1: using textbook definitions: mean \pm se_mean * t-value
> 
> mu <- mean(Cs137); n <- length(Cs137)
> se.mean <- sqrt(var(Cs137)/n)
> # two-tail alphas
> alpha <- c(1, 5, 10, 20)/100
> # t-values for each tail
> t.vals <- qt(1-(alpha/2), n-1)
> # name them for the respective alpha
> names(t.vals) <- alpha
> # low and high ends of the confidence interval round(ci.low 
> <- mu - se.mean * t.vals, 2) round(ci.hi <- mu + se.mean * t.vals, 2)
> 
> Output:
> 0.01 0.05  0.1  0.2
> 5.66 5.81 5.90 5.99 
> 
> 0.01 0.05  0.1  0.2
> 6.69 6.54 6.46 6.36 
> 
> -----------------
> 
> So for the 95% confidence level I seem to get a CI of 5.81 .. 6.54
> 
> ------------------
> method 2: using t.test.  I am not really testing for any 
> specific mean, I just want the confidence interval of the 
> mean, which t.test seems to give to me:
> 
> Input:
> t.test(Cs137)
> 
> Output:
> 
>         One Sample t-test
> 
> data:  Cs137 
> t = 11.5122, df = 124, p-value < 2.2e-16              <-- not relevant
> alternative hypothesis: true mean is not equal to 0   <-- not relevant
> 95 percent confidence interval:
>  5.115488 7.239712
> sample estimates:
> mean of x 
>    6.1776
> ------------------------------
> 
> So with t.test I seem to get a CI of 5.12 .. 7.24 which is 
> considerably wider than the directly computed interval 5.81 
> .. 6.54.  Perhaps I am  mis-understanding the CI which t.test 
> is reporting?
> 
> Any help would be appreciated.
> 
> Thank you.
> 
> D G Rossiter
> Senior University Lecturer
> Department of Earth Systems Analysis (DESA) International 
> Institute for Geo-Information Science and Earth Observation 
> (ITC) Hengelosestraat 99 PO Box 6, 7500 AA Enschede, The 
> Netherlands mailto:rossiter at itc.nl,  Internet: 
> http://www.itc.nl/personal/rossiter
>  
>



From ripley at stats.ox.ac.uk  Wed Jan  4 10:37:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 09:37:38 +0000 (GMT)
Subject: [R] Discrepency between confidence intervals from t.test and
 computed manually -- why?
In-Reply-To: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>
References: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>
Message-ID: <Pine.LNX.4.61.0601040923420.24265@gannet.stats>

Some user error it appears.  I googled, got the data file from

http://www3.interscience.wiley.com:8100/legacy/college/davis/0471172758/datafiles/data_index.html

and did

> temp <-  read.table("c:/TEMP/CROATRAD.TXT", header=T)
> Cs137 <- temp$X137Cs
> t.test(Cs137)$conf.int
[1] 5.115488 7.239712
attr(,"conf.level")
[1] 0.95

which agrees with your report

> mu <- mean(Cs137); n <- length(Cs137)
> se.mean <- sqrt(var(Cs137)/n)
> # two-tail alphas
> alpha <- c(1, 5, 10, 20)/100
> # t-values for each tail
> t.vals <- qt(1-(alpha/2), n-1)
> # name them for the respective alpha
> names(t.vals) <- alpha
> # low and high ends of the confidence interval
> round(ci.low <- mu - se.mean * t.vals, 2)
0.01 0.05  0.1  0.2
4.77 5.12 5.29 5.49
> round(ci.hi <- mu + se.mean * t.vals, 2)
0.01 0.05  0.1  0.2
7.58 7.24 7.07 6.87
> c(ci.low[2], ci.hi[2])
     0.05     0.05
5.115488 7.239712

which agrees with t.test and not what you reported you got.


On Wed, 4 Jan 2006, David Rossiter wrote:

> I am sure there is something simple here I am missing, so please bear
> with  me.
>
> It concerns the computation of the confidence interval for a population
> mean.
>
> The data are 125 measurements of Cs137 radation, a sample data set from
> Davis "Statistics and Data Analysis in Geology" 3rd ed. (CROATRAD.TXT)
> ------------------
> method 1: using textbook definitions: mean \pm se_mean * t-value
>
> mu <- mean(Cs137); n <- length(Cs137)
> se.mean <- sqrt(var(Cs137)/n)
> # two-tail alphas
> alpha <- c(1, 5, 10, 20)/100
> # t-values for each tail
> t.vals <- qt(1-(alpha/2), n-1)
> # name them for the respective alpha
> names(t.vals) <- alpha
> # low and high ends of the confidence interval
> round(ci.low <- mu - se.mean * t.vals, 2)
> round(ci.hi <- mu + se.mean * t.vals, 2)
>
> Output:
> 0.01 0.05  0.1  0.2
> 5.66 5.81 5.90 5.99
>
> 0.01 0.05  0.1  0.2
> 6.69 6.54 6.46 6.36
>
> -----------------
>
> So for the 95% confidence level I seem to get a CI of 5.81 .. 6.54
>
> ------------------
> method 2: using t.test.  I am not really testing for any specific mean,
> I just want the confidence interval of the mean, which t.test seems to
> give to me:
>
> Input:
> t.test(Cs137)
>
> Output:
>
>        One Sample t-test
>
> data:  Cs137
> t = 11.5122, df = 124, p-value < 2.2e-16              <-- not relevant
> alternative hypothesis: true mean is not equal to 0   <-- not relevant
> 95 percent confidence interval:
> 5.115488 7.239712
> sample estimates:
> mean of x
>   6.1776
> ------------------------------
>
> So with t.test I seem to get a CI of 5.12 .. 7.24 which is considerably
> wider than the directly computed interval 5.81 .. 6.54.  Perhaps I am
> mis-understanding the CI which t.test is reporting?
>
> Any help would be appreciated.
>
> Thank you.
>
> D G Rossiter
> Senior University Lecturer
> Department of Earth Systems Analysis (DESA)
> International Institute for Geo-Information Science and Earth
> Observation (ITC)
> Hengelosestraat 99
> PO Box 6, 7500 AA Enschede, The Netherlands
> mailto:rossiter at itc.nl,  Internet: http://www.itc.nl/personal/rossiter

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mk36 at aub.edu.lb  Wed Jan  4 10:47:55 2006
From: mk36 at aub.edu.lb (Marwan Khawaja)
Date: Wed, 4 Jan 2006 11:47:55 +0200
Subject: [R] A comment about R:
In-Reply-To: <5.1.0.14.0.20060104091630.023aa008@pop3.brisnet.org.au>
Message-ID: <E1Eu5Dx-0007dc-00@cool.aub.edu.lb>

Dear Bob,
The reasons you mentioned are supposedly good features in R -- not giving
lots of output you do not necessarily need. I guess the question is why do
you want R to produce what you get from SPSS?  SPSS is hardly a gold
standard in statistical software.  
But I agree that it is quite difficult for users of SPSS to unlearn SPSS (or
SAS) while using R. 

Best Marwan

----------------------------------------------
Marwan Khawaja   http://staff.aub.edu.lb/~mk36
---------------------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bob Green
> Sent: Wednesday, January 04, 2006 3:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] A comment about R:
> 
> 
> >Hello,
> 
> 
> >Unlike most posts on the R mailing list I feel qualified to 
> comment on 
> >this one.  For about 3 months I have been trying to learn 
> use R,  after 
> >having  used various versions of SPSS for about  10 years.
> 
> 
> I think it is far too simplistic to ascribe non-use of R to 
> laziness.  This may well be the case for some, however, I 
> have read 5-6 books on R, waded through on-line resources,  
> read the documentation and asked multiple questions via 
> e-mails - and still find even some of the basics very difficult.
> 
> There are several reasons for this:
> 
> 1. For some tasks R is extremely user-unfriendly.  Some 
> comparative examples:
> 
> (a) In running a chi-square analysis in SPSS the following 
> syntax is included
> 
> /STATISTIC=CHISQ
>    /CELLS= COUNT EXPECTED ROW COLUMN TOTAL RESID .
> 
> this produces expected and observed counts, row & column 
> percentages, residuals, chi-square & Fisher's exact  test + 
> other output.
> 
> In R, it is a herculean task to produce similar output . It 
> certainly, can't be produced in 2 lines as far as I can tell.
> 
> (b)  in SPSS if I want to compare multiple variables by a 
> single dependent variable this is readily performed
> 
> CROSSTABS
>    /TABLES=baserdis  baserenh  basersoc baseradd socbest 
> disbest entbest addbest worsdis worsphy by group
> 
> I used the chi-square example again, but the same applies for 
> a t-test. I started looking into how  to do something similar 
> in R, with the t-test command but gave up. R does force the 
> user to take a more considered approach to analysis.
> 
> (c) To obtain a correlation matrix in R with the correlation 
> & p-value is no simple task -
> 
> In SPSS this is obtained via:
> 
> GET
>    FILE='D:\a study\data\dat\key data\master data.sav'.
> NONPAR CORR
>    /VARIABLES= goodnum badnum good5 bad5 avfreq avdayamt
>    /PRINT=KENDALL TWOTAIL
>    /MISSING=PAIRWISE .
> 
> In R something like this is required -
> 
>  > by(mydat, mydat$group, function(x) {
> + nm <- names(x)
> + rho <- matrix(, 6, 2)
> + rho.nm <- matrix(, 6, 2)
> + k <- 1
> + for(i in 2:4) {
> + for(j in (i + 1):5) {
> + x.i <- x[, i]
> + x.j <- x[, j]
> + ct <- cor.test(x.i, x.j, method=c("kendall") , alternative 
> + =c("two-sided")) rho[k, 1] <- ct$estimate rho[k, 2] <- 
> + round(ct$p-value, 3) rho.nm[k, ] <- c(nm[i], nm[j]) k <- k 
> + 1 } } rho 
> + <- cbind(as.data.frame(rho.nm), as.data.frame(rho))
> + names(rho) <- c("freq.i", "freq.j", "cor", "p-value") rho
> + })
> 
> 2) It is not always clear what the output produced by R, is. 
> The Mann-Whitney U-test is a good example. In R, it seems a 
> standardised value is obtained. I was advised that it is easy 
> enough to check this as R is open-source, but at least for 
> me, I don't believe I would understand this code anyway. It 
> is confusing when comparative programs such as R and SPSS 
> produce dis-similar results. For the user it is important to 
> be able to fairly easily reconcile such differences, to 
> engender confidence in results.
> 
> 3) I find the help files in R quite difficult to understand.  
> For example, see help(t.test).  It is almost assumed by the 
> examples that you know what to do. Personally, I would find 
> some form of simple decision tree easier -e.g. If you want to 
> perform a t-test with the dependent variable in one column 
> and the dependent use in another use t.test(AVFREQ~GROUP) . 
> If you want to perform a t-test with the dependent variable 
> in separate columns (each column representing a different 
> group) use - t.test(AVFREQ1, AVFREQ2) .
> 
> 4) My initial approach to using R, was to run commands I had 
> used commonly in SPSS and compare the results. I have only 
> got as far  as basic ANOVA. 
> This has been time-consuming and at times it has been 
> difficult to obtain advice. Some people on the R list have 
> been extremely generous with their time and knowledge, and I 
> have much appreciated this assistance. At other times I see 
> responses met  with something like arrogance. With the 
> sophistication of R, there is also an elitism.  This is a 
> barrier to R being more widely accepted and used.
> 
> 5) differences in terminology - this is just part of the 
> learning process, but I still found it took quite some time 
> to work out simple commands and what different analyses were called.
> 
> 6) system administrators may be wary of freeware.
> 
> No doubt for the sophisticated user, my comments may seem 
> trite and easily resolved, however I believe my comments have 
> some relevance as to why R is not more readily used or accepted.
> 
> 
> Bob Green
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ccleland at optonline.net  Wed Jan  4 10:54:01 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 04 Jan 2006 04:54:01 -0500
Subject: [R] Discrepency between confidence intervals from t.test and
 computed manually -- why?
In-Reply-To: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>
References: <36141C39871E4D4DAE92F79D18385436017F5FB7@itcnt14.itc.nl>
Message-ID: <43BB9B39.2040204@optonline.net>

Your two methods agree for me:

 > Cs137 <- 
read.table("http://geomechanics.geol.pdx.edu/Courses/G423/Texts/Davis3/CROATRAD.TXT", 
skip=1)[,5]

 > mu <- mean(Cs137); n <- length(Cs137)
 > se.mean <- sqrt(var(Cs137)/n)
 > # two-tail alphas
 > alpha <- c(1, 5, 10, 20)/100
 > # t-values for each tail
 > t.vals <- qt(1-(alpha/2), n-1)
 > # name them for the respective alpha
 > names(t.vals) <- alpha
 > # low and high ends of the confidence interval
 > round(ci.low <- mu - se.mean * t.vals, 2)
0.01 0.05  0.1  0.2
4.77 5.12 5.29 5.49
 > round(ci.hi <- mu + se.mean * t.vals, 2)
0.01 0.05  0.1  0.2
7.58 7.24 7.07 6.87

 > t.test(Cs137)

         One Sample t-test

data:  Cs137
t = 11.5122, df = 124, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  5.115488 7.239712
sample estimates:
mean of x
    6.1776

David Rossiter wrote:
> I am sure there is something simple here I am missing, so please bear
> with  me.
> 
> It concerns the computation of the confidence interval for a population
> mean.
> 
> The data are 125 measurements of Cs137 radation, a sample data set from
> Davis "Statistics and Data Analysis in Geology" 3rd ed. (CROATRAD.TXT)
> ------------------
> method 1: using textbook definitions: mean \pm se_mean * t-value
> 
> mu <- mean(Cs137); n <- length(Cs137)
> se.mean <- sqrt(var(Cs137)/n)
> # two-tail alphas
> alpha <- c(1, 5, 10, 20)/100
> # t-values for each tail
> t.vals <- qt(1-(alpha/2), n-1)
> # name them for the respective alpha
> names(t.vals) <- alpha
> # low and high ends of the confidence interval
> round(ci.low <- mu - se.mean * t.vals, 2)
> round(ci.hi <- mu + se.mean * t.vals, 2)
> 
> Output:
> 0.01 0.05  0.1  0.2 
> 5.66 5.81 5.90 5.99 
> 
> 0.01 0.05  0.1  0.2 
> 6.69 6.54 6.46 6.36 
> 
> -----------------
> 
> So for the 95% confidence level I seem to get a CI of 5.81 .. 6.54
> 
> ------------------
> method 2: using t.test.  I am not really testing for any specific mean,
> I just want the confidence interval of the mean, which t.test seems to
> give to me:
> 
> Input:
> t.test(Cs137)
> 
> Output:
> 
>         One Sample t-test
> 
> data:  Cs137 
> t = 11.5122, df = 124, p-value < 2.2e-16              <-- not relevant
> alternative hypothesis: true mean is not equal to 0   <-- not relevant
> 95 percent confidence interval:
>  5.115488 7.239712 
> sample estimates:
> mean of x 
>    6.1776 
> ------------------------------
> 
> So with t.test I seem to get a CI of 5.12 .. 7.24 which is considerably
> wider than the directly computed interval 5.81 .. 6.54.  Perhaps I am
> mis-understanding the CI which t.test is reporting?
> 
> Any help would be appreciated.
> 
> Thank you.
> 
> D G Rossiter
> Senior University Lecturer
> Department of Earth Systems Analysis (DESA)
> International Institute for Geo-Information Science and Earth
> Observation (ITC)
> Hengelosestraat 99
> PO Box 6, 7500 AA Enschede, The Netherlands
> mailto:rossiter at itc.nl,  Internet: http://www.itc.nl/personal/rossiter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From calstats05 at yahoo.com  Wed Jan  4 04:09:18 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Tue, 3 Jan 2006 19:09:18 -0800 (PST)
Subject: [R] Indefinite Integral in R
Message-ID: <20060104030918.84886.qmail@web34001.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060103/1360af6b/attachment.pl

From mikewhite.diu at btconnect.com  Tue Jan  3 17:01:00 2006
From: mikewhite.diu at btconnect.com (Mike White)
Date: Tue, 3 Jan 2006 16:01:00 -0000
Subject: [R]  Unlink a directory with leading and trailing space
Message-ID: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>

Using paste without defining a separator to generate a directory name for
dir.create, I have inadvertently created a directory with a leading and
trailing space. I cannot now delete this directory with unlink or from
Windows explorer.  Any help deleting this directory would be appreciated.

Thanks
Mike White



From drf5n at maplepark.com  Tue Jan  3 21:15:43 2006
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 3 Jan 2006 14:15:43 -0600 (CST)
Subject: [R] A comment about R:
In-Reply-To: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0601031403280.19586@maplepark.com>

On Tue, 3 Jan 2006, Gabor Grothendieck wrote:
...
> In fact there are some things that are very easy
> to do in Stata and can be done in R but only with more difficulty.
> For example, consider this introductory session in Stata:
>
> http://www.stata.com/capabilities/session.html
>
> Looking at the first few queries,
> see how easy it is to take the top few in Stata whereas in R one would
> have a complex use of order.  Its not hard in R to write a function
> that would make it just as easy but its not available off the top
> of one's head though RSiteSearch("sort.data.frame") will find one
> if one knew what to search for.

This sort of thing points to an opportunity for documentation.  Building a
tutorial session in R on how one would do a similar analysis would provide
another method of learning R.  "An Introduction to R" is a good bottom-up
introduction, which if you work through it does teach you how to do
several things.  Adapting other tutorials or extended problems, like the
Stata session, to R would give additional entry points.  A few end-to-end
tutorials on some interesting analyses would be helpful.

Any volunteers?

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From zh107 at york.ac.uk  Wed Jan  4 11:53:44 2006
From: zh107 at york.ac.uk (Zhesi He)
Date: Wed, 4 Jan 2006 10:53:44 +0000
Subject: [R] dendrogram
Message-ID: <c69aefe1349498d363eb0ed61c42f714@york.ac.uk>

Dear list,

Sorry if my question is too easy.
I now have a class list like

1.01.01.01
1.01.01.02
1.02.01.01
1.03.01.01
...
9.09.09

I have no problem transferring it to a matrix without those zeros. But 
I really want to have a dengrogram class object so that I can have a 
hierarchical plot.

Also... is there any package that contains interactive dendrogram? for 
example, i can select the levels of cutting the  tree on the plot?
something like that would be useful.

Thanks a lot.

___________________________________________________

Zhesi He
Computational Biology Laboratory, University of York
York YO10 5YW, U.K.
Phone:  +44-(0)1904-328279
Email:  zh107 at york.ac.uk



From yvonnick.noel at uhb.fr  Wed Jan  4 12:02:13 2006
From: yvonnick.noel at uhb.fr (yvonnick noel)
Date: Wed, 04 Jan 2006 12:02:13 +0100
Subject: [R] Replacing backslashes with slashes
Message-ID: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>

Hello,

I've seen this question asked in the archives but no clear reply or solution
provided. So, just to be sure it is not possible in R: Can I replace
backslashes with slashes in a string ?

I am writing a GUI for R with the Rpad library. I have a "browse" button for
data loading and Windows return a path string with backslashes. I need to
convert them into slashes to use the string with read.table.

I would have expected something like:

gsub("\\","\/","c:\My Documents\data.dat")

to work but it does not (incorrect regular expression).

Note that I have no control on the string which is returned from the system (no
such problem under Linux BTW).

Any idea ?

Yvonnick NOEL
U. of Rennes 2
FRANCE



From hezhesi at gmail.com  Wed Jan  4 12:03:58 2006
From: hezhesi at gmail.com (Zhesi He)
Date: Wed, 4 Jan 2006 11:03:58 +0000
Subject: [R] dendrogram
Message-ID: <8da7ddffcf09a187f022b38194bce9ee@ysbl.york.ac.uk>

Dear list,

Sorry if my question is too easy.
I now have a class list like

1.01.01.01
1.01.01.02
1.02.01.01
1.03.01.01
...
9.09.09

I have no problem transferring it to a matrix without those zeros. But 
I really want to have a dengrogram class object so that I can have a 
hierarchical plot.

Also... is there any package that contains interactive dendrogram? for 
example, i can select the levels of cutting the  tree on the plot?
something like that would be useful.

Thanks a lot.

___________________________________________________

Zhesi He
Computational Biology Laboratory, University of York
York YO10 5YW, U.K.
Phone:  +44-(0)1904-328279
Email:  zh107 at york.ac.uk



From amurta at ipimar.pt  Wed Jan  4 10:25:34 2006
From: amurta at ipimar.pt (Alberto Murta)
Date: Wed, 4 Jan 2006 10:25:34 +0100
Subject: [R] A comment about R:
In-Reply-To: <43BAC254.50105@pburns.seanet.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<1115a2b00601030940nc6488e7m193b62fb05517e39@mail.gmail.com>
	<43BAC254.50105@pburns.seanet.com>
Message-ID: <200601041025.35285.amurta@ipimar.pt>

Mensagem original de Patrick Burns (Ter??a, 3 de Janeiro de 2006 19:28):
> Wensui Liu wrote:
> >Another big difference between R and other computing language such as
> >SPSS/SAS/STATA.
> >You can easily get a job using SPSS/SAS/STATA. But it is extremely
> > difficult to find a job using R. ^_^.
>
> Actually in finance it is getting easier all the time for
> knowledge of R to be a significant benefit.
>

That is also true in fisheries assessment and modelling (at least in Europe).

-- 
 Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal
Phone: +351 213027120 | Fax:+351 213015948



From RRoa at fisheries.gov.fk  Wed Jan  4 11:28:31 2006
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 4 Jan 2006 08:28:31 -0200
Subject: [R] A comment about R:
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC6AA@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Forrest
> Sent:	Tuesday, January 03, 2006 6:16 PM
> To:	Gabor Grothendieck
> Cc:	Thomas Lumley; R-help at stat.math.ethz.ch; Patrick Burns; Peter Dalgaard
> Subject:	Re: [R] A comment about R:
> 
> On Tue, 3 Jan 2006, Gabor Grothendieck wrote:
> ...
> > In fact there are some things that are very easy
> > to do in Stata and can be done in R but only with more difficulty.
> > For example, consider this introductory session in Stata:
> >
> > http://www.stata.com/capabilities/session.html
> >
> > Looking at the first few queries,
> > see how easy it is to take the top few in Stata whereas in R one would
> > have a complex use of order.  Its not hard in R to write a function
> > that would make it just as easy but its not available off the top
> > of one's head though RSiteSearch("sort.data.frame") will find one
> > if one knew what to search for.
> 
> This sort of thing points to an opportunity for documentation.  Building a
> tutorial session in R on how one would do a similar analysis would provide
> another method of learning R.  "An Introduction to R" is a good bottom-up
> introduction, which if you work through it does teach you how to do
> several things.  Adapting other tutorials or extended problems, like the
> Stata session, to R would give additional entry points.  A few end-to-end
> tutorials on some interesting analyses would be helpful.
> 
> Any volunteers?
> 
> Dave
> -- 
>  Dr. David Forrest
>  drf at vims.edu                                    (804)684-7900w
>  drf5n at maplepark.com                             (804)642-0662h
>                                    http://maplepark.com/~drf5n/
> 
--------
I am not volunteering but i would like to point out that Paulo Ribeiro's illustrative session
on package geoR and Ole Christensen's similar document for geoRglm are, IMO,
excellent examples on how to make things easier for beginners.
Ruben



From sdavis2 at mail.nih.gov  Wed Jan  4 12:35:05 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 04 Jan 2006 06:35:05 -0500
Subject: [R] Questions about cbind
In-Reply-To: <455343d90601032128v7b06748avd8fb03c0eb170573@mail.gmail.com>
Message-ID: <BFE11D19.2CF3%sdavis2@mail.nih.gov>




On 1/4/06 12:28 AM, "Vincent Deng" <bioflash at gmail.com> wrote:

> Dear R-helpers
> 
> I have a stupid question about cbind function. Suppose I have a
> dataframe like this
> Frame:
> A 10
> C 20
> B 40
> 
> and a numeric matrix like this
> Matrix:
> A 1
> B 2
> C 3
> 
> cbind(Frame[,2],Matrix[,1]) simply binds these two columns without
> checking the order, I mean, the result will be
> A 10 1
> B 20 2
> C 30 3
> 
> rather than
> A 10 1
> B 30 2
> C 20 3
> 
> So my problem is: Is there any solution for R to bind columns with
> correct order?

Look at the merge function.

Sean



From tuechler at gmx.at  Wed Jan  4 12:37:47 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Wed, 04 Jan 2006 12:37:47 +0100
Subject: [R] A comment about R:
In-Reply-To: <43BA7818.B875.00C9.0@ndri.org>
References: <CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>
	<CA612484A337C6479EA341DF9EEE14AC0460DEBE@hercules.ssainfo>
Message-ID: <3.0.6.32.20060104123747.0079eb30@pop.gmx.net>

At 13:11 03.01.2006 -0500, Peter Flom wrote:
>>>> "Ben Fairbank" <BEN at SSANET.COM> 1/3/2006 12:42 pm >>> wrote
><<<
>One implicit point in Kjetil's message is the difficulty of learning
>enough of R to make its use a natural and desired "first choice
>alternative," which I see as the point at which real progress and
>learning commence with any new language.  I agree that the long
>learning
>curve is a serious problem, and in the past I have discussed, off
>list,
>with one of the very senior contributors to this list the possibility
>of
>splitting the list into sections for newcomers and for advanced users.
>He gave some very cogent reasons for not splitting, such as the
>possibility of newcomers' getting bad advice from others only slightly
>more advanced than themselves.  And yet I suspect that a newcomers'
>section would encourage the kind of mutually helpful collegiality
>among
>newcomers that now characterizes the exchanges of the more experienced
>users on this list.  I know that I have occasionally been reluctant to
>post issues that seem too elementary or trivial to vex the others on
>the
>list with and so have stumbled around for an hour or so seeking the
>solution to a simple problem.  Had I the counsel of others similarly
>situated progress might have been far faster.  Have other newcomers or
>occasional users had the same experience?
>>>>
>
>I, for one, have had this experience.  I am usually hesitant to post
>elementary questions here.
>
My experiences are similar. Since you are expected to search for hours all
available documents before asking a question, I am sometimes inclined to
try for hours to solve a trivial problem that would be solved by an answer
like "see FAQ 3.3.3" (my yesterday's problem).
I would be happy, if it was accepted to ask also trivial or very basic
questions, and one way not to bother the experts could be, instead of
splitting the list, simply to flag such questions in the header by some
keyword like "basic or BQ". (Starting the subject with the keyword, not
replacing it! It's not too convenient for readers, just to state "newbie
question" _instead_ of a meaningful subject.)
This keyword should be defined in the posting guide.
This way, every reader/expert can decide on a personal level to split the
list by filtering the messages accordingly.

Heinz

>However, I think that the 'cogent reasons' given by 'one of the very
>senior contributors' are valid.
>I think that  a 'newcomers list' would only really be useful if it
>included some experts who could respond,
>out of generosity.  I don't think the R community lacks generosity -
>obviously not, given all the thousands of 
>hours people have spent writing the language and all the packages and
>so on.  
>
>But these generous people have different abilities and get pleasure in
>different ways.  Some people get a thrill
>out of answering complex questions that require them to come up with
>novel solutions involving complex code.
>Some people get a thrill out of helping newbies over the humps. 
>Dividing the lists might help the experts, as much as it helps the
>beginners. 
>
>
>Peter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From sdavis2 at mail.nih.gov  Wed Jan  4 12:48:42 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 04 Jan 2006 06:48:42 -0500
Subject: [R] dendrogram
In-Reply-To: <8da7ddffcf09a187f022b38194bce9ee@ysbl.york.ac.uk>
Message-ID: <BFE1204A.2CF8%sdavis2@mail.nih.gov>




On 1/4/06 6:03 AM, "Zhesi He" <hezhesi at gmail.com> wrote:

> Dear list,
> 
> Sorry if my question is too easy.
> I now have a class list like
> 
> 1.01.01.01
> 1.01.01.02
> 1.02.01.01
> 1.03.01.01
> ...
> 9.09.09
> 
> I have no problem transferring it to a matrix without those zeros. But
> I really want to have a dengrogram class object so that I can have a
> hierarchical plot.

You didn't really explain what kind of clustering you want to do.  However,
you probably want to do help.search('cluster').  That will give you many
methods for clustering.  Most of them work with either a matrix or a
distance object.  See ?dist for help with the latter.

Sean



From hezhesi at gmail.com  Wed Jan  4 13:12:42 2006
From: hezhesi at gmail.com (Zhesi He)
Date: Wed, 4 Jan 2006 12:12:42 +0000
Subject: [R] dendrogram
In-Reply-To: <BFE1204A.2CF8%sdavis2@mail.nih.gov>
References: <BFE1204A.2CF8%sdavis2@mail.nih.gov>
Message-ID: <3e005231dad980e214d4b9195bf9211a@gmail.com>

Thanks for reply.
The problem is I don't need to do clustering. What I meant was the 
matrix was already in a hierarchical format.
each number before the dot is one layer of the tree
so 1>1.01 > 1.01.01 > 1.01.01.01

so part of lowest nodes of the tree might look like this

1.01
|
|__1.01.01
|	|______1.01.01.01
|	|______1.01.01.02
|
|__1.01.02
	|______1.01.02.01
	|______1.01.02.02


so how can I convert the list into such format? each one is a node of 
the tree and there are 4 layers.
Also. I'm looking for any interactive dendrogram visualisation.

Thanks in advance.
Zhesi.

On 4 Jan 2006, at 11:48, Sean Davis wrote:
>
> On 1/4/06 6:03 AM, "Zhesi He" <hezhesi at gmail.com> wrote:
>
>> Dear list,
>>
>> Sorry if my question is too easy.
>> I now have a class list like
>>
>> 1.01.01.01
>> 1.01.01.02
>> 1.02.01.01
>> 1.03.01.01
>> ...
>> 9.09.09
>>
>> I have no problem transferring it to a matrix without those zeros. But
>> I really want to have a dengrogram class object so that I can have a
>> hierarchical plot.
>
> You didn't really explain what kind of clustering you want to do.  
> However,
> you probably want to do help.search('cluster').  That will give you 
> many
> methods for clustering.  Most of them work with either a matrix or a
> distance object.  See ?dist for help with the latter.
>
> Sean
>
>



From murdoch at stats.uwo.ca  Wed Jan  4 13:22:59 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 07:22:59 -0500
Subject: [R] Unlink a directory with leading and trailing space
In-Reply-To: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
References: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
Message-ID: <43BBBE23.7070600@stats.uwo.ca>

On 1/3/2006 11:01 AM, Mike White wrote:
> Using paste without defining a separator to generate a directory name for
> dir.create, I have inadvertently created a directory with a leading and
> trailing space. I cannot now delete this directory with unlink or from
> Windows explorer.  Any help deleting this directory would be appreciated.

What name did you end up with?  I don't have any problem removing the 
directory " test " just by right-clicking and choosing Delete.  You can 
also open a command window, and put the name in quotes, e.g.

rmdir " test "

I do see the problem in unlink().

Duncan Murdoch



From sdavis2 at mail.nih.gov  Wed Jan  4 13:34:34 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 04 Jan 2006 07:34:34 -0500
Subject: [R] dendrogram
In-Reply-To: <BFE1204A.2CF8%sdavis2@mail.nih.gov>
Message-ID: <BFE12B0A.2D0A%sdavis2@mail.nih.gov>




On 1/4/06 6:48 AM, "Sean Davis" <sdavis2 at mail.nih.gov> wrote:

> 
> 
> 
> On 1/4/06 6:03 AM, "Zhesi He" <hezhesi at gmail.com> wrote:
> 
>> Dear list,
>> 
>> Sorry if my question is too easy.
>> I now have a class list like
>> 
>> 1.01.01.01
>> 1.01.01.02
>> 1.02.01.01
>> 1.03.01.01
>> ...
>> 9.09.09
>> 
>> I have no problem transferring it to a matrix without those zeros. But
>> I really want to have a dengrogram class object so that I can have a
>> hierarchical plot.
> 
> You didn't really explain what kind of clustering you want to do.  However,
> you probably want to do help.search('cluster').  That will give you many
> methods for clustering.  Most of them work with either a matrix or a
> distance object.  See ?dist for help with the latter.

You could probably try creating your own dendrogram object from scratch.
See help(dendrogram) for what might be involved.  However, if you are
looking to browse your list and you have a webserver available, you might
consider using DHTML to create a nice collapsible list like those shown
here:

http://www.oreillynet.com/pub/a/javascript/2002/02/22/hierarchical_menus.htm
l

Doing a google search for hierarchical list javascript might get you what
you need.

Sean



From Eric.Kort at vai.org  Wed Jan  4 14:29:13 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Wed, 4 Jan 2006 08:29:13 -0500
Subject: [R] removal of an element from a vector
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC172@VAIEXCH04.vai.org>

Gynmeerut asks...
> Dear All,
>   I have some problem in R which I'm explaining using an example:
> x<-(120,235,172,95,175,200,233,142)
> i want to remove the elements which are lesser than 100 and as a
result i
> want two vectors
> 
> y<-(containing elements <100)
> z<-(remaining elements)

x<-c(120,235,172,95,175,200,233,142)
y <- x[which(x < 100)]
z <- x[which(x >= 100)]

> Moreover if I wish to use two different programs for vectors y and z.
> which command shall I use(will IF-ELSE  work ?)

This part of the question I do not quite understand.  Now that you have
y and z, you can do whatever you like with them.  But R certainly does
have conditional control statements (if/else) as described in An
Introduction to R. 

-Eric

> 
> 
> 
> Thanks and regards,
> 
> GS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
This email message, including any attachments, is for the so...{{dropped}}



From g.monopoli at biosidus.com.ar  Wed Jan  4 14:27:32 2006
From: g.monopoli at biosidus.com.ar (Gustavo J Monopoli)
Date: Wed, 4 Jan 2006 10:27:32 -0300
Subject: [R] Newb
Message-ID: <OFEEA77BD6.105A84B8-ON032570EC.00487CCF-032570EC.0049F117@sidus.com.ar>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/9b0b422a/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jan  4 14:49:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 13:49:21 +0000 (GMT)
Subject: [R] Unlink a directory with leading and trailing space
In-Reply-To: <43BBBE23.7070600@stats.uwo.ca>
References: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
	<43BBBE23.7070600@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0601041340570.27135@gannet.stats>

On Wed, 4 Jan 2006, Duncan Murdoch wrote:

> On 1/3/2006 11:01 AM, Mike White wrote:
>> Using paste without defining a separator to generate a directory name for
>> dir.create, I have inadvertently created a directory with a leading and
>> trailing space. I cannot now delete this directory with unlink or from
>> Windows explorer.  Any help deleting this directory would be appreciated.
>
> What name did you end up with?  I don't have any problem removing the
> directory " test " just by right-clicking and choosing Delete.  You can
> also open a command window, and put the name in quotes, e.g.
>
> rmdir " test "

But AFAICS Windows always drops trailing spaces.

> dir.create(" test ")
> dir()[1]
[1] " test"

and similarly at the command prompt.

> I do see the problem in unlink().

Which in turn is a problem in MSVCRT's _rmdir.  The usual trick here is 
to use short path names instead, and that works e.g.

 	unlink("TEST~1", recursive=TRUE)

R-devel has a shortPathName() function, so you could do

 	unlink(shortPathName(" test "), recursive = TRUE)

(but I've now fixed the source code to use the short name).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan  4 15:01:10 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 04 Jan 2006 14:01:10 -0000 (GMT)
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
Message-ID: <XFMail.060104140110.Ted.Harding@nessie.mcc.ac.uk>

On 04-Jan-06 yvonnick noel wrote:
> Hello,
> 
> I've seen this question asked in the archives but no clear reply or
> solution
> provided. So, just to be sure it is not possible in R: Can I replace
> backslashes with slashes in a string ?
> 
> I am writing a GUI for R with the Rpad library. I have a "browse"
> button for
> data loading and Windows return a path string with backslashes. I need
> to
> convert them into slashes to use the string with read.table.
> 
> I would have expected something like:
> 
> gsub("\\","\/","c:\My Documents\data.dat")
> 
> to work but it does not (incorrect regular expression).
> 
> Note that I have no control on the string which is returned from the
> system (no
> such problem under Linux BTW).
> 
> Any idea ?
> 
> Yvonnick NOEL
> U. of Rennes 2
> FRANCE

It would work with

gsub("\\\\","/","c:\\My Documents\\data.dat")
[1] "c:/My Documents/data.dat"

which of course is not your case :(

To see what the real problem is, just enter the string itself:

"c:\My Documents\data.dat"
[1] "c:My Documentsdata.dat"

so 'gsub' is not seeing the "\" in the first place.

I don't know what the solution is (though others might),
but that's the problem!

Good luck,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jan-06                                       Time: 14:01:07
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Wed Jan  4 15:08:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 14:08:06 +0000 (GMT)
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
Message-ID: <Pine.LNX.4.61.0601041349470.27135@gannet.stats>

On Wed, 4 Jan 2006, yvonnick noel wrote:

> Hello,
>
> I've seen this question asked in the archives but no clear reply or solution
> provided.

I've seen the answer many times: what were you searching on?

> So, just to be sure it is not possible in R: Can I replace
> backslashes with slashes in a string ?

Yes.  The Windows R code does it in several places.

> I am writing a GUI for R with the Rpad library. I have a "browse" button for
> data loading and Windows return a path string with backslashes. I need to
> convert them into slashes to use the string with read.table.

Actually, read.table accepts any valid file path, so I don't see why.

> I would have expected something like:
>
> gsub("\\","\/","c:\My Documents\data.dat")
> to work but it does not (incorrect regular expression).

You need one of

gsub("\\\\","/","c:\\My Documents\\data.dat")
gsub("\\","/","c:\\My Documents\\data.dat", fixed = TRUE)
chartr("\\", "/", "c:\\My Documents\\data.dat")

Note that you need to be careful in certain multi-byte locales to change 
characters not bytes, but nowadays (post 2.1.0) R is careful.

> Note that I have no control on the string which is returned from the 
> system (no such problem under Linux BTW).

Really?  What happens with file names containing backslashes on Linux?

The string will have embedded backslashes, represented by \\ in quoted 
strings in R ...

> Any idea ?
>
> Yvonnick NOEL
> U. of Rennes 2
> FRANCE

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aleszib at gmail.com  Wed Jan  4 15:14:25 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Wed, 4 Jan 2006 15:14:25 +0100
Subject: [R] Putting an object in to a function that calls the current
	function
Message-ID: <004501c61139$3dd0a5d0$0300a8c0@TAMARA>

Hello!

I would like to put an object in to a function that calls the current
function.

I thought the answer will be clear to me after reading the help files:
?assign
?sys.parent

However it is not.
Here is an example I thought should work, however it dose not exactly:

f<-function(){s();print(a)}
s<-function()assign(x="a",value="ok",pos=sys.parent())
f() #I want to get "ok"
a #I do not want "a" in global enviorment, so here I should get 
#Error: Object "a" not found
ff<-function()f() #here I also want to get "ok" - it should not matter if
the parent fuction has any parents

Thank you in advance for suggestions!

Ales Ziberna



From ggrothendieck at gmail.com  Wed Jan  4 15:21:45 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 09:21:45 -0500
Subject: [R] Putting an object in to a function that calls the current
	function
In-Reply-To: <004501c61139$3dd0a5d0$0300a8c0@TAMARA>
References: <004501c61139$3dd0a5d0$0300a8c0@TAMARA>
Message-ID: <971536df0601040621v73f0f65y4bcac0fbb9168433@mail.gmail.com>

Check out:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64929.html

On 1/4/06, Ales Ziberna <aleszib at gmail.com> wrote:
> Hello!
>
> I would like to put an object in to a function that calls the current
> function.
>
> I thought the answer will be clear to me after reading the help files:
> ?assign
> ?sys.parent
>
> However it is not.
> Here is an example I thought should work, however it dose not exactly:
>
> f<-function(){s();print(a)}
> s<-function()assign(x="a",value="ok",pos=sys.parent())
> f() #I want to get "ok"
> a #I do not want "a" in global enviorment, so here I should get
> #Error: Object "a" not found
> ff<-function()f() #here I also want to get "ok" - it should not matter if
> the parent fuction has any parents
>
> Thank you in advance for suggestions!
>
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Jan  4 15:23:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 09:23:35 -0500
Subject: [R] removal of an element from a vector
In-Reply-To: <200601040857.OAA31306@WS0005.indiatimes.com>
References: <200601040857.OAA31306@WS0005.indiatimes.com>
Message-ID: <971536df0601040623y3bb3db13w156218c57581f5da@mail.gmail.com>

idx <- x < 100
x[idx]
x[!idx]

On 1/4/06, gynmeerut <gynmeerut at indiatimes.com> wrote:
>
> Dear All,
>  I have some problem in R which I'm explaining using an example:
> x<-(120,235,172,95,175,200,233,142)
> i want to remove the elements which are lesser than 100 and as a result i want two vectors
>
> y<-(containing elements <100)
> z<-(remaining elements)
>
>
> Moreover if I wish to use two different programs for vectors y and z.
> which command shall I use(will IF-ELSE  work ?)
>
>
>
> Thanks and regards,
>
> GS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Jan  4 15:26:27 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 09:26:27 -0500
Subject: [R] Putting an object in to a function that calls the current
 function
In-Reply-To: <004501c61139$3dd0a5d0$0300a8c0@TAMARA>
References: <004501c61139$3dd0a5d0$0300a8c0@TAMARA>
Message-ID: <43BBDB13.2060505@stats.uwo.ca>

On 1/4/2006 9:14 AM, Ales Ziberna wrote:
> Hello!
> 
> I would like to put an object in to a function that calls the current
> function.
> 
> I thought the answer will be clear to me after reading the help files:
> ?assign
> ?sys.parent
> 
> However it is not.
> Here is an example I thought should work, however it dose not exactly:
> 
> f<-function(){s();print(a)}
> s<-function()assign(x="a",value="ok",pos=sys.parent())
> f() #I want to get "ok"
> a #I do not want "a" in global enviorment, so here I should get 
> #Error: Object "a" not found
> ff<-function()f() #here I also want to get "ok" - it should not matter if
> the parent fuction has any parents
> 
> Thank you in advance for suggestions!

That's not a good idea.  Why would you want to do something like that?

That out of the way, here's a function that does it:

f<-function(){s();print(a)}
s<-function()assign(x="a",value="ok",env=parent.frame())

The difference between pos=sys.parent() and env=parent.frame() is that 
the pos is interpreted as a position in the search list (see ?assign), 
while parent.frame() gives you the environment from the stack, 
equivalent to sys.frame(sys.parent()).

In R you're almost certainly better off working directly with 
environments, rather than going through integer indexing the way you 
(used to?) have to do in S-PLUS.

Did I mention that messing with the environment of your caller is a bad 
idea?  It's not yours, don't touch it.

Duncan Murdoch



From ihok at hotmail.com  Wed Jan  4 15:59:49 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 04 Jan 2006 09:59:49 -0500
Subject: [R] unexpected "false convergence"
In-Reply-To: <43BB51D8.9040407@pdf.com>
Message-ID: <BAY102-F2554C596F7511680E98C81CA2F0@phx.gbl>

Thank you, Spencer.

I've discovered that if I drop the value parameter from the call to 
corSymm(), R 2.2.1 converges successfully, but the values it converges to 
are slighly (in the hundredths and thousandths) different from those that R 
2.1.1 produces.

In the same vein, I also have an example where the R 2.2.1 glmmPQL produces 
the warning "non-integer #successes in a binomial glm! in: eval(expr, envir, 
enclos)", and another where R 2.2.1 says "iteration limit reached without 
convergence" unless I invoke glmmPQL() with lmeControl(msMaxIter=100). 
Neither of those happen in R 2.1.1.

I'm fine with the results above. I don't know if they're of interest to 
anyone else; if they are, I'm happy to provide more details.



>From: Spencer Graves <spencer.graves at pdf.com>
>To: Jack Tanner <ihok at hotmail.com>
>CC: r-help at stat.math.ethz.ch, Douglas Bates <dmbates at gmail.com>,        
>Prof Brian Ripley <ripley at stats.ox.ac.uk>
>Subject: Re: [R] unexpected "false convergence"
>Date: Tue, 03 Jan 2006 20:40:56 -0800
>
>	  I replicated your 'false convergence' using R 2.2.0:
> > sessionInfo()
>R version 2.2.0, 2005-10-06, i386-pc-mingw32
>
>attached base packages:
>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>[7] "base"
>
>other attached packages:
>     nlme     MASS
>"3.1-66" "7.2-23"
>
>	  Since the error message said, "Error in lme.formula", I listed the code 
>for "lme.formula" and traced it using "debug(lme.formula)",  The function 
>"glmmPQL" calls "lme.formula" repeatedly.  The function "lme.formula" in 
>turn calls "nlminb" when it's available, though it used to call "optim".  
>The fifth time "lme.formula" was called, "nlminb" returned the error 
>message "false convergence (8)".
>
>	  Under R 2.2, "nlminb" is part of the "base" package.  I'm not certain, 
>but I don't think it was available in "base" under R 2.1.1.
>
>	  I think this explains the problem, but not how to fix it.  I tried 
>modifying the code fo "lme.formula" to force it to call "optim", but this 
>generated a different error.  I am therefore copying Professors Bates & 
>Ripley in case one of them might want to look at this.
>
>	  hope this helps.
>	  spencer graves
>
>Jack Tanner wrote:
>>I've come into some code that produces different results under R 2.1.1 and 
>>R 2.2.1. I'm really unfamiliar with the libraries in question (MASS and 
>>nlme), so I don't know if this is a bug in my code, or a regression in R. 
>>If it's a bug on my end, I'd appreciate any advice on potential causes and 
>>relevant documentation.
>>
>>The code:
>>
>>score<-c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
>>id<-rep(1:13,rep(4,13))
>>test<-gl(2,1,52,labels=c("pre","post"))
>>coder<-gl(2,2,52,labels=c("two","three"))
>>il<-data.frame(id,score,test,coder)
>>attach(il)
>>cs1<-corSymm(value=c(.396,.786,.718,.639,.665,.849),form=~1|id)
>>cs1<-Initialize(cs1,data=il)
>>run<-glmmPQL(score~test+coder, 
>>random=~1|id,family=poisson,data=il,correlation=cs1)
>>
>>The output under R 2.2.1, which leaves the run object (last line of the 
>>code) undefined:
>>
>>iteration 1
>>iteration 2
>>iteration 3
>>iteration 4
>>Error in lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data = 
>>list( :
>>         false convergence (8)
>>
>>Under R 2.1.1, I get exactly 4 iterations as well, but no "false 
>>convergence" message, and run is defined.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan  4 16:17:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 15:17:45 +0000 (GMT)
Subject: [R] unexpected "false convergence"
In-Reply-To: <BAY102-F2554C596F7511680E98C81CA2F0@phx.gbl>
References: <BAY102-F2554C596F7511680E98C81CA2F0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0601041508470.32162@gannet.stats>

nlme in 2.2.x uses a different optimizer (nlminb) from that used earlier 
(optim). The author (Douglas Bates) believes it to be better, but my 
experience is rather the opposite.

For some reason he choose not to give you the option via lmeControl of 
selecting one or the other.  I also cannot find any announcement of the 
change in the package itself (other than the ChangeLog).

On Wed, 4 Jan 2006, Jack Tanner wrote:

> Thank you, Spencer.
>
> I've discovered that if I drop the value parameter from the call to
> corSymm(), R 2.2.1 converges successfully, but the values it converges to
> are slighly (in the hundredths and thousandths) different from those that R
> 2.1.1 produces.
>
> In the same vein, I also have an example where the R 2.2.1 glmmPQL produces
> the warning "non-integer #successes in a binomial glm! in: eval(expr, envir,
> enclos)", and another where R 2.2.1 says "iteration limit reached without
> convergence" unless I invoke glmmPQL() with lmeControl(msMaxIter=100).
> Neither of those happen in R 2.1.1.
>
> I'm fine with the results above. I don't know if they're of interest to
> anyone else; if they are, I'm happy to provide more details.
>
>
>
>> From: Spencer Graves <spencer.graves at pdf.com>
>> To: Jack Tanner <ihok at hotmail.com>
>> CC: r-help at stat.math.ethz.ch, Douglas Bates <dmbates at gmail.com>,
>> Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> Subject: Re: [R] unexpected "false convergence"
>> Date: Tue, 03 Jan 2006 20:40:56 -0800
>>
>> 	  I replicated your 'false convergence' using R 2.2.0:
>>> sessionInfo()
>> R version 2.2.0, 2005-10-06, i386-pc-mingw32
>>
>> attached base packages:
>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> [7] "base"
>>
>> other attached packages:
>>     nlme     MASS
>> "3.1-66" "7.2-23"
>>
>> 	  Since the error message said, "Error in lme.formula", I listed the code
>> for "lme.formula" and traced it using "debug(lme.formula)",  The function
>> "glmmPQL" calls "lme.formula" repeatedly.  The function "lme.formula" in
>> turn calls "nlminb" when it's available, though it used to call "optim".
>> The fifth time "lme.formula" was called, "nlminb" returned the error
>> message "false convergence (8)".
>>
>> 	  Under R 2.2, "nlminb" is part of the "base" package.  I'm not certain,
>> but I don't think it was available in "base" under R 2.1.1.
>>
>> 	  I think this explains the problem, but not how to fix it.  I tried
>> modifying the code fo "lme.formula" to force it to call "optim", but this
>> generated a different error.  I am therefore copying Professors Bates &
>> Ripley in case one of them might want to look at this.
>>
>> 	  hope this helps.
>> 	  spencer graves
>>
>> Jack Tanner wrote:
>>> I've come into some code that produces different results under R 2.1.1 and
>>> R 2.2.1. I'm really unfamiliar with the libraries in question (MASS and
>>> nlme), so I don't know if this is a bug in my code, or a regression in R.
>>> If it's a bug on my end, I'd appreciate any advice on potential causes and
>>> relevant documentation.
>>>
>>> The code:
>>>
>>> score<-c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
>>> id<-rep(1:13,rep(4,13))
>>> test<-gl(2,1,52,labels=c("pre","post"))
>>> coder<-gl(2,2,52,labels=c("two","three"))
>>> il<-data.frame(id,score,test,coder)
>>> attach(il)
>>> cs1<-corSymm(value=c(.396,.786,.718,.639,.665,.849),form=~1|id)
>>> cs1<-Initialize(cs1,data=il)
>>> run<-glmmPQL(score~test+coder,
>>> random=~1|id,family=poisson,data=il,correlation=cs1)
>>>
>>> The output under R 2.2.1, which leaves the run object (last line of the
>>> code) undefined:
>>>
>>> iteration 1
>>> iteration 2
>>> iteration 3
>>> iteration 4
>>> Error in lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data =
>>> list( :
>>>         false convergence (8)
>>>
>>> Under R 2.1.1, I get exactly 4 iterations as well, but no "false
>>> convergence" message, and run is defined.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aleszib at gmail.com  Wed Jan  4 16:32:01 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Wed, 4 Jan 2006 16:32:01 +0100
Subject: [R] Putting an object in to a function that calls the current
	function
In-Reply-To: <43BBDB13.2060505@stats.uwo.ca>
Message-ID: <004c01c61144$2859cd20$0300a8c0@TAMARA>

Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
Both work and my problem is solved.

I do aggree with Duncan Murdoch that usually messing with the environment of
your caller is a bad idea. The reason why I still want to do it in this case
is that I exactly know which functions are calling (the function is NEVER
called directly) it and it was in this case easier to use this than to
modify each of the fuctions that are calling it.

Thanks again!
Ales Ziberna

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Wednesday, January 04, 2006 3:26 PM
To: Ales Ziberna
Cc: R-help
Subject: Re: [R] Putting an object in to a function that calls the current
function

On 1/4/2006 9:14 AM, Ales Ziberna wrote:
> Hello!
> 
> I would like to put an object in to a function that calls the current 
> function.
> 
> I thought the answer will be clear to me after reading the help files:
> ?assign
> ?sys.parent
> 
> However it is not.
> Here is an example I thought should work, however it dose not exactly:
> 
> f<-function(){s();print(a)}
> s<-function()assign(x="a",value="ok",pos=sys.parent())
> f() #I want to get "ok"
> a #I do not want "a" in global enviorment, so here I should get
> #Error: Object "a" not found
> ff<-function()f() #here I also want to get "ok" - it should not matter 
> if the parent fuction has any parents
> 
> Thank you in advance for suggestions!

That's not a good idea.  Why would you want to do something like that?

That out of the way, here's a function that does it:

f<-function(){s();print(a)}
s<-function()assign(x="a",value="ok",env=parent.frame())

The difference between pos=sys.parent() and env=parent.frame() is that the
pos is interpreted as a position in the search list (see ?assign), while
parent.frame() gives you the environment from the stack, equivalent to
sys.frame(sys.parent()).

In R you're almost certainly better off working directly with environments,
rather than going through integer indexing the way you (used to?) have to do
in S-PLUS.

Did I mention that messing with the environment of your caller is a bad
idea?  It's not yours, don't touch it.

Duncan Murdoch



From mikewhite.diu at btconnect.com  Wed Jan  4 16:23:04 2006
From: mikewhite.diu at btconnect.com (Mike White)
Date: Wed, 4 Jan 2006 15:23:04 -0000
Subject: [R] Unlink a directory with leading and trailing space
References: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
	<43BBBE23.7070600@stats.uwo.ca>
	<Pine.LNX.4.61.0601041340570.27135@gannet.stats>
Message-ID: <004101c61142$c6648cf0$fa01a8c0@FSSFQCV7BGDVED>

Thanks for your suggestions.
I am using Windows 2000 Professional with R 2.1.0 and have replicated the
problem with the code below which creates the directory " testdir "

# pathin is the parent path and ends with /
pathtest<-paste(pathin, "testdir", "/")  # I forgot to include sep=""
dir.create(pathtest) # creates directory " testdir "

The directory cannot be deleted with the short name as below
setwd(pathin)
unlink("testdi~1")
unlink(" testdi~1")

>From the console I have also tried
rmdir " testdir "
but without success.

Mike

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
Cc: "Mike White" <mikewhite.diu at btconnect.com>; <R-help at stat.math.ethz.ch>
Sent: Wednesday, January 04, 2006 1:49 PM
Subject: Re: [R] Unlink a directory with leading and trailing space


> On Wed, 4 Jan 2006, Duncan Murdoch wrote:
>
> > On 1/3/2006 11:01 AM, Mike White wrote:
> >> Using paste without defining a separator to generate a directory name
for
> >> dir.create, I have inadvertently created a directory with a leading and
> >> trailing space. I cannot now delete this directory with unlink or from
> >> Windows explorer.  Any help deleting this directory would be
appreciated.
> >
> > What name did you end up with?  I don't have any problem removing the
> > directory " test " just by right-clicking and choosing Delete.  You can
> > also open a command window, and put the name in quotes, e.g.
> >
> > rmdir " test "
>
> But AFAICS Windows always drops trailing spaces.
>
> > dir.create(" test ")
> > dir()[1]
> [1] " test"
>
> and similarly at the command prompt.
>
> > I do see the problem in unlink().
>
> Which in turn is a problem in MSVCRT's _rmdir.  The usual trick here is
> to use short path names instead, and that works e.g.
>
>   unlink("TEST~1", recursive=TRUE)
>
> R-devel has a shortPathName() function, so you could do
>
>   unlink(shortPathName(" test "), recursive = TRUE)
>
> (but I've now fixed the source code to use the short name).
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From jtanelson at centurytel.net  Wed Jan  4 16:44:20 2006
From: jtanelson at centurytel.net (Teresa Nelson)
Date: Wed, 4 Jan 2006 09:44:20 -0600
Subject: [R] Why doesn't this nested loop work?
Message-ID: <200601041544.k04FiGJi018813@msa1-mx.centurytel.net>

Hi there,

 

I can get the for-loop to work, I can get the while loop to work.  But I
can't get a for loop to work nested within the while loop - why?  

 

Please help,

Teresa

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: nested_loop_question.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/7efc5039/nested_loop_question.txt

From Greg.Snow at intermountainmail.org  Wed Jan  4 16:47:37 2006
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Wed, 4 Jan 2006 08:47:37 -0700
Subject: [R] Newbie question--locally weighted regression
Message-ID: <07E228A5BE53C24CAD490193A7381BBB198013@LP-EXCHVS07.CO.IHC.COM>

Using a gam model (package gam, possibly others) will take care of the
link function (and variance function) for you and allow using loess to
fit the data.  Here is a quick example to get you started, though you
should read up on gam models yourself as well.

library(gam)

x <- seq(0,1, length=250)
y <- rpois(250, (sin(x*2*pi)+1.2)*3)

plot(x,y)
lines(x,(sin(x*2*pi)+1.2)*3, col='blue')

fit <- gam(y~lo(x), family=poisson)

lines(x, predict(fit, data.frame(x=x), type='response'), col='green')

fit2 <- gam(y~lo(x, span=0.75, degree=2), family=poisson)
lines(x, predict(fit2, data.frame(x=x), type='response'), col='red')


Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas L Jones
> Sent: Wednesday, January 04, 2006 2:11 AM
> To: R-project help
> Subject: [R] Newbie question--locally weighted regression
> 
> 
> I have a dataset, a time series comprising count data at five 
> minute intervals. These are the number of people who voted at 
> a particular voting place during a recent election. The next 
> step is to smooth the data and estimate a demand vs 
> time-of-day function; the problem is of interest in 
> preventing long lines at voting places. I am using the R 
> Project software.
> 
> However, I am not a statistician, and I am somewhat baffled 
> by how to do the smoothing. These are integers with roughly 
> Poisson distribution, and the use of a least-squares 
> regression would create large errors. Apparently something 
> called a "link function" factors into the equation somehow.
> 
> Question: Do I want a link function? If so, do I want a 
> logarithmic link function? Unless I change my mind, I will 
> use lowess or loess for the smoothing; how do I tell it to 
> use a link function?
> 
> Doc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pmuhl1848 at gmail.com  Wed Jan  4 16:48:58 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Wed, 04 Jan 2006 10:48:58 -0500
Subject: [R] Bug in bootcov; R 2.2
Message-ID: <BFE1589A.12788%pmuhl1848@gmail.com>

There's a bug in my version of bootcov.  I'm not sure whether to report it
here or in r-bugs, because it is in a contributed package.  The bug is
straightforward, so perhaps it has been reported, tho I found no reference
in a search of the archive.

Any attempt to run bootcov with both cluster and strata will throw an error
indicating that jadd is not defined (Error: object "jadd" not found).  The
source of the error is the line:

jadd <- c(j, jadd)

jadd does not appear elsewhere in the code, so it is undefined.  I believe
the correct line is:

 j <- c(j, obs.gci)



Information on my R version:

> R.Version()
$platform
[1] "powerpc-apple-darwin7.9.0"

$os
[1] "darwin7.9.0"

$system
[1] "powerpc, darwin7.9.0"

$status
[1] ""

$major
[1] "2"

$minor
[1] "2.0"

$"svn rev"
[1] "35749"



From ripley at stats.ox.ac.uk  Wed Jan  4 16:49:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 15:49:38 +0000 (GMT)
Subject: [R] Unlink a directory with leading and trailing space
In-Reply-To: <004101c61142$c6648cf0$fa01a8c0@FSSFQCV7BGDVED>
References: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
	<43BBBE23.7070600@stats.uwo.ca>
	<Pine.LNX.4.61.0601041340570.27135@gannet.stats>
	<004101c61142$c6648cf0$fa01a8c0@FSSFQCV7BGDVED>
Message-ID: <Pine.LNX.4.61.0601041541530.32537@gannet.stats>

On Wed, 4 Jan 2006, Mike White wrote:

> Thanks for your suggestions.
> I am using Windows 2000 Professional with R 2.1.0 and have replicated the

So as the posting guide says, upgrade if you have a problem: there are
three later versions of R.

> problem with the code below which creates the directory " testdir "
>
> # pathin is the parent path and ends with /
> pathtest<-paste(pathin, "testdir", "/")  # I forgot to include sep=""

using file.path() might help you out here ....

> dir.create(pathtest) # creates directory " testdir "

I get " testdir".

> The directory cannot be deleted with the short name as below
> setwd(pathin)
> unlink("testdi~1")
> unlink(" testdi~1")

Of course not!  Those do not correspond to the example given or to the 
help page's

      If 'recursive = FALSE' directories are not deleted, not even empty
      ones.

which is why my example used recursive=TRUE.

>
> From the console I have also tried
> rmdir " testdir "
> but without success.
>
> Mike
>
> ----- Original Message -----
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
> Cc: "Mike White" <mikewhite.diu at btconnect.com>; <R-help at stat.math.ethz.ch>
> Sent: Wednesday, January 04, 2006 1:49 PM
> Subject: Re: [R] Unlink a directory with leading and trailing space
>
>
>> On Wed, 4 Jan 2006, Duncan Murdoch wrote:
>>
>>> On 1/3/2006 11:01 AM, Mike White wrote:
>>>> Using paste without defining a separator to generate a directory name
> for
>>>> dir.create, I have inadvertently created a directory with a leading and
>>>> trailing space. I cannot now delete this directory with unlink or from
>>>> Windows explorer.  Any help deleting this directory would be
> appreciated.
>>>
>>> What name did you end up with?  I don't have any problem removing the
>>> directory " test " just by right-clicking and choosing Delete.  You can
>>> also open a command window, and put the name in quotes, e.g.
>>>
>>> rmdir " test "
>>
>> But AFAICS Windows always drops trailing spaces.
>>
>>> dir.create(" test ")
>>> dir()[1]
>> [1] " test"
>>
>> and similarly at the command prompt.
>>
>>> I do see the problem in unlink().
>>
>> Which in turn is a problem in MSVCRT's _rmdir.  The usual trick here is
>> to use short path names instead, and that works e.g.
>>
>>   unlink("TEST~1", recursive=TRUE)
>>
>> R-devel has a shortPathName() function, so you could do
>>
>>   unlink(shortPathName(" test "), recursive = TRUE)
>>
>> (but I've now fixed the source code to use the short name).
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From herodote at oreka.com  Wed Jan  4 16:55:18 2006
From: herodote at oreka.com (=?iso-8859-1?Q?herodote@oreka.com?=)
Date: Wed,  4 Jan 2006 16:55:18 +0100
Subject: [R] =?iso-8859-1?q?produce_hours_greater_than_23?=
Message-ID: <ISKS86$F2D4D1EA7C93CAFC6BD4EA3F041D5488@oreka.com>

Hy all,

I wish to use the date function to draw againt the lifetime of a motor.

This lifetime is given to me in Hours (it can go over 5000 hours)

I'm unable to find how to convert this lifetime value to something like %H:%M:%S because when R see 24H it says 1 day, i don't want that, i just want %H:%M:%S with a value of %H higher than 24...
for example: 
i've got this value in hours: 345.05 H

I wish that R gives me : "345:3:0" or "345:03:00"

What R function could do it?

I've search for as.Date strptime... but none of these seems to be able to put a value for %H greater than 23.

i've browse the help and docs and found nothing (am i blind?).

thks all
guillaume.



From murdoch at stats.uwo.ca  Wed Jan  4 16:56:24 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 10:56:24 -0500
Subject: [R] Unlink a directory with leading and trailing space
In-Reply-To: <004101c61142$c6648cf0$fa01a8c0@FSSFQCV7BGDVED>
References: <000f01c6107e$e49c2580$fa01a8c0@FSSFQCV7BGDVED>
	<43BBBE23.7070600@stats.uwo.ca>
	<Pine.LNX.4.61.0601041340570.27135@gannet.stats>
	<004101c61142$c6648cf0$fa01a8c0@FSSFQCV7BGDVED>
Message-ID: <43BBF028.7080700@stats.uwo.ca>

On 1/4/2006 10:23 AM, Mike White wrote:
> Thanks for your suggestions.
> I am using Windows 2000 Professional with R 2.1.0 and have replicated the
> problem with the code below which creates the directory " testdir "
> 
> # pathin is the parent path and ends with /
> pathtest<-paste(pathin, "testdir", "/")  # I forgot to include sep=""
> dir.create(pathtest) # creates directory " testdir "
> 
> The directory cannot be deleted with the short name as below
> setwd(pathin)
> unlink("testdi~1")
> unlink(" testdi~1")
> 
>>From the console I have also tried
> rmdir " testdir "
> but without success.

I just tried this, and confirmed your trouble with unlink (but haven't 
incorporated Brian's change, as below).

However, from the console "rmdir TESTDI~1" worked for me.

I've just tried again, and this time list.files() shows the name to have 
been created as " testdir", not " testdir " (as Brian mentioned); in the 
console

rmdir " testdir"

worked.

Duncan Murdoch
> 
> Mike
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Duncan Murdoch" <murdoch at stats.uwo.ca>
> Cc: "Mike White" <mikewhite.diu at btconnect.com>; <R-help at stat.math.ethz.ch>
> Sent: Wednesday, January 04, 2006 1:49 PM
> Subject: Re: [R] Unlink a directory with leading and trailing space
> 
> 
>> On Wed, 4 Jan 2006, Duncan Murdoch wrote:
>>
>> > On 1/3/2006 11:01 AM, Mike White wrote:
>> >> Using paste without defining a separator to generate a directory name
> for
>> >> dir.create, I have inadvertently created a directory with a leading and
>> >> trailing space. I cannot now delete this directory with unlink or from
>> >> Windows explorer.  Any help deleting this directory would be
> appreciated.
>> >
>> > What name did you end up with?  I don't have any problem removing the
>> > directory " test " just by right-clicking and choosing Delete.  You can
>> > also open a command window, and put the name in quotes, e.g.
>> >
>> > rmdir " test "
>>
>> But AFAICS Windows always drops trailing spaces.
>>
>> > dir.create(" test ")
>> > dir()[1]
>> [1] " test"
>>
>> and similarly at the command prompt.
>>
>> > I do see the problem in unlink().
>>
>> Which in turn is a problem in MSVCRT's _rmdir.  The usual trick here is
>> to use short path names instead, and that works e.g.
>>
>>   unlink("TEST~1", recursive=TRUE)
>>
>> R-devel has a shortPathName() function, so you could do
>>
>>   unlink(shortPathName(" test "), recursive = TRUE)
>>
>> (but I've now fixed the source code to use the short name).
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>



From lizzylaws at yahoo.com  Wed Jan  4 17:00:38 2006
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Wed, 4 Jan 2006 08:00:38 -0800 (PST)
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
Message-ID: <20060104160038.33020.qmail@web32111.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/544245df/attachment.pl

From tlumley at u.washington.edu  Wed Jan  4 17:04:15 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 4 Jan 2006 08:04:15 -0800 (PST)
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.64.0601040755550.15250@homer23.u.washington.edu>

On Wed, 4 Jan 2006, Roger Bivand wrote:
> Could I ask for comments on:
>
> source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
>
> as a reproduction of the Stata capabilities session? Both the t test and
> the chi-square from our side point up oddities. I didn't succeed on
> putting fit lines on a grouped xyplot, so backed out to base graphics.
> This could be Swoven, possibly using the RweaveHTML driver.
>

Personally, I would have used with() for some of these, eg

with(auto, table(rep78, foreign))
with(auto, chisq.test(table(rep78, foreign)))

and I'm sure that someone will point out the right way to get lines on 
coplot() or xyplot().

Apart from that it looks good.

 	-thomas



From murdoch at stats.uwo.ca  Wed Jan  4 17:04:45 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 11:04:45 -0500
Subject: [R] Putting an object in to a function that calls the current
 function
In-Reply-To: <004c01c61144$2859cd20$0300a8c0@TAMARA>
References: <004c01c61144$2859cd20$0300a8c0@TAMARA>
Message-ID: <43BBF21D.4040304@stats.uwo.ca>

On 1/4/2006 10:32 AM, Ales Ziberna wrote:
> Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
> Both work and my problem is solved.
> 
> I do aggree with Duncan Murdoch that usually messing with the environment of
> your caller is a bad idea. The reason why I still want to do it in this case
> is that I exactly know which functions are calling (the function is NEVER
> called directly) it and it was in this case easier to use this than to
> modify each of the fuctions that are calling it.

Using R's lexical scope may lead to a cleaner solution.  That is, you 
define the functions within the one that calls them; then a <<- "ok" 
would do what you want (provided "a" existed in the enclosure at the time).

For example,

f <- function() {
	a <- "init"
	s <- function() {
		a <<- "ok"
         }
	s()
	print(a)
}

Duncan Murdoch

> 
> Thanks again!
> Ales Ziberna
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Wednesday, January 04, 2006 3:26 PM
> To: Ales Ziberna
> Cc: R-help
> Subject: Re: [R] Putting an object in to a function that calls the current
> function
> 
> On 1/4/2006 9:14 AM, Ales Ziberna wrote:
>> Hello!
>> 
>> I would like to put an object in to a function that calls the current 
>> function.
>> 
>> I thought the answer will be clear to me after reading the help files:
>> ?assign
>> ?sys.parent
>> 
>> However it is not.
>> Here is an example I thought should work, however it dose not exactly:
>> 
>> f<-function(){s();print(a)}
>> s<-function()assign(x="a",value="ok",pos=sys.parent())
>> f() #I want to get "ok"
>> a #I do not want "a" in global enviorment, so here I should get
>> #Error: Object "a" not found
>> ff<-function()f() #here I also want to get "ok" - it should not matter 
>> if the parent fuction has any parents
>> 
>> Thank you in advance for suggestions!
> 
> That's not a good idea.  Why would you want to do something like that?
> 
> That out of the way, here's a function that does it:
> 
> f<-function(){s();print(a)}
> s<-function()assign(x="a",value="ok",env=parent.frame())
> 
> The difference between pos=sys.parent() and env=parent.frame() is that the
> pos is interpreted as a position in the search list (see ?assign), while
> parent.frame() gives you the environment from the stack, equivalent to
> sys.frame(sys.parent()).
> 
> In R you're almost certainly better off working directly with environments,
> rather than going through integer indexing the way you (used to?) have to do
> in S-PLUS.
> 
> Did I mention that messing with the environment of your caller is a bad
> idea?  It's not yours, don't touch it.
> 
> Duncan Murdoch



From yvonnick.noel at uhb.fr  Wed Jan  4 17:05:38 2006
From: yvonnick.noel at uhb.fr (yvonnick noel)
Date: Wed, 04 Jan 2006 17:05:38 +0100
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <XFMail.060104140110.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060104140110.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20060104170538.ohvclcb74ok80g4g@webmail.uhb.fr>

> It would work with
>
> gsub("\\\\","/","c:\\My Documents\\data.dat")
> [1] "c:/My Documents/data.dat"
>
> which of course is not your case :(

Absolutely. What I get from the interface is the string "c:\My
Documents\data.dat" and NOT the string "c:\\My Documents\\data.dat". That is
why the solutions previously proposed do not solve my problem (but thank you
for your replies !).

Yvonnick NOEL
U. of Rennes 2
FRANCE



From p.dalgaard at biostat.ku.dk  Wed Jan  4 17:08:37 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jan 2006 17:08:37 +0100
Subject: [R] Why doesn't this nested loop work?
In-Reply-To: <200601041544.k04FiGJi018813@msa1-mx.centurytel.net>
References: <200601041544.k04FiGJi018813@msa1-mx.centurytel.net>
Message-ID: <x2hd8kdlh6.fsf@viggo.kubism.ku.dk>

"Teresa Nelson" <jtanelson at centurytel.net> writes:

> Hi there,
> 
>  
> 
> I can get the for-loop to work, I can get the while loop to work.  But I
> can't get a for loop to work nested within the while loop - why?  
> 

What do you mean it doesn't work??

I get:

>
> results
       [,1] [,2]
  [1,]    1   25
  [2,]    2   50
  [3,]    3   75
  [4,]    4  100
  [5,]    5  125
  [6,]    6  150
  [7,]    7  175
  [8,]    8  200
  [9,]    9  225
 [10,]   10  250
 [11,]   11  275
 [12,]   12  300
 [13,]    0    0
...
[107,]    0    0
[108,]  108  300
[109,]    0    0
[110,]  110  275
[111,]    0    0
[112,]    0    0
[113,]    0    0
[114,]    0    0
[115,]    0    0
[116,]    0    0
[117,]    0    0
[118,]    0    0
[119,]    0    0
[120,]  120  300

If that's not what you expected, then you need to adjust your
expectations. Notice that your k's are effectively

> outer(1:12,1:10)
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    1    2    3    4    5    6    7    8    9    10
 [2,]    2    4    6    8   10   12   14   16   18    20
 [3,]    3    6    9   12   15   18   21   24   27    30
 [4,]    4    8   12   16   20   24   28   32   36    40
 [5,]    5   10   15   20   25   30   35   40   45    50
 [6,]    6   12   18   24   30   36   42   48   54    60
 [7,]    7   14   21   28   35   42   49   56   63    70
 [8,]    8   16   24   32   40   48   56   64   72    80
 [9,]    9   18   27   36   45   54   63   72   81    90
[10,]   10   20   30   40   50   60   70   80   90   100
[11,]   11   22   33   44   55   66   77   88   99   110
[12,]   12   24   36   48   60   72   84   96  108   120

and that in that table some numbers occur up to five times and others
not at all.

  
> 
> Please help,
> 
> Teresa
> 
> 
> #  Why doesn't this nested loop work?
> 
> n.max <- 300
> NUM <- 25
> 
> n.sim <- 10 
> j <- (n.max/NUM)*n.sim
> 
> results <- matrix(0, nrow=j, ncol=2)
> 
> while(NUM <= n.max){
> 
> for(i in 1:n.sim){
> 
> k <- (NUM/25)*i
> 
> results[k,1] <- k
> results[k,2] <- NUM
> 
> } 
> 
> NUM <- NUM + 25
> 
> }
> 
> results
> 
> #### TRY WHILE LOOP ONLY
> 
> results <- matrix(0, nrow=12, ncol=2)
> 
> n.max <- 300 
> NUM <- 25
> 
> while(NUM <= n.max){ 
> 
> k <- NUM/25
> 
> results[k,1] <- k 
> results[k,2] <- NUM 
> 
> NUM <- NUM + 25
> 
> }
> 
> results
> 
> # It works, here are the results
> 
> #      [,1] [,2]
> # [1,]    1   25
> # [2,]    2   50
> # [3,]    3   75
> # [4,]    4  100
> # [5,]    5  125
> # [6,]    6  150
> # [7,]    7  175
> # [8,]    8  200
> # [9,]    9  225
> #[10,]   10  250
> #[11,]   11  275
> #[12,]   12  300
> 
> 
> 
> ### Try For Loop Only 
> 
> n.sim <- 10
> NUM <- 25
> 
> results <- matrix(0, nrow=10, ncol=2)
> 
> for(i in 1:n.sim){
> 
> results[i,1] <- i
> results[i,2] <- NUM
> 
> } 
> 
> results
> 
> # it works, here are the results 
> #      [,1] [,2]
> # [1,]    1   25
> # [2,]    2   25
> # [3,]    3   25
> # [4,]    4   25
> # [5,]    5   25
> # [6,]    6   25
> # [7,]    7   25
> # [8,]    8   25
> # [9,]    9   25
> #[10,]   10   25
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Eric.Kort at vai.org  Wed Jan  4 17:11:42 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Wed, 4 Jan 2006 11:11:42 -0500
Subject: [R] Why doesn't this nested loop work?
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC174@VAIEXCH04.vai.org>


Teresa Nelson
> Hi there,
> 
> 
> 
> I can get the for-loop to work, I can get the while loop to work.  But
I
> can't get a for loop to work nested within the while loop - why?
> 
> 
> 
> Please help,
> 
> Teresa

It actually does work, but I think the problem is with your matrix
indexing.  See the sample below.  

You could also check out ?seq, ?rep, and ?apply for solving this problem
with fewer lines of code.


 n.max <- 300
 NUM <- 25
 
 n.sim <- 10 
 j <- (n.max/NUM)*n.sim
 
 results <- matrix(0, nrow=j, ncol=2)
 
 while(NUM <= n.max){
 
 for(i in 1:n.sim){
 
 k <- (NUM/25)*i
 
 results[((NUM / 25) - 1) * n.sim + i, 1] <- k
 results[((NUM / 25) - 1) * n.sim + i, 2] <- NUM
 cat("Iteration", i, ": NUM=", NUM, "k=", k, "\n")
 
 } 
 
 NUM <- NUM + 25
 }
This email message, including any attachments, is for the so...{{dropped}}



From justin_bem at yahoo.fr  Wed Jan  4 17:14:48 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 4 Jan 2006 17:14:48 +0100 (CET)
Subject: [R] A comment about R:
In-Reply-To: <03DCBBA079F2324786E8715BE538968A3DC6AA@FIGMAIL-CLUS01.FIG.FK>
Message-ID: <20060104161448.61548.qmail@web25703.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/f1746996/attachment.pl

From hvermei1 at vrcbe.jnj.com  Wed Jan  4 17:17:08 2006
From: hvermei1 at vrcbe.jnj.com (Vermeiren, Hans [VRCBE])
Date: Wed, 4 Jan 2006 17:17:08 +0100 
Subject: [R] e1071::SVM calculate distance to separating hyperplane
Message-ID: <9AC105024CEA64458BF66D1DE13CA50D070FB45F@tibbemeexs1.eu.jnj.com>

Hi,
I know this question has been posed before, but I didnt find the answer in
the R-help archive, so please accept my sincere apologies for being
repetitive:
How can one (elegantly) calculate the distance between data points (in the
transformed space, I suppose) and the hyperplane that separates the 2
categories when using svm() from the e1071 library?

thanks a lot,
Hans



From aleszib2 at gmail.com  Wed Jan  4 17:22:33 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Wed, 4 Jan 2006 17:22:33 +0100
Subject: [R] Putting an object in to a function that calls the current
	function
In-Reply-To: <43BBF21D.4040304@stats.uwo.ca>
Message-ID: <005601c6114b$36d3d6f0$0300a8c0@TAMARA>

I do not belive this would work in my case, since as I said, the function is
called by several different functions.

Ales Ziberna 

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
Sent: Wednesday, January 04, 2006 5:05 PM
To: Ales Ziberna
Cc: 'R-help'
Subject: Re: [R] Putting an object in to a function that calls the current
function

On 1/4/2006 10:32 AM, Ales Ziberna wrote:
> Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
> Both work and my problem is solved.
> 
> I do aggree with Duncan Murdoch that usually messing with the 
> environment of your caller is a bad idea. The reason why I still want 
> to do it in this case is that I exactly know which functions are 
> calling (the function is NEVER called directly) it and it was in this 
> case easier to use this than to modify each of the fuctions that are
calling it.

Using R's lexical scope may lead to a cleaner solution.  That is, you define
the functions within the one that calls them; then a <<- "ok" 
would do what you want (provided "a" existed in the enclosure at the time).

For example,

f <- function() {
	a <- "init"
	s <- function() {
		a <<- "ok"
         }
	s()
	print(a)
}

Duncan Murdoch

> 
> Thanks again!
> Ales Ziberna
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: Wednesday, January 04, 2006 3:26 PM
> To: Ales Ziberna
> Cc: R-help
> Subject: Re: [R] Putting an object in to a function that calls the 
> current function
> 
> On 1/4/2006 9:14 AM, Ales Ziberna wrote:
>> Hello!
>> 
>> I would like to put an object in to a function that calls the current 
>> function.
>> 
>> I thought the answer will be clear to me after reading the help files:
>> ?assign
>> ?sys.parent
>> 
>> However it is not.
>> Here is an example I thought should work, however it dose not exactly:
>> 
>> f<-function(){s();print(a)}
>> s<-function()assign(x="a",value="ok",pos=sys.parent())
>> f() #I want to get "ok"
>> a #I do not want "a" in global enviorment, so here I should get
>> #Error: Object "a" not found
>> ff<-function()f() #here I also want to get "ok" - it should not 
>> matter if the parent fuction has any parents
>> 
>> Thank you in advance for suggestions!
> 
> That's not a good idea.  Why would you want to do something like that?
> 
> That out of the way, here's a function that does it:
> 
> f<-function(){s();print(a)}
> s<-function()assign(x="a",value="ok",env=parent.frame())
> 
> The difference between pos=sys.parent() and env=parent.frame() is that 
> the pos is interpreted as a position in the search list (see ?assign), 
> while
> parent.frame() gives you the environment from the stack, equivalent to 
> sys.frame(sys.parent()).
> 
> In R you're almost certainly better off working directly with 
> environments, rather than going through integer indexing the way you 
> (used to?) have to do in S-PLUS.
> 
> Did I mention that messing with the environment of your caller is a 
> bad idea?  It's not yours, don't touch it.
> 
> Duncan Murdoch



From yvonnick.noel at uhb.fr  Wed Jan  4 17:34:11 2006
From: yvonnick.noel at uhb.fr (yvonnick noel)
Date: Wed, 04 Jan 2006 17:34:11 +0100
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <Pine.LNX.4.61.0601041349470.27135@gannet.stats>
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
	<Pine.LNX.4.61.0601041349470.27135@gannet.stats>
Message-ID: <20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>

> You need one of
>
> gsub("\\\\","/","c:\\My Documents\\data.dat")
> gsub("\\","/","c:\\My Documents\\data.dat", fixed = TRUE)
> chartr("\\", "/", "c:\\My Documents\\data.dat")

The string I get is an ASCII string in a web page, through the use of 
an <INPUT
type="file" ... > tag (with a "browse" button). This string is caught as is by
R through the Rpad interface (using tcltk as a mini local webserver).

So it is not manually input by the user. As it appears in a textfield on a web
page, I could of course ask the user to change it manually and double the
antislashes. But this is not user-friendly.

>> Note that I have no control on the string which is returned from the 
>> system (no such problem under Linux BTW).
>
> Really?  What happens with file names containing backslashes on Linux?

I just meant no conversion was needed under Linux since pathnames use slashes.
That's why I didn't see this problem until I had my student use the graphical
interface under Windows.

Thank you,

Yvonnick.



From aleszib2 at gmail.com  Wed Jan  4 17:39:07 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Wed, 4 Jan 2006 17:39:07 +0100
Subject: [R] Why doesn't this nested loop work?
In-Reply-To: <200601041544.k04FiGJi018813@msa1-mx.centurytel.net>
Message-ID: <005801c6114d$7d553d10$0300a8c0@TAMARA>

Is this what you are searching for?

n.max <- 300
NUM <- 25
id<-0

n.sim <- 10 
j <- (n.max/NUM)*n.sim

results <- matrix(0, nrow=j, ncol=2)

while(NUM <= n.max){

for(i in 1:n.sim){

k <- (NUM/25)*i
id<-id+1
results[id,1] <- k
results[id,2] <- NUM

} 

NUM <- NUM + 25

}

results
<<<


If not, plase give an example how you would like the results to look like!

Best,
Ales Ziberna

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Teresa Nelson
Sent: Wednesday, January 04, 2006 4:44 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Why doesn't this nested loop work?

Hi there,

 

I can get the for-loop to work, I can get the while loop to work.  But I
can't get a for loop to work nested within the while loop - why?  

 

Please help,

Teresa



From justin_bem at yahoo.fr  Wed Jan  4 17:40:25 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 4 Jan 2006 17:40:25 +0100 (CET)
Subject: [R] newbie R question
In-Reply-To: <x2d5j9vwxa.fsf@turmalin.kubism.ku.dk>
Message-ID: <20060104164025.5185.qmail@web25710.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/4b8e234d/attachment.pl

From kubovy at virginia.edu  Wed Jan  4 17:38:37 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 4 Jan 2006 11:38:37 -0500
Subject: [R] Difficulty with 'merge'
Message-ID: <202EEF7E-3225-4495-878E-3EB949D01F8A@virginia.edu>

Dear R-helpers,

Happy New Year to all the helpful members of the list.

Here is the behavior I'm looking for:
 > v1 <- c("a","b","c")
 > n1 <- c(0, 1, 2)
 > v2 <- c("c", "a", "b")
 > n2 <- c(0, 1 , 2)
 > (f1  <- data.frame(v1, n1))
   v1 n1
1  a  0
2  b  1
3  c  2
 > (f2 <- data.frame(v2, n2))
   v2 n2
1  c  0
2  a  1
3  b  2
 > (m12 <- merge(f1, f2, by.x = "v1", by.y = "v2", sort = F))
   v1 n1 n2
1  c  2  0
2  a  0  1
3  b  1  2

Now to my data:
 > summary(pL)
         pairL
a fondo   :  41
alto      :  41
ampio     :  41
angoloso  :  41
aperto    :  41
appoggiato:  41
(Other)   :1271

 > pL$pairL[c(1,42)]
[1] appoggiato dentro
37 Levels: a fondo alto ampio angoloso aperto appoggiato asimmetrico  
complicato convesso davanti dentro destra ... verticale

 > summary(oppN)
         pairL              pairR         subject            
L                LL                RR               M
a fondo   :  41   a galla    :  41   S1     :  37   Min.   :0.3646    
Min.   :0.02083   Min.   :0.0010   Min.   :0.0000
alto      :  41   acuto      :  41   S10    :  37   1st Qu.:0.5521    
1st Qu.:0.37500   1st Qu.:0.1771   1st Qu.:0.1042
ampio     :  41   arrotondato:  41   S11    :  37   Median :0.6354    
Median :0.47917   Median :0.2708   Median :0.2292
angoloso  :  41   basso      :  41   S12    :  37   Mean   :0.6403    
Mean   :0.46452   Mean   :0.2760   Mean   :0.2598
aperto    :  41   chiuso     :  41   S13    :  37   3rd Qu.:0.7188    
3rd Qu.:0.55208   3rd Qu.:0.3750   3rd Qu.:0.3854
appoggiato:  41   compl      :  41   S14    :  37   Max.   :0.9375    
Max.   :0.92708   Max.   :0.6042   Max.   :0.7812
(Other)   :1271   (Other)    :1271   (Other): 
1295                                      NA's   :3.0000   NA's   : 
3.0000
       asym             polar            polar_a1          clust
Min.   :-0.5555   Min.   :-1.2410   Min.   :-2.949e+00   c1:492
1st Qu.: 0.2091   1st Qu.: 0.4571   1st Qu.:-1.902e-01   c2:287
Median : 0.5555   Median : 1.1832   Median :-1.110e-16   c3: 82
Mean   : 0.6265   Mean   : 1.3428   Mean   :-5.745e-02   c4:246
3rd Qu.: 0.9383   3rd Qu.: 2.0712   3rd Qu.: 1.168e-01   c5: 82
Max.   : 2.7081   Max.   : 4.6151   Max.   : 4.218e+00   c6:328
                    NA's   : 3.0000   NA's   : 3.000e+00

 > oppN$pairL[c(1,42)]
[1] spesso fine
37 Levels: a fondo alto ampio angoloso aperto appoggiato asimmetrico  
complicato convesso davanti dentro destra ... verticale

 > unique(sort(oppM$pairL)) == unique(sort(pL$pairL))
[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  
TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
[26] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE

In other words I think that pL$pairL and oppN$pairL consists of 37  
blocks of 41 repetitions of names, and that these blocks are  
permutations of each other,

However:

 > summary(m1 <- merge(oppM, pairL, by.x = "pairL", by.y = "pairL",  
sort = F))
         pairL               pairR          subject             
L                LL                RR               M
a fondo   : 1681   a galla    : 1681   S1     : 1517   Min.   : 
0.3646   Min.   :0.02083   Min.   :0.0010   Min.   :0.0000
alto      : 1681   acuto      : 1681   S10    : 1517   1st Qu.: 
0.5521   1st Qu.:0.37500   1st Qu.:0.1771   1st Qu.:0.1042
ampio     : 1681   arrotondato: 1681   S11    : 1517   Median : 
0.6354   Median :0.47917   Median :0.2708   Median :0.2292
angoloso  : 1681   basso      : 1681   S12    : 1517   Mean   : 
0.6398   Mean   :0.46402   Mean   :0.2760   Mean   :0.2598
aperto    : 1681   chiuso     : 1681   S13    : 1517   3rd Qu.: 
0.7188   3rd Qu.:0.55208   3rd Qu.:0.3750   3rd Qu.:0.3854
appoggiato: 1681   compl      : 1681   S14    : 1517   Max.   : 
0.9375   Max.   :0.92708   Max.   :0.6042   Max.   :0.7812
(Other)   :51988   (Other)    :51988   (Other):52972
       asym             polar            polar_a1          clust
Min.   :-0.5555   Min.   :-1.2410   Min.   :-2.949e+00   c1:20172
1st Qu.: 0.2091   1st Qu.: 0.4571   1st Qu.:-1.904e-01   c2:11644
Median : 0.5555   Median : 1.1832   Median :-1.110e-16   c3: 3362
Mean   : 0.6234   Mean   : 1.3428   Mean   :-5.745e-02   c4:10086
3rd Qu.: 0.9383   3rd Qu.: 2.0712   3rd Qu.: 1.169e-01   c5: 3362
Max.   : 2.7081   Max.   : 4.6151   Max.   : 4.218e+00   c6:13448

I was expecting pairL to be 41 items longs, not 1681 = 41^2.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From dsonneborn at ucdavis.edu  Wed Jan  4 17:53:40 2006
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Wed, 04 Jan 2006 08:53:40 -0800
Subject: [R] multiple lowess line in one plot
Message-ID: <43BBFD94.8050405@yellow.ucdavis.edu>

I'm using this code to plot a smoothed line.  These two columns of data 
really represent 4 groups and I'd like to plot a separate line for each 
group but have them all in the same plot. The R-Docs for lowess do not 
seem to indicate some type of "GROUPS=var_name" option. What would be 
the syntax for this?

plot(AWGT ~ lipid )
lines(lowess(lipid , AWGT, f=.8))


-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516



From ripley at stats.ox.ac.uk  Wed Jan  4 18:08:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 17:08:12 +0000 (GMT)
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
	<Pine.LNX.4.61.0601041349470.27135@gannet.stats>
	<20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
Message-ID: <Pine.LNX.4.61.0601041658450.1064@gannet.stats>

On Wed, 4 Jan 2006, yvonnick noel wrote:

>> You need one of
>> 
>> gsub("\\\\","/","c:\\My Documents\\data.dat")
>> gsub("\\","/","c:\\My Documents\\data.dat", fixed = TRUE)
>> chartr("\\", "/", "c:\\My Documents\\data.dat")
>
> The string I get is an ASCII string in a web page, through the use of an 
> <INPUT
> type="file" ... > tag (with a "browse" button). This string is caught as is 
> by
> R through the Rpad interface (using tcltk as a mini local webserver).
>
> So it is not manually input by the user. As it appears in a textfield on a 
> web
> page, I could of course ask the user to change it manually and double the
> antislashes. But this is not user-friendly.

You really don't understand what I wrote (and it is in many places in the 
R documentation).  How are you getting that string into R? It does not 
contain doubled backslashes: that is the way C represents backslashes. 
For example

> foo <- readLines(n=1)
c:\My Documents\data.dat
> foo
[1] "c:\\My Documents\\data.dat"

So if you are reading the file path into R, you do not need to double the 
backslashes, only if you are entering it as a character string within 
quotes at the R prompt.

>>> Note that I have no control on the string which is returned from the 
>>> system (no such problem under Linux BTW).
>> 
>> Really?  What happens with file names containing backslashes on Linux?
>
> I just meant no conversion was needed under Linux since pathnames use 
> slashes.

As I did say, they can also use backslashes: it is a perfectly valid 
character in a Linux file name.

> That's why I didn't see this problem until I had my student use the graphical
> interface under Windows.
>
> Thank you,
>
> Yvonnick.
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Wed Jan  4 18:10:17 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 12:10:17 -0500
Subject: [R] Putting an object in to a function that calls the current
	function
In-Reply-To: <43BBF21D.4040304@stats.uwo.ca>
References: <004c01c61144$2859cd20$0300a8c0@TAMARA>
	<43BBF21D.4040304@stats.uwo.ca>
Message-ID: <971536df0601040910r35401fv8ed3825834193c40@mail.gmail.com>

Often situations like this can be represented using object
oriented concepts.  The data that you want to access that is
outside your function is wrapped in an object and the function
becomes a method of that object.

Using Duncan's example and the proto package we
define object oo with variable a and method s and
then invoke s.

library(proto)
oo <- proto(a = "init", s = function(.)  .$a <- "ok")
oo$a
oo$s()
oo$a

This could also be done using the R.oo package.

On 1/4/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 1/4/2006 10:32 AM, Ales Ziberna wrote:
> > Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
> > Both work and my problem is solved.
> >
> > I do aggree with Duncan Murdoch that usually messing with the environment of
> > your caller is a bad idea. The reason why I still want to do it in this case
> > is that I exactly know which functions are calling (the function is NEVER
> > called directly) it and it was in this case easier to use this than to
> > modify each of the fuctions that are calling it.
>
> Using R's lexical scope may lead to a cleaner solution.  That is, you
> define the functions within the one that calls them; then a <<- "ok"
> would do what you want (provided "a" existed in the enclosure at the time).
>
> For example,
>
> f <- function() {
>        a <- "init"
>        s <- function() {
>                a <<- "ok"
>         }
>        s()
>        print(a)
> }
>
> Duncan Murdoch
>
> >
> > Thanks again!
> > Ales Ziberna
> >
> > -----Original Message-----
> > From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> > Sent: Wednesday, January 04, 2006 3:26 PM
> > To: Ales Ziberna
> > Cc: R-help
> > Subject: Re: [R] Putting an object in to a function that calls the current
> > function
> >
> > On 1/4/2006 9:14 AM, Ales Ziberna wrote:
> >> Hello!
> >>
> >> I would like to put an object in to a function that calls the current
> >> function.
> >>
> >> I thought the answer will be clear to me after reading the help files:
> >> ?assign
> >> ?sys.parent
> >>
> >> However it is not.
> >> Here is an example I thought should work, however it dose not exactly:
> >>
> >> f<-function(){s();print(a)}
> >> s<-function()assign(x="a",value="ok",pos=sys.parent())
> >> f() #I want to get "ok"
> >> a #I do not want "a" in global enviorment, so here I should get
> >> #Error: Object "a" not found
> >> ff<-function()f() #here I also want to get "ok" - it should not matter
> >> if the parent fuction has any parents
> >>
> >> Thank you in advance for suggestions!
> >
> > That's not a good idea.  Why would you want to do something like that?
> >
> > That out of the way, here's a function that does it:
> >
> > f<-function(){s();print(a)}
> > s<-function()assign(x="a",value="ok",env=parent.frame())
> >
> > The difference between pos=sys.parent() and env=parent.frame() is that the
> > pos is interpreted as a position in the search list (see ?assign), while
> > parent.frame() gives you the environment from the stack, equivalent to
> > sys.frame(sys.parent()).
> >
> > In R you're almost certainly better off working directly with environments,
> > rather than going through integer indexing the way you (used to?) have to do
> > in S-PLUS.
> >
> > Did I mention that messing with the environment of your caller is a bad
> > idea?  It's not yours, don't touch it.
> >
> > Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jtanelson at centurytel.net  Wed Jan  4 18:15:05 2006
From: jtanelson at centurytel.net (Teresa Nelson)
Date: Wed, 4 Jan 2006 11:15:05 -0600
Subject: [R] Why doesn't this nested loop work?
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC174@VAIEXCH04.vai.org>
Message-ID: <200601041715.k04HF2hK014202@msa1-mx.centurytel.net>

Hello there,

Thanks to everyone that responded.  Thank-you Eric for showing me MY ALGEBRA
ERROR!  Geesh!  I apologize.

Again - thank-you,
Teresa

-----Original Message-----
From: Kort, Eric [mailto:Eric.Kort at vai.org] 
Sent: Wednesday, January 04, 2006 10:12 AM
To: Teresa Nelson; r-help at stat.math.ethz.ch
Subject: RE: [R] Why doesn't this nested loop work?


Teresa Nelson
> Hi there,
> 
> 
> 
> I can get the for-loop to work, I can get the while loop to work.  But
I
> can't get a for loop to work nested within the while loop - why?
> 
> 
> 
> Please help,
> 
> Teresa

It actually does work, but I think the problem is with your matrix
indexing.  See the sample below.  

You could also check out ?seq, ?rep, and ?apply for solving this problem
with fewer lines of code.


 n.max <- 300
 NUM <- 25
 
 n.sim <- 10 
 j <- (n.max/NUM)*n.sim
 
 results <- matrix(0, nrow=j, ncol=2)
 
 while(NUM <= n.max){
 
 for(i in 1:n.sim){
 
 k <- (NUM/25)*i
 
 results[((NUM / 25) - 1) * n.sim + i, 1] <- k
 results[((NUM / 25) - 1) * n.sim + i, 2] <- NUM
 cat("Iteration", i, ": NUM=", NUM, "k=", k, "\n")
 
 } 
 
 NUM <- NUM + 25
 }
This email message, including any attachments, is for the so...{{dropped}}



From pinard at iro.umontreal.ca  Wed Jan  4 17:04:12 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 4 Jan 2006 11:04:12 -0500
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.58.0601031403280.19586@maplepark.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
Message-ID: <20060104160412.GA8708@phenix.sram.qc.ca>

[David Forrest]

>[...] A few end-to-end tutorials on some interesting analyses would be
>helpful.

I'm in the process of learning R.  While tutorials are undoubtedly very 
useful, and understanding that working and studying methods vary between 
individuals, what I (for one) would like to have is a fairly complete 
reference manual to the library.

Of course, we already have one, and that's marvellous already.  Yet, it 
is organised by library and, within each library, by function name: this
organisation means that the manual is mainly used as a reference, or 
else, that it ought to be studied from cover to cover, dauntingly.

The very same material could be organised by topics.  Chapters could be 
named like "General Help", "Language features", "Data types", "Data 
Handling", "Input/Output", "Graphics", "Statistics", and such.  The 
chapter "Language features", to take one example, could hold sections 
like "Expressions", "Statements", "Functions", "Environments", 
"Packages", "Execution" and "Debugging".  Sections could then hold 
current reference pages.  References by library and/or by function name 
could be stated either in appendices or as a general index at the end.

For those who happen to know it, I find the "Emacs Lisp Reference 
Manual" to be a good example for organising, in a very usable way,
a comprehensive reference to a flurry of library functions.  When one 
needs string handling functions, they are likely grouped together in the 
manual, and are likely all present.  A tutorial, by comparison, usually 
presents a subset, or even a tiny subset, of what is available.

>Any volunteers?

Not me, or at least, not before quite a long while.  The overall 
organisation of a reference should not be handled by beginners.  On the 
contrary, it rather requires someone who has comprehensive knowledge of 
all the material to be considered.

Just an idea.  A good work plan would be to establish a new structure 
for a reference manual, and once competent people (or this community as 
a whole) agrees on a structure, to develop mechanical means for 
generating a reference manual out of the current material.  The 
mechanism should likely allow for added glue text, about everywhere 
reasonable, and for diagnosing any lone, unreachable page in the current 
reference.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From lisawang at uhnres.utoronto.ca  Wed Jan  4 18:30:24 2006
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Wed, 04 Jan 2006 12:30:24 -0500
Subject: [R] How to produce this graphic
Message-ID: <43BC0630.74FB055D@uhnres.utoronto.ca>

Hello there,

I would like to produce a plot of x<-c(4,5,6),which is the mean of each
group and y<-c('groupA','groupB','groupC'). 

plot (x,y) can not produce any graphics because y is not numerical. 

What should I do to produce this graphic?


Thank you in advance

Lisa Wang
Princess Margaret Hospital
Toronto,Ca



From sfalcon at fhcrc.org  Wed Jan  4 18:35:58 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 04 Jan 2006 09:35:58 -0800
Subject: [R] ANN: Advanced R programming course, Seattle, Jan 18-20
Message-ID: <m2d5j7c2v5.fsf@ziti.local>

Spots are still available for an advanced R programming course to be
held Jan 18-20 in Seattle.

Instructors: Robert Gentleman
             Seth Falcon

Topics: 
    Lexical scope
    Vectorization
    S3 and S4 OOP
    R packages
    Database connectivity
    Interfacing to C via .C and .Call

Details:

    January 18-20, 2006 
    Fred Hutchinson Cancer Research Center
    Seattle, WA, USA

    http://bioconductor.org/rforbioc/



From kubovy at virginia.edu  Wed Jan  4 18:33:12 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 4 Jan 2006 12:33:12 -0500
Subject: [R] bwplot reorder factor on y axis
Message-ID: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>

Dear R-helpers,

In bwplot(pairL ~ asym, oppK) is a factor. It displays pairL in  
alphabetical order. How do I tell it to display, pairL in the order I  
wish , i.e., so the medians of asym are in ascending order?


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From Roger.Bivand at nhh.no  Wed Jan  4 18:47:11 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 4 Jan 2006 18:47:11 +0100 (CET)
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.64.0601040755550.15250@homer23.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0601041842570.9402-100000@reclus.nhh.no>

On Wed, 4 Jan 2006, Thomas Lumley wrote:

> On Wed, 4 Jan 2006, Roger Bivand wrote:
> > Could I ask for comments on:
> >
> > source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
> >
> > as a reproduction of the Stata capabilities session? Both the t test and
> > the chi-square from our side point up oddities. I didn't succeed on
> > putting fit lines on a grouped xyplot, so backed out to base graphics.
> > This could be Swoven, possibly using the RweaveHTML driver.
> >
> 
> Personally, I would have used with() for some of these, eg
> 
> with(auto, table(rep78, foreign))
> with(auto, chisq.test(table(rep78, foreign)))

Thanks. For Windows users (sorry, I only checked on Linux), a new version 
of capabilities.R is now available adding the mode="wb" flag to 
download.file() - the absence of the flag was corrupting the saving of the 
auto.dta file, because it was not being saved as binary - it is now, 
checked under R 2.2.0 and 2.2.1 patched under Windows XP.

Roger

> 
> and I'm sure that someone will point out the right way to get lines on 
> coplot() or xyplot().
> 
> Apart from that it looks good.
> 
>  	-thomas
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From yvonnick.noel at uhb.fr  Wed Jan  4 18:54:08 2006
From: yvonnick.noel at uhb.fr (yvonnick noel)
Date: Wed, 04 Jan 2006 18:54:08 +0100
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <Pine.LNX.4.61.0601041658450.1064@gannet.stats>
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
	<Pine.LNX.4.61.0601041349470.27135@gannet.stats>
	<20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
	<Pine.LNX.4.61.0601041658450.1064@gannet.stats>
Message-ID: <20060104185408.yt0r30auc8coo48w@webmail.uhb.fr>

> You really don't understand what I wrote (and it is in many places in 
> the R documentation).  How are you getting that string into R?

OK. A practical example is probably necessary here. I have rewritten a small
Rpad page to show you what I mean. It is copied at the end of this post.

Just save it under your current R directory, under the name, say,
"example.Rpad", and then:

library(Rpad)
Rpad("example.Rpad")

Your browser will open and display the page (I tried it with Mozilla Firefox).
Then browse your disk using the browse button and select any file. Then click
the "Print the name" button.

The full file name is sent to R and internally stored in the R variable
"datafile". The embedded R code is just :

print(datafile)

This is the way I get this string.

Yvonnick.

Here is the script :

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
  <head>
    <title>An example</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso8859-1">
    <script type="text/javascript" src="editor/Rpad_loader.js">
    </script>
  </head>
  <body>
  <H1>Load data</H1>
  <h3><a name="fil"> From a local or remote file </a></h3>
  <span contenteditable="false">
    <p><input type="file" name="datafile" size="40" class="Rpad_input"
id="datafile" rpad_type="Rstring"></p>
    <P><input type="button" onclick="javascript:top.Rpad_calculate();"
value="Print the name"></p>
  </span>
  <span class="wrapperForHidden" contenteditable="false">
    <div class="Rpad_input" rpad_type="R" style="DISPLAY: none">
      <pre>
        # This is the R code
        if(exists("datafile")){
        print(datafile)
        }
      </pre>
    </div>
  </span>
  <hr>
  </body>
</html>



From p.dalgaard at biostat.ku.dk  Wed Jan  4 19:19:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jan 2006 19:19:12 +0100
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
	<Pine.LNX.4.61.0601041349470.27135@gannet.stats>
	<20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
Message-ID: <x2zmmbdffj.fsf@viggo.kubism.ku.dk>

yvonnick noel <yvonnick.noel at uhb.fr> writes:

> > You need one of
> >
> > gsub("\\\\","/","c:\\My Documents\\data.dat")
> > gsub("\\","/","c:\\My Documents\\data.dat", fixed = TRUE)
> > chartr("\\", "/", "c:\\My Documents\\data.dat")
> 
> The string I get is an ASCII string in a web page, through the use of 
> an <INPUT
> type="file" ... > tag (with a "browse" button). This string is caught as is by
> R through the Rpad interface (using tcltk as a mini local webserver).
> 
> So it is not manually input by the user. As it appears in a textfield on a web
> page, I could of course ask the user to change it manually and double the
> antislashes. But this is not user-friendly.

But what does R see? I.e., what is in the string that gets passed to
R, and how does it get passed?

You sound confused (many are) about character escapes: Notice that
"\\" is a string with *one* character in it, namely the backslash.
"\"" has no backslash inside; it contains one double-quote character,
etc. The first backslash is just for representation - it is used to
print or specify a string with a quote, a newline, a bell character,
or the backslash escape character itself.

In general, if you read a string from a Tcl/Tk form or text widget or
whatever using the tcltk package, everything should be OK. However, if
the string is somehow being parsed by R, the *interface* has a problem
and needs to do some escaping first.
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Jan  4 19:24:00 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 13:24:00 -0500
Subject: [R] Putting an object in to a function that calls the current
 function
In-Reply-To: <005601c6114b$36d3d6f0$0300a8c0@TAMARA>
References: <005601c6114b$36d3d6f0$0300a8c0@TAMARA>
Message-ID: <43BC12C0.5020607@stats.uwo.ca>

On 1/4/2006 11:22 AM, Ales Ziberna wrote:
> I do not belive this would work in my case, since as I said, the function is
> called by several different functions.

This may not be welcome advice, but I don't think that is a great idea 
either.  Conventionally using the same name for the same purpose in 
different functions is generally a good idea, but writing functions that 
depend on things being named identically is risky.  You may find that in 
2 years when you are editing this code that you change the name, but 
miss one of the locations:  then you may get a very obscure bug.  Or you 
may be reading through your function and forget that s() changes the 
value of a variable, so you won't understand what is going on in your code.

High level source code is meant equally for humans to read as it is for 
the interpreter to act on, so you should write it clearly.

I don't know the context of your problem, but I'd think a better 
approach would be to have your s() function return the variable a as 
part of its return value (so the several different functions could store 
it where they liked), or to define a in an environment where it is 
accessible from all the functions that need to use it including s(), and 
do something like the lexical scoping solution below.

Duncan Murdoch


> 
> Ales Ziberna 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca] 
> Sent: Wednesday, January 04, 2006 5:05 PM
> To: Ales Ziberna
> Cc: 'R-help'
> Subject: Re: [R] Putting an object in to a function that calls the current
> function
> 
> On 1/4/2006 10:32 AM, Ales Ziberna wrote:
>> Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
>> Both work and my problem is solved.
>> 
>> I do aggree with Duncan Murdoch that usually messing with the 
>> environment of your caller is a bad idea. The reason why I still want 
>> to do it in this case is that I exactly know which functions are 
>> calling (the function is NEVER called directly) it and it was in this 
>> case easier to use this than to modify each of the fuctions that are
> calling it.
> 
> Using R's lexical scope may lead to a cleaner solution.  That is, you define
> the functions within the one that calls them; then a <<- "ok" 
> would do what you want (provided "a" existed in the enclosure at the time).
> 
> For example,
> 
> f <- function() {
> 	a <- "init"
> 	s <- function() {
> 		a <<- "ok"
>          }
> 	s()
> 	print(a)
> }
> 
> Duncan Murdoch
> 
>> 
>> Thanks again!
>> Ales Ziberna
>> 
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
>> Sent: Wednesday, January 04, 2006 3:26 PM
>> To: Ales Ziberna
>> Cc: R-help
>> Subject: Re: [R] Putting an object in to a function that calls the 
>> current function
>> 
>> On 1/4/2006 9:14 AM, Ales Ziberna wrote:
>>> Hello!
>>> 
>>> I would like to put an object in to a function that calls the current 
>>> function.
>>> 
>>> I thought the answer will be clear to me after reading the help files:
>>> ?assign
>>> ?sys.parent
>>> 
>>> However it is not.
>>> Here is an example I thought should work, however it dose not exactly:
>>> 
>>> f<-function(){s();print(a)}
>>> s<-function()assign(x="a",value="ok",pos=sys.parent())
>>> f() #I want to get "ok"
>>> a #I do not want "a" in global enviorment, so here I should get
>>> #Error: Object "a" not found
>>> ff<-function()f() #here I also want to get "ok" - it should not 
>>> matter if the parent fuction has any parents
>>> 
>>> Thank you in advance for suggestions!
>> 
>> That's not a good idea.  Why would you want to do something like that?
>> 
>> That out of the way, here's a function that does it:
>> 
>> f<-function(){s();print(a)}
>> s<-function()assign(x="a",value="ok",env=parent.frame())
>> 
>> The difference between pos=sys.parent() and env=parent.frame() is that 
>> the pos is interpreted as a position in the search list (see ?assign), 
>> while
>> parent.frame() gives you the environment from the stack, equivalent to 
>> sys.frame(sys.parent()).
>> 
>> In R you're almost certainly better off working directly with 
>> environments, rather than going through integer indexing the way you 
>> (used to?) have to do in S-PLUS.
>> 
>> Did I mention that messing with the environment of your caller is a 
>> bad idea?  It's not yours, don't touch it.
>> 
>> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan  4 19:29:15 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 04 Jan 2006 18:29:15 -0000 (GMT)
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104173411.bzurxycaowc4cwwk@webmail.uhb.fr>
Message-ID: <XFMail.060104182915.Ted.Harding@nessie.mcc.ac.uk>

On 04-Jan-06 yvonnick noel wrote:
> [...]
> The string I get is an ASCII string in a web page, through
> the use of an <INPUT> type="file" ... > tag (with a "browse"
> button). This string is caught as is by R through the Rpad
> interface (using tcltk as a mini local webserver).
> 
> So it is not manually input by the user.
> [...]

Yvonnik, the only line of approach I can think of at the
moment is to somehow read from the web page into a file
(though perhaps a socket/FIFO might work as well).

For example, if I make a file names.txt with contents

  c:\My Documents\data.dat

(even without quotes) then

  A<-readLines("names.txt",n=1)
  > A
  [1] "c:\\My Documents\\data.dat"

so the result is now in the format such that gsub will work.

E.g.

  gsub("\\\\","/",readLines("names.txt",n=1))
  [1] "c:/My Documents/data.dat"

So now all that would remain is for you to work out how best
to get the filename from the web page into the file which is
read by 'readLines'. One would need more information to work
out how to do that in the context of your usage.

Might reading it from the Windows clipboard work?

Hoping, anyway, that this helps a bit!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jan-06                                       Time: 18:29:12
------------------------------ XFMail ------------------------------



From sfalcon at fhcrc.org  Wed Jan  4 19:32:50 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 04 Jan 2006 10:32:50 -0800
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr> (yvonnick noel's
	message of "Wed, 04 Jan 2006 12:02:13 +0100")
References: <20060104120213.zubxjezwg0kwwgg8@webmail.uhb.fr>
Message-ID: <m2u0cjalnx.fsf@ziti.local>

Hi Yvonnick,

On  4 Jan 2006, yvonnick.noel at uhb.fr wrote:
> I am writing a GUI for R with the Rpad library. I have a "browse"
> button for data loading and Windows return a path string with
> backslashes. I need to convert them into slashes to use the string
> with read.table.

Can you provide some further detail on how the string is created?  I'm
not familiar with Rpad, but I wonder if the issue isn't at that level.

Note that if you create a string with single '\', R eats them up and
at that point there is nothing you can do:

s <- "x\y\z"
s
[1] "xyz"

Do you have a way of printing your path string?  

+ seth



From maustin at amgen.com  Wed Jan  4 19:41:18 2006
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 4 Jan 2006 10:41:18 -0800 
Subject: [R] multiple lowess line in one plot
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD64F@teal-exch.amgen.com>

Try using the panel.plsmo function in Dr. Harrell's Hmisc package.

xyplot(AWGT ~ lipid, groups = var_name, panel=panel.plsmo)

--Matt

Matt Austin

Statistician
Amgen, Inc.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Dean Sonneborn
Sent: Wednesday, January 04, 2006 8:54 AM
To: r-help at stat.math.ethz.ch
Subject: [R] multiple lowess line in one plot


I'm using this code to plot a smoothed line.  These two columns of data 
really represent 4 groups and I'd like to plot a separate line for each 
group but have them all in the same plot. The R-Docs for lowess do not 
seem to indicate some type of "GROUPS=var_name" option. What would be 
the syntax for this?

plot(AWGT ~ lipid )
lines(lowess(lipid , AWGT, f=.8))


-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From crepaldi_fabio at yahoo.it  Wed Jan  4 20:07:53 2006
From: crepaldi_fabio at yahoo.it (fabio crepaldi)
Date: Wed, 4 Jan 2006 20:07:53 +0100 (CET)
Subject: [R] How to create a correlation table for categorical data???
Message-ID: <20060104190753.61631.qmail@web26309.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/691246bf/attachment.pl

From yvonnick.noel at uhb.fr  Wed Jan  4 20:11:04 2006
From: yvonnick.noel at uhb.fr (yvonnick noel)
Date: Wed, 04 Jan 2006 20:11:04 +0100
Subject: [R] Replacing backslashes with slashes
In-Reply-To: <XFMail.060104182915.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060104182915.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20060104201104.0emo376d4wgwc0ww@webmail.uhb.fr>

Ted,

> For example, if I make a file names.txt with contents
>
>  c:\My Documents\data.dat
>
> (even without quotes) then
>
>  A<-readLines("names.txt",n=1)
>  > A
>  [1] "c:\\My Documents\\data.dat"
>
> so the result is now in the format such that gsub will work.
>
> E.g.
>
>  gsub("\\\\","/",readLines("names.txt",n=1))
>  [1] "c:/My Documents/data.dat"

Yes. I agree that this work. But I am sure you'll agree that it is not 
a simple
way to get double backslashes ! But it looks like it is the only way to get it
in R. Jim Holtman also suggested a similar solution with scan().

Probably the simplest way to go is to double the backslashes in javascript,
before the string is passed to R.

> So now all that would remain is for you to work out how best
> to get the filename from the web page into the file which is
> read by 'readLines'. One would need more information to work
> out how to do that in the context of your usage.

This would not be a problem. I could use R functions to do this, since Rpad
allows you to mix R code with standard web scripting.

Thank you.

Yvonnick.



From MMcIntosh at kvpharmaceutical.com  Wed Jan  4 20:09:46 2006
From: MMcIntosh at kvpharmaceutical.com (MMcIntosh@kvpharmaceutical.com)
Date: Wed, 4 Jan 2006 13:09:46 -0600
Subject: [R] multiple lowess line in one plot
Message-ID: <03C1DDD7C6E26C4EBB28BF16DEE302BC0B88BE4D@kvmail1.kv.kvph.dom>

## Copy and paste this into R.
x1<-rbinom(dim(cars)[1],2,.5)
plot(cars,main="lowess(modified cars)",col=x1+1)
for(i in unique(x1)) lines(lowess(cars[x1==i,]),col=i+1)

This might help...

Matthew McIntosh



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dean Sonneborn
Sent: Wednesday, January 04, 2006 10:54 AM
To: r-help at stat.math.ethz.ch
Subject: [R] multiple lowess line in one plot

I'm using this code to plot a smoothed line.  These two columns of data 
really represent 4 groups and I'd like to plot a separate line for each 
group but have them all in the same plot. The R-Docs for lowess do not 
seem to indicate some type of "GROUPS=var_name" option. What would be 
the syntax for this?

plot(AWGT ~ lipid )
lines(lowess(lipid , AWGT, f=.8))


-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Eric.Kort at vai.org  Wed Jan  4 20:27:57 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Wed, 4 Jan 2006 14:27:57 -0500
Subject: [R] How to produce this graphic
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC17D@VAIEXCH04.vai.org>



Lisa Wang asks...
> Subject: [R] How to produce this graphic
> 
> Hello there,
> 
> I would like to produce a plot of x<-c(4,5,6),which is the mean of
each
> group and y<-c('groupA','groupB','groupC').
> 
> plot (x,y) can not produce any graphics because y is not numerical.
> 
> What should I do to produce this graphic?
> 

One possibility:
x <- c(4,5,6)
plot(x, axes=F)
axis(1, c(1:3), labels=y)

or see ?bar.plot

-Eric

> 
> Thank you in advance
> 
> Lisa Wang
> Princess Margaret Hospital
> Toronto,Ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
This email message, including any attachments, is for the so...{{dropped}}



From ggrothendieck at gmail.com  Wed Jan  4 20:39:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 14:39:25 -0500
Subject: [R] Putting an object in to a function that calls the current
	function
In-Reply-To: <005601c6114b$36d3d6f0$0300a8c0@TAMARA>
References: <43BBF21D.4040304@stats.uwo.ca>
	<005601c6114b$36d3d6f0$0300a8c0@TAMARA>
Message-ID: <971536df0601041139x2c4e474cm45a867b8eed70dbd@mail.gmail.com>

Try

   demo(scoping)

which shows a situation similar to the one Duncan describes
but uses several functions.

On 1/4/06, Ales Ziberna <aleszib2 at gmail.com> wrote:
> I do not belive this would work in my case, since as I said, the function is
> called by several different functions.
>
> Ales Ziberna
>
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: Wednesday, January 04, 2006 5:05 PM
> To: Ales Ziberna
> Cc: 'R-help'
> Subject: Re: [R] Putting an object in to a function that calls the current
> function
>
> On 1/4/2006 10:32 AM, Ales Ziberna wrote:
> > Thank you both (Duncan Murdoch and  Gabor Grotehendieck) for your answers.
> > Both work and my problem is solved.
> >
> > I do aggree with Duncan Murdoch that usually messing with the
> > environment of your caller is a bad idea. The reason why I still want
> > to do it in this case is that I exactly know which functions are
> > calling (the function is NEVER called directly) it and it was in this
> > case easier to use this than to modify each of the fuctions that are
> calling it.
>
> Using R's lexical scope may lead to a cleaner solution.  That is, you define
> the functions within the one that calls them; then a <<- "ok"
> would do what you want (provided "a" existed in the enclosure at the time).
>
> For example,
>
> f <- function() {
>        a <- "init"
>        s <- function() {
>                a <<- "ok"
>         }
>        s()
>        print(a)
> }
>
> Duncan Murdoch
>
> >
> > Thanks again!
> > Ales Ziberna
> >
> > -----Original Message-----
> > From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> > Sent: Wednesday, January 04, 2006 3:26 PM
> > To: Ales Ziberna
> > Cc: R-help
> > Subject: Re: [R] Putting an object in to a function that calls the
> > current function
> >
> > On 1/4/2006 9:14 AM, Ales Ziberna wrote:
> >> Hello!
> >>
> >> I would like to put an object in to a function that calls the current
> >> function.
> >>
> >> I thought the answer will be clear to me after reading the help files:
> >> ?assign
> >> ?sys.parent
> >>
> >> However it is not.
> >> Here is an example I thought should work, however it dose not exactly:
> >>
> >> f<-function(){s();print(a)}
> >> s<-function()assign(x="a",value="ok",pos=sys.parent())
> >> f() #I want to get "ok"
> >> a #I do not want "a" in global enviorment, so here I should get
> >> #Error: Object "a" not found
> >> ff<-function()f() #here I also want to get "ok" - it should not
> >> matter if the parent fuction has any parents
> >>
> >> Thank you in advance for suggestions!
> >
> > That's not a good idea.  Why would you want to do something like that?
> >
> > That out of the way, here's a function that does it:
> >
> > f<-function(){s();print(a)}
> > s<-function()assign(x="a",value="ok",env=parent.frame())
> >
> > The difference between pos=sys.parent() and env=parent.frame() is that
> > the pos is interpreted as a position in the search list (see ?assign),
> > while
> > parent.frame() gives you the environment from the stack, equivalent to
> > sys.frame(sys.parent()).
> >
> > In R you're almost certainly better off working directly with
> > environments, rather than going through integer indexing the way you
> > (used to?) have to do in S-PLUS.
> >
> > Did I mention that messing with the environment of your caller is a
> > bad idea?  It's not yours, don't touch it.
> >
> > Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pmuhl1848 at gmail.com  Wed Jan  4 20:43:08 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Wed, 04 Jan 2006 14:43:08 -0500
Subject: [R]  A comment about R:
Message-ID: <BFE18F7C.1279F%pmuhl1848@gmail.com>

I'm someone who from time to time comes to R to do applied stats for social
science research.  I think the R language is excellent--much better than
Stata for writing complex statistical programs.  I am thrilled that I can do
complex stats readily in R--sem, maximum likelihood, bootstrapping, some
Bayesian analysis.  I wish I could make R my main statistical package, but
find that a few stats that are important to my work are difficult to find or
produce in R.  Before I list some examples, I recognize that people view R
not as a statistical package but rather as a statistical programming
environment.  That said, however, it seems, from my admittedly limited
perspective, that it would be fairly easy to make a few adjustments to R
that would make it a lot more practical and friendly for a broader range of
people--including people like me who from time to time want to do
statistical programming but more often need to run canned procedures.  I'm
not a statistician, so I don't want to have to learn everything there is to
know about common procedures I use, including how to write them from
scratch.  I want to be able to focus my efforts on more novel problems w/o
reinventing the wheel.  I would also prefer not to have to work through a
couple books on R or S+ to learn how to meet common needs in R.  If R were
extended a bit in the direction of helping people like me, I wonder whether
it would not acquire a much broader audience.  Then again, these may just be
the rantings of someone not sufficiently familiar w/ R or the community of
stat package users--so take my comments w/ a grain of salt.

Some examples of statistics I typically use that are difficult to find and /
or produce or produce in a usefully formatted way in R--

Ex. 1)  Wald tests of linear hypotheses after max. likelihood or even after
a regression.  "Wald" does not even appear in my standard R package on a
search.  There's no comment in the lm help or optim help about what function
to use for hypothesis tests.  I know that statisticians prefer likelihood
ratio tests, but Wald tests are still useful and indeed crucial for
first-pass analysis.  After searching with Google for some time, I found
several Wald functions in various contributed R packages I did not have
installed.  One confusion was which one would be relevant to my needs.  This
took some time to resolve.  I concluded, perhaps on insufficient evidence,
that package car's Wald test would be most helpful.  To use it, however, one
has to put together a matrix for the hypotheses, which can be arduous for a
many-term regression or a complex hypothesis.  In comparison, in Stata one
simply states the hypothesis in symbolic terms.  I also don't know for
certain that this function in car will work or work properly w/ various
kinds of output, say from lm or from optim.  To be sure, I'd need to run
time-consuming tests comparing it with Stata output or examine the
function's code.  In Stata the test is easy to find, and there's no
uncertainty about where it can be run or its accuracy.  Simply having a
comment or "see also" in lm help or mle or optim help pointing the user to
the right Wald function would be of enormous help.

Ex. 2) Getting neat output of a regression with Huberized variance matrix.
I frequently have to run regressions w/ robust variances.  In Stata, one
simply adds the word "robust" to the end of the command or
"cluster(cluster.variable)" for a cluster-robust error.  In R, there are two
functions, robcov and hccm.  I had to run tests to figure out what the
relationship is between them and between them and Stata (robcov w/o cluster
gives hccm's hc0; hccm's hc1 is equivalent to Stata's 'robust' w/o cluster;
etc.).  A single sentence in hccm's help saying something to the effect that
statisticians prefer hc3 for most types of data might save me from having to
scramble through the statistical literature to try to figure out which of
these I should be using.  A few sentences on what the differences are
between these methods would be even better.  Then, there's the problem of
output.  Given that hc1 or hc3 are preferred for non-clustered data, I'd
need to be able to get regression output of the form summary(lm) out of
hccm, for any practical use.  Getting this, however, would require
programming my own function.  Huberized t-stats for regressions are
commonplace needs, an R oriented a little toward more everyday needs would
not require programming of such needs.  Also, I'm not sure yet how well any
of the existing functions handle missing data.

Ex. 3)  I need to do bootstrapping w/ clustered data, again a common
statistical need.  I wasted a good deal of time reading the help contents of
boot and Bootstrap, only to conclude that I'd need to write my own, probably
inefficient, function to bootstrap clustered data if I were to use boot.
It's odd that boot can't handle this more directly.  After more digging, I
learned that bootcov in package Design would handle the cluster bootstrap
and save the parameters.  I wouldn't have found this if I had not needed
bootcov for another purpose.  Again, maybe a few words in the boot help
saying that 'for clustered data, you could use bootcov or program a function
in boot' would be very helpful.  I still don't know whether I can feed the
results of bootcov back into functions in the boot package for further
analysis.

My 2 bits for what they're worth,

Peter



From peteoutside at yahoo.com  Wed Jan  4 20:52:10 2006
From: peteoutside at yahoo.com (Pete Cap)
Date: Wed, 4 Jan 2006 11:52:10 -0800 (PST)
Subject: [R] Selecting significant peaks in periodograms
Message-ID: <20060104195210.1580.qmail@web52405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/a58b2362/attachment.pl

From murdoch at stats.uwo.ca  Wed Jan  4 20:54:31 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 14:54:31 -0500
Subject: [R] How to produce this graphic
In-Reply-To: <43BC0630.74FB055D@uhnres.utoronto.ca>
References: <43BC0630.74FB055D@uhnres.utoronto.ca>
Message-ID: <43BC27F7.4080607@stats.uwo.ca>

On 1/4/2006 12:30 PM, Lisa Wang wrote:
> Hello there,
> 
> I would like to produce a plot of x<-c(4,5,6),which is the mean of each
> group and y<-c('groupA','groupB','groupC'). 
> 
> plot (x,y) can not produce any graphics because y is not numerical. 
> 
> What should I do to produce this graphic?

dotchart(x,y) does a nice job.  See ?dotchart for a lot of optional 
arguments to change the look of it.

Duncan Murdoch



From mherzog at prbo.org  Wed Jan  4 20:58:13 2006
From: mherzog at prbo.org (Mark Herzog)
Date: Wed, 04 Jan 2006 11:58:13 -0800
Subject: [R] bwplot reorder factor on y axis
In-Reply-To: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
References: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
Message-ID: <43BC28D5.5040503@prbo.org>

?factor (specifically, the levels= option)

oppK<-oppK[order(median(oppK$asym),]
oppK$pairL<-factor(oppK$pairL, levels=unique(oppK$pairL))
bwplot(pairL ~ asym, oppK)


Mark
Mark Herzog, Ph.D.
Program Leader, San Francisco Bay Research
Wetland Division, PRBO Conservation Science
4990 Shoreline Highway 1
Stinson Beach, CA 94970
(415) 893-7677 x308
mherzog at prbo.org

Michael Kubovy wrote:
> Dear R-helpers,
> 
> In bwplot(pairL ~ asym, oppK) is a factor. It displays pairL in  
> alphabetical order. How do I tell it to display, pairL in the order I  
> wish , i.e., so the medians of asym are in ascending order?
> 
> 
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
>



From Greg.Snow at intermountainmail.org  Wed Jan  4 21:04:23 2006
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Wed, 4 Jan 2006 13:04:23 -0700
Subject: [R] multiple lowess line in one plot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB19807E@LP-EXCHVS07.CO.IHC.COM>

Here is one approach using lattice (trellis) graphics:

tmp.state <- data.frame( Frost=state.x77[,'Frost'], 
	Murder=state.x77[,'Murder'], 
	Region=state.region)

library(lattice)
trellis.par.set(col.whitebg())
xyplot( Murder ~ Frost, groups=Region, data=tmp.state,
	panel.groups=function(...){panel.loess(...);panel.xyplot(...)}
,span=.8)


Just substitute in your variables and variable names.

Hope this helps,



-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dean Sonneborn
> Sent: Wednesday, January 04, 2006 9:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] multiple lowess line in one plot
> 
> I'm using this code to plot a smoothed line.  These two 
> columns of data really represent 4 groups and I'd like to 
> plot a separate line for each group but have them all in the 
> same plot. The R-Docs for lowess do not seem to indicate some 
> type of "GROUPS=var_name" option. What would be the syntax for this?
> 
> plot(AWGT ~ lipid )
> lines(lowess(lipid , AWGT, f=.8))
> 
> 
> --
> Dean Sonneborn, MS
> Programmer Analyst
> Department of Public Health Sciences
> University of California, Davis
> (530) 754-9516
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pauljohn32 at gmail.com  Wed Jan  4 21:15:10 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Wed, 4 Jan 2006 14:15:10 -0600
Subject: [R] multiple lowess line in one plot
In-Reply-To: <43BBFD94.8050405@yellow.ucdavis.edu>
References: <43BBFD94.8050405@yellow.ucdavis.edu>
Message-ID: <13e802630601041215w6c3d9375t9b2b2a5f53fb7cd8@mail.gmail.com>

It appears to me lowess has no "subset" or "strata" option. The
brute-force way (which my students like best) is just to create 4
columns, one for each group (with "unstack" or such) and then run one
lines(lowess()) command for each one.

I think there is a bit more art in using by, which will create subsets
on the fly for you.

Here is a self contained example that I just worked out to illustrate.
grp is the grouping factor, x and y are just illustrative data. The by
function creates the data subsets and they are accessed in the FUN as
"sub1".

x <- rnorm(100)
grp <- gl(4,25)
y <- rpois(100, lambda=4)
mydf <- data.frame(x,y,grp)
with(mydf, plot(x,y))
by (mydf, list(grp), function(sub1) lines(lowess(sub1$x, sub1$y)))

Hope that helps

On 1/4/06, Dean Sonneborn <dsonneborn at ucdavis.edu> wrote:
> I'm using this code to plot a smoothed line.  These two columns of data
> really represent 4 groups and I'd like to plot a separate line for each
> group but have them all in the same plot. The R-Docs for lowess do not
> seem to indicate some type of "GROUPS=var_name" option. What would be
> the syntax for this?
>
> plot(AWGT ~ lipid )
> lines(lowess(lipid , AWGT, f=.8))
>
>
> --
> Dean Sonneborn, MS
> Programmer Analyst
> Department of Public Health Sciences
> University of California, Davis
> (530) 754-9516
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From gynmeerut at indiatimes.com  Wed Jan  4 21:16:25 2006
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Thu, 05 Jan 2006 01:46:25 +0530
Subject: [R] comparision and removal
Message-ID: <200601041959.BAA18023@WS0005.indiatimes.com>



Dear All,


I am using R and I am putting my problem in form of an example:

X<-c(128,34,153,987,345,45,3454,23,123)
I want to remove the entries which are lesser than 100(? How to compare every element with 100 and how to create subsets )
and I need two vectors y and z s.t
y<-c(entries < 100)
z<- c(remaining entries)

Moreover, Please tell me which command to use if I want to use different programs for y and z.
X is the whole dataset and y,z are its disjoint subsets.

Thanks 
GS



From ripley at stats.ox.ac.uk  Wed Jan  4 21:18:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jan 2006 20:18:12 +0000 (GMT)
Subject: [R] bwplot reorder factor on y axis
In-Reply-To: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
References: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
Message-ID: <Pine.LNX.4.61.0601041959580.12023@gannet.stats>

On Wed, 4 Jan 2006, Michael Kubovy wrote:

> In bwplot(pairL ~ asym, oppK) is a factor. It displays pairL in
> alphabetical order. How do I tell it to display, pairL in the order I
> wish , i.e., so the medians of asym are in ascending order?

Actually, it displays pairL in the order of its levels. Trellis for S came 
with a function reorder.factor to re-order the levels, and base R has it 
too.  There is a very similar example on ?reorder.  More generally, see 
?levels

(There are other reorder.factor functions in Hmisc and gdata, which you 
may need to avoid.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan.sarkar at gmail.com  Wed Jan  4 21:29:10 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 4 Jan 2006 14:29:10 -0600
Subject: [R] bwplot reorder factor on y axis
In-Reply-To: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
References: <71C71E06-0E53-458E-A093-71C2725974FB@virginia.edu>
Message-ID: <eb555e660601041229n2ff976fbxc5e9494e1a7c85e@mail.gmail.com>

On 1/4/06, Michael Kubovy <kubovy at virginia.edu> wrote:
> Dear R-helpers,
>
> In bwplot(pairL ~ asym, oppK) is a factor. It displays pairL in
> alphabetical order. How do I tell it to display, pairL in the order I
> wish , i.e., so the medians of asym are in ascending order?

See ?reorder.factor, which has a very similar example you can adapt.
In your case, you probably want something like:

bwplot(reorder(factor(pairL), asym, median) ~ asym, oppK)

The call to factor is unnecessary if pairL is already a factor.

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From Mleeds at kellogggroup.com  Wed Jan  4 21:46:37 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Wed, 4 Jan 2006 15:46:37 -0500
Subject: [R] R newbie configuration
Message-ID: <A8B87FDB74320349A9D1CC9021052A764662FA@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/fb3fe40d/attachment.pl

From pmuhl1848 at gmail.com  Wed Jan  4 21:50:38 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Wed, 04 Jan 2006 15:50:38 -0500
Subject: [R] Bug in bootcov; R 2.2
Message-ID: <BFE19F4E.127AA%pmuhl1848@gmail.com>

I see that I need to send my bug report to the package maintainer.
Apologies for sending it to this list.  There's a lot to absorb from various
online pages when reporting a bug & I missed the part about sending to the
maintainer.

Peter



From deepayan.sarkar at gmail.com  Wed Jan  4 22:10:50 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 4 Jan 2006 15:10:50 -0600
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
References: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
Message-ID: <eb555e660601041310u69e90caei3102d88e6a10a3aa@mail.gmail.com>

On 1/4/06, Roger Bivand <Roger.Bivand at nhh.no> wrote:
> [...]
>
> Could I ask for comments on:
>
> source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
>
> as a reproduction of the Stata capabilities session? Both the t test and
> the chi-square from our side point up oddities. I didn't succeed on
> putting fit lines on a grouped xyplot, so backed out to base graphics.

Something like this perhaps:

xyplot(mpg + mpghat ~ weight | foreign, auto_wgt,
       panel = panel.superpose.2, type = c('p', 'l'))

-Deepayan

[...]



From freeman at u.washington.edu  Wed Jan  4 22:20:47 2006
From: freeman at u.washington.edu (Ted Freeman)
Date: Wed, 4 Jan 2006 13:20:47 -0800
Subject: [R] Using 'polygon' in a 3d plot
Message-ID: <73407e8e32ddfda614a299fc21c69e86@u.washington.edu>

I'm new to R, after many years of using Matlab. I've found the R 
function 'polygon' to be nearly equivalent to the Matlab function 
'patch'. For example, the R commands:

plot(c(0, 5), c(0, 4), type = 'n', asp = 1, ann = FALSE)
x <- c(1, 2, 2, 1.5, 1)
z <- c(1, 1, 2, 1.7, 2)
polygon(x, z, col = 'green')

produce a plot with a small green shape exactly as I expect. A nearly 
identical plot can be made in Matlab with these commands:

x = [1, 2, 2, 1.5, 1];
z = [1, 1, 2, 1.7, 2];
patch(x, z, 'g')
axis([0, 5, 0, 4])
box on

In Matlab I am able to extend this quite easily to a three-dimensional 
plot by simply adding one more vector argument to 'patch'. I don't see 
how to do this in R using 'polygon'. (I've primarily looked at 
scatterplot3d.) Is there another function I can use?

Since I expect that many of you do not use Matlab, I've put two 
graphics showing the example above (Plot 1) and a similar 
three-dimensional plot (Plot 2) on this page:
http://staff.washington.edu/freeman/patch.html.

It's Plot 2 that I'd like to be able to reproduce in R.

Thanks for any advice!

  -- Ted



From deepayan.sarkar at gmail.com  Wed Jan  4 22:39:26 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 4 Jan 2006 15:39:26 -0600
Subject: [R] multiple lowess line in one plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB19807E@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB19807E@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <eb555e660601041339m7d8b5992p1e8c10466086a093@mail.gmail.com>

On 1/4/06, Gregory Snow <Greg.Snow at intermountainmail.org> wrote:
> Here is one approach using lattice (trellis) graphics:
>
> tmp.state <- data.frame( Frost=state.x77[,'Frost'],
> 	Murder=state.x77[,'Murder'],
> 	Region=state.region)
>
> library(lattice)
> trellis.par.set(col.whitebg())
> xyplot( Murder ~ Frost, groups=Region, data=tmp.state,
> 	panel.groups=function(...){panel.loess(...);panel.xyplot(...)}
> ,span=.8)

which can be shortened to

xyplot(Murder ~ Frost, groups=Region, data=tmp.state,
       type = c('p', 'smooth'), span = .8)

-Deepayan



From h.wickham at gmail.com  Wed Jan  4 22:51:23 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 4 Jan 2006 21:51:23 +0000
Subject: [R] multiple lowess line in one plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB19807E@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB19807E@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <f8e6ff050601041351s4fd49f1fv7983dbcfbbfb4355@mail.gmail.com>

> xyplot( Murder ~ Frost, groups=Region, data=tmp.state,
>         panel.groups=function(...){panel.loess(...);panel.xyplot(...)}
> ,span=.8)

or more simply (but with less control over options)

xyplot( Murder ~ Frost, groups=Region, data=tmp.state, type=c("p","smooth"))

Hadley



From yzhang4 at une.edu.au  Wed Jan  4 23:07:09 2006
From: yzhang4 at une.edu.au (Yuandan Zhang)
Date: Thu, 05 Jan 2006 09:07:09 +1100
Subject: [R] write out data in format
Message-ID: <1136412429.3383.8.camel@zhangqiang.une.edu.au>

Hi,
how to write out data frame in format? for examples, I wrote out a data
using 'write.table(siredata, file='siredata.txt', row.names=F,
quote=F)'.

it produced data alike this:


2882 1 0 0 0 0 L1600481991910012 L1600011988880196
2883 0.79 0.21 0 0 21 L1622881993930001 L1600481991910012
2884 0.84 0.16 0 0 23 L1622881993930005 L1600481991910012
2885 0.9 0.1 0 0 23 L1622881993930027 L1600481991910012
2886 0.87 0.13 0 0 23 L1622881993930038 L1600481991910012
2887 0.9 0.1 0 0 27 L1622881993930111 L1600481991910012
2888 0.81 0.19 0 0 22 L1622881993930113 L1600481991910012

In fact i want it formated as 
2882 1.00 0.00 0.00 0.00  0 L1600481991910012 L1600011988880196
2883 0.79 0.21 0.00 0.00 21 L1622881993930001 L1600481991910012
2884 0.84 0.16 0.00 0.00 23 L1622881993930005 L1600481991910012
2885 0.90 0.10 0.00 0.00 23 L1622881993930027 L1600481991910012
2886 0.87 0.13 0.00 0.00 23 L1622881993930038 L1600481991910012
2887 0.90 0.10 0.00 0.00 27 L1622881993930111 L1600481991910012
2888 0.81 0.19 0.00 0.00 22 L1622881993930113 L1600481991910012

Thanks for your help

Yuandan



From ruser2006 at yahoo.com  Wed Jan  4 23:10:24 2006
From: ruser2006 at yahoo.com (r user)
Date: Wed, 4 Jan 2006 14:10:24 -0800 (PST)
Subject: [R] matrix math
Message-ID: <20060104221024.17795.qmail@web37009.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/0cf6fd60/attachment.pl

From ihok at hotmail.com  Wed Jan  4 23:25:04 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 04 Jan 2006 17:25:04 -0500
Subject: [R] data order affects glmmPQL
Message-ID: <BAY102-F3944070732B5C70C225C1BCA2F0@phx.gbl>

Is it to be expected that the way a data frame is sorted should affect the 
model fit by glmmPQL?

Example:

library(MASS)
library(nlme)

fit.model <- function(il, model.family) {
  cs <- Initialize(corSymm(form=~1|id), data=il)
  glmmPQL(score~test+coder, random=~1|id, # 
control=lmeControl(msMaxIter=100),
          family=model.family, data=il, correlation=cs)
}

score <- 
c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
id <- rep(1:13, rep(4, 13))
test <- gl(2, 1, length(score), labels=c("pre", "post"))
coder <- gl(2, 2, length(score), labels=c("two", "three"))

foo <- data.frame(id, score, test, coder) # Define data frame
print(summary(fit.model(foo, poisson)))

bar <- foo[order(id, score, test, coder),] # Reorder data frame
print(summary(fit.model(bar, poisson)))

The two summaries are clearly different. Is this to be expected? Is there a 
canonical way one should order a data frame before passing it to glmmPQL?



From mzarkov at EUnet.yu  Wed Jan  4 23:31:34 2006
From: mzarkov at EUnet.yu (Milos Zarkovic)
Date: Wed, 4 Jan 2006 23:31:34 +0100
Subject: [R] A comment about R:
References: <556e90a80601010636m64d61693s3244b0cf553c9553@mail.gmail.com>
Message-ID: <002001c6117e$a4c73cf0$0401a8c0@milos>

I am just beginning to use R. And I am just clinical endocrinologist, not 
statistician.

R is definitively not for casual user. Learning curve is very steep and 
previous experience in programming is essential. Therefore, some kind of 
menu system is extremely useful. I use combination of R-Commander and 
SciViews which is good, but some more functionality would be nice. On the 
other hand, function returning an object is great, as is simultaneous 
presence of multiple data sets.

Introductory documentation is excellent, both in electronic and paper form 
(books by Verzani, Dalgaard, Venables et al, Maindonald etc - not to forget 
Zoonekynd and The R Graph Gallery). However, package documentation is 
consistently cryptic (written for experts?)  - examples with explanations 
would be nice. I believe that database of packages and methods would help to 
find appropriate package.

This list is impressive. People are knowledgable, opinionated, ready to help 
and to flame you for asking elementary question or asking how to use type 
III SSQ. So, speak softly and carry a beagle. Seriously, sometimes it would 
be quicker just to give an answer, than to flame a poor soul.





Milos Zarkovic



******************************************************
Milos Zarkovic MD, Ph.D.
Associate Professor of Internal Medicine
Institute of Endocrinology
Dr Subotica 13
11000 Beograd
Serbia

Tel +381-63-202-925
Fax +381-11-685-357

Email mzarkov at eunet.yu
******************************************************



----- Original Message ----- 
From: "Kjetil Halvorsen" <kjetilbrinchmannhalvorsen at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Sunday, January 01, 2006 3:36 PM
Subject: [R] A comment about R:


> Readers of this list might be interested in the following commenta about 
> R.
>
>
> In a recent report, by Michael N. Mitchell
> http://www.ats.ucla.edu/stat/technicalreports/
> says about R:
>
>
> "Perhaps the most notable exception to this discussion is R, a language 
> for
> statistical computing and graphics.
> R is free to download under the terms of the GNU General Public License 
> (see
> http://www.r-project.
> org/). Our web site has resources on R and I have tried, sometimes in 
> great
> earnest, to learn and understand
> R. I have learned and used a number of statistical packages (well over 10)
> and a number of programming
> languages (over 5), and I regret to say that I have had enormous 
> diffculties
> learning and using R. I know
> that R has a great fan base composed of skilled and excellent 
> statisticians,
> and that includes many people
> from the UCLA statistics department. However, I feel like R is not so much
> of a statistical package as much
> as it is a statistical programming environment that has many new and 
> cutting
> edge features. For me learning
> R has been very diffcult and I have had a very hard time finding answers 
> to
> many questions about using
> it. Since the R community tends to be composed of experts deeply enmeshed 
> in
> R, I often felt that I was
> missing half of the pieces of the puzzle when reading information about 
> the
> use of R { it often feels like there
> is an assumption that readers are also experts in R. I often found the
> documentation for R quite sparse and
> many essential terms or constructs were used but not defined or
> cross-referenced. While there are mailing
> lists regarding R where people can ask questions, there is no offcial
> "technical support". Because R is free
> and is based on the contributions of the R community, it is extremely
> extensible and programmable and I
> have been told that it has many cutting edge features, some not available
> anywhere else. Although R is free,
> it may be more costly in terms of your time to learn, use, and obtain
> support for it.
> My feeling is that R is much more suited to the sort of statistician who 
> is
> oriented towards working
> very deeply with it. I think R is the kind of package that you really need
> to become immersed in (like a
> foreign language) and then need to use on a regular basis. I think that it
> is much more diffcult to use it
> casually as compared to SAS, Stata or SPSS. But by devoting time and 
> effort
> to it would give you access
> to a programming environment where you can write R programs and 
> collaborate
> with others who are also
> using R. Those who are able to access its power, even at an applied level,
> would be able to access tools that
> may not be found in other packages, but this might come with a serious
> investment of time to suffciently
> use R and maintain your skills with R."
>
>
> Kjetil
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vitorchagas at yahoo.com  Wed Jan  4 23:38:50 2006
From: vitorchagas at yahoo.com (Vitor Chagas)
Date: Wed, 4 Jan 2006 14:38:50 -0800 (PST)
Subject: [R] removal of an element from a vector
In-Reply-To: <200601040857.OAA31306@WS0005.indiatimes.com>
Message-ID: <20060104223850.72542.qmail@web30208.mail.mud.yahoo.com>

Hi GS,

You could simply do the following

y <- x[<100]
z <- x[x>=100]

That's one of the wonderful things about R.

Vitor


--- gynmeerut <gynmeerut at indiatimes.com> wrote:

> 
> Dear All,
>   I have some problem in R which I'm explaining
> using an example:
> x<-(120,235,172,95,175,200,233,142)
> i want to remove the elements which are lesser than
> 100 and as a result i want two vectors
> 
> y<-(containing elements <100)
> z<-(remaining elements)
> 
> 
> Moreover if I wish to use two different programs for
> vectors y and z.
> which command shall I use(will IF-ELSE  work ?)
> 
> 
> 
> Thanks and regards,
> 
> GS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


Vitor Chagas
Actuary
Portugal



From ruser2006 at yahoo.com  Wed Jan  4 23:39:45 2006
From: ruser2006 at yahoo.com (r user)
Date: Wed, 4 Jan 2006 14:39:45 -0800 (PST)
Subject: [R] matrix math
Message-ID: <20060104223945.30250.qmail@web37006.mail.mud.yahoo.com>

>   I am using R 2.1.1 in an windows XP environment.
>    
>   I have 2 dataframes, temp1 and temp2.
>    
>   Each dataframe has 20 variables (?cocolumns") and
> 525 observations (?rows?).  All variables are
> numeric.
>    
>   I want to create a new dataframe that also has 20
> columns and 525 rows.  The values in this dataframe
> should be the sum of the 2 other dataframe.
>    
>   (i.e. temp1$column 1+temp2$column1,
> temp1$column2+temp2$column2, etc)
>    
>   What is the best/easiest way to accomplish this?
>    
>   Is I wish to "multiply" (instead of sum) the
> columns, how do I?
>    
>   I tried:
>    
>   temp3<-as.matrix(temp1)+as.matrix(temp2)
>    
>   I get the following error message: ?Error in
> as.matrix(temp1) + as.matrix(temp2) : 
>           non-numeric argument to binary operator? 
> 
> 
> 		
> ---------------------------------

> $16.99/mo. or less



From mcclatchie.sam at saugov.sa.gov.au  Wed Jan  4 23:42:26 2006
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Thu, 5 Jan 2006 09:12:26 +1030 
Subject: [R] A comment about R:
Message-ID: <C9A3316E261F6E45B9ED5F4C3487C101073B9E45@sagemsg0010.sagemsmrd01.sa.gov.au>

>From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>Subject: Re: [R] A comment about R:
>
>One thing that is often overlooked, and hasn't yet been mentioned in
>the thread, is how much *simpler* R can be for certain completely
>basic tasks of practical or pedagogical relevance: Calculate a simple
>derived statistic, confidence intervals from estimate and SE,
>percentage points of the binomial distribution - using dbinom or from
>the formula, take the sum of each of 10 random samples from a set of
>numbers, etc. This is where other packages get stuck in the
>procedure+dataset mindset.
>

Colleagues

I really agree with Peter's comment. Matlab is much the same, in this sense.


I've had a lot of trouble getting people at my lab to take on learning R. It
is interesting to me that the one person who has taken the plunge was
educated in Singapore.  I read recently that on measures of science
performance, Singapore schools do very well.  Perhaps students now expect
science to be easier.

I have found also that it makes it much clearer for people to decide if they
need to learn R to be clear that it is like learning a language. You start
with a few packaged functions (e.g. mean), and move on from there. You have
to take a structured approach to learning it, and need to use it very
frequently. You have to learn the syntax, and just like learning Spanish,
don't  expect to read Vargas Llosa before you can say buenas tardes
correctly. Once they understand that, they can decide if they need R, before
they decide whether to invest in learning it. 

Suerte
Good luck!

Sam
----
Sam McClatchie,
Oceanography subprogram 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Cellular: 0431 304 497 
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From ihok at hotmail.com  Wed Jan  4 23:56:26 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 04 Jan 2006 17:56:26 -0500
Subject: [R] matrix math
Message-ID: <BAY102-F117C23C05FC29546CA2D0ECA2F0@phx.gbl>

>  I have 2 dataframes, temp1 and temp2.
>
>  Each dataframe has 20 variables (&#147;cocolumns") and 525 observations 
>(&#147;rows&#148;).  All variables are numeric.
>
>  I want to create a new dataframe that also has 20 columns and 525 rows.  
>The values in this dataframe should be
>the sum of the 2 other dataframe.

>foo <- data.frame(c(1,1,1,1),c(2,2,2,2))
>bar <- data.frame(c(1,2,3,4),c(5,6,7,8))
>foo+bar
  c.1..1..1..1. c.2..2..2..2.
1             2             7
2             3             8
3             4             9
4             5            10

If your data frames are actually data frames, it should work.
>class(foo)
[1] "data.frame"



From hb at maths.lth.se  Thu Jan  5 01:07:58 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 05 Jan 2006 11:07:58 +1100
Subject: [R] Using 'polygon' in a 3d plot
In-Reply-To: <73407e8e32ddfda614a299fc21c69e86@u.washington.edu>
References: <73407e8e32ddfda614a299fc21c69e86@u.washington.edu>
Message-ID: <43BC635E.1010806@maths.lth.se>

Ted Freeman wrote:
> I'm new to R, after many years of using Matlab. I've found the R 
> function 'polygon' to be nearly equivalent to the Matlab function 
> 'patch'. For example, the R commands:
> 
> plot(c(0, 5), c(0, 4), type = 'n', asp = 1, ann = FALSE)
> x <- c(1, 2, 2, 1.5, 1)
> z <- c(1, 1, 2, 1.7, 2)
> polygon(x, z, col = 'green')
> 
> produce a plot with a small green shape exactly as I expect. A nearly 
> identical plot can be made in Matlab with these commands:
> 
> x = [1, 2, 2, 1.5, 1];
> z = [1, 1, 2, 1.7, 2];
> patch(x, z, 'g')
> axis([0, 5, 0, 4])
> box on
> 
> In Matlab I am able to extend this quite easily to a three-dimensional 
> plot by simply adding one more vector argument to 'patch'. I don't see 
> how to do this in R using 'polygon'. (I've primarily looked at 
> scatterplot3d.) Is there another function I can use?
> 
> Since I expect that many of you do not use Matlab, I've put two 
> graphics showing the example above (Plot 1) and a similar 
> three-dimensional plot (Plot 2) on this page:
> http://staff.washington.edu/freeman/patch.html.

Hi, I've got a plot3d() and polygon3d() function in the R.basic package 
(http://www.braju.com/R/).  Example:

library(R.basic)  # plot3d() and polygon3d()
xb <- c(0, 4, 4, 0, 0)
yb <- c(0, 3, 3, 0, 0)
zb <- c(0, 0, 4, 4, 0)
plot3d(xb,yb,zb, type="l", theta=35, phi=30)
x <- c(0.8, 1.6, 1.6, 1.2, 0.8)
y <- c(0.6, 1.2, 1.2, 0.9, 0.6)
z <- c(1, 1, 2, 1.7, 2)
polygon3d(x,y,z, col="green")

This gives a similar plot to what Matlab creates.

/Henrik

> It's Plot 2 that I'd like to be able to reproduce in R.
> 
> Thanks for any advice!
> 
>   -- Ted
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From john.maindonald at anu.edu.au  Thu Jan  5 01:17:47 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 5 Jan 2006 11:17:47 +1100
Subject: [R] A comment about R
In-Reply-To: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
Message-ID: <6C3364DB-17AC-43EC-A8D0-6CCBE629058A@anu.edu.au>

Quoting from Thomas's message -

>  "On the question of which system really is easier to learn I can  
> only comment that this isn't the only question   where education,  
> as a field, would benefit from some good randomized controlled  
> trials."

A Randomized Controlled Trial?:
Doing such trials would be a 30-year project.   The entry criterion  
might be at least a pass score on a test that was designed to  
identify students with the potential to be reasonable statistical  
practitioners.  (To make this work, coaching at a summer camp might  
be a necessary preliminary.)  Students would be introduced to  
whatever system at various times in their educational development --  
ages 11, 14, 18 or 24.  For each age/system combination, there'd be a  
variety of dose levels(!).  Half would be introduced via the GUI and  
half via the command line.  Outcome measures would be (1) liking for  
the system; (2) quality of analysis, on several analysis tasks of a  
type that are likely to arise in several different analysis areas.   
Assessments would be made in early career and in mid-career.   
Analyses would of course be done using both SAS Proc Mixed and lmer(0  
in lme4.  There'd be bound to be enough missing data to make the  
design unbalanced, hence allowing plenty of room for argument about  
the informativeness of the missingness, and about the adequacy of the  
degrees of freedom approximation, or whether an approach that uses a  
df approximation was even worth considering.

What happens with those who decide, of their own accord or from  
necessity, to learn a system additional to the one to which they were  
assigned? (This may itself be an outcome.)  Should there be control  
for exposure to another language?

The more one thinks about it, the worse the design problem gets.  The  
situation is a bit different from the teaching of reading, where high  
quality randomized trials can and should be done, notwithstanding the  
complications of controlling for teacher effects.  As always, it is  
however insightful to think about the randomized trial that would be  
required.

I can envisage a simple randomized trial, still extending over some  
years, where the outcome measure is the quality of statistical  
analysis, on problems that meet the criteria given above.


The height of the bar:
For proper comparison of ease of doing analyses, a staged set of  
analysis problems is required, from cases where most would agree that  
a t-test or chi-square test ot CI or ... "answers" the question of  
interest, through to a variety of realistic regression problems.   
Agreement on some minimal set of steps needed to do an adequate  
analysis would be a necessary part of the process.  This insists that  
the goalposts are always at the same height. Such an exercise could  
be highly insightful, and a useful contribution to the public  
scientific good.


Research questions:
To a smaller or larger extent, R is a component of a research  
exercise in the development of statistical computational abilities.   
Perhaps to the majority of users on this list, it is primarily an  
effective tool for the handling of statistical and other scientific  
computing tasks.  Some see these two goals as somewhat distinct (at  
the boundaries, they obviously are); others see a large  overlap.

In any case, this latter role has enormous importance, actual and  
potential, for the scientific community, and indeed for any area  
(especially business) where there is a continual and insistent demand  
to make sense of data.  A variety of research questions that warrant  
attention:

(1) Who should learn R?

[In my view R is such a versatile tool for scientific computing that  
anyone contemplating a career in science, and who expects to to their  
own computations that have a substantial data analysis component,  
should learn R.  The only serious competitors, in my view and  
depending on the area of application, are Genstat, Stata, and Matlab  
-- Genstat for the analysis of designed experiments and for the  
quality of its GUI, Stata for the reasons given by others, and Matlab  
for signal processsing.  SAS may be important for its efficiency in  
certain types of batch processing with large data sets, and because  
of the extent of existing large SAS repositories,  SPSS may be  
important because of the extent of existing large SPSS data  
repositories. Some comment is also needed on S-PLUS?  I am of course  
ignoring the skill investment that many researchers have made in  
these other packages.  While this has somehow to be factored in, it  
surely has limited relevance to assessing priorities for those who  
are currently starting out.]

(2) R has clearly reduced the time lag between the development of new  
theory, and availability of the associated methodology to statistical  
practitioners.  It has also, incidentally, raised the bar for  
commercial statistical software systems.  What are the implications  
for statistical research, and for professional practice and training?

(3) Should learners use a GUI, or the command line, in getting started?

[A major issue for GUIs is documentation of steps in an analysis.   
This will become increasingly important as more journals demand, as I  
hope will happen, publication of Sweave or other reproducible  
versions of analyses.  Some ultimate familiarity with the command  
line may in the medium term be essential.]

(4) When should students start learning R?

[Students should get their first exposure to a high-level programming  
language, in the style of R then Python or Octave, at age 11-14.   
There are now good alternatives to the former use of Fortran or  
Pascal, languages which have for good reason dropped out of favour  
for learning experience. They should start on R while their minds are  
still malleable, and long before they need it for serious research use.]

(5) What are the traps, in using R, for relative novices?

[Mechanisms are needed for identifying traps that routinely catch  
novices (even novices who may be quite sophisticated statistically),  
with a program to tackle these, in the medium to long term.]

(6) Default output requires (continuing) careful scrutiny from a  
"what will encourage good statistical practice" perspective.

(7) What, more widely, should go on the wish list?

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



On 4 Jan 2006, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Thomas Lumley <tlumley at u.washington.edu>
> Date: 4 January 2006 6:23:18 AM
> To: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Cc: R-help at stat.math.ethz.ch, Patrick Burns <pburns at pburns.seanet.com>
> Subject: Re: [R] A comment about R:
>
>
> On Tue, 3 Jan 2006, Peter Dalgaard wrote:
>> One thing that is often overlooked, and hasn't yet been mentioned in
>> the thread, is how much *simpler* R can be for certain completely
>> basic tasks of practical or pedagogical relevance: Calculate a simple
>> derived statistic, confidence intervals from estimate and SE,
>> percentage points of the binomial distribution - using dbinom or from
>> the formula, take the sum of each of 10 random samples from a set of
>> numbers, etc. This is where other packages get stuck in the
>> procedure+dataset mindset.
>
> Some of these things are actually fairly straightforward in Stata.  
> For example, Stata will give confidence intervals and tests for  
> linear combinations of coefficients and even (using symbolic  
> differentiation and the delta method) for nonlinear combinations.   
> The first is available in packages in R, the second is in "S  
> Programming" but doesn't seem to be packaged.
>
> <snip>
>
> Now, I still prefer R both for data analysis and (even more so) for  
> programming. There are some things that it is genuinely difficult  
> to program in Stata -- and as evidence that this isn't just my  
> ignorance of the best approaches, the language was substantially  
> reworked in both versions 8 and 9 to allow the vendor to implement  
> better graphics and
> linear mixed models.
>
> On the question of which system really is easier to learn I can  
> only comment that this isn't the only question where education, as  
> a field, would benefit from some good randomized controlled trials.
>
> 	-thomas
>
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle



From john.maindonald at anu.edu.au  Thu Jan  5 01:56:43 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 5 Jan 2006 11:56:43 +1100
Subject: [R] Splitting the list
In-Reply-To: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
Message-ID: <6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/63cdbe5e/attachment.pl

From ggrothendieck at gmail.com  Thu Jan  5 02:14:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 20:14:28 -0500
Subject: [R] produce hours greater than 23
In-Reply-To: <ISKS86$F2D4D1EA7C93CAFC6BD4EA3F041D5488@oreka.com>
References: <ISKS86$F2D4D1EA7C93CAFC6BD4EA3F041D5488@oreka.com>
Message-ID: <971536df0601041714m16819c44l7956228c7172723c@mail.gmail.com>

The chron package has a "times" class that represents times in
days and fractions of a day.  See the help desk article in R News 4/1
and the references therein for more info.   Using "times" class
the following one-line function, hms, will display produce the
hours:mm:ss format:

library(chron)
hms <- function(x)
   sub("..:", ":", sprintf("%02.f%s", floor(24*x), format(x%%1)))

# test this by creating a vector 3 "times" objects

x <- structure(pi*seq(3), class = "times")
hms(x) # "75:23:54"  "150:47:47" "226:11:41"


On 1/4/06, herodote at oreka.com <herodote at oreka.com> wrote:
> Hy all,
>
> I wish to use the date function to draw againt the lifetime of a motor.
>
> This lifetime is given to me in Hours (it can go over 5000 hours)
>
> I'm unable to find how to convert this lifetime value to something like %H:%M:%S because when R see 24H it says 1 day, i don't want that, i just want %H:%M:%S with a value of %H higher than 24...
> for example:
> i've got this value in hours: 345.05 H
>
> I wish that R gives me : "345:3:0" or "345:03:00"
>
> What R function could do it?
>
> I've search for as.Date strptime... but none of these seems to be able to put a value for %H greater than 23.
>
> i've browse the help and docs and found nothing (am i blind?).
>
> thks all
> guillaume.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jan  5 02:24:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jan 2006 20:24:10 -0500
Subject: [R] produce hours greater than 23
In-Reply-To: <971536df0601041714m16819c44l7956228c7172723c@mail.gmail.com>
References: <ISKS86$F2D4D1EA7C93CAFC6BD4EA3F041D5488@oreka.com>
	<971536df0601041714m16819c44l7956228c7172723c@mail.gmail.com>
Message-ID: <971536df0601041724j5f0ad158keea1a2555749f3c7@mail.gmail.com>

In thinking about this some more here is an even shorter version of
hms.  Note that %02.f produces at least 2 digits using leading
zeros to fill in if necessary and %5.5s truncates the string to the
first 5 characters (since our string is of the form mm:ss:hh and we only
need the mm:ss part).

hms2 <- function(x) sprintf("%02.f:%5.5s", floor(24*x), format(x%%1, "m:s:h"))


On 1/4/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The chron package has a "times" class that represents times in
> days and fractions of a day.  See the help desk article in R News 4/1
> and the references therein for more info.   Using "times" class
> the following one-line function, hms, will display produce the
> hours:mm:ss format:
>
> library(chron)
> hms <- function(x)
>   sub("..:", ":", sprintf("%02.f%s", floor(24*x), format(x%%1)))
>
> # test this by creating a vector 3 "times" objects
>
> x <- structure(pi*seq(3), class = "times")
> hms(x) # "75:23:54"  "150:47:47" "226:11:41"
>
>
> On 1/4/06, herodote at oreka.com <herodote at oreka.com> wrote:
> > Hy all,
> >
> > I wish to use the date function to draw againt the lifetime of a motor.
> >
> > This lifetime is given to me in Hours (it can go over 5000 hours)
> >
> > I'm unable to find how to convert this lifetime value to something like %H:%M:%S because when R see 24H it says 1 day, i don't want that, i just want %H:%M:%S with a value of %H higher than 24...
> > for example:
> > i've got this value in hours: 345.05 H
> >
> > I wish that R gives me : "345:3:0" or "345:03:00"
> >
> > What R function could do it?
> >
> > I've search for as.Date strptime... but none of these seems to be able to put a value for %H greater than 23.
> >
> > i've browse the help and docs and found nothing (am i blind?).
> >
> > thks all
> > guillaume.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From atoledot at gmail.com  Thu Jan  5 03:13:43 2006
From: atoledot at gmail.com (angel toledo)
Date: Wed, 4 Jan 2006 20:13:43 -0600
Subject: [R] information
Message-ID: <8d9cc5050601041813j4158a85at@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060104/c92cef72/attachment.pl

From murdoch at stats.uwo.ca  Thu Jan  5 03:56:17 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jan 2006 21:56:17 -0500
Subject: [R] Using 'polygon' in a 3d plot
In-Reply-To: <73407e8e32ddfda614a299fc21c69e86@u.washington.edu>
References: <73407e8e32ddfda614a299fc21c69e86@u.washington.edu>
Message-ID: <43BC8AD1.6050503@stats.uwo.ca>

On 1/4/2006 4:20 PM, Ted Freeman wrote:
> I'm new to R, after many years of using Matlab. I've found the R 
> function 'polygon' to be nearly equivalent to the Matlab function 
> 'patch'. For example, the R commands:
> 
> plot(c(0, 5), c(0, 4), type = 'n', asp = 1, ann = FALSE)
> x <- c(1, 2, 2, 1.5, 1)
> z <- c(1, 1, 2, 1.7, 2)
> polygon(x, z, col = 'green')
> 
> produce a plot with a small green shape exactly as I expect. A nearly 
> identical plot can be made in Matlab with these commands:
> 
> x = [1, 2, 2, 1.5, 1];
> z = [1, 1, 2, 1.7, 2];
> patch(x, z, 'g')
> axis([0, 5, 0, 4])
> box on
> 
> In Matlab I am able to extend this quite easily to a three-dimensional 
> plot by simply adding one more vector argument to 'patch'. I don't see 
> how to do this in R using 'polygon'. (I've primarily looked at 
> scatterplot3d.) Is there another function I can use?
> 
> Since I expect that many of you do not use Matlab, I've put two 
> graphics showing the example above (Plot 1) and a similar 
> three-dimensional plot (Plot 2) on this page:
> http://staff.washington.edu/freeman/patch.html.
> 
> It's Plot 2 that I'd like to be able to reproduce in R.

scatterplot3d returns xyz.convert, a function that can project 3d 
coordinates down to 2d for doing this sort of thing:

xb <- c(0, 4, 4, 0, 0)
yb <- c(0, 3, 3, 0, 0)
zb <- c(0, 0, 4, 4, 0)
s <- scatterplot3d(xb,yb,zb, type='l')

x = c(0.8, 1.6, 1.6, 1.2, 0.8)
y = c(0.6, 1.2, 1.2, 0.9, 0.6)
z = c(1, 1, 2, 1.7, 2)
polygon(s$xyz.convert(x,y,z),col="green")

You have to be careful about hidden lines; things drawn later obscure 
things drawn earlier, even if they should really be behind.  But in this 
case that doesn't matter, because the polygon doesn't need to be behind 
anything.

Another possibility is to use the rgl package, but it doesn't have the 
high level plot functions, so the axes are harder to draw.

Duncan Murdoch



From rbaer at atsu.edu  Thu Jan  5 04:59:20 2006
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 4 Jan 2006 21:59:20 -0600
Subject: [R] How to produce this graphic
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC17D@VAIEXCH04.vai.org>
Message-ID: <00f401c611ac$68bfdf90$6401a8c0@ALKAID>

----- Original Message ----- 
From: "Kort, Eric"  
> Lisa Wang asks...
>> Subject: [R] How to produce this graphic
>> Hello there,
>> I would like to produce a plot of x<-c(4,5,6),which is the mean of
> each
>> group and y<-c('groupA','groupB','groupC').
>> plot (x,y) can not produce any graphics because y is not numerical.
>> What should I do to produce this graphic?
> 
> One possibility:
> x <- c(4,5,6)
> plot(x, axes=F)
> axis(1, c(1:3), labels=y)
> 
> or see ?bar.plot

I think Eric meant: ?barplot

For example:
barplot(x, names.arg=y)



From madhurima_b at persistent.co.in  Thu Jan  5 07:03:49 2006
From: madhurima_b at persistent.co.in (madhurima bhattacharjee)
Date: Thu, 05 Jan 2006 11:33:49 +0530
Subject: [R] problem with command line arguments
Message-ID: <43BCB6C5.7050009@persistent.co.in>

Hello Everybody,

I am running a R script through a perl code from command line.
The perl script is like:

my $cmd= 'R CMD BATCH D:/try5.R';
system($cmd);

I run the perl code from command line.
Now I want to pass some command line arguments to the R script.
Its like the argv concept of perl.

Do I pass the arguments through my $cmd in the perl script?
If yes, then how to access that in the R script?
Any help will really be appreciated.

Thanks and Regards,
Madhurima.



From ripley at stats.ox.ac.uk  Thu Jan  5 07:38:49 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jan 2006 06:38:49 +0000 (GMT)
Subject: [R] write out data in format
In-Reply-To: <1136412429.3383.8.camel@zhangqiang.une.edu.au>
References: <1136412429.3383.8.camel@zhangqiang.une.edu.au>
Message-ID: <Pine.LNX.4.61.0601050635061.28685@gannet.stats>

The word 'format' is the clue: please consult the help page for the R 
function format().

write.table is designed to write a table to be processed by a program, not 
for human consumption.  The print() method for data frames calls format().

On Thu, 5 Jan 2006, Yuandan Zhang wrote:

> Hi,
> how to write out data frame in format? for examples, I wrote out a data
> using 'write.table(siredata, file='siredata.txt', row.names=F,
> quote=F)'.
>
> it produced data alike this:
>
>
> 2882 1 0 0 0 0 L1600481991910012 L1600011988880196
> 2883 0.79 0.21 0 0 21 L1622881993930001 L1600481991910012
> 2884 0.84 0.16 0 0 23 L1622881993930005 L1600481991910012
> 2885 0.9 0.1 0 0 23 L1622881993930027 L1600481991910012
> 2886 0.87 0.13 0 0 23 L1622881993930038 L1600481991910012
> 2887 0.9 0.1 0 0 27 L1622881993930111 L1600481991910012
> 2888 0.81 0.19 0 0 22 L1622881993930113 L1600481991910012
>
> In fact i want it formated as
> 2882 1.00 0.00 0.00 0.00  0 L1600481991910012 L1600011988880196
> 2883 0.79 0.21 0.00 0.00 21 L1622881993930001 L1600481991910012
> 2884 0.84 0.16 0.00 0.00 23 L1622881993930005 L1600481991910012
> 2885 0.90 0.10 0.00 0.00 23 L1622881993930027 L1600481991910012
> 2886 0.87 0.13 0.00 0.00 23 L1622881993930038 L1600481991910012
> 2887 0.90 0.10 0.00 0.00 27 L1622881993930111 L1600481991910012
> 2888 0.81 0.19 0.00 0.00 22 L1622881993930113 L1600481991910012
>
> Thanks for your help
>
> Yuandan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Jan  5 08:11:48 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 05 Jan 2006 08:11:48 +0100
Subject: [R] R newbie configuration
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A764662FA@exchange.psg.com>
Message-ID: <43BCD4C4.13249.401083@localhost>

Hi

here is my example of .Rprofile file


require(graphics)
require(utils)

# setHook(packageEvent("graphics", "onLoad"), function(...) 
# graphics::par(bg="white"))  ## did not manage to persuade setHook 
# to work properly

par(bg="white")
RNGkind("Mersenne-Twister", "Inversion")

# some set of my functions and data

.libPaths("D:/programy/R/R-2.2.0/library/fun")
library(fun)
data(stand)


HTH
Petr


On 4 Jan 2006 at 15:46, Mark Leeds wrote:

Date sent:      	Wed, 4 Jan 2006 15:46:37 -0500
From:           	"Mark Leeds" <Mleeds at kellogggroup.com>
To:             	"R-Stat Help" <R-help at stat.math.ethz.ch>
Subject:        	[R] R newbie configuration

> I think I did enough reading on my
> Own about startup ( part of the morning
> And most of this afternoon )
> to not feel uncomfortable asking
> for confirmation of my understanding of this startup stuff.
> 
> Obviously, the startup process is more complicated
> Than below but, for my R newbie purposes,
> It seems like I can think of the startup process as follows :
> 
> Suppose my  home directory = "c:documents and settings/mleeds" =
> $HOME.
> 
> Put things in $HOME/.Rprofile that are more generic on startup and not
> specific to any particular R project.
> 
> Put various .First() functions in the working directories of the
> particular projects that
> they are associated with so that they loaded in when their .RData
> directory gets loaded. 
> 
> If above is correct  ( emphasis on correct for a newbie. I know there
> is a lot more going on And things can be done more elegantly etc ),
> Could someone send me an example of a .Rprofile file. I didn't use
> these in S+ and I am wondering what you put in them ? 
> 
>                                    Thanks
> 
> 
> 
> 
> **********************************************************************
> This email and any files transmitted with it are
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Thu Jan  5 08:16:52 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 05 Jan 2006 08:16:52 +0100
Subject: [R] matrix math
In-Reply-To: <20060104221024.17795.qmail@web37009.mail.mud.yahoo.com>
Message-ID: <43BCD5F4.16712.44B606@localhost>



On 4 Jan 2006 at 14:10, r user wrote:

Date sent:      	Wed, 4 Jan 2006 14:10:24 -0800 (PST)
From:           	r user <ruser2006 at yahoo.com>
To:             	rhelp <r-help at stat.math.ethz.ch>
Subject:        	[R] matrix math

>   I am using R 2.1.1 in an windows XP environment.
> 
>   I have 2 dataframes, temp1 and temp2.
> 
>   Each dataframe has 20 variables (?cocolumns") and 525 observations
>   (?rows?).  All variables are numeric.
> 
>   I want to create a new dataframe that also has 20 columns and 525
>   rows.  The values in this dataframe should be the sum of the 2 other
>   dataframe.
> 
>   (i.e. temp1$column 1+temp2$column1, temp1$column2+temp2$column2,
>   etc)
> 
>   What is the best/easiest way to accomplish this?
> 
>   Is I wish to "multiply" (instead of sum) the columns, how do I?
> 
>   I tried:
> 
>   temp3<-as.matrix(temp1)+as.matrix(temp2)
> 
>   I get the following error message: ?Error in as.matrix(temp1) +
>   as.matrix(temp2) : 
>           non-numeric argument to binary operator? 

Hi

although you think all variables are numeric they probably are not.

what does

str(temp1) or str(temp2)

tells you

HTH
Petr





> 
> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From rbaer at atsu.edu  Thu Jan  5 08:20:41 2006
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Thu, 5 Jan 2006 01:20:41 -0600
Subject: [R] A comment about R:
References: <Pine.LNX.4.44.0601041842570.9402-100000@reclus.nhh.no>
Message-ID: <01b801c611c8$89989f10$6401a8c0@ALKAID>

>> On Wed, 4 Jan 2006, Roger Bivand wrote:
>> > Could I ask for comments on:
>> >
>> > source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
>> >
>> > as a reproduction of the Stata capabilities session? Both the t test 
>> > and
>> > the chi-square from our side point up oddities. I didn't succeed on
>> > putting fit lines on a grouped xyplot, so backed out to base graphics.
>> > This could be Swoven, possibly using the RweaveHTML driver.
>> >
>>
Excellent!  Although I will point out that the Stata summarize command is a 
little different than the R summary command.  The summarize command is a 
little more like:

 summarize <- function(x){
   obs=length(x)
   mn=mean(x)
   sd=sd(x)
   min=min(x)
   max=max(x)
  cat('obs \t Average \t Std. Dev. \t Min \t Max \n', 
obs,'\t',mn,'\t',sd,'\t',min,'\t',max,'\n')
 }

As a user of statistics rather than a statistician, I have to agree with the 
original author whose premise was that different statistical packages have 
different strengths.  I think the main basis for his comments on R were, 
reading between the lines, that he knew it mostly from talking to friends. 
Any statistical tool for those of us in the back rows is as easy as our 
mentor make it.  At my institution there is a paucity of good mentors, and I 
have found the learning curve equally steep for Stata 7 for which I have 
many, many volumes of documentation and R for which I have greatly benefited 
from several of the terrific contributed documentation and books already 
mentioned.

The original article was about SAS, Stata, and SPSS strengths for carrying 
out 'tradtional statistics'.  What are R's strengths?  Too numerous to 
mention in the hands of the right users.  However, I would point to things 
like the tools at the Bioconductor site  as a broad illustration of the 
nearly infinite flexibility and extensibility of R for specialized 
statistical tasks.  Does this mean that R is a poor tool to choose for the 
basic and traditional procedures?  Hardly!  (Well written documentation like 
John Fox's cars, Peter Dalgard's ISwR, and John Verzani's Simple R 
contributed documentation put introductory R statistical procedures within 
easy grasp of users.  I have found that non-statistics students rapidly 
catch on with 'problem-specific'  guidance once they overcome the lack of 
GUI.  (R-commander is certainly a solution there).  As the number of R 
mentors grows to rival SAS, Stata, and SPSS, the everyday tasks might even 
appear easier to new initiates than the corresponding syntax and thought 
processes in the other programs.

So, what are R's major weaknesses?  I do not think they are statistical. 
Rather, it is having 'mentors' who have gone before to do the type of 
analysis that you (the end user) wish to do, and who have graciously left 
behind a paper trail of how to syntactically address a specific statistical 
task.  There is a huge amount out there, but it is hard to find at the 
beginning.  [BTW: This listserve is of course a tremendous resource, and why 
should we not read the posting guide out of simple respect for those who 
have given us such a great resource.  I don't like getting flammed either, 
but darn it, sometimes I deserve it ;-).]

Finally, this thread has made me think back 3-4 years to when I  first 
discovered R.  The think that frustrated me the most in the early weeks was 
getting data into R.  It took me no time to learn to generate data with all 
kinds of distrbutions, no time to discover 'build in' datasets from the 
data() function, or to enter data a number at a time with the c() funtion. 
BUT HOW was I to get the datasets (spreadsheet, database) from my laboratory 
into R?  This somehow has been much easier to figure out in the other (often 
GUI) statistical environments I have used.  [Of course, I finally discovered 
the documentation for the foreign package and later learned about RODBC, and 
I was blown away by the flexibility available.

Well just the thoughts of one end user type...

Rob



From btguan at ntu.edu.tw  Thu Jan  5 09:05:46 2006
From: btguan at ntu.edu.tw (Bing T. Guan)
Date: Thu, 5 Jan 2006 16:05:46 +0800
Subject: [R] Problem with nlme version 3.1-68
Message-ID: <000001c611ce$d5b4aa00$8952708c@fmgbtgaunp5>

Dear All:
I updated my R program as well as associated packages yesterday. Currently
my R version is 2.2.1 running under WINXP SP-2. 
When I tried to list (summary) an nlme object that I developed before, I got
the following error message:

[ Error in .C("ARMA_constCoef", as.integer(attr(object, "p")),
as.integer(attr(object,  : 
        C entry point "ARMA_constCoef" not in DLL for package "nlme" ]

The nlme object was fitted with corr = corARMA(q=2) option. I refitted the
model, and the same error message appeared. I then refitted the model with
option corr = corARMA(p=1), then no problem; but for p = 2, or q = 1 or 2,
then the error occurred. When I listed the same fitted nlme objects under R
2.1.1 with nlme 3.1-65, then no problem.

I fitted the Ovary data (Pinheiro and Bates 2000, p.397) using the script
provided in nlme package 
fm3Ovar.nlme <- update(fm1Ovar.nlme, correlation = corARMA(p=0, q=2)), and
tried to list the result. The same error occurred. I tried it out on several
of PCs (WINXP SP-2, R 2.2.1, nlme 3.1-68) and the same situation happened on
every machine.

Is there a bug in the latest version of nlme (3.1-68), or the problem only
happened to me and my machines?
***************************************
Biing T. Guan
School of Forestry and Resource Conservation
National Taiwan University
btguan at ntu.edu.tw



From buser at stat.math.ethz.ch  Thu Jan  5 09:45:28 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 5 Jan 2006 09:45:28 +0100
Subject: [R] Difficulty with 'merge'
In-Reply-To: <202EEF7E-3225-4495-878E-3EB949D01F8A@virginia.edu>
References: <202EEF7E-3225-4495-878E-3EB949D01F8A@virginia.edu>
Message-ID: <17340.56488.174107.488322@stat.math.ethz.ch>

Dear Michael

Please remark that merge calculates all possible combinations if
you have repeated elements as you can see in the example below. 

?merge

"... If there is more than one match, all possible matches
contribute one row each. ..."

Maybe you can apply "aggregate" in a reasonable way on your 
data.frame first to summarize your repeated values to unique
ones and the proceed with merge, but that depends on your
problem. 

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

example with repeated values
----------------------------

v1 <- c("a", "b", "a", "b", "a")
n1 <- 1:5
v2 <- c("b", "b", "a", "a", "a")
n2 <- 6:10
(f1  <- data.frame(v1, n1))
(f2 <- data.frame(v2, n2))
(m12 <- merge(f1, f2, by.x = "v1", by.y = "v2", sort = F))





Michael Kubovy writes:
 > Dear R-helpers,
 > 
 > Happy New Year to all the helpful members of the list.
 > 
 > Here is the behavior I'm looking for:
 >  > v1 <- c("a","b","c")
 >  > n1 <- c(0, 1, 2)
 >  > v2 <- c("c", "a", "b")
 >  > n2 <- c(0, 1 , 2)
 >  > (f1  <- data.frame(v1, n1))
 >    v1 n1
 > 1  a  0
 > 2  b  1
 > 3  c  2
 >  > (f2 <- data.frame(v2, n2))
 >    v2 n2
 > 1  c  0
 > 2  a  1
 > 3  b  2
 >  > (m12 <- merge(f1, f2, by.x = "v1", by.y = "v2", sort = F))
 >    v1 n1 n2
 > 1  c  2  0
 > 2  a  0  1
 > 3  b  1  2
 > 
 > Now to my data:
 >  > summary(pL)
 >          pairL
 > a fondo   :  41
 > alto      :  41
 > ampio     :  41
 > angoloso  :  41
 > aperto    :  41
 > appoggiato:  41
 > (Other)   :1271
 > 
 >  > pL$pairL[c(1,42)]
 > [1] appoggiato dentro
 > 37 Levels: a fondo alto ampio angoloso aperto appoggiato asimmetrico  
 > complicato convesso davanti dentro destra ... verticale
 > 
 >  > summary(oppN)
 >          pairL              pairR         subject            
 > L                LL                RR               M
 > a fondo   :  41   a galla    :  41   S1     :  37   Min.   :0.3646    
 > Min.   :0.02083   Min.   :0.0010   Min.   :0.0000
 > alto      :  41   acuto      :  41   S10    :  37   1st Qu.:0.5521    
 > 1st Qu.:0.37500   1st Qu.:0.1771   1st Qu.:0.1042
 > ampio     :  41   arrotondato:  41   S11    :  37   Median :0.6354    
 > Median :0.47917   Median :0.2708   Median :0.2292
 > angoloso  :  41   basso      :  41   S12    :  37   Mean   :0.6403    
 > Mean   :0.46452   Mean   :0.2760   Mean   :0.2598
 > aperto    :  41   chiuso     :  41   S13    :  37   3rd Qu.:0.7188    
 > 3rd Qu.:0.55208   3rd Qu.:0.3750   3rd Qu.:0.3854
 > appoggiato:  41   compl      :  41   S14    :  37   Max.   :0.9375    
 > Max.   :0.92708   Max.   :0.6042   Max.   :0.7812
 > (Other)   :1271   (Other)    :1271   (Other): 
 > 1295                                      NA's   :3.0000   NA's   : 
 > 3.0000
 >        asym             polar            polar_a1          clust
 > Min.   :-0.5555   Min.   :-1.2410   Min.   :-2.949e+00   c1:492
 > 1st Qu.: 0.2091   1st Qu.: 0.4571   1st Qu.:-1.902e-01   c2:287
 > Median : 0.5555   Median : 1.1832   Median :-1.110e-16   c3: 82
 > Mean   : 0.6265   Mean   : 1.3428   Mean   :-5.745e-02   c4:246
 > 3rd Qu.: 0.9383   3rd Qu.: 2.0712   3rd Qu.: 1.168e-01   c5: 82
 > Max.   : 2.7081   Max.   : 4.6151   Max.   : 4.218e+00   c6:328
 >                     NA's   : 3.0000   NA's   : 3.000e+00
 > 
 >  > oppN$pairL[c(1,42)]
 > [1] spesso fine
 > 37 Levels: a fondo alto ampio angoloso aperto appoggiato asimmetrico  
 > complicato convesso davanti dentro destra ... verticale
 > 
 >  > unique(sort(oppM$pairL)) == unique(sort(pL$pairL))
 > [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE  
 > TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
 > [26] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
 > 
 > In other words I think that pL$pairL and oppN$pairL consists of 37  
 > blocks of 41 repetitions of names, and that these blocks are  
 > permutations of each other,
 > 
 > However:
 > 
 >  > summary(m1 <- merge(oppM, pairL, by.x = "pairL", by.y = "pairL",  
 > sort = F))
 >          pairL               pairR          subject             
 > L                LL                RR               M
 > a fondo   : 1681   a galla    : 1681   S1     : 1517   Min.   : 
 > 0.3646   Min.   :0.02083   Min.   :0.0010   Min.   :0.0000
 > alto      : 1681   acuto      : 1681   S10    : 1517   1st Qu.: 
 > 0.5521   1st Qu.:0.37500   1st Qu.:0.1771   1st Qu.:0.1042
 > ampio     : 1681   arrotondato: 1681   S11    : 1517   Median : 
 > 0.6354   Median :0.47917   Median :0.2708   Median :0.2292
 > angoloso  : 1681   basso      : 1681   S12    : 1517   Mean   : 
 > 0.6398   Mean   :0.46402   Mean   :0.2760   Mean   :0.2598
 > aperto    : 1681   chiuso     : 1681   S13    : 1517   3rd Qu.: 
 > 0.7188   3rd Qu.:0.55208   3rd Qu.:0.3750   3rd Qu.:0.3854
 > appoggiato: 1681   compl      : 1681   S14    : 1517   Max.   : 
 > 0.9375   Max.   :0.92708   Max.   :0.6042   Max.   :0.7812
 > (Other)   :51988   (Other)    :51988   (Other):52972
 >        asym             polar            polar_a1          clust
 > Min.   :-0.5555   Min.   :-1.2410   Min.   :-2.949e+00   c1:20172
 > 1st Qu.: 0.2091   1st Qu.: 0.4571   1st Qu.:-1.904e-01   c2:11644
 > Median : 0.5555   Median : 1.1832   Median :-1.110e-16   c3: 3362
 > Mean   : 0.6234   Mean   : 1.3428   Mean   :-5.745e-02   c4:10086
 > 3rd Qu.: 0.9383   3rd Qu.: 2.0712   3rd Qu.: 1.169e-01   c5: 3362
 > Max.   : 2.7081   Max.   : 4.6151   Max.   : 4.218e+00   c6:13448
 > 
 > I was expecting pairL to be 41 items longs, not 1681 = 41^2.
 > _____________________________
 > Professor Michael Kubovy
 > University of Virginia
 > Department of Psychology
 > USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
 > Parcels:    Room 102        Gilmer Hall
 >          McCormick Road    Charlottesville, VA 22903
 > Office:    B011    +1-434-982-4729
 > Lab:        B019    +1-434-982-4751
 > Fax:        +1-434-982-4766
 > WWW:    http://www.people.virginia.edu/~mk9y/
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jan  5 10:03:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jan 2006 09:03:53 +0000 (GMT)
Subject: [R] .Rprofile files (was R newbie configuration)
In-Reply-To: <43BCD4C4.13249.401083@localhost>
References: <43BCD4C4.13249.401083@localhost>
Message-ID: <Pine.LNX.4.61.0601050901450.20686@gannet.stats>

And here is one with a working setHook call.

options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),
         function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
options(repos=c(CRAN="http://cran.uk.r-project.org"))


On Thu, 5 Jan 2006, Petr Pikal wrote:

> Hi
>
> here is my example of .Rprofile file
>
>
> require(graphics)
> require(utils)
>
> # setHook(packageEvent("graphics", "onLoad"), function(...)
> # graphics::par(bg="white"))  ## did not manage to persuade setHook
> # to work properly
>
> par(bg="white")
> RNGkind("Mersenne-Twister", "Inversion")
>
> # some set of my functions and data
>
> .libPaths("D:/programy/R/R-2.2.0/library/fun")
> library(fun)
> data(stand)
>
>
> HTH
> Petr
>
>
> On 4 Jan 2006 at 15:46, Mark Leeds wrote:
>
> Date sent:      	Wed, 4 Jan 2006 15:46:37 -0500
> From:           	"Mark Leeds" <Mleeds at kellogggroup.com>
> To:             	"R-Stat Help" <R-help at stat.math.ethz.ch>
> Subject:        	[R] R newbie configuration
>
>> I think I did enough reading on my
>> Own about startup ( part of the morning
>> And most of this afternoon )
>> to not feel uncomfortable asking
>> for confirmation of my understanding of this startup stuff.
>>
>> Obviously, the startup process is more complicated
>> Than below but, for my R newbie purposes,
>> It seems like I can think of the startup process as follows :
>>
>> Suppose my  home directory = "c:documents and settings/mleeds" =
>> $HOME.
>>
>> Put things in $HOME/.Rprofile that are more generic on startup and not
>> specific to any particular R project.
>>
>> Put various .First() functions in the working directories of the
>> particular projects that
>> they are associated with so that they loaded in when their .RData
>> directory gets loaded.
>>
>> If above is correct  ( emphasis on correct for a newbie. I know there
>> is a lot more going on And things can be done more elegantly etc ),
>> Could someone send me an example of a .Rprofile file. I didn't use
>> these in S+ and I am wondering what you put in them ?
>>
>>                                    Thanks
>>
>>
>>
>>
>> **********************************************************************
>> This email and any files transmitted with it are
>> confidentia...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jan  5 10:09:55 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jan 2006 09:09:55 +0000 (GMT)
Subject: [R] Problem with nlme version 3.1-68
In-Reply-To: <000001c611ce$d5b4aa00$8952708c@fmgbtgaunp5>
References: <000001c611ce$d5b4aa00$8952708c@fmgbtgaunp5>
Message-ID: <Pine.LNX.4.61.0601050847120.20686@gannet.stats>

It's a bug.  So nothing in the test suites uses this (nor any example in 
any package on CRAN, which were tested prior to release).

Note that 3.1-68 is not the version of nlme which ships with R 2.2.1 
(deliberately not introducing a new feature until after release).
Look for 3.1-68.1 in due course.


On Thu, 5 Jan 2006, Bing T. Guan wrote:

> Dear All:
> I updated my R program as well as associated packages yesterday. Currently
> my R version is 2.2.1 running under WINXP SP-2.
> When I tried to list (summary) an nlme object that I developed before, I got
> the following error message:
>
> [ Error in .C("ARMA_constCoef", as.integer(attr(object, "p")),
> as.integer(attr(object,  :
>        C entry point "ARMA_constCoef" not in DLL for package "nlme" ]
>
> The nlme object was fitted with corr = corARMA(q=2) option. I refitted the
> model, and the same error message appeared. I then refitted the model with
> option corr = corARMA(p=1), then no problem; but for p = 2, or q = 1 or 2,
> then the error occurred. When I listed the same fitted nlme objects under R
> 2.1.1 with nlme 3.1-65, then no problem.
>
> I fitted the Ovary data (Pinheiro and Bates 2000, p.397) using the script
> provided in nlme package
> fm3Ovar.nlme <- update(fm1Ovar.nlme, correlation = corARMA(p=0, q=2)), and
> tried to list the result. The same error occurred. I tried it out on several
> of PCs (WINXP SP-2, R 2.2.1, nlme 3.1-68) and the same situation happened on
> every machine.
>
> Is there a bug in the latest version of nlme (3.1-68), or the problem only
> happened to me and my machines?
> ***************************************
> Biing T. Guan
> School of Forestry and Resource Conservation
> National Taiwan University
> btguan at ntu.edu.tw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Jan  5 10:16:33 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jan 2006 10:16:33 +0100
Subject: [R] problem with command line arguments
In-Reply-To: <43BCB6C5.7050009@persistent.co.in>
References: <43BCB6C5.7050009@persistent.co.in>
Message-ID: <43BCE3F1.4090203@statistik.uni-dortmund.de>

madhurima bhattacharjee wrote:

> Hello Everybody,
> 
> I am running a R script through a perl code from command line.
> The perl script is like:
> 
> my $cmd= 'R CMD BATCH D:/try5.R';
> system($cmd);
> 
> I run the perl code from command line.
> Now I want to pass some command line arguments to the R script.
> Its like the argv concept of perl.
> 
> Do I pass the arguments through my $cmd in the perl script?
> If yes, then how to access that in the R script?
> Any help will really be appreciated.


See ?commandArgs

Uwe Ligges


> Thanks and Regards,
> Madhurima.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jan  5 10:19:13 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jan 2006 10:19:13 +0100
Subject: [R] comparision and removal
In-Reply-To: <200601041959.BAA18023@WS0005.indiatimes.com>
References: <200601041959.BAA18023@WS0005.indiatimes.com>
Message-ID: <43BCE491.5010009@statistik.uni-dortmund.de>

gynmeerut wrote:

> 
> Dear All,
> 
> 
> I am using R and I am putting my problem in form of an example:
> 
> X<-c(128,34,153,987,345,45,3454,23,123)
> I want to remove the entries which are lesser than 100(? How to compare every element with 100 and how to create subsets )
> and I need two vectors y and z s.t
> y<-c(entries < 100)
> z<- c(remaining entries)
> 
> Moreover, Please tell me which command to use if I want to use different programs for y and z.
> X is the whole dataset and y,z are its disjoint subsets.


Arbitrary basic documentation on R programming (e.g. the manual "An 
Introduction to R") explains how to compare and how to use index 
operations. Please don't ask on R-help for your homework questions, but 
please read the posting guide.

Uwe Ligges



> Thanks 
> GS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tuechler at gmx.at  Thu Jan  5 09:19:27 2006
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 05 Jan 2006 09:19:27 +0100
Subject: [R] Splitting the list
In-Reply-To: <6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
	<mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
Message-ID: <3.0.6.32.20060105091927.00821c90@pop.gmx.net>

At 11:56 05.01.2006 +1100, John Maindonald wrote:
>I've changed the heading because this really is another thread.  I  
>think it inevitable that there will, in the course of time, be other  
>lists that are devoted, in some shape or form, to the concerns of  
>practitioners (at all levels) who are using R.  One development I'd  
>not like to see is fracture along application area lines, allowing  
>those who are comfortable in coteries whose focus was somewhat  
>relevant to standards of use of statistics in that area 15 or 20  
>years ago to continue that way.  One of the great things about R, in  
>its development to date, has been its role in exposing people from a  
>variety of application area communities to statistical traditions  
>different from that in which they have been nurtured. I expect it to  
>have a continuing role in raising statistical analysis standards, in  
>"raising the bar".
>
>Another possibility is fracture along geographic boundaries.  This  
>has both benefits (one being that its is easier within a smaller  
>circle of people who are more likely to know each other for  
>contributors to establish a rapport that will make the list really  
>effective; also there will be notices and discussion that are of  
>local interest) and drawbacks (it risks separating subscribers off  
>from important discussions on the official R lists.)  On balance,  
>this may be the better way to go. Indeed subscribers to ANZSTAT  
>(Australian and NZ statistical list) will know that an R-downunder  
>list, hosted at Auckland, is currently in test-drive mode. There  
>should be enough subscribers in common between this and the official  
>R lists that the south-eastern portion of Gondwana does not, at any  
>time in the very near future, float off totally on its own.
>
>There are of course other possibilities, and it may be useful to  
>canvass them.
>

Repeating a comment under the subject "Splitting the list":
I would considere to use flags at the beginning of the subject line, like
e.g. "BQ" for basic question. Of course, also geographic boundaries could
be considered.
This flags should be defined in the posting guide.
This way, every reader/expert can decide on a personal level to split the
list by filtering the messages accordingly.

Heinz Tuechler

>John Maindonald             email: john.maindonald at anu.edu.au
>phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
>Mathematical Sciences Institute, Room 1194,
>John Dedman Mathematical Sciences Building (Building 27)
>Australian National University, Canberra ACT 0200.
>
>
>
>On 4 Jan 2006, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:
>
>> From: Ben Fairbank <BEN at SSANET.COM>
>> Date: 4 January 2006 4:42:31 AM
>> To: R-help at stat.math.ethz.ch
>> Subject: Re: [R] A comment about R:
>>
>>
>> One implicit point in Kjetil's message is the difficulty of learning
>> enough of R to make its use a natural and desired "first choice
>> alternative," which I see as the point at which real progress and
>> learning commence with any new language.  I agree that the long  
>> learning
>> curve is a serious problem, and in the past I have discussed, off  
>> list,
>> with one of the very senior contributors to this list the  
>> possibility of
>> splitting the list into sections for newcomers and for advanced users.
>> He gave some very cogent reasons for not splitting, such as the
>> possibility of newcomers' getting bad advice from others only slightly
>> more advanced than themselves.  And yet I suspect that a newcomers'
>> section would encourage the kind of mutually helpful collegiality  
>> among
>> newcomers that now characterizes the exchanges of the more experienced
>> users on this list.  I know that I have occasionally been reluctant to
>> post issues that seem too elementary or trivial to vex the others  
>> on the
>> list with and so have stumbled around for an hour or so seeking the
>> solution to a simple problem.  Had I the counsel of others similarly
>> situated progress might have been far faster.  Have other newcomers or
>> occasional users had the same experience?
>>
>> Is it time to reconsider splitting this list into two sections?
>> Certainly the volume of traffic could justify it.
>>
>> Ben Fairbank
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From baron at psych.upenn.edu  Wed Jan  4 20:30:29 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 4 Jan 2006 14:30:29 -0500
Subject: [R] A comment about R:
In-Reply-To: <20060104160412.GA8708@phenix.sram.qc.ca>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<20060104160412.GA8708@phenix.sram.qc.ca>
Message-ID: <20060104193029.GB19366@psych.upenn.edu>

On 01/04/06 11:04, Franois Pinard wrote:
> I'm in the process of learning R.  While tutorials are undoubtedly very
> useful, and understanding that working and studying methods vary between
> individuals, what I (for one) would like to have is a fairly complete
> reference manual to the library.
> 
> Of course, we already have one, and that's marvellous already.  Yet, it
> is organised by library and, within each library, by function name: this
> organisation means that the manual is mainly used as a reference, or
> else, that it ought to be studied from cover to cover, dauntingly.

I think that many search facilities are helpful here:

1. help.search() searches all libraries on your computer by
default.

2. RSiteSearch() has an option of searching all functions in all
existing packages, in case you don't have a package and turn out
to need it: RSiteSearch([topic],restrict="functions")

I doubt that the sort of manual you describe is possible given
the very rapid growth of CRAN, and it would be really inadequate
if it did not include those packages.  Many of them are designed
for people in particular fields and turn out to be extremely
useful.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From phgrosjean at sciviews.org  Wed Jan  4 20:35:17 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 04 Jan 2006 20:35:17 +0100
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.58.0601031403280.19586@maplepark.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>	<43BA98BD.7060707@pburns.seanet.com>	<x2psn9fe23.fsf@viggo.kubism.ku.dk>	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
Message-ID: <43BC2375.7040309@sciviews.org>

David Forrest wrote:
 > [...]
 > Any volunteers?

Yes, me (well, partly...)! Here is what I propose: this is a very 
lengthy thread in R-Help, with many interesting ideas and suggestions. I 
fear that, as it happens too often, those nice ideas will be lost 
because of the support used: email! By nature, emails are read and then 
deleted (well, there is the R-Help archive, but anyway, threads in a 
mailing list is not at all the best tool to make collaborative documents 
like those tutorials and co).

I just cooked a little Wiki *dedicated to R beginners* (meaning they can 
contribute too, and are very welcome to discuss their problems -possibly 
trivial for others-). It is available at 
http://www.sciviews.org/_rgui/wiki. For the moment, everyone can edit 
and add pages, but I will restrict rights in the future to logged users 
only (with everybody allowed to log in at any time). So that we will be 
able to track who made changes (authorship).

For those who do not know the Wiki concept, it is a very simple way of 
working together in the same documents. The concept has proven very 
powerful with a good example being Wikipedia, that is becoming one of 
the largest encyclopedia in the world... and also as accurate as 
Encyclopedia Britannica (but read this: 
http://www.nature.com/news/2005/051212/full/438900a.html).

Here is the introduction of the R (GUI) Wiki:

This Wiki is mainly dedicated to deal with R beginners problems. 
Although we would like to emphasize using R GUIs (Graphical User 
Interfaces), this Wiki is not restricted to those GUIs: one can also 
deal with command-line approaches. The main idea is thus to have 
material contributed by both beginners, and by more advanced R users, 
that will help novices or casual users of R (http://www.r-project.org).

Overview

* The various documents in the [[wiki section]] explain how to use 
DokuWiki to edit documents in this site.

* The [[beginners section]] is dedicated to... beginners (share 
experience, expose problems and difficulties useful to share with other 
beginners, or to get help from more advanced people).

* The [[tutorials section]] is the place where you can put various R 
session examples, or short tutorials on either general or specific use of R.

* The [[easier section]] aims to collect together various pieces of R 
code that simplifies various tasks (especially for beginners) and that 
will ultimately be compiled in a easieR R packages on CRAN.

* The [[varia section]] is for any material that does not fit in the 
previous sections.


Final note: working with Wikis requires some learning... So, I am not 
sure at all that many R beginners will contribute to this wiki, but, of 
course, I hope so. Just let's pretend that it is a small experiment to 
try answering requests for another Internet space than R-Help, 
specifically dedicated to beginners...

A good starting point would be the following: all people that expressed 
interesting points in this thread could "copy and paste their ideas" to 
new pages in the Wiki.

Best,

Philippe Grosjean

..............................................<}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

David Forrest wrote:
> On Tue, 3 Jan 2006, Gabor Grothendieck wrote:
> ...
> 
>>In fact there are some things that are very easy
>>to do in Stata and can be done in R but only with more difficulty.
>>For example, consider this introductory session in Stata:
>>
>>http://www.stata.com/capabilities/session.html
>>
>>Looking at the first few queries,
>>see how easy it is to take the top few in Stata whereas in R one would
>>have a complex use of order.  Its not hard in R to write a function
>>that would make it just as easy but its not available off the top
>>of one's head though RSiteSearch("sort.data.frame") will find one
>>if one knew what to search for.
> 
> 
> This sort of thing points to an opportunity for documentation.  Building a
> tutorial session in R on how one would do a similar analysis would provide
> another method of learning R.  "An Introduction to R" is a good bottom-up
> introduction, which if you work through it does teach you how to do
> several things.  Adapting other tutorials or extended problems, like the
> Stata session, to R would give additional entry points.  A few end-to-end
> tutorials on some interesting analyses would be helpful.
> 
> Any volunteers?
> 
> Dave



From pinard at iro.umontreal.ca  Wed Jan  4 22:54:58 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 4 Jan 2006 16:54:58 -0500
Subject: [R] A comment about R:
In-Reply-To: <20060104193029.GB19366@psych.upenn.edu>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<20060104160412.GA8708@phenix.sram.qc.ca>
	<20060104193029.GB19366@psych.upenn.edu>
Message-ID: <20060104215458.GA14897@phenix.sram.qc.ca>

[Jonathan Baron]

>> [the current reference manual] is organised by library and, within 
>> each library, by function name: this organisation means that the 
>> manual is mainly used as a reference, or else, that it ought to be 
>> studied from cover to cover, dauntingly.

>I think that many search facilities are helpful here: [...]
>help.search() [...] >2. RSiteSearch() [...]

Sure they are!  Yet, we do not all learn or work the same way.  Given
full choice, I prefer reading a reference than go fish for information,
as this tends to build stronger information nets within my brain :-).

>I doubt that the sort of manual you describe is possible given the very
>rapid growth of CRAN, and it would be really inadequate if it did not
>include those packages.

The current reference manual does not cover CRAN, and even if it does 
not, I would not be tempted to qualify it as inadequate (at least for 
the novice I am).  There seems to be a lot to know about R, initially 
"as a language", and then, for learning to shuffle and organise data in 
preparation for later processing.  I would guess every new R user has to 
learn his way in there.  The current reference says a lot, but is big to 
grasp as it stands, its organisation is not as helpful as it could for 
learning and retaining.

The kind of manual I described seems possible to me, because it could be
mechanically derived out of a plan, and the derivation mechanics could
diagnose what is being forgotten (this could even yield some "Unsorted
functions" chapter or appendix).  The mechanic could be made general
enough to accept glue text at appropriate places.  [Not completely
dissimilar to, for those who happen to remember it, the way C code was
mechanically derived out of Pascal, initially, for Knuth's TeX.]

>Many of [CRAN packages] are designed for people in particular fields
>and turn out to be extremely useful.

Undoubtedly!  I envy you all, who know already! :-)

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From ligges at statistik.uni-dortmund.de  Thu Jan  5 09:27:35 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jan 2006 09:27:35 +0100
Subject: [R] A comment about R:
In-Reply-To: <20060104160412.GA8708@phenix.sram.qc.ca>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>	<43BA98BD.7060707@pburns.seanet.com>	<x2psn9fe23.fsf@viggo.kubism.ku.dk>	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<20060104160412.GA8708@phenix.sram.qc.ca>
Message-ID: <43BCD877.7020800@statistik.uni-dortmund.de>

Fran??ois Pinard wrote:
> [David Forrest]
> 
> 
>>[...] A few end-to-end tutorials on some interesting analyses would be
>>helpful.
> 
> 
> I'm in the process of learning R.  While tutorials are undoubtedly very 
> useful, and understanding that working and studying methods vary between 
> individuals, what I (for one) would like to have is a fairly complete 
> reference manual to the library.
> 
> Of course, we already have one, and that's marvellous already.  Yet, it 
> is organised by library and, within each library, by function name: this
> organisation means that the manual is mainly used as a reference, or 
> else, that it ought to be studied from cover to cover, dauntingly.
> 
> The very same material could be organised by topics.  Chapters could be 
> named like "General Help", "Language features", "Data types", "Data 
> Handling", "Input/Output", "Graphics", "Statistics", and such.  The 
> chapter "Language features", to take one example, could hold sections 
> like "Expressions", "Statements", "Functions", "Environments", 
> "Packages", "Execution" and "Debugging".  Sections could then hold 
> current reference pages.  References by library and/or by function name 
> could be stated either in appendices or as a general index at the end.


Have a look at  help.start() --> Search Engine & Keywords --> Section 
"Keywords by Topic".

Uwe Ligges



> For those who happen to know it, I find the "Emacs Lisp Reference 
> Manual" to be a good example for organising, in a very usable way,
> a comprehensive reference to a flurry of library functions.  When one 
> needs string handling functions, they are likely grouped together in the 
> manual, and are likely all present.  A tutorial, by comparison, usually 
> presents a subset, or even a tiny subset, of what is available.
> 
> 
>>Any volunteers?
> 
> 
> Not me, or at least, not before quite a long while.  The overall 
> organisation of a reference should not be handled by beginners.  On the 
> contrary, it rather requires someone who has comprehensive knowledge of 
> all the material to be considered.
> 
> Just an idea.  A good work plan would be to establish a new structure 
> for a reference manual, and once competent people (or this community as 
> a whole) agrees on a structure, to develop mechanical means for 
> generating a reference manual out of the current material.  The 
> mechanism should likely allow for added glue text, about everywhere 
> reasonable, and for diagnosing any lone, unreachable page in the current 
> reference.
>



From rechung at gmail.com  Thu Jan  5 10:46:42 2006
From: rechung at gmail.com (Robert Chung)
Date: Thu, 5 Jan 2006 10:46:42 +0100
Subject: [R] A comment about R:
References: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
Message-ID: <dpipu3$d6p$1@sea.gmane.org>

Roger Bivand wrote:
> Gabor Grothendieck wrote:
>
>> For example, consider this introductory session in Stata:
>> http://www.stata.com/capabilities/session.html
>>
> Could I ask for comments on:
> source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
> as a reproduction of the Stata capabilities session?

Roger, I think your reproduction of the Stata session is excellent.

However, in a deeper sense, perhaps it's *too* faithful a replication. I
don't normally do analyses exactly the same way in R and in Stata, so
although it's possible to contort R into producing Stata-like output, why
would anyone want to? For example, in the sample Stata session, they run a
t-test before plotting any data. In R, I'd tend to plot early and test
hypotheses after. Rather that print out the top and bottom 5 mileage cars,
I might plot(weight,mpg,col=as.integer(foreign)) and identify() the
bivariate oddities. Rather than start into linear models, I might do some
lowess() lines. I'd probably do a splom() pretty early. Depending on what
I was doing, maybe I'd do something like
stars(auto[,-c(1,12)],labels=make).

Stata and R are both fine products, but I sometimes wonder how the tools
one chooses affect the analyses one does.



From fcombes at gmail.com  Thu Jan  5 11:00:35 2006
From: fcombes at gmail.com (Florence Combes)
Date: Thu, 5 Jan 2006 11:00:35 +0100
Subject: [R] Splitting the list
In-Reply-To: <3.0.6.32.20060105091927.00821c90@pop.gmx.net>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
	<6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>
	<3.0.6.32.20060105091927.00821c90@pop.gmx.net>
Message-ID: <73dae3060601050200xb99ed5fn45a196e6a722888e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/5e6f5441/attachment.pl

From e.commandeur at uvt.nl  Thu Jan  5 11:27:01 2006
From: e.commandeur at uvt.nl (Edwin Commandeur)
Date: Thu, 5 Jan 2006 11:27:01 +0100
Subject: [R] problem with using lines command on windows XP machine
Message-ID: <NJEJJJHLCHDHKNFJBCEAIEEJCBAA.e.commandeur@uvt.nl>

Hello,

I'm using R version 2.2.0 installed on windows XP machine, with SP2 (maybe
it's also interesting to note it's laptop, so it outputs to a laptop screen)
a l and I wanted to draw a line in a graph, but it does not seem to work.

To test it I use the following code:

x = c(-1,0,1)
y = c(-1,0,1)
plot(x,y, type="l", xlim=c(-1,1), ylim=c(-1,1))
lines(0)

If I understand the documentation right this should draw a line (with
default settings, I'm not setting any parameters) at x=0.

I tried goofing around a bit setting linewidth and color differently, I
tried using xy.coords etc, but no line appeared in the graph.

The commands abline and segments work perfectly fine (so I am now using
segments to plot the line I want), but I still think the lines command
should work.

Does anybody has similar problems drawing lines on XP machines (or laptops
in general?)? Or I am doing something abominably wrong?

Greetings and thanks in advance for any replies,
Edwin Commandeur



From yfhuang at math.ccu.edu.tw  Thu Jan  5 10:40:54 2006
From: yfhuang at math.ccu.edu.tw (Yufen)
Date: Thu, 5 Jan 2006 17:40:54 +0800
Subject: [R] problem in install "kidpack" package
Message-ID: <000801c611dc$233a04c0$8e3e7b8c@mathyufen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/2bea5db9/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Thu Jan  5 11:42:16 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 5 Jan 2006 11:42:16 +0100
Subject: [R] problem with using lines command on windows XP machine
References: <NJEJJJHLCHDHKNFJBCEAIEEJCBAA.e.commandeur@uvt.nl>
Message-ID: <013e01c611e4$b1ebbcb0$0540210a@www.domain>

I think  you need "abline(v = 0)".

I hope it helps.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Edwin Commandeur" <e.commandeur at uvt.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 05, 2006 11:27 AM
Subject: [R] problem with using lines command on windows XP machine


> Hello,
>
> I'm using R version 2.2.0 installed on windows XP machine, with SP2 
> (maybe
> it's also interesting to note it's laptop, so it outputs to a laptop 
> screen)
> a l and I wanted to draw a line in a graph, but it does not seem to 
> work.
>
> To test it I use the following code:
>
> x = c(-1,0,1)
> y = c(-1,0,1)
> plot(x,y, type="l", xlim=c(-1,1), ylim=c(-1,1))
> lines(0)
>
> If I understand the documentation right this should draw a line 
> (with
> default settings, I'm not setting any parameters) at x=0.
>
> I tried goofing around a bit setting linewidth and color 
> differently, I
> tried using xy.coords etc, but no line appeared in the graph.
>
> The commands abline and segments work perfectly fine (so I am now 
> using
> segments to plot the line I want), but I still think the lines 
> command
> should work.
>
> Does anybody has similar problems drawing lines on XP machines (or 
> laptops
> in general?)? Or I am doing something abominably wrong?
>
> Greetings and thanks in advance for any replies,
> Edwin Commandeur
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From kjetilbrinchmannhalvorsen at gmail.com  Thu Jan  5 11:40:26 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Halvorsen)
Date: Thu, 5 Jan 2006 11:40:26 +0100
Subject: [R] Splitting the list
In-Reply-To: <6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
	<6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>
Message-ID: <556e90a80601050240o7c6f5e1dp8b158be3a388e068@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/b3ff4d1e/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jan  5 11:46:34 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jan 2006 11:46:34 +0100
Subject: [R] problem in install "kidpack" package
In-Reply-To: <000801c611dc$233a04c0$8e3e7b8c@mathyufen>
References: <000801c611dc$233a04c0$8e3e7b8c@mathyufen>
Message-ID: <43BCF90A.7070001@statistik.uni-dortmund.de>

Yufen wrote:

> Dear Sir,
>      I use the followoing command to install the library("kidpack"). BTW I install Biobase already.
> 
>> install.packages("kidpack",type="source")
> 
>     However, there is an error message occurred as follows.
>     > library("kidpack")
>      Error in library("kidpack") : 'kidpack' is not a valid package -- installed <
> 
>     I have problem in install "kidpack" package, could you please give me some help.
> Thank you!


Please check out 
http://tolstoy.newcastle.edu.au/~rking/R/help/05/12/16693.html

If you think that does not apply for you:
a) Where did you find a recent version of kidpack?
b) Which version of kidpack?
c) Which OS?
d) Which version of R?


Uwe Ligges




> Best,
> Yufen Huang
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ronggui.huang at gmail.com  Thu Jan  5 11:52:29 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 5 Jan 2006 18:52:29 +0800
Subject: [R] A comment about R:
In-Reply-To: <dpipu3$d6p$1@sea.gmane.org>
References: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
	<dpipu3$d6p$1@sea.gmane.org>
Message-ID: <38b9f0350601050252yeb6acb1t@mail.gmail.com>

R's week when handling large data file.
I has a data file : 807 vars, 118519 obs.and its CVS format.
Stata can read it in in 2 minus,but In my PC,R almost can not handle.
my pc's cpu 1.7G ;RAM 512M.


--

Deparment of Sociology
Fudan University



From ahimsa at camposarceiz.com  Thu Jan  5 11:55:53 2006
From: ahimsa at camposarceiz.com (ahimsa campos arceiz)
Date: Thu, 05 Jan 2006 19:55:53 +0900
Subject: [R] Fwd: Re:  Splitting the list
Message-ID: <6.0.1.1.0.20060105195447.0390b280@pop.notfound.org>



>Another possibi8lity, of course, is language-based lists. Any interest for
>r-spanish@ ...    ?
>
>Kjetil

I am ready to contribute traducing original English texts into Spanish, but 
not to produce original ones (I'm too new with these matters).

Ahimsa


Ahimsa Campos Arceiz
The University Museum,
The University of Tokyo
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033
phone +81-(0)3-5841-2824
cell +81-(0)80-5402-7702



From adi at roda.ro  Thu Jan  5 12:07:20 2006
From: adi at roda.ro (Adrian DUSA)
Date: Thu, 5 Jan 2006 13:07:20 +0200
Subject: [R] more on the daisy function
Message-ID: <200601051307.20640.adi@roda.ro>


Dear R-helpers,

First of all, a happy new year to everyone!

I succesfully used the daisy function (from package cluster) to find which two 
rows from a dataframe differ by only one value, and I now want to come up with 
a simpler way to find _which_ value makes the difference between any such 
pair of two rows.
Consider a very small example (the actual data counts thousands of rows):

   input <- matrix(letters[c(1,2,1,2,2,3,2,1,1,2,2,2)], ncol=3)

   > input
     X1 X2 X3
   1  a  b  a
   2  b  c  b
   3  a  b  b
   4  b  a  b

I am interested by the rows which differ by one value only; I easily do that 
with:

   library(cluster)
   distance <- daisy(as.data.frame(input))*ncol(input)

   > distance
   Dissimilarities :
     1 2 3
   2 3
   3 1 2
   4 3 1 2

   Metric :  mixed ;  Types = N, N, N
   Number of objects : 4


The first and the third rows differ only with respect to variable V3, and the 
second and the fourth rows differ only with respect to variable V2.


Now I want to replace the different values by an "x"; currently my code is:

   distance <- as.matrix(distance)
   distance[!upper.tri(distance)] <- NA
   to.be.compared <- as.matrix(which(distance == 1, arr.ind=T))
   logical.result <- t(apply(to.be.compared, 1,
              function(idx) {input[idx[1], ] == input[idx[2], ]}))
   result <- t(sapply(1:nrow(to.be.compared), 
             function(idx) {input[to.be.compared[idx, 1], ]})) 
   result[!logical.result] <- "x"

   > as.data.frame(result)
     V1 V2 V3
   1  a  b  x
   2  b  x  b

I wonder if the daisy function could be persuaded to output a similar object 
as the dissimilarities one; it would be fantastic to also get something like:

   First.difference.found:
     1 2 3
   2 1
   3 3 1
   4 1 2 1

Here, 3 means the third variable (V3) that the first and third rows differ on. 
I could try to do that myself, but I don't know where to find the Fortran 
code daisy uses.

Thanks for any hint,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From maechler at stat.math.ethz.ch  Thu Jan  5 12:10:19 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Jan 2006 12:10:19 +0100
Subject: [R] Wikis for R
In-Reply-To: <43BC2375.7040309@sciviews.org>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<43BC2375.7040309@sciviews.org>
Message-ID: <17340.65179.89485.345795@stat.math.ethz.ch>

>>>>> "PhGr" == Philippe Grosjean <phgrosjean at sciviews.org>
>>>>>     on Wed, 04 Jan 2006 20:35:17 +0100 writes:

    PhGr> David Forrest wrote:
    >> [...]
    >> Any volunteers?

    PhGr> Yes, me (well, partly...)! Here is what I propose: this is a very 
    PhGr> lengthy thread in R-Help, with many interesting ideas and suggestions. I 
    PhGr> fear that, as it happens too often, those nice ideas will be lost 
    PhGr> because of the support used: email! By nature, emails are read and then 
    PhGr> deleted (well, there is the R-Help archive, but anyway, threads in a 
    PhGr> mailing list is not at all the best tool to make collaborative documents 
    PhGr> like those tutorials and co).

    PhGr> I just cooked a little Wiki *dedicated to R beginners* (meaning they can 
    PhGr> contribute too, and are very welcome to discuss their problems -possibly 
    PhGr> trivial for others-). It is available at 
    PhGr> http://www.sciviews.org/_rgui/wiki. 

I you google for "R Wiki" you get (on the first page of hits)
- the japanase R Wiki "RjpWiki" [which has been in existence for
  quite a while, but that's all I know about it].

- the  Wikipedia entry for R
    http://en.wikipedia.org/wiki/R_programming_language

    (which is quite good, but probably could benefit from more volunteer input)

 If you go to the bottom of that wikipedia page,
 you see that there is an "R Wiki" -- and has been for several
 years now (!) at a Hamburg (De) university.
 http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

- Simon Urbanek's "R Wiki" mainly (but not exclusively) aimed at
  R for Mac OSX.

So, are you sure that another R Wiki is desirable, rather than
have people who "believe in Wiki's for R" use the existing
one(s)?   I believe the main challenge will (similar as for
an "R-beginners" mailing list) to have well-qualified "editors"
to be willing to review and amend what others have written.

I think it's an experiment that should be tried; but it has been
started already a while ago, and instead of restarting it, one
should try to agree on some cooperation with existing (Wiki)
approaches.

Hopefully some agreement on this is reached quickly, and we
could also add a link to the R wiki {or maybe several ones?}
from www.r-project.org.

Martin Maechler, ETH Zurich



From pburns at pburns.seanet.com  Thu Jan  5 12:11:22 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 05 Jan 2006 11:11:22 +0000
Subject: [R] A comment about R
In-Reply-To: <6C3364DB-17AC-43EC-A8D0-6CCBE629058A@anu.edu.au>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
	<6C3364DB-17AC-43EC-A8D0-6CCBE629058A@anu.edu.au>
Message-ID: <43BCFEDA.1060308@pburns.seanet.com>

John Maindonald wrote:

> ...
>
>(4) When should students start learning R?
>
>[Students should get their first exposure to a high-level programming  
>language, in the style of R then Python or Octave, at age 11-14.   
>There are now good alternatives to the former use of Fortran or  
>Pascal, languages which have for good reason dropped out of favour  
>for learning experience. They should start on R while their minds are  
>still malleable, and long before they need it for serious research use.]
>  
>

I think 11-14 years old might better be halved.  Kids are
playing very complicated video games barely after they
learn to walk.

R is a quite reasonable programming language for children.
You don't need to worry about low-level issues, and it is
easy to produce graphics with it.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From Tim.Smits at oce.kuleuven.be  Thu Jan  5 12:20:17 2006
From: Tim.Smits at oce.kuleuven.be (Tim Smits)
Date: Thu, 05 Jan 2006 12:20:17 +0100
Subject: [R] jointprior in deal package
Message-ID: <43BD00F1.8020803@oce.kuleuven.be>

Dear all,

I recently started using the deal package for learning Bayesian 
networks. When using the jointprior function on a particular dataset, I 
get the following message:
 >tor.prior<-jointprior(tor.nw)
Error in array(1, Dim) : 'dim' specifies too large an array

What is the problem? How can I resolve it?

Thanks,
Tim

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From nassar at noos.fr  Thu Jan  5 12:59:40 2006
From: nassar at noos.fr (Naji)
Date: Thu, 05 Jan 2006 12:59:40 +0100
Subject: [R] A comment about R:
In-Reply-To: <dpipu3$d6p$1@sea.gmane.org>
Message-ID: <BFE2C8BC.8A6D%nassar@noos.fr>

Hi all,

Roger thanks for the reproduction.
As a user of Stata & R, for common analysis I do use Stata and often, I have
to adapt some computations or to do some complex hierarchical modeling and
then I switch to R.
For me switching from Stata (or other statistical software, SO) to R (or
other statistical language) requests a double effort:
- Programming (laziness?) : writing and testing the code; considering the
data as N array or any data frame in order to optimize performance
- Statistical testing : I test the model over a simulated data set and
validate that the statistical process is giving me back the adequate
parameter estimates. An additional step one doesn't need when using an
established SO.

For me using Stata (or any other SO), has the advantage of using a high
quality code  written & tested by an organization & their clients.
Getting back to Roger replication, I find such replication very useful.
Test whether the R-code is giving back adequate results. So it's a very good
starting point before adapting the R-code to one's needs.

Stata advantage : one can download additional ado files ('package' like) and
with the permission of the author, adapt them or translate them into R-code.
Not only R &Stata are good products, they also show a valuable asset : the
users community


Happy new year
Le 5/01/06 10:46, ????Robert Chung???? <rechung at gmail.com> a ??crit??:

> Roger Bivand wrote:
>> Gabor Grothendieck wrote:
>> 
>>> For example, consider this introductory session in Stata:
>>> http://www.stata.com/capabilities/session.html
>>> 
>> Could I ask for comments on:
>> source(url("http://spatial.nhh.no/R/etc/capabilities.R"), echo=TRUE)
>> as a reproduction of the Stata capabilities session?
> 
> Roger, I think your reproduction of the Stata session is excellent.
> 
> However, in a deeper sense, perhaps it's *too* faithful a replication. I
> don't normally do analyses exactly the same way in R and in Stata, so
> although it's possible to contort R into producing Stata-like output, why
> would anyone want to? For example, in the sample Stata session, they run a
> t-test before plotting any data. In R, I'd tend to plot early and test
> hypotheses after. Rather that print out the top and bottom 5 mileage cars,
> I might plot(weight,mpg,col=as.integer(foreign)) and identify() the
> bivariate oddities. Rather than start into linear models, I might do some
> lowess() lines. I'd probably do a splom() pretty early. Depending on what
> I was doing, maybe I'd do something like
> stars(auto[,-c(1,12)],labels=make).
> 
> Stata and R are both fine products, but I sometimes wonder how the tools
> one chooses affect the analyses one does.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Tim.Smits at oce.kuleuven.be  Thu Jan  5 12:57:50 2006
From: Tim.Smits at oce.kuleuven.be (Tim Smits)
Date: Thu, 05 Jan 2006 12:57:50 +0100
Subject: [R] jointprior in deal package
Message-ID: <43BD09BE.6000307@oce.kuleuven.be>

Dear all,

I recently started using the deal package for learning Bayesian 
networks. When using the jointprior function on a particular dataset, I 
get the following message:
 >tor.prior<-jointprior(tor.nw)
Error in array(1, Dim) : 'dim' specifies too large an array

What is the problem? How can I resolve it?

Thanks,
Tim

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From rb.glists at gmail.com  Thu Jan  5 13:05:27 2006
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Thu, 05 Jan 2006 13:05:27 +0100
Subject: [R] A comment about R:
In-Reply-To: <dpipu3$d6p$1@sea.gmane.org>
References: <971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>	<Pine.LNX.4.44.0601041003400.9402-100000@reclus.nhh.no>
	<dpipu3$d6p$1@sea.gmane.org>
Message-ID: <43BD0B87.5090906@gmail.com>

As someone who has been using Stata for a while now (and I started without a programming background), I recently had to 
move to R because of the rich spatial packages. Here is my 0.001 cent to this thread.

-----------------WHAT I LOVE ABOUT STATA--------------------------
a) Total control
In Stata I feel like I had TOTAL CONTROL. I put my data in a directory, I can look at it, generate new variables 
(columns), reshape, collapse, and expand my data, and all the while I use the list command [list (my variables) in 
1/10)] over and over again to make sure I am doing what I want. List is probably my favorite Stata command.

b) Structure
As far as I am concerned Stata, has three main types of files

1. The data file (*.dta) which is my "spread sheet" in which I have my variables (columns, vectors or whatever you want 
to call it)

2. The do file (*.do) which is my set of commands for a particular analysis

3. The log file (*.log) (which is text of smcl output from my do file)

Just looking at the extensions in any given directory, I would know what is what and I am able to organise my project, 
(infact I put the three types in different sub-directories but work in one main project directory). Some have said R 
allows you to think through your analysis, well, I can swear that Stata has brought the same discipline to me. Key 
questions I always ask myself

- what peculiarities are there about my data (do I have unique observations...1 record per household, or multiple 
records and what does this mean for my analysis...do I need to collapse it, or reshape it).
- what do I want to do (write down a few lines of what I want to do and expected output)

c) Ease of use
I feel that most of Stata's commands were intuitively named and I find it easy to use (a choice of the GUI, command 
prompt, or the dofile editor)


-----------------MY FIRST 30 DAYS WITH R--------------------------
Moving to R was a totally different experience, and in part its the whole concept of objects (and I still dont get them 
:-) ). My first assignment was to get the R equivalents of the three files as well as my main Stata commands (and 
frankly, the only one that is clear now is the script which is R's equivalent of the Stata do file).

A few have asked about the relevance of reproducing Stata (or SAS for that matter) commands in R. Well someone correctly 
pointed out that the challenge is in the mind set. Stata users have a Stata mindset so by being able to reproduce some 
basic work done in Stata in R, you are many steps closer to understanding the workings of R.

So yes I did whine in the first few weeks about how hard R is. Some have attributed the whining about R to laziness...I 
disagree, the learning curve is simply steep. I there salute Roger Bivand's effort to reproduce the example on the Stata 
website and I second efforts by others to do this for other programs.

Now dont get me wrong, I am not ungrateful for the tons of material make freely available by the R community (top on 
this list being R itself), however, most of this material is terse and most of the time I have had to go over it a few 
times (and may still not get it).

But even more, I am yet to find material dedicated to basic data management (indeed bits of data management are dropped 
here and there in the manuals and online material) however, a dedicated book (which I would gladly buy) is lacking.


-----------------R-Help List--------------------------
In this same thread there has been discussion on splitting the R-Help list. I have reservations about this (we had the 
same discussion on the Stata list and the consensus was to maintain the status quo). Geographically splitting the list 
simply reinforces the inequalities birthed out of the original development of R. Some countries or regions are bound to 
have more exciting lists thanks to the initial distribution of resource persons. Sending the beginners to their own list 
is nothing short of crippling them (let the one eyed lead the blind....hmmm....bad idea). Not only will it cripple your 
thinking, but it can instill bad prgramming practices that may be hard to drop. I look back at the Stata stuff I wrote 6 
years ago and I am ashamed by how much real estate I wasted writing line onto line that could be cut down in less than 
1/10th. How did I learn...well, I passively and faithfully read each email that was posted and saved in my scrap book 
elegant bits of code.

Finally, I have been on the Statalist for close to six years and we do get our fair share of "homework type" questions 
and people get told off (though not with the frequency and "harshness" of this list). Infact some one once whined about 
a rude reply he got from his posting and someone wrote to inform him that there were much harsher lists adding that 
R-Help list is not for the faint hearted (two reasons, one being that the typical posting may sound like rocket science 
to most, the other being that there is very little tolerance for those who fail to adhere to the posting guide). May be 
this is a good thing because it forces people to think twice (100 times for me) before posting, but on the downside, 
this could traumatize a poor soul and put him/her off R all together (but then you may say....this is not a Church nor 
is it Dr Phils show and we are not in the business of making you feel good. Well....R is open source and the notion of 
strength in numbers certainly holds). It is not hard to see who is posting a cry for help for the first time (my first 
subject line was mayday mayday and I was told off :-), ofcourse now I get it ). My approach is usually to help such a 
one but point them towards the posting guide (hopefully, they dont make the same mistake again and yet they dont feel 
like big "fools")

To conclude
This thread was birthed out of the Micheal Mitchells article (I have read his book as well as the great amounts of 
helpful material he has made available on his website). The key questions asked as a result of his article were
- Was he praising with damnation or damning with Praise?
- Did what he posted about R hold water? If so, what can be made better?

 From the emails posted so far, the jury is still out on these questions and I am enjoying the discussion.


Ronnie



From petr.pikal at precheza.cz  Thu Jan  5 13:32:12 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 05 Jan 2006 13:32:12 +0100
Subject: [R] problem with using lines command on windows XP machine
In-Reply-To: <NJEJJJHLCHDHKNFJBCEAIEEJCBAA.e.commandeur@uvt.nl>
Message-ID: <43BD1FDC.9113.16573B0@localhost>

Hi

On 5 Jan 2006 at 11:27, Edwin Commandeur wrote:

From:           	"Edwin Commandeur" <e.commandeur at uvt.nl>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Thu, 5 Jan 2006 11:27:01 +0100
Subject:        	[R] problem with using lines command on windows XP machine

> Hello,
> 
> I'm using R version 2.2.0 installed on windows XP machine, with SP2
> (maybe it's also interesting to note it's laptop, so it outputs to a
> laptop screen) a l and I wanted to draw a line in a graph, but it does
> not seem to work.
> 
> To test it I use the following code:
> 
> x = c(-1,0,1)
> y = c(-1,0,1)
> plot(x,y, type="l", xlim=c(-1,1), ylim=c(-1,1))
> lines(0)

quite close

abline(v=0)

draws a vertical line at x=0

from help page
Arguments:

    x, y: coordinate vectors of points to join.

Maybe to mention abline in See also of lines help page could be good

HTH
Petr



> 
> If I understand the documentation right this should draw a line (with
> default settings, I'm not setting any parameters) at x=0.
> 
> I tried goofing around a bit setting linewidth and color differently,
> I tried using xy.coords etc, but no line appeared in the graph.
> 
> The commands abline and segments work perfectly fine (so I am now
> using segments to plot the line I want), but I still think the lines
> command should work.
> 
> Does anybody has similar problems drawing lines on XP machines (or
> laptops in general?)? Or I am doing something abominably wrong?
> 
> Greetings and thanks in advance for any replies,
> Edwin Commandeur
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From pmilin at ff.ns.ac.yu  Thu Jan  5 13:47:35 2006
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Thu, 05 Jan 2006 13:47:35 +0100
Subject: [R] Understanding and translating lme() into lmer() model
Message-ID: <1136465255.7946.17.camel@localhost.localdomain>

I am newbie in R, trying to understand and compare syntax in nlme and
lme4. lme() model from the nlme package I am interested in is:
	lme.m1.1 = lme(Y~A+B+C,random=~1|D/E,data=data,method="ML")
(for simplicity reason, I am giving generic names of factors)
If I understand well, there are three fixed factors: A, B and C, and two
random factors: D and E. In addition to that, E is nested in D, isn't
it? Of course, method is Maximum Likelihood.
If I would like to translate the above model to one suitable for lmer(),
it should look like this:
	lmer.m1.1 = lmer(Y~A+B+C+(1|D:E),data=data,method="ML")
Am I right? Is '/' in nlme same as ':' in lme4?

Sincerely,
Peter M.



From paul.bliese at us.army.mil  Thu Jan  5 14:01:17 2006
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Thu, 5 Jan 2006 14:01:17 +0100
Subject: [R] ylim problem in barplot
Message-ID: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/e0a1e844/attachment.pl

From academic at feferraz.net  Thu Jan  5 14:09:42 2006
From: academic at feferraz.net (Fernando Henrique Ferraz P. da Rosa)
Date: Thu, 5 Jan 2006 11:09:42 -0200
Subject: [R] A comment about R:
In-Reply-To: <x2psn9fe23.fsf@viggo.kubism.ku.dk>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
Message-ID: <20060105130942.GA30865@ime.usp.br>

Peter Dalgaard writes:
> Patrick Burns <pburns at pburns.seanet.com> writes:
> 
> whereas you could quite conceivably do it in R. (What *is* the
> equivalent of rnorm(25) in those languages, actually?)
> 
        In SAS, it would go along the lines of:

data randvec(drop=seed);
 seed = 459437845;
 do obs = 1 to 25;
   x = rannor(seed);
   output;
   end;
 run;
 
--
"Though this be randomness, yet there is structure in't."
                                           Rosa, F.H.F.P

Instituto de Matem??tica e Estat??stica
Universidade de S??o Paulo
Fernando Henrique Ferraz P. da Rosa
http://www.feferraz.net



From HDoran at air.org  Thu Jan  5 14:19:02 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 5 Jan 2006 08:19:02 -0500
Subject: [R] Understanding and translating lme() into lmer() model
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A013964F0@dc1ex3.air.org>

Peter:

Almost correct. You need to add the variance component for the highest
level of nesting, so your model would be

lmer.m1.1 = lmer(Y~A+B+C+(1|D:E) + (1|E), data=data,method="ML")

But, yes, the : is used to note implicit nesting in lmer similar to the
syntax used for / in lme. The syntax varies a bit because lme was useful
for models with nested random effects. But, lmer can handle models with
more complex structures such as crossed random effects. It doesn't make
sense to use strict nesting structures when units are migrating, so that
is part of the reason for the evolution of lmer().  

If you use RSiteSearch('lmer syntax') you will find a few threads on the
topic that might be helpful.

Harold



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petar Milin
Sent: Thursday, January 05, 2006 7:48 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Understanding and translating lme() into lmer() model

I am newbie in R, trying to understand and compare syntax in nlme and
lme4. lme() model from the nlme package I am interested in is:
	lme.m1.1 = lme(Y~A+B+C,random=~1|D/E,data=data,method="ML")
(for simplicity reason, I am giving generic names of factors) If I
understand well, there are three fixed factors: A, B and C, and two
random factors: D and E. In addition to that, E is nested in D, isn't
it? Of course, method is Maximum Likelihood.
If I would like to translate the above model to one suitable for lmer(),
it should look like this:
	lmer.m1.1 = lmer(Y~A+B+C+(1|D:E),data=data,method="ML")
Am I right? Is '/' in nlme same as ':' in lme4?

Sincerely,
Peter M.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From academic at feferraz.net  Thu Jan  5 14:23:04 2006
From: academic at feferraz.net (Fernando Henrique Ferraz P. da Rosa)
Date: Thu, 5 Jan 2006 11:23:04 -0200
Subject: [R] Wikis for R
In-Reply-To: <17340.65179.89485.345795@stat.math.ethz.ch>
Message-ID: <20060105132304.GB30865@ime.usp.br>

Martin Maechler writes:
>  If you go to the bottom of that wikipedia page,
>  you see that there is an "R Wiki" -- and has been for several
>  years now (!) at a Hamburg (De) university.
>  http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
> 
> (...) 
> So, are you sure that another R Wiki is desirable, rather than
> have people who "believe in Wiki's for R" use the existing
> one(s)?   I believe the main challenge will (similar as for
> an "R-beginners" mailing list) to have well-qualified "editors"
> to be willing to review and amend what others have written.

        I??ve tried to colaborate on the R Wiki hosted by the Hamburg
university but the Wiki would get regularlly vandalized by some spam
bot, and then I'd have to manually keep reverting it several times. Also
the wiki engine used by this wiki is very rudimentary. I think the
DokuWiki engine, which is used by Philippe Grosjean is more promising as
a workhorse for an 'official' R-wiki. 

        I think that the title could be perhaps changed to Rwiki
and the contents currently hosted on the Hamburg wiki 'transfered' to 
the new location, if the current mantainers of the Hamburg Wiki and
Philippe Grosjean agree (I??m cc-ing this msg to them).

        This could emerge then as official or semi-oficial R-wiki, to be
linked to from the R-project home. 


--
"Though this be randomness, yet there is structure in't."
                                           Rosa, F.H.F.P

Instituto de Matem??tica e Estat??stica
Universidade de S??o Paulo
Fernando Henrique Ferraz P. da Rosa
http://www.feferraz.net



From andy_liaw at merck.com  Thu Jan  5 14:25:33 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 Jan 2006 08:25:33 -0500
Subject: [R] A comment about R
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6B9@usctmx1106.merck.com>

From: Patrick Burns
> 
> John Maindonald wrote:
> 
> > ...
> >
> >(4) When should students start learning R?
> >
> >[Students should get their first exposure to a high-level 
> programming  
> >language, in the style of R then Python or Octave, at age 11-14.   
> >There are now good alternatives to the former use of Fortran or  
> >Pascal, languages which have for good reason dropped out of favour  
> >for learning experience. They should start on R while their 
> minds are  
> >still malleable, and long before they need it for serious 
> research use.]
> >  
> >
> 
> I think 11-14 years old might better be halved.  Kids are
> playing very complicated video games barely after they
> learn to walk.

My kids (7- and 5-year old) barely get an hour on video games a week, and I
can see that they lag behind their peers at the games (though I don't feel
sorry for that).  I hope I won't be acused of `endangering welfare of
children'...
 
> R is a quite reasonable programming language for children.
> You don't need to worry about low-level issues, and it is
> easy to produce graphics with it.

Any suggestion on how to go about getting kids that young on (R)
programming?

Cheers,
Andy

 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From academic at feferraz.net  Thu Jan  5 14:40:35 2006
From: academic at feferraz.net (Fernando Henrique Ferraz P. da Rosa)
Date: Thu, 5 Jan 2006 11:40:35 -0200
Subject: [R] Splitting the list
In-Reply-To: <556e90a80601050240o7c6f5e1dp8b158be3a388e068@mail.gmail.com>
References: <mailman.10.1136372402.22906.r-help@stat.math.ethz.ch>
	<6F775C89-0BE1-438F-8CC8-55995BFB2043@anu.edu.au>
	<556e90a80601050240o7c6f5e1dp8b158be3a388e068@mail.gmail.com>
Message-ID: <20060105134035.GC30865@ime.usp.br>

Kjetil Halvorsen writes:
> 
> Another possibi8lity, of course, is language-based lists. Any interest for
> r-spanish@ ...    ?
> 
> Kjetil

        Since you??ve mentioned the topic, anyone reading this thread
knows of currently active R language-based lists? I am a member of R_STAT,
 an R list for Portuguese speakers [1]. It would be nice to collect
links for such lists and have them on the R-project website. I tried
e-mailing r-devel regarding this on last July, but got no reply [2].


References:
[1] http://br.groups.yahoo.com/group/R_STAT/
[2] http://tolstoy.newcastle.edu.au/~rking/R/devel/05/07/1623.html



--
"Though this be randomness, yet there is structure in't."
                                           Rosa, F.H.F.P

Instituto de Matem??tica e Estat??stica
Universidade de S??o Paulo
Fernando Henrique Ferraz P. da Rosa
http://www.feferraz.net



From MSchwartz at mn.rr.com  Thu Jan  5 14:40:32 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 05 Jan 2006 07:40:32 -0600
Subject: [R] ylim problem in barplot
In-Reply-To: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
Message-ID: <1136468432.4592.2.camel@localhost.localdomain>

On Thu, 2006-01-05 at 14:01 +0100, Bliese, Paul D LTC USAMH wrote:
> R Version 2.2.0
> 
> Platform:  Windows

> When I use barplot but select a ylim value greater than zero, the graph
> is distorted.  The bars extend below the bottom of the graph.

> For instance the command produces a problematic graph.

> barplot(c(200,300,250,350),ylim=c(150,400))

> Any help would be appreciated.

> Paul


Use:

  barplot(c(200, 300, 250, 350), ylim = c(150, 400), xpd = FALSE)

The 'xpd = FALSE' will enable clipping of the graphic at the boundary of
the plot region.

See ?par for more information on 'xpd'.

HTH,

Marc Schwartz



From bolker at ufl.edu  Thu Jan  5 14:54:59 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 5 Jan 2006 13:54:59 +0000 (UTC)
Subject: [R] ylim problem in barplot
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
Message-ID: <loom.20060105T144258-290@post.gmane.org>

Bliese, Paul D LTC USAMH <paul.bliese <at> us.army.mil> writes:

> 
> R Version 2.2.0
> 
> Platform:  Windows
> 
> When I use barplot but select a ylim value greater than zero, the graph
> is distorted.  The bars extend below the bottom of the graph.
>

  The problem is that barplot() is really designed to work
with zero-based data.  I don't know if the Powers That Be
will say that "fixing" this would violate the spirit of
barplot (although I see there is some code in barplot 
that deals with figuring out the base of the rectangle
in the logarithmic case, where 0 obviously doesn't work)

 Here's a workaround:

barplot(c(200,300,250,350)-150,axes=FALSE)
axis(side=2,at=seq(0,200,by=50),labels=seq(150,350,by=50))

 And here's a diff: if you want to hack barplot yourself,

sink("newbarplot.R")
barplot.default
sink()
## go edit newbarplot.R; add barplot.default <- to
## the first line, remove the namespace information
## from the last line, and substitute the lines
## in the first chunk below with exclamation points for 
## the lines in the second chunk below with exclamation
## points
source("newbarplot.R")

  cheers
    Ben

*** newbarplot2.R       2006-01-05 08:52:11.000000000 -0500
--- /usr/local/src/R/R-2.2.1/src/library/graphics/R/barplot.R   2005-10-06
06:22:59.000000000 -0400
***************
*** 85,97 ****
            if      (logy && !horiz && !is.null(ylim))  ylim[1]
            else if (logx && horiz  && !is.null(xlim))  xlim[1]
            else 0.9 * min(height)
!     } else {
!       rectbase <- if (!horiz && !is.null(ylim))
!         ylim[1]
!       else if (horiz && !is.null(xlim))
!         xlim[1]
!       else 0
!     }
      ## if stacked bar, set up base/cumsum levels, adjusting for log scale
      if (!beside)
        height <- rbind(rectbase, apply(height, 2, cumsum))
--- 85,92 ----
            if      (logy && !horiz && !is.null(ylim))  ylim[1]
            else if (logx && horiz  && !is.null(xlim))  xlim[1]
            else 0.9 * min(height)
!     } else rectbase <- 0
!
      ## if stacked bar, set up base/cumsum levels, adjusting for log scale
      if (!beside)
        height <- rbind(rectbase, apply(height, 2, cumsum))



From e.commandeur at uvt.nl  Thu Jan  5 14:58:53 2006
From: e.commandeur at uvt.nl (Edwin Commandeur)
Date: Thu, 5 Jan 2006 14:58:53 +0100
Subject: [R] problem with using lines command on windows XP machine
In-Reply-To: <43BD1FDC.9113.16573B0@localhost>
Message-ID: <NJEJJJHLCHDHKNFJBCEAEEEMCBAA.e.commandeur@uvt.nl>

Hi Petr and Eric,

Thanks for your comments.

To plot a vertical line, using "lines(0)" does not work, but

lines(c(-1,0),c(0,1))

does the work in my simple test example. I just interpreted the ?lines
documentation wrong.

So the "lines" command does work on my pc. Off course "abline(v=0)" will
also do the job in this specific example...

Sorry for the trouble,

Edwin

-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz]
Sent: donderdag 5 januari 2006 13:32
To: Edwin Commandeur; r-help at stat.math.ethz.ch
Subject: Re: [R] problem with using lines command on windows XP machine


Hi

On 5 Jan 2006 at 11:27, Edwin Commandeur wrote:

From:           	"Edwin Commandeur" <e.commandeur at uvt.nl>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Thu, 5 Jan 2006 11:27:01 +0100
Subject:        	[R] problem with using lines command on windows XP machine

> Hello,
>
> I'm using R version 2.2.0 installed on windows XP machine, with SP2
> (maybe it's also interesting to note it's laptop, so it outputs to a
> laptop screen) a l and I wanted to draw a line in a graph, but it does
> not seem to work.
>
> To test it I use the following code:
>
> x = c(-1,0,1)
> y = c(-1,0,1)
> plot(x,y, type="l", xlim=c(-1,1), ylim=c(-1,1))
> lines(0)

quite close

abline(v=0)

draws a vertical line at x=0

from help page
Arguments:

    x, y: coordinate vectors of points to join.

Maybe to mention abline in See also of lines help page could be good

HTH
Petr



>
> If I understand the documentation right this should draw a line (with
> default settings, I'm not setting any parameters) at x=0.
>
> I tried goofing around a bit setting linewidth and color differently,
> I tried using xy.coords etc, but no line appeared in the graph.
>
> The commands abline and segments work perfectly fine (so I am now
> using segments to plot the line I want), but I still think the lines
> command should work.
>
> Does anybody has similar problems drawing lines on XP machines (or
> laptops in general?)? Or I am doing something abominably wrong?
>
> Greetings and thanks in advance for any replies,
> Edwin Commandeur
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From bolker at ufl.edu  Thu Jan  5 15:06:33 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 5 Jan 2006 14:06:33 +0000 (UTC)
Subject: [R] ylim problem in barplot
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
	<loom.20060105T144258-290@post.gmane.org>
Message-ID: <loom.20060105T150251-362@post.gmane.org>

Ben Bolker <bolker <at> ufl.edu> writes:

> 
> Bliese, Paul D LTC USAMH <paul.bliese <at> us.army.mil> writes:
> 
> > 
> > R Version 2.2.0
> > 
> > Platform:  Windows
> > 
> > When I use barplot but select a ylim value greater than zero, the graph
> > is distorted.  The bars extend below the bottom of the graph.
> >
> 
>   The problem is that barplot() is really designed to work
> with zero-based data.  I don't know if the Powers That Be
> will say that "fixing" this would violate the spirit of
> barplot (although I see there is some code in barplot 
> that deals with figuring out the base of the rectangle
> in the logarithmic case, where 0 obviously doesn't work)
> 

  hmm, replying to myself ...
  Now that I think about it, I don't know if the default behavior should
necessarily be to set the baseline at ylim[1] or xlim[1] ...  (i.e., you
can imagine setting ylim negative to allow more space
below the bars ... you could allow a "baseline" argument,
but this would then be ripe for abuse ...  perhaps this
discussion should move to r-devel, if anyone cares  ... )



From maechler at stat.math.ethz.ch  Thu Jan  5 15:11:08 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Jan 2006 15:11:08 +0100
Subject: [R] ylim problem in barplot
In-Reply-To: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
Message-ID: <17341.10492.905351.841579@stat.math.ethz.ch>


>>>>> "PaulB" == Bliese, Paul D LTC USAMH <paul.bliese at us.army.mil>
>>>>>     on Thu, 5 Jan 2006 14:01:17 +0100 writes:

    PaulB> R Version 2.2.0
    PaulB> Platform:  Windows

 
    PaulB> When I use barplot but select a ylim value greater
    PaulB> than zero, the graph is distorted.  The bars extend
    PaulB> below the bottom of the graph.

Well, my question would be if that is not a feature :-)
Many people would consider barplots that do not start at 0 as
 "Cheating with Graphics"  (in the vein of "Lying with Statistics").
 
    PaulB> For instance the command produces a problematic graph.

    PaulB> barplot(c(200,300,250,350),ylim=c(150,400))

The advantage of the current graphic drawn is that everyone *sees*
that the bars were cut off {and that one should really think
twice before producing such cheating graphics.. :-)}

 plot(c(200,300,250,350), ylim=c(150,400), type = "h", 
      lwd=20, xaxt="n", col="gray")

produces something closer to what you like.
[yes, you can get rid of the roundedness of the thick-line ends;
 --> ?par and look for 'lend';
 --> op <- par(lend = 1) ; plot(.........) ; par(op)
 In R-devel (i.e. from R 2.3.0 on) you can even say
  plot(c(200,300,250,350), ylim=c(150,400), type = "h", 
       lwd=20, xaxt="n", col="gray", lend = 1)
]

But after all, I tend to agree that R should behave a bit differently
here, 
e.g., first giving a warning about the non-approriate ylim 
but then still obey the ylim specification more nicely.  

Regards,
Martin Maechler



From Mleeds at kellogggroup.com  Thu Jan  5 15:13:58 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Thu, 5 Jan 2006 09:13:58 -0500
Subject: [R] .Rprofile files (was R newbie configuration)
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466340@exchange.psg.com>

Thanks a lot. setHook is
Currently not in my knowledge set
But it's great to save these
Thing so I can look them up
When I  feel more comfortable.

Just to add to that Stata versus R discussion :

I believe, anyone who uses
any other package than R, is probably missing
out in the long run. It's truly unbelievable
what has been done here. I feel like I
fell asleep for 5 years ( by not using it )
and just woke up to all of these new
packages, facilities etc.

                        Mark




-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, January 05, 2006 4:04 AM
To: Petr Pikal
Cc: Mark Leeds; R-Stat Help
Subject: Re: [R] .Rprofile files (was R newbie configuration)

And here is one with a working setHook call.

options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),
         function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
options(repos=c(CRAN="http://cran.uk.r-project.org"))


On Thu, 5 Jan 2006, Petr Pikal wrote:

> Hi
>
> here is my example of .Rprofile file
>
>
> require(graphics)
> require(utils)
>
> # setHook(packageEvent("graphics", "onLoad"), function(...)
> # graphics::par(bg="white"))  ## did not manage to persuade setHook
> # to work properly
>
> par(bg="white")
> RNGkind("Mersenne-Twister", "Inversion")
>
> # some set of my functions and data
>
> .libPaths("D:/programy/R/R-2.2.0/library/fun")
> library(fun)
> data(stand)
>
>
> HTH
> Petr
>
>
> On 4 Jan 2006 at 15:46, Mark Leeds wrote:
>
> Date sent:      	Wed, 4 Jan 2006 15:46:37 -0500
> From:           	"Mark Leeds" <Mleeds at kellogggroup.com>
> To:             	"R-Stat Help" <R-help at stat.math.ethz.ch>
> Subject:        	[R] R newbie configuration
>
>> I think I did enough reading on my
>> Own about startup ( part of the morning
>> And most of this afternoon )
>> to not feel uncomfortable asking
>> for confirmation of my understanding of this startup stuff.
>>
>> Obviously, the startup process is more complicated
>> Than below but, for my R newbie purposes,
>> It seems like I can think of the startup process as follows :
>>
>> Suppose my  home directory = "c:documents and settings/mleeds" =
>> $HOME.
>>
>> Put things in $HOME/.Rprofile that are more generic on startup and
not
>> specific to any particular R project.
>>
>> Put various .First() functions in the working directories of the
>> particular projects that
>> they are associated with so that they loaded in when their .RData
>> directory gets loaded.
>>
>> If above is correct  ( emphasis on correct for a newbie. I know there
>> is a lot more going on And things can be done more elegantly etc ),
>> Could someone send me an example of a .Rprofile file. I didn't use
>> these in S+ and I am wondering what you put in them ?
>>
>>                                    Thanks
>>
>>
>>
>>
>>
**********************************************************************
>> This email and any files transmitted with it are
>> confidentia...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From andy_liaw at merck.com  Thu Jan  5 15:14:38 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 Jan 2006 09:14:38 -0500
Subject: [R] problem with using lines command on windows XP machine
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6BB@usctmx1106.merck.com>

lines() connects the `dots' given.  If you want straight lines spanning the
entire graph, you are better off with abline().

Andy

From: Edwin Commandeur
> 
> Hi Petr and Eric,
> 
> Thanks for your comments.
> 
> To plot a vertical line, using "lines(0)" does not work, but
> 
> lines(c(-1,0),c(0,1))
> 
> does the work in my simple test example. I just interpreted the ?lines
> documentation wrong.
> 
> So the "lines" command does work on my pc. Off course 
> "abline(v=0)" will
> also do the job in this specific example...
> 
> Sorry for the trouble,
> 
> Edwin
> 
> -----Original Message-----
> From: Petr Pikal [mailto:petr.pikal at precheza.cz]
> Sent: donderdag 5 januari 2006 13:32
> To: Edwin Commandeur; r-help at stat.math.ethz.ch
> Subject: Re: [R] problem with using lines command on windows 
> XP machine
> 
> 
> Hi
> 
> On 5 Jan 2006 at 11:27, Edwin Commandeur wrote:
> 
> From:           	"Edwin Commandeur" <e.commandeur at uvt.nl>
> To:             	<r-help at stat.math.ethz.ch>
> Date sent:      	Thu, 5 Jan 2006 11:27:01 +0100
> Subject:        	[R] problem with using lines command on 
> windows XP machine
> 
> > Hello,
> >
> > I'm using R version 2.2.0 installed on windows XP machine, with SP2
> > (maybe it's also interesting to note it's laptop, so it outputs to a
> > laptop screen) a l and I wanted to draw a line in a graph, 
> but it does
> > not seem to work.
> >
> > To test it I use the following code:
> >
> > x = c(-1,0,1)
> > y = c(-1,0,1)
> > plot(x,y, type="l", xlim=c(-1,1), ylim=c(-1,1))
> > lines(0)
> 
> quite close
> 
> abline(v=0)
> 
> draws a vertical line at x=0
> 
> from help page
> Arguments:
> 
>     x, y: coordinate vectors of points to join.
> 
> Maybe to mention abline in See also of lines help page could be good
> 
> HTH
> Petr
> 
> 
> 
> >
> > If I understand the documentation right this should draw a 
> line (with
> > default settings, I'm not setting any parameters) at x=0.
> >
> > I tried goofing around a bit setting linewidth and color 
> differently,
> > I tried using xy.coords etc, but no line appeared in the graph.
> >
> > The commands abline and segments work perfectly fine (so I am now
> > using segments to plot the line I want), but I still think the lines
> > command should work.
> >
> > Does anybody has similar problems drawing lines on XP machines (or
> > laptops in general?)? Or I am doing something abominably wrong?
> >
> > Greetings and thanks in advance for any replies,
> > Edwin Commandeur
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at gmail.com  Thu Jan  5 15:38:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 Jan 2006 09:38:39 -0500
Subject: [R] A comment about R
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6B9@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6B9@usctmx1106.merck.com>
Message-ID: <971536df0601050638j151b0f63p5bc8c77a91e51e3f@mail.gmail.com>

On 1/5/06, Liaw, Andy <andy_liaw at merck.com> wrote:
> From: Patrick Burns
> >
> > John Maindonald wrote:
> >
> > > ...
> > >
> > >(4) When should students start learning R?
> > >
> > >[Students should get their first exposure to a high-level
> > programming
> > >language, in the style of R then Python or Octave, at age 11-14.
> > >There are now good alternatives to the former use of Fortran or
> > >Pascal, languages which have for good reason dropped out of favour
> > >for learning experience. They should start on R while their
> > minds are
> > >still malleable, and long before they need it for serious
> > research use.]
> > >
> > >
> >
> > I think 11-14 years old might better be halved.  Kids are
> > playing very complicated video games barely after they
> > learn to walk.
>
> My kids (7- and 5-year old) barely get an hour on video games a week, and I
> can see that they lag behind their peers at the games (though I don't feel
> sorry for that).  I hope I won't be acused of `endangering welfare of
> children'...
>
> > R is a quite reasonable programming language for children.
> > You don't need to worry about low-level issues, and it is
> > easy to produce graphics with it.
>
> Any suggestion on how to go about getting kids that young on (R)
> programming?

I have introduced a number of computer software tools to my nephew
who is a teenager.  I think the key item is motivation and attention
span -- which is short.  They will want to get results fast and want
results to be of interest to them.

I have taught him elements of HTML, javascript and R.  In retrospect,
the most successful was HTML and to a lesser extent javascript.
When I asked him which of the three he wanted to learn more of
after not having done it for a while it was javascript.

The advantage of starting with HTML is that its relatively simple and within
one or two sessions he/she will be able to be putting together
web pages for themelves so its obviously useful and they can
be creative almost immediately. Also that leads naturally to javascript
and one can download lots of fancy mouse tails and other
motivating javascript snippets.

Previously we did it in person but now we are in different cities and
do it via instant messaging.  We started with javascript (which of the
three was the one he favored to get back into) again but
found that it was difficult to communicate javascript over instant
messaging so we tried R instead.

Because R is interactive one can easily discuss a line at a time and
include it right in the instant messaging dialogue so in that mode
I found R was possible to communicate whereas javascript difficult.  There
are some nice graphics demos in R which are motivating although
I think the mouse javascript tails are still more appealing to someone
that age.



From Achim.Zeileis at wu-wien.ac.at  Thu Jan  5 16:41:27 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 5 Jan 2006 16:41:27 +0100 (CET)
Subject: [R] Wald tests and Huberized variances (was: A comment about R:)
In-Reply-To: <43BD2522.7070201@statistik.uni-dortmund.de>
References: <43BCFB3C.8080200@statistik.uni-dortmund.de>
	<Pine.LNX.4.58.0601051212570.9039@thorin.ci.tuwien.ac.at>
	<43BD1F52.4030808@statistik.uni-dortmund.de>
	<Pine.LNX.4.58.0601051448010.9109@thorin.ci.tuwien.ac.at>
	<43BD2522.7070201@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.58.0601051616190.9109@thorin.ci.tuwien.ac.at>

On Wed, 4 Jan 2006, Peter Muhlberger wrote:

One comment in advance: please use a more meaningful subject. I would have
missed this mail if a colleague hadn't pointed me to it.

> I'm someone who from time to time comes to R to do applied stats for social
> science research.
[snip]
> I would also prefer not to have to work through a
> couple books on R or S+ to learn how to meet common needs in R.  If R were

There are some overviews and pointers available for certain topics,
so-called CRAN task views:
  http://CRAN.R-project.org/src/contrib/Views/
Currently, there is not yet a "SocialSciences" view (but John Fox is
working on one). However, it might as be interesting for you to look
at the "Econometrics" view which has some remarks about Wald tests.

> Ex. 1)  Wald tests of linear hypotheses after max. likelihood or even after
> a regression.  "Wald" does not even appear in my standard R package on a
> search.

You might want to look at waldtest() and coeftest() in package lmtest. And
you seem to have discovered linear.hypothesis() in package car. All three
perform Wald tests, providing different means of specifying the
hypothesis/alternative of the tests.

> There's no comment in the lm help or optim help about what function
> to use for hypothesis tests.

Well, the lm() man page does say:
  The functions 'summary' and 'anova' are used to obtain and print a
  summary and analysis of variance table of the results.

As for optim() it is not that straightforward, because optim() does not
know whether it maximizes a proper likelihood or not.

> I know that statisticians prefer likelihood
> ratio tests, but Wald tests are still useful and indeed crucial for
> first-pass analysis.  After searching with Google for some time, I found
> several Wald functions in various contributed R packages I did not have
> installed.  One confusion was which one would be relevant to my needs.  This
> took some time to resolve.

Yes, this is a problem that is at least partly addressed by the CRAN task
views.

> I concluded, perhaps on insufficient evidence,
> that package car's Wald test would be most helpful.  To use it, however, one
> has to put together a matrix for the hypotheses, which can be arduous for a
> many-term regression or a complex hypothesis.  In comparison, in Stata one
> simply states the hypothesis in symbolic terms.

waldtest() does the latter and is linked in the "See Also" section of
linear.hypothesis()

> I also don't know for
> certain that this function in car will work or work properly w/ various
> kinds of output, say from lm or from optim.

The man page of linear.hypothesis() does say that there are methods for
"lm" and "glm" objects (but not for results from optim).

> Ex. 2) Getting neat output of a regression with Huberized variance matrix.
> I frequently have to run regressions w/ robust variances.  In Stata, one
> simply adds the word "robust" to the end of the command or
> "cluster(cluster.variable)" for a cluster-robust error.  In R, there are two
> functions, robcov and hccm.  I had to run tests to figure out what the
> relationship is between them and between them and Stata (robcov w/o cluster
> gives hccm's hc0; hccm's hc1 is equivalent to Stata's 'robust' w/o cluster;
> etc.).

This is rather clearly document on the respective man pages. hccm()
provides HC covariance matrices without clustering, as does vcovHC() in
package sandwich. I plan to extend vcovHC() to also deal with clustered
data, but I didn't get round to do so, yet.

> A single sentence in hccm's help saying something to the effect that
> statisticians prefer hc3 for most types of data might save me from having to
> scramble through the statistical literature to try to figure out which of
> these I should be using.  A few sentences on what the differences are
> between these methods would be even better.

Yes and no. I'll add some more comments about the different HC-type
covariance matrices, but on the other hand this is just the software which
cannot replace understanding the underlying theory.

> Then, there's the problem of
> output.  Given that hc1 or hc3 are preferred for non-clustered data, I'd
> need to be able to get regression output of the form summary(lm) out of
> hccm, for any practical use.  Getting this, however, would require
> programming my own function.

Or using coeftest() from package lmtest intended particularly for this.

> Huberized t-stats for regressions are
> commonplace needs, an R oriented a little toward more everyday needs would
> not require programming of such needs.  Also, I'm not sure yet how well any
> of the existing functions handle missing data.

When fitting a linear model via lm() you can specify a suitable na.action.

The released version of lmtest and sandwich can deal with Wald tests and
sandwich covariance matrix estimators for linear models. I've got
development versions ready which make the functions fully object-oriented
and thus applicable to "glm" or "survreg" objects (for censored/tobit
regression) as well. I plan to release these soon, contact me if you want
to have a devel snapshot.

Best wishes,
Z



From hezhesi at gmail.com  Thu Jan  5 16:43:07 2006
From: hezhesi at gmail.com (Zhesi He)
Date: Thu, 5 Jan 2006 15:43:07 +0000
Subject: [R] about terminate an identification
Message-ID: <dd86cbbea00ac0da1eea0b469203a4e9@ysbl.york.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/c8d4f831/attachment.pl

From pinard at iro.umontreal.ca  Thu Jan  5 16:46:09 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 5 Jan 2006 10:46:09 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
Message-ID: <20060105154609.GA7009@phenix.sram.qc.ca>

[ronggui]

>R's week when handling large data file.  I has a data file : 807 vars,
>118519 obs.and its CVS format.  Stata can read it in in 2 minus,but In
>my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.

Just (another) thought.  I used to use SPSS, many, many years ago, on 
CDC machines, where the CPU had limited memory and no kind of paging 
architecture.  Files did not need to be very large for being too large.

SPSS had a feature that was then useful, about the capability of 
sampling a big dataset directly at file read time, quite before 
processing starts.  Maybe something similar could help in R (that is, 
instead of reading the whole data in memory, _then_ sampling it.)

One can read records from a file, up to a preset amount of them.  If the 
file happens to contain more records than that preset number (the number 
of records in the whole file is not known beforehand), already read 
records may be dropped at random and replaced by other records coming 
from the file being read.  If the random selection algorithm is properly 
chosen, it can be made so that all records in the original file have 
equal probability of being kept in the final subset.

If such a sampling facility was built right within usual R reading 
routines (triggered by an extra argument, say), it could offer 
a compromise for processing large files, and also sometimes accelerate 
computations for big problems, even when memory is not at stake.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From detlef.steuer at hsu-hamburg.de  Thu Jan  5 11:03:57 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Thu, 5 Jan 2006 11:03:57 +0100
Subject: [R] A comment about R:
In-Reply-To: <43BC2375.7040309@sciviews.org>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<43BC2375.7040309@sciviews.org>
Message-ID: <20060105110357.7723b4f3.detlef.steuer@hsu-hamburg.de>

Only or the record:

There is a wiki for R in general, used by only but a few people,  annouced here some year(s) ago:
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl

The question is: one or more wikis? 

Detlef

On Wed, 04 Jan 2006 20:35:17 +0100
Philippe Grosjean <phgrosjean at sciviews.org> wrote:

> David Forrest wrote:
>  > [...]
>  > Any volunteers?
> 
> Yes, me (well, partly...)! Here is what I propose: this is a very 
> lengthy thread in R-Help, with many interesting ideas and suggestions. I 
> fear that, as it happens too often, those nice ideas will be lost 
> because of the support used: email! By nature, emails are read and then 
> deleted (well, there is the R-Help archive, but anyway, threads in a 
> mailing list is not at all the best tool to make collaborative documents 
> like those tutorials and co).
> 
> I just cooked a little Wiki *dedicated to R beginners* (meaning they can 
> contribute too, and are very welcome to discuss their problems -possibly 
> trivial for others-). It is available at 
> http://www.sciviews.org/_rgui/wiki. For the moment, everyone can edit 
> and add pages, but I will restrict rights in the future to logged users 
> only (with everybody allowed to log in at any time). So that we will be 
> able to track who made changes (authorship).
> 
> For those who do not know the Wiki concept, it is a very simple way of 
> working together in the same documents. The concept has proven very 
> powerful with a good example being Wikipedia, that is becoming one of 
> the largest encyclopedia in the world... and also as accurate as 
> Encyclopedia Britannica (but read this: 
> http://www.nature.com/news/2005/051212/full/438900a.html).
> 
> Here is the introduction of the R (GUI) Wiki:
> 
> This Wiki is mainly dedicated to deal with R beginners problems. 
> Although we would like to emphasize using R GUIs (Graphical User 
> Interfaces), this Wiki is not restricted to those GUIs: one can also 
> deal with command-line approaches. The main idea is thus to have 
> material contributed by both beginners, and by more advanced R users, 
> that will help novices or casual users of R (http://www.r-project.org).
> 
> Overview
> 
> * The various documents in the [[wiki section]] explain how to use 
> DokuWiki to edit documents in this site.
> 
> * The [[beginners section]] is dedicated to... beginners (share 
> experience, expose problems and difficulties useful to share with other 
> beginners, or to get help from more advanced people).
> 
> * The [[tutorials section]] is the place where you can put various R 
> session examples, or short tutorials on either general or specific use of R.
> 
> * The [[easier section]] aims to collect together various pieces of R 
> code that simplifies various tasks (especially for beginners) and that 
> will ultimately be compiled in a easieR R packages on CRAN.
> 
> * The [[varia section]] is for any material that does not fit in the 
> previous sections.
> 
> 
> Final note: working with Wikis requires some learning... So, I am not 
> sure at all that many R beginners will contribute to this wiki, but, of 
> course, I hope so. Just let's pretend that it is a small experiment to 
> try answering requests for another Internet space than R-Help, 
> specifically dedicated to beginners...
> 
> A good starting point would be the following: all people that expressed 
> interesting points in this thread could "copy and paste their ideas" to 
> new pages in the Wiki.
> 
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>   ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>   ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>   ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>   ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
> 
> David Forrest wrote:
> > On Tue, 3 Jan 2006, Gabor Grothendieck wrote:
> > ...
> > 
> >>In fact there are some things that are very easy
> >>to do in Stata and can be done in R but only with more difficulty.
> >>For example, consider this introductory session in Stata:
> >>
> >>http://www.stata.com/capabilities/session.html
> >>
> >>Looking at the first few queries,
> >>see how easy it is to take the top few in Stata whereas in R one would
> >>have a complex use of order.  Its not hard in R to write a function
> >>that would make it just as easy but its not available off the top
> >>of one's head though RSiteSearch("sort.data.frame") will find one
> >>if one knew what to search for.
> > 
> > 
> > This sort of thing points to an opportunity for documentation.  Building a
> > tutorial session in R on how one would do a similar analysis would provide
> > another method of learning R.  "An Introduction to R" is a good bottom-up
> > introduction, which if you work through it does teach you how to do
> > several things.  Adapting other tutorials or extended problems, like the
> > Stata session, to R would give additional entry points.  A few end-to-end
> > tutorials on some interesting analyses would be helpful.
> > 
> > Any volunteers?
> > 
> > Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Eric.Kort at vai.org  Thu Jan  5 17:09:47 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Thu, 5 Jan 2006 11:09:47 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>

> -----Original Message-----
> 
> [ronggui]
> 
> >R's week when handling large data file.  I has a data file : 807 vars,
> >118519 obs.and its CVS format.  Stata can read it in in 2 minus,but In
> >my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.
> 
> Just (another) thought.  I used to use SPSS, many, many years ago, on
> CDC machines, where the CPU had limited memory and no kind of paging
> architecture.  Files did not need to be very large for being too large.
> 
> SPSS had a feature that was then useful, about the capability of
> sampling a big dataset directly at file read time, quite before
> processing starts.  Maybe something similar could help in R (that is,
> instead of reading the whole data in memory, _then_ sampling it.)
> 
> One can read records from a file, up to a preset amount of them.  If the
> file happens to contain more records than that preset number (the number
> of records in the whole file is not known beforehand), already read
> records may be dropped at random and replaced by other records coming
> from the file being read.  If the random selection algorithm is properly
> chosen, it can be made so that all records in the original file have
> equal probability of being kept in the final subset.
> 
> If such a sampling facility was built right within usual R reading
> routines (triggered by an extra argument, say), it could offer
> a compromise for processing large files, and also sometimes accelerate
> computations for big problems, even when memory is not at stake.
> 

Since I often work with images and other large data sets, I have been thinking about a "BLOb" (binary large object--though it wouldn't necessarily have to be binary) package for R--one that would handle I/O for such creatures and only bring as much data into the R space as was actually needed.

So I see 3 possibilities:

1. The sort of functionality you describe is implemented in the R internals (by people other than me).
2. Some individuals (perhaps myself included) write such a package.
3. This thread fizzles out and we do nothing.

I guess I will see what, if any, discussion ensues from this point to see which of these three options seems worth pursuing.

> --
> Fran??ois Pinard   http://pinard.progiciels-bpi.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
This email message, including any attachments, is for the so...{{dropped}}



From jfox at mcmaster.ca  Thu Jan  5 17:17:52 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 5 Jan 2006 11:17:52 -0500
Subject: [R] A comment about R:
In-Reply-To: <BFE18F7C.1279F%pmuhl1848@gmail.com>
Message-ID: <20060105161752.TFDO16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter 
> Muhlberger
> Sent: Wednesday, January 04, 2006 2:43 PM
> To: rhelp
> Subject: [R] A comment about R:
> 

. . .
 
> Ex. 1)  Wald tests of linear hypotheses after max. likelihood 
> or even after a regression.  "Wald" does not even appear in 
> my standard R package on a search.  There's no comment in the 
> lm help or optim help about what function to use for 
> hypothesis tests.  I know that statisticians prefer 
> likelihood ratio tests, but Wald tests are still useful and 
> indeed crucial for first-pass analysis.  After searching with 
> Google for some time, I found several Wald functions in 
> various contributed R packages I did not have installed.  One 
> confusion was which one would be relevant to my needs.  This 
> took some time to resolve.  I concluded, perhaps on 
> insufficient evidence, that package car's Wald test would be 
> most helpful.  To use it, however, one has to put together a 
> matrix for the hypotheses, which can be arduous for a 
> many-term regression or a complex hypothesis.  
> In comparison, 
> in Stata one simply states the hypothesis in symbolic terms.  
> I also don't know for certain that this function in car will 
> work or work properly w/ various kinds of output, say from lm 
> or from optim.  To be sure, I'd need to run time-consuming 
> tests comparing it with Stata output or examine the 
> function's code.  In Stata the test is easy to find, and 
> there's no uncertainty about where it can be run or its 
> accuracy.  Simply having a comment or "see also" in lm help 
> or mle or optim help pointing the user to the right Wald 
> function would be of enormous help.
> 


The reference, I believe, is to the linear.hypothesis() function, which has
methods for lm and glm objects. [To see what kinds of objects
linear.hypothesis is suitable for, use the command
methods(linear.hypothesis).] For lm objects, you get an F-test by default.
Note that the Anova() function, also in car, can more conveniently compute
Wald tests for certain kinds of hypotheses. More generally, however, I'd be
interested in your suggestions for an alternative method of specifying
linear hypotheses. There is currently no method for mle objects, but adding
one is a good idea, and I'll do that when I have a chance. (In the meantime,
it's very easy to compute Wald tests from the coefficients and the
hypothesis and coefficient-covariance matrices. Writing a small function to
do so, without the bells and whistles of something like linear.hypothesis(),
should not be hard. Indeed, the ability to do this kind of thing easily is
what I see as the primary advantage of working in a statistical computing
environment like R -- or Stata.

> Ex. 2) Getting neat output of a regression with Huberized 
> variance matrix.
> I frequently have to run regressions w/ robust variances.  In 
> Stata, one simply adds the word "robust" to the end of the 
> command or "cluster(cluster.variable)" for a cluster-robust 
> error.  In R, there are two functions, robcov and hccm.  I 
> had to run tests to figure out what the relationship is 
> between them and between them and Stata (robcov w/o cluster 
> gives hccm's hc0; hccm's hc1 is equivalent to Stata's 
> 'robust' w/o cluster; etc.).  A single sentence in hccm's 
> help saying something to the effect that statisticians prefer 
> hc3 for most types of data might save me from having to 
> scramble through the statistical literature to try to figure 
> out which of these I should be using.  A few sentences on 
> what the differences are between these methods would be even 
> better.  Then, there's the problem of output.  Given that hc1 
> or hc3 are preferred for non-clustered data, I'd need to be 
> able to get regression output of the form summary(lm) out of 
> hccm, for any practical use.  Getting this, however, would 
> require programming my own function.  Huberized t-stats for 
> regressions are commonplace needs, an R oriented a little 
> toward more everyday needs would not require programming of 
> such needs.  Also, I'm not sure yet how well any of the 
> existing functions handle missing data.
> 

I think that we have a philosophical difference here: I don't like giving
advice in documentation. An egregious extended example of this, in my
opinion, is the SPSS documentation. The hccm() function uses hc3 as the
default, which is an implicit recommendation, but more usefully, in my view,
points to Long and Erwin's American Statistician paper on the subject, which
does give advice and which is quite accessible. As well, and more generally,
the car package is associated with a book (my R and S-PLUS Companion to
Applied Regression), which gives advice, though, admittedly, tersely in this
case.

The Anova() function with argument white=TRUE will give you F-tests
corresponding to the t-tests to which you refer (though it will combine df
for multiple-df terms in the model). To get the kind of summary you
describe, you could use something like

mysummary <- function(model){
	coef <- coef(model)	
	se <- sqrt(diag(hccm(model)))
	t <- coef/se
	p <- 2*pt(abs(t), df=model$df.residual, lower=FALSE)
	table <- cbind(coef, se, t, p)
	rownames(table) <- names(coef)
	colnames(table) <- c("Estimate", "Std. Error", "t value",
"Pr(>|t|)")
	table
	}

Again, it's not time-consuming to write simple functions like this for one's
own use, and the ability to do so is a strength of R, in my view. 

I'm not sure what you mean about handling missing data: functions like
hccm(), linear.hypothesis(), and Anova() start with a model object for which
missing data have already been handled.

Regards,
 John



From Achim.Zeileis at wu-wien.ac.at  Thu Jan  5 17:27:40 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 5 Jan 2006 17:27:40 +0100 (CET)
Subject: [R] A comment about R:
In-Reply-To: <20060105161752.TFDO16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20060105161752.TFDO16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.58.0601051722290.9109@thorin.ci.tuwien.ac.at>

As John and myself seem to have written our replies in parallel, hence
I added some more clarifying remarks in this mail:

> Note that the Anova() function, also in car, can more conveniently compute
> Wald tests for certain kinds of hypotheses. More generally, however, I'd be
> interested in your suggestions for an alternative method of specifying
> linear hypotheses.

My understanding was that Peter just wants to eliminate various elements
from the terms(obj) which is what waldtest() in lmtest supports. If some
other way of specifying nested models is required, I'd also be interested
in that.

> The Anova() function with argument white=TRUE will give you F-tests
> corresponding to the t-tests to which you refer (though it will combine df
> for multiple-df terms in the model). To get the kind of summary you
> describe, you could use something like
>
> mysummary <- function(model){
> 	coef <- coef(model)
> 	se <- sqrt(diag(hccm(model)))
> 	t <- coef/se
> 	p <- 2*pt(abs(t), df=model$df.residual, lower=FALSE)
> 	table <- cbind(coef, se, t, p)
> 	rownames(table) <- names(coef)
> 	colnames(table) <- c("Estimate", "Std. Error", "t value",
> "Pr(>|t|)")
> 	table
> 	}

This is supported out of the box in coeftest() in lmtest.
Z



From lisawang at uhnres.utoronto.ca  Thu Jan  5 17:34:33 2006
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Thu, 05 Jan 2006 11:34:33 -0500
Subject: [R] what can I do to make the lines straight
Message-ID: <43BD4A99.A49C12FA@uhnres.utoronto.ca>

Hello there,

I did a few graphics but only one looks like the attached. Why the line
is not straight and how I can fix it?

thank you very much

Lisa Wang
Princess Margaret Hospital
Toronto, Canada

From Greg.Snow at intermountainmail.org  Thu Jan  5 17:37:41 2006
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Thu, 5 Jan 2006 09:37:41 -0700
Subject: [R] A comment about R
Message-ID: <07E228A5BE53C24CAD490193A7381BBB198118@LP-EXCHVS07.CO.IHC.COM>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Thursday, January 05, 2006 6:26 AM
> To: 'Patrick Burns'; John Maindonald
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] A comment about R

[snip]

> Any suggestion on how to go about getting kids that young on 
> (R) programming?

For those of us in the US look at:
http://www.amstat.org/education/index.cfm?fuseaction=adoptas

I expect that some of the other stats organizations have similar
Adopt-A-School programs.

Last year I was in my daughter's 3rd grade class helping with a party
when I noticed a large posterboard that had the heights in inches of all
the students, since I had run out of apple juice to pour and was getting
a little bored, a went over to the chalk board next to it and made a
quick stem-and-leaf plot of the data.  The teacher was interested in
what I had done and came over and had me explain the stem and leaf plot
to her (she had used the data to talk about averages (mean and median,
but not by that name) and spread (general concept, not computing
anything)).

My other daughter (6) also brought home a homework to show me where they
had been given candy hearts (it was in February) and they had colored in
boxes corresponding to the colors of their hearts to make a basic bar
graph.  I showed her how I could do the same thing on my laptop using R
(I even colored the bars to match her graph and used the symbol font
with text to put colored hearts under the bars like hers had), she was
impressed enough to make me print the graph so she could show her
teacher.

There are 2 opourtunities that I should have followed up on more, now I
just need to get things in gear and do a more formal adopting of their
school.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111



From ripley at stats.ox.ac.uk  Thu Jan  5 17:48:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jan 2006 16:48:26 +0000 (GMT)
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
Message-ID: <Pine.LNX.4.61.0601051641470.1468@gannet.stats>

Another possibility is to make use of the several DBMS interfaces already 
available for R.  It is very easy to pull in a sample from one of those, 
and surely keeping such large data files as ASCII not good practice.

One problem with Francois Pinard's suggestion (the credit has got lost) is 
that R's I/O is not line-oriented but stream-oriented.  So selecting lines 
is not particularly easy in R.  That's a deliberate design decision, given 
the DBMS interfaces.

I rather thought that using a DBMS was standard practice in the R 
community for those using large datasets: it gets discussed rather often.

On Thu, 5 Jan 2006, Kort, Eric wrote:

>> -----Original Message-----
>>
>> [ronggui]
>>
>>> R's week when handling large data file.  I has a data file : 807 vars,
>>> 118519 obs.and its CVS format.  Stata can read it in in 2 minus,but In
>>> my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.
>>
>> Just (another) thought.  I used to use SPSS, many, many years ago, on
>> CDC machines, where the CPU had limited memory and no kind of paging
>> architecture.  Files did not need to be very large for being too large.
>>
>> SPSS had a feature that was then useful, about the capability of
>> sampling a big dataset directly at file read time, quite before
>> processing starts.  Maybe something similar could help in R (that is,
>> instead of reading the whole data in memory, _then_ sampling it.)
>>
>> One can read records from a file, up to a preset amount of them.  If the
>> file happens to contain more records than that preset number (the number
>> of records in the whole file is not known beforehand), already read
>> records may be dropped at random and replaced by other records coming
>> from the file being read.  If the random selection algorithm is properly
>> chosen, it can be made so that all records in the original file have
>> equal probability of being kept in the final subset.
>>
>> If such a sampling facility was built right within usual R reading
>> routines (triggered by an extra argument, say), it could offer
>> a compromise for processing large files, and also sometimes accelerate
>> computations for big problems, even when memory is not at stake.
>>
>
> Since I often work with images and other large data sets, I have been thinking about a "BLOb" (binary large object--though it wouldn't necessarily have to be binary) package for R--one that would handle I/O for such creatures and only bring as much data into the R space as was actually needed.
>
> So I see 3 possibilities:
>
> 1. The sort of functionality you describe is implemented in the R internals (by people other than me).
> 2. Some individuals (perhaps myself included) write such a package.
> 3. This thread fizzles out and we do nothing.
>
> I guess I will see what, if any, discussion ensues from this point to see which of these three options seems worth pursuing.
>
>> --
>> Fran?ois Pinard   http://pinard.progiciels-bpi.ca

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jholtman at gmail.com  Thu Jan  5 17:59:12 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jan 2006 11:59:12 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <20060105154609.GA7009@phenix.sram.qc.ca>
References: <20060105154609.GA7009@phenix.sram.qc.ca>
Message-ID: <644e1f320601050859l6d573435q9b3004690758cfda@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/7d78b665/attachment.pl

From f.harrell at vanderbilt.edu  Thu Jan  5 18:42:59 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 05 Jan 2006 11:42:59 -0600
Subject: [R] A comment about R:
In-Reply-To: <20060105161752.TFDO16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20060105161752.TFDO16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <43BD5AA3.5040604@vanderbilt.edu>

John Fox wrote:
> Dear Peter,
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter 
>>Muhlberger
>>Sent: Wednesday, January 04, 2006 2:43 PM
>>To: rhelp
>>Subject: [R] A comment about R:
>>
> 
> 
> . . .
>  
> 
>>Ex. 1)  Wald tests of linear hypotheses after max. likelihood 
>>or even after a regression.  "Wald" does not even appear in 
>>my standard R package on a search.  There's no comment in the 
>>lm help or optim help about what function to use for 
>>hypothesis tests.  I know that statisticians prefer 
>>likelihood ratio tests, but Wald tests are still useful and 
>>indeed crucial for first-pass analysis.  After searching with 
>>Google for some time, I found several Wald functions in 
>>various contributed R packages I did not have installed.  One 
>>confusion was which one would be relevant to my needs.  This 
>>took some time to resolve.  I concluded, perhaps on 
>>insufficient evidence, that package car's Wald test would be 
>>most helpful.  To use it, however, one has to put together a 
>>matrix for the hypotheses, which can be arduous for a 
>>many-term regression or a complex hypothesis.  
>>In comparison, 
>>in Stata one simply states the hypothesis in symbolic terms.  
>>I also don't know for certain that this function in car will 
>>work or work properly w/ various kinds of output, say from lm 
>>or from optim.  To be sure, I'd need to run time-consuming 
>>tests comparing it with Stata output or examine the 
>>function's code.  In Stata the test is easy to find, and 
>>there's no uncertainty about where it can be run or its 
>>accuracy.  Simply having a comment or "see also" in lm help 
>>or mle or optim help pointing the user to the right Wald 
>>function would be of enormous help.

The Design package's anova.Design and contrast.Design make many Wald 
tests very easy.  contrast( ) will allow you to test all kinds of 
hypotheses by stating which differences in predicted values you are 
interested in.

Frank Harrell

>>
> 
> 
> 
> The reference, I believe, is to the linear.hypothesis() function, which has
> methods for lm and glm objects. [To see what kinds of objects
> linear.hypothesis is suitable for, use the command
> methods(linear.hypothesis).] For lm objects, you get an F-test by default.
> Note that the Anova() function, also in car, can more conveniently compute
> Wald tests for certain kinds of hypotheses. More generally, however, I'd be
> interested in your suggestions for an alternative method of specifying
> linear hypotheses. There is currently no method for mle objects, but adding
> one is a good idea, and I'll do that when I have a chance. (In the meantime,
> it's very easy to compute Wald tests from the coefficients and the
> hypothesis and coefficient-covariance matrices. Writing a small function to
> do so, without the bells and whistles of something like linear.hypothesis(),
> should not be hard. Indeed, the ability to do this kind of thing easily is
> what I see as the primary advantage of working in a statistical computing
> environment like R -- or Stata.
> 
> 
>>Ex. 2) Getting neat output of a regression with Huberized 
>>variance matrix.
>>I frequently have to run regressions w/ robust variances.  In 
>>Stata, one simply adds the word "robust" to the end of the 
>>command or "cluster(cluster.variable)" for a cluster-robust 
>>error.  In R, there are two functions, robcov and hccm.  I 
>>had to run tests to figure out what the relationship is 
>>between them and between them and Stata (robcov w/o cluster 
>>gives hccm's hc0; hccm's hc1 is equivalent to Stata's 
>>'robust' w/o cluster; etc.).  A single sentence in hccm's 
>>help saying something to the effect that statisticians prefer 
>>hc3 for most types of data might save me from having to 
>>scramble through the statistical literature to try to figure 
>>out which of these I should be using.  A few sentences on 
>>what the differences are between these methods would be even 
>>better.  Then, there's the problem of output.  Given that hc1 
>>or hc3 are preferred for non-clustered data, I'd need to be 
>>able to get regression output of the form summary(lm) out of 
>>hccm, for any practical use.  Getting this, however, would 
>>require programming my own function.  Huberized t-stats for 
>>regressions are commonplace needs, an R oriented a little 
>>toward more everyday needs would not require programming of 
>>such needs.  Also, I'm not sure yet how well any of the 
>>existing functions handle missing data.
>>
> 
> 
> I think that we have a philosophical difference here: I don't like giving
> advice in documentation. An egregious extended example of this, in my
> opinion, is the SPSS documentation. The hccm() function uses hc3 as the
> default, which is an implicit recommendation, but more usefully, in my view,
> points to Long and Erwin's American Statistician paper on the subject, which
> does give advice and which is quite accessible. As well, and more generally,
> the car package is associated with a book (my R and S-PLUS Companion to
> Applied Regression), which gives advice, though, admittedly, tersely in this
> case.
> 
> The Anova() function with argument white=TRUE will give you F-tests
> corresponding to the t-tests to which you refer (though it will combine df
> for multiple-df terms in the model). To get the kind of summary you
> describe, you could use something like
> 
> mysummary <- function(model){
> 	coef <- coef(model)	
> 	se <- sqrt(diag(hccm(model)))
> 	t <- coef/se
> 	p <- 2*pt(abs(t), df=model$df.residual, lower=FALSE)
> 	table <- cbind(coef, se, t, p)
> 	rownames(table) <- names(coef)
> 	colnames(table) <- c("Estimate", "Std. Error", "t value",
> "Pr(>|t|)")
> 	table
> 	}
> 
> Again, it's not time-consuming to write simple functions like this for one's
> own use, and the ability to do so is a strength of R, in my view. 
> 
> I'm not sure what you mean about handling missing data: functions like
> hccm(), linear.hypothesis(), and Anova() start with a model object for which
> missing data have already been handled.
> 
> Regards,
>  John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From pmuhl1848 at gmail.com  Thu Jan  5 18:43:53 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Thu, 05 Jan 2006 12:43:53 -0500
Subject: [R] Wald tests and Huberized variances (was: A comment about R:)
In-Reply-To: <Pine.LNX.4.58.0601051616190.9109@thorin.ci.tuwien.ac.at>
Message-ID: <BFE2C509.127CB%pmuhl1848@gmail.com>

Hi Achim:  Your reply is tremendously helpful in addressing some of the
outstanding questions I had about R.  The 'econometrics view' materials look
exactly like what I needed.  Many thanks!

But, there is a second point here, which is how difficult it was for me, as
someone just becoming more familiar w/ R's more basic capabilities (in the
past I've focused on features like optim, sem), to find what seem to me like
standard & key features I've taken for granted in other packages.  I looked
high & low in my existing installed packages for the standard version of R,
I googled, I looked in the r-help archives, I looked through several manuals
/ introductions to R I had downloaded.  I've asked questions about all of
the points I raised in my email on this email list before.  I believe I
passed through the parent directory for the econometric view material at the
website w/o realizing what it contained because I thought of "computational
econometrics" as having to do w/ running Monte Carlo models of economic
processes.

If R wants to bring in a wider audience, one thing that might help is a
denser set of cross-references.  For example, perhaps lm's help should
mention the econometrics view materials as well as other places to look for
tests and procedures people may want to do w/ lm.  Another thought is that
perhaps the standard R package help should allow people to find
non-installed but commonly used contributed packages and perhaps their help
page contents.  A feature that would be very helpful for me is the capacity
to search all the contents of help files, not just keywords that at times
seem to miss what I'm trying to find.

Cheers,

Peter



From ronggui.huang at gmail.com  Thu Jan  5 18:48:09 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 6 Jan 2006 01:48:09 +0800
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <644e1f320601050859l6d573435q9b3004690758cfda@mail.gmail.com>
References: <20060105154609.GA7009@phenix.sram.qc.ca>
	<644e1f320601050859l6d573435q9b3004690758cfda@mail.gmail.com>
Message-ID: <38b9f0350601050948n75f9e7abs@mail.gmail.com>

2006/1/6, jim holtman <jholtman at gmail.com>:
> If what you are reading in is numeric data, then it would require (807 *
> 118519 * 8) 760MB just to store a single copy of the object -- more memory
> than you have on your computer.  If you were reading it in, then the problem
> is the paging that was occurring.
In fact,If I read it in 3 pieces, each is about 170M.

>
> You have to look at storing this in a database and working on a subset of
> the data.  Do you really need to have all 807 variables in memory at the
> same time?

Yip,I don't need all the variables.But I don't know how to get the
necessary  variables into R.

At last I  read the data in piece and use RSQLite package to write it
to a database.and do then do the analysis. If i am familiar with
database software, using database (and R) is the best choice,but
convert the file into database format is not an easy job for me.I ask
for help in SQLite list,but the solution is not satisfying as that
required the knowledge about the third script language.After searching
the internet,I get this solution:

#begin
rm(list=ls())
f<-file("D:\wvsevs_sb_v4.csv","r")
i <- 0
done <- FALSE
library(RSQLite)
con<-dbConnect("SQLite","c:\sqlite\database.db3")
tim1<-Sys.time()

while(!done){
i<-i+1
tt<-readLines(f,2500)
if (length(tt)<2500) done <- TRUE
tt<-textConnection(tt)
if (i==1) {
           assign("dat",read.table(tt,head=T,sep=",",quote=""));
         }
else assign("dat",read.table(tt,head=F,sep=",",quote=""))
close(tt)
ifelse(dbExistsTable(con, "wvs"),dbWriteTable(con,"wvs",dat,append=T),
  dbWriteTable(con,"wvs",dat) )
}
close(f)
#end
It's not the best solution,but it works.



> If you use 'scan', you could specify that you do not want some of the
> variables read in so it might make a more reasonably sized objects.
>
>
> On 1/5/06, Franois Pinard <pinard at iro.umontreal.ca> wrote:
> > [ronggui]
> >
> > >R's week when handling large data file.  I has a data file : 807 vars,
> > >118519 obs.and its CVS format.  Stata can read it in in 2 minus,but In
> > >my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.
> >
> > Just (another) thought.  I used to use SPSS, many, many years ago, on
> > CDC machines, where the CPU had limited memory and no kind of paging
> > architecture.  Files did not need to be very large for being too large.
> >
> > SPSS had a feature that was then useful, about the capability of
> > sampling a big dataset directly at file read time, quite before
> > processing starts.  Maybe something similar could help in R (that is,
> > instead of reading the whole data in memory, _then_ sampling it.)
> >
> > One can read records from a file, up to a preset amount of them.  If the
> > file happens to contain more records than that preset number (the number
> > of records in the whole file is not known beforehand), already read
> > records may be dropped at random and replaced by other records coming
> > from the file being read.  If the random selection algorithm is properly
> > chosen, it can be made so that all records in the original file have
> > equal probability of being kept in the final subset.
> >
> > If such a sampling facility was built right within usual R reading
> > routines (triggered by an extra argument, say), it could offer
> > a compromise for processing large files, and also sometimes accelerate
> > computations for big problems, even when memory is not at stake.
> >
> > --
> > Franois Pinard   http://pinard.progiciels-bpi.ca
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?


--

Deparment of Sociology
Fudan University



From dgoliche at sclc.ecosur.mx  Thu Jan  5 19:11:27 2006
From: dgoliche at sclc.ecosur.mx (Duncan Golicher)
Date: Thu, 05 Jan 2006 12:11:27 -0600
Subject: [R] Replacing backslashes with slashes
Message-ID: <43BD614F.8090707@sclc.ecosur.mx>

It  is often convenient to quickly set the working directory to a path 
copied onto the windows clipboard. A simple trick I have been using for 
a while is along the lines given in the previous posts.

setwd.clip<-function()
{
  options(warn=-1)
  setwd(gsub("\\\\","/",readLines("clipboard")))
  options(warn=0)
  getwd()
}


I load this at the start of every session and then write setwd.clip() 
whenever I have a path I want to change to on the clipboard. You can of 
course write

setwd(gsub("\\\\","/",readLines("clipboard")))

everytime you need it. Obviously it takes longer and there is the minor 
detail that the path read from the clipboard is incomplete (no EOL 
marker) which leads to an unnecessary warning.


Dr Duncan Golicher
Ecologia y Sistematica Terrestre
Conservaci??n de la Biodiversidad
El Colegio de la Frontera Sur
San Cristobal de Las Casas, 
Chiapas, Mexico

Email: dgoliche at sclc.ecosur.mx 

Tel: 967 674 9000 ext 1310
Fax: 967 678 2322
Celular: 044 9671041021

United Kingdom Skypein; 020 7870 6251
Skype name: duncangolicher 
Download Skype from http://www.skype.com



From rbaer at atsu.edu  Thu Jan  5 19:21:20 2006
From: rbaer at atsu.edu (Robert Baer)
Date: Thu, 5 Jan 2006 12:21:20 -0600
Subject: [R] ylim problem in barplot
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
	<17341.10492.905351.841579@stat.math.ethz.ch>
Message-ID: <014101c61224$d3b75d50$a00c010a@BigBaer>

>     PaulB> When I use barplot but select a ylim value greater
>     PaulB> than zero, the graph is distorted.  The bars extend
>     PaulB> below the bottom of the graph.
>     PaulB> For instance the command produces a problematic graph.
>     PaulB> barplot(c(200,300,250,350),ylim=c(150,400))

> Well, my question would be if that is not a feature :-)
> Many people would consider barplots that do not start at 0 as
>  "Cheating with Graphics"  (in the vein of "Lying with Statistics").

Well, consider this example:
barplot(c(-200,300,-250,350),ylim=c(-99,400))

It seems that barplot uses ylim and pretty to decide things about the axis
but does some slightly unexpected things with the bars themselves that are
not just at the 'zero' end of the bar.

Rob

____________________________
Robert W. Baer, Ph.D.
Associate Professor
Department of Physiology
A. T. Still University of Health Science
800 W. Jefferson St.
Kirksville, MO 63501-1497 USA



From pmuhl1848 at gmail.com  Thu Jan  5 19:36:47 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Thu, 05 Jan 2006 13:36:47 -0500
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.58.0601051722290.9109@thorin.ci.tuwien.ac.at>
Message-ID: <BFE2D16F.127DD%pmuhl1848@gmail.com>

On 1/5/06 11:27 AM, "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at> wrote:

> As John and myself seem to have written our replies in parallel, hence
> I added some more clarifying remarks in this mail:
>> Note that the Anova() function, also in car, can more conveniently compute
>> Wald tests for certain kinds of hypotheses. More generally, however, I'd be
>> interested in your suggestions for an alternative method of specifying
>> linear hypotheses.
> My understanding was that Peter just wants to eliminate various elements
> from the terms(obj) which is what waldtest() in lmtest supports. If some
> other way of specifying nested models is required, I'd also be interested
> in that.


My two most immediate problems were a) to test whether a set of coefficients
were jointly zero (as Achim suggests, though the complication here is that
the varcov matrix is bootstrapped), but also b) to test whether the average
of a set of coefficients was equal to zero.  At other points in time, I
remember having had to test more complex linear hypotheses involving joint
combinations of equality, non-zero, and 'averages.'  The Stata interface for
linear hypothesis tests is amazingly straightforward.  For example, after a
regression, I could use the following to test the joint hypothesis that
v1=v2 and the average (or sum) of v3 through v5 is zero and .75v6+.25v7 is
zero:

test v1=v2
test v3+v4+v5=0, accum
test .75*v6+.25*v7=0, accum

I don't even have to set up a matrix for my test ];-) !  The output would
show not merely the joint test of all the hypotheses but the tests along the
way, one for each line of commands.  I vaguely remember the hypothesis
testing command after an ml run is much the same and cross-equation
hypothesis tests simply involve adding an equation indicator to the terms.
I can get huberized var-cov matrices simply by adding "robust" to the
regression command.  I believe there's also a command that will huberize a
var-cov matrix after the fact.  Subsequent hypothesis tests would be on the
huberized matrix.

I won't claim to know what's good for R or the R community, but it would be
nice for me and perhaps others if there were a comparable straightforward
command as in Stata that could meet a variety of needs.  I need to play w/
the commands that have been suggested to me by you guys recently, but I'm
looking at a multitude of commands none of which I suspect have the
flexibility and ease of use of the above Stata commands, at least for the
kind of applications I'd like.  Perhaps the point of R isn't to serve as a
package for a wider set of non-statisticians, but if it wishes to develop in
that direction, facilities like this may be helpful.  It's interesting that
Achim points out that a function John suggests is already available in R--an
indication that even R experts don't have a complete handle on everything in
R even on a relatively straightforward topic like hypothesis tests.

John is no doubt right that editorializing about statistics would be out of
place on an R help page.  But when I have gone to statistical papers, many
have been difficult to access & not very helpful for practical concerns.
I'm glad to hear that Long and Erwin's paper is helpful, but there's a
goodly list of papers mentioned in help.  Perhaps something that would be
useful is some way of highlighting on a help page which reference is most
helpful for practical concerns?

Again, thanks for all the great input from everyone!

Peter



From gunter.berton at gene.com  Thu Jan  5 20:11:40 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 Jan 2006 11:11:40 -0800
Subject: [R] Setting the working directory on Windows (was: Replacing
	backslashes with slashes)
In-Reply-To: <43BD614F.8090707@sclc.ecosur.mx>
Message-ID: <200601051911.k05JBUOu012402@meitner.gene.com>

In a similar vein, a GUI version is:

setwd(dirname(choose.files()))

This gives you a standard Windows file browser -- you just click on any file
in the directory you want to set. Obviously, dirname(choose.files()) is an
easy interactive way to get directories as strings if you need them. See
also ?basename . 


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Golicher
> Sent: Thursday, January 05, 2006 10:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Replacing backslashes with slashes
> 
> It  is often convenient to quickly set the working directory 
> to a path 
> copied onto the windows clipboard. A simple trick I have been 
> using for 
> a while is along the lines given in the previous posts.
> 
> setwd.clip<-function()
> {
>   options(warn=-1)
>   setwd(gsub("\\\\","/",readLines("clipboard")))
>   options(warn=0)
>   getwd()
> }
> 
> 
> I load this at the start of every session and then write setwd.clip() 
> whenever I have a path I want to change to on the clipboard. 
> You can of 
> course write
> 
> setwd(gsub("\\\\","/",readLines("clipboard")))
> 
> everytime you need it. Obviously it takes longer and there is 
> the minor 
> detail that the path read from the clipboard is incomplete (no EOL 
> marker) which leads to an unnecessary warning.
> 
> 
> Dr Duncan Golicher
> Ecologia y Sistematica Terrestre
> Conservaci??n de la Biodiversidad
> El Colegio de la Frontera Sur
> San Cristobal de Las Casas, 
> Chiapas, Mexico
> 
> Email: dgoliche at sclc.ecosur.mx 
> 
> Tel: 967 674 9000 ext 1310
> Fax: 967 678 2322
> Celular: 044 9671041021
> 
> United Kingdom Skypein; 020 7870 6251
> Skype name: duncangolicher 
> Download Skype from http://www.skype.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bolker at ufl.edu  Thu Jan  5 20:21:48 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 5 Jan 2006 19:21:48 +0000 (UTC)
Subject: [R] ylim problem in barplot
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
	<17341.10492.905351.841579@stat.math.ethz.ch>
	<014101c61224$d3b75d50$a00c010a@BigBaer>
Message-ID: <loom.20060105T201951-100@post.gmane.org>

Robert Baer <rbaer <at> atsu.edu> writes:


> Well, consider this example:
> barplot(c(-200,300,-250,350),ylim=c(-99,400))
> 
> It seems that barplot uses ylim and pretty to decide things about the axis
> but does some slightly unexpected things with the bars themselves that are
> not just at the 'zero' end of the bar.
> 
> Rob
> 

  in previous cases I think there was room for debate about
the appropriate behavior.  What do you think should happen
in this case?  Cutting off the bars seems like the right thing
to do; is your point that the axis being confined to positive
values (a side effect of setting ylim) is weird?

  Ben



From murdoch at stats.uwo.ca  Thu Jan  5 20:48:09 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Jan 2006 14:48:09 -0500
Subject: [R] Setting the working directory on Windows
In-Reply-To: <200601051911.k05JBUOu012402@meitner.gene.com>
References: <200601051911.k05JBUOu012402@meitner.gene.com>
Message-ID: <43BD77F9.2040306@stats.uwo.ca>

On 1/5/2006 2:11 PM, Berton Gunter wrote:
> In a similar vein, a GUI version is:
> 
> setwd(dirname(choose.files()))
> 
> This gives you a standard Windows file browser -- you just click on any file
> in the directory you want to set. Obviously, dirname(choose.files()) is an
> easy interactive way to get directories as strings if you need them. See
> also ?basename . 
> 

Brian Ripley added a function choose.dir() to R-devel, which will give 
you access to the code in the File|Change dir... menu item in R code. 
Won't be out for a few more months, though.

Duncan Murdoch



From patrick.giraudoux at univ-fcomte.fr  Thu Jan  5 20:49:02 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Thu, 05 Jan 2006 20:49:02 +0100
Subject: [R] Memory limitation in GeoR - Windows or R?
In-Reply-To: <43BD6D44.1040902@gspia.pitt.edu>
References: <43BD6D44.1040902@gspia.pitt.edu>
Message-ID: <43BD782E.6020307@univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/ccb0df5d/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Thu Jan  5 20:52:07 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 5 Jan 2006 20:52:07 +0100 (CET)
Subject: [R] Wald tests and Huberized variances (was: A comment about R:)
In-Reply-To: <BFE2C509.127CB%pmuhl1848@gmail.com>
References: <BFE2C509.127CB%pmuhl1848@gmail.com>
Message-ID: <Pine.LNX.4.58.0601052034010.12589@thorin.ci.tuwien.ac.at>

Peter:

> If R wants to bring in a wider audience, one thing that might help is a
> denser set of cross-references.  For example, perhaps lm's help should
> mention the econometrics view materials as well as other places to look for
> tests and procedures people may want to do w/ lm.  Another thought is that

This is difficult because the core development team has to ensure a
certain stability of the system and you wouldn't start cross-linking to
potentially unstable contributed packages. Furthermore, what is obvious to
you (or me) as further desired functionality for linear models might be
completely counter-intuitive for someone in genomics or biostatistics or
environmetrics or ... and you can't link to all of these without confusing
everybody.

> perhaps the standard R package help should allow people to find
> non-installed but commonly used contributed packages and perhaps their help
> page contents.  A feature that would be very helpful for me is the capacity
> to search all the contents of help files, not just keywords that at times
> seem to miss what I'm trying to find.

This is surely desirable but unfortunately not that simple to implement,
you'll find some discussion in the list archives about this. However,
there are various very helpful search facilites like RSiteSearch().

Best,
Z



From helprhelp at gmail.com  Thu Jan  5 21:01:28 2006
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 5 Jan 2006 14:01:28 -0600
Subject: [R] Looking for packages to do Feature Selection and
	Classification
In-Reply-To: <BE216486E4154040BF783AE5DAAA3ED401C2D1D9@SRVEXCH1.cnio.es>
References: <BE216486E4154040BF783AE5DAAA3ED401C2D1D9@SRVEXCH1.cnio.es>
Message-ID: <cdf817830601051201s50b69314w5aa3c2c331c0cd76@mail.gmail.com>

FYI:

check the following paper on svm (using libsvm) as well as random
forest in the context of feature selection.

http://www.csie.ntu.edu.tw/~cjlin/papers/features.pdf

HTH

On 1/4/06, Diaz.Ramon <rdiaz at cnio.es> wrote:
> Dear Frank,
> I expect you'll get many different answers since a wide variety of approaches have been suggested. So I'll stick to self-advertisment: I've written an R package, varSelRF (available from R), that uses random forest together with a simple variable selection approach, and provides also bootstrap estimates of the error rate of the procedure. Andy Liaw and collaborators previously developed and published a somewhat similar procedure. You probably also want to take a look at several packages available from BioConductor.
>
> Best,
>
> R.
>
>
> -----Original Message-----
> From:   r-help-bounces at stat.math.ethz.ch on behalf of Frank Duan
> Sent:   Wed 1/4/2006 4:23 AM
> To:     r-help
> Cc:
> Subject:        [R] Looking for packages to do Feature Selection and Classification
>
> Hi All,
>
> Sorry if this is a repost (a quick browse didn't give me the answer).
>
> I wonder if there are packages that can do the feature selection and
> classification at the same time. For instance, I am using SVM to classify my
> samples, but it's easy to get overfitted if using all of the features. Thus,
> it is necessary to select "good" features to build an optimum hyperplane
> (?). Here is a simple example: Suppose I have 100 "useful" features and 100
> "useless" features (or noise features), I want the SVM to give me the
> same results when 1) using only 100 useful features or 2) using all 200
> features.
>
> Any suggestions or point me to a reference?
>
> Thanks in advance!
>
> Frank
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> --
> Ram??n D??az-Uriarte
> Bioinformatics Unit
> Centro Nacional de Investigaciones Oncol??gicas (CNIO)
> (Spanish National Cancer Center)
> Melchor Fern??ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
>
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
>
>
>
> **NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From Achim.Zeileis at wu-wien.ac.at  Thu Jan  5 21:13:46 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 5 Jan 2006 21:13:46 +0100 (CET)
Subject: [R] A comment about R:
In-Reply-To: <BFE2D16F.127DD%pmuhl1848@gmail.com>
References: <BFE2D16F.127DD%pmuhl1848@gmail.com>
Message-ID: <Pine.LNX.4.58.0601052052270.12589@thorin.ci.tuwien.ac.at>

Peter:

> My two most immediate problems were a) to test whether a set of coefficients
> were jointly zero (as Achim suggests, though the complication here is that
> the varcov matrix is bootstrapped), but also b) to test whether the average

This can be tested with both waldtest() and linear.hypothesis() when
you've got the bootstrapped vcov estimator of your choice available. This
can be conveniently plugged into both functions (either as a vcov matrix
or as a function extracting the vcov matrix from the fitted model object).
There is some discussion about this in the vignette accompanying the
sandwich package.

> of a set of coefficients was equal to zero.  At other points in time, I
> remember having had to test more complex linear hypotheses involving joint
> combinations of equality, non-zero, and 'averages.'  The Stata interface for
> linear hypothesis tests is amazingly straightforward.  For example, after a
> regression, I could use the following to test the joint hypothesis that
> v1=v2 and the average (or sum) of v3 through v5 is zero and .75v6+.25v7 is
> zero:
>
> test v1=v2
> test v3+v4+v5=0, accum
> test .75*v6+.25*v7=0, accum

Mmmh, should be possible to derive the restriction matrix from this
together with the terms structure...I'll think about this.

> I don't even have to set up a matrix for my test ];-) !  The output would
> show not merely the joint test of all the hypotheses but the tests along the
> way, one for each line of commands.  I vaguely remember the hypothesis
> testing command after an ml run is much the same and cross-equation
> hypothesis tests simply involve adding an equation indicator to the terms.
> I can get huberized var-cov matrices simply by adding "robust" to the
> regression command.

Whether you find this simple or not depends on what you might want to
have. Personally, I always find it very limiting if I've only got a switch
to choose one or another vcov matrix when there is a multitude of vcov
matrices in use in the literature. What if you would want to do HC3
instead of the HC(0) that is offered by Eviews...or HC4...or HAC...or
something bootstrapped...or...
In my view, this is the stengths of many implementation in R: you can make
programs very modular so that the user can easily extend the software or
re-use it for other purposes. The price you pay for that is that it is not
as easy to as a point-and-click software that offers some standard tools.
Of course, both sides have advantages or disadvantages.

> I won't claim to know what's good for R or the R community, but it would be
> nice for me and perhaps others if there were a comparable straightforward
> command as in Stata that could meet a variety of needs.  I need to play w/
> the commands that have been suggested to me by you guys recently, but I'm
> looking at a multitude of commands none of which I suspect have the
> flexibility and ease of use of the above Stata commands, at least for the
> kind of applications I'd like.  Perhaps the point of R isn't to serve as a
> package for a wider set of non-statisticians, but if it wishes to develop in
> that direction, facilities like this may be helpful.

The point of R is hard to determine, R itself does not wish this or that,
it is an open source project which is driven by many contributors. If
there are people out there that want to use R for social sciences, they
are free to contribute to the project. And in this particular case, I
think that there has been some activity in the last one or two years
aiming at providing tools for econometrics, quantitative methods in the
social and political sciences.
However, you won't be very happy with R when you want R to be Stata. If
you want Stata, use it.

> It's interesting that
> Achim points out that a function John suggests is already available in R--an
> indication that even R experts don't have a complete handle on everything in
> R even on a relatively straightforward topic like hypothesis tests.

In fairness to John, this functionality became available rather recently.
And it's not surprising that John knows his car package better and that
I'm more familiar with my lmtest package. Therefore, it's very natural to
think first how you would do a certain task using your own package...in
particular given that you specifically asked about car.

> John is no doubt right that editorializing about statistics would be out of
> place on an R help page.  But when I have gone to statistical papers, many
> have been difficult to access & not very helpful for practical concerns.
> I'm glad to hear that Long and Erwin's paper is helpful, but there's a
> goodly list of papers mentioned in help.

I would think this to be an advantage not a drawback. It's the user's
responsiblity to know what he/she is doing.

Best wishes,
Z



From br44114 at gmail.com  Thu Jan  5 21:26:51 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 5 Jan 2006 15:26:51 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
Message-ID: <8d5a36350601051226g3ed720b2md99fdca1fb478235@mail.gmail.com>

ronggui wrote:
> If i am familiar with
> database software, using database (and R) is the best choice,but
> convert the file into database format is not an easy job for me.

Good working knowledge of a DBMS is almost invaluable when it comes to
working with very large data sets. In addition, learning SQL is piece
of cake compared to learning R. On top of that, knowledge of another
(SQL) scripting language is not needed (except perhaps for special
tasks): you can easily use R to generate the SQL syntax to import and
work with arbitrarily wide tables. (I'm not familiar with SQLite, but
MySQL comes with a command line tool that can run syntax files.)
Better start learning SQL today.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> Sent: Thursday, January 05, 2006 12:48 PM
> To: jim holtman
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Suggestion for big files [was: Re: A comment
> about R:]
>
>
> 2006/1/6, jim holtman <jholtman at gmail.com>:
> > If what you are reading in is numeric data, then it would
> require (807 *
> > 118519 * 8) 760MB just to store a single copy of the object
> -- more memory
> > than you have on your computer.  If you were reading it in,
> then the problem
> > is the paging that was occurring.
> In fact,If I read it in 3 pieces, each is about 170M.
>
> >
> > You have to look at storing this in a database and working
> on a subset of
> > the data.  Do you really need to have all 807 variables in
> memory at the
> > same time?
>
> Yip,I don't need all the variables.But I don't know how to get the
> necessary  variables into R.
>
> At last I  read the data in piece and use RSQLite package to write it
> to a database.and do then do the analysis. If i am familiar with
> database software, using database (and R) is the best choice,but
> convert the file into database format is not an easy job for me.I ask
> for help in SQLite list,but the solution is not satisfying as that
> required the knowledge about the third script language.After searching
> the internet,I get this solution:
>
> #begin
> rm(list=ls())
> f<-file("D:\wvsevs_sb_v4.csv","r")
> i <- 0
> done <- FALSE
> library(RSQLite)
> con<-dbConnect("SQLite","c:\sqlite\database.db3")
> tim1<-Sys.time()
>
> while(!done){
> i<-i+1
> tt<-readLines(f,2500)
> if (length(tt)<2500) done <- TRUE
> tt<-textConnection(tt)
> if (i==1) {
>            assign("dat",read.table(tt,head=T,sep=",",quote=""));
>          }
> else assign("dat",read.table(tt,head=F,sep=",",quote=""))
> close(tt)
> ifelse(dbExistsTable(con, "wvs"),dbWriteTable(con,"wvs",dat,append=T),
>   dbWriteTable(con,"wvs",dat) )
> }
> close(f)
> #end
> It's not the best solution,but it works.
>
>
>
> > If you use 'scan', you could specify that you do not want
> some of the
> > variables read in so it might make a more reasonably sized objects.
> >
> >
> > On 1/5/06, Franois Pinard <pinard at iro.umontreal.ca> wrote:
> > > [ronggui]
> > >
> > > >R's week when handling large data file.  I has a data
> file : 807 vars,
> > > >118519 obs.and its CVS format.  Stata can read it in in
> 2 minus,but In
> > > >my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.
> > >
> > > Just (another) thought.  I used to use SPSS, many, many
> years ago, on
> > > CDC machines, where the CPU had limited memory and no
> kind of paging
> > > architecture.  Files did not need to be very large for
> being too large.
> > >
> > > SPSS had a feature that was then useful, about the capability of
> > > sampling a big dataset directly at file read time, quite before
> > > processing starts.  Maybe something similar could help in
> R (that is,
> > > instead of reading the whole data in memory, _then_ sampling it.)
> > >
> > > One can read records from a file, up to a preset amount
> of them.  If the
> > > file happens to contain more records than that preset
> number (the number
> > > of records in the whole file is not known beforehand),
> already read
> > > records may be dropped at random and replaced by other
> records coming
> > > from the file being read.  If the random selection
> algorithm is properly
> > > chosen, it can be made so that all records in the
> original file have
> > > equal probability of being kept in the final subset.
> > >
> > > If such a sampling facility was built right within usual R reading
> > > routines (triggered by an extra argument, say), it could offer
> > > a compromise for processing large files, and also
> sometimes accelerate
> > > computations for big problems, even when memory is not at stake.
> > >
> > > --
> > > Franois Pinard   http://pinard.progiciels-bpi.ca
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 247 0281
> >
> > What the problem you are trying to solve?
>
>
> --
> 
> Deparment of Sociology
> Fudan University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From daniel.metzler at gmx.net  Thu Jan  5 21:36:40 2006
From: daniel.metzler at gmx.net (Daniel Metzler)
Date: Thu, 5 Jan 2006 21:36:40 +0100
Subject: [R] build boot object for boot.ci
Message-ID: <0538871C-A5A5-4BAA-8AC0-9A20B9BA58BC@gmx.net>

Hello all,

I have a dataset with several variables v1...vn and five groups f1...fn.
For each group I took a subset of the data

f1<-subset(data,f==1)
f2<-subset(data,f==2)

and bootstrapped the weighted mean for v1...vn, which worked
nicely.

data.boot<-boot(f1$v1,mymean,R=2000)

Afterwards I combined all boot.out$t for each group:

boot$f1v1<-data.boot$t

data.boot<-boot(f1$v2,mymean,R=2000)
boot$f1v2<-data.boot$t
...
data.boot<-boot(f2$v1,mymean,R=2000)
boot$f2v1<-data.boot$t
...

Each row of boot is used as input in a model resulting in boot$W.

My idea is to find (bootstrap) CIs for W.
Sorry, I'm new to bootstrap (and R...): Does it make sense to treat  
the Ws like a bootstrapped variable (and calculate e.g. BCa CIs),  
because only v1..vn
are bootstrapped? If yes, how is a boot object build on the basis of  
boot$W?

I'm aware that boot() handles strata, weights and complex functions,  
but I didn't manage to get my model bootstrapped right. Hopefully, I  
can circumvent turning my
script upside down.


Daniel



From epurdom at stanford.edu  Thu Jan  5 21:44:15 2006
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Thu, 05 Jan 2006 12:44:15 -0800
Subject: [R] Generic Functions
Message-ID: <6.1.2.0.2.20060105114548.04412ac0@epurdom.pobox.stanford.edu>

Hi,
I've been using the package "graph" in the BioConductor assortment and 
writing some functions based on its classes. My question is not specific to 
this package or BioConductor (I think), but it serves as a useful example. 
I recently wanted to look at the code for the function "edgeMatrix" for the 
class "graphNEL".

Usually I would type
	> func.foo
and the code for the function func for class foo would appear (where func 
and foo are edgeMatrix and graphNEL respectively). Similarly I would type
	> methods(func)
to see for what classes the function func is defined.

However, these do not work for these functions (they are not S3 functions I 
am told, though I don't know what that means). After a great deal of 
guessing and help.search requests, I finally found functions that seem to work:
	> getMethod(func,"foo")
	> showMethods(func)
I get the corresponding code and possible methods available.

What is this about and is there a section in the R language definition that 
explains the difference?

Similarly I'm use to the object oriented program described in the R 
language online based on the command UseMethod:
	> func <- function(x, ...) UseMethod("func", x)
Under this system, I could just create a function "func.foo" and it would 
work for my class "foo" -- most notably I would create a print command 
"print.foo" and it would just seamlessly work. However my function 
"print.graphNEL", for example, never worked and I'm just now guessing from 
the pieces of documentation from R, that I have to set it up differently 
but it is not clear to me how. How can I add a method to an existing 
function under this setup?

On another note: even if these different functions are internally quite 
different, can't the functions everyone is already accustomed to be made to 
access the properties, rather than creating new, similar functions? (what 
is the need for a different function "showMethods" when a function 
"methods" already exists? I have the same issue for slots where I have to 
use a function "slotNames" rather than the more commonly known function 
"names"). It becomes such a learning curve, that I shy away from packages 
that use new techniques in coding and stick with packages and functions I 
can comfortably dissect and personalize.

Thank you for any assistance,
Elizabeth Purdom



From pmuhl1848 at gmail.com  Thu Jan  5 22:10:44 2006
From: pmuhl1848 at gmail.com (Peter Muhlberger)
Date: Thu, 05 Jan 2006 16:10:44 -0500
Subject: [R] Wald tests and Huberized variances (was: A comment about	R:)
In-Reply-To: <Pine.LNX.4.58.0601052034010.12589@thorin.ci.tuwien.ac.at>
Message-ID: <BFE2F584.127E5%pmuhl1848@gmail.com>

Thanks Z, it's coming more into focus.  I don't know what would work, though
maybe it's not impossible to have a richer set of cross-references by
interest area--e.g. People interested in econometrics may wish to
examine....  The views help in this regard, tho something in help itself
would be handy.

Peter

On 1/5/06 2:52 PM, "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at> wrote:

> Peter:
> 
>> If R wants to bring in a wider audience, one thing that might help is a
>> denser set of cross-references.  For example, perhaps lm's help should
>> mention the econometrics view materials as well as other places to look for
>> tests and procedures people may want to do w/ lm.  Another thought is that
> 
> This is difficult because the core development team has to ensure a
> certain stability of the system and you wouldn't start cross-linking to
> potentially unstable contributed packages. Furthermore, what is obvious to
> you (or me) as further desired functionality for linear models might be
> completely counter-intuitive for someone in genomics or biostatistics or
> environmetrics or ... and you can't link to all of these without confusing
> everybody.
> 
>> perhaps the standard R package help should allow people to find
>> non-installed but commonly used contributed packages and perhaps their help
>> page contents.  A feature that would be very helpful for me is the capacity
>> to search all the contents of help files, not just keywords that at times
>> seem to miss what I'm trying to find.
> 
> This is surely desirable but unfortunately not that simple to implement,
> you'll find some discussion in the list archives about this. However,
> there are various very helpful search facilites like RSiteSearch().
> 
> Best,



From med at aghmed.fsnet.co.uk  Thu Jan  5 21:57:16 2006
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 05 Jan 2006 20:57:16 +0000
Subject: [R] How to create a correlation table for categorical  data???
In-Reply-To: <20060104190753.61631.qmail@web26309.mail.ukl.yahoo.com>
References: <20060104190753.61631.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <6.2.1.2.0.20060105205609.028fceb0@pop.freeserve.net>

At 19:07 04/01/06, fabio crepaldi wrote:
>Hi,
>   I need to create the correlation table of a set of categorical data 
> (sex, marital_status, car_type, etc.) for a given population.
>   Basically what I'm looking for is a function like cor( ) working on 
> factors (and, if possible, considering NAs as a level).

If they are ordered have you considered downloading the polycor package?


Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html



From info at aghmed.fsnet.co.uk  Thu Jan  5 22:08:22 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 05 Jan 2006 21:08:22 +0000
Subject: [R] How do I get sub to insert a single backslash?
Message-ID: <6.2.1.2.0.20060105210359.02902eb0@pop.freeserve.net>

Something about the way R processes backslashes is defeating me.
Perhaps this is because I have only just started using R for text processing.

I would like to change occurrences of the ampersand & into ampersand 
preceded by a backslash.

 > temp <- "R & D"
 > sub("&", "\&", temp)
[1] "R & D"
 > sub("&", "\\&", temp)
[1] "R & D"
 > sub("&", "\\\&", temp)
[1] "R & D"
 > sub("&", "\\\\&", temp)
[1] "R \\& D"
 >

So I can get zero, or two backslashes, but not one. I am sure this is 
really simple but I did not find the answer by doing, for example ?regexp 
or ?Quotes


Michael Dewey
http://www.aghmed.fsnet.co.uk



From p.dalgaard at biostat.ku.dk  Thu Jan  5 22:26:18 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jan 2006 22:26:18 +0100
Subject: [R] How do I get sub to insert a single backslash?
In-Reply-To: <6.2.1.2.0.20060105210359.02902eb0@pop.freeserve.net>
References: <6.2.1.2.0.20060105210359.02902eb0@pop.freeserve.net>
Message-ID: <x2d5j61i4l.fsf@turmalin.kubism.ku.dk>

Michael Dewey <info at aghmed.fsnet.co.uk> writes:

> Something about the way R processes backslashes is defeating me.
> Perhaps this is because I have only just started using R for text processing.
> 
> I would like to change occurrences of the ampersand & into ampersand 
> preceded by a backslash.
> 
>  > temp <- "R & D"
>  > sub("&", "\&", temp)
> [1] "R & D"
>  > sub("&", "\\&", temp)
> [1] "R & D"
>  > sub("&", "\\\&", temp)
> [1] "R & D"
>  > sub("&", "\\\\&", temp)
> [1] "R \\& D"
>  >
> 
> So I can get zero, or two backslashes, but not one. I am sure this is 
> really simple but I did not find the answer by doing, for example ?regexp 
> or ?Quotes


None of those result strings  have two backslashes!


Hint:

> nchar("R \\& D")
[1] 6

and ?Quotes tellse the entire story.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From gunter.berton at gene.com  Thu Jan  5 22:28:15 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 5 Jan 2006 13:28:15 -0800
Subject: [R] Generic Functions
In-Reply-To: <6.1.2.0.2.20060105114548.04412ac0@epurdom.pobox.stanford.edu>
Message-ID: <200601052128.k05LS45N018398@faraday.gene.com>

Briefly, S4 classes and methods are entirely different -- and often do not
comfortably coexist with -- the older S3 class/method system (which really
isn't, since the classes of the objects aren't really guaranteed as one
would expect). 

Probably the best place to learn about S4 is The Green Book (Chambers:
PROGRAMMING WITH DATA). Also you can load the "methods" package and type
?Methods for a shorter overview. See also ?setClass.

S4 provides a lot better control and encapsulation than S3, but it also
makes considerably greater demands of the programmer. You can decide whether
you think the tradeoff is justified for your work.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Elizabeth Purdom
> Sent: Thursday, January 05, 2006 12:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generic Functions
> 
> Hi,
> I've been using the package "graph" in the BioConductor 
> assortment and 
> writing some functions based on its classes. My question is 
> not specific to 
> this package or BioConductor (I think), but it serves as a 
> useful example. 
> I recently wanted to look at the code for the function 
> "edgeMatrix" for the 
> class "graphNEL".
> 
> Usually I would type
> 	> func.foo
> and the code for the function func for class foo would appear 
> (where func 
> and foo are edgeMatrix and graphNEL respectively). Similarly 
> I would type
> 	> methods(func)
> to see for what classes the function func is defined.
> 
> However, these do not work for these functions (they are not 
> S3 functions I 
> am told, though I don't know what that means). After a great deal of 
> guessing and help.search requests, I finally found functions 
> that seem to work:
> 	> getMethod(func,"foo")
> 	> showMethods(func)
> I get the corresponding code and possible methods available.
> 
> What is this about and is there a section in the R language 
> definition that 
> explains the difference?
> 
> Similarly I'm use to the object oriented program described in the R 
> language online based on the command UseMethod:
> 	> func <- function(x, ...) UseMethod("func", x)
> Under this system, I could just create a function "func.foo" 
> and it would 
> work for my class "foo" -- most notably I would create a 
> print command 
> "print.foo" and it would just seamlessly work. However my function 
> "print.graphNEL", for example, never worked and I'm just now 
> guessing from 
> the pieces of documentation from R, that I have to set it up 
> differently 
> but it is not clear to me how. How can I add a method to an existing 
> function under this setup?
> 
> On another note: even if these different functions are 
> internally quite 
> different, can't the functions everyone is already accustomed 
> to be made to 
> access the properties, rather than creating new, similar 
> functions? (what 
> is the need for a different function "showMethods" when a function 
> "methods" already exists? I have the same issue for slots 
> where I have to 
> use a function "slotNames" rather than the more commonly 
> known function 
> "names"). It becomes such a learning curve, that I shy away 
> from packages 
> that use new techniques in coding and stick with packages and 
> functions I 
> can comfortably dissect and personalize.
> 
> Thank you for any assistance,
> Elizabeth Purdom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From br44114 at gmail.com  Thu Jan  5 22:42:50 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 5 Jan 2006 16:42:50 -0500
Subject: [R] Wald tests and Huberized variances (was: A comment about R:)
Message-ID: <8d5a36350601051342x37fdf2f9g2515fa654c513b@mail.gmail.com>

Peter Muhlberger wrote:
> But, there is a second point here, which is how difficult it
> was for me [...] to find what seem to me like standard & key
> features I've taken for granted in other packages.

There is another side to this. Don't consider only how difficult it
was to find what you were looking for; also remember to be _glad_ that
there are so many packages and features to choose from. IMHO, the
benefit of having a lot of packages dwarfs all the efforts needed to
locate the right ones.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter
> Muhlberger
> Sent: Thursday, January 05, 2006 12:44 PM
> To: Achim Zeileis
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Wald tests and Huberized variances (was: A
> comment about R:)
>
>
> Hi Achim:  Your reply is tremendously helpful in addressing
> some of the
> outstanding questions I had about R.  The 'econometrics view'
> materials look
> exactly like what I needed.  Many thanks!
>
> But, there is a second point here, which is how difficult it
> was for me, as
> someone just becoming more familiar w/ R's more basic
> capabilities (in the
> past I've focused on features like optim, sem), to find what
> seem to me like
> standard & key features I've taken for granted in other
> packages.  I looked
> high & low in my existing installed packages for the standard
> version of R,
> I googled, I looked in the r-help archives, I looked through
> several manuals
> / introductions to R I had downloaded.  I've asked questions
> about all of
> the points I raised in my email on this email list before.  I
> believe I
> passed through the parent directory for the econometric view
> material at the
> website w/o realizing what it contained because I thought of
> "computational
> econometrics" as having to do w/ running Monte Carlo models
> of economic
> processes.
>
> If R wants to bring in a wider audience, one thing that might
> help is a
> denser set of cross-references.  For example, perhaps lm's help should
> mention the econometrics view materials as well as other
> places to look for
> tests and procedures people may want to do w/ lm.  Another
> thought is that
> perhaps the standard R package help should allow people to find
> non-installed but commonly used contributed packages and
> perhaps their help
> page contents.  A feature that would be very helpful for me
> is the capacity
> to search all the contents of help files, not just keywords
> that at times
> seem to miss what I'm trying to find.
>
> Cheers,
>
> Peter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From patrick.giraudoux at univ-fcomte.fr  Thu Jan  5 23:02:07 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Thu, 05 Jan 2006 23:02:07 +0100
Subject: [R] Memory limitation in GeoR - Windows or R?
In-Reply-To: <644e1f320601051250s44b9dc1erc5d8a0d9974a2aa4@mail.gmail.com>
References: <43BD6D44.1040902@gspia.pitt.edu> <43BD782E.6020307@univ-fcomte.fr>
	<644e1f320601051250s44b9dc1erc5d8a0d9974a2aa4@mail.gmail.com>
Message-ID: <43BD975F.7010105@univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/11cc8aa3/attachment.pl

From yen.lin.chia at intel.com  Fri Jan  6 00:11:18 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Thu, 5 Jan 2006 15:11:18 -0800
Subject: [R] convert matrix to data frame
Message-ID: <E305A4AFB7947540BC487567B5449BA80904CC97@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060105/4254c27f/attachment.pl

From neuro3000 at hotmail.com  Fri Jan  6 00:11:55 2006
From: neuro3000 at hotmail.com (=?iso-8859-1?B?TmV1cm8gTGVTdXBlckjpcm9z?=)
Date: Thu, 05 Jan 2006 18:11:55 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <8d5a36350601051226g3ed720b2md99fdca1fb478235@mail.gmail.com>
Message-ID: <BAY112-F89BA5D4F8357B4E4C7F99AF2E0@phx.gbl>

Rongui,

I'm not familiar with SQLite, but using MySQL would solve your problem.

MySQL has a "LOAD DATA INFILE" statement that loads text/csv files rapidly.

In R, assuming a test table exists in MySQL (blank table is fine), something 
like this would load the data directly in MySQL.

library(DBI)
library(RMySQL)
dbSendQuery(mycon,"LOAD DATA INFILE 'C:/textfile.csv'
INTO TABLE test3 FIELDS TERMINATED BY ',' ") #for csv files

Then a normal SQL query would allow you to work with a manageable size of 
data.



>From: bogdan romocea <br44114 at gmail.com>
>To: ronggui.huang at gmail.com
>CC: r-help <R-help at stat.math.ethz.ch>
>Subject: Re: [R] Suggestion for big files [was: Re: A comment about R:]
>Date: Thu, 5 Jan 2006 15:26:51 -0500
>
>ronggui wrote:
> > If i am familiar with
> > database software, using database (and R) is the best choice,but
> > convert the file into database format is not an easy job for me.
>
>Good working knowledge of a DBMS is almost invaluable when it comes to
>working with very large data sets. In addition, learning SQL is piece
>of cake compared to learning R. On top of that, knowledge of another
>(SQL) scripting language is not needed (except perhaps for special
>tasks): you can easily use R to generate the SQL syntax to import and
>work with arbitrarily wide tables. (I'm not familiar with SQLite, but
>MySQL comes with a command line tool that can run syntax files.)
>Better start learning SQL today.
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> > Sent: Thursday, January 05, 2006 12:48 PM
> > To: jim holtman
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Suggestion for big files [was: Re: A comment
> > about R:]
> >
> >
> > 2006/1/6, jim holtman <jholtman at gmail.com>:
> > > If what you are reading in is numeric data, then it would
> > require (807 *
> > > 118519 * 8) 760MB just to store a single copy of the object
> > -- more memory
> > > than you have on your computer.  If you were reading it in,
> > then the problem
> > > is the paging that was occurring.
> > In fact,If I read it in 3 pieces, each is about 170M.
> >
> > >
> > > You have to look at storing this in a database and working
> > on a subset of
> > > the data.  Do you really need to have all 807 variables in
> > memory at the
> > > same time?
> >
> > Yip,I don't need all the variables.But I don't know how to get the
> > necessary  variables into R.
> >
> > At last I  read the data in piece and use RSQLite package to write it
> > to a database.and do then do the analysis. If i am familiar with
> > database software, using database (and R) is the best choice,but
> > convert the file into database format is not an easy job for me.I ask
> > for help in SQLite list,but the solution is not satisfying as that
> > required the knowledge about the third script language.After searching
> > the internet,I get this solution:
> >
> > #begin
> > rm(list=ls())
> > f<-file("D:\wvsevs_sb_v4.csv","r")
> > i <- 0
> > done <- FALSE
> > library(RSQLite)
> > con<-dbConnect("SQLite","c:\sqlite\database.db3")
> > tim1<-Sys.time()
> >
> > while(!done){
> > i<-i+1
> > tt<-readLines(f,2500)
> > if (length(tt)<2500) done <- TRUE
> > tt<-textConnection(tt)
> > if (i==1) {
> >            assign("dat",read.table(tt,head=T,sep=",",quote=""));
> >          }
> > else assign("dat",read.table(tt,head=F,sep=",",quote=""))
> > close(tt)
> > ifelse(dbExistsTable(con, "wvs"),dbWriteTable(con,"wvs",dat,append=T),
> >   dbWriteTable(con,"wvs",dat) )
> > }
> > close(f)
> > #end
> > It's not the best solution,but it works.
> >
> >
> >
> > > If you use 'scan', you could specify that you do not want
> > some of the
> > > variables read in so it might make a more reasonably sized objects.
> > >
> > >
> > > On 1/5/06, Fran????ois Pinard <pinard at iro.umontreal.ca> wrote:
> > > > [ronggui]
> > > >
> > > > >R's week when handling large data file.  I has a data
> > file : 807 vars,
> > > > >118519 obs.and its CVS format.  Stata can read it in in
> > 2 minus,but In
> > > > >my PC,R almost can not handle. my pc's cpu 1.7G ;RAM 512M.
> > > >
> > > > Just (another) thought.  I used to use SPSS, many, many
> > years ago, on
> > > > CDC machines, where the CPU had limited memory and no
> > kind of paging
> > > > architecture.  Files did not need to be very large for
> > being too large.
> > > >
> > > > SPSS had a feature that was then useful, about the capability of
> > > > sampling a big dataset directly at file read time, quite before
> > > > processing starts.  Maybe something similar could help in
> > R (that is,
> > > > instead of reading the whole data in memory, _then_ sampling it.)
> > > >
> > > > One can read records from a file, up to a preset amount
> > of them.  If the
> > > > file happens to contain more records than that preset
> > number (the number
> > > > of records in the whole file is not known beforehand),
> > already read
> > > > records may be dropped at random and replaced by other
> > records coming
> > > > from the file being read.  If the random selection
> > algorithm is properly
> > > > chosen, it can be made so that all records in the
> > original file have
> > > > equal probability of being kept in the final subset.
> > > >
> > > > If such a sampling facility was built right within usual R reading
> > > > routines (triggered by an extra argument, say), it could offer
> > > > a compromise for processing large files, and also
> > sometimes accelerate
> > > > computations for big problems, even when memory is not at stake.
> > > >
> > > > --
> > > > Fran????ois Pinard   http://pinard.progiciels-bpi.ca
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> > >
> > > --
> > > Jim Holtman
> > > Cincinnati, OH
> > > +1 513 247 0281
> > >
> > > What the problem you are trying to solve?
> >
> >
> > --
> > ????????????????
> > Deparment of Sociology
> > Fudan University
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Fri Jan  6 02:04:08 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 05 Jan 2006 19:04:08 -0600
Subject: [R] Wikis etc.
Message-ID: <43BDC208.5060909@vanderbilt.edu>

I feel that as long as people continue to provide help on r-help wikis 
will not be successful.  I think we need to move to a central wiki or 
discussion board and to move away from e-mail.  People are extremely 
helpful but e-mail seems to be to always be memory-less and messages get 
too long without factorization of old text.  R-help is now too active 
and too many new users are asking questions asked dozens of times for 
e-mail to be effective.

The wiki also needs to collect and organize example code, especially for 
data manipulation.  I think that new users would profit immensely from a 
compendium of examples.

Just my .02 Euros

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From rdiaz at cnio.es  Fri Jan  6 02:18:23 2006
From: rdiaz at cnio.es (Diaz.Ramon)
Date: Fri, 6 Jan 2006 02:18:23 +0100
Subject: [R] Looking for packages to do Feature Selection and
	Classification
Message-ID: <BE216486E4154040BF783AE5DAAA3ED401C2D1EC@SRVEXCH1.cnio.es>


Thanks for the reference, it looks very interesting.

Best,

R.

-----Original Message-----
From:	Weiwei Shi [mailto:helprhelp at gmail.com]
Sent:	Thu 1/5/2006 9:01 PM
To:	Diaz.Ramon
Cc:	Frank Duan; r-help
Subject:	Re: [R] Looking for packages to do Feature Selection and Classification

FYI:

check the following paper on svm (using libsvm) as well as random
forest in the context of feature selection.

http://www.csie.ntu.edu.tw/~cjlin/papers/features.pdf

HTH

On 1/4/06, Diaz.Ramon <rdiaz at cnio.es> wrote:
> Dear Frank,
> I expect you'll get many different answers since a wide variety of approaches have been suggested. So I'll stick to self-advertisment: I've written an R package, varSelRF (available from R), that uses random forest together with a simple variable selection approach, and provides also bootstrap estimates of the error rate of the procedure. Andy Liaw and collaborators previously developed and published a somewhat similar procedure. You probably also want to take a look at several packages available from BioConductor.
>
> Best,
>
> R.
>
>
> -----Original Message-----
> From:   r-help-bounces at stat.math.ethz.ch on behalf of Frank Duan
> Sent:   Wed 1/4/2006 4:23 AM
> To:     r-help
> Cc:
> Subject:        [R] Looking for packages to do Feature Selection and Classification
>
> Hi All,
>
> Sorry if this is a repost (a quick browse didn't give me the answer).
>
> I wonder if there are packages that can do the feature selection and
> classification at the same time. For instance, I am using SVM to classify my
> samples, but it's easy to get overfitted if using all of the features. Thus,
> it is necessary to select "good" features to build an optimum hyperplane
> (?). Here is a simple example: Suppose I have 100 "useful" features and 100
> "useless" features (or noise features), I want the SVM to give me the
> same results when 1) using only 100 useful features or 2) using all 200
> features.
>
> Any suggestions or point me to a reference?
>
> Thanks in advance!
>
> Frank
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> --
> Ram??n D??az-Uriarte
> Bioinformatics Unit
> Centro Nacional de Investigaciones Oncol??gicas (CNIO)
> (Spanish National Cancer Center)
> Melchor Fern??ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
>
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
>
>
>
> **NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III




**NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en s...{{dropped}}



From kevin.thorpe at utoronto.ca  Fri Jan  6 02:24:07 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 05 Jan 2006 20:24:07 -0500
Subject: [R] Wikis etc.
In-Reply-To: <43BDC208.5060909@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu>
Message-ID: <43BDC6B7.3010503@utoronto.ca>

Frank makes an intersting point.  For those interested, A site I spend
quite a bit of time on for Linux related stuff is IMHO really well done.
There are fora for many different linux distrubtions.  There is a wiki,
a collection of tutorials, etc.  If you want to take a look, the url is
http://www.linuxquestions.org/

Kevin

Frank E Harrell Jr wrote:
> I feel that as long as people continue to provide help on r-help wikis 
> will not be successful.  I think we need to move to a central wiki or 
> discussion board and to move away from e-mail.  People are extremely 
> helpful but e-mail seems to be to always be memory-less and messages get 
> too long without factorization of old text.  R-help is now too active 
> and too many new users are asking questions asked dozens of times for 
> e-mail to be effective.
> 
> The wiki also needs to collect and organize example code, especially for 
> data manipulation.  I think that new users would profit immensely from a 
> compendium of examples.
> 
> Just my .02 Euros
> 
> Frank


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297



From lebouton at msu.edu  Fri Jan  6 03:27:38 2006
From: lebouton at msu.edu (Joseph LeBouton)
Date: Thu, 05 Jan 2006 20:27:38 -0600
Subject: [R] Ordering boxplot factors
Message-ID: <43BDD59A.4010308@msu.edu>

Hi all,

what a great help list!  I hope someone can help me with this puzzle...

I'm trying to find a simple way to do:

boxplot(obs~factor)

so that the factors are ordered left-to-right along the x-axis by
median, not alphabetically by factor name.

Complicated ways abound, but I'm hoping for a magical one-liner that'll
do the trick.

Any suggestions would be treasured.

Thanks,

-jlb
-- 
************************************
Joseph P. LeBouton
Forest Ecology PhD Candidate
Department of Forestry
Michigan State University
East Lansing, Michigan 48824

Office phone: 517-355-7744
email: lebouton at msu.edu



From MSchwartz at mn.rr.com  Fri Jan  6 03:48:01 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 05 Jan 2006 20:48:01 -0600
Subject: [R] Ordering boxplot factors
In-Reply-To: <43BDD59A.4010308@msu.edu>
References: <43BDD59A.4010308@msu.edu>
Message-ID: <1136515681.4295.15.camel@localhost.localdomain>

On Thu, 2006-01-05 at 20:27 -0600, Joseph LeBouton wrote:
> Hi all,
> 
> what a great help list!  I hope someone can help me with this puzzle...
> 
> I'm trying to find a simple way to do:
> 
> boxplot(obs~factor)
> 
> so that the factors are ordered left-to-right along the x-axis by
> median, not alphabetically by factor name.
> 
> Complicated ways abound, but I'm hoping for a magical one-liner that'll
> do the trick.
> 
> Any suggestions would be treasured.
> 
> Thanks,
> 
> -jlb


Using the first example in ?boxplot, which is:

boxplot(count ~ spray, data = InsectSprays, col = "lightgray")



Get the medians for 'count by spray' using tapply() and then sort the
results in increasing order, by median:

  med <- sort(with(InsectSprays, tapply(count, spray, median)))

> med
   C    E    D    A    F    B 
 1.5  3.0  5.0 14.0 15.0 16.5 


Now do the boxplot, setting the factor levels in order by median:

  boxplot(count ~ factor(spray, levels = names(med)), 
          data = InsectSprays, col = "lightgray")


So...technically two lines of code.

HTH,

Marc Schwartz



From leif at reflectivity.com  Fri Jan  6 04:02:09 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Thu, 5 Jan 2006 19:02:09 -0800
Subject: [R] A comment about R:
Message-ID: <200601060302.k0632CUn028784@hypatia.math.ethz.ch>

A few thoughts about R vs SAS:
I started learning SAS 8 years ago at IBM, I believe it was version 6.10.
I started with R 7 months ago.

Learning curve:
  I think I can do everything in R after 7 months that I could do in SAS after about 4 years.

Bugs:
  I suffered through several SAS version changes, 7.0, 7.1, 7.2, 8.0, 9.0 (I may have misquoted some version numbers). Every version change gave me headaches, as every version release (of an expensive commercially produced software set) had bugs which upset or crashed previously working code. I had code which ran fine under Windows 2000 and terribly under Windows XP. Most bugs I found were noted by SAS, but never fixed.
  With R I have encounted very few bugs, except for an occasional crash of R, which I usually ascribe to some bug in Windows XP.

Help:
  SAS help was OK. As others have mentioned, there is too much. I even had the set of printed manuals on my desk (stretching 4 feet or so), which were quote impenetrable. I had almost no support from colleagues: even within IBM the number of advanced SAS users was small.
  With R this mailing list has been of great help: almost every issue I copy some program and save it as a "R hint xxxx" file.
--> A REQUEST
I would say that I would appreciate a few more program examples with the help pages for some functions. For instance, "?Control" tells me about "if(cond) cons.expr  else  alt.expr", however an example of
   if(i==1) { print("one") 
   } else if(i==2) { print("two")
   } else if(i>2) { print("bigger than two") }
 at the end of that help section would have been very helpful for me a few months ago.

Functions:
  Writing my own functions in SAS was by use of macros, and usually depended heavily on macro substitution. Learning SAS's macro language, especially macro substitution, was very difficult and it took me years to be able to write complicated functions. Quite different situation in R. Some functions I have written by dint of copying code from other people's packages, which has been very helpful.
  I wanted to generate arbitrary k-values (the k-multiplier of sigma for a given alpha, beta, and N to establish confidence limits around a mean for small populations). I had a table from a years old microfiche book giving values but wanted to generate my own. I had to find the correct integrals to approximate the k-values and then write two SAS macros which iterated to the desired level of tolerance to generate values. I would guess that there is either an R base function or a package which will do this for me (when I need to start generating AQL tables). Given the utility of these numbers, I was disappointed with SAS.

Data manipulation:
  All SAS data is in 2-dimensional datasets, which was very frustrating after having used variables, arrays, and matrices in BASIC, APL, FORTRAN, C, Pascal, and LabVIEW. SAS allows you to access only 1 row of a dataset at a time which was terribly horribly incomprehensibly frustrating. There were so many many problems I had to solve where I had to work around this SAS paradigm.
  In R, I can access all the elements of a matrix/dataframe at once, and I can use >2 dimensional matrices. In fact, the limitations of SAS I had ingrained from 7.5 years has sometimes made me forget how I can do something so easily in R, like be able to know when a value in a column of a dataframe changes:
  DF$marker <- DF[1:(nrow(DF)-1),icol] != DF[2:nrow(DF),icol]
This was hard to do in SAS...and even after years it was sometimes buggy, keeping variable values from previous iterations of a SAS program.
  One very nice advantage with SAS is that after data is saved in libraries, there is a GUI showing all the libraries and the datasets inside the libraries with sizes and dates. While we can save Rdata objects in an external file, the base package doesn't seem to have the same capabilities as SAS.

Graphics:
  SAS graphics were quite mediocre, and generating customized labels was cumbersome. Porting code from one Windows platform to another produced unpredictable and sometimes unworkable results.
  It has been easier in R: I anticipate that I will be able to port R Windows code to *NIX and generate the same graphics.

Batch commands:
  I am working on porting some of my R code to our *NIX server to generate reports and graphs on a scheduled basis. Although a few at IBM did this with SAS, I would have found doing this fairly daunting.


-Leif

-----------------------------
 Leif Kirschenbaum, Ph.D.
 Senior Yield Engineer
 Reflectivity
 leif at reflectivity.com



From pinard at iro.umontreal.ca  Fri Jan  6 04:41:21 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 5 Jan 2006 22:41:21 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <Pine.LNX.4.61.0601051641470.1468@gannet.stats>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
Message-ID: <20060106034121.GA8861@alcyon.progiciels-bpi.ca>

[Brian Ripley]

>I rather thought that using a DBMS was standard practice in the 
>R community for those using large datasets: it gets discussed rather 
>often.

Indeed.  (I tried RMySQL even before speaking of R to my co-workers.)

>Another possibility is to make use of the several DBMS interfaces already 
>available for R.  It is very easy to pull in a sample from one of those, 
>and surely keeping such large data files as ASCII not good practice.

Selecting a sample is easy.  Yet, I'm not aware of any SQL device for 
easily selecting a _random_ sample of the records of a given table.  On 
the other hand, I'm no SQL specialist, others might know better.

We do not have a need yet for samples where I work, but if we ever need 
such, they will have to be random, or else, I will always fear biases.

>One problem with Francois Pinard's suggestion (the credit has got lost) 
>is that R's I/O is not line-oriented but stream-oriented.  So selecting 
>lines is not particularly easy in R.

I understand that you mean random access to lines, instead of random 
selection of lines.  Once again, this chat comes out of reading someone 
else's problem, this is not a problem I actually have.  SPSS was not 
randomly accessing lines, as data files could well be hold on magnetic 
tapes, where random access is not possible on average practice.  SPSS 
reads (or was reading) lines sequentially from beginning to end, and the 
_random_ sample is built while the reading goes.

Suppose the file (or tape) holds N records (N is not known in advance), 
from which we want a sample of M records at most.  If N <= M, then we 
use the whole file, no sampling is possible nor necessary.  Otherwise, 
we first initialise M records with the first M records of the file.  
Then, for each record in the file after the M'th, the algorithm has to 
decide if the record just read will be discarded or if it will replace 
one of the M records already saved, and in the latter case, which of 
those records will be replaced.  If the algorithm is carefully designed, 
when the last (N'th) record of the file will have been processed this 
way, we may then have M records randomly selected from N records, in 
such a a way that each of the N records had an equal probability to end 
up in the selection of M records.  I may seek out for details if needed.

This is my suggestion, or in fact, more a thought that a suggestion.  It 
might represent something useful either for flat ASCII files or even for 
a stream of records coming out of a database, if those effectively do 
not offer ready random sampling devices.


P.S. - In the (rather unlikely, I admit) case the gang I'm part of would 
have the need described above, and if I then dared implementing it 
myself, would it be welcome?

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From f.harrell at vanderbilt.edu  Fri Jan  6 05:23:11 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 05 Jan 2006 22:23:11 -0600
Subject: [R] A comment about R:
In-Reply-To: <200601060302.k0632CUn028784@hypatia.math.ethz.ch>
References: <200601060302.k0632CUn028784@hypatia.math.ethz.ch>
Message-ID: <43BDF0AF.7060805@vanderbilt.edu>

Leif Kirschenbaum wrote:
> A few thoughts about R vs SAS:
> I started learning SAS 8 years ago at IBM, I believe it was version 6.10.
> I started with R 7 months ago.
> 
> Learning curve:
>   I think I can do everything in R after 7 months that I could do in SAS after about 4 years.
> 
> Bugs:
>   I suffered through several SAS version changes, 7.0, 7.1, 7.2, 8.0, 9.0 (I may have misquoted some version numbers). Every version change gave me headaches, as every version release (of an expensive commercially produced software set) had bugs which upset or crashed previously working code. I had code which ran fine under Windows 2000 and terribly under Windows XP. Most bugs I found were noted by SAS, but never fixed.
>   With R I have encounted very few bugs, except for an occasional crash of R, which I usually ascribe to some bug in Windows XP.
> 
> Help:
>   SAS help was OK. As others have mentioned, there is too much. I even had the set of printed manuals on my desk (stretching 4 feet or so), which were quote impenetrable. I had almost no support from colleagues: even within IBM the number of advanced SAS users was small.
>   With R this mailing list has been of great help: almost every issue I copy some program and save it as a "R hint xxxx" file.
> --> A REQUEST
> I would say that I would appreciate a few more program examples with the help pages for some functions. For instance, "?Control" tells me about "if(cond) cons.expr  else  alt.expr", however an example of
>    if(i==1) { print("one") 
>    } else if(i==2) { print("two")
>    } else if(i>2) { print("bigger than two") }
>  at the end of that help section would have been very helpful for me a few months ago.
> 
> Functions:
>   Writing my own functions in SAS was by use of macros, and usually depended heavily on macro substitution. Learning SAS's macro language, especially macro substitution, was very difficult and it took me years to be able to write complicated functions. Quite different situation in R. Some functions I have written by dint of copying code from other people's packages, which has been very helpful.
>   I wanted to generate arbitrary k-values (the k-multiplier of sigma for a given alpha, beta, and N to establish confidence limits around a mean for small populations). I had a table from a years old microfiche book giving values but wanted to generate my own. I had to find the correct integrals to approximate the k-values and then write two SAS macros which iterated to the desired level of tolerance to generate values. I would guess that there is either an R base function or a package which will do this for me (when I need to start generating AQL tables). Given the utility of these numbers, I was disappointed with SAS.
> 
> Data manipulation:
>   All SAS data is in 2-dimensional datasets, which was very frustrating after having used variables, arrays, and matrices in BASIC, APL, FORTRAN, C, Pascal, and LabVIEW. SAS allows you to access only 1 row of a dataset at a time which was terribly horribly incomprehensibly frustrating. There were so many many problems I had to solve where I had to work around this SAS paradigm.
>   In R, I can access all the elements of a matrix/dataframe at once, and I can use >2 dimensional matrices. In fact, the limitations of SAS I had ingrained from 7.5 years has sometimes made me forget how I can do something so easily in R, like be able to know when a value in a column of a dataframe changes:
>   DF$marker <- DF[1:(nrow(DF)-1),icol] != DF[2:nrow(DF),icol]
> This was hard to do in SAS...and even after years it was sometimes buggy, keeping variable values from previous iterations of a SAS program.
>   One very nice advantage with SAS is that after data is saved in libraries, there is a GUI showing all the libraries and the datasets inside the libraries with sizes and dates. While we can save Rdata objects in an external file, the base package doesn't seem to have the same capabilities as SAS.
> 
> Graphics:
>   SAS graphics were quite mediocre, and generating customized labels was cumbersome. Porting code from one Windows platform to another produced unpredictable and sometimes unworkable results.
>   It has been easier in R: I anticipate that I will be able to port R Windows code to *NIX and generate the same graphics.
> 
> Batch commands:
>   I am working on porting some of my R code to our *NIX server to generate reports and graphs on a scheduled basis. Although a few at IBM did this with SAS, I would have found doing this fairly daunting.
> 
> 
> -Leif

Leif,

Those are excellent points.  I'm especially glad you mentioned data 
manipulation.  I find that R is far ahead of SAS in this respect 
although most people are shocked to hear me say that.  We are doing all 
our data manipulation (merging, recoding, etc.) in R for pharmaceutical 
research.  The ability to deal with lists of data frames also helps us a 
great deal when someone sends us a clinical trial database made of 50 
SAS datasets.

Frank

> 
> -----------------------------
>  Leif Kirschenbaum, Ph.D.
>  Senior Yield Engineer
>  Reflectivity
>  leif at reflectivity.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Fri Jan  6 05:23:35 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 05 Jan 2006 22:23:35 -0600
Subject: [R] A comment about R:
In-Reply-To: <200601060302.k0632CUn028784@hypatia.math.ethz.ch>
References: <200601060302.k0632CUn028784@hypatia.math.ethz.ch>
Message-ID: <43BDF0C7.90304@vanderbilt.edu>

Leif Kirschenbaum wrote:
> A few thoughts about R vs SAS:
> I started learning SAS 8 years ago at IBM, I believe it was version 6.10.
> I started with R 7 months ago.
> 
> Learning curve:
>   I think I can do everything in R after 7 months that I could do in SAS after about 4 years.
> 
> Bugs:
>   I suffered through several SAS version changes, 7.0, 7.1, 7.2, 8.0, 9.0 (I may have misquoted some version numbers). Every version change gave me headaches, as every version release (of an expensive commercially produced software set) had bugs which upset or crashed previously working code. I had code which ran fine under Windows 2000 and terribly under Windows XP. Most bugs I found were noted by SAS, but never fixed.
>   With R I have encounted very few bugs, except for an occasional crash of R, which I usually ascribe to some bug in Windows XP.
> 
> Help:
>   SAS help was OK. As others have mentioned, there is too much. I even had the set of printed manuals on my desk (stretching 4 feet or so), which were quote impenetrable. I had almost no support from colleagues: even within IBM the number of advanced SAS users was small.
>   With R this mailing list has been of great help: almost every issue I copy some program and save it as a "R hint xxxx" file.
> --> A REQUEST
> I would say that I would appreciate a few more program examples with the help pages for some functions. For instance, "?Control" tells me about "if(cond) cons.expr  else  alt.expr", however an example of
>    if(i==1) { print("one") 
>    } else if(i==2) { print("two")
>    } else if(i>2) { print("bigger than two") }
>  at the end of that help section would have been very helpful for me a few months ago.
> 
> Functions:
>   Writing my own functions in SAS was by use of macros, and usually depended heavily on macro substitution. Learning SAS's macro language, especially macro substitution, was very difficult and it took me years to be able to write complicated functions. Quite different situation in R. Some functions I have written by dint of copying code from other people's packages, which has been very helpful.
>   I wanted to generate arbitrary k-values (the k-multiplier of sigma for a given alpha, beta, and N to establish confidence limits around a mean for small populations). I had a table from a years old microfiche book giving values but wanted to generate my own. I had to find the correct integrals to approximate the k-values and then write two SAS macros which iterated to the desired level of tolerance to generate values. I would guess that there is either an R base function or a package which will do this for me (when I need to start generating AQL tables). Given the utility of these numbers, I was disappointed with SAS.
> 
> Data manipulation:
>   All SAS data is in 2-dimensional datasets, which was very frustrating after having used variables, arrays, and matrices in BASIC, APL, FORTRAN, C, Pascal, and LabVIEW. SAS allows you to access only 1 row of a dataset at a time which was terribly horribly incomprehensibly frustrating. There were so many many problems I had to solve where I had to work around this SAS paradigm.
>   In R, I can access all the elements of a matrix/dataframe at once, and I can use >2 dimensional matrices. In fact, the limitations of SAS I had ingrained from 7.5 years has sometimes made me forget how I can do something so easily in R, like be able to know when a value in a column of a dataframe changes:
>   DF$marker <- DF[1:(nrow(DF)-1),icol] != DF[2:nrow(DF),icol]
> This was hard to do in SAS...and even after years it was sometimes buggy, keeping variable values from previous iterations of a SAS program.
>   One very nice advantage with SAS is that after data is saved in libraries, there is a GUI showing all the libraries and the datasets inside the libraries with sizes and dates. While we can save Rdata objects in an external file, the base package doesn't seem to have the same capabilities as SAS.
> 
> Graphics:
>   SAS graphics were quite mediocre, and generating customized labels was cumbersome. Porting code from one Windows platform to another produced unpredictable and sometimes unworkable results.
>   It has been easier in R: I anticipate that I will be able to port R Windows code to *NIX and generate the same graphics.
> 
> Batch commands:
>   I am working on porting some of my R code to our *NIX server to generate reports and graphs on a scheduled basis. Although a few at IBM did this with SAS, I would have found doing this fairly daunting.
> 
> 
> -Leif

Leif,

Those are excellent points.  I'm especially glad you mentioned data 
manipulation.  I find that R is far ahead of SAS in this respect 
although most people are shocked to hear me say that.  We are doing all 
our data manipulation (merging, recoding, etc.) in R for pharmaceutical 
research.  The ability to deal with lists of data frames also helps us a 
great deal when someone sends us a clinical trial database made of 50 
SAS datasets.

Frank

> 
> -----------------------------
>  Leif Kirschenbaum, Ph.D.
>  Senior Yield Engineer
>  Reflectivity
>  leif at reflectivity.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From petr.pikal at precheza.cz  Fri Jan  6 08:06:15 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 06 Jan 2006 08:06:15 +0100
Subject: [R] convert matrix to data frame
In-Reply-To: <E305A4AFB7947540BC487567B5449BA80904CC97@scsmsx402.amr.corp.intel.com>
Message-ID: <43BE24F7.16923.62AB27@localhost>

Hi

On 5 Jan 2006 at 15:11, Chia, Yen Lin wrote:

Date sent:      	Thu, 5 Jan 2006 15:11:18 -0800
From:           	"Chia, Yen Lin" <yen.lin.chia at intel.com>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] convert matrix to data frame

> Hi all,
> 
> 
> 
> Suppose I have a 4 x 2 matrix  A and I want to select the values in
> second column such that the value in first column equals to k.
> 
> 
> 
> I gave the colnames as alpha beta, so I was trying to access the info
> using
> 
> 
> 
> A$beta[A[,1]==k], however, I was told it's not a data frame, I can get
> the object by using dollar sign.  I tried data.frame(A), but it didn't
> work.  

I believe its because matrix has a bit different structure from 
data.frame so its columns can not be simply called by names. But I 
wonder why data.frame(A) does not work for you? See below.

 > str(A)
 `data.frame':   4 obs. of  2 variables:
  $ alpha: num  -1.181 -0.415  2.087  1.422
  $ beta : num  -0.0889  0.6828 -0.7035 -0.3351 

 > str(mat)
   num [1:4, 1:2] -1.1813 -0.4152  2.0865  1.4216 -0.0889 ...
  - attr(*, "dimnames")=List of 2
   ..$ : chr [1:4] "1" "2" "3" "4"
   ..$ : chr [1:2] "alpha" "beta" 

 > B<-data.frame(mat) 

 > str(B)
 `data.frame':   4 obs. of  2 variables:
  $ alpha: num  -1.181 -0.415  2.087  1.422
  $ beta : num  -0.0889  0.6828 -0.7035 -0.3351

 > mat$alpha[mat[,1]<0]
 NULL
 > B$alpha[mat[,1]<0]
 [1] -1.181258 -0.415175




> 
> 
> 
> Any input on this will be very appreciated.  Thanks.
> 
> 
> 
> I tried looking in the manual, but I think I'm might be wrong about
> the keywords.
> 
> 
> 
> Yen Lin
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From h.wickham at gmail.com  Fri Jan  6 08:28:27 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 6 Jan 2006 07:28:27 +0000
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <20060106034121.GA8861@alcyon.progiciels-bpi.ca>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
Message-ID: <f8e6ff050601052328y3ada32bdh28455bd493973ac8@mail.gmail.com>

> Selecting a sample is easy.  Yet, I'm not aware of any SQL device for
> easily selecting a _random_ sample of the records of a given table.  On
> the other hand, I'm no SQL specialist, others might know better.

There are a number of such devices, which tend to be rather SQL
variant specific.  Try googling for select random rows mysql, select
random rows pgsql, etc.

Another possibility is to generate a large table of randomly
distributed ids and then use that (with randomly generated limits) to
select the appropriate number of records.

Hadley



From jwd at surewest.net  Fri Jan  6 08:37:55 2006
From: jwd at surewest.net (J Dougherty)
Date: Thu, 5 Jan 2006 23:37:55 -0800
Subject: [R] A comment about R:
In-Reply-To: <Pine.LNX.4.58.0601052052270.12589@thorin.ci.tuwien.ac.at>
References: <BFE2D16F.127DD%pmuhl1848@gmail.com>
	<Pine.LNX.4.58.0601052052270.12589@thorin.ci.tuwien.ac.at>
Message-ID: <200601052337.55434.jwd@surewest.net>

On Thursday 05 January 2006 12:13, Achim Zeileis wrote:
> . . . snip
> Whether you find this simple or not depends on what you might want to
> have. Personally, I always find it very limiting if I've only got a switch
> to choose one or another vcov matrix when there is a multitude of vcov
> matrices in use in the literature. What if you would want to do HC3
> instead of the HC(0) that is offered by Eviews...or HC4...or HAC...or
> something bootstrapped...or...
> In my view, this is the stengths of many implementation in R: you can make
> programs very modular so that the user can easily extend the software or
> re-use it for other purposes. The price you pay for that is that it is not
> as easy to as a point-and-click software that offers some standard tools.
> Of course, both sides have advantages or disadvantages.
> . . .snip

Stata's ADO scripting language has the ability to access intermediate steps 
and local variables used by various commands.  These are typically held in 
memory until they are purged.  The difference between Stata and R is more 
that Stata has been streamlined into an application, the nuts and bolts 
hidden away, the rivet heads counter sunk and polished, so that unless you 
really need to use them, they aren't visible.  It only LOOKS like you are 
constrained to the readily available results of specific commands.  Stata 
output will tend to look very much like the standard output one becomes 
accustomed to in undergraduate stat courses.  

R assumes you _will_ want access to the nuts and bolts, and don't much care 
about visible rivets if the system is both accurate and functional.  R is 
much more a programming environment in that sense.  It is an important 
difference.  There is going to be a continuing growth in users of R as 
companies see cost savings in OS.  They will often be people who happily 
dragged .xls files into SPSS or SPSS for analysis and then printed the 
resulting reports.  (Personally, I became a strong believer in statistical 
analysis packages after receiving a _negative_ variance in Excel once upon a 
time.  I don't see how that could even be possible, but apparently it was a 
known issue.  Some ad hoc experimentation then demonstrated that no 
spreadsheet was all that precise).

One place where R and Stata have a great deal in common is in the manner in 
which graphs and charts are formatted.  Stata is perhaps slightly less 
bizantine, but only slightly.  Both systems emphasize flexibility and quality 
graphics at the price of learning to know what you are doing.  That said, you 
can still do a lot more with R in some areas than Stata, especially in 
spatial graphics and analysis.

JD



From ripley at stats.ox.ac.uk  Fri Jan  6 09:08:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jan 2006 08:08:59 +0000 (GMT)
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <20060106034121.GA8861@alcyon.progiciels-bpi.ca>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
Message-ID: <Pine.LNX.4.61.0601060802190.18569@gannet.stats>

[Just one point extracted: Hadley Wickham has answered the random sample 
one]

On Thu, 5 Jan 2006, Fran?ois Pinard wrote:

> [Brian Ripley]
>> One problem with Francois Pinard's suggestion (the credit has got lost)
>> is that R's I/O is not line-oriented but stream-oriented.  So selecting
>> lines is not particularly easy in R.
>
> I understand that you mean random access to lines, instead of random
> selection of lines.  Once again, this chat comes out of reading someone
> else's problem, this is not a problem I actually have.  SPSS was not
> randomly accessing lines, as data files could well be hold on magnetic
> tapes, where random access is not possible on average practice.  SPSS
> reads (or was reading) lines sequentially from beginning to end, and the
> _random_ sample is built while the reading goes.

That was not my point.  R's standard I/O is through connections, which 
allow for pushbacks, changing line endings and re-encoding character sets. 
That does add overhead compared to C/Fortran line-buffered reading of a 
file.  Skipping lines you do not need will take longer than you might 
guess (based on some limited experience).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Fri Jan  6 09:22:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jan 2006 08:22:02 +0000 (GMT)
Subject: [R] Ordering boxplot factors
In-Reply-To: <1136515681.4295.15.camel@localhost.localdomain>
References: <43BDD59A.4010308@msu.edu>
	<1136515681.4295.15.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0601060816570.18569@gannet.stats>

On Thu, 5 Jan 2006, Marc Schwartz wrote:

> On Thu, 2006-01-05 at 20:27 -0600, Joseph LeBouton wrote:
>> Hi all,
>>
>> what a great help list!  I hope someone can help me with this puzzle...
>>
>> I'm trying to find a simple way to do:
>>
>> boxplot(obs~factor)
>>
>> so that the factors are ordered left-to-right along the x-axis by
>> median, not alphabetically by factor name.

The thing to realize is that they are not alphabetic, but ordered by 
factor levels.  So the key is to set the levels.  (The help page for 
boxplot does say that, as I was relieved to find.)

>> Complicated ways abound, but I'm hoping for a magical one-liner that'll
>> do the trick.
>>
>> Any suggestions would be treasured.
>>
>> Thanks,
>>
>> -jlb
>
>
> Using the first example in ?boxplot, which is:
>
> boxplot(count ~ spray, data = InsectSprays, col = "lightgray")
>
>
>
> Get the medians for 'count by spray' using tapply() and then sort the
> results in increasing order, by median:
>
>  med <- sort(with(InsectSprays, tapply(count, spray, median)))
>
>> med
>   C    E    D    A    F    B
> 1.5  3.0  5.0 14.0 15.0 16.5
>
>
> Now do the boxplot, setting the factor levels in order by median:
>
>  boxplot(count ~ factor(spray, levels = names(med)),
>          data = InsectSprays, col = "lightgray")
>
>
> So...technically two lines of code.

This was answered yesterday in terms of bwplot.  See ?reorder.factor
for the same example done using reorder.factor.  That will give you the 
single line asked for, and be self-explanatory.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Jan  6 09:33:05 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 6 Jan 2006 09:33:05 +0100
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <20060106034121.GA8861@alcyon.progiciels-bpi.ca>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
Message-ID: <17342.11073.712221.863881@stat.math.ethz.ch>

>>>>> "FrPi" == Fran??ois Pinard <pinard at iro.umontreal.ca>
>>>>>     on Thu, 5 Jan 2006 22:41:21 -0500 writes:

    FrPi> [Brian Ripley]
    >> I rather thought that using a DBMS was standard practice in the 
    >> R community for those using large datasets: it gets discussed rather 
    >> often.

    FrPi> Indeed.  (I tried RMySQL even before speaking of R to my co-workers.)

    >> Another possibility is to make use of the several DBMS interfaces already 
    >> available for R.  It is very easy to pull in a sample from one of those, 
    >> and surely keeping such large data files as ASCII not good practice.

    FrPi> Selecting a sample is easy.  Yet, I'm not aware of any
    FrPi> SQL device for easily selecting a _random_ sample of
    FrPi> the records of a given table.  On the other hand, I'm
    FrPi> no SQL specialist, others might know better.

    FrPi> We do not have a need yet for samples where I work,
    FrPi> but if we ever need such, they will have to be random,
    FrPi> or else, I will always fear biases.

    >> One problem with Francois Pinard's suggestion (the credit has got lost) 
    >> is that R's I/O is not line-oriented but stream-oriented.  So selecting 
    >> lines is not particularly easy in R.

    FrPi> I understand that you mean random access to lines,
    FrPi> instead of random selection of lines.  Once again,
    FrPi> this chat comes out of reading someone else's problem,
    FrPi> this is not a problem I actually have.  SPSS was not
    FrPi> randomly accessing lines, as data files could well be
    FrPi> hold on magnetic tapes, where random access is not
    FrPi> possible on average practice.  SPSS reads (or was
    FrPi> reading) lines sequentially from beginning to end, and
    FrPi> the _random_ sample is built while the reading goes.

    FrPi> Suppose the file (or tape) holds N records (N is not
    FrPi> known in advance), from which we want a sample of M
    FrPi> records at most.  If N <= M, then we use the whole
    FrPi> file, no sampling is possible nor necessary.
    FrPi> Otherwise, we first initialise M records with the
    FrPi> first M records of the file.  Then, for each record in
    FrPi> the file after the M'th, the algorithm has to decide
    FrPi> if the record just read will be discarded or if it
    FrPi> will replace one of the M records already saved, and
    FrPi> in the latter case, which of those records will be
    FrPi> replaced.  If the algorithm is carefully designed,
    FrPi> when the last (N'th) record of the file will have been
    FrPi> processed this way, we may then have M records
    FrPi> randomly selected from N records, in such a a way that
    FrPi> each of the N records had an equal probability to end
    FrPi> up in the selection of M records.  I may seek out for
    FrPi> details if needed.

    FrPi> This is my suggestion, or in fact, more a thought that
    FrPi> a suggestion.  It might represent something useful
    FrPi> either for flat ASCII files or even for a stream of
    FrPi> records coming out of a database, if those effectively
    FrPi> do not offer ready random sampling devices.


    FrPi> P.S. - In the (rather unlikely, I admit) case the gang
    FrPi> I'm part of would have the need described above, and
    FrPi> if I then dared implementing it myself, would it be welcome?

I think this would be a very interesting tool and
I'm also intrigued about the details of the algorithm you
outline above.

If it would be made to work on all kind of read.table()-readable
files, (i.e. of course including *.csv);   that might be a valuable
tool for all those -- and there are many -- for whom working
with DBMs is too daunting initially.

Martin Maechler, ETH Zurich



From nassar at noos.fr  Fri Jan  6 11:31:04 2006
From: nassar at noos.fr (Naji)
Date: Fri, 06 Jan 2006 11:31:04 +0100
Subject: [R] A comment about R -> Link to a technical report from ATS,
 UCLA
In-Reply-To: <43BDF0AF.7060805@vanderbilt.edu>
Message-ID: <BFE40578.8AC4%nassar@noos.fr>

Hi all,

UCLA ATS Statistical Consulting Group has just launched a very interesting
paper comparing SPSS, SAS & Stata as Statistical Packages.. "Perhaps the
most notable exception to this discussion is R"
http://www.ats.ucla.edu/stat/technicalreports/
It's an interesting reading for this thread.

Best regards
Naji



From ripley at stats.ox.ac.uk  Fri Jan  6 12:08:23 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jan 2006 11:08:23 +0000 (GMT)
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <17342.11073.712221.863881@stat.math.ethz.ch>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<17342.11073.712221.863881@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0601061103210.641@gannet.stats>

On Fri, 6 Jan 2006, Martin Maechler wrote:

>>>>>> "FrPi" == Fran?ois Pinard <pinard at iro.umontreal.ca>
>>>>>>     on Thu, 5 Jan 2006 22:41:21 -0500 writes:
>
>    FrPi> [Brian Ripley]
>    >> I rather thought that using a DBMS was standard practice in the
>    >> R community for those using large datasets: it gets discussed rather
>    >> often.
>
>    FrPi> Indeed.  (I tried RMySQL even before speaking of R to my co-workers.)
>
>    >> Another possibility is to make use of the several DBMS interfaces already
>    >> available for R.  It is very easy to pull in a sample from one of those,
>    >> and surely keeping such large data files as ASCII not good practice.
>
>    FrPi> Selecting a sample is easy.  Yet, I'm not aware of any
>    FrPi> SQL device for easily selecting a _random_ sample of
>    FrPi> the records of a given table.  On the other hand, I'm
>    FrPi> no SQL specialist, others might know better.
>
>    FrPi> We do not have a need yet for samples where I work,
>    FrPi> but if we ever need such, they will have to be random,
>    FrPi> or else, I will always fear biases.
>
>    >> One problem with Francois Pinard's suggestion (the credit has got lost)
>    >> is that R's I/O is not line-oriented but stream-oriented.  So selecting
>    >> lines is not particularly easy in R.
>
>    FrPi> I understand that you mean random access to lines,
>    FrPi> instead of random selection of lines.  Once again,
>    FrPi> this chat comes out of reading someone else's problem,
>    FrPi> this is not a problem I actually have.  SPSS was not
>    FrPi> randomly accessing lines, as data files could well be
>    FrPi> hold on magnetic tapes, where random access is not
>    FrPi> possible on average practice.  SPSS reads (or was
>    FrPi> reading) lines sequentially from beginning to end, and
>    FrPi> the _random_ sample is built while the reading goes.
>
>    FrPi> Suppose the file (or tape) holds N records (N is not
>    FrPi> known in advance), from which we want a sample of M
>    FrPi> records at most.  If N <= M, then we use the whole
>    FrPi> file, no sampling is possible nor necessary.
>    FrPi> Otherwise, we first initialise M records with the
>    FrPi> first M records of the file.  Then, for each record in
>    FrPi> the file after the M'th, the algorithm has to decide
>    FrPi> if the record just read will be discarded or if it
>    FrPi> will replace one of the M records already saved, and
>    FrPi> in the latter case, which of those records will be
>    FrPi> replaced.  If the algorithm is carefully designed,
>    FrPi> when the last (N'th) record of the file will have been
>    FrPi> processed this way, we may then have M records
>    FrPi> randomly selected from N records, in such a a way that
>    FrPi> each of the N records had an equal probability to end
>    FrPi> up in the selection of M records.  I may seek out for
>    FrPi> details if needed.
>
>    FrPi> This is my suggestion, or in fact, more a thought that
>    FrPi> a suggestion.  It might represent something useful
>    FrPi> either for flat ASCII files or even for a stream of
>    FrPi> records coming out of a database, if those effectively
>    FrPi> do not offer ready random sampling devices.
>
>
>    FrPi> P.S. - In the (rather unlikely, I admit) case the gang
>    FrPi> I'm part of would have the need described above, and
>    FrPi> if I then dared implementing it myself, would it be welcome?
>
> I think this would be a very interesting tool and
> I'm also intrigued about the details of the algorithm you
> outline above.

It's called `reservoir sampling' and is described in my simulation book 
and Knuth and elsewhere.

> If it would be made to work on all kind of read.table()-readable
> files, (i.e. of course including *.csv);   that might be a valuable
> tool for all those -- and there are many -- for whom working
> with DBMs is too daunting initially.

It would be better (for the reasons I gave) to do this in a separate file 
preprocessor: read.table reads from a connection not a file, of course.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Fri Jan  6 12:34:38 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 6 Jan 2006 12:34:38 +0100
Subject: [R] ylim problem in barplot
In-Reply-To: <loom.20060105T201951-100@post.gmane.org>
References: <77C117DE9E87354CB7804A2DE7D566C2387293@amedmlermc132>
	<17341.10492.905351.841579@stat.math.ethz.ch>
	<014101c61224$d3b75d50$a00c010a@BigBaer>
	<loom.20060105T201951-100@post.gmane.org>
Message-ID: <17342.21966.501078.566590@stat.math.ethz.ch>

>>>>> "Ben" == Ben Bolker <bolker at ufl.edu>
>>>>>     on Thu, 5 Jan 2006 19:21:48 +0000 (UTC) writes:

    Ben> Robert Baer <rbaer <at> atsu.edu> writes:
    >> Well, consider this example:
    >> barplot(c(-200,300,-250,350),ylim=c(-99,400))
    >> 
    >> It seems that barplot uses ylim and pretty to decide things about the axis
    >> but does some slightly unexpected things with the bars themselves that are
    >> not just at the 'zero' end of the bar.
    >> 
    >> Rob

no, there's no pretty() involved.  
Maybe it helps you to just type box()
after the plot.  Simply, the usual par("mar") margins are set.

I think ___in conclusion___  that Marc Schwartz'  solution has been
right on target all along:

>>>>   Use 'xpd = FALSE' if you set 'ylim' because otherwise, the
>>>>   result may be confusing.

The real "problem" of barplot.default() is the fact that 
'xpd = TRUE' is the default, and AFAIK that's not the case
for other high-level plot functions.

One could debate if the default setting for xpd should not be changed to
  
   xpd = (is.null(ylim) && !horiz) || (is.null(xlim) && horiz)

Now this has definitely gotten a topic for R-devel, and not
R-help anymore.

    Ben> in previous cases I think there was room for debate about
    Ben> the appropriate behavior.  What do you think should happen
    Ben> in this case?  Cutting off the bars seems like the right thing
    Ben> to do; 

    Ben> is your point that the axis being confined to positive values (a side effect of setting ylim) is weird?

    Ben> Ben



From p.dalgaard at biostat.ku.dk  Fri Jan  6 13:42:08 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jan 2006 13:42:08 +0100
Subject: [R] A comment about R -> Link to a technical report from ATS,
	UCLA
In-Reply-To: <BFE40578.8AC4%nassar@noos.fr>
References: <BFE40578.8AC4%nassar@noos.fr>
Message-ID: <x2oe2pttnj.fsf@viggo.kubism.ku.dk>

Naji <nassar at noos.fr> writes:

> Hi all,
> 
> UCLA ATS Statistical Consulting Group has just launched a very interesting
> paper comparing SPSS, SAS & Stata as Statistical Packages.. "Perhaps the
> most notable exception to this discussion is R"
> http://www.ats.ucla.edu/stat/technicalreports/
> It's an interesting reading for this thread.

In fact, if you trace the thread back to its root, this is what
started it...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Arne.Muller at sanofi-aventis.com  Fri Jan  6 14:47:37 2006
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Fri, 6 Jan 2006 14:47:37 +0100
Subject: [R] RMySQL/DBI
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF49F@CRBSMXSUSR04>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/ca317d84/attachment.pl

From john.marsland at mac.com  Fri Jan  6 14:40:27 2006
From: john.marsland at mac.com (John Marsland)
Date: Fri, 6 Jan 2006 13:40:27 +0000 (UTC)
Subject: [R] Wikis etc.
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
Message-ID: <loom.20060106T142637-324@post.gmane.org>

I agree.

In desperation at my inbox being swamped by messages I contacted the R-core team
to ask about other solutions. They recommended gmane.org who compile a
web-viewable archive of thousands of email lists - it even provides RSS feeds
for new topics.

Going back to the wiki issue, it might be wise to this about using Trac
<http://projects.edgewall.com/trac/> which is an open source project that
integrates a wiki with the SVN code versioning system (used by R-project) and a
replacement for bugzilla's ticketing system. We use it to document our own code.

Trac would have the advantage of pushing questions on the R list back towards
the  actual source code and allowing all users to participate in the future
development of the software.

John Marsland



From academic at feferraz.net  Fri Jan  6 15:06:01 2006
From: academic at feferraz.net (Fernando Henrique Ferraz P. da Rosa)
Date: Fri, 6 Jan 2006 12:06:01 -0200
Subject: [R] Wikis etc.
In-Reply-To: <loom.20060106T142637-324@post.gmane.org>
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
	<loom.20060106T142637-324@post.gmane.org>
Message-ID: <20060106140601.GA17930@ime.usp.br>

John Marsland writes:
> Trac would have the advantage of pushing questions on the R list back towards
> the  actual source code and allowing all users to participate in the future
> development of the software.
> 

        I see that this could be useful for R-devel, but considering the
volume of traffic and the kind of contents on R-help, I don't think such
tying to the actual source code would be so useful. Perhaps trac could
be used as an integrated interface for r-devel/svn and the bug track
system, and another wiki solution be used exclusiverly for the r-help
community (which includes many people not directly interested in coding
or development issues).

--
"Though this be randomness, yet there is structure in't."
                                           Rosa, F.H.F.P

Instituto de Matem??tica e Estat??stica
Universidade de S??o Paulo
Fernando Henrique Ferraz P. da Rosa
http://www.feferraz.net



From spencer.graves at pdf.com  Fri Jan  6 15:14:11 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Jan 2006 06:14:11 -0800
Subject: [R] Use Of makeARIMA
In-Reply-To: <14850601FF012647A90A5DB31F96DB3731396D@INBLRDC01.BANG.irpvl.com>
References: <14850601FF012647A90A5DB31F96DB3731396D@INBLRDC01.BANG.irpvl.com>
Message-ID: <43BE7B33.9090309@pdf.com>

	  I have not seen a reply to this post, so I will attempt a feeble 
response.  I've been wanting to learn more about these commands and 
suffering, like you, from the paucity of examples to follow.  To get 
started, after reading the help pages for all the commands you 
mentioned, I tried to think of the simplest example that might help me 
learn something about this.  This question led to the following:


set.seed(3)
y3 <- rep(0:2, 10)+0.1*rnorm(30)
acf(y3) # ACF suggest a period 3 seasonal
pacf(y3)# PACF suggests a pure AR of order at most 3

(fit3 <- arima(y3, seasonal=list(order=c(1,0,0), period=3)))

attributes(fit3)
fit3$model # Compare with the documentation for 'makeARIMA'

KalmanForecast(mod=fit3$model)

	  If I wanted to understand better "makeARIMA" in particular, I listed 
"arima" and did a search for "makeARIMA":  "arima" clearly uses 
"makeARIMA".  If you run 'debug(arima)' then the above 'arima' command, 
you can step through the 'arima' function line by line and look at (and 
modify) any of the objects that function creates and uses.  In 
particular, you will be able to see exactly how the "arima" command uses 
the "makeARIMA" function.

	  Hope this helps.
	  spencer graves
p.s.  If you'd like more help from this group, please submit another 
question.  Before you do, however, I suggest you first read the posting 
guide! "www.R-project.org/posting-guide.html".  Anecdotal evidence 
suggests that posts more consistent with that guide are more likely to 
receive more useful replies quicker.


Sumanta Basak wrote:

> Hi R-Experts,
> 
>  
> 
> Currently I'm using an univariate time series in which I'm going to
> apply KalmanLike(),KalmanForecast (),KalmanSmooth(), KalmanRun(). For I
> use it before makeARIMA () but I don't understand and i don't know to
> include the seasonal coefficients. Can anyone help me citing a suitable
> example? Thanks in advance.
> 
>  
> 
>  
> 
> ------------------------------------------
> 
> SUMANTA BASAK.
> 
> ------------------------------------------
> 
> <http://www.drsb24.blogspot.com/>  
> 
>  
> 
> 
> -------------------------------------------------------------------------------------------------------------------
> This e-mail may contain confidential and/or privileged infor...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Fri Jan  6 15:15:56 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 6 Jan 2006 09:15:56 -0500
Subject: [R] Wikis etc.
In-Reply-To: <loom.20060106T142637-324@post.gmane.org>
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
	<loom.20060106T142637-324@post.gmane.org>
Message-ID: <20060106141556.GA25875@psych.upenn.edu>

On 01/06/06 13:40, John Marsland wrote:
> Going back to the wiki issue, it might be wise to this about using Trac
> <http://projects.edgewall.com/trac/> which is an open source project that
> integrates a wiki with the SVN code versioning system (used by R-project) and a
> replacement for bugzilla's ticketing system. We use it to document our own code.
> 
> Trac would have the advantage of pushing questions on the R list back towards
> the  actual source code and allowing all users to participate in the future
> development of the software.

It isn't clear to me what this would be for.  I'm not sure that I
trust users to modify code.

I was thinking myself that user input might be most useful for
the documentation of functions.  Not that this is so bad, but
rather it might be possible to have an extended system of
documentation on the web, with FAQ-type questions answered as
part of the documentation itself, so that people would not have
to rely on R-help so much (even in its archived forms).

And I was thinking of setting up a Wiki with one page per
function.  (Given that there are now hundreds or thousands of
functions, setting this up would have to be automated.)  I've
just installed (for another purpose) TWiki, which seems to have
some nice features for this sort of thing (in particular, data
stored as text files, hence easily manipulated by other
programs), but I will not have time to think through how to do
this for some time.  Just another idea to throw into the hopper.

In principle, another possibility is to do something like the PHP 
manual at http://www.php.net/manual/en/, which is not a wiki but
more like a bulletin board, with discussion of each command.  But 
I think a wiki is better.  I found it time consuming to read
through all those comments, almost as bad as reading through
R-help postings. :)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From john.marsland at wmgfunds.com  Fri Jan  6 15:18:29 2006
From: john.marsland at wmgfunds.com (John Marsland)
Date: Fri, 6 Jan 2006 14:18:29 +0000
Subject: [R] Wikis etc.
In-Reply-To: <93e8cdf40601060618x6668a5b9y6cdfd81ea292c5b1@mail.gmail.com>
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
	<loom.20060106T142637-324@post.gmane.org>
	<20060106140601.GA17930@ime.usp.br>
	<93e8cdf40601060618x6668a5b9y6cdfd81ea292c5b1@mail.gmail.com>
Message-ID: <93e8cdf40601060618v5702ce99ye3765f90f84f33d2@mail.gmail.com>

On 1/6/06, John Marsland <john.marsland at mac.com> wrote:
> I see your point. Maybe the answer is to use the list for R-help style
> questions, but encourage people who answer questions to point the the
> answers in the wiki - which they might have enhanced if necessary.
>
> On 1/6/06, Fernando Henrique Ferraz P. da Rosa <academic at feferraz.net> wrote:
> > John Marsland writes:
> > > Trac would have the advantage of pushing questions on the R list back towards
> > > the  actual source code and allowing all users to participate in the future
> > > development of the software.
> > >
> >
> >         I see that this could be useful for R-devel, but considering the
> > volume of traffic and the kind of contents on R-help, I don't think such
> > tying to the actual source code would be so useful. Perhaps trac could
> > be used as an integrated interface for r-devel/svn and the bug track
> > system, and another wiki solution be used exclusiverly for the r-help
> > community (which includes many people not directly interested in coding
> > or development issues).
> >
> > --
> > "Though this be randomness, yet there is structure in't."
> >                                            Rosa, F.H.F.P
> >
> > Instituto de Matem??tica e Estat??stica
> > Universidade de S??o Paulo
> > Fernando Henrique Ferraz P. da Rosa
> > http://www.feferraz.net
> >
> >
>



From john.marsland at mac.com  Fri Jan  6 15:23:23 2006
From: john.marsland at mac.com (John Marsland)
Date: Fri, 6 Jan 2006 14:23:23 +0000
Subject: [R] Wikis etc.
In-Reply-To: <20060106141556.GA25875@psych.upenn.edu>
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
	<loom.20060106T142637-324@post.gmane.org>
	<20060106141556.GA25875@psych.upenn.edu>
Message-ID: <93e8cdf40601060623x765fd0cckc71bb1eebcbfad22@mail.gmail.com>

It isn't so much that users modify the code as they would have to do
that in the usual way by checking out the project from the SVN.

Rather that extended documentation, features and enhancements etc. can
easily locate and quote from the code base and the differencing engine
as applied to the code base between versions.

On 1/6/06, Jonathan Baron <baron at psych.upenn.edu> wrote:
>
> It isn't clear to me what this would be for.  I'm not sure that I
> trust users to modify code.



From B.Rowlingson at lancaster.ac.uk  Fri Jan  6 15:36:21 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 06 Jan 2006 14:36:21 +0000
Subject: [R] Wikis etc.
In-Reply-To: <20060106141556.GA25875@psych.upenn.edu>
References: <43BDC208.5060909@vanderbilt.edu>
	<43BDC6B7.3010503@utoronto.ca>	<loom.20060106T142637-324@post.gmane.org>
	<20060106141556.GA25875@psych.upenn.edu>
Message-ID: <43BE8065.1080909@lancaster.ac.uk>

Jonathan Baron wrote:

> And I was thinking of setting up a Wiki with one page per
> function.  (Given that there are now hundreds or thousands of
> functions, setting this up would have to be automated.) 

  One page per R manual page file would probably suffice. You could do 
something along the lines of the Zope book, where users can add comments 
but you can browse with comments off:

http://www.zope.org/Documentation/Books/ZopeBook/2_6Edition/AdvDTML.stx

  then toggle the 'Com On' button. This is less of a wiki and more of an 
annotation service.

but I think you'd run into problems with losing all the annotation when 
a new R version comes out.

Barry



From Stefan.Eichenberger at se-kleve.com  Fri Jan  6 15:38:35 2006
From: Stefan.Eichenberger at se-kleve.com (Stefan Eichenberger)
Date: Fri, 6 Jan 2006 15:38:35 +0100
Subject: [R]  A comment about R:
Message-ID: <000501c612ce$e45a1670$6602a8c0@DELL266>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/dac37835/attachment.pl

From murdoch at stats.uwo.ca  Fri Jan  6 15:41:30 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jan 2006 09:41:30 -0500
Subject: [R] Wikis etc.
In-Reply-To: <20060106141556.GA25875@psych.upenn.edu>
References: <43BDC208.5060909@vanderbilt.edu>
	<43BDC6B7.3010503@utoronto.ca>	<loom.20060106T142637-324@post.gmane.org>
	<20060106141556.GA25875@psych.upenn.edu>
Message-ID: <43BE819A.2020002@stats.uwo.ca>

On 1/6/2006 9:15 AM, Jonathan Baron wrote:
> On 01/06/06 13:40, John Marsland wrote:
>> Going back to the wiki issue, it might be wise to this about using Trac
>> <http://projects.edgewall.com/trac/> which is an open source project that
>> integrates a wiki with the SVN code versioning system (used by R-project) and a
>> replacement for bugzilla's ticketing system. We use it to document our own code.
>> 
>> Trac would have the advantage of pushing questions on the R list back towards
>> the  actual source code and allowing all users to participate in the future
>> development of the software.
> 
> It isn't clear to me what this would be for.  I'm not sure that I
> trust users to modify code.
> 
> I was thinking myself that user input might be most useful for
> the documentation of functions.  Not that this is so bad, but
> rather it might be possible to have an extended system of
> documentation on the web, with FAQ-type questions answered as
> part of the documentation itself, so that people would not have
> to rely on R-help so much (even in its archived forms).
> 
> And I was thinking of setting up a Wiki with one page per
> function.  (Given that there are now hundreds or thousands of
> functions, setting this up would have to be automated.)  I've
> just installed (for another purpose) TWiki, which seems to have
> some nice features for this sort of thing (in particular, data
> stored as text files, hence easily manipulated by other
> programs), but I will not have time to think through how to do
> this for some time.  Just another idea to throw into the hopper.

I think this sounds like a great idea.  I would like to see two way 
connections between this and the existing man pages, e.g. in the HTML or 
PDF versions, links that go directly to the Wiki, and links from the 
Wiki to an online copy of the man pages.

If your automatic setup permitted it, then showing the output of the 
examples on the man pages would be nice.

One issue that you'll need to think about is whether there is one page 
per function, or one page per .Rd file, or some other organization:  and 
you'll need to be prepared for changes in the organization of the 
documentation with new R releases (and changes in function names, and 
changes in the examples...).

Duncan Murdoch

> 
> In principle, another possibility is to do something like the PHP 
> manual at http://www.php.net/manual/en/, which is not a wiki but
> more like a bulletin board, with discussion of each command.  But 
> I think a wiki is better.  I found it time consuming to read
> through all those comments, almost as bad as reading through
> R-help postings. :)
> 
> Jon



From sfalcon at fhcrc.org  Fri Jan  6 16:04:51 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 06 Jan 2006 07:04:51 -0800
Subject: [R] Wikis etc.
In-Reply-To: <43BE8065.1080909@lancaster.ac.uk> (Barry Rowlingson's message of
	"Fri, 06 Jan 2006 14:36:21 +0000")
References: <43BDC208.5060909@vanderbilt.edu> <43BDC6B7.3010503@utoronto.ca>
	<loom.20060106T142637-324@post.gmane.org>
	<20060106141556.GA25875@psych.upenn.edu>
	<43BE8065.1080909@lancaster.ac.uk>
Message-ID: <m2wthd1jos.fsf@ziti.local>

Regarding systems for presenting documentation and allowing user
comments, I recently came across Commentary (see homepage
http://pythonpaste.org/commentary/).

Haven't used it, but my impression is that comments and the main doc
are both stored in svn (and auto-committed for comment changes).  This
might help solve the problem of updating the doc upon a new R release
because you could take advantage of svn merge.

Of course, svn merge won't know whether the comments are still
appropriate or not :-(

Nevermind.

+ seth



From Stefan.Eichenberger at se-kleve.com  Fri Jan  6 16:18:16 2006
From: Stefan.Eichenberger at se-kleve.com (Stefan Eichenberger)
Date: Fri, 6 Jan 2006 16:18:16 +0100
Subject: [R]   A comment about R:
Message-ID: <000201c612d4$74ee5160$6602a8c0@DELL266>

~~~~~~~~~~~~~~~
... blame me for not having sent below message initially in
plain text format. Sorry!
~~~~~~~~~~~~~~~

I just got into R for most of the Xmas vacations and was about to ask 
for helping  pointer on how to get a hold of R when I came across this 
thread. I've read through  most it and would like to comment from a 
novice user point of view. I've a strong  programming background but 
limited statistical experience and no knowledge on  competing packages. 
I'm working as a senior engineer in electronics.

Yes, the learning curve is steep. Most of the docu is extremely terse. 
Learning is mostly from examples (a wiki was proposed in another 
mail...), documentation uses no graphical elements at all. So, when it 
comes to things like xyplot in lattice: where would I get the concepts 
behind panels, superpanels, and the like?

ok., this is steep and terse, but after a while I'll get over it... 
That's life. The general concept is great, things can be expressed very 
densly: Potential  is here.... I quickly had 200 lines of my own code 
together, doing what it should -  or so I believed.

Next I did:
  matrix<-matrix(1:100, 10, 10)
  image(matrix)
  locator()
Great: I can interactively work with my graphs... But then:
  filled.contour(matrix)
  locator()
Oops - wrong coordinates returned. Bug. Apparently, locator() doen't 
realize that fitted.contour() has a color bar to the right and scales x 
wrongly...

Here is what really shocked me:

> str(bar) `data.frame':   206858 obs. of  12 variables:  ...
> str(mean(bar[,6:12]))
  Named num [1:7] 1.828 2.551 3.221 1.875 0.915 ...
  ...
> str(sd(bar[,6:12]))
  Named num [1:7] 0.0702 0.1238 0.1600 0.1008 0.0465 ...
  ...
> prcomp(bar[,6:12])->foo
> str(foo$x)
  num [1:206858, 1:7] -0.4187 -0.4015  0.0218 -0.4438 -0.3650 ...
  ...
> str(mean(foo$x))
  num -1.07e-13
> str(sd(foo$x))
  Named num [1:7] 0.32235 0.06380 0.02254 0.00337 0.00270 ...
  ...

So, sd returns a vector independent on whether the arguement is a matrix 
or data.frame, but mean reacts differently and returns a vector only 
against a data.frame?

The problem here is not that this is difficult to learn - the problem is 
the complete absense of a concept. Is a data.frame an 'extended' matrix 
with columns of different types or  something different? Since the 
numeric mean (I expected a vector) is recycled nicely  when used in a 
vector context, this makes debugging code close to impossible. Since  sd 
returns a vector, things like mean + 4*sd vary sufficiently across the 
data elements that I assume working code... I don't get any warning 
signal that something is wrong here.

The point in case is the behavior of locator() on a filled.contour() 
object: Things apparently  have been programmed and debugged from 
example rather than concept.

Now, in another posting I read that all this is a feature to discourge 
inexperienced users from statistics and force you to think before you do 
things. Whilst I support this concept of thinking: Did I miss something 
in statistics? I was in the believe that mean and sd were relatively 
close to each other conceptually... (here, they are even in different 
packages...)

I will continue using R for the time being. But whether I can recommend 
it to my work  collegues remains to be seen: How could I ever trust 
results returned?

I'm still impressed by some of the efficiency, but my trust is deeply 
shaken...
-----------------------------------------------------------------------
Stefan Eichenberger        mailto:Stefan.Eichenberger at se-kleve.com



From liuwensui at gmail.com  Fri Jan  6 16:23:58 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 6 Jan 2006 10:23:58 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <38b9f0350601050948n75f9e7abs@mail.gmail.com>
References: <20060105154609.GA7009@phenix.sram.qc.ca>
	<644e1f320601050859l6d573435q9b3004690758cfda@mail.gmail.com>
	<38b9f0350601050948n75f9e7abs@mail.gmail.com>
Message-ID: <1115a2b00601060723g3c954efdwa67ce775e76218e2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/46d3cceb/attachment.pl

From olof.leimar at zoologi.su.se  Fri Jan  6 16:27:31 2006
From: olof.leimar at zoologi.su.se (Olof Leimar)
Date: Fri, 06 Jan 2006 16:27:31 +0100
Subject: [R] lmer p-vales are sometimes too small
Message-ID: <43BE8C63.1050600@zoologi.su.se>

This concerns whether p-values from lmer can be trusted. From 
simulations, it seems that lmer can produce very small, and probably 
spurious, p-values. I realize that lmer is not yet a finished product. 
Is it likely that the problem will be fixed in a future release of the 
lme4 package?

Using simulated data for a quite standard mixed-model anova (a balanced 
two-way design; see code for the function SimMixed pasted below), I 
compared the output of lmer, for three slightly different models, with 
the output of aov. For an example where there is no fixed treatment 
effect (null hypothesis is true), with 4 blocks, 2 treatments, and 40 
observations per treatment-block combination, I find that lmer gives 
more statistical significances than it should, whereas aov does not have 
this problem. An example of output I generated by calling
  > SimMixed(1000)
is the following:

Proportion significances at the 0.05 level
aov:     0.05
lmer.1:  0.148
lmer.2:  0.148
lmer.3:  0.151

Proportion significances at the 0.01 level
aov:     0.006
lmer.1:  0.076
lmer.2:  0.076
lmer.3:  0.077

Proportion significances at the 0.001 level
aov:     0.001
lmer.1:  0.047
lmer.2:  0.047
lmer.3:  0.047

which is based on 1000 simulations (and takes about 5 min on my PowerMac 
G5). The different models fitted are:

fm.aov <- aov(y ~ Treat + Error(Block/Treat), data = dat)
fm.lmer.1 <- lmer(y ~ Treat + (Treat|Block), data = dat)
fm.lmer.2 <- lmer(y ~ Treat + (Treat-1|Block), data = dat)
fm.lmer.3 <- lmer(y ~ Treat + (1|Block) + (Treat-1|Block), data = dat)

It seems that, depending on the level of the test, lmer gives between a 
factor of 3 to a factor of around 50 times too many significances. The 
first two lmer models seem to give identical results, whereas the third 
(which I think perhaps is the one that best represents the data 
generated by the simulation) differs slightly. In running the 
simulations, warnings like this are occasionally generated:

Warning message:
optim or nlminb returned message false convergence (8)
  in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance = 
1.49011611938477e-08,

They seem to derive from the third of the lmer models. Perhaps there is 
some numerical issue in the lmer function? From running SimMixed() 
several times, I have noticed that large p-values (say, larger than 0.5) 
agree very well between lmer and aov, but there seems to be a systematic 
discrepancy for smaller p-values, where lmer gives smaller values than 
aov. The F-values agree between all analyzes (except for fm.lmer.3 when 
there is a warning), so there is a systematic difference between lmer 
and aov in how a p-value is obtained from the F-value, which becomes 
severe for small p-values.



My output from sessionInfo()

R version 2.2.1, 2005-12-20, powerpc-apple-darwin7.9.0

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils" 
"datasets"  "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-3"



Pasted code for the SimMixed function (some lines might wrap):

# This function generates n.sims random data sets for a design with 4
# blocks, 2 treatments applied to each block, and 40 replicate
# observations for each block-treatment combination. There is no true
# fixed treatment effect, so a statistical significance of a test for
# a fixed treatment effect ought to occur with a probability equal to
# the nominal level of the test. Four tests are applied to each
# simulated data set: the classical aov and three versions of lmer,
# corresponding to different model formulations. The proportion of
# tests for a fixed treatment effect that become significant at the
# 0.05 0.01 and 0.001 levels are printed, as well as the p-values for
# the last of the simulations. In my runs, lmer gives significance
# more often than indicated by the nominal level, for each of the
# three models, whereas aov is OK. The package lme4 needs to be loaded
# to run the code.

SimMixed <- function(n.sims = 1) {
   k <- 4                # number of blocks
   n <- 40               # num obs per block X treatment combination
   m1 <- 1.0             # fixed effect of level 1 of treatment
   m2 <- m1              # fixed effect of level 2 of treatment
   sd.block <- 0.5       # SD of block random effect
   sd.block.trt <- 1.0   # SD of random effect for block X treatm
   sd.res <- 0.1         # Residual SD
   Block <- factor( rep(1:k, each=2*n) )
   Treat <- factor( rep( rep(c("Tr1","Tr2"), k), each=n) )
   m <- rep( rep(c(m1, m2), k), each=n) # fixed effects
   # storage for p-values
   p.aov <- rep(0, n.sims)
   p.lmer.1 <- rep(0, n.sims)
   p.lmer.2 <- rep(0, n.sims)
   p.lmer.3 <- rep(0, n.sims)
   for (i in 1:n.sims) {
     # first get block and treatment random deviations
     b <- rep( rep(rnorm(k, 0, sd.block), each=2) +
              rnorm(2*k, 0, sd.block.trt), each=n )
     # then get response
     y <- m + b + rnorm(2*k*n, 0, sd.res)
     dat <- data.frame(Block, Treat, y)
     # perform the tests
     fm.aov <- aov(y ~ Treat+Error(Block/Treat), data = dat)
     fm.lmer.1 <- lmer(y ~ Treat+(Treat|Block), data = dat)
     fm.lmer.2 <- lmer(y ~ Treat+(Treat-1|Block), data = dat)
     fm.lmer.3 <- lmer(y ~ Treat+(1|Block)+(Treat-1|Block), data = dat)
     # store the p-values
     p.aov[i] <- summary(fm.aov)$"Error: Block:Treat"[[1]]$"Pr(>F)"[1]
     p.lmer.1[i] <- anova(fm.lmer.1)[6]
     p.lmer.2[i] <- anova(fm.lmer.2)[6]
     p.lmer.3[i] <- anova(fm.lmer.3)[6]
   }

   cat("\nProportion significances at the 0.05 level \n")
   cat("aov:    ", sum(p.aov<0.05)/n.sims, "\n")
   cat("lmer.1: ", sum(p.lmer.1<0.05)/n.sims, "\n")
   cat("lmer.2: ", sum(p.lmer.2<0.05)/n.sims, "\n")
   cat("lmer.3: ", sum(p.lmer.3<0.05)/n.sims, "\n")

   cat("\nProportion significances at the 0.01 level \n")
   cat("aov:    ", sum(p.aov<0.01)/n.sims, "\n")
   cat("lmer.1: ", sum(p.lmer.1<0.01)/n.sims, "\n")
   cat("lmer.2: ", sum(p.lmer.2<0.01)/n.sims, "\n")
   cat("lmer.3: ", sum(p.lmer.3<0.01)/n.sims, "\n")

   cat("\nProportion significances at the 0.001 level \n")
   cat("aov:    ", sum(p.aov<0.001)/n.sims, "\n")
   cat("lmer.1: ", sum(p.lmer.1<0.001)/n.sims, "\n")
   cat("lmer.2: ", sum(p.lmer.2<0.001)/n.sims, "\n")
   cat("lmer.3: ", sum(p.lmer.3<0.001)/n.sims, "\n")

   cat("\nFinal aov analysis: \n")
   print(summary(fm.aov)$"Error: Block:Treat")
   cat("\nFinal lmer analysis 1: \n")
   print(anova(fm.lmer.1))
   cat("\nFinal lmer analysis 2: \n")
   print(anova(fm.lmer.2))
   cat("\nFinal lmer analysis 3: \n")
   print(anova(fm.lmer.3))
}




-- 
Olof Leimar, Professor
Department of Zoology
Stockholm University
SE-106 91 Stockholm
Sweden

olof.leimar at zoologi.su.se
http://www.zoologi.su.se/research/leimar/



From brian_cade at usgs.gov  Fri Jan  6 16:44:06 2006
From: brian_cade at usgs.gov (Brian S Cade)
Date: Fri, 6 Jan 2006 08:44:06 -0700
Subject: [R] inverse prediction intervals for nonlinear least squares
Message-ID: <OF9C5C39C3.A528F138-ON872570EE.0054FE33-872570EE.005687FF@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/a3c54315/attachment.pl

From tplate at acm.org  Fri Jan  6 16:54:54 2006
From: tplate at acm.org (Tony Plate)
Date: Fri, 06 Jan 2006 08:54:54 -0700
Subject: [R] Wikis etc.
In-Reply-To: <43BDC208.5060909@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu>
Message-ID: <43BE92CE.4070500@acm.org>

I second Frank's comment!  I wonder if questioners who receive a bunch 
of useful replies could be encouraged to enter a summary of those on a 
Wiki, in much the same way as users of S-news were expected to post a 
summary of their answers as a way of giving something back.

An existing R Wiki is located at 
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

However, there's currently not much on it.  Recently on R-help there was 
  a summary of using databases with R, which looked very useful, so I 
put that on the Wiki.  Maybe if others just start putting things there 
it can gather momentum?

-- Tony Plate

Frank E Harrell Jr wrote:
> I feel that as long as people continue to provide help on r-help wikis 
> will not be successful.  I think we need to move to a central wiki or 
> discussion board and to move away from e-mail.  People are extremely 
> helpful but e-mail seems to be to always be memory-less and messages get 
> too long without factorization of old text.  R-help is now too active 
> and too many new users are asking questions asked dozens of times for 
> e-mail to be effective.
> 
> The wiki also needs to collect and organize example code, especially for 
> data manipulation.  I think that new users would profit immensely from a 
> compendium of examples.
> 
> Just my .02 Euros
> 
> Frank



From slacey at umich.edu  Fri Jan  6 17:20:13 2006
From: slacey at umich.edu (Steven Lacey)
Date: Fri, 6 Jan 2006 11:20:13 -0500
Subject: [R] help with strip.default
Message-ID: <000501c612dd$16a0b0e0$6700a8c0@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/0f3aee2b/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Fri Jan  6 16:26:10 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 06 Jan 2006 15:26:10 +0000
Subject: [R] [Rd] Multiplication
In-Reply-To: <Pine.LNX.4.64.0601060712540.7293@homer24.u.washington.edu>
References: <20060106144345.5682219AC0@slim.kubism.ku.dk>
	<Pine.LNX.4.64.0601060712540.7293@homer24.u.washington.edu>
Message-ID: <43BE8C12.1010400@lancaster.ac.uk>

[crossed over to r-help since its not a bug and not a devel thing any more]

Thomas Lumley wrote:

> So is -2^2.  The precedence of ^ is higher than that of unary minus. It 
> may be surprising, but it *is* documented and has been in S for a long 
> time.

And just about every other programming language:

Matlab:

 >> -2^2

ans =

     -4


Maxima:

(C1) -2^2;
(D1) 				      - 4

Fortran:
       print *,-2**2
  -4

Perl:

$ perl -e 'print -2^2'
4294967292

  Oops. I mean:

$ perl -e 'print -2**2'
-4

  The precendence of operators is remarkably consistent over programming 
languages over time. It seems natural for me now that ^ is done before 
unary minus, but I don't know if that's because I've been doing that for 
25 years or because its really more natural.

  Anyone got a counter example where unary minus is higher than power?

Barry



From macq at llnl.gov  Fri Jan  6 17:38:27 2006
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 6 Jan 2006 08:38:27 -0800
Subject: [R] Wikis etc.
In-Reply-To: <43BDC208.5060909@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu>
Message-ID: <p06210200bfe4486c7adb@[128.115.153.6]>

I don't have any significant experience with wikis, but I have yet to 
use any discussion board that was anywhere near as useful to me, or 
as easy to use, as an email list.

Discussion boards have a web browser interface. Typically, they 
display at most a dozen topics at a time. Scrolling to get the next 
dozen is slow, as it requires a download from some web server. There 
is a huge amount of wasted screen space. When there is a topic that 
generates many messages scrolling through them is slow, as some 
discussion board interfaces show only 6 or 7 at a time. Search 
engines provided by the discussion board software are limited and 
slow.

In contrast, in my email client I can show about three dozen subject 
lines at a time, I can quickly scroll up and down through the list, I 
can quickly group all the messages with the same subject line with a 
single click of the mouse. I can easily and quickly store selected 
messages of particular interest to a place where I can easily find 
them again. My email software searches very quickly through a huge 
number of messages.

Then there's the question of administration and maintenance. Who is 
going to set up the wiki or discussion board categories? As far as I 
can tell (and that's actually not very far), either of them would 
require a lot more time and effort to set up and maintain than the 
present email list.

Yes, r-help has a huge volume -- right now, my R-help mailbox has 
almost 22,000 messages in it, 2004-01-02 to the present; its size is 
about 124 mb. Yes, there is a lot of duplication. None the less, I 
find it easier and quicker to scan the subject lines a few times a 
day for interesting-looking topics than it would be to go to a 
browser and have to navigate up and down through various categories, 
looking for interesting-looking topics.

As far as I can tell, the wiki concept is more along the lines of a 
reference library, whereas mailing lists and discussion boards are 
meant for people to ask each other questions, and give each other 
answers. If that perception is at all accurate, I would have to say 
that a wiki is by no means a suitable replacement for an email list. 
And when it comes to a choice between an email list and a discussion 
board, I have a strong preference for the email list.

-Don

At 7:04 PM -0600 1/5/06, Frank E Harrell Jr wrote:
>I feel that as long as people continue to provide help on r-help wikis
>will not be successful.  I think we need to move to a central wiki or
>discussion board and to move away from e-mail.  People are extremely
>helpful but e-mail seems to be to always be memory-less and messages get
>too long without factorization of old text.  R-help is now too active
>and too many new users are asking questions asked dozens of times for
>e-mail to be effective.
>
>The wiki also needs to collect and organize example code, especially for
>data manipulation.  I think that new users would profit immensely from a
>compendium of examples.
>
>Just my .02 Euros
>
>Frank
>--
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From pburns at pburns.seanet.com  Fri Jan  6 17:42:23 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 06 Jan 2006 16:42:23 +0000
Subject: [R] [Rd] Multiplication
In-Reply-To: <43BE8C12.1010400@lancaster.ac.uk>
References: <20060106144345.5682219AC0@slim.kubism.ku.dk>	<Pine.LNX.4.64.0601060712540.7293@homer24.u.washington.edu>
	<43BE8C12.1010400@lancaster.ac.uk>
Message-ID: <43BE9DEF.60303@pburns.seanet.com>

BR>   Anyone got a counter example where unary minus is 
BR> higher than power?

Excel (at least sort of).  See

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Barry Rowlingson wrote:

>[crossed over to r-help since its not a bug and not a devel thing any more]
>
>Thomas Lumley wrote:
>
>  
>
>>So is -2^2.  The precedence of ^ is higher than that of unary minus. It 
>>may be surprising, but it *is* documented and has been in S for a long 
>>time.
>>    
>>
>
>And just about every other programming language:
>
>Matlab:
>
> >> -2^2
>
>ans =
>
>     -4
>
>
>Maxima:
>
>(C1) -2^2;
>(D1) 				      - 4
>
>Fortran:
>       print *,-2**2
>  -4
>
>Perl:
>
>$ perl -e 'print -2^2'
>4294967292
>
>  Oops. I mean:
>
>$ perl -e 'print -2**2'
>-4
>
>  The precendence of operators is remarkably consistent over programming 
>languages over time. It seems natural for me now that ^ is done before 
>unary minus, but I don't know if that's because I've been doing that for 
>25 years or because its really more natural.
>
>  Anyone got a counter example where unary minus is higher than power?
>
>Barry
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From gunter.berton at gene.com  Fri Jan  6 17:48:38 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 6 Jan 2006 08:48:38 -0800
Subject: [R] help with strip.default
In-Reply-To: <000501c612dd$16a0b0e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
Message-ID: <200601061648.k06GmSEw008589@hertz.gene.com>

Steve:

This is a question for **super Deepayan,** and hopefully he'll respond. 

However, in the interim, let me give it a shot. Basically, I think what
you've asked for falls outside the bounds of what lattice is designed to do.
But I think there's a simple way to fool it. Basically what you need to do
is to combine your two factors into one with level names and ordering as you
want. See ?factor (?ordered may also be useful, but you don't need it). For
example:

comb.factor=factor(paste(A,B,sep='.'))

As I said, you may have to reorder the levels from the default that factor()
gives you to get your panels to display the way you want. Also see the
perm.cond and index.cond arguments of xyplot, which might also suffice for
that purpose.

Again, Deepayan will hopefully suggest a cleverer way that I missed. But I
think this approach will get you what you want.

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Steven Lacey
> Sent: Friday, January 06, 2006 8:20 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help with strip.default
> 
> Hi, 
>  
> I am creating a multi-conditioned trellis plot. My data look 
> something like
> this:
>  
> Factor A    Factor B    IV    DV
> X           1               
> X           2
> X           3
> X           4  
> Y           1
> Y           2
> Y           3
> Y           5 
> Z           1
> Z           2
> Z           3
> Z           4
>  
> In one sense these data are suitable for trellis because for 
> every level of
> factor A there are four levels of factor B. However, the 
> names of the factor
> B levels depend on the level of factor A. 
>  
> How would I create a 3 x 4 trellis plot where each panel is a 
> combination of
> factor A and factor B where the names of factor B are 
> preserved and the
> strip has two levels, one for factor A and another for factor B?
>  
> This was more difficult than I thought because trellis wants 
> to generate 15
> panels, as there are 3 levels of factor A and 5 levels of 
> factor B. But
> these 5 levels of factor B are in name only. There are only 4 
> different
> levels of factor B for each level of factor A.
>  
> As a work around I am considering renaming the levels in 
> factor A from 1 to
> 4 for all levels of factor B. Then, write a custom 
> strip.default to specify
> the names. However, I am not sure how to write this function. 
> Would someone
> help me get started?
>  
> Thanks,
> Steve
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From David.Brahm at geodecapital.com  Fri Jan  6 17:43:44 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 6 Jan 2006 11:43:44 -0500
Subject: [R] [R-pkgs] sudoku
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>

Any doubts about R's big-league status should be put to rest, now that
we have a
Sudoku Puzzle Solver.  Take that, SAS!  See package "sudoku" on CRAN.

The package could really use a puzzle generator -- contributors are
welcome!

-- David Brahm (brahm at alum.mit.edu) 


	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ecoinformatics at gmail.com  Fri Jan  6 17:54:41 2006
From: ecoinformatics at gmail.com (ecoinfo)
Date: Fri, 6 Jan 2006 18:54:41 +0200
Subject: [R] Wikis etc.
In-Reply-To: <p06210200bfe4486c7adb@128.115.153.6>
References: <43BDC208.5060909@vanderbilt.edu>
	<p06210200bfe4486c7adb@128.115.153.6>
Message-ID: <15f8e67d0601060854r5dce3bd3ye151297ca8fa1140@mail.gmail.com>

I use free Gmail to receive R-help emails. It is nice since

1. You can search your email quickly.
2. Replies to the same emails would be grouped together chronologically.
3. You can set up filters to put emails to different labels according
to the key words.

Please check the detailed description at
http://mail.google.com/mail/help/why_gmail.html

On 1/6/06, Don MacQueen <macq at llnl.gov> wrote:
> I don't have any significant experience with wikis, but I have yet to
> use any discussion board that was anywhere near as useful to me, or
> as easy to use, as an email list.
>
> Discussion boards have a web browser interface. Typically, they
> display at most a dozen topics at a time. Scrolling to get the next
> dozen is slow, as it requires a download from some web server. There
> is a huge amount of wasted screen space. When there is a topic that
> generates many messages scrolling through them is slow, as some
> discussion board interfaces show only 6 or 7 at a time. Search
> engines provided by the discussion board software are limited and
> slow.
>
> In contrast, in my email client I can show about three dozen subject
> lines at a time, I can quickly scroll up and down through the list, I
> can quickly group all the messages with the same subject line with a
> single click of the mouse. I can easily and quickly store selected
> messages of particular interest to a place where I can easily find
> them again. My email software searches very quickly through a huge
> number of messages.
>
> Then there's the question of administration and maintenance. Who is
> going to set up the wiki or discussion board categories? As far as I
> can tell (and that's actually not very far), either of them would
> require a lot more time and effort to set up and maintain than the
> present email list.
>
> Yes, r-help has a huge volume -- right now, my R-help mailbox has
> almost 22,000 messages in it, 2004-01-02 to the present; its size is
> about 124 mb. Yes, there is a lot of duplication. None the less, I
> find it easier and quicker to scan the subject lines a few times a
> day for interesting-looking topics than it would be to go to a
> browser and have to navigate up and down through various categories,
> looking for interesting-looking topics.
>
> As far as I can tell, the wiki concept is more along the lines of a
> reference library, whereas mailing lists and discussion boards are
> meant for people to ask each other questions, and give each other
> answers. If that perception is at all accurate, I would have to say
> that a wiki is by no means a suitable replacement for an email list.
> And when it comes to a choice between an email list and a discussion
> board, I have a strong preference for the email list.
>
> -Don
>
> At 7:04 PM -0600 1/5/06, Frank E Harrell Jr wrote:
> >I feel that as long as people continue to provide help on r-help wikis
> >will not be successful.  I think we need to move to a central wiki or
> >discussion board and to move away from e-mail.  People are extremely
> >helpful but e-mail seems to be to always be memory-less and messages get
> >too long without factorization of old text.  R-help is now too active
> >and too many new users are asking questions asked dozens of times for
> >e-mail to be effective.
> >
> >The wiki also needs to collect and organize example code, especially for
> >data manipulation.  I think that new users would profit immensely from a
> >compendium of examples.
> >
> >Just my .02 Euros
> >
> >Frank
> >--
> >Frank E Harrell Jr   Professor and Chair           School of Medicine
> >                       Department of Biostatistics   Vanderbilt University
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
> --
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



--
Xiaohua Dai, Dr.



From Jan.Verbesselt at biw.kuleuven.be  Fri Jan  6 17:57:16 2006
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Fri,  6 Jan 2006 17:57:16 +0100
Subject: [R] How to visualise spatial raster data?
Message-ID: <1136566636.43bea16c68346@webmail2.kuleuven.be>

Dear R help,

We are trying to visualise spatial raster data. We have per line, X & Y
coordinates and Z(data). How could we visualise this type of data? We
also would like to add extra data points to this plot based on new X,Y
and Z data.

We used the following function but would like to use only the  graph in
the upper right corner (spatial one) . Similar to the graph cfr.
http://www.est.ufpr.br/geoR/geoRdoc/vignette/geoRintro/geoRintrose3.html#x4-60003.1

geo_iRVI <- as.geodata(pixels_blok,coords.col=2:3, data.col=4)
plot(geo_iRVI)

How can this plot be optimized? And can we add other points to it?

 Another solution could be:
filled.contour(AVG,color=terrain.colors, xlab="Longitude (??)",
ylab="Latitude (??)) but therefore the data needs to be organised
differently, not per line of X,Y coordinates but in a raster form.

Can anyone advise functions to visualise spatial raster data optimally?

thanks,
Jan

windows R 2.2
library(geoR) 
library(akima) 



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From rogeriorosas at gmail.com  Fri Jan  6 18:04:33 2006
From: rogeriorosas at gmail.com (=?ISO-8859-1?Q?Rog=E9rio_Rosa_da_Silva?=)
Date: Fri, 06 Jan 2006 15:04:33 -0200
Subject: [R] distribution maps
Message-ID: <43BEA321.709@gmail.com>

Dears,

I would like to know if there is a R package(s) on CRAN that can
generate distribution maps  of species.

I think that this issue not has been discussed, but I did not  search
extensively on CRAN or help archives.

Best regards

Rog??rio



From petr.pikal at precheza.cz  Fri Jan  6 18:26:06 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 06 Jan 2006 18:26:06 +0100
Subject: [R] A comment about R:
In-Reply-To: <000201c612d4$74ee5160$6602a8c0@DELL266>
Message-ID: <43BEB63E.4357.29A37F0@localhost>

Hi

just to difference between matrix and data.frame

 > str(data.frame(mat))
`data.frame':   4 obs. of  5 variables:
 $ X1: num  -0.1940 -0.7629  0.0446 -0.5408
 $ X2: num  -1.092 -0.040  1.070  0.868
 $ X3: num  0.634 0.823 0.693 1.152
 $ X4: num   0.0258 -1.6507  1.2052  0.9714
 $ X5: num   0.673  0.380 -1.531 -0.426

> str((mat))
 num [1:4, 1:5] -0.1940 -0.7629  0.0446 -0.5408 -1.0925 ...

matrix is a numeric vector with dim attributes, data frame is matrix 
like structure which can hold different types of variables (columns).

sd is function based on var

> sd
function (x, na.rm = FALSE) 
{
    if (is.matrix(x)) 
        apply(x, 2, sd, na.rm = na.rm)
    else if (is.vector(x)) 
        sqrt(var(x, na.rm = na.rm))
    else if (is.data.frame(x)) 
        sapply(x, sd, na.rm = na.rm)
    else sqrt(var(as.vector(x), na.rm = na.rm))
}
<environment: namespace:stats>

and therefore behaves in similar manner for data.frames and matrices,
but mean accepts only data.frames, numeric vectors and dates

Arguments:

       x: An R object.  Currently there are methods for numeric data
          frames, numeric vectors and dates.  A complex vector is
          allowed for 'trim = 0', only.

So therefore matrix is treated as a numeric vector by mean but as a 
set of vectors by sd.

Don't know why.
I believe that it is because with var(matrix) you expect output as a 
variance matrix.

Maybe somebody can explain it better.

If you wanted similar behaviour for mean for matrices as sd you can 
try

mymean<-function(x, na.rm=FALSE)
{
if(is.matrix(x))
colMeans(x, na.rm=na.rm)
else mean(x, na.rm=na.rm)
}

> mymean(mat)
[1] -0.3632682  0.2013843  0.8251625  0.1379205 -0.2259909


HTH
Petr


On 6 Jan 2006 at 16:18, Stefan Eichenberger wrote:

From:           	"Stefan Eichenberger" <Stefan.Eichenberger at se-kleve.com>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Fri, 6 Jan 2006 16:18:16 +0100
Subject:        	[R]   A comment about R:

> ~~~~~~~~~~~~~~~
> ... blame me for not having sent below message initially in
> plain text format. Sorry!
> ~~~~~~~~~~~~~~~
> 
> I just got into R for most of the Xmas vacations and was about to ask
> for helping  pointer on how to get a hold of R when I came across this
> thread. I've read through  most it and would like to comment from a
> novice user point of view. I've a strong  programming background but
> limited statistical experience and no knowledge on  competing
> packages. I'm working as a senior engineer in electronics.
> 
> Yes, the learning curve is steep. Most of the docu is extremely terse.
> Learning is mostly from examples (a wiki was proposed in another
> mail...), documentation uses no graphical elements at all. So, when it
> comes to things like xyplot in lattice: where would I get the concepts
> behind panels, superpanels, and the like?
> 
> ok., this is steep and terse, but after a while I'll get over it...
> That's life. The general concept is great, things can be expressed
> very densly: Potential  is here.... I quickly had 200 lines of my own
> code together, doing what it should -  or so I believed.
> 
> Next I did:
>   matrix<-matrix(1:100, 10, 10)
>   image(matrix)
>   locator()
> Great: I can interactively work with my graphs... But then:
>   filled.contour(matrix)
>   locator()
> Oops - wrong coordinates returned. Bug. Apparently, locator() doen't
> realize that fitted.contour() has a color bar to the right and scales
> x wrongly...
> 
> Here is what really shocked me:
> 
> > str(bar) `data.frame':   206858 obs. of  12 variables:  ...
> > str(mean(bar[,6:12]))
>   Named num [1:7] 1.828 2.551 3.221 1.875 0.915 ...
>   ...
> > str(sd(bar[,6:12]))
>   Named num [1:7] 0.0702 0.1238 0.1600 0.1008 0.0465 ...
>   ...
> > prcomp(bar[,6:12])->foo
> > str(foo$x)
>   num [1:206858, 1:7] -0.4187 -0.4015  0.0218 -0.4438 -0.3650 ... ...
> > str(mean(foo$x))
>   num -1.07e-13
> > str(sd(foo$x))
>   Named num [1:7] 0.32235 0.06380 0.02254 0.00337 0.00270 ...
>   ...
> 
> So, sd returns a vector independent on whether the arguement is a
> matrix or data.frame, but mean reacts differently and returns a vector
> only against a data.frame?
> 
> The problem here is not that this is difficult to learn - the problem
> is the complete absense of a concept. Is a data.frame an 'extended'
> matrix with columns of different types or  something different? Since
> the numeric mean (I expected a vector) is recycled nicely  when used
> in a vector context, this makes debugging code close to impossible.
> Since  sd returns a vector, things like mean + 4*sd vary sufficiently
> across the data elements that I assume working code... I don't get any
> warning signal that something is wrong here.
> 
> The point in case is the behavior of locator() on a filled.contour()
> object: Things apparently  have been programmed and debugged from
> example rather than concept.
> 
> Now, in another posting I read that all this is a feature to discourge
> inexperienced users from statistics and force you to think before you
> do things. Whilst I support this concept of thinking: Did I miss
> something in statistics? I was in the believe that mean and sd were
> relatively close to each other conceptually... (here, they are even in
> different packages...)
> 
> I will continue using R for the time being. But whether I can
> recommend it to my work  collegues remains to be seen: How could I
> ever trust results returned?
> 
> I'm still impressed by some of the efficiency, but my trust is deeply
> shaken...
> ----------------------------------------------------------------------
> - Stefan Eichenberger        mailto:Stefan.Eichenberger at se-kleve.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From pdebruic at gmail.com  Fri Jan  6 18:28:36 2006
From: pdebruic at gmail.com (Paul DeBruicker)
Date: Fri, 6 Jan 2006 12:28:36 -0500
Subject: [R] Can R plot multicolor lines?
Message-ID: <f2e3401f0601060928x46f2d2bfof740752081dcc9cf@mail.gmail.com>

I have a number of continuous data series I'd like to plot with the
first 2/3 or so of each plotted in one color with the last 1/3 plotted
in another color.

I've thought of plotting 2 lines that abut each other by determining
where the first portion ends and attach the second portion.


Is there a simpler way that i have not thought of or discovered
through the mailing list, Intro to R, or Lattice PDF?

Thanks
Paul



From dwinner-lists at att.net  Fri Jan  6 18:45:00 2006
From: dwinner-lists at att.net (DW)
Date: Fri, 06 Jan 2006 12:45:00 -0500
Subject: [R] installation question/problem
Message-ID: <43BEAC9C.8020800@att.net>

Hello,

Can anybody tell me why I am getting the error below when I run "make 
check" and if it has any consequences I may regret later?

I run:

#  ./configure --enable-R-shlib
# make
# make check
# make install


configure, make and make install all work without errors, and it seems 
to install ok, and I even test the R binary after install, so I guess 
it's working. But I want to make sure. I'm not going to be using R, but 
I'm the net admin who has been tasked with installing it our servers, so 
I don't want any nasty surprises.

I wonder if it's possible that I'm missing libraries because I'm not 
running X on the servers?

This is:
FreeBSD 5.4 p8
R-2.2.1


make check output:

(snip)
.....
running code in 'grDevices-Ex.R' ... OK
comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
running code in 'graphics-Ex.R' ... OK
comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
running code in 'stats-Ex.R' ...*** Error code 1

Stop in /usr/home/dwinner/tmp/R-2.2.1/tests/Examples.
*** Error code 1

Stop in /usr/home/dwinner/tmp/R-2.2.1/tests/Examples.
*** Error code 1

Stop in /usr/home/dwinner/tmp/R-2.2.1/tests.
*** Error code 1

Stop in /usr/home/dwinner/tmp/R-2.2.1/tests.
*** Error code 1

Stop in /usr/home/dwinner/tmp/R-2.2.1.



Thanks for any info,
DW



From petr.pikal at precheza.cz  Fri Jan  6 18:52:34 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 06 Jan 2006 18:52:34 +0100
Subject: [R] Can R plot multicolor lines?
In-Reply-To: <f2e3401f0601060928x46f2d2bfof740752081dcc9cf@mail.gmail.com>
Message-ID: <43BEBC72.28452.2B2644B@localhost>

Hi

one way is to use segments
 x<-rnorm(200)
plot(1:200, x, type="n")
segments(1:199,x[1:199], 2:200, x[2:200], col=c(rep(1,150), 
rep(2,50)))

HTH
Petr


On 6 Jan 2006 at 12:28, Paul DeBruicker wrote:

Date sent:      	Fri, 6 Jan 2006 12:28:36 -0500
From:           	Paul DeBruicker <pdebruic at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Can R plot multicolor lines?

> I have a number of continuous data series I'd like to plot with the
> first 2/3 or so of each plotted in one color with the last 1/3 plotted
> in another color.
> 
> I've thought of plotting 2 lines that abut each other by determining
> where the first portion ends and attach the second portion.
> 
> 
> Is there a simpler way that i have not thought of or discovered
> through the mailing list, Intro to R, or Lattice PDF?
> 
> Thanks
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Fri Jan  6 18:53:17 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jan 2006 18:53:17 +0100
Subject: [R] installation question/problem
In-Reply-To: <43BEAC9C.8020800@att.net>
References: <43BEAC9C.8020800@att.net>
Message-ID: <x2u0chs0oi.fsf@viggo.kubism.ku.dk>

DW <dwinner-lists at att.net> writes:

> Hello,
> 
> Can anybody tell me why I am getting the error below when I run "make 
> check" and if it has any consequences I may regret later?
> 
> I run:
> 
> #  ./configure --enable-R-shlib
> # make
> # make check
> # make install
> 
> 
> configure, make and make install all work without errors, and it seems 
> to install ok, and I even test the R binary after install, so I guess 
> it's working. But I want to make sure. I'm not going to be using R, but 
> I'm the net admin who has been tasked with installing it our servers, so 
> I don't want any nasty surprises.
> 
> I wonder if it's possible that I'm missing libraries because I'm not 
> running X on the servers?
> 
> This is:
> FreeBSD 5.4 p8
> R-2.2.1
> 
> 
> make check output:
> 
> (snip)
> .....
> running code in 'grDevices-Ex.R' ... OK
> comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
> running code in 'graphics-Ex.R' ... OK
> comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
> running code in 'stats-Ex.R' ...*** Error code 1
 
Ouch.

Please look for stats-Ex.Rout.fail at tell us what is in it (you
should find it in tests/Examples in your builddir, interesting stuff
should be towards the end of the file).

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From calstats05 at yahoo.com  Fri Jan  6 18:57:53 2006
From: calstats05 at yahoo.com (Cal Stats)
Date: Fri, 6 Jan 2006 09:57:53 -0800 (PST)
Subject: [R] Problem with Integral of Indicator Function
Message-ID: <20060106175753.13661.qmail@web34004.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/cf24e99d/attachment.pl

From deepayan.sarkar at gmail.com  Fri Jan  6 18:58:27 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 6 Jan 2006 11:58:27 -0600
Subject: [R] help with strip.default
In-Reply-To: <200601061648.k06GmSEw008589@hertz.gene.com>
References: <000501c612dd$16a0b0e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
	<200601061648.k06GmSEw008589@hertz.gene.com>
Message-ID: <eb555e660601060958g1fc89337k8a61c95791262dfa@mail.gmail.com>

On 1/6/06, Berton Gunter <gunter.berton at gene.com> wrote:
> Steve:
>
> This is a question for **super Deepayan,** and hopefully he'll respond.
>
> However, in the interim, let me give it a shot. Basically, I think what
> you've asked for falls outside the bounds of what lattice is designed to do.
> But I think there's a simple way to fool it. Basically what you need to do
> is to combine your two factors into one with level names and ordering as you
> want. See ?factor (?ordered may also be useful, but you don't need it). For
> example:
>
> comb.factor=factor(paste(A,B,sep='.'))

That's what I would have suggested. I would recommend using
interaction() instead of paste(), since it is designed for this and is
presumably more efficient (not that it matters in this small example).
For the record, the 'layout' and 'skip' arguments (of xyplot etc) are
often useful in conjunction with this sort of use.

Deepayan

> As I said, you may have to reorder the levels from the default that factor()
> gives you to get your panels to display the way you want. Also see the
> perm.cond and index.cond arguments of xyplot, which might also suffice for
> that purpose.
>
> Again, Deepayan will hopefully suggest a cleverer way that I missed. But I
> think this approach will get you what you want.
>
> Cheers,
> Bert
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Steven Lacey
> > Sent: Friday, January 06, 2006 8:20 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] help with strip.default
> >
> > Hi,
> >
> > I am creating a multi-conditioned trellis plot. My data look
> > something like
> > this:
> >
> > Factor A    Factor B    IV    DV
> > X           1
> > X           2
> > X           3
> > X           4
> > Y           1
> > Y           2
> > Y           3
> > Y           5
> > Z           1
> > Z           2
> > Z           3
> > Z           4
> >
> > In one sense these data are suitable for trellis because for
> > every level of
> > factor A there are four levels of factor B. However, the
> > names of the factor
> > B levels depend on the level of factor A.
> >
> > How would I create a 3 x 4 trellis plot where each panel is a
> > combination of
> > factor A and factor B where the names of factor B are
> > preserved and the
> > strip has two levels, one for factor A and another for factor B?
> >
> > This was more difficult than I thought because trellis wants
> > to generate 15
> > panels, as there are 3 levels of factor A and 5 levels of
> > factor B. But
> > these 5 levels of factor B are in name only. There are only 4
> > different
> > levels of factor B for each level of factor A.
> >
> > As a work around I am considering renaming the levels in
> > factor A from 1 to
> > 4 for all levels of factor B. Then, write a custom
> > strip.default to specify
> > the names. However, I am not sure how to write this function.
> > Would someone
> > help me get started?
> >
> > Thanks,
> > Steve



From dwinner-lists at att.net  Fri Jan  6 19:01:49 2006
From: dwinner-lists at att.net (DW)
Date: Fri, 06 Jan 2006 13:01:49 -0500
Subject: [R] installation question/problem
In-Reply-To: <x2u0chs0oi.fsf@viggo.kubism.ku.dk>
References: <43BEAC9C.8020800@att.net> <x2u0chs0oi.fsf@viggo.kubism.ku.dk>
Message-ID: <43BEB08D.7010706@att.net>

Peter Dalgaard wrote:

>DW <dwinner-lists at att.net> writes:
>
>  
>
>>Hello,
>>
>>Can anybody tell me why I am getting the error below when I run "make 
>>check" and if it has any consequences I may regret later?
>>
>>I run:
>>
>>#  ./configure --enable-R-shlib
>># make
>># make check
>># make install
>>
>>
>>configure, make and make install all work without errors, and it seems 
>>to install ok, and I even test the R binary after install, so I guess 
>>it's working. But I want to make sure. I'm not going to be using R, but 
>>I'm the net admin who has been tasked with installing it our servers, so 
>>I don't want any nasty surprises.
>>
>>I wonder if it's possible that I'm missing libraries because I'm not 
>>running X on the servers?
>>
>>This is:
>>FreeBSD 5.4 p8
>>R-2.2.1
>>
>>
>>make check output:
>>
>>(snip)
>>.....
>>running code in 'grDevices-Ex.R' ... OK
>>comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
>>running code in 'graphics-Ex.R' ... OK
>>comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
>>running code in 'stats-Ex.R' ...*** Error code 1
>>    
>>
> 
>Ouch.
>
>Please look for stats-Ex.Rout.fail at tell us what is in it (you
>should find it in tests/Examples in your builddir, interesting stuff
>should be towards the end of the file).
>
>  
>
Here is what I found:

 > ## using the nl2sol algorithm
 > fm4DNase1 <- nls( density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
+                   data = DNase1,
+                   start = list(Asym = 3, xmid = 0, scal = 1),
+                   trace = TRUE, algorithm = "port")
  0      0.00000:  3.00000  0.00000  1.00000
Error in nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)), data = 
DNase1,  :
        Convergence failure: See PORT documentation.  Code (27)
Execution halted


Thanks,
DW



From Mleeds at kellogggroup.com  Fri Jan  6 19:06:05 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Fri, 6 Jan 2006 13:06:05 -0500
Subject: [R] Using R in a production /real time windows environment
Message-ID: <A8B87FDB74320349A9D1CC9021052A7646645F@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/b035c071/attachment.pl

From tlumley at u.washington.edu  Fri Jan  6 19:19:33 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 6 Jan 2006 10:19:33 -0800 (PST)
Subject: [R] Problem with Integral of Indicator Function
In-Reply-To: <20060106175753.13661.qmail@web34004.mail.mud.yahoo.com>
References: <20060106175753.13661.qmail@web34004.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0601061018240.22158@homer22.u.washington.edu>

On Fri, 6 Jan 2006, Cal Stats wrote:

> Hi..
>
>     i was trying to integrate the indicator funtion but had problems 
> when limits where negative or equal to the indicator condition
>
>  my function is
>  ________________________
>  fun1<-function(x){
>      as.numeric(x>=2)
>  }
>  _________________________
>
>  which should be   Ind(x>=2)*x

No. It should be Ind(x>=2), and it is.  You appear to want

   function(x) (x>=2)*x

 	-thomas

>  seems to work for the following two cases
>  ------------------------------------------------------------
>  > integrate(fun1,3,5)
>  2 with absolute error < 2.2e-14
>
>  > integrate(fun1,5,100)
>  95 with absolute error < 1.1e-12
>  ----------------------------------------------------------
>   Does not work for the following
>
>  > integrate(fun1,0,2)
>  0 with absolute error < 0     ( i was expecting  = 2)
>
>  > integrate(fun1,-1,5)
>  3 with absolute error < 3.3e-14   (i was expecting =5)
>
>  > integrate(fun1,-2,5)
>  3 with absolute error < 5.3e-15    (i was expecting =5)
>
>  Any suggestions?
>
>  Thanks.
>
>  Harsh,
>
>
>
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From justin_bem at yahoo.fr  Fri Jan  6 19:23:39 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 6 Jan 2006 19:23:39 +0100 (CET)
Subject: [R] Can R plot multicolor lines?
In-Reply-To: <f2e3401f0601060928x46f2d2bfof740752081dcc9cf@mail.gmail.com>
Message-ID: <20060106182339.65690.qmail@web25706.mail.ukl.yahoo.com>


Try this if you have sort data in ascending order

part1x<-x[1:round(2/3*length,digits=0)]
part2x<-x[round(2/3*length,digits=0)+1:length(x)]
part1y<-y[1:round(2/3*length,digits=0)]
part2y<-y[round(2/3*length,digits=0)+1:length(x)]

after plot
points(part1x,part1y,col="col1",type="l")
points(part2x,part2y,col="col2",type="l")


--- Paul DeBruicker <pdebruic at gmail.com> a ??crit :

> I have a number of continuous data series I'd like
> to plot with the
> first 2/3 or so of each plotted in one color with
> the last 1/3 plotted
> in another color.
> 
> I've thought of plotting 2 lines that abut each
> other by determining
> where the first portion ends and attach the second
> portion.
> 
> 
> Is there a simpler way that i have not thought of or
> discovered
> through the mailing list, Intro to R, or Lattice
> PDF?
> 
> Thanks
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Fri Jan  6 19:41:03 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 6 Jan 2006 19:41:03 +0100 (CET)
Subject: [R] How to visualise spatial raster data?
In-Reply-To: <1136566636.43bea16c68346@webmail2.kuleuven.be>
Message-ID: <Pine.LNX.4.44.0601061920550.14375-100000@reclus.nhh.no>

On Fri, 6 Jan 2006, Jan Verbesselt wrote:

> Dear R help,
> 
> We are trying to visualise spatial raster data. We have per line, X & Y
> coordinates and Z(data). How could we visualise this type of data? We
> also would like to add extra data points to this plot based on new X,Y
> and Z data.
> 
> We used the following function but would like to use only the  graph in
> the upper right corner (spatial one) . Similar to the graph cfr.
> http://www.est.ufpr.br/geoR/geoRdoc/vignette/geoRintro/geoRintrose3.html#x4-60003.1
> 
> geo_iRVI <- as.geodata(pixels_blok,coords.col=2:3, data.col=4)
> plot(geo_iRVI)
> 
> How can this plot be optimized? And can we add other points to it?
> 
>  Another solution could be:
> filled.contour(AVG,color=terrain.colors, xlab="Longitude (??)",
> ylab="Latitude (??)) but therefore the data needs to be organised
> differently, not per line of X,Y coordinates but in a raster form.
> 
> Can anyone advise functions to visualise spatial raster data optimally?

The SpatialGrid and SpatialPixels classes in the sp package provide a 
starting point:

library(sp)
xygrid <- expand.grid(1:25, 1:25)
xysubset <- xygrid[1:400,]
xysp <- SpatialPixels(SpatialPoints(xysubset))
xyz <- SpatialPixelsDataFrame(xysp, data=data.frame(z=runif(400)))
image(xyz, xlim=c(0,26), ylim=c(0,26))
xyadd <- xygrid[501:625,]
xyspadd <- SpatialPixels(SpatialPoints(xyadd))
xyzadd <- SpatialPixelsDataFrame(xyspadd, data=data.frame(z=runif(125)))
image(xyzadd, add=TRUE)

SpatialPixels need to be regularly spaced, but do not have to be a full 
rectangular grid, and have an image() method.

You may find the R-sig-geo mailing list more appropriate for this kind of 
question, access via the "Spatial" Task View on CRAN.

> 
> thanks,
> Jan
> 
> windows R 2.2
> library(geoR) 
> library(akima) 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Fri Jan  6 19:45:54 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jan 2006 19:45:54 +0100
Subject: [R] installation question/problem
In-Reply-To: <43BEB08D.7010706@att.net>
References: <43BEAC9C.8020800@att.net> <x2u0chs0oi.fsf@viggo.kubism.ku.dk>
	<43BEB08D.7010706@att.net>
Message-ID: <x2oe2pry8t.fsf@viggo.kubism.ku.dk>

DW <dwinner-lists at att.net> writes:

> Peter Dalgaard wrote:
> 
> >DW <dwinner-lists at att.net> writes:
> >
> >
> >>Hello,
> >>
> >> Can anybody tell me why I am getting the error below when I run
> >> "make check" and if it has any consequences I may regret later?
> >>
> >>I run:
> >>
> >>#  ./configure --enable-R-shlib
> >># make
> >># make check
> >># make install
> >>
> >>
> >> configure, make and make install all work without errors, and it
> >> seems to install ok, and I even test the R binary after install, so
> >> I guess it's working. But I want to make sure. I'm not going to be
> >> using R, but I'm the net admin who has been tasked with installing
> >> it our servers, so I don't want any nasty surprises.
> >>
> >> I wonder if it's possible that I'm missing libraries because I'm
> >> not running X on the servers?
> >>
> >>This is:
> >>FreeBSD 5.4 p8
> >>R-2.2.1
> >>
> >>
> >>make check output:
> >>
> >>(snip)
> >>.....
> >>running code in 'grDevices-Ex.R' ... OK
> >>comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
> >>running code in 'graphics-Ex.R' ... OK
> >>comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
> >>running code in 'stats-Ex.R' ...*** Error code 1
> >>
> > Ouch.
> >
> >Please look for stats-Ex.Rout.fail at tell us what is in it (you
> >should find it in tests/Examples in your builddir, interesting stuff
> >should be towards the end of the file).
> >
> >
> Here is what I found:
> 
>  > ## using the nl2sol algorithm
>  > fm4DNase1 <- nls( density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
> +                   data = DNase1,
> +                   start = list(Asym = 3, xmid = 0, scal = 1),
> +                   trace = TRUE, algorithm = "port")
>   0      0.00000:  3.00000  0.00000  1.00000
> Error in nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)), data =
> DNase1,  :
>         Convergence failure: See PORT documentation.  Code (27)
> Execution halted
> 
> 
> Thanks,
> DW

Hmm. Faint bell ringing:

We had a thread in R-core involving that error message (code 27 too)
in a different context, back in August. It could indicate that you are
tickling an intermittent bug in nlminb. Are you any good with a
debugger?

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From br44114 at gmail.com  Fri Jan  6 20:02:22 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 6 Jan 2006 14:02:22 -0500
Subject: [R] Using R in a production /real time windows environment
Message-ID: <8d5a36350601061102m5edc1ff4u6e28e94ec4157eb7@mail.gmail.com>

See this thread,
https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000568.html
You could also have R query a real-time database (via RMySQL, ROracle
etc) every few seconds.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Leeds
> Sent: Friday, January 06, 2006 1:06 PM
> To: R-Stat Help
> Subject: [R] Using R in a production /real time windows environment
>
>
> I just had a question for anyone who has done this.
> I am currently using R  in a research environment
> but I was wondering if anyone has had experience using
> it in a production/real time environment where data
> is coming in quite quickly ( every second or so ) and
> things have to be calculated quickly and sent
> back out to some kind of front end.GUI environment
>
> Was this successful or difficult and how did
> one make the connection between R and
> the real time data. Thanks.
>
>                                      Mark
>
>
>
>
>
>
>
> **********************************************************************
> This email and any files transmitted with it are
> confidentia...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Fri Jan  6 20:14:31 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 6 Jan 2006 20:14:31 +0100 (CET)
Subject: [R] distribution maps
In-Reply-To: <43BEA321.709@gmail.com>
Message-ID: <Pine.LNX.4.44.0601062011180.14375-100000@reclus.nhh.no>

On Fri, 6 Jan 2006, Rog??rio Rosa da Silva wrote:

> Dears,
> 
> I would like to know if there is a R package(s) on CRAN that can
> generate distribution maps  of species.
> 
> I think that this issue not has been discussed, but I did not  search
> extensively on CRAN or help archives.

Could I suggest the "Spatial" and "Environmetrics" Task Views reached from 
the Task View item in the navigation bar on CRAN? You may also find the 
R-sig-geo mailing list a useful place to make your question a little more 
detailed - you do not say anything about your data, and a helpful reply 
would depend on knowing that.

> 
> Best regards
> 
> Rog??rio
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From fredrik.bg.lundgren at bredband.net  Fri Jan  6 20:47:29 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Fri, 6 Jan 2006 20:47:29 +0100
Subject: [R] "Missing value representation in Excel before extraction to R
	with RODBC"
Message-ID: <000501c612fa$079cac30$109d72d5@Larissa>

Dear list,

How should missing values be expressed in Excel before extraction to R 
via RODBC. I'm bewildered. Sometimes the representation with NA in Excel 
appears to work and shows up in R as <NA> but sometimes the use of NA in 
Excel changes the whole vector to NA's. Blank or nothing or NA as 
representation for missing values in Excel with dateformat gives NA's of 
the whole vector in R but with  general format in Excel gives blanks for 
missing values in R. How should I represent missing values in Excel?


Best wishes and thanks for any help
Fredrik Lundgren



From dwinner-lists at att.net  Fri Jan  6 20:48:44 2006
From: dwinner-lists at att.net (DW)
Date: Fri, 06 Jan 2006 14:48:44 -0500
Subject: [R] installation question/problem
In-Reply-To: <x2oe2pry8t.fsf@viggo.kubism.ku.dk>
References: <43BEAC9C.8020800@att.net>
	<x2u0chs0oi.fsf@viggo.kubism.ku.dk>	<43BEB08D.7010706@att.net>
	<x2oe2pry8t.fsf@viggo.kubism.ku.dk>
Message-ID: <43BEC99C.4080600@att.net>

Peter Dalgaard wrote:

>DW <dwinner-lists at att.net> writes:
>
>  
>
>>Peter Dalgaard wrote:
>>
>>    
>>
>>>DW <dwinner-lists at att.net> writes:
>>>
>>>
>>>      
>>>
>>>>Hello,
>>>>
>>>>Can anybody tell me why I am getting the error below when I run
>>>>"make check" and if it has any consequences I may regret later?
>>>>
>>>>I run:
>>>>
>>>>#  ./configure --enable-R-shlib
>>>># make
>>>># make check
>>>># make install
>>>>
>>>>
>>>>configure, make and make install all work without errors, and it
>>>>seems to install ok, and I even test the R binary after install, so
>>>>I guess it's working. But I want to make sure. I'm not going to be
>>>>using R, but I'm the net admin who has been tasked with installing
>>>>it our servers, so I don't want any nasty surprises.
>>>>
>>>>I wonder if it's possible that I'm missing libraries because I'm
>>>>not running X on the servers?
>>>>
>>>>This is:
>>>>FreeBSD 5.4 p8
>>>>R-2.2.1
>>>>
>>>>
>>>>make check output:
>>>>
>>>>(snip)
>>>>.....
>>>>running code in 'grDevices-Ex.R' ... OK
>>>>comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
>>>>running code in 'graphics-Ex.R' ... OK
>>>>comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
>>>>running code in 'stats-Ex.R' ...*** Error code 1
>>>>
>>>>        
>>>>
>>>Ouch.
>>>
>>>Please look for stats-Ex.Rout.fail at tell us what is in it (you
>>>should find it in tests/Examples in your builddir, interesting stuff
>>>should be towards the end of the file).
>>>
>>>
>>>      
>>>
>>Here is what I found:
>>
>> > ## using the nl2sol algorithm
>> > fm4DNase1 <- nls( density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
>>+                   data = DNase1,
>>+                   start = list(Asym = 3, xmid = 0, scal = 1),
>>+                   trace = TRUE, algorithm = "port")
>>  0      0.00000:  3.00000  0.00000  1.00000
>>Error in nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)), data =
>>DNase1,  :
>>        Convergence failure: See PORT documentation.  Code (27)
>>Execution halted
>>
>>
>>Thanks,
>>DW
>>    
>>
>
>Hmm. Faint bell ringing:
>
>We had a thread in R-core involving that error message (code 27 too)
>in a different context, back in August. It could indicate that you are
>tickling an intermittent bug in nlminb. Are you any good with a
>debugger?
>
>  
>
(laughs) No, wish I was though. I can write "Hello World" in about 5 
different languages, but that's about the extent of my coding skills. 
But if there is anything else I can do to provide helpful info 
(following instructions, replicating things, etc.), I'd be more than 
happy to help.

-DW



From liqiunews at yahoo.com  Fri Jan  6 21:10:43 2006
From: liqiunews at yahoo.com (Liqiu Jiang)
Date: Fri, 6 Jan 2006 12:10:43 -0800 (PST)
Subject: [R] A question on summation of functions
Message-ID: <20060106201043.48200.qmail@web31101.mail.mud.yahoo.com>

Dear Rers,
 I am trying to do a 2 dimmensional intergration for a
function. the function is summation of another
function evaluated at a  series of vector values. I
have difficulty to code this function. 
 
For example: I have function f which is a bivariate
normal density function:


#define some constants
err<-0.5
m<-5
times<-seq(0, m-1)
rou<-sum(times)/sqrt(m*sum(times^2))
sig.w<- sqrt(m*err)
sig.wt<-sqrt(sum(times^2)*err)


#bivariate normal density 
f<-function(x, y, u.x, u.y)
exp(-((x-u.x)^2/sig.w^2+(y-u.y)^2/sig.wt^2-2*rou*(x-u.x)*(y-u.y)/(sig.w*sig.wt))/(2*(1-rou^2)))/(2*pi*sig.w*sig.wt*sqrt(1-rou^2))

###

I would like to have a function g which is defined as
######
uw = 1:n
uwt = (n+1):2n

g = function(x, y) f(x,y, uw[1], uw[1])+f(x,y, uw[2],
uwt[2])+
...+f(x,y, uw[n], uwt[n])
#######
if n is very large, I am not able to write all them
down, How can I code the function g. Thank you for
your consideration. 

Best wishes,
Liqiu



From fhduan at gmail.com  Fri Jan  6 21:11:55 2006
From: fhduan at gmail.com (Frank Duan)
Date: Fri, 6 Jan 2006 14:11:55 -0600
Subject: [R] Looking for packages to do Feature Selection and
	Classification
In-Reply-To: <cdf817830601051201s50b69314w5aa3c2c331c0cd76@mail.gmail.com>
References: <BE216486E4154040BF783AE5DAAA3ED401C2D1D9@SRVEXCH1.cnio.es>
	<cdf817830601051201s50b69314w5aa3c2c331c0cd76@mail.gmail.com>
Message-ID: <3b9172310601061211k7a957e18t907c16bc2838ce95@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/44c806c7/attachment.pl

From dwinner-lists at att.net  Fri Jan  6 21:40:14 2006
From: dwinner-lists at att.net (DW)
Date: Fri, 06 Jan 2006 15:40:14 -0500
Subject: [R] installation question/problem
In-Reply-To: <x2oe2pry8t.fsf@viggo.kubism.ku.dk>
References: <43BEAC9C.8020800@att.net>
	<x2u0chs0oi.fsf@viggo.kubism.ku.dk>	<43BEB08D.7010706@att.net>
	<x2oe2pry8t.fsf@viggo.kubism.ku.dk>
Message-ID: <43BED5AE.1010402@att.net>

Peter Dalgaard wrote:

>DW <dwinner-lists at att.net> writes:
>
>  
>
>>Peter Dalgaard wrote:
>>
>>    
>>
>>>DW <dwinner-lists at att.net> writes:
>>>
>>>
>>>      
>>>
>>>>Hello,
>>>>
>>>>Can anybody tell me why I am getting the error below when I run
>>>>"make check" and if it has any consequences I may regret later?
>>>>
>>>>I run:
>>>>
>>>>#  ./configure --enable-R-shlib
>>>># make
>>>># make check
>>>># make install
>>>>
>>>>
>>>>configure, make and make install all work without errors, and it
>>>>seems to install ok, and I even test the R binary after install, so
>>>>I guess it's working. But I want to make sure. I'm not going to be
>>>>using R, but I'm the net admin who has been tasked with installing
>>>>it our servers, so I don't want any nasty surprises.
>>>>
>>>>I wonder if it's possible that I'm missing libraries because I'm
>>>>not running X on the servers?
>>>>
>>>>This is:
>>>>FreeBSD 5.4 p8
>>>>R-2.2.1
>>>>
>>>>
>>>>make check output:
>>>>
>>>>(snip)
>>>>.....
>>>>running code in 'grDevices-Ex.R' ... OK
>>>>comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.prev' ... OK
>>>>running code in 'graphics-Ex.R' ... OK
>>>>comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.prev' ... OK
>>>>running code in 'stats-Ex.R' ...*** Error code 1
>>>>
>>>>        
>>>>
>>>Ouch.
>>>
>>>Please look for stats-Ex.Rout.fail at tell us what is in it (you
>>>should find it in tests/Examples in your builddir, interesting stuff
>>>should be towards the end of the file).
>>>
>>>
>>>      
>>>
>>Here is what I found:
>>
>> > ## using the nl2sol algorithm
>> > fm4DNase1 <- nls( density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
>>+                   data = DNase1,
>>+                   start = list(Asym = 3, xmid = 0, scal = 1),
>>+                   trace = TRUE, algorithm = "port")
>>  0      0.00000:  3.00000  0.00000  1.00000
>>Error in nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)), data =
>>DNase1,  :
>>        Convergence failure: See PORT documentation.  Code (27)
>>Execution halted
>>
>>
>>Thanks,
>>DW
>>    
>>
>
>Hmm. Faint bell ringing:
>
>We had a thread in R-core involving that error message (code 27 too)
>in a different context, back in August. It could indicate that you are
>tickling an intermittent bug in nlminb. Are you any good with a
>debugger?
>
>  
>
Also, for whatever it's worth, I tried this on FreeBSD 6.0 and it worked.

I then went back and tried again on a 3rd machine running FreeBSD 5.4 
with Xorg (because I'm running X on the 6.0 box and thought maybe the 
problem is a missing library or some other component), and it produced 
the same error as the first 5.4 box.

-DW



From pgilbert at bank-banque-canada.ca  Fri Jan  6 21:46:10 2006
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 06 Jan 2006 15:46:10 -0500
Subject: [R] ouml in an .Rd
Message-ID: <43BED712.2030705@bank-banque-canada.ca>

I am trying to put an ouml in an .Rd file with no success. Writing R 
Extensions suggests:

Text which might need to be represented differently in different 
encodings should be marked by |\enc|, e.g. |\enc{J??reskog}{Joreskog}| 
where the first argument will be used where encodings are allowed and 
the second should be ASCII (and is used for e.g. the text conversion).

(Above may get mangled by the mail.) I have tried variations

   \enc{J"oreskog}{Joreskog}
   \enc{J\"oreskog}{Joreskog}
   \enc{Jo\"reskog}{Joreskog}
   \enc{Jo\"reskog}{Joreskog}
   \enc{J\"{o}reskog}{Joreskog}
   \enc{J\\"{o}reskog}{Joreskog}
   \enc{J&ouml;oreskog}{Joreskog}

all with no effect on the generated pdf file.  Suggestions would be 
appreciated.

Thanks,
Paul Gilbert



From liqiunews at yahoo.com  Fri Jan  6 22:01:34 2006
From: liqiunews at yahoo.com (Liqiu Jiang)
Date: Fri, 6 Jan 2006 13:01:34 -0800 (PST)
Subject: [R] Got it--Re:A question on summation of functions
Message-ID: <20060106210134.85862.qmail@web31108.mail.mud.yahoo.com>

Dear Rers,
 It seems the usual sum function can work. Anyway, I
appreciate your time on this. 

Best wishes,
Liqiu 




Dear Rers,
 I am trying to do a 2 dimmensional intergration for a
function. the function is summation of another
function evaluated at a  series of vector values. I
have difficulty to code this function. 
 
For example: I have function f which is a bivariate
normal density function:


#define some constants
err<-0.5
m<-5
times<-seq(0, m-1)
rou<-sum(times)/sqrt(m*sum(times^2))
sig.w<- sqrt(m*err)
sig.wt<-sqrt(sum(times^2)*err)


#bivariate normal density 
f<-function(x, y, u.x, u.y)
exp(-((x-u.x)^2/sig.w^2+(y-u.y)^2/sig.wt^2-2*rou*(x-u.x)*(y-u.y)/(sig.w*sig.wt))/(2*(1-rou^2)))/(2*pi*sig.w*sig.wt*sqrt(1-rou^2))

###

I would like to have a function g which is defined as
######
uw = 1:n
uwt = (n+1):2n

g = function(x, y) f(x,y, uw[1], uw[1])+f(x,y, uw[2],
uwt[2])+
...+f(x,y, uw[n], uwt[n])
#######
if n is very large, I am not able to write all them
down, How can I code the function g. Thank you for
your consideration. 

Best wishes,
Liqiu



From ripley at stats.ox.ac.uk  Fri Jan  6 22:10:33 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jan 2006 21:10:33 +0000 (GMT)
Subject: [R] ouml in an .Rd
In-Reply-To: <43BED712.2030705@bank-banque-canada.ca>
References: <43BED712.2030705@bank-banque-canada.ca>
Message-ID: <Pine.LNX.4.61.0601062105570.11196@gannet.stats>

It means what it says: you need to put the actual character in the file, 
and specify the encoding for the file via \encoding.  (For you, UTF-8 or 
latin1, I would guess.)

It's not a question of trying variations, rather of following 
instructions.

On Fri, 6 Jan 2006, Paul Gilbert wrote:

> I am trying to put an ouml in an .Rd file with no success. Writing R
> Extensions suggests:
>
> Text which might need to be represented differently in different
> encodings should be marked by |\enc|, e.g. |\enc{J?reskog}{Joreskog}|
> where the first argument will be used where encodings are allowed and
> the second should be ASCII (and is used for e.g. the text conversion).
>
> (Above may get mangled by the mail.) I have tried variations
>
>   \enc{J"oreskog}{Joreskog}
>   \enc{J\"oreskog}{Joreskog}
>   \enc{Jo\"reskog}{Joreskog}
>   \enc{Jo\"reskog}{Joreskog}
>   \enc{J\"{o}reskog}{Joreskog}
>   \enc{J\\"{o}reskog}{Joreskog}
>   \enc{J&ouml;oreskog}{Joreskog}
>
> all with no effect on the generated pdf file.  Suggestions would be
> appreciated.
>
> Thanks,
> Paul Gilbert
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Peter.Rossi at chicagogsb.edu  Fri Jan  6 22:56:25 2006
From: Peter.Rossi at chicagogsb.edu (Rossi, Peter E.)
Date: Fri, 6 Jan 2006 15:56:25 -0600
Subject: [R] vectorization of groups of dot products
Message-ID: <1E7B167439290641966EB161D4330798C1CC5A@GSBEX.gsb.uchicago.edu>

 
I have a set of n vectors, x_1, ..., x_n, of the same length.
I would like to form the vector of dot products -- x_1'x_1, ..., x_n'x_n

the fastest way I can think to do this is to put the vectors into a
matrix
and do 

diag(crossprod(X))

however, this seems to be very wasteful since this computes n(n+1)/2-n
unnecessary
dot products.

Is there a better way using existing functions in R?

thanks!

peter


................................
  Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 353, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637, USA
 Tel: (773) 702-7513   |   Fax: (773) 834-2081



From jukka.ruohonen at helsinki.fi  Fri Jan  6 22:56:08 2006
From: jukka.ruohonen at helsinki.fi (jukka ruohonen)
Date: Fri,  6 Jan 2006 23:56:08 +0200
Subject: [R] panel data unit root tests
Message-ID: <1136584568.43bee778927e6@www2.helsinki.fi>

When finally got some time to do some coding, I started and stopped right 
after. The stationary test is a good starting point because it demonstrates 
how we should be able to move the very basic R matrices. I have a real-
world small N data set with 

rows:
id(n=1)---t1---variable1
...
id=(N=20)---T=21---variable1

Thus, a good test case. For first id I was considering something like this:

lag <- as.integer(lags)
lags.p <- lags + 1
id <- unique(group)
id.l <- length(id)
y.l <- length(y)
yid.l <- length(y)/id.l
  if (lag > yid.l -2) 
        stop("\nlag too long for defined cross-sections.\n")

#for (i in id) {
  lagy <- y[2:yid.l]
      lagy.em <- embed(lagy, lags)
  id.l <- length(id)
  dy <- diff(y)[1:yid.l-1]
      dy.em <- embed(dy, lags)
#     }
print(levinlin(ws, year, id, lags = 3))

Couldn't figure the loop over units out but with N = 1 the data 
transformation seemed to work just fine. Now we should pool the new 
variables within the panel and regress y over yt-1 and dy-t1 +....+ dy-t-j 
with, say, BIC doing the job for d's (H0: y-1 ~ 0) for each in the panel. 

Now the above example puts the right-hand on columns, and if we are dealing 
with panel models in general, we should store the new variables together 
with dX's, which should then give clues to IV estimator with e.g. 
orthogonal deviations, e.g. k <- y ~ yt-1 + x + as.factor(id)). So one 
confusing part is the requirement of some big storage base for different 
matrices doing the IV business with lags/levels - the amount of instruments 
can be enormous with possibly calculation problems in a GMM dynamic panel 
estimator a la Arellano & Bond. Therefore one should code the theoretically 
relevant instruments beforehand with various transformation matrices. Thus, 
should I start to study something that can be done with the newly added 
SparseM package? 

Regards,

Jukka Ruohonen.



From gunter.berton at gene.com  Fri Jan  6 23:11:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 6 Jan 2006 14:11:58 -0800
Subject: [R] vectorization of groups of dot products
In-Reply-To: <1E7B167439290641966EB161D4330798C1CC5A@GSBEX.gsb.uchicago.edu>
Message-ID: <200601062211.k06MBmmG002714@ohm.gene.com>

colSums(X^2) -- or am I missing something?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rossi, Peter E.
> Sent: Friday, January 06, 2006 1:56 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] vectorization of groups of dot products
> 
>  
> I have a set of n vectors, x_1, ..., x_n, of the same length.
> I would like to form the vector of dot products -- x_1'x_1, 
> ..., x_n'x_n
> 
> the fastest way I can think to do this is to put the vectors into a
> matrix
> and do 
> 
> diag(crossprod(X))
> 
> however, this seems to be very wasteful since this computes n(n+1)/2-n
> unnecessary
> dot products.
> 
> Is there a better way using existing functions in R?
> 
> thanks!
> 
> peter
> 
> 
> ................................
>   Peter E. Rossi
>  Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
>  Editor, Quantitative Marketing and Economics
>  Rm 353, Graduate School of Business, U of Chicago
>  5807 S. Woodlawn Ave, Chicago IL 60637, USA
>  Tel: (773) 702-7513   |   Fax: (773) 834-2081
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From David.Brahm at geodecapital.com  Sat Jan  7 00:16:43 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Fri, 6 Jan 2006 18:16:43 -0500
Subject: [R] Daylight Savings Time unknown in R-2.2.1
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDE@MSGBOSCLF2WIN.DMN1.FMR.COM>

Under R-2.2.1, a POSIXlt date created with "strptime" has an unknown
Daylight Savings Time flag:

> strptime(20051208, "%Y%m%d")$isdst
[1] -1

This is true on both Linux (details below) and Windows.  It did not
occur under R-2.1.0.  Any ideas?  TIA!


> Sys.getenv("TZ")
TZ 
"" 

Version:
 platform = i686-pc-linux-gnu
 arch = i686
 os = linux-gnu
 system = i686, linux-gnu
 status = 
 major = 2
 minor = 2.1
 year = 2005
 month = 12
 day = 20
 svn rev = 36812
 language = R

Locale:
C

Search Path:
 .GlobalEnv, package:methods, package:stats, package:graphics,
package:grDevices, package:utils, package:datasets, Autoloads,
package:base

-- David Brahm (brahm at alum.mit.edu)



From manderse at nmsu.edu  Sat Jan  7 01:30:03 2006
From: manderse at nmsu.edu (Mark Andersen)
Date: Fri, 6 Jan 2006 17:30:03 -0700
Subject: [R] Installing Task Views
Message-ID: <003b01c61321$806a0c80$0ffd7b80@MCANotebook>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/1a2b3c42/attachment.pl

From sourceforge at metrak.com  Sat Jan  7 02:02:57 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Sat, 07 Jan 2006 12:02:57 +1100
Subject: [R] Wikis etc.
In-Reply-To: <43BDC208.5060909@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu>
Message-ID: <43BF1341.4020800@metrak.com>

I am a fan of wiki's and I reckon it would really help with making R 
more accessible.  On one extreme you have this email list and on the 
other extreme you have RNews and the PDF's on CRAN.  A wiki might hit 
the spot between them and reduce the traffic on the email list.


Frank E Harrell Jr wrote:
> I feel that as long as people continue to provide help on r-help wikis 
> will not be successful.  I think we need to move to a central wiki or 
> discussion board and to move away from e-mail.  People are extremely 
> helpful but e-mail seems to be to always be memory-less and messages get 
> too long without factorization of old text.  R-help is now too active 
> and too many new users are asking questions asked dozens of times for 
> e-mail to be effective.
> 
> The wiki also needs to collect and organize example code, especially for 
> data manipulation.  I think that new users would profit immensely from a 
> compendium of examples.
> 
> Just my .02 Euros
> 
> Frank



From liuwensui at gmail.com  Sat Jan  7 04:09:36 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 6 Jan 2006 22:09:36 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <38b9f0350601060745v5c18c60au@mail.gmail.com>
References: <20060105154609.GA7009@phenix.sram.qc.ca>
	<644e1f320601050859l6d573435q9b3004690758cfda@mail.gmail.com>
	<38b9f0350601050948n75f9e7abs@mail.gmail.com>
	<1115a2b00601060723g3c954efdwa67ce775e76218e2@mail.gmail.com>
	<38b9f0350601060745v5c18c60au@mail.gmail.com>
Message-ID: <1115a2b00601061909m1de1f9caod7be9222a6b9e97f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060106/55a6d8bc/attachment.pl

From tkobayas at indiana.edu  Sat Jan  7 04:44:55 2006
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Fri, 06 Jan 2006 22:44:55 -0500
Subject: [R] LOCFIT help
Message-ID: <43BF3937.3010101@indiana.edu>

Hi,

I have started to learn local regression models so as to identify
statistically significant peaks in urban areas, such as population
densities and congestion. I successfully ran /locfit/ and got several
information on the fit. Now I got stuck. This is a very silly question,
but isn't the first derivative of the fitted curve zero or close to
zero? I got some very high numbers on that.

The commands I put are:

x<-Longitude
y<-Latitude
model.local<-locfit(log(POPDENSITY)~lp(x,y, nn=0.55))

span was determined by spgwr adaptive bandwidth.

Any help appreciated.

Thank you very much.

Taka

PhD student
Indiana University, Geography



From ripley at stats.ox.ac.uk  Sat Jan  7 09:37:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 Jan 2006 08:37:12 +0000 (GMT)
Subject: [R] Daylight Savings Time unknown in R-2.2.1
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDE@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDE@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0601070749150.17836@gannet.stats>

POSIX does not require strptime() to set the timezone.  This changed in 
2.2.1 as part of

     o	ISODateTime() mistakenly corrected non-existent times (when
 	DST was being started) in the current time zone.

You can get the isdst by doing a conversion, e.g.

> x <- strptime(20051208, "%Y%m%d")
> x
[1] "2005-12-08"
> as.POSIXlt(as.POSIXct(x))
[1] "2005-12-08 GMT"
> as.POSIXlt(as.POSIXct(x))$isdst
[1] 0

It's easy to set it again, and I will do so in 2.2.1 patched.


On Fri, 6 Jan 2006, Brahm, David wrote:

> Under R-2.2.1, a POSIXlt date created with "strptime" has an unknown
> Daylight Savings Time flag:
>
>> strptime(20051208, "%Y%m%d")$isdst
> [1] -1
>
> This is true on both Linux (details below) and Windows.  It did not
> occur under R-2.1.0.  Any ideas?  TIA!
>
>
>> Sys.getenv("TZ")
> TZ
> ""
>
> Version:
> platform = i686-pc-linux-gnu
> arch = i686
> os = linux-gnu
> system = i686, linux-gnu
> status =
> major = 2
> minor = 2.1
> year = 2005
> month = 12
> day = 20
> svn rev = 36812
> language = R
>
> Locale:
> C
>
> Search Path:
> .GlobalEnv, package:methods, package:stats, package:graphics,
> package:grDevices, package:utils, package:datasets, Autoloads,
> package:base
>
> -- David Brahm (brahm at alum.mit.edu)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bitwrit at ozemail.com.au  Sun Jan  8 03:13:35 2006
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 07 Jan 2006 21:13:35 -0500
Subject: [R] ylim problem in barplot
Message-ID: <43C0754F.5030909@ozemail.com.au>

Bliese, Paul D LTC USAMH wrote:
 >
 > When I use barplot but select a ylim value greater
 > than zero, the graph is distorted.  The bars extend
 > below the bottom of the graph.

Have a look at the gap.barplot function in the plotrix package. A new 
version (2.0.1) has just been uploaded and should turn up soon.

Jim



From patrick.giraudoux at univ-fcomte.fr  Sat Jan  7 12:07:20 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 07 Jan 2006 12:07:20 +0100
Subject: [R] glmmPQL and variance structure
In-Reply-To: <43B81DC4.9070806@pdf.com>
References: <43B17D9B.3080107@univ-fcomte.fr> <43B81DC4.9070806@pdf.com>
Message-ID: <43BFA0E8.1090600@univ-fcomte.fr>

Dear listers,

On the line of a last (unanswered) question about glmmPQL() of the 
library MASS, I am still wondering if it is possible to pass a variance 
structure object  to the call to lme() within the functions (e.g. 
weights=varPower(1), etc...). The current weights argument of glmmPQL is 
actually used for a call to glm -and not for lme). I have tried to go 
through the code, and gathered that the variance structure passed to the 
call to lme()  was:

mcall$weights <- quote(varFixed(~invwt))

and this cannot be modified by and argument of glmmPQL().

I have tried to modify the script a bit wildly  and changed varFixed 
into VarPower(~1), in a glmmPQL2 function. I get the following error:

 > glmmPQL2(y ~ trt + I(week > 2), random = ~ 1 | ID,
+ family = binomial, data = bacteria)
iteration 1
Error in unlist(x, recursive, use.names) :
        argument not a list

I get the same error whatever the change in variance structure on this line.

Beyond this I wonder why variance structure cannot be passed to lme via 
glmmPQL...

Any idea?

Patrick Giraudoux



From ros110 at gmail.com  Sat Jan  7 12:11:59 2006
From: ros110 at gmail.com (Richard van Wingerden)
Date: Sat, 7 Jan 2006 12:11:59 +0100
Subject: [R] level sets of factors are different
Message-ID: <a3e689eb0601070311p6198d529t508c1704ee7c919f@mail.gmail.com>

Hi,

I have two data frames A en B.
I want to filter B with values of A

data_frame_b
names <- colnames(data_frame_b[1:1, ])

filtera <- data.frame(data_frame_a[1:1])
	
	print(nrow(filtera))
	
	if (nrow(filtera)>0){
		
		filtered_frame_b <- subset(data_frame_b, ColumnX == filtera[1, 1], names)
         }
The results are:

[1] 124
Error in Ops.factor(ColumnX, filtera[1, 1]) :
        level sets of factors are different


What is wrong??

Richard



From neo27 at t-online.de  Sat Jan  7 13:38:30 2006
From: neo27 at t-online.de (Mark Hempelmann)
Date: Sat, 07 Jan 2006 13:38:30 +0100
Subject: [R] Clustering and Rand Index
Message-ID: <43BFB646.7080305@t-online.de>

Dear WizaRds,

I am trying to compute the (adjusted) Rand Index in order to comprehend 
the variable selection heuristic (VS-KM) according to Brusco/ Cradit 
2001 (Psychometrika 66 No.2 p.249-270, 2001).

Unfortunately, I am unable to correctly use
cl_ensemble and cl_agreement (package: clue). Here is what I am trying 
to do:

library(clue)

##	Let p1..p4 be four partitions of the kind

p1=c(1,1,1,2,2,2,3,3,3)
p2=c(1,1,1,3,2,2,3,3,2)
p3=c(1,2,1,3,1,3,1,3,2)
p4=c(1,2,1,3,1,3,1,3,2)

Each object within the partitions is assigned to cluster 1,2,3 
respectively. Now I have to create a cl_ensemble object, so that I can 
calculate the Rand index:

ens <- cl_ensemble(list=c(p1,p2,p3,p4))

which only leads to
"Ensemble elements must be all partitions or all hierarchies."

Although I understand that p1..p4 are vectors in this example, they 
represent the partitions I want to use. I don't know how to create the 
necessary partition object in order to transform it into an ensemble 
object, so that I can run cl_agreement - so much transformation, so 
little time...

I have also tried to work around this prbl, creating partitions via 
k-means, but I do not get the same partitions I need to validate. I am 
sure the following algorithm needs improvement, especially the use of 
putting matrices into a list through a for loop (ouch) - I am very 
grateful for your comments of improving this terrible piece of R-work 
(is it easier to do sthg with apply?).

Thank you very much for your help and support
Mark

mat <- matrix( c(6,7,8,2,3,4,12,14,14, 14,15,13,3,1,2,3,4,2, 
15,3,10,5,11,7,13,6,1, 15,4,10,6,12,8,12,7,1), ncol=9, byrow=T )
rownames(mat) <- paste("v", 1:4, sep="" )

clus.mat <- vector(mode="list", length=4)
for (i in 1:4){
	clus.mat[[i]] <- kmeans(mat[i,], centers=3, nstart=1, 
algorithm="MacQueen") ## run kmeans on each row (clustering per single 
variable)
}

clus.mat



From ripley at stats.ox.ac.uk  Sat Jan  7 13:42:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 Jan 2006 12:42:12 +0000 (GMT)
Subject: [R] level sets of factors are different
In-Reply-To: <a3e689eb0601070311p6198d529t508c1704ee7c919f@mail.gmail.com>
References: <a3e689eb0601070311p6198d529t508c1704ee7c919f@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601071231590.20874@gannet.stats>

The message is really explicit.

You are trying to compare the equality of two factors with different level 
sets.  Such factors are not comparable (and this is discussed on ?factor). 
You didn't give a reproducible example, and you have not told us what you 
are trying to do, so all we can do is repeat R's report that what you 
actually did is not sensible.

On Sat, 7 Jan 2006, Richard van Wingerden wrote:

> Hi,
>
> I have two data frames A en B.
> I want to filter B with values of A
>
> data_frame_b
> names <- colnames(data_frame_b[1:1, ])
>
> filtera <- data.frame(data_frame_a[1:1])

This is bit strange. data_frame_a[1:1] is data_frame_a[1] and is a 
single-column data-frame.  I think you might just as well use

filtera <- data_frame_a[[1]]

which is (probably) a factor.  You then seem to want to extract values 
equal to its first element, so maybe you actually wanted
columnX == as.character(data_frame_a[1,1]) ?

>
> 	print(nrow(filtera))
>
> 	if (nrow(filtera)>0){
>
> 		filtered_frame_b <- subset(data_frame_b, ColumnX == filtera[1, 1], names)
>         }
> The results are:
>
> [1] 124
> Error in Ops.factor(ColumnX, filtera[1, 1]) :
>        level sets of factors are different
>
>
> What is wrong??
>
> Richard


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From detlef.steuer at hsu-hamburg.de  Fri Jan  6 08:56:53 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Fri, 6 Jan 2006 08:56:53 +0100
Subject: [R] Wikis for R
In-Reply-To: <20060105132304.GB30865@ime.usp.br>
References: <17340.65179.89485.345795@stat.math.ethz.ch>
	<20060105132304.GB30865@ime.usp.br>
Message-ID: <20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>


On Thu, 5 Jan 2006 11:23:04 -0200
"Fernando Henrique Ferraz P. da Rosa" <academic at feferraz.net> wrote:

> Martin Maechler writes:
> >  If you go to the bottom of that wikipedia page,
> >  you see that there is an "R Wiki" -- and has been for several
> >  years now (!) at a Hamburg (De) university.
> >  http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
> > 
> > (...) 
> > So, are you sure that another R Wiki is desirable, rather than
> > have people who "believe in Wiki's for R" use the existing
> > one(s)?   I believe the main challenge will (similar as for
> > an "R-beginners" mailing list) to have well-qualified "editors"
> > to be willing to review and amend what others have written.
> 
>         Ive tried to colaborate on the R Wiki hosted by the Hamburg
> university but the Wiki would get regularlly vandalized by some spam
> bot, and then I'd have to manually keep reverting it several times. Also
> the wiki engine used by this wiki is very rudimentary. I think the
> DokuWiki engine, which is used by Philippe Grosjean is more promising as
> a workhorse for an 'official' R-wiki. 

What Fernando says is mostly true. I installed the UseMod Wiki after talking to some of "us" as a test balloon. The engine was chosen according to simplistic installation and a "just-enough" feature set. Perhaps that was a wrong decision. I`m one of the believers in wikis, but mine over here did not fly.

Probably this and any wiki needs a critical mass for visitors beginning to return and collaborate and therein the Hamburg wiki failed. (One day I wanted to take down the wiki for apparently having no users, but just then someone gave some feedback)

As Frank Harrel pointed out, this may be because R-help is just too helpful.
In contrast to Frank I don`t think we should abandon e-mail because of its success. The mailing list is, in my opinion, the single biggest plus R has above all competition.
The wiki should provide something complementary to r-help. Btw. those who do not search for information in the mailing list archives or on CRAN before asking simple questions won't do so in a wiki or in a bulletin board system. (I find those simply unusable. Am I getting old? I want to edit using my favourite editor, not in some browser window.)
 
A central place for example code was my intention, when opening the wiki back then. 

Back to operating wikis:
The wiki spamming is a serious problem, especially because I HATE to login to read or edit anything. So the choice is: take the wiki as seriously as work and have a look every other day to remove the spam (or better: form a group of volunteers). That hurts or at least is no fun. Or put restrictions on it. That hurts even more. Perhaps I do not understand Philippe`s "loggable". What does a logfile with IPs help? The spammers are strangers selling viagra; I don`t want to find them :-)

To sum it up:
There is a very simple way to proceed:
Philippe uses his Docuwiki install  as official, _general_ Rwiki and I close down mine. The beginners will find their niche in there, if there is a real demand. 
I wouldnt mind to give up "my" wiki, because I have to admit it failed to achieve what I would have liked.

So, Philippe, if you like, you can take over. I would replace my wiki with a notice where to find yours and the community gets a second chance :-)

Detlef



> 
>         I think that the title could be perhaps changed to Rwiki
> and the contents currently hosted on the Hamburg wiki 'transfered' to 
> the new location, if the current mantainers of the Hamburg Wiki and
> Philippe Grosjean agree (Im cc-ing this msg to them).
> 
>         This could emerge then as official or semi-oficial R-wiki, to be
> linked to from the R-project home. 
> 
> 
> --
> "Though this be randomness, yet there is structure in't."
>                                            Rosa, F.H.F.P
> 
> Instituto de Matemtica e Estatstica
> Universidade de So Paulo
> Fernando Henrique Ferraz P. da Rosa
> http://www.feferraz.net
>



From f.harrell at vanderbilt.edu  Sat Jan  7 15:04:30 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 07 Jan 2006 08:04:30 -0600
Subject: [R] Wikis for R
In-Reply-To: <20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>
References: <17340.65179.89485.345795@stat.math.ethz.ch>	<20060105132304.GB30865@ime.usp.br>
	<20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>
Message-ID: <43BFCA6E.8040301@vanderbilt.edu>

Detlef Steuer wrote:
> On Thu, 5 Jan 2006 11:23:04 -0200
> "Fernando Henrique Ferraz P. da Rosa" <academic at feferraz.net> wrote:
> 
> 
>>Martin Maechler writes:
>>
>>> If you go to the bottom of that wikipedia page,
>>> you see that there is an "R Wiki" -- and has been for several
>>> years now (!) at a Hamburg (De) university.
>>> http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>>>
>>>(...) 
>>>So, are you sure that another R Wiki is desirable, rather than
>>>have people who "believe in Wiki's for R" use the existing
>>>one(s)?   I believe the main challenge will (similar as for
>>>an "R-beginners" mailing list) to have well-qualified "editors"
>>>to be willing to review and amend what others have written.
>>
>>        Ive tried to colaborate on the R Wiki hosted by the Hamburg
>>university but the Wiki would get regularlly vandalized by some spam
>>bot, and then I'd have to manually keep reverting it several times. Also
>>the wiki engine used by this wiki is very rudimentary. I think the
>>DokuWiki engine, which is used by Philippe Grosjean is more promising as
>>a workhorse for an 'official' R-wiki. 
> 
> 
> What Fernando says is mostly true. I installed the UseMod Wiki after talking to some of "us" as a test balloon. The engine was chosen according to simplistic installation and a "just-enough" feature set. Perhaps that was a wrong decision. I`m one of the believers in wikis, but mine over here did not fly.
> 
> Probably this and any wiki needs a critical mass for visitors beginning to return and collaborate and therein the Hamburg wiki failed. (One day I wanted to take down the wiki for apparently having no users, but just then someone gave some feedback)
> 
> As Frank Harrel pointed out, this may be because R-help is just too helpful.
> In contrast to Frank I don`t think we should abandon e-mail because of its success. The mailing list is, in my opinion, the single biggest plus R has above all competition.
> The wiki should provide something complementary to r-help. Btw. those who do not search for information in the mailing list archives or on CRAN before asking simple questions won't do so in a wiki or in a bulletin board system. (I find those simply unusable. Am I getting old? I want to edit using my favourite editor, not in some browser window.)
>  
> A central place for example code was my intention, when opening the wiki back then. 
> 
> Back to operating wikis:
> The wiki spamming is a serious problem, especially because I HATE to login to read or edit anything. So the choice is: take the wiki as seriously as work and have a look every other day to remove the spam (or better: form a group of volunteers). That hurts or at least is no fun. Or put restrictions on it. That hurts even more. Perhaps I do not understand Philippe`s "loggable". What does a logfile with IPs help? The spammers are strangers selling viagra; I don`t want to find them :-)
> 
> To sum it up:
> There is a very simple way to proceed:
> Philippe uses his Docuwiki install  as official, _general_ Rwiki and I close down mine. The beginners will find their niche in there, if there is a real demand. 
> I wouldnt mind to give up "my" wiki, because I have to admit it failed to achieve what I would have liked.
> 
> So, Philippe, if you like, you can take over. I would replace my wiki with a notice where to find yours and the community gets a second chance :-)
> 
> Detlef

The current e-mail system places a low burden on users if they follow 
basic posting rules.  The burden is too low and users still do not 
search for past answers and we also get dozens of separate messages on a 
single topic (e.g., ylim on barplots).  As long as everyone allows this, 
a wiki or discussion board will not work.  We need to rethink the e-mail 
system in my view to create motivations for approaches with true 
memories and hierarchical keyword organization instead of using the 
apparent memory-less system.  This should be thought through to include 
the new graphics gallery and a data manipulation examples gallery, and 
other things, and needs to be launched from www.r-project.org IMHO.

Frank

> 
> 
> 
> 
>>        I think that the title could be perhaps changed to Rwiki
>>and the contents currently hosted on the Hamburg wiki 'transfered' to 
>>the new location, if the current mantainers of the Hamburg Wiki and
>>Philippe Grosjean agree (Im cc-ing this msg to them).
>>
>>        This could emerge then as official or semi-oficial R-wiki, to be
>>linked to from the R-project home. 
>>
>>
>>--
>>"Though this be randomness, yet there is structure in't."
>>                                           Rosa, F.H.F.P
>>
>>Instituto de Matemtica e Estatstica
>>Universidade de So Paulo
>>Fernando Henrique Ferraz P. da Rosa
>>http://www.feferraz.net
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ka4alin at yandex.ru  Sat Jan  7 15:32:51 2006
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Sat, 07 Jan 2006 17:32:51 +0300
Subject: [R] R-help Digest, Vol 35, Issue 7
In-Reply-To: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
Message-ID: <43BFD113.50908@yandex.ru>

Hello, dear participants!

Could you tip me, is there any simple and nice way to build scatter-plot 
for three different types of data (, and o and * - signs, for example) 
with legend.

Now i can guess only that way:

plot(x~y,data=subset(mydata,factor1=='1'), pch='.',col='blue')
points(x~y,data=subset(mydata,factor1=='2'), pch='*',col='green')
points(.... etc

What is the simple and nice way?
Thank you very much for your kindness and help.

-- 
Evgeniy Kachalin



From ggrothendieck at gmail.com  Sat Jan  7 15:55:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Jan 2006 09:55:05 -0500
Subject: [R] Wikis for R
In-Reply-To: <43BFCA6E.8040301@vanderbilt.edu>
References: <17340.65179.89485.345795@stat.math.ethz.ch>
	<20060105132304.GB30865@ime.usp.br>
	<20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>
	<43BFCA6E.8040301@vanderbilt.edu>
Message-ID: <971536df0601070655l63b675ddv569dc866fe79d05@mail.gmail.com>

On 1/7/06, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>
> The current e-mail system places a low burden on users if they follow
> basic posting rules.  The burden is too low and users still do not
> search for past answers and we also get dozens of separate messages on a
> single topic (e.g., ylim on barplots).  As long as everyone allows this,
> a wiki or discussion board will not work.  We need to rethink the e-mail

One technical comment here.  The new R wiki at
http://www.sciviews.org/_rgui/wiki.
appears to support RSS so one does not have to have the burden of actually
visiting the wiki to read its content.  If you regularly check an RSS reader
in the same way that you regularly check email then the content will come
to you just you as it does in email.    In fact there are even readers that
come email and RSS together so the difference is transparent.

I personally use SharpReader on Windows XP.   If one simply drags the
url of the wiki to the SharpReader address bar then all changed and new
pages display and if one wants to keep it as a subscription one just clicks
the Subscribe button.  amphetadesk is another popular RSS reader.



From murdoch at stats.uwo.ca  Sat Jan  7 16:15:42 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 07 Jan 2006 10:15:42 -0500
Subject: [R] Wikis etc.
In-Reply-To: <43BF1341.4020800@metrak.com>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
Message-ID: <43BFDB1E.5030804@stats.uwo.ca>

On 1/6/2006 8:02 PM, paul sorenson wrote:
> I am a fan of wiki's and I reckon it would really help with making R 
> more accessible.  On one extreme you have this email list and on the 
> other extreme you have RNews and the PDF's on CRAN.  A wiki might hit 
> the spot between them and reduce the traffic on the email list.

The difficulty is getting it going.  I haven't used Wikis, and visited 
the two that have been mentioned, aiming to answer the example question 
Frank posed ("ylim in barplots").  It's not addressed on either, which 
is not too surprising, but then I didn't know what to do next, either as 
someone who wanted the answer, or someone who wanted to provide it.

Duncan Murdoch

> 
> Frank E Harrell Jr wrote:
> 
>>I feel that as long as people continue to provide help on r-help wikis 
>>will not be successful.  I think we need to move to a central wiki or 
>>discussion board and to move away from e-mail.  People are extremely 
>>helpful but e-mail seems to be to always be memory-less and messages get 
>>too long without factorization of old text.  R-help is now too active 
>>and too many new users are asking questions asked dozens of times for 
>>e-mail to be effective.
>>
>>The wiki also needs to collect and organize example code, especially for 
>>data manipulation.  I think that new users would profit immensely from a 
>>compendium of examples.
>>
>>Just my .02 Euros
>>
>>Frank
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friedm69 at msu.edu  Sat Jan  7 16:38:04 2006
From: friedm69 at msu.edu (Steven K Friedman)
Date: Sat, 07 Jan 2006 10:38:04 -0500
Subject: [R] Generating raster simulations
Message-ID: <E1EvG8r-0000kf-9Q@sys23.mail.msu.edu>


Greetings, 

I'm trying to find a function to generate a random landscape consisting of x 
by y cells, allows a specification of number of classes and their 
proportional abundance in the landscape. 

Does R support such a function ? 

Thanks in advance 

Steve 

Steve Friedman, Assistant Professor
Center for Global Change and Earth Observations
Michigan State University
East Lansing, Michigan 48824
Phone - 517 - 648 - 6290



From jsorkin at grecc.umaryland.edu  Sat Jan  7 16:51:31 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 07 Jan 2006 10:51:31 -0500
Subject: [R] Wikis etc.
Message-ID: <s3bf9d55.076@medicine.umaryland.edu>

Several people have stated that one of the problems with the current Email help model is that many questions are asked over, and over again and that people do not search for past answers. Let me point out that the existence of past answers and how to find and search is not known by many people, particularly novice R users. The situation would be greatly helped if the mailing list would automatically add a header or footer to all Email messages giving the URL of the archived Email threads. Don't expect people to know that what are not told! Those people who, in their answers, suggest that people should search the archives should include the URL of the archives in their response (http://carn.us.r-project.org then click on the word SEARCH in the left-hand column ). Even if a novice would think about searching the R archives he, or she, might have difficulty finding the correct site. A naive google search of "R-archive" and "R archive" did not return the correct URL. Even if a novice would know about the CRAN web site (CRAN is not an intuitive acronym for R), when the novice would get to the CRAN web site they would have to know to click on SEARCH. It might help if the CRAN web site contained a link titled SEARCH R ARCHIVES.
John 


John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> Duncan Murdoch <murdoch at stats.uwo.ca> 01/07/06 10:15 AM >>>
On 1/6/2006 8:02 PM, paul sorenson wrote:
> I am a fan of wiki's and I reckon it would really help with making R 
> more accessible.  On one extreme you have this email list and on the 
> other extreme you have RNews and the PDF's on CRAN.  A wiki might hit 
> the spot between them and reduce the traffic on the email list.

The difficulty is getting it going.  I haven't used Wikis, and visited 
the two that have been mentioned, aiming to answer the example question 
Frank posed ("ylim in barplots").  It's not addressed on either, which 
is not too surprising, but then I didn't know what to do next, either as 
someone who wanted the answer, or someone who wanted to provide it.

Duncan Murdoch

> 
> Frank E Harrell Jr wrote:
> 
>>I feel that as long as people continue to provide help on r-help wikis 
>>will not be successful.  I think we need to move to a central wiki or 
>>discussion board and to move away from e-mail.  People are extremely 
>>helpful but e-mail seems to be to always be memory-less and messages get 
>>too long without factorization of old text.  R-help is now too active 
>>and too many new users are asking questions asked dozens of times for 
>>e-mail to be effective.
>>
>>The wiki also needs to collect and organize example code, especially for 
>>data manipulation.  I think that new users would profit immensely from a 
>>compendium of examples.
>>
>>Just my .02 Euros
>>
>>Frank
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Sat Jan  7 17:00:37 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 7 Jan 2006 11:00:37 -0500
Subject: [R] Wikis etc.
In-Reply-To: <s3bf9d55.076@medicine.umaryland.edu>
References: <s3bf9d55.076@medicine.umaryland.edu>
Message-ID: <20060107160037.GA19836@psych.upenn.edu>

On 01/07/06 10:51, John Sorkin wrote:
> The situation
> would be greatly helped if the mailing list would automatically add a header or
> footer to all Email messages giving the URL of the archived Email threads. Don't
> expect people to know that what are not told! Those people who, in their answers,
> suggest that people should search the archives should include the URL of the archives
> in their response (http://carn.us.r-project.org then click on the word SEARCH in the
> left-hand column ).

Or, more tersely, http://cran.r-project.org/search.html.

http://cran.us.r-project.org/search.html is a mirror, and there
are other mirrors.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From jsorkin at grecc.umaryland.edu  Sat Jan  7 17:05:01 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 07 Jan 2006 11:05:01 -0500
Subject: [R] Wikis etc.
Message-ID: <s3bfa074.082@medicine.umaryland.edu>

Jon,
Thank you for the terse form of the URL. I hope the mailing list will automatically include it in there Email messages.
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> Jonathan Baron <baron at psych.upenn.edu> 01/07/06 11:00 AM >>>
On 01/07/06 10:51, John Sorkin wrote:
> The situation
> would be greatly helped if the mailing list would automatically add a header or
> footer to all Email messages giving the URL of the archived Email threads. Don't
> expect people to know that what are not told! Those people who, in their answers,
> suggest that people should search the archives should include the URL of the archives
> in their response (http://carn.us.r-project.org then click on the word SEARCH in the
> left-hand column ).

Or, more tersely, http://cran.r-project.org/search.html.

http://cran.us.r-project.org/search.html is a mirror, and there
are other mirrors.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Jan  7 17:09:41 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jan 2006 17:09:41 +0100
Subject: [R] R-help Digest, Vol 35, Issue 7
In-Reply-To: <43BFD113.50908@yandex.ru>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
	<43BFD113.50908@yandex.ru>
Message-ID: <43BFE7C5.1030307@statistik.uni-dortmund.de>

Evgeniy Kachalin wrote:

> Hello, dear participants!
> 
> Could you tip me, is there any simple and nice way to build scatter-plot 
> for three different types of data (, and o and * - signs, for example) 
> with legend.
> 
> Now i can guess only that way:
> 
> plot(x~y,data=subset(mydata,factor1=='1'), pch='.',col='blue')
> points(x~y,data=subset(mydata,factor1=='2'), pch='*',col='green')
> points(.... etc
> 
> What is the simple and nice way?
> Thank you very much for your kindness and help.
> 


Example:


with(iris,
   plot(Sepal.Length, Sepal.Width, pch = as.integer(Species)))
with(iris,
   legend(7, 4.4, legend = unique(as.character(Species)),
                     pch = unique(as.integer(Species))))


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sat Jan  7 17:14:48 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jan 2006 17:14:48 +0100
Subject: [R] Wikis etc.
In-Reply-To: <20060107160037.GA19836@psych.upenn.edu>
References: <s3bf9d55.076@medicine.umaryland.edu>
	<20060107160037.GA19836@psych.upenn.edu>
Message-ID: <43BFE8F8.4060806@statistik.uni-dortmund.de>

Jonathan Baron wrote:

> On 01/07/06 10:51, John Sorkin wrote:
> 
>>The situation
>>would be greatly helped if the mailing list would automatically add a header or
>>footer to all Email messages giving the URL of the archived Email threads. Don't
>>expect people to know that what are not told! Those people who, in their answers,
>>suggest that people should search the archives should include the URL of the archives
>>in their response (http://carn.us.r-project.org then click on the word SEARCH in the
>>left-hand column ).
> 
> 
> Or, more tersely, http://cran.r-project.org/search.html.


But then, everybody loads stuff from CRAN master and does not use an 
appropriate mirror.
I'd like to suggest *not* to use any mirror URL explicitly in R-help 
mails (always, not only related to this thread).
In this case, I'd propose to write something like "CRAN-mirror/search.html"

Uwe



> http://cran.us.r-project.org/search.html is a mirror, and there
> are other mirrors.
> 
> Jon



From ligges at statistik.uni-dortmund.de  Sat Jan  7 17:18:45 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jan 2006 17:18:45 +0100
Subject: [R] Generating raster simulations
In-Reply-To: <E1EvG8r-0000kf-9Q@sys23.mail.msu.edu>
References: <E1EvG8r-0000kf-9Q@sys23.mail.msu.edu>
Message-ID: <43BFE9E5.8020603@statistik.uni-dortmund.de>

Steven K Friedman wrote:

> Greetings, 
> 
> I'm trying to find a function to generate a random landscape consisting of x 
> by y cells, allows a specification of number of classes and their 
> proportional abundance in the landscape. 
> 
> Does R support such a function ? 

1. I do not know.

2. A look at the many packages on spatial statistics might reveal some 
already existing packages.

3. Question: Can one cell contain more than one object (e.g. objects of 
two different classes)? Ideas:
  - If "No": sample() and make a matrix
  - If "Yes": sample() a logical matrix in for each class and make an 
array, if there are not too many classes.

Uwe Ligges


> Thanks in advance 
> 
> Steve 
> 
> Steve Friedman, Assistant Professor
> Center for Global Change and Earth Observations
> Michigan State University
> East Lansing, Michigan 48824
> Phone - 517 - 648 - 6290
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Sat Jan  7 17:24:19 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 7 Jan 2006 17:24:19 +0100
Subject: [R] Finding R mailing list archives {was "Wikis etc."}
In-Reply-To: <s3bfa074.082@medicine.umaryland.edu>
References: <s3bfa074.082@medicine.umaryland.edu>
Message-ID: <17343.60211.358181.704410@stat.math.ethz.ch>

>>>>> "John" == John Sorkin <jsorkin at grecc.umaryland.edu>
>>>>>     on Sat, 07 Jan 2006 11:05:01 -0500 writes:

    John> Jon, Thank you for the terse form of the URL. I hope
    John> the mailing list will automatically include it in
    John> there Email messages.  John

well, it *already* contains 

> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

where the first URL is **the** R-help page, and has itself links
to more than one archive of the mailing list,
and the 2nd URL (posting guide) also explains many things
(including the mailing list page and archives).

I really wonder if adding yet another URL to the footer of every
message will be the solution; as others have correctly remarked,
the problem is that for many newbies it is more convenient to ask
rather than to first read something that contains more than three
words. ;-)

Well, then, maybe for some people, an extra line with another link
("click click" instead of reading) might be the solution...

Martin Maechler, ETH Zurich, (your mailing list maintainer)



From jsorkin at grecc.umaryland.edu  Sat Jan  7 17:28:18 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 07 Jan 2006 11:28:18 -0500
Subject: [R] Wikis etc.
Message-ID: <s3bfa5f1.096@medicine.umaryland.edu>

Uwe,
I think you suggestion for giving the URL of the archives as, 
 "cran-MIRROR/search.html" (emphasis added) is not optimal, because the URL as does not work. The URL given should work as given. Thus,
 http://cran.us.r-project.org/search.html 
http://www.stats.bris.ac.uk/R/search.html 
or 
 http://cran.r-project.org/search.htm
are more helpful.
John


John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 01/07/06 11:14 AM >>>
Jonathan Baron wrote:

> On 01/07/06 10:51, John Sorkin wrote:
> 
>>The situation
>>would be greatly helped if the mailing list would automatically add a header or
>>footer to all Email messages giving the URL of the archived Email threads. Don't
>>expect people to know that what are not told! Those people who, in their answers,
>>suggest that people should search the archives should include the URL of the archives
>>in their response (http://carn.us.r-project.org then click on the word SEARCH in the
>>left-hand column ).
> 
> 
> Or, more tersely, http://cran.r-project.org/search.html.


But then, everybody loads stuff from CRAN master and does not use an 
appropriate mirror.
I'd like to suggest *not* to use any mirror URL explicitly in R-help 
mails (always, not only related to this thread).
In this case, I'd propose to write something like "CRAN-mirror/search.html"

Uwe



> http://cran.us.r-project.org/search.html is a mirror, and there
> are other mirrors.
> 
> Jon



From ligges at statistik.uni-dortmund.de  Sat Jan  7 17:31:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jan 2006 17:31:58 +0100
Subject: [R] Installing Task Views
In-Reply-To: <003b01c61321$806a0c80$0ffd7b80@MCANotebook>
References: <003b01c61321$806a0c80$0ffd7b80@MCANotebook>
Message-ID: <43BFECFE.6020908@statistik.uni-dortmund.de>

Does all work for me with the latest release of R (2.2.1) and ctv 
(0.3-1) on Windows NT 4.0 SP6.

Hence: Which version of R, which version of ctv, and which OS do you use?

Uwe Ligges



Mark Andersen wrote:

> Hello,
> 
>  
> 
> I am just beginning to use R, after several years of using S-Plus (with
> mixed success). I saw a recommendation on another mailing list for the
> Environmetrics and Spatial Task Views, as a good way for a new user to get
> started actually using R. The Task Views page at CRAN says:
> 
> "To automatically install these views, the ctv package needs to be
> installed, e.g., via
> install.packages("ctv")and then the views can be installed via install.views
> (after loading ctv), e.g.,
> install.views("Econometrics")"
> 
> I have installed "ctv" and I'm assuming that "loading ctv" means entering 
> 
>  
> 
> 
>>library("ctv")
> 
> 
>  
> 
> If I then enter
> 
>  
> 
> 
>>install.views("Environmetrics")
> 
> 
>  
> 
> I get the error message 
> 
>  
> 
> "Warning message:
> 
> CRAN task view Environmetrics not available in:
> install.views("Environmetrics")"
> 
> If I then go up to the Packages menu and Set CRAN Mirror to, for example,
> USA (CA 2) and again enter
> 
> 
>>install.views("Environmetrics")
> 
> 
> I now get the error message
> 
> "Error in install.packages(pkgs, CRAN = views[[i]]$repository, dependencies
> = dependencies,  : 
> 
>         unused argument(s) (CRAN ...)"
> 
>  
> 
> Entering "CRAN.views" in the R GUI does indeed give a complete list of task
> view names, topics, maintainers, and repositories. Selecting different CRAN
> mirrors produces the same error messages as above, as does attempting to
> install a different task view.
> 
>  
> 
> I have searched the R-help mailing list archive and found several postings
> announcing the availability of task views, but not on how to install them. I
> have searched the pdf manuals, and found no instances of "task view". I have
> also searched the FAQs. The help for "install.views" basically repeats the
> information on the CRAN site, providing information on using the function,
> but not on actually installing task views. The example in the article on
> task views in the May 2005 issue of R News uses the "lib = " argument, which
> is not mentioned in the help for "install.views". It appears that the
> instructions at CRAN for installing task views are missing at least one
> step. Can anyone point me to a reliable set of instructions for installing
> (not to mention actually using) a task view? Many thanks in advance.
> 
>  
> 
> Regards,
> 
> Mark C. Andersen
> 
>  
> 
> Dr. Mark C. Andersen
> 
> Associate Professor
> 
> Department of Fishery and Wildlife Sciences
> 
> New Mexico State University
> 
> Las Cruces NM 88003-0003
> 
> phone: 505-646-8034
> 
> fax: 505-646-1281
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sat Jan  7 17:52:38 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Jan 2006 11:52:38 -0500
Subject: [R] Wikis etc.
In-Reply-To: <s3bfa5f1.096@medicine.umaryland.edu>
References: <s3bfa5f1.096@medicine.umaryland.edu>
Message-ID: <971536df0601070852p1cad8c11t8de51d2fe2d4fc5f@mail.gmail.com>

The trouble with a fixed link is that either that server gets overloaded
or one has to post to a list of servers which is a nuisance.  Perhaps
this could be included at the bottom of each post instead:

Before posting, search docs for xyz via R command: RSiteSearch("xyz")
and if you still need to post please include a REPRODUCIBLE code snippet.
See posting guide for more info: http://www.R-project.org/posting-guide.html

On 1/7/06, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> Uwe,
> I think you suggestion for giving the URL of the archives as,
>  "cran-MIRROR/search.html" (emphasis added) is not optimal, because the URL as does not work. The URL given should work as given. Thus,
>  http://cran.us.r-project.org/search.html
> http://www.stats.bris.ac.uk/R/search.html
> or
>  http://cran.r-project.org/search.htm
> are more helpful.
> John
>
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> 410-605-7119
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
>
> >>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 01/07/06 11:14 AM >>>
> Jonathan Baron wrote:
>
> > On 01/07/06 10:51, John Sorkin wrote:
> >
> >>The situation
> >>would be greatly helped if the mailing list would automatically add a header or
> >>footer to all Email messages giving the URL of the archived Email threads. Don't
> >>expect people to know that what are not told! Those people who, in their answers,
> >>suggest that people should search the archives should include the URL of the archives
> >>in their response (http://carn.us.r-project.org then click on the word SEARCH in the
> >>left-hand column ).
> >
> >
> > Or, more tersely, http://cran.r-project.org/search.html.
>
>
> But then, everybody loads stuff from CRAN master and does not use an
> appropriate mirror.
> I'd like to suggest *not* to use any mirror URL explicitly in R-help
> mails (always, not only related to this thread).
> In this case, I'd propose to write something like "CRAN-mirror/search.html"
>
> Uwe
>
>
>
> > http://cran.us.r-project.org/search.html is a mirror, and there
> > are other mirrors.
> >
> > Jon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ros110 at gmail.com  Sat Jan  7 17:59:25 2006
From: ros110 at gmail.com (Richard van Wingerden)
Date: Sat, 7 Jan 2006 17:59:25 +0100
Subject: [R] level sets of factors are different
In-Reply-To: <Pine.LNX.4.61.0601071231590.20874@gannet.stats>
References: <a3e689eb0601070311p6198d529t508c1704ee7c919f@mail.gmail.com>
	<Pine.LNX.4.61.0601071231590.20874@gannet.stats>
Message-ID: <a3e689eb0601070859r5d3442fehd00cf02af1bd63f0@mail.gmail.com>

Thanks!

The as.character fixed it!

Regards,
Richard

On 1/7/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> The message is really explicit.
>
> You are trying to compare the equality of two factors with different level
> sets.  Such factors are not comparable (and this is discussed on ?factor).
> You didn't give a reproducible example, and you have not told us what you
> are trying to do, so all we can do is repeat R's report that what you
> actually did is not sensible.
>
> On Sat, 7 Jan 2006, Richard van Wingerden wrote:
>
> > Hi,
> >
> > I have two data frames A en B.
> > I want to filter B with values of A
> >
> > data_frame_b
> > names <- colnames(data_frame_b[1:1, ])
> >
> > filtera <- data.frame(data_frame_a[1:1])
>
> This is bit strange. data_frame_a[1:1] is data_frame_a[1] and is a
> single-column data-frame.  I think you might just as well use
>
> filtera <- data_frame_a[[1]]
>
> which is (probably) a factor.  You then seem to want to extract values
> equal to its first element, so maybe you actually wanted
> columnX == as.character(data_frame_a[1,1]) ?
>
> >
> >       print(nrow(filtera))
> >
> >       if (nrow(filtera)>0){
> >
> >               filtered_frame_b <- subset(data_frame_b, ColumnX == filtera[1, 1], names)
> >         }
> > The results are:
> >
> > [1] 124
> > Error in Ops.factor(ColumnX, filtera[1, 1]) :
> >        level sets of factors are different
> >
> >
> > What is wrong??
> >
> > Richard
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ronggui.huang at gmail.com  Sat Jan  7 18:00:40 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Sun, 8 Jan 2006 01:00:40 +0800
Subject: [R] Installing Task Views
In-Reply-To: <43BFECFE.6020908@statistik.uni-dortmund.de>
References: <003b01c61321$806a0c80$0ffd7b80@MCANotebook>
	<43BFECFE.6020908@statistik.uni-dortmund.de>
Message-ID: <38b9f0350601070900l18eefb03t@mail.gmail.com>

try  install.views("Environmetrics",rep="http://www.cran.r-project.org")

2006/1/8, Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> Does all work for me with the latest release of R (2.2.1) and ctv
> (0.3-1) on Windows NT 4.0 SP6.
>
> Hence: Which version of R, which version of ctv, and which OS do you use?
>
> Uwe Ligges
>
>
>
> Mark Andersen wrote:
>
> > Hello,
> >
> >
> >
> > I am just beginning to use R, after several years of using S-Plus (with
> > mixed success). I saw a recommendation on another mailing list for the
> > Environmetrics and Spatial Task Views, as a good way for a new user to get
> > started actually using R. The Task Views page at CRAN says:
> >
> > "To automatically install these views, the ctv package needs to be
> > installed, e.g., via
> > install.packages("ctv")and then the views can be installed via install.views
> > (after loading ctv), e.g.,
> > install.views("Econometrics")"
> >
> > I have installed "ctv" and I'm assuming that "loading ctv" means entering
> >
> >
> >
> >
> >>library("ctv")
> >
> >
> >
> >
> > If I then enter
> >
> >
> >
> >
> >>install.views("Environmetrics")
> >
> >
> >
> >
> > I get the error message
> >
> >
> >
> > "Warning message:
> >
> > CRAN task view Environmetrics not available in:
> > install.views("Environmetrics")"
> >
> > If I then go up to the Packages menu and Set CRAN Mirror to, for example,
> > USA (CA 2) and again enter
> >
> >
> >>install.views("Environmetrics")
> >
> >
> > I now get the error message
> >
> > "Error in install.packages(pkgs, CRAN = views[[i]]$repository, dependencies
> > = dependencies,  :
> >
> >         unused argument(s) (CRAN ...)"
> >
> >
> >
> > Entering "CRAN.views" in the R GUI does indeed give a complete list of task
> > view names, topics, maintainers, and repositories. Selecting different CRAN
> > mirrors produces the same error messages as above, as does attempting to
> > install a different task view.
> >
> >
> >
> > I have searched the R-help mailing list archive and found several postings
> > announcing the availability of task views, but not on how to install them. I
> > have searched the pdf manuals, and found no instances of "task view". I have
> > also searched the FAQs. The help for "install.views" basically repeats the
> > information on the CRAN site, providing information on using the function,
> > but not on actually installing task views. The example in the article on
> > task views in the May 2005 issue of R News uses the "lib = " argument, which
> > is not mentioned in the help for "install.views". It appears that the
> > instructions at CRAN for installing task views are missing at least one
> > step. Can anyone point me to a reliable set of instructions for installing
> > (not to mention actually using) a task view? Many thanks in advance.
> >
> >
> >
> > Regards,
> >
> > Mark C. Andersen
> >
> >
> >
> > Dr. Mark C. Andersen
> >
> > Associate Professor
> >
> > Department of Fishery and Wildlife Sciences
> >
> > New Mexico State University
> >
> > Las Cruces NM 88003-0003
> >
> > phone: 505-646-8034
> >
> > fax: 505-646-1281
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From chrish at stats.ucl.ac.uk  Sat Jan  7 18:21:22 2006
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Sat, 7 Jan 2006 17:21:22 +0000 (GMT)
Subject: [R] Clustering and Rand Index
In-Reply-To: <43BFB646.7080305@t-online.de>
References: <43BFB646.7080305@t-online.de>
Message-ID: <Pine.LNX.4.64.0601071720210.1663@egon.stats.ucl.ac.uk>

Hi Mark,

I don't have the time at the moment to work through your code - but the 
adjusted Rand index can be computed by function clusterstats in package 
fpc.

Best,
Christian

On Sat, 7 Jan 2006, Mark Hempelmann wrote:

> Dear WizaRds,
>
> I am trying to compute the (adjusted) Rand Index in order to comprehend
> the variable selection heuristic (VS-KM) according to Brusco/ Cradit
> 2001 (Psychometrika 66 No.2 p.249-270, 2001).
>
> Unfortunately, I am unable to correctly use
> cl_ensemble and cl_agreement (package: clue). Here is what I am trying
> to do:
>
> library(clue)
>
> ##	Let p1..p4 be four partitions of the kind
>
> p1=c(1,1,1,2,2,2,3,3,3)
> p2=c(1,1,1,3,2,2,3,3,2)
> p3=c(1,2,1,3,1,3,1,3,2)
> p4=c(1,2,1,3,1,3,1,3,2)
>
> Each object within the partitions is assigned to cluster 1,2,3
> respectively. Now I have to create a cl_ensemble object, so that I can
> calculate the Rand index:
>
> ens <- cl_ensemble(list=c(p1,p2,p3,p4))
>
> which only leads to
> "Ensemble elements must be all partitions or all hierarchies."
>
> Although I understand that p1..p4 are vectors in this example, they
> represent the partitions I want to use. I don't know how to create the
> necessary partition object in order to transform it into an ensemble
> object, so that I can run cl_agreement - so much transformation, so
> little time...
>
> I have also tried to work around this prbl, creating partitions via
> k-means, but I do not get the same partitions I need to validate. I am
> sure the following algorithm needs improvement, especially the use of
> putting matrices into a list through a for loop (ouch) - I am very
> grateful for your comments of improving this terrible piece of R-work
> (is it easier to do sthg with apply?).
>
> Thank you very much for your help and support
> Mark
>
> mat <- matrix( c(6,7,8,2,3,4,12,14,14, 14,15,13,3,1,2,3,4,2,
> 15,3,10,5,11,7,13,6,1, 15,4,10,6,12,8,12,7,1), ncol=9, byrow=T )
> rownames(mat) <- paste("v", 1:4, sep="" )
>
> clus.mat <- vector(mode="list", length=4)
> for (i in 1:4){
> 	clus.mat[[i]] <- kmeans(mat[i,], centers=3, nstart=1,
> algorithm="MacQueen") ## run kmeans on each row (clustering per single
> variable)
> }
>
> clus.mat
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From erwig at eecs.oregonstate.edu  Sat Jan  7 18:47:11 2006
From: erwig at eecs.oregonstate.edu (Martin Erwig)
Date: Sat, 7 Jan 2006 09:47:11 -0800
Subject: [R] Question about graphics in R
Message-ID: <A49FA2C7-B144-4E5C-AA8F-751727512A01@eecs.oregonstate.edu>

Considering the R function/plot shown below, I wonder whether
it is possible to do the following changes:

(1) Change the color of each point to be picked from
list of colors according to its z-value. (The range
should be from blue (z=0) to red (z=1).) The grid
should then be omitted. [I have seen "terrain.colors", but
don't know how to use it for this purpose.]

(2) Add two lines to the surface for, say z=0.8 and
z=0.3. [Can contour or contourLines be used?]


---

x <- seq(0, 1, length = 50)
y <- x
f <- function(x,y) { sin ((1-x)*y) }
z <- outer(x,y,f)

persp(x, y, z,
	theta = 30, phi = 30,
	shade = 0.3, col = "red"
	)

---


Finally, I would also produce a flattened 2D map
of the same function, i.e. a map in which each point
(x,y) is mapped to a color in a range according to
f(x,y). Also two lines for f(x,y)=c1 and f(x,y)=c2
should be added.


Is this possible?

I would be grateful for any hints.


Thanks,
Martin



From patrick.giraudoux at univ-fcomte.fr  Sat Jan  7 19:01:22 2006
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 07 Jan 2006 19:01:22 +0100
Subject: [R] maptools, write.polylistShape
Message-ID: <43C001F2.9010900@univ-fcomte.fr>

Dear Roger,

I am trying to use the write.polylistShape() function of maptools for 
the first time and realize that it handles list of polygons of class 
'polylist'. However, it seems that no as.polylist() function exist in 
the package. The question behind that is: in your opinion, which would 
be the best  way to convert a list of matrix of polygon nodes 
coordinates into an object of  class polylist?

All the best,

Patrick

-- 

Department of Environmental Biology
EA3184 usc INRA
University of Franche-Comte
25030 Besancon Cedex
(France)

tel. +33 381 665 745
fax +33 381 665 797
http://lbe.univ-fcomte.fr



From edd at debian.org  Sat Jan  7 19:25:13 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 7 Jan 2006 12:25:13 -0600
Subject: [R] Wikis etc.
In-Reply-To: <43BFDB1E.5030804@stats.uwo.ca>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
	<43BFDB1E.5030804@stats.uwo.ca>
Message-ID: <17344.1929.451916.557388@basebud.nulle.part>


On 7 January 2006 at 10:15, Duncan Murdoch wrote:
| The difficulty is getting it going.  

Right. Which goes along with 'needs official endorsement'.  Debian recently
moved a more-or-less grassroots wiki to an official domain of the project,
and for the last few days I have been hitting

	http://wiki.debian.org/RecentChanges

frequently just to see what's happening. And it's brisk. It seems to have
take off. Debian has the same problem of list that have too many posts (and a
much lower signal/noise ratio that r-help) so this is welcome addition as a
means of communication and information store.

Yet the 'no silver bullet' rule still holds. There isn't one media to suit
everybody. Some prefer a list as list, some as digest, some channelled as a
news group, some read it via Gmane over the web, some read the rss feed. This
will offer another option, but we all will have to work jointly at it.

So my $0.02 would be to a) go for it, if possible but b) make it visible, and
closely tied to R Core / CRAN / R News / ....  Which poses the chicken/egg
problem of people running out of spare time to setup, admin, monitor,
hand-hold the wiki, its database, watch out for spammers, etc.  Volunteers to
do this, and maybe help Jon Baron on a first run using TWiki?

Regards, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From edd at debian.org  Sat Jan  7 19:41:18 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 7 Jan 2006 12:41:18 -0600
Subject: [R] Wikis etc.
In-Reply-To: <43BFE8F8.4060806@statistik.uni-dortmund.de>
References: <s3bf9d55.076@medicine.umaryland.edu>
	<20060107160037.GA19836@psych.upenn.edu>
	<43BFE8F8.4060806@statistik.uni-dortmund.de>
Message-ID: <17344.2894.588349.867615@basebud.nulle.part>


On 7 January 2006 at 17:14, Uwe Ligges wrote:
| Jonathan Baron wrote:
| > Or, more tersely, http://cran.r-project.org/search.html.
| 
| But then, everybody loads stuff from CRAN master and does not use an 

Round-robin DNS to spread the load among several machines answering for
cran.r-project.org would fix that. They'd have to be reasonably in-sync and
cover identical features such as search etc pp, of course.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From baron at psych.upenn.edu  Sat Jan  7 19:51:24 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 7 Jan 2006 13:51:24 -0500
Subject: [R] Wikis etc.
In-Reply-To: <17344.1929.451916.557388@basebud.nulle.part>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
	<43BFDB1E.5030804@stats.uwo.ca>
	<17344.1929.451916.557388@basebud.nulle.part>
Message-ID: <20060107185124.GB518@psych.upenn.edu>

On 01/07/06 12:25, Dirk Eddelbuettel wrote:
> So my $0.02 would be to a) go for it, if possible but b) make it visible, and
> closely tied to R Core / CRAN / R News / ....  Which poses the chicken/egg
> problem of people running out of spare time to setup, admin, monitor,
> hand-hold the wiki, its database, watch out for spammers, etc.  Volunteers to
> do this, and maybe help Jon Baron on a first run using TWiki?

Great.  I agree.  But there are now at least 3 proposals floating
around.  Mine was to base the wiki on (I think) the individual
help pages, each file in the help or html subdirectory of each
package would be a wiki page.  Since there are about 17000 of
these at last count - oh, my, could that really be true? - this
must be automated.  It should be fairly easy, but I just haven't
done it yet.  What this would amount to is an extended help
system.

Then there is the decision about what happens with updates.  I'm
beginning to think that there is just no other way to handle this
than to INCLUDE the latest version of each help file in the wicki
page, have the comments come AFTER it, and just don't worry about
it if the comments become out of date.  Eventually, someone will
edit them if it is important.

But I'm not at all sure this is the best way to go.  Given that
all the functions in each package are related, it might be better 
to have one wiki page per package.  In this case, there is less
difference between my proposal and the other existing wikis.

It is possible that too many wikis is bad.  On the other hand, it 
is possible that too few is bad.

> Hell, there are no rules here - we're trying to accomplish something.
>                                                   -- Thomas A. Edison

I love this.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From br44114 at gmail.com  Sat Jan  7 20:34:05 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Sat, 7 Jan 2006 14:34:05 -0500
Subject: [R] need palette of topographic colors similar to topo.colors()
Message-ID: <8d5a36350601071134g43c8cd1bqa84170f5b2cec9db@mail.gmail.com>

Dear useRs,

I got stuck trying to generate a palette of topographic colors that
would satisfy these two requirements:
   - the pallete must be 'anchored' at 0 (just like on a map), with
light blue/lawn green corresponding to data values close to 0 (dark
blue to light blue for negative values, green-yellow-brown for
positive values)
   - the brown must get darker for higher positive values.

topo.colors() fails both requirements and AFAICS lacks any options to
control its behavior.
  #---unsatisfactory topo.colors() behavior
  topoclr <- function(tgt)
  {
  clr <- topo.colors(length(tgt))
  clr <- clr[round(rank(tgt),0)]
  plot(tgt,pch=15,col=clr)
  }
  par(mfrow=c(2,1)) ; topoclr(-50:50) ; topoclr(-20:80)

An acceptable solution would be something like this
  grayclr <- function(tgt)
  {
  tgt <- sort(tgt) ; neg <- which(tgt < 0)
  clrneg <- gray(0:length(tgt[neg])/length(tgt[neg]))
  clrpos <- gray(length(tgt[-neg]):0/length(tgt[-neg]))
  clr <- c(clrneg,clrpos)
  plot(tgt,pch=15,col=clr)
  }
  par(mfrow=c(2,1)) ; grayclr(-50:50) ; grayclr(-20:80)
if only I could make gray() use blue/brown instead of black (I tried a
couple of things but got stuck again).

Any suggestions?

Thank you,
b.



From Roger.Bivand at nhh.no  Sat Jan  7 21:10:27 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 7 Jan 2006 21:10:27 +0100 (CET)
Subject: [R] need palette of topographic colors similar to topo.colors()
In-Reply-To: <8d5a36350601071134g43c8cd1bqa84170f5b2cec9db@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0601072106330.14967-100000@reclus.nhh.no>

On Sat, 7 Jan 2006, bogdan romocea wrote:

> Dear useRs,
> 
> I got stuck trying to generate a palette of topographic colors that
> would satisfy these two requirements:
>    - the pallete must be 'anchored' at 0 (just like on a map), with
> light blue/lawn green corresponding to data values close to 0 (dark
> blue to light blue for negative values, green-yellow-brown for
> positive values)
>    - the brown must get darker for higher positive values.
> 
> topo.colors() fails both requirements and AFAICS lacks any options to
> control its behavior.
>   #---unsatisfactory topo.colors() behavior
>   topoclr <- function(tgt)
>   {
>   clr <- topo.colors(length(tgt))
>   clr <- clr[round(rank(tgt),0)]
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; topoclr(-50:50) ; topoclr(-20:80)
> 
> An acceptable solution would be something like this
>   grayclr <- function(tgt)
>   {
>   tgt <- sort(tgt) ; neg <- which(tgt < 0)
>   clrneg <- gray(0:length(tgt[neg])/length(tgt[neg]))
>   clrpos <- gray(length(tgt[-neg]):0/length(tgt[-neg]))
>   clr <- c(clrneg,clrpos)
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; grayclr(-50:50) ; grayclr(-20:80)
> if only I could make gray() use blue/brown instead of black (I tried a
> couple of things but got stuck again).
> 
> Any suggestions?

Use colorRampPalette() to roll your own, or something better tuned, 
perhaps catenating two ramps together.

> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From hodgess at gator.dt.uh.edu  Sat Jan  7 21:32:53 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sat, 7 Jan 2006 14:32:53 -0600
Subject: [R]  packages and tex files
Message-ID: <200601072032.k07KWrMn017456@gator.dt.uh.edu>

Dear R People:

I am trying to build a package (yet again!)

I have both PCTex and WinEdt.  I want the *.tex files to use WinEdt.  How
should I set that, please?  Just in the path?

Also, where would I get Rd.sty, please?

Thanks,
R Version 2.2.1 Windows
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From cbilder at earthlink.net  Sat Jan  7 22:12:16 2006
From: cbilder at earthlink.net (Christopher R. Bilder)
Date: Sat, 7 Jan 2006 15:12:16 -0600
Subject: [R] Where is the stud.ci() function used in boot.ci()?
Message-ID: <E1EvLMC-0000Lz-Tc@smtpauth09.mail.atl.earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060107/3a7b99a8/attachment.pl

From ggrothendieck at gmail.com  Sat Jan  7 22:17:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Jan 2006 16:17:28 -0500
Subject: [R] Where is the stud.ci() function used in boot.ci()?
In-Reply-To: <E1EvLMC-0000Lz-Tc@smtpauth09.mail.atl.earthlink.net>
References: <E1EvLMC-0000Lz-Tc@smtpauth09.mail.atl.earthlink.net>
Message-ID: <971536df0601071317r5db34222u2112a4a6686d4e03@mail.gmail.com>

boot:::stud.ci

On 1/7/06, Christopher R. Bilder <cbilder at earthlink.net> wrote:
> Hello!
>
> I am trying to duplicate the studentized bootstrap confidence interval
> calculations in boot.ci() without using boot.ci().  My w/o boot.ci()
> calculations are close to boot.ci() using the same object from
> implementation of boot(), but not quite the same (I think the differences
> are due to how quantiles are computed for z*).  Through examining the
> boot.ci() code, I can see there is a call out to a function called
> stud.ci().  Obviously, examining this function would be very helpful to see
> how the calculations are truly done!  So, I am trying to find the actual
> function code for the stud.ci() function.  Sorry if this is a simple
> question, but does anyone know how this function code can be seen?
> Unfortunately, this function can not be seen in R through simply typing its
> name at the command prompt and there is no help entry for it.  Also, there
> is no obvious file name in boot_1.2-24.zip at
> http://cran.r-project.org/src/contrib/Descriptions/boot.html for the
> specific function.
>
> Thank you in advance!
>
> Chris Bilder
>
>
> University of Nebraska-Lincoln
> Department of Statistics
> Lincoln, NE 68583
> E-mail: cbilder3 at unl.edu or chris at chrisbilder.com
> Website: www.chrisbilder.com
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sat Jan  7 23:15:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 Jan 2006 22:15:04 +0000 (GMT)
Subject: [R] Where is the stud.ci() function used in boot.ci()?
In-Reply-To: <E1EvLMC-0000Lz-Tc@smtpauth09.mail.atl.earthlink.net>
References: <E1EvLMC-0000Lz-Tc@smtpauth09.mail.atl.earthlink.net>
Message-ID: <Pine.LNX.4.61.0601072211590.4271@gannet.stats>

Try one of

getAnywhere("stud.ci")
boot:::stud.ci

However, I think you would do better to get the source package 
(boot_1.2-24.tar.gz) and study the source code.  (The .zip is a Windows 
binary, and that is not supposed to be human-readable.)

On Sat, 7 Jan 2006, Christopher R. Bilder wrote:

> Hello!
>
> I am trying to duplicate the studentized bootstrap confidence interval
> calculations in boot.ci() without using boot.ci().  My w/o boot.ci()
> calculations are close to boot.ci() using the same object from
> implementation of boot(), but not quite the same (I think the differences
> are due to how quantiles are computed for z*).  Through examining the
> boot.ci() code, I can see there is a call out to a function called
> stud.ci().  Obviously, examining this function would be very helpful to see
> how the calculations are truly done!  So, I am trying to find the actual
> function code for the stud.ci() function.  Sorry if this is a simple
> question, but does anyone know how this function code can be seen?
> Unfortunately, this function can not be seen in R through simply typing its
> name at the command prompt and there is no help entry for it.  Also, there
> is no obvious file name in boot_1.2-24.zip at
> http://cran.r-project.org/src/contrib/Descriptions/boot.html for the
> specific function.
>
> Thank you in advance!
>
> Chris Bilder
>
>
> University of Nebraska-Lincoln
> Department of Statistics
> Lincoln, NE 68583
> E-mail: cbilder3 at unl.edu or chris at chrisbilder.com
> Website: www.chrisbilder.com
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vitorchagas at yahoo.com  Sat Jan  7 23:30:26 2006
From: vitorchagas at yahoo.com (Vitor Chagas)
Date: Sat, 7 Jan 2006 14:30:26 -0800 (PST)
Subject: [R] Finding R mailing list archives {was "Wikis etc."}
In-Reply-To: <17343.60211.358181.704410@stat.math.ethz.ch>
Message-ID: <20060107223027.11608.qmail@web30203.mail.mud.yahoo.com>

First of all, Happy New Year to all.

Don't know if the following can be done, its just an
idea:

- A new link in R HTML Help pointing to a searchable
mailing list archive

- the Set CRAN mirror option in R could produce a
small file with the appropriate URL for the link.


Best regards,

Vitor Chagas
actuary
Portugal


--- Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> "John" == John Sorkin
> <jsorkin at grecc.umaryland.edu>
> >>>>>     on Sat, 07 Jan 2006 11:05:01 -0500 writes:
> 
>     John> Jon, Thank you for the terse form of the
> URL. I hope
>     John> the mailing list will automatically
> include it in
>     John> there Email messages.  John
> 
> well, it *already* contains 
> 
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> where the first URL is **the** R-help page, and has
> itself links
> to more than one archive of the mailing list,
> and the 2nd URL (posting guide) also explains many
> things
> (including the mailing list page and archives).
> 
> I really wonder if adding yet another URL to the
> footer of every
> message will be the solution; as others have
> correctly remarked,
> the problem is that for many newbies it is more
> convenient to ask
> rather than to first read something that contains
> more than three
> words. ;-)
> 
> Well, then, maybe for some people, an extra line
> with another link
> ("click click" instead of reading) might be the
> solution...
> 
> Martin Maechler, ETH Zurich, (your mailing list
> maintainer)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


Vitor Chagas
Actuary
Portugal



From spencer.graves at pdf.com  Sun Jan  8 00:49:03 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 07 Jan 2006 15:49:03 -0800
Subject: [R] lmer error message
In-Reply-To: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>
References: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>
Message-ID: <43C0536F.7080108@pdf.com>

	  I am not familiar with this particular error message, but I will 
offer a few suggestions that might help you isolate it.

	  1.  PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  The limited information 
supplied with your question does NOT include data requested of all 
posts.  The command "sessionInfo()" can proved some of the standard 
basics that many potential respondants want to know before they consider 
replying.

	  2.  Have you tried "traceback()"?  This may or may not help you, but 
it's quick and worth a try.

	  3.  Can you provide this list with a very simple, self contained 
example that produces the error message you mention?

	  4.  I suggest you list "lmer" by typing the function name and a 
commmand prompt.  In this case, "lmer" consists solely of a call to 
"standardGeneric".  The documentation for "standardGeneric" led me to 
the documentation for "GenericFunctions, which led me to "showMethods". 
  'showMethods("lmer")' indicated only one method, namely for 'formula = 
"formula"'.  Then 'dumpMethod("lmer", file="lmer.R", 
signature="formula")' produced a listing of that function for me in the 
working directory.  If you have trouble finding the working directory, 
try 'getwd()'.  I would then modify the function it "lmer.R" to create a 
new function "lmer.formula".  Then I would try "debug(lmer.formula)". 
Then I would replace "lmer" by "lmer.formula" in the command that 
generated the error and execute that modified formula.  This will open a 
browser and allow you to walk through the function line by line, 
examining (and changing) anything you want in the environment of that 
function.  Doing this will, I believe, lead you to exactly the line in 
the "lmer" code that generated the error message you don't understand. 
With only a modest amount of luck, you should be able to find in this 
way what you can do to avoid that error.

	  Anecdotal evidence suggests that people who use the techniques 
described in suggestions 1-3 tend to get quicker, more useful replies 
from this list.  Moreover, in virtually every case that I've tried 
suggestion 4, I've been able to figure out how to overcome that 
particular difficulty.  In addition, I've often learned useful things 
about R that I didn't know befor.

	  hope this helps.
	  spencer graves

Abderrahim Oulhaj wrote:

> Dear All,
> 
> I have the following error message when I fitted  lmer to a  binary data with the "AGQ" option:
> 
> Error in family$mu.eta(eta) : NAs are not allowed in subscripted assignments
> In addition: Warning message:
> IRLS iterations for PQL did not converge 
> 
> Any help?
> 
> Thanks in advance,
> 
> Abderrahim
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hodgess at gator.dt.uh.edu  Sun Jan  8 01:36:34 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sat, 7 Jan 2006 18:36:34 -0600
Subject: [R]  sending methods to a new package
Message-ID: <200601080036.k080aYC2017950@gator.dt.uh.edu>

Dear R People:

When creating a package, how do you include new methods and classes, 
please?

I'm using the pacakge.skeleton command as a starting point.

Thanks,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ggrothendieck at gmail.com  Sun Jan  8 01:58:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Jan 2006 19:58:15 -0500
Subject: [R] sending methods to a new package
In-Reply-To: <200601080036.k080aYC2017950@gator.dt.uh.edu>
References: <200601080036.k080aYC2017950@gator.dt.uh.edu>
Message-ID: <971536df0601071658m59f7b35sb4a8c7e8ef80f0c2@mail.gmail.com>

See

  https://www.stat.math.ethz.ch/pipermail/r-help/2006-January/083783.html

where I mentioned both S3 and S4 examples you can use
as examples.  The source to these packages is available on
CRAN.

On 1/7/06, Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> Dear R People:
>
> When creating a package, how do you include new methods and classes,
> please?
>
> I'm using the pacakge.skeleton command as a starting point.
>
> Thanks,
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From edd at debian.org  Sun Jan  8 02:05:23 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 7 Jan 2006 19:05:23 -0600
Subject: [R] sending methods to a new package
In-Reply-To: <200601080036.k080aYC2017950@gator.dt.uh.edu>
References: <200601080036.k080aYC2017950@gator.dt.uh.edu>
Message-ID: <17344.25939.367882.87882@basebud.nulle.part>


Erin,

On 7 January 2006 at 18:36, Erin Hodgess wrote:
| When creating a package, how do you include new methods and classes, 
| please?

As you are undoubtedly aware, there are over 600 packages on CRAN. All of
these have source code that you can study. If you find one too obscure, drop
it and pick another. There _really_ is a lot to choose from. Some have S4
classes, some have S3 classes. You didn't even ask which one you're after.

Moreover, essentially the same question came up with the last week. You could
for example compare its (S4) with zoo (S3).

Lastly, by what we could call the 'Liaw-Baron principle', every question that
can be asked has in fact alreadey been asked -- RSiteSearch() really is your
friend. E.g.  
	> RSiteSearch("S3 class examples")
	> RSiteSearch("S4 class examples")
yield, respectively, 1752 and 245 hits. 

Hope this helps, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From hb at maths.lth.se  Sun Jan  8 02:41:00 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sun, 08 Jan 2006 12:41:00 +1100
Subject: [R] repeat { readline() }
Message-ID: <43C06DAC.3020704@maths.lth.se>

Hi.

Using Rterm v2.2.1 on WinXP, is there a way to interrupt a call like

  repeat { readline() }

without killing the Command window?  Ctrl+C is not interrupting the loop:

R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.2.1 Patched (2006-01-01 r36947)
<snip></snip>

 > repeat { readline() }
^C
^C
^C
^C

On Unix it works.  The problem seems to get the interrupt signal to 
"occur" outside the readline() call so that "repeat" is interrupted. Doing

repeat { readline(); Sys.sleep(3) }

and it is likely that can generate an interrupt signal outside readline().

It seem like readline()/readLines(n=1) or an underlying method catches 
the interrupt signal quietly and just waits for a symbol to come 
through.  Try readline() by itself and press Ctrl+C.  Maybe this is a 
property of the Windows Command terminal, I don't know, but is it a 
wanted feature and are R core aware of it?  Note that, in Rgui it is 
possible to interrupting such a loop by pressing ESC.

Cheers

Henrik



From sourceforge at metrak.com  Sun Jan  8 03:52:33 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Sun, 08 Jan 2006 13:52:33 +1100
Subject: [R] Wikis etc.
In-Reply-To: <43BFC4EE.209@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
	<43BFC4EE.209@vanderbilt.edu>
Message-ID: <43C07E71.3040303@metrak.com>

Frank E Harrell Jr wrote:
> paul sorenson wrote:
> 
>> I am a fan of wiki's and I reckon it would really help with making R 
>> more accessible.  On one extreme you have this email list and on the 
>> other extreme you have RNews and the PDF's on CRAN.  A wiki might hit 
>> the spot between them and reduce the traffic on the email list.
> 
> 
> Thanks Paul.  But as long as the email list is active I fear a wiki 
> won't be.

That would be sad if that were true.  They are different beasts, as 
would be an IRC channel.  I say complementary, not mutually exclusive.

A wiki takes time to reach critical mass (eg my home brew wiki 
http://brewiki.org/ or wikipedia) and you couldn't just pull the plug on 
this list without a serious impact on the uptake of R I would have thought.

Contributions to the wiki from mugs like me with less R/statistics 
experience would hopefully make R more accessible to newbies - pointing 
out the traps for new players.

One way to bootstrap it is to simply add a "wiki" menu entry into the 
r-project.org menu. This is what the guys over at 
http://wiki.wxpython.org/ have done.  Over time, some of the other items 
there might morph in to wiki pages as appropriate.

I have no doubt that if the R-Wiki was supported in the same thoughtful, 
thorough and patient way in which questions on R-Help are answered, it 
would be one of the lowest entropy wiki's around.

I have some experience with moinmoin (a python wiki) and would be 
willing to contribute some time and skills if that would help.



From murdoch at stats.uwo.ca  Sun Jan  8 04:06:22 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 07 Jan 2006 22:06:22 -0500
Subject: [R] Question about graphics in R
In-Reply-To: <A49FA2C7-B144-4E5C-AA8F-751727512A01@eecs.oregonstate.edu>
References: <A49FA2C7-B144-4E5C-AA8F-751727512A01@eecs.oregonstate.edu>
Message-ID: <43C081AE.9040207@stats.uwo.ca>

On 1/7/2006 12:47 PM, Martin Erwig wrote:
> Considering the R function/plot shown below, I wonder whether
> it is possible to do the following changes:
> 
> (1) Change the color of each point to be picked from
> list of colors according to its z-value. (The range
> should be from blue (z=0) to red (z=1).) The grid
> should then be omitted. [I have seen "terrain.colors", but
> don't know how to use it for this purpose.]
> 
> (2) Add two lines to the surface for, say z=0.8 and
> z=0.3. [Can contour or contourLines be used?]
> 
> 
> ---
> 
> x <- seq(0, 1, length = 50)
> y <- x
> f <- function(x,y) { sin ((1-x)*y) }
> z <- outer(x,y,f)
> 
> persp(x, y, z,
> 	theta = 30, phi = 30,
> 	shade = 0.3, col = "red"
> 	)

It's not that hard, but there are a couple of tricks to it.  The 
colorRamp() and colorRampPalette() functions give you the way to go from 
z values to colors.  persp() wants output in the form colorRampPalette 
gives, so use that.

But the col option to persp() wants one color per facet, not one per z 
value, so you need to be a little careful calculating them.  However, 
this should give what you want:

colfn <- colorRampPalette(c("blue","white","red"))

# You don't need white in the middle, but I think it looks better

cols <- colfn(256)[1 + 255*z[1:49, 1:49]]

trans <- persp(x, y, z, theta=30, phi=30,  col = cols)

To add the lines, use contourLines() to calculate them, then trans3d() 
(from the ?persp examples) to convert them to coordinates that can be 
plotted.  E.g.

clines <- contourLines(x,y,z, levels=c(0.8, 0.3))

trans3d <- function(x,y,z, pmat) {
        tr <- cbind(x,y,z,1) %*% pmat
        list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
      }

for (i in seq(along=clines)) lines(trans3d(clines[[i]]$x, clines[[i]]$y,
  clines[[i]]$level, trans))



> ---
> 
> 
> Finally, I would also produce a flattened 2D map
> of the same function, i.e. a map in which each point
> (x,y) is mapped to a color in a range according to
> f(x,y). Also two lines for f(x,y)=c1 and f(x,y)=c2
> should be added.

Use image() for that, e.g.

image(x,y,z)

and then contour():

contour(x,y,z,levels=c(0.8,0.3),add=T)

Duncan Murdoch
> 
> Is this possible?
> 
> I would be grateful for any hints.
> 
> 
> Thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gynmeerut at indiatimes.com  Sun Jan  8 07:22:33 2006
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Sun, 08 Jan 2006 11:52:33 +0530
Subject: [R] Dates
Message-ID: <200601080610.LAA02346@WS0005.indiatimes.com>

Dear All,

I have a dataset containing Date column. I need to breakdown this dataset into two parts one which has datapoints before a particular date and the otherone on or after a particular date.

for example:
"2005-12-15",
"2008-12-14",
"2012-10-01",
"2009-11-15",
"2018-10-10",
"2014-08-31",
"2016-03-27",
"2010-01-01".
  say I want dates before  and after "2009-12-31"
that means I want ("2005-12-15","2008-12-14","2009-11-15")->x1
and ("2012-10-01","2018-10-10","2014-08-31","2016-03-27","2010-01-01")->x2


I wish to use different programs for these subsets.  
I could not find any way to compare dates with respect to some date.
Moreover dates are random I vary in no. So above is just an example.

  Any help will be appreciated.

regards,

GS



From ronggui.huang at gmail.com  Sun Jan  8 07:36:19 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Sun, 8 Jan 2006 14:36:19 +0800
Subject: [R] Dates
In-Reply-To: <200601080610.LAA02346@WS0005.indiatimes.com>
References: <200601080610.LAA02346@WS0005.indiatimes.com>
Message-ID: <38b9f0350601072236r560b2081l@mail.gmail.com>

this is one way to do it.

> x<-as.Date(c("2005/2/10","2002/2/1"))
> x
[1] "2005-02-10" "2002-02-01"
> ind<-x>as.Date("2005/1/1")
> x[ind]
[1] "2005-02-10"
> x[!ind]
[1] "2002-02-01"


2006/1/8, gynmeerut <gynmeerut at indiatimes.com>:
> Dear All,
>
> I have a dataset containing Date column. I need to breakdown this dataset into two parts one which has datapoints before a particular date and the otherone on or after a particular date.
>
> for example:
> "2005-12-15",
> "2008-12-14",
> "2012-10-01",
> "2009-11-15",
> "2018-10-10",
> "2014-08-31",
> "2016-03-27",
> "2010-01-01".
>   say I want dates before  and after "2009-12-31"
> that means I want ("2005-12-15","2008-12-14","2009-11-15")->x1
> and ("2012-10-01","2018-10-10","2014-08-31","2016-03-27","2010-01-01")->x2
>
>
> I wish to use different programs for these subsets.
> I could not find any way to compare dates with respect to some date.
> Moreover dates are random I vary in no. So above is just an example.
>
>   Any help will be appreciated.
>
> regards,
>
> GS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From ka4alin at yandex.ru  Sun Jan  8 08:19:53 2006
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Sun, 08 Jan 2006 10:19:53 +0300
Subject: [R] Mass 'identify' on 2d-plot
In-Reply-To: <43940D8C.3020309@lancaster.ac.uk>
References: <439409E8.8070206@yandex.ru> <43940D8C.3020309@lancaster.ac.uk>
Message-ID: <43C0BD19.6090703@yandex.ru>

Barry Rowlingson :
> Evgeniy Kachalin wrote:
> 
>> What is ability in R to graphically (per mouse) define some area and 
>> to select all the cases felt in it?
>>
>> 'identify' is OK for 5-10 cases, but what if cases=1000?
> 
> 
>  You can use 'locator' to let the user click a number of points to 
> define a polygon, and then use one of the point-in-polygon functions 
> provided by one of the spatial packages to work out whats in your polygon.
> 
>  Look at splancs, spatstat, sp - pretty much anything beginning with 
> 'sp' - on CRAN.
> 
>  In splancs you can just do:
> 
>  poly = getpoly()
> 
>  - which lets the user draw a polygon on screen, then:
> 
>  inPoly = inpip(xypts,poly)
>  points(xypts[inpip,], pch=19,col="red")
> 
>  and that will plot the selected points in solid red dots.
> 
>  I don't think there's a way to draw a freehand figure on an R plot, you 
> have to go click, click, click, and draw straight lines.
> 

I don't get what is 'xypts' in this case... One step earlier i've 
plotted plot(y~x,data=dat). What is xypts?

-- 
Evgeniy Kachalin



From hodgess at gator.dt.uh.edu  Sun Jan  8 08:30:17 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sun, 8 Jan 2006 01:30:17 -0600
Subject: [R]  exporting methods/classes
Message-ID: <200601080730.k087UGXR009345@gator.dt.uh.edu>

Dear R People:

I'm still struggling with sending methods and classes as part of 
creating a new package.

Where does the .onLoad function go?  Within R itself or in a file
in one of the new package directories?


Here are my latest efforts:


Here's the last part of the woof1-Ex.Rout

> library('woof1')
Error in loadNamespace(package, c(which.lib.loc, lib.loc), 
keep.source = keep.source) : 
	in 'woof1' classes for export not defined: dog
Error: package/namespace load failed for 'woof1'
Execution halted



Here's the NAMESPACE
importFrom(graphics,plot)
exportClasses("dog")
exportMethods("plot","show")


thanks yet again,
Sincerely,
Erin 
mailto: hodgess at gator.uhd.edu



From ripley at stats.ox.ac.uk  Sun Jan  8 09:31:56 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Jan 2006 08:31:56 +0000 (GMT)
Subject: [R] repeat { readline() }
In-Reply-To: <43C06DAC.3020704@maths.lth.se>
References: <43C06DAC.3020704@maths.lth.se>
Message-ID: <Pine.LNX.4.61.0601080809150.20500@gannet.stats>

Ctrl-Break works: see the rw-FAQ and README.rterm.  (You'll need a return 
to see a new prompt.)

It is related to your reading directly from the console, so Ctrl-C is 
getting sent to the wrong place, I believe.  (There's a comment from Guido 
somewhere in the sources about this, and this seems corroborated by the 
fact that Ctrl-C will interrupt under Rterm --ess.)

On Sun, 8 Jan 2006, Henrik Bengtsson wrote:

> Hi.
>
> Using Rterm v2.2.1 on WinXP, is there a way to interrupt a call like
>
>  repeat { readline() }
>
> without killing the Command window?  Ctrl+C is not interrupting the loop:
>
> R : Copyright 2006, The R Foundation for Statistical Computing
> Version 2.2.1 Patched (2006-01-01 r36947)
> <snip></snip>
>
> > repeat { readline() }
> ^C
> ^C
> ^C
> ^C
>
> On Unix it works.  The problem seems to get the interrupt signal to
> "occur" outside the readline() call so that "repeat" is interrupted. Doing
>
> repeat { readline(); Sys.sleep(3) }
>
> and it is likely that can generate an interrupt signal outside readline().
>
> It seem like readline()/readLines(n=1) or an underlying method catches
> the interrupt signal quietly and just waits for a symbol to come
> through.  Try readline() by itself and press Ctrl+C.  Maybe this is a
> property of the Windows Command terminal, I don't know, but is it a
> wanted feature and are R core aware of it?  Note that, in Rgui it is
> possible to interrupting such a loop by pressing ESC.
>
> Cheers
>
> Henrik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From h.wickham at gmail.com  Sun Jan  8 09:37:09 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 8 Jan 2006 08:37:09 +0000
Subject: [R] repeat { readline() }
In-Reply-To: <43C06DAC.3020704@maths.lth.se>
References: <43C06DAC.3020704@maths.lth.se>
Message-ID: <f8e6ff050601080037v40eb6c77p4467c0fe47336e31@mail.gmail.com>

On a related note, does anyone know how to exit:

repeat { try( readline() ) }

The try block captures Ctrl-C.

Hadley



From detlef.steuer at hsu-hamburg.de  Sun Jan  8 12:15:43 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Sun, 8 Jan 2006 12:15:43 +0100
Subject: [R] Wikis etc.
In-Reply-To: <20060107185124.GB518@psych.upenn.edu>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
	<43BFDB1E.5030804@stats.uwo.ca>
	<17344.1929.451916.557388@basebud.nulle.part>
	<20060107185124.GB518@psych.upenn.edu>
Message-ID: <20060108121543.1798e51a.detlef.steuer@hsu-hamburg.de>

On Sat, 7 Jan 2006 13:51:24 -0500
Jonathan Baron <baron at psych.upenn.edu> wrote:

> On 01/07/06 12:25, Dirk Eddelbuettel wrote:
> > So my $0.02 would be to a) go for it, if possible but b) make it
> > visible, and closely tied to R Core / CRAN / R News / ....  Which
> > poses the chicken/egg problem of people running out of spare time to
> > setup, admin, monitor, hand-hold the wiki, its database, watch out
> > for spammers, etc.  Volunteers to do this, and maybe help Jon Baron
> > on a first run using TWiki?
> 
> Great.  I agree.  But there are now at least 3 proposals floating
> around.  Mine was to base the wiki on (I think) the individual
> help pages, each file in the help or html subdirectory of each
> package would be a wiki page.  Since there are about 17000 of
> these at last count - oh, my, could that really be true? - this
> must be automated.  It should be fairly easy, but I just haven't
> done it yet.  What this would amount to is an extended help
> system.
> 
> Then there is the decision about what happens with updates.  I'm
> beginning to think that there is just no other way to handle this
> than to INCLUDE the latest version of each help file in the wicki
> page, have the comments come AFTER it, and just don't worry about
> it if the comments become out of date.  Eventually, someone will
> edit them if it is important.

Here I strongly disagree. Nothing is worse than searching for help and
stumble upon wrong hintsight. It is better to find nothing, than to
_think_ to have found an answer only to find it not working.

I was just bitten by outdated docu after updating qemu. Did cost some
time and frustration.

Think of the beginner left with useless, or worse, wrong advise.

Detlef


> 
> But I'm not at all sure this is the best way to go.  Given that
> all the functions in each package are related, it might be better 
> to have one wiki page per package.  In this case, there is less
> difference between my proposal and the other existing wikis.
> 
> It is possible that too many wikis is bad.  On the other hand, it 
> is possible that too few is bad.

May be true.

Have a nice sunday,
Detlef

> 
> > Hell, there are no rules here - we're trying to accomplish
> > something.
> >                                                   -- Thomas A.
> >                                                   Edison
> 
> I love this.
> 
> Jon
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


-- 
"Keinen Gedanken zweimal denken, au??er er ist sch??n." Unbekannte Quelle



From detlef.steuer at hsu-hamburg.de  Sun Jan  8 12:21:52 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Sun, 8 Jan 2006 12:21:52 +0100
Subject: [R] [R-pkgs] sudoku
In-Reply-To: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>
Message-ID: <20060108122152.7bcc1f6f.detlef.steuer@hsu-hamburg.de>

Hey, you spoiled my course!

:-)

I planned using this as an excersise.
Alternative ideas anyone ...

Detlef

On Fri, 6 Jan 2006 11:43:44 -0500
"Brahm, David" <David.Brahm at geodecapital.com> wrote:

> Any doubts about R's big-league status should be put to rest, now that
> we have a
> Sudoku Puzzle Solver.  Take that, SAS!  See package "sudoku" on CRAN.
> 
> The package could really use a puzzle generator -- contributors are
> welcome!
> 
> -- David Brahm (brahm at alum.mit.edu) 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-packages mailing list
> R-packages at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-packages
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


-- 
"Keinen Gedanken zweimal denken, au??er er ist sch??n." Unbekannte Quelle



From ripley at stats.ox.ac.uk  Sun Jan  8 13:05:25 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Jan 2006 12:05:25 +0000 (GMT)
Subject: [R] repeat { readline() }
In-Reply-To: <Pine.LNX.4.61.0601080809150.20500@gannet.stats>
References: <43C06DAC.3020704@maths.lth.se>
	<Pine.LNX.4.61.0601080809150.20500@gannet.stats>
Message-ID: <Pine.LNX.4.61.0601081202460.24213@gannet.stats>

On Sun, 8 Jan 2006, Prof Brian Ripley wrote:

> Ctrl-Break works: see the rw-FAQ and README.rterm.  (You'll need a return
> to see a new prompt.)
>
> It is related to your reading directly from the console, so Ctrl-C is
> getting sent to the wrong place, I believe.  (There's a comment from Guido
> somewhere in the sources about this, and this seems corroborated by the
> fact that Ctrl-C will interrupt under Rterm --ess.)

Although the comment is there (in psignal.c), on closer examination the 
cause is a change Guido made to getline.c, so Ctrl-C is treated as a 
character during keyboard input.  I doubt if that was intentional (0 is 
not the default state) and I have changed it for R-devel.\

>
> On Sun, 8 Jan 2006, Henrik Bengtsson wrote:
>
>> Hi.
>>
>> Using Rterm v2.2.1 on WinXP, is there a way to interrupt a call like
>>
>>  repeat { readline() }
>>
>> without killing the Command window?  Ctrl+C is not interrupting the loop:
>>
>> R : Copyright 2006, The R Foundation for Statistical Computing
>> Version 2.2.1 Patched (2006-01-01 r36947)
>> <snip></snip>
>>
>>> repeat { readline() }
>> ^C
>> ^C
>> ^C
>> ^C
>>
>> On Unix it works.  The problem seems to get the interrupt signal to
>> "occur" outside the readline() call so that "repeat" is interrupted. Doing
>>
>> repeat { readline(); Sys.sleep(3) }
>>
>> and it is likely that can generate an interrupt signal outside readline().
>>
>> It seem like readline()/readLines(n=1) or an underlying method catches
>> the interrupt signal quietly and just waits for a symbol to come
>> through.  Try readline() by itself and press Ctrl+C.  Maybe this is a
>> property of the Windows Command terminal, I don't know, but is it a
>> wanted feature and are R core aware of it?  Note that, in Rgui it is
>> possible to interrupting such a loop by pressing ESC.
>>
>> Cheers
>>
>> Henrik
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kevin.gamble at extension.org  Sun Jan  8 13:25:48 2006
From: kevin.gamble at extension.org (Kevin Gamble)
Date: Sun, 8 Jan 2006 07:25:48 -0500
Subject: [R] Wikis etc.
In-Reply-To: <20060108121543.1798e51a.detlef.steuer@hsu-hamburg.de>
References: <43BDC208.5060909@vanderbilt.edu> <43BF1341.4020800@metrak.com>
	<43BFDB1E.5030804@stats.uwo.ca>
	<17344.1929.451916.557388@basebud.nulle.part>
	<20060107185124.GB518@psych.upenn.edu>
	<20060108121543.1798e51a.detlef.steuer@hsu-hamburg.de>
Message-ID: <AED1334F-CB25-4A28-9C20-2B8516E54044@extension.org>

Kris over at Wiki That! has a post today about what makes for a good  
wiki: http://www.wikithat.com/wiki_that/2006/01/wiki_of_the_wee_1.html

> Most wikis I?ve looked at are in danger of facing the same fate as  
> most websites and CMS - death by boredom. They are focused on  
> content that is used as reference for some off-line activity OR as  
> the end-result of an off-line activity.
>
> Collaboration and participation is more than just sharing  
> information and making it accessible. It?s all about the PROCESS of  
> planning and executing events, projects, tasks, and deliverables.  
> Content is boring - action is engaging. Make your wiki activity- 
> centric, not just data-centric (content).

If the wiki is a place where people just go to look things up --  
we're toast. It needs to be made into an active learning environment,  
a place where experts and beginners alike come to interact. Some to  
share their knowledge, and others to learn from the masters. It's the  
same dynamic  that makes the mailing list so active, and yet also  
what exposes its weakness (i.e. it doesn't scale well).

Kevin



Kevin J. Gamble. Ph.D.
Associate Director eXtension Initiative
Box 7647 NCSU
Raleigh, NC 27694-7641
v: 919.515.8447
c: 919.605.5815
AIM: k1v1n
Web: intranet.extension.org



From amsa36060 at yahoo.com  Sun Jan  8 13:46:47 2006
From: amsa36060 at yahoo.com (Amir Safari)
Date: Sun, 8 Jan 2006 04:46:47 -0800 (PST)
Subject: [R] Filters in waveslim
Message-ID: <20060108124647.94058.qmail@web60415.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060108/9fa9a59f/attachment.pl

From ligges at statistik.uni-dortmund.de  Sun Jan  8 14:52:33 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 Jan 2006 14:52:33 +0100
Subject: [R] packages and tex files
In-Reply-To: <200601072032.k07KWrMn017456@gator.dt.uh.edu>
References: <200601072032.k07KWrMn017456@gator.dt.uh.edu>
Message-ID: <43C11921.5060001@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> I am trying to build a package (yet again!)
> 
> I have both PCTex and WinEdt.  I want the *.tex files to use WinEdt. 

Erin,

please check the R Installation and Administration manual on how to set 
up a working environment to build and install packages.
I don't know PCTex, it is not mentioned in the manual cited above, hence 
it might not work without some hiccups.

You do not need to care about .tex files, they are produced and 
afterwards compiled into the appropriate format automatically from .Rd 
files by the R tools. I have looked into the produced tex files only in 
very rare circumstances where some debugging of Rd files was required.

Or do you want to edit the Rd files in WinEdt? Then, I recommend to set 
up WinEdt appropriately to handle them in the same manner as LaTeX files.

> How should I set that, please?  Just in the path?

You LaTeX binaries must be in the path (but probably they already are ...).

> Also, where would I get Rd.sty, please?

The supported LaTeX distributions are told to look into the right place 
by the R CMD tools. And this is: path-to-R/share/texmf/Rd.sty

Uwe Ligges


> Thanks,
> R Version 2.2.1 Windows
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Jan  8 15:09:06 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 Jan 2006 15:09:06 +0100
Subject: [R] exporting methods/classes
In-Reply-To: <200601080730.k087UGXR009345@gator.dt.uh.edu>
References: <200601080730.k087UGXR009345@gator.dt.uh.edu>
Message-ID: <43C11D02.8010400@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> I'm still struggling with sending methods and classes as part of 
> creating a new package.
> 
> Where does the .onLoad function go?  Within R itself or in a file
> in one of the new package directories?

Simply save the .onLoad function in some .R file (e.g. zzz.R) in the 
package's ./R directory.



> Here are my latest efforts:
> 
> 
> Here's the last part of the woof1-Ex.Rout
> 
> 
>>library('woof1')
> 
> Error in loadNamespace(package, c(which.lib.loc, lib.loc), 
> keep.source = keep.source) : 
> 	in 'woof1' classes for export not defined: dog
> Error: package/namespace load failed for 'woof1'
> Execution halted

So this looks like you have defined S4 classes for export in your 
Namespace but you have no call that starts with

setClass("dog", ......

in the R code in your package, hence the class "dog" is undefined.


> Here's the NAMESPACE
> importFrom(graphics,plot)
> exportClasses("dog")
> exportMethods("plot","show")


You don't export any other functions?


Uwe Ligges




> 
> thanks yet again,
> Sincerely,
> Erin 
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Sun Jan  8 17:00:44 2006
From: phgrosjean at sciviews.org (phgrosjean@sciviews.org)
Date: Sun, 8 Jan 2006 17:00:44 +0100 (CET)
Subject: [R] Wikis etc.
In-Reply-To: <43BDC208.5060909@vanderbilt.edu>
References: <43BDC208.5060909@vanderbilt.edu>
Message-ID: <20060108160043.E66F345E@wmailp01.st2.lyceu.net>

Hello all,

Sorry for not taking part of this discussion earlier, and for not
answering Detlef Steuer, Martin Maechler, and others that asked more
direct questions to me. I am away from my office and my computer until the
16th of January.

Just quick and partial answers:
1) I did not know about Hamburg RWiki. But I would be happy to merge both
in one or the other way, as Detlef suggests it.

2) I choose DokuWiki as the best engine after a careful comparison of
various Wiki engines. It is the best one, as far as I know, for the
purpose of writting software documentation and similar pages. There is an
extensive and clearly presented comparison of many Wikki engines at:
http://www.wikimatrix.org/.

3) I started to change DokuWiki (addition of various plugins, addition of
R code syntax coloring with GESHI, etc...). So, it goes well beyond all
current Wiki engines regarding its suitability to present R stuff.

4) The reasons I did this is because I think the Wiki format could be of a
wider use. I plan to change a little bit the DokuWiki syntax, so that it
works with plain .R code files (Wiki part is simply embedded in commented
lines, and the rest is recognized and formatted as R code by the Wiki
engine). That way, the same Wiki document can either rendered by the Wiki
engine for a nice presentation, or sourced in R indifferently.

5) My last idea is to add a Rpad engine to the Wiki, so that one could
play with R code presented in the Wiki pages and see the effects of
changes directly in the Wiki.

6) Regarding the content of the Wiki, it should be nice to propose to the
authors of various existing document to put them in a Wiki form. Something
like "Statistics with R" (http://zoonek2.free.fr/UNIX/48_R/all.html) is
written in a way that stimulates additions to pages in perpetual
construction, if it was presented in a Wiki form. It is licensed as
Creative Commons Attribution-NonCommercial-ShareAlike 2.5 license, that
is, exactly the same one as DokuWiki that I choose for R Wiki. Of course,
I plan to ask its author to do so before putting its hundreds of very
interesting pages on the Wiki... I think it is vital to have already
something in the Wiki, in order to attract enough readers, and then enough
contributors!

7) Regarding spamming and vandalism, DokuWiki allows to manage rights and
users, even individually for pages. I think it would be fine to lock pages
that reach a certain maturity (read-only / editable by selected users
only) , with link to a discussion page which remaining freely accessible
at the bottom of locked pages.

8) I would be happy to contribute this work to the R foundation in one way
or the other to integrate it in http://www.r-project.org or
http://cran.r-project.org. But if it is fine keeping it in
http://www.sciviews.org as well, it is also fine for me.

I suggest that all interested people drop a little email to my mailbox.
I'll recontact you when I will be back to my office to work on a more
elaborate solution altogether when I am back at my office.
Best,

Philippe Grosjean



From dmbates at gmail.com  Sun Jan  8 18:49:26 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 8 Jan 2006 11:49:26 -0600
Subject: [R] lmer error message
In-Reply-To: <43C0536F.7080108@pdf.com>
References: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>
	<43C0536F.7080108@pdf.com>
Message-ID: <40e66e0b0601080949y6d26f97vf485d2dbce8b1ae7@mail.gmail.com>

Also, please try setting

options(verbose = TRUE)

immediately before your call to lmer.  This will provide verbose
output on the progress of the iterations and will probably give an
indication of where the problem lies.


On 1/7/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>           I am not familiar with this particular error message, but I will
> offer a few suggestions that might help you isolate it.
>
>           1.  PLEASE do read the posting guide!
> "www.R-project.org/posting-guide.html".  The limited information
> supplied with your question does NOT include data requested of all
> posts.  The command "sessionInfo()" can proved some of the standard
> basics that many potential respondants want to know before they consider
> replying.
>
>           2.  Have you tried "traceback()"?  This may or may not help you, but
> it's quick and worth a try.
>
>           3.  Can you provide this list with a very simple, self contained
> example that produces the error message you mention?
>
>           4.  I suggest you list "lmer" by typing the function name and a
> commmand prompt.  In this case, "lmer" consists solely of a call to
> "standardGeneric".  The documentation for "standardGeneric" led me to
> the documentation for "GenericFunctions, which led me to "showMethods".
>   'showMethods("lmer")' indicated only one method, namely for 'formula =
> "formula"'.  Then 'dumpMethod("lmer", file="lmer.R",
> signature="formula")' produced a listing of that function for me in the
> working directory.  If you have trouble finding the working directory,
> try 'getwd()'.  I would then modify the function it "lmer.R" to create a
> new function "lmer.formula".  Then I would try "debug(lmer.formula)".
> Then I would replace "lmer" by "lmer.formula" in the command that
> generated the error and execute that modified formula.  This will open a
> browser and allow you to walk through the function line by line,
> examining (and changing) anything you want in the environment of that
> function.  Doing this will, I believe, lead you to exactly the line in
> the "lmer" code that generated the error message you don't understand.
> With only a modest amount of luck, you should be able to find in this
> way what you can do to avoid that error.
>
>           Anecdotal evidence suggests that people who use the techniques
> described in suggestions 1-3 tend to get quicker, more useful replies
> from this list.  Moreover, in virtually every case that I've tried
> suggestion 4, I've been able to figure out how to overcome that
> particular difficulty.  In addition, I've often learned useful things
> about R that I didn't know befor.
>
>           hope this helps.
>           spencer graves
>
> Abderrahim Oulhaj wrote:
>
> > Dear All,
> >
> > I have the following error message when I fitted  lmer to a  binary data with the "AGQ" option:
> >
> > Error in family$mu.eta(eta) : NAs are not allowed in subscripted assignments
> > In addition: Warning message:
> > IRLS iterations for PQL did not converge
> >
> > Any help?
> >
> > Thanks in advance,
> >
> > Abderrahim
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pinard at iro.umontreal.ca  Sun Jan  8 19:06:20 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 8 Jan 2006 13:06:20 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <Pine.LNX.4.61.0601060802190.18569@gannet.stats>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<Pine.LNX.4.61.0601060802190.18569@gannet.stats>
Message-ID: <20060108180620.GB10698@phenix.sram.qc.ca>

[Brian Ripley]
>[Fran??ois Pinard]
>>[Brian Ripley]

>>>One problem [...] is that R's I/O is not line-oriented but
>>>stream-oriented.  So selecting lines is not particularly easy in R.

>>I understand that you mean random access to lines, instead of random
>>selection of lines.

>That was not my point. [...] Skipping lines you do not need will take 
>longer than you might guess (based on some limited experience).

Thanks for telling (and also for the expression "reservoir sampling").
OK, then.  All summarized, if I ever need this for bigger datasets, 
selection might better be done outside of R.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From spencer.graves at pdf.com  Sun Jan  8 19:23:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jan 2006 10:23:21 -0800
Subject: [R] lmer error message
In-Reply-To: <40e66e0b0601080949y6d26f97vf485d2dbce8b1ae7@mail.gmail.com>
References: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>	
	<43C0536F.7080108@pdf.com>
	<40e66e0b0601080949y6d26f97vf485d2dbce8b1ae7@mail.gmail.com>
Message-ID: <43C15899.7070600@pdf.com>

Hi, Doug:

	  Thanks.  My copy of the 'lmer' documentation does not list the 
'verbose' argument.  Is this something you plan to discontinue or 
modify, or was it recently added to the script but not to the 
documentation I have?

	  Also, I just tried it modifying one of the examples in the 
documentation:

 > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,
+              verbose=TRUE)
trace: lmer(Reaction ~ Days + (Days | Subject), sleepstudy, verbose = TRUE)

	  Maybe this example converges so quickly it senses no need for greater 
verbosity.

	  Thanks,
	  Spencer Graves
#################

R version 2.2.0, 2005-10-06, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-4"
 >

Douglas Bates wrote:

> Also, please try setting
> 
> options(verbose = TRUE)
> 
> immediately before your call to lmer.  This will provide verbose
> output on the progress of the iterations and will probably give an
> indication of where the problem lies.
> 
> 
> On 1/7/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>          I am not familiar with this particular error message, but I will
>>offer a few suggestions that might help you isolate it.
>>
>>          1.  PLEASE do read the posting guide!
>>"www.R-project.org/posting-guide.html".  The limited information
>>supplied with your question does NOT include data requested of all
>>posts.  The command "sessionInfo()" can proved some of the standard
>>basics that many potential respondants want to know before they consider
>>replying.
>>
>>          2.  Have you tried "traceback()"?  This may or may not help you, but
>>it's quick and worth a try.
>>
>>          3.  Can you provide this list with a very simple, self contained
>>example that produces the error message you mention?
>>
>>          4.  I suggest you list "lmer" by typing the function name and a
>>commmand prompt.  In this case, "lmer" consists solely of a call to
>>"standardGeneric".  The documentation for "standardGeneric" led me to
>>the documentation for "GenericFunctions, which led me to "showMethods".
>>  'showMethods("lmer")' indicated only one method, namely for 'formula =
>>"formula"'.  Then 'dumpMethod("lmer", file="lmer.R",
>>signature="formula")' produced a listing of that function for me in the
>>working directory.  If you have trouble finding the working directory,
>>try 'getwd()'.  I would then modify the function it "lmer.R" to create a
>>new function "lmer.formula".  Then I would try "debug(lmer.formula)".
>>Then I would replace "lmer" by "lmer.formula" in the command that
>>generated the error and execute that modified formula.  This will open a
>>browser and allow you to walk through the function line by line,
>>examining (and changing) anything you want in the environment of that
>>function.  Doing this will, I believe, lead you to exactly the line in
>>the "lmer" code that generated the error message you don't understand.
>>With only a modest amount of luck, you should be able to find in this
>>way what you can do to avoid that error.
>>
>>          Anecdotal evidence suggests that people who use the techniques
>>described in suggestions 1-3 tend to get quicker, more useful replies
>>from this list.  Moreover, in virtually every case that I've tried
>>suggestion 4, I've been able to figure out how to overcome that
>>particular difficulty.  In addition, I've often learned useful things
>>about R that I didn't know befor.
>>
>>          hope this helps.
>>          spencer graves
>>
>>Abderrahim Oulhaj wrote:
>>
>>
>>>Dear All,
>>>
>>>I have the following error message when I fitted  lmer to a  binary data with the "AGQ" option:
>>>
>>>Error in family$mu.eta(eta) : NAs are not allowed in subscripted assignments
>>>In addition: Warning message:
>>>IRLS iterations for PQL did not converge
>>>
>>>Any help?
>>>
>>>Thanks in advance,
>>>
>>>Abderrahim
>>>
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sun Jan  8 20:10:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Jan 2006 19:10:11 +0000 (GMT)
Subject: [R] lmer error message
In-Reply-To: <43C15899.7070600@pdf.com>
References: <01c401c61053$1a3fda00$adca01a3@optima.ox.ac.uk>
	<43C0536F.7080108@pdf.com>
	<40e66e0b0601080949y6d26f97vf485d2dbce8b1ae7@mail.gmail.com>
	<43C15899.7070600@pdf.com>
Message-ID: <Pine.LNX.4.61.0601081905170.32022@gannet.stats>

Spencer:

It is an option, not an argument, and sets the default for the lmerControl 
arguments msVerbose and EMverbose (see ?lmer)

> options(verbose=TRUE)
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
   EM iterations
   0 1768.412 ( 3.75000  106.875  0.00000:    3.21    0.174    0.663)
   1 1749.892 ( 2.67539  62.5305 -5.17591:    2.36    0.176    0.479)
...
   0      1743.63: 0.918936 0.0531527 -0.304877
...

On Sun, 8 Jan 2006, Spencer Graves wrote:

> Hi, Doug:
>
> 	  Thanks.  My copy of the 'lmer' documentation does not list the
> 'verbose' argument.  Is this something you plan to discontinue or
> modify, or was it recently added to the script but not to the
> documentation I have?
>
> 	  Also, I just tried it modifying one of the examples in the
> documentation:
>
> > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,
> +              verbose=TRUE)
> trace: lmer(Reaction ~ Days + (Days | Subject), sleepstudy, verbose = TRUE)
>
> 	  Maybe this example converges so quickly it senses no need for greater
> verbosity.
>
> 	  Thanks,
> 	  Spencer Graves
> #################
>
> R version 2.2.0, 2005-10-06, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>      lme4   lattice    Matrix
>  "0.98-1" "0.12-11"  "0.99-4"
> >
>
> Douglas Bates wrote:
>
>> Also, please try setting
>>
>> options(verbose = TRUE)
>>
>> immediately before your call to lmer.  This will provide verbose
>> output on the progress of the iterations and will probably give an
>> indication of where the problem lies.
>>
>>
>> On 1/7/06, Spencer Graves <spencer.graves at pdf.com> wrote:
>>
>>>          I am not familiar with this particular error message, but I will
>>> offer a few suggestions that might help you isolate it.
>>>
>>>          1.  PLEASE do read the posting guide!
>>> "www.R-project.org/posting-guide.html".  The limited information
>>> supplied with your question does NOT include data requested of all
>>> posts.  The command "sessionInfo()" can proved some of the standard
>>> basics that many potential respondants want to know before they consider
>>> replying.
>>>
>>>          2.  Have you tried "traceback()"?  This may or may not help you, but
>>> it's quick and worth a try.
>>>
>>>          3.  Can you provide this list with a very simple, self contained
>>> example that produces the error message you mention?
>>>
>>>          4.  I suggest you list "lmer" by typing the function name and a
>>> commmand prompt.  In this case, "lmer" consists solely of a call to
>>> "standardGeneric".  The documentation for "standardGeneric" led me to
>>> the documentation for "GenericFunctions, which led me to "showMethods".
>>>  'showMethods("lmer")' indicated only one method, namely for 'formula =
>>> "formula"'.  Then 'dumpMethod("lmer", file="lmer.R",
>>> signature="formula")' produced a listing of that function for me in the
>>> working directory.  If you have trouble finding the working directory,
>>> try 'getwd()'.  I would then modify the function it "lmer.R" to create a
>>> new function "lmer.formula".  Then I would try "debug(lmer.formula)".
>>> Then I would replace "lmer" by "lmer.formula" in the command that
>>> generated the error and execute that modified formula.  This will open a
>>> browser and allow you to walk through the function line by line,
>>> examining (and changing) anything you want in the environment of that
>>> function.  Doing this will, I believe, lead you to exactly the line in
>>> the "lmer" code that generated the error message you don't understand.
>>> With only a modest amount of luck, you should be able to find in this
>>> way what you can do to avoid that error.
>>>
>>>          Anecdotal evidence suggests that people who use the techniques
>>> described in suggestions 1-3 tend to get quicker, more useful replies
>>> from this list.  Moreover, in virtually every case that I've tried
>>> suggestion 4, I've been able to figure out how to overcome that
>>> particular difficulty.  In addition, I've often learned useful things
>>> about R that I didn't know befor.
>>>
>>>          hope this helps.
>>>          spencer graves
>>>
>>> Abderrahim Oulhaj wrote:
>>>
>>>
>>>> Dear All,
>>>>
>>>> I have the following error message when I fitted  lmer to a  binary data with the "AGQ" option:
>>>>
>>>> Error in family$mu.eta(eta) : NAs are not allowed in subscripted assignments
>>>> In addition: Warning message:
>>>> IRLS iterations for PQL did not converge
>>>>
>>>> Any help?
>>>>
>>>> Thanks in advance,
>>>>
>>>> Abderrahim
>>>>
>>>>
>>>>      [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dataanalytics at rediffmail.com  Sun Jan  8 20:18:15 2006
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 8 Jan 2006 19:18:15 -0000
Subject: [R] wicked wikis for R
Message-ID: <20060108191815.27733.qmail@webmail29.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060108/20bde221/attachment.pl

From dataanalytics at rediffmail.com  Sun Jan  8 20:18:17 2006
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 8 Jan 2006 19:18:17 -0000
Subject: [R] wicked wikis for R
Message-ID: <20060108191817.3943.qmail@webmail49.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060108/20a40d53/attachment.pl

From pinard at iro.umontreal.ca  Sun Jan  8 20:25:08 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 8 Jan 2006 14:25:08 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <17342.11073.712221.863881@stat.math.ethz.ch>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<17342.11073.712221.863881@stat.math.ethz.ch>
Message-ID: <20060108192508.GD10698@phenix.sram.qc.ca>

[Martin Maechler]

>    FrPi> Suppose the file (or tape) holds N records (N is not known
>    FrPi> in advance), from which we want a sample of M records at
>    FrPi> most. [...] If the algorithm is carefully designed, when
>    FrPi> the last (N'th) record of the file will have been processed
>    FrPi> this way, we may then have M records randomly selected from
>    FrPi> N records, in such a a way that each of the N records had an
>    FrPi> equal probability to end up in the selection of M records.  I
>    FrPi> may seek out for details if needed.

>[...] I'm also intrigued about the details of the algorithm you
>outline above.

I went into my old SPSS books and related references to find it for you, 
to no avail (yet I confess I did not try very hard).  I vaguely remember 
it was related to Spearman's correlation computation: I did find notes 
about the "severe memory limitation" of this computation, but nothing 
about the implemented workaround.  I did find other sampling devices, 
but not the very one I remember having read about, many years ago.

On the other hand, Googling tells that this topic has been much studied, 
and that Vitter's algorithm Z seems to be popular nowadays (even if not 
the simplest) because it is more efficient than others.  Google found 
a copy of the paper:

   http://www.cs.duke.edu/~jsv/Papers/Vit85.Reservoir.pdf

Here is an implementation for Postgres: 

   http://svr5.postgresql.org/pgsql-patches/2004-05/msg00319.php

yet I do not find it very readable -- but this is only an opinion: I'm 
rather demanding in the area of legibility, while many or most people 
are more courageous than me! :-).

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From ka4alin at yandex.ru  Sun Jan  8 21:01:44 2006
From: ka4alin at yandex.ru (Evgeniy Kachalin)
Date: Sun, 08 Jan 2006 23:01:44 +0300
Subject: [R] R-help Digest, Vol 35, Issue 7
In-Reply-To: <43BFE7C5.1030307@statistik.uni-dortmund.de>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
	<43BFD113.50908@yandex.ru>
	<43BFE7C5.1030307@statistik.uni-dortmund.de>
Message-ID: <43C16FA8.9020803@yandex.ru>

Uwe Ligges :
> Evgeniy Kachalin wrote:
> 
>> Hello, dear participants!
>>
>> Could you tip me, is there any simple and nice way to build 
>> scatter-plot for three different types of data (, and o and * - signs, 
>> for example) with legend.
>>
>> Now i can guess only that way:
>>
>> plot(x~y,data=subset(mydata,factor1=='1'), pch='.',col='blue')
>> points(x~y,data=subset(mydata,factor1=='2'), pch='*',col='green')
>> points(.... etc
>>
>> What is the simple and nice way?
>> Thank you very much for your kindness and help.
>>
> 
> 
> Example:
> 
> 
> with(iris,
>   plot(Sepal.Length, Sepal.Width, pch = as.integer(Species)))
> with(iris,
>   legend(7, 4.4, legend = unique(as.character(Species)),
>                     pch = unique(as.integer(Species))))
> 

Uwe, sorry for my stupid question. You mean that when pch=factor , plot 
can recycle the factor and use it for subscripts or marks.

Then pch=as.integer(Species) results in c(1,2,3) for 3 factor levels. 
And I need symbols 15,16,17 and colors red, blue, green.

So then I do:
iris$Species->spec.symb
iris$Species->spec.col
levels(spec.symb)<-c(15,16,17)
levels(spec.col)<-c('red','green','blue')

That's the only way?
More of that!!! 'Plot' does not like factors in 'pch'. So it must be so:
plot(x~y,data, pch=as.integer(as.character(spec.symb))).
That's totally crazy...

-- 
Evgeniy



From ihok at hotmail.com  Sun Jan  8 21:12:57 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Sun, 08 Jan 2006 15:12:57 -0500
Subject: [R] Wikis etc.
Message-ID: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>

Philippe's idea to start a wiki that grows out of the content on 
http://zoonek2.free.fr/UNIX/48_R/all.html is really great. Here's why.

My hypothesis is that the basic reason that people ask questions on R-help 
rather than first looking elsewhere is that looking elsewhere doesn't get 
them the info they need.

People think in terms of the tasks they have to do. The documentation for R, 
which can be very good, is organized in terms of the structure of R, its 
functions. This mismatch -- people think of tasks, the documentation "thinks 
in" functions -- causes people to turn to the mailing list.

What we need is documentation that can be browsed in terms of tasks, like 
http://zoonek2.free.fr/UNIX/48_R/all.html. If that can be edited by the 
community, all the better. This is especially good for newbies (like myself) 
who try a tutorial, find that it lacks in some aspect, and can give 
immediate feedback, e.g., via a Wiki.

As far as keeping current with the latest versions of R, I think we'll have 
to arrive at some sort of convention that says: the code in this example 
works with R version X, package version Y. Then, if that code is found to 
fail in some future version, it's easy enough to make a second exampe. (As a 
bonus, these examples could be an automated test suite for R.)

Philippe, if you find you'd like assistance, I'd like to help.



From linux.zzz at gmail.com  Sun Jan  8 21:53:12 2006
From: linux.zzz at gmail.com (zzz haha)
Date: Mon, 9 Jan 2006 04:53:12 +0800
Subject: [R] Finding R mailing list archives {was "Wikis etc."}
In-Reply-To: <17343.60211.358181.704410@stat.math.ethz.ch>
References: <s3bfa074.082@medicine.umaryland.edu>
	<17343.60211.358181.704410@stat.math.ethz.ch>
Message-ID: <dc6097310601081253h6c135b47se1e7d0c4be041ccb@mail.gmail.com>

> I really wonder if adding yet another URL to the footer of every
> message will be the solution; as others have correctly remarked,
> the problem is that for many newbies it is more convenient to ask
> rather than to first read something that contains more than three
> words. ;-)

hahaha. the balance between (i) harsh to the newbie and (ii) boring to
the experts.

balance is hard. :)



From Roger.Bivand at nhh.no  Sun Jan  8 22:03:50 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 8 Jan 2006 22:03:50 +0100 (CET)
Subject: [R] maptools, write.polylistShape
In-Reply-To: <43C001F2.9010900@univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0601082154590.26148-100000@reclus.nhh.no>

On Sat, 7 Jan 2006, Patrick Giraudoux wrote:

> Dear Roger,
> 
> I am trying to use the write.polylistShape() function of maptools for 
> the first time and realize that it handles list of polygons of class 
> 'polylist'. However, it seems that no as.polylist() function exist in 
> the package. The question behind that is: in your opinion, which would 
> be the best  way to convert a list of matrix of polygon nodes 
> coordinates into an object of  class polylist?

Certainly through sp classes - construct a list of Polygons objects each 
with one (or more) Polygon objects, raise to SpatialPolygons, then back 
out through writePolyShape(). Assuming each list member matrix is the 
perimeter of a single polygon, and each Polygons object only has one 
Polygon:

n <- length(list_of_matrices)
list_of_polygons <- vector(mode="list", length=n)
for (i in 1:n) {
  Pl <- Polygon(list_of_matrices[[i]])
  list_of_polygons[[i]] <- Polygons(list(Pl), ID=as.character(i))
}
# could be done with lapply too
SPs <- SpatialPolygons(list_of_polygons)
rownames(mydata) <- as.character(1:n)
SPDF <- SpatialPolygonsDataFrame(SPs, mydata)
writePolyShape(SPDF, "outfile")

(untried - but close enough).

Best wishes,

Roger


> 
> All the best,
> 
> Patrick
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ales.ziberna at gmail.com  Sun Jan  8 22:22:24 2006
From: ales.ziberna at gmail.com (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Sun, 8 Jan 2006 22:22:24 +0100
Subject: [R] How to unload a package or "undo" library("package")
Message-ID: <000b01c61499$9f4ad070$a7fdfea9@TAMARA>

Hello!

I would like to unload a package form a current R session. I tried
datach(package:packagename), however it does not work. The reason I want to
unload it is that I want to correct some files in the package and reinstall
it without closing an R session.

Best,
Ales Ziberna

PS: I am using R 2.1.1 on Windows XP



From ggrothendieck at gmail.com  Sun Jan  8 22:41:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 8 Jan 2006 16:41:55 -0500
Subject: [R] How to unload a package or "undo" library("package")
In-Reply-To: <000b01c61499$9f4ad070$a7fdfea9@TAMARA>
References: <000b01c61499$9f4ad070$a7fdfea9@TAMARA>
Message-ID: <971536df0601081341sb6b05baq3f36ceb41a565b6d@mail.gmail.com>

Here are some things to try:

detach() - detach most recent attached package
detach(2) - detach package which is in position 2 on search list. 
Same as detach()
detach("package:mypackage") - mypackage from search list
search() - display search list

On 1/8/06, Ale? ?iberna <ales.ziberna at gmail.com> wrote:
> Hello!
>
> I would like to unload a package form a current R session. I tried
> datach(package:packagename), however it does not work. The reason I want to
> unload it is that I want to correct some files in the package and reinstall
> it without closing an R session.
>
> Best,
> Ales Ziberna
>
> PS: I am using R 2.1.1 on Windows XP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From h.wickham at gmail.com  Sun Jan  8 22:42:37 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 8 Jan 2006 21:42:37 +0000
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <20060108184707.GC10698@phenix.sram.qc.ca>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<f8e6ff050601052328y3ada32bdh28455bd493973ac8@mail.gmail.com>
	<20060108184707.GC10698@phenix.sram.qc.ca>
Message-ID: <f8e6ff050601081342j4d6fe99fm5a0637475f6f76d5@mail.gmail.com>

> Thanks as well for these hints.  Googling around as your suggested (yet
> keeping my eyes in the MySQL direction, because this is what we use),
> getting MySQL itself to do the selection is a bit discouraging, as
> according to comments I've read, MySQL does not seem to scale well with
> the database size according to the comments I've read, especially when
> records have to be decorated with random numbers and later sorted.

With SQL there is always a way to do what you want quickly, but you
need to think carefully about what operations are most common in your
database.  For example, the problem is much easier if you can assume
that the rows are numbered sequentially from 1 to n.  This could be
enfored using a trigger whenever a record is added/deleted.  This
would slow insertions/deletions but speed selects.

> Just for fun: here, "sample(100000000, 10)" in R is slowish already :-).

This is another example where greater knowledge of problem can yield
speed increases.  Here (where the number of selections is much smaller
than the total number of objects) you are better off generating 10
numbers with runif(10, 0, 1000000) and then checking that they are
unique

> >Another possibility is to generate a large table of randomly
> >distributed ids and then use that (with randomly generated limits) to
> >select the appropriate number of records.
>
> I'm not sure I understand your idea (what mixes me in the "randomly
> generated limits" part).  If the "large table" is much larger than the
> size of the wanted sample, we might not be gaining much.

Think about using a table of random numbers.  They are pregenerated
for you, you just choose a starting and ending index.  It will be slow
to generate the table the first time, but then it will be fast.  It
will also take up quite a bit of space, but space is cheap (and time
is not!)

Hadley



From dmbates at gmail.com  Sun Jan  8 23:03:02 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 8 Jan 2006 16:03:02 -0600
Subject: [R] lmer p-vales are sometimes too small
In-Reply-To: <43BE8C63.1050600@zoologi.su.se>
References: <43BE8C63.1050600@zoologi.su.se>
Message-ID: <40e66e0b0601081403n32eaa2bave5aafb443d5182e3@mail.gmail.com>

I have just uploaded version 0.99-6 of the Matrix package to the
incoming area at CRAN.  It should appear on the archives in the next
day or two.

In this version all degrees of freedom, test statistics and p-values
have been removed from the summary, show and anova methods.  I agree
with John Maindonald that it is better not to give any p-values than
to give misleading, sometimes seriously misleading, p-values.

All the code for lmer is, of course, open source.  At present it
resides in the Matrix package but it may return to the lme4 package
following the release of R-2.3.0.  The sources for the Matrix package
are available at

  https://svn.r-project.org/R-packages/trunk/Matrix

An experimental version of lmer based on a supernodal sparse Cholesky
factorization is at

  https://svn.r-project.org/R-packages/branches/Matrix-mer2

That is the current development branch.  Once I get issues with the
PQL and Laplace methods for GLMMs resolved this branch will be merged
back into the trunk.  If you are only interested in linear mixed
models it is better to work with this branch.

In both branches there is an R function called getFixDF that currently
is a stub returning incorrect answers (as indicated in the comments). 
At present I don't know to get correct answers for the range of models
that can be fit by lmer.  I welcome submissions of patches.


On 1/6/06, Olof Leimar <olof.leimar at zoologi.su.se> wrote:
> This concerns whether p-values from lmer can be trusted. From
> simulations, it seems that lmer can produce very small, and probably
> spurious, p-values. I realize that lmer is not yet a finished product.
> Is it likely that the problem will be fixed in a future release of the
> lme4 package?
>
> Using simulated data for a quite standard mixed-model anova (a balanced
> two-way design; see code for the function SimMixed pasted below), I
> compared the output of lmer, for three slightly different models, with
> the output of aov. For an example where there is no fixed treatment
> effect (null hypothesis is true), with 4 blocks, 2 treatments, and 40
> observations per treatment-block combination, I find that lmer gives
> more statistical significances than it should, whereas aov does not have
> this problem. An example of output I generated by calling
>   > SimMixed(1000)
> is the following:
>
> Proportion significances at the 0.05 level
> aov:     0.05
> lmer.1:  0.148
> lmer.2:  0.148
> lmer.3:  0.151
>
> Proportion significances at the 0.01 level
> aov:     0.006
> lmer.1:  0.076
> lmer.2:  0.076
> lmer.3:  0.077
>
> Proportion significances at the 0.001 level
> aov:     0.001
> lmer.1:  0.047
> lmer.2:  0.047
> lmer.3:  0.047
>
> which is based on 1000 simulations (and takes about 5 min on my PowerMac
> G5). The different models fitted are:
>
> fm.aov <- aov(y ~ Treat + Error(Block/Treat), data = dat)
> fm.lmer.1 <- lmer(y ~ Treat + (Treat|Block), data = dat)
> fm.lmer.2 <- lmer(y ~ Treat + (Treat-1|Block), data = dat)
> fm.lmer.3 <- lmer(y ~ Treat + (1|Block) + (Treat-1|Block), data = dat)
>
> It seems that, depending on the level of the test, lmer gives between a
> factor of 3 to a factor of around 50 times too many significances. The
> first two lmer models seem to give identical results, whereas the third
> (which I think perhaps is the one that best represents the data
> generated by the simulation) differs slightly. In running the
> simulations, warnings like this are occasionally generated:
>
> Warning message:
> optim or nlminb returned message false convergence (8)
>   in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
> 1.49011611938477e-08,
>
> They seem to derive from the third of the lmer models. Perhaps there is
> some numerical issue in the lmer function? From running SimMixed()
> several times, I have noticed that large p-values (say, larger than 0.5)
> agree very well between lmer and aov, but there seems to be a systematic
> discrepancy for smaller p-values, where lmer gives smaller values than
> aov. The F-values agree between all analyzes (except for fm.lmer.3 when
> there is a warning), so there is a systematic difference between lmer
> and aov in how a p-value is obtained from the F-value, which becomes
> severe for small p-values.
>
>
>
> My output from sessionInfo()
>
> R version 2.2.1, 2005-12-20, powerpc-apple-darwin7.9.0
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"  "base"
>
> other attached packages:
>       lme4   lattice    Matrix
>   "0.98-1" "0.12-11"  "0.99-3"
>
>
>
> Pasted code for the SimMixed function (some lines might wrap):
>
> # This function generates n.sims random data sets for a design with 4
> # blocks, 2 treatments applied to each block, and 40 replicate
> # observations for each block-treatment combination. There is no true
> # fixed treatment effect, so a statistical significance of a test for
> # a fixed treatment effect ought to occur with a probability equal to
> # the nominal level of the test. Four tests are applied to each
> # simulated data set: the classical aov and three versions of lmer,
> # corresponding to different model formulations. The proportion of
> # tests for a fixed treatment effect that become significant at the
> # 0.05 0.01 and 0.001 levels are printed, as well as the p-values for
> # the last of the simulations. In my runs, lmer gives significance
> # more often than indicated by the nominal level, for each of the
> # three models, whereas aov is OK. The package lme4 needs to be loaded
> # to run the code.
>
> SimMixed <- function(n.sims = 1) {
>    k <- 4                # number of blocks
>    n <- 40               # num obs per block X treatment combination
>    m1 <- 1.0             # fixed effect of level 1 of treatment
>    m2 <- m1              # fixed effect of level 2 of treatment
>    sd.block <- 0.5       # SD of block random effect
>    sd.block.trt <- 1.0   # SD of random effect for block X treatm
>    sd.res <- 0.1         # Residual SD
>    Block <- factor( rep(1:k, each=2*n) )
>    Treat <- factor( rep( rep(c("Tr1","Tr2"), k), each=n) )
>    m <- rep( rep(c(m1, m2), k), each=n) # fixed effects
>    # storage for p-values
>    p.aov <- rep(0, n.sims)
>    p.lmer.1 <- rep(0, n.sims)
>    p.lmer.2 <- rep(0, n.sims)
>    p.lmer.3 <- rep(0, n.sims)
>    for (i in 1:n.sims) {
>      # first get block and treatment random deviations
>      b <- rep( rep(rnorm(k, 0, sd.block), each=2) +
>               rnorm(2*k, 0, sd.block.trt), each=n )
>      # then get response
>      y <- m + b + rnorm(2*k*n, 0, sd.res)
>      dat <- data.frame(Block, Treat, y)
>      # perform the tests
>      fm.aov <- aov(y ~ Treat+Error(Block/Treat), data = dat)
>      fm.lmer.1 <- lmer(y ~ Treat+(Treat|Block), data = dat)
>      fm.lmer.2 <- lmer(y ~ Treat+(Treat-1|Block), data = dat)
>      fm.lmer.3 <- lmer(y ~ Treat+(1|Block)+(Treat-1|Block), data = dat)
>      # store the p-values
>      p.aov[i] <- summary(fm.aov)$"Error: Block:Treat"[[1]]$"Pr(>F)"[1]
>      p.lmer.1[i] <- anova(fm.lmer.1)[6]
>      p.lmer.2[i] <- anova(fm.lmer.2)[6]
>      p.lmer.3[i] <- anova(fm.lmer.3)[6]
>    }
>
>    cat("\nProportion significances at the 0.05 level \n")
>    cat("aov:    ", sum(p.aov<0.05)/n.sims, "\n")
>    cat("lmer.1: ", sum(p.lmer.1<0.05)/n.sims, "\n")
>    cat("lmer.2: ", sum(p.lmer.2<0.05)/n.sims, "\n")
>    cat("lmer.3: ", sum(p.lmer.3<0.05)/n.sims, "\n")
>
>    cat("\nProportion significances at the 0.01 level \n")
>    cat("aov:    ", sum(p.aov<0.01)/n.sims, "\n")
>    cat("lmer.1: ", sum(p.lmer.1<0.01)/n.sims, "\n")
>    cat("lmer.2: ", sum(p.lmer.2<0.01)/n.sims, "\n")
>    cat("lmer.3: ", sum(p.lmer.3<0.01)/n.sims, "\n")
>
>    cat("\nProportion significances at the 0.001 level \n")
>    cat("aov:    ", sum(p.aov<0.001)/n.sims, "\n")
>    cat("lmer.1: ", sum(p.lmer.1<0.001)/n.sims, "\n")
>    cat("lmer.2: ", sum(p.lmer.2<0.001)/n.sims, "\n")
>    cat("lmer.3: ", sum(p.lmer.3<0.001)/n.sims, "\n")
>
>    cat("\nFinal aov analysis: \n")
>    print(summary(fm.aov)$"Error: Block:Treat")
>    cat("\nFinal lmer analysis 1: \n")
>    print(anova(fm.lmer.1))
>    cat("\nFinal lmer analysis 2: \n")
>    print(anova(fm.lmer.2))
>    cat("\nFinal lmer analysis 3: \n")
>    print(anova(fm.lmer.3))
> }
>
>
>
>
> --
> Olof Leimar, Professor
> Department of Zoology
> Stockholm University
> SE-106 91 Stockholm
> Sweden
>
> olof.leimar at zoologi.su.se
> http://www.zoologi.su.se/research/leimar/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bitwrit at ozemail.com.au  Mon Jan  9 15:47:02 2006
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Mon, 09 Jan 2006 09:47:02 -0500
Subject: [R] Question about graphics in R
In-Reply-To: <A49FA2C7-B144-4E5C-AA8F-751727512A01@eecs.oregonstate.edu>
References: <A49FA2C7-B144-4E5C-AA8F-751727512A01@eecs.oregonstate.edu>
Message-ID: <43C27766.40100@ozemail.com.au>

Martin Erwig wrote:
> Considering the R function/plot shown below, I wonder whether
> it is possible to do the following changes:
> 
> (1) Change the color of each point to be picked from
> list of colors according to its z-value. (The range
> should be from blue (z=0) to red (z=1).) The grid
> should then be omitted. [I have seen "terrain.colors", but
> don't know how to use it for this purpose.]
> 
> (2) Add two lines to the surface for, say z=0.8 and
> z=0.3. [Can contour or contourLines be used?]
> 
> 
> ---
> 
> x <- seq(0, 1, length = 50)
> y <- x
> f <- function(x,y) { sin ((1-x)*y) }
> z <- outer(x,y,f)
> 
> persp(x, y, z,
> 	theta = 30, phi = 30,
> 	shade = 0.3, col = "red"
> 	)
> 
> ---
> 
> 
> Finally, I would also produce a flattened 2D map
> of the same function, i.e. a map in which each point
> (x,y) is mapped to a color in a range according to
> f(x,y). Also two lines for f(x,y)=c1 and f(x,y)=c2
> should be added.
> 
> 
> Is this possible?
> 
Hi Martin,

The function color.scale will linearly transform numeric values into
colors between arbitrary color endpoints specified as red, green and blue:

Your example would be
redrange=c(0,1),greenrange=c(0,0),bluerange=c(1,0).

For the 2D plot, have a look at color2D.matplot. Both are in the plotrix
package.

You might also want to look at colorRampPalette in the grDevices package.

Jim



From hb at maths.lth.se  Sun Jan  8 23:45:15 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 09 Jan 2006 09:45:15 +1100
Subject: [R] repeat { readline() }
In-Reply-To: <Pine.LNX.4.61.0601081202460.24213@gannet.stats>
References: <43C06DAC.3020704@maths.lth.se>
	<Pine.LNX.4.61.0601080809150.20500@gannet.stats>
	<Pine.LNX.4.61.0601081202460.24213@gannet.stats>
Message-ID: <43C195FB.7010401@maths.lth.se>

Prof Brian Ripley wrote:
> On Sun, 8 Jan 2006, Prof Brian Ripley wrote:
> 
>> Ctrl-Break works: see the rw-FAQ and README.rterm.  (You'll need a return
>> to see a new prompt.)
>>
>> It is related to your reading directly from the console, so Ctrl-C is
>> getting sent to the wrong place, I believe.  (There's a comment from 
>> Guido
>> somewhere in the sources about this, and this seems corroborated by the
>> fact that Ctrl-C will interrupt under Rterm --ess.)
> 
> 
> Although the comment is there (in psignal.c), on closer examination the 
> cause is a change Guido made to getline.c, so Ctrl-C is treated as a 
> character during keyboard input.  I doubt if that was intentional (0 is 
> not the default state) and I have changed it for R-devel.\

Thank you very much this.

Henrik

>>
>> On Sun, 8 Jan 2006, Henrik Bengtsson wrote:
>>
>>> Hi.
>>>
>>> Using Rterm v2.2.1 on WinXP, is there a way to interrupt a call like
>>>
>>>  repeat { readline() }
>>>
>>> without killing the Command window?  Ctrl+C is not interrupting the 
>>> loop:
>>>
>>> R : Copyright 2006, The R Foundation for Statistical Computing
>>> Version 2.2.1 Patched (2006-01-01 r36947)
>>> <snip></snip>
>>>
>>>> repeat { readline() }
>>>
>>> ^C
>>> ^C
>>> ^C
>>> ^C
>>>
>>> On Unix it works.  The problem seems to get the interrupt signal to
>>> "occur" outside the readline() call so that "repeat" is interrupted. 
>>> Doing
>>>
>>> repeat { readline(); Sys.sleep(3) }
>>>
>>> and it is likely that can generate an interrupt signal outside 
>>> readline().
>>>
>>> It seem like readline()/readLines(n=1) or an underlying method catches
>>> the interrupt signal quietly and just waits for a symbol to come
>>> through.  Try readline() by itself and press Ctrl+C.  Maybe this is a
>>> property of the Windows Command terminal, I don't know, but is it a
>>> wanted feature and are R core aware of it?  Note that, in Rgui it is
>>> possible to interrupting such a loop by pressing ESC.
>>>
>>> Cheers
>>>
>>> Henrik
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From pinard at iro.umontreal.ca  Sun Jan  8 23:51:23 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 8 Jan 2006 17:51:23 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <f8e6ff050601081342j4d6fe99fm5a0637475f6f76d5@mail.gmail.com>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<f8e6ff050601052328y3ada32bdh28455bd493973ac8@mail.gmail.com>
	<20060108184707.GC10698@phenix.sram.qc.ca>
	<f8e6ff050601081342j4d6fe99fm5a0637475f6f76d5@mail.gmail.com>
Message-ID: <20060108225123.GA15353@phenix.sram.qc.ca>

[hadley wickham]

>> [...] according to comments I've read, MySQL does not seem to scale
>> well with the database size according to the comments I've read,
>> especially when records have to be decorated with random numbers and
>> later sorted.

>With SQL there is always a way to do what you want quickly, but you
>need to think carefully about what operations are most common in your
>database.  For example, the problem is much easier if you can assume
>that the rows are numbered sequentially from 1 to n.  This could be
>enfored using a trigger whenever a record is added/deleted.  This would
>slow insertions/deletions but speed selects.

Sure (for a caricature example) that if database records are already 
decorated with random numbers, and an index is built over the 
decoration, random sampling may indeed be done quicker :-). The fact is 
that (at least our) databases are not especially designed for random 
sampling, and people in charge would resist redesigning them merely 
because there would be a few needs for random sampling.

What would be ideal is being able to build random samples out of any big 
database or file, with equal ease.  The fact is that it's doable.  
(Brian Ripley points out that R textual I/O has too much overhead for 
being usable, so one should rather say, sadly: "It's doable outside R".)

>> Just for fun: here, "sample(100000000, 10)" in R is slowish already
>> :-).

>This is another example where greater knowledge of problem can yield
>speed increases.  Here (where the number of selections is much smaller
>than the total number of objects) you are better off generating 10
>numbers with runif(10, 0, 1000000) and then checking that they are
>unique

Of course, my remark about "sample()" is related to the previous 
discussion.  If "sample(N, M)" was more on the O(M) side than being on 
the O(N) side (both memory-wise and cpu-wise), it could be used for
preselecting which rows of a big database to include in a random sample, 
so building on your idea of using a set of IDs.  As the sample of 
M records will have to be processed in-memory by R anyway, computing 
a vector of M indices does not (or should not) increase complexity.

However, "sample(N, M)" is likely less usable for randomly sampling 
a database, if it is O(N) to start with.  About your suggestion of using 
"runif" and later checking uniqueness, "sample()" could well be 
implemented this way, when the arguments are proper.  The "greater 
knowledge of the problem" could be built in right into the routine meant 
to solve it.  "sample(N, M)" could even know how to take advantage of 
some simplified case of a "reservoir sampling" technique :-).

>> >[...] a large table of randomly distributed ids [...] (with randomly
>> >generated limits) to select the appropriate number of records.

>[...] a table of random numbers [...] pregenerated for you, you just
>choose a starting and ending index.  It will be slow to generate the
>table the first time, but then it will be fast.  It will also take up
>quite a bit of space, but space is cheap (and time is not!)

Thanks for the explanation.

In the case under consideration here (random sampling of a big file or 
database), I would be tempted to guess that the time required for 
generating pseudo-random numbers is negligible when compared to the 
overall input/output time, so it might be that pregenerating randomized 
IDs is not worth the trouble.  Also given that whenever the database 
size changes, the list of pregenerated IDs is not valid anymore.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From drf5n at maplepark.com  Mon Jan  9 00:35:30 2006
From: drf5n at maplepark.com (David Forrest)
Date: Sun, 8 Jan 2006 17:35:30 -0600 (CST)
Subject: [R] Wikis for R
In-Reply-To: <20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>
References: <17340.65179.89485.345795@stat.math.ethz.ch>
	<20060105132304.GB30865@ime.usp.br>
	<20060106085653.b6a5f129.detlef.steuer@hsu-hamburg.de>
Message-ID: <Pine.LNX.4.58.0601081639440.1422@maplepark.com>

On Fri, 6 Jan 2006, Detlef Steuer wrote:
...
> Back to operating wikis:The wiki spamming is a serious problem,
> especially because I HATE to login to read or edit anything. So the
> choice is: take the wiki as seriously as work and have a look every
> other day to remove the spam (or better: form a group of volunteers).
> That hurts or at least is no fun. Or put restrictions on it. That hurts
> even more. Perhaps I do not understand Philippe`s "loggable". What does
> a logfile with IPs help? The spammers are strangers selling viagra; I
> don`t want to find them :-)

Sometimes you can mark the spammer's IP's as spammers and then ban editing
by them.  For my own UseMod wiki, I avoid spam by rejecting edits that
change more than 3 URLS.  But this is getting off of the R help topic.

> To sum it up:There is a very simple way to proceed:Philippe uses his
> Docuwiki install as official, _general_ Rwiki and I close down mine. The
> beginners will find their niche in there, if there is a real demand. I
> wouldnt mind to give up "my" wiki, because I have to admit it failed
> to achieve what I would have liked.

I like wikis too, and contributed a few pages to your wiki.  The low
use-rate and high wiki spamming content makes it not a place I frequent.

> So, Philippe, if you like, you can take over. I would replace my wiki
> with a notice where to find yours and the community gets a second chance
> :-)

http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl does have some
useful content.  Maybe it would be good to wade through it and figure out
how to patch the standard R documentation to include those contributions.

An advantage of a wiki is the low barrier to adding correctable
documentation.  The email list also provides low-barrier-to-entry
documentation, and its success demonstrates the clear need for additional
documentation.

Considering that, maybe there would be a benefit in rolling references to
good email threads into the documentation in some sort of an automatic
method.  Perhaps if an email question leads to a clarification or good
example of a feature, someone could post a message to the thread that
tags it for inclusion by reference to relevant documentation in the next
release cycle.

If this wishful thinking would come to pass, then the standard
documentation could point people towards using the mail archive in a more
directly useful manner, and we'd retain the peer-reviewed answer quality
of the email list.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From neo27 at t-online.de  Mon Jan  9 00:43:21 2006
From: neo27 at t-online.de (Mark Hempelmann)
Date: Mon, 09 Jan 2006 00:43:21 +0100
Subject: [R] Clustering and Rand Index - VS-KM
Message-ID: <43C1A399.9090006@t-online.de>

Dear WizaRds,

I have been trying to compute the adjusted Rand index as by Hubert/ 
Arabie, and could not correctly approach how to define a partition 
object as in my last request yesterday.

With package fpc I try to work around the problem, using my original data:

mat <- matrix( c(6,7,8,2,3,4,12,14,14, 14,15,13,3,1,2,3,4,2, 
15,3,10,5,11,7,13,6,1, 15,4,10,6,12,8,12,7,1), ncol=9, byrow=T )
rownames(mat) <- paste("v", 1:4, sep="" )

## and the given partitions:

p1=c(1,1,1,2,2,2,3,3,3)
p2=c(1,1,1,3,2,2,3,3,2)
p3=c(1,2,1,3,1,3,1,3,2)
p4=c(1,2,1,3,1,3,1,3,2)

## Now

cluster.stats(d=dist(mat), clustering=p1, alt.clustering=p2)

## just gives
Error in as.dist(dmat[clustering == i, clustering == i]) :
	(subscript) logical subscript too long

I think I don't understand the use of 'd' here. How can I calculate the 
corrected Rand matrix:
( .000  .407 -.071 -.071)
( .407  .000 -.071 -.071)
(-.071 -.071  .000 1.000)
(-.071 -.071 1.000  .000)

Does the clue package help me here? Does anyone know if there is a VS-KM 
algorithm (Variable Selection Heuristic for K-Means Clustering) 
implemented in R? Unfortunately, I did not find any serach entries.

Thank you for your help and support
Mark



From pinard at iro.umontreal.ca  Mon Jan  9 00:48:14 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 8 Jan 2006 18:48:14 -0500
Subject: [R] A comment about R:
In-Reply-To: <43BCD877.7020800@statistik.uni-dortmund.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>
	<43BA98BD.7060707@pburns.seanet.com>
	<x2psn9fe23.fsf@viggo.kubism.ku.dk>
	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>
	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>
	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>
	<20060104160412.GA8708@phenix.sram.qc.ca>
	<43BCD877.7020800@statistik.uni-dortmund.de>
Message-ID: <20060108234814.GB16820@phenix.sram.qc.ca>

[Uwe Ligges]
>Fran??ois Pinard wrote:
>>[David Forrest]

>>>[...] A few end-to-end tutorials on some interesting analyses would
>>>be helpful.

>>I'm in the process of learning R.  While tutorials are undoubtedly
>>very useful, and understanding that working and studying methods vary
>>between individuals, what I (for one) would like to have is a fairly
>>complete reference manual to the library [...] organised by topics.

>Have a look at  help.start() --> Search Engine & Keywords --> Section 
>"Keywords by Topic".

Yes, thanks.  This is quite in the spirit, or direction, of what I was 
proposing.  Is that resource exhaustive?  (I'm asking out of laziness, 
as it might take me several months to really check.)

One serious drawback (for me) is that it requires an heavy weight
browser to be used, with Javascript enabled.  I do not find this very
practical.  Another point is that the presentation, while useful, is a
rather dry.  In another message, I suggested the "Emacs Lisp Reference 
Manual" as a good example of a fluid presentation of a voluminous 
library.  There might be some workable compromise between the current 
situation with R, even through the "Keywords by Topic", and that 
fluidity.  (Wikis also have the drawback of requiring heavy machinery,
and the editor they force us into if usually unbearable.)

I may be back with this subject, but only in a good while.  I'm slowly 
building a kind of documentation plan I want (yet in French), as I learn 
R, and guess I may complete my base learning in one or two years from 
now (hoping I'll stay courageous enough).  If I then get something 
usable or shareable enough, I'll offer it -- because I like returning 
a little something for the nice tools given to me!  :-)

                                In any case, thanks for listening!

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From lebouton at msu.edu  Mon Jan  9 02:57:46 2006
From: lebouton at msu.edu (Joseph LeBouton)
Date: Sun, 08 Jan 2006 19:57:46 -0600
Subject: [R] Ordering boxplot factors; thank you!
In-Reply-To: <Pine.LNX.4.61.0601060816570.18569@gannet.stats>
References: <43BDD59A.4010308@msu.edu>
	<1136515681.4295.15.camel@localhost.localdomain>
	<Pine.LNX.4.61.0601060816570.18569@gannet.stats>
Message-ID: <43C1C31A.8050403@msu.edu>

Profs. Ripley and Schwartz,

Thank you both very much for the suggestions!  These are exactly what I 
was looking for.  I'll re-read the boxplot help yet again; every time I 
read it something essential worms its way into my consciousness, but it 
enters more freely when I have a hint where to look.  The sad truth is 
that I was heading for a ~10-line hacker solution.  You've saved me from 
my usual code-writing ignomy.

Happy new year,

-Joseph

Prof Brian Ripley wrote:
> On Thu, 5 Jan 2006, Marc Schwartz wrote:
> 
>> On Thu, 2006-01-05 at 20:27 -0600, Joseph LeBouton wrote:
>>
>>> Hi all,
>>>
>>> what a great help list!  I hope someone can help me with this puzzle...
>>>
>>> I'm trying to find a simple way to do:
>>>
>>> boxplot(obs~factor)
>>>
>>> so that the factors are ordered left-to-right along the x-axis by
>>> median, not alphabetically by factor name.
> 
> 
> The thing to realize is that they are not alphabetic, but ordered by 
> factor levels.  So the key is to set the levels.  (The help page for 
> boxplot does say that, as I was relieved to find.)
> 
>>> Complicated ways abound, but I'm hoping for a magical one-liner that'll
>>> do the trick.
>>>
>>> Any suggestions would be treasured.
>>>
>>> Thanks,
>>>
>>> -jlb
>>
>>
>>
>> Using the first example in ?boxplot, which is:
>>
>> boxplot(count ~ spray, data = InsectSprays, col = "lightgray")
>>
>>
>>
>> Get the medians for 'count by spray' using tapply() and then sort the
>> results in increasing order, by median:
>>
>>  med <- sort(with(InsectSprays, tapply(count, spray, median)))
>>
>>> med
>>
>>   C    E    D    A    F    B
>> 1.5  3.0  5.0 14.0 15.0 16.5
>>
>>
>> Now do the boxplot, setting the factor levels in order by median:
>>
>>  boxplot(count ~ factor(spray, levels = names(med)),
>>          data = InsectSprays, col = "lightgray")
>>
>>
>> So...technically two lines of code.
> 
> 
> This was answered yesterday in terms of bwplot.  See ?reorder.factor
> for the same example done using reorder.factor.  That will give you the 
> single line asked for, and be self-explanatory.
> 

-- 
************************************
Joseph P. LeBouton
Forest Ecology PhD Candidate
Department of Forestry
Michigan State University
East Lansing, Michigan 48824

Office phone: 517-355-7744
email: lebouton at msu.edu



From aleszib2 at gmail.com  Mon Jan  9 07:45:53 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Mon, 9 Jan 2006 07:45:53 +0100
Subject: [R] Clustering and Rand Index - VS-KM
In-Reply-To: <43C1A399.9090006@t-online.de>
Message-ID: <001201c614e8$56ba5ca0$a7fdfea9@TAMARA>

You can comput t<he adjusted Rand with function classAgreement form package
e1071:
classAgreement(table(p1,p2))$crand

You can also use
 cluster.stats(d=dist(t(mat)), clustering=p1, alt.clustering=p2)

However in your code below, the orientation of mat is wrong (that's why
there is a "t()" around the mat in my code above). The variables should be
represented by rows and the cases by columns.

Best,
Ales Ziberna



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Hempelmann
Sent: Monday, January 09, 2006 12:43 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Clustering and Rand Index - VS-KM

Dear WizaRds,

I have been trying to compute the adjusted Rand index as by Hubert/ Arabie,
and could not correctly approach how to define a partition object as in my
last request yesterday.

With package fpc I try to work around the problem, using my original data:

mat <- matrix( c(6,7,8,2,3,4,12,14,14, 14,15,13,3,1,2,3,4,2,
15,3,10,5,11,7,13,6,1, 15,4,10,6,12,8,12,7,1), ncol=9, byrow=T )
rownames(mat) <- paste("v", 1:4, sep="" )

## and the given partitions:

p1=c(1,1,1,2,2,2,3,3,3)
p2=c(1,1,1,3,2,2,3,3,2)
p3=c(1,2,1,3,1,3,1,3,2)
p4=c(1,2,1,3,1,3,1,3,2)

## Now

cluster.stats(d=dist(mat), clustering=p1, alt.clustering=p2)

## just gives
Error in as.dist(dmat[clustering == i, clustering == i]) :
	(subscript) logical subscript too long

I think I don't understand the use of 'd' here. How can I calculate the
corrected Rand matrix:
( .000  .407 -.071 -.071)
( .407  .000 -.071 -.071)
(-.071 -.071  .000 1.000)
(-.071 -.071 1.000  .000)

Does the clue package help me here? Does anyone know if there is a VS-KM
algorithm (Variable Selection Heuristic for K-Means Clustering) implemented
in R? Unfortunately, I did not find any serach entries.

Thank you for your help and support
Mark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From aponsero at cabri22.com  Mon Jan  9 07:54:02 2006
From: aponsero at cabri22.com (PONSERO Alain)
Date: Mon, 9 Jan 2006 07:54:02 +0100
Subject: [R] (sans objet)
Message-ID: <872B8777DE7F7B44B0ED4172BF3EF099145197@messcabri2.cabri22.loc>

 Dear R People:

in the function loess, how can one add the weight of the points which is
contained in the variable "nbtotal"

Data :
Nbtotal	P_alim	H_eau


xyplot(P_alim ~ H_eau, 
	auto.key = list(points = T, lines = F),data = data,
	type = c("p", "smooth"), span=.2,
 	scales = "free", layout = c(1, 1),
	main="",
        xlab="Hauteur d'eau (en m)",
        ylab="taux d'alimentation")


the graph xyplot is not modified if the weights term is added
xyplot(P_alim ~ H_eau, 
	auto.key = list(points = T, lines = F),data = data,
	type = c("p", "smooth"), span=.2,weights=Nb_total,
 	scales = "free", layout = c(1, 1),
	main="",
        xlab="Hauteur d'eau (en m)",
        ylab="taux d'alimentation")


Thanks in advance,

Alain ponsero



From aleszib2 at gmail.com  Mon Jan  9 07:56:54 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Mon, 9 Jan 2006 07:56:54 +0100
Subject: [R] How to unload a package or "undo" library("package")
In-Reply-To: <971536df0601081341sb6b05baq3f36ceb41a565b6d@mail.gmail.com>
Message-ID: <001301c614e9$e0a19860$a7fdfea9@TAMARA>

If I do  detach(package:blockmodeling)

My package "blockmodeling" does not appear in (.packages())
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"   

However, if I want to install a newer version from a local zip file, I get:
utils:::menuInstallLocal()
package 'blockmodeling' successfully unpacked and MD5 sums checked
Error: cannot remove prior installation of package 'blockmodeling'


Is there any way around that?

Best,
Ales Ziberna

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Sunday, January 08, 2006 10:42 PM
To: Ale?? ??iberna
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to unload a package or "undo" library("package")

Here are some things to try:

detach() - detach most recent attached package
detach(2) - detach package which is in position 2 on search list. 
Same as detach()
detach("package:mypackage") - mypackage from search list
search() - display search list

On 1/8/06, Ale?? ??iberna <ales.ziberna at gmail.com> wrote:
> Hello!
>
> I would like to unload a package form a current R session. I tried 
> datach(package:packagename), however it does not work. The reason I 
> want to unload it is that I want to correct some files in the package 
> and reinstall it without closing an R session.
>
> Best,
> Ales Ziberna
>
> PS: I am using R 2.1.1 on Windows XP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kkurikka at cc.helsinki.fi  Mon Jan  9 08:10:33 2006
From: kkurikka at cc.helsinki.fi (Kyosti H Kurikka)
Date: Mon, 9 Jan 2006 09:10:33 +0200 (EET)
Subject: [R] R-help Digest, Vol 35, Issue 7
In-Reply-To: <43C16FA8.9020803@yandex.ru>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
	<43BFD113.50908@yandex.ru> <43BFE7C5.1030307@statistik.uni-dortmund.de>
	<43C16FA8.9020803@yandex.ru>
Message-ID: <Pine.OSF.4.58.0601090847070.116203@sirppi.helsinki.fi>

Hi!

Just use your factors for indexing c(15,16,17) and
c("red","green","blue"). So, with the iris data:

>with(iris, plot(Sepal.Length, Sepal.Width,
       pch=c(15,16,17)[as.integer(Species)],
       col=c("red","green","blue")[as.integer(Species)] ))

Best regards,
Kyosti Kurikka


> > Evgeniy Kachalin wrote:
> >
> >> Hello, dear participants!
> >>
> >> Could you tip me, is there any simple and nice way to build
> >> scatter-plot for three different types of data (, and o and * - signs,
> >> for example) with legend.
> >>
> >> Now i can guess only that way:
> >>
> >> plot(x~y,data=subset(mydata,factor1=='1'), pch='.',col='blue')
> >> points(x~y,data=subset(mydata,factor1=='2'), pch='*',col='green')
> >> points(.... etc
> >>
> >> What is the simple and nice way?
> >> Thank you very much for your kindness and help.
> >>
> >
> >
> > Example:
> >
> >
> > with(iris,
> >   plot(Sepal.Length, Sepal.Width, pch = as.integer(Species)))
> > with(iris,
> >   legend(7, 4.4, legend = unique(as.character(Species)),
> >                     pch = unique(as.integer(Species))))
> >
>
> Uwe, sorry for my stupid question. You mean that when pch=factor , plot
> can recycle the factor and use it for subscripts or marks.
>
> Then pch=as.integer(Species) results in c(1,2,3) for 3 factor levels.
> And I need symbols 15,16,17 and colors red, blue, green.
>
> So then I do:
> iris$Species->spec.symb
> iris$Species->spec.col
> levels(spec.symb)<-c(15,16,17)
> levels(spec.col)<-c('red','green','blue')
>
> That's the only way?
> More of that!!! 'Plot' does not like factors in 'pch'. So it must be so:
> plot(x~y,data, pch=as.integer(as.character(spec.symb))).
> That's totally crazy...
>
> --
> Evgeniy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From vincent at 7d4.com  Mon Jan  9 08:26:34 2006
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 09 Jan 2006 08:26:34 +0100
Subject: [R] paste tab and print
Message-ID: <43C2102A.70809@7d4.com>

Dear all,

info = paste('a', 'b', sep='\t')
print(info , quote=F)

doesn't produce the same result with R201 and R220
(under Windows2000)
R 2.0.1 : [1] a   b
R 2.2.0 : [1] a\tb

I did read the CHANGESR220 file and tried also the search
engine but couldn't find an answer.
I certainly missed the point, and apologize about that.

So if somebody could tell me how to insert a tab inside
strings under R.2.2.0., it would be very kind.

Thanks
Vincent



From ligges at statistik.uni-dortmund.de  Mon Jan  9 08:39:06 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jan 2006 08:39:06 +0100
Subject: [R] R-help Digest, Vol 35, Issue 7
In-Reply-To: <43C16FA8.9020803@yandex.ru>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
	<43BFD113.50908@yandex.ru>
	<43BFE7C5.1030307@statistik.uni-dortmund.de>
	<43C16FA8.9020803@yandex.ru>
Message-ID: <43C2131A.6080207@statistik.uni-dortmund.de>

Evgeniy Kachalin wrote:

> Uwe Ligges :
> 
>> Evgeniy Kachalin wrote:
>>
>>> Hello, dear participants!
>>>
>>> Could you tip me, is there any simple and nice way to build 
>>> scatter-plot for three different types of data (, and o and * - 
>>> signs, for example) with legend.
>>>
>>> Now i can guess only that way:
>>>
>>> plot(x~y,data=subset(mydata,factor1=='1'), pch='.',col='blue')
>>> points(x~y,data=subset(mydata,factor1=='2'), pch='*',col='green')
>>> points(.... etc
>>>
>>> What is the simple and nice way?
>>> Thank you very much for your kindness and help.
>>>
>>
>>
>> Example:
>>
>>
>> with(iris,
>>   plot(Sepal.Length, Sepal.Width, pch = as.integer(Species)))
>> with(iris,
>>   legend(7, 4.4, legend = unique(as.character(Species)),
>>                     pch = unique(as.integer(Species))))
>>
> 
> Uwe, sorry for my stupid question. You mean that when pch=factor , plot 
> can recycle the factor and use it for subscripts or marks.

Yes, it can recycle, but in the example above it does not recycle but 
takes the whole "Species" vector.


> Then pch=as.integer(Species) results in c(1,2,3) for 3 factor levels. 
> And I need symbols 15,16,17 and colors red, blue, green.

What about adding 14 as in as.integer(Species)+14, or 1 for the colors, 
respectively?



> So then I do:
> iris$Species->spec.symb
> iris$Species->spec.col
> levels(spec.symb)<-c(15,16,17)
> levels(spec.col)<-c('red','green','blue')
> 
> That's the only way?

This is one qay of many.


> More of that!!! 'Plot' does not like factors in 'pch'. So it must be so:
> plot(x~y,data, pch=as.integer(as.character(spec.symb))).
> That's totally crazy...

You can set up your own pch variable of course, if you don't like it 
this fast and easy way.

Uwe Ligges



From petr.pikal at precheza.cz  Mon Jan  9 08:48:07 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 09 Jan 2006 08:48:07 +0100
Subject: [R] "Missing value representation in Excel before extraction to
	R	with RODBC"
In-Reply-To: <000501c612fa$079cac30$109d72d5@Larissa>
Message-ID: <43C22347.12686.2B96C0@localhost>

Hi

I believe it has something to do with the column identification 
decision. When R decides what is in a column it uses only some values 
from the beginning of a file.

I do not use RODBC as read.delim("clipboard", ...) is usually more 
convenient but probably there is a way how to tell RODBC what is in 
the column instead of let R decide from the top of the file.

But I may be completely mistaken.

HTH
Petr


On 6 Jan 2006 at 20:47, Fredrik Lundgren wrote:

From:           	"Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
To:             	"R-help" <r-help at stat.math.ethz.ch>
Date sent:      	Fri, 6 Jan 2006 20:47:29 +0100
Subject:        	[R] "Missing value representation in Excel before extraction to R
	with RODBC"

> Dear list,
> 
> How should missing values be expressed in Excel before extraction to R
> via RODBC. I'm bewildered. Sometimes the representation with NA in
> Excel appears to work and shows up in R as <NA> but sometimes the use
> of NA in Excel changes the whole vector to NA's. Blank or nothing or
> NA as representation for missing values in Excel with dateformat gives
> NA's of the whole vector in R but with  general format in Excel gives
> blanks for missing values in R. How should I represent missing values
> in Excel?
> 
> 
> Best wishes and thanks for any help
> Fredrik Lundgren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Mon Jan  9 09:18:42 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jan 2006 08:18:42 +0000 (GMT)
Subject: [R] How to unload a package or "undo" library("package")
In-Reply-To: <001301c614e9$e0a19860$a7fdfea9@TAMARA>
References: <001301c614e9$e0a19860$a7fdfea9@TAMARA>
Message-ID: <Pine.LNX.4.61.0601090817530.8090@gannet.stats>

See the rw-FAQ, Q4.8

On Mon, 9 Jan 2006, Ales Ziberna wrote:

> If I do  detach(package:blockmodeling)
>
> My package "blockmodeling" does not appear in (.packages())
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> However, if I want to install a newer version from a local zip file, I get:
> utils:::menuInstallLocal()
> package 'blockmodeling' successfully unpacked and MD5 sums checked
> Error: cannot remove prior installation of package 'blockmodeling'
>
>
> Is there any way around that?
>
> Best,
> Ales Ziberna
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Sunday, January 08, 2006 10:42 PM
> To: Alea }iberna
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to unload a package or "undo" library("package")
>
> Here are some things to try:
>
> detach() - detach most recent attached package
> detach(2) - detach package which is in position 2 on search list.
> Same as detach()
> detach("package:mypackage") - mypackage from search list
> search() - display search list
>
> On 1/8/06, Alea }iberna <ales.ziberna at gmail.com> wrote:
>> Hello!
>>
>> I would like to unload a package form a current R session. I tried
>> datach(package:packagename), however it does not work. The reason I
>> want to unload it is that I want to correct some files in the package
>> and reinstall it without closing an R session.
>>
>> Best,
>> Ales Ziberna
>>
>> PS: I am using R 2.1.1 on Windows XP
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan  9 09:31:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jan 2006 08:31:09 +0000 (GMT)
Subject: [R] paste tab and print
In-Reply-To: <43C2102A.70809@7d4.com>
References: <43C2102A.70809@7d4.com>
Message-ID: <Pine.LNX.4.61.0601090820510.8090@gannet.stats>

On Mon, 9 Jan 2006 vincent at 7d4.com wrote:

> Dear all,
>
> info = paste('a', 'b', sep='\t')
> print(info , quote=F)
>
> doesn't produce the same result with R201 and R220
> (under Windows2000)

(There are no such versions of R, and neither is current.)

> R 2.0.1 : [1] a   b
> R 2.2.0 : [1] a\tb
>
> I did read the CHANGESR220 file

Did you mean CHANGES?  That is for Windows-specific changes.  You 
meed to look in the NEWS file.

> and tried also the search engine but couldn't find an answer. I 
> certainly missed the point, and apologize about that.
>
> So if somebody could tell me how to insert a tab inside
> strings under R.2.2.0., it would be very kind.

You have.  A tab inside a string is printed as \t (see ?print.default).
However,  cat will show you the effect

> info = paste('a', 'b', sep='\t')
> print(info)
[1] "a\tb"
> cat(info, "\n")
a       b

(if this survives emailing, or try it yourself).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan  9 09:36:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jan 2006 08:36:06 +0000 (GMT)
Subject: [R] "Missing value representation in Excel before extraction to
 R with RODBC"
In-Reply-To: <43C22347.12686.2B96C0@localhost>
References: <43C22347.12686.2B96C0@localhost>
Message-ID: <Pine.LNX.4.61.0601090833090.8090@gannet.stats>

On Mon, 9 Jan 2006, Petr Pikal wrote:

> Hi
>
> I believe it has something to do with the column identification
> decision. When R decides what is in a column it uses only some values
> from the beginning of a file.

Not R, Excel.  Excel tells ODBC what the column types are.

> I do not use RODBC as read.delim("clipboard", ...) is usually more
> convenient but probably there is a way how to tell RODBC what is in
> the column instead of let R decide from the top of the file.

Using as.is=TRUE stops RODBC doing any conversion.

> But I may be completely mistaken.
>
> HTH
> Petr
>
>
> On 6 Jan 2006 at 20:47, Fredrik Lundgren wrote:
>
> From:           	"Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
> To:             	"R-help" <r-help at stat.math.ethz.ch>
> Date sent:      	Fri, 6 Jan 2006 20:47:29 +0100
> Subject:        	[R] "Missing value representation in Excel before extraction to R
> 	with RODBC"
>
>> Dear list,
>>
>> How should missing values be expressed in Excel before extraction to R
>> via RODBC. I'm bewildered. Sometimes the representation with NA in
>> Excel appears to work and shows up in R as <NA> but sometimes the use
>> of NA in Excel changes the whole vector to NA's. Blank or nothing or
>> NA as representation for missing values in Excel with dateformat gives
>> NA's of the whole vector in R but with  general format in Excel gives
>> blanks for missing values in R. How should I represent missing values
>> in Excel?
>>
>>
>> Best wishes and thanks for any help
>> Fredrik Lundgren

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Jan  9 09:40:57 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 9 Jan 2006 09:40:57 +0100
Subject: [R] [R-pkgs] sudoku
In-Reply-To: <20060108122152.7bcc1f6f.detlef.steuer@hsu-hamburg.de>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>
	<20060108122152.7bcc1f6f.detlef.steuer@hsu-hamburg.de>
Message-ID: <17346.8601.975737.17359@stat.math.ethz.ch>

First, "thanks a lot!" to David Brahms for finally tackling this
important problem, and keeping the R language "major league" ! 
;-) :-)  {but the "thanks!" is meant seriously!}

>>>>> "Detlef" == Detlef Steuer <detlef.steuer at hsu-hamburg.de>
>>>>>     on Sun, 8 Jan 2006 12:21:52 +0100 writes:

    Detlef> Hey, you spoiled my course!  :-)

    Detlef> I planned using this as an excersise.  Alternative
    Detlef> ideas anyone ...

Well, you could *add* to it:

1) When I have been thinking about doing this myself (occasionally
  in the past weeks), I had always thought that finding *ALL*
  solutions was a very important property of the algorithm I would
  want to design.
  (since this is slightly more general and useful than proofing
  uniqueness which the current algorithm does not yet do anyway).

2) The current sudoku() prints the result itself and returns a
   matrix; improved, it should return an object of class "sudoku",
   with a print() and a plot() method;
3) The plot() method should of course also work for unfinished
   "sudoku" objects, and in fact, the *input* to sudoku() should
   also be allowed to be a (typically unfinished) "sudoku" object.

4) Then you could have your students use "grid" and
   grid.locator() for GUI *input* of a sudoku; i.e. you'd have
   another function which returns a (typically unfinished)
   "sudoku" object.

5) You could start looking at *solving* the more general sudokus
   where the blocks are not 3x3 squares anymore, but more
   general rectangular polygons of 9 squares each.

6) Now you need to refine the GUI from "4)" because your users
   need to be able to *draw* the block shapes for the
   generalized sudokus.

7) Given "1)" is solved, the problem of *generating* sudokus,
   that David already mentioned in his announcement, becomes
   more relevant: You want to be sure that the sudokus you
   generate have exactly one solution.  And your generating
   algorithm could start with a very full sudoku (that has
   exactly 1 solution) and "erases" squares as much as possible,
   always checking that no other solution becomes possible.

You see, there's lot of interesting exercises left for your
course. (;-)

Martin

    Detlef> On Fri, 6 Jan 2006 11:43:44 -0500 "Brahm, David"
    Detlef> <David.Brahm at geodecapital.com> wrote:

    >> Any doubts about R's big-league status should be put to
    >> rest, now that we have a Sudoku Puzzle Solver.  Take
    >> that, SAS!  See package "sudoku" on CRAN.
    >> 
    >> The package could really use a puzzle generator --
    >> contributors are welcome!
    >> 
    >> -- David Brahm (brahm at alum.mit.edu)



From aleszib2 at gmail.com  Mon Jan  9 10:10:17 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Mon, 9 Jan 2006 10:10:17 +0100
Subject: [R] How to unload a package or "undo" library("package")
In-Reply-To: <Pine.LNX.4.61.0601090817530.8090@gannet.stats>
Message-ID: <002a01c614fc$9db00330$a7fdfea9@TAMARA>

Thank you!

I guess it can not be done!


Best,
Ales Ziberna

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, January 09, 2006 9:19 AM
To: Ales Ziberna
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to unload a package or "undo" library("package")

See the rw-FAQ, Q4.8

On Mon, 9 Jan 2006, Ales Ziberna wrote:

> If I do  detach(package:blockmodeling)
>
> My package "blockmodeling" does not appear in (.packages())
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> However, if I want to install a newer version from a local zip file, I
get:
> utils:::menuInstallLocal()
> package 'blockmodeling' successfully unpacked and MD5 sums checked
> Error: cannot remove prior installation of package 'blockmodeling'
>
>
> Is there any way around that?
>
> Best,
> Ales Ziberna
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Sunday, January 08, 2006 10:42 PM
> To: Alea }iberna
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to unload a package or "undo" library("package")
>
> Here are some things to try:
>
> detach() - detach most recent attached package
> detach(2) - detach package which is in position 2 on search list.
> Same as detach()
> detach("package:mypackage") - mypackage from search list
> search() - display search list
>
> On 1/8/06, Alea }iberna <ales.ziberna at gmail.com> wrote:
>> Hello!
>>
>> I would like to unload a package form a current R session. I tried 
>> datach(package:packagename), however it does not work. The reason I 
>> want to unload it is that I want to correct some files in the package 
>> and reinstall it without closing an R session.
>>
>> Best,
>> Ales Ziberna
>>
>> PS: I am using R 2.1.1 on Windows XP
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From helene.wagner at wsl.ch  Mon Jan  9 10:26:50 2006
From: helene.wagner at wsl.ch (Helene Wagner)
Date: Mon, 09 Jan 2006 10:26:50 +0100
Subject: [R] How to restore workspace from .RDataTmp?
Message-ID: <5.2.1.1.1.20060109102128.022007a8@mail.wsl.ch>

Can I restore a workspace from .RDataTmp (under Windows), and how should I 
do this? Whenever I try, I get the messages "Error in load(name, envir = 
.GlobalEnv) : error reading from connection" (in the R command window) 
and/or "Fatal error: unable to restore saved data in .RData" (Windows). I 
was not able to find any documentation of this.

Any help is appreciated.
Helene



****************************
PD Dr. Helene Wagner
Senior scientist
Swiss Federal Research Institute WSL
Zuercherstrasse 111
CH-8903 Birmensdorf

Phone:  +41-44-739 25 87
Fax:    +41-44-739 22 15
Email:  helene.wagner at wsl.ch
http://www.wsl.ch/staff/helene.wagner/



From druau at ukaachen.de  Mon Jan  9 11:07:45 2006
From: druau at ukaachen.de (David Ruau)
Date: Mon, 09 Jan 2006 11:07:45 +0100
Subject: [R] need palette of topographic colors similar to topo.colors()
In-Reply-To: <8d5a36350601071134g43c8cd1bqa84170f5b2cec9db@mail.gmail.com>
References: <8d5a36350601071134g43c8cd1bqa84170f5b2cec9db@mail.gmail.com>
Message-ID: <ce33e94eba8bd186927b1b8d9f229eac@ukaachen.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

have a look at http://colorbrewer.org/
and install the package RColorBrewer

David

On Jan 7, 2006, at 20:34, bogdan romocea wrote:

> Dear useRs,
>
> I got stuck trying to generate a palette of topographic colors that
> would satisfy these two requirements:
>    - the pallete must be 'anchored' at 0 (just like on a map), with
> light blue/lawn green corresponding to data values close to 0 (dark
> blue to light blue for negative values, green-yellow-brown for
> positive values)
>    - the brown must get darker for higher positive values.
>
> topo.colors() fails both requirements and AFAICS lacks any options to
> control its behavior.
>   #---unsatisfactory topo.colors() behavior
>   topoclr <- function(tgt)
>   {
>   clr <- topo.colors(length(tgt))
>   clr <- clr[round(rank(tgt),0)]
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; topoclr(-50:50) ; topoclr(-20:80)
>
> An acceptable solution would be something like this
>   grayclr <- function(tgt)
>   {
>   tgt <- sort(tgt) ; neg <- which(tgt < 0)
>   clrneg <- gray(0:length(tgt[neg])/length(tgt[neg]))
>   clrpos <- gray(length(tgt[-neg]):0/length(tgt[-neg]))
>   clr <- c(clrneg,clrpos)
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; grayclr(-50:50) ; grayclr(-20:80)
> if only I could make gray() use blue/brown instead of black (I tried a
> couple of things but got stuck again).
>
> Any suggestions?
>
> Thank you,
> b.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (Darwin)

iD8DBQFDwjXx7EoGVUIQyhERAq/0AKCPKQjlqtVl+RxVlOVlGXf2WxnvSwCfdZcD
mEpL3eSW8KI+eYFSSQZJvzM=
=/5Ix
-----END PGP SIGNATURE-----



From petr.pikal at precheza.cz  Mon Jan  9 11:25:51 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 09 Jan 2006 11:25:51 +0100
Subject: [R] How to restore workspace from .RDataTmp?
In-Reply-To: <5.2.1.1.1.20060109102128.022007a8@mail.wsl.ch>
Message-ID: <43C2483F.30925.BC0C90@localhost>

Hi

See
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/63390.html 

There is not much to do in such a case. Maybe only to use some binary 
editor.

Recommendation: do not use .Rdata file for storing data!!!!

Cheers
Petr

On 9 Jan 2006 at 10:26, Helene Wagner wrote:

Date sent:      	Mon, 09 Jan 2006 10:26:50 +0100
To:             	r-help at stat.math.ethz.ch
From:           	Helene Wagner <helene.wagner at wsl.ch>
Subject:        	[R] How to restore workspace from .RDataTmp?

> Can I restore a workspace from .RDataTmp (under Windows), and how
> should I do this? Whenever I try, I get the messages "Error in
> load(name, envir = .GlobalEnv) : error reading from connection" (in
> the R command window) and/or "Fatal error: unable to restore saved
> data in .RData" (Windows). I was not able to find any documentation of
> this.
> 
> Any help is appreciated.
> Helene
> 
> 
> 
> ****************************
> PD Dr. Helene Wagner
> Senior scientist
> Swiss Federal Research Institute WSL
> Zuercherstrasse 111
> CH-8903 Birmensdorf
> 
> Phone:  +41-44-739 25 87
> Fax:    +41-44-739 22 15
> Email:  helene.wagner at wsl.ch
> http://www.wsl.ch/staff/helene.wagner/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From justin_bem at yahoo.fr  Mon Jan  9 11:29:23 2006
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 9 Jan 2006 11:29:23 +0100 (CET)
Subject: [R] repeat { readline() }
In-Reply-To: <43C06DAC.3020704@maths.lth.se>
Message-ID: <20060109102923.93977.qmail@web25713.mail.ukl.yahoo.com>


--- Henrik Bengtsson <hb at maths.lth.se> a ??crit :

> Hi.
> 
> Using Rterm v2.2.1 on WinXP, is there a way to
> interrupt a call like
> 
>   repeat { readline() }
> 
> without killing the Command window?  Ctrl+C is not
> interrupting the loop:

To interupt the loop add and condition for a break
like this :
if () break



> 
> R : Copyright 2006, The R Foundation for Statistical
> Computing
> Version 2.2.1 Patched (2006-01-01 r36947)
> <snip></snip>
> 
>  > repeat { readline() }
> ^C
> ^C
> ^C
> ^C
> 
> On Unix it works.  The problem seems to get the
> interrupt signal to 
> "occur" outside the readline() call so that "repeat"
> is interrupted. Doing
> 
> repeat { readline(); Sys.sleep(3) }
> 
> and it is likely that can generate an interrupt
> signal outside readline().
> 
> It seem like readline()/readLines(n=1) or an
> underlying method catches 
> the interrupt signal quietly and just waits for a
> symbol to come 
> through.  Try readline() by itself and press Ctrl+C.
>  Maybe this is a 
> property of the Windows Command terminal, I don't
> know, but is it a 
> wanted feature and are R core aware of it?  Note
> that, in Rgui it is 
> possible to interrupting such a loop by pressing
> ESC.
> 
> Cheers
> 
> Henrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From drewbrewit at yahoo.com  Sun Jan  8 13:56:57 2006
From: drewbrewit at yahoo.com (Drew)
Date: Sun, 8 Jan 2006 04:56:57 -0800 (PST)
Subject: [R] Wikis for R
Message-ID: <20060108125657.34950.qmail@web50909.mail.yahoo.com>

Frank uses the term "hierarchical keyword
organization" which I agree is a good way to organize
a system designed to help users. In fact, this is one
reason why I like the R graphics gallery which allows
one to quickly find a particular type of plot based on
keyword, examine the plot to see if it's close to
what's desired, and then get the detailed code to
examine or modify for one's own specific purpose. It's
an example of going from general concept to specific
details. 

R's HTML help pages already have a keyword system but
it takes you to the specifics too quickly. I'd
recommend revising the current keyword system in the
help pages (or adding to it) to create what Frank
calls "data manipulation examples gallery." In other
words a code gallery that users could browse quickly
and simply copy and paste code from it to their
application. Think of the keywords as different "How
to" topics (e.g. how to create a data frame, how to
reshape data, etc.). 

** I think this sort of thing would save time for
users of all levels.**  New users could find code
quickly WITHOUT submitting a request to R-HELP email
list. Advanced, experts, and gurus would no longer
have to respond to such requests for the umpteenth
time. Think of this as the examples that did not make
it to the help files. Because it's a wiki folks could
add to it the examples that are truly helpful.

Creating such a system requires 3 steps (at a very
high level):

1) ** Develop a hierarchical keyword system **
Combine the current keyword system
(http://finzi.psych.upenn.edu/R/doc/html/search/SearchEngine.html)
with something more like the left-hand frame of the
function finder
(http://biostat.mc.vanderbilt.edu/s/finder/finder.html).
Essentially a list of keywords organized by "how to do
X in R". Users could browse this list and click on a
"how to" topic to get to more specific keyworks or see
the code gallery for that particular keyword. Once
this list of keywords gets generated then it would
would remain mostly static over time. If changes are
required then have the site maintainer add the word,
but the keyword list is not open to public revision.


2)  ** Add code as content for each keyword. **
Without quality and quantity of content, this proposed
system is not worth much. No one will use it without
content -- I don't care what fancy tool we use to
create or manage such a thing. I suspect that much of
the code to start this gallery could come from R tips
(http://www.ku.edu/~pauljohn/R/Rtips.html). After
amassing a good size collection of code, allow the
public to start adding to the code (but not the
keywords identified in #1 above.)


3)  ** Link 1 and 2 above, which are on the wiki, with
the existing help files that get installed with R **. 
So when a user chooses "HTML Help" from the Help menu
(in the windows version of R, for example) he/she
would see an additional clickable link that says
something like "How To". Clicking on this would take
the user to the list of keywords for the code gallery
on the online wiki. This is their starting point. From
here the user would click on appropriate keywords to
browse the code gallery. The code itself could be
clickable to bring up the very detailed help pages
much like the R Graphics Gallery does (for example
clicking on the green color "rnorm" in the first graph
in the Graphics Gallery takes me to the following
hyperlinked help page:
http://addictedtor.free.fr/graphiques/help/index.html?pack=stats&alias=rnorm&fun=Normal.html).


I'm wondering how much of the above could be done
without requiring much work from the R CORE team? I'd
recommend a small group of individuals (10 - 20 or so)
to start getting the content together since that may
be the most time intensive part. (You can't expect 1
person to do it all, and 1000's of people won't make
much progress.) I can lend a few hours per month if
there was a cadre of dedicated individuals who are
passionate about making this happen. However, I have
very few programming skills, almost none outside of R,
of which I'm a relative beginner.

This is the vision I have for the wiki. What do others
think?

~Nick


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Frank E Harrell Jr
Sent: Saturday, January 07, 2006 6:05 AM
To: Detlef Steuer
Cc: r-help at stat.math.ethz.ch; phgrosjean at sciviews.org;
steuer at unibw-hamburg.de
Subject: Re: [R] Wikis for R


Detlef Steuer wrote:
> On Thu, 5 Jan 2006 11:23:04 -0200
> "Fernando Henrique Ferraz P. da Rosa"
<academic at feferraz.net> wrote:
> 
> 
>>Martin Maechler writes:
>>
>>> If you go to the bottom of that wikipedia page,
>>> you see that there is an "R Wiki" -- and has been
for several
>>> years now (!) at a Hamburg (De) university.
>>>
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>>>
>>>(...) 
>>>So, are you sure that another R Wiki is desirable,
rather than
>>>have people who "believe in Wiki's for R" use the
existing
>>>one(s)?   I believe the main challenge will
(similar as for
>>>an "R-beginners" mailing list) to have
well-qualified "editors"
>>>to be willing to review and amend what others have
written.
>>
>>        I????ve tried to colaborate on the R Wiki
hosted by the Hamburg
>>university but the Wiki would get regularlly
vandalized by some spam
>>bot, and then I'd have to manually keep reverting it
several times. Also
>>the wiki engine used by this wiki is very
rudimentary. I think the
>>DokuWiki engine, which is used by Philippe Grosjean
is more promising as
>>a workhorse for an 'official' R-wiki. 
> 
> 
> What Fernando says is mostly true. I installed the
UseMod Wiki after talking to some of "us" as a test
balloon. The engine was chosen according to simplistic
installation and a "just-enough" feature set. Perhaps
that was a wrong decision. I`m one of the believers in
wikis, but mine over here did not fly.
> 
> Probably this and any wiki needs a critical mass for
visitors beginning to return and collaborate and
therein the Hamburg wiki failed. (One day I wanted to
take down the wiki for apparently having no users, but
just then someone gave some feedback)
> 
> As Frank Harrel pointed out, this may be because
R-help is just too helpful.
> In contrast to Frank I don`t think we should abandon
e-mail because of its success. The mailing list is, in
my opinion, the single biggest plus R has above all
competition.
> The wiki should provide something complementary to
r-help. Btw. those who do not search for information
in the mailing list archives or on CRAN before asking
simple questions won't do so in a wiki or in a
bulletin board system. (I find those simply unusable.
Am I getting old? I want to edit using my favourite
editor, not in some browser window.)
>  
> A central place for example code was my intention,
when opening the wiki back then. 
> 
> Back to operating wikis:
> The wiki spamming is a serious problem, especially
because I HATE to login to read or edit anything. So
the choice is: take the wiki as seriously as work and
have a look every other day to remove the spam (or
better: form a group of volunteers). That hurts or at
least is no fun. Or put restrictions on it. That hurts
even more. Perhaps I do not understand Philippe`s
"loggable". What does a logfile with IPs help? The
spammers are strangers selling viagra; I don`t want to
find them :-)
> 
> To sum it up:
> There is a very simple way to proceed:
> Philippe uses his Docuwiki install  as official,
_general_ Rwiki and I close down mine. The beginners
will find their niche in there, if there is a real
demand. 
> I wouldn????t mind to give up "my" wiki, because I
have to admit it failed to achieve what I would have
liked.
> 
> So, Philippe, if you like, you can take over. I
would replace my wiki with a notice where to find
yours and the community gets a second chance :-)
> 
> Detlef

The current e-mail system places a low burden on users
if they follow 
basic posting rules.  The burden is too low and users
still do not 
search for past answers and we also get dozens of
separate messages on a 
single topic (e.g., ylim on barplots).  As long as
everyone allows this, 
a wiki or discussion board will not work.  We need to
rethink the e-mail 
system in my view to create motivations for approaches
with true 
memories and hierarchical keyword organization instead
of using the 
apparent memory-less system.  This should be thought
through to include 
the new graphics gallery and a data manipulation
examples gallery, and 
other things, and needs to be launched from
www.r-project.org IMHO.

Frank

> 
> 
> 
> 
>>        I think that the title could be perhaps
changed to Rwiki
>>and the contents currently hosted on the Hamburg
wiki 'transfered' to 
>>the new location, if the current mantainers of the
Hamburg Wiki and
>>Philippe Grosjean agree (I????m cc-ing this msg to
them).
>>
>>        This could emerge then as official or
semi-oficial R-wiki, to be
>>linked to from the R-project home. 
>>
>>
>>--
>>"Though this be randomness, yet there is structure
in't."
>>                                           Rosa,
F.H.F.P
>>
>>Instituto de Matem????tica e Estat????stica
>>Universidade de S????o Paulo
>>Fernando Henrique Ferraz P. da Rosa
>>http://www.feferraz.net
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair          
School of Medicine
                      Department of Biostatistics  
Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pinard at iro.umontreal.ca  Sun Jan  8 19:47:07 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sun, 8 Jan 2006 13:47:07 -0500
Subject: [R] Suggestion for big files [was: Re: A comment about R:]
In-Reply-To: <f8e6ff050601052328y3ada32bdh28455bd493973ac8@mail.gmail.com>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<f8e6ff050601052328y3ada32bdh28455bd493973ac8@mail.gmail.com>
Message-ID: <20060108184707.GC10698@phenix.sram.qc.ca>

[hadley wickham]

>[Fran??ois Pinard]

>> Selecting a sample is easy.  Yet, I'm not aware of any SQL device for
>> easily selecting a _random_ sample of the records of a given table.
>> On the other hand, I'm no SQL specialist, others might know better.

>There are a number of such devices, which tend to be rather SQL variant
>specific.  Try googling for select random rows mysql, select random
>rows pgsql, etc.

Thanks as well for these hints.  Googling around as your suggested (yet 
keeping my eyes in the MySQL direction, because this is what we use), 
getting MySQL itself to do the selection is a bit discouraging, as 
according to comments I've read, MySQL does not seem to scale well with 
the database size according to the comments I've read, especially when 
records have to be decorated with random numbers and later sorted.

Yet, I did not drive any benchmark myself, and would not blindly take 
everything I read for granted, given that MySQL developers have speed in 
mind, and there are ways to interrupt a sort before running it to full 
completion, when only a few sorted records are wanted.

>Another possibility is to generate a large table of randomly
>distributed ids and then use that (with randomly generated limits) to
>select the appropriate number of records.

I'm not sure I understand your idea (what mixes me in the "randomly 
generated limits" part).  If the "large table" is much larger than the 
size of the wanted sample, we might not be gaining much.

Just for fun: here, "sample(100000000, 10)" in R is slowish already :-).

All in all, if I ever have such a problem, a practical solution probably 
has to be outside of R, and maybe outside SQL as well.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From hodgess at gator.dt.uh.edu  Sat Jan  7 21:32:10 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sat, 7 Jan 2006 14:32:10 -0600
Subject: [R] (no subject)
Message-ID: <200601072032.k07KWAfu017199@gator.dt.uh.edu>

Dear R People:

I am trying to build a package (yet again!)

I have both PCTex and WinEdt.  I want the *.tex files to use WinEdt.  How
should I set that, please?  Just in the path?

Also, where would I get Rd.sty, please?

Thanks,
R Version 2.2.1 Windows
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From Antje.Schuele at komdat.com  Mon Jan  9 12:13:02 2006
From: Antje.Schuele at komdat.com (=?iso-8859-1?Q?Antje_Sch=FCle?=)
Date: Mon, 9 Jan 2006 12:13:02 +0100
Subject: [R] two y-axis in xy-plot
Message-ID: <F5076E7EAA58F448A0EEC05ADE2317BD0303B0@muc-exch001.munich.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/6b010375/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jan  9 12:37:39 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jan 2006 12:37:39 +0100
Subject: [R] A comment about R:
In-Reply-To: <20060108234814.GB16820@phenix.sram.qc.ca>
References: <8B08A3A1EA7AAC41BE24C750338754E6013DAE43@HERMES.demogr.mpg.de>	<43BA98BD.7060707@pburns.seanet.com>	<x2psn9fe23.fsf@viggo.kubism.ku.dk>	<Pine.LNX.4.64.0601031107410.22515@homer22.u.washington.edu>	<971536df0601031137t7c88d9ddm3a8f2fc2e083581d@mail.gmail.com>	<Pine.LNX.4.58.0601031403280.19586@maplepark.com>	<20060104160412.GA8708@phenix.sram.qc.ca>	<43BCD877.7020800@statistik.uni-dortmund.de>
	<20060108234814.GB16820@phenix.sram.qc.ca>
Message-ID: <43C24B03.1000309@statistik.uni-dortmund.de>

Fran??ois Pinard wrote:

> [Uwe Ligges]
> 
>>Fran??ois Pinard wrote:
>>
>>>[David Forrest]
> 
> 
>>>>[...] A few end-to-end tutorials on some interesting analyses would
>>>>be helpful.
> 
> 
>>>I'm in the process of learning R.  While tutorials are undoubtedly
>>>very useful, and understanding that working and studying methods vary
>>>between individuals, what I (for one) would like to have is a fairly
>>>complete reference manual to the library [...] organised by topics.
> 
> 
>>Have a look at  help.start() --> Search Engine & Keywords --> Section 
>>"Keywords by Topic".
> 
> 
> Yes, thanks.  This is quite in the spirit, or direction, of what I was 
> proposing.  Is that resource exhaustive?  (I'm asking out of laziness, 
> as it might take me several months to really check.)

It is exhaustive in the following sense:
- the help page has the appropriate keyword
- the package is installed


> One serious drawback (for me) is that it requires an heavy weight
> browser to be used, with Javascript enabled.  I do not find this very
> practical.  Another point is that the presentation, while useful, is a
> rather dry.  In another message, I suggested the "Emacs Lisp Reference 
> Manual" as a good example of a fluid presentation of a voluminous 
> library.  There might be some workable compromise between the current 
> situation with R, even through the "Keywords by Topic", and that 
> fluidity.  (Wikis also have the drawback of requiring heavy machinery,
> and the editor they force us into if usually unbearable.)
> 
> I may be back with this subject, but only in a good while.  I'm slowly 
> building a kind of documentation plan I want (yet in French), as I learn 
> R, and guess I may complete my base learning in one or two years from 
> now (hoping I'll stay courageous enough).  If I then get something 
> usable or shareable enough, I'll offer it -- because I like returning 
> a little something for the nice tools given to me!  :-)


So we are looking forward to see the better system.

Uwe Ligges


>                                 In any case, thanks for listening!
>



From info at aghmed.fsnet.co.uk  Mon Jan  9 13:23:45 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 09 Jan 2006 12:23:45 +0000
Subject: [R] Wikis etc.
In-Reply-To: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
References: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
Message-ID: <6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>

At 20:12 08/01/06, Jack Tanner wrote:
>Philippe's idea to start a wiki that grows out of the content on 
>http://zoonek2.free.fr/UNIX/48_R/all.html is really great. Here's why.
>
>My hypothesis is that the basic reason that people ask questions on R-help 
>rather than first looking elsewhere is that looking elsewhere doesn't get 
>them the info they need.
>
>People think in terms of the tasks they have to do. The documentation for 
>R, which can be very good, is organized in terms of the structure of R, 
>its functions. This mismatch -- people think of tasks, the documentation 
>"thinks in" functions -- causes people to turn to the mailing list.

Further to that I feel that (perhaps because they do not like to blow their 
own trumpet too much) the authors of books on R do not stress how much most 
questioners could gain by buying and reading at least one of the many books 
on R. When I started I found the free documents useful but I made most 
progress when I bought MASS. I do realise that liking books is a bit last 
millennium.




Michael Dewey
http://www.aghmed.fsnet.co.uk



From jcbouette at gmail.com  Mon Jan  9 14:16:07 2006
From: jcbouette at gmail.com (Jean-Christophe BOUETTE)
Date: Mon, 9 Jan 2006 14:16:07 +0100
Subject: [R] wicked wikis for R
Message-ID: <11544d000601090516g3ccd6a9dg@mail.gmail.com>

> From: "Arin Basu" <dataanalytics at rediffmail.com>
> To: r-help at stat.math.ethz.ch
> Date: 8 Jan 2006 19:18:17 -0000
> Subject: [R] wicked wikis for R
> >Message: 41
> >Date: Sun, 08 Jan 2006 13:52:33 +1100
> > From: paul sorenson <sourceforge at metrak.com>
> >Subject: Re: [R] Wikis etc.
> >To: Frank E Harrell Jr <f.harrell at vanderbilt.edu>,     r-help
> >       <r-help at stat.math.ethz.ch>

-snip-

> Among others, here's one long-term benefit for the newbies. Instead of people getting admonished/thrashed with harsh expressions/advices like "go see the mailing list publishing etiquettes", or "you should search the archives and help files, and read all manuals, and ask others first before posting here..." (which can turn away many a newcomer from posting or using the mailing list or using R for that matter), wiki could make life a little easy for newbies/less experienced who could then receive more polite one liners like, "please check the wikipages...", or "solution #xyz in the wikipages for the solution".

Sorry, I don't get the point here. Some people will keep feeling
offensed when they're just told to read the man/wiki pages, and others
will simply change their answers from RTFM to RTFW.
Nobody can force people into reading the manuals, or reading the
posting guide. This is definitely one problem that the wiki will not
solve.

I like the idea of the wiki, but we have to consider pragmatically,
not as a panacea from problems beyond its scope.

Regards,
Jean-Christophe Bou??tt??.



From r.ghezzo at staff.mcgill.ca  Mon Jan  9 14:44:19 2006
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Mon,  9 Jan 2006 08:44:19 -0500
Subject: [R] Suggestion for big files [was: Re:  A comment about R:]
In-Reply-To: <20060108192508.GD10698@phenix.sram.qc.ca>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC18A@VAIEXCH04.vai.org>
	<Pine.LNX.4.61.0601051641470.1468@gannet.stats>
	<20060106034121.GA8861@alcyon.progiciels-bpi.ca>
	<17342.11073.712221.863881@stat.math.ethz.ch>
	<20060108192508.GD10698@phenix.sram.qc.ca>
Message-ID: <1136814259.43c268b3765de@webmail.mcgill.ca>

I found Reservoir-Sampling algorithms of time complexity O(n(1+log(N/n))) by
Kim-Hung Li , ACM Transactions on Mathematical Software Vol 20 No 4 Dec 94
p481-492.
He mentions algorithm Z and K and proposed 2 improved versions alg L and M.
Algorith L is really easy to implement but relatively slow, M doesn't look very
difficult and is the fastest.
Heberto Ghezzo
McGill University
Montreal - Canada

Quoting Fran??ois Pinard <pinard at iro.umontreal.ca>:

> [Martin Maechler]
>
> >    FrPi> Suppose the file (or tape) holds N records (N is not known
> >    FrPi> in advance), from which we want a sample of M records at
> >    FrPi> most. [...] If the algorithm is carefully designed, when
> >    FrPi> the last (N'th) record of the file will have been processed
> >    FrPi> this way, we may then have M records randomly selected from
> >    FrPi> N records, in such a a way that each of the N records had an
> >    FrPi> equal probability to end up in the selection of M records.  I
> >    FrPi> may seek out for details if needed.
>
> >[...] I'm also intrigued about the details of the algorithm you
> >outline above.
>
> I went into my old SPSS books and related references to find it for you,
> to no avail (yet I confess I did not try very hard).  I vaguely remember
> it was related to Spearman's correlation computation: I did find notes
> about the "severe memory limitation" of this computation, but nothing
> about the implemented workaround.  I did find other sampling devices,
> but not the very one I remember having read about, many years ago.
>
> On the other hand, Googling tells that this topic has been much studied,
> and that Vitter's algorithm Z seems to be popular nowadays (even if not
> the simplest) because it is more efficient than others.  Google found
> a copy of the paper:
>
>    http://www.cs.duke.edu/~jsv/Papers/Vit85.Reservoir.pdf
>
> Here is an implementation for Postgres:
>
>    http://svr5.postgresql.org/pgsql-patches/2004-05/msg00319.php
>
> yet I do not find it very readable -- but this is only an opinion: I'm
> rather demanding in the area of legibility, while many or most people
> are more courageous than me! :-).
>
> --
> Fran??ois Pinard   http://pinard.progiciels-bpi.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From roger.bos at gmail.com  Mon Jan  9 14:52:19 2006
From: roger.bos at gmail.com (roger bos)
Date: Mon, 9 Jan 2006 08:52:19 -0500
Subject: [R] [R-pkgs] sudoku
In-Reply-To: <17346.8601.975737.17359@stat.math.ethz.ch>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>
	<20060108122152.7bcc1f6f.detlef.steuer@hsu-hamburg.de>
	<17346.8601.975737.17359@stat.math.ethz.ch>
Message-ID: <1db726800601090552k6d34d2d5l59cf03ba515aa480@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/db1b8b21/attachment.pl

From f.harrell at vanderbilt.edu  Mon Jan  9 15:03:19 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 09 Jan 2006 08:03:19 -0600
Subject: [R] wicked wikis for R
In-Reply-To: <11544d000601090516g3ccd6a9dg@mail.gmail.com>
References: <11544d000601090516g3ccd6a9dg@mail.gmail.com>
Message-ID: <43C26D27.3000202@vanderbilt.edu>

Jean-Christophe BOUETTE wrote:
>>From: "Arin Basu" <dataanalytics at rediffmail.com>
>>To: r-help at stat.math.ethz.ch
>>Date: 8 Jan 2006 19:18:17 -0000
>>Subject: [R] wicked wikis for R
>>
>>>Message: 41
>>>Date: Sun, 08 Jan 2006 13:52:33 +1100
>>>From: paul sorenson <sourceforge at metrak.com>
>>>Subject: Re: [R] Wikis etc.
>>>To: Frank E Harrell Jr <f.harrell at vanderbilt.edu>,     r-help
>>>      <r-help at stat.math.ethz.ch>
> 
> 
> -snip-
> 
> 
>>Among others, here's one long-term benefit for the newbies. Instead of people getting admonished/thrashed with harsh expressions/advices like "go see the mailing list publishing etiquettes", or "you should search the archives and help files, and read all manuals, and ask others first before posting here..." (which can turn away many a newcomer from posting or using the mailing list or using R for that matter), wiki could make life a little easy for newbies/less experienced who could then receive more polite one liners like, "please check the wikipages...", or "solution #xyz in the wikipages for the solution".
> 
> 
> Sorry, I don't get the point here. Some people will keep feeling
> offensed when they're just told to read the man/wiki pages, and others
> will simply change their answers from RTFM to RTFW.
> Nobody can force people into reading the manuals, or reading the
> posting guide. This is definitely one problem that the wiki will not
> solve.
> 
> I like the idea of the wiki, but we have to consider pragmatically,
> not as a panacea from problems beyond its scope.

That would be an excellent comment if people seeking help were paying 
for a service.  But they are not.

Frank

> 
> Regards,
> Jean-Christophe Bou??tt??.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From leog at anicca-vijja.de  Mon Jan  9 14:59:06 2006
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Mon, 09 Jan 2006 14:59:06 +0100
Subject: [R] decide between polynomial vs ordered factor model (lme)
Message-ID: <43C26C2A.20804@anicca-vijja.de>

Dear alltogether,

two lme's, the data are available at:

http://www.anicca-vijja.de/lg/hlm3_nachw.Rdata

explanations of the data:

nachw = post hox knowledge tests over 6 measure time points (= equally 
spaced)
zeitn = time points (n = 6)
subgr = small learning groups (n = 28)
gru = 4 different groups = treatment factor

levels: time (=zeitn) (n=6) within subject (n=4) within smallgroups 
(=gru) (n = 28), i.e. n = 4 * 28 = 112 persons and 112 * 6 = 672 data points

library(nlme)
fitlme7 <- lme(nachw ~ I(zeitn-3.5) + I((zeitn-3.5)^2) +
I((zeitn-3.5)^3) + I((zeitn-3.5)^4)*gru, random = list(subgr = ~ 1,
subject = ~ zeitn), data = hlm3)

fit5 <- lme(nachw ~ ordered(I(zeitn-3.5))*gru, random = list(subgr =
~ 1, subject = ~ zeitn), data = hlm3)

anova( update(fit5, method="ML"), update(fitlme7, method="ML") )

 > anova( update(fit5, method="ML"), update(fitlme7, method="ML") )
                                Model df      AIC      BIC    logLik   Test
update(fit5, method = "ML")        1 29 2535.821 2666.619 -1238.911
update(fitlme7, method = "ML")     2 16 2529.719 2601.883 -1248.860 1 vs 2
                                 L.Ratio p-value
update(fit5, method = "ML")
update(fitlme7, method = "ML") 19.89766  0.0978
 >

shows that both are ~ equal, although I know about the uncertainty of ML 
tests with lme(). Both models show that the ^2 and the ^4 terms are 
important parts of the model.

My question is:

- Is it legitim to choose a model based on these outputs according to 
theoretical considerations instead of statistical tests that not really 
show a superiority of one model over the other one?

- Is there another criterium I've overseen to decide which model can be 
clearly prefered?

- The idea behind that is that in the one model (fit5) the second 
contrast of the factor (gru) is statistically significant, although not 
the whole factor in the anova output.
In the other model, this is not the case.
Theoretically interesting is of course the significance of the second 
contrast of gru, as it shows a tendency of one treatment being slightly 
superior. I want to choose this model but I am not sure whether this is 
proper action. Both models shows this trend, but only one model clearly 
indicates that this trend bears some empirical meaning.

Thanks for any suggestions,

leo


here are the outputs for each model:

> fitlme7 <- lme(nachw ~ I(zeitn-3.5) + I((zeitn-3.5)^2) + 
I((zeitn-3.5)^3) + I((zeitn-3.5)^4)*gru, random = list(subgr = ~ 1,
subject = ~ zeitn), data = hlm3)
> plot(augPred(fitlme7), layout=c(14,8))
> summary(fitlme7); anova(fitlme7); intervals(fitlme7)
Linear mixed-effects model fit by REML
Data: hlm3
       AIC      BIC    logLik
  2582.934 2654.834 -1275.467

Random effects:
Formula: ~1 | subgr
        (Intercept)
StdDev:   0.5833797

Formula: ~zeitn | subject %in% subgr
Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr
(Intercept) 0.6881908 (Intr)
zeitn       0.1936087 -0.055
Residual    1.3495785

Fixed effects: nachw ~ I(zeitn - 3.5) + I((zeitn - 3.5)^2) + I((zeitn -
3.5)^3) +      I((zeitn - 3.5)^4) * gru
                            Value  Std.Error  DF   t-value p-value
(Intercept)              4.528757 0.17749012 553 25.515542  0.0000
I(zeitn - 3.5)           0.010602 0.08754449 553  0.121100  0.9037
I((zeitn - 3.5)^2)       0.815693 0.09765075 553  8.353171  0.0000
I((zeitn - 3.5)^3)       0.001336 0.01584169 553  0.084329  0.9328
I((zeitn - 3.5)^4)      -0.089655 0.01405811 553 -6.377486  0.0000
gru1                     0.187181 0.30805090  24  0.607630  0.5491
gru2                     0.532665 0.30805090  24  1.729147  0.0966
gru3                    -0.046305 0.30805090  24 -0.150317  0.8818
I((zeitn - 3.5)^4):gru1 -0.007860 0.00600928 553 -1.307993  0.1914
I((zeitn - 3.5)^4):gru2 -0.001259 0.00600928 553 -0.209516  0.8341
I((zeitn - 3.5)^4):gru3 -0.000224 0.00600928 553 -0.037225  0.9703
Correlation:
                        (Intr) I(-3.5 I((-3.5)^2 I((-3.5)^3 I((z-3.5)^4)
I(zeitn - 3.5)           0.071
I((zeitn - 3.5)^2)      -0.465  0.000
I((zeitn - 3.5)^3)       0.000 -0.914  0.000
I((zeitn - 3.5)^4)       0.401  0.000 -0.977      0.000
gru1                     0.000  0.000  0.000      0.000      0.000
gru2                     0.000  0.000  0.000      0.000      0.000
gru3                     0.000  0.000  0.000      0.000      0.000
I((zeitn - 3.5)^4):gru1  0.000  0.000  0.000      0.000      0.000
I((zeitn - 3.5)^4):gru2  0.000  0.000  0.000      0.000      0.000
I((zeitn - 3.5)^4):gru3  0.000  0.000  0.000      0.000      0.000
                        gru1   gru2   gru3   I((-3.5)^4):1 I((-3.5)^4):2
I(zeitn - 3.5)
I((zeitn - 3.5)^2)
I((zeitn - 3.5)^3)
I((zeitn - 3.5)^4)
gru1
gru2                     0.000
gru3                     0.000  0.000
I((zeitn - 3.5)^4):gru1 -0.287  0.000  0.000
I((zeitn - 3.5)^4):gru2  0.000 -0.287  0.000  0.000
I((zeitn - 3.5)^4):gru3  0.000  0.000 -0.287  0.000         0.000

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-3.1326192 -0.5888543  0.0239228  0.6519002  2.1238820

Number of Observations: 672
Number of Groups:
             subgr subject %in% subgr
                28                112
                       numDF denDF   F-value p-value
(Intercept)                1   553 1426.5275  <.0001
I(zeitn - 3.5)             1   553    0.2381  0.6258
I((zeitn - 3.5)^2)         1   553   98.6712  <.0001
I((zeitn - 3.5)^3)         1   553    0.0071  0.9328
I((zeitn - 3.5)^4)         1   553   40.6723  <.0001
gru                        3    24    1.0410  0.3924
I((zeitn - 3.5)^4):gru     3   553    0.5854  0.6248
Approximate 95% confidence intervals

Fixed effects:
                              lower          est.        upper
(Intercept)              4.18011938  4.5287566579  4.877393940
I(zeitn - 3.5)          -0.16135875  0.0106016498  0.182562052
I((zeitn - 3.5)^2)       0.62388162  0.8156933820  1.007505144
I((zeitn - 3.5)^3)      -0.02978133  0.0013359218  0.032453178
I((zeitn - 3.5)^4)      -0.11726922 -0.0896553959 -0.062041570
gru1                    -0.44860499  0.1871808283  0.822966643
gru2                    -0.10312045  0.5326653686  1.168451183
gru3                    -0.68209096 -0.0463051419  0.589480673
I((zeitn - 3.5)^4):gru1 -0.01966389 -0.0078600880  0.003943709
I((zeitn - 3.5)^4):gru2 -0.01306284 -0.0012590380  0.010544759
I((zeitn - 3.5)^4):gru3 -0.01202749 -0.0002236923  0.011580105
attr(,"label")
[1] "Fixed effects:"

Random Effects:
  Level: subgr
                    lower      est.     upper
sd((Intercept)) 0.3459779 0.5833797 0.9836812
  Level: subject
                            lower        est.     upper
sd((Intercept))         0.4388885  0.68819079 1.0791046
sd(zeitn)               0.1320591  0.19360866 0.2838449
cor((Intercept),zeitn) -0.4835884 -0.05541043 0.3941661

Within-group standard error:
   lower     est.    upper
1.267548 1.349579 1.436918

#########################################################
an the other model:

> summary(fit5); anova(fit5); intervals(fit5)
Linear mixed-effects model fit by REML
Data: hlm3
       AIC      BIC    logLik
  2564.135 2693.878 -1253.067

Random effects:
Formula: ~1 | subgr
        (Intercept)
StdDev:   0.5833753

Formula: ~zeitn | subject %in% subgr
Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr
(Intercept) 0.6453960 (Intr)
zeitn       0.1709843 0.13
Residual    1.3497627

Fixed effects: nachw ~ ordered(I(zeitn - 3.5)) + gru + ordered(I(zeitn -
3.5)):gru
                                   Value Std.Error  DF  t-value p-value
(Intercept)                     5.587313 0.1505852 540 37.10400  0.0000
ordered(I(zeitn - 3.5)).L       0.072572 0.1443422 540  0.50278  0.6153
ordered(I(zeitn - 3.5)).Q       1.266731 0.1275406 540  9.93198  0.0000
ordered(I(zeitn - 3.5)).C       0.010754 0.1275406 540  0.08432  0.9328
ordered(I(zeitn - 3.5))^4      -0.813277 0.1275406 540 -6.37662  0.0000
ordered(I(zeitn - 3.5))^5       0.070373 0.1275406 540  0.55177  0.5813
gru1                            0.056700 0.3011704  24  0.18826  0.8523
gru2                            0.679057 0.3011704  24  2.25473  0.0335
gru3                           -0.141425 0.3011704  24 -0.46958  0.6429
ordered(I(zeitn - 3.5)).L:gru1 -0.070352 0.2886844 540 -0.24370  0.8076
ordered(I(zeitn - 3.5)).Q:gru1 -0.360380 0.2550812 540 -1.41281  0.1583
ordered(I(zeitn - 3.5)).C:gru1 -0.162411 0.2550812 540 -0.63670  0.5246
ordered(I(zeitn - 3.5))^4:gru1  0.086343 0.2550812 540  0.33849  0.7351
ordered(I(zeitn - 3.5))^5:gru1 -0.017207 0.2550812 540 -0.06746  0.9462
ordered(I(zeitn - 3.5)).L:gru2  0.788896 0.2886844 540  2.73273  0.0065
ordered(I(zeitn - 3.5)).Q:gru2  0.033386 0.2550812 540  0.13089  0.8959
ordered(I(zeitn - 3.5)).C:gru2  0.089757 0.2550812 540  0.35188  0.7251
ordered(I(zeitn - 3.5))^4:gru2 -0.402616 0.2550812 540 -1.57839  0.1151
ordered(I(zeitn - 3.5))^5:gru2 -0.507855 0.2550812 540 -1.99095  0.0470
ordered(I(zeitn - 3.5)).L:gru3 -0.439200 0.2886844 540 -1.52138  0.1287
ordered(I(zeitn - 3.5)).Q:gru3  0.026105 0.2550812 540  0.10234  0.9185
ordered(I(zeitn - 3.5)).C:gru3 -0.273643 0.2550812 540 -1.07277  0.2839
ordered(I(zeitn - 3.5))^4:gru3 -0.163738 0.2550812 540 -0.64191  0.5212
ordered(I(zeitn - 3.5))^5:gru3  0.204174 0.2550812 540  0.80043  0.4238
Correlation:
                               (Intr) or(I(-3.5)).L or(I(-3.5)).Q
or(I(-3.5)).C or(I(-3.5))^4 or(I(-3.5))^5 gru1 gru2 gru3 o(I(-3.5)).L:1
o(I(-3.5)).Q:1 o(I(-3.5)).C:1
ordered(I(zeitn - 3.5)).L
0.2 


ordered(I(zeitn - 3.5)).Q      0.0
0.0 


ordered(I(zeitn - 3.5)).C      0.0    0.0
0.0 


ordered(I(zeitn - 3.5))^4      0.0    0.0           0.0
0.0 


ordered(I(zeitn - 3.5))^5      0.0    0.0           0.0
0.0
0.0 


gru1                           0.0    0.0           0.0
0.0           0.0
0.0
gru2                           0.0    0.0           0.0
0.0           0.0           0.0
0.0
gru3                           0.0    0.0           0.0
0.0           0.0           0.0           0.0
0.0
ordered(I(zeitn - 3.5)).L:gru1 0.0    0.0           0.0
0.0           0.0           0.0           0.2  0.0
0.0
ordered(I(zeitn - 3.5)).Q:gru1 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0
0.0
ordered(I(zeitn - 3.5)).C:gru1 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0
ordered(I(zeitn - 3.5))^4:gru1 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5))^5:gru1 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).L:gru2 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.2  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).Q:gru2 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).C:gru2 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5))^4:gru2 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5))^5:gru2 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).L:gru3 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.2  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).Q:gru3 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5)).C:gru3 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5))^4:gru3 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
ordered(I(zeitn - 3.5))^5:gru3 0.0    0.0           0.0
0.0           0.0           0.0           0.0  0.0  0.0  0.0
0.0            0.0
                               o(I(-3.5))^4:1 o(I(-3.5))^5:1
o(I(-3.5)).L:2 o(I(-3.5)).Q:2 o(I(-3.5)).C:2 o(I(-3.5))^4:2
o(I(-3.5))^5:2 o(I(-3.5)).L:3 o(I(-3.5)).Q:3
ordered(I(zeitn -
3.5)).L 


ordered(I(zeitn -
3.5)).Q 


ordered(I(zeitn -
3.5)).C 


ordered(I(zeitn -
3.5))^4 


ordered(I(zeitn -
3.5))^5 


gru1 
 


gru2 
 


gru3 
 


ordered(I(zeitn -
3.5)).L:gru1 


ordered(I(zeitn -
3.5)).Q:gru1 


ordered(I(zeitn -
3.5)).C:gru1 


ordered(I(zeitn -
3.5))^4:gru1 


ordered(I(zeitn - 3.5))^5:gru1
0.0 


ordered(I(zeitn - 3.5)).L:gru2 0.0
0.0 


ordered(I(zeitn - 3.5)).Q:gru2 0.0            0.0
0.0 


ordered(I(zeitn - 3.5)).C:gru2 0.0            0.0
0.0
0.0 


ordered(I(zeitn - 3.5))^4:gru2 0.0            0.0
0.0            0.0
0.0
ordered(I(zeitn - 3.5))^5:gru2 0.0            0.0
0.0            0.0            0.0
0.0
ordered(I(zeitn - 3.5)).L:gru3 0.0            0.0
0.0            0.0            0.0            0.0
0.0
ordered(I(zeitn - 3.5)).Q:gru3 0.0            0.0
0.0            0.0            0.0            0.0
0.0            0.0
ordered(I(zeitn - 3.5)).C:gru3 0.0            0.0
0.0            0.0            0.0            0.0
0.0            0.0            0.0
ordered(I(zeitn - 3.5))^4:gru3 0.0            0.0
0.0            0.0            0.0            0.0
0.0            0.0            0.0
ordered(I(zeitn - 3.5))^5:gru3 0.0            0.0
0.0            0.0            0.0            0.0
0.0            0.0            0.0
                               o(I(-3.5)).C:3 o(I(-3.5))^4:3
ordered(I(zeitn - 3.5)).L
ordered(I(zeitn - 3.5)).Q
ordered(I(zeitn - 3.5)).C
ordered(I(zeitn - 3.5))^4
ordered(I(zeitn - 3.5))^5
gru1
gru2
gru3
ordered(I(zeitn - 3.5)).L:gru1
ordered(I(zeitn - 3.5)).Q:gru1
ordered(I(zeitn - 3.5)).C:gru1
ordered(I(zeitn - 3.5))^4:gru1
ordered(I(zeitn - 3.5))^5:gru1
ordered(I(zeitn - 3.5)).L:gru2
ordered(I(zeitn - 3.5)).Q:gru2
ordered(I(zeitn - 3.5)).C:gru2
ordered(I(zeitn - 3.5))^4:gru2
ordered(I(zeitn - 3.5))^5:gru2
ordered(I(zeitn - 3.5)).L:gru3
ordered(I(zeitn - 3.5)).Q:gru3
ordered(I(zeitn - 3.5)).C:gru3
ordered(I(zeitn - 3.5))^4:gru3 0.0
ordered(I(zeitn - 3.5))^5:gru3 0.0            0.0

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-3.10206117 -0.62626454  0.02807962  0.64554138  2.13155536

Number of Observations: 672
Number of Groups:
             subgr subject %in% subgr
                28                112
                            numDF denDF   F-value p-value
(Intercept)                     1   540 1426.5315  <.0001
ordered(I(zeitn - 3.5))         5   540   27.9740  <.0001
gru                             3    24    1.0410  0.3924
ordered(I(zeitn - 3.5)):gru    15   540    1.4115  0.1363
Approximate 95% confidence intervals

Fixed effects:
                                    lower        est.        upper
(Intercept)                     5.2915086  5.58731309  5.883117621
ordered(I(zeitn - 3.5)).L      -0.2109689  0.07257212  0.356113124
ordered(I(zeitn - 3.5)).Q       1.0161942  1.26673073  1.517267227
ordered(I(zeitn - 3.5)).C      -0.2397825  0.01075396  0.261290456
ordered(I(zeitn - 3.5))^4      -1.0638138 -0.81327731 -0.562740815
ordered(I(zeitn - 3.5))^5      -0.1801634  0.07037312  0.320909612
gru1                           -0.5648856  0.05669953  0.678284624
gru2                            0.0574723  0.67905739  1.300642487
gru3                           -0.7630097 -0.14142458  0.480160517
ordered(I(zeitn - 3.5)).L:gru1 -0.6374343 -0.07035232  0.496729683
ordered(I(zeitn - 3.5)).Q:gru1 -0.8614532 -0.36038020  0.140692783
ordered(I(zeitn - 3.5)).C:gru1 -0.6634839 -0.16241093  0.338662057
ordered(I(zeitn - 3.5))^4:gru1 -0.4147301  0.08634286  0.587415843
ordered(I(zeitn - 3.5))^5:gru1 -0.5182803 -0.01720729  0.483865692
ordered(I(zeitn - 3.5)).L:gru2  0.2218139  0.78889594  1.355977946
ordered(I(zeitn - 3.5)).Q:gru2 -0.4676866  0.03338637  0.534459352
ordered(I(zeitn - 3.5)).C:gru2 -0.4113159  0.08975711  0.590830099
ordered(I(zeitn - 3.5))^4:gru2 -0.9036894 -0.40261640  0.098456584
ordered(I(zeitn - 3.5))^5:gru2 -1.0089275 -0.50785453 -0.006781542
ordered(I(zeitn - 3.5)).L:gru3 -1.0062815 -0.43919953  0.127882479
ordered(I(zeitn - 3.5)).Q:gru3 -0.4749680  0.02610502  0.527178001
ordered(I(zeitn - 3.5)).C:gru3 -0.7747163 -0.27364336  0.227429629
ordered(I(zeitn - 3.5))^4:gru3 -0.6648114 -0.16373838  0.337334604
ordered(I(zeitn - 3.5))^5:gru3 -0.2968991  0.20417390  0.705246883
attr(,"label")
[1] "Fixed effects:"

Random Effects:
  Level: subgr
                    lower      est.     upper
sd((Intercept)) 0.3464888 0.5833753 0.9822158
  Level: subject
                            lower      est.     upper
sd((Intercept))         0.3640439 0.6453960 1.1441916
sd(zeitn)               0.1000264 0.1709843 0.2922790
cor((Intercept),zeitn) -0.6712236 0.1295558 0.7907922

Within-group standard error:
   lower     est.    upper
1.265702 1.349763 1.439406



From liujcheng at gmail.com  Mon Jan  9 15:01:25 2006
From: liujcheng at gmail.com (=?GB2312?B?wfW807PJ?=)
Date: Mon, 9 Jan 2006 22:01:25 +0800
Subject: [R] how to run the data editor by command?Thanks!
Message-ID: <32f489f80601090601ke47c22t@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/88db1fa9/attachment.pl

From ronggui.huang at gmail.com  Mon Jan  9 15:10:37 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 9 Jan 2006 22:10:37 +0800
Subject: [R] how to run the data editor by command?Thanks!
In-Reply-To: <32f489f80601090601ke47c22t@mail.gmail.com>
References: <32f489f80601090601ke47c22t@mail.gmail.com>
Message-ID: <38b9f0350601090610w60953196q@mail.gmail.com>

?fix
?edit

2006/1/9,  <liujcheng at gmail.com>:
> Thanks!
>
> --
> Liu jcheng
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From Jan.Verbesselt at biw.kuleuven.be  Mon Jan  9 15:28:16 2006
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Mon, 09 Jan 2006 15:28:16 +0100
Subject: [R] How to use filled.contour(x,y,z) data for levelplot(z)?
Message-ID: <1136816896.43c273004463a@webmail1.kuleuven.be>

Dear all,

We used the following function to create a spatial plot of a raster image:

filled.contour(xx,yy,zz, color = terrain.colors, nlevels=10,
main=naamjaar, plot.axes = { contour(Xcoord/1000,Ycoord/1000,lim.data,
nlevels = 4, col=4,drawlabels = T, axes = FALSE, frame.plot = FFALSE,
add = TRUE);axis(1); axis(2);                        
points(fire[,2:3]/1000,col="red",pch="*",cex=2,lwd=3)
                                })

we however would like to create 5 maps with only one legend. This seems
not to be possible with filled.contour follow the help().

We therefore started looking at levelplot(zz) to create a multipanel view.  

*However how could we use x and y coordinates in levelplot without using
a formula (since x,y are only the coordinates of the matrix z)?

libary(lattice)
rownames(zz) <- xx
colnames(zz) <- yy
levelplot(zz)

like this the all the row and col names are displayed in x and y axis
without summarizing data similar to filled.contour function?

* or can this be done with an upgrade of filled.contour?

Thanks,
Jan

windows R 2.2


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From dimitris.rizopoulos at med.kuleuven.be  Mon Jan  9 15:28:53 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 9 Jan 2006 15:28:53 +0100
Subject: [R] decide between polynomial vs ordered factor model (lme)
References: <43C26C2A.20804@anicca-vijja.de>
Message-ID: <006d01c61529$04686230$0540210a@www.domain>

I think that these models are not nested and thus the LRT produced by 
anova.lme() will not be valid; AIC and BIC could be more relevant. In 
terms of interpretability, I'd say that a model treating 'zeitn' as a 
factor is much easier to explain than a model with 4th order 
polynomial.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Leo G??rtler" <leog at anicca-vijja.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, January 09, 2006 2:59 PM
Subject: [R] decide between polynomial vs ordered factor model (lme)


> Dear alltogether,
>
> two lme's, the data are available at:
>
> http://www.anicca-vijja.de/lg/hlm3_nachw.Rdata
>
> explanations of the data:
>
> nachw = post hox knowledge tests over 6 measure time points (= 
> equally
> spaced)
> zeitn = time points (n = 6)
> subgr = small learning groups (n = 28)
> gru = 4 different groups = treatment factor
>
> levels: time (=zeitn) (n=6) within subject (n=4) within smallgroups
> (=gru) (n = 28), i.e. n = 4 * 28 = 112 persons and 112 * 6 = 672 
> data points
>
> library(nlme)
> fitlme7 <- lme(nachw ~ I(zeitn-3.5) + I((zeitn-3.5)^2) +
> I((zeitn-3.5)^3) + I((zeitn-3.5)^4)*gru, random = list(subgr = ~ 1,
> subject = ~ zeitn), data = hlm3)
>
> fit5 <- lme(nachw ~ ordered(I(zeitn-3.5))*gru, random = list(subgr =
> ~ 1, subject = ~ zeitn), data = hlm3)
>
> anova( update(fit5, method="ML"), update(fitlme7, method="ML") )
>
> > anova( update(fit5, method="ML"), update(fitlme7, method="ML") )
>                                Model df      AIC      BIC    logLik 
> Test
> update(fit5, method = "ML")        1 29 2535.821 2666.619 -1238.911
> update(fitlme7, method = "ML")     2 16 2529.719 2601.883 -1248.860 
> 1 vs 2
>                                 L.Ratio p-value
> update(fit5, method = "ML")
> update(fitlme7, method = "ML") 19.89766  0.0978
> >
>
> shows that both are ~ equal, although I know about the uncertainty 
> of ML
> tests with lme(). Both models show that the ^2 and the ^4 terms are
> important parts of the model.
>
> My question is:
>
> - Is it legitim to choose a model based on these outputs according 
> to
> theoretical considerations instead of statistical tests that not 
> really
> show a superiority of one model over the other one?
>
> - Is there another criterium I've overseen to decide which model can 
> be
> clearly prefered?
>
> - The idea behind that is that in the one model (fit5) the second
> contrast of the factor (gru) is statistically significant, although 
> not
> the whole factor in the anova output.
> In the other model, this is not the case.
> Theoretically interesting is of course the significance of the 
> second
> contrast of gru, as it shows a tendency of one treatment being 
> slightly
> superior. I want to choose this model but I am not sure whether this 
> is
> proper action. Both models shows this trend, but only one model 
> clearly
> indicates that this trend bears some empirical meaning.
>
> Thanks for any suggestions,
>
> leo
>
>
> here are the outputs for each model:
>
>> fitlme7 <- lme(nachw ~ I(zeitn-3.5) + I((zeitn-3.5)^2) +
> I((zeitn-3.5)^3) + I((zeitn-3.5)^4)*gru, random = list(subgr = ~ 1,
> subject = ~ zeitn), data = hlm3)
>> plot(augPred(fitlme7), layout=c(14,8))
>> summary(fitlme7); anova(fitlme7); intervals(fitlme7)
> Linear mixed-effects model fit by REML
> Data: hlm3
>       AIC      BIC    logLik
>  2582.934 2654.834 -1275.467
>
> Random effects:
> Formula: ~1 | subgr
>        (Intercept)
> StdDev:   0.5833797
>
> Formula: ~zeitn | subject %in% subgr
> Structure: General positive-definite, Log-Cholesky parametrization
>            StdDev    Corr
> (Intercept) 0.6881908 (Intr)
> zeitn       0.1936087 -0.055
> Residual    1.3495785
>
> Fixed effects: nachw ~ I(zeitn - 3.5) + I((zeitn - 3.5)^2) + 
> I((zeitn -
> 3.5)^3) +      I((zeitn - 3.5)^4) * gru
>                            Value  Std.Error  DF   t-value p-value
> (Intercept)              4.528757 0.17749012 553 25.515542  0.0000
> I(zeitn - 3.5)           0.010602 0.08754449 553  0.121100  0.9037
> I((zeitn - 3.5)^2)       0.815693 0.09765075 553  8.353171  0.0000
> I((zeitn - 3.5)^3)       0.001336 0.01584169 553  0.084329  0.9328
> I((zeitn - 3.5)^4)      -0.089655 0.01405811 553 -6.377486  0.0000
> gru1                     0.187181 0.30805090  24  0.607630  0.5491
> gru2                     0.532665 0.30805090  24  1.729147  0.0966
> gru3                    -0.046305 0.30805090  24 -0.150317  0.8818
> I((zeitn - 3.5)^4):gru1 -0.007860 0.00600928 553 -1.307993  0.1914
> I((zeitn - 3.5)^4):gru2 -0.001259 0.00600928 553 -0.209516  0.8341
> I((zeitn - 3.5)^4):gru3 -0.000224 0.00600928 553 -0.037225  0.9703
> Correlation:
>                        (Intr) I(-3.5 I((-3.5)^2 I((-3.5)^3 
> I((z-3.5)^4)
> I(zeitn - 3.5)           0.071
> I((zeitn - 3.5)^2)      -0.465  0.000
> I((zeitn - 3.5)^3)       0.000 -0.914  0.000
> I((zeitn - 3.5)^4)       0.401  0.000 -0.977      0.000
> gru1                     0.000  0.000  0.000      0.000      0.000
> gru2                     0.000  0.000  0.000      0.000      0.000
> gru3                     0.000  0.000  0.000      0.000      0.000
> I((zeitn - 3.5)^4):gru1  0.000  0.000  0.000      0.000      0.000
> I((zeitn - 3.5)^4):gru2  0.000  0.000  0.000      0.000      0.000
> I((zeitn - 3.5)^4):gru3  0.000  0.000  0.000      0.000      0.000
>                        gru1   gru2   gru3   I((-3.5)^4):1 
> I((-3.5)^4):2
> I(zeitn - 3.5)
> I((zeitn - 3.5)^2)
> I((zeitn - 3.5)^3)
> I((zeitn - 3.5)^4)
> gru1
> gru2                     0.000
> gru3                     0.000  0.000
> I((zeitn - 3.5)^4):gru1 -0.287  0.000  0.000
> I((zeitn - 3.5)^4):gru2  0.000 -0.287  0.000  0.000
> I((zeitn - 3.5)^4):gru3  0.000  0.000 -0.287  0.000         0.000
>
> Standardized Within-Group Residuals:
>       Min         Q1        Med         Q3        Max
> -3.1326192 -0.5888543  0.0239228  0.6519002  2.1238820
>
> Number of Observations: 672
> Number of Groups:
>             subgr subject %in% subgr
>                28                112
>                       numDF denDF   F-value p-value
> (Intercept)                1   553 1426.5275  <.0001
> I(zeitn - 3.5)             1   553    0.2381  0.6258
> I((zeitn - 3.5)^2)         1   553   98.6712  <.0001
> I((zeitn - 3.5)^3)         1   553    0.0071  0.9328
> I((zeitn - 3.5)^4)         1   553   40.6723  <.0001
> gru                        3    24    1.0410  0.3924
> I((zeitn - 3.5)^4):gru     3   553    0.5854  0.6248
> Approximate 95% confidence intervals
>
> Fixed effects:
>                              lower          est.        upper
> (Intercept)              4.18011938  4.5287566579  4.877393940
> I(zeitn - 3.5)          -0.16135875  0.0106016498  0.182562052
> I((zeitn - 3.5)^2)       0.62388162  0.8156933820  1.007505144
> I((zeitn - 3.5)^3)      -0.02978133  0.0013359218  0.032453178
> I((zeitn - 3.5)^4)      -0.11726922 -0.0896553959 -0.062041570
> gru1                    -0.44860499  0.1871808283  0.822966643
> gru2                    -0.10312045  0.5326653686  1.168451183
> gru3                    -0.68209096 -0.0463051419  0.589480673
> I((zeitn - 3.5)^4):gru1 -0.01966389 -0.0078600880  0.003943709
> I((zeitn - 3.5)^4):gru2 -0.01306284 -0.0012590380  0.010544759
> I((zeitn - 3.5)^4):gru3 -0.01202749 -0.0002236923  0.011580105
> attr(,"label")
> [1] "Fixed effects:"
>
> Random Effects:
>  Level: subgr
>                    lower      est.     upper
> sd((Intercept)) 0.3459779 0.5833797 0.9836812
>  Level: subject
>                            lower        est.     upper
> sd((Intercept))         0.4388885  0.68819079 1.0791046
> sd(zeitn)               0.1320591  0.19360866 0.2838449
> cor((Intercept),zeitn) -0.4835884 -0.05541043 0.3941661
>
> Within-group standard error:
>   lower     est.    upper
> 1.267548 1.349579 1.436918
>
> #########################################################
> an the other model:
>
>> summary(fit5); anova(fit5); intervals(fit5)
> Linear mixed-effects model fit by REML
> Data: hlm3
>       AIC      BIC    logLik
>  2564.135 2693.878 -1253.067
>
> Random effects:
> Formula: ~1 | subgr
>        (Intercept)
> StdDev:   0.5833753
>
> Formula: ~zeitn | subject %in% subgr
> Structure: General positive-definite, Log-Cholesky parametrization
>            StdDev    Corr
> (Intercept) 0.6453960 (Intr)
> zeitn       0.1709843 0.13
> Residual    1.3497627
>
> Fixed effects: nachw ~ ordered(I(zeitn - 3.5)) + gru + 
> ordered(I(zeitn -
> 3.5)):gru
>                                   Value Std.Error  DF  t-value 
> p-value
> (Intercept)                     5.587313 0.1505852 540 37.10400 
> 0.0000
> ordered(I(zeitn - 3.5)).L       0.072572 0.1443422 540  0.50278 
> 0.6153
> ordered(I(zeitn - 3.5)).Q       1.266731 0.1275406 540  9.93198 
> 0.0000
> ordered(I(zeitn - 3.5)).C       0.010754 0.1275406 540  0.08432 
> 0.9328
> ordered(I(zeitn - 3.5))^4      -0.813277 0.1275406 540 -6.37662 
> 0.0000
> ordered(I(zeitn - 3.5))^5       0.070373 0.1275406 540  0.55177 
> 0.5813
> gru1                            0.056700 0.3011704  24  0.18826 
> 0.8523
> gru2                            0.679057 0.3011704  24  2.25473 
> 0.0335
> gru3                           -0.141425 0.3011704  24 -0.46958 
> 0.6429
> ordered(I(zeitn - 3.5)).L:gru1 -0.070352 0.2886844 540 -0.24370 
> 0.8076
> ordered(I(zeitn - 3.5)).Q:gru1 -0.360380 0.2550812 540 -1.41281 
> 0.1583
> ordered(I(zeitn - 3.5)).C:gru1 -0.162411 0.2550812 540 -0.63670 
> 0.5246
> ordered(I(zeitn - 3.5))^4:gru1  0.086343 0.2550812 540  0.33849 
> 0.7351
> ordered(I(zeitn - 3.5))^5:gru1 -0.017207 0.2550812 540 -0.06746 
> 0.9462
> ordered(I(zeitn - 3.5)).L:gru2  0.788896 0.2886844 540  2.73273 
> 0.0065
> ordered(I(zeitn - 3.5)).Q:gru2  0.033386 0.2550812 540  0.13089 
> 0.8959
> ordered(I(zeitn - 3.5)).C:gru2  0.089757 0.2550812 540  0.35188 
> 0.7251
> ordered(I(zeitn - 3.5))^4:gru2 -0.402616 0.2550812 540 -1.57839 
> 0.1151
> ordered(I(zeitn - 3.5))^5:gru2 -0.507855 0.2550812 540 -1.99095 
> 0.0470
> ordered(I(zeitn - 3.5)).L:gru3 -0.439200 0.2886844 540 -1.52138 
> 0.1287
> ordered(I(zeitn - 3.5)).Q:gru3  0.026105 0.2550812 540  0.10234 
> 0.9185
> ordered(I(zeitn - 3.5)).C:gru3 -0.273643 0.2550812 540 -1.07277 
> 0.2839
> ordered(I(zeitn - 3.5))^4:gru3 -0.163738 0.2550812 540 -0.64191 
> 0.5212
> ordered(I(zeitn - 3.5))^5:gru3  0.204174 0.2550812 540  0.80043 
> 0.4238
> Correlation:
>                               (Intr) or(I(-3.5)).L or(I(-3.5)).Q
> or(I(-3.5)).C or(I(-3.5))^4 or(I(-3.5))^5 gru1 gru2 gru3 
> o(I(-3.5)).L:1
> o(I(-3.5)).Q:1 o(I(-3.5)).C:1
> ordered(I(zeitn - 3.5)).L
> 0.2
>
>
> ordered(I(zeitn - 3.5)).Q      0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5)).C      0.0    0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5))^4      0.0    0.0           0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5))^5      0.0    0.0           0.0
> 0.0
> 0.0
>
>
> gru1                           0.0    0.0           0.0
> 0.0           0.0
> 0.0
> gru2                           0.0    0.0           0.0
> 0.0           0.0           0.0
> 0.0
> gru3                           0.0    0.0           0.0
> 0.0           0.0           0.0           0.0
> 0.0
> ordered(I(zeitn - 3.5)).L:gru1 0.0    0.0           0.0
> 0.0           0.0           0.0           0.2  0.0
> 0.0
> ordered(I(zeitn - 3.5)).Q:gru1 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0
> 0.0
> ordered(I(zeitn - 3.5)).C:gru1 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0
> ordered(I(zeitn - 3.5))^4:gru1 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5))^5:gru1 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).L:gru2 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.2  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).Q:gru2 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).C:gru2 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5))^4:gru2 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5))^5:gru2 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).L:gru3 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.2  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).Q:gru3 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).C:gru3 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5))^4:gru3 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5))^5:gru3 0.0    0.0           0.0
> 0.0           0.0           0.0           0.0  0.0  0.0  0.0
> 0.0            0.0
>                               o(I(-3.5))^4:1 o(I(-3.5))^5:1
> o(I(-3.5)).L:2 o(I(-3.5)).Q:2 o(I(-3.5)).C:2 o(I(-3.5))^4:2
> o(I(-3.5))^5:2 o(I(-3.5)).L:3 o(I(-3.5)).Q:3
> ordered(I(zeitn -
> 3.5)).L
>
>
> ordered(I(zeitn -
> 3.5)).Q
>
>
> ordered(I(zeitn -
> 3.5)).C
>
>
> ordered(I(zeitn -
> 3.5))^4
>
>
> ordered(I(zeitn -
> 3.5))^5
>
>
> gru1
>
>
>
> gru2
>
>
>
> gru3
>
>
>
> ordered(I(zeitn -
> 3.5)).L:gru1
>
>
> ordered(I(zeitn -
> 3.5)).Q:gru1
>
>
> ordered(I(zeitn -
> 3.5)).C:gru1
>
>
> ordered(I(zeitn -
> 3.5))^4:gru1
>
>
> ordered(I(zeitn - 3.5))^5:gru1
> 0.0
>
>
> ordered(I(zeitn - 3.5)).L:gru2 0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5)).Q:gru2 0.0            0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5)).C:gru2 0.0            0.0
> 0.0
> 0.0
>
>
> ordered(I(zeitn - 3.5))^4:gru2 0.0            0.0
> 0.0            0.0
> 0.0
> ordered(I(zeitn - 3.5))^5:gru2 0.0            0.0
> 0.0            0.0            0.0
> 0.0
> ordered(I(zeitn - 3.5)).L:gru3 0.0            0.0
> 0.0            0.0            0.0            0.0
> 0.0
> ordered(I(zeitn - 3.5)).Q:gru3 0.0            0.0
> 0.0            0.0            0.0            0.0
> 0.0            0.0
> ordered(I(zeitn - 3.5)).C:gru3 0.0            0.0
> 0.0            0.0            0.0            0.0
> 0.0            0.0            0.0
> ordered(I(zeitn - 3.5))^4:gru3 0.0            0.0
> 0.0            0.0            0.0            0.0
> 0.0            0.0            0.0
> ordered(I(zeitn - 3.5))^5:gru3 0.0            0.0
> 0.0            0.0            0.0            0.0
> 0.0            0.0            0.0
>                               o(I(-3.5)).C:3 o(I(-3.5))^4:3
> ordered(I(zeitn - 3.5)).L
> ordered(I(zeitn - 3.5)).Q
> ordered(I(zeitn - 3.5)).C
> ordered(I(zeitn - 3.5))^4
> ordered(I(zeitn - 3.5))^5
> gru1
> gru2
> gru3
> ordered(I(zeitn - 3.5)).L:gru1
> ordered(I(zeitn - 3.5)).Q:gru1
> ordered(I(zeitn - 3.5)).C:gru1
> ordered(I(zeitn - 3.5))^4:gru1
> ordered(I(zeitn - 3.5))^5:gru1
> ordered(I(zeitn - 3.5)).L:gru2
> ordered(I(zeitn - 3.5)).Q:gru2
> ordered(I(zeitn - 3.5)).C:gru2
> ordered(I(zeitn - 3.5))^4:gru2
> ordered(I(zeitn - 3.5))^5:gru2
> ordered(I(zeitn - 3.5)).L:gru3
> ordered(I(zeitn - 3.5)).Q:gru3
> ordered(I(zeitn - 3.5)).C:gru3
> ordered(I(zeitn - 3.5))^4:gru3 0.0
> ordered(I(zeitn - 3.5))^5:gru3 0.0            0.0
>
> Standardized Within-Group Residuals:
>        Min          Q1         Med          Q3         Max
> -3.10206117 -0.62626454  0.02807962  0.64554138  2.13155536
>
> Number of Observations: 672
> Number of Groups:
>             subgr subject %in% subgr
>                28                112
>                            numDF denDF   F-value p-value
> (Intercept)                     1   540 1426.5315  <.0001
> ordered(I(zeitn - 3.5))         5   540   27.9740  <.0001
> gru                             3    24    1.0410  0.3924
> ordered(I(zeitn - 3.5)):gru    15   540    1.4115  0.1363
> Approximate 95% confidence intervals
>
> Fixed effects:
>                                    lower        est.        upper
> (Intercept)                     5.2915086  5.58731309  5.883117621
> ordered(I(zeitn - 3.5)).L      -0.2109689  0.07257212  0.356113124
> ordered(I(zeitn - 3.5)).Q       1.0161942  1.26673073  1.517267227
> ordered(I(zeitn - 3.5)).C      -0.2397825  0.01075396  0.261290456
> ordered(I(zeitn - 3.5))^4      -1.0638138 -0.81327731 -0.562740815
> ordered(I(zeitn - 3.5))^5      -0.1801634  0.07037312  0.320909612
> gru1                           -0.5648856  0.05669953  0.678284624
> gru2                            0.0574723  0.67905739  1.300642487
> gru3                           -0.7630097 -0.14142458  0.480160517
> ordered(I(zeitn - 3.5)).L:gru1 -0.6374343 -0.07035232  0.496729683
> ordered(I(zeitn - 3.5)).Q:gru1 -0.8614532 -0.36038020  0.140692783
> ordered(I(zeitn - 3.5)).C:gru1 -0.6634839 -0.16241093  0.338662057
> ordered(I(zeitn - 3.5))^4:gru1 -0.4147301  0.08634286  0.587415843
> ordered(I(zeitn - 3.5))^5:gru1 -0.5182803 -0.01720729  0.483865692
> ordered(I(zeitn - 3.5)).L:gru2  0.2218139  0.78889594  1.355977946
> ordered(I(zeitn - 3.5)).Q:gru2 -0.4676866  0.03338637  0.534459352
> ordered(I(zeitn - 3.5)).C:gru2 -0.4113159  0.08975711  0.590830099
> ordered(I(zeitn - 3.5))^4:gru2 -0.9036894 -0.40261640  0.098456584
> ordered(I(zeitn - 3.5))^5:gru2 -1.0089275 -0.50785453 -0.006781542
> ordered(I(zeitn - 3.5)).L:gru3 -1.0062815 -0.43919953  0.127882479
> ordered(I(zeitn - 3.5)).Q:gru3 -0.4749680  0.02610502  0.527178001
> ordered(I(zeitn - 3.5)).C:gru3 -0.7747163 -0.27364336  0.227429629
> ordered(I(zeitn - 3.5))^4:gru3 -0.6648114 -0.16373838  0.337334604
> ordered(I(zeitn - 3.5))^5:gru3 -0.2968991  0.20417390  0.705246883
> attr(,"label")
> [1] "Fixed effects:"
>
> Random Effects:
>  Level: subgr
>                    lower      est.     upper
> sd((Intercept)) 0.3464888 0.5833753 0.9822158
>  Level: subject
>                            lower      est.     upper
> sd((Intercept))         0.3640439 0.6453960 1.1441916
> sd(zeitn)               0.1000264 0.1709843 0.2922790
> cor((Intercept),zeitn) -0.6712236 0.1295558 0.7907922
>
> Within-group standard error:
>   lower     est.    upper
> 1.265702 1.349763 1.439406
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Jan  9 15:43:46 2006
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 9 Jan 2006 09:43:46 -0500 
Subject: [R] need palette of topographic colors similar to topo.colors	()
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD505BAD913@us-arlington-0668.mail.saic.com>



I will second Roger's suggestion, colorRampPalette is a great function for
creating your own palettes. For example, Matlab's jet palette (also
available in fields package under peculiar name 'tim.colors') can be defined
by:

jet.colors = colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan",
"#7FFF7F", "yellow", "#FF7F00", "red", "#7F0000")) 

Other predefined functions for creating and managing color palettes that I
know of, are:
    * R provides functions for creating palettes of continuous colors:
rainbow, topo.colors, heat.colors, terrain.colors.colors, gray
    * tim.colors in package fields contains palette similar to Matlab's jet
palette (see examples for simpler implementation)
    * rich.colors in package gplots contains two palettes of continuous
colors.
    * Functions brewer.pal from RColorBrewer package and colorbrewer.palette
from epitools package contain tools for generating palettes
    * rgb and hsv creates palette from RGB or HSV 3-vectors.

Maybe one of those will work for you.

Jarek

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Saturday, January 07, 2006 3:10 PM
To: bogdan romocea
Cc: r-help
Subject: Re: [R] need palette of topographic colors similar to topo.colors()

On Sat, 7 Jan 2006, bogdan romocea wrote:

> Dear useRs,
> 
> I got stuck trying to generate a palette of topographic colors that
> would satisfy these two requirements:
>    - the pallete must be 'anchored' at 0 (just like on a map), with
> light blue/lawn green corresponding to data values close to 0 (dark
> blue to light blue for negative values, green-yellow-brown for
> positive values)
>    - the brown must get darker for higher positive values.
> 
> topo.colors() fails both requirements and AFAICS lacks any options to
> control its behavior.
>   #---unsatisfactory topo.colors() behavior
>   topoclr <- function(tgt)
>   {
>   clr <- topo.colors(length(tgt))
>   clr <- clr[round(rank(tgt),0)]
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; topoclr(-50:50) ; topoclr(-20:80)
> 
> An acceptable solution would be something like this
>   grayclr <- function(tgt)
>   {
>   tgt <- sort(tgt) ; neg <- which(tgt < 0)
>   clrneg <- gray(0:length(tgt[neg])/length(tgt[neg]))
>   clrpos <- gray(length(tgt[-neg]):0/length(tgt[-neg]))
>   clr <- c(clrneg,clrpos)
>   plot(tgt,pch=15,col=clr)
>   }
>   par(mfrow=c(2,1)) ; grayclr(-50:50) ; grayclr(-20:80)
> if only I could make gray() use blue/brown instead of black (I tried a
> couple of things but got stuck again).
> 
> Any suggestions?

Use colorRampPalette() to roll your own, or something better tuned, 
perhaps catenating two ramps together.

> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Jan  9 16:06:40 2006
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 9 Jan 2006 10:06:40 -0500 
Subject: [R] Looking for packages to do Feature Selection and
	Classifi	cation
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD505BAD940@us-arlington-0668.mail.saic.com>

Hi,

You should also check my msc.features.select from caMassClass package. It
has feature selection algorithm that I found useful in case of mass-spectra
data. It performs individual feature selection and/or removes highly
correlated neighbor features. 

Jarek

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] 
Sent: Friday, January 06, 2006 3:12 PM
To: Weiwei Shi
Cc: Diaz.Ramon; r-help
Subject: Re: [R] Looking for packages to do Feature Selection and
Classification

Thanks. It's indeed an interesting paper. Besides RF (using Ramon's varSelRF
package), I am also testing Guyon et al's (2002) Recursive Feature
Elimination for my feature-selection part.

On 1/5/06, Weiwei Shi <helprhelp at gmail.com> wrote:
>
> FYI:
>
> check the following paper on svm (using libsvm) as well as random
> forest in the context of feature selection.
>
> http://www.csie.ntu.edu.tw/~cjlin/papers/features.pdf
>
> HTH
>
> On 1/4/06, Diaz.Ramon <rdiaz at cnio.es> wrote:
> > Dear Frank,
> > I expect you'll get many different answers since a wide variety of
> approaches have been suggested. So I'll stick to self-advertisment: I've
> written an R package, varSelRF (available from R), that uses random forest
> together with a simple variable selection approach, and provides also
> bootstrap estimates of the error rate of the procedure. Andy Liaw and
> collaborators previously developed and published a somewhat similar
> procedure. You probably also want to take a look at several packages
> available from BioConductor.
> >
> > Best,
> >
> > R.
> >
> >
> > -----Original Message-----
> > From:   r-help-bounces at stat.math.ethz.ch on behalf of Frank Duan
> > Sent:   Wed 1/4/2006 4:23 AM
> > To:     r-help
> > Cc:
> > Subject:        [R] Looking for packages to do Feature Selection and
> Classification
> >
> > Hi All,
> >
> > Sorry if this is a repost (a quick browse didn't give me the answer).
> >
> > I wonder if there are packages that can do the feature selection and
> > classification at the same time. For instance, I am using SVM to
> classify my
> > samples, but it's easy to get overfitted if using all of the features.
> Thus,
> > it is necessary to select "good" features to build an optimum hyperplane
> > (?). Here is a simple example: Suppose I have 100 "useful" features and
> 100
> > "useless" features (or noise features), I want the SVM to give me the
> > same results when 1) using only 100 useful features or 2) using all 200
> > features.
> >
> > Any suggestions or point me to a reference?
> >
> > Thanks in advance!
> >
> > Frank
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> > --
> > Ram??n D??az-Uriarte
> > Bioinformatics Unit
> > Centro Nacional de Investigaciones Oncol??gicas (CNIO)
> > (Spanish National Cancer Center)
> > Melchor Fern??ndez Almagro, 3
> > 28029 Madrid (Spain)
> > Fax: +-34-91-224-6972
> > Phone: +-34-91-224-6900
> >
> > http://ligarto.org/rdiaz
> > PGP KeyID: 0xE89B3462
> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> >
> >
> >
> > **NOTA DE CONFIDENCIALIDAD** Este correo electr??nico, y en
> s...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
> --
> Weiwei Shi, Ph.D
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>

	[[alternative HTML version deleted]]



From rand at amgen.com  Mon Jan  9 16:37:51 2006
From: rand at amgen.com (Rand, Hugh)
Date: Mon, 9 Jan 2006 07:37:51 -0800 
Subject: [R] trouble with extraction/interpretation of variance structure
	para	meters from a model built using gnls and varConstPower
Message-ID: <567ACB2E39C83543B746F1AD7F5E5E04062E537F@wa-mb2-sea.amgen.com>

I have been using gnls with the weights argument (and varConstPower) to
specify a variance structure for curve fits. In attempting to extract the
parameters for the variance model I am seeing results I don't understand.
When I simply display the model (or use "summary" on the model), I get what
seem like reasonable values for both "power" and "const". When I actually
try to extract the values, I get the same number for the "power", but a
different (and less sensible) value for "const".

The simplest example I can come up with that shows the problem is as
follows: 


#Set up data

x    = c(0,0,0,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7);
y0   = c(.1,.1,.1, .5,.5,.5, 1,1,1, 2,2,2, 4,4,4, 7,7,7, 9,9,9, 10,10,10);
yp   = c(0,.03,.05, 0,.05,.01, 0,.07,.03, .1,0,.2, .2,.1,0, .3,0,.1,
0,.3,.4, .3,.5,0);
y    = y0 + 4*yp;
data = data.frame(x=x,y=y);

#Run model

library(nlme)
model3 = try(gnls(y ~
SSfpl(x,A,B,xmid,scal),data=data,weights=varConstPower(const=1,power=0,form=
~fitted(.))));

#Examine results

model3;                                         #const = .6551298,   power =
.8913665
summary(model3);                                #const = .6551298,   power =
.8913665               

coef(model3$modelStruct$varStruct)["power"];    #                    power =
.8913665
coef(model3$modelStruct$varStruct)["const"];    #const = -0.4229219 
coef.varFunc(model3$modelStruct$varStruct);     #R can't seem to find this
function, Splus can


Any advice on what I am doing wrong would be appreciated.

Hugh Rand
Senior Scientist
Amgen
rand at amgen.com



From tlumley at u.washington.edu  Mon Jan  9 16:46:18 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Jan 2006 07:46:18 -0800 (PST)
Subject: [R] Wikis etc.
In-Reply-To: <6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
References: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
	<6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
Message-ID: <Pine.LNX.4.64.0601090718060.13171@homer24.u.washington.edu>

On Mon, 9 Jan 2006, Michael Dewey wrote:
>
> Further to that I feel that (perhaps because they do not like to blow their
> own trumpet too much) the authors of books on R do not stress how much most
> questioners could gain by buying and reading at least one of the many books
> on R. When I started I found the free documents useful but I made most
> progress when I bought MASS. I do realise that liking books is a bit last
> millennium.
>

Very late last millenium, though.
"When I were young[er] we didn't have all these fancy yellow books."

More seriously, yes, reading books about R and S is very effective and is 
how most of the R experts learned.  In my case it was the Blue Book, the 
White Book, and the Ripley/Venables/Smith notes on S-plus (which have 
evolved to the Introduction to R).

 	-thomas

"Ptolemy once asked [Euclid] if there was in geometry any shorter way that 
that of the Elements, and he replied that there was no royal road to 
geometry." Proclus (410-485 CE)



From dmbates at gmail.com  Mon Jan  9 16:59:24 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 9 Jan 2006 09:59:24 -0600
Subject: [R] decide between polynomial vs ordered factor model (lme)
In-Reply-To: <43C26C2A.20804@anicca-vijja.de>
References: <43C26C2A.20804@anicca-vijja.de>
Message-ID: <40e66e0b0601090759r45d8b154y50065d0e960c2ab@mail.gmail.com>

On 1/9/06, Leo G??rtler <leog at anicca-vijja.de> wrote:
> Dear alltogether,
>
> two lme's, the data are available at:
>
> http://www.anicca-vijja.de/lg/hlm3_nachw.Rdata
>
> explanations of the data:
>
> nachw = post hox knowledge tests over 6 measure time points (= equally
> spaced)
> zeitn = time points (n = 6)
> subgr = small learning groups (n = 28)
> gru = 4 different groups = treatment factor
>
> levels: time (=zeitn) (n=6) within subject (n=4) within smallgroups
> (=gru) (n = 28), i.e. n = 4 * 28 = 112 persons and 112 * 6 = 672 data points
>
> library(nlme)
> fitlme7 <- lme(nachw ~ I(zeitn-3.5) + I((zeitn-3.5)^2) +
> I((zeitn-3.5)^3) + I((zeitn-3.5)^4)*gru, random = list(subgr = ~ 1,
> subject = ~ zeitn), data = hlm3)
>
> fit5 <- lme(nachw ~ ordered(I(zeitn-3.5))*gru, random = list(subgr =
> ~ 1, subject = ~ zeitn), data = hlm3)
>
> anova( update(fit5, method="ML"), update(fitlme7, method="ML") )
>
>  > anova( update(fit5, method="ML"), update(fitlme7, method="ML") )
>                                 Model df      AIC      BIC    logLik   Test
> update(fit5, method = "ML")        1 29 2535.821 2666.619 -1238.911
> update(fitlme7, method = "ML")     2 16 2529.719 2601.883 -1248.860 1 vs 2
>                                  L.Ratio p-value
> update(fit5, method = "ML")
> update(fitlme7, method = "ML") 19.89766  0.0978
>  >
>
> shows that both are ~ equal, although I know about the uncertainty of ML
> tests with lme(). Both models show that the ^2 and the ^4 terms are
> important parts of the model.
>
> My question is:
>
> - Is it legitimate to choose a model based on these outputs according to
> theoretical considerations instead of statistical tests that not really
> show a superiority of one model over the other one?
>
> - Is there another criterium I've overlooked to decide which model can be
> clearly preferred?
>
> - The idea behind that is that in the one model (fit5) the second
> contrast of the factor (gru) is statistically significant, although not
> the whole factor in the anova output.
> In the other model, this is not the case.
> Theoretically interesting is of course the significance of the second
> contrast of gru, as it shows a tendency of one treatment being slightly
> superior. I want to choose this model but I am not sure whether this is
> proper action. Both models shows this trend, but only one model clearly
> indicates that this trend bears some empirical meaning.
>
> Thanks for any suggestions,

The comparisons may be more clearly shown if you create the ordered
factor and a second version of the ordered factor what has the
contrasts set so it produces a 4th order polynomial.  That is, set

hlm3$ozeit <- ordered(hlm3$zeitn)
hlm3$ozeit4 <- C(hlm3$ozeit, contr.poly, 4)

then define one model in terms of ozeit and a second model in terms of ozeit4.

I would go further and create a new binary factor from gru that
contrasted level 2 against the other three levels and use that instead
of gru.

For a model fit by lme I would use the one-argument form of anova to
assess the significance of terms in the fixed effects.  (That advice
doesn't hold for models fit by lmer - at least at present.)



From e.pebesma at geog.uu.nl  Mon Jan  9 17:03:42 2006
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Mon, 09 Jan 2006 17:03:42 +0100
Subject: [R]  How to use filled.contour(x,y,z) data for levelplot(z)?
Message-ID: <43C2895E.3030500@geog.uu.nl>

Jan, 

libary(lattice)
rownames(zz) <- xx
colnames(zz) <- yy
levelplot(zz)

will not work -- levelplot needs a data frame with
zz, xx and yy values next to each other, so read
about data.frame(), rep() and as.vector(zz); you
will need the each= argument for one of xx or yy.

Sounds like you'd be helped by using package sp, 
see also http://r-spatial.sourceforge.net/ and look
for the graph gallery, function spplot.
--
Edzer



From ggrothendieck at gmail.com  Mon Jan  9 17:06:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 9 Jan 2006 11:06:42 -0500
Subject: [R] Wikis etc.
In-Reply-To: <Pine.LNX.4.64.0601090718060.13171@homer24.u.washington.edu>
References: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
	<6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
	<Pine.LNX.4.64.0601090718060.13171@homer24.u.washington.edu>
Message-ID: <971536df0601090806s1b58b40ag7bbb34e4212932cd@mail.gmail.com>

On 1/9/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Mon, 9 Jan 2006, Michael Dewey wrote:
> >
> > Further to that I feel that (perhaps because they do not like to blow their
> > own trumpet too much) the authors of books on R do not stress how much most
> > questioners could gain by buying and reading at least one of the many books
> > on R. When I started I found the free documents useful but I made most
> > progress when I bought MASS. I do realise that liking books is a bit last
> > millennium.
> >
>
> Very late last millenium, though.
> "When I were young[er] we didn't have all these fancy yellow books."
>
> More seriously, yes, reading books about R and S is very effective and is
> how most of the R experts learned.  In my case it was the Blue Book, the
> White Book, and the Ripley/Venables/Smith notes on S-plus (which have
> evolved to the Introduction to R).

In addition to books, the various manuals, contributed documents and
mailing list archives, all of which one should review,
the key thing to do if you want to really learn R is to read source code
and lots of it.  I think there is no other way.  Furthermore, the fact that
you can do this is really a key advantage of open source.



From hb at maths.lth.se  Mon Jan  9 17:52:00 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 10 Jan 2006 03:52:00 +1100
Subject: [R] [R-pkgs] sudoku
In-Reply-To: <1db726800601090552k6d34d2d5l59cf03ba515aa480@mail.gmail.com>
References: <4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB@MSGBOSCLF2WIN.DMN1.FMR.COM>	<20060108122152.7bcc1f6f.detlef.steuer@hsu-hamburg.de>	<17346.8601.975737.17359@stat.math.ethz.ch>
	<1db726800601090552k6d34d2d5l59cf03ba515aa480@mail.gmail.com>
Message-ID: <43C294B0.7030806@maths.lth.se>

I "replied all" to the original message, but since that was to 
"r-packages at stat.math.ethz.ch" it might not have gone out there, did it? 
    If not, below is my reply again. [You have restrict the 
randomization so that you permute within and between block rows/columns.]

/Henrik

-------- Original Message --------
Subject: Re: [R] [R-pkgs] sudoku
Date: Sat, 07 Jan 2006 09:36:54 +1100
From: Henrik Bengtsson <hb at maths.lth.se>
To: Brahm, David <David.Brahm at geodecapital.com>
CC: r-packages at stat.math.ethz.ch
References: 
<4DD6F8B8782D584FABF50BF3A32B03D801A2BCDB at MSGBOSCLF2WIN.DMN1.FMR.COM>

Brahm, David wrote:
 > Any doubts about R's big-league status should be put to rest, now that
 > we have a
 > Sudoku Puzzle Solver.  Take that, SAS!  See package "sudoku" on CRAN.
 >
 > The package could really use a puzzle generator -- contributors are
 > welcome!

Last summer I put a quick generator together after discussing with some
friends how these games a generate (and enumerated).  I don't know if it
is a correct/complete generator, but consider an empty game with 3x3
grids each with 3x3 cells.  Create the initial board  by adding 1:9 in
the first row, the c(2:9,1), in the second and so on, to make sure you
have one correct board.  From this you can now generate all(?) other
possible boards by permuting rows and columns.  You can for instance use
a random seed enumerate all such boards.  Finally, you want to remove
some of cells, which you also can by sampling using known random seeds.

See attached code.  Example:

  > source("Sudoku.R")
  > Sudoku$generate()
        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
   [1,]    1   NA    3    4   NA   NA   NA    8   NA
   [2,]    4   NA   NA   NA    8   NA   NA   NA   NA
   [3,]   NA    8   NA    1   NA    3    4    5    6
   [4,]    2   NA   NA   NA   NA   NA   NA   NA    1
   [5,]   NA    6    7   NA   NA   NA    2   NA    4
   [6,]    8   NA   NA   NA   NA   NA   NA    6   NA
   [7,]   NA   NA    5   NA   NA    8    9   NA   NA
   [8,]    6   NA    8    9    1   NA    3   NA   NA
   [9,]   NA    1   NA    3   NA   NA   NA    7    8
  > Sudoku$generate(1)
        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
   [1,]    3   NA    2    5   NA   NA   NA    9   NA
   [2,]    9   NA   NA   NA    3   NA   NA   NA   NA
   [3,]   NA    4   NA    8   NA    7    2    3    1
   [4,]    2   NA   NA   NA   NA   NA   NA   NA    6
   [5,]   NA    3    4   NA   NA   NA    1   NA    9
   [6,]    8   NA   NA   NA   NA   NA   NA    5   NA
   [7,]   NA   NA    9   NA   NA    2    6   NA   NA
   [8,]    7   NA    6    9    1   NA    3   NA   NA
   [9,]   NA    2   NA    6   NA   NA   NA    1    8
  > Sudoku$generate(2)
        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
   [1,]    7   NA    6    1   NA   NA   NA    2   NA
   [2,]    1   NA   NA   NA    3   NA   NA   NA   NA
   [3,]   NA    2   NA    7   NA    5    9    8    1
   [4,]    8   NA   NA   NA   NA   NA   NA   NA    5
   [5,]   NA    9    1   NA   NA   NA    7   NA    8
   [6,]    5   NA   NA   NA   NA   NA   NA    9   NA
   [7,]   NA   NA    5   NA   NA    7    2   NA   NA
   [8,]    9   NA    8    3    2   NA    5   NA   NA
   [9,]   NA    1   NA    6   NA   NA   NA    7    9

/Henrik

 > -- David Brahm (brahm at alum.mit.edu)
 >
 >
 > 	[[alternative HTML version deleted]]
 >
 > _______________________________________________
 > R-packages mailing list
 > R-packages at stat.math.ethz.ch
 > https://stat.ethz.ch/mailman/listinfo/r-packages
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >
 >

roger bos wrote:
> As far as generating a sudoku, it can't be too hard because I have a program
> on my cell phone that does it with a size less than 325K.  I don't know the
> best way to generate these, but one way I was thinking of was starting with
> a filled up one then randomize the columns and rows. Then make some of them
> blank.  The cell-phone version often generates puzzles that have non-unique
> solutions.  Though I admit this is sometimes annoying, it also can make the
> puzzle harder.
> 
> Thanks,
> 
> Roger
> 
> 
> 
> On 1/9/06, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
>>First, "thanks a lot!" to David Brahms for finally tackling this
>>important problem, and keeping the R language "major league" !
>>;-) :-)  {but the "thanks!" is meant seriously!}
>>
>>
>>>>>>>"Detlef" == Detlef Steuer <detlef.steuer at hsu-hamburg.de>
>>>>>>>    on Sun, 8 Jan 2006 12:21:52 +0100 writes:
>>
>>   Detlef> Hey, you spoiled my course!  :-)
>>
>>   Detlef> I planned using this as an excersise.  Alternative
>>   Detlef> ideas anyone ...
>>
>>Well, you could *add* to it:
>>
>>1) When I have been thinking about doing this myself (occasionally
>>in the past weeks), I had always thought that finding *ALL*
>>solutions was a very important property of the algorithm I would
>>want to design.
>>(since this is slightly more general and useful than proofing
>>uniqueness which the current algorithm does not yet do anyway).
>>
>>2) The current sudoku() prints the result itself and returns a
>>  matrix; improved, it should return an object of class "sudoku",
>>  with a print() and a plot() method;
>>3) The plot() method should of course also work for unfinished
>>  "sudoku" objects, and in fact, the *input* to sudoku() should
>>  also be allowed to be a (typically unfinished) "sudoku" object.
>>
>>4) Then you could have your students use "grid" and
>>  grid.locator() for GUI *input* of a sudoku; i.e. you'd have
>>  another function which returns a (typically unfinished)
>>  "sudoku" object.
>>
>>5) You could start looking at *solving* the more general sudokus
>>  where the blocks are not 3x3 squares anymore, but more
>>  general rectangular polygons of 9 squares each.
>>
>>6) Now you need to refine the GUI from "4)" because your users
>>  need to be able to *draw* the block shapes for the
>>  generalized sudokus.
>>
>>7) Given "1)" is solved, the problem of *generating* sudokus,
>>  that David already mentioned in his announcement, becomes
>>  more relevant: You want to be sure that the sudokus you
>>  generate have exactly one solution.  And your generating
>>  algorithm could start with a very full sudoku (that has
>>  exactly 1 solution) and "erases" squares as much as possible,
>>  always checking that no other solution becomes possible.
>>
>>You see, there's lot of interesting exercises left for your
>>course. (;-)
>>
>>Martin
>>
>>   Detlef> On Fri, 6 Jan 2006 11:43:44 -0500 "Brahm, David"
>>   Detlef> <David.Brahm at geodecapital.com> wrote:
>>
>>   >> Any doubts about R's big-league status should be put to
>>   >> rest, now that we have a Sudoku Puzzle Solver.  Take
>>   >> that, SAS!  See package "sudoku" on CRAN.
>>   >>
>>   >> The package could really use a puzzle generator --
>>   >> contributors are welcome!
>>   >>
>>   >> -- David Brahm (brahm at alum.mit.edu)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Sudoku.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/70eba65f/Sudoku.pl

From fredrik.bg.lundgren at bredband.net  Mon Jan  9 18:06:49 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Mon, 9 Jan 2006 18:06:49 +0100
Subject: [R] "Missing value representation in Excel before extraction to
	R with RODBC"
References: <43C22347.12686.2B96C0@localhost>
	<Pine.LNX.4.61.0601090833090.8090@gannet.stats>
Message-ID: <000801c6153f$14ab6e60$4a9d72d5@Larissa>

Dear list,

Well, those columns in Excel that starts with NA (actually 8 NA's in my 
case) is imported as all NA in R but if the columns starts with at least 
3 cells with values (i.e not NA) the are imported correctly to R. When 
as.is=TRUE is used a simular conversion takes place but now as all <NA> 
and dates are represented as date-and-time.
Is there any way to get this correct even when the Excel columns start 
with several NA's?

Sincerely
Fredrik


----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Petr Pikal" <petr.pikal at precheza.cz>
Cc: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help" 
<r-help at stat.math.ethz.ch>
Sent: Monday, January 09, 2006 9:36 AM
Subject: Re: [R] "Missing value representation in Excel before 
extraction to R with RODBC"


> On Mon, 9 Jan 2006, Petr Pikal wrote:
>
>> Hi
>>
>> I believe it has something to do with the column identification
>> decision. When R decides what is in a column it uses only some values
>> from the beginning of a file.
>
> Not R, Excel.  Excel tells ODBC what the column types are.
>
>> I do not use RODBC as read.delim("clipboard", ...) is usually more
>> convenient but probably there is a way how to tell RODBC what is in
>> the column instead of let R decide from the top of the file.
>
> Using as.is=TRUE stops RODBC doing any conversion.
>
>> But I may be completely mistaken.
>>
>> HTH
>> Petr
>>
>>
>> On 6 Jan 2006 at 20:47, Fredrik Lundgren wrote:
>>
>> From:           "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
>> To:             "R-help" <r-help at stat.math.ethz.ch>
>> Date sent:      Fri, 6 Jan 2006 20:47:29 +0100
>> Subject:        [R] "Missing value representation in Excel before 
>> extraction to R
>> with RODBC"
>>
>>> Dear list,
>>>
>>> How should missing values be expressed in Excel before extraction to 
>>> R
>>> via RODBC. I'm bewildered. Sometimes the representation with NA in
>>> Excel appears to work and shows up in R as <NA> but sometimes the 
>>> use
>>> of NA in Excel changes the whole vector to NA's. Blank or nothing or
>>> NA as representation for missing values in Excel with dateformat 
>>> gives
>>> NA's of the whole vector in R but with  general format in Excel 
>>> gives
>>> blanks for missing values in R. How should I represent missing 
>>> values
>>> in Excel?
>>>
>>>
>>> Best wishes and thanks for any help
>>> Fredrik Lundgren
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From deepayan.sarkar at gmail.com  Mon Jan  9 18:07:33 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 9 Jan 2006 11:07:33 -0600
Subject: [R] two y-axis in xy-plot
In-Reply-To: <F5076E7EAA58F448A0EEC05ADE2317BD0303B0@muc-exch001.munich.komdat.intern>
References: <F5076E7EAA58F448A0EEC05ADE2317BD0303B0@muc-exch001.munich.komdat.intern>
Message-ID: <eb555e660601090907s18ac9451x2f89e61976924382@mail.gmail.com>

On 1/9/06, Antje Schle <Antje.Schuele at komdat.com> wrote:
> Hi there,
>
>
>
> I am wondering if it is possible to do an xyplot with two y-axes. I'd like
> to print two parameters in a time series but they both have different
> scales.
>
>
>
> Which parameter in xyplot can I add to achieve this result?

The answer depends on the details of what you are doing, so please
provide us with a reproducible example.

Deepayan



From yen.lin.chia at intel.com  Mon Jan  9 18:24:31 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Mon, 9 Jan 2006 09:24:31 -0800
Subject: [R] warning message from nlme
Message-ID: <E305A4AFB7947540BC487567B5449BA8090EE27A@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/405134d5/attachment.pl

From yen.lin.chia at intel.com  Mon Jan  9 18:42:20 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Mon, 9 Jan 2006 09:42:20 -0800
Subject: [R] warning message from nlme
Message-ID: <E305A4AFB7947540BC487567B5449BA8090EE30C@scsmsx402.amr.corp.intel.com>

Hi all,

Sorry for the sloppy description of problem.  

I have a dataset called dense.  I want to decompose the
variability/variation into different components such as wafer, axis,
orientation and data.  These variables are all categorical data.  I
converted axis, orientation and location into factors before I ran the
analysis:

lme(Data~1,random=~1|Wafer/axi/ori/loc,data=Dense)

The algorithm returns the standard deviation of each random effect, but
also gives the warning message:

Warning message:
- not meaningful for factors in: Ops.factor(y[revOrder], Fitted)


I wonder what does this warning message mean?  If the factors are not
meaningful, how can I find out which factors the warning message is
referring to?  Thanks.

Yen Lin





-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Monday, January 09, 2006 9:34 AM
To: Chia, Yen Lin
Subject: RE: [R] warning message from nlme

> 
> Can someone point out what is the meaning of this warning message? I
> tried to look at Ops.factor, but I don't understand it since I'm
> relatively new to R.  Thanks.
> 

No. **Read the posting guide**  and provide sufficient details in your
post
so that an answer can be given.

-- Bert



From leif at reflectivity.com  Mon Jan  9 18:53:31 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Mon, 9 Jan 2006 09:53:31 -0800
Subject: [R] Wikis etc.
Message-ID: <200601091753.k09HrS1R005597@hypatia.math.ethz.ch>

To avoid spam on the R wikis pages:
  If we assume that anyone who we would want to be empowered to modify the R wiki pages is an R-user, would it be possible to somehow incorporate a function into the next R release which provides a user with a key/password?
  A new R function would generate a day-of-the year dependent key: if you want to modify an R wiki page you need to enter the key for that day (This is not a proposal to make keys user specific: every R user worldwide would have the same key each day). Then only a person who has installed R would be able to run the function to get a key to modify R wiki pages. Of course anyone could read the wikis.

I supposed that if we wanted, that the key provided could somehow encode the O/S and R version being run, and then the wiki page modified would note which O/S and version the annotator is running, however for ease of use I suggest that the key generated each day be short for simplicy in typing it in.

I suppose a more complex solution would be for an R function to make a call to open a web-browser with a cookie or something set which thus allows the user to modify R wiki pages.

Leif Kirschenbaum
Senior Yield Engineer
Reflectivity, Inc.
(408) 737-8100 x307
leif at reflectivity.com



From drf5n at maplepark.com  Mon Jan  9 18:54:30 2006
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 9 Jan 2006 11:54:30 -0600 (CST)
Subject: [R] Wikis etc.
In-Reply-To: <971536df0601090806s1b58b40ag7bbb34e4212932cd@mail.gmail.com>
References: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
	<6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
	<Pine.LNX.4.64.0601090718060.13171@homer24.u.washington.edu>
	<971536df0601090806s1b58b40ag7bbb34e4212932cd@mail.gmail.com>
Message-ID: <Pine.LNX.4.58.0601091021040.1422@maplepark.com>

On Mon, 9 Jan 2006, Gabor Grothendieck wrote:

> On 1/9/06, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Mon, 9 Jan 2006, Michael Dewey wrote:
> > >
> > > Further to that I feel that (perhaps because they do not like to blow their
> > > own trumpet too much) the authors of books on R do not stress how much most
> > > questioners could gain by buying and reading at least one of the many books
> > > on R. When I started I found the free documents useful but I made most
> > > progress when I bought MASS. I do realise that liking books is a bit last
> > > millennium.
> > >
> >
> > Very late last millenium, though.
> > "When I were young[er] we didn't have all these fancy yellow books."
> >
> > More seriously, yes, reading books about R and S is very effective and is
> > how most of the R experts learned.  In my case it was the Blue Book, the
> > White Book, and the Ripley/Venables/Smith notes on S-plus (which have
> > evolved to the Introduction to R).
>
> In addition to books, the various manuals, contributed documents and
> mailing list archives, all of which one should review,
> the key thing to do if you want to really learn R is to read source code
> and lots of it.  I think there is no other way.  Furthermore, the fact that
> you can do this is really a key advantage of open source.

There has to be some reason to dig into the source code.  Just starting at
line 1 and reading until you are enlightened would be frustrating,
repetetive, and nearly pointless.  The great benefits of the R books is
that they have interesting results (a fancy graph, analysis, or report)
that you can trace back to the constituent parts and (with open source
code) learn everything you want to and be confident that the rest is there
if you need it.  Books using R do an excellent job of showing what is
possible and, through recursive study of the open source code, how to do
it.  Books connect high-level tasks to low-level functions.

R has plenty of documentation, but if the measure of excellence is simply
number of pages or weight, SAS's documentation might still win even if we
include the pages of R source code.  Both packages have lots of detailed
documentation, where if you understood everything that was written, you'd
know how to do what you want.  The authors of neither R nor SAS have
failed to document their functions.

Where I think the R (and SAS) documentation is lacking is in the
connections between the documentation elements.  RTFM isn't helpful if you
can't find TFM.  For instance, there is more than one way to make a graph,
(see http://addictedtor.free.fr/graphiques/thumbs.php?sort=package for 135
of them) how does a novice know which function to use?  How do you find
out the alternate ways to do things?  The hateful MS Excel solves this by
registering the alternate graphic capabilities under a hierarchical GUI
menu.  We solve it with an email list and several fuzzy searches.

Since R has such an extensive set of extensions, maybe we need a section
in the R-intro documentation near
http://cran.r-project.org/doc/manuals/R-intro.html#Writing-your-own-functions
titled "Finding existing functions".  It could explain the difference
between base and recommended, installed, CRAN, and how someone can find
and use things in these areas using help(), '?', help.search(),
help.start(), RSiteSearch(), and the mailing lists.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From Mleeds at kellogggroup.com  Mon Jan  9 19:20:24 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Mon, 9 Jan 2006 13:20:24 -0500
Subject: [R] R newbie example code question
Message-ID: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/284846da/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jan  9 19:29:53 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jan 2006 19:29:53 +0100
Subject: [R] R newbie example code question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
Message-ID: <43C2ABA1.60102@statistik.uni-dortmund.de>

Mark Leeds wrote:

> Sometimes I print out a package
> and read about it and there
> are sometimes nice examples
> that I would like to run myself.
>  
> Is there a way to bring them
> into R from the package or
> are they only meant to be typed
> in manually ? If manual is the
> only way, that's fine. I was
> just checking whether there was
> a quicker way. Thanks.

See (you won't believe it): ?example

Uwe Ligges




>                            Mark
> 
> 
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ihok at hotmail.com  Mon Jan  9 19:33:40 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 09 Jan 2006 13:33:40 -0500
Subject: [R] Wikis etc.
In-Reply-To: <6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
Message-ID: <BAY102-F161E3367A25BF8C8C87497CA220@phx.gbl>

Michael Dewey wrote:
>At 20:12 08/01/06, Jack Tanner wrote:
>>My hypothesis is that the basic reason that people ask questions on R-help 
>>rather than first looking elsewhere is that looking elsewhere doesn't get 
>>them the info they need.
>>
>>People think in terms of the tasks they have to do. The documentation for 
>>R, which can be very good, is organized in terms of the structure of R, 
>>its functions. This mismatch -- people think of tasks, the documentation 
>>"thinks in" functions -- causes people to turn to the mailing list.
>
>Further to that I feel that (perhaps because they do not like to blow their 
>own trumpet too much) the authors of books on R do not stress how much most 
>questioners could gain by buying and reading at least one of the many books 
>on R. When I started I found the free documents useful but I made most 
>progress when I bought MASS. I do realise that liking books is a bit last 
>millennium.

I certainly agree about the value of books. After struggling with 
lme/glmmPQL documentation for a while, I found a copy of MASS, and it's been 
nothing short of illuminating.

Gabor Grothendieck <ggrothendieck <at> gmail.com> wrote:
>In addition to books, the various manuals, contributed documents and
>mailing list archives, all of which one should review,

I do not wish to disparage all these valuable resources. But it is apparent 
that they do not answer the (real or perceived) needs of those who ask the 
same more or less basic questions over and over on R-help. It doesn't help 
if one, or ten, or hundreds of newbies are told -- go thee and RTFM, 
because, by definition, there will be other "newbies" (presumably, until the 
entire human race consists of R experts).

>the key thing to do if you want to really learn R is to read source code
>and lots of it.  I think there is no other way.  Furthermore, the fact that
>you can do this is really a key advantage of open source.

Absolutely, the R sources are the highest fidelity representation of the 
knowledge on R. But there's also knowledge about the sources, e.g., why a 
particular function was coded that way, and why one would want to use that 
function over others that seem similar.

The wiki proposal is thus twofold: first, to take the key advantage of open 
source, and apply it not only to the code, but to the knowledge about the 
code as well. Second, to structure the experience of learning and using R 
such that the wiki is very prominent.

Unfortunately, the site is down at the moment, but there's a really good 
example of how wikified, task-oriented documentation can help an open source 
project: http://codex.wordpress.org/Main_Page .



From Mleeds at kellogggroup.com  Mon Jan  9 19:38:37 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Mon, 9 Jan 2006 13:38:37 -0500
Subject: [R] R newbie example code question
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466575@exchange.psg.com>

wow. example() was exactly what
I wanted.  you all have made all these
incredible facilities 
for a new person.  thanks.

                mark


-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Monday, January 09, 2006 1:30 PM
To: Mark Leeds
Cc: R-Stat Help
Subject: Re: [R] R newbie example code question

Mark Leeds wrote:

> Sometimes I print out a package
> and read about it and there
> are sometimes nice examples
> that I would like to run myself.
>  
> Is there a way to bring them
> into R from the package or
> are they only meant to be typed
> in manually ? If manual is the
> only way, that's fine. I was
> just checking whether there was
> a quicker way. Thanks.

See (you won't believe it): ?example

Uwe Ligges




>                            Mark
> 
> 
> **********************************************************************
> This email and any files transmitted with it are
confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From jsorkin at grecc.umaryland.edu  Mon Jan  9 19:46:31 2006
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 09 Jan 2006 13:46:31 -0500
Subject: [R] R newbie example code question
Message-ID: <s3c26944.026@medicine.umaryland.edu>

Mark,
I am not user where you find your reading material, but if it is online, perhaps you can copy and paste it into an R session.
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu

>>> "Mark Leeds" <Mleeds at kellogggroup.com> 1/9/2006 1:20 PM >>>
Sometimes I print out a package
and read about it and there
are sometimes nice examples
that I would like to run myself.
 
Is there a way to bring them
into R from the package or
are they only meant to be typed
in manually ? If manual is the
only way, that's fine. I was
just checking whether there was
a quicker way. Thanks.
 
                           Mark


**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luke at stat.uiowa.edu  Mon Jan  9 20:11:47 2006
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 9 Jan 2006 13:11:47 -0600 (CST)
Subject: [R] repeat { readline() }
In-Reply-To: <f8e6ff050601080037v40eb6c77p4467c0fe47336e31@mail.gmail.com>
References: <43C06DAC.3020704@maths.lth.se>
	<f8e6ff050601080037v40eb6c77p4467c0fe47336e31@mail.gmail.com>
Message-ID: <Pine.LNX.4.63.0601091310570.10685@nokomis.stat.uiowa.edu>

Use tryCatch; try behaves the way it does with respect to interrupts
for historical compatibility.

luke

On Sun, 8 Jan 2006, hadley wickham wrote:

> On a related note, does anyone know how to exit:
>
> repeat { try( readline() ) }
>
> The try block captures Ctrl-C.
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From HDoran at air.org  Mon Jan  9 20:12:54 2006
From: HDoran at air.org (Doran, Harold)
Date: Mon, 9 Jan 2006 14:12:54 -0500
Subject: [R] warning message from nlme
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A0139676C@dc1ex3.air.org>

Yen Lin:

I'd like to try and offer a little help, but I'm still unclear on your
data structure and problem. You say, they are all categorical variables.
But, does this apply also to your dependent variable? If so, lme is the
wrong function to use.

You also say you want to decompose tha variation into wafer, axis,
orientation and data. But then, what is "loc". Do you mean you want to
decompose the variation in data, your DV, into multiple components
including wafer, axis, orientation and loc?

Maybe if you can take some time to clarify your data structure in more
detail and why your lme call is structured as it is it might be easier
to help.

Harold


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chia, Yen Lin
Sent: Monday, January 09, 2006 12:42 PM
To: Berton Gunter
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] warning message from nlme

Hi all,

Sorry for the sloppy description of problem.  

I have a dataset called dense.  I want to decompose the
variability/variation into different components such as wafer, axis,
orientation and data.  These variables are all categorical data.  I
converted axis, orientation and location into factors before I ran the
analysis:

lme(Data~1,random=~1|Wafer/axi/ori/loc,data=Dense)

The algorithm returns the standard deviation of each random effect, but
also gives the warning message:

Warning message:
- not meaningful for factors in: Ops.factor(y[revOrder], Fitted)


I wonder what does this warning message mean?  If the factors are not
meaningful, how can I find out which factors the warning message is
referring to?  Thanks.

Yen Lin





-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com]
Sent: Monday, January 09, 2006 9:34 AM
To: Chia, Yen Lin
Subject: RE: [R] warning message from nlme

> 
> Can someone point out what is the meaning of this warning message? I 
> tried to look at Ops.factor, but I don't understand it since I'm 
> relatively new to R.  Thanks.
> 

No. **Read the posting guide**  and provide sufficient details in your
post so that an answer can be given.

-- Bert

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Mon Jan  9 20:31:50 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 9 Jan 2006 14:31:50 -0500
Subject: [R] Wikis etc.
In-Reply-To: <BAY102-F161E3367A25BF8C8C87497CA220@phx.gbl>
References: <6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
	<BAY102-F161E3367A25BF8C8C87497CA220@phx.gbl>
Message-ID: <971536df0601091131j1350f6f1ue67bd70ac0d817cf@mail.gmail.com>

On 1/9/06, Jack Tanner <ihok at hotmail.com> wrote:
> Michael Dewey wrote:
> >At 20:12 08/01/06, Jack Tanner wrote:
> >>My hypothesis is that the basic reason that people ask questions on R-help
> >>rather than first looking elsewhere is that looking elsewhere doesn't get
> >>them the info they need.
> >>
> >>People think in terms of the tasks they have to do. The documentation for
> >>R, which can be very good, is organized in terms of the structure of R,
> >>its functions. This mismatch -- people think of tasks, the documentation
> >>"thinks in" functions -- causes people to turn to the mailing list.
> >
> >Further to that I feel that (perhaps because they do not like to blow their
> >own trumpet too much) the authors of books on R do not stress how much most
> >questioners could gain by buying and reading at least one of the many books
> >on R. When I started I found the free documents useful but I made most
> >progress when I bought MASS. I do realise that liking books is a bit last
> >millennium.
>
> I certainly agree about the value of books. After struggling with
> lme/glmmPQL documentation for a while, I found a copy of MASS, and it's been
> nothing short of illuminating.
>
> Gabor Grothendieck <ggrothendieck <at> gmail.com> wrote:
> >In addition to books, the various manuals, contributed documents and
> >mailing list archives, all of which one should review,
>
> I do not wish to disparage all these valuable resources. But it is apparent
> that they do not answer the (real or perceived) needs of those who ask the
> same more or less basic questions over and over on R-help. It doesn't help
> if one, or ten, or hundreds of newbies are told -- go thee and RTFM,
> because, by definition, there will be other "newbies" (presumably, until the
> entire human race consists of R experts).

I certainly was not disparaging books.  I said _in addition to_ books,
not _insted of_.  The reason I pointed this out is that I think
most people already read the books.  What many people don't
do as far as can tell is read the code.  Obviously if you are just
starting out you are going to be relying on the documentation,
books, etc. but once you get past the intro stage you need to get
into code.  One repeatedly sees questions on this list where just
a minute or two spent with the code would have answered the
question.



From nsahoo at gmail.com  Mon Jan  9 19:43:08 2006
From: nsahoo at gmail.com (Nachiketa Sahoo)
Date: Mon, 9 Jan 2006 13:43:08 -0500
Subject: [R] product of multidimensional tables
Message-ID: <62e8fe7d0601091043m6ba52a28gdeb9b6b3ce2928f3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/14ee9dc4/attachment.pl

From trittihn at web.de  Mon Jan  9 21:31:01 2006
From: trittihn at web.de (stefan semmelring)
Date: Mon, 09 Jan 2006 21:31:01 +0100
Subject: [R] <kein Betreff>
Message-ID: <1553110896@web.de>

Hallo,



I??m trying to find out how I can start WinEdt always directly when R is started.

Does anybody know how to do so? I tried to find it out by myself but had no fortune.

Maybe you have to compile (?) a certain file. (I am usind R in the latest version on Win XP)

Thank You



Stefan



From trittihn at web.de  Mon Jan  9 21:40:23 2006
From: trittihn at web.de (stefan semmelring)
Date: Mon, 09 Jan 2006 21:40:23 +0100
Subject: [R] automatic start of RWinEdt
Message-ID: <1553124410@web.de>


"stefan semmelring" <trittihn at web.de> schrieb am 09.01.06 21:31:01:
> 
> Hallo,
> 
> 
> 
> I??m trying to find out how I can start WinEdt always directly when R is started.
> 
> Does anybody know how to do so? I tried to find it out by myself but had no fortune.
> 
> Maybe you have to compile (?) a certain file. (I am usind R in the latest version on Win XP)
> 
> Thank You
> 
> 
> 
> Stefan
>  
>



From Mleeds at kellogggroup.com  Mon Jan  9 23:00:52 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Mon, 9 Jan 2006 17:00:52 -0500
Subject: [R] brown, durbin , evans ( 1975 )
Message-ID: <A8B87FDB74320349A9D1CC9021052A764665F2@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060109/c1447759/attachment.pl

From vincent at 7d4.com  Mon Jan  9 18:04:29 2006
From: vincent at 7d4.com (vincent@7d4.com)
Date: Mon, 09 Jan 2006 18:04:29 +0100
Subject: [R] paste tab and print
In-Reply-To: <Pine.LNX.4.61.0601090820510.8090@gannet.stats>
References: <43C2102A.70809@7d4.com>
	<Pine.LNX.4.61.0601090820510.8090@gannet.stats>
Message-ID: <43C2979D.1050405@7d4.com>

Prof Brian Ripley a ??crit :

>> cat(info, "\n")

Thank you very much. It works.



From dataanalytics at rediffmail.com  Tue Jan 10 02:14:29 2006
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 10 Jan 2006 01:14:29 -0000
Subject: [R] StatsRus and wiki
Message-ID: <20060110011429.12570.qmail@webmail49.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/883d5c3e/attachment.pl

From ihok at hotmail.com  Tue Jan 10 02:27:57 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 09 Jan 2006 20:27:57 -0500
Subject: [R] bug in either glmmPQL or lme/lmer
Message-ID: <BAY102-F22241EFB77A07FF9456055CA250@phx.gbl>

I know it's conventional to report bugs to the maintainer, but I'm not sure 
which package actually contains this bug(s), so I apologize for sending this 
to the list at large. I see the bug under both R 2.1.1, and R 2.2.1. (I sent 
a related message a while ago, but this one has more detail.)

library(MASS)
library(nlme)

fit.model <- function(il, model.family) {
  cs <- Initialize(corSymm(form=~1|id), data=il)
  glmmPQL(score~test+coder, random=~1|id, family=model.family, data=il, 
correlation=cs)
}

score <- 
c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
id <- rep(1:13, rep(4, 13))
test <- gl(2, 1, length(score), labels=c("pre", "post"))
coder <- gl(2, 2, length(score), labels=c("two", "three"))

original <- data.frame(id, score, test, coder)

sorted.ok <- original[order(original$id, original$score, original$test, 
original$coder),]
sorted2.ok <- original[order(original$id, original$test, original$score, 
original$coder),]
sorted3.not.ok <- original[order(original$score, original$id, original$test, 
original$coder),]

print(summary(fit.model(original, poisson)))
print(summary(fit.model(sorted.ok, poisson)))
print(summary(fit.model(sorted2.ok, poisson)))
print(summary(fit.model(sorted3.not.ok, poisson)))

That last line produces, under R version 2.1.1, 2005-06-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
    MASS     nlme
"7.2-20" "3.1-65"

iteration 1
Error in logLik.reStruct(object, conLin) :
	NA/NaN/Inf in foreign function call (arg 3)

That same last line produces, under R version 2.2.1, 2005-12-20, 
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      MASS       nlme
  "7.2-24" "3.1-68.1"

iteration 1
Error in lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data = 
list( :
	false convergence (8)

Moreover, summary(fit.model(original, poisson)) produces a different output 
than either
summary(fit.model(sorted.ok, poisson)) or summary(fit.model(sorted2.ok, 
poisson)), but the latter two do equal each other.



From jwd at surewest.net  Tue Jan 10 02:38:36 2006
From: jwd at surewest.net (J Dougherty)
Date: Mon, 9 Jan 2006 17:38:36 -0800
Subject: [R] Wikis etc.
In-Reply-To: <971536df0601091131j1350f6f1ue67bd70ac0d817cf@mail.gmail.com>
References: <6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
	<BAY102-F161E3367A25BF8C8C87497CA220@phx.gbl>
	<971536df0601091131j1350f6f1ue67bd70ac0d817cf@mail.gmail.com>
Message-ID: <200601091738.36456.jwd@surewest.net>

On Monday 09 January 2006 11:31, Gabor Grothendieck wrote:
> . . .
>
> I certainly was not disparaging books.  I said _in addition to_ books,
> not _insted of_.  The reason I pointed this out is that I think
> most people already read the books.  What many people don't
> do as far as can tell is read the code.  Obviously if you are just
> starting out you are going to be relying on the documentation,
> books, etc. but once you get past the intro stage you need to get
> into code.  One repeatedly sees questions on this list where just
> a minute or two spent with the code would have answered the
> question.
>
Gabor,  

The issue IS the INTRO stage though, in fact well beyond the intro stage.  
Reading the code is well and good for a programmer, but many are not and 
never will be coders, and this group is going to continue to expand.  Even an 
advanced statistician migrating from a proprietary system like SPSS to R is 
going to need documentation.  They would not even know where in the code to 
look!  Code is cool, but it really isn't expository.

John



From jwd at surewest.net  Tue Jan 10 02:44:40 2006
From: jwd at surewest.net (J Dougherty)
Date: Mon, 9 Jan 2006 17:44:40 -0800
Subject: [R] wicked wikis for R
In-Reply-To: <11544d000601090516g3ccd6a9dg@mail.gmail.com>
References: <11544d000601090516g3ccd6a9dg@mail.gmail.com>
Message-ID: <200601091744.40494.jwd@surewest.net>

On Monday 09 January 2006 05:16, Jean-Christophe BOUETTE wrote:
> > From: "Arin Basu" <dataanalytics at rediffmail.com>
> > To: r-help at stat.math.ethz.ch
> > Date: 8 Jan 2006 19:18:17 -0000
> > Subject: [R] wicked wikis for R
> >
> > >Message: 41
> > >Date: Sun, 08 Jan 2006 13:52:33 +1100
> > > From: paul sorenson <sourceforge at metrak.com>
> > >Subject: Re: [R] Wikis etc.
> > >To: Frank E Harrell Jr <f.harrell at vanderbilt.edu>,     r-help
> > >       <r-help at stat.math.ethz.ch>
>
> -snip-
>
> > Among others, here's one long-term benefit for the newbies. Instead of
> > people getting admonished/thrashed with harsh expressions/advices like
> > "go see the mailing list publishing etiquettes", or "you should search
> > the archives and help files, and read all manuals, and ask others first
> > before posting here..." (which can turn away many a newcomer from posting
> > or using the mailing list or using R for that matter), wiki could make
> > life a little easy for newbies/less experienced who could then receive
> > more polite one liners like, "please check the wikipages...", or
> > "solution #xyz in the wikipages for the solution".
>
> Sorry, I don't get the point here. Some people will keep feeling
> offensed when they're just told to read the man/wiki pages, and others
> will simply change their answers from RTFM to RTFW.
> Nobody can force people into reading the manuals, or reading the
> posting guide. This is definitely one problem that the wiki will not
> solve.
>
>
The real issue is indexing and cross-referencing.  Quite often requestors HAVE 
tried the manuals, but without a real clue about where to look, that can be 
unproductive.  On the other hand, being pulled up short and Ripleyed can have 
a salutary effect on analyzing the problem first and researching carefully 
before posting a clueless query.  You could see as a useful part of the 
learning process.

JD



From ronggui.huang at gmail.com  Tue Jan 10 05:47:34 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Tue, 10 Jan 2006 12:47:34 +0800
Subject: [R] automatic start of RWinEdt
In-Reply-To: <1553124410@web.de>
References: <1553124410@web.de>
Message-ID: <38b9f0350601092047r2d1af523t@mail.gmail.com>

the readme file tells what you should do:


    - In R use something like (for example in your .Rprofile):
       options(editor="\"c:/program files/winedt team/winedt/winedt\"
-c=\"R-WinEdt-edit\" -e=r.ini -V")
       options(pager="\"c:/program files/winedt team/winedt/winedt\"
-C=\"R-WinEdt\" -e=r.ini -V")


    Used WinEdt parameters (Remark: There is a difference between -c and -C !):
        -c="name": New instance of WinEdt called "name" will be started.
        -C="name": If an instance "name" of WinEdt is already running,
it will be used.
                   So you can run WinEdt as LaTeX and R editor at the same time.
        -e="name": Using "name" as initialization-file.
        -V       : Running in "virgin"-mode

    Examples:

    ###### Recommended procedure:
    ## Open WinEdt with something like:
     "c:\program files\winedt team\winedt\winedt" -C="R-WinEdt" -e=r.ini
    ## i.e. ideally the shortcut you have already created.
    ##
    ## Then create a new document and write an R function.
    ## Click on the symbol "R source" or press ALT+S.
    ## You will be asked to specify a filename (e.g. "R-prog1.R").
    ## If RGui is running, it will be focussed and source(.) will be called.

    ######
     options(pager="\"c:/program files/winedt team/winedt/winedt\"
-C=\"R-WinEdt\" -e=r.ini -V")
     file.show(".Rhistory")
    ## Mark some lines, click the "R paste" button or use Alt+P as shortcut:
    ## Marked lines will be executed in RGui.

    ######
    ## The following way to edit function is possible, but *not* recommended:
     options(editor="\"c:/program files/winedt team/winedt/winedt\"
-c=\"R-WinEdt-edit\" -e=r.ini -V")
     my.legend <- legend
     fix(my.legend)

2006/1/10, stefan semmelring <trittihn at web.de>:
>
> "stefan semmelring" <trittihn at web.de> schrieb am 09.01.06 21:31:01:
> >
> > Hallo,
> >
> >
> >
> > Im trying to find out how I can start WinEdt always directly when R is started.
> >
> > Does anybody know how to do so? I tried to find it out by myself but had no fortune.
> >
> > Maybe you have to compile (?) a certain file. (I am usind R in the latest version on Win XP)
> >
> > Thank You
> >
> >
> >
> > Stefan
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From bioflash at gmail.com  Tue Jan 10 07:33:35 2006
From: bioflash at gmail.com (Vincent Deng)
Date: Tue, 10 Jan 2006 14:33:35 +0800
Subject: [R] How to plot legend to the margin area of the graph?
Message-ID: <455343d90601092233v15dfbb08y7a4e61947bd341e3@mail.gmail.com>

Dear R-helpers,


When plotting a graph, what command should I use to mark legend to the
margin area of that graph?



From gelman at stat.columbia.edu  Sun Jan  8 19:19:30 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sun, 08 Jan 2006 13:19:30 -0500
Subject: [R] lmer with nested/nonnested groupings?
Message-ID: <43C157B2.3010800@stat.columbia.edu>

I'm trying to figure out how to use lmer to fit models with factors that 
have some nesting and some non-nested groupings.  For example, in this 
paper:
http://www.stat.columbia.edu/~gelman/research/published/parkgelmanbafumi.pdf
we have a logistic regression of survey respondents' political 
preferences (1=Republican, 0=Democrat), regressing on sex, ethnicity, 
state (51 states within 5 regions), 4 age categories, and 4 education 
categories.  I'd like to include states (nested within regions), and 
also age, education, and age x education.  (That is, 5 batches of 
varying coefs:  50 states, 5 regions, 4 age categories, 4 education 
categories, and 16 age x education categories.)  The age x education 
factor is kinda tricky because it's connected both to age and to education.

I'm thinking of a model like this:

lmer (y ~ black*female + (1 | state) + (1 | region) + (1 | age) + (1 | 
edu) + (1 | age.edu), family=binomial(link="logit"))

(Here, I'm thinking of age.edu as a variable with 16 levels.)

Anyway, it blows up when i try to put in these nested things.  I read 
Doug Bates's article in R-news and there seems to be a  way of doing 
nested groupings (unfortunately, I can't quite figure out how to do it), 
but I don't see any references to situations such as age, edu, and age*edu .

For the article, we used Bugs, which is fine, but I'd like to see how 
far I can take it using lmer.  I could kludge it by, for example, 
including age, edu, and region as unmodeled factors:

lmer (y ~ black*female + (1 | state) + factor(region) + factor(age) + 
factor(edu) + (1 | age.edu), family=binomial(link="logit"))

but I'd like to do the full multilevel version.

Thanks!
Andrew

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From detlef.steuer at hsu-hamburg.de  Tue Jan 10 08:39:25 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Tue, 10 Jan 2006 08:39:25 +0100
Subject: [R] wiki in Hamburg set read-only
In-Reply-To: <43C2FF2E.5060601@zoo.ufl.edu>
References: <43C2FF2E.5060601@zoo.ufl.edu>
Message-ID: <20060110083925.e85117b2.detlef.steuer@hsu-hamburg.de>

Hi, 

to clear up the situation regarding wikis a bit and
since, whatever happens, Philippe's wiki is superior, I decided to set the old wiki read-only. A visitor will be asked to visit the "new" wiki and add contents there. Good luck Philippe! Your plans sound very nice!

The new wiki should get visibly mentioned on the R frontpage, so that especially beginners have a good chance to find it. Let's add content!

Detlef



From Achim.Zeileis at wu-wien.ac.at  Tue Jan 10 08:44:20 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 10 Jan 2006 08:44:20 +0100 (CET)
Subject: [R] brown, durbin , evans ( 1975 )
Message-ID: <Pine.LNX.4.58.0601100843440.26827@thorin.ci.tuwien.ac.at>

Mark:

> Does anyone know where
> I can get  R code for plotting
> the Brown , Durbin
> and Evans cumsum
> procedure ( 1975 ) ?

Via
library("strucchange")

> P.S : Strucchange looks like a great package but
> it would take me way longer than I
> have to only partially understand the theory behind
> what is going on there.

Come on, it's not that difficult, e.g., for detecting the mean shift in
  plot(Nile)
you can apply BDE's Recursive CUSUM test via
  rcus <- efp(Nile ~ 1, type = "Rec-CUSUM")
  plot(rcus)
which shows quite clearly the structural change that occured in 1898 after
the Ashwan dam was built. You can get test statistic and p-value via
  sctest(rcus)

See the package vignette and the man pages for more examples.

As I already told you privately, a lot of research was done after BDE 1975
and the test is not the most suitable in various situations. For example,
it can perform quite badly in the presence of multiple shifts or shifts
that occur late in the sample period.

Best,
Z



From ligges at statistik.uni-dortmund.de  Tue Jan 10 08:50:14 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 Jan 2006 08:50:14 +0100
Subject: [R] How to plot legend to the margin area of the graph?
In-Reply-To: <455343d90601092233v15dfbb08y7a4e61947bd341e3@mail.gmail.com>
References: <455343d90601092233v15dfbb08y7a4e61947bd341e3@mail.gmail.com>
Message-ID: <43C36736.7000604@statistik.uni-dortmund.de>

Vincent Deng wrote:

> Dear R-helpers,
> 
> 
> When plotting a graph, what command should I use to mark legend to the
> margin area of that graph?

Example:

plot(1:10)
par(xpd=TRUE)
legend(8,11.5,"Hello World")

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jan 10 09:06:23 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 Jan 2006 09:06:23 +0100
Subject: [R] automatic start of RWinEdt
In-Reply-To: <1553124410@web.de>
References: <1553124410@web.de>
Message-ID: <43C36AFF.2010908@statistik.uni-dortmund.de>

stefan semmelring wrote:

> "stefan semmelring" <trittihn at web.de> schrieb am 09.01.06 21:31:01:
> 
>>Hallo,
>>
>>
>>
>>I??m trying to find out how I can start WinEdt always directly when R is started.
>>
>>Does anybody know how to do so? I tried to find it out by myself but had no fortune.
>>
>>Maybe you have to compile (?) a certain file. (I am usind R in the latest version on Win XP)



For example add the line
    library(RWinEdt)
to your Rprofile file. For details see ?Startup.

Uwe Ligges



>>Thank You
>>
>>
>>
>>Stefan
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Tue Jan 10 10:08:30 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 10 Jan 2006 10:08:30 +0100
Subject: [R] "Missing value representation in Excel before extraction to
	R with RODBC"
In-Reply-To: <000801c6153f$14ab6e60$4a9d72d5@Larissa>
Message-ID: <43C3879E.22583.6BFC88@localhost>

Hi

I tried to reproduce what you have told us by copy and paste

read.delim("clipboard")

but was not successful.

Even with several blank values in each column in Excel i got correct 
import to R by this process. As I do not use RODBC I do not know all 
possible settings and features. If colClasses is available you can 
force the columns to by character, numeric, factor, Date or some 
other class.

BTW Excel can be quite tricky and hides e.g. spaces in cells so you 
see them as empty even if they are not. So if I get some weird 
conversions of numeric columns there is often something hidden in 
Excel.

HTH
Petr


On 9 Jan 2006 at 18:06, Fredrik Lundgren wrote:

From:           	"Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
To:             	"Prof Brian Ripley" <ripley at stats.ox.ac.uk>,
	"Petr Pikal" <petr.pikal at precheza.cz>
Copies to:      	"R-help" <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] "Missing value representation in Excel before extraction to R with RODBC"
Date sent:      	Mon, 9 Jan 2006 18:06:49 +0100

> Dear list,
> 
> Well, those columns in Excel that starts with NA (actually 8 NA's in
> my case) is imported as all NA in R but if the columns starts with at
> least 3 cells with values (i.e not NA) the are imported correctly to
> R. When as.is=TRUE is used a simular conversion takes place but now as
> all <NA> and dates are represented as date-and-time. Is there any way
> to get this correct even when the Excel columns start with several
> NA's?
> 
> Sincerely
> Fredrik
> 
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Petr Pikal" <petr.pikal at precheza.cz>
> Cc: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help"
> <r-help at stat.math.ethz.ch> Sent: Monday, January 09, 2006 9:36 AM
> Subject: Re: [R] "Missing value representation in Excel before
> extraction to R with RODBC"
> 
> 
> > On Mon, 9 Jan 2006, Petr Pikal wrote:
> >
> >> Hi
> >>
> >> I believe it has something to do with the column identification
> >> decision. When R decides what is in a column it uses only some
> >> values from the beginning of a file.
> >
> > Not R, Excel.  Excel tells ODBC what the column types are.
> >
> >> I do not use RODBC as read.delim("clipboard", ...) is usually more
> >> convenient but probably there is a way how to tell RODBC what is in
> >> the column instead of let R decide from the top of the file.
> >
> > Using as.is=TRUE stops RODBC doing any conversion.
> >
> >> But I may be completely mistaken.
> >>
> >> HTH
> >> Petr
> >>
> >>
> >> On 6 Jan 2006 at 20:47, Fredrik Lundgren wrote:
> >>
> >> From:           "Fredrik Lundgren"
> >> <fredrik.bg.lundgren at bredband.net> To:             "R-help"
> >> <r-help at stat.math.ethz.ch> Date sent:      Fri, 6 Jan 2006 20:47:29
> >> +0100 Subject:        [R] "Missing value representation in Excel
> >> before extraction to R with RODBC"
> >>
> >>> Dear list,
> >>>
> >>> How should missing values be expressed in Excel before extraction
> >>> to R via RODBC. I'm bewildered. Sometimes the representation with
> >>> NA in Excel appears to work and shows up in R as <NA> but
> >>> sometimes the use of NA in Excel changes the whole vector to NA's.
> >>> Blank or nothing or NA as representation for missing values in
> >>> Excel with dateformat gives NA's of the whole vector in R but with
> >>>  general format in Excel gives blanks for missing values in R. How
> >>> should I represent missing values in Excel?
> >>>
> >>>
> >>> Best wishes and thanks for any help
> >>> Fredrik Lundgren
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self) 1
> > South Parks Road,                     +44 1865 272866 (PA) Oxford
> > OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From hodgess at gator.dt.uh.edu  Tue Jan 10 10:15:39 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 10 Jan 2006 03:15:39 -0600
Subject: [R]  graphics pages?
Message-ID: <200601100915.k0A9Fdwp027837@gator.dt.uh.edu>

Dear R People:

In S Plus, if you have a function which calls the plot
function several times, you get several "pages" of graphics
output.

Is there an eqivalent in R, please?

R version 2.2.1 windows

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ligges at statistik.uni-dortmund.de  Tue Jan 10 10:21:31 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 Jan 2006 10:21:31 +0100
Subject: [R] graphics pages?
In-Reply-To: <200601100915.k0A9Fdwp027837@gator.dt.uh.edu>
References: <200601100915.k0A9Fdwp027837@gator.dt.uh.edu>
Message-ID: <43C37C9B.4040005@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> In S Plus, if you have a function which calls the plot
> function several times, you get several "pages" of graphics
> output.
> 
> Is there an eqivalent in R, please?

Yes, for e.g. postscript() and pdf() devices the default is to plot each 
new plot on a separate page.

Uwe Ligges



> R version 2.2.1 windows
> 
> Thanks in advance!
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jan 10 10:34:39 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jan 2006 09:34:39 +0000 (GMT)
Subject: [R] graphics pages?
In-Reply-To: <200601100915.k0A9Fdwp027837@gator.dt.uh.edu>
References: <200601100915.k0A9Fdwp027837@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.61.0601100924430.17469@gannet.stats>

On Tue, 10 Jan 2006, Erin Hodgess wrote:

> Dear R People:
>
> In S Plus, if you have a function which calls the plot
> function several times, you get several "pages" of graphics
> output.

Only on a graphsheet, I believe.

> Is there an eqivalent in R, please?

Pretty close, on a windows() device.  Look for `record' on its help page, 
or look at the README.R-2.2.1 or the rw-FAQ or explore the history menu on 
the device.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Antje.Schuele at komdat.com  Tue Jan 10 11:04:02 2006
From: Antje.Schuele at komdat.com (=?iso-8859-1?Q?Antje_Sch=FCle?=)
Date: Tue, 10 Jan 2006 11:04:02 +0100
Subject: [R] two y-axis in xy-plot
Message-ID: <F5076E7EAA58F448A0EEC05ADE2317BD030450@muc-exch001.munich.komdat.intern>


It is nearly the same example I wrote about in http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg54238.html. I'll print it out again:

In the first column I have numbers from 0 to 23 (hours of a day), the second 
column contains the name of a weekday (Day as factor) and the third column 
contains the number I am interested in. So as an example, the first five rows look like that:

     Hour Day Freq

1     0  Mo    23

2     1  Mo    20

3     2  Mo    14

4     3  Mo    27

5     4  Mo    26

 
To read: On Monday between 0 and 1 o'clock 23 things happened. 

Now I add a new parameter, so that the data looks somehow like that:

     Hour Day Freq Freq2

1     0  Mo    23	  874

2     1  Mo    20   476

3     2  Mo    14	  201

4     3  Mo    27   912

5     4  Mo    26   172

Now I'd like to have a plot for every weekday containing the two frequencies as a line during the hours.

Is this possible with two y-axes?

Thanks for your help, 

Antje




-----Urspr??ngliche Nachricht-----
Von: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Gesendet: Montag, 9. Januar 2006 18:08
An: Antje Sch??le
Cc: r-help at stat.math.ethz.ch
Betreff: Re: two y-axis in xy-plot

On 1/9/06, Antje Sch??le <Antje.Schuele at komdat.com> wrote:
> Hi there,
>
>
>
> I am wondering if it is possible to do an xyplot with two y-axes. I'd like
> to print two parameters in a time series but they both have different
> scales.
>
>
>
> Which parameter in xyplot can I add to achieve this result?

The answer depends on the details of what you are doing, so please
provide us with a reproducible example.

Deepayan



From jcbouette at gmail.com  Tue Jan 10 11:45:36 2006
From: jcbouette at gmail.com (Jean-Christophe BOUETTE)
Date: Tue, 10 Jan 2006 11:45:36 +0100
Subject: [R] exporting methods/classes
Message-ID: <11544d000601100245g5b6441bfn@mail.gmail.com>

> ---------- Message transf??r?? ----------
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> To: Erin Hodgess <hodgess at gator.dt.uh.edu>
> Date: Sun, 08 Jan 2006 15:09:06 +0100
> Subject: Re: [R] exporting methods/classes
> Erin Hodgess wrote:
>
> > Dear R People:
> >
> > I'm still struggling with sending methods and classes as part of
> > creating a new package.
> >
> > Where does the .onLoad function go?  Within R itself or in a file
> > in one of the new package directories?
>
> Simply save the .onLoad function in some .R file (e.g. zzz.R) in the
> package's ./R directory.
>
>
>
> > Here are my latest efforts:
> >
> >
> > Here's the last part of the woof1-Ex.Rout
> >
> >
> >>library('woof1')
> >
> > Error in loadNamespace(package, c(which.lib.loc, lib.loc),
> > keep.source = keep.source) :
> >       in 'woof1' classes for export not defined: dog
> > Error: package/namespace load failed for 'woof1'
> > Execution halted
>
> So this looks like you have defined S4 classes for export in your
> Namespace but you have no call that starts with
>
> setClass("dog", ......
>
Well, it's not that easy for a newbie (like me) to track that problem.
Using package.skeleton() (on my old R 2.1.1) puts all the function
definitions in the ./R subdirectory, but not the call to setClass (or
did I make something wrong?). Maybe it's one of the things that should
be documented to make package creation easier. Of course, I know
somebody will point me to some doc where it is stated but the fact is
that it was not obvious.

By the way, I'd like to thank Uwe Ligges and Duncan Murdoch for the
helpful article in R-Help 5/2, it was really useful to me. If I manage
to understand all the steps necessary to make an S4 namespace package
I promise to write a "S4 Package step by step for newbies" :-)

Regards,
Jean-Christophe Bou??tt??.


> in the R code in your package, hence the class "dog" is undefined.
>
>
> > Here's the NAMESPACE
> > importFrom(graphics,plot)
> > exportClasses("dog")
> > exportMethods("plot","show")
>
>
> You don't export any other functions?
>
>
> Uwe Ligges
>
>
>
>
> >
> > thanks yet again,
> > Sincerely,
> > Erin
> > mailto: hodgess at gator.uhd.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>



From jonathan.williams at pharmacology.oxford.ac.uk  Tue Jan 10 11:54:34 2006
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Tue, 10 Jan 2006 10:54:34 -0000
Subject: [R] extracting coefficients from lmer
Message-ID: <NGBBKJEMOMLJFCOIEGCEOEAJJPAA.jonathan.williams@pharm.ox.ac.uk>

Dear R-Helpers,

I want to compare the results of outputs from glmmPQL and lmer analyses.
I could do this if I could extract the coefficients and standard errors
from the summaries of the lmer models. This is easy to do for the glmmPQL
summaries, using

> glmm.fit <- try(glmmPQL(score ~ x*type, random = ~ 1 | subject, data = df,
	family = binomial), TRUE)
> summary(glmmPQL.fit)$tTable

Linear mixed-effects model fit by maximum likelihood
 Data: df
       AIC      BIC    logLik
  1800.477 1840.391 -890.2384

Random effects:
 Formula: ~1 | subject
        (Intercept)  Residual
StdDev:   0.6355517 0.9650671

Variance function:
 Structure: fixed weights
 Formula: ~invwt
Fixed effects: score ~ x * type
                 Value Std.Error  DF    t-value p-value
(Intercept) -0.0812834 0.2933314 294 -0.2771043  0.7819
x1           0.4143072 0.4180624  98  0.9910176  0.3241
type2        0.8509166 0.4084443 294  2.0833112  0.0381
type3        0.6691275 0.4024369 294  1.6626894  0.0974
type4       -0.7830413 0.4123851 294 -1.8988109  0.0586
x1:type2     1.0643239 0.6791126 294  1.5672274  0.1181
x1:type3    -0.7533085 0.5674532 294 -1.3275253  0.1854
x1:type4    -0.0549616 0.5777216 294 -0.0951351  0.9243
etc.

However, there seems to be no route to extract the corresponding information
from the lmer model:-

> lmer.fit=try(lmer(score~x*type+(1|subject), data=df, family=binomial,
	method='AGQ'),TRUE)
> summary(lmer.fit)

Generalized linear mixed model fit using AGQ
Formula: score ~ x * type + (1 | subject)
   Data: df
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 510.2616 550.1762 -245.1308 490.2616
Random effects:
     Groups        Name    Variance    Std.Dev.
    subject (Intercept)     0.46269     0.68021
# of obs: 400, groups: subject, 100

Estimated scale (compare to 1)  1.019134

Fixed effects:
             Estimate Std. Error  z value Pr(>|z|)
(Intercept) -0.087284   0.300896 -0.29008  0.77175
x1           0.446289   0.428844  1.04068  0.29803
type2        0.913571   0.418978  2.18047  0.02922 *
type3        0.719023   0.412816  1.74175  0.08155 .
type4       -0.839842   0.423021 -1.98534  0.04711 *
x1:type2     1.112673   0.696629  1.59722  0.11022
x1:type3    -0.809599   0.582089 -1.39085  0.16427
x1:type4    -0.062235   0.592623 -0.10502  0.91636
etc.

> summary(lmer.fit)$tTable
NULL
> names(summary(lmer.fit))
NULL
> names(lmer.fit)
NULL
> lmer.fit$coef
NULL

So, then I tried to find out if lmer returns different information.
> help(lmer)
This says "see lmer-class" ->
> help(lmer-class)
No documentation for 'lmer - class' in specified packages and libraries:
you could try 'help.search("lmer - class")'

So, then I tried
> help.search('lmer-class')

This returns
Help files with alias or concept or title matching 'lmer-class' using fuzzy
matching:
lmer-class(Matrix)               Mixed model representations

So, I loaded library Matrix and tried again
> library(Matrix)
> help(lmer-class)
No documentation for 'lmer - class' in specified packages and libraries:
you could try 'help.search("lmer - class")'

If someone could tell me how to extract the Estimates and Std. Errors from
the lmer summary, I'd be very grateful. I would also be very grateful if
someone
could let me know why the Estimates and Std. Errors from the lmer model are
both
larger than those from the glmmPQL model.

I am running R 2.2.1 on a Windows XP machine.

Thanks,

Jonathan Williams



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 10 12:05:44 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 10 Jan 2006 12:05:44 +0100
Subject: [R] extracting coefficients from lmer
References: <NGBBKJEMOMLJFCOIEGCEOEAJJPAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <00c401c615d5$cd8bb3e0$0540210a@www.domain>

you can use something like:

Vcov <- vcov(lmer.fit, useScale = FALSE)
betas <- fixef(lmer.fit)
se <- sqrt(diag(Vcov))
zval <- betas / se
pval <- 2 * pnorm(abs(zval), lower.tail = FALSE)
###############
cbind(betas, se, zval, pval)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jonathan Williams" 
<jonathan.williams at pharmacology.oxford.ac.uk>
To: "Ethz. Ch" <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 10, 2006 11:54 AM
Subject: [R] extracting coefficients from lmer


> Dear R-Helpers,
>
> I want to compare the results of outputs from glmmPQL and lmer 
> analyses.
> I could do this if I could extract the coefficients and standard 
> errors
> from the summaries of the lmer models. This is easy to do for the 
> glmmPQL
> summaries, using
>
>> glmm.fit <- try(glmmPQL(score ~ x*type, random = ~ 1 | subject, 
>> data = df,
> family = binomial), TRUE)
>> summary(glmmPQL.fit)$tTable
>
> Linear mixed-effects model fit by maximum likelihood
> Data: df
>       AIC      BIC    logLik
>  1800.477 1840.391 -890.2384
>
> Random effects:
> Formula: ~1 | subject
>        (Intercept)  Residual
> StdDev:   0.6355517 0.9650671
>
> Variance function:
> Structure: fixed weights
> Formula: ~invwt
> Fixed effects: score ~ x * type
>                 Value Std.Error  DF    t-value p-value
> (Intercept) -0.0812834 0.2933314 294 -0.2771043  0.7819
> x1           0.4143072 0.4180624  98  0.9910176  0.3241
> type2        0.8509166 0.4084443 294  2.0833112  0.0381
> type3        0.6691275 0.4024369 294  1.6626894  0.0974
> type4       -0.7830413 0.4123851 294 -1.8988109  0.0586
> x1:type2     1.0643239 0.6791126 294  1.5672274  0.1181
> x1:type3    -0.7533085 0.5674532 294 -1.3275253  0.1854
> x1:type4    -0.0549616 0.5777216 294 -0.0951351  0.9243
> etc.
>
> However, there seems to be no route to extract the corresponding 
> information
> from the lmer model:-
>
>> lmer.fit=try(lmer(score~x*type+(1|subject), data=df, 
>> family=binomial,
> method='AGQ'),TRUE)
>> summary(lmer.fit)
>
> Generalized linear mixed model fit using AGQ
> Formula: score ~ x * type + (1 | subject)
>   Data: df
> Family: binomial(logit link)
>      AIC      BIC    logLik deviance
> 510.2616 550.1762 -245.1308 490.2616
> Random effects:
>     Groups        Name    Variance    Std.Dev.
>    subject (Intercept)     0.46269     0.68021
> # of obs: 400, groups: subject, 100
>
> Estimated scale (compare to 1)  1.019134
>
> Fixed effects:
>             Estimate Std. Error  z value Pr(>|z|)
> (Intercept) -0.087284   0.300896 -0.29008  0.77175
> x1           0.446289   0.428844  1.04068  0.29803
> type2        0.913571   0.418978  2.18047  0.02922 *
> type3        0.719023   0.412816  1.74175  0.08155 .
> type4       -0.839842   0.423021 -1.98534  0.04711 *
> x1:type2     1.112673   0.696629  1.59722  0.11022
> x1:type3    -0.809599   0.582089 -1.39085  0.16427
> x1:type4    -0.062235   0.592623 -0.10502  0.91636
> etc.
>
>> summary(lmer.fit)$tTable
> NULL
>> names(summary(lmer.fit))
> NULL
>> names(lmer.fit)
> NULL
>> lmer.fit$coef
> NULL
>
> So, then I tried to find out if lmer returns different information.
>> help(lmer)
> This says "see lmer-class" ->
>> help(lmer-class)
> No documentation for 'lmer - class' in specified packages and 
> libraries:
> you could try 'help.search("lmer - class")'
>
> So, then I tried
>> help.search('lmer-class')
>
> This returns
> Help files with alias or concept or title matching 'lmer-class' 
> using fuzzy
> matching:
> lmer-class(Matrix)               Mixed model representations
>
> So, I loaded library Matrix and tried again
>> library(Matrix)
>> help(lmer-class)
> No documentation for 'lmer - class' in specified packages and 
> libraries:
> you could try 'help.search("lmer - class")'
>
> If someone could tell me how to extract the Estimates and Std. 
> Errors from
> the lmer summary, I'd be very grateful. I would also be very 
> grateful if
> someone
> could let me know why the Estimates and Std. Errors from the lmer 
> model are
> both
> larger than those from the glmmPQL model.
>
> I am running R 2.2.1 on a Windows XP machine.
>
> Thanks,
>
> Jonathan Williams
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ligges at statistik.uni-dortmund.de  Tue Jan 10 12:12:42 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 Jan 2006 12:12:42 +0100
Subject: [R] exporting methods/classes
In-Reply-To: <11544d000601100245g5b6441bfn@mail.gmail.com>
References: <11544d000601100245g5b6441bfn@mail.gmail.com>
Message-ID: <43C396AA.1030609@statistik.uni-dortmund.de>

Jean-Christophe BOUETTE wrote:

>>---------- Message transf??r?? ----------
>>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>To: Erin Hodgess <hodgess at gator.dt.uh.edu>
>>Date: Sun, 08 Jan 2006 15:09:06 +0100
>>Subject: Re: [R] exporting methods/classes
>>Erin Hodgess wrote:
>>
>>
>>>Dear R People:
>>>
>>>I'm still struggling with sending methods and classes as part of
>>>creating a new package.
>>>
>>>Where does the .onLoad function go?  Within R itself or in a file
>>>in one of the new package directories?
>>
>>Simply save the .onLoad function in some .R file (e.g. zzz.R) in the
>>package's ./R directory.
>>
>>
>>
>>
>>>Here are my latest efforts:
>>>
>>>
>>>Here's the last part of the woof1-Ex.Rout
>>>
>>>
>>>
>>>>library('woof1')
>>>
>>>Error in loadNamespace(package, c(which.lib.loc, lib.loc),
>>>keep.source = keep.source) :
>>>      in 'woof1' classes for export not defined: dog
>>>Error: package/namespace load failed for 'woof1'
>>>Execution halted
>>
>>So this looks like you have defined S4 classes for export in your
>>Namespace but you have no call that starts with
>>
>>setClass("dog", ......
>>
> 
> Well, it's not that easy for a newbie (like me) to track that problem.
> Using package.skeleton() (on my old R 2.1.1) puts all the function
> definitions in the ./R subdirectory, but not the call to setClass (or
> did I make something wrong?). Maybe it's one of the things that should
> be documented to make package creation easier. Of course, I know
> somebody will point me to some doc where it is stated but the fact is
> that it was not obvious.

Well, according to the documentation, package.skeleton "saves functions 
and data to appropriate places" (but not calls like setClass() and friends).

Please consider package.skeleton() only as a function that helps to set 
up the basic directory structure and required files. You have to edit 
the files afterwards, and add stuff like the mentioned calls to all 
those S4 generators.


> By the way, I'd like to thank Uwe Ligges and Duncan Murdoch for the
> helpful article in R-Help 5/2, it was really useful to me. If I manage

Nice to hear (you mean R News, though), thank you.
Although I still do not understand why it helps so much more to have 
those examples (compared to the information given in the manual).

Uwe Ligges



> to understand all the steps necessary to make an S4 namespace package
> I promise to write a "S4 Package step by step for newbies" :-)
> 
> Regards,
> Jean-Christophe Bou??tt??.
> 
> 
> 
>>in the R code in your package, hence the class "dog" is undefined.
>>
>>
>>
>>>Here's the NAMESPACE
>>>importFrom(graphics,plot)
>>>exportClasses("dog")
>>>exportMethods("plot","show")
>>
>>
>>You don't export any other functions?
>>
>>
>>Uwe Ligges
>>
>>
>>
>>
>>
>>>thanks yet again,
>>>Sincerely,
>>>Erin
>>>mailto: hodgess at gator.uhd.edu
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 10 12:28:44 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 10 Jan 2006 11:28:44 -0000 (GMT)
Subject: [R] R newbie example code question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
Message-ID: <XFMail.060110112844.Ted.Harding@nessie.mcc.ac.uk>


On 09-Jan-06 Mark Leeds wrote:
> Sometimes I print out a package
> and read about it and there
> are sometimes nice examples
> that I would like to run myself.
>  
> Is there a way to bring them
> into R from the package or
> are they only meant to be typed
> in manually ? If manual is the
> only way, that's fine. I was
> just checking whether there was
> a quicker way. Thanks.
>  
>                            Mark
> 
> 
> **********************************************************************
> This email and any files transmitted with it are
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 10 12:40:47 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 10 Jan 2006 11:40:47 -0000 (GMT)
Subject: [R] R newbie example code question
In-Reply-To: <XFMail.060110112844.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.060110114047.Ted.Harding@nessie.mcc.ac.uk>

On 10-Jan-06 Ted Harding wrote:
> 
> On 09-Jan-06 Mark Leeds wrote:
>> Sometimes I print out a package
>> and read about it and there
>> are sometimes nice examples
>> that I would like to run myself.
>>  
>> Is there a way to bring them
>> into R from the package or
>> are they only meant to be typed
>> in manually ? If manual is the
>> only way, that's fine. I was
>> just checking whether there was
>> a quicker way. Thanks.
>>  
>>                            Mark

Apologies for sending a NULL response!
My machine froze as I was working how to write the
response, and on re-boot it seems to have sent the
message all by itself!

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Jan-06                                       Time: 11:40:44
------------------------------ XFMail ------------------------------



From e.pebesma at geog.uu.nl  Tue Jan 10 13:36:20 2006
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Tue, 10 Jan 2006 13:36:20 +0100
Subject: [R] [R-pkgs] new gstat version
Message-ID: <43C3AA44.4000303@geog.uu.nl>

Soon on CRAN a new version of package gstat will emerge, which
has a few minor changes and possible incompatibilities w.r.t. the
previous version(s).

The new gstat (0.9-23) now:
+ depends on sp, and uses internally with Spatial* classes from sp
   if data are provided in the old-fashoned way (as data.frame)
+ has a vignette to get you started with the classes in sp
+ defines krige as a generic; two typical uses are:

# "old-style", using data.frame's:
library(gstat)
data(meuse)
data(meuse.grid)
v = vgm(0.6, "Sph", 900, 0.1) # spherical variogram model
zn.kri1 = krige(log(zinc)~1, ~x+y, meuse, meuse.grid, v)
image(zn.kri1)

# "new-style", using SpatialPointsDataFrame's:
coordinates(meuse) = c("x", "y") # promote meuse to SpatialPointsDataFrame
coordinates(meuse.grid) = c("x", "y") # promote to SpatialPointsDataFrame
gridded(meuse.grid) = TRUE # promote to SpatialPixelsDataFrame
zn.kri2 = krige(log(zinc)~1, meuse, meuse.grid, v)
spplot(zn.kri2[1])

+ provides a generic idw for inverse distance interpolation
(calling sequence identical to krige)

INCOMPATIBLE CHANGE:
  the krige generic is

     krige(formula, locations, ...)

  based on the class of argument 2 (locations) either of the two
  versions (see above) is called. This means that no named argument
  "data" should be passed in case locations is missing (i.e., when
  data is of class SpatialPointsDataFrame). Instead, pass it as
  argument 2 or as named argument "locations".

--
Edzer

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tom at maladmin.com  Mon Jan  9 16:54:34 2006
From: tom at maladmin.com (tom wright)
Date: Mon, 09 Jan 2006 10:54:34 -0500
Subject: [R] data aquistion cards
Message-ID: <1136822074.4525.54.camel@localhost.localdomain>

Another year another project.
Does anyone have any experience / pointers in obtaining data from an
analog / digital converter hardware such as a National Instruments data
aquisition card using R?
I really need to obtain and process this data in a real time enviroment
and I'm hoping that R already has packages to help me with this.
If anyone has any experience with this I'd love to hear about it.
Many thanks
Tom



From Antje.Schuele at komdat.com  Tue Jan 10 14:29:21 2006
From: Antje.Schuele at komdat.com (=?iso-8859-1?Q?Antje_Sch=FCle?=)
Date: Tue, 10 Jan 2006 14:29:21 +0100
Subject: [R] two y-axis in xy-plot
Message-ID: <F5076E7EAA58F448A0EEC05ADE2317BD0304A2@muc-exch001.munich.komdat.intern>


Hi all,

I think there is something I have forgotten to emphasize. I'd like to have an xyplot (with lattice library). A trellis graphic. For every weekday I like to have a graphic with all hours. And inside the graphic there are the two frequencies as lines.  

The problem finally is the use of the xyplot in library(lattice) in combination with the second y-axis.

Thanks.

Antje


-------------------------------------------------------

It is nearly the same example I wrote about in http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg54238.html. I'll print it out again:

In the first column I have numbers from 0 to 23 (hours of a day), the second 
column contains the name of a weekday (Day as factor) and the third column 
contains the number I am interested in. So as an example, the first five rows look like that:

     Hour Day Freq

1     0  Mo    23

2     1  Mo    20

3     2  Mo    14

4     3  Mo    27

5     4  Mo    26

 
To read: On Monday between 0 and 1 o'clock 23 things happened. 

Now I add a new parameter, so that the data looks somehow like that:

     Hour Day Freq Freq2

1     0  Mo    23	 874

2     1  Mo    20   476

3     2  Mo    14	 201

4     3  Mo    27   912

5     4  Mo    26   172

Now I'd like to have a plot for every weekday containing the two frequencies as a line during the hours.

Is this possible with two y-axes?

Thanks for your help, 

Antje




-----Urspr??ngliche Nachricht-----
Von: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Gesendet: Montag, 9. Januar 2006 18:08
An: Antje Sch??le
Cc: r-help at stat.math.ethz.ch
Betreff: Re: two y-axis in xy-plot

On 1/9/06, Antje Sch??le <Antje.Schuele at komdat.com> wrote:
> Hi there,
>
>
>
> I am wondering if it is possible to do an xyplot with two y-axes. I'd like
> to print two parameters in a time series but they both have different
> scales.
>
>
>
> Which parameter in xyplot can I add to achieve this result?

The answer depends on the details of what you are doing, so please
provide us with a reproducible example.

Deepayan



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 10 14:30:37 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 10 Jan 2006 13:30:37 -0000 (GMT)
Subject: [R] R newbie example code question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
Message-ID: <XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>

On 09-Jan-06 Mark Leeds wrote:
> Sometimes I print out a package
> and read about it and there
> are sometimes nice examples
> that I would like to run myself.
>  
> Is there a way to bring them
> into R from the package or
> are they only meant to be typed
> in manually ? If manual is the
> only way, that's fine. I was
> just checking whether there was
> a quicker way. Thanks.
>  
>                            Mark

I had the same issue many moons ago, and for the
same reasons! It was discussed in a fairly extended,
and interesting, thread from 24-27 May 2003 -- go to
the R-help archives for that month, by thread, and
find the thread

  [R] help output paged in separate window

The solution I finally opted for, and still use,
is based (in a Linux environment) on including
the following code in your .Rprofile file:


.xthelp <- function() {
    tdir <- tempdir()
    pgr <- paste(tdir, "/pgr", sep="")
    con <- file(pgr, "w")
    cat("#! /bin/bash\n", file=con)
    cat("export HLPFIL=`mktemp ", tdir, "/R_hlp.XXXXXX`\n",
         sep="", file=con)
    cat("cat > $HLPFIL\nxterm -e less $HLPFIL &\n", file=con)
    close(con)
    system(paste("chmod 755 ", pgr, sep=""))
    options(pager=pgr)
}
.xthelp()
rm(.xthelp)


(and it's also specific to the 'bash' shell because
of the "#! /bin/bash\n", but you should be able to
change this appropriately). The above was posted by
Roger Bivand on 27 May.

When you start an R session, this code is executed as
part of sourcing your .Rprofile, and it has the effect that
any output from R which would be paged is stored in a
temporary  file which is then read by 'less' in a
separate X window which is detached from your R session
(i.e. your command interface will not hang while it is
being displayed). You can close the X window displaying
the 'less' output by closing 'less' (e.g. type "q"), or
you can leave it open and any new paged output will go
into a new window -- so you can for instance do

?glm
?family
?binomial

and you will have three mutually relevant help windows
open at once between which you can cross-reference.

As to extracting the code for examples, this is easy
in X windows since you just use your mouse: left-button
drag to hghlight a block of text, middle-button click
to paste the block into another window (of course the
mouse must be over the correct window!)

So you can use the mouse to copy code from the "help"
pages to the command window.

As I say, this is a Linux-oriented solution, and I don't
know what details would be required for anything similar
in a Windows environment.

It is also worth reading the various contributions to
the above thread, since several suggestions were made.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Jan-06                                       Time: 13:30:35
------------------------------ XFMail ------------------------------



From fredrik.bg.lundgren at bredband.net  Tue Jan 10 14:37:30 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Tue, 10 Jan 2006 14:37:30 +0100
Subject: [R] "Missing value representation in Excel before extraction to
	R with RODBC"
References: <43C3879E.22583.6BFC88@localhost>
Message-ID: <000d01c615eb$010c29b0$4a9d72d5@Larissa>

Dear Petr,

Thank you for your help. I have tried (and succeded) to import myfile 
after conversion to text and with the help of read.table (also with the 
file = 'clipboard' alternative). Both methods give correct results, 
albeit dateformat turns up as a factor (minor problem). Also the 
read.xls from library gdata has been successful, albeit with some 
different representation of dateformat( minor problem). At least my 
Excelfile isn't corrupted in such a way to make this three ways 
impossible. No, my problem appears to be connected to the use of RODBC 
and that was what I wanted to get working. The first 8 rows are excluded 
from the file and columns with many NA's at the start are tranformed to 
all NA. If the NA's at the beginning of a column are given values (i. e. 
not NA) the tranformation of the column doesn't take place but the first 
8 rows are still excluded. I have tried - not necessarily in a correct 
way - to use as.is (keeps the dateformat correct) and  colClasses 
(doesn't apply?) but haven't been able to sort the problem out with 
these options.

Best wishes

Fredrik
----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help" 
<r-help at stat.math.ethz.ch>
Sent: Tuesday, January 10, 2006 10:08 AM
Subject: Re: [R] "Missing value representation in Excel before 
extraction to R with RODBC"


> Hi
>
> I tried to reproduce what you have told us by copy and paste
>
> read.delim("clipboard")
>
> but was not successful.
>
> Even with several blank values in each column in Excel i got correct
> import to R by this process. As I do not use RODBC I do not know all
> possible settings and features. If colClasses is available you can
> force the columns to by character, numeric, factor, Date or some
> other class.
>
> BTW Excel can be quite tricky and hides e.g. spaces in cells so you
> see them as empty even if they are not. So if I get some weird
> conversions of numeric columns there is often something hidden in
> Excel.
>
> HTH
> Petr
>
>
> On 9 Jan 2006 at 18:06, Fredrik Lundgren wrote:
>
> From:           "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
> To:             "Prof Brian Ripley" <ripley at stats.ox.ac.uk>,
> "Petr Pikal" <petr.pikal at precheza.cz>
> Copies to:      "R-help" <r-help at stat.math.ethz.ch>
> Subject:        Re: [R] "Missing value representation in Excel before 
> extraction to R with RODBC"
> Date sent:      Mon, 9 Jan 2006 18:06:49 +0100
>
>> Dear list,
>>
>> Well, those columns in Excel that starts with NA (actually 8 NA's in
>> my case) is imported as all NA in R but if the columns starts with at
>> least 3 cells with values (i.e not NA) the are imported correctly to
>> R. When as.is=TRUE is used a simular conversion takes place but now 
>> as
>> all <NA> and dates are represented as date-and-time. Is there any way
>> to get this correct even when the Excel columns start with several
>> NA's?
>>
>> Sincerely
>> Fredrik
>>
>>
>> ----- Original Message ----- 
>> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
>> To: "Petr Pikal" <petr.pikal at precheza.cz>
>> Cc: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help"
>> <r-help at stat.math.ethz.ch> Sent: Monday, January 09, 2006 9:36 AM
>> Subject: Re: [R] "Missing value representation in Excel before
>> extraction to R with RODBC"
>>
>>
>> > On Mon, 9 Jan 2006, Petr Pikal wrote:
>> >
>> >> Hi
>> >>
>> >> I believe it has something to do with the column identification
>> >> decision. When R decides what is in a column it uses only some
>> >> values from the beginning of a file.
>> >
>> > Not R, Excel.  Excel tells ODBC what the column types are.
>> >
>> >> I do not use RODBC as read.delim("clipboard", ...) is usually more
>> >> convenient but probably there is a way how to tell RODBC what is 
>> >> in
>> >> the column instead of let R decide from the top of the file.
>> >
>> > Using as.is=TRUE stops RODBC doing any conversion.
>> >
>> >> But I may be completely mistaken.
>> >>
>> >> HTH
>> >> Petr
>> >>
>> >>
>> >> On 6 Jan 2006 at 20:47, Fredrik Lundgren wrote:
>> >>
>> >> From:           "Fredrik Lundgren"
>> >> <fredrik.bg.lundgren at bredband.net> To:             "R-help"
>> >> <r-help at stat.math.ethz.ch> Date sent:      Fri, 6 Jan 2006 
>> >> 20:47:29
>> >> +0100 Subject:        [R] "Missing value representation in Excel
>> >> before extraction to R with RODBC"
>> >>
>> >>> Dear list,
>> >>>
>> >>> How should missing values be expressed in Excel before extraction
>> >>> to R via RODBC. I'm bewildered. Sometimes the representation with
>> >>> NA in Excel appears to work and shows up in R as <NA> but
>> >>> sometimes the use of NA in Excel changes the whole vector to 
>> >>> NA's.
>> >>> Blank or nothing or NA as representation for missing values in
>> >>> Excel with dateformat gives NA's of the whole vector in R but 
>> >>> with
>> >>>  general format in Excel gives blanks for missing values in R. 
>> >>> How
>> >>> should I represent missing values in Excel?
>> >>>
>> >>>
>> >>> Best wishes and thanks for any help
>> >>> Fredrik Lundgren
>> >
>> > -- 
>> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> > Professor of Applied Statistics, 
>> > http://www.stats.ox.ac.uk/~ripley/
>> > University of Oxford,             Tel:  +44 1865 272861 (self) 1
>> > South Parks Road,                     +44 1865 272866 (PA) Oxford
>> > OX1 3TG, UK                Fax:  +44 1865 272595
>> >
>>
>>
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>



From maechler at stat.math.ethz.ch  Tue Jan 10 14:48:04 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 10 Jan 2006 14:48:04 +0100
Subject: [R] R - Wikis and R-core
In-Reply-To: <20060108160043.E66F345E@wmailp01.st2.lyceu.net>
References: <43BDC208.5060909@vanderbilt.edu>
	<20060108160043.E66F345E@wmailp01.st2.lyceu.net>
Message-ID: <17347.47892.343299.844202@stat.math.ethz.ch>

We've had a small "review time" within R-core on this topic,
amd would like to state the following:

--------------------------------------------------------------------------
The R-core team welcomes proposals to develop an R-wiki.

- We would consider linking a very small number of Wikis (ideally one) 
  from www.r-project.org and offering an address in the r-project.org 
  domain (such as 'wiki.r-project.org').

- The core team has no support time to offer, and would be looking for
  a medium-term commitment from a maintainer team for the Wiki(s).

- Suggestions for the R documentation would best be filtered through the 
  Wiki maintainers, who could e.g. supply suggested patches during the alpha 
  phase of an R release.
--------------------------------------------------------------------------

Our main concerns have been about ensuring the quality of such extra
documentation projects, hence the 2nd point above.
Several of our more general, not mainly R, experiences have been
of outdated web pages which are continued to be used as
reference when their advice has long been superseded.  
I think it's very important to try ensuring that this won't
happen with an R Wiki.

Martin Maechler, ETH Zurich

>>>>> "PhGr" == Philippe Grosjean <phgrosjean at sciviews.org>
>>>>>     on Sun, 8 Jan 2006 17:00:44 +0100 (CET) writes:

    PhGr> Hello all, Sorry for not taking part of this
    PhGr> discussion earlier, and for not answering Detlef
    PhGr> Steuer, Martin Maechler, and others that asked more
    PhGr> direct questions to me. I am away from my office and
    PhGr> my computer until the 16th of January.

    PhGr> Just quick and partial answers: 1) I did not know
    PhGr> about Hamburg RWiki. But I would be happy to merge
    PhGr> both in one or the other way, as Detlef suggests it.

    PhGr> 2) I choose DokuWiki as the best engine after a
    PhGr> careful comparison of various Wiki engines. It is the
    PhGr> best one, as far as I know, for the purpose of
    PhGr> writting software documentation and similar
    PhGr> pages. There is an extensive and clearly presented
    PhGr> comparison of many Wikki engines at:
    PhGr> http://www.wikimatrix.org/.

    PhGr> 3) I started to change DokuWiki (addition of various
    PhGr> plugins, addition of R code syntax coloring with
    PhGr> GESHI, etc...). So, it goes well beyond all current
    PhGr> Wiki engines regarding its suitability to present R
    PhGr> stuff.

    PhGr> 4) The reasons I did this is because I think the Wiki
    PhGr> format could be of a wider use. I plan to change a
    PhGr> little bit the DokuWiki syntax, so that it works with
    PhGr> plain .R code files (Wiki part is simply embedded in
    PhGr> commented lines, and the rest is recognized and
    PhGr> formatted as R code by the Wiki engine). That way, the
    PhGr> same Wiki document can either rendered by the Wiki
    PhGr> engine for a nice presentation, or sourced in R
    PhGr> indifferently.

    PhGr> 5) My last idea is to add a Rpad engine to the Wiki,
    PhGr> so that one could play with R code presented in the
    PhGr> Wiki pages and see the effects of changes directly in
    PhGr> the Wiki.

    PhGr> 6) Regarding the content of the Wiki, it should be
    PhGr> nice to propose to the authors of various existing
    PhGr> document to put them in a Wiki form. Something like
    PhGr> "Statistics with R"
    PhGr> (http://zoonek2.free.fr/UNIX/48_R/all.html) is written
    PhGr> in a way that stimulates additions to pages in
    PhGr> perpetual construction, if it was presented in a Wiki
    PhGr> form. It is licensed as Creative Commons
    PhGr> Attribution-NonCommercial-ShareAlike 2.5 license, that
    PhGr> is, exactly the same one as DokuWiki that I choose for
    PhGr> R Wiki. Of course, I plan to ask its author to do so
    PhGr> before putting its hundreds of very interesting pages
    PhGr> on the Wiki... I think it is vital to have already
    PhGr> something in the Wiki, in order to attract enough
    PhGr> readers, and then enough contributors!

    PhGr> 7) Regarding spamming and vandalism, DokuWiki allows
    PhGr> to manage rights and users, even individually for
    PhGr> pages. I think it would be fine to lock pages that
    PhGr> reach a certain maturity (read-only / editable by
    PhGr> selected users only) , with link to a discussion page
    PhGr> which remaining freely accessible at the bottom of
    PhGr> locked pages.

    PhGr> 8) I would be happy to contribute this work to the R
    PhGr> foundation in one way or the other to integrate it in
    PhGr> http://www.r-project.org or
    PhGr> http://cran.r-project.org. But if it is fine keeping
    PhGr> it in http://www.sciviews.org as well, it is also fine
    PhGr> for me.

    PhGr> I suggest that all interested people drop a little
    PhGr> email to my mailbox.  I'll recontact you when I will
    PhGr> be back to my office to work on a more elaborate
    PhGr> solution altogether when I am back at my office.
    PhGr> Best,

    PhGr> Philippe Grosjean

    PhGr> ______________________________________________
    PhGr> R-help at stat.math.ethz.ch mailing list
    PhGr> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    PhGr> read the posting guide!
    PhGr> http://www.R-project.org/posting-guide.html



From tom at maladmin.com  Tue Jan 10 10:17:08 2006
From: tom at maladmin.com (tom wright)
Date: Tue, 10 Jan 2006 04:17:08 -0500
Subject: [R] data collection with R
Message-ID: <1136884628.4525.56.camel@localhost.localdomain>

Another year another project.
Does anyone have any experience / pointers in obtaining data from an
analog / digital converter hardware such as a National Instruments data
aquisition card using R?
I really need to obtain and process this data in a real time enviroment
and I'm hoping that R already has packages to help me with this.
If anyone has any experience with this I'd love to hear about it.
Many thanks
Tom



From dargosch at gmail.com  Tue Jan 10 15:40:23 2006
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Tue, 10 Jan 2006 15:40:23 +0100
Subject: [R] Repeated measures aov with post hoc tests?
Message-ID: <376e97ec0601100640h4cd3f01am60d2da17d914589a@mail.gmail.com>

Dear list,

I would like to perform an analysis on the following model:

aov(ampratio ~ Type * Place * agemF + Error(speakerid/Place) ,data=aspvotwork)

using the approach from http://www.psych.upenn.edu/~baron/rpsych/rpsych.html .

Now, I got the test results, wich indicate a significant interaction
and main effects of the agemF variable. How do I find at what level of
agemF the effect may be found.

How do I do this?

I found a reference to TukeyHSD in the archives, but I cannot use it:

> TukeyHSD(aov(ampratio ~ Type * Place * agemF + Error(speakerid/Place),data=aspvotwork))
Error in TukeyHSD(aov(ampratio ~ Type * Place * agemF +
Error(speakerid/Place),  :
	no applicable method for "TukeyHSD"

Please help me.

/Fredrik



From gsxej2 at cam.ac.uk  Tue Jan 10 15:47:43 2006
From: gsxej2 at cam.ac.uk (Gregory Jefferis)
Date: Tue, 10 Jan 2006 14:47:43 +0000
Subject: [R] Correct way to test for exact dimensions of matrix or array
Message-ID: <BFE9798F.797A%gsxej2@cam.ac.uk>

Dear R Users,

I want to test the dimensions of an incoming vector, matrix or array safely
and succinctly.  Specifically I want to check if the unknown object has
exactly 2 dimensions with a specified number of rows and columns.

I thought that the following would work:

> obj=matrix(1,nrow=3,ncol=5)
> identical( dim( obj) , c(3,5) )
[1] FALSE  

But it doesn't because c(3,5) is numeric and the dims are integer.  I
therefore ended up doing something like:

> identical( dim( obj) , as.integer(c(3,5)))

OR

> isTRUE(all( dim( obj) == c(3,5) ))

Neither of which feel quite right.  Is there a 'correct' way to do this?

Many thanks,

Greg Jefferis.

PS Thinking about it, the second form is (doubly) wrong because:

> obj=array(1,dim=c(3,5,3,5))
> isTRUE(all( dim( obj) == c(3,5) ))
[1] TRUE

OR 
> obj=numeric(10)
> isTRUE(all( dim( obj) == c(3,5) ))
[1] TRUE

(neither of which are equalities that I am happy with!)

-- 
Gregory Jefferis, PhD                               and:
Research Fellow    
Department of Zoology                               St John's College
University of Cambridge                             Cambridge
Downing Street                                      CB2 1TP
Cambridge, CB2 3EJ 
United Kingdom

Tel: +44 (0)1223 336683                             +44 (0)1223 339899
Fax: +44 (0)1223 336676                             +44 (0)1223 337720

gsxej2 at cam.ac.uk



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 10 16:03:35 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 10 Jan 2006 16:03:35 +0100
Subject: [R] Correct way to test for exact dimensions of matrix or array
References: <BFE9798F.797A%gsxej2@cam.ac.uk>
Message-ID: <012401c615f7$0784ffe0$0540210a@www.domain>

you could use: isTRUE(all.equal(dim(obj), c(3, 5)))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gregory Jefferis" <gsxej2 at cam.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 10, 2006 3:47 PM
Subject: [R] Correct way to test for exact dimensions of matrix or 
array


> Dear R Users,
>
> I want to test the dimensions of an incoming vector, matrix or array 
> safely
> and succinctly.  Specifically I want to check if the unknown object 
> has
> exactly 2 dimensions with a specified number of rows and columns.
>
> I thought that the following would work:
>
>> obj=matrix(1,nrow=3,ncol=5)
>> identical( dim( obj) , c(3,5) )
> [1] FALSE
>
> But it doesn't because c(3,5) is numeric and the dims are integer. 
> I
> therefore ended up doing something like:
>
>> identical( dim( obj) , as.integer(c(3,5)))
>
> OR
>
>> isTRUE(all( dim( obj) == c(3,5) ))
>
> Neither of which feel quite right.  Is there a 'correct' way to do 
> this?
>
> Many thanks,
>
> Greg Jefferis.
>
> PS Thinking about it, the second form is (doubly) wrong because:
>
>> obj=array(1,dim=c(3,5,3,5))
>> isTRUE(all( dim( obj) == c(3,5) ))
> [1] TRUE
>
> OR
>> obj=numeric(10)
>> isTRUE(all( dim( obj) == c(3,5) ))
> [1] TRUE
>
> (neither of which are equalities that I am happy with!)
>
> -- 
> Gregory Jefferis, PhD                               and:
> Research Fellow
> Department of Zoology                               St John's 
> College
> University of Cambridge                             Cambridge
> Downing Street                                      CB2 1TP
> Cambridge, CB2 3EJ
> United Kingdom
>
> Tel: +44 (0)1223 336683                             +44 (0)1223 
> 339899
> Fax: +44 (0)1223 336676                             +44 (0)1223 
> 337720
>
> gsxej2 at cam.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From maechler at stat.math.ethz.ch  Tue Jan 10 16:13:30 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 10 Jan 2006 16:13:30 +0100
Subject: [R] Correct way to test for exact dimensions of matrix or array
In-Reply-To: <BFE9798F.797A%gsxej2@cam.ac.uk>
References: <BFE9798F.797A%gsxej2@cam.ac.uk>
Message-ID: <17347.53018.474709.164656@stat.math.ethz.ch>

>>>>> "Gregory" == Gregory Jefferis <gsxej2 at cam.ac.uk>
>>>>>     on Tue, 10 Jan 2006 14:47:43 +0000 writes:

    Gregory> Dear R Users,

     Gregory> I want to test the dimensions of an incoming
     Gregory> vector, matrix or array safely


    Gregory> and succinctly.  Specifically I want to check if
    Gregory> the unknown object has exactly 2 dimensions with a
    Gregory> specified number of rows and columns.

    Gregory> I thought that the following would work:

    >> obj=matrix(1,nrow=3,ncol=5)
    >> identical( dim( obj) , c(3,5) )
    Gregory> [1] FALSE  

    Gregory> But it doesn't because c(3,5) is numeric and the dims are integer.  I
    Gregory> therefore ended up doing something like:

    >> identical( dim( obj) , as.integer(c(3,5)))

    Gregory> OR

    >> isTRUE(all( dim( obj) == c(3,5) ))

the last one is almost perfect if you leave a way the superfluous
isTRUE(..).

But, you say that it's part of your function checking it's
arguments.
In that case, I'd recommend

     if(length(d <- dim(obj)) != 2) 
	   stop("'d' must be matrix-like")
     if(!all(d == c(3,5)))
	   stop("the matrix must be  3 x 5")

which also provides for nice error messages in case of error.
A more concise form with less nice error messages is

  stopifnot(length(d <- dim(obj)) == 2,
            d == c(3,50)) 

  ## you can leave away  all(.)  for things in stopifnot(.) 




    Gregory> Neither of which feel quite right.  Is there a 'correct' way to do this?

    Gregory> Many thanks,

You're welcome,
Martin Maechler, ETH Zurich

    Gregory> Greg Jefferis.

    Gregory> PS Thinking about it, the second form is (doubly) wrong because:

    >> obj=array(1,dim=c(3,5,3,5))
    >> isTRUE(all( dim( obj) == c(3,5) ))
    Gregory> [1] TRUE

    Gregory> OR 
    >> obj=numeric(10)
    >> isTRUE(all( dim( obj) == c(3,5) ))
    Gregory> [1] TRUE

    Gregory> (neither of which are equalities that I am happy with!)



From paladini at rz.uni-potsdam.de  Tue Jan 10 16:27:40 2006
From: paladini at rz.uni-potsdam.de (paladini@rz.uni-potsdam.de)
Date: Tue, 10 Jan 2006 16:27:40 +0100
Subject: [R] (no subject)
Message-ID: <1136906859.43c3d26c003cc@webmail.uni-potsdam.de>

Dear ladies and gentlemen!
When I use the plot funtion how can I change the size of the title for the x and
y axes (xlab, ylab)and the size of the axes label ?

Thank you very much.

With best regards

Claudia



From aitor_doctorado at yahoo.es  Tue Jan 10 16:33:32 2006
From: aitor_doctorado at yahoo.es (Aitor Mata Conde)
Date: Tue, 10 Jan 2006 16:33:32 +0100 (CET)
Subject: [R] Working with R in a multi-processor machine.
Message-ID: <20060110153332.5511.qmail@web30115.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/ff09fdf6/attachment.pl

From kubovy at virginia.edu  Tue Jan 10 16:33:41 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 10 Jan 2006 10:33:41 -0500
Subject: [R] Hmisc xYplot: two ablines?
Message-ID: <3ABCC7EA-BA7A-4ABE-96A3-25BFACD7B7B8@virginia.edu>

Dear r-helpers,

Happy New Year.

To a plot

xYplot(lo ~ vaR, groups = v, data = abc1.fp,aspect = "xy",
	xlab=expression(frac(abs( bold(v) ),abs( bold(a) ))),
	ylab = grid::textGrob(expression(paste(log, frac( italic(p) ( italic 
(v) ), italic(p) ( italic(a) )))) ),
	abline =list(a = 5.71442, b = -5.71442, col = 2)
)

I would like to add another abline such as abline = list(a = 8.8460,  
b = -8.8460, col = 3). Any advice?


_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From pensterfuzzer at yahoo.de  Tue Jan 10 16:49:15 2006
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Tue, 10 Jan 2006 16:49:15 +0100 (CET)
Subject: [R] matching country name tables from different sources
Message-ID: <20060110154915.21206.qmail@web25805.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/07ed49b7/attachment.pl

From gelman at stat.columbia.edu  Tue Jan 10 16:50:46 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Tue, 10 Jan 2006 10:50:46 -0500
Subject: [R] another question about lmer, this time involving coef()
Message-ID: <43C3D7D6.4050304@stat.columbia.edu>

I'm having another problem with lmer(), this time something simpler (I 
think) involving the coef() function for a model with varying 
coefficients.  Here's the R code.  It's a simple model with 2 
observations per group and 10 groups:

# set up the predictors

n.groups <- 10
n.reps <- 2
n <- n.groups*n.reps
group.id <- rep (1:n.groups, each=n.reps)

# simulate the varying parameters

a.group <- rnorm (n.groups, 1, 2)

# simulate the data and print to check that i did it right

y <- rnorm (n, a.group[group.id], 1)
print (cbind (y, group.id))

# fit and summarize the model

fit.1 <- lmer (y ~ 1 + (1 | group.id))
summary (fit.1)

# coef() doesn't work!
coef (fit.1)

--

The following error message came:

Error in "rownames<-"(x, value) : attempt to set rownames on object with 
no dimensions

--

So I went into the code and the coefficient info is there.  There just 
seems to be some "bookkeeping problem" within coef().  I could kludge 
something but I'd rather work within the existing structures and use 
coef() which seems intended for this purpose.

OK, then I tried adding a predictor and it worked fine:

x <- rnorm (n)
fit.2 <- lmer (y ~ 1 + x + (1 | group.id))
summary (fit.2)
coef (fit.2)

--

Am I doing something stupid here or is it actually a (minor) bug?

Thanks
Andrew


-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From info at aghmed.fsnet.co.uk  Tue Jan 10 16:51:00 2006
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 10 Jan 2006 15:51:00 +0000
Subject: [R] Wikis etc.
In-Reply-To: <971536df0601090806s1b58b40ag7bbb34e4212932cd@mail.gmail.co
 m>
References: <BAY102-F268BA8F5347CA44DD7E38CA230@phx.gbl>
	<6.2.1.2.0.20060109121728.029032d0@pop.freeserve.net>
	<Pine.LNX.4.64.0601090718060.13171@homer24.u.washington.edu>
	<971536df0601090806s1b58b40ag7bbb34e4212932cd@mail.gmail.com>
Message-ID: <6.2.1.2.0.20060110154058.0290a290@pop.freeserve.net>

At 16:06 09/01/06, Gabor Grothendieck wrote:

[snip various earlier posts]


>In addition to books, the various manuals, contributed documents and
>mailing list archives, all of which one should review,
>the key thing to do if you want to really learn R is to read source code
>and lots of it.  I think there is no other way.  Furthermore, the fact that
>you can do this is really a key advantage of open source.

But that is the solution to a different problem.
Reading the source for merge tells you how R merges two dataframes, the 
beginning user wants to know how to link together the information s/he has 
in two files but does not know what the name of the relevant command is, or 
indeed whether it is even possible.

To give you some idea of how ignorant some of us are it was only quite 
recently that I realised (despite several years reading free documentation, 
books on R and R-help) that if I type cor at the prompt what I see looks 
like source code but is not _the_ source code.


Michael Dewey
http://www.aghmed.fsnet.co.uk



From ligges at statistik.uni-dortmund.de  Tue Jan 10 16:55:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 10 Jan 2006 16:55:20 +0100
Subject: [R] (no subject)
In-Reply-To: <1136906859.43c3d26c003cc@webmail.uni-potsdam.de>
References: <1136906859.43c3d26c003cc@webmail.uni-potsdam.de>
Message-ID: <43C3D8E8.5090609@statistik.uni-dortmund.de>

paladini at rz.uni-potsdam.de wrote:

> Dear ladies and gentlemen!
> When I use the plot funtion how can I change the size of the title for the x and
> y axes (xlab, ylab)and the size of the axes label ?
> 
> Thank you very much.
> 
> With best regards
> 
> Claudia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
     ^^^^ ^^^^ ^^^^ ^^^^ ^^^^ ^

- Yes, please do!!!
- Please use a sensible subject line.
- Please read the documentation:
?plot points you to ?par and ?par tells it all.

Uwe Ligges



From Robert.McGehee at geodecapital.com  Tue Jan 10 16:55:47 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 10 Jan 2006 10:55:47 -0500
Subject: [R] R newbie example code question
Message-ID: <67DCA285A2D7754280D3B8E88EB548020F8EF0CA@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
Why not just copy and paste the examples from the help pages? No need to
type anything.

However, if you'd like to run the entire help section for a function,
check out the example function, i.e.:
> example(plot)

Robert

-----Original Message-----
From: Mark Leeds [mailto:Mleeds at kellogggroup.com] 
Sent: Monday, January 09, 2006 1:20 PM
To: R-Stat Help
Subject: [R] R newbie example code question

Sometimes I print out a package
and read about it and there
are sometimes nice examples
that I would like to run myself.
 
Is there a way to bring them
into R from the package or
are they only meant to be typed
in manually ? If manual is the
only way, that's fine. I was
just checking whether there was
a quicker way. Thanks.
 
                           Mark


**********************************************************************
This email and any files transmitted with it are
confidentia...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From alexandrarma at yahoo.com.br  Tue Jan 10 17:07:43 2006
From: alexandrarma at yahoo.com.br (Alexandra R. M. de Almeida)
Date: Tue, 10 Jan 2006 13:07:43 -0300 (ART)
Subject: [R] Obtaining the adjusted r-square given the regression
	coefficients
Message-ID: <20060110160743.19355.qmail@web33307.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/67a16124/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Jan 10 17:24:25 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jan 2006 17:24:25 +0100
Subject: [R] another question about lmer, this time involving coef()
In-Reply-To: <43C3D7D6.4050304@stat.columbia.edu>
References: <43C3D7D6.4050304@stat.columbia.edu>
Message-ID: <x2ek3g9hl2.fsf@viggo.kubism.ku.dk>

Andrew Gelman <gelman at stat.columbia.edu> writes:

> Error in "rownames<-"(x, value) : attempt to set rownames on object with 
> no dimensions
....
> Am I doing something stupid here or is it actually a (minor) bug?

The latter. There's a drop=FALSE missing in the coef() method,
specifically in the line

   val <- lapply(ref, function(x) fef[rep(1, nrow(x)), ])

which should read

   val <- lapply(ref, function(x) fef[rep(1, nrow(x)), , drop=FALSE])


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Tue Jan 10 17:29:03 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 11:29:03 -0500
Subject: [R] matching country name tables from different sources
In-Reply-To: <20060110154915.21206.qmail@web25805.mail.ukl.yahoo.com>
References: <20060110154915.21206.qmail@web25805.mail.ukl.yahoo.com>
Message-ID: <971536df0601100829x2c4f6129sdf7b584de2cd2464@mail.gmail.com>

If they were the same you could use merge.   To figure out
the correspondence automatically or semiautomatically, try this:

x <- c("Canada", "US", "Mexico")
y <- c("Kanada", "United States", "Mehico")
result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
result[] <- sapply(result, nchar)
# try both which.max and which.min and if you are lucky
# one of them will give unique values and that is the one to use
# In this case which.max does.
apply(result, 1, which.max)  # 1 2 3

# calculate longest common subsequence between 2 strings
lcs2 <- function(s1,s2) {
     longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
     # Make sure args are strings
     a <- as.character(s1); an <- nchar(s1)+1
     b <- as.character(s2); bn <- nchar(s2)+1


     # If one arg is an empty string, returns the length of the other
     if (nchar(a)==0) return(nchar(b))
     if (nchar(b)==0) return(nchar(a))


     # Initialize matrix for calculations
     m <- matrix("", nrow=an, ncol=bn)

     for (i in 2:an)
          for (j in 2:bn)
		m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
			paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
		else
			longest(m[i-1,j], m[i,j-1])

     # Returns the distance
     m[an,bn]
}



On 1/10/06, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> Hi,
>
>  Before I reinvent the wheel I wanted to kindly ask you for your opinion if there is a simple way to do it.
>
>  I want to merge a larger number of tables from different data sources  in R and the matching criterium are country names. The tables are of  different size and sometimes the country names do differ slightly.
>
>  Has anyone done this or any recommendation on what commands I should look at to automize this task as much as possible?
>
>  Thanks a lot for your effort in advance.
>
>  All the best,
>    Werner
>
>
>
> ---------------------------------
> Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Tue Jan 10 17:31:29 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 10 Jan 2006 17:31:29 +0100
Subject: [R] R newbie example code question
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020F8EF0CA@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020F8EF0CA@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <17347.57697.443106.547586@stat.math.ethz.ch>

>>>>> "RobMcG" == McGehee, Robert <Robert.McGehee at geodecapital.com>
>>>>>     on Tue, 10 Jan 2006 10:55:47 -0500 writes:

    RobMcG> Hello,
    RobMcG> Why not just copy and paste the examples from the help pages? No need to
    RobMcG> type anything.

    RobMcG> However, if you'd like to run the entire help section for a function,
    RobMcG> check out the example function, i.e.:

    >> example(plot)

    RobMcG> Robert

Hmm, since this thread seems to perpetuate ..
let me take the opportunity to explain how enormously nicely (:-)
this is achieved in ESS (Emacs Speaks Statistics):

1) ?plot opens a separate emacs buffer with the help page.
2)       and emacs gains a separate "ESS-help" menu.

3) From that menu you can learn that you can very quickly move
   around in the help buffer using "n"(ext) and "p"(revious) or
   skip to a section via  "s <letter>."
   Now the most used keystroke in ESS help buffers I use is 
   "s e" := "[S]kip to the [E]xamples section".

4) Inside the examples section, 'l' (single lowercase "L" for [L]ine)
   sends single lines to the running *R* process, or 'r' sends a
   whole [R]egion (that you typically have marked with the mouse).


Since it seems that there are ESS users who don't know about
this feature, I'm also CCing this to ESS-help.

(and yes, I could go on: You can also send code from the
 examples when *editing* the *.Rd  (R help) files ...)

Martin Maechler, ETH Zurich


   > -----Original Message-----
   > From: Mark Leeds [mailto:Mleeds at kellogggroup.com] 
   > Sent: Monday, January 09, 2006 1:20 PM
   > To: R-Stat Help
   > Subject: [R] R newbie example code question

   > Sometimes I print out a package
   > and read about it and there
   > are sometimes nice examples
   > that I would like to run myself.
 
   > Is there a way to bring them
   > into R from the package or
   > are they only meant to be typed
   > in manually ? If manual is the
   > only way, that's fine. I was
   > just checking whether there was
   > a quicker way. Thanks.
 
   > Mark



From lizzylaws at yahoo.com  Tue Jan 10 17:38:38 2006
From: lizzylaws at yahoo.com (Elizabeth Lawson)
Date: Tue, 10 Jan 2006 08:38:38 -0800 (PST)
Subject: [R] Filters in waveslim
In-Reply-To: <20060108124647.94058.qmail@web60415.mail.yahoo.com>
Message-ID: <20060110163838.9195.qmail@web32105.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/ae542408/attachment.pl

From sfalcon at fhcrc.org  Tue Jan 10 17:41:09 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 10 Jan 2006 08:41:09 -0800
Subject: [R] Working with R in a multi-processor machine.
In-Reply-To: <20060110153332.5511.qmail@web30115.mail.mud.yahoo.com> (Aitor
	Mata Conde's message of "Tue, 10 Jan 2006 16:33:32 +0100 (CET)")
References: <20060110153332.5511.qmail@web30115.mail.mud.yahoo.com>
Message-ID: <m2lkxo2fyy.fsf@ziti.local>

On 10 Jan 2006, aitor_doctorado at yahoo.es wrote:

> Hi everyone!!
>
> This is my first message to the list, so I hope not to disturb
> anyone if the subject of my message has been already treated.
>
> The question is that I have a tool, a GUI made with Java, connected
> to R using Rserve, and I'd like to get R and Rserve in a
> multi-processor machine.  Now, when I'm going to start the migration
> I wonder whether R is prepared, 'itself' to optimize the use of
> multiple processors or if I should change the code so that it could
> be a real multi-processor tool.
>
> In other words... Will the R code adapt itself to the new machine
> (Unix with al least 4 processors)? Or shall I change the code to
> have multiple real threads and transform the algorithms into
> parallel computing strategies?

R is not multi-threaded and will not be able to automagically make use
of a SMP machine.  I'm pretty sure what you want to do is configure
your app to launch 4 R instances and manage of queue of some sort.

--
+ seth



From ggrothendieck at gmail.com  Tue Jan 10 18:02:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 12:02:00 -0500
Subject: [R] R newbie example code question
In-Reply-To: <43C2ABA1.60102@statistik.uni-dortmund.de>
References: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
	<43C2ABA1.60102@statistik.uni-dortmund.de>
Message-ID: <971536df0601100902j536b9bbayb24a63d0f7a96424@mail.gmail.com>

You can also get access to the code chunks in vignettes
as shown here:

   http://tolstoy.newcastle.edu.au/~rking/R/help/05/12/17822.html

On 1/9/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Mark Leeds wrote:
>
> > Sometimes I print out a package
> > and read about it and there
> > are sometimes nice examples
> > that I would like to run myself.
> >
> > Is there a way to bring them
> > into R from the package or
> > are they only meant to be typed
> > in manually ? If manual is the
> > only way, that's fine. I was
> > just checking whether there was
> > a quicker way. Thanks.
>
> See (you won't believe it): ?example
>
> Uwe Ligges
>
>
>
>
> >                            Mark
> >
> >
> > **********************************************************************
> > This email and any files transmitted with it are confidentia...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From aitor_doctorado at yahoo.es  Tue Jan 10 18:08:35 2006
From: aitor_doctorado at yahoo.es (Aitor Mata Conde)
Date: Tue, 10 Jan 2006 18:08:35 +0100 (CET)
Subject: [R]  Working with R in a multi-processor machine.
Message-ID: <20060110170835.77296.qmail@web30101.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/d1c14bd5/attachment.pl

From mtmorgan at fhcrc.org  Tue Jan 10 18:16:20 2006
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 10 Jan 2006 09:16:20 -0800
Subject: [R] Working with R in a multi-processor machine.
In-Reply-To: <20060110153332.5511.qmail@web30115.mail.mud.yahoo.com> (Aitor
	Mata Conde's message of "Tue, 10 Jan 2006 16:33:32 +0100 (CET)")
References: <20060110153332.5511.qmail@web30115.mail.mud.yahoo.com>
Message-ID: <6ph64osxau3.fsf@gopher3.fhcrc.org>

R is not thread safe, so you must not use it in a
re-entrant way.

If you want to exploit multiple processors, you can write code (e.g.,
in C) called from R (e.g., through .Call or .C) that performs
parallel/threaded computations in a thread-safe way (e.g., without
calling back into R).

Another possibility is to replace the BLAS/LAPACK library with a
thread-safe version. This provides a boost to those R algorithms
exploiting these libraries. Haven't done this myself, but there is
some info in this post

https://stat.ethz.ch/pipermail/r-devel/2005-December/035695.html

Hope that helps,

Martin


Aitor Mata Conde <aitor_doctorado at yahoo.es> writes:

> Hi everyone!!
>
> This is my first message to the list, so I hope not to disturb anyone if the
> subject of my message has been already treated.
>
> The question is that I have a tool, a GUI made with Java, connected to R 
> using Rserve, and I'd like to get R and Rserve in a multi-processor machine.
> Now, when I'm going to start the migration I wonder whether R is prepared,
> 'itself' to optimize the use of multiple processors or if I should change the
> code so that it could be a real multi-processor tool.
>
> In other words... Will the R code adapt itself to the new machine (Unix with
> al least 4 processors)? Or shall I change the code to have multiple real
> threads and transform the algorithms into parallel computing strategies?
>
> Thanks in advance,
> Aitor.
>
>
> 		
> ---------------------------------
>
> LLama Gratis a cualquier PC del Mundo.
> Llamadas a fijos y moviles desde 1 centimo por minuto.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Tue Jan 10 18:46:30 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 10 Jan 2006 12:46:30 -0500
Subject: [R] Working with R in a multi-processor machine.
In-Reply-To: <m2lkxo2fyy.fsf@ziti.local>
Message-ID: <BFE95D26.364C%sdavis2@mail.nih.gov>




On 1/10/06 11:41 AM, "Seth Falcon" <sfalcon at fhcrc.org> wrote:

> On 10 Jan 2006, aitor_doctorado at yahoo.es wrote:
> 
>> Hi everyone!!
>> 
>> This is my first message to the list, so I hope not to disturb
>> anyone if the subject of my message has been already treated.
>> 
>> The question is that I have a tool, a GUI made with Java, connected
>> to R using Rserve, and I'd like to get R and Rserve in a
>> multi-processor machine.  Now, when I'm going to start the migration
>> I wonder whether R is prepared, 'itself' to optimize the use of
>> multiple processors or if I should change the code so that it could
>> be a real multi-processor tool.
>> 
>> In other words... Will the R code adapt itself to the new machine
>> (Unix with al least 4 processors)? Or shall I change the code to
>> have multiple real threads and transform the algorithms into
>> parallel computing strategies?
> 
> R is not multi-threaded and will not be able to automagically make use
> of a SMP machine.  I'm pretty sure what you want to do is configure
> your app to launch 4 R instances and manage of queue of some sort.

An alternative is to do all of the parallelization within R using nice tools
like the snow package combined with Rmpi.  If your task is computationally
intensive on the R side, but not on the client, then parallelizing R code
may be the better way to go.  All depends on your application, I think.

Sean



From amiller at a2software.com  Tue Jan 10 18:56:18 2006
From: amiller at a2software.com (allan miller)
Date: Tue, 10 Jan 2006 09:56:18 -0800
Subject: [R] Return a Vector of Positions
Message-ID: <43C3F542.8090006@a2software.com>

Hello,

I wrote a version of which.na (similar to S's) in R that returns a 
vector of positions at which NA appears in the target vector v:

which.na<-function(v) {
  retv <- c()
  for (i in 1:length(v)) {
     if (is.na(v[i])) {
       retv<-append(i, retv, after = length(retv))
     } 
     
  }
  return(retv)
}

nv <- c(2,4,NA,NA)
nas<-which.r(nv)
print(nas)

[run]


 > source("which.r")
[1] 4 3
 >

Two questions about this:

(1) It seems that append actually "prepends"

(2) Is there a more simpler way to build the vector of positions (versus 
using append in a loop, as above), using functions such as match() and 
apply()?

Thank You.



From h.wickham at gmail.com  Tue Jan 10 18:59:35 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 10 Jan 2006 17:59:35 +0000
Subject: [R] Return a Vector of Positions
In-Reply-To: <43C3F542.8090006@a2software.com>
References: <43C3F542.8090006@a2software.com>
Message-ID: <f8e6ff050601100959s434a216euc3d9cb77884ddd8c@mail.gmail.com>

> (2) Is there a more simpler way to build the vector of positions (versus
> using append in a loop, as above), using functions such as match() and
> apply()?

How about
which.na <- function(x) which(is.na(x))
?

Hadley



From rdporto1 at terra.com.br  Tue Jan 10 19:09:03 2006
From: rdporto1 at terra.com.br (Rogerio Porto)
Date: Tue, 10 Jan 2006 16:09:03 -0200
Subject: [R] Filters in waveslim
References: <mailman.9.1136804412.1188.r-help@stat.math.ethz.ch>
Message-ID: <004301c61610$f1653f30$9edda7c8@THINQ>

Amir,

if you try the following code:

require(wavelslim)
data(doppler)
dwt(doppler, wf="la6")

you get:

Error in switch(name, haar = select.haar(), d4 = select.d4(), mb4 = 
select.mb4(),  :   Invalid selection for wave.filter

So, it seems that instead of "la8" (default) and "haar" you can choose also 
"d4" and "mb4" only.

HTH,

Rogerio.

>Message: 5
>Date: Sun, 8 Jan 2006 04:46:47 -0800 (PST)
>From: Amir Safari <amsa36060 at yahoo.com>
>Subject: [R] Filters in waveslim
>To: R-help at stat.math.ethz.ch
>Message-ID: ><20060108124647.94058.qmail at web60415.mail.yahoo.com>
>Content-Type: text/plain
>
>
>
>  Dear R Users,
> For running  wavelet functions using dwt( ), modwt( ), and mra( ), a 
> wavelet >filter  algorithm is applied. For all these functions, default is 
> "la8" and  other >possibility is "haar". In related documents, another 
> possibilities like as symlet >and coiflet ... are not cited.
> Besides "la8" and "haar", which wavelet filters can be used?
>Thank you for any help,
>  Amir Safari



From Andrew.McCallum at sf.frb.org  Tue Jan 10 19:30:21 2006
From: Andrew.McCallum at sf.frb.org (Andrew.McCallum@sf.frb.org)
Date: Tue, 10 Jan 2006 10:30:21 -0800
Subject: [R] R for Windows Proxy Solution
Message-ID: <20060110183025.20330D4221@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/0af48d2e/attachment.pl

From david.reitter at gmail.com  Tue Jan 10 19:37:06 2006
From: david.reitter at gmail.com (David Reitter)
Date: Tue, 10 Jan 2006 18:37:06 +0000
Subject: [R] glmmPQL / "system is computationally singular"
Message-ID: <A02D1532-5494-4777-9A41-32C57A6DE701@gmail.com>

Hi,

I'm having trouble with glmmPQL from the MASS package.
I'm trying to fit a model with a binary response variable, two fixed  
and two random variables (nested), with a sample of about 200,000  
data points.

Unfortunately, I'm getting an error message that is difficult to  
understand without knowing the internals of the glmmPQL function.

> model <-  glmmPQL(primed ~ log(dist) * role , random = ~ dist |  
> target.utt / prime.utt , family=binomial(link = "logit"),  
> data=data.utts, niter=5, verbose = TRUE)
> Loading required package: nlme
> iteration 1
> iteration 2
> iteration 3
> Error in solve.default(pdMatrix(a, fact = TRUE)) :
>         system is computationally singular: reciprocal condition  
> number = 8.65949e-32
> In addition: Warning messages:
> 1: Singular precision matrix in level -1, block 4
> 2: Singular precision matrix in level -1, block 4
> 3: Singular precision matrix in level -1, block 4
> 4: Singular precision matrix in level -1, block 4
> 5: Singular precision matrix in level -1, block 4

Any suggestions? Will a larger dataset (possible) solve the problem?

Thanks
David

--
David Reitter - ICCS/HCRC, Informatics, University of Edinburgh
Blog: http://www.davids-world.com   Homepage: http://www.david- 
reitter.com



From gsxej2 at cam.ac.uk  Tue Jan 10 19:58:49 2006
From: gsxej2 at cam.ac.uk (Gregory Jefferis)
Date: Tue, 10 Jan 2006 18:58:49 +0000
Subject: [R] Correct way to test for exact dimensions of matrix or array
In-Reply-To: <17347.53018.474709.164656@stat.math.ethz.ch>
Message-ID: <BFE9B469.798A%gsxej2@cam.ac.uk>

Thanks for suggestions.  This is a simple question in principle, but there
seem to be some wrinkles - I am always having to think quite carefully about
how to test for equality in R.  I should also have said that I would like
the check to be efficient as well safe and succinct.

One suggestion was:
    
    isTRUE(all.equal(dim(obj), c(3, 5)))

But that is not so efficient because all.equal does lots of work esp if it
the objects are not equal.

Another suggestion was:

    all( dim( obj) == c(3,5) )
    
But that is not safe eg because dim(vector(10)) is NULL and
all(NULL==c(3,5)) is actually TRUE (to my initial surprise) so vectors would
pass through the net.

So, so far the only way that is efficient, safe and succinct is:

    identical( dim( obj) , as.integer(c(3,5)))

Martin Maechler pointed out that at the beginning of a function you might
want to break down the test into something less succinct, that printed more
specific error messages - a good suggestion for a top level function that is
supposed to be user friendly.

Any other suggestions?  Many thanks,

Greg Jefferis.

On 10/1/06 15:13, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:

>>>>>> "Gregory" == Gregory Jefferis <gsxej2 at cam.ac.uk>
>>>>>>     on Tue, 10 Jan 2006 14:47:43 +0000 writes:
> 
>     Gregory> Dear R Users,
> 
>      Gregory> I want to test the dimensions of an incoming
>      Gregory> vector, matrix or array safely
> 
> 
>     Gregory> and succinctly.  Specifically I want to check if
>     Gregory> the unknown object has exactly 2 dimensions with a
>     Gregory> specified number of rows and columns.
> 
>     Gregory> I thought that the following would work:
> 
>>> obj=matrix(1,nrow=3,ncol=5)
>>> identical( dim( obj) , c(3,5) )
>     Gregory> [1] FALSE
> 
>     Gregory> But it doesn't because c(3,5) is numeric and the dims are
> integer.  I
>     Gregory> therefore ended up doing something like:
> 
>>> identical( dim( obj) , as.integer(c(3,5)))
> 
>     Gregory> OR
> 
>>> isTRUE(all( dim( obj) == c(3,5) ))
> 
> the last one is almost perfect if you leave a way the superfluous
> isTRUE(..).
> 
> But, you say that it's part of your function checking it's
> arguments.
> In that case, I'd recommend
> 
>      if(length(d <- dim(obj)) != 2)
>   stop("'d' must be matrix-like")
>      if(!all(d == c(3,5)))
>   stop("the matrix must be  3 x 5")
> 
> which also provides for nice error messages in case of error.
> A more concise form with less nice error messages is
> 
>   stopifnot(length(d <- dim(obj)) == 2,
>             d == c(3,50))
> 
>   ## you can leave away  all(.)  for things in stopifnot(.)
> 
> 
> 
> 
>     Gregory> Neither of which feel quite right.  Is there a 'correct' way to
> do this?
> 
>     Gregory> Many thanks,
> 
> You're welcome,
> Martin Maechler, ETH Zurich
> 
>     Gregory> Greg Jefferis.
> 
>     Gregory> PS Thinking about it, the second form is (doubly) wrong because:
> 
>>> obj=array(1,dim=c(3,5,3,5))
>>> isTRUE(all( dim( obj) == c(3,5) ))
>     Gregory> [1] TRUE
> 
>     Gregory> OR 
>>> obj=numeric(10)
>>> isTRUE(all( dim( obj) == c(3,5) ))
>     Gregory> [1] TRUE
> 
>     Gregory> (neither of which are equalities that I am happy with!)
> 

-- 
Gregory Jefferis, PhD                               and:
Research Fellow    
Department of Zoology                               St John's College
University of Cambridge                             Cambridge
Downing Street                                      CB2 1TP
Cambridge, CB2 3EJ 
United Kingdom

Tel: +44 (0)1223 336683                             +44 (0)1223 339899
Fax: +44 (0)1223 336676                             +44 (0)1223 337720

gsxej2 at cam.ac.uk



From pensterfuzzer at yahoo.de  Tue Jan 10 20:00:08 2006
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Tue, 10 Jan 2006 20:00:08 +0100 (CET)
Subject: [R] matching country name tables from different sources
In-Reply-To: <971536df0601100829x2c4f6129sdf7b584de2cd2464@mail.gmail.com>
Message-ID: <20060110190009.55295.qmail@web25811.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/7a9a3541/attachment.pl

From kubovy at virginia.edu  Tue Jan 10 20:17:28 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 10 Jan 2006 14:17:28 -0500
Subject: [R] Hmisc xYplot: two ablines?
In-Reply-To: <9AB9FB1124563249B0B39A1FEF1E94B37E1CD2@cdsx06.cder.fda.gov>
References: <9AB9FB1124563249B0B39A1FEF1E94B37E1CD2@cdsx06.cder.fda.gov>
Message-ID: <7E96B688-9CC3-4A95-8EB1-61299CF6AE9B@virginia.edu>

To r-help:

Thanks to Mat Soukup for the help:

To draw two ablines:

xYplot(lo ~ vaR, groups = v, data = abc.fp, aspect = "xy", col = c 
("red","blue"),
	xlab=expression(frac(abs( bold(v) ),abs( bold(a) ))),
	ylab = grid::textGrob(expression(paste(log, frac( italic(p) ( italic 
(v) ), italic(p) ( italic(a) )))) ),
	panel=function(x,y,...){
		panel.xYplot(x, y, ...)
		panel.abline(a = 7.985090, b= -7.985090, col = "red")
		panel.abline(a = 7.926507, b = -7.926507, col = "blue", lty = 2)
	}
)

This is (to me) a surprising syntactic leap from one abline in  
xYplot, which can be requested without writing a panel function:

xYplot(lo ~ vaR, groups = v, data = abc.fp, aspect = "xy", col = c 
("red","blue"),
	xlab=expression(frac(abs( bold(v) ),abs( bold(a) ))),
	ylab = grid::textGrob(expression(paste(log, frac( italic(p) ( italic 
(v) ), italic(p) ( italic(a) )))) ),
	panel.abline(a = 7.985090, b= -7.985090, col = "red")
)

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From br44114 at gmail.com  Tue Jan 10 20:24:49 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 10 Jan 2006 14:24:49 -0500
Subject: [R] matching country name tables from different sources
Message-ID: <8d5a36350601101124x78978410iad06fad0c9d37640@mail.gmail.com>

See
http://en.wikipedia.org/wiki/Levenshtein_distance
http://thread.gmane.org/gmane.comp.lang.r.general/31499


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Werner
> Wernersen
> Sent: Tuesday, January 10, 2006 2:00 PM
> To: Gabor Grothendieck
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] matching country name tables from different sources
>
> Thanks for the nice code, Gabor!
>
>   Unfortunately, it seems not to work for my purpose,
> confuses lots of  countries when I compare two lists of over
> 150 countries each.
>   Do you have any other suggestions?
>
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:  If
> they were the same you could use merge.   To figure out
> the correspondence automatically or semiautomatically, try this:
>
> x <- c("Canada", "US", "Mexico")
> y <- c("Kanada", "United States", "Mehico")
> result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
> result[] <- sapply(result, nchar)
> # try both which.max and which.min and if you are lucky
> # one of them will give unique values and that is the one to use
> # In this case which.max does.
> apply(result, 1, which.max)  # 1 2 3
>
> # calculate longest common subsequence between 2 strings
> lcs2 <- function(s1,s2) {
>      longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
>      # Make sure args are strings
>      a <- as.character(s1); an <- nchar(s1)+1
>      b <- as.character(s2); bn <- nchar(s2)+1
>
>
>      # If one arg is an empty string, returns the length of the other
>      if (nchar(a)==0) return(nchar(b))
>      if (nchar(b)==0) return(nchar(a))
>
>
>      # Initialize matrix for calculations
>      m <- matrix("", nrow=an, ncol=bn)
>
>      for (i in 2:an)
>           for (j in 2:bn)
>   m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
>    paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
>   else
>    longest(m[i-1,j], m[i,j-1])
>
>      # Returns the distance
>      m[an,bn]
> }
>
>
>
> On 1/10/06, Werner Wernersen
>  wrote:
> > Hi,
> >
> >  Before I reinvent the wheel I wanted to kindly ask you for
> your opinion if there is a simple way to do it.
> >
> >  I want to merge a larger number of tables from different
> data sources  in R and the matching criterium are country
> names. The tables are of  different size and sometimes the
> country names do differ slightly.
> >
> >  Has anyone done this or any recommendation on what
> commands I should look at to automize this task as much as possible?
> >
> >  Thanks a lot for your effort in advance.
> >
> >  All the best,
> >    Werner
> >
> >
> >
> > ---------------------------------
> > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von
> PC zu PC!
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>
>
> 		
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Robert.McGehee at geodecapital.com  Tue Jan 10 20:28:27 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 10 Jan 2006 14:28:27 -0500
Subject: [R] matching country name tables from different sources
Message-ID: <67DCA285A2D7754280D3B8E88EB548020F8EF0CF@MSGBOSCLB2WIN.DMN1.FMR.COM>

I would throw a tolower() around s1 and s2 so that 'canada' matches with
'CANADA', and perhaps consider using a Levenshtein distance rather than
the longest common subsequence.

An algorithm for Levenshtein distance can be found here (courtesy of
Stephen Upton)
https://stat.ethz.ch/pipermail/r-help/2005-January/062254.html

Robert

-----Original Message-----
From: Werner Wernersen [mailto:pensterfuzzer at yahoo.de] 
Sent: Tuesday, January 10, 2006 2:00 PM
To: Gabor Grothendieck
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] matching country name tables from different sources

Thanks for the nice code, Gabor! 
  
  Unfortunately, it seems not to work for my purpose, confuses lots of
countries when I compare two lists of over 150 countries each. 
  Do you have any other suggestions?
  
  

Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:  If they were the
same you could use merge.   To figure out
the correspondence automatically or semiautomatically, try this:

x <- c("Canada", "US", "Mexico")
y <- c("Kanada", "United States", "Mehico")
result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
result[] <- sapply(result, nchar)
# try both which.max and which.min and if you are lucky
# one of them will give unique values and that is the one to use
# In this case which.max does.
apply(result, 1, which.max)  # 1 2 3

# calculate longest common subsequence between 2 strings
lcs2 <- function(s1,s2) {
     longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
     # Make sure args are strings
     a <- as.character(s1); an <- nchar(s1)+1
     b <- as.character(s2); bn <- nchar(s2)+1


     # If one arg is an empty string, returns the length of the other
     if (nchar(a)==0) return(nchar(b))
     if (nchar(b)==0) return(nchar(a))


     # Initialize matrix for calculations
     m <- matrix("", nrow=an, ncol=bn)

     for (i in 2:an)
          for (j in 2:bn)
  m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
   paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
  else
   longest(m[i-1,j], m[i,j-1])

     # Returns the distance
     m[an,bn]
}



On 1/10/06, Werner Wernersen 
 wrote:
> Hi,
>
>  Before I reinvent the wheel I wanted to kindly ask you for your
opinion if there is a simple way to do it.
>
>  I want to merge a larger number of tables from different data sources
in R and the matching criterium are country names. The tables are of
different size and sometimes the country names do differ slightly.
>
>  Has anyone done this or any recommendation on what commands I should
look at to automize this task as much as possible?
>
>  Thanks a lot for your effort in advance.
>
>  All the best,
>    Werner
>
>
>
> ---------------------------------
> Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>




		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gelman at stat.columbia.edu  Tue Jan 10 20:29:00 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Tue, 10 Jan 2006 14:29:00 -0500
Subject: [R] lmer():  nested and non-nested factors in logistic regression
Message-ID: <43C40AFC.4040003@stat.columbia.edu>

Thanks to some help by Doug Bates (and the updated version of the Matrix 
package), I've refined my question about fitting nested and non-nested 
factors in lmer().  I can get it to work in linear regression but it 
crashes in logistic regression.  Here's my example:

# set up the predictors

n.age <- 4
n.edu <- 4
n.rep <- 100
n.state <- 50
n <- n.age*n.edu*n.rep
age.id <- rep (1:n.age, each=n.edu*n.rep)
edu.id <- rep (1:n.edu, n.age, each=n.rep)
age.edu.id <- n.edu*(age.id - 1) + edu.id
state.id <- sample (1:n.state, n, replace=TRUE)

# simulate the varying parameters

a.age <- rnorm (n.age, 1, 2)
a.edu <- rnorm (n.edu, 3, 4)
a.age.edu <- rnorm (n.age*n.edu, 0, 5)
a.state <- rnorm (n.state, 0, 6)

# simulate the data and print to check that i did it right

y.hat <- a.age[age.id] + a.edu[edu.id] + a.age.edu[age.edu.id] + 
a.state[state.id]
y <- rnorm (n, y.hat, 1)
print (cbind (age.id, edu.id, age.edu.id, state.id, y.hat, y))

# this model (and simpler versions) work fine:

fit.1 <- lmer (y ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | age.edu.id) + 
(1 | state.id))

# now go to logistic regression

ypos <- ifelse (y > mean(y), 1, 0)

# these work fine:

fit.2 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | 
age.edu.id), family=binomial(link="logit"))
fit.3 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | state.id), 
family=binomial(link="logit"))

# this one causes R to crash!!!!!!!

fit.4 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | age.edu.id) 
+ (1 | state.id), family=binomial(link="logit"))

--

All help appreciated.  This is for our book on regression and multilevel 
models, and it would be great if people could get started fitting these 
models in R before having to do the more elaborate modeling in Bugs.

Andrew

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From ggrothendieck at gmail.com  Tue Jan 10 20:43:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 14:43:43 -0500
Subject: [R] matching country name tables from different sources
In-Reply-To: <20060110190009.55295.qmail@web25811.mail.ukl.yahoo.com>
References: <971536df0601100829x2c4f6129sdf7b584de2cd2464@mail.gmail.com>
	<20060110190009.55295.qmail@web25811.mail.ukl.yahoo.com>
Message-ID: <971536df0601101143u3d6702abnef736638bbda6312@mail.gmail.com>

You can improve it somewhat by first accepting all the largest
matches and removing the rows and columns for those and
repeatedly doing that with what is left.

On 1/10/06, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> Thanks for the nice code, Gabor!
>
> Unfortunately, it seems not to work for my purpose, confuses lots of
> countries when I compare two lists of over 150 countries each.
> Do you have any other suggestions?
>
>
>
> Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:
> If they were the same you could use merge. To figure out
> the correspondence automatically or semiautomatically, try this:
>
> x <- c("Canada", "US", "Mexico")
> y <- c("Kanada", "United States", "Mehico")
> result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
> result[] <- sapply(result, nchar)
> # try both which.max and which.min and if you are lucky
> # one of them will give unique values and that is the one to use
> # In this case which.max does.
> apply(result, 1, which.max) # 1 2 3
>
> # calculate longest common subsequence between 2 strings
> lcs2 <- function(s1,s2) {
> longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
> # Make sure args are strings
> a <- as.character(s1); an <- nchar(s1)+1
> b <- as.character(s2); bn <- nchar(s2)+1
>
>
> # If one arg is an empty string, returns the length of the other
> if (nchar(a)==0) return(nchar(b))
> if (nchar(b)==0) return(nchar(a))
>
>
> # Initialize matrix for calculations
> m <- matrix("", nrow=an, ncol=bn)
>
> for (i in 2:an)
> for (j in 2:bn)
> m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
> paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
> else
> longest(m[i-1,j], m[i,j-1])
>
> # Returns the distance
> m[an,bn]
> }
>
>
>
> On 1/10/06, Werner Wernersen wrote:
> > Hi,
> >
> > Before I reinvent the wheel I wanted to kindly ask you for your opinion if
> there is a simple way to do it.
> >
> > I want to merge a larger number of tables from different data sources in R
> and the matching criterium are country names. The tables are of different
> size and sometimes the country names do differ slightly.
> >
> > Has anyone done this or any recommendation on what commands I should look
> at to automize this task as much as possible?
> >
> > Thanks a lot for your effort in advance.
> >
> > All the best,
> > Werner
> >
> >
> >
> > ---------------------------------
> > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>
>
> ________________________________
> Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
> Jetzt Yahoo! Messenger installieren!
>
>



From ggrothendieck at gmail.com  Tue Jan 10 20:47:57 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 14:47:57 -0500
Subject: [R] Correct way to test for exact dimensions of matrix or array
In-Reply-To: <BFE9B469.798A%gsxej2@cam.ac.uk>
References: <17347.53018.474709.164656@stat.math.ethz.ch>
	<BFE9B469.798A%gsxej2@cam.ac.uk>
Message-ID: <971536df0601101147j6010514dieae9d722406857a7@mail.gmail.com>

If its just succint you are after then this is slightly
shorter:

   identical(dim(x)+0, c(3,5))


On 1/10/06, Gregory Jefferis <gsxej2 at cam.ac.uk> wrote:
> Thanks for suggestions.  This is a simple question in principle, but there
> seem to be some wrinkles - I am always having to think quite carefully about
> how to test for equality in R.  I should also have said that I would like
> the check to be efficient as well safe and succinct.
>
> One suggestion was:
>
>    isTRUE(all.equal(dim(obj), c(3, 5)))
>
> But that is not so efficient because all.equal does lots of work esp if it
> the objects are not equal.
>
> Another suggestion was:
>
>    all( dim( obj) == c(3,5) )
>
> But that is not safe eg because dim(vector(10)) is NULL and
> all(NULL==c(3,5)) is actually TRUE (to my initial surprise) so vectors would
> pass through the net.
>
> So, so far the only way that is efficient, safe and succinct is:
>
>    identical( dim( obj) , as.integer(c(3,5)))
>
> Martin Maechler pointed out that at the beginning of a function you might
> want to break down the test into something less succinct, that printed more
> specific error messages - a good suggestion for a top level function that is
> supposed to be user friendly.
>
> Any other suggestions?  Many thanks,
>
> Greg Jefferis.
>
> On 10/1/06 15:13, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:
>
> >>>>>> "Gregory" == Gregory Jefferis <gsxej2 at cam.ac.uk>
> >>>>>>     on Tue, 10 Jan 2006 14:47:43 +0000 writes:
> >
> >     Gregory> Dear R Users,
> >
> >      Gregory> I want to test the dimensions of an incoming
> >      Gregory> vector, matrix or array safely
> >
> >
> >     Gregory> and succinctly.  Specifically I want to check if
> >     Gregory> the unknown object has exactly 2 dimensions with a
> >     Gregory> specified number of rows and columns.
> >
> >     Gregory> I thought that the following would work:
> >
> >>> obj=matrix(1,nrow=3,ncol=5)
> >>> identical( dim( obj) , c(3,5) )
> >     Gregory> [1] FALSE
> >
> >     Gregory> But it doesn't because c(3,5) is numeric and the dims are
> > integer.  I
> >     Gregory> therefore ended up doing something like:
> >
> >>> identical( dim( obj) , as.integer(c(3,5)))
> >
> >     Gregory> OR
> >
> >>> isTRUE(all( dim( obj) == c(3,5) ))
> >
> > the last one is almost perfect if you leave a way the superfluous
> > isTRUE(..).
> >
> > But, you say that it's part of your function checking it's
> > arguments.
> > In that case, I'd recommend
> >
> >      if(length(d <- dim(obj)) != 2)
> >   stop("'d' must be matrix-like")
> >      if(!all(d == c(3,5)))
> >   stop("the matrix must be  3 x 5")
> >
> > which also provides for nice error messages in case of error.
> > A more concise form with less nice error messages is
> >
> >   stopifnot(length(d <- dim(obj)) == 2,
> >             d == c(3,50))
> >
> >   ## you can leave away  all(.)  for things in stopifnot(.)
> >
> >
> >
> >
> >     Gregory> Neither of which feel quite right.  Is there a 'correct' way to
> > do this?
> >
> >     Gregory> Many thanks,
> >
> > You're welcome,
> > Martin Maechler, ETH Zurich
> >
> >     Gregory> Greg Jefferis.
> >
> >     Gregory> PS Thinking about it, the second form is (doubly) wrong because:
> >
> >>> obj=array(1,dim=c(3,5,3,5))
> >>> isTRUE(all( dim( obj) == c(3,5) ))
> >     Gregory> [1] TRUE
> >
> >     Gregory> OR
> >>> obj=numeric(10)
> >>> isTRUE(all( dim( obj) == c(3,5) ))
> >     Gregory> [1] TRUE
> >
> >     Gregory> (neither of which are equalities that I am happy with!)
> >
>
> --
> Gregory Jefferis, PhD                               and:
> Research Fellow
> Department of Zoology                               St John's College
> University of Cambridge                             Cambridge
> Downing Street                                      CB2 1TP
> Cambridge, CB2 3EJ
> United Kingdom
>
> Tel: +44 (0)1223 336683                             +44 (0)1223 339899
> Fax: +44 (0)1223 336676                             +44 (0)1223 337720
>
> gsxej2 at cam.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From epurdom at stanford.edu  Tue Jan 10 20:50:56 2006
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Tue, 10 Jan 2006 11:50:56 -0800
Subject: [R] R CMD not recognized at command-line
Message-ID: <6.1.2.0.2.20060110112927.023feb80@epurdom.pobox.stanford.edu>

Hi,
I am trying to run a batch command on Windows XP and R CMD is not 
recognized. I get the error,

"'R' is not recognized as an internal or external command, operable program 
or batch file."

I have "C:\Program Files\R\rw2010\bin" in my $PATH variable and Rcmd.exe 
has been installed in that folder. I have restarted the computer to make 
sure any changes in the $PATH variable registered. I have tried directly 
calling Rcmd.exe or R.exe. None of this had any effect and I can't think 
what I'm forgetting. In Cygwin,  I can get around this by explicitly giving 
the path at the prompt line, for example,

 > ~/Program\ Files/R/rw2010/bin/R CMD --help

This work-around does not seem work from the usual DOS Command Prompt, but 
I rarely use DOS commands so I may be missing something in syntax. 
Similarly, if I want to run several successive input files with a .bat 
file, this work-around won't work as a line in the .bat file.

I have two questions:
1) how can I make the computer recognize R so I can just type R CMD at the 
prompt
2) what is a work-around like I did in cygwin that will work in a .bat file 
or the standard command-line prompt so that you don't have to change the 
path variable? (often students do not have permission to add the R folder 
to the path variable of a networked computer, so I'd like to know an 
alternative if someone asks me)

Thanks,
Elizabeth Purdom



From tplate at acm.org  Tue Jan 10 20:55:52 2006
From: tplate at acm.org (Tony Plate)
Date: Tue, 10 Jan 2006 12:55:52 -0700
Subject: [R] Correct way to test for exact dimensions of matrix or array
In-Reply-To: <971536df0601101147j6010514dieae9d722406857a7@mail.gmail.com>
References: <17347.53018.474709.164656@stat.math.ethz.ch>	<BFE9B469.798A%gsxej2@cam.ac.uk>
	<971536df0601101147j6010514dieae9d722406857a7@mail.gmail.com>
Message-ID: <43C41148.6050402@acm.org>

There's a gotcha in using identical() to compare dimensions -- it also 
compares names, e.g.:

 > x <- array(1:14, dim=c(rows=3,cols=5))
 > dim(x)
rows cols
    3    5
 > identical(dim(x)+0, c(3,5))
[1] FALSE
 > identical(as.numeric(dim(x)+0), c(3,5))
[1] TRUE
 >

Gabor Grothendieck wrote:
> If its just succint you are after then this is slightly
> shorter:
> 
>    identical(dim(x)+0, c(3,5))
> 
> 
> On 1/10/06, Gregory Jefferis <gsxej2 at cam.ac.uk> wrote:
> 
>>Thanks for suggestions.  This is a simple question in principle, but there
>>seem to be some wrinkles - I am always having to think quite carefully about
>>how to test for equality in R.  I should also have said that I would like
>>the check to be efficient as well safe and succinct.
>>
>>One suggestion was:
>>
>>   isTRUE(all.equal(dim(obj), c(3, 5)))
>>
>>But that is not so efficient because all.equal does lots of work esp if it
>>the objects are not equal.
>>
>>Another suggestion was:
>>
>>   all( dim( obj) == c(3,5) )
>>
>>But that is not safe eg because dim(vector(10)) is NULL and
>>all(NULL==c(3,5)) is actually TRUE (to my initial surprise) so vectors would
>>pass through the net.
>>
>>So, so far the only way that is efficient, safe and succinct is:
>>
>>   identical( dim( obj) , as.integer(c(3,5)))
>>
>>Martin Maechler pointed out that at the beginning of a function you might
>>want to break down the test into something less succinct, that printed more
>>specific error messages - a good suggestion for a top level function that is
>>supposed to be user friendly.
>>
>>Any other suggestions?  Many thanks,
>>
>>Greg Jefferis.
>>
>>On 10/1/06 15:13, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:
>>
>>
>>>>>>>>"Gregory" == Gregory Jefferis <gsxej2 at cam.ac.uk>
>>>>>>>>    on Tue, 10 Jan 2006 14:47:43 +0000 writes:
>>>
>>>    Gregory> Dear R Users,
>>>
>>>     Gregory> I want to test the dimensions of an incoming
>>>     Gregory> vector, matrix or array safely
>>>
>>>
>>>    Gregory> and succinctly.  Specifically I want to check if
>>>    Gregory> the unknown object has exactly 2 dimensions with a
>>>    Gregory> specified number of rows and columns.
>>>
>>>    Gregory> I thought that the following would work:
>>>
>>>
>>>>>obj=matrix(1,nrow=3,ncol=5)
>>>>>identical( dim( obj) , c(3,5) )
>>>
>>>    Gregory> [1] FALSE
>>>
>>>    Gregory> But it doesn't because c(3,5) is numeric and the dims are
>>>integer.  I
>>>    Gregory> therefore ended up doing something like:
>>>
>>>
>>>>>identical( dim( obj) , as.integer(c(3,5)))
>>>
>>>    Gregory> OR
>>>
>>>
>>>>>isTRUE(all( dim( obj) == c(3,5) ))
>>>
>>>the last one is almost perfect if you leave a way the superfluous
>>>isTRUE(..).
>>>
>>>But, you say that it's part of your function checking it's
>>>arguments.
>>>In that case, I'd recommend
>>>
>>>     if(length(d <- dim(obj)) != 2)
>>>  stop("'d' must be matrix-like")
>>>     if(!all(d == c(3,5)))
>>>  stop("the matrix must be  3 x 5")
>>>
>>>which also provides for nice error messages in case of error.
>>>A more concise form with less nice error messages is
>>>
>>>  stopifnot(length(d <- dim(obj)) == 2,
>>>            d == c(3,50))
>>>
>>>  ## you can leave away  all(.)  for things in stopifnot(.)
>>>
>>>
>>>
>>>
>>>    Gregory> Neither of which feel quite right.  Is there a 'correct' way to
>>>do this?
>>>
>>>    Gregory> Many thanks,
>>>
>>>You're welcome,
>>>Martin Maechler, ETH Zurich
>>>
>>>    Gregory> Greg Jefferis.
>>>
>>>    Gregory> PS Thinking about it, the second form is (doubly) wrong because:
>>>
>>>
>>>>>obj=array(1,dim=c(3,5,3,5))
>>>>>isTRUE(all( dim( obj) == c(3,5) ))
>>>
>>>    Gregory> [1] TRUE
>>>
>>>    Gregory> OR
>>>
>>>>>obj=numeric(10)
>>>>>isTRUE(all( dim( obj) == c(3,5) ))
>>>
>>>    Gregory> [1] TRUE
>>>
>>>    Gregory> (neither of which are equalities that I am happy with!)
>>>
>>
>>--
>>Gregory Jefferis, PhD                               and:
>>Research Fellow
>>Department of Zoology                               St John's College
>>University of Cambridge                             Cambridge
>>Downing Street                                      CB2 1TP
>>Cambridge, CB2 3EJ
>>United Kingdom
>>
>>Tel: +44 (0)1223 336683                             +44 (0)1223 339899
>>Fax: +44 (0)1223 336676                             +44 (0)1223 337720
>>
>>gsxej2 at cam.ac.uk
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Tue Jan 10 21:16:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 15:16:29 -0500
Subject: [R] matching country name tables from different sources
In-Reply-To: <971536df0601101143u3d6702abnef736638bbda6312@mail.gmail.com>
References: <971536df0601100829x2c4f6129sdf7b584de2cd2464@mail.gmail.com>
	<20060110190009.55295.qmail@web25811.mail.ukl.yahoo.com>
	<971536df0601101143u3d6702abnef736638bbda6312@mail.gmail.com>
Message-ID: <971536df0601101216q2510414ped625d5205c3d2ea@mail.gmail.com>

One other thing to try could be soundex.  ITs normally used for
last names but it might work here too.  Google to find the
soundex encoding rules.  Reviewing the country names might
suggest minor modifications to the soundex algorithm to
improve it for your case.

On 1/10/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> You can improve it somewhat by first accepting all the largest
> matches and removing the rows and columns for those and
> repeatedly doing that with what is left.
>
> On 1/10/06, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> > Thanks for the nice code, Gabor!
> >
> > Unfortunately, it seems not to work for my purpose, confuses lots of
> > countries when I compare two lists of over 150 countries each.
> > Do you have any other suggestions?
> >
> >
> >
> > Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:
> > If they were the same you could use merge. To figure out
> > the correspondence automatically or semiautomatically, try this:
> >
> > x <- c("Canada", "US", "Mexico")
> > y <- c("Kanada", "United States", "Mehico")
> > result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
> > result[] <- sapply(result, nchar)
> > # try both which.max and which.min and if you are lucky
> > # one of them will give unique values and that is the one to use
> > # In this case which.max does.
> > apply(result, 1, which.max) # 1 2 3
> >
> > # calculate longest common subsequence between 2 strings
> > lcs2 <- function(s1,s2) {
> > longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
> > # Make sure args are strings
> > a <- as.character(s1); an <- nchar(s1)+1
> > b <- as.character(s2); bn <- nchar(s2)+1
> >
> >
> > # If one arg is an empty string, returns the length of the other
> > if (nchar(a)==0) return(nchar(b))
> > if (nchar(b)==0) return(nchar(a))
> >
> >
> > # Initialize matrix for calculations
> > m <- matrix("", nrow=an, ncol=bn)
> >
> > for (i in 2:an)
> > for (j in 2:bn)
> > m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
> > paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
> > else
> > longest(m[i-1,j], m[i,j-1])
> >
> > # Returns the distance
> > m[an,bn]
> > }
> >
> >
> >
> > On 1/10/06, Werner Wernersen wrote:
> > > Hi,
> > >
> > > Before I reinvent the wheel I wanted to kindly ask you for your opinion if
> > there is a simple way to do it.
> > >
> > > I want to merge a larger number of tables from different data sources in R
> > and the matching criterium are country names. The tables are of different
> > size and sometimes the country names do differ slightly.
> > >
> > > Has anyone done this or any recommendation on what commands I should look
> > at to automize this task as much as possible?
> > >
> > > Thanks a lot for your effort in advance.
> > >
> > > All the best,
> > > Werner
> > >
> > >
> > >
> > > ---------------------------------
> > > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >
> > ________________________________
> > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
> > Jetzt Yahoo! Messenger installieren!
> >
> >
>



From ggrothendieck at gmail.com  Tue Jan 10 21:26:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jan 2006 15:26:05 -0500
Subject: [R] R CMD not recognized at command-line
In-Reply-To: <6.1.2.0.2.20060110112927.023feb80@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20060110112927.023feb80@epurdom.pobox.stanford.edu>
Message-ID: <971536df0601101226l18a2ecfan69057c718ec2b7ee@mail.gmail.com>

In

http://cran.r-project.org/contrib/extra/batchfiles/

there are Windows XP batch files that you can use that will
automatically locate R using the registry and run it so that you
don't have to change your path.  Just place them anywhere
in your path.  See the README for more info.

I use them for building packages and also have a shortcut on my
desktop to one of them with a shortcut key defined so that after
installing a new version of R it automatically uses the new
version without any action on my part to reset the shortcut and
shortcut key.

On 1/10/06, Elizabeth Purdom <epurdom at stanford.edu> wrote:
> Hi,
> I am trying to run a batch command on Windows XP and R CMD is not
> recognized. I get the error,
>
> "'R' is not recognized as an internal or external command, operable program
> or batch file."
>
> I have "C:\Program Files\R\rw2010\bin" in my $PATH variable and Rcmd.exe
> has been installed in that folder. I have restarted the computer to make
> sure any changes in the $PATH variable registered. I have tried directly
> calling Rcmd.exe or R.exe. None of this had any effect and I can't think
> what I'm forgetting. In Cygwin,  I can get around this by explicitly giving
> the path at the prompt line, for example,
>
>  > ~/Program\ Files/R/rw2010/bin/R CMD --help
>
> This work-around does not seem work from the usual DOS Command Prompt, but
> I rarely use DOS commands so I may be missing something in syntax.
> Similarly, if I want to run several successive input files with a .bat
> file, this work-around won't work as a line in the .bat file.
>
> I have two questions:
> 1) how can I make the computer recognize R so I can just type R CMD at the
> prompt
> 2) what is a work-around like I did in cygwin that will work in a .bat file
> or the standard command-line prompt so that you don't have to change the
> path variable? (often students do not have permission to add the R folder
> to the path variable of a networked computer, so I'd like to know an
> alternative if someone asks me)
>
> Thanks,
> Elizabeth Purdom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Tue Jan 10 21:36:57 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 10 Jan 2006 14:36:57 -0600
Subject: [R] [R-pkgs] Representation of lmer objects will change soon
Message-ID: <40e66e0b0601101236u61c7c269u9c1bea16cb532867@mail.gmail.com>

This is a "heads up" to those using the lmer function for fitting
linear mixed models or generalized linear mixed models.  In the next
few days Martin and I will upload a new version of the Matrix package
that employs a new representation of linear mixed models based on the
supernodal Cholesky factorization provided by Tim Davis's CHOLMOD
library of sparse matrix code.

This representation will be incompatible with the current
representation, which uses a class of blocked compressed sparse
matrices called the dBCMatrix class.  If you have an lmer object
created with the current code you will need to convert it to use it
with the new package.  By far the easiest way to do this is to
recreate it.  (The new code is very fast.)  However, we will also
offer a conversion function for those who no longer have the
information used to produce the model fit.

So the takehome message is that if you are currently using lmer then
you should save the information used to fit a model in addition to
saving the fitted model.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From leif at reflectivity.com  Tue Jan 10 21:50:13 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Tue, 10 Jan 2006 12:50:13 -0800
Subject: [R] "Missing value representation in Excel before
Message-ID: <200601102050.k0AKoFKa017481@hypatia.math.ethz.ch>

I reproduce from memory my exhaustive look into this issue.
RODBC uses the Microsoft ODBC DLL's developed by Microsoft.
  These DLL's perform an automatic determination of column type based on the contents of the first N rows of cells in each column, where N [0,16]. N may be set in the Windows system registry, and there are a few other things that may be set in the system registry which control how the DLL parses an Excel spreadsheet. Unfortunately, the Microsoft DLL's do not always pay attention to the registry settings and do not always interpret them in the same manner.
  The end result is that no matter what you do with RODBC, and no matter how the authors of RODBC re-write it, some Excel spreadsheets will always be unreadable via RODBC given particular insidious combinations of data in some columns of your spreadsheet. (until such time as Microsoft fixes their DLL bugs, I mean features) I have some faint recollection that the Microsoft DLL incorrectly parses a column with non-empty rows due to some formatting issue of those particular columns, which I was unable to cure by re-formatting the source worksheet.
  I have had to resort to using the gdata package which runs a Perl script "xls2csv.pl", which converts an Excel spreadsheet to CSV, for a few Excel spreadsheets which exhibit the particular anomalies preventing use of RODBC.

Leif Kirschenbaum
Senior Yield Engineer
Reflectivity, Inc.
(408) 737-8100 x307
leif at reflectivity.com 

> Message: 21
> Date: Mon, 9 Jan 2006 18:06:49 +0100
> From: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
> Subject: Re: [R] "Missing value representation in Excel before
> 	extraction to	R with RODBC"
> To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>,	"Petr Pikal"
> 	<petr.pikal at precheza.cz>
> Cc: R-help <r-help at stat.math.ethz.ch>
> Message-ID: <000801c6153f$14ab6e60$4a9d72d5 at Larissa>
> Content-Type: text/plain; format=flowed; charset="iso-8859-1";
> 	reply-type=response
> 
> Dear list,
> 
> Well, those columns in Excel that starts with NA (actually 8 
> NA's in my 
> case) is imported as all NA in R but if the columns starts 
> with at least 
> 3 cells with values (i.e not NA) the are imported correctly 
> to R. When 
> as.is=TRUE is used a simular conversion takes place but now 
> as all <NA> 
> and dates are represented as date-and-time.
> Is there any way to get this correct even when the Excel 
> columns start 
> with several NA's?
> 
> Sincerely
> Fredrik
> 
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Petr Pikal" <petr.pikal at precheza.cz>
> Cc: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help" 
> <r-help at stat.math.ethz.ch>
> Sent: Monday, January 09, 2006 9:36 AM
> Subject: Re: [R] "Missing value representation in Excel before 
> extraction to R with RODBC"
> 
> 
> > On Mon, 9 Jan 2006, Petr Pikal wrote:
> >
> >> Hi
> >>
> >> I believe it has something to do with the column identification
> >> decision. When R decides what is in a column it uses only 
> some values
> >> from the beginning of a file.
> >
> > Not R, Excel.  Excel tells ODBC what the column types are.
> >



From deepayan.sarkar at gmail.com  Tue Jan 10 21:59:58 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 10 Jan 2006 14:59:58 -0600
Subject: [R] two y-axis in xy-plot
In-Reply-To: <F5076E7EAA58F448A0EEC05ADE2317BD030450@muc-exch001.munich.komdat.intern>
References: <F5076E7EAA58F448A0EEC05ADE2317BD030450@muc-exch001.munich.komdat.intern>
Message-ID: <eb555e660601101259x653a451eq47f22be4a6972bfb@mail.gmail.com>

On 1/10/06, Antje Schle <Antje.Schuele at komdat.com> wrote:
>
> It is nearly the same example I wrote about in
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg54238.html. I'll
> print it out again:
>
> In the first column I have numbers from 0 to 23 (hours of a day), the second
> column contains the name of a weekday (Day as factor) and the third column
> contains the number I am interested in. So as an example, the first five
> rows look like that:
>
>      Hour Day Freq
>
> 1     0  Mo    23
>
> 2     1  Mo    20
>
> 3     2  Mo    14
>
> 4     3  Mo    27
>
> 5     4  Mo    26
>
>
> To read: On Monday between 0 and 1 o'clock 23 things happened.
>
> Now I add a new parameter, so that the data looks somehow like that:
>
>      Hour Day Freq Freq2
>
> 1     0  Mo    23	  874
>
> 2     1  Mo    20   476
>
> 3     2  Mo    14	  201
>
> 4     3  Mo    27   912
>
> 5     4  Mo    26   172
>
> Now I'd like to have a plot for every weekday containing the two frequencies
> as a line during the hours.
>
> Is this possible with two y-axes?

Take a look at this thread:

https://stat.ethz.ch/pipermail/r-help/2005-November/081286.html

-Deepayan



From leif at reflectivity.com  Tue Jan 10 22:03:48 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Tue, 10 Jan 2006 13:03:48 -0800
Subject: [R] Find last row (observation) for each combination of variables
Message-ID: <200601102103.k0AL3nrI021234@hypatia.math.ethz.ch>

Let's say I have a data.frame like
A	B	C	TS	other columns
1	1	1	12345
1	1	1	56789
1	2	1	23456
1	2	2	23457
2	4	7	23458
2	4	7	34567
2	4	7	45678

and I want the last row for each unique combination of A/B/C, where by "last" I mean greatest TS.
A	B	C	TS	other columns
1	1	1	56789
1	2	1	23456
1	2	2	23457
2	4	7	45678

I did this simply in SAS:
 proc sort data=DF;
   by A B C descending TS
 run;
 proc sort data=DF NODUPKEY;
   by A B C;
 run;

I tried using "aggregate" to find the maximum TS for each combination of A/B/C, but it's slow.
I also tried "by" but it's also slow.
My current (faster) solution is:

 DF$abc<-paste(DF$A,DF$B,DF$C,sep="")
 abclist<-unique(DF$ABC)
 numtest<-length(abclist)
 maxTS<-rep(0,numtest)
 for(i in 1:numtest){
  maxTS[i]<-max(DF$TS[DF$abc==abclist[i]],na.rm=TRUE)
 }
 maxTSdf<-data.frame(device=I(abc),maxTS=maxTS )
 DF<-merge(DF,maxTSdf,by="abc",all.x=TRUE)
 DF<-Df[DF$TS==DF$maxTS,,drop=TRUE]
 DF$maxTS<-NULL

This seems a bit lengthy for such a simple task.

Any simpler suggestions?

-Leif K.

Leif Kirschenbaum
Senior Yield Engineer
Reflectivity, Inc.
(408) 737-8100 x307
leif at reflectivity.com



From aoganyan at niss.org  Tue Jan 10 22:28:18 2006
From: aoganyan at niss.org (Anna Oganyan)
Date: Tue, 10 Jan 2006 16:28:18 -0500
Subject: [R] expected values of order statistics
Message-ID: <43C426F2.9030904@niss.org>

Hello,
Could somebody point me, is there any function in R which returns 
expected values of order statistics for normal distribution? I have been 
looking and couldn't find it.
Thanks!
Anna



From gunter.berton at gene.com  Tue Jan 10 22:55:52 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 10 Jan 2006 13:55:52 -0800
Subject: [R] Find last row (observation) for each combination of
	variables
In-Reply-To: <200601102103.k0AL3nrI021234@hypatia.math.ethz.ch>
Message-ID: <200601102155.k0ALtfLE016082@volta.gene.com>

Leif:

Rather than trying to mimic what you might do in SAS take advantage of R's
ability to use arbitrary data structures, e.g. lists. So, one approach is:

(your.df is the data frame)

your.list<-split(your.df,your.ts[,1:3],drop=TRUE)
t(sapply(your.list,function(x)x[which.max(x$TS),]))

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leif 
> Kirschenbaum
> Sent: Tuesday, January 10, 2006 1:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Find last row (observation) for each combination 
> of variables
> 
> Let's say I have a data.frame like
> A	B	C	TS	other columns
> 1	1	1	12345
> 1	1	1	56789
> 1	2	1	23456
> 1	2	2	23457
> 2	4	7	23458
> 2	4	7	34567
> 2	4	7	45678
> 
> and I want the last row for each unique combination of A/B/C, 
> where by "last" I mean greatest TS.
> A	B	C	TS	other columns
> 1	1	1	56789
> 1	2	1	23456
> 1	2	2	23457
> 2	4	7	45678
> 
> I did this simply in SAS:
>  proc sort data=DF;
>    by A B C descending TS
>  run;
>  proc sort data=DF NODUPKEY;
>    by A B C;
>  run;
> 
> I tried using "aggregate" to find the maximum TS for each 
> combination of A/B/C, but it's slow.
> I also tried "by" but it's also slow.
> My current (faster) solution is:
> 
>  DF$abc<-paste(DF$A,DF$B,DF$C,sep="")
>  abclist<-unique(DF$ABC)
>  numtest<-length(abclist)
>  maxTS<-rep(0,numtest)
>  for(i in 1:numtest){
>   maxTS[i]<-max(DF$TS[DF$abc==abclist[i]],na.rm=TRUE)
>  }
>  maxTSdf<-data.frame(device=I(abc),maxTS=maxTS )
>  DF<-merge(DF,maxTSdf,by="abc",all.x=TRUE)
>  DF<-Df[DF$TS==DF$maxTS,,drop=TRUE]
>  DF$maxTS<-NULL
> 
> This seems a bit lengthy for such a simple task.
> 
> Any simpler suggestions?
> 
> -Leif K.
> 
> Leif Kirschenbaum
> Senior Yield Engineer
> Reflectivity, Inc.
> (408) 737-8100 x307
> leif at reflectivity.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Tue Jan 10 22:58:25 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 10 Jan 2006 13:58:25 -0800
Subject: [R] expected values of order statistics
In-Reply-To: <43C426F2.9030904@niss.org>
Message-ID: <200601102158.k0ALwE7d005203@meitner.gene.com>

See 3.3.1 in the R FAQ (** not ** the Windows FAQ).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anna Oganyan
> Sent: Tuesday, January 10, 2006 1:28 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] expected values of order statistics
> 
> Hello,
> Could somebody point me, is there any function in R which returns 
> expected values of order statistics for normal distribution? 
> I have been 
> looking and couldn't find it.
> Thanks!
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fredrik.bg.lundgren at bredband.net  Tue Jan 10 23:01:43 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Tue, 10 Jan 2006 23:01:43 +0100
Subject: [R] "Missing value representation in Excel before - solved in a
	way- summary"
References: <200601102050.k0AKoFKa017481@hypatia.math.ethz.ch>
Message-ID: <000c01c61631$71648720$4a9d72d5@Larissa>

Thanks to Leif Kirschenbaum, Brian Ripley and Petr Pikal,

who helped with this problem. Unfortunately it appears as if the problem 
with columns beginning with NAs is deeply connected to Microsoft ODBC 
DLL's automatic determination of column type based on the contents of 
the first N rows of each column and further that this behaviour can't be 
reliably tweaked so that RODBC can function with every file.xls. Until 
the Microsoft ODBC DLL is fixed I will have to use read.table from 
'textfile' or 'clipboard' or Gregory Warnes read.xls in package 'gdata'. 
All three methods have worked OK (inspite of some minor formatting 
aspects of dateformats) even with this tricky file.

Fredrik
----- Original Message ----- 
From: "Leif Kirschenbaum" <leif at reflectivity.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 10, 2006 9:50 PM
Subject: Re: [R] "Missing value representation in Excel before


>I reproduce from memory my exhaustive look into this issue.
> RODBC uses the Microsoft ODBC DLL's developed by Microsoft.
>  These DLL's perform an automatic determination of column type based 
> on the contents of the first N rows of cells in each column, where N 
> [0,16]. N may be set in the Windows system registry, and there are a 
> few other things that may be set in the system registry which control 
> how the DLL parses an Excel spreadsheet. Unfortunately, the Microsoft 
> DLL's do not always pay attention to the registry settings and do not 
> always interpret them in the same manner.
>  The end result is that no matter what you do with RODBC, and no 
> matter how the authors of RODBC re-write it, some Excel spreadsheets 
> will always be unreadable via RODBC given particular insidious 
> combinations of data in some columns of your spreadsheet. (until such 
> time as Microsoft fixes their DLL bugs, I mean features) I have some 
> faint recollection that the Microsoft DLL incorrectly parses a column 
> with non-empty rows due to some formatting issue of those particular 
> columns, which I was unable to cure by re-formatting the source 
> worksheet.
>  I have had to resort to using the gdata package which runs a Perl 
> script "xls2csv.pl", which converts an Excel spreadsheet to CSV, for a 
> few Excel spreadsheets which exhibit the particular anomalies 
> preventing use of RODBC.
>
> Leif Kirschenbaum
> Senior Yield Engineer
> Reflectivity, Inc.
> (408) 737-8100 x307
> leif at reflectivity.com
>
>> Message: 21
>> Date: Mon, 9 Jan 2006 18:06:49 +0100
>> From: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
>> Subject: Re: [R] "Missing value representation in Excel before
>> extraction to R with RODBC"
>> To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>, "Petr Pikal"
>> <petr.pikal at precheza.cz>
>> Cc: R-help <r-help at stat.math.ethz.ch>
>> Message-ID: <000801c6153f$14ab6e60$4a9d72d5 at Larissa>
>> Content-Type: text/plain; format=flowed; charset="iso-8859-1";
>> reply-type=response
>>
>> Dear list,
>>
>> Well, those columns in Excel that starts with NA (actually 8
>> NA's in my
>> case) is imported as all NA in R but if the columns starts
>> with at least
>> 3 cells with values (i.e not NA) the are imported correctly
>> to R. When
>> as.is=TRUE is used a simular conversion takes place but now
>> as all <NA>
>> and dates are represented as date-and-time.
>> Is there any way to get this correct even when the Excel
>> columns start
>> with several NA's?
>>
>> Sincerely
>> Fredrik
>>
>>
>> ----- Original Message ----- 
>> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
>> To: "Petr Pikal" <petr.pikal at precheza.cz>
>> Cc: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>; "R-help"
>> <r-help at stat.math.ethz.ch>
>> Sent: Monday, January 09, 2006 9:36 AM
>> Subject: Re: [R] "Missing value representation in Excel before
>> extraction to R with RODBC"
>>
>>
>> > On Mon, 9 Jan 2006, Petr Pikal wrote:
>> >
>> >> Hi
>> >>
>> >> I believe it has something to do with the column identification
>> >> decision. When R decides what is in a column it uses only
>> some values
>> >> from the beginning of a file.
>> >
>> > Not R, Excel.  Excel tells ODBC what the column types are.
>> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From naiara at mail.utexas.edu  Tue Jan 10 23:02:07 2006
From: naiara at mail.utexas.edu (Naiara S. Pinto)
Date: Tue, 10 Jan 2006 16:02:07 -0600 (CST)
Subject: [R] reading contigency tables
Message-ID: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>

Hi all,

I need some help using read.ftable to read a contingency table. My columns
are organized as follows:
order--family--species--location--number of individuals

I couldn't figure out how to change the data on my text file to be
imported into R; and after you do that, is it possible to convert the
table into a data frame? Any tips would be greatly appreciatted!

Thanks a lot,

Naiara.

--------------------------------------------
Naiara S. Pinto
Ecology, Evolution and Behavior
1 University Station A6700
Austin, TX, 78712



From msakals at interchange.ubc.ca  Wed Jan 11 01:00:06 2006
From: msakals at interchange.ubc.ca (Matt Sakals)
Date: Tue, 10 Jan 2006 16:00:06 -0800
Subject: [R] Levelplot not working from file
Message-ID: <3e08341c63dce3d8ba090008062ee7ee@interchange.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/ec84b3ea/attachment.pl

From Andre.Anderson at saude.gov.br  Wed Jan 11 01:23:31 2006
From: Andre.Anderson at saude.gov.br (=?iso-8859-1?Q?Andr=E9_Anderson_Carvalho?=)
Date: Tue, 10 Jan 2006 22:23:31 -0200
Subject: [R] Odds Ratios in Logistic Regression with Effect Modifiers
Message-ID: <E1DD7B55E18ED4449E6D6A4300A8C3917641C5@srvdf039.saude.gov>

Hi R Users...

I'm trying to get the Odds Ratios for Logistic Regression Coefficients and their confidence intervals when there are interactions, or effect modifiers, in the regression model. May anyone help me how to get them?

Thanks

Andr?? Anderson
Bras??lia, Brazil



From gunter.berton at gene.com  Wed Jan 11 01:23:34 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 10 Jan 2006 16:23:34 -0800
Subject: [R] Levelplot not working from file
In-Reply-To: <3e08341c63dce3d8ba090008062ee7ee@interchange.ubc.ca>
Message-ID: <200601110023.k0B0NNOD013498@meitner.gene.com>

I think this is an FAQ. Anyway, levelplot() is a lattice plot and therefore
must be explicitly printed when source()ed or in a function; i.e.

print(levelplot(....)) 

is required.

-- Bert
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Matt Sakals
> Sent: Tuesday, January 10, 2006 4:00 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Levelplot not working from file
> 
> I am trying to use the levelplot function from a command file.
> Here is the code:
> 
> library(sp)
> library(gstat)
> library(lattice)
> 
> gatherData <- read.table("~/gather.txt", header = TRUE)
> grd = makegrid(gatherData$x, gatherData$y, cell.size = 5)
> k <- krige(z~x+y, ~x+y, data = gatherData, newdata = grd, nmax = 5)
> 
> levelplot(var1.pred~x+y, k, aspect = mapasp(k), main = "Predicted 5m")
> 
> It executes fine (it plots up) when I cut and paste to the command 
> prompt but it seems to be missed (nothing occurs) when I call the 
> source file.
> I am using OSX.
> Thanks for any help.
> Matt
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Wed Jan 11 01:32:27 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 10 Jan 2006 16:32:27 -0800
Subject: [R] Levelplot not working from file
In-Reply-To: <3e08341c63dce3d8ba090008062ee7ee@interchange.ubc.ca>
References: <3e08341c63dce3d8ba090008062ee7ee@interchange.ubc.ca>
Message-ID: <43C4521B.70000@pdf.com>

Hi, Matt,

See FAQ 7.22.

--sundar

Matt Sakals wrote:
> I am trying to use the levelplot function from a command file.
> Here is the code:
> 
> library(sp)
> library(gstat)
> library(lattice)
> 
> gatherData <- read.table("~/gather.txt", header = TRUE)
> grd = makegrid(gatherData$x, gatherData$y, cell.size = 5)
> k <- krige(z~x+y, ~x+y, data = gatherData, newdata = grd, nmax = 5)
> 
> levelplot(var1.pred~x+y, k, aspect = mapasp(k), main = "Predicted 5m")
> 
> It executes fine (it plots up) when I cut and paste to the command 
> prompt but it seems to be missed (nothing occurs) when I call the 
> source file.
> I am using OSX.
> Thanks for any help.
> Matt
> 	[[alternative text/enriched version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ruser2006 at yahoo.com  Wed Jan 11 02:25:23 2006
From: ruser2006 at yahoo.com (r user)
Date: Tue, 10 Jan 2006 17:25:23 -0800 (PST)
Subject: [R] matrix logic
Message-ID: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>

I have 2 dataframes, each with 5 columns and 20 rows.
They are called data1 and data2.I wish to create a
third dataframe called data3, also with 5 columns and
20 rows.

I want data3 to contains the values in data1 when the
value in data1 is not NA.  Otherwise it should contain
the values in data2.

I have tried afew methids, but they do not seem to
work as intended.:

data3<-ifelse(is.na(data1)=F,data1,data2)

and 

data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])

Please suggest the ?best? way.



From dhinds at sonic.net  Wed Jan 11 02:53:30 2006
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Wed, 11 Jan 2006 01:53:30 +0000 (UTC)
Subject: [R] hypothesis testing for rank-deficient linear models
Message-ID: <dq1oeq$1ri$1@sea.gmane.org>

Take the following example:

    a <- rnorm(100)
    b <- trunc(3*runif(100))
    g <- factor(trunc(4*runif(100)),labels=c('A','B','C','D'))
    y <- rnorm(100) + a + (b+1) * (unclass(g)+2)
    m <- lm(y~a+b*g)
    summary(m)

Here b is discrete but not treated as a factor.  I am interested in
computing the effect of b within groups defined by the factor g.  One
way to do that is with the estimable() function from gmodels:

    ct <- cbind(1,contr.treatment(4))
    dimnames(ct)[[2]] <- c('b','b:gB','b:gC','b:gD')
    estimable(m, ct, conf.int=0.95)

Another way is the contrast() function from the Design library:

    dd <- datadist(a,b,g)
    options(datadist='dd')
    o <- ols(y~a+b*g)
    contrast(o, list(b=1,g=levels(g)), list(b=0,g=levels(g)))

Now take the situation where there are empty cells in the b x g table,
so that the model is rank deficient, i.e.:

    m <- lm(y~a+b*g, subset=(b==0 | g!='B'))
    summary(m)

Now there's trouble.  The estimable() function and Design library do
not seem to handle rank deficient models well.  Here is my current
best attempt to get what I want:

    my.coef <- function(n)
    {
        ct <- contr.treatment(levels(g), base=n)
        u <- update(m, contrasts=list(g=ct))
        uc <- coef(summary(u))['b',]
        if (any(is.na(coef(u))) && any(!is.na(alias(u)$Complete)))
            uc[1:4] <- NA
        uc
    }
    t(sapply(1:4, my.coef))

This seems adequate for my immediate purposes and I can clean it up to
be more general.  But I'm wondering if there is an easier/better way
using existing R facilities?  Here, I'm doing more model fitting than
necessary, but (for now) speed is not a factor and I was having
trouble writing a concise solution that worked on just the original
model object.

-- David Hinds



From Mleeds at kellogggroup.com  Wed Jan 11 02:55:18 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Tue, 10 Jan 2006 20:55:18 -0500
Subject: [R] complex matrix manipulation question
Message-ID: <A8B87FDB74320349A9D1CC9021052A764666C1@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/1163c8bc/attachment.pl

From spencer.graves at pdf.com  Wed Jan 11 03:48:18 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 10 Jan 2006 18:48:18 -0800
Subject: [R] Selecting significant peaks in periodograms
In-Reply-To: <20060104195210.1580.qmail@web52405.mail.yahoo.com>
References: <20060104195210.1580.qmail@web52405.mail.yahoo.com>
Message-ID: <43C471F2.50201@pdf.com>

	  I just got 2 hits for 'RSiteSearch("significant peaks in 
periodogram")', the first of which was 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/61482.html".  If you 
would like more help from this group, PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  Anecdotal evidence suggests 
that posts more consistent with that guide tend to get more useful 
replies quicker.

	  spencer graves

Pete Cap wrote:

> Greetings all,
>  
>  I am using Fourier analysis to search for periodicities in IP 
network traffic by generating periodograms and then visually
examining them for large, distinct peaks.
>  
>  However, in many cases it is not readily apparent where there 
are periodicities.  I have no experience with discrete maths so
I've come up against a block here: How do I define what the
"noise floor" is and what peaks rising above it are significant
enough to warrant further investigation?
>  
>  I had thought to try to detect peaks as outliers by using 
confidence intervals (assuming that the "Power" vector was
normally distributed) but I'm not sure if this is statistically
valid.  If anyone can provide help, or point me to some
resources on the subject, then I'd appreciate it.
>  
>  Incidentally, I have tried to use other methods (the 
Lomb-Scargle method in particular) but haven't found any
especially well-suited to the problem.
>  
>  Best regards,
>  Pete
>  
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From alexandrarma at yahoo.com.br  Wed Jan 11 03:47:47 2006
From: alexandrarma at yahoo.com.br (Alexandra R. M. de Almeida)
Date: Tue, 10 Jan 2006 23:47:47 -0300 (ART)
Subject: [R] Obtaining the adjusted r-square given the regression
	coefficients
Message-ID: <20060111024747.72057.qmail@web33309.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060110/af734e50/attachment.pl

From A.Robinson at ms.unimelb.edu.au  Wed Jan 11 04:12:11 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 11 Jan 2006 14:12:11 +1100
Subject: [R] Problem with making Matrix
Message-ID: <20060111031211.GH75255@ms.unimelb.edu.au>

Hi R-help citizens,

I'm having trouble making version 0.99-6 of Matrix on FreeBSD 6.0.
The error message is:


* Installing *source* package 'Matrix' ...
** libs
gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c Csparse.c -o Csparse.o

... numerous lines deleted ...

gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c triplet_to_col.c -o triplet_to_col.o
f77   -fPIC  -g -O2 -c zpotf2.f -o zpotf2.o
f77   -fPIC  -g -O2 -c zpotrf.f -o zpotrf.o
touch CHOLMOD.stamp UMFPACK.stamp COLAMD.stamp CCOLAMD.stamp AMD.stamp Metis.stamp LDL.stamp
gmake[1]: Entering directory `/tmp/R.INSTALL.WMODs1/Matrix/src/CHOLMOD'
( cd Lib ; make )
make: don't know how to make w. Stop
gmake[1]: *** [library] Error 2


I am running:


> version  
         _                      
platform i386-unknown-freebsd6.0
arch     i386                   
os       freebsd6.0             
system   i386, freebsd6.0       
status                          
major    2                      
minor    2.1                    
year     2005                   
month    12                     
day      20                     
svn rev  36812                  
language R                      

> sessionInfo()
R version 2.2.1, 2005-12-20, i386-unknown-freebsd6.0 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     


NB I was able to install Matrix 0.98-7 using the FreeBSD make without any
problem.  If I try to make version 0.99-6 using the FreeBSD make then
it fails with "Missing dependency operator" errors.

Does anyone have any suggestions?

Thanks much,

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From tom at maladmin.com  Wed Jan 11 04:26:48 2006
From: tom at maladmin.com (Tom)
Date: Tue, 10 Jan 2006 22:26:48 -0500
Subject: [R] matrix logic
In-Reply-To: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>
References: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>
Message-ID: <op.s262iydqmj1dqq@wysiwyg.mshome.net>

On Tue, 10 Jan 2006 20:25:23 -0500, r user <ruser2006 at yahoo.com> wrote:

> I have 2 dataframes, each with 5 columns and 20 rows.
> They are called data1 and data2.I wish to create a
> third dataframe called data3, also with 5 columns and
> 20 rows.
>
> I want data3 to contains the values in data1 when the
> value in data1 is not NA.  Otherwise it should contain
> the values in data2.
>
> I have tried afew methids, but they do not seem to
> work as intended.:
>
> data3<-ifelse(is.na(data1)=F,data1,data2)
>
> and
>
> data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
>
> Please suggest the best way.
>
Not sure about the bast but...

a<-c(1,2,3,NA,5)
b<-c(4,4,4,4,4)

c<-a
c[which(is.na(a))]<-b[which(is.na(a))]




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Jan 11 04:23:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 10 Jan 2006 19:23:45 -0800
Subject: [R] data order affects glmmPQL
In-Reply-To: <BAY102-F3944070732B5C70C225C1BCA2F0@phx.gbl>
References: <BAY102-F3944070732B5C70C225C1BCA2F0@phx.gbl>
Message-ID: <43C47A41.2020500@pdf.com>

	  The correlation between the predictions from your two model fits is 
0.95.  This suggests to me that the differences between the two sets of 
answers have little practical importance, and anyone who disagrees may 
be trying to read more from the results than can actually be supported 
by the data.  It should be fairly easy to select the apparent "best" 
from among several such answers being the one that had a higher 
log(likelihood).  This pushes me to prefer "fit.bar" with a 
log(likelihood) of -32.31 to "fit.foo" with -33.05.

	  I agree that the differences are somewhat disturbing, but you are 
dealing with the output from an iterative solution of a notoriously 
difficult problem, and the standard wisdom is that it is wise to try 
several sets of starting values.  By modifying the order of the 
observations in the data.frame, you have effectively done that.

	  You could probably reduce these differences by creating a local copy 
of "glmmPQL" and modifying the code so a user could provide starting 
values and so you could restart the iteration for either fit with the 
results of the other, possibly iterating to tighter tolerances.

	  hope this helps.
	  spencer graves
p.s.  To got this correlation, I modified the latter portion of your 
script as follows:

fit.foo <- fit.model(foo, poisson)
o.bar <- order(id, score, test, coder)
bar <- foo[o.bar,] # Reorder data frame
fit.bar <- fit.model(bar, poisson)
predict.foo <- predict(fit.foo)
predict.bar <- predict(fit.bar)

cor(predict.foo[o.bar], predict.bar)
[1] 0.9544287

Jack Tanner wrote:

> Is it to be expected that the way a data frame is sorted should affect the 
> model fit by glmmPQL?
> 
> Example:
> 
> library(MASS)
> library(nlme)
> 
> fit.model <- function(il, model.family) {
>   cs <- Initialize(corSymm(form=~1|id), data=il)
>   glmmPQL(score~test+coder, random=~1|id, # 
> control=lmeControl(msMaxIter=100),
>           family=model.family, data=il, correlation=cs)
> }
> 
> score <- 
> c(1,8,1,3,4,4,2,5,3,6,0,3,1,5,0,5,1,11,1,2,4,5,2,4,1,6,1,2,8,16,5,16,3,15,3,12,4,9,2,4,1,8,2,6,4,11,2,9,3,17,2,6)
> id <- rep(1:13, rep(4, 13))
> test <- gl(2, 1, length(score), labels=c("pre", "post"))
> coder <- gl(2, 2, length(score), labels=c("two", "three"))
> 
> foo <- data.frame(id, score, test, coder) # Define data frame
> print(summary(fit.model(foo, poisson)))
> 
> bar <- foo[order(id, score, test, coder),] # Reorder data frame
> print(summary(fit.model(bar, poisson)))
> 
> The two summaries are clearly different. Is this to be expected? Is there a 
> canonical way one should order a data frame before passing it to glmmPQL?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ronggui.huang at gmail.com  Wed Jan 11 04:51:30 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 11 Jan 2006 11:51:30 +0800
Subject: [R] reading contigency tables
In-Reply-To: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>
References: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>
Message-ID: <38b9f0350601101951h50d78cd5i@mail.gmail.com>

I think it can.But if you provide more information,you will be more help.
for example,you had better give a reproducable example in you email.

2006/1/11, Naiara S. Pinto <naiara at mail.utexas.edu>:
> Hi all,
>
> I need some help using read.ftable to read a contingency table. My columns
> are organized as follows:
> order--family--species--location--number of individuals
>
> I couldn't figure out how to change the data on my text file to be
> imported into R; and after you do that, is it possible to convert the
> table into a data frame? Any tips would be greatly appreciatted!
>
> Thanks a lot,
>
> Naiara.
>
> --------------------------------------------
> Naiara S. Pinto
> Ecology, Evolution and Behavior
> 1 University Station A6700
> Austin, TX, 78712
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From f.harrell at vanderbilt.edu  Wed Jan 11 05:29:18 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 10 Jan 2006 22:29:18 -0600
Subject: [R] Odds Ratios in Logistic Regression with Effect Modifiers
In-Reply-To: <E1DD7B55E18ED4449E6D6A4300A8C3917641C5@srvdf039.saude.gov>
References: <E1DD7B55E18ED4449E6D6A4300A8C3917641C5@srvdf039.saude.gov>
Message-ID: <43C4899E.1020403@vanderbilt.edu>

Andr?? Anderson Carvalho wrote:
> Hi R Users...
> 
> I'm trying to get the Odds Ratios for Logistic Regression Coefficients and their confidence intervals when there are interactions, or effect modifiers, in the regression model. May anyone help me how to get them?
> 
> Thanks
> 
> Andr?? Anderson
> Bras??lia, Brazil

Install the Hmisc and Design packages then type ?summary.Design after 
library(Design)


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From bioflash at gmail.com  Wed Jan 11 05:30:24 2006
From: bioflash at gmail.com (Vincent Deng)
Date: Wed, 11 Jan 2006 12:30:24 +0800
Subject: [R] Question about plotting a hclust tree
Message-ID: <455343d90601102030u1c1aaec4pf8915da0e1210166@mail.gmail.com>

Dear R-helpers,

While plotting a hclust tree, is it possible to mark group information on it?

Suppose I have a hclust tree "ClusTree", and I use cutree to cut the
tree into different groups as following

a=cutree(ClusTree,h=10)

How do I mark the grouping info stored in a  while I plot "ClusTree"?


Best Regards....



From hhecwsc at hkucc.hku.hk  Wed Jan 11 06:33:39 2006
From: hhecwsc at hkucc.hku.hk (S.C. Wong)
Date: Wed, 11 Jan 2006 13:33:39 +0800
Subject: [R] Log-likelihood for Multinominal Probit Regression Model
Message-ID: <6.2.1.2.2.20060111133012.056bfb80@hkucc.hku.hk>

I use mnp to run a multinominal probit regression model, but the summary 
doesn't contain the model statistics, such as the log-likelihood and degree 
of freedom, for the assessment of the goodness-of-fit of the fitted model. 
Is there any way that I can generate these statistics for the fitted model 
in R?

Many thanks in advance!
SC



From ronggui.huang at gmail.com  Wed Jan 11 06:54:01 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 11 Jan 2006 13:54:01 +0800
Subject: [R] Log-likelihood for Multinominal Probit Regression Model
In-Reply-To: <6.2.1.2.2.20060111133012.056bfb80@hkucc.hku.hk>
References: <6.2.1.2.2.20060111133012.056bfb80@hkucc.hku.hk>
Message-ID: <38b9f0350601102154n665ed6a9n@mail.gmail.com>

the usage of MNP is described in "MNP: R Package for Fitting the
Multinomial Probit Model"
http://www.jstatsoft.org/counter.php?id=128&url=v14/i03/v14i03.pdf&ct=1

If the Dependent Variables is Unordered ,why not use Multinomial
Logistic Regression.see
http://gking.harvard.edu/zelig/docs/_TT_mlogit_TT__Multino.html

Hope this helps.

2006/1/11, S.C. Wong <hhecwsc at hkucc.hku.hk>:
> I use mnp to run a multinominal probit regression model, but the summary
> doesn't contain the model statistics, such as the log-likelihood and degree
> of freedom, for the assessment of the goodness-of-fit of the fitted model.
> Is there any way that I can generate these statistics for the fitted model
> in R?
>
> Many thanks in advance!
> SC
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From johannes at huesing.name  Wed Jan 11 08:16:53 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Wed, 11 Jan 2006 08:16:53 +0100 (CET)
Subject: [R] graphics: axis label
Message-ID: <30028.129.206.90.2.1136963813.squirrel@mail.panix.com>

Hello,
par(las=1) sets the orientation of the axis labels
to horizontal. That is, the tick mark labels. How
do I set the orientation of the axis label, which
annotates the variable plotted along the axis, to
horizontal?

Sorry for asking such a basic question here, but I
haven't found anything in the description of the
"par"s.

Greetings


Johannes



From johannes at huesing.name  Wed Jan 11 08:42:20 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Wed, 11 Jan 2006 08:42:20 +0100 (CET)
Subject: [R] reading contigency tables
In-Reply-To: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>
References: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>
Message-ID: <41034.129.206.90.2.1136965340.squirrel@mail.panix.com>

Just a remark:

> I need some help using read.ftable to read a contingency table. My columns
> are organized as follows:
> order--family--species--location--number of individuals

As the first three variables are nested, I would expect this table
to contain a lot of structural zeroes. I understand you just get
the data in table form and do not intend to work on them as a
contingency table.

> I couldn't figure out how to change the data on my text file to be
> imported into R; and after you do that, is it possible to convert the
> table into a data frame? Any tips would be greatly appreciatted!

data(HairEyeColor)
condensed <- as.data.frame(HairEyeColor)

If this is not sufficient, then

extended <- condensed[-ncol(condensed),
                      rep(1:nrow(condensed),
                          condensed[,ncol(condensed)])]

should do the trick.

Greetings


Johannes



From ligges at statistik.uni-dortmund.de  Wed Jan 11 08:47:01 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Jan 2006 08:47:01 +0100
Subject: [R] matrix logic
In-Reply-To: <op.s262iydqmj1dqq@wysiwyg.mshome.net>
References: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>
	<op.s262iydqmj1dqq@wysiwyg.mshome.net>
Message-ID: <43C4B7F5.5030700@statistik.uni-dortmund.de>

Tom wrote:
> On Tue, 10 Jan 2006 20:25:23 -0500, r user <ruser2006 at yahoo.com> wrote:
> 
> 
>>I have 2 dataframes, each with 5 columns and 20 rows.
>>They are called data1 and data2.I wish to create a
>>third dataframe called data3, also with 5 columns and
>>20 rows.
>>
>>I want data3 to contains the values in data1 when the
>>value in data1 is not NA.  Otherwise it should contain
>>the values in data2.
>>
>>I have tried afew methids, but they do not seem to
>>work as intended.:
>>
>>data3<-ifelse(is.na(data1)=F,data1,data2)
>>
>>and
>>
>>data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
>>
>>Please suggest the best way.

"Better" way is to have the Syntax correct:

data3 <- ifelse(is.na(data1), data2, data1)


Please check the archives for almost millions of posts asking more or 
less this question...!


> Not sure about the bast but...
> 
> a<-c(1,2,3,NA,5)
> b<-c(4,4,4,4,4)
> 
> c<-a
> c[which(is.na(a))]<-b[which(is.na(a))]

Why do you want to know which()?

  na <- is.na(a)
  c[na] <- b[na]


Uwe Ligges


> 
> 
> 
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!  
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jan 11 08:50:46 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Jan 2006 08:50:46 +0100
Subject: [R] reading contigency tables
In-Reply-To: <38b9f0350601101951h50d78cd5i@mail.gmail.com>
References: <33287.129.116.71.233.1136930527.squirrel@129.116.71.233>
	<38b9f0350601101951h50d78cd5i@mail.gmail.com>
Message-ID: <43C4B8D6.1030707@statistik.uni-dortmund.de>

ronggui wrote:

> I think it can.But if you provide more information,you will be more help.
> for example,you had better give a reproducable example in you email.
> 
> 2006/1/11, Naiara S. Pinto <naiara at mail.utexas.edu>:
> 
>>Hi all,
>>
>>I need some help using read.ftable to read a contingency table. My columns
>>are organized as follows:
>>order--family--species--location--number of individuals

What is the problem  with read.table()?

Uwe Ligges


>>I couldn't figure out how to change the data on my text file to be
>>imported into R; and after you do that, is it possible to convert the
>>table into a data frame? Any tips would be greatly appreciatted!
>>
>>Thanks a lot,
>>
>>Naiara.
>>
>>--------------------------------------------
>>Naiara S. Pinto
>>Ecology, Evolution and Behavior
>>1 University Station A6700
>>Austin, TX, 78712
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> --
> 
> Deparment of Sociology
> Fudan University
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jan 11 08:56:03 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Jan 2006 08:56:03 +0100
Subject: [R] graphics: axis label
In-Reply-To: <30028.129.206.90.2.1136963813.squirrel@mail.panix.com>
References: <30028.129.206.90.2.1136963813.squirrel@mail.panix.com>
Message-ID: <43C4BA13.8040108@statistik.uni-dortmund.de>

Johannes H??sing wrote:

> Hello,
> par(las=1) sets the orientation of the axis labels
> to horizontal. That is, the tick mark labels. How
> do I set the orientation of the axis label, which
> annotates the variable plotted along the axis, to
> horizontal?
> 
> Sorry for asking such a basic question here, but I
> haven't found anything in the description of the
> "par"s.


You have to use a call to text() and place it into the margins by 
specifying, e.g.,
  par(xpd=TRUE)

as in:

   plot(1:10, ylab="")
   par(xpd=TRUE, mar=c(4,8,0,0)+.1)
   text(0, 5.5, "Hallo Johannes!", adj=1)


Uwe


> Greetings
> 
> 
> Johannes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From christian.hoffmann at wsl.ch  Wed Jan 11 08:58:49 2006
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Wed, 11 Jan 2006 08:58:49 +0100
Subject: [R] Thunderbird misrepresents manuals
In-Reply-To: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
References: <mailman.13.1136631602.24515.r-help@stat.math.ethz.ch>
Message-ID: <43C4BAB9.8070909@wsl.ch>

Hi there,

I hope that I am in the right forum.

I am using Exceed on a WinNT2000 machine, connected to Solaris
SunOS fluke 5.9 Generic_118558-11 sun4u sparc SUNW,Sun-Fire-480R
with Mozilla Firefox 1.0.7.

Problem: Manuals like R-exts.html, R-lang.html, R-intro.html, 
R-admin.html,  but *not*

-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann

International Conference 5.-7.6.2006 Ekaterinburg Russia
"Climate changes and their impact on boreal and temperate forests"
http://ecoinf.uran.ru/conference/



From ligges at statistik.uni-dortmund.de  Wed Jan 11 09:08:21 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Jan 2006 09:08:21 +0100
Subject: [R] Obtaining the adjusted r-square given the
	regression	coefficients
In-Reply-To: <20060111024747.72057.qmail@web33309.mail.mud.yahoo.com>
References: <20060111024747.72057.qmail@web33309.mail.mud.yahoo.com>
Message-ID: <43C4BCF5.1080205@statistik.uni-dortmund.de>

Alexandra R. M. de Almeida wrote:

> Dear list
> 
> I want to obtain the adjusted r-square given a set of coefficients
> (without the intercept), and I don't know if there is a function that
> does it. Exist???????????????? I know that if you make a linear

You can read the code of summary.lm and adapt it.

Uwe Ligges


> regression, you enter the dataset and have in "summary" the adjusted
> r-square. But this is calculated using the coefficients that R
> obtained,and I want other coefficients that i calculated separately
> and differently (without the intercept term too). I have made a
> function based in the equations of the book "Linear Regression
> Analisys" (Wiley Series in probability and mathematical statistics),
> but it doesn't return values between 0 and 1. What is wrong???? The
> functions is given by:
> 
> 
> adjustedR2<-function(Y,X,saM) { if(is.matrix(Y)==F) (Y<-as.matrix(Y))
>  if(is.matrix(X)==F) (X<-as.matrix(X)) if(is.matrix(saM)==F)
> (saM<-as.matrix(saM)) RX<-rent.matrix(X,1)$Rentabilidade.tipo 
> RY<-rent.matrix(Y,1)$Rentabilidade.tipo 
> r2m<-matrix(0,nrow=ncol(Y),ncol=1) RSS<-matrix(0,ncol=ncol(Y),nrow=1)
>  SYY<-matrix(0,ncol=ncol(Y),nrow=1) for (i in 1:ncol(RY)) { 
> RSS[,i]<-(t(RY[,i])%*%RY[,i])-(saM[i,]%*%(t(RX)%*%RX)%*%t(saM)[,i])
>  SYY[,i]<-sum((RY[,i]-mean(RY[,i]))^2) 
> r2m[i,]<-1-(RSS[,i]/SYY[,i])*((nrow(RY))/(nrow(RY)-ncol(saM)-1)) }
>  dimnames(r2m)<-list(colnames(Y),c("Adjusted R-square")) return(r2m)
>  }
> 
> 
> 
> Thanks! Alexandra
> 
> 
> 
> Alexandra R. Mendes de Almeida
> 
> 
> 
> 
>  ---------------------------------
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan 11 09:15:23 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jan 2006 08:15:23 +0000 (GMT)
Subject: [R] matrix logic
In-Reply-To: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>
References: <20060111012523.83556.qmail@web37014.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0601110811200.27784@gannet.stats>

The equality operator is == not =.  So you need is.na(data1) == FALSE (F 
is a variable, and FALSE is the non-truth value), or, clearer,

ifelse(!is.na(data1), data1, data2)

Another way is

data3 <- data1
data3[is.na(data1)] <- data2[is.na(data1)]

which is more efficient but less clear.

On Tue, 10 Jan 2006, r user wrote:

> I have 2 dataframes, each with 5 columns and 20 rows.
> They are called data1 and data2.I wish to create a
> third dataframe called data3, also with 5 columns and
> 20 rows.
>
> I want data3 to contains the values in data1 when the
> value in data1 is not NA.  Otherwise it should contain
> the values in data2.
>
> I have tried afew methids, but they do not seem to
> work as intended.:
>
> data3<-ifelse(is.na(data1)=F,data1,data2)
>
> and
>
> data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
>
> Please suggest the ?best? way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From maechler at stat.math.ethz.ch  Wed Jan 11 09:23:32 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jan 2006 09:23:32 +0100
Subject: [R] Correct way to test for exact dimensions of matrix or array
In-Reply-To: <971536df0601101147j6010514dieae9d722406857a7@mail.gmail.com>
References: <17347.53018.474709.164656@stat.math.ethz.ch>
	<BFE9B469.798A%gsxej2@cam.ac.uk>
	<971536df0601101147j6010514dieae9d722406857a7@mail.gmail.com>
Message-ID: <17348.49284.527013.85981@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Tue, 10 Jan 2006 14:47:57 -0500 writes:

    Gabor> If its just succint you are after then this is slightly
    Gabor> shorter:

    Gabor> identical(dim(x)+0, c(3,5))

indeed, or, less succinct, but maybe more readable (and along the
"top-level function checks" I had proposed yesterday):

     !is.null(d <- dim(x)) && all(d == c(3,5))



    Gabor> On 1/10/06, Gregory Jefferis <gsxej2 at cam.ac.uk> wrote:
    >> Thanks for suggestions.  This is a simple question in principle, but there
    >> seem to be some wrinkles - I am always having to think quite carefully about
    >> how to test for equality in R.  I should also have said that I would like
    >> the check to be efficient as well safe and succinct.
    >> 
    >> One suggestion was:
    >> 
    >> isTRUE(all.equal(dim(obj), c(3, 5)))
    >> 
    >> But that is not so efficient because all.equal does lots of work esp if it
    >> the objects are not equal.
    >> 
    >> Another suggestion was:
    >> 
    >> all( dim( obj) == c(3,5) )
    >> 
    >> But that is not safe eg because dim(vector(10)) is NULL and
    >> all(NULL==c(3,5)) is actually TRUE (to my initial surprise) so vectors would
    >> pass through the net.
    >> 
    >> So, so far the only way that is efficient, safe and succinct is:
    >> 
    >> identical( dim( obj) , as.integer(c(3,5)))
    >> 
    >> Martin Maechler pointed out that at the beginning of a function you might
    >> want to break down the test into something less succinct, that printed more
    >> specific error messages - a good suggestion for a top level function that is
    >> supposed to be user friendly.
    >> 
    >> Any other suggestions?  Many thanks,
    >> 
    >> Greg Jefferis.
    >> 
    >> On 10/1/06 15:13, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:
    >> 
    >> >>>>>> "Gregory" == Gregory Jefferis <gsxej2 at cam.ac.uk>
    >> >>>>>>     on Tue, 10 Jan 2006 14:47:43 +0000 writes:
    >> >
    >> >     Gregory> Dear R Users,
    >> >
    >> >      Gregory> I want to test the dimensions of an incoming
    >> >      Gregory> vector, matrix or array safely
    >> >
    >> >
    >> >     Gregory> and succinctly.  Specifically I want to check if
    >> >     Gregory> the unknown object has exactly 2 dimensions with a
    >> >     Gregory> specified number of rows and columns.
    >> >
    >> >     Gregory> I thought that the following would work:
    >> >
    >> >>> obj=matrix(1,nrow=3,ncol=5)
    >> >>> identical( dim( obj) , c(3,5) )
    >> >     Gregory> [1] FALSE
    >> >
    >> >     Gregory> But it doesn't because c(3,5) is numeric and the dims are
    >> > integer.  I
    >> >     Gregory> therefore ended up doing something like:
    >> >
    >> >>> identical( dim( obj) , as.integer(c(3,5)))
    >> >
    >> >     Gregory> OR
    >> >
    >> >>> isTRUE(all( dim( obj) == c(3,5) ))
    >> >
    >> > the last one is almost perfect if you leave a way the superfluous
    >> > isTRUE(..).
    >> >
    >> > But, you say that it's part of your function checking it's
    >> > arguments.
    >> > In that case, I'd recommend
    >> >
    >> >      if(length(d <- dim(obj)) != 2)
    >> >   stop("'d' must be matrix-like")
    >> >      if(!all(d == c(3,5)))
    >> >   stop("the matrix must be  3 x 5")
    >> >
    >> > which also provides for nice error messages in case of error.
    >> > A more concise form with less nice error messages is
    >> >
    >> >   stopifnot(length(d <- dim(obj)) == 2,
    >> >             d == c(3,50))
    >> >
    >> >   ## you can leave away  all(.)  for things in stopifnot(.)
    >> >
    >> >
    >> >
    >> >
    >> >     Gregory> Neither of which feel quite right.  Is there a 'correct' way to
    >> > do this?
    >> >
    >> >     Gregory> Many thanks,
    >> >
    >> > You're welcome,
    >> > Martin Maechler, ETH Zurich
    >> >
    >> >     Gregory> Greg Jefferis.
    >> >
    >> >     Gregory> PS Thinking about it, the second form is (doubly) wrong because:
    >> >
    >> >>> obj=array(1,dim=c(3,5,3,5))
    >> >>> isTRUE(all( dim( obj) == c(3,5) ))
    >> >     Gregory> [1] TRUE
    >> >
    >> >     Gregory> OR
    >> >>> obj=numeric(10)
    >> >>> isTRUE(all( dim( obj) == c(3,5) ))
    >> >     Gregory> [1] TRUE
    >> >
    >> >     Gregory> (neither of which are equalities that I am happy with!)
    >> >
    >> 
    >> --
    >> Gregory Jefferis, PhD                               and:
    >> Research Fellow
    >> Department of Zoology                               St John's College
    >> University of Cambridge                             Cambridge
    >> Downing Street                                      CB2 1TP
    >> Cambridge, CB2 3EJ
    >> United Kingdom
    >> 
    >> Tel: +44 (0)1223 336683                             +44 (0)1223 339899
    >> Fax: +44 (0)1223 336676                             +44 (0)1223 337720
    >> 
    >> gsxej2 at cam.ac.uk
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >>



From ripley at stats.ox.ac.uk  Wed Jan 11 09:05:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jan 2006 08:05:22 +0000 (GMT)
Subject: [R] Problem with making Matrix
In-Reply-To: <20060111031211.GH75255@ms.unimelb.edu.au>
References: <20060111031211.GH75255@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.61.0601110800400.27784@gannet.stats>

You are mixing makes.  GNU make (presumably gmake) passes on its -w 
argument to sub-makes, and my guess is that make is a BSD make that does 
not accept it.

The simplest way out is to have the 'make' first in your path as GNU make 
whilst doing this.

BTW, this really is not the appropriate place: the posting guide suggests 
the maintainers and then R-devel.

On Wed, 11 Jan 2006, Andrew Robinson wrote:

> Hi R-help citizens,
>
> I'm having trouble making version 0.99-6 of Matrix on FreeBSD 6.0.
> The error message is:
>
>
> * Installing *source* package 'Matrix' ...
> ** libs
> gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c Csparse.c -o Csparse.o
>
> ... numerous lines deleted ...
>
> gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c triplet_to_col.c -o triplet_to_col.o
> f77   -fPIC  -g -O2 -c zpotf2.f -o zpotf2.o
> f77   -fPIC  -g -O2 -c zpotrf.f -o zpotrf.o
> touch CHOLMOD.stamp UMFPACK.stamp COLAMD.stamp CCOLAMD.stamp AMD.stamp Metis.stamp LDL.stamp
> gmake[1]: Entering directory `/tmp/R.INSTALL.WMODs1/Matrix/src/CHOLMOD'
> ( cd Lib ; make )
> make: don't know how to make w. Stop
> gmake[1]: *** [library] Error 2
>
>
> I am running:
>
>
>> version
>         _
> platform i386-unknown-freebsd6.0
> arch     i386
> os       freebsd6.0
> system   i386, freebsd6.0
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
>> sessionInfo()
> R version 2.2.1, 2005-12-20, i386-unknown-freebsd6.0
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
>
> NB I was able to install Matrix 0.98-7 using the FreeBSD make without any
> problem.  If I try to make version 0.99-6 using the FreeBSD make then
> it fails with "Missing dependency operator" errors.
>
> Does anyone have any suggestions?
>
> Thanks much,
>
> Andrew
> -- 
> Andrew Robinson
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From SAULEAUEA at ch-mulhouse.fr  Wed Jan 11 09:44:48 2006
From: SAULEAUEA at ch-mulhouse.fr (=?iso-8859-1?Q?SAULEAU_Erik-Andr=E9?=)
Date: Wed, 11 Jan 2006 09:44:48 +0100
Subject: [R] matching country name tables from different sources
Message-ID: <A91EF0B9121F834EA6484582DFE1CF4436FB7C@messagerie.chm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/3bbd486f/attachment.pl

From Bernhard_Pfaff at fra.invesco.com  Wed Jan 11 10:16:46 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 11 Jan 2006 09:16:46 -0000
Subject: [R] Obtaining the adjusted r-square given the regression
	coef	ficients
Message-ID: <25D1C2585277D311B9A20000F6CCC71B077C0389@DEFRAEX02>

Hello Alexandra,

R2 is only defined for regressions with intercept. See a decent econometrics
textbook for its derivation.

HTH,
Bernhard

-----Urspr??ngliche Nachricht-----
Von: Alexandra R. M. de Almeida [mailto:alexandrarma at yahoo.com.br] 
Gesendet: Mittwoch, 11. Januar 2006 03:48
An: r-help at stat.math.ethz.ch
Betreff: [R] Obtaining the adjusted r-square given the regression
coefficients

Dear list
  
I want to obtain the adjusted r-square given a set of coefficients (without
the intercept), and I don't know if there is a function that does it.
Exist????????????????
I know that if you make a linear regression, you enter the dataset and have
in "summary" the adjusted r-square. But this is calculated using the
coefficients that R obtained,and I want other coefficients that i calculated
separately and differently (without the intercept term too).
I have made a function based in the equations of the book "Linear Regression
Analisys" (Wiley Series in probability and mathematical statistics), but it
doesn't return values between 0 and 1. What is wrong????
The functions is given by:

                  
adjustedR2<-function(Y,X,saM) 
{
 if(is.matrix(Y)==F) (Y<-as.matrix(Y))    
 if(is.matrix(X)==F) (X<-as.matrix(X))    
 if(is.matrix(saM)==F) (saM<-as.matrix(saM))  
 RX<-rent.matrix(X,1)$Rentabilidade.tipo   
 RY<-rent.matrix(Y,1)$Rentabilidade.tipo   
 r2m<-matrix(0,nrow=ncol(Y),ncol=1)   
 RSS<-matrix(0,ncol=ncol(Y),nrow=1)   
 SYY<-matrix(0,ncol=ncol(Y),nrow=1)   
 for (i in 1:ncol(RY))    
 {    
    RSS[,i]<-(t(RY[,i])%*%RY[,i])-(saM[i,]%*%(t(RX)%*%RX)%*%t(saM)[,i])   
    SYY[,i]<-sum((RY[,i]-mean(RY[,i]))^2)    
    r2m[i,]<-1-(RSS[,i]/SYY[,i])*((nrow(RY))/(nrow(RY)-ncol(saM)-1))    
 }    
 dimnames(r2m)<-list(colnames(Y),c("Adjusted R-square"))  
 return(r2m) 
} 

  

  Thanks!
Alexandra



  Alexandra R. Mendes de Almeida

                                                 


		
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}



From A.Robinson at ms.unimelb.edu.au  Wed Jan 11 10:27:23 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 11 Jan 2006 20:27:23 +1100
Subject: [R] Problem with making Matrix
In-Reply-To: <Pine.LNX.4.61.0601110800400.27784@gannet.stats>
References: <20060111031211.GH75255@ms.unimelb.edu.au>
	<Pine.LNX.4.61.0601110800400.27784@gannet.stats>
Message-ID: <20060111092723.GO75255@ms.unimelb.edu.au>

On Wed, Jan 11, 2006 at 08:05:22AM +0000, Prof Brian Ripley wrote:
> You are mixing makes.  GNU make (presumably gmake) passes on its -w 
> argument to sub-makes, and my guess is that make is a BSD make that does 
> not accept it.
> 
> The simplest way out is to have the 'make' first in your path as GNU make 
> whilst doing this.

Thanks very much, that was just right.  I moved the original make, and
placed a symlink to gmake there instead.  Matrix then installed just fine. 

> BTW, this really is not the appropriate place: the posting guide suggests 
> the maintainers and then R-devel.

Ah, that was a mental slip.  My apologies.

Andrew

> On Wed, 11 Jan 2006, Andrew Robinson wrote:
> 
> >Hi R-help citizens,
> >
> >I'm having trouble making version 0.99-6 of Matrix on FreeBSD 6.0.
> >The error message is:
> >
> >
> >* Installing *source* package 'Matrix' ...
> >** libs
> >gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  
> >-fPIC  -g -O2 -c Csparse.c -o Csparse.o
> >
> >... numerous lines deleted ...
> >
> >gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  
> >-fPIC  -g -O2 -c triplet_to_col.c -o triplet_to_col.o
> >f77   -fPIC  -g -O2 -c zpotf2.f -o zpotf2.o
> >f77   -fPIC  -g -O2 -c zpotrf.f -o zpotrf.o
> >touch CHOLMOD.stamp UMFPACK.stamp COLAMD.stamp CCOLAMD.stamp AMD.stamp 
> >Metis.stamp LDL.stamp
> >gmake[1]: Entering directory `/tmp/R.INSTALL.WMODs1/Matrix/src/CHOLMOD'
> >( cd Lib ; make )
> >make: don't know how to make w. Stop
> >gmake[1]: *** [library] Error 2
> >
> >
> >I am running:
> >
> >
> >>version
> >        _
> >platform i386-unknown-freebsd6.0
> >arch     i386
> >os       freebsd6.0
> >system   i386, freebsd6.0
> >status
> >major    2
> >minor    2.1
> >year     2005
> >month    12
> >day      20
> >svn rev  36812
> >language R
> >
> >>sessionInfo()
> >R version 2.2.1, 2005-12-20, i386-unknown-freebsd6.0
> >
> >attached base packages:
> >[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> >[7] "base"
> >
> >
> >NB I was able to install Matrix 0.98-7 using the FreeBSD make without any
> >problem.  If I try to make version 0.99-6 using the FreeBSD make then
> >it fails with "Missing dependency operator" errors.
> >
> >Does anyone have any suggestions?
> >
> >Thanks much,
> >
> >Andrew
> >-- 
> >Andrew Robinson
> >Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> >University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> >Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From ked at nilu.no  Wed Jan 11 10:52:16 2006
From: ked at nilu.no (Kare Edvardsen)
Date: Wed, 11 Jan 2006 10:52:16 +0100
Subject: [R] Space between axis label and tick labels
Message-ID: <43C4D550.6010205@nilu.no>

I'm writing an publication in two column format and need to shrink some 
plots. After increasing the axis labels it does not look nice at all. 
The y-axis label and tick labels almost touch each other and the x-axis 
tick labels expand into the plot instead of away from it. Is there a 
better way than "cex" to control the:

1) font size of axis and tick labels

2) font thickness

3) placement of both axis and yick labels


Cheers,

Kare

-- 
###########################################
Kare Edvardsen <kare.edvardsen at nilu.no>
Norwegian Institute for Air Research (NILU)
Polarmiljosenteret
NO-9296 Tromso       http://www.nilu.no
Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
###########################################



From maechler at stat.math.ethz.ch  Wed Jan 11 11:18:30 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jan 2006 11:18:30 +0100
Subject: [R] Improving R-Intro {was "Wikis etc."}
In-Reply-To: <Pine.LNX.4.58.0601091021040.1422@maplepark.com>
Message-ID: <17348.56182.402749.959000@stat.math.ethz.ch>

>>>>> "David" == David Forrest <drf5n at maplepark.com>
>>>>>     on Mon, 9 Jan 2006 11:54:30 -0600 (CST) writes:

	  ..................
	  ..................


    David> Since R has such an extensive set of extensions,
    David> maybe we need a section in the R-intro documentation
    David> near

    David> http://cran.r-project.org/doc/manuals/R-intro.html#Writing-your-own-functions
    David> titled "Finding existing functions".  It could
    David> explain the difference between base and recommended,
    David> installed, CRAN, and how someone can find and use
    David> things in these areas using help(), '?',
    David> help.search(), help.start(), RSiteSearch(), and the
    David> mailing lists.

That's a good suggestion.
The file to improve is the texinfo source file (the *.html is
produced from it, as well as the *.pdf version of the manual),
is always available from the subversion archive (as all the rest of
the R sources, past and present), the intro manual being
  https://svn.r-project.org/R/trunk/doc/manual/R-intro.texi

So, yes, we'd welcome (a patch against / an improved version of)
the above file!

Martin



From Roger.Bivand at nhh.no  Wed Jan 11 11:49:41 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 11 Jan 2006 11:49:41 +0100 (CET)
Subject: [R] matching country name tables from different sources
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020F8EF0CF@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.44.0601111147200.30033-100000@reclus.nhh.no>

On Tue, 10 Jan 2006, McGehee, Robert wrote:

> I would throw a tolower() around s1 and s2 so that 'canada' matches with
> 'CANADA', and perhaps consider using a Levenshtein distance rather than
> the longest common subsequence.
> 
> An algorithm for Levenshtein distance can be found here (courtesy of
> Stephen Upton)
> https://stat.ethz.ch/pipermail/r-help/2005-January/062254.html

Or even ?agrep - uses Levenshtein edit distance and has an argument for 
ignoring case. First hit in RSiteSearch("fuzzy match"), by the way.

> 
> Robert
> 
> -----Original Message-----
> From: Werner Wernersen [mailto:pensterfuzzer at yahoo.de] 
> Sent: Tuesday, January 10, 2006 2:00 PM
> To: Gabor Grothendieck
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] matching country name tables from different sources
> 
> Thanks for the nice code, Gabor! 
>   
>   Unfortunately, it seems not to work for my purpose, confuses lots of
> countries when I compare two lists of over 150 countries each. 
>   Do you have any other suggestions?
>   
>   
> 
> Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:  If they were the
> same you could use merge.   To figure out
> the correspondence automatically or semiautomatically, try this:
> 
> x <- c("Canada", "US", "Mexico")
> y <- c("Kanada", "United States", "Mehico")
> result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
> result[] <- sapply(result, nchar)
> # try both which.max and which.min and if you are lucky
> # one of them will give unique values and that is the one to use
> # In this case which.max does.
> apply(result, 1, which.max)  # 1 2 3
> 
> # calculate longest common subsequence between 2 strings
> lcs2 <- function(s1,s2) {
>      longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
>      # Make sure args are strings
>      a <- as.character(s1); an <- nchar(s1)+1
>      b <- as.character(s2); bn <- nchar(s2)+1
> 
> 
>      # If one arg is an empty string, returns the length of the other
>      if (nchar(a)==0) return(nchar(b))
>      if (nchar(b)==0) return(nchar(a))
> 
> 
>      # Initialize matrix for calculations
>      m <- matrix("", nrow=an, ncol=bn)
> 
>      for (i in 2:an)
>           for (j in 2:bn)
>   m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
>    paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
>   else
>    longest(m[i-1,j], m[i,j-1])
> 
>      # Returns the distance
>      m[an,bn]
> }
> 
> 
> 
> On 1/10/06, Werner Wernersen 
>  wrote:
> > Hi,
> >
> >  Before I reinvent the wheel I wanted to kindly ask you for your
> opinion if there is a simple way to do it.
> >
> >  I want to merge a larger number of tables from different data sources
> in R and the matching criterium are country names. The tables are of
> different size and sometimes the country names do differ slightly.
> >
> >  Has anyone done this or any recommendation on what commands I should
> look at to automize this task as much as possible?
> >
> >  Thanks a lot for your effort in advance.
> >
> >  All the best,
> >    Werner
> >
> >
> >
> > ---------------------------------
> > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu PC!
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From maechler at stat.math.ethz.ch  Wed Jan 11 11:53:23 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jan 2006 11:53:23 +0100
Subject: [R] Problem with making Matrix
In-Reply-To: <20060111031211.GH75255@ms.unimelb.edu.au>
References: <20060111031211.GH75255@ms.unimelb.edu.au>
Message-ID: <17348.58275.423755.860803@stat.math.ethz.ch>

>>>>> "Andrew" == Andrew Robinson <A.Robinson at ms.unimelb.edu.au>
>>>>>     on Wed, 11 Jan 2006 14:12:11 +1100 writes:

    Andrew> Hi R-help citizens,
    Andrew> I'm having trouble making version 0.99-6 of Matrix on FreeBSD 6.0.
    Andrew> The error message is:

    Andrew> * Installing *source* package 'Matrix' ...
    Andrew> ** libs
    Andrew> gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c Csparse.c -o Csparse.o


    Andrew> ... numerous lines deleted ...

    Andrew> gcc -I/usr/local/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES  -fPIC  -g -O2 -c triplet_to_col.c -o triplet_to_col.o
    Andrew> f77   -fPIC  -g -O2 -c zpotf2.f -o zpotf2.o
    Andrew> f77   -fPIC  -g -O2 -c zpotrf.f -o zpotrf.o
    Andrew> touch CHOLMOD.stamp UMFPACK.stamp COLAMD.stamp CCOLAMD.stamp AMD.stamp Metis.stamp LDL.stamp
    Andrew> gmake[1]: Entering directory `/tmp/R.INSTALL.WMODs1/Matrix/src/CHOLMOD'
    Andrew> ( cd Lib ; make )
    Andrew> make: don't know how to make w. Stop
    Andrew> gmake[1]: *** [library] Error 2


    Andrew> I am running:


    >> version  
    Andrew> _                      
    Andrew> platform i386-unknown-freebsd6.0
    Andrew> arch     i386                   
    Andrew> os       freebsd6.0             
    Andrew> system   i386, freebsd6.0       
    Andrew> status                          
    Andrew> major    2                      
    Andrew> minor    2.1                    
    Andrew> year     2005                   
    Andrew> month    12                     
    Andrew> day      20                     
    Andrew> svn rev  36812                  
    Andrew> language R                      

    >> sessionInfo()
    Andrew> R version 2.2.1, 2005-12-20, i386-unknown-freebsd6.0 

    Andrew> attached base packages:
    Andrew> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
    Andrew> [7] "base"     


    Andrew> NB I was able to install Matrix 0.98-7 using the FreeBSD make without any
    Andrew> problem.  

Yes, 0.98-7  did not have the new CHOLMOD soureces yet.

    Andrew> problem.  If I try to make version 0.99-6 using the FreeBSD make then
    Andrew> it fails with "Missing dependency operator" errors.

    Andrew> Does anyone have any suggestions?

It could be that in FreeBSD behaves differently from GNU make
and there's something GNU specific in one of the various 'Makefile's...

Ahh, yes, I think I have good guess:
The src/CHOLMOD/Makefile has explicit calls to 'make' as in

>>   # Compile the C-callable libraries and the Demo programs.
>>   all:
>> 	  ( cd Lib ; make )

but from the error message above I see you are using 'gmake'
which I assume is an alias for GNU make.

Of course the explicit 'make' in these Makefiles is "bad" -- 
We (the Matrix authors) may be excused by the fact that it is
"not our code" and we tried to change as little as possible
in order to facilitate updates (when new versions of  the
"upstream" CHOLMOD code would come about).


Can you try and replace 'make' by '$(MAKE)' in the following
three places, and see if it works
possibly after writing (in your shell)
       export MAKE=gmake  
or     setenv MAKE gmake
(depending on the kind of shell you have) 

?

AMD/Makefile:	( cd Source ; $(MAKE) lib )
AMD/Makefile:	( cd Source ; $(MAKE) clean )
CHOLMOD/Makefile:	( cd Lib ; $(MAKE) )
CHOLMOD/Makefile:	( cd Lib ; $(MAKE) )
CHOLMOD/Makefile:	( cd Lib ; $(MAKE) purge )
CHOLMOD/Makefile:	( cd Lib ; $(MAKE) clean )
UMFPACK/Makefile:	( cd Source ; $(MAKE) lib )
UMFPACK/Makefile:	( cd Source ; $(MAKE) clean )


Regards,
Martin Maechler, ETH Zurich



From leog at anicca-vijja.de  Wed Jan 11 13:41:25 2006
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Wed, 11 Jan 2006 13:41:25 +0100
Subject: [R] how to obtain "par(ask=TRUE)" with trellis-plots
Message-ID: <43C4FCF5.70001@anicca-vijja.de>

Dear alltogether,

how can a delay like possible with par(ask=TRUE) be attained while using 
trellis-plots within a loop or something like that?
the following draws each plot without waiting for a signal 
(mouse-klick), so par() does not work for that:

library(nlme)
for(i in 1:3)
{
  fitlme <- lme(Orthodont)
  par(ask=TRUE)                     # does not work with trellis....
  print( plot(augPred(fitlme)) )
}

thanks,

leo

-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/



From rwheeler at echip.com  Wed Jan 11 14:33:40 2006
From: rwheeler at echip.com (Bob Wheeler)
Date: Wed, 11 Jan 2006 08:33:40 -0500
Subject: [R] expected values of order statistics
In-Reply-To: <43C426F2.9030904@niss.org>
References: <43C426F2.9030904@niss.org>
Message-ID: <43C50934.60008@echip.com>

normOrder() in SuppDists

Anna Oganyan wrote:
> Hello,
> Could somebody point me, is there any function in R which returns 
> expected values of order statistics for normal distribution? I have been 
> looking and couldn't find it.
> Thanks!
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Bob Wheeler --- http://www.bobwheeler.com/
    ECHIP, Inc. --- Randomness comes in bunches.



From petri.palmu at geneos.fi  Wed Jan 11 15:00:27 2006
From: petri.palmu at geneos.fi (Petri Palmu)
Date: Wed, 11 Jan 2006 15:00:27 +0100
Subject: [R] gregexpr() - length of the matched text to a vector
In-Reply-To: <43C4FCF5.70001@anicca-vijja.de>
Message-ID: <5.2.0.9.0.20060111145724.02334758@pop.song.fi>



Hi,

I'm using gregexpr(). As a result something like this:

# starting positions of the match:
[[1]]
[1] 7 18

# length of the matched text:
attr(,"match.length")
[1] 4 4

Now, I'd like to have a matrix,
  7    4
18   4

but I don't know how to handle the attr(,"match.length") ...?
The format of the output is pretty unclear to me in that respect.

Thanks in advance,
Petri
----------------------
Petri Palmu, M.Soc.Sc
Statistician
petri.palmu at geneos.fi

Geneos Ltd
tel:+358 9 4366 2512
gsm: +35840 55 249 55
fax:+ 358 9 4366 2523
P.O. Box 25 (Tukholmankatu 2)
FIN-00251, Helsinki, Finland



From a.menicacci at fr.fournierpharma.com  Wed Jan 11 15:26:11 2006
From: a.menicacci at fr.fournierpharma.com (a.menicacci@fr.fournierpharma.com)
Date: Wed, 11 Jan 2006 15:26:11 +0100
Subject: [R] Homogenic groups generation - Randomisation
Message-ID: <OFD98B0E84.3684A675-ONC12570F3.004E6A95-C12570F3.004F4D6E@fr.fournierpharma.com>





Dear R-users,

We expect to create N homogenic groups of n features from an
experimentation including N*n mesures. The aim of this is to prevent from
group effects. How to do that with R functionalities. Does anyone know any
methodes enabling this ?

Best regards.

Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



From Jan.Verbesselt at biw.kuleuven.be  Wed Jan 11 15:31:06 2006
From: Jan.Verbesselt at biw.kuleuven.be (Jan Verbesselt)
Date: Wed, 11 Jan 2006 15:31:06 +0100
Subject: [R] Binary logistic modelling: setting conditions (defining
	thresholds) in the fitted model (lrm)
Message-ID: <1136989866.43c516aa3d5b9@webmail2.kuleuven.be>

Dear Rlist,

We are working with library(Design) & R 2.2.1//
When using the following fitted model:
	knots  <- 5
	lrm.1        <- lrm(X8~rcs(X1,5),x=T,y=T)

X8 (binary 0/1 vector)
X1, X2 explantory variables

We would like to set the probability of X8=1 to zero when the X2
variable is smaller than a defined threshold,
e.g. X2<50, because the X1 variable is not correct (contains more
errors) anymore when X2<50.

How could we  define this in the model smoothly without changing the
values of the variables? 

We keep in mind that setting thresholds in not a good solution because
then information is lost. Therefore we also tested the following model.
However, towards operational methods or techniques setting thresholds is
simplifying relationships. Especially in this case were we saw that X1
could contain more errors when X2 < 50.

lrm.1        <- lrm(X8~rcs(X1,5)+ rcs(X2,5),x=T,y=T)

Thanks a lot for feedback & discussion,
Jan




Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From petri.palmu at geneos.fi  Wed Jan 11 15:38:08 2006
From: petri.palmu at geneos.fi (Petri Palmu)
Date: Wed, 11 Jan 2006 15:38:08 +0100
Subject: [R] gregexpr() - length of the matched text to a vector
In-Reply-To: <5.2.0.9.0.20060111145724.02334758@pop.song.fi>
References: <43C4FCF5.70001@anicca-vijja.de>
Message-ID: <5.2.0.9.0.20060111153110.0222e698@pop.song.fi>

Now I found a solution that seems to work OK for me:
attributes(gregexpr(expression, text)[[1]])

Petri

At 15:00 11.1.2006 +0100, Petri Palmu wrote:


>Hi,
>
>I'm using gregexpr(). As a result something like this:
>
># starting positions of the match:
>[[1]]
>[1] 7 18
>
># length of the matched text:
>attr(,"match.length")
>[1] 4 4
>
>Now, I'd like to have a matrix,
>   7    4
>18   4
>
>but I don't know how to handle the attr(,"match.length") ...?
>The format of the output is pretty unclear to me in that respect.
>
>Thanks in advance,
>Petri
>----------------------
>Petri Palmu, M.Soc.Sc
>Statistician
>petri.palmu at geneos.fi
>
>Geneos Ltd
>tel:+358 9 4366 2512
>gsm: +35840 55 249 55
>fax:+ 358 9 4366 2523
>P.O. Box 25 (Tukholmankatu 2)
>FIN-00251, Helsinki, Finland
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


----------------------
Petri Palmu, M.Soc.Sc
Statistician
petri.palmu at geneos.fi

Geneos Ltd
tel:+358 9 4366 2512
gsm: +35840 55 249 55
fax:+ 358 9 4366 2523
P.O. Box 25 (Tukholmankatu 2)
FIN-00251, Helsinki, Finland



From ggrothendieck at gmail.com  Wed Jan 11 15:41:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jan 2006 09:41:30 -0500
Subject: [R] matching country name tables from different sources
In-Reply-To: <A91EF0B9121F834EA6484582DFE1CF4436FB7C@messagerie.chm.com>
References: <A91EF0B9121F834EA6484582DFE1CF4436FB7C@messagerie.chm.com>
Message-ID: <971536df0601110641u77f43ecdke06f8a9240f13dcd@mail.gmail.com>

I was aware of that which is why I mentioned that it is usually used
for matching last names rather than countries and noted possible need to
modify the algorithm slightly.  soundex is a relatively simple algorithm
so its not too hard.  For example, one could just code the first
letter too.

On 1/11/06, SAULEAU Erik-Andr?? <SAULEAUEA at ch-mulhouse.fr> wrote:
>
>
>  dear all,
>
> yes but the problem with soundex for example is that it does not work when
> an error occur in the first place (Canada vs Kanada) as it keeps the fist
> character. It seems that you have to look after an approximate string
> matching algorithm (for example, a very good one if from Porter-Jaro and
> Winkler at the US Census bureau or have o look to the book of Navarro about
> classification of algorithm).
>
> HTH and an happy new year, erik.
>
>
> -----Message d'origine-----
> De: Gabor Grothendieck
> A: Werner Wernersen
> Cc: r-help at stat.math.ethz.ch
> Date: 10/01/2006 21:16
> Objet: Re: [R] matching country name tables from different sources
>
>
> One other thing to try could be soundex.  ITs normally used for
> last names but it might work here too.  Google to find the
> soundex encoding rules.  Reviewing the country names might
> suggest minor modifications to the soundex algorithm to
> improve it for your case.
>
> On 1/10/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > You can improve it somewhat by first accepting all the largest
> > matches and removing the rows and columns for those and
> > repeatedly doing that with what is left.
> >
> > On 1/10/06, Werner Wernersen <pensterfuzzer at yahoo.de> wrote:
> > > Thanks for the nice code, Gabor!
> > >
> > > Unfortunately, it seems not to work for my purpose, confuses lots of
> > > countries when I compare two lists of over 150 countries each.
> > > Do you have any other suggestions?
> > >
> > >
> > >
> > > Gabor Grothendieck <ggrothendieck at gmail.com> schrieb:
> > > If they were the same you could use merge. To figure out
> > > the correspondence automatically or semiautomatically, try this:
> > >
> > > x <- c("Canada", "US", "Mexico")
> > > y <- c("Kanada", "United States", "Mehico")
> > > result <- outer(x, y, function(x,y) mapply(lcs2, x, y))
> > > result[] <- sapply(result, nchar)
> > > # try both which.max and which.min and if you are lucky
> > > # one of them will give unique values and that is the one to use
> > > # In this case which.max does.
> > > apply(result, 1, which.max) # 1 2 3
> > >
> > > # calculate longest common subsequence between 2 strings
> > > lcs2 <- function(s1,s2) {
> > > longest <- function(x,y) if (nchar(x) > nchar(y)) x else y
> > > # Make sure args are strings
> > > a <- as.character(s1); an <- nchar(s1)+1
> > > b <- as.character(s2); bn <- nchar(s2)+1
> > >
> > >
> > > # If one arg is an empty string, returns the length of the other
> > > if (nchar(a)==0) return(nchar(b))
> > > if (nchar(b)==0) return(nchar(a))
> > >
> > >
> > > # Initialize matrix for calculations
> > > m <- matrix("", nrow=an, ncol=bn)
> > >
> > > for (i in 2:an)
> > > for (j in 2:bn)
> > > m[i,j] <- if (substr(a,i-1,i-1)==substr(b,j-1,j-1))
> > > paste(m[i-1,j-1], substr(a,i-1,i-1), sep = "")
> > > else
> > > longest(m[i-1,j], m[i,j-1])
> > >
> > > # Returns the distance
> > > m[an,bn]
> > > }
> > >
> > >
> > >
> > > On 1/10/06, Werner Wernersen wrote:
> > > > Hi,
> > > >
> > > > Before I reinvent the wheel I wanted to kindly ask you for your
> opinion if
> > > there is a simple way to do it.
> > > >
> > > > I want to merge a larger number of tables from different data
> sources in R
> > > and the matching criterium are country names. The tables are of
> different
> > > size and sometimes the country names do differ slightly.
> > > >
> > > > Has anyone done this or any recommendation on what commands I
> should look
> > > at to automize this task as much as possible?
> > > >
> > > > Thanks a lot for your effort in advance.
> > > >
> > > > All the best,
> > > > Werner
> > > >
> > > >
> > > >
> > > > ---------------------------------
> > > > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu
> PC!
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> > >
> > > ________________________________
> > > Telefonieren Sie ohne weitere Kosten mit Ihren Freunden von PC zu
> PC!
> > > Jetzt Yahoo! Messenger installieren!
> > >
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
> ************************************************************************
> **********
> Afin d'eviter toute propagation de virus informatique, et en complement
> des dispositifs en place, ce message (et ses pieces jointes s'il y en a)
>
> a ete automatiquement analyse par un antivirus de messagerie.
> ************************************************************************
> **********
>
> **********************************************************************************
> Afin d'eviter toute propagation de virus informatique, et en complement
> des dispositifs en place, ce message (et ses pieces jointes s'il y en a)
> a ete automatiquement analyse par un antivirus de messagerie.
> **********************************************************************************
>



From bolker at ufl.edu  Wed Jan 11 15:49:52 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 11 Jan 2006 14:49:52 +0000 (UTC)
Subject: [R] gregexpr() - length of the matched text to a vector
References: <43C4FCF5.70001@anicca-vijja.de>
	<5.2.0.9.0.20060111145724.02334758@pop.song.fi>
Message-ID: <loom.20060111T154058-337@post.gmane.org>

Petri Palmu <petri.palmu <at> geneos.fi> writes:


> I'm using gregexpr(). As a result something like this:
> 
> # starting positions of the match:
> [[1]]
> [1] 7 18
> 
> # length of the matched text:
> attr(,"match.length")
> [1] 4 4
> 
> Now, I'd like to have a matrix,
>   7    4
> 18   4
> 


  something like
 x1 = gregexpr("iss",c("mississippi"))
 x2 = rbind(x1[[1]],attr(x1[[1]],"match.length"))
 x2
     [,1] [,2]
[1,]    2    5
[2,]    3    3



From sfalcon at fhcrc.org  Wed Jan 11 16:04:33 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 11 Jan 2006 07:04:33 -0800
Subject: [R] gregexpr() - length of the matched text to a vector
In-Reply-To: <5.2.0.9.0.20060111145724.02334758@pop.song.fi> (Petri Palmu's
	message of "Wed, 11 Jan 2006 15:00:27 +0100")
References: <5.2.0.9.0.20060111145724.02334758@pop.song.fi>
Message-ID: <m2bqyin6v2.fsf@fhcrc.org>

Hi Petri,

On 11 Jan 2006, petri.palmu at geneos.fi wrote:
> I'm using gregexpr(). As a result something like this:
>
> # starting positions of the match:
> [[1]]
> [1] 7 18
>
> # length of the matched text:
> attr(,"match.length")
> [1] 4 4
>
> Now, I'd like to have a matrix,
> 7    4
> 18   4
>
> but I don't know how to handle the attr(,"match.length") ...?
> The format of the output is pretty unclear to me in that respect.

Brief description of the format: a list.  Each element of the list
is a result that corresponds to a string element in the input
character vector.  Each element consists of an integer vector of
starting positions for a match.  The integer vector has a match.length
atttribute consisting of an integer vector of match lengths.

Whew.  Would a matrix be better?  Probably.

To get a list of matrices you can do:

> txt
[1] "foobarfoobazfoofoo" "foo"                "bar"               
[4] "foofoofoo"         
> lapply(gregexpr("foo", txt), function(x) cbind(x, attr(x, "match.length")))
[[1]]
      x  
[1,]  1 3
[2,]  7 3
[3,] 13 3
[4,] 16 3

[[2]]
     x  
[1,] 1 3

[[3]]
      x   
[1,] -1 -1

[[4]]
     x  
[1,] 1 3
[2,] 4 3
[3,] 7 3


HTH,

+ seth



From f.harrell at vanderbilt.edu  Wed Jan 11 16:10:34 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 11 Jan 2006 09:10:34 -0600
Subject: [R] Binary logistic modelling: setting conditions
 (defining	thresholds) in the fitted model (lrm)
In-Reply-To: <1136989866.43c516aa3d5b9@webmail2.kuleuven.be>
References: <1136989866.43c516aa3d5b9@webmail2.kuleuven.be>
Message-ID: <43C51FEA.7090208@vanderbilt.edu>

Jan Verbesselt wrote:
> Dear Rlist,
> 
> We are working with library(Design) & R 2.2.1//
> When using the following fitted model:
> 	knots  <- 5
> 	lrm.1        <- lrm(X8~rcs(X1,5),x=T,y=T)
> 
> X8 (binary 0/1 vector)
> X1, X2 explantory variables
> 
> We would like to set the probability of X8=1 to zero when the X2
> variable is smaller than a defined threshold,
> e.g. X2<50, because the X1 variable is not correct (contains more
> errors) anymore when X2<50.

Are you sure you want the prob(X8=1) to be zero or to you want to just 
constrain the regression function to be of a certain form?  And keep in 
mind that if the measurement errors are moderate or better it is usually 
  better to use the variable in its original form because otherwise real 
predictive information is lost.

Frank

> 
> How could we  define this in the model smoothly without changing the
> values of the variables? 
> 
> We keep in mind that setting thresholds in not a good solution because
> then information is lost. Therefore we also tested the following model.
> However, towards operational methods or techniques setting thresholds is
> simplifying relationships. Especially in this case were we saw that X1
> could contain more errors when X2 < 50.
> 
> lrm.1        <- lrm(X8~rcs(X1,5)+ rcs(X2,5),x=T,y=T)
> 
> Thanks a lot for feedback & discussion,
> Jan



-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Giovanni_Millo at generali.com  Wed Jan 11 16:45:40 2006
From: Giovanni_Millo at generali.com (Millo Giovanni)
Date: Wed, 11 Jan 2006 16:45:40 +0100
Subject: [R] Obtaining the adjusted r-square given the regression
	coefficients
Message-ID: <74F2D4ED68558643B63A6CC21746040D056F0865@BEMAILEXTS1.ad.generali.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/b4ce3469/attachment.pl

From Reinecke at consultic.com  Wed Jan 11 16:57:56 2006
From: Reinecke at consultic.com (Michael Reinecke)
Date: Wed, 11 Jan 2006 16:57:56 +0100
Subject: [R] SPSS and R ? do they like each other?
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF19686E@sbs2003.CMI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/2f496d1c/attachment.pl

From quantpm at yahoo.com  Wed Jan 11 16:58:05 2006
From: quantpm at yahoo.com (t c)
Date: Wed, 11 Jan 2006 07:58:05 -0800 (PST)
Subject: [R] matrix logic
In-Reply-To: <43C4B7F5.5030700@statistik.uni-dortmund.de>
Message-ID: <20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/0d69762f/attachment.pl

From ccleland at optonline.net  Wed Jan 11 17:09:13 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 11 Jan 2006 11:09:13 -0500
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF19686E@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF19686E@sbs2003.CMI.local>
Message-ID: <43C52DA9.50507@optonline.net>

Michael Reinecke wrote:
> ... and is there also such a nice tool (like spss.get) for exporting
> data frames to SPSS? write.table does not keep the data frame labels -
> neither did the other exporting tools that I found.
> ... 

library(foreign)
?write.foreign

write.foreign(df, datafile, codefile, package = "SPSS")

   The codefile generated is SPSS syntax which will read the datafile 
and create SPSS variable and value labels.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Wed Jan 11 17:13:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Jan 2006 17:13:58 +0100
Subject: [R] matrix logic
In-Reply-To: <20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>
References: <20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>
Message-ID: <43C52EC6.7090904@statistik.uni-dortmund.de>

t c wrote:

> Uwe,
>   FYI:
>    
>   I tried: "data3 <- ifelse(is.na(data1), data2, data1)"
>    
>   It seems to me that data3 is an array of length 100.
>    
>   I do NOT end up with a dataset of 5 columns and 20 rows.

I have not read carefully enough, for a data.frame you can generalize 
the approach as follows:

   data.frame(mapply(function(x,y,z) ifelse(is.na(y), z, y),
              names(D), D, D2, SIMPLIFY=FALSE))

Uwe Ligges



> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>   Tom wrote:
> 
>>On Tue, 10 Jan 2006 20:25:23 -0500, r user wrote:
>>
>>
>>
>>>I have 2 dataframes, each with 5 columns and 20 rows.
>>>They are called data1 and data2.I wish to create a
>>>third dataframe called data3, also with 5 columns and
>>>20 rows.
>>>
>>>I want data3 to contains the values in data1 when the
>>>value in data1 is not NA. Otherwise it should contain
>>>the values in data2.
>>>
>>>I have tried afew methids, but they do not seem to
>>>work as intended.:
>>>
>>>data3<-ifelse(is.na(data1)=F,data1,data2)
>>>
>>>and
>>>
>>>data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
>>>
>>>Please suggest the best way.
> 
> 
> "Better" way is to have the Syntax correct:
> 
> data3 <- ifelse(is.na(data1), data2, data1)
> 
> 
> Please check the archives for almost millions of posts asking more or 
> less this question...!
> 
> 
> 
>>Not sure about the bast but...
>>
>>a<-c(1,2,3,NA,5)
>>b<-c(4,4,4,4,4)
>>
>>c<-a
>>c[which(is.na(a))]<-b[which(is.na(a))]
> 
> 
> Why do you want to know which()?
> 
> na <- is.na(a)
> c[na] <- b[na]
> 
> 
> Uwe Ligges
> 
> 
> 
>>
>>
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html  
> 
> 
> 		
> ---------------------------------
> Yahoo! Photos  Showcase holiday pictures in hardcover
>  Photo Books. You design it and well bind it!



From spencer.graves at pdf.com  Wed Jan 11 17:17:01 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 11 Jan 2006 08:17:01 -0800
Subject: [R] information
In-Reply-To: <8d9cc5050601041813j4158a85at@mail.gmail.com>
References: <8d9cc5050601041813j4158a85at@mail.gmail.com>
Message-ID: <43C52F7D.40300@pdf.com>

	  I just got 59 hits from ' RSiteSearch("space-time")'.  Have you tried 
this?

	  If you would like more help from this listserve, please read the 
posting guide! "www.R-project.org/posting-guide.html" then submit 
another question.  La experiencia sugiere que las prejuntas sigiendo 
esta guia tipicamente receiben contestaciones mas rapido y mas utiles.

	  spencer graves

angel toledo wrote:

> Hi.
> My name is Angel, I am Mexican, and I write by the following thing: I am in
> search of commands or options in R that can be used in regional economics.
> Specially  I am interested in commands who can be interacted with geographic
> information systems, to get a regionalization, using a lot of many
> indicators.
> 
> 
> 
> 
> --
>                            Atentamente
> 
>                     ??ngel Toledo Tolentino
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Wed Jan 11 17:22:42 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 11 Jan 2006 10:22:42 -0600
Subject: [R] how to obtain "par(ask=TRUE)" with trellis-plots
In-Reply-To: <43C4FCF5.70001@anicca-vijja.de>
References: <43C4FCF5.70001@anicca-vijja.de>
Message-ID: <eb555e660601110822s517f1840j6618997b3b90ac0a@mail.gmail.com>

On 1/11/06, Leo Grtler <leog at anicca-vijja.de> wrote:
> Dear alltogether,
>
> how can a delay like possible with par(ask=TRUE) be attained while using
> trellis-plots within a loop or something like that?
> the following draws each plot without waiting for a signal
> (mouse-klick), so par() does not work for that:
>
> library(nlme)
> for(i in 1:3)
> {
>   fitlme <- lme(Orthodont)
>   par(ask=TRUE)                     # does not work with trellis....
>   print( plot(augPred(fitlme)) )
> }

See ?grid.prompt in the grid package. To use it you can either attach
grid, or do

grid::grid.prompt(TRUE)

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From bonneu at cict.fr  Wed Jan 11 17:53:51 2006
From: bonneu at cict.fr (bonneu@cict.fr)
Date: Wed, 11 Jan 2006 17:53:51 +0100
Subject: [R] Datetimes differences
Message-ID: <1136998431.43c53820000b7@webmail.cict.fr>

I want to obtain datetime differences in mins in an other column, in front of my
datetimes. I have tried this :

T1 <- c("12/31/03 23:49","1/1/04 1:14","1/1/04 0:02")
T2 <- c("1/1/04 0:58","1/1/04 1:16","")
toto <- data.frame(T1,T2)
toto

y <- strptime(T1,"%m/%d/%y %H:%M")
x <- strptime(T2,"%m/%d/%y %H:%M")
difftime(x,y)


but, i don't know how can i do in order to obtain something like this :

ans <- c(69,2,NA)
res <- data.frame(T1,T2,ans)
res

what is to be done ?
Thanks.

Florent Bonneu
Laboratoire de Statistique et Probabilit??s
bureau 148  b??t. 1R2
Universit?? Toulouse 3
118 route de Narbonne - 31062 Toulouse cedex 9
bonneu at cict.fr



From ggrothendieck at gmail.com  Wed Jan 11 17:54:39 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jan 2006 11:54:39 -0500
Subject: [R] matrix logic
In-Reply-To: <20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>
References: <43C4B7F5.5030700@statistik.uni-dortmund.de>
	<20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>
Message-ID: <971536df0601110854g5aafc18fq2772d30296e54e87@mail.gmail.com>

The following seems close to the form you were trying.  It works
for matrices, not dataframes.  You can use as.matrix and
as.data.frame to convert back and forth:

# test data
data1 <- data2 <- matrix(1:6,3)
data1[2,2] <- NA

data1[] <- ifelse(is.na(data1), data2, data1)


On 1/11/06, t c <quantpm at yahoo.com> wrote:
> Uwe,
>  FYI:
>
>  I tried: "data3 <- ifelse(is.na(data1), data2, data1)"
>
>  It seems to me that data3 is an array of length 100.
>
>  I do NOT end up with a dataset of 5 columns and 20 rows.
>
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>  Tom wrote:
> > On Tue, 10 Jan 2006 20:25:23 -0500, r user wrote:
> >
> >
> >>I have 2 dataframes, each with 5 columns and 20 rows.
> >>They are called data1 and data2.I wish to create a
> >>third dataframe called data3, also with 5 columns and
> >>20 rows.
> >>
> >>I want data3 to contains the values in data1 when the
> >>value in data1 is not NA. Otherwise it should contain
> >>the values in data2.
> >>
> >>I have tried afew methids, but they do not seem to
> >>work as intended.:
> >>
> >>data3<-ifelse(is.na(data1)=F,data1,data2)
> >>
> >>and
> >>
> >>data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
> >>
> >>Please suggest the best way.
>
> "Better" way is to have the Syntax correct:
>
> data3 <- ifelse(is.na(data1), data2, data1)
>
>
> Please check the archives for almost millions of posts asking more or
> less this question...!
>
>
> > Not sure about the bast but...
> >
> > a<-c(1,2,3,NA,5)
> > b<-c(4,4,4,4,4)
> >
> > c<-a
> > c[which(is.na(a))]<-b[which(is.na(a))]
>
> Why do you want to know which()?
>
> na <- is.na(a)
> c[na] <- b[na]
>
>
> Uwe Ligges
>
>
> >
> >
> >
> >
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
> ---------------------------------
>
>  Photo Books. You design it and we'll bind it!
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ggrothendieck at gmail.com  Wed Jan 11 18:03:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jan 2006 12:03:15 -0500
Subject: [R] Datetimes differences
In-Reply-To: <1136998431.43c53820000b7@webmail.cict.fr>
References: <1136998431.43c53820000b7@webmail.cict.fr>
Message-ID: <971536df0601110903r6e5a2285pf25944dded41d6dd@mail.gmail.com>

Try difftime(x,y,unit="min") or as.numeric(difftime(x,y,unit="min"))
depending on what you want.

On 1/11/06, bonneu at cict.fr <bonneu at cict.fr> wrote:
> I want to obtain datetime differences in mins in an other column, in front of my
> datetimes. I have tried this :
>
> T1 <- c("12/31/03 23:49","1/1/04 1:14","1/1/04 0:02")
> T2 <- c("1/1/04 0:58","1/1/04 1:16","")
> toto <- data.frame(T1,T2)
> toto
>
> y <- strptime(T1,"%m/%d/%y %H:%M")
> x <- strptime(T2,"%m/%d/%y %H:%M")
> difftime(x,y)
>
>
> but, i don't know how can i do in order to obtain something like this :
>
> ans <- c(69,2,NA)
> res <- data.frame(T1,T2,ans)
> res
>
> what is to be done ?
> Thanks.
>
> Florent Bonneu
> Laboratoire de Statistique et Probabilit??s
> bureau 148  b??t. 1R2
> Universit?? Toulouse 3
> 118 route de Narbonne - 31062 Toulouse cedex 9
> bonneu at cict.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jholtman at gmail.com  Wed Jan 11 18:03:39 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 11 Jan 2006 12:03:39 -0500
Subject: [R] Datetimes differences
In-Reply-To: <1136998431.43c53820000b7@webmail.cict.fr>
References: <1136998431.43c53820000b7@webmail.cict.fr>
Message-ID: <644e1f320601110903i666f865dk4c45d82bf0898c20@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/4644330a/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jan 11 18:06:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jan 2006 17:06:01 +0000 (GMT)
Subject: [R] Obtaining the adjusted r-square given the regression
 coefficients
In-Reply-To: <74F2D4ED68558643B63A6CC21746040D056F0865@BEMAILEXTS1.ad.generali.com>
References: <74F2D4ED68558643B63A6CC21746040D056F0865@BEMAILEXTS1.ad.generali.com>
Message-ID: <Pine.LNX.4.61.0601111704260.31289@gannet.stats>

A much shorter (but complete) description of this is on the summary.lm 
help page.  It includes the definitions R (and most statistics references) 
uses.

On Wed, 11 Jan 2006, Millo Giovanni wrote:

> Alexandra,
> some additional remarks taken from my past struggles with R2 :^) Without
> intercept the definition is indeed problematic, as Bernhard notes.
>
> First, to estimate a model omitting the intercept you simply have to
> specify "-1" in the model formula (example on an in-built dataset, for
> data description see help(mtcars)):
>
>> data(mtcars)
>> attach(mtcars)
>> mod<-lm(mpg~hp+wt+qsec) # with intercept
>> summary(mod)
>
> and
>
>> mod0<-lm(mpg~hp+wt+qsec-1) # without
>> summary(mod0)
>
> The reported R2s are different not only in value (which is obvious) but
> also in the definition.
> In fact, there are 2 definitions of R2. With reference to the usual
> analysis of variance in OLS regression (see e.g. Ch.3 in Greene 2003,
> Econometric Analysis, and 3.5.2. in particular), let, in our example,
>
>> SST<-sum(mpg^2)          # total sum of squares
>> SSR<-sum(fitted(mod)^2)  # regression sum of squares
>> SSE<-sum(resid(mod)^2)   # error sum of squares
>
> where (a) SST=SSR+SSE, as you may readily check,
> then the *uncentered* R2 is defined as
>
>> uR2<-SSR/SST
>
> while the *centered* R2 as
>
>> cSST<-sum((mpg-mean(mpg))^2)
>> cSSR<-sum((fitted(mod)-mean(mpg))^2) # as 1) mean(y)=mean(y_hat)
>> cSSE<-sum(resid(mod)^2)              # as 2) mean(e)=0
>> cR2<-cSSR/cSST
>
> and (b) cSST=cSSR+cSSE.
>
> The problem is that the meaning of R2 derives from decompositions (a)
> and (b), but while (a) always holds for OLS models, (b) only holds for
> models with an intercept (as do (1-2) above, on which it is based). Thus
> *centered R2 is meaningless in models without intercept*. People are
> used to cR2, though, so R reports cR2 for models with intercept, uR2 for
> those without (EViews, e.g., reports cR2 for both).
> Adjusted R2s are the same, adjusted by a factor penalizing for df. See
> Greene, who gives
> adjR2 = 1-(n-1)/(n-K)(1-R2) for n obs. and K regressors.
>
> Finally, it is of course feasible to calculate the model coefficients on
> your own, but it would be inefficient (R has an optimized routine for
> OLS, so you'd better use coef(lm(y~X))). Anyway, if you like,
>
>> y<-mpg               # just for notational simplicity..
>> X<-cbind(hp,wt,qsec) # add rep(1,length(hp)) to this data matrix
>                       # if you want an intercept
>
>> b<-solve(crossprod(X),crossprod(X,y))  # the coefficients for mod0
>> y_hat<-X%*%b  # fitted values for y
>> e<-y-y_hat    # model residuals
>
> from which you can obtain anything you need.
>
> Cheers
> Giovanni
>
> Giovanni Millo
> Ufficio Studi
> Assicurazioni Generali SpA
> Via Machiavelli 4, 34131 Trieste (I)
> tel. +39 040 671184
> fax  +39 040 671160
>
> *****************
> Original message:
>
> Date: Wed, 11 Jan 2006 09:16:46 -0000
> From: "Pfaff, Bernhard Dr." <Bernhard_Pfaff at fra.invesco.com>
> Subject: Re: [R] Obtaining the adjusted r-square given the regression
> 	coef	ficients
> To: "'Alexandra R. M. de Almeida'" <alexandrarma at yahoo.com.br>,
> 	r-help at stat.math.ethz.ch
> Message-ID: <25D1C2585277D311B9A20000F6CCC71B077C0389 at DEFRAEX02>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Hello Alexandra,
>
> R2 is only defined for regressions with intercept. See a decent
> econometrics
> textbook for its derivation.
>
> HTH,
> Bernhard
>
> -----Urspr?ngliche Nachricht-----
> Von: Alexandra R. M. de Almeida [mailto:alexandrarma at yahoo.com.br]
> Gesendet: Mittwoch, 11. Januar 2006 03:48
> An: r-help at stat.math.ethz.ch
> Betreff: [R] Obtaining the adjusted r-square given the regression
> coefficients
>
> Dear list
>
> I want to obtain the adjusted r-square given a set of coefficients
> (without
> the intercept), and I don't know if there is a function that does it.
> Exist????????????????
> I know that if you make a linear regression, you enter the dataset and
> have
> in "summary" the adjusted r-square. But this is calculated using the
> coefficients that R obtained,and I want other coefficients that i
> calculated
> separately and differently (without the intercept term too).
> I have made a function based in the equations of the book "Linear
> Regression
> Analisys" (Wiley Series in probability and mathematical statistics), but
> it
> doesn't return values between 0 and 1. What is wrong????
> The functions is given by:
>
>
> adjustedR2<-function(Y,X,saM)
> {
> if(is.matrix(Y)==F) (Y<-as.matrix(Y))
> if(is.matrix(X)==F) (X<-as.matrix(X))
> if(is.matrix(saM)==F) (saM<-as.matrix(saM))
> RX<-rent.matrix(X,1)$Rentabilidade.tipo
> RY<-rent.matrix(Y,1)$Rentabilidade.tipo
> r2m<-matrix(0,nrow=ncol(Y),ncol=1)
> RSS<-matrix(0,ncol=ncol(Y),nrow=1)
> SYY<-matrix(0,ncol=ncol(Y),nrow=1)
> for (i in 1:ncol(RY))
> {
>    RSS[,i]<-(t(RY[,i])%*%RY[,i])-(saM[i,]%*%(t(RX)%*%RX)%*%t(saM)[,i])
>
>    SYY[,i]<-sum((RY[,i]-mean(RY[,i]))^2)
>    r2m[i,]<-1-(RSS[,i]/SYY[,i])*((nrow(RY))/(nrow(RY)-ncol(saM)-1))
> }
> dimnames(r2m)<-list(colnames(Y),c("Adjusted R-square"))
> return(r2m)
> }
>
>
>
>  Thanks!
> Alexandra
>
>
>
>  Alexandra R. Mendes de Almeida
>
>
>
>
>
> ---------------------------------
> Ai sensi del D.Lgs. 196/2003 si precisa che le informazioni ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 11 18:08:38 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jan 2006 17:08:38 +0000 (GMT)
Subject: [R] Datetimes differences
In-Reply-To: <1136998431.43c53820000b7@webmail.cict.fr>
References: <1136998431.43c53820000b7@webmail.cict.fr>
Message-ID: <Pine.LNX.4.61.0601111707270.31289@gannet.stats>

On Wed, 11 Jan 2006 bonneu at cict.fr wrote:

> I want to obtain datetime differences in mins in an other column, in front of my
> datetimes. I have tried this :
>
> T1 <- c("12/31/03 23:49","1/1/04 1:14","1/1/04 0:02")
> T2 <- c("1/1/04 0:58","1/1/04 1:16","")
> toto <- data.frame(T1,T2)
> toto
>
> y <- strptime(T1,"%m/%d/%y %H:%M")
> x <- strptime(T2,"%m/%d/%y %H:%M")
> difftime(x,y)
>
>
> but, i don't know how can i do in order to obtain something like this :
>
> ans <- c(69,2,NA)
> res <- data.frame(T1,T2,ans)
> res

data.frame(T1, T2, mins=as.numeric(difftime(x,y, units="mins")))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Wed Jan 11 18:22:13 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 11 Jan 2006 09:22:13 -0800
Subject: [R] complex matrix manipulation question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A764666C1@exchange.psg.com>
Message-ID: <200601111722.k0BHM2Hi017962@faraday.gene.com>

Mark:

I did not see a reply to your question. Did you get one? If not,here's a
solution using a while() loop which should be fast. One could also use
recursion here in a natural way. This solution assumes that there are no
NA's anywhere -- it's a bit trickier if there are NA's in the x column.
Also, I have omitted matrix notation and just assumed x and y are vectors. I
didn't test this exhaustively, so there might be a few fussy details that
still need debugging. The major problem would be that I did not interpret
your question correctly, but I hope I got it right.

xcsum<-cumsum(x)
i <- 1; k <- 0; n <-length(x); z <- rep(NA,n)
while(i <=  n) {
    xcsum <- xcsum - k
    inew <- which(xcsum > W)[1]
    if(is.na(inew)) break
    else{
        z[inew]<-sum(y[i:inew])
        i<-inew+1
        k<-xcsum[inew]
        }
    }

-- Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Leeds
> Sent: Tuesday, January 10, 2006 5:55 PM
> To: R-Stat Help
> Subject: [R] complex matrix manipulation question
> 
> I've done stuff like this before but
> it's been a while and I'm stuck.
>  
> Suppose I have a matrix with one
> column x and another column y
> and both are numeric and let the
> row index of the matrix be i
>  
> Starting at index i ( i would equal on the first iteration )
> when the cumulative sum of x_i+1 - x_i
> is greater than W = some constant, I want to mark that spot in the
> row, call it  i^* and sum all the values in y between  i and  i^* and
> put that value
> a third column z. Otherwise, the values in the indices of z
> between i and  i^*-1  should be NA.
>  
> Then, start at i^*+1 and do the same thing again.
> and keep doing thisn until I get all the way through the rows
> of the matrix.
>  
> I think this is tricky but I used to do it and I forgot how to.
> If it has to be done using loops, that's okay but
> from previous experience, I don't think looping is necessary.
>  
> Thanks.
>  
>                                       Mark
>   
> 
> 
> **********************************************************************
> This email and any files transmitted with it are 
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Reinecke at consultic.com  Wed Jan 11 18:33:22 2006
From: Reinecke at consultic.com (Michael Reinecke)
Date: Wed, 11 Jan 2006 18:33:22 +0100
Subject: [R] SPSS and R ? do they like each other?
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF196878@sbs2003.CMI.local>

 
Thanks again for your answer! I tried it out. write.foreign produces SPSS syntax, but unfortunally this syntax tells SPSS to take the names (and not the labels) in order to produce SPSS variable labels. The former labels get lost.

I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):

jjread<-  read.spss("test2.sav", use.value.labels=TRUE, to.data.frame=TRUE)
> str(jjread)
`data.frame':   30 obs. of  3 variables:
 $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
 $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
 $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
 - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
  ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
>      datafile<-tempfile()
>      codefile<-tempfile()
>      write.foreign(jjread,datafile,codefile,package="SPSS")
>      file.show(datafile)
>      file.show(codefile)


The syntax file I get is:

DATA LIST FILE= "C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free
/ VAR00001 VAR00002 VAR00003  .

VARIABLE LABELS
VAR00001 "VAR00001" 
 VAR00002 "VAR00002" 
 VAR00003 "VAR00003" 
 .

EXECUTE.


I am working on R 2.2.0. But I think a newer version won ??t fix it either, will it?

Greetings,

Michael


-----Urspr??ngliche Nachricht-----
Von: Chuck Cleland [mailto:ccleland at optonline.net] 
Gesendet: Mittwoch, 11. Januar 2006 17:16
An: Michael Reinecke
Cc: R-help at stat.math.ethz.ch
Betreff: Re: [R] SPSS and R ? do they like each other?

Michael Reinecke wrote:
> ... and is there also such a nice tool (like spss.get) for exporting 
> data frames to SPSS? write.table does not keep the data frame labels - 
> neither did the other exporting tools that I found.
> ... 

library(foreign)
?write.foreign

write.foreign(df, datafile, codefile, package = "SPSS")

   The codefile generated is SPSS syntax which will read the datafile and create SPSS variable and value labels.

--
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Jonathan_Wang at notes.ntrs.com  Wed Jan 11 18:40:40 2006
From: Jonathan_Wang at notes.ntrs.com (Jonathan Wang)
Date: Wed, 11 Jan 2006 11:40:40 -0600
Subject: [R] Looking for functions that do the "nearest neighbor method" and
 the "variable kernel method"
Message-ID: <OFB5DAC077.49938CC8-ON862570F3.005F3F7D-862570F3.00612B6A@notes.ntrs.com>


Dear List,

Please confirm the following:

It may be my eyes playing trick on me, but I can't seem to find functions
that do the "nearest neighbor method" and the "variable kernel method" for
kernel smoothing and density estimation corresponding to the book:

Silverman, B.W., (1986) "Density Estimation for Statistics and Data
Analysis".

With appreciation,



From danova_fr at hotmail.com  Wed Jan 11 18:41:37 2006
From: danova_fr at hotmail.com (david v)
Date: Wed, 11 Jan 2006 17:41:37 +0000
Subject: [R] Permutation columns or boostrapping
Message-ID: <BAY108-F219C10D98A13B0467CAB8797240@phx.gbl>

Hi,
I want to permutate the following matrix and replace  permutated columns. Is 
it possible to control the number of columns permutated. Let's say I only 
want to permute two columns. Can i do that with the sample method or should 
i any bootstrapping method ??  I'm not sure this is the best statisticaly 
way of doing it...??
So the idea behind is to ramdonly generate 1000 permutated matrices from the 
original data matrix and estimated the significance  of each of the values.
Any help would be extremely apreciated..

Here is the code i have so far... that works

x <- matrix(1:10,nr=5,nc=6)
>x
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    6    1    6    1    6
[2,]    2    7    2    7    2    7
[3,]    3    8    3    8    3    8
[4,]    4    9    4    9    4    9
[5,]    5   10    5   10    5   10

>y<-x[,sample(1:6,replace=TRUE)]
>y
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    6    6    1    6    6
[2,]    2    7    7    2    7    7
[3,]    3    8    8    3    8    8
[4,]    4    9    9    4    9    9
[5,]    5   10   10    5   10   10

best,
david



From tlumley at u.washington.edu  Wed Jan 11 18:45:58 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 11 Jan 2006 09:45:58 -0800 (PST)
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF196878@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196878@sbs2003.CMI.local>
Message-ID: <Pine.LNX.4.64.0601110944080.16823@homer23.u.washington.edu>

On Wed, 11 Jan 2006, Michael Reinecke wrote:

>
> Thanks again for your answer! I tried it out. write.foreign produces 
> SPSS syntax, but unfortunally this syntax tells SPSS to take the names 
> (and not the labels) in order to produce SPSS variable labels. The 
> former labels get lost.

Well, yes. That's because write.foreign is basically intended for 
exporting R data frame, which don't have variable labels. It should be a 
fairly simple change. Look at
   foreign:::writeForeignSPSS
which is the function that does the work.

 	-thomas

>
> I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):
>
> jjread<-  read.spss("test2.sav", use.value.labels=TRUE, to.data.frame=TRUE)
>> str(jjread)
> `data.frame':   30 obs. of  3 variables:
> $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
> $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
> $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
> - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
>  ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
>>      datafile<-tempfile()
>>      codefile<-tempfile()
>>      write.foreign(jjread,datafile,codefile,package="SPSS")
>>      file.show(datafile)
>>      file.show(codefile)
>
>
> The syntax file I get is:
>
> DATA LIST FILE= "C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free
> / VAR00001 VAR00002 VAR00003  .
>
> VARIABLE LABELS
> VAR00001 "VAR00001"
> VAR00002 "VAR00002"
> VAR00003 "VAR00003"
> .
>
> EXECUTE.
>
>
> I am working on R 2.2.0. But I think a newer version won ?t fix it either, will it?
>
> Greetings,
>
> Michael
>
>
> -----Urspr?ngliche Nachricht-----
> Von: Chuck Cleland [mailto:ccleland at optonline.net]
> Gesendet: Mittwoch, 11. Januar 2006 17:16
> An: Michael Reinecke
> Cc: R-help at stat.math.ethz.ch
> Betreff: Re: [R] SPSS and R ? do they like each other?
>
> Michael Reinecke wrote:
>> ... and is there also such a nice tool (like spss.get) for exporting
>> data frames to SPSS? write.table does not keep the data frame labels -
>> neither did the other exporting tools that I found.
>> ...
>
> library(foreign)
> ?write.foreign
>
> write.foreign(df, datafile, codefile, package = "SPSS")
>
>   The codefile generated is SPSS syntax which will read the datafile and create SPSS variable and value labels.
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From aleszib at gmail.com  Wed Jan 11 19:12:30 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Wed, 11 Jan 2006 19:12:30 +0100
Subject: [R] Regular expressions
Message-ID: <003701c616da$ae4002b0$a7fdfea9@TAMARA>

Matching regular expressions

Dear useRs!

I have the following problem. I would like to find objects in my environment
that have two strings in it. For example, I might want to find objects that
have in their names "MY" and "TARGET". I do not care about the ordering of
these two substrings in the name, neither what is in front, behind or
between them, the only thing important is that both words are present. I
apologize if this is covered in help pages (then I did not understand it by
reading them several times) or it was answered previously (then I did not
find it).

Since "ls" with argument pattern essentially uses "grep" (if I am not
mistaken), I have an example for "grep"

text<-c("somethigMYsomthing elseTARGET another thing","MY somthing TARGET
another thing","somethig somthing elseTARGETMY another
thing","somethigMTARGETY another thing")

grep(pattern="MY&TARGET", x=text)
#I would like to get 1 2 3  and not 4 or actually their names using
text[grep(pattern="MY&TARGET", x=text)]
#of course, the "pattern" in this case is wrong

I know I can do

text[grep(pattern="MY", x=text)][grep(pattern="TARGET",
x=text[grep(pattern="MY",x=text)])] 

However I hope there exists a more elegant way.

Thanks in advance for any suggestions!

Best,
Ales Ziberna



From p.dalgaard at biostat.ku.dk  Wed Jan 11 19:23:41 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jan 2006 19:23:41 +0100
Subject: [R] Regular expressions
In-Reply-To: <003701c616da$ae4002b0$a7fdfea9@TAMARA>
References: <003701c616da$ae4002b0$a7fdfea9@TAMARA>
Message-ID: <x2d5iylj2q.fsf@viggo.kubism.ku.dk>

"Ales Ziberna" <aleszib at gmail.com> writes:

> Matching regular expressions
> 
> Dear useRs!
> 
> I have the following problem. I would like to find objects in my environment
> that have two strings in it. For example, I might want to find objects that
> have in their names "MY" and "TARGET". I do not care about the ordering of
> these two substrings in the name, neither what is in front, behind or
> between them, the only thing important is that both words are present. I
> apologize if this is covered in help pages (then I did not understand it by
> reading them several times) or it was answered previously (then I did not
> find it).
> 
> Since "ls" with argument pattern essentially uses "grep" (if I am not
> mistaken), I have an example for "grep"
> 
> text<-c("somethigMYsomthing elseTARGET another thing","MY somthing TARGET
> another thing","somethig somthing elseTARGETMY another
> thing","somethigMTARGETY another thing")
> 
> grep(pattern="MY&TARGET", x=text)
> #I would like to get 1 2 3  and not 4 or actually their names using
> text[grep(pattern="MY&TARGET", x=text)]
> #of course, the "pattern" in this case is wrong
> 
> I know I can do
> 
> text[grep(pattern="MY", x=text)][grep(pattern="TARGET",
> x=text[grep(pattern="MY",x=text)])] 
> 
> However I hope there exists a more elegant way.

Perhaps this?

text[intersect(grep("MY",text), grep("TARGET",text))]


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dmbates at gmail.com  Wed Jan 11 19:36:28 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 11 Jan 2006 12:36:28 -0600
Subject: [R] lmer(): nested and non-nested factors in logistic regression
In-Reply-To: <43C40AFC.4040003@stat.columbia.edu>
References: <43C40AFC.4040003@stat.columbia.edu>
Message-ID: <40e66e0b0601111036r22550017xec47538fd00f4177@mail.gmail.com>

The version of lmer based on the supernodal Cholesky factorization,
which we will release "real soon", does not crash on this example.  It
does give very large estimates of the variances in that model fit, at
least for the simulation that I ran.

It is best if you use set.seed(123454321) (or whatever seed appeals to
you) before you simulate data if you are going to post the results. 
That way we can be sure we are running on the same data you did.

On 1/10/06, Andrew Gelman <gelman at stat.columbia.edu> wrote:
> Thanks to some help by Doug Bates (and the updated version of the Matrix
> package), I've refined my question about fitting nested and non-nested
> factors in lmer().  I can get it to work in linear regression but it
> crashes in logistic regression.  Here's my example:
>
> # set up the predictors
>
> n.age <- 4
> n.edu <- 4
> n.rep <- 100
> n.state <- 50
> n <- n.age*n.edu*n.rep
> age.id <- rep (1:n.age, each=n.edu*n.rep)
> edu.id <- rep (1:n.edu, n.age, each=n.rep)
> age.edu.id <- n.edu*(age.id - 1) + edu.id
> state.id <- sample (1:n.state, n, replace=TRUE)
>
> # simulate the varying parameters
>
> a.age <- rnorm (n.age, 1, 2)
> a.edu <- rnorm (n.edu, 3, 4)
> a.age.edu <- rnorm (n.age*n.edu, 0, 5)
> a.state <- rnorm (n.state, 0, 6)
>
> # simulate the data and print to check that i did it right
>
> y.hat <- a.age[age.id] + a.edu[edu.id] + a.age.edu[age.edu.id] +
> a.state[state.id]
> y <- rnorm (n, y.hat, 1)
> print (cbind (age.id, edu.id, age.edu.id, state.id, y.hat, y))
>
> # this model (and simpler versions) work fine:
>
> fit.1 <- lmer (y ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | age.edu.id) +
> (1 | state.id))
>
> # now go to logistic regression
>
> ypos <- ifelse (y > mean(y), 1, 0)
>
> # these work fine:
>
> fit.2 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 |
> age.edu.id), family=binomial(link="logit"))
> fit.3 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | state.id),
> family=binomial(link="logit"))
>
> # this one causes R to crash!!!!!!!
>
> fit.4 <- lmer (ypos ~ 1 + (1 | age.id) + (1 | edu.id) + (1 | age.edu.id)
> + (1 | state.id), family=binomial(link="logit"))
>
> --
>
> All help appreciated.  This is for our book on regression and multilevel
> models, and it would be great if people could get started fitting these
> models in R before having to do the more elaborate modeling in Bugs.
>
> Andrew
>
> --
> Andrew Gelman
> Professor, Department of Statistics
> Professor, Department of Political Science
> gelman at stat.columbia.edu
> www.stat.columbia.edu/~gelman
>
> Tues, Wed, Thurs:
>   Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
>   212-851-2142
> Mon, Fri:
>   International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
>   212-854-7075
>
> Mailing address:
>   1255 Amsterdam Ave, Room 1016
>   Columbia University
>   New York, NY 10027-5904
>   212-851-2142
>   (fax) 212-851-2164
>
>



From dsonneborn at ucdavis.edu  Wed Jan 11 19:39:34 2006
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Wed, 11 Jan 2006 10:39:34 -0800
Subject: [R] 4 smoothed lines on xyplot
Message-ID: <43C550E6.10907@yellow.ucdavis.edu>

I am using the R code listed below to create 4 smoothed lines on a 
xyplot. I'm having trouble fine tuning it. First I think I may need a 
black and white plot so how do I get it to plot the lines with different 
characters, preferable the same characters used in the key (plus, X 
circle and triangle). I might also be interest in a version that draws 
four solid lines of different colors but when I try to use a white 
background the lines change to dots and dashs. When I don't use a white 
background it seems to use the solid colors lines.

plotchar <- c(3, 4 ,1 ,2 )

colr<- c(?green?, ?blue? , ?red?, ?black?)

library(lattice)

trellis.par.set(col.whitebg() )

xyplot(AWGT ~ lipid_adj_lpcb2_cent, groups=grpx, data=pcb_graph3, 
auto.key=TRUE, col=colr,

pch=plotchar, type=c(?1?, ?smooth?), span=.8,

key=list(x=.14, y=.84,

points=list(col=colr, pch=plotchar),

lines=list(col=colr, pch=plotchar),

text=list(levels(pcb_graph3$grpx) , col=colr, pch=plotchar)))


-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516



From todd.taylor at pnl.gov  Wed Jan 11 19:50:27 2006
From: todd.taylor at pnl.gov (Taylor, Z Todd)
Date: Wed, 11 Jan 2006 10:50:27 -0800
Subject: [R] Regular expressions
Message-ID: <656E0E8676B66C4CAE912DDDB0B5BD1E03DC1A3B@pnlmse24.pnl.gov>

"Ales Ziberna" <aleszib at gmail.com> writes:

> Dear useRs!
> 
> I have the following problem. I would like to find objects in 
> my environment
> that have two strings in it. For example, I might want to 
> find objects that
> have in their names "MY" and "TARGET". I do not care about 
> the ordering of
> these two substrings in the name, neither what is in front, behind or
> between them, the only thing important is that both words are 
> present. I
> apologize if this is covered in help pages (then I did not 
> understand it by
> reading them several times) or it was answered previously 
> (then I did not
> find it).
> 
> Since "ls" with argument pattern essentially uses "grep" (if I am not
> mistaken), I have an example for "grep"
> 
> text<-c("somethigMYsomthing elseTARGET another thing","MY 
> somthing TARGET
> another thing","somethig somthing elseTARGETMY another
> thing","somethigMTARGETY another thing")
> 
> grep(pattern="MY&TARGET", x=text)
> #I would like to get 1 2 3  and not 4 or actually their names using
> text[grep(pattern="MY&TARGET", x=text)]
> #of course, the "pattern" in this case is wrong
> 
> I know I can do
> 
> text[grep(pattern="MY", x=text)][grep(pattern="TARGET",
> x=text[grep(pattern="MY",x=text)])] 
> 
> However I hope there exists a more elegant way.
> 
> Thanks in advance for any suggestions!
> 
> Best,
> Ales Ziberna

How about:

    text[grep("(MY|TARGET)", text)]

That works on my Redhat box, R version 2.2.0.

--Todd
-- 
Why does clip mean both cut apart and fasten together?



From magillb at sbcglobal.net  Wed Jan 11 19:52:28 2006
From: magillb at sbcglobal.net (Brett Magill)
Date: Wed, 11 Jan 2006 12:52:28 -0600
Subject: [R] Improving R-Intro {was "Wikis etc."}
In-Reply-To: <17348.56182.402749.959000@stat.math.ethz.ch>
References: <Pine.LNX.4.58.0601091021040.1422@maplepark.com>
	<17348.56182.402749.959000@stat.math.ethz.ch>
Message-ID: <dq3k5c$hj$3@sea.gmane.org>

On an improved R wiki, R-intro:

I think the issue of user-friendliness of documentation has been raised. 
  When I first started using R, I found the S-PLUS online documentation 
very useful.  It is very user-friendly and a great introduction, 
organized by application. See:

S-PLUS 6 Guide to Statistics, Volume I
S-PLUS 6 Guide to Statistics, Volume II

at http://www.insightful.com/support/doc_splus_win.asp

How about a wiki based on this as a model, with some preliminaries and 
then user additions.  Of course, the bottom line is, we need something 
targeted at end-users, not developers.

Brett

Martin Maechler wrote:

> That's a good suggestion.
> The file to improve is the texinfo source file (the *.html is
> produced from it, as well as the *.pdf version of the manual),
> is always available from the subversion archive (as all the rest of
> the R sources, past and present), the intro manual being
>   https://svn.r-project.org/R/trunk/doc/manual/R-intro.texi
> 
> So, yes, we'd welcome (a patch against / an improved version of)
> the above file!
> 
> Martin



From tolga.uzuner at csfb.com  Wed Jan 11 20:04:26 2006
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Wed, 11 Jan 2006 19:04:26 -0000
Subject: [R] Dates
Message-ID: <D8B41C349763B14BB57F4E04339322DB388D80@elon12p32001.csfp.co.uk>

Dear R Users,

I am trying to use its, and for that, I need to use as.POSIXct .

My dates are of the format:"10 January 2006".

How do I convert this into the format acceptable to its ?

Thanks,
Tolga











==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}



From ggrothendieck at gmail.com  Wed Jan 11 20:15:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jan 2006 14:15:22 -0500
Subject: [R] Dates
In-Reply-To: <D8B41C349763B14BB57F4E04339322DB388D80@elon12p32001.csfp.co.uk>
References: <D8B41C349763B14BB57F4E04339322DB388D80@elon12p32001.csfp.co.uk>
Message-ID: <971536df0601111115j25dd0f8fy757b363326f58725@mail.gmail.com>

See ?strptime

Also the help desk article in RNews 4/1 contains info on dates.

On 1/11/06, Uzuner, Tolga <tolga.uzuner at csfb.com> wrote:
> Dear R Users,
>
> I am trying to use its, and for that, I need to use as.POSIXct .
>
> My dates are of the format:"10 January 2006".
>
> How do I convert this into the format acceptable to its ?
>
> Thanks,
> Tolga
>
>
>
>
>
>
>
>
>
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electr...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Mleeds at kellogggroup.com  Wed Jan 11 20:17:31 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Wed, 11 Jan 2006 14:17:31 -0500
Subject: [R] statistical formulation of a problem
Message-ID: <A8B87FDB74320349A9D1CC9021052A76466715@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/fa352fc6/attachment.pl

From sundar.dorai-raj at pdf.com  Wed Jan 11 20:34:05 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 11 Jan 2006 11:34:05 -0800
Subject: [R] statistical formulation of a problem
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A76466715@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A76466715@exchange.psg.com>
Message-ID: <43C55DAD.3090705@pdf.com>

Try the sci.stat.*:

http://groups.google.com/groups/dir?sel=33580657

--sundar

Mark Leeds wrote:
> Does anyone know of a newsgroup where
> I can ask a question about formulating
> a problem statistically ? I don't
> think this is where this is done.
> It has to do ( I think ) with logistic
> regression.
>  
>                                 Thanks
> 
> 
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.connolly at hortresearch.co.nz  Wed Jan 11 20:42:08 2006
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 12 Jan 2006 08:42:08 +1300
Subject: [R] Space between axis label and tick labels
In-Reply-To: <43C4D550.6010205@nilu.no>
References: <43C4D550.6010205@nilu.no>
Message-ID: <20060111194208.GQ18619@hortresearch.co.nz>

On Wed, 11-Jan-2006 at 10:52AM +0100, Kare Edvardsen wrote:

|> I'm writing an publication in two column format and need to shrink some 
|> plots. After increasing the axis labels it does not look nice at all. 
|> The y-axis label and tick labels almost touch each other and the x-axis 
|> tick labels expand into the plot instead of away from it. Is there a 
|> better way than "cex" to control the:
|> 
|> 1) font size of axis and tick labels
|> 
|> 2) font thickness
|> 
|> 3) placement of both axis and yick labels


Try ?par and check out what it has to say about cex.axis and cex.lab.

Without any example code, I'm not clear on what you've tried, but you
might need to check out the axis function as well.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From aleszib2 at gmail.com  Wed Jan 11 22:02:12 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Wed, 11 Jan 2006 22:02:12 +0100
Subject: [R] Regular expressions
In-Reply-To: <x2d5iylj2q.fsf@viggo.kubism.ku.dk>
Message-ID: <003e01c616f2$621d4560$a7fdfea9@TAMARA>

Thank you!

This is definitely an improvement! 

Best,
Ales Ziberna  

-----Original Message-----
From: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] On Behalf Of Peter
Dalgaard
Sent: Wednesday, January 11, 2006 7:24 PM
To: Ales Ziberna
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Regular expressions

"Ales Ziberna" <aleszib at gmail.com> writes:

> Matching regular expressions
> 
> Dear useRs!
> 
> I have the following problem. I would like to find objects in my 
> environment that have two strings in it. For example, I might want to 
> find objects that have in their names "MY" and "TARGET". I do not care 
> about the ordering of these two substrings in the name, neither what 
> is in front, behind or between them, the only thing important is that 
> both words are present. I apologize if this is covered in help pages 
> (then I did not understand it by reading them several times) or it was 
> answered previously (then I did not find it).
> 
> Since "ls" with argument pattern essentially uses "grep" (if I am not 
> mistaken), I have an example for "grep"
> 
> text<-c("somethigMYsomthing elseTARGET another thing","MY somthing 
> TARGET another thing","somethig somthing elseTARGETMY another 
> thing","somethigMTARGETY another thing")
> 
> grep(pattern="MY&TARGET", x=text)
> #I would like to get 1 2 3  and not 4 or actually their names using 
> text[grep(pattern="MY&TARGET", x=text)] #of course, the "pattern" in 
> this case is wrong
> 
> I know I can do
> 
> text[grep(pattern="MY", x=text)][grep(pattern="TARGET", 
> x=text[grep(pattern="MY",x=text)])]
> 
> However I hope there exists a more elegant way.

Perhaps this?

text[intersect(grep("MY",text), grep("TARGET",text))]


-- 
   O__  ---- Peter Dalgaard             ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From aleszib2 at gmail.com  Wed Jan 11 22:10:50 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Wed, 11 Jan 2006 22:10:50 +0100
Subject: [R] Regular expressions
In-Reply-To: <656E0E8676B66C4CAE912DDDB0B5BD1E03DC1A3B@pnlmse24.pnl.gov>
Message-ID: <003f01c616f3$88ba90f0$a7fdfea9@TAMARA>

I guess I have not been clear enough.

I want both words in the results. So if we have:

text<-c("somethigMYsomthing elseTARGET another thing","MY somthing TARGET
another thing","somethig somthing elseTARGETMY another
thing","somethigMTARGETY another thing", "somthingMY somthing else")

The last element should not be returned.

The best suggestion was given by Gabor Grothendieck:
grep("MY.*TARGET|TARGET.*MY", text)

While the one by Peter Dalgaard also does the trick:
text[intersect(grep("MY",text), grep("TARGET",text))]

I was just supriessed that "or" (|) works and "and" (&) does not.

Thanks to all!

Best,
Ales Ziberna 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Taylor, Z Todd
Sent: Wednesday, January 11, 2006 7:50 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Regular expressions

"Ales Ziberna" <aleszib at gmail.com> writes:

> Dear useRs!
> 
> I have the following problem. I would like to find objects in my 
> environment that have two strings in it. For example, I might want to 
> find objects that have in their names "MY" and "TARGET". I do not care 
> about the ordering of these two substrings in the name, neither what 
> is in front, behind or between them, the only thing important is that 
> both words are present. I apologize if this is covered in help pages 
> (then I did not understand it by reading them several times) or it was 
> answered previously (then I did not find it).
> 
> Since "ls" with argument pattern essentially uses "grep" (if I am not 
> mistaken), I have an example for "grep"
> 
> text<-c("somethigMYsomthing elseTARGET another thing","MY somthing 
> TARGET another thing","somethig somthing elseTARGETMY another 
> thing","somethigMTARGETY another thing")
> 
> grep(pattern="MY&TARGET", x=text)
> #I would like to get 1 2 3  and not 4 or actually their names using 
> text[grep(pattern="MY&TARGET", x=text)] #of course, the "pattern" in 
> this case is wrong
> 
> I know I can do
> 
> text[grep(pattern="MY", x=text)][grep(pattern="TARGET", 
> x=text[grep(pattern="MY",x=text)])]
> 
> However I hope there exists a more elegant way.
> 
> Thanks in advance for any suggestions!
> 
> Best,
> Ales Ziberna

How about:

    text[grep("(MY|TARGET)", text)]

That works on my Redhat box, R version 2.2.0.

--Todd
--
Why does clip mean both cut apart and fasten together?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wb at arb-phys.uni-dortmund.de  Wed Jan 11 17:53:25 2006
From: wb at arb-phys.uni-dortmund.de (Wilhelm B. Kloke)
Date: Wed, 11 Jan 2006 16:53:25 +0000 (UTC)
Subject: [R] F-test degree of freedoms in lme4 ?
Message-ID: <1136998412.613028@yorikke.arb-phys.uni-dortmund.de>

I have a problem moving from multistratum aov analysis to lmer.

My dataset has observations of ampl at 4 levels of gapf and 2 levels of bl
on 6 subjects levels VP, with 2 replicates wg each, and is balanced.

Here is the summary of this set with aov:
>> summary(aov(ampl~gapf*bl+Error(VP/(bl*gapf)),hframe2))
>
>Error: VP
>          Df Sum Sq Mean Sq F value Pr(>F)
>Residuals  5    531     106               
>
>Error: VP:bl
>          Df Sum Sq Mean Sq F value Pr(>F)   
>bl         1   1700    1700    37.8 0.0017 **
>Residuals  5    225      45                  
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Error: VP:gapf
>          Df Sum Sq Mean Sq F value  Pr(>F)    
>gapf       3    933     311    24.2 5.3e-06 ***
>Residuals 15    193      13                    
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Error: VP:bl:gapf
>          Df Sum Sq Mean Sq F value Pr(>F)  
>gapf:bl    3   93.9    31.3    3.68  0.036 *
>Residuals 15  127.6     8.5                 
>---
>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>
>Error: Within
>          Df Sum Sq Mean Sq F value Pr(>F)
>Residuals 48    318       7               
>
This is mostly identical the analysis by BMDP 4V, except for the
Greenhouse-Geisser epsilons, which are not estimated this way.

I have to analyse a similar dataset, which is not balanced. So I need to
change the method. Following Pinheiro/Bates p.90f, I tried
> hf2.lme <- lme(ampl~gapf*bl,hframe2,random=list(VP=pdDiag(~gapf*bl),bl=pdDiag(~gapf)))
and some variations of this to get the same F tests generated. At least,
I got the F-test on error stratum VP:bl this way, but not the other two:
>> anova(hf2.lme)
>            numDF denDF F-value p-value
>(Intercept)     1    78  764.86  <.0001
>gapf            3    78   17.68  <.0001
>bl              1     5   37.81  0.0017
>gapf:bl         3    78    2.99  0.0362

Then I tried to move to lmer.
I tried to find something equivalent to the above lme call, with no
success at all.

In case, that the problem is in the data, here is the set:

VP ampl wg bl gapf
1 WJ 22 w s 144
2 CR 23 w s 144
3 MZ 25 w s 144
4 MP 34 w s 144
5 HJ 36 w s 144
6 SJ 26 w s 144
7 WJ 34 w s 80
8 CR 31 w s 80
9 MZ 33 w s 80
10 MP 36 w s 80
11 HJ 37 w s 80
12 SJ 32 w s 80
13 WJ 34 w s 48
14 CR 37 w s 48
15 MZ 38 w s 48
16 MP 38 w s 48
17 HJ 40 w s 48
18 SJ 32 w s 48
19 WJ 36 w s 16
20 CR 40 w s 16
21 MZ 39 w s 16
22 MP 40 w s 16
23 HJ 40 w s 16
24 SJ 38 w s 16
25 WJ 16 g s 144
26 CR 28 g s 144
27 MZ 18 g s 144
28 MP 33 g s 144
29 HJ 37 g s 144
30 SJ 28 g s 144
31 WJ 28 g s 80
32 CR 33 g s 80
33 MZ 24 g s 80
34 MP 34 g s 80
35 HJ 36 g s 80
36 SJ 30 g s 80
37 WJ 32 g s 48
38 CR 38 g s 48
39 MZ 34 g s 48
40 MP 37 g s 48
41 HJ 39 g s 48
42 SJ 30 g s 48
43 WJ 36 g s 16
44 CR 34 g s 16
45 MZ 36 g s 16
46 MP 40 g s 16
47 HJ 40 g s 16
48 SJ 36 g s 16
49 WJ 22 w b 144
50 CR 24 w b 144
51 MZ 20 w b 144
52 MP 26 w b 144
53 HJ 22 w b 144
54 SJ 16 w b 144
55 WJ 26 w b 80
56 CR 24 w b 80
57 MZ 26 w b 80
58 MP 27 w b 80
59 HJ 26 w b 80
60 SJ 18 w b 80
61 WJ 28 w b 48
62 CR 23 w b 48
63 MZ 28 w b 48
64 MP 29 w b 48
65 HJ 27 w b 48
66 SJ 24 w b 48
67 WJ 32 w b 16
68 CR 26 w b 16
69 MZ 30 w b 16
70 MP 28 w b 16
71 HJ 30 w b 16
72 SJ 22 w b 16
73 WJ 22 g b 144
74 CR 18 g b 144
75 MZ 18 g b 144
76 MP 26 g b 144
77 HJ 22 g b 144
78 SJ 18 g b 144
79 WJ 24 g b 80
80 CR 26 g b 80
81 MZ 30 g b 80
82 MP 26 g b 80
83 HJ 26 g b 80
84 SJ 24 g b 80
85 WJ 28 g b 48
86 CR 28 g b 48
87 MZ 27 g b 48
88 MP 30 g b 48
89 HJ 26 g b 48
90 SJ 16 g b 48
91 WJ 28 g b 16
92 CR 19 g b 16
93 MZ 24 g b 16
94 MP 32 g b 16
95 HJ 30 g b 16
96 SJ 22 g b 16
-- 
Dipl.-Math. Wilhelm Bernhard Kloke
Institut fuer Arbeitsphysiologie an der Universitaet Dortmund
Ardeystrasse 67, D-44139 Dortmund, Tel. 0231-1084-257



From deepayan.sarkar at gmail.com  Wed Jan 11 22:35:37 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 11 Jan 2006 15:35:37 -0600
Subject: [R] 4 smoothed lines on xyplot
In-Reply-To: <43C550E6.10907@yellow.ucdavis.edu>
References: <43C550E6.10907@yellow.ucdavis.edu>
Message-ID: <eb555e660601111335r4c296ef5wbe4d5afa1aecfee2@mail.gmail.com>

On 1/11/06, Dean Sonneborn <dsonneborn at ucdavis.edu> wrote:
> I am using the R code listed below to create 4 smoothed lines on a
> xyplot. I'm having trouble fine tuning it. First I think I may need a
> black and white plot so how do I get it to plot the lines with different
> characters, preferable the same characters used in the key (plus, X
> circle and triangle). I might also be interest in a version that draws
> four solid lines of different colors but when I try to use a white
> background the lines change to dots and dashs. When I don't use a white
> background it seems to use the solid colors lines.
>
> plotchar <- c(3, 4 ,1 ,2 )
>
> colr<- c("green", "blue" , "red", "black")
>
> library(lattice)
>
> trellis.par.set(col.whitebg() )
>
> xyplot(AWGT ~ lipid_adj_lpcb2_cent, groups=grpx, data=pcb_graph3,
> auto.key=TRUE, col=colr,
>
> pch=plotchar, type=c("1", "smooth"), span=.8,
>
> key=list(x=.14, y=.84,
>
> points=list(col=colr, pch=plotchar),
>
> lines=list(col=colr, pch=plotchar),
>
> text=list(levels(pcb_graph3$grpx) , col=colr, pch=plotchar)))

Why are you using both auto.key and key? Anyway, the easiest way is to
change the settings and use auto.key, e.g.


xyplot(AWGT ~ lipid_adj_lpcb2_cent, groups=grpx, data=pcb_graph3,
       auto.key = list(lines = TRUE, points = TRUE),

       par.settings =
       list(superpose.symbol = list(col = colr, pch = plotchar),
            superpose.line = list(col = colr, lty = 1)),

       type=c("p", "smooth"), span=.8)

(I'm not sure what type you meant to use, your email has "1", which
doesn't do anything.)

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From bieli at biomillaufen.ch  Wed Jan 11 22:37:55 2006
From: bieli at biomillaufen.ch (Christian Bieli)
Date: Wed, 11 Jan 2006 22:37:55 +0100
Subject: [R] updating formula inside function
Message-ID: <43C57AB3.8050008@biomillaufen.ch>

Dear R-Helpers

Given a function like
foo <- function(data,var1,var2,var3) {
	f <- formula(paste(var1,'~',paste(var2,var3,sep='+'),sep=''))
	linmod <- lm(f)
	return(linmod)
}
By typing
foo(mydata,'a','b','c')
I get the result of the linear model a~b+c.
How can I rewrite the function so that the formula can be updated inside 
the function, i.e.
foo <- function(data,var1,var2,var3,var4) {
	f <- formula(paste(var1,'~',paste(var2,var3,sep='+'),sep=''))
	linmod <- lm(f)
	return(linmod)
	f2 <- update.formula(f,.~.-var3+var4)
}
Like that it won't work because var3 and var4 are characters, but also 
with substitute() and eval() I did not manage to get the favoured result.
Can somebody help me out?
Thank you in advance



From Mleeds at kellogggroup.com  Wed Jan 11 22:46:01 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Wed, 11 Jan 2006 16:46:01 -0500
Subject: [R] a series of 1's and -1's
Message-ID: <A8B87FDB74320349A9D1CC9021052A7646675F@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/a869d174/attachment.pl

From ggrothendieck at gmail.com  Wed Jan 11 22:52:12 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jan 2006 16:52:12 -0500
Subject: [R] updating formula inside function
In-Reply-To: <43C57AB3.8050008@biomillaufen.ch>
References: <43C57AB3.8050008@biomillaufen.ch>
Message-ID: <971536df0601111352r39684c7ex79c2749e80361497@mail.gmail.com>

This was just discussed last week:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-January/083812.html

On 1/11/06, Christian Bieli <bieli at biomillaufen.ch> wrote:
> Dear R-Helpers
>
> Given a function like
> foo <- function(data,var1,var2,var3) {
>        f <- formula(paste(var1,'~',paste(var2,var3,sep='+'),sep=''))
>        linmod <- lm(f)
>        return(linmod)
> }
> By typing
> foo(mydata,'a','b','c')
> I get the result of the linear model a~b+c.
> How can I rewrite the function so that the formula can be updated inside
> the function, i.e.
> foo <- function(data,var1,var2,var3,var4) {
>        f <- formula(paste(var1,'~',paste(var2,var3,sep='+'),sep=''))
>        linmod <- lm(f)
>        return(linmod)
>        f2 <- update.formula(f,.~.-var3+var4)
> }
> Like that it won't work because var3 and var4 are characters, but also
> with substitute() and eval() I did not manage to get the favoured result.
> Can somebody help me out?
> Thank you in advance
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lisas at salford-systems.com  Wed Jan 11 23:08:52 2006
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 11 Jan 2006 14:08:52 -0800
Subject: [R] March 29-31, Data Mining Conference, Southern California,
 Early-bird Deadline Savings of $50
Message-ID: <43C581F4.5060602@salford-systems.com>

SALFORD SYSTEMS DATA MINING CONFERENCE 2006
San Diego, California, March 29-31, 2006
Focusing on the Contributions of Data Mining to Solving Real-World 
Challenges

Business, Biomedical and Environmental Real-World Case Study Presentations

TOPICS INCLUDE:
Credit Risk Modeling; Targeted Marketing and Campaign Optimization; New 
Methods for Personalization; Analytical CRM; Fraud Detection; Military 
Applications; Crime Analysis; Drug Discovery; Data Analysis Related to 
Insurance, Epidemiology, Clinical Medicine, Proteomics and Genomics, 
Mass Spectrometry and Demographic Data; Tools for "Tall and Wide" Data

State-of-the-Art Research from Leading Academic Institutions

**A Commemoration and Celebration of the Lifetime Achievements of Data 
Mining Visionary and World-Renowned Statistician Leo Breiman

PRE-CONFERENCE TRAINING
Sharpen your expertise!
In-depth courses available for attendees who are new to data mining.

REGISTER NOW!  EARLY-BIRD DEADLINE SAVINGS OF $50
http://www.salforddatamining.com/docs/regform06.pdf

CONFERENCE PROGRAM:
http://www.salforddatamining.com/program-sd.htm

GREAT NETWORKING OPPORTUNITY
Attendees at Prior Conferences Included:
The International Monetary Fund, Barnes and Noble, Pfizer, Union Bank, 
Wells Fargo, Ciphergen, Stanford Linear Accelerator, Johns Hopkins 
Medical School, UC Berkeley, Cold Spring Harbor Laboratory, Novartis, 
Columbia University School of Public Health, Harvard Medical School, 
HSBC, International Steel Group(Bethlehem Steel), Cap Gemini, AT&T 
Labs-Research, PricewaterhouseCoopers

Sincerely,
Lisa Solomon



From A.Robinson at ms.unimelb.edu.au  Wed Jan 11 23:27:20 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 12 Jan 2006 09:27:20 +1100
Subject: [R] Problem with making Matrix
In-Reply-To: <17348.58275.423755.860803@stat.math.ethz.ch>
References: <20060111031211.GH75255@ms.unimelb.edu.au>
	<17348.58275.423755.860803@stat.math.ethz.ch>
Message-ID: <20060111222720.GT75255@ms.unimelb.edu.au>

Dear Martin,

That works just fine too.

Thanks for the suggestion,

Andrew

> Can you try and replace 'make' by '$(MAKE)' in the following
> three places, and see if it works
> possibly after writing (in your shell)
>        export MAKE=gmake  
> or     setenv MAKE gmake
> (depending on the kind of shell you have) 
> 
> ?
> 
> AMD/Makefile:	( cd Source ; $(MAKE) lib )
> AMD/Makefile:	( cd Source ; $(MAKE) clean )
> CHOLMOD/Makefile:	( cd Lib ; $(MAKE) )
> CHOLMOD/Makefile:	( cd Lib ; $(MAKE) )
> CHOLMOD/Makefile:	( cd Lib ; $(MAKE) purge )
> CHOLMOD/Makefile:	( cd Lib ; $(MAKE) clean )
> UMFPACK/Makefile:	( cd Source ; $(MAKE) lib )
> UMFPACK/Makefile:	( cd Source ; $(MAKE) clean )
> 
> 
> Regards,
> Martin Maechler, ETH Zurich

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From Robert.McGehee at geodecapital.com  Thu Jan 12 00:29:45 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 11 Jan 2006 18:29:45 -0500
Subject: [R] a series of 1's and -1's
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946748@MSGBOSCLB2WIN.DMN1.FMR.COM>

I would compare the Shannon entropy of your test vector with the entropy
of your expected probability distribution to see if they are close. That
is, if you're binary probability distribution is half 1 and half -1,
then if your string is long you would expect about half the numbers in
your vector to be 1 and half to be -1, i.e. H(s)=1. Moreover, you should
also look at the entropy of every subset of the vector and compare that
to your distribution as well. For instance, does the sequence (1, 1)
show up just as often as (1, -1), (-1, 1) and (-1, 1)? As this problem
is specific to a certain random process, I doubt there is a canned test
in R. 

Also, the sample entropy should converge to the distribution of the
underlying process as the sample size increases for all subsets of the
sample, probably following a t-distribution (Central Limit Theorem),
although I'd need to noodle on this a bit more. You can then construct a
test of significance if you know the sample size and how far the sample
entropy is from the hypothesized process's distribution. Unfortunately,
it's been a while since I've done information encoding, but hopefully
this gets you started.

You can read up on informational entropy here:
http://en.wikipedia.org/wiki/Informational_entropy

And if you do find a test in R, I would be interested as well.

Best,
Robert

-----Original Message-----
From: Mark Leeds [mailto:Mleeds at kellogggroup.com] 
Sent: Wednesday, January 11, 2006 4:46 PM
To: R-Stat Help
Subject: [R] a series of 1's and -1's

Does anyone know of a simple test
in any R package that given
a series of negative ones and positive
ones ( no other values are possible in the series )
returns a test of whether the series is random or not.
( a test at each point would be good but
I can use the apply function to implement
that ) ?
 
                                       thanks.
                                 


**********************************************************************
This email and any files transmitted with it are
confidentia...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Thu Jan 12 01:01:35 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 11 Jan 2006 19:01:35 -0500
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF196878@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196878@sbs2003.CMI.local>
Message-ID: <43C59C5F.1080906@optonline.net>

Michael Reinecke wrote:
>  
> Thanks again for your answer! I tried it out. write.foreign produces SPSS syntax, but unfortunally this syntax tells SPSS to take the names (and not the labels) in order to produce SPSS variable labels. The former labels get lost.
> 
> I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):
> 
> jjread<-  read.spss("test2.sav", use.value.labels=TRUE, to.data.frame=TRUE)
> 
>>str(jjread)
> 
> `data.frame':   30 obs. of  3 variables:
>  $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
>  $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
>  $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
>  - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
>   ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
> 
>>     datafile<-tempfile()
>>     codefile<-tempfile()
>>     write.foreign(jjread,datafile,codefile,package="SPSS")
>>     file.show(datafile)
>>     file.show(codefile)
> 
> 
> 
> The syntax file I get is:
> 
> DATA LIST FILE= "C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free
> / VAR00001 VAR00002 VAR00003  .
> 
> VARIABLE LABELS
> VAR00001 "VAR00001" 
>  VAR00002 "VAR00002" 
>  VAR00003 "VAR00003" 
>  .
> 
> EXECUTE.
> 
> 
> I am working on R 2.2.0. But I think a newer version won ??t fix it either, will it?

Here is a functiong based on modifying foreign:::writeForeignSPSS (by 
Thomas Lumley) which might work for you:

write.SPSS <- function (df, datafile, codefile, varnames = NULL)
{
adQuote <- function(x){paste("\"", x, "\"", sep = "")}
     dfn <- lapply(df, function(x) if (is.factor(x))
         as.numeric(x)
     else x)
     write.table(dfn, file = datafile, row = FALSE, col = FALSE)
     if(is.null(attributes(df)$variable.labels)) varlabels <- names(df) 
else varlabels <- attributes(df)$variable.labels
     if (is.null(varnames)) {
         varnames <- abbreviate(names(df), 8)
         if (any(sapply(varnames, nchar) > 8))
             stop("I cannot abbreviate the variable names to eight or 
fewer letters")
         if (any(varnames != names(df)))
             warning("some variable names were abbreviated")
     }
     cat("DATA LIST FILE=", dQuote(datafile), " free\n", file = codefile)
     cat("/", varnames, " .\n\n", file = codefile, append = TRUE)
     cat("VARIABLE LABELS\n", file = codefile, append = TRUE)
     cat(paste(varnames, adQuote(varlabels), "\n"), ".\n", file = codefile,
         append = TRUE)
     factors <- sapply(df, is.factor)
     if (any(factors)) {
         cat("\nVALUE LABELS\n", file = codefile, append = TRUE)
         for (v in which(factors)) {
             cat("/\n", file = codefile, append = TRUE)
             cat(varnames[v], " \n", file = codefile, append = TRUE)
             levs <- levels(df[[v]])
             cat(paste(1:length(levs), adQuote(levs), "\n", sep = " "),
                 file = codefile, append = TRUE)
         }
         cat(".\n", file = codefile, append = TRUE)
     }
     cat("\nEXECUTE.\n", file = codefile, append = TRUE)
}

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ihok at hotmail.com  Thu Jan 12 02:41:34 2006
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 11 Jan 2006 20:41:34 -0500
Subject: [R] data order affects glmmPQL
In-Reply-To: <43C47A41.2020500@pdf.com>
Message-ID: <BAY102-F1803DEF143B4DBDFFF2761CA270@phx.gbl>

>From: Spencer Graves 	  The correlation between the predictions from your 
>two model fits is 0.95.  This suggests to me that the differences between 
>the two sets of answers have little practical importance, and anyone who 
>disagrees may be trying to read more from the results than can actually be 
>supported by the data.  It should be fairly easy to select the apparent 
>"best" from among several such answers being the one that had a higher 
>log(likelihood).  This pushes me to prefer "fit.bar" with a log(likelihood) 
>of -32.31 to "fit.foo" with -33.05.
>
>	  I agree that the differences are somewhat disturbing, but you are 
>dealing with the output from an iterative solution of a notoriously 
>difficult problem, and the standard wisdom is that it is wise to try 
>several sets of starting values.  By modifying the order of the 
>observations in the data.frame, you have effectively done that.

Spencer, thank you for setting my mind at ease. Still, I suspect there's a 
bug here, as the convergence procedure halts entirely when I sort the data 
yet another way. See  
http://article.gmane.org/gmane.comp.lang.r.general/53559 .

Also, I wonder if it's appropriate to simply cherry-pick a model based on 
logLik, since there's no final test that of goodness of fit that happens on 
independent data after one has picked a model in this way.



From hhecwsc at hkucc.hku.hk  Thu Jan 12 02:57:54 2006
From: hhecwsc at hkucc.hku.hk (S.C. Wong)
Date: Thu, 12 Jan 2006 09:57:54 +0800
Subject: [R] Log-likelihood for Multinominal Probit Regression Model
In-Reply-To: <38b9f0350601102154n665ed6a9n@mail.gmail.com>
References: <6.2.1.2.2.20060111133012.056bfb80@hkucc.hku.hk>
	<38b9f0350601102154n665ed6a9n@mail.gmail.com>
Message-ID: <6.2.1.2.2.20060112095706.056e0a70@hkucc.hku.hk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/66ac4783/attachment.pl

From liuliang at stat.ohio-state.edu  Thu Jan 12 03:04:36 2006
From: liuliang at stat.ohio-state.edu (liuliang@stat.ohio-state.edu)
Date: Wed, 11 Jan 2006 21:04:36 -0500 (EST)
Subject: [R] question for mshapiro test
Message-ID: <1536.164.107.248.223.1137031476.squirrel@www.stat.ohio-state.edu>

Hi,
I have a question about the p-value of mshapiro test. I simulated data
from bivariate normal 1000 times and used mshapiro test to see how many
times the test would reject the null hypothesis when the p-value is 0.05.
The answer should be around 50 since the p-value is 0.05. But I got a much
higher value. Here is the R code I used and the result.

library(mvnormtest,lib.loc="~/mshapiro")
library(MASS)
n=50
dim=2

      ntrial = 1000
      x<-matrix(1:(dim*n),ncol=dim)
      count = 0

      for( trial in 1:ntrial)
           {
              x<-mvrnorm(n,rep(0,dim),diag(1,dim,dim))
              data_hn<-x
              p<-mshapiro.test(t(data_hn))$p.value
              if( p<= 0.05)     count <- count+1
           }
print(count)
[1] 117

Can you help me out? Thank you very much.

Liang



From spencer.graves at pdf.com  Thu Jan 12 04:03:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 11 Jan 2006 19:03:08 -0800
Subject: [R] glmmPQL "error" message (was 'data order affects glmmPQL')
In-Reply-To: <BAY102-F1803DEF143B4DBDFFF2761CA270@phx.gbl>
References: <BAY102-F1803DEF143B4DBDFFF2761CA270@phx.gbl>
Message-ID: <43C5C6EC.3090308@pdf.com>

	  1.  The function "glmmPQL" is in the MASS package, as can be seen by 
looking at the top line in the help file for "glmmPQL".  To find the 
maintainer, type 'help(package="MASS")'.  The results say, "Maintainer: 
Brian Ripley <ripley at stats.ox.ac.uk>".

	  2.  It is generally NOT "appropriate to simply cherry-pick a model 
based on logLik", as you suggested.  However, your example does NOT 
involve this issue, because you are making multiple attempts to fit the 
same model to the same data set.  With any iterative algorithm, it is 
considered legitimate to try fitting the same model with the same data 
with different starting values and select the one with the largest 
log(likelihood), considering that all others had not adequately 
converged.  In this case, the algorithm runs and produces similar but 
different answers when the order is changed.  Since the model does not 
seem to consider anything that would theoretically be affected by the 
sort order, it seems to me that this is crudely equivalent to changing 
the starting values, as I mentioned before.  Therefore, I would consider 
it quite legitimate to pick the fit with the highest logLik.

	  3.  I agree it is disturbing when glmmPQL generates "Error in 
lme.formula(fixed = zz ~ test + coder, random = ~1 | id, data =
list( :  false convergence (8)".  If it were my problem, I might make 
local compies of glmmPQL and lme.formula and trace through the code line 
by line using "debug" until I developed an idea about how I might change 
the code to get it past this error and on to something close to 
convergence.

	  Hope this helps.
	  spencer graves

Jack Tanner wrote:

>> From: Spencer Graves       The correlation between the predictions 
>> from your two model fits is 0.95.  This suggests to me that the 
>> differences between the two sets of answers have little practical 
>> importance, and anyone who disagrees may be trying to read more from 
>> the results than can actually be supported by the data.  It should be 
>> fairly easy to select the apparent "best" from among several such 
>> answers being the one that had a higher log(likelihood).  This pushes 
>> me to prefer "fit.bar" with a log(likelihood) of -32.31 to "fit.foo" 
>> with -33.05.
>>
>>       I agree that the differences are somewhat disturbing, but you 
>> are dealing with the output from an iterative solution of a 
>> notoriously difficult problem, and the standard wisdom is that it is 
>> wise to try several sets of starting values.  By modifying the order 
>> of the observations in the data.frame, you have effectively done that.
> 
> 
> Spencer, thank you for setting my mind at ease. Still, I suspect there's 
> a bug here, as the convergence procedure halts entirely when I sort the 
> data yet another way. See  
> http://article.gmane.org/gmane.comp.lang.r.general/53559 .
> 
> Also, I wonder if it's appropriate to simply cherry-pick a model based 
> on logLik, since there's no final test that of goodness of fit that 
> happens on independent data after one has picked a model in this way.
> 
>



From ng296 at cam.ac.uk  Thu Jan 12 04:48:26 2006
From: ng296 at cam.ac.uk (N. Goodacre)
Date: 12 Jan 2006 03:48:26 +0000
Subject: [R] Loading Excel file into Limma
Message-ID: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>

Dear mailing group,

  This is my first time here. Glad to have this resource!

  I am currently trying to load an Excel file into R (limma package loaded) 
using the source(*name of directory*) command, but it cannot open the file. 
I renamed the file as .R and .RData, to no avail. The Excel data contains 
one gene name per row and about 100 data points per gene (columns).

  I am only used to loading preprepared microarray data with all the t's 
crossed and i's dotted, with the read.maimages command. Can anyone help me 
out with this silly-sounding "challenge"?

  Sincerely - in the truest sense - 

Norman Goodacre



From ahimsa at camposarceiz.com  Thu Jan 12 05:13:29 2006
From: ahimsa at camposarceiz.com (ahimsa campos arceiz)
Date: Thu, 12 Jan 2006 13:13:29 +0900
Subject: [R] Loading Excel file into Limma
In-Reply-To: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
Message-ID: <6.0.1.1.0.20060112130428.0376bd90@pop.notfound.org>

well, I don't know anything about the limma package and I might be 
misunderstanding your apparently simple question

What I do for excel files is the following:
1. I save a copy of the file as .csv (comma separated values) in the 
working directory. This format allows you work perfectly with the file in 
excel.
2. open in R using:

 > read.csv("filename.csv")

Hope it helps



At 12:48 12/01/2006, you wrote:
>Dear mailing group,
>
>   This is my first time here. Glad to have this resource!
>
>   I am currently trying to load an Excel file into R (limma package loaded)
>using the source(*name of directory*) command, but it cannot open the file.
>I renamed the file as .R and .RData, to no avail. The Excel data contains
>one gene name per row and about 100 data points per gene (columns).
>
>   I am only used to loading preprepared microarray data with all the t's
>crossed and i's dotted, with the read.maimages command. Can anyone help me
>out with this silly-sounding "challenge"?
>
>   Sincerely - in the truest sense -
>
>Norman Goodacre
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Ahimsa Campos Arceiz
The University Museum,
The University of Tokyo
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033
phone +81-(0)3-5841-2824
cell +81-(0)80-5402-7702



From liuwensui at gmail.com  Thu Jan 12 05:25:42 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 11 Jan 2006 23:25:42 -0500
Subject: [R] Loading Excel file into Limma
In-Reply-To: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
Message-ID: <1115a2b00601112025w74fc07a4r31202de437a04448@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/fdfbed2b/attachment.pl

From jwd at surewest.net  Thu Jan 12 05:40:56 2006
From: jwd at surewest.net (J Dougherty)
Date: Wed, 11 Jan 2006 20:40:56 -0800
Subject: [R] Loading Excel file into Limma
In-Reply-To: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
Message-ID: <200601112040.56722.jwd@surewest.net>

R won't read an Excel sheet directly.  You need to export it, saving it as a 
CSV or tab delimited file.  You can then import using read.table.  The entire 
path and file have to be in double-quotes as well.  Try ?read.table for more 
info.

JWD

On Wednesday 11 January 2006 19:48, N. Goodacre wrote:
> Dear mailing group,
>
>   This is my first time here. Glad to have this resource!
>
>   I am currently trying to load an Excel file into R (limma package loaded)
> using the source(*name of directory*) command, but it cannot open the file.
> I renamed the file as .R and .RData, to no avail. The Excel data contains
> one gene name per row and about 100 data points per gene (columns).
>
>   I am only used to loading preprepared microarray data with all the t's
> crossed and i's dotted, with the read.maimages command. Can anyone help me
> out with this silly-sounding "challenge"?
>
>   Sincerely - in the truest sense -
>
> Norman Goodacre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ronggui.huang at gmail.com  Thu Jan 12 06:11:34 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 12 Jan 2006 13:11:34 +0800
Subject: [R] Loading Excel file into Limma
In-Reply-To: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.16.0601120348260.28343@hermes-2.csi.cam.ac.uk>
Message-ID: <38b9f0350601112111r3d6304bax@mail.gmail.com>

another options: use read.xls in gdata pcakges if you have installed
perl in you machine .

12 Jan 2006 03:48:26 +0000, N. Goodacre <ng296 at cam.ac.uk>:
> Dear mailing group,
>
>   This is my first time here. Glad to have this resource!
>
>   I am currently trying to load an Excel file into R (limma package loaded)
> using the source(*name of directory*) command, but it cannot open the file.
> I renamed the file as .R and .RData, to no avail. The Excel data contains
> one gene name per row and about 100 data points per gene (columns).
>
>   I am only used to loading preprepared microarray data with all the t's
> crossed and i's dotted, with the read.maimages command. Can anyone help me
> out with this silly-sounding "challenge"?
>
>   Sincerely - in the truest sense -
>
> Norman Goodacre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From jacques.veslot at cirad.fr  Thu Jan 12 07:05:26 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 12 Jan 2006 10:05:26 +0400
Subject: [R] matrix logic
In-Reply-To: <43C52EC6.7090904@statistik.uni-dortmund.de>
References: <20060111155805.18427.qmail@web35014.mail.mud.yahoo.com>
	<43C52EC6.7090904@statistik.uni-dortmund.de>
Message-ID: <43C5F1A6.5000505@cirad.fr>

I don't know how to keep factors' levels with :

data.frame(mapply(function(x,y,z) ifelse(is.na(y), z, y),
             names(D), D, D2, SIMPLIFY=FALSE))

but in that way it's ok :

data.frame(mapply(function(z,x,y) { y[is.na(y)] <- x[is.na(y)] ; y },
    names(D), D, D2, SIMPLIFY=F))


(?)


Uwe Ligges a crit :

> t c wrote:
>
>> Uwe,
>>   FYI:
>>      I tried: "data3 <- ifelse(is.na(data1), data2, data1)"
>>      It seems to me that data3 is an array of length 100.
>>      I do NOT end up with a dataset of 5 columns and 20 rows.
>
>
> I have not read carefully enough, for a data.frame you can generalize 
> the approach as follows:
>
>   data.frame(mapply(function(x,y,z) ifelse(is.na(y), z, y),
>              names(D), D, D2, SIMPLIFY=FALSE))
>
> Uwe Ligges
>
>
>
>> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>   Tom wrote:
>>
>>> On Tue, 10 Jan 2006 20:25:23 -0500, r user wrote:
>>>
>>>
>>>
>>>> I have 2 dataframes, each with 5 columns and 20 rows.
>>>> They are called data1 and data2.I wish to create a
>>>> third dataframe called data3, also with 5 columns and
>>>> 20 rows.
>>>>
>>>> I want data3 to contains the values in data1 when the
>>>> value in data1 is not NA. Otherwise it should contain
>>>> the values in data2.
>>>>
>>>> I have tried afew methids, but they do not seem to
>>>> work as intended.:
>>>>
>>>> data3<-ifelse(is.na(data1)=F,data1,data2)
>>>>
>>>> and
>>>>
>>>> data3[,]<-ifelse(is.na(data1[,])=F,data1[,],data2[,])
>>>>
>>>> Please suggest the best way.
>>>
>>
>>
>> "Better" way is to have the Syntax correct:
>>
>> data3 <- ifelse(is.na(data1), data2, data1)
>>
>>
>> Please check the archives for almost millions of posts asking more or 
>> less this question...!
>>
>>
>>
>>> Not sure about the bast but...
>>>
>>> a<-c(1,2,3,NA,5)
>>> b<-c(4,4,4,4,4)
>>>
>>> c<-a
>>> c[which(is.na(a))]<-b[which(is.na(a))]
>>
>>
>>
>> Why do you want to know which()?
>>
>> na <- is.na(a)
>> c[na] <- b[na]
>>
>>
>> Uwe Ligges
>>
>>
>>
>>>
>>>
>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html 
>>
>>        
>> ---------------------------------
>> Yahoo! Photos  Showcase holiday pictures in hardcover
>>  Photo Books. You design it and well bind it!
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From parrinel at med.unibs.it  Wed Jan 11 22:38:11 2006
From: parrinel at med.unibs.it (giovanni parrinello)
Date: Wed, 11 Jan 2006 22:38:11 +0100
Subject: [R] Strange behaviour of load
Message-ID: <000101c6174a$feea6230$2b18a7c0@pc43>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060111/18927221/attachment.pl

From Roger.Bivand at nhh.no  Thu Jan 12 08:42:25 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 12 Jan 2006 08:42:25 +0100 (CET)
Subject: [R] a series of 1's and -1's
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A7646675F@exchange.psg.com>
Message-ID: <Pine.LNX.4.44.0601120839150.31307-100000@reclus.nhh.no>

On Wed, 11 Jan 2006, Mark Leeds wrote:

> Does anyone know of a simple test
> in any R package that given
> a series of negative ones and positive
> ones ( no other values are possible in the series )
> returns a test of whether the series is random or not.
> ( a test at each point would be good but
> I can use the apply function to implement
> that ) ?

help.search("runs") points to function runs.test() in package tseries, 
with examples:

x <- factor(sign(rnorm(100))) # randomness
runs.test(x)
x <- factor(rep(c(-1, 1), 50)) # over-mixing
runs.test(x)

which looks like your case

>  
>                                        thanks.
>                                  
> 
> 
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From christoph.lehmann at gmx.ch  Thu Jan 12 09:02:17 2006
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Thu, 12 Jan 2006 09:02:17 +0100
Subject: [R] (no subject)
In-Reply-To: <1136906859.43c3d26c003cc@webmail.uni-potsdam.de>
References: <1136906859.43c3d26c003cc@webmail.uni-potsdam.de>
Message-ID: <43C60D09.3050601@gmx.ch>

type ?par and then have a look at:
cex.lab, cex.main

cheers
christoph
paladini at rz.uni-potsdam.de wrote:
> Dear ladies and gentlemen!
> When I use the plot funtion how can I change the size of the title for the x and
> y axes (xlab, ylab)and the size of the axes label ?
> 
> Thank you very much.
> 
> With best regards
> 
> Claudia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Thu Jan 12 09:21:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jan 2006 08:21:35 +0000 (GMT)
Subject: [R] Strange behaviour of load
In-Reply-To: <000101c6174a$feea6230$2b18a7c0@pc43>
References: <000101c6174a$feea6230$2b18a7c0@pc43>
Message-ID: <Pine.LNX.4.61.0601120817290.13271@gannet.stats>

On Wed, 11 Jan 2006, giovanni parrinello wrote:

> Dear All,
> simetimes when I load an Rdata I get this message
>
> #######
> Code:
>
> load('bladder1.RData')
> Carico il pacchetto richiesto: rpart ( Bad traslastion: Load required package-...)
> Carico il pacchetto richiesto: MASS
> Carico il pacchetto richiesto: mlbench
> Carico il pacchetto richiesto: survival
> Carico il pacchetto richiesto: splines
>
> Carico il pacchetto richiesto: 'survival'
>
>
>        The following object(s) are masked from package:Hmisc :
>
>         untangle.specials
>
> Carico il pacchetto richiesto: class
> Carico il pacchetto richiesto: nnet
> #########
>
> So  I have many unrequired packages loaded.
> Any idea?

They are required!  My guess is that you have object(s) saved with 
environment the namespace of some package, and loading that namespace is 
pulling these in.  The only CRAN package which requires mlbench appears to 
be ipred, and that requires all of those except splines, required by 
survival.

So I believe you have been using ipred and have saved a reference to its 
namespace.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From erich.neuwirth at univie.ac.at  Wed Jan 11 16:02:10 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Wed, 11 Jan 2006 16:02:10 +0100
Subject: [R] dataframes with only one variable
Message-ID: <43C51DF2.4020907@univie.ac.at>

Subsetting from a dataframe with only one variable
returns a vector, not a dataframe.
This seems somewhat inconsistent.
Wouldn't it be better if subsetting would respect
the structure completely?


v1<-1:4
v2<-4:1
df1<-data.frame(v1)
df2<-data.frame(v1,v2)
sel1<-c(TRUE,TRUE,TRUE,TRUE)

> df1[sel1,]
[1] 1 2 3 4
> df2[sel1,]
  v1 v2
1  1  4
2  2  3
3  3  2
4  4  1

-- 
Erich Neuwirth
Institute for Scientific Computing and
Didactic Center for Computer Science
University of Vienna
phone: +43-1-4277-39464  fax: +43-1-4277-39459



From Matthias.Templ at statistik.gv.at  Thu Jan 12 09:35:02 2006
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 12 Jan 2006 09:35:02 +0100
Subject: [R] dataframes with only one variable
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BADC6@xchg1.statistik.local>

> Subsetting from a dataframe with only one variable
> returns a vector, not a dataframe.
> This seems somewhat inconsistent.
> Wouldn't it be better if subsetting would respect
> the structure completely?
> 
> 
> v1<-1:4
> v2<-4:1
> df1<-data.frame(v1)
> df2<-data.frame(v1,v2)
> sel1<-c(TRUE,TRUE,TRUE,TRUE)
> 
> > df1[sel1,]


df1[[sel1, , drop=FALSE]

Should do what you want.

Best,
Matthias

> [1] 1 2 3 4
> > df2[sel1,]
>   v1 v2
> 1  1  4
> 2  2  3
> 3  3  2
> 4  4  1
> 
> -- 
> Erich Neuwirth
> Institute for Scientific Computing and
> Didactic Center for Computer Science
> University of Vienna
> phone: +43-1-4277-39464  fax: +43-1-4277-39459
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From buser at stat.math.ethz.ch  Thu Jan 12 10:07:28 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 12 Jan 2006 10:07:28 +0100
Subject: [R] F-test degree of freedoms in lme4 ?
In-Reply-To: <1136998412.613028@yorikke.arb-phys.uni-dortmund.de>
References: <1136998412.613028@yorikke.arb-phys.uni-dortmund.de>
Message-ID: <17350.7248.634002.674079@stat.math.ethz.ch>

Dear Wilhelm

There is an article, including a part about fitting linear mixed
models. There the problem with the degrees of freedom is
described.
You can have a look to the second link, too, discussing the
problem as well.

http://cran.r-project.org/doc/Rnews/Rnews_2005-1.pdf
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67414.html

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Wilhelm B. Kloke writes:
 > I have a problem moving from multistratum aov analysis to lmer.
 > 
 > My dataset has observations of ampl at 4 levels of gapf and 2 levels of bl
 > on 6 subjects levels VP, with 2 replicates wg each, and is balanced.
 > 
 > Here is the summary of this set with aov:
 > >> summary(aov(ampl~gapf*bl+Error(VP/(bl*gapf)),hframe2))
 > >
 > >Error: VP
 > >          Df Sum Sq Mean Sq F value Pr(>F)
 > >Residuals  5    531     106               
 > >
 > >Error: VP:bl
 > >          Df Sum Sq Mean Sq F value Pr(>F)   
 > >bl         1   1700    1700    37.8 0.0017 **
 > >Residuals  5    225      45                  
 > >---
 > >Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
 > >
 > >Error: VP:gapf
 > >          Df Sum Sq Mean Sq F value  Pr(>F)    
 > >gapf       3    933     311    24.2 5.3e-06 ***
 > >Residuals 15    193      13                    
 > >---
 > >Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
 > >
 > >Error: VP:bl:gapf
 > >          Df Sum Sq Mean Sq F value Pr(>F)  
 > >gapf:bl    3   93.9    31.3    3.68  0.036 *
 > >Residuals 15  127.6     8.5                 
 > >---
 > >Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
 > >
 > >Error: Within
 > >          Df Sum Sq Mean Sq F value Pr(>F)
 > >Residuals 48    318       7               
 > >
 > This is mostly identical the analysis by BMDP 4V, except for the
 > Greenhouse-Geisser epsilons, which are not estimated this way.
 > 
 > I have to analyse a similar dataset, which is not balanced. So I need to
 > change the method. Following Pinheiro/Bates p.90f, I tried
 > > hf2.lme <- lme(ampl~gapf*bl,hframe2,random=list(VP=pdDiag(~gapf*bl),bl=pdDiag(~gapf)))
 > and some variations of this to get the same F tests generated. At least,
 > I got the F-test on error stratum VP:bl this way, but not the other two:
 > >> anova(hf2.lme)
 > >            numDF denDF F-value p-value
 > >(Intercept)     1    78  764.86  <.0001
 > >gapf            3    78   17.68  <.0001
 > >bl              1     5   37.81  0.0017
 > >gapf:bl         3    78    2.99  0.0362
 > 
 > Then I tried to move to lmer.
 > I tried to find something equivalent to the above lme call, with no
 > success at all.
 > 
 > In case, that the problem is in the data, here is the set:
 > 
 > VP ampl wg bl gapf
 > 1 WJ 22 w s 144
 > 2 CR 23 w s 144
 > 3 MZ 25 w s 144
 > 4 MP 34 w s 144
 > 5 HJ 36 w s 144
 > 6 SJ 26 w s 144
 > 7 WJ 34 w s 80
 > 8 CR 31 w s 80
 > 9 MZ 33 w s 80
 > 10 MP 36 w s 80
 > 11 HJ 37 w s 80
 > 12 SJ 32 w s 80
 > 13 WJ 34 w s 48
 > 14 CR 37 w s 48
 > 15 MZ 38 w s 48
 > 16 MP 38 w s 48
 > 17 HJ 40 w s 48
 > 18 SJ 32 w s 48
 > 19 WJ 36 w s 16
 > 20 CR 40 w s 16
 > 21 MZ 39 w s 16
 > 22 MP 40 w s 16
 > 23 HJ 40 w s 16
 > 24 SJ 38 w s 16
 > 25 WJ 16 g s 144
 > 26 CR 28 g s 144
 > 27 MZ 18 g s 144
 > 28 MP 33 g s 144
 > 29 HJ 37 g s 144
 > 30 SJ 28 g s 144
 > 31 WJ 28 g s 80
 > 32 CR 33 g s 80
 > 33 MZ 24 g s 80
 > 34 MP 34 g s 80
 > 35 HJ 36 g s 80
 > 36 SJ 30 g s 80
 > 37 WJ 32 g s 48
 > 38 CR 38 g s 48
 > 39 MZ 34 g s 48
 > 40 MP 37 g s 48
 > 41 HJ 39 g s 48
 > 42 SJ 30 g s 48
 > 43 WJ 36 g s 16
 > 44 CR 34 g s 16
 > 45 MZ 36 g s 16
 > 46 MP 40 g s 16
 > 47 HJ 40 g s 16
 > 48 SJ 36 g s 16
 > 49 WJ 22 w b 144
 > 50 CR 24 w b 144
 > 51 MZ 20 w b 144
 > 52 MP 26 w b 144
 > 53 HJ 22 w b 144
 > 54 SJ 16 w b 144
 > 55 WJ 26 w b 80
 > 56 CR 24 w b 80
 > 57 MZ 26 w b 80
 > 58 MP 27 w b 80
 > 59 HJ 26 w b 80
 > 60 SJ 18 w b 80
 > 61 WJ 28 w b 48
 > 62 CR 23 w b 48
 > 63 MZ 28 w b 48
 > 64 MP 29 w b 48
 > 65 HJ 27 w b 48
 > 66 SJ 24 w b 48
 > 67 WJ 32 w b 16
 > 68 CR 26 w b 16
 > 69 MZ 30 w b 16
 > 70 MP 28 w b 16
 > 71 HJ 30 w b 16
 > 72 SJ 22 w b 16
 > 73 WJ 22 g b 144
 > 74 CR 18 g b 144
 > 75 MZ 18 g b 144
 > 76 MP 26 g b 144
 > 77 HJ 22 g b 144
 > 78 SJ 18 g b 144
 > 79 WJ 24 g b 80
 > 80 CR 26 g b 80
 > 81 MZ 30 g b 80
 > 82 MP 26 g b 80
 > 83 HJ 26 g b 80
 > 84 SJ 24 g b 80
 > 85 WJ 28 g b 48
 > 86 CR 28 g b 48
 > 87 MZ 27 g b 48
 > 88 MP 30 g b 48
 > 89 HJ 26 g b 48
 > 90 SJ 16 g b 48
 > 91 WJ 28 g b 16
 > 92 CR 19 g b 16
 > 93 MZ 24 g b 16
 > 94 MP 32 g b 16
 > 95 HJ 30 g b 16
 > 96 SJ 22 g b 16
 > -- 
 > Dipl.-Math. Wilhelm Bernhard Kloke
 > Institut fuer Arbeitsphysiologie an der Universitaet Dortmund
 > Ardeystrasse 67, D-44139 Dortmund, Tel. 0231-1084-257
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jan 12 10:59:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jan 2006 09:59:11 +0000 (GMT)
Subject: [R] dataframes with only one variable
In-Reply-To: <43C51DF2.4020907@univie.ac.at>
References: <43C51DF2.4020907@univie.ac.at>
Message-ID: <Pine.LNX.4.61.0601120953300.14716@gannet.stats>

On Wed, 11 Jan 2006, Erich Neuwirth wrote:

> Subsetting from a dataframe with only one variable
> returns a vector, not a dataframe.
> This seems somewhat inconsistent.

Not at all.  It is entirely consistent with matrix-like indexing (the form 
you used).

> Wouldn't it be better if subsetting would respect
> the structure completely?

It depends how you do it.  [sel1,]  parallels a matrix, and drops 
dimensions unless drop == FALSE is supplied.  [sel1] returns a 
one-column df, and [[sel1]] returns a vector.

It is just a question of choosing the appropriate tool.  And any changes 
to this sort of thing (from the White book) would break a lot of careful 
code.

>
>
> v1<-1:4
> v2<-4:1
> df1<-data.frame(v1)
> df2<-data.frame(v1,v2)
> sel1<-c(TRUE,TRUE,TRUE,TRUE)
>
>> df1[sel1,]
> [1] 1 2 3 4
>> df2[sel1,]
>  v1 v2
> 1  1  4
> 2  2  3
> 3  3  2
> 4  4  1
>
> -- 
> Erich Neuwirth
> Institute for Scientific Computing and
> Didactic Center for Computer Science
> University of Vienna
> phone: +43-1-4277-39464  fax: +43-1-4277-39459

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From goran.brostrom at gmail.com  Thu Jan 12 11:19:28 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Thu, 12 Jan 2006 11:19:28 +0100
Subject: [R] Indentation in emacs
Message-ID: <148ed8180601120219xc554f16q5ab4dedbe3f55f34@mail.gmail.com>

I'm using emacs-21.4 on debian unstable, together with the latest ESS
implementation. I try to change indentation to 4 by following the
advise in "R-exts": It results in the following lines in my .emacs
file:

 (custom-set-variables
  ;; custom-set-variables was added by Custom -- don't edit or cut/paste it!
  ;; Your init file should contain only one such instance.
 '(c-basic-offset 4)
 '(c-default-style "bsd")
 '(latin1-display t nil (latin1-disp)))
(custom-set-faces
  ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
  ;; Your init file should contain only one such instance.
 )

But it doesn't work with R code  (with C code it works). So what is missing?
--
Gran Brostrm



From dargosch at gmail.com  Thu Jan 12 11:24:11 2006
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Thu, 12 Jan 2006 11:24:11 +0100
Subject: [R] Repeated measures aov with post hoc tests?
In-Reply-To: <376e97ec0601100640h4cd3f01am60d2da17d914589a@mail.gmail.com>
References: <376e97ec0601100640h4cd3f01am60d2da17d914589a@mail.gmail.com>
Message-ID: <376e97ec0601120224n4af7e8a8w294b5803c18c078f@mail.gmail.com>

Dear list,

I posted the message below a cople of days ago, and have not been able
to find any solution to this. I do really want some help.

/Fredrik

On 1/10/06, Fredrik Karlsson <dargosch at gmail.com> wrote:
> Dear list,
>
> I would like to perform an analysis on the following model:
>
> aov(ampratio ~ Type * Place * agemF + Error(speakerid/Place) ,data=aspvotwork)
>
> using the approach from http://www.psych.upenn.edu/~baron/rpsych/rpsych.html .
>
> Now, I got the test results, wich indicate a significant interaction
> and main effects of the agemF variable. How do I find at what level of
> agemF the effect may be found.
>
> How do I do this?
>
> I found a reference to TukeyHSD in the archives, but I cannot use it:
>
> > TukeyHSD(aov(ampratio ~ Type * Place * agemF + Error(speakerid/Place),data=aspvotwork))
> Error in TukeyHSD(aov(ampratio ~ Type * Place * agemF +
> Error(speakerid/Place),  :
>         no applicable method for "TukeyHSD"
>
> Please help me.
>
> /Fredrik
>


--
My Gentoo + PVR-350 + IVTV + MythTV blog is on
http://gentoomythtv.blogspot.com/



From I.Visser at uva.nl  Thu Jan 12 11:32:05 2006
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 12 Jan 2006 11:32:05 +0100
Subject: [R] a series of 1's and -1's
In-Reply-To: <Pine.LNX.4.44.0601120839150.31307-100000@reclus.nhh.no>
Message-ID: <BFEBEEB5.C1B9%I.Visser@uva.nl>

You could try to zip your data file and see whether there is a change in
file size that you feel is significant in which case the series is not
random (-:
To be sure, there is no such thing as a positive test for randomness, only
tests for specific deviations of randomness of which the runs.test and the
entropy are just two examples. Zip programs use a whole bunch of these tests
to compress files by finding structure in the data.
best, ingmar

> From: Roger Bivand <Roger.Bivand at nhh.no>
> Reply-To: Roger.Bivand at nhh.no
> Date: Thu, 12 Jan 2006 08:42:25 +0100 (CET)
> To: Mark Leeds <Mleeds at kellogggroup.com>
> Cc: R-Stat Help <R-help at stat.math.ethz.ch>
> Subject: Re: [R] a series of 1's and -1's
> 
> On Wed, 11 Jan 2006, Mark Leeds wrote:
> 
>> Does anyone know of a simple test
>> in any R package that given
>> a series of negative ones and positive
>> ones ( no other values are possible in the series )
>> returns a test of whether the series is random or not.
>> ( a test at each point would be good but
>> I can use the apply function to implement
>> that ) ?
> 
> help.search("runs") points to function runs.test() in package tseries,
> with examples:
> 
> x <- factor(sign(rnorm(100))) # randomness
> runs.test(x)
> x <- factor(rep(c(-1, 1), 50)) # over-mixing
> runs.test(x)
> 
> which looks like your case
> 
>>  
>>                                        thanks.
>>                 
>> 
>> 
>> **********************************************************************
>> This email and any files transmitted with it are confidentia...{{dropped}}
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Jesus.Frias at dit.ie  Thu Jan 12 11:32:53 2006
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Thu, 12 Jan 2006 10:32:53 +0000
Subject: [R] Equal number of cuts in a contourplot with trellis
Message-ID: <000201c61763$8eb18e80$44b2fc93@ditbus3zletw5e>

Dear R-helpers,

	I need some help to produce a set of contour plots that I am
trying to make in order to compare surfaces between the levels of a
factor. For example: 

library(lattice)
g <- expand.grid(x = 60:100, y = 1:25, ti = c("a","b","c"))
g$z <-with(g,
  (-1e-4*x-1e-3*y-1e-5*x*y)*(ti=="a") +
  (1e-2*x-1e-3*y-1e-4*x*y)*(ti=="b") +
  (1e-3*x-1e-3*y-1.e-5*x*y)*(ti=="c")
               )

contourplot(z~ x * y|ti, data = g,
            cuts=20,
            pretty=T,
            screen = list(z = 30, x = -60))

As you can see in the figure, most of the contour lines are in one of
the levels and we are not able to see how the other levels look like.

I would like to display the same number of cuts in each of the trellis.
I can make each of the contourplots separately and control the number of
cuts but I am not able to plot all of them in one. 

Thanks in advance,

Jesus

Jes??s Mar??a Fr??as Celayeta
School of Food Science and Environmental Health
Dublin Institute of Technology
Cathal Brugha St. Dublin 1
p + 353 1 4024459
f + 353 1 4024495
w http://www.dit.ie/DIT/tourismfood/science/



-- 
This message has been scanned for content and 
viruses by the DIT Information Services MailScanner 
Service, and is believed to be clean.
http://www.dit.ie



From voodooochild at gmx.de  Thu Jan 12 11:56:32 2006
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Thu, 12 Jan 2006 11:56:32 +0100
Subject: [R] Software Reliability Using Cox Proportional Hazard
Message-ID: <43C635E0.90805@gmx.de>

Hello,

i want to use coxph() for software reliability analysis, i use the 
following implementation

#######################################################################################
failure<-read.table("failure.dat", header=T)
attach(failure)

f<-FailureNumber
t<-TimeBetweenFailure

# filling vector f with ones

for(i in 1:length(f)) {
  f[i]<-1
}

library(survival)

# cox proportional hazard with covariable LOC

cox.res<-coxph(Surv(t,f)~LOC,data=failure)
plot(survfit(cox.res))
##########################################################################################


failure.dat is
FailureNumber TimeBetweenFailure    LOC
1    7        120
2    11        135
3    8        141
4    10        150
5    15        162
6    22        169
7    20        170
8    25        181
9    28        188
10    35        193

here TimeBetweenFailure gives the time between following failures and 
LOC is the actuall Lines of Code.
What i do, is filling the vector f with ones. f is then in Surv the 
censoring variable. in survival analysis the censoring variable is 1 if 
the patient dies, now i interpret the event death as
the appearence of a failure. And i use LOC as covariable.

My question now is, is this approach right or is there any big mistake 
or wrong interpretation?


Thanks a Lot!
best regards
Andreas



From johannes at huesing.name  Thu Jan 12 11:56:52 2006
From: johannes at huesing.name (Johannes =?iso-8859-1?Q?H=FCsing?=)
Date: Thu, 12 Jan 2006 11:56:52 +0100 (CET)
Subject: [R] a series of 1's and -1's
In-Reply-To: <BFEBEEB5.C1B9%I.Visser@uva.nl>
References: <Pine.LNX.4.44.0601120839150.31307-100000@reclus.nhh.no>
	<BFEBEEB5.C1B9%I.Visser@uva.nl>
Message-ID: <16142.129.206.90.2.1137063412.squirrel@mail.panix.com>

> You could try to zip your data file and see whether there is a change in
> file size that you feel is significant in which case the series is not
> random (-:

... after converting the -1s and 1s to bits, of course.



From Keith.Chamberlain at colorado.edu  Thu Jan 12 12:33:01 2006
From: Keith.Chamberlain at colorado.edu (Keith Chamberlain)
Date: Thu, 12 Jan 2006 04:33:01 -0700
Subject: [R]  Obtaining the adjusted r-square given the regression
Message-ID: <004e01c6176b$f2846420$742b8a80@komelandpc>

Hi people,
 I want to obtain the adjusted r-square given a set of coefficients (without
the intercept), and I don't know if there is a function that does it.
Exist????????????????

Dear Alexandra,

Without knowing what routine you were using that returned an R-Square value
too you, it is a little difficult to tell. I am not able to comment on your
routine, but I'll try to give a satisfactory response to your first
question.

Assuming that you used lm() for your linear regression, then all you need to
do is see the help page for the lm() summary method, which explains how to
get at the values in the summary. In your console type the following:

>?summary.lm

That help page describes how to get at the values in the summary, and
provides an example with getting the full correlation value (whereas the one
printed is in a 'pretty' form). I assume the other ones can be accessed
similarly, including the adjusted R^2. Your code might look something like
the following:

# Where object is the result of a call to lm()
>summary(object)$adj.r.squared

The help also has a demonstration for how to exclude the intercept, where
you simply add a -1 term too the end of your formula. 

There may still be an accessor function available, which I do not know about
(BUT am certainly interested in if anyone has feedback). 

In the future, it would help people on the list decode your function when it
too is printed in a readable form. The layout of your function in your
message may have become garbled when it was stripped of all of its HTML
information. If the code was formatted in a readable way when you sent the
message, then that's what happened. If that is the case try composing your
message in plain text to begin with. 

I hope this helps.

Rgds,
KeithC.



From BPikouni at CNTUS.JNJ.COM  Thu Jan 12 13:38:18 2006
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 12 Jan 2006 07:38:18 -0500
Subject: [R] Space between axis label and tick labels
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB04C9EBB7@CNTUSMAEXS4.na.jnj.com>

Hello Kare:
I think some global graphics settings can get you what you need, assuming
you are using base graphics functions such as plot(). See ?par for
comprehensive details. For the specific characteristics you mention, I have
found the arguments in par for:

> 1) font size of axis and tick labels
> 
> 2) font thickness
> 

cex.axis & cex.lab are helpful; and perhaps the font.* series

> 3) placement of both axis and tick labels

mgp & tck are helpful.

Hope that helps,
Bill

-------------------------------
Bill Pikounis, PhD
Nonclinical Statistics
Centocor, Inc.
Malvern, PA 19355 (USA)


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kare Edvardsen
> Sent: Wednesday, January 11, 2006 4:52 AM
> To: R-help
> Subject: [R] Space between axis label and tick labels
> 
> 
> I'm writing an publication in two column format and need to 
> shrink some 
> plots. After increasing the axis labels it does not look nice at all. 
> The y-axis label and tick labels almost touch each other and 
> the x-axis 
> tick labels expand into the plot instead of away from it. Is there a 
> better way than "cex" to control the:
> 
> 1) font size of axis and tick labels
> 
> 2) font thickness
> 
> 3) placement of both axis and yick labels
> 
> 
> Cheers,
> 
> Kare
> 
> -- 
> ###########################################
> Kare Edvardsen <kare.edvardsen at nilu.no>
> Norwegian Institute for Air Research (NILU)
> Polarmiljosenteret
> NO-9296 Tromso       http://www.nilu.no
> Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
> Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
> ###########################################
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From kilian.plank at m-lehrstuhl.de  Thu Jan 12 14:20:56 2006
From: kilian.plank at m-lehrstuhl.de (Kilian Plank)
Date: Thu, 12 Jan 2006 14:20:56 +0100
Subject: [R] Problem with NLSYSTEMFIT()
Message-ID: <F4C605A5EE93EA418CC0C5747357D4344980AD@letterman.intranet.m-lehrstuhl.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/73ae3649/attachment.pl

From sara at gmesintra.com  Thu Jan 12 14:20:02 2006
From: sara at gmesintra.com (Sara Mouro)
Date: Thu, 12 Jan 2006 13:20:02 -0000
Subject: [R] envelopes of simulations
Message-ID: <200601121320.k0CDK9Pj019684@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/b0e9bf0b/attachment.pl

From Reinecke at consultic.com  Thu Jan 12 14:27:36 2006
From: Reinecke at consultic.com (Michael Reinecke)
Date: Thu, 12 Jan 2006 14:27:36 +0100
Subject: [R] SPSS and R ? do they like each other?
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF19689A@sbs2003.CMI.local>

Thank you very much! write.SPSS works fine.

I just wonder, why this very useful function is not part of any package. I have not even found it in the web. For experts it may not be a big deal to write their own export functions, but for newcomers like me it is almost impossible - and at the same time it is essential to us to have good facilities for exchange with our familiar statistical package. I think a lack of exchange tools might be something that scares many people off and keeps them from getting to know R.

Well, just do give a greenhorn ??s perspective.

Best regards,

Michael


-----Urspr??ngliche Nachricht-----
Von: Chuck Cleland [mailto:ccleland at optonline.net] 
Gesendet: Donnerstag, 12. Januar 2006 01:16
An: Michael Reinecke
Cc: R-help at stat.math.ethz.ch
Betreff: Re: [R] SPSS and R ? do they like each other?

Michael Reinecke wrote:
>  
> Thanks again for your answer! I tried it out. write.foreign produces SPSS syntax, but unfortunally this syntax tells SPSS to take the names (and not the labels) in order to produce SPSS variable labels. The former labels get lost.
> 
> I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):
> 
> jjread<-  read.spss("test2.sav", use.value.labels=TRUE, 
> to.data.frame=TRUE)
> 
>>str(jjread)
> 
> `data.frame':   30 obs. of  3 variables:
>  $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
>  $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
>  $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
>  - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
>   ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
> 
>>     datafile<-tempfile()
>>     codefile<-tempfile()
>>     write.foreign(jjread,datafile,codefile,package="SPSS")
>>     file.show(datafile)
>>     file.show(codefile)
> 
> 
> 
> The syntax file I get is:
> 
> DATA LIST FILE= 
> "C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free / VAR00001 VAR00002 VAR00003  .
> 
> VARIABLE LABELS
> VAR00001 "VAR00001" 
>  VAR00002 "VAR00002" 
>  VAR00003 "VAR00003" 
>  .
> 
> EXECUTE.
> 
> 
> I am working on R 2.2.0. But I think a newer version won ??t fix it either, will it?

Here is a functiong based on modifying foreign:::writeForeignSPSS (by Thomas Lumley) which might work for you:

write.SPSS <- function (df, datafile, codefile, varnames = NULL) { adQuote <- function(x){paste("\"", x, "\"", sep = "")}
     dfn <- lapply(df, function(x) if (is.factor(x))
         as.numeric(x)
     else x)
     write.table(dfn, file = datafile, row = FALSE, col = FALSE)
     if(is.null(attributes(df)$variable.labels)) varlabels <- names(df) else varlabels <- attributes(df)$variable.labels
     if (is.null(varnames)) {
         varnames <- abbreviate(names(df), 8)
         if (any(sapply(varnames, nchar) > 8))
             stop("I cannot abbreviate the variable names to eight or fewer letters")
         if (any(varnames != names(df)))
             warning("some variable names were abbreviated")
     }
     cat("DATA LIST FILE=", dQuote(datafile), " free\n", file = codefile)
     cat("/", varnames, " .\n\n", file = codefile, append = TRUE)
     cat("VARIABLE LABELS\n", file = codefile, append = TRUE)
     cat(paste(varnames, adQuote(varlabels), "\n"), ".\n", file = codefile,
         append = TRUE)
     factors <- sapply(df, is.factor)
     if (any(factors)) {
         cat("\nVALUE LABELS\n", file = codefile, append = TRUE)
         for (v in which(factors)) {
             cat("/\n", file = codefile, append = TRUE)
             cat(varnames[v], " \n", file = codefile, append = TRUE)
             levs <- levels(df[[v]])
             cat(paste(1:length(levs), adQuote(levs), "\n", sep = " "),
                 file = codefile, append = TRUE)
         }
         cat(".\n", file = codefile, append = TRUE)
     }
     cat("\nEXECUTE.\n", file = codefile, append = TRUE) }

--
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From schaber at molgen.mpg.de  Thu Jan 12 14:49:45 2006
From: schaber at molgen.mpg.de (=?ISO-8859-1?Q?J=F6rg_Schaber?=)
Date: Thu, 12 Jan 2006 14:49:45 +0100
Subject: [R] extract variables from linear model
Message-ID: <43C65E79.3010201@molgen.mpg.de>

Hi,

I fitted a linear model:
fit <- lm(y ~ a * b + c - 1 , na.action='na.omit')

Now I want to extract only the a * b effects with confidence intervals. 
Of course, I can just add the coefficients by hand, but I think there 
should an easier way.
I tried with predict.lm using the 'terms' argument, but I didn't manage 
to do it.
Any hints are appreciated,

best,

joerg



From ccleland at optonline.net  Thu Jan 12 14:45:42 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 12 Jan 2006 08:45:42 -0500
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF19689A@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF19689A@sbs2003.CMI.local>
Message-ID: <43C65D86.3060800@optonline.net>

Michael Reinecke wrote:
> Thank you very much! write.SPSS works fine.
> 
> I just wonder, why this very useful function is not part of any package. I have not even found it in the web. For experts it may not be a big deal to write their own export functions, but for newcomers like me it is almost impossible - and at the same time it is essential to us to have good facilities for exchange with our familiar statistical package. I think a lack of exchange tools might be something that scares many people off and keeps them from getting to know R.
> 
> Well, just do give a greenhorn ??s perspective.
> ...

   The tool for exporting to SPSS *is* available in the foreign package 
thanks to Thomas Lumley.  I just made a *small modification* to use the 
variable.labels attribute of a data frame if it's available and the 
names of the data frame if that attribute is not available.  Maybe 
Thomas will consider making a change to foreign:::writeForeignSPSS along 
those lines.

Chuck Cleland

> -----Urspr??ngliche Nachricht-----
> Von: Chuck Cleland [mailto:ccleland at optonline.net] 
> Gesendet: Donnerstag, 12. Januar 2006 01:16
> An: Michael Reinecke
> Cc: R-help at stat.math.ethz.ch
> Betreff: Re: [R] SPSS and R ? do they like each other?
> 
> Michael Reinecke wrote:
> 
>> 
>>Thanks again for your answer! I tried it out. write.foreign produces SPSS syntax, but unfortunally this syntax tells SPSS to take the names (and not the labels) in order to produce SPSS variable labels. The former labels get lost.
>>
>>I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):
>>
>>jjread<-  read.spss("test2.sav", use.value.labels=TRUE, 
>>to.data.frame=TRUE)
>>
>>
>>>str(jjread)
>>
>>`data.frame':   30 obs. of  3 variables:
>> $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
>> $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
>> $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
>> - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
>>  ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
>>
>>
>>>    datafile<-tempfile()
>>>    codefile<-tempfile()
>>>    write.foreign(jjread,datafile,codefile,package="SPSS")
>>>    file.show(datafile)
>>>    file.show(codefile)
>>
>>
>>
>>The syntax file I get is:
>>
>>DATA LIST FILE= 
>>"C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free / VAR00001 VAR00002 VAR00003  .
>>
>>VARIABLE LABELS
>>VAR00001 "VAR00001" 
>> VAR00002 "VAR00002" 
>> VAR00003 "VAR00003" 
>> .
>>
>>EXECUTE.
>>
>>
>>I am working on R 2.2.0. But I think a newer version won ??t fix it either, will it?
> 
> 
> Here is a functiong based on modifying foreign:::writeForeignSPSS (by Thomas Lumley) which might work for you:
> 
> write.SPSS <- function (df, datafile, codefile, varnames = NULL) { adQuote <- function(x){paste("\"", x, "\"", sep = "")}
>      dfn <- lapply(df, function(x) if (is.factor(x))
>          as.numeric(x)
>      else x)
>      write.table(dfn, file = datafile, row = FALSE, col = FALSE)
>      if(is.null(attributes(df)$variable.labels)) varlabels <- names(df) else varlabels <- attributes(df)$variable.labels
>      if (is.null(varnames)) {
>          varnames <- abbreviate(names(df), 8)
>          if (any(sapply(varnames, nchar) > 8))
>              stop("I cannot abbreviate the variable names to eight or fewer letters")
>          if (any(varnames != names(df)))
>              warning("some variable names were abbreviated")
>      }
>      cat("DATA LIST FILE=", dQuote(datafile), " free\n", file = codefile)
>      cat("/", varnames, " .\n\n", file = codefile, append = TRUE)
>      cat("VARIABLE LABELS\n", file = codefile, append = TRUE)
>      cat(paste(varnames, adQuote(varlabels), "\n"), ".\n", file = codefile,
>          append = TRUE)
>      factors <- sapply(df, is.factor)
>      if (any(factors)) {
>          cat("\nVALUE LABELS\n", file = codefile, append = TRUE)
>          for (v in which(factors)) {
>              cat("/\n", file = codefile, append = TRUE)
>              cat(varnames[v], " \n", file = codefile, append = TRUE)
>              levs <- levels(df[[v]])
>              cat(paste(1:length(levs), adQuote(levs), "\n", sep = " "),
>                  file = codefile, append = TRUE)
>          }
>          cat(".\n", file = codefile, append = TRUE)
>      }
>      cat("\nEXECUTE.\n", file = codefile, append = TRUE) }
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From rmh at temple.edu  Thu Jan 12 14:49:00 2006
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 12 Jan 2006 08:49:00 -0500
Subject: [R] dataframes with only one variable
Message-ID: <9a879811.797cc9b7.81a0400@po-d.temple.edu>

> df1
  v1
1  1
2  2
3  3
4  4
> df1[,]
[1] 1 2 3 4
> df1[,1]
[1] 1 2 3 4
> df1[,,drop=F]
  v1
1  1
2  2
3  3
4  4
> df1[,1,drop=F]
  v1
1  1
2  2
3  3
4  4
> df1[1]
  v1
1  1
2  2
3  3
4  4
> df1[[1]]
[1] 1 2 3 4
> 


For transfers from Excel to R using the "[put/get] R dataframe" commands,
I think it is important always to use the drop=FALSE argument
(as I assume you are doing in RExcel V1.55).  The reason
for this is to maintain a rigid relationship between the only partially
compatible conventions of Excel and R.

For strictly within R use, the case is less clear.  I have trained myself 
always (well 85% on the first try) to use the drop=FALSE argument when I care
about the structure after the copy.

The tension between keeping the structure and demoting the structure predates
data.frames.  This was a design issue in matrices as well.
> tmp <- matrix(1:6,2,3)
> tmp
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
> tmp[1,]
[1] 1 3 5
> tmp[1,,drop=FALSE]
     [,1] [,2] [,3]
[1,]    1    3    5
> 

Rich



From ronggui.huang at gmail.com  Thu Jan 12 15:00:36 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 12 Jan 2006 22:00:36 +0800
Subject: [R] SPSS and R ? do they like each other?
In-Reply-To: <43C65D86.3060800@optonline.net>
References: <D1A363788EC8F946A56DAF95C0FBE7CF19689A@sbs2003.CMI.local>
	<43C65D86.3060800@optonline.net>
Message-ID: <38b9f0350601120600n30d279bct@mail.gmail.com>

2006/1/12, Chuck Cleland <ccleland at optonline.net>:
> Michael Reinecke wrote:
> > Thank you very much! write.SPSS works fine.
> >
> > I just wonder, why this very useful function is not part of any package. I have not even found it in the web. For experts it may not be a big deal to write their own export functions, but for newcomers like me it is almost impossible - and at the same time it is essential to us to have good facilities for exchange with our familiar statistical package. I think a lack of exchange tools might be something that scares many people off and keeps them from getting to know R.
> >
> > Well, just do give a greenhorn s perspective.
> > ...
>
>    The tool for exporting to SPSS *is* available in the foreign package
> thanks to Thomas Lumley.  I just made a *small modification* to use the
> variable.labels attribute of a data frame if it's available and the
> names of the data frame if that attribute is not available.  Maybe
> Thomas will consider making a change to foreign:::writeForeignSPSS along
> those lines.

I agree with this point. it 's usefull when one get the spss data file
into R to do something and export that data back to spss data file.

>
> Chuck Cleland
>
> > -----Ursprngliche Nachricht-----
> > Von: Chuck Cleland [mailto:ccleland at optonline.net]
> > Gesendet: Donnerstag, 12. Januar 2006 01:16
> > An: Michael Reinecke
> > Cc: R-help at stat.math.ethz.ch
> > Betreff: Re: [R] SPSS and R ? do they like each other?
> >
> > Michael Reinecke wrote:
> >
> >>
> >>Thanks again for your answer! I tried it out. write.foreign produces SPSS syntax, but unfortunally this syntax tells SPSS to take the names (and not the labels) in order to produce SPSS variable labels. The former labels get lost.
> >>
> >>I tried a data frame produced by read.spss and one by spss.get. Here is the read.spss one (the labels meant to be exported are called "Text 1", ...):
> >>
> >>jjread<-  read.spss("test2.sav", use.value.labels=TRUE,
> >>to.data.frame=TRUE)
> >>
> >>
> >>>str(jjread)
> >>
> >>`data.frame':   30 obs. of  3 variables:
> >> $ VAR00001: num  101 102 103 104 105 106 107 108 109 110 ...
> >> $ VAR00002: num  6 6 5 6 6 6 6 6 6 6 ...
> >> $ VAR00003: num  0 0 6 7 0 7 0 0 0 8 ...
> >> - attr(*, "variable.labels")= Named chr  "Text 1" "Text2" "text 3"
> >>  ..- attr(*, "names")= chr  "VAR00001" "VAR00002" "VAR00003"
> >>
> >>
> >>>    datafile<-tempfile()
> >>>    codefile<-tempfile()
> >>>    write.foreign(jjread,datafile,codefile,package="SPSS")
> >>>    file.show(datafile)
> >>>    file.show(codefile)
> >>
> >>
> >>
> >>The syntax file I get is:
> >>
> >>DATA LIST FILE=
> >>"C:\DOKUME~1\reinecke\LOKALE~1\Temp\Rtmp15028\file27910"  free / VAR00001 VAR00002 VAR00003  .
> >>
> >>VARIABLE LABELS
> >>VAR00001 "VAR00001"
> >> VAR00002 "VAR00002"
> >> VAR00003 "VAR00003"
> >> .
> >>
> >>EXECUTE.
> >>
> >>
> >>I am working on R 2.2.0. But I think a newer version won t fix it either, will it?
> >
> >
> > Here is a functiong based on modifying foreign:::writeForeignSPSS (by Thomas Lumley) which might work for you:
> >
> > write.SPSS <- function (df, datafile, codefile, varnames = NULL) { adQuote <- function(x){paste("\"", x, "\"", sep = "")}
> >      dfn <- lapply(df, function(x) if (is.factor(x))
> >          as.numeric(x)
> >      else x)
> >      write.table(dfn, file = datafile, row = FALSE, col = FALSE)
> >      if(is.null(attributes(df)$variable.labels)) varlabels <- names(df) else varlabels <- attributes(df)$variable.labels
> >      if (is.null(varnames)) {
> >          varnames <- abbreviate(names(df), 8)
> >          if (any(sapply(varnames, nchar) > 8))
> >              stop("I cannot abbreviate the variable names to eight or fewer letters")
> >          if (any(varnames != names(df)))
> >              warning("some variable names were abbreviated")
> >      }
> >      cat("DATA LIST FILE=", dQuote(datafile), " free\n", file = codefile)
> >      cat("/", varnames, " .\n\n", file = codefile, append = TRUE)
> >      cat("VARIABLE LABELS\n", file = codefile, append = TRUE)
> >      cat(paste(varnames, adQuote(varlabels), "\n"), ".\n", file = codefile,
> >          append = TRUE)
> >      factors <- sapply(df, is.factor)
> >      if (any(factors)) {
> >          cat("\nVALUE LABELS\n", file = codefile, append = TRUE)
> >          for (v in which(factors)) {
> >              cat("/\n", file = codefile, append = TRUE)
> >              cat(varnames[v], " \n", file = codefile, append = TRUE)
> >              levs <- levels(df[[v]])
> >              cat(paste(1:length(levs), adQuote(levs), "\n", sep = " "),
> >                  file = codefile, append = TRUE)
> >          }
> >          cat(".\n", file = codefile, append = TRUE)
> >      }
> >      cat("\nEXECUTE.\n", file = codefile, append = TRUE) }
> >
> > --
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 452-1424 (M, W, F)
> > fax: (917) 438-0894
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From sfalcon at fhcrc.org  Thu Jan 12 15:38:16 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 12 Jan 2006 06:38:16 -0800
Subject: [R] Indentation in emacs
In-Reply-To: <148ed8180601120219xc554f16q5ab4dedbe3f55f34@mail.gmail.com>
	(=?iso-8859-1?Q?G=F6ran_Brostr=F6m's?= message of "Thu, 12 Jan 2006
	11:19:28 +0100")
References: <148ed8180601120219xc554f16q5ab4dedbe3f55f34@mail.gmail.com>
Message-ID: <m2y81lh5pj.fsf@fhcrc.org>

On 12 Jan 2006, goran.brostrom at gmail.com wrote:

> I'm using emacs-21.4 on debian unstable, together with the latest
> ESS implementation. I try to change indentation to 4 by following
> the advise in "R-exts": It results in the following lines in my
> .emacs file:
>
> (custom-set-variables
> ;; custom-set-variables was added by Custom -- don't edit or
> ;; cut/paste it!  Your init file should contain only one such
> ;; instance.
> '(c-basic-offset 4)
> '(c-default-style "bsd")
> '(latin1-display t nil (latin1-disp)))
> (custom-set-faces
> ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
> ;; Your init file should contain only one such instance.
> )
>
> But it doesn't work with R code (with C code it works). So what is
> missing?

Try:
(setq ess-indent-level 4)

You may also be interested in the ESS mail list (a better place for
such questions): 
 ESS-help at stat.math.ethz.ch mailing list
 https://stat.ethz.ch/mailman/listinfo/ess-help


+ seth



From f_bresson at yahoo.fr  Thu Jan 12 15:44:06 2006
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Thu, 12 Jan 2006 15:44:06 +0100 (CET)
Subject: [R] tapply and weighted means
Message-ID: <20060112144406.65965.qmail@web26815.mail.ukl.yahoo.com>

I' m trying to compute weighted mean on different
groups but it only returns NA. If I use the following
data.frame truc:

x  y  w
1  1  1
1  2  2
1  3  1
1  4  2
0  2  1
0  3  2
0  4  1
0  5  1

where x is a factor, and then use the command :

tapply(truc$y,list(truc$x),wtd.mean, weights=truc$w)

I just get NA. What's the problem ? What can I do ?



From dimitris.rizopoulos at med.kuleuven.be  Thu Jan 12 15:57:58 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 12 Jan 2006 15:57:58 +0100
Subject: [R] tapply and weighted means
References: <20060112144406.65965.qmail@web26815.mail.ukl.yahoo.com>
Message-ID: <002c01c61788$93bac5c0$0540210a@www.domain>

you need also to split the 'w' column, for each level of 'x'; you 
could use:

lapply(split(truc, truc$x), function(z) weighted.mean(z$y, z$w))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Florent Bresson" <f_bresson at yahoo.fr>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Thursday, January 12, 2006 3:44 PM
Subject: [R] tapply and weighted means


> I' m trying to compute weighted mean on different
> groups but it only returns NA. If I use the following
> data.frame truc:
>
> x  y  w
> 1  1  1
> 1  2  2
> 1  3  1
> 1  4  2
> 0  2  1
> 0  3  2
> 0  4  1
> 0  5  1
>
> where x is a factor, and then use the command :
>
> tapply(truc$y,list(truc$x),wtd.mean, weights=truc$w)
>
> I just get NA. What's the problem ? What can I do ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ahenningsen at email.uni-kiel.de  Thu Jan 12 16:09:40 2006
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 12 Jan 2006 16:09:40 +0100
Subject: [R] Problem with NLSYSTEMFIT()
In-Reply-To: <F4C605A5EE93EA418CC0C5747357D4344980AD@letterman.intranet.m-lehrstuhl.de>
References: <F4C605A5EE93EA418CC0C5747357D4344980AD@letterman.intranet.m-lehrstuhl.de>
Message-ID: <200601121609.40415.ahenningsen@email.uni-kiel.de>

Dear Kilian,

On Thursday 12 January 2006 14:20, Kilian Plank wrote:
> Hello,
>
>
>
> I want to solve a nonlinear 3SLS problem with "nlsystemfit()". The
> equations
>
> are of the form
>
>             y_it = f_i(x,t,theta)
>
> The functions f_i(.) have to be formulated as R-functions. When invoking
>
> "nlsystemfit()" I get the error
>
>
>
> Error in deriv.formula(eqns[[i]], names(parmnames)) :
>
>         Function 'f1' is not in the derivatives table
>
>
>
> Isn't it possible to provide equations in the form
>
> eq1 ~ f1(x,t,theta) etc. to "nlsystemfit()" ?

Unfortunately, this is not (yet) possible. You have to specify your equations 
like "eq1 <- y ~ b0 + x1^b1 + b1^2 * log( x2)" (see the documentation of 
nlsystemfit). Furthermore, I suggest that you ask the author of nlsystemfit 
(Jeff Hamann, see cc) how reliable the algorithms for the non-linear 
estimation are in the current version of systemfit (while the code for the 
linear estimation is very mature now, the non-linear estimation is still 
under development). You are invited to help us improving the code of 
nlsystemfit ;-)

Best wishes,
Arne

> Kind regards,
>
>
>
> Kilian Plank
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From f.harrell at vanderbilt.edu  Thu Jan 12 16:20:30 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 12 Jan 2006 09:20:30 -0600
Subject: [R] tapply and weighted means
In-Reply-To: <002c01c61788$93bac5c0$0540210a@www.domain>
References: <20060112144406.65965.qmail@web26815.mail.ukl.yahoo.com>
	<002c01c61788$93bac5c0$0540210a@www.domain>
Message-ID: <43C673BE.9070406@vanderbilt.edu>

Dimitris Rizopoulos wrote:
> you need also to split the 'w' column, for each level of 'x'; you 
> could use:
> 
> lapply(split(truc, truc$x), function(z) weighted.mean(z$y, z$w))
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris

Or:
library(Hmisc)
?wtd.mean
The help file has a built-in example of this.
Frank

> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://www.med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> 
> ----- Original Message ----- 
> From: "Florent Bresson" <f_bresson at yahoo.fr>
> To: "R-help" <r-help at stat.math.ethz.ch>
> Sent: Thursday, January 12, 2006 3:44 PM
> Subject: [R] tapply and weighted means
> 
> 
> 
>>I' m trying to compute weighted mean on different
>>groups but it only returns NA. If I use the following
>>data.frame truc:
>>
>>x  y  w
>>1  1  1
>>1  2  2
>>1  3  1
>>1  4  2
>>0  2  1
>>0  3  2
>>0  4  1
>>0  5  1
>>
>>where x is a factor, and then use the command :
>>
>>tapply(truc$y,list(truc$x),wtd.mean, weights=truc$w)
>>
>>I just get NA. What's the problem ? What can I do ?
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From admin at biostatistic.de  Thu Jan 12 16:16:39 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 12 Jan 2006 16:16:39 +0100
Subject: [R] wilcox.test warnig message p-value where are the zeros in the
	data?
Message-ID: <43C672D7.7060609@biostatistic.de>

does anybody know why there are the two warnings in the example above?

Regards Knut

 > day_4
 [1] 540   1   1   1   1   1   1 300 720 480
 > day_1
 [1]  438  343    1  475    1  562  500  435 1045  890

> is.vector (day_1)
[1] TRUE
> is.vector (day_4)

[1] TRUE

 > wilcox.test(day_4 
,day_1,paired=TRUE,alternative="two.sided",exact=TRUE,conf.int=TRUE)

        Wilcoxon signed rank test with continuity correction

data:  day_4 and day_1
V = 1, p-value = 0.02086
alternative hypothesis: true mu is not equal to 0
95 percent confidence interval:
 -486.5 -120.0
sample estimates:
(pseudo)median
          -348

Warning messages:
1: cannot compute exact p-value with zeroes in: 
wilcox.test.default(day_4, day_1, paired = TRUE, alternative = 
"two.sided", 
2: cannot compute exact confidence interval with zeroes in: 
wilcox.test.default(day_4, day_1, paired = TRUE, alternative = 
"two.sided",



From jruoho at gmx.net  Thu Jan 12 15:18:23 2006
From: jruoho at gmx.net (jruohonen)
Date: Thu, 12 Jan 2006 16:18:23 +0200
Subject: [R] extract variables from linear model
In-Reply-To: <43C65E79.3010201@molgen.mpg.de>
References: <43C65E79.3010201@molgen.mpg.de>
Message-ID: <200601121618.23488.jruoho@gmx.net>

> I fitted a linear model:
> fit <- lm(y ~ a * b + c - 1 , na.action='na.omit')

wouldn't a simple 

coef(fit)[2] 

work?

Jukka Ruohonen.

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Jan 12 16:28:17 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2006 16:28:17 +0100
Subject: [R] wilcox.test warnig message p-value where are the zeros in
	the data?
In-Reply-To: <43C672D7.7060609@biostatistic.de>
References: <43C672D7.7060609@biostatistic.de>
Message-ID: <x2u0c9v52m.fsf@viggo.kubism.ku.dk>

Knut Krueger <admin at biostatistic.de> writes:

> does anybody know why there are the two warnings in the example above?
> 
> Regards Knut
> 
>  > day_4
>  [1] 540   1   1   1   1   1   1 300 720 480
>  > day_1
>  [1]  438  343    1  475    1  562  500  435 1045  890
> 
> > is.vector (day_1)
> [1] TRUE
> > is.vector (day_4)
> 
> [1] TRUE

The paired Wilcoxon test depends on pairwise differences and as far as
I can see you have two of those being zero, in cases 3 and 5.
 
>  > wilcox.test(day_4 
> ,day_1,paired=TRUE,alternative="two.sided",exact=TRUE,conf.int=TRUE)
> 
>         Wilcoxon signed rank test with continuity correction
> 
> data:  day_4 and day_1
> V = 1, p-value = 0.02086
> alternative hypothesis: true mu is not equal to 0
> 95 percent confidence interval:
>  -486.5 -120.0
> sample estimates:
> (pseudo)median
>           -348
> 
> Warning messages:
> 1: cannot compute exact p-value with zeroes in: 
> wilcox.test.default(day_4, day_1, paired = TRUE, alternative = 
> "two.sided", 
> 2: cannot compute exact confidence interval with zeroes in: 
> wilcox.test.default(day_4, day_1, paired = TRUE, alternative = 
> "two.sided",
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From gavin.simpson at ucl.ac.uk  Thu Jan 12 16:28:10 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 12 Jan 2006 15:28:10 +0000
Subject: [R] tapply and weighted means
In-Reply-To: <20060112144406.65965.qmail@web26815.mail.ukl.yahoo.com>
References: <20060112144406.65965.qmail@web26815.mail.ukl.yahoo.com>
Message-ID: <1137079690.22632.69.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2006-01-12 at 15:44 +0100, Florent Bresson wrote:
> I' m trying to compute weighted mean on different
> groups but it only returns NA. If I use the following
> data.frame truc:
> 
> x  y  w
> 1  1  1
> 1  2  2
> 1  3  1
> 1  4  2
> 0  2  1
> 0  3  2
> 0  4  1
> 0  5  1
> 
> where x is a factor, and then use the command :
> 
> tapply(truc$y,list(truc$x),wtd.mean, weights=truc$w)
> 
> I just get NA. What's the problem ? What can I do ?

Florent,

I guess you didn't read the help for tapply, which in the Value section
states:

     Note that optional arguments to 'FUN' supplied by the '...'
     argument are not divided into cells.  It is therefore
     inappropriate for 'FUN' to expect additional arguments with the
     same length as 'X'.

So tapply is not the right tool for this job. We can use by() instead (a
wrapper for tapply) as so:

dat <- matrix(scan(), byrow = TRUE, ncol = 3)
1  1  1
1  2  2
1  3  1
1  4  2
0  2  1
0  3  2
0  4  1
0  5  1

colnames(dat) <- c("x", "y", "w")
dat <- as.data.frame(dat)
dat
(res <- by(dat, dat$x, function(z) weighted.mean(z$y, z$w)))

but if you want to easily access the numbers you need to do a little
work, e.g.

as.vector(res)

Also, I don't see a function wtd.mean in standard R and weighted.mean()
doesn't have a weights argument, so I guess you are using a function
from another package and did not tell us.

HTH,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Mleeds at kellogggroup.com  Thu Jan 12 16:56:08 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Thu, 12 Jan 2006 10:56:08 -0500
Subject: [R] I think simple R question
Message-ID: <A8B87FDB74320349A9D1CC9021052A766F33F4@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/ff2fceb6/attachment.pl

From rkoenker at uiuc.edu  Thu Jan 12 17:27:39 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 12 Jan 2006 10:27:39 -0600
Subject: [R] I think simple R question
In-Reply-To: <A8B87FDB74320349A9D1CC9021052A766F33F4@exchange.psg.com>
References: <A8B87FDB74320349A9D1CC9021052A766F33F4@exchange.psg.com>
Message-ID: <B9C1A269-AB1C-4D9C-8B2F-64EF669949AE@uiuc.edu>

see ?rle


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jan 12, 2006, at 9:56 AM, Mark Leeds wrote:

> I have a vector x with #'s ( 1 or -1 in them ) in it and I want to
> "mark" a new vector with the sign of the value of the a streak
> of H where H = some number ( at the next spot in the vector )
>
> So, say H was equal to 3 and
> I had a vector of
>
> [1]  [2]  [3]  [4]  [5]   [6]  [7]  [8]  [9]  [10]
>
> 1   -1    1     1    1   -1    1     1  -1    -1
>
> then, I would want a function to return a new
> vector of
>
>
> [1]  [2]  [3]  [4]  [5]   [6]  [7]  [8]  [9]  [10]
>
> 0     0    0    0     0   1     0     0   0     0
>
> As I said, I used to do these things like this
> it's been a while and I'm rusty with this stuff.
>
> Without looping is preferred but looping is okay
> also.
>
>                                        Mark
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> **********************************************************************
> This email and any files transmitted with it are confidentia... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From deepayan.sarkar at gmail.com  Thu Jan 12 17:47:26 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 12 Jan 2006 10:47:26 -0600
Subject: [R] Equal number of cuts in a contourplot with trellis
In-Reply-To: <000201c61763$8eb18e80$44b2fc93@ditbus3zletw5e>
References: <000201c61763$8eb18e80$44b2fc93@ditbus3zletw5e>
Message-ID: <eb555e660601120847s2a7f0e2bxbad89d2dc1fae008@mail.gmail.com>

On 1/12/06, Jesus Frias <Jesus.Frias at dit.ie> wrote:
> Dear R-helpers,
>
> 	I need some help to produce a set of contour plots that I am
> trying to make in order to compare surfaces between the levels of a
> factor. For example:
>
> library(lattice)
> g <- expand.grid(x = 60:100, y = 1:25, ti = c("a","b","c"))
> g$z <-with(g,
>   (-1e-4*x-1e-3*y-1e-5*x*y)*(ti=="a") +
>   (1e-2*x-1e-3*y-1e-4*x*y)*(ti=="b") +
>   (1e-3*x-1e-3*y-1.e-5*x*y)*(ti=="c")
>                )
>
> contourplot(z~ x * y|ti, data = g,
>             cuts=20,
>             pretty=T,
>             screen = list(z = 30, x = -60))
>
> As you can see in the figure, most of the contour lines are in one of
> the levels and we are not able to see how the other levels look like.
>
> I would like to display the same number of cuts in each of the trellis.
> I can make each of the contourplots separately and control the number of
> cuts but I am not able to plot all of them in one.

The simplest solution is to recompute the levels for each panel function:

contourplot(z~ x * y|ti, data = g,
            label.style = "align",
            panel = function(x, y, z, subscripts, at, ...) {
                at <- pretty(z[subscripts], 10)
                panel.contourplot(x, y, z,
                                  subscripts = subscripts,
                                  at = at,
                                  ...)
            })

Alternatively, you could pass in a suitable 'at' vector computed externally.

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From dieter.menne at menne-biomed.de  Thu Jan 12 17:54:30 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 12 Jan 2006 17:54:30 +0100
Subject: [R] "infinite recursion" in do.call when lme4 loaded only
Message-ID: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>

A larg program which worked with lme4/R about a year ago failed when I
re-run it today. I reproduced the problem with the program below.

-- When lme4 is not loaded, the program runs ok and fast enough
-- When lme4 is loaded (but never used), the do.call fails
   with infinite recursion after 60 seconds. Memory used increases
   beyond bonds in task manager.
-- I tested a few S3 based packages (MASS, nlme) and did not get
   similar problems

Current workaround: do lme4-processing in a separate program.


------
#library(lme4) # uncomment this to see the problem
np <- 12
nq <- 20
nph <- 3
nrep <- 30
grd <- expand.grid(Pat=as.factor(1:np),
            Q=as.factor(1:nq),
            Phase=as.factor(1:nph))
df <- with (grd,
  data.frame(Pat=Pat,Q=Q,Phase=Phase,Resp = rnorm(np*nq*nph*nrep)))

score <- function(x) {
 data.frame(Pat=x$Pat[1],Phase=x$Phase[1],Q=x$Q[1],score = mean(x$Resp))
}

# about 20 sec
caScore <- by(df,list(Pat=df$Pat,Phase=df$Phase,Q=df$Q),FUN = score )

ca1 = do.call("rbind",caScore)
# Without lme4: 3 seconds
# With lme4: After 60 sec:
#Error: evaluation nested too deeply: infinite recursion /
#options(expressions=)?


-----------------------

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R



From Greg.Snow at intermountainmail.org  Thu Jan 12 18:05:00 2006
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Thu, 12 Jan 2006 10:05:00 -0700
Subject: [R] Repeated measures aov with post hoc tests?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB19865B@LP-EXCHVS07.CO.IHC.COM>

The multcomp package may do what you want (there is mention of nested
variables in the help).

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Fredrik Karlsson
> Sent: Thursday, January 12, 2006 3:24 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Repeated measures aov with post hoc tests?
> 
> Dear list,
> 
> I posted the message below a cople of days ago, and have not 
> been able to find any solution to this. I do really want some help.
> 
> /Fredrik
> 
> On 1/10/06, Fredrik Karlsson <dargosch at gmail.com> wrote:
> > Dear list,
> >
> > I would like to perform an analysis on the following model:
> >
> > aov(ampratio ~ Type * Place * agemF + Error(speakerid/Place) 
> > ,data=aspvotwork)
> >
> > using the approach from 
> http://www.psych.upenn.edu/~baron/rpsych/rpsych.html .
> >
> > Now, I got the test results, wich indicate a significant 
> interaction 
> > and main effects of the agemF variable. How do I find at 
> what level of 
> > agemF the effect may be found.
> >
> > How do I do this?
> >
> > I found a reference to TukeyHSD in the archives, but I 
> cannot use it:
> >
> > > TukeyHSD(aov(ampratio ~ Type * Place * agemF + 
> > > Error(speakerid/Place),data=aspvotwork))
> > Error in TukeyHSD(aov(ampratio ~ Type * Place * agemF + 
> > Error(speakerid/Place),  :
> >         no applicable method for "TukeyHSD"
> >
> > Please help me.
> >
> > /Fredrik
> >
> 
> 
> --
> My Gentoo + PVR-350 + IVTV + MythTV blog is on 
> http://gentoomythtv.blogspot.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Jan 12 18:26:55 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2006 18:26:55 +0100
Subject: [R] "infinite recursion" in do.call when lme4 loaded only
In-Reply-To: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
Message-ID: <x2psmxuzkw.fsf@viggo.kubism.ku.dk>

"Dieter Menne" <dieter.menne at menne-biomed.de> writes:

> A larg program which worked with lme4/R about a year ago failed when I
> re-run it today. I reproduced the problem with the program below.
> 
> -- When lme4 is not loaded, the program runs ok and fast enough
> -- When lme4 is loaded (but never used), the do.call fails
>    with infinite recursion after 60 seconds. Memory used increases
>    beyond bonds in task manager.
> -- I tested a few S3 based packages (MASS, nlme) and did not get
>    similar problems
> 
> Current workaround: do lme4-processing in a separate program.

Looks like it conks out when the number of frames to rbind is bigger
than about 110. 

Current releases have
> options("expressions")
$expressions
[1] 1000

It was 5000 for a while, but we found that it could overflow the C
stack on some systems. Since your example has 720 lines, I can't quite
rule out that that the problem was really there all the time.

However, it surely has to do with methods dispatch:

> system.time(do.call("rbind.data.frame",caScore))
[1] 0.99 0.00 0.99 0.00 0.00

which provides you with another workaround.

 
> 
> ------
> #library(lme4) # uncomment this to see the problem
> np <- 12
> nq <- 20
> nph <- 3
> nrep <- 30
> grd <- expand.grid(Pat=as.factor(1:np),
>             Q=as.factor(1:nq),
>             Phase=as.factor(1:nph))
> df <- with (grd,
>   data.frame(Pat=Pat,Q=Q,Phase=Phase,Resp = rnorm(np*nq*nph*nrep)))
> 
> score <- function(x) {
>  data.frame(Pat=x$Pat[1],Phase=x$Phase[1],Q=x$Q[1],score = mean(x$Resp))
> }
> 
> # about 20 sec
> caScore <- by(df,list(Pat=df$Pat,Phase=df$Phase,Q=df$Q),FUN = score )
> 
> ca1 = do.call("rbind",caScore)
> # Without lme4: 3 seconds
> # With lme4: After 60 sec:
> #Error: evaluation nested too deeply: infinite recursion /
> #options(expressions=)?
> 
> 
> -----------------------
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From schaber at molgen.mpg.de  Thu Jan 12 18:40:55 2006
From: schaber at molgen.mpg.de (=?ISO-8859-1?Q?J=F6rg_Schaber?=)
Date: Thu, 12 Jan 2006 18:40:55 +0100
Subject: [R] extract variables from linear model
Message-ID: <43C694A7.6090305@molgen.mpg.de>

Hi,

I fitted a mixed linear model y = a + b + a*b + c + error, with c being 
the random factor:
lmefit <- lme(y ~ a * b - 1 , random = ~ 1 | c, na.action='na.omit')

Is there a way to omit some level combinations of the cross-term a:b? 
E.g. those that are not significant?

When I add the coefficients of a, b and a:b to get the combined effect 
without the effect of c, do then the standard errors of the coefficients 
also add?

best,

joerg



From phgrosjean at sciviews.org  Thu Jan 12 19:08:36 2006
From: phgrosjean at sciviews.org (phgrosjean@sciviews.org)
Date: Thu, 12 Jan 2006 19:08:36 +0100 (CET)
Subject: [R] R - Wikis and R-core
In-Reply-To: <17347.47892.343299.844202@stat.math.ethz.ch>
References: <43BDC208.5060909@vanderbilt.edu>
	<20060108160043.E66F345E@wmailp01.st2.lyceu.net>
	<17347.47892.343299.844202@stat.math.ethz.ch>
Message-ID: <20060112180836.C89DA31B@wmailp01.st2.lyceu.net>

Hello Martin and others,

I am happy with this decision. I'll look a little bit at this next week.

Best,

Philippe Grosjean

> We've had a small "review time" within R-core on this topic,
> amd would like to state the following:
>
> --------------------------------------------------------------------------
> The R-core team welcomes proposals to develop an R-wiki.
>
> - We would consider linking a very small number of Wikis (ideally one)
>   from www.r-project.org and offering an address in the r-project.org
> domain (such as 'wiki.r-project.org').
>
> - The core team has no support time to offer, and would be looking for
>   a medium-term commitment from a maintainer team for the Wiki(s).
>
> - Suggestions for the R documentation would best be filtered through the
>
>   Wiki maintainers, who could e.g. supply suggested patches during the
> alpha  phase of an R release.
> --------------------------------------------------------------------------
>
> Our main concerns have been about ensuring the quality of such extra
> documentation projects, hence the 2nd point above.
> Several of our more general, not mainly R, experiences have been
> of outdated web pages which are continued to be used as
> reference when their advice has long been superseded.
> I think it's very important to try ensuring that this won't
> happen with an R Wiki.
>
> Martin Maechler, ETH Zurich
>
>>>>>> "PhGr" == Philippe Grosjean <phgrosjean at sciviews.org>
>>>>>>     on Sun, 8 Jan 2006 17:00:44 +0100 (CET) writes:
>
>     PhGr> Hello all, Sorry for not taking part of this
>     PhGr> discussion earlier, and for not answering Detlef
>     PhGr> Steuer, Martin Maechler, and others that asked more
>     PhGr> direct questions to me. I am away from my office and
>     PhGr> my computer until the 16th of January.
>
>     PhGr> Just quick and partial answers: 1) I did not know
>     PhGr> about Hamburg RWiki. But I would be happy to merge
>     PhGr> both in one or the other way, as Detlef suggests it.
>
>     PhGr> 2) I choose DokuWiki as the best engine after a
>     PhGr> careful comparison of various Wiki engines. It is the
>     PhGr> best one, as far as I know, for the purpose of
>     PhGr> writting software documentation and similar
>     PhGr> pages. There is an extensive and clearly presented
>     PhGr> comparison of many Wikki engines at:
>     PhGr> http://www.wikimatrix.org/.
>
>     PhGr> 3) I started to change DokuWiki (addition of various
>     PhGr> plugins, addition of R code syntax coloring with
>     PhGr> GESHI, etc...). So, it goes well beyond all current
>     PhGr> Wiki engines regarding its suitability to present R
>     PhGr> stuff.
>
>     PhGr> 4) The reasons I did this is because I think the Wiki
>     PhGr> format could be of a wider use. I plan to change a
>     PhGr> little bit the DokuWiki syntax, so that it works with
>     PhGr> plain .R code files (Wiki part is simply embedded in
>     PhGr> commented lines, and the rest is recognized and
>     PhGr> formatted as R code by the Wiki engine). That way, the
>     PhGr> same Wiki document can either rendered by the Wiki
>     PhGr> engine for a nice presentation, or sourced in R
>     PhGr> indifferently.
>
>     PhGr> 5) My last idea is to add a Rpad engine to the Wiki,
>     PhGr> so that one could play with R code presented in the
>     PhGr> Wiki pages and see the effects of changes directly in
>     PhGr> the Wiki.
>
>     PhGr> 6) Regarding the content of the Wiki, it should be
>     PhGr> nice to propose to the authors of various existing
>     PhGr> document to put them in a Wiki form. Something like
>     PhGr> "Statistics with R"
>     PhGr> (http://zoonek2.free.fr/UNIX/48_R/all.html) is written
>     PhGr> in a way that stimulates additions to pages in
>     PhGr> perpetual construction, if it was presented in a Wiki
>     PhGr> form. It is licensed as Creative Commons
>     PhGr> Attribution-NonCommercial-ShareAlike 2.5 license, that
>     PhGr> is, exactly the same one as DokuWiki that I choose for
>     PhGr> R Wiki. Of course, I plan to ask its author to do so
>     PhGr> before putting its hundreds of very interesting pages
>     PhGr> on the Wiki... I think it is vital to have already
>     PhGr> something in the Wiki, in order to attract enough
>     PhGr> readers, and then enough contributors!
>
>     PhGr> 7) Regarding spamming and vandalism, DokuWiki allows
>     PhGr> to manage rights and users, even individually for
>     PhGr> pages. I think it would be fine to lock pages that
>     PhGr> reach a certain maturity (read-only / editable by
>     PhGr> selected users only) , with link to a discussion page
>     PhGr> which remaining freely accessible at the bottom of
>     PhGr> locked pages.
>
>     PhGr> 8) I would be happy to contribute this work to the R
>     PhGr> foundation in one way or the other to integrate it in
>     PhGr> http://www.r-project.org or
>     PhGr> http://cran.r-project.org. But if it is fine keeping
>     PhGr> it in http://www.sciviews.org as well, it is also fine
>     PhGr> for me.
>
>     PhGr> I suggest that all interested people drop a little
>     PhGr> email to my mailbox.  I'll recontact you when I will
>     PhGr> be back to my office to work on a more elaborate
>     PhGr> solution altogether when I am back at my office.
>     PhGr> Best,
>
>     PhGr> Philippe Grosjean
>
>     PhGr> ______________________________________________
>     PhGr> R-help at stat.math.ethz.ch mailing list
>     PhGr> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>     PhGr> read the posting guide!
>     PhGr> http://www.R-project.org/posting-guide.html



From ndurand at fr.abx.fr  Thu Jan 12 19:11:12 2006
From: ndurand at fr.abx.fr (ndurand@fr.abx.fr)
Date: Thu, 12 Jan 2006 19:11:12 +0100
Subject: [R] Curve fitting
Message-ID: <OFD3B68CE5.FC275ECB-ONC12570F4.006262CB-C12570F4.0063F166@fr.abx.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/1a492e3c/attachment.pl

From dieter.menne at menne-biomed.de  Thu Jan 12 19:14:32 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 12 Jan 2006 18:14:32 +0000 (UTC)
Subject: [R] "infinite recursion" in do.call when lme4 loaded only
References: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
	<x2psmxuzkw.fsf@viggo.kubism.ku.dk>
Message-ID: <loom.20060112T191150-51@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

> > A larg program which worked with lme4/R about a year ago failed when I
> > re-run it today. I reproduced the problem with the program below.

> > -- When lme4 is loaded (but never used), the do.call fails
> >    with infinite recursion after 60 seconds. Memory used increases
> >    beyond bonds in task manager.
> 
> However, it surely has to do with methods dispatch:
> 
> > system.time(do.call("rbind.data.frame",caScore))
> [1] 0.99 0.00 0.99 0.00 0.00
> 
> which provides you with another workaround.

Peter, I had increased the optional value already, but I still don't understand 
what this recursion overflow has to do with the lm4 loading.

Dieter



From mirko.sanpietrucci at email.it  Thu Jan 12 19:34:01 2006
From: mirko.sanpietrucci at email.it (mirko sanpietrucci)
Date: Thu, 12 Jan 2006 19:34:01 +0100
Subject: [R] t-test for standard deviations
Message-ID: <001601c617a6$c31808a0$9ac90b3e@dms.unina.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/00e7aa76/attachment.pl

From charles.edwin.white at us.army.mil  Thu Jan 12 19:50:24 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 12 Jan 2006 13:50:24 -0500
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R 2.1.1
Message-ID: <8BAEC5E546879B4FAA536200A292C614E6A4ED@AMEDMLNARMC135.amed.ds.army.mil>

I am currently using R 2.1.1 under Windows and I do not seem to be able
to load the current versions of lme4/Matrix. I have run
'update.packages.' I understand this is still experimental software but
I would like access to a working version.

Thanks.

Chuck

R Output:

> library(lme4)
Loading required package: Matrix
Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
        ReadItem: unknown type 241
In addition: Warning messages:
1: package 'lme4' was built under R version 2.3.0 
2: package 'Matrix' was built under R version 2.3.0 
Error: package 'Matrix' could not be loaded
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Jan 12 19:52:33 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jan 2006 18:52:33 -0000 (GMT)
Subject: [R] t-test for standard deviations
In-Reply-To: <001601c617a6$c31808a0$9ac90b3e@dms.unina.it>
Message-ID: <XFMail.060112185233.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jan-06 mirko sanpietrucci wrote:
> Dear R-users,
> I am new to the list and I would like to submit (probably!!!!)
> a stupid question:
> 
> I found in a paper a reference to a t-test for the evaluationg the
> difference between the standard deviations of 2 samples.
> This test is performed in the paper but the methodology is not
> explained and any reference is reported.
> 
> Does anyone know where I can find references to this test and if it is
> implemented in R?
> 
> Thenks in advance for your help,
> 
> Mirko

If the paper says that a

1) "t-test"

was used for evaluating the difference between the

2) "standard deviations"

of 2 samples

then I suspect that one or the other of these is a misprint.

To compare standard deviations (more precisely, variances)
you could use a (1)F-test.

Or you would use a t-test to evaluate the difference between
the (2)means of 2 samples.

If it is really obscure what was done, perhaps an appropriate
quotation from the paper would help to ascertain the problem.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jan-06                                       Time: 18:52:31
------------------------------ XFMail ------------------------------



From yen.lin.chia at intel.com  Thu Jan 12 19:59:11 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Thu, 12 Jan 2006 10:59:11 -0800
Subject: [R] Convert matrix to data.frame
Message-ID: <E305A4AFB7947540BC487567B5449BA809191D70@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/f3b6a999/attachment.pl

From bolker at ufl.edu  Thu Jan 12 20:00:07 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 12 Jan 2006 19:00:07 +0000 (UTC)
Subject: [R] Curve fitting
References: <OFD3B68CE5.FC275ECB-ONC12570F4.006262CB-C12570F4.0063F166@fr.abx.fr>
Message-ID: <loom.20060112T195602-468@post.gmane.org>

 <ndurand <at> fr.abx.fr> writes:

> 
> Hi!
> 
> I have a problem of curve fitting.

> 
>  I perform parametric fits using custom equations
> 
> when I use this equation :   y  =  yo + K *(1/(1+exp(-(a+b*ln(x)))))   the 
> fitting result is OK
> but when I use this more general equation :    y  =  yo + K 
> *(1/(1+exp(-(a+b*log(x)+c*x))))  , then I get an aberrant curve!
> 
> I don't understand that... The second fitting should be at least as good 
> as the first one because when taking c=0, both equations are identical!
> 

  Can you specify *exactly* what R code you're using?
Are you using nls()?

  You're trying to fit a five-parameter model to
five data points, which is likely to be difficult if
not impossible to do statistically.  Furthermore, your
data points don't have very much information in them
about all the parameters you're trying to estimate --
they are steadily decreasing, with very mild
curvature.  Finally, if these "data" happen to be
points that you have generated as theoretical
values, without adding noise, nls will give you
problems (see ?nls).

  If you give us more detail about what you're trying
to do we might be able to help (or possibly tell you
that it really can't work ...)

  Ben Bolker



From Mleeds at kellogggroup.com  Thu Jan 12 20:10:52 2006
From: Mleeds at kellogggroup.com (Mark Leeds)
Date: Thu, 12 Jan 2006 14:10:52 -0500
Subject: [R] I think not so hard question
Message-ID: <A8B87FDB74320349A9D1CC9021052A766F341C@exchange.psg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060112/f68dbae7/attachment.pl

From andy_liaw at merck.com  Thu Jan 12 20:41:29 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 12 Jan 2006 14:41:29 -0500
Subject: [R] I think not so hard question
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6F7@usctmx1106.merck.com>

Mark,

It's a bit unclear whether you're looking for a run of exactly 3, or at
least 3.  If it's exactly 3, what Prof. Koenker suggested (using rle())
should help you a lot.  If you want runs of at least 3, it should work
similarly as well.

> set.seed(1)
> (x <- sample(c(-1, 1), 100, replace=TRUE))
  [1] -1 -1  1  1 -1  1  1  1  1 -1 -1 -1  1 -1  1 -1  1  1 -1  1  1 -1  1
-1 -1 -1
 [27] -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1 -1  1  1  1  1  1  1 -1 -1  1
1 -1  1
 [53] -1 -1 -1 -1 -1  1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1 -1  1 -1 -1 -1
1  1 -1
 [79]  1  1 -1  1 -1 -1  1 -1  1 -1 -1 -1 -1 -1  1  1  1  1 -1 -1  1  1
> r <- rle(x)$length
> p <- which(r == 3)
> idx <- cumsum(r)[p] - 2
> sapply(idx, function(i) x[i:(i+2)])
     [,1] [,2] [,3] [,4]
[1,]   -1    1   -1   -1
[2,]   -1    1   -1   -1
[3,]   -1    1   -1   -1
> idx
[1] 10 35 62 73

Andy


From: Mark Leeds
> 
> I'm sorry to bother this list so much
> But I haven't programmed in
> A while and I'm struggling.
>  
> I have a vector in R of 1's and -1's
> And I want to use a streak of size Y
> To predict that the same value will
> Be next.
>  
> So, suppose Y = 3. Then,  if there is a streak of three 
> ones in a row, then I will predict that the next value is
> a 1. But, if there is a streak of 3 -1's in a row,
> then I will predict that a -1 is next. Otherwise,
> I don't predict anything.
>  
> I am really new to R and kind of struggling
> And I was wondering if someone could show
> how to do this ?
>  
> In other words, given a vector of -1's
> And 1's,  I am unable ( I've been trying
> For 2 days ) to create a new vector that
> Has the predictions in it at the appropriate
> places ? Thanks.
>  
>  
> 
> 
> **********************************************************************
> This email and any files transmitted with it are 
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From CPartridge at hec.ohio-state.edu  Thu Jan 12 20:54:04 2006
From: CPartridge at hec.ohio-state.edu (Charles Partridge)
Date: Thu, 12 Jan 2006 14:54:04 -0500
Subject: [R] Multilevel models with mixed effects in R?
Message-ID: <C3AE68CDBFAEB34DBCF9F41A8A0136CE35C8AD@maverick.hec.ohio-state.edu>

Group,

I am new to R.  In my work as a program evaluator, I am regularly asked
to estimate effect sizes of prevention/intervention and educational
programs on various student outcomes (e.g. academic achievement).  In
many cases, I have access to data over three or more time periods (e.g.
growth in proficiency test scores). 

I usually have multiple independent and dependent variables in each
model along with covariates.  I have historically utilized latent growth
curve structural equation models, but would like to include random
effects in the model.  Does R have the ability to run such analyses?  

Regards,

Charles R. Partridge
Evaluation Specialist
Center for Learning Excellence
The John Glenn Institute for Public Service & Public Policy
807 Kinnear Road, Room 214
Columbus, Ohio 43212-1421
Phone: 614.292.2419
FAX: 614.247.6447
Email: cpartridge at hec.ohio-state.edu
http://cle.osu.edu

CONFIDENTIALITY NOTICE: This message is intended only for th...{{dropped}}



From jones at reed.edu  Thu Jan 12 20:54:38 2006
From: jones at reed.edu (Albyn Jones)
Date: Thu, 12 Jan 2006 11:54:38 -0800
Subject: [R] Curve fitting
In-Reply-To: <OFD3B68CE5.FC275ECB-ONC12570F4.006262CB-C12570F4.0063F166@fr.abx.fr>
References: <OFD3B68CE5.FC275ECB-ONC12570F4.006262CB-C12570F4.0063F166@fr.abx.fr>
Message-ID: <20060112195438.GA25779@laplace.reed.edu>

You haven't told us how you are fitting the model; are you using
nls(), and if so with what initial values?  The models don't make
sense at x=0, due to the inclusion of the log(x) term.  Ignoring that,
you have 5 observations and 5 parameters in your second model. What is
the reason you are including both "b*log(x)" and "c*x" terms in the
model?  

regards

albyn
-----------------------------------------------------------------------
On Thu, Jan 12, 2006 at 07:11:12PM +0100, ndurand at fr.abx.fr wrote:
> Hi!
> 
> I have a problem of curve fitting.
> 
> I use the following data :
> 
>  - vector of predictor data : 
> 0
> 0.4
> 0.8
> 1.2
> 1.6
> 
> - vector of response data : 
> 0.81954
> 0.64592
> 0.51247
> 0.42831
> 0.35371
> 
>  I perform parametric fits using custom equations
> 
> when I use this equation :   y  =  yo + K *(1/(1+exp(-(a+b*ln(x)))))   the 
> fitting result is OK
> but when I use this more general equation :    y  =  yo + K 
> *(1/(1+exp(-(a+b*log(x)+c*x))))  , then I get an aberrant curve!
> 
> I don't understand that... The second fitting should be at least as good 
> as the first one because when taking c=0, both equations are identical!
> 
> There is here a mathematical phenomenon that I don't understand!....could 
> someone help me????
> 
> Thanks a lot in advance!
> 
> Nad?ge 
> 
> 	[[alternative HTML version deleted]]
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu Jan 12 20:57:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 12 Jan 2006 11:57:58 -0800
Subject: [R] t-test for standard deviations
In-Reply-To: <XFMail.060112185233.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200601121957.k0CJvkEl015050@meitner.gene.com>

Sorry, Ted:

Google on "Brown-Forsythe" and "Levene's test" and you will, indeed, find
that rather robust and powerful t-tests can be used for testing homogeneity
of spreads. In fact, on a variety of accounts, these tests are preferable to
F-tests, which are notoriously non-robust (sensitive to non-normality) and
which should long ago have been banned from statistics tects (IMHO).

OTOH, whether one **should** test for homogeneity of spread instead of using
statistical procedures robust to moderate heteroscedascity is another
question. IMO, and I think on theoretical grounds, that is a better way to
do things.

Best yet is to use balanced designs in which most anything you do is less
affected by any of these deviations from standard statistical assumptions.
But that requires malice aforethought, rather than data dredging ...

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
> Sent: Thursday, January 12, 2006 10:53 AM
> To: mirko sanpietrucci
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] t-test for standard deviations
> 
> On 12-Jan-06 mirko sanpietrucci wrote:
> > Dear R-users,
> > I am new to the list and I would like to submit (probably!!!!)
> > a stupid question:
> > 
> > I found in a paper a reference to a t-test for the evaluationg the
> > difference between the standard deviations of 2 samples.
> > This test is performed in the paper but the methodology is not
> > explained and any reference is reported.
> > 
> > Does anyone know where I can find references to this test 
> and if it is
> > implemented in R?
> > 
> > Thenks in advance for your help,
> > 
> > Mirko
> 
> If the paper says that a
> 
> 1) "t-test"
> 
> was used for evaluating the difference between the
> 
> 2) "standard deviations"
> 
> of 2 samples
> 
> then I suspect that one or the other of these is a misprint.
> 
> To compare standard deviations (more precisely, variances)
> you could use a (1)F-test.
> 
> Or you would use a t-test to evaluate the difference between
> the (2)means of 2 samples.
> 
> If it is really obscure what was done, perhaps an appropriate
> quotation from the paper would help to ascertain the problem.
> 
> Best wishes,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 12-Jan-06                                       Time: 18:52:31
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mk90-40 at gmx.de  Thu Jan 12 21:02:59 2006
From: mk90-40 at gmx.de (mk90-40@gmx.de)
Date: Thu, 12 Jan 2006 21:02:59 +0100 (MET)
Subject: [R] bandwidth - Hmise.mixt - ks-package
Message-ID: <21968.1137096179@www18.gmx.net>

Hello,

I want to use Hmise.mixt for finding the optimal bandwidth. To be honest, I
have only poor knowledge of the mathematical background. Using the
help-file, I wrote:

 hopt<-Hmise.mixt(c(0.0,1.5),c(1,1/9),c(0.75,0.25),100,0.4)

but got the message:

Error in ((i - 1) * d + 1):(i * d) : NA/NaN Argument

Has anyone an idea, what's wrong?

Thanks!
Mala

--



From HDoran at air.org  Thu Jan 12 21:15:58 2006
From: HDoran at air.org (Doran, Harold)
Date: Thu, 12 Jan 2006 15:15:58 -0500
Subject: [R] Multilevel models with mixed effects in R?
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A017D3446@dc1ex3.air.org>

Yes, there are now multiple functions. One is the lmer() function in the
matrix package. Another is in the nlme package and is the lme()
function. Lmer is the newer version and the syntax has changed just
slightly.  To see samples of the lmer function type the following at
your R command prompt

> library(mlmRev)
> vignette("MlmSoftRev")

This will open a pdf file with examples. You'll need to make sure to
obtain the mlmRev package from cran.

You can also see examples of student achievement analyses using these
functions in the following papers

http://cran.r-project.org/doc/Rnews/Rnews_2003-3.pdf
http://cran.r-project.org/doc/Rnews/Rnews_2005-1.pdf 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charles Partridge
Sent: Thursday, January 12, 2006 2:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Multilevel models with mixed effects in R?

Group,

I am new to R.  In my work as a program evaluator, I am regularly asked
to estimate effect sizes of prevention/intervention and educational
programs on various student outcomes (e.g. academic achievement).  In
many cases, I have access to data over three or more time periods (e.g.
growth in proficiency test scores). 

I usually have multiple independent and dependent variables in each
model along with covariates.  I have historically utilized latent growth
curve structural equation models, but would like to include random
effects in the model.  Does R have the ability to run such analyses?  

Regards,

Charles R. Partridge
Evaluation Specialist
Center for Learning Excellence
The John Glenn Institute for Public Service & Public Policy
807 Kinnear Road, Room 214
Columbus, Ohio 43212-1421
Phone: 614.292.2419
FAX: 614.247.6447
Email: cpartridge at hec.ohio-state.edu
http://cle.osu.edu

CONFIDENTIALITY NOTICE: This message is intended only for\ t...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Thu Jan 12 21:19:06 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jan 2006 20:19:06 -0000 (GMT)
Subject: [R] Basis of fisher.test
Message-ID: <XFMail.060112201906.Ted.Harding@nessie.mcc.ac.uk>

I want to ascertain the basis of the table ranking,
i.e. the meaning of "extreme", in Fisher's Exact Test
as implemented in 'fisher.test', when applied to RxC
tables which are larger than 2x2.

One can summarise a strategy for the test as

1) For each table compatible with the margins
   of the observed table, compute the probability
   of this table conditional on the marginal totals.

2) Rank the possible tables in order of a measure
   of discrepancy between the table and the null
   hypothesis of "no association".

3) Locate the observed table, and compute the sum
   of the probabilties, computed in (1), for this
   table and more "extreme" tables in the sense of
   the ranking in (2).

The question is: what "measure of discrepancy" is
used in 'fisher.test' corresponding to stage (2)?

(There are in principle several possibilities, e.g.
value of a Pearson chi-squared, large values being
discrepant; the probability calculated in (2),
small values being discrepant; ... )

"?fisher.test" says only:

     In the one-sided 2 by 2 cases, p-values are obtained
     directly using the hypergeometric distribution.
     Otherwise, computations are based on a C version of
     the FORTRAN subroutine FEXACT which implements the
     network developed by Mehta and Patel (1986) and
     improved by Clarkson, Fan & Joe (1993). The FORTRAN
     code can be obtained from
     <URL: http://www.netlib.org/toms/643>.

I have had a look at this FORTRAN code, and cannot ascertain
it from the code itself. However, there is a Comment to the
effect:

c     PRE    - Table p-value.  (Output)
c              PRE is the probability of a more extreme table, where
c              'extreme' is in a probabilistic sense.

which suggests that the tables are ranked in order of their
probabilities as computed in (2).

Can anyone confirm definitively what goes on?

With thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jan-06                                       Time: 20:19:02
------------------------------ XFMail ------------------------------



From tplate at acm.org  Thu Jan 12 21:27:30 2006
From: tplate at acm.org (Tony Plate)
Date: Thu, 12 Jan 2006 13:27:30 -0700
Subject: [R] Convert matrix to data.frame
In-Reply-To: <E305A4AFB7947540BC487567B5449BA809191D70@scsmsx402.amr.corp.intel.com>
References: <E305A4AFB7947540BC487567B5449BA809191D70@scsmsx402.amr.corp.intel.com>
Message-ID: <43C6BBB2.9020302@acm.org>

When I try converting a matrix to a data frame, it works for me:

 > x <- matrix(1:6,ncol=2,dimnames=list(LETTERS[1:3],letters[24:25]))
 > data.frame(x)
   x y
A 1 4
B 2 5
C 3 6
 > str(data.frame(x))
`data.frame':   3 obs. of  2 variables:
  $ x: int  1 2 3
  $ y: int  4 5 6
 >

You can also use as.data.frame() to convert a matrix to a data.frame 
(but note that if colnames are missing form the matrix, as.data.frame() 
  constructs different colnames than does data.frame().

You say "it didn't work" -- it's difficult to help with such a 
non-specific complaint.  Can you explain exactly how it didn't work for 
you?  (e.g., show the exact error message).

-- Tony Plate

Chia, Yen Lin wrote:
> Hi all,
> 
>  
> 
> I wonder how could I convert a matrix A to a dataframe such that
> whenever I'm running a linear model such lme, I can use A$x1?  I tried
> data.frame(A), it didn't work.  Should I initialize A not as a matrix?
> Thanks.
> 
>  
> 
> Yen Lin
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdfrost at UCSD.Edu  Thu Jan 12 21:54:11 2006
From: sdfrost at UCSD.Edu (sdfrost@UCSD.Edu)
Date: Thu, 12 Jan 2006 20:54:11 GMT
Subject: [R] Firths bias correction for log-linear models
Message-ID: <200601122054.k0CKsATa039809@smtp.ucsd.edu>

Dear R-Help List,

I'm trying to implement Firth's (1993) bias correction for log-linear models.
Firth (1993) states that such a correction can be implemented by supplementing
the data with a function of h_i, the diagonals from the hat matrix, but doesn't
provide further details. I can see that for a saturated log-linear model,  h_i=1
for all i, hence one just adds 1/2 to each count, which is equivalent to the
Jeffrey's prior, but I'd also like to get bias corrected estimates for other log
linear models. It appears that I need to iterate using GLM, with the weights
option and h_i, which I can get from the function hatvalues. For logistic
regression, this can be performed by splitting up each observation into response
and nonresponse, and using weights as described in Heinze, G. and Schemper, M.
(2002), but I'm unsure of how to implement the analogue for log-linear models. A
procedure using IWLS is described by Firth (1992) in Dodge and Whittaker (1992),
but this book isn't in the local library, and its $141+ on Amazon. I've tried
looking at the code in the logistf and brlr libraries, but I haven't had any
(successful) ideas. Can anyone help me in describing how to implement this in R?

Thanks!
Simon



From spencer.graves at pdf.com  Thu Jan 12 22:08:07 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 12 Jan 2006 13:08:07 -0800
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
 2.1.1
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614E6A4ED@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614E6A4ED@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <43C6C537.1090100@pdf.com>

	  Did you try upgrading to R 2.2.1?  I just installed R 2.2.1 with the 
latest version of "lme4" and "Matrix", and they loaded fine for me.
 > library(lme4)
Loading required package: Matrix
Loading required package: lattice
 > sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-6"

	  Running Windows XP.  (This may be obvious from 'i386-pc-mingw23', but 
I'm no sufficiently aware to know.)

	  Doug Bates has worked very hard to create this, and I for one would 
not want to ask him to take the time to make it backward compatible for 
those who for whatever reason haven't yet upgraded to R 2.2.1, 
especially since for most of us it is fairly easy to upgrade.

	  Spencer Graves

White, Charles E WRAIR-Wash DC wrote:
> I am currently using R 2.1.1 under Windows and I do not seem to be able
> to load the current versions of lme4/Matrix. I have run
> 'update.packages.' I understand this is still experimental software but
> I would like access to a working version.
> 
> Thanks.
> 
> Chuck
> 
> R Output:
> 
> 
>>library(lme4)
> 
> Loading required package: Matrix
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
>         ReadItem: unknown type 241
> In addition: Warning messages:
> 1: package 'lme4' was built under R version 2.3.0 
> 2: package 'Matrix' was built under R version 2.3.0 
> Error: package 'Matrix' could not be loaded
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Jan 12 22:09:22 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jan 2006 21:09:22 -0000 (GMT)
Subject: [R] t-test for standard deviations
In-Reply-To: <200601121957.k0CJvkEl015050@meitner.gene.com>
Message-ID: <XFMail.060112210922.Ted.Harding@nessie.mcc.ac.uk>


On 12-Jan-06 Berton Gunter wrote:
> Sorry, Ted:
> 
> Google on "Brown-Forsythe" and "Levene's test" and you will,
> indeed, find that rather robust and powerful t-tests can be
> used for testing homogeneity of spreads. In fact, on a variety
> of accounts, these tests are preferable to F-tests, which are
> notoriously non-robust (sensitive to non-normality) and
> which should long ago have been banned from statistics tects (IMHO).

Not sure that I would consider either of these as a "t-test"
as usually undestood.
.
Both are based on deriving a "dispersion variable" transform
of the observations in each group (dquared or absolute deviation
from the mean for Levene, absolute deivation from the mean for
Brown-Forsythe), and performing an analysis of variance with
the derived variable.

Granted, in the case of two groups the ANOVA is equivalent to
a "squared t-test" and one could indeed use a t-test instead
of a 2-group ANOVA to get the directional information as well.

But I would be surprised to find such a procedure referred to
as a "t-test" as cited by Mirko. I think it would help if he
told us a bit more about what the paper actually says.

> OTOH, whether one **should** test for homogeneity of spread
> instead of using statistical procedures robust to moderate
> heteroscedascity is another question. IMO, and I think on
> theoretical grounds, that is a better way to do things.
> 
> Best yet is to use balanced designs in which most anything
> you do is less affected by any of these deviations from standard
> statistical assumptions.
> But that requires malice aforethought, rather than data dredging ...

Your comments on the merits and advisability of these things
are good -- but not forgetting that it is also a good idea to
have enough understanding of what one is dealing with to be
able to judge what might be best for the case in hand. However,
I'm entirely with you when it comes to uncircumspect habitual
use of standard procedures.

Best wishes,
Ted.

> 
> Cheers,
> Bert
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific
> learning
> process."  - George E. P. Box
>  
>  
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
>> Sent: Thursday, January 12, 2006 10:53 AM
>> To: mirko sanpietrucci
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] t-test for standard deviations
>> 
>> On 12-Jan-06 mirko sanpietrucci wrote:
>> > Dear R-users,
>> > I am new to the list and I would like to submit (probably!!!!)
>> > a stupid question:
>> > 
>> > I found in a paper a reference to a t-test for the evaluationg the
>> > difference between the standard deviations of 2 samples.
>> > This test is performed in the paper but the methodology is not
>> > explained and any reference is reported.
>> > 
>> > Does anyone know where I can find references to this test 
>> and if it is
>> > implemented in R?
>> > 
>> > Thenks in advance for your help,
>> > 
>> > Mirko
>> 
>> If the paper says that a
>> 
>> 1) "t-test"
>> 
>> was used for evaluating the difference between the
>> 
>> 2) "standard deviations"
>> 
>> of 2 samples
>> 
>> then I suspect that one or the other of these is a misprint.
>> 
>> To compare standard deviations (more precisely, variances)
>> you could use a (1)F-test.
>> 
>> Or you would use a t-test to evaluate the difference between
>> the (2)means of 2 samples.
>> 
>> If it is really obscure what was done, perhaps an appropriate
>> quotation from the paper would help to ascertain the problem.
>> 
>> Best wishes,
>> Ted.
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 12-Jan-06                                       Time: 18:52:31
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jan-06                                       Time: 21:09:19
------------------------------ XFMail ------------------------------



From jerk_alert at hotmail.com  Thu Jan 12 22:15:20 2006
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 12 Jan 2006 21:15:20 +0000
Subject: [R] bug with mai , pdf, and heatmap ?
Message-ID: <BAY101-F23AF6E4012B1E735D1023FE8270@phx.gbl>

Hi all,

When using heatmap() with a pdf driver, and specifying parameters for mai in 
heatmap, I get a printout of the mai parameters at the top of the pdf...see 
attachment.

This is on win2k pro with R2.2.1

Thanks,
Ken


From elvis at xlsolutions-corp.com  Thu Jan 12 22:22:02 2006
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 12 Jan 2006 14:22:02 -0700
Subject: [R] February course *** R/Splus Fundamentals and Programming
	Techniques
Message-ID: <20060112142202.9f08cc34deb45d78e54b3b5664e21546.ee19f75cd8.wbe@email.secureserver.net>


   XLSolutions Corporation ([1]www.xlsolutions-corp.com) is proud to
   announce  2-day "R/S-plus Fundamentals and Programming
   Techniques" in San Francisco: [2]www.xlsolutions-corp.com/Rfund.htm
   **** San Francisco,   February 16-17

   **** Seattle,            February 20-21

   **** Boston,            February 23-24

   **** New York,         February 27-28
   Reserve your seat now at the early bird rates! Payment due AFTER
   the class
   Course Description:
   This two-day beginner to intermediate R/S-plus course focuses on a
   broad spectrum of topics, from reading raw data to a comparison of R
   and S. We will learn the essentials of data manipulation, graphical
   visualization and R/S-plus programming. We will explore statistical
   data analysis tools,including graphics with data sets. How to enhance
   your plots, build your own packages (librairies) and connect via
   ODBC,etc.
   We will perform some statistical modeling and fit linear regression
   models. Participants are encouraged to bring data for interactive
   sessions
   With the following outline:
   - An Overview of R and S
   - Data Manipulation and Graphics
   - Using Lattice Graphics
   - A Comparison of R and S-Plus
   - How can R Complement SAS?
   - Writing Functions
   - Avoiding Loops
   - Vectorization
   - Statistical Modeling
   - Project Management
   - Techniques for Effective use of R and S
   - Enhancing Plots
   - Using High-level Plotting Functions
   - Building and Distributing Packages (libraries)
   - Connecting; ODBC, Rweb, Orca via sockets and via Rjava
   Email us for group discounts.
   Email Sue Turner: [3]sue at xlsolutions-corp.com
   Phone: 206-686-1578
   Visit us: [4]www.xlsolutions-corp.com/training.htm
   Please let us know if you and your colleagues are interested in this
   classto take advantage of group discount. Register now to secure your
   seat!
   Interested in R/Splus Advanced course? email us.
   Cheers,
   Elvis Miller, PhD
   Manager Training.
   XLSolutions Corporation
   206 686 1578
   [5]www.xlsolutions-corp.com
   [6]elvis at xlsolutions-corp.com

References

   1. http://www.xlsolutions-corp.com/
   2. http://www.xlsolutions-corp.com/Rfund.htm
   3. http://email.secureserver.net/view.php?folder=INBOX&uid=2791#Compose
   4. http://www.xlsolutions-corp.com/training.htm
   5. http://www.xlsolutions-corp.com/
   6. http://email.secureserver.net/view.php?folder=INBOX&uid=2791#Compose


From p.dalgaard at biostat.ku.dk  Thu Jan 12 22:22:08 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2006 22:22:08 +0100
Subject: [R] Basis of fisher.test
In-Reply-To: <XFMail.060112201906.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060112201906.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2slrt5egv.fsf@turmalin.kubism.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> I want to ascertain the basis of the table ranking,
> i.e. the meaning of "extreme", in Fisher's Exact Test
> as implemented in 'fisher.test', when applied to RxC
> tables which are larger than 2x2.
> 
> One can summarise a strategy for the test as
> 
> 1) For each table compatible with the margins
>    of the observed table, compute the probability
>    of this table conditional on the marginal totals.
> 
> 2) Rank the possible tables in order of a measure
>    of discrepancy between the table and the null
>    hypothesis of "no association".
> 
> 3) Locate the observed table, and compute the sum
>    of the probabilties, computed in (1), for this
>    table and more "extreme" tables in the sense of
>    the ranking in (2).
> 
> The question is: what "measure of discrepancy" is
> used in 'fisher.test' corresponding to stage (2)?
> 
> (There are in principle several possibilities, e.g.
> value of a Pearson chi-squared, large values being
> discrepant; the probability calculated in (2),
> small values being discrepant; ... )
> 
> "?fisher.test" says only:
> 
>      In the one-sided 2 by 2 cases, p-values are obtained
>      directly using the hypergeometric distribution.
>      Otherwise, computations are based on a C version of
>      the FORTRAN subroutine FEXACT which implements the
>      network developed by Mehta and Patel (1986) and
>      improved by Clarkson, Fan & Joe (1993). The FORTRAN
>      code can be obtained from
>      <URL: http://www.netlib.org/toms/643>.
> 
> I have had a look at this FORTRAN code, and cannot ascertain
> it from the code itself. However, there is a Comment to the
> effect:
> 
> c     PRE    - Table p-value.  (Output)
> c              PRE is the probability of a more extreme table, where
> c              'extreme' is in a probabilistic sense.
> 
> which suggests that the tables are ranked in order of their
> probabilities as computed in (2).
> 
> Can anyone confirm definitively what goes on?

To my knowledge, it is the "table probability", according to the
hypergeometric distribution, i.e. the probability of the table given
the marginals, which can be translated to sampling a+b balls without
replacement from a box with a+c white and b+d black balls. 

Playing around with dhyper should be instructive.

(You're right that the "two-sided" p values are obtained by summing
all smaller or equal table probabilities. This is the traditional way,
but there are alternatives, e.g. tail balancing.)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From sundar.dorai-raj at pdf.com  Thu Jan 12 22:32:41 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 12 Jan 2006 13:32:41 -0800
Subject: [R] bug with mai , pdf, and heatmap ?
In-Reply-To: <BAY101-F23AF6E4012B1E735D1023FE8270@phx.gbl>
References: <BAY101-F23AF6E4012B1E735D1023FE8270@phx.gbl>
Message-ID: <43C6CAF9.3050307@pdf.com>


Ken Termiso wrote:
> Hi all,
> 
> When using heatmap() with a pdf driver, and specifying parameters for 
> mai in heatmap, I get a printout of the mai parameters at the top of the 
> pdf...see attachment.
> 
> This is on win2k pro with R2.2.1
> 
> Thanks,
> Ken
> 
> 

Not a bug since ?heatmap has a "main" argument. Thus,

heatmap(..., mai = c(1,2,3,4))

is actually interpretted as

heatmap(..., main = c(1,2,3,4))

due to R's partial matching of the argument list. You should try:

pdf()
par(mai = c(1,2,3,4))
heatmap(...)
dev.off()

Hopefully I've assessed this correctly.

HTH,

--sundar

P.S. See the posting guide regarding attachments to the list.



From charles.edwin.white at us.army.mil  Thu Jan 12 22:51:46 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 12 Jan 2006 16:51:46 -0500
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
	2.1.1
Message-ID: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>

I blew away my existing R directory structure, reinstalled R 2.2.1 from
scratch, downloaded lme4 and associated packages from scratch, tried to
load lme4 and got the same error message. Again, I am using the Windows
version of R on XP.

Chuck

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.2.1  (2005-12-20 r36812)
ISBN 3-900051-07-0
....
> library(lme4)
Loading required package: Matrix
Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
        ReadItem: unknown type 241
In addition: Warning messages:
1: package 'lme4' was built under R version 2.3.0 
2: package 'Matrix' was built under R version 2.3.0 
Error: package 'Matrix' could not be loaded
>



From p.dalgaard at biostat.ku.dk  Thu Jan 12 22:56:27 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2006 22:56:27 +0100
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
	2.1.1
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <x2hd895cvo.fsf@turmalin.kubism.ku.dk>

"White, Charles E WRAIR-Wash DC" <charles.edwin.white at us.army.mil> writes:

> I blew away my existing R directory structure, reinstalled R 2.2.1 from
> scratch, downloaded lme4 and associated packages from scratch, tried to
> load lme4 and got the same error message. Again, I am using the Windows
> version of R on XP.

Which CRAN mirror? The Statlib one has been acting up lately.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From d.firth at warwick.ac.uk  Thu Jan 12 23:00:23 2006
From: d.firth at warwick.ac.uk (David Firth)
Date: Thu, 12 Jan 2006 22:00:23 +0000
Subject: [R] Firths bias correction for log-linear models
In-Reply-To: <200601122054.k0CKsATa039809@smtp.ucsd.edu>
References: <200601122054.k0CKsATa039809@smtp.ucsd.edu>
Message-ID: <c167c8efb2ef3ba35daf729c6bc9146f@warwick.ac.uk>

On 12 Jan 2006, at 20:54, sdfrost at ucsd.edu wrote:

> Dear R-Help List,
>
> I'm trying to implement Firth's (1993) bias correction for log-linear 
> models.
> Firth (1993) states that such a correction can be implemented by 
> supplementing
> the data with a function of h_i, the diagonals from the hat matrix, 
> but doesn't
> provide further details. I can see that for a saturated log-linear 
> model,  h_i=1
> for all i, hence one just adds 1/2 to each count, which is equivalent 
> to the
> Jeffrey's prior, but I'd also like to get bias corrected estimates for 
> other log
> linear models. It appears that I need to iterate using GLM, with the 
> weights
> option and h_i, which I can get from the function hatvalues. For 
> logistic
> regression, this can be performed by splitting up each observation 
> into response
> and nonresponse, and using weights as described in Heinze, G. and 
> Schemper, M.
> (2002), but I'm unsure of how to implement the analogue for log-linear 
> models. A
> procedure using IWLS is described by Firth (1992) in Dodge and 
> Whittaker (1992),
> but this book isn't in the local library, and its $141+ on Amazon. 
> I've tried
> looking at the code in the logistf and brlr libraries, but I haven't 
> had any
> (successful) ideas. Can anyone help me in describing how to implement 
> this in R?

I don't recommend the adjusted IWLS approach in practice, because that 
algorithm is only first-order convergent.  It is mainly of theoretical 
interest.

The brlr function (in the brlr package) provides a template for a more 
direct approach in practice.  The central operation there is an 
application of optim(), with objective function
  - (l + (0.5 * log(detinfo)))
in which l is the log likelihood and detinfo is the determinant of the 
Fisher information matrix.  In the case of a Poisson log-linear model, 
the Fisher information is, using standard GLM-type notation, t(X) %*% 
diag(mu) %*% X.  It is straightforward to differentiate this penalized 
log-likelihood function, so (as in brlr) derivatives can be supplied 
for use with a second-order convergent optim() algorithm such as BFGS 
(see ?optim for a reference on the algorithm).

I hope that helps.  Please feel free to contact me off the list if 
anything is unclear.

Kind regards,
David

--
Professor David Firth
http://www.warwick.ac.uk/go/dfirth



From fredrik.bg.lundgren at bredband.net  Thu Jan 12 23:17:16 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Thu, 12 Jan 2006 23:17:16 +0100
Subject: [R]  edit.data.frame
Message-ID: <000501c617c5$f29e88f0$4a9d72d5@Larissa>

Dear list,

Sometimes I have huge data.frames and the small spreadsheetlike 
edit.data.frame is quite handy to get an overview of the data. However, 
when I close the editor all data are rolled over the console window, 
which takes time and clutters the window. Is there a way to avoid this?

Fredrik



From jsteedle at stanford.edu  Thu Jan 12 23:26:06 2006
From: jsteedle at stanford.edu (Jeffrey T. Steedle)
Date: Thu, 12 Jan 2006 14:26:06 -0800
Subject: [R] Data with no separator
Message-ID: <000001c617c7$31514be0$d8bb40ab@jsteedle3>

I have data in which each row consists of a long string of number,
letters, symbols, and blank spaces.  I would like to simply scan in
strings of length 426, but R takes the spaces that occur in the data as
separators.  Is there any way around this?

Thanks,
Jeff Steedle 

-- 
Jeffrey T. Steedle (jsteedle at stanford.edu)
Psychological Studies in Education
Stanford University School of Education



From spencer.graves at pdf.com  Thu Jan 12 23:40:09 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 12 Jan 2006 14:40:09 -0800
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
 2.1.1
In-Reply-To: <x2hd895cvo.fsf@turmalin.kubism.ku.dk>
References: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>
	<x2hd895cvo.fsf@turmalin.kubism.ku.dk>
Message-ID: <43C6DAC9.7060604@pdf.com>

	  It worked fine for me using
http://cran.fhcrc.org/  Fred Hutchinson Cancer Research Center, Seattle, 
WA

	  spencer graves

Peter Dalgaard wrote:

> "White, Charles E WRAIR-Wash DC" <charles.edwin.white at us.army.mil> writes:
> 
> 
>>I blew away my existing R directory structure, reinstalled R 2.2.1 from
>>scratch, downloaded lme4 and associated packages from scratch, tried to
>>load lme4 and got the same error message. Again, I am using the Windows
>>version of R on XP.
> 
> 
> Which CRAN mirror? The Statlib one has been acting up lately.
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Jan 12 23:47:38 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jan 2006 22:47:38 -0000 (GMT)
Subject: [R] Data with no separator
In-Reply-To: <000001c617c7$31514be0$d8bb40ab@jsteedle3>
Message-ID: <XFMail.060112224738.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jan-06 Jeffrey T. Steedle wrote:
> I have data in which each row consists of a long string of number,
> letters, symbols, and blank spaces.  I would like to simply scan in
> strings of length 426, but R takes the spaces that occur in the data as
> separators.  Is there any way around this?
> 
> Thanks,
> Jeff Steedle 

You could use readLines(), perhaps?

  Data<-readLines("datafile")

should give you a vector Data of which each element is a
character string which is one line read from your datafile.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jan-06                                       Time: 22:46:39
------------------------------ XFMail ------------------------------



From charles.edwin.white at us.army.mil  Thu Jan 12 22:31:59 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 12 Jan 2006 16:31:59 -0500
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
	2.1.1
Message-ID: <8BAEC5E546879B4FAA536200A292C614E6A5DA@AMEDMLNARMC135.amed.ds.army.mil>


I made a typographical error, since I am running R 2.2.1. I wouldn't
dream of asking Professor Bates to make things backwards compatible to
an old version. Since the packages are loading fine for you, I will
assume there was a download error with the packages I have, uninstall
them, and try again.

Chuck

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Thursday, January 12, 2006 4:08 PM
To: White, Charles E WRAIR-Wash DC
Cc: r-help at stat.math.ethz.ch; Douglas Bates
Subject: Re: [R] CRAN versions of lme4/Matrix don't appear to work with
R 2.1.1

	  Did you try upgrading to R 2.2.1?  I just installed R 2.2.1
with the 
latest version of "lme4" and "Matrix", and they loaded fine for me.
....

	  Doug Bates has worked very hard to create this, and I for one
would 
not want to ask him to take the time to make it backward compatible for 
those who for whatever reason haven't yet upgraded to R 2.2.1, 
especially since for most of us it is fairly easy to upgrade.

	  Spencer Graves



From charles.edwin.white at us.army.mil  Thu Jan 12 23:42:42 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 12 Jan 2006 17:42:42 -0500
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with R
	2.1.1
References: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>
	<x2hd895cvo.fsf@turmalin.kubism.ku.dk>
Message-ID: <8BAEC5E546879B4FAA536200A292C6140DD14F@AMEDMLNARMC135.amed.ds.army.mil>

1) It was the Statlib mirror from which I was downloading. If I don't get any more interesting messages before I return to work in the morning, I'll try installing off of a different mirror.

2) On general principles, I just updated the lme4 related packages on the SuSE 10 machine that I run at home (NC mirror). That worked fine.

Chuck

-----Original Message-----
From: pd at pubhealth.ku.dk on behalf of Peter Dalgaard
Sent: Thu 1/12/2006 4:56 PM
....
Which CRAN mirror? The Statlib one has been acting up lately.
....



From manderse at nmsu.edu  Fri Jan 13 02:01:00 2006
From: manderse at nmsu.edu (Mark Andersen)
Date: Thu, 12 Jan 2006 18:01:00 -0700
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with
	R	2.1.1
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6140DD14F@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <00f801c617dc$d23d8950$29b27b80@MCANotebook>

Hi, all, 

This is interesting, since the problem I posted on a week ago was resolved
by downloading (the current versions of) R and ctv from the Austria site;
the UCLA site, from which I had downloaded before, had an old version of R
labeled as the current version, and it was not compatible with the version
of ctv that they had.

Moral of story: It's not just the Statlib site that is (or at least can be)
unreliable.

Is it common knowledge that there are other "mirrors" which do not provide a
very good reflection?

Regards,
Mark A.

Dr. Mark C. Andersen
Associate Professor
Department of Fishery and Wildlife Sciences
New Mexico State University
Las Cruces NM 88003-0003
phone: 505-646-8034
fax: 505-646-1281

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of White, Charles E
WRAIR-Wash DC
Sent: Thursday, January 12, 2006 3:43 PM
To: Peter Dalgaard
Cc: Douglas Bates; r-help at stat.math.ethz.ch; Spencer Graves
Subject: Re: [R] CRAN versions of lme4/Matrix don't appear to work with R
2.1.1

1) It was the Statlib mirror from which I was downloading. If I don't get
any more interesting messages before I return to work in the morning, I'll
try installing off of a different mirror.

2) On general principles, I just updated the lme4 related packages on the
SuSE 10 machine that I run at home (NC mirror). That worked fine.

Chuck

-----Original Message-----
From: pd at pubhealth.ku.dk on behalf of Peter Dalgaard
Sent: Thu 1/12/2006 4:56 PM
....
Which CRAN mirror? The Statlib one has been acting up lately.
....

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Fri Jan 13 02:06:28 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 13 Jan 2006 12:06:28 +1100
Subject: [R] R - Wikis and R-core
In-Reply-To: <20060112180836.C89DA31B@wmailp01.st2.lyceu.net>
References: <43BDC208.5060909@vanderbilt.edu>	<20060108160043.E66F345E@wmailp01.st2.lyceu.net>	<17347.47892.343299.844202@stat.math.ethz.ch>
	<20060112180836.C89DA31B@wmailp01.st2.lyceu.net>
Message-ID: <43C6FD14.6010000@maths.lth.se>

phgrosjean at sciviews.org wrote:
> Hello Martin and others,
> 
> I am happy with this decision. I'll look a little bit at this next week.
> 
> Best,
> 
> Philippe Grosjean
> 
> 
>>We've had a small "review time" within R-core on this topic,
>>amd would like to state the following:
>>
>>--------------------------------------------------------------------------
>>The R-core team welcomes proposals to develop an R-wiki.
>>
>>- We would consider linking a very small number of Wikis (ideally one)
>>  from www.r-project.org and offering an address in the r-project.org
>>domain (such as 'wiki.r-project.org').
>>
>>- The core team has no support time to offer, and would be looking for
>>  a medium-term commitment from a maintainer team for the Wiki(s).
>>
>>- Suggestions for the R documentation would best be filtered through the
>>
>>  Wiki maintainers, who could e.g. supply suggested patches during the
>>alpha  phase of an R release.
>>--------------------------------------------------------------------------
>>
>>Our main concerns have been about ensuring the quality of such extra
>>documentation projects, hence the 2nd point above.
>>Several of our more general, not mainly R, experiences have been
>>of outdated web pages which are continued to be used as
>>reference when their advice has long been superseded.
>>I think it's very important to try ensuring that this won't
>>happen with an R Wiki.

[Tried to send the following a few days ago, but had a problem with my 
connection:]

What about adding a "best before" date on Wiki pages and let moderators 
extend such dates (by a simple click).  If the date for a page is not 
updated, there will be a warning on that page telling the reader that 
the content might not be fully valid.

MediaWiki is a good solution because there you can write equations in 
LaTeX, which are generated as Math-ML(?) and/or bitmap images. This 
feature might be in other wiki system too, I don't know.

That's my $0.02

Henrik

>>Martin Maechler, ETH Zurich
>>
>>
>>>>>>>"PhGr" == Philippe Grosjean <phgrosjean at sciviews.org>
>>>>>>>    on Sun, 8 Jan 2006 17:00:44 +0100 (CET) writes:
>>
>>    PhGr> Hello all, Sorry for not taking part of this
>>    PhGr> discussion earlier, and for not answering Detlef
>>    PhGr> Steuer, Martin Maechler, and others that asked more
>>    PhGr> direct questions to me. I am away from my office and
>>    PhGr> my computer until the 16th of January.
>>
>>    PhGr> Just quick and partial answers: 1) I did not know
>>    PhGr> about Hamburg RWiki. But I would be happy to merge
>>    PhGr> both in one or the other way, as Detlef suggests it.
>>
>>    PhGr> 2) I choose DokuWiki as the best engine after a
>>    PhGr> careful comparison of various Wiki engines. It is the
>>    PhGr> best one, as far as I know, for the purpose of
>>    PhGr> writting software documentation and similar
>>    PhGr> pages. There is an extensive and clearly presented
>>    PhGr> comparison of many Wikki engines at:
>>    PhGr> http://www.wikimatrix.org/.
>>
>>    PhGr> 3) I started to change DokuWiki (addition of various
>>    PhGr> plugins, addition of R code syntax coloring with
>>    PhGr> GESHI, etc...). So, it goes well beyond all current
>>    PhGr> Wiki engines regarding its suitability to present R
>>    PhGr> stuff.
>>
>>    PhGr> 4) The reasons I did this is because I think the Wiki
>>    PhGr> format could be of a wider use. I plan to change a
>>    PhGr> little bit the DokuWiki syntax, so that it works with
>>    PhGr> plain .R code files (Wiki part is simply embedded in
>>    PhGr> commented lines, and the rest is recognized and
>>    PhGr> formatted as R code by the Wiki engine). That way, the
>>    PhGr> same Wiki document can either rendered by the Wiki
>>    PhGr> engine for a nice presentation, or sourced in R
>>    PhGr> indifferently.
>>
>>    PhGr> 5) My last idea is to add a Rpad engine to the Wiki,
>>    PhGr> so that one could play with R code presented in the
>>    PhGr> Wiki pages and see the effects of changes directly in
>>    PhGr> the Wiki.
>>
>>    PhGr> 6) Regarding the content of the Wiki, it should be
>>    PhGr> nice to propose to the authors of various existing
>>    PhGr> document to put them in a Wiki form. Something like
>>    PhGr> "Statistics with R"
>>    PhGr> (http://zoonek2.free.fr/UNIX/48_R/all.html) is written
>>    PhGr> in a way that stimulates additions to pages in
>>    PhGr> perpetual construction, if it was presented in a Wiki
>>    PhGr> form. It is licensed as Creative Commons
>>    PhGr> Attribution-NonCommercial-ShareAlike 2.5 license, that
>>    PhGr> is, exactly the same one as DokuWiki that I choose for
>>    PhGr> R Wiki. Of course, I plan to ask its author to do so
>>    PhGr> before putting its hundreds of very interesting pages
>>    PhGr> on the Wiki... I think it is vital to have already
>>    PhGr> something in the Wiki, in order to attract enough
>>    PhGr> readers, and then enough contributors!
>>
>>    PhGr> 7) Regarding spamming and vandalism, DokuWiki allows
>>    PhGr> to manage rights and users, even individually for
>>    PhGr> pages. I think it would be fine to lock pages that
>>    PhGr> reach a certain maturity (read-only / editable by
>>    PhGr> selected users only) , with link to a discussion page
>>    PhGr> which remaining freely accessible at the bottom of
>>    PhGr> locked pages.
>>
>>    PhGr> 8) I would be happy to contribute this work to the R
>>    PhGr> foundation in one way or the other to integrate it in
>>    PhGr> http://www.r-project.org or
>>    PhGr> http://cran.r-project.org. But if it is fine keeping
>>    PhGr> it in http://www.sciviews.org as well, it is also fine
>>    PhGr> for me.
>>
>>    PhGr> I suggest that all interested people drop a little
>>    PhGr> email to my mailbox.  I'll recontact you when I will
>>    PhGr> be back to my office to work on a more elaborate
>>    PhGr> solution altogether when I am back at my office.
>>    PhGr> Best,
>>
>>    PhGr> Philippe Grosjean
>>
>>    PhGr> ______________________________________________
>>    PhGr> R-help at stat.math.ethz.ch mailing list
>>    PhGr> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>>    PhGr> read the posting guide!
>>    PhGr> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Fri Jan 13 02:18:56 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 12 Jan 2006 17:18:56 -0800
Subject: [R] jointprior in deal package
In-Reply-To: <43BD09BE.6000307@oce.kuleuven.be>
References: <43BD09BE.6000307@oce.kuleuven.be>
Message-ID: <43C70000.5020603@pdf.com>

	  In case you haven't already solved this problem (I have seen no 
replies to this post), I will offer a suggestion.  First, I've never 
used the deal package.  I installed it and tried the example provided 
with the documentation for "jointprior".  It seemed to return something 
sensible -- certainly NOT an error message.

	  Have you considered making a local copy of "jointprior", then 
invoking 'debug(jointprior)', then walking trough the function line by 
line (as described in teh 'debug' documentation)?  If you do this, you 
will find exactly the place the command that generates the error 
message.  The 'debug' procedure also allows you to look at any object in 
the local environment created by "jointprior".  By doing this, you might 
get a better idea of the problem.

	  Another alternative would be to consider the differences between the 
example provided with the documentation and your "tor.nw".  You may be 
able to identify the problem from that.  If that failed, I would then 
try to modify "tor.nw" to produce the simplest possible example I could 
think of that would still produce the error message.  In the course of 
doing that, you may be able to resolve the issue.  If not, if you send 
your simplest possible example to this list, someone else might be able 
to help you.

	  A reproducible example is nearly always easier to diagnose than a 
relatively vague description like you provided.  To get an answer to the 
question that you asked, (a) your question must reach someone who has 
used the deal package and knows that specific error message and (b) that 
person must have the time and interest to respond.  The probability of 
that happening may be quite low.  By contrast, if you submit a toy 
example that doesn't quite work, anyone interested in playing a few 
minutes with the "deal" package can study your example and possibly take 
it to the next step.

	  In general, I believe that providing a simple reproducible example 
probably increases by a couple of orders of magnitude the odds of 
receiving a useful answer quickly.

	  hope this helps.
	  spencer graves
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html

Tim Smits wrote:

> Dear all,
> 
> I recently started using the deal package for learning Bayesian 
> networks. When using the jointprior function on a particular dataset, I 
> get the following message:
>  >tor.prior<-jointprior(tor.nw)
> Error in array(1, Dim) : 'dim' specifies too large an array
> 
> What is the problem? How can I resolve it?
> 
> Thanks,
> Tim
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jan 13 02:22:08 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 12 Jan 2006 17:22:08 -0800
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with	R
 2.1.1
In-Reply-To: <00f801c617dc$d23d8950$29b27b80@MCANotebook>
References: <00f801c617dc$d23d8950$29b27b80@MCANotebook>
Message-ID: <43C700C0.4010005@pdf.com>

	  I don't know, but I had a similar problem a few weeks ago with the 
one or both of the California sites (I don't remember which now).  The 
problems disappeared after I switched to "http://cran.fhcrc.org/" (Fred 
Hutchinson Cancer Research Center, Seattle, WA).

	  spencer graves

Mark Andersen wrote:

> Hi, all, 
> 
> This is interesting, since the problem I posted on a week ago was resolved
> by downloading (the current versions of) R and ctv from the Austria site;
> the UCLA site, from which I had downloaded before, had an old version of R
> labeled as the current version, and it was not compatible with the version
> of ctv that they had.
> 
> Moral of story: It's not just the Statlib site that is (or at least can be)
> unreliable.
> 
> Is it common knowledge that there are other "mirrors" which do not provide a
> very good reflection?
> 
> Regards,
> Mark A.
> 
> Dr. Mark C. Andersen
> Associate Professor
> Department of Fishery and Wildlife Sciences
> New Mexico State University
> Las Cruces NM 88003-0003
> phone: 505-646-8034
> fax: 505-646-1281
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of White, Charles E
> WRAIR-Wash DC
> Sent: Thursday, January 12, 2006 3:43 PM
> To: Peter Dalgaard
> Cc: Douglas Bates; r-help at stat.math.ethz.ch; Spencer Graves
> Subject: Re: [R] CRAN versions of lme4/Matrix don't appear to work with R
> 2.1.1
> 
> 1) It was the Statlib mirror from which I was downloading. If I don't get
> any more interesting messages before I return to work in the morning, I'll
> try installing off of a different mirror.
> 
> 2) On general principles, I just updated the lme4 related packages on the
> SuSE 10 machine that I run at home (NC mirror). That worked fine.
> 
> Chuck
> 
> -----Original Message-----
> From: pd at pubhealth.ku.dk on behalf of Peter Dalgaard
> Sent: Thu 1/12/2006 4:56 PM
> ....
> Which CRAN mirror? The Statlib one has been acting up lately.
> ....
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dhinds at sonic.net  Fri Jan 13 02:22:04 2006
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Fri, 13 Jan 2006 01:22:04 +0000 (UTC)
Subject: [R] hypothesis testing for rank-deficient linear models
References: <dq1oeq$1ri$1@sea.gmane.org>
Message-ID: <dq6vbs$vi0$1@sea.gmane.org>

dhinds at sonic.net wrote:
> Take the following example:

>     a <- rnorm(100)
>     b <- trunc(3*runif(100))
>     g <- factor(trunc(4*runif(100)),labels=c('A','B','C','D'))
>     y <- rnorm(100) + a + (b+1) * (unclass(g)+2)
...

Here's a cleaned-up function to compute estimable within-group effects
for rank deficient models.  For the above data, it could be invoked
as:

    > m <- lm(y~a+b*g, subset=(b==0 | g!='B'))
    > subgroup.effects(m, 'b',  g=c('A','B','C','D'))
      g Estimate  StdError  t.value      p.value
    1 A 2.779167 0.4190213  6.63252 4.722978e-09
    2 B       NA        NA       NA           NA
    3 C 4.572431 0.3074402 14.87258 6.226445e-24
    4 D 5.920809 0.3502251 16.90572 3.995266e-27

Again, I'm not sure whether this is a good approach, or whether there
is an easier way using existing R functions.  One problem is figuring
exactly which terms are not estimable from the available data.  My
hack using alias() is not satisfactory and I've already run into cases
where it fails.  But I'm having trouble coming up with a more general,
correct test?

-- David Hinds

--------------------

subgroup.effects <- function(model, term, ...)
{
    my.coef <- function(n)
    {
        contr <- lapply(names(args), function(i)
                        contr.treatment(args[[i]], unclass(gr[n,i])))
        names(contr) <- names(args)
        u <- update(model, formula=model$formula,
                    data=model$data, contrasts=contr)
        uc <- coef(summary(u))[term,]
        if (any(is.na(coef(u))) &&
            any(!is.na(alias(u)$Complete)))
            uc[1:4] <- NA
        uc
    }
    args <- list(...)
    gr <- expand.grid(...)
    d <- data.frame(gr, t(sapply(1:nrow(gr), my.coef)))
    names(d) <- c(names(gr),'Estimate','StdError','t.value','p.value')
    d
}



From ripley at stats.ox.ac.uk  Fri Jan 13 03:00:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 02:00:37 +0000 (GMT)
Subject: [R] Basis of fisher.test
In-Reply-To: <XFMail.060112201906.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060112201906.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0601130155450.6845@gannet.stats>

On Thu, 12 Jan 2006 Ted.Harding at nessie.mcc.ac.uk wrote:

> I want to ascertain the basis of the table ranking,
> i.e. the meaning of "extreme", in Fisher's Exact Test
> as implemented in 'fisher.test', when applied to RxC
> tables which are larger than 2x2.
>
> One can summarise a strategy for the test as
>
> 1) For each table compatible with the margins
>   of the observed table, compute the probability
>   of this table conditional on the marginal totals.
>
> 2) Rank the possible tables in order of a measure
>   of discrepancy between the table and the null
>   hypothesis of "no association".
>
> 3) Locate the observed table, and compute the sum
>   of the probabilties, computed in (1), for this
>   table and more "extreme" tables in the sense of
>   the ranking in (2).
>
> The question is: what "measure of discrepancy" is
> used in 'fisher.test' corresponding to stage (2)?
>
> (There are in principle several possibilities, e.g.
> value of a Pearson chi-squared, large values being
> discrepant; the probability calculated in (2),
> small values being discrepant; ... )
>
> "?fisher.test" says only:

[That following is not a quote from a current version of R.]

>     In the one-sided 2 by 2 cases, p-values are obtained
>     directly using the hypergeometric distribution.
>     Otherwise, computations are based on a C version of
>     the FORTRAN subroutine FEXACT which implements the
>     network developed by Mehta and Patel (1986) and
>     improved by Clarkson, Fan & Joe (1993). The FORTRAN
>     code can be obtained from
>     <URL: http://www.netlib.org/toms/643>.

No, it *also* says

      Two-sided tests are based on the probabilities of the tables, and
      take as 'more extreme' all tables with probabilities less than or
      equal to that of the observed table, the p-value being the sum of
      such probabilities.

which answers the question (there are only two-sided tests for such 
tables).

Now, what does the posting guide say about stating the R version and 
updating before posting?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jan 13 03:08:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 02:08:11 +0000 (GMT)
Subject: [R] edit.data.frame
In-Reply-To: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
Message-ID: <Pine.LNX.4.61.0601130207010.6845@gannet.stats>

On Thu, 12 Jan 2006, Fredrik Lundgren wrote:

> Sometimes I have huge data.frames and the small spreadsheetlike
> edit.data.frame is quite handy to get an overview of the data. However,
> when I close the editor all data are rolled over the console window,
> which takes time and clutters the window. Is there a way to avoid this?

If you mean printed to the R terminal/console, assign the result or use 
invisible(edit(object)).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ronggui.huang at gmail.com  Fri Jan 13 08:15:02 2006
From: ronggui.huang at gmail.com (ronggui)
Date: Fri, 13 Jan 2006 15:15:02 +0800
Subject: [R] edit.data.frame
In-Reply-To: <Pine.LNX.4.61.0601130207010.6845@gannet.stats>
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
	<Pine.LNX.4.61.0601130207010.6845@gannet.stats>
Message-ID: <38b9f0350601122315t32b97887x@mail.gmail.com>

I think fix(data.frame.name) is the best way .

2006/1/13, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> On Thu, 12 Jan 2006, Fredrik Lundgren wrote:
>
> > Sometimes I have huge data.frames and the small spreadsheetlike
> > edit.data.frame is quite handy to get an overview of the data. However,
> > when I close the editor all data are rolled over the console window,
> > which takes time and clutters the window. Is there a way to avoid this?
>
> If you mean printed to the R terminal/console, assign the result or use
> invisible(edit(object)).
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--

Deparment of Sociology
Fudan University



From aleszib at gmail.com  Fri Jan 13 08:04:03 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 13 Jan 2006 08:04:03 +0100
Subject: [R] Taking code from packages
Message-ID: <000101c6180f$912bca20$a7fdfea9@TAMARA>

Hello!

I am currently in the process of creating (my first) package, which (when
ready) I intend to publish to CRAN. In the process of creating this package
I have taken some code form existing packages. I have actually copied parts
of functions in to new functions. This code is usually something very basic
such as Rand index. What is the proper procedure for this?

Since most of R (and also the packages I have taken code form) is published
under GPL, I think this should be OK. However I do not know if:
1.	I should still ask authors of the packages for permission or at
least notify them.
2.	Ad references to the functions (and packages) from which I had taken
the code or only to the references they use.

What about regarding code that was sent to the list, usually as a response
to one of my problems. I assume that in this case it is best to consult the
author?

Any comments and opinions are very welcomed!

Best regards,
Ales Ziberna



From ripley at stats.ox.ac.uk  Fri Jan 13 09:03:50 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 08:03:50 +0000 (GMT)
Subject: [R] edit.data.frame
In-Reply-To: <38b9f0350601122315t32b97887x@mail.gmail.com>
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa> 
	<Pine.LNX.4.61.0601130207010.6845@gannet.stats>
	<38b9f0350601122315t32b97887x@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601130802590.13511@gannet.stats>

That changes the object if you make a change in the editor, so definitely 
NOT to be recommended.

On Fri, 13 Jan 2006, ronggui wrote:

> I think fix(data.frame.name) is the best way .
> 2006/1/13, Prof Brian Ripley <ripley at stats.ox.ac.uk>:> On Thu, 12 Jan 2006, Fredrik Lundgren wrote:>> > Sometimes I have huge data.frames and the small spreadsheetlike> > edit.data.frame is quite handy to get an overview of the data. However,> > when I close the editor all data are rolled over the console window,> > which takes time and clutters the window. Is there a way to avoid this?>> If you mean printed to the R terminal/console, assign the result or use> invisible(edit(object)).>>> --> Brian D. Ripley,                  ripley at stats.ox.ac.uk> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/> University of Oxford,             Tel:  +44 1865 272861 (self)> 1 South Parks Road,                     +44 1865 272866 (PA)> Oxford OX1 3TG, UK                Fax:  +44 1865 272595>> ______________________________________________> R-help at stat.math.ethz.ch mailing list> https://stat.ethz.ch/mailman/listinfo/r-help> PLEASE do read the posting guide! http://www.R-p!
> roject.org/posting-guide.html>
>
> --Deparment of SociologyFudan University
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ndurand at fr.abx.fr  Fri Jan 13 09:17:19 2006
From: ndurand at fr.abx.fr (ndurand@fr.abx.fr)
Date: Fri, 13 Jan 2006 09:17:19 +0100
Subject: [R] Curve fitting
Message-ID: <OFE015C5B3.2198BA97-ONC12570F5.002D54CD-C12570F5.002D921F@fr.abx.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/f2b6f350/attachment.pl

From d.firth at warwick.ac.uk  Fri Jan 13 09:21:24 2006
From: d.firth at warwick.ac.uk (David Firth)
Date: Fri, 13 Jan 2006 08:21:24 +0000
Subject: [R] edit.data.frame
In-Reply-To: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
Message-ID: <6CC52219-EA49-4762-884C-6D014E110FFA@warwick.ac.uk>

On 12 Jan 2006, at 22:17, Fredrik Lundgren wrote:

> Dear list,
>
> Sometimes I have huge data.frames and the small spreadsheetlike
> edit.data.frame is quite handy to get an overview of the data.  
> However,
> when I close the editor all data are rolled over the console window,
> which takes time and clutters the window. Is there a way to avoid  
> this?
>

An alternative to the editor is showData() from the relimp package.   
It is modeless, meaning that your data window can be left open/ 
minimized while you work in R.  I haven't tested it with _very_ large  
data frames though.

David

--
Professor David Firth
http://www.warwick.ac.uk/go/dfirth



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan 13 09:55:14 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 13 Jan 2006 08:55:14 -0000 (GMT)
Subject: [R] Basis of fisher.test
In-Reply-To: <Pine.LNX.4.61.0601130155450.6845@gannet.stats>
Message-ID: <XFMail.060113085514.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jan-06 Prof Brian Ripley wrote:
> On Thu, 12 Jan 2006 Ted.Harding at nessie.mcc.ac.uk wrote:
>>[...]
>> "?fisher.test" says only:
> 
> [That following is not a quote from a current version of R.]
> 
>>     In the one-sided 2 by 2 cases, p-values are obtained
>>     directly using the hypergeometric distribution.
>>     Otherwise, computations are based on a C version of
>>     the FORTRAN subroutine FEXACT which implements the
>>     network developed by Mehta and Patel (1986) and
>>     improved by Clarkson, Fan & Joe (1993). The FORTRAN
>>     code can be obtained from
>>     <URL: http://www.netlib.org/toms/643>.
> 
> No, it *also* says
> 
>       Two-sided tests are based on the probabilities of the tables, and
>       take as 'more extreme' all tables with probabilities less than or
>       equal to that of the observed table, the p-value being the sum of
>       such probabilities.
> 
> which answers the question (there are only two-sided tests for such 
> tables).

Thanks for the above information, which is indeed the definitive
straightforward answer to my question!

(Not sure that I quite agree with the "two-sided" terminology, though,
since the ranking is unidirectional based on decreasing probability,
and the P-value is that of the least-probability tail -- i.e. analagous
to the "large (-2*loglik)" tail of a likelihood-ratio test -- which
I've always visualised as a 1-tailed test (depite the fact that
the "other tail" can on occasion be indicative of a fit "too good to
be true").

> Now, what does the posting guide say about stating the R version and 
> updating before posting?

Well, I plead that in practice there is necessarily a grey area
here! My quotation was from "?fisher.test" in R-2.1.0beta of
2004/04/08, the most recent version installed on any of my machines.
Admittedly a bit behind the times, but not grossly; and that help
page has not changed in this respect since the earliest version I
have installed, which is R-1.2.3 of 2001/04/26.

Contents of help pages can change overnight as R evolves.
While it is better to be up-to-date than behind the times (even
slightly), there is a compromise to be struck between upgrading
to the latest R every time one has a question which might be
answered thereby, or going on-line to read the latest PDF
documentation from CRAN, on the one hand, and on the other asking
a straightforward question to the list.

Thanks again, and best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jan-06                                       Time: 08:55:11
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Fri Jan 13 10:44:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 09:44:48 +0000 (GMT)
Subject: [R] Basis of fisher.test
In-Reply-To: <XFMail.060113085514.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060113085514.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0601130940370.15459@gannet.stats>

On Fri, 13 Jan 2006 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 13-Jan-06 Prof Brian Ripley wrote:
>> On Thu, 12 Jan 2006 Ted.Harding at nessie.mcc.ac.uk wrote:
>>> [...]
>>> "?fisher.test" says only:
>>
>> [That following is not a quote from a current version of R.]
>>
>>>     In the one-sided 2 by 2 cases, p-values are obtained
>>>     directly using the hypergeometric distribution.
>>>     Otherwise, computations are based on a C version of
>>>     the FORTRAN subroutine FEXACT which implements the
>>>     network developed by Mehta and Patel (1986) and
>>>     improved by Clarkson, Fan & Joe (1993). The FORTRAN
>>>     code can be obtained from
>>>     <URL: http://www.netlib.org/toms/643>.
>>
>> No, it *also* says
>>
>>       Two-sided tests are based on the probabilities of the tables, and
>>       take as 'more extreme' all tables with probabilities less than or
>>       equal to that of the observed table, the p-value being the sum of
>>       such probabilities.
>>
>> which answers the question (there are only two-sided tests for such
>> tables).
>
> Thanks for the above information, which is indeed the definitive
> straightforward answer to my question!
>
> (Not sure that I quite agree with the "two-sided" terminology, though,
> since the ranking is unidirectional based on decreasing probability,
> and the P-value is that of the least-probability tail -- i.e. analagous
> to the "large (-2*loglik)" tail of a likelihood-ratio test -- which
> I've always visualised as a 1-tailed test (depite the fact that
> the "other tail" can on occasion be indicative of a fit "too good to
> be true").

As statistics is usually taught, significance tests are always one-tailed. 
The two-sided t-test is one-tailed, the test statistic being |T|.

In any case, the `two-sided' is part of the arguments given to the 
function, so this para is just using the already-established terminology.

>> Now, what does the posting guide say about stating the R version and
>> updating before posting?
>
> Well, I plead that in practice there is necessarily a grey area
> here! My quotation was from "?fisher.test" in R-2.1.0beta of
> 2004/04/08, the most recent version installed on any of my machines.
> Admittedly a bit behind the times, but not grossly; and that help
> page has not changed in this respect since the earliest version I
> have installed, which is R-1.2.3 of 2001/04/26.
>
> Contents of help pages can change overnight as R evolves.
> While it is better to be up-to-date than behind the times (even
> slightly), there is a compromise to be struck between upgrading
> to the latest R every time one has a question which might be
> answered thereby, or going on-line to read the latest PDF
> documentation from CRAN, on the one hand, and on the other asking
> a straightforward question to the list.

Well, if you had given the R version number the problem would have been 
much more obvious.

> Thanks again, and best wishes,
> Ted.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 13-Jan-06                                       Time: 08:55:11
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fredrik.bg.lundgren at bredband.net  Fri Jan 13 12:00:32 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Fri, 13 Jan 2006 12:00:32 +0100
Subject: [R] edit.data.frame - summary
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
	<6CC52219-EA49-4762-884C-6D014E110FFA@warwick.ac.uk>
Message-ID: <001a01c61830$92ff4ea0$4a9d72d5@Larissa>

Thanks to

David Firth
Brian Ripley
ronggui
jim holtman

who help with this question.

showData() from the relimp package works and shows the dataframe "as.is"
but scrolls rather weak with huge data.frames (15 000 x 120)

fix(...) is somewhat dangerous as you usually don't want to correct 
individual data but
get an overview

invisible(edit(...)) does the trick and scrolls with excellent speed 
even with huge dataframes
but not exactly "as.is" (dates as negative or positive integer, minor 
problem).

Fredrik



----- Original Message ----- 
From: "David Firth" <d.firth at warwick.ac.uk>
To: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, January 13, 2006 9:21 AM
Subject: Re: [R] edit.data.frame


> On 12 Jan 2006, at 22:17, Fredrik Lundgren wrote:
>
>> Dear list,
>>
>> Sometimes I have huge data.frames and the small spreadsheetlike
>> edit.data.frame is quite handy to get an overview of the data. 
>> However,
>> when I close the editor all data are rolled over the console window,
>> which takes time and clutters the window. Is there a way to avoid 
>> this?
>>
>
> An alternative to the editor is showData() from the relimp package. 
> It is modeless, meaning that your data window can be left open/ 
> minimized while you work in R.  I haven't tested it with _very_ large 
> data frames though.
>
> David
>
> --
> Professor David Firth
> http://www.warwick.ac.uk/go/dfirth
>



From joueg at tcd.ie  Fri Jan 13 12:14:54 2006
From: joueg at tcd.ie (gj)
Date: Fri, 13 Jan 2006 11:14:54 +0000
Subject: [R] first derivative of a time series
Message-ID: <1137150894.43c78bae59a4b@mymail.tcd.ie>

Hi,

I need to derive a time series that represents the first derivative of an
original time series. The function coefDeriv in the cyclones package seemed to
be the ticket, but I'm not sure if I am interpreting the output of the function
correctly...or even using the function correctly.

This is a snipbit of what I've been trying:
--------
library(cyclones)

## read in my 1-column of values, each line is a different time step
ts.table <- read.table("timeseries.1d")
ts.values <- ts.table[,1]

## first calculate coefficients to pass to coefDeriv
ts.coef <- coefFit(ts.values)

d.ts <- coefDeriv(ts.coef)
-------

However, when I try to look at d.ts$y.deriv, assumning that this will plot the
derivative I want, all the numbers are huge (on the order of e+02 to e+08) when
my original time series ranged from 0 to 970. Am I going about this the wrong
way? Or are there other tools that would lead me to estimating the derivative
of a time series?

Thanks in advance for any help!
g



From ernesto at ipimar.pt  Fri Jan 13 12:24:53 2006
From: ernesto at ipimar.pt (ernesto)
Date: Fri, 13 Jan 2006 11:24:53 +0000
Subject: [R] help with gepRglm::likfit.glsm
Message-ID: <43C78E05.1000900@ipimar.pt>

Hi,

I'm exploring likfit.glsm and I need some help. I have to say that I'm
not an MCMC expert ...

I did a first run of likfit.glsm with S.scale=0.002 and it worked
whithout problems but there was strong autocorrelation and the chain
convergence for the ramdom effects was quite poor, so I changed S.scale
to 0.4, which gave acceptance rates close to 0.6 as proposed on the
documentation, and the autocorrelation and chain convergence was ok.
However when I tried to run likfit.glsm it gave the following error:

> gdn.glsm2.lf <- likfit.glsm(gdn.glsm2.prelf, cov.model =
"exponential", ini.phi=26, lambda=0)
--------------------------------------------------------------------
likfit.glsm: likelihood maximisation using the function optim.
phi =  26 tausq.rel =  0
Error in if (det(Delta2) != 0) { : missing value where TRUE/FALSE needed
In addition: Warning message:
cannot use argument lambda with the given objects in mcmc.obj in:
likfit.glsm(gdn.glsm2.prelf, cov.model = "exponential", ini.phi = 26,

below is the code for both runs.

Thanks

EJ

# first run

mod <- list(beta=gdn.lf$beta, cov.pars=gdn.lf$cov.pars,
cov.model=gdn.lf$cov.model, nugget=gdn.lf$nugget,
aniso.pars=gdn.lf$aniso.pars, family="poisson", lambda=gdn.lf$lambda)

mcc <- mcmc.control(S.scale=0.002)
gdn.glsm1 <- glsm.mcmc(gdn, model=mod, mcmc.input=mcc)
gdn.glsm1.prelf <- prepare.likfit.glsm(gdn.glsm1)
gdn.glsm1.lf <- likfit.glsm(gdn.glsm1.prelf, cov.model = "exponential",
ini.phi=26, lambda = 0)

# second run

mcc <- mcmc.control(S.scale=0.4)
gdn.glsm2 <- glsm.mcmc(gdn, model=mod, mcmc.input=mcc)
gdn.glsm2.prelf <- prepare.likfit.glsm(gdn.glsm2)
gdn.glsm2.lf <- likfit.glsm(gdn.glsm2.prelf, cov.model = "exponential",
ini.phi=26, lambda=0)



From maechler at stat.math.ethz.ch  Fri Jan 13 12:24:51 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jan 2006 12:24:51 +0100
Subject: [R] "infinite recursion" in do.call when lme4 loaded only
In-Reply-To: <loom.20060112T191150-51@post.gmane.org>
References: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
	<x2psmxuzkw.fsf@viggo.kubism.ku.dk>
	<loom.20060112T191150-51@post.gmane.org>
Message-ID: <17351.36355.296014.525531@stat.math.ethz.ch>

>>>>> "Dieter" == Dieter Menne <dieter.menne at menne-biomed.de>
>>>>>     on Thu, 12 Jan 2006 18:14:32 +0000 (UTC) writes:

    Dieter> Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:
    >> > A larg program which worked with lme4/R about a year ago failed when I
    >> > re-run it today. I reproduced the problem with the program below.

    >> > -- When lme4 is loaded (but never used), the do.call fails
    >> >    with infinite recursion after 60 seconds. Memory used increases
    >> >    beyond bonds in task manager.
    >> 
    >> However, it surely has to do with methods dispatch:
    >> 
    >> > system.time(do.call("rbind.data.frame",caScore))
    >> [1] 0.99 0.00 0.99 0.00 0.00
    >> 
    >> which provides you with another workaround.

    Dieter> Peter, I had increased the optional value already, but I still don't understand 
    Dieter> what this recursion overflow has to do with the lm4 loading.

Aahh, you've hit a secret ;-)  no, but a semi-hidden feature:
lme4 loads Matrix and Matrix  activates versions of rbind() and
cbind() which use rbind2/cbind2 which are S4 generics and
default methods that are slightly different than then the
original base rbind() and cbind(). 
This was a necessity since the original rbind(), cbind() have
first argument "...", i.e. an invalid signature for S4 method
dispatch.

This was in NEWS for R 2.2.0 :

    o	Experimental versions of cbind() and rbind() in methods package,
	based on new generic function cbind2(x,y) and rbind2().	 This will
	allow the equivalent of S4 methods for cbind() and rbind() ---
	currently only after an explicit activation call, see ?cbind2.

And 'Matrix' uses the activation call in its .OnLoad hook.
This is now getting much too technical to explain for R-help, so
if we want to go there, we should move this topic to R-devel,
and I'd like to do so, and will be glad if you can provide more
details on how exactly you're using rbind.

Martin Maechler,
ETH Zurich



From maechler at stat.math.ethz.ch  Fri Jan 13 12:42:34 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jan 2006 12:42:34 +0100
Subject: [R] "infinite recursion" in do.call when lme4 loaded only
In-Reply-To: <17351.36355.296014.525531@stat.math.ethz.ch>
References: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
	<x2psmxuzkw.fsf@viggo.kubism.ku.dk>
	<loom.20060112T191150-51@post.gmane.org>
	<17351.36355.296014.525531@stat.math.ethz.ch>
Message-ID: <17351.37418.31624.869070@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Fri, 13 Jan 2006 12:24:51 +0100 writes:

>>>>> "Dieter" == Dieter Menne <dieter.menne at menne-biomed.de>
>>>>>     on Thu, 12 Jan 2006 18:14:32 +0000 (UTC) writes:

    Dieter> Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:
    >>> > A larg program which worked with lme4/R about a year ago failed when I
    >>> > re-run it today. I reproduced the problem with the program below.

    >>> > -- When lme4 is loaded (but never used), the do.call fails
    >>> >    with infinite recursion after 60 seconds. Memory used increases
    >>> >    beyond bonds in task manager.
    >>> 
    >>> However, it surely has to do with methods dispatch:
    >>> 
    >>> > system.time(do.call("rbind.data.frame",caScore))
    >>> [1] 0.99 0.00 0.99 0.00 0.00
    >>> 
    >>> which provides you with another workaround.

    Dieter> Peter, I had increased the optional value already, but I still don't understand 
    Dieter> what this recursion overflow has to do with the lm4 loading.

    MM> Aahh, you've hit a secret ;-)  no, but a semi-hidden feature:
    MM> lme4 loads Matrix and Matrix  activates versions of rbind() and
    MM> cbind() which use rbind2/cbind2 which are S4 generics and
    MM> default methods that are slightly different than then the
    MM> original base rbind() and cbind(). 
    MM> This was a necessity since the original rbind(), cbind() have
    MM> first argument "...", i.e. an invalid signature for S4 method
    MM> dispatch.

    MM> This was in NEWS for R 2.2.0 :

    MM> o	Experimental versions of cbind() and rbind() in methods package,
    MM> based on new generic function cbind2(x,y) and rbind2().	 This will
    MM> allow the equivalent of S4 methods for cbind() and rbind() ---
    MM> currently only after an explicit activation call, see ?cbind2.

    MM> And 'Matrix' uses the activation call in its .OnLoad hook.
    MM> This is now getting much too technical to explain for R-help, so
    MM> if we want to go there, we should move this topic to R-devel,
    MM> and I'd like to do so, and will be glad if you can provide more
    MM> details on how exactly you're using rbind.

One thing -- very useful for you -- I forgot to add:

You can easily quickly revert the  "other cbind/rbind
activation" by using

    methods:::bind_activation(FALSE)

so you don't need to unload lme4 or Matrix,  and you can
reactivate them again after your special computation by

    methods:::bind_activation(on = TRUE)

Martin



From nood at ext.sir.no  Fri Jan 13 12:49:54 2006
From: nood at ext.sir.no (=?ISO-8859-1?Q?Oddmund_Nordg=C3=A5rd?=)
Date: Fri, 13 Jan 2006 12:49:54 +0100 (CET)
Subject: [R] Scientific notation in plots
Message-ID: <Pine.LNX.4.56.0601131247300.25418@dna.sir.no>


Is it possible to use scientific notation of numbers on the axis of plots
without using the xEy notation. That means: a beatiful 1x10^3 instead of 1E3.
Logarithmic scale, in my case.

Thank you very much!

Oddmund

******************************************

  Oddmund Nordg??rd

  Department of Haematology and Oncology
  Stavanger University Hospital
  P.O. Box 8100
  4068 STAVANGER
  Phone: 51 51 89 34
  Email: nood at ext.sir.no



From ripley at stats.ox.ac.uk  Fri Jan 13 12:49:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 11:49:30 +0000 (GMT)
Subject: [R] edit.data.frame - summary
In-Reply-To: <001a01c61830$92ff4ea0$4a9d72d5@Larissa>
References: <000501c617c5$f29e88f0$4a9d72d5@Larissa>
	<6CC52219-EA49-4762-884C-6D014E110FFA@warwick.ac.uk>
	<001a01c61830$92ff4ea0$4a9d72d5@Larissa>
Message-ID: <Pine.LNX.4.61.0601131144440.17301@gannet.stats>

On Fri, 13 Jan 2006, Fredrik Lundgren wrote:

> Thanks to
>
> David Firth
> Brian Ripley
> ronggui
> jim holtman
>
> who help with this question.
>
> showData() from the relimp package works and shows the dataframe "as.is"
> but scrolls rather weak with huge data.frames (15 000 x 120)
>
> fix(...) is somewhat dangerous as you usually don't want to correct
> individual data but
> get an overview
>
> invisible(edit(...)) does the trick and scrolls with excellent speed
> even with huge dataframes

It has been tested on tens of thousands of columns.

> but not exactly "as.is" (dates as negative or positive integer, minor
> problem).

Well, actually this is `as.is' and showData is not (and that _is_ 
documented on the help page).  If you want to see the printed 
representation, call invisible(edit(format(your_df)))


A non-modal read-only version of the dataeditors would be a nice 
programming exercise.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From RRoa at fisheries.gov.fk  Fri Jan 13 12:00:23 2006
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Fri, 13 Jan 2006 09:00:23 -0200
Subject: [R] help with gepRglm::likfit.glsm
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC6CC@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of ernesto
> Sent:	Friday, January 13, 2006 9:25 AM
> To:	Mailing List R
> Subject:	[R] help with gepRglm::likfit.glsm
> 
> Hi,
> 
> I'm exploring likfit.glsm and I need some help. I have to say that I'm
> not an MCMC expert ...
> 
> I did a first run of likfit.glsm with S.scale=0.002 and it worked
> whithout problems but there was strong autocorrelation and the chain
> convergence for the ramdom effects was quite poor, so I changed S.scale
> to 0.4, which gave acceptance rates close to 0.6 as proposed on the
> documentation, and the autocorrelation and chain convergence was ok.
> However when I tried to run likfit.glsm it gave the following error:
> 
> > gdn.glsm2.lf <- likfit.glsm(gdn.glsm2.prelf, cov.model =
> "exponential", ini.phi=26, lambda=0)
> --------------------------------------------------------------------
> likfit.glsm: likelihood maximisation using the function optim.
> phi =  26 tausq.rel =  0
> Error in if (det(Delta2) != 0) { : missing value where TRUE/FALSE needed
> In addition: Warning message:
> cannot use argument lambda with the given objects in mcmc.obj in:
> likfit.glsm(gdn.glsm2.prelf, cov.model = "exponential", ini.phi = 26,
-------------
Regarding the warning: the argument 'lambda' corresponds to the model for 
continuous variables (used in geoR) whereas you seem to have counts.
The error has occurred to me when i try to fit a spatial correlation function 
that the data does not support. For example when i try to fit the matern model
with too few observations. Then i fall back into exponential or gaussian and
then the error disappears. Try fitting the gaussian. Also try changing the initial
values for the correlation distance.
Ruben



From mbibo at aanet.com.au  Fri Jan 13 13:08:00 2006
From: mbibo at aanet.com.au (Michael Bibo)
Date: Fri, 13 Jan 2006 12:08:00 +0000 (UTC)
Subject: [R] R and filemaker pro DB
Message-ID: <loom.20060113T124207-85@post.gmane.org>

I have been looking for a database application to use in conjunction with R (on
a Windows network).  When I approached my organization's IT department to ask
about using MySQL, they made a counter-offer of Filemaker Pro (v8).  It is not
specifically mentioned in 'R Data Import/Export', nor do searches of the
archives turn up much information.

Does anyone have any experience using Filemaker Pro with R?
Should package RODBC work with Filemaker, as it is odbc-compliant? (I will be
doing some trials anyway).
Any other relevant advice welcome.

Michael Bibo
Queensland Health
michael_bibo at health.qld.gov.au



From murdoch at stats.uwo.ca  Fri Jan 13 13:46:08 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 13 Jan 2006 07:46:08 -0500
Subject: [R] Taking code from packages
In-Reply-To: <000101c6180f$912bca20$a7fdfea9@TAMARA>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
Message-ID: <43C7A110.10808@stats.uwo.ca>

On 1/13/2006 2:04 AM, Ales Ziberna wrote:
> Hello!
> 
> I am currently in the process of creating (my first) package, which (when
> ready) I intend to publish to CRAN. In the process of creating this package
> I have taken some code form existing packages. I have actually copied parts
> of functions in to new functions. This code is usually something very basic
> such as Rand index. What is the proper procedure for this?
> 
> Since most of R (and also the packages I have taken code form) is published
> under GPL, I think this should be OK. However I do not know if:
> 1.	I should still ask authors of the packages for permission or at
> least notify them.

It is polite to notify them.

> 2.	Ad references to the functions (and packages) from which I had taken
> the code or only to the references they use.

The GPL requires that you maintain their copyright notices.  You have 
the right to use their work, the GPL doesn't give you ownership of it.
The usual way to do this is to leave their copyright notice intact, and 
add your own if you have made modifications.

An alternative which is usually (but not always) better is to say that 
your package depends on theirs, and then just use their functions.  The 
advantage is that it avoids any of the above mixed copyright issues, and 
it makes sure that when the author fixes a bug, you benefit too.  The 
disadvantage is that it makes your package dependent on theirs, so if 
changes are needed in it for some future version of R, you'll have to 
wait for the other maintainer to do them (or copy their code at that point).

(I'm a little sensitive about dependencies now, since the LaTeX seminar 
template I've used a few times no longer works.  It depends on too many 
LaTeX packages, and someone, somewhere has introduced incompatibilities 
in them.  Seems like I'll be forced to use Powerpoint or Impress.)

Duncan Murdoch

> 
> What about regarding code that was sent to the list, usually as a response
> to one of my problems. I assume that in this case it is best to consult the
> author?

> Any comments and opinions are very welcomed!
> 
> Best regards,
> Ales Ziberna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fezzi at stat.unibo.it  Fri Jan 13 13:50:04 2006
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Fri, 13 Jan 2006 13:50:04 +0100 (CET)
Subject: [R] multivariate markov switching
Message-ID: <2482430.1137156604816.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear helpers,

Does anyone know about a package or a function that allows to estimate
Multivariate Markov-Switching Models, like MS-VAR as introduced by
Krolzig(1997) with R ?

Thanks a lot!!

Carlo



From christian.neumann at ai.wu-wien.ac.at  Fri Jan 13 14:05:07 2006
From: christian.neumann at ai.wu-wien.ac.at (Christian Neumann)
Date: Fri, 13 Jan 2006 14:05:07 +0100
Subject: [R] Getting the numeric value of difftime
Message-ID: <43C7A583.6010308@ai.wu-wien.ac.at>

Hi folks,

I have a small, maybe newbie, question concerning date operations.
The follwing snippet

date1 = "2005-11-20";
date2 = "2005-11-17";
difftime(date1, date2)

results in "Time difference of 3 days". How can I extract just the 
numerical value 3?
As a workaround I apply mean() on the result which gives me just a 
numerical value.

Thanks a lot,

Christian



From iaingallagher at btopenworld.com  Fri Jan 13 14:08:53 2006
From: iaingallagher at btopenworld.com (IAIN GALLAGHER)
Date: Fri, 13 Jan 2006 13:08:53 +0000 (GMT)
Subject: [R] Scientific notation in plots
In-Reply-To: <Pine.LNX.4.56.0601131247300.25418@dna.sir.no>
Message-ID: <20060113130853.61785.qmail@web86703.mail.ukl.yahoo.com>

try something like this:

>axis(side=2, at=c(0, 5e+5, 1.5e+6, 2.5e+6),
labels=expression(0, 5%*%10^5, 1.5%*%10^6, 2.5%*%10^6)

it will place your 5e+5, 1.5e+6, 2.5e+6 as scientific
notation at the correct positions on the y axis. the
key is the "labels=expression...." part.

Cheers

iain

--- Oddmund Nordg????rd <nood at ext.sir.no> wrote:

> 
> Is it possible to use scientific notation of numbers
> on the axis of plots
> without using the xEy notation. That means: a
> beatiful 1x10^3 instead of 1E3.
> Logarithmic scale, in my case.
> 
> Thank you very much!
> 
> Oddmund
> 
> ******************************************
> 
>   Oddmund Nordg??rd
> 
>   Department of Haematology and Oncology
>   Stavanger University Hospital
>   P.O. Box 8100
>   4068 STAVANGER
>   Phone: 51 51 89 34
>   Email: nood at ext.sir.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From I.Visser at uva.nl  Fri Jan 13 14:09:07 2006
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 13 Jan 2006 14:09:07 +0100
Subject: [R] multivariate markov switching
In-Reply-To: <2482430.1137156604816.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <BFED6503.C277%I.Visser@uva.nl>

I don't know the MS-VAR model. The graphical models page on the r-project
site refers to a number of packages that deal with these types of models of
which I don't know the specifics. The depmix package does multivariate
hidden markov models.
best, ingmar

> From: Carlo Fezzi <fezzi at stat.unibo.it>
> Organization: Dip. Scienze Statistiche - Univ. di Bologna
> Date: Fri, 13 Jan 2006 13:50:04 +0100 (CET)
> To: r-help at stat.math.ethz.ch
> Subject: [R] multivariate markov switching
> 
> Dear helpers,
> 
> Does anyone know about a package or a function that allows to estimate
> Multivariate Markov-Switching Models, like MS-VAR as introduced by
> Krolzig(1997) with R ?
> 
> Thanks a lot!!
> 
> Carlo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Fri Jan 13 14:16:21 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 13 Jan 2006 13:16:21 +0000
Subject: [R] Getting the numeric value of difftime
In-Reply-To: <43C7A583.6010308@ai.wu-wien.ac.at>
References: <43C7A583.6010308@ai.wu-wien.ac.at>
Message-ID: <43C7A825.9040004@lancaster.ac.uk>

Christian Neumann wrote:
> Hi folks,
> 
> I have a small, maybe newbie, question concerning date operations.
> The follwing snippet
> 
> date1 = "2005-11-20";
> date2 = "2005-11-17";
> difftime(date1, date2)
> 
> results in "Time difference of 3 days". How can I extract just the 
> numerical value 3?
> As a workaround I apply mean() on the result which gives me just a 
> numerical value.

  If you want to get one thing _as_ another the R-ish solution is nearly 
always to do as.another(oneThing).

  You want it numeric? You got it!

 > date1 = "2005-11-20";
 > date2 = "2005-11-17";
 > difftime(date1, date2)
Time difference of 3 days
 > as.numeric(difftime(date1, date2))
[1] 3

Barry



From ggrothendieck at gmail.com  Fri Jan 13 14:17:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jan 2006 08:17:15 -0500
Subject: [R] Getting the numeric value of difftime
In-Reply-To: <43C7A583.6010308@ai.wu-wien.ac.at>
References: <43C7A583.6010308@ai.wu-wien.ac.at>
Message-ID: <971536df0601130517r4a19e06fv16bcf86f8def154@mail.gmail.com>

This was just discussed two days ago:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-January/084453.html

On 1/13/06, Christian Neumann <christian.neumann at ai.wu-wien.ac.at> wrote:
> Hi folks,
>
> I have a small, maybe newbie, question concerning date operations.
> The follwing snippet
>
> date1 = "2005-11-20";
> date2 = "2005-11-17";
> difftime(date1, date2)
>
> results in "Time difference of 3 days". How can I extract just the
> numerical value 3?
> As a workaround I apply mean() on the result which gives me just a
> numerical value.
>
> Thanks a lot,
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From charles.edwin.white at us.army.mil  Fri Jan 13 14:14:44 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Fri, 13 Jan 2006 08:14:44 -0500
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with	R
	2.1.1
Message-ID: <8BAEC5E546879B4FAA536200A292C614E6A681@AMEDMLNARMC135.amed.ds.army.mil>

After downloading from the WA mirror, the lme4 related packages appear
to run fine . Thanks for everyone's help.

Chuck



From dieter.menne at menne-biomed.de  Fri Jan 13 14:23:19 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 13 Jan 2006 13:23:19 +0000 (UTC)
Subject: [R] [Rd] "infinite recursion" in do.call when lme4 loaded only
References: <LPEJLJACLINDNMBMFAFIGEOBCBAA.dieter.menne@menne-biomed.de>
	<x2psmxuzkw.fsf@viggo.kubism.ku.dk>
	<loom.20060112T191150-51@post.gmane.org>
	<17351.36355.296014.525531@stat.math.ethz.ch>
	<17351.37418.31624.869070@stat.math.ethz.ch>
Message-ID: <loom.20060113T141411-544@post.gmane.org>

Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

> One thing -- very useful for you -- I forgot to add:
> 
> You can easily quickly revert the  "other cbind/rbind
> activation" by using
> 
>     methods:::bind_activation(FALSE)
> 
> so you don't need to unload lme4 or Matrix,  and you can
> reactivate them again after your special computation by
> 
>     methods:::bind_activation(on = TRUE)

Do haut's di hi... Looks like we need a non-recursive replacement for
the elegant do.call("rbind"..) constructs.

Dieter



From dimitris.rizopoulos at med.kuleuven.be  Fri Jan 13 14:26:12 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 13 Jan 2006 14:26:12 +0100
Subject: [R] Getting the numeric value of difftime
References: <43C7A583.6010308@ai.wu-wien.ac.at>
Message-ID: <00f501c61844$ec69d780$0540210a@www.domain>

just try

as.numeric(difftime(date1, date2))


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christian Neumann" <christian.neumann at ai.wu-wien.ac.at>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 13, 2006 2:05 PM
Subject: [R] Getting the numeric value of difftime


> Hi folks,
>
> I have a small, maybe newbie, question concerning date operations.
> The follwing snippet
>
> date1 = "2005-11-20";
> date2 = "2005-11-17";
> difftime(date1, date2)
>
> results in "Time difference of 3 days". How can I extract just the
> numerical value 3?
> As a workaround I apply mean() on the result which gives me just a
> numerical value.
>
> Thanks a lot,
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From nood at ext.sir.no  Fri Jan 13 14:28:10 2006
From: nood at ext.sir.no (=?ISO-8859-1?Q?Oddmund_Nordg=C3=A5rd?=)
Date: Fri, 13 Jan 2006 14:28:10 +0100 (CET)
Subject: [R] Scientific notation in plots
In-Reply-To: <20060113130853.61785.qmail@web86703.mail.ukl.yahoo.com>
References: <20060113130853.61785.qmail@web86703.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.56.0601131426150.25516@dna.sir.no>


Just what I needed!

Thank you very much!

Oddmund

On Fri, 13 Jan 2006, IAIN GALLAGHER wrote:

> try something like this:
>
> >axis(side=2, at=c(0, 5e+5, 1.5e+6, 2.5e+6),
> labels=expression(0, 5%*%10^5, 1.5%*%10^6, 2.5%*%10^6)
>
> it will place your 5e+5, 1.5e+6, 2.5e+6 as scientific
> notation at the correct positions on the y axis. the
> key is the "labels=expression...." part.
>
> Cheers
>
> iain
>
> --- Oddmund Nordg????rd <nood at ext.sir.no> wrote:
>
> >
> > Is it possible to use scientific notation of numbers
> > on the axis of plots
> > without using the xEy notation. That means: a
> > beatiful 1x10^3 instead of 1E3.
> > Logarithmic scale, in my case.
> >
> > Thank you very much!
> >
> > Oddmund
> >
> > ******************************************
> >
> >   Oddmund Nordg??rd
> >
> >   Department of Haematology and Oncology
> >   Stavanger University Hospital
> >   P.O. Box 8100
> >   4068 STAVANGER
> >   Phone: 51 51 89 34
> >   Email: nood at ext.sir.no
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From ggrothendieck at gmail.com  Fri Jan 13 14:27:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jan 2006 08:27:02 -0500
Subject: [R] Taking code from packages
In-Reply-To: <000101c6180f$912bca20$a7fdfea9@TAMARA>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
Message-ID: <971536df0601130527j41aca954n38d7441e457aa3a@mail.gmail.com>

I usually place in my mypackage-package.Rd file a pointer
to the existence of a THANKS file and then put the THANKS
file in the inst directory (which gets copied to the top level
directory by the build tools during the build).  For example,
after installing dyn try:

   library(dyn)
   package?dyn

for instructions on accessing the THANKS file from within R.

On 1/13/06, Ales Ziberna <aleszib at gmail.com> wrote:
> Hello!
>
> I am currently in the process of creating (my first) package, which (when
> ready) I intend to publish to CRAN. In the process of creating this package
> I have taken some code form existing packages. I have actually copied parts
> of functions in to new functions. This code is usually something very basic
> such as Rand index. What is the proper procedure for this?
>
> Since most of R (and also the packages I have taken code form) is published
> under GPL, I think this should be OK. However I do not know if:
> 1.      I should still ask authors of the packages for permission or at
> least notify them.
> 2.      Ad references to the functions (and packages) from which I had taken
> the code or only to the references they use.
>
> What about regarding code that was sent to the list, usually as a response
> to one of my problems. I assume that in this case it is best to consult the
> author?
>
> Any comments and opinions are very welcomed!
>
> Best regards,
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bxc at steno.dk  Fri Jan 13 14:34:21 2006
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 13 Jan 2006 14:34:21 +0100
Subject: [R] Course in Statistical Practise in Epidemiology using R
Message-ID: <40D3930AC1C8EA469E39536E5BC80835018E798B@EXDKBA021.corp.novocorp.net>

Course in
STATISTICAL PRACTICE IN EPIDEMIOLOGY USING R
============================================
Tartu, Estonia, Thursday 8  - Tuesday 13 June 2006

The course is aimed at epidemiologists and statisticians who wish to
use R for statistical modelling and analysis of epidemiological data.
The course requires basic knowledge of epidemiological concepts and
study types. These will only be briefly reviewed, whereas the more
advanced epidemiological and statistical concepts will be treated in
depth.

Contents:
---------
History of R. Language. Objects. Functions. 
Interface to other dataformats. Dataframes. 
Classical methods: Mantel-Haenszel etc.
Tabulation of data. 
Logistic regression for case-control-studies. 
Poisson regression for follow-up studies. 
Parametrization of models. 
Causal inference.
Graphics in R. 
Graphical reporting of results. 
Time-splitting & SMR. 
Survival analysis in continuous time. 
Parametric survival models. 
Interval censoring. 
Nested and matched case-control studies. 
Case-cohort studies. 
Competing risk models. 
Multistage models. 
Bootstrap and simulation.

The methods will be illustrated using R in practical exercises.  

The Epi package for epidemiological analysis in R will be introduced.

Participants are required to have a fairly good understanding of
statistical principles and some familiarity with epidemiological
concepts. The course will be mainly practically oriented with more
than half the time at the computer.

Price: 500 EUR. (250 EUR for cuntries outside EU-2003 and the like).

Application deadline: 15 April 2006.

Further information at: www.biostat.ku.dk/~bxc/SPE

------------------------------------------------------
Krista Fischer, University of Tartu, Estonia
Esa L????r??, University of Oulu, Finland
Bendix Carstensen, Steno Diabetes Center, Denmark 

Organizers



From andy_liaw at merck.com  Fri Jan 13 14:40:56 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Jan 2006 08:40:56 -0500
Subject: [R] Data with no separator
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6FA@usctmx1106.merck.com>

Perhaps read.fwf() can be of use...

Andy

From: Jeffrey T. Steedle
> 
> I have data in which each row consists of a long string of number,
> letters, symbols, and blank spaces.  I would like to simply scan in
> strings of length 426, but R takes the spaces that occur in 
> the data as
> separators.  Is there any way around this?
> 
> Thanks,
> Jeff Steedle 
> 
> -- 
> Jeffrey T. Steedle (jsteedle at stanford.edu)
> Psychological Studies in Education
> Stanford University School of Education
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From edd at debian.org  Fri Jan 13 02:08:16 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 12 Jan 2006 19:08:16 -0600
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with
	R	2.1.1
In-Reply-To: <8BAEC5E546879B4FAA536200A292C6140DD14F@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614E6A5EA@AMEDMLNARMC135.amed.ds.army.mil>
	<x2hd895cvo.fsf@turmalin.kubism.ku.dk>
	<8BAEC5E546879B4FAA536200A292C6140DD14F@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <17350.64896.187835.796591@basebud.nulle.part>


On 12 January 2006 at 17:42, White, Charles E WRAIR-Wash DC wrote:
| 1) It was the Statlib mirror from which I was downloading. If I don't get any more interesting messages before I return to work in the morning, I'll try installing off of a different mirror.

IIRC we had repeated 'bug reports' from users of Statlib. That mirror seems
to not value internal consistency too highly. Maybe it is aiming to become a
'randomized mirror' ?  Just kidding...

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From bolker at zoo.ufl.edu  Fri Jan 13 14:48:53 2006
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 13 Jan 2006 13:48:53 +0000 (UTC)
Subject: [R] Curve fitting
References: <OFE015C5B3.2198BA97-ONC12570F5.002D54CD-C12570F5.002D921F@fr.abx.fr>
Message-ID: <loom.20060113T144547-967@post.gmane.org>

 <ndurand <at> fr.abx.fr> writes:

> 
> I made a mistake in my equations : all the logarithms are neperian!
> 
> ----- Rachemin par Nadege ND Durand/RD/abx/FR le 01/13/2006 09:15 -----
> 

  it doesn't matter; all the basic issues raised by me and
by Albyn Jones still apply.  If we are going to help you,
you need to provide more detail about what you tried in R
and more background on what you're trying to achieve in general.
As we both said, fitting a 5-parameter model to 5 data
points is nearly impossible.  If you provide more detail
about your data and your broader goal, we might be able
to help.

  Ben Bolker



From Robert.McGehee at geodecapital.com  Fri Jan 13 15:27:08 2006
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 13 Jan 2006 09:27:08 -0500
Subject: [R] Scientific notation in plots
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C94674F@MSGBOSCLB2WIN.DMN1.FMR.COM>

You could also use a function like this to transform a number x into your "beautiful" notation, where x is the number and digits is the number of digits you would like to see after the decimal. Then you could use the axis and labels syntax suggested by Iain, as such:

sciNotation <- function(x, digits = 1) {
    if (length(x) > 1) {
        return(append(sciNotation(x[1]), sciNotation(x[-1])))
    }
    if (!x) return(0)
    exponent <- floor(log10(x))
    base <- round(x / 10^exponent, digits)
    as.expression(substitute(base %*% 10^exponent, 
			list(base = base, exponent = exponent)))
}

plot(1:1000, axes = FALSE, type = "l", frame.plot = TRUE)
axis(1, at = axTicks(1), label = sciNotation(axTicks(1), 1))

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of IAIN GALLAGHER
Sent: Friday, January 13, 2006 8:09 AM
To: Oddmund Nordg????rd
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Scientific notation in plots

try something like this:

>axis(side=2, at=c(0, 5e+5, 1.5e+6, 2.5e+6),
labels=expression(0, 5%*%10^5, 1.5%*%10^6, 2.5%*%10^6)

it will place your 5e+5, 1.5e+6, 2.5e+6 as scientific
notation at the correct positions on the y axis. the
key is the "labels=expression...." part.

Cheers

iain

--- Oddmund Nordg????rd <nood at ext.sir.no> wrote:

> 
> Is it possible to use scientific notation of numbers
> on the axis of plots
> without using the xEy notation. That means: a
> beatiful 1x10^3 instead of 1E3.
> Logarithmic scale, in my case.
> 
> Thank you very much!
> 
> Oddmund
> 
> ******************************************
> 
>   Oddmund Nordg??rd
> 
>   Department of Haematology and Oncology
>   Stavanger University Hospital
>   P.O. Box 8100
>   4068 STAVANGER
>   Phone: 51 51 89 34
>   Email: nood at ext.sir.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ashelton at albany.edu  Fri Jan 13 15:43:35 2006
From: ashelton at albany.edu (Anne P Shelton)
Date: Fri, 13 Jan 2006 09:43:35 -0500
Subject: [R] Problems installing R 2.2.1
Message-ID: <9D95C2906FCCE04F836ECA17C4CE09210268C3E9@UAEXCH.univ.albany.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/0f679243/attachment.pl

From jgentry at jimmy.harvard.edu  Fri Jan 13 15:52:07 2006
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Fri, 13 Jan 2006 09:52:07 -0500 (EST)
Subject: [R] Taking code from packages
In-Reply-To: <000101c6180f$912bca20$a7fdfea9@TAMARA>
Message-ID: <Pine.SOL.4.20.0601130951360.1444-100000@santiam.dfci.harvard.edu>


On Fri, 13 Jan 2006, Ales Ziberna wrote:
> I am currently in the process of creating (my first) package, which (when
> ready) I intend to publish to CRAN. In the process of creating this package
> I have taken some code form existing packages. I have actually copied parts
> of functions in to new functions. This code is usually something very basic
> such as Rand index. What is the proper procedure for this?

Why not simply specify that their package is a dependency and use their
code directly?



From friendly at yorku.ca  Fri Jan 13 15:54:47 2006
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 13 Jan 2006 09:54:47 -0500
Subject: [R] R newbie example code question
In-Reply-To: <XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>
References: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
	<XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <43C7BF37.90704@yorku.ca>

Ted:

Your .xthelp is extremely useful, help on Linux being otherwise
quite awkward to use since a pager in the same window make it hard
to cut/paste examples --- where 'more' or 'less' really means
'instead of' :-)

Suggestion: include -title

      cat("cat > $HLPFIL\nxterm -title 'R-help' -e less $HLPFIL &\n", 
file=con)

<hint>It would be nice if this solution made it into the R-FAQ or R-Admin
manual.</hint>
Failing that, it is a good example of something that would
work well in a wiki.  But, thankfully, you included the subject line
from the original thread to make it searchable in the news archives.


(Ted Harding) wrote:


>   [R] help output paged in separate window
> 
> The solution I finally opted for, and still use,
> is based (in a Linux environment) on including
> the following code in your .Rprofile file:
> 
> 
> .xthelp <- function() {
>     tdir <- tempdir()
>     pgr <- paste(tdir, "/pgr", sep="")
>     con <- file(pgr, "w")
>     cat("#! /bin/bash\n", file=con)
>     cat("export HLPFIL=`mktemp ", tdir, "/R_hlp.XXXXXX`\n",
>          sep="", file=con)
>     cat("cat > $HLPFIL\nxterm -e less $HLPFIL &\n", file=con)
>     close(con)
>     system(paste("chmod 755 ", pgr, sep=""))
>     options(pager=pgr)
> }
> .xthelp()
> rm(.xthelp)
> 
> 
> (and it's also specific to the 'bash' shell because
> of the "#! /bin/bash\n", but you should be able to
> change this appropriately). The above was posted by
> Roger Bivand on 27 May.
> 
> When you start an R session, this code is executed as
> part of sourcing your .Rprofile, and it has the effect that
> any output from R which would be paged is stored in a
> temporary  file which is then read by 'less' in a
> separate X window which is detached from your R session
> (i.e. your command interface will not hang while it is
> being displayed). You can close the X window displaying
> the 'less' output by closing 'less' (e.g. type "q"), or
> you can leave it open and any new paged output will go
> into a new window -- so you can for instance do
> 
> ?glm
> ?family
> ?binomial
> 
> and you will have three mutually relevant help windows
> open at once between which you can cross-reference.
> 
> As to extracting the code for examples, this is easy
> in X windows since you just use your mouse: left-button
> drag to hghlight a block of text, middle-button click
> to paste the block into another window (of course the
> mouse must be over the correct window!)
> 
> So you can use the mouse to copy code from the "help"
> pages to the command window.
> 
> As I say, this is a Linux-oriented solution, and I don't
> know what details would be required for anything similar
> in a Windows environment.
> 
> It is also worth reading the various contributions to
> the above thread, since several suggestions were made.
> 
> Hoping this helps,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 10-Jan-06                                       Time: 13:30:35
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From friendly at yorku.ca  Fri Jan 13 15:54:47 2006
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 13 Jan 2006 09:54:47 -0500
Subject: [R] R newbie example code question
In-Reply-To: <XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>
References: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
	<XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <43C7BF37.90704@yorku.ca>

Ted:

Your .xthelp is extremely useful, help on Linux being otherwise
quite awkward to use since a pager in the same window make it hard
to cut/paste examples --- where 'more' or 'less' really means
'instead of' :-)

Suggestion: include -title

      cat("cat > $HLPFIL\nxterm -title 'R-help' -e less $HLPFIL &\n", 
file=con)

<hint>It would be nice if this solution made it into the R-FAQ or R-Admin
manual.</hint>
Failing that, it is a good example of something that would
work well in a wiki.  But, thankfully, you included the subject line
from the original thread to make it searchable in the news archives.


(Ted Harding) wrote:


>   [R] help output paged in separate window
> 
> The solution I finally opted for, and still use,
> is based (in a Linux environment) on including
> the following code in your .Rprofile file:
> 
> 
> .xthelp <- function() {
>     tdir <- tempdir()
>     pgr <- paste(tdir, "/pgr", sep="")
>     con <- file(pgr, "w")
>     cat("#! /bin/bash\n", file=con)
>     cat("export HLPFIL=`mktemp ", tdir, "/R_hlp.XXXXXX`\n",
>          sep="", file=con)
>     cat("cat > $HLPFIL\nxterm -e less $HLPFIL &\n", file=con)
>     close(con)
>     system(paste("chmod 755 ", pgr, sep=""))
>     options(pager=pgr)
> }
> .xthelp()
> rm(.xthelp)
> 
> 
> (and it's also specific to the 'bash' shell because
> of the "#! /bin/bash\n", but you should be able to
> change this appropriately). The above was posted by
> Roger Bivand on 27 May.
> 
> When you start an R session, this code is executed as
> part of sourcing your .Rprofile, and it has the effect that
> any output from R which would be paged is stored in a
> temporary  file which is then read by 'less' in a
> separate X window which is detached from your R session
> (i.e. your command interface will not hang while it is
> being displayed). You can close the X window displaying
> the 'less' output by closing 'less' (e.g. type "q"), or
> you can leave it open and any new paged output will go
> into a new window -- so you can for instance do
> 
> ?glm
> ?family
> ?binomial
> 
> and you will have three mutually relevant help windows
> open at once between which you can cross-reference.
> 
> As to extracting the code for examples, this is easy
> in X windows since you just use your mouse: left-button
> drag to hghlight a block of text, middle-button click
> to paste the block into another window (of course the
> mouse must be over the correct window!)
> 
> So you can use the mouse to copy code from the "help"
> pages to the command window.
> 
> As I say, this is a Linux-oriented solution, and I don't
> know what details would be required for anything similar
> in a Windows environment.
> 
> It is also worth reading the various contributions to
> the above thread, since several suggestions were made.
> 
> Hoping this helps,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 10-Jan-06                                       Time: 13:30:35
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From aleszib at gmail.com  Fri Jan 13 14:15:43 2006
From: aleszib at gmail.com (Ales Ziberna)
Date: Fri, 13 Jan 2006 14:15:43 +0100
Subject: [R] Taking code from packages
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca>
Message-ID: <054201c61843$7836ca90$0100a8c0@ALES>

First thank you for you reply!



First let me assure you that I did not think or intent to just use their 
code and use GPL as an excuse.



Although I would like to just make my package dependant on theirs and use 
their functions that is not possible. The main reason is that I do not want 
to use the whole functions, but just parts of it. For example, I only need 
one statistics, while their function computes several. Since I call my 
function several thousand times, I want to make it as fast as possible.



I am also modified (although only slightly) the functions, I am a little 
worried about naming them as sole authors of the function, since I do not 
want to make them responsible for any mistakes I have made.



What I was thinking of doing is:

  1.. Notify the original authors!
  2.. In the Author (s) section of the function help, I would write "Their 
name (Modifications made by My Name").
  3.. Use the same license (GPL 2) as they do.

I did not intent to write them as the package authors, since their code 
represent a very small part of the whole package.



How does that sound?



Best regards,

Ales Ziberna



P.S.: I did not find any other special copyright notices.





----- Original Message ----- 
From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
To: "Ales Ziberna" <aleszib at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, January 13, 2006 1:46 PM
Subject: Re: [R] Taking code from packages


On 1/13/2006 2:04 AM, Ales Ziberna wrote:
> Hello!
>
> I am currently in the process of creating (my first) package, which (when
> ready) I intend to publish to CRAN. In the process of creating this 
> package
> I have taken some code form existing packages. I have actually copied 
> parts
> of functions in to new functions. This code is usually something very 
> basic
> such as Rand index. What is the proper procedure for this?
>
> Since most of R (and also the packages I have taken code form) is 
> published
> under GPL, I think this should be OK. However I do not know if:
> 1. I should still ask authors of the packages for permission or at
> least notify them.

It is polite to notify them.

> 2. Ad references to the functions (and packages) from which I had taken
> the code or only to the references they use.

The GPL requires that you maintain their copyright notices.  You have
the right to use their work, the GPL doesn't give you ownership of it.
The usual way to do this is to leave their copyright notice intact, and
add your own if you have made modifications.

An alternative which is usually (but not always) better is to say that
your package depends on theirs, and then just use their functions.  The
advantage is that it avoids any of the above mixed copyright issues, and
it makes sure that when the author fixes a bug, you benefit too.  The
disadvantage is that it makes your package dependent on theirs, so if
changes are needed in it for some future version of R, you'll have to
wait for the other maintainer to do them (or copy their code at that point).

(I'm a little sensitive about dependencies now, since the LaTeX seminar
template I've used a few times no longer works.  It depends on too many
LaTeX packages, and someone, somewhere has introduced incompatibilities
in them.  Seems like I'll be forced to use Powerpoint or Impress.)

Duncan Murdoch

>
> What about regarding code that was sent to the list, usually as a response
> to one of my problems. I assume that in this case it is best to consult 
> the
> author?

> Any comments and opinions are very welcomed!
>
> Best regards,
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Jan 13 16:34:42 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jan 2006 16:34:42 +0100
Subject: [R] Problems installing R 2.2.1
In-Reply-To: <9D95C2906FCCE04F836ECA17C4CE09210268C3E9@UAEXCH.univ.albany.edu>
References: <9D95C2906FCCE04F836ECA17C4CE09210268C3E9@UAEXCH.univ.albany.edu>
Message-ID: <x2ek3c6t0t.fsf@turmalin.kubism.ku.dk>

"Anne P Shelton" <ashelton at albany.edu> writes:

> We are trying to install R2.2.1 on a  IBM P655 Cluster, SuSE LE 9.1 
> We are using gcc v 3.3.3
>  
> and we are getting this error on make
>  
> g77 -fPIC  -g -O2 -ffloat-store -c dlamc.f -o dlamc.lo
> g77  -fPIC  -g -O2 -c dlapack0.f -o dlapack0.lo
> g77  -fPIC  -g -O2 -c dlapack1.f -o dlapack1.lo
> g77  -fPIC  -g -O2 -c dlapack2.f -o dlapack2.lo
> g77  -fPIC  -g -O2 -c dlapack3.f -o dlapack3.lo
> g77  -fPIC  -g -O2 -c cmplx.f -o cmplx.lo
> gcc -shared -L/usr/local/lib  -o libRlapack.so dlamc.lo dlapack0.lo
> dlapack1.lo dlapack2.lo dlapack3.lo cmplx.lo  -lf77blas -latlas -lg2c
> -lm -lgcc_s
> /usr/lib/gcc-lib/powerpc-suse-linux/3.3.3/../../../../powerpc-suse-linux
> /bin/ld: Error: The symbol `s_wsfe' has a R_PPC_REL24 relocation, that
> means '/usr/local/lib/libf77blas.a(xerbla.o)' was compiled without
> -fPIC.
> /usr/lib/gcc-lib/powerpc-suse-linux/3.3.3/../../../../powerpc-suse-linux
> /bin/ld: Error: The symbol `do_fio' has a R_PPC_REL24 relocation, that
> means '/usr/local/lib/libf77blas.a(xerbla.o)' was compiled without
> -fPIC.
> /usr/lib/gcc-lib/powerpc-suse-linux/3.3.3/../../../../powerpc-suse-linux
> /bin/ld: Error: The symbol `do_fio' has a R_PPC_REL24 relocation, that
> means '/usr/local/lib/libf77blas.a(xerbla.o)' was compiled without
> -fPIC.
> /usr/lib/gcc-lib/powerpc-suse-linux/3.3.3/../../../../powerpc-suse-linux
> /bin/ld: Error: The symbol `e_wsfe' has a R_PPC_REL24 relocation, that
> means '/usr/local/lib/libf77blas.a(xerbla.o)' was compiled without
> -fPIC.
> /usr/lib/gcc-lib/powerpc-suse-linux/3.3.3/../../../../powerpc-suse-linux
> /bin/ld: Error: The symbol `s_stop' has a R_PPC_REL24 relocation, that
> means '/usr/local/lib/libf77blas.a(xerbla.o)' was compiled without
> -fPIC.
> collect2: ld returned 1 exit status
> make[4]: *** [libRlapack.so] Error 1
> make[4]: Leaving directory
> `/opt/devel/POWER/R/R-2.2.1/src/modules/lapack'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory
> `/opt/devel/POWER/R/R-2.2.1/src/modules/lapack'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/opt/devel/POWER/R/R-2.2.1/src/modules'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/opt/devel/POWER/R/R-2.2.1/src'
> make: *** [R] Error 1
>  
> Any ideas would be appreciated.  I will continue reading the
> documentation.

Looks like you need to recompile ATLAS (or stop trying to link against
it). This is slightly tricky because ATLAS is not too keen on making
PIC code. AFAIR (it's been a while) you may or may not be given the
choice of modifying flags during the configuration phase (where need
to stick in -fPIC all over the place). Alternatively, you can just let
it run and diddle the Make.Linux_* file afterwards.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Fri Jan 13 16:41:37 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jan 2006 10:41:37 -0500
Subject: [R] Taking code from packages
In-Reply-To: <054201c61843$7836ca90$0100a8c0@ALES>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca> <054201c61843$7836ca90$0100a8c0@ALES>
Message-ID: <971536df0601130741x7bc72b7fy14385540e7d487b5@mail.gmail.com>

On 1/13/06, Ales Ziberna <aleszib at gmail.com> wrote:
> First thank you for you reply!
>
>
>
> First let me assure you that I did not think or intent to just use their
> code and use GPL as an excuse.
>
>
>
> Although I would like to just make my package dependant on theirs and use
> their functions that is not possible. The main reason is that I do not want
> to use the whole functions, but just parts of it. For example, I only need
> one statistics, while their function computes several. Since I call my
> function several thousand times, I want to make it as fast as possible.

You might want to time the difference just in case.    Predicting performance
is notoriously difficult to do.



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan 13 16:45:53 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 13 Jan 2006 15:45:53 -0000 (GMT)
Subject: [R] R newbie example code question
In-Reply-To: <43C7BF37.90704@yorku.ca>
Message-ID: <XFMail.060113154553.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jan-06 Michael Friendly wrote:
> Ted:
> 
> Your .xthelp is extremely useful, help on Linux being otherwise
> quite awkward to use since a pager in the same window make it hard
> to cut/paste examples --- where 'more' or 'less' really means
> 'instead of' :-)

Glad you found it useful. I find it indispensable!
For the record: this is not my code but Roger Bivand's,
it being the one out of several suggestions on that thread
which I decided to adopt. I still admire the neat way he
wrapped it all up.

One of the beautiful features is that *anything* which would
be paged comes up in the separate window, so for instance
you could execute in R

  page(glm)

and you can then scan up and down, and cut&paste chunks if
you want, etc., and in several different windows for different
things at once (or even the same thing several times over if
you want to see different parts at the same time). Great for
playing with variants.

> Suggestion: include -title
> 
> cat("cat > $HLPFIL\nxterm -title 'R-help' -e less $HLPFIL &\n", 
> file=con)
> 
> <hint>It would be nice if this solution made it into the R-FAQ
> or R-Admin manual.</hint> Failing that, it is a good example of
> something that would work well in a wiki.  But, thankfully, you
> included the subject line from the original thread to make it
> searchable in the news archives.

A good <hint>! In fact a good place for it (or for a URL to it)
could be the help file for "help" and friends. In the circumstances,
this is where I would have looked first.</hint>

Best wishes,
Ted.

> (Ted Harding) wrote:
> 
> 
>>   [R] help output paged in separate window
>> 
>> The solution I finally opted for, and still use,
>> is based (in a Linux environment) on including
>> the following code in your .Rprofile file:
>> 
>> 
>> .xthelp <- function() {
>>     tdir <- tempdir()
>>     pgr <- paste(tdir, "/pgr", sep="")
>>     con <- file(pgr, "w")
>>     cat("#! /bin/bash\n", file=con)
>>     cat("export HLPFIL=`mktemp ", tdir, "/R_hlp.XXXXXX`\n",
>>          sep="", file=con)
>>     cat("cat > $HLPFIL\nxterm -e less $HLPFIL &\n", file=con)
>>     close(con)
>>     system(paste("chmod 755 ", pgr, sep=""))
>>     options(pager=pgr)
>> }
>> .xthelp()
>> rm(.xthelp)
>> 
>> 
>> (and it's also specific to the 'bash' shell because
>> of the "#! /bin/bash\n", but you should be able to
>> change this appropriately). The above was posted by
>> Roger Bivand on 27 May.
>> 
>> When you start an R session, this code is executed as
>> part of sourcing your .Rprofile, and it has the effect that
>> any output from R which would be paged is stored in a
>> temporary  file which is then read by 'less' in a
>> separate X window which is detached from your R session
>> (i.e. your command interface will not hang while it is
>> being displayed). You can close the X window displaying
>> the 'less' output by closing 'less' (e.g. type "q"), or
>> you can leave it open and any new paged output will go
>> into a new window -- so you can for instance do
>> 
>> ?glm
>> ?family
>> ?binomial
>> 
>> and you will have three mutually relevant help windows
>> open at once between which you can cross-reference.
>> 
>> As to extracting the code for examples, this is easy
>> in X windows since you just use your mouse: left-button
>> drag to hghlight a block of text, middle-button click
>> to paste the block into another window (of course the
>> mouse must be over the correct window!)
>> 
>> So you can use the mouse to copy code from the "help"
>> pages to the command window.
>> 
>> As I say, this is a Linux-oriented solution, and I don't
>> know what details would be required for anything similar
>> in a Windows environment.
>> 
>> It is also worth reading the various contributions to
>> the above thread, since several suggestions were made.
>> 
>> Hoping this helps,
>> Ted.
>> 
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 10-Jan-06                                       Time: 13:30:35
>> ------------------------------ XFMail ------------------------------
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
> 
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jan-06                                       Time: 15:45:47
------------------------------ XFMail ------------------------------



From Yingfu.Xie at sekon.slu.se  Fri Jan 13 17:04:43 2006
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Fri, 13 Jan 2006 17:04:43 +0100
Subject: [R] Can I ask for the C code inside an R function using .C?
Message-ID: <CA871298CD1882459F7859BD08DC06E4C530FB@slumail.ad.slu.se>

Hello, all,

It is a general question, but I couldn't find the answer elsewhere. 
I am using an R function using .C but don't understand one of its
behaviors without the C code. I am wondering the so-called 'open
source'. It doesn't include the C code together with the R function,
does it? So what I want to ask is whether it is justified, possible or
polite to ask for the C code behind the R function.
Sorry if I miss anything! Thank you as always!

Regards,
Yingfu
###########################################

This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Jan 13 17:06:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 16:06:04 +0000 (GMT)
Subject: [R] Taking code from packages
In-Reply-To: <054201c61843$7836ca90$0100a8c0@ALES>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca> <054201c61843$7836ca90$0100a8c0@ALES>
Message-ID: <Pine.LNX.4.61.0601131558520.21422@gannet.stats>

The thing that is most important and has not been mentioned is to ensure 
that modified functions have a changed name.

People have taken my code without consulting me, changed what it did and 
put it up in a package with the original name (and no indication that I 
was the author), and then quoted GPL at me when I complained.  So this may 
perhaps help you understand where some of your answers are coming from.

I would suggest adding to your suggested documentation the exact source 
you used (it makes it easier to find out if that had been updated later).

R itself borrows software from other packages, and you will see some 
examples of how it is referenced in various places including COPYRIGHTS.
In retrospect we should have been more careful to rename things, e.g. 
entry points in conpiled code, even if we made no changes as the original 
author might want to use his current versions with R.


On Fri, 13 Jan 2006, Ales Ziberna wrote:

> First thank you for you reply!
>
>
>
> First let me assure you that I did not think or intent to just use their
> code and use GPL as an excuse.
>
>
>
> Although I would like to just make my package dependant on theirs and use
> their functions that is not possible. The main reason is that I do not want
> to use the whole functions, but just parts of it. For example, I only need
> one statistics, while their function computes several. Since I call my
> function several thousand times, I want to make it as fast as possible.
>
>
>
> I am also modified (although only slightly) the functions, I am a little
> worried about naming them as sole authors of the function, since I do not
> want to make them responsible for any mistakes I have made.
>
>
>
> What I was thinking of doing is:
>
>  1.. Notify the original authors!
>  2.. In the Author (s) section of the function help, I would write "Their
> name (Modifications made by My Name").
>  3.. Use the same license (GPL 2) as they do.
>
> I did not intent to write them as the package authors, since their code
> represent a very small part of the whole package.
>
>
>
> How does that sound?
>
>
>
> Best regards,
>
> Ales Ziberna
>
>
>
> P.S.: I did not find any other special copyright notices.
>
>
>
>
>
> ----- Original Message -----
> From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
> To: "Ales Ziberna" <aleszib at gmail.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Friday, January 13, 2006 1:46 PM
> Subject: Re: [R] Taking code from packages
>
>
> On 1/13/2006 2:04 AM, Ales Ziberna wrote:
>> Hello!
>>
>> I am currently in the process of creating (my first) package, which (when
>> ready) I intend to publish to CRAN. In the process of creating this
>> package
>> I have taken some code form existing packages. I have actually copied
>> parts
>> of functions in to new functions. This code is usually something very
>> basic
>> such as Rand index. What is the proper procedure for this?
>>
>> Since most of R (and also the packages I have taken code form) is
>> published
>> under GPL, I think this should be OK. However I do not know if:
>> 1. I should still ask authors of the packages for permission or at
>> least notify them.
>
> It is polite to notify them.
>
>> 2. Ad references to the functions (and packages) from which I had taken
>> the code or only to the references they use.
>
> The GPL requires that you maintain their copyright notices.  You have
> the right to use their work, the GPL doesn't give you ownership of it.
> The usual way to do this is to leave their copyright notice intact, and
> add your own if you have made modifications.
>
> An alternative which is usually (but not always) better is to say that
> your package depends on theirs, and then just use their functions.  The
> advantage is that it avoids any of the above mixed copyright issues, and
> it makes sure that when the author fixes a bug, you benefit too.  The
> disadvantage is that it makes your package dependent on theirs, so if
> changes are needed in it for some future version of R, you'll have to
> wait for the other maintainer to do them (or copy their code at that point).
>
> (I'm a little sensitive about dependencies now, since the LaTeX seminar
> template I've used a few times no longer works.  It depends on too many
> LaTeX packages, and someone, somewhere has introduced incompatibilities
> in them.  Seems like I'll be forced to use Powerpoint or Impress.)
>
> Duncan Murdoch
>
>>
>> What about regarding code that was sent to the list, usually as a response
>> to one of my problems. I assume that in this case it is best to consult
>> the
>> author?
>
>> Any comments and opinions are very welcomed!
>>
>> Best regards,
>> Ales Ziberna
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Jan 13 17:13:51 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Jan 2006 11:13:51 -0500
Subject: [R] Can I ask for the C code inside an R function using .C?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6FE@usctmx1106.merck.com>

Source code for R and all CRAN packages are on CRAN.  If you want the C or
Fortran code used in R or add-on packages, you need to download the source
(the .tar.gz files).  You won't see the code if you just install the binary.

Andy

From: Yingfu Xie
> 
> Hello, all,
> 
> It is a general question, but I couldn't find the answer elsewhere. 
> I am using an R function using .C but don't understand one of its
> behaviors without the C code. I am wondering the so-called 'open
> source'. It doesn't include the C code together with the R function,
> does it? So what I want to ask is whether it is justified, possible or
> polite to ask for the C code behind the R function.
> Sorry if I miss anything! Thank you as always!
> 
> Regards,
> Yingfu
> ###########################################
> 
> This message has been scanned by F-Secure Anti-Virus for 
> Mic...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From I.Visser at uva.nl  Fri Jan 13 17:16:12 2006
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 13 Jan 2006 17:16:12 +0100
Subject: [R] Can I ask for the C code inside an R function using .C?
In-Reply-To: <CA871298CD1882459F7859BD08DC06E4C530FB@slumail.ad.slu.se>
Message-ID: <BFED90DC.C29B%I.Visser@uva.nl>

You can download the source from any package from a CRAN mirror near you and
the src directory will contain the source code of the included C and fortran
functions.
hth, ingmar

> From: "Yingfu Xie" <Yingfu.Xie at sekon.slu.se>
> Date: Fri, 13 Jan 2006 17:04:43 +0100
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] Can I ask for the C code inside an R function using .C?
> 
> Hello, all,
> 
> It is a general question, but I couldn't find the answer elsewhere.
> I am using an R function using .C but don't understand one of its
> behaviors without the C code. I am wondering the so-called 'open
> source'. It doesn't include the C code together with the R function,
> does it? So what I want to ask is whether it is justified, possible or
> polite to ask for the C code behind the R function.
> Sorry if I miss anything! Thank you as always!
> 
> Regards,
> Yingfu
> ###########################################
> 
> This message has been scanned by F-Secure Anti-Virus for Mic...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Soren.Hojsgaard at agrsci.dk  Fri Jan 13 17:19:28 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 13 Jan 2006 17:19:28 +0100
Subject: [R] Saving data in an R package - how to maintain that t avariable
	is a 'factor' when it is coded as 1, 2, 3...
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387813B@DJFPOST01.djf.agrsci.dk>

I have a .txt file obtained by saving a data frame in which the first four columns are factors (but represented as 1,2,3 etc). The first four lines are
 
"Pig" "Evit" "Cu" "Litter" "Start" "Weight" "Feed" "Time"
"4601" "1" "1" "1" 26.5 26.5 NA 1
"4601" "1" "1" "1" 26.5 27.59999 5.200005 2
"4601" "1" "1" "1" 26.5 36.5 17.6 3
"4601" "1" "1" "1" 26.5 40.29999 28.5 4

I would like to include that data set in an R-package. When I load the data from the package the first four columns are read in as numeric variables. This is consistent with the documentation of read.table - but it is not what I want! I can of course change the coding of the variables, but there ought to be another way. Can anyone help me on that?
Best regards
S??ren H??jsgaard



From spencer.graves at pdf.com  Fri Jan 13 17:38:14 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 13 Jan 2006 08:38:14 -0800
Subject: [R] CRAN versions of lme4/Matrix don't appear to work with	R
 2.1.1
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614E6A681@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614E6A681@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <43C7D776.10503@pdf.com>

	  Are you familiar with "sessionInfo()", e.g.:

 > sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      lme4   lattice    Matrix
  "0.98-1" "0.12-11"  "0.99-6"

	  This can almost eliminate the possibilities for typographical errors 
in specifying the version name.  The posting guide 
(www.R-project.org/posting-guide.html) also asks posters to mention 
mention the platform (Windows2000, Linux, MacOS X).

	  Best Wishes,
	  spencer graves

White, Charles E WRAIR-Wash DC wrote:
> After downloading from the WA mirror, the lme4 related packages appear
> to run fine . Thanks for everyone's help.
> 
> Chuck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Fri Jan 13 17:39:05 2006
From: jholtman at gmail.com (jim holtman)
Date: Fri, 13 Jan 2006 11:39:05 -0500
Subject: [R] Saving data in an R package - how to maintain that t
	avariable is a 'factor' when it is coded as 1, 2, 3...
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387813B@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387813B@DJFPOST01.djf.agrsci.dk>
Message-ID: <644e1f320601130839w8b8231cob1083a1fbd2bbb27@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/74699d78/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jan 13 17:39:47 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jan 2006 16:39:47 +0000 (GMT)
Subject: [R] Saving data in an R package - how to maintain that t
 avariable is a 'factor' when it is coded as 1, 2, 3...
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387813B@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387813B@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.61.0601131638380.27240@gannet.stats>

?read.table, see argument colClasses.

You can use a .R wrapper to a .tab file in the data directory of a 
package.  Or, perhaps better, include it as a .rda file.

On Fri, 13 Jan 2006, S?ren H?jsgaard wrote:

> I have a .txt file obtained by saving a data frame in which the first four columns are factors (but represented as 1,2,3 etc). The first four lines are
>
> "Pig" "Evit" "Cu" "Litter" "Start" "Weight" "Feed" "Time"
> "4601" "1" "1" "1" 26.5 26.5 NA 1
> "4601" "1" "1" "1" 26.5 27.59999 5.200005 2
> "4601" "1" "1" "1" 26.5 36.5 17.6 3
> "4601" "1" "1" "1" 26.5 40.29999 28.5 4
>
> I would like to include that data set in an R-package. When I load the data from the package the first four columns are read in as numeric variables. This is consistent with the documentation of read.table - but it is not what I want! I can of course change the coding of the variables, but there ought to be another way. Can anyone help me on that?
> Best regards
> S?ren H?jsgaard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From tom at maladmin.com  Fri Jan 13 13:06:54 2006
From: tom at maladmin.com (tom wright)
Date: Fri, 13 Jan 2006 07:06:54 -0500
Subject: [R] find mean of a list of timeseries
Message-ID: <1137154015.4525.84.camel@localhost.localdomain>

Can someone please give me a clue how to 're'write this so I dont need
to use loops.

a<-ts(matrix(c(1,1,1,10,10,10,20,20,20),nrow=3),names=c('var1','var2','var3'))
b<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3'))
c<-ts(matrix(c(3,3,3,12,12,12,22,22,22),nrow=3),names=c('var1','var2','var3'))

data<-list(a,b,c)

I now want to find the means of all vectors var1,var2 and var3

i.e. I need to end up with a new time series with three data vectors
(var1, var2 and var3)
result<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3))

I think its the list thats throwing my use of apply, I might be wrong
but what other data structure could I use?

Many thanks
Tom



From david.reitter at gmail.com  Fri Jan 13 18:53:03 2006
From: david.reitter at gmail.com (David Reitter)
Date: Fri, 13 Jan 2006 17:53:03 +0000
Subject: [R] glmmPQL: Na/NaN/Inf in foreign function call
Message-ID: <0138E72D-22BE-40EB-A80F-5882321DDDB9@gmail.com>

I'm using glmmPQL, and I still have a few problems with it.
In addition to the issue reported earlier, I'm getting the following  
error and I was wondering if there's something I can do about it.


Error in logLik.reStruct(object, conLin) : Na/NaN/Inf in foreign  
function call (arg 3)

... Warnings:
1: Singular precistion matrix in level -1, block 4
(...)
4: ""

The interaction terms are

primed ~ log(dist) * role
random = ~ dist | target.utt / prime.utt

The family is binomial (logit).

I ensured that log(dist) is always [0; 1]. Role is a factor (binary).  
target.utt and prime.utt are categories as well.

This is version 2.1.1 - is the latest version more reliable?

Thanks for any help you can give me.
Dave



From gerifalte28 at hotmail.com  Fri Jan 13 19:01:49 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 13 Jan 2006 18:01:49 +0000
Subject: [R] Can I ask for the C code inside an R function using .C?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6FE@usctmx1106.merck.com>
Message-ID: <BAY103-F37C2E13B710BD46722452AA6260@phx.gbl>

If you have a slow connection and/or you don't want to download the entire 
source code you can find the sources for R on this site 
https://svn.r-project.org/R/trunk/

Francisco


>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Yingfu Xie'" <Yingfu.Xie at sekon.slu.se>, r-help at stat.math.ethz.ch
>Subject: Re: [R] Can I ask for the C code inside an R function using .C?
>Date: Fri, 13 Jan 2006 11:13:51 -0500
>
>Source code for R and all CRAN packages are on CRAN.  If you want the C or
>Fortran code used in R or add-on packages, you need to download the source
>(the .tar.gz files).  You won't see the code if you just install the 
>binary.
>
>Andy
>
>From: Yingfu Xie
> >
> > Hello, all,
> >
> > It is a general question, but I couldn't find the answer elsewhere.
> > I am using an R function using .C but don't understand one of its
> > behaviors without the C code. I am wondering the so-called 'open
> > source'. It doesn't include the C code together with the R function,
> > does it? So what I want to ask is whether it is justified, possible or
> > polite to ask for the C code behind the R function.
> > Sorry if I miss anything! Thank you as always!
> >
> > Regards,
> > Yingfu
> > ###########################################
> >
> > This message has been scanned by F-Secure Anti-Virus for
> > Mic...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From roger.bos at gmail.com  Fri Jan 13 19:22:05 2006
From: roger.bos at gmail.com (roger bos)
Date: Fri, 13 Jan 2006 13:22:05 -0500
Subject: [R] better way to replace missing values with zero
Message-ID: <1db726800601131022sb2af6d0j9445bb94fa503315@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/5ac95009/attachment.pl

From andy_liaw at merck.com  Fri Jan 13 19:26:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Jan 2006 13:26:02 -0500
Subject: [R] Can I ask for the C code inside an R function using .C?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6FF@usctmx1106.merck.com>

Just wondering:  How do people with slow connections get R?  The Windows
binary is about 25MB, while the source .tar.gz is only 13.7MB.

Andy

From: Francisco J. Zagmutt
> 
> 
> If you have a slow connection and/or you don't want to 
> download the entire 
> source code you can find the sources for R on this site 
> https://svn.r-project.org/R/trunk/
> 
> Francisco
> 
> 
> >From: "Liaw, Andy" <andy_liaw at merck.com>
> >To: "'Yingfu Xie'" <Yingfu.Xie at sekon.slu.se>, 
> r-help at stat.math.ethz.ch
> >Subject: Re: [R] Can I ask for the C code inside an R 
> function using .C?
> >Date: Fri, 13 Jan 2006 11:13:51 -0500
> >
> >Source code for R and all CRAN packages are on CRAN.  If you 
> want the C or
> >Fortran code used in R or add-on packages, you need to 
> download the source
> >(the .tar.gz files).  You won't see the code if you just install the 
> >binary.
> >
> >Andy
> >
> >From: Yingfu Xie
> > >
> > > Hello, all,
> > >
> > > It is a general question, but I couldn't find the answer 
> elsewhere.
> > > I am using an R function using .C but don't understand one of its
> > > behaviors without the C code. I am wondering the so-called 'open
> > > source'. It doesn't include the C code together with the 
> R function,
> > > does it? So what I want to ask is whether it is 
> justified, possible or
> > > polite to ask for the C code behind the R function.
> > > Sorry if I miss anything! Thank you as always!
> > >
> > > Regards,
> > > Yingfu
> > > ###########################################
> > >
> > > This message has been scanned by F-Secure Anti-Virus for
> > > Mic...{{dropped}}
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> 
>



From dyang at NRCan.gc.ca  Fri Jan 13 19:59:57 2006
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Fri, 13 Jan 2006 13:59:57 -0500
Subject: [R] Variance-covariance by factor
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE5889@s2-edm-r1.nrn.nrcan.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/4b71e26e/attachment.pl

From Cameron.Guenther at MyFWC.com  Fri Jan 13 20:03:17 2006
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Fri, 13 Jan 2006 14:03:17 -0500
Subject: [R] apply
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F304C86AE@FWC-TLEX3.fwc.state.fl.us>

Hello,
I have a dataset d which is
 >d
         pop     catch 
1   66462.01 10807.757 
2   87486.73 46257.885 
3   57211.64  9345.058 
4   71321.62  4892.868 
5  100024.89 27334.248 
6  104504.91 48535.092 
7   95295.51 39348.195 
8   93737.35 34343.489 
9   89375.05 28750.743 
10  95312.65 30755.064 
11 100888.17 55404.370 
12  84790.23 37751.074 
13  81129.82 29277.443 
14  69797.09 21500.398 
15  61690.34 18199.664 
16  60671.08 19349.051 
17  62852.57 16300.982 
18  90646.32 34793.148 

And a function opt1:

opt1 <- function (x) {
  Z <- x + M
  c.hat <- (x/Z)*pop*(1-exp(-Z))
  y <- (catch - c.hat)^2
  return(y)
  }

And define M = 0.25 
For each row I want to return a value F that is a minimization of the
opt1 function

I have tried many variations of:
d$F<-apply(d,2,optim(0.3,opt1,method="BFGS")
and even:
For (I in 1:length(pop))apply(d,2,optim(0.3,opt1,method="BFGS"))

Every time I get the same error message

Error in optim(0.3, opt1, method = "BFGS") : 
        objective function in optim evaluates to length 18 not 1

So the apply function is returning 18 values of F which I want but the
function only wants to return 1 value.

Any Suggestions.
Thanks,
Cam



From ggrothendieck at gmail.com  Fri Jan 13 20:15:23 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jan 2006 14:15:23 -0500
Subject: [R] find mean of a list of timeseries
In-Reply-To: <1137154015.4525.84.camel@localhost.localdomain>
References: <1137154015.4525.84.camel@localhost.localdomain>
Message-ID: <971536df0601131115v91e823cje258dc0cc87f725a@mail.gmail.com>

Try:

(a+b+c)/3

On 1/13/06, tom wright <tom at maladmin.com> wrote:
> Can someone please give me a clue how to 're'write this so I dont need
> to use loops.
>
> a<-ts(matrix(c(1,1,1,10,10,10,20,20,20),nrow=3),names=c('var1','var2','var3'))
> b<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3'))
> c<-ts(matrix(c(3,3,3,12,12,12,22,22,22),nrow=3),names=c('var1','var2','var3'))
>
> data<-list(a,b,c)
>
> I now want to find the means of all vectors var1,var2 and var3
>
> i.e. I need to end up with a new time series with three data vectors
> (var1, var2 and var3)
> result<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3))
>
> I think its the list thats throwing my use of apply, I might be wrong
> but what other data structure could I use?
>
> Many thanks
> Tom



From marcodoc75 at yahoo.com  Fri Jan 13 21:46:32 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Fri, 13 Jan 2006 12:46:32 -0800 (PST)
Subject: [R] update 'groupedData' and 'lme' objects
Message-ID: <20060113204632.28797.qmail@web31306.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/2a51a16c/attachment.pl

From gerifalte28 at hotmail.com  Fri Jan 13 22:24:31 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 13 Jan 2006 21:24:31 +0000
Subject: [R] Can I ask for the C code inside an R function using .C?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED6FF@usctmx1106.merck.com>
Message-ID: <BAY103-F2574D017E6ADC338B09ED9A6260@phx.gbl>

Well, if you have a slow modem connection and you are an average user that 
doesn't want (or doesn't know how) to compile the source code, you may spend 
half an hour or more downloading the windows binary. Then you realize you 
want to see a .C function and you will have to spend another 15 to 20 
minutes downloading the source tar.gz...OR you can lookup the specific 
function in the mentioned website in a fraction of the time.  Off course, 
overall if you check C or Fortran functions all the time you may want to 
have a local copy of the source.  I think is great that users have that 
flexibility, hence they should be aware of both options.

Cheers

Francisco



,
you may prefer just to visit a web site and see the specific function that 
you want to learn about, rather than waiting another 15 to 20 minutes to 
download the source tar.gz.

>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Francisco J. Zagmutt'" 
><gerifalte28 at hotmail.com>,Yingfu.Xie at sekon.slu.se,r-help at stat.math.ethz.ch
>Subject: RE: [R] Can I ask for the C code inside an R function using .C?
>Date: Fri, 13 Jan 2006 13:26:02 -0500
>
>Just wondering:  How do people with slow connections get R?  The Windows
>binary is about 25MB, while the source .tar.gz is only 13.7MB.
>
>Andy
>
>From: Francisco J. Zagmutt
> >
> >
> > If you have a slow connection and/or you don't want to
> > download the entire
> > source code you can find the sources for R on this site
> > https://svn.r-project.org/R/trunk/
> >
> > Francisco
> >
> >
> > >From: "Liaw, Andy" <andy_liaw at merck.com>
> > >To: "'Yingfu Xie'" <Yingfu.Xie at sekon.slu.se>,
> > r-help at stat.math.ethz.ch
> > >Subject: Re: [R] Can I ask for the C code inside an R
> > function using .C?
> > >Date: Fri, 13 Jan 2006 11:13:51 -0500
> > >
> > >Source code for R and all CRAN packages are on CRAN.  If you
> > want the C or
> > >Fortran code used in R or add-on packages, you need to
> > download the source
> > >(the .tar.gz files).  You won't see the code if you just install the
> > >binary.
> > >
> > >Andy
> > >
> > >From: Yingfu Xie
> > > >
> > > > Hello, all,
> > > >
> > > > It is a general question, but I couldn't find the answer
> > elsewhere.
> > > > I am using an R function using .C but don't understand one of its
> > > > behaviors without the C code. I am wondering the so-called 'open
> > > > source'. It doesn't include the C code together with the
> > R function,
> > > > does it? So what I want to ask is whether it is
> > justified, possible or
> > > > polite to ask for the C code behind the R function.
> > > > Sorry if I miss anything! Thank you as always!
> > > >
> > > > Regards,
> > > > Yingfu
> > > > ###########################################
> > > >
> > > > This message has been scanned by F-Secure Anti-Virus for
> > > > Mic...{{dropped}}
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > >
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide!
> > >http://www.R-project.org/posting-guide.html
> >
> >
> >
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments...{{dropped}}



From gunter.berton at gene.com  Fri Jan 13 22:24:28 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 13 Jan 2006 13:24:28 -0800
Subject: [R] apply
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F304C86AE@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <200601132124.k0DLOG3g006656@hertz.gene.com>

You are not calling apply() properly. Please read relevant reference
material carefully. You might also wish to pick up one of the several good
books on R (check CRAN website) -- I like V&R's S PROGRAMMING.

I did not go through your example in detail, but your apply() call should be
of the form

apply(d,2,function(x)optim(x,...))

You may or may not get into scoping problems with the opt1 argument and have
to pass it in explicitly -- I can't remember how things work with optim. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Guenther, Cameron
> Sent: Friday, January 13, 2006 11:03 AM
> To: R-help at r-project.org
> Subject: [R] apply
> 
> Hello,
> I have a dataset d which is
>  >d
>          pop     catch 
> 1   66462.01 10807.757 
> 2   87486.73 46257.885 
> 3   57211.64  9345.058 
> 4   71321.62  4892.868 
> 5  100024.89 27334.248 
> 6  104504.91 48535.092 
> 7   95295.51 39348.195 
> 8   93737.35 34343.489 
> 9   89375.05 28750.743 
> 10  95312.65 30755.064 
> 11 100888.17 55404.370 
> 12  84790.23 37751.074 
> 13  81129.82 29277.443 
> 14  69797.09 21500.398 
> 15  61690.34 18199.664 
> 16  60671.08 19349.051 
> 17  62852.57 16300.982 
> 18  90646.32 34793.148 
> 
> And a function opt1:
> 
> opt1 <- function (x) {
>   Z <- x + M
>   c.hat <- (x/Z)*pop*(1-exp(-Z))
>   y <- (catch - c.hat)^2
>   return(y)
>   }
> 
> And define M = 0.25 
> For each row I want to return a value F that is a minimization of the
> opt1 function
> 
> I have tried many variations of:
> d$F<-apply(d,2,optim(0.3,opt1,method="BFGS")
> and even:
> For (I in 1:length(pop))apply(d,2,optim(0.3,opt1,method="BFGS"))
> 
> Every time I get the same error message
> 
> Error in optim(0.3, opt1, method = "BFGS") : 
>         objective function in optim evaluates to length 18 not 1
> 
> So the apply function is returning 18 values of F which I want but the
> function only wants to return 1 value.
> 
> Any Suggestions.
> Thanks,
> Cam
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Fri Jan 13 22:39:19 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 13 Jan 2006 16:39:19 -0500
Subject: [R] R and filemaker pro DB
In-Reply-To: <loom.20060113T124207-85@post.gmane.org>
Message-ID: <BFED8837.3B7E%sdavis2@mail.nih.gov>




On 1/13/06 7:08 AM, "Michael Bibo" <mbibo at aanet.com.au> wrote:

> I have been looking for a database application to use in conjunction with R
> (on
> a Windows network).  When I approached my organization's IT department to ask
> about using MySQL, they made a counter-offer of Filemaker Pro (v8).  It is not
> specifically mentioned in 'R Data Import/Export', nor do searches of the
> archives turn up much information.
> 
> Does anyone have any experience using Filemaker Pro with R?
> Should package RODBC work with Filemaker, as it is odbc-compliant? (I will be
> doing some trials anyway).
> Any other relevant advice welcome.

I have used Fmpro via RODBC a bit.  I also use MySQL and PostgreSQL quite a
bit with R.  For smaller datasets, Fmpro is probably fine, but for larger
datasets (more than a few thousand rows), fmpro is pretty slow compared to a
dedicated SQL server.  Also, there is a great deal of flexibility in
database design, tuning, backup, and "enterpriseness".  However, note that
SQL servers do not come with GUI front ends like fmpro, so if you are
building a database for "use" other than data management, fmpro is a good
quick solution.  So, I would do what you suggest you are going to do and set
up both MySQL or PostgreSQL and fmpro and see which better meets your needs.
They are quite different in many ways, though.

Sean



From johanfaux at yahoo.com  Fri Jan 13 22:57:48 2006
From: johanfaux at yahoo.com (johan Faux)
Date: Fri, 13 Jan 2006 13:57:48 -0800 (PST)
Subject: [R] for loop should check the looping index !!
Message-ID: <20060113215748.21313.qmail@web31410.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/eaf85b90/attachment.pl

From h.wickham at gmail.com  Fri Jan 13 23:10:01 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 13 Jan 2006 16:10:01 -0600
Subject: [R] Idempotent apply
Message-ID: <f8e6ff050601131410m4bf834a9n8f29283916e16299@mail.gmail.com>

One small problem with apply is that it is not idempotent - it always
puts the result vector in the first dimension of the result array. For
example:

a <- array(1:24, c(2,3,4))
dim(apply(a, c(1,2), I))
dim(apply(a, c(1,2), diff))

I've written a wrapper around apply that rearranges the dimensions
into the original order.

# Idempotent apply
iapply <- function (X, MARGIN, FUN, ...)  {
	dims <- c((1:length(dim(X)))[!(1:length(dim(X)) %in% MARGIN)], MARGIN)
	res <- apply(X, MARGIN, FUN)

	if (length(dim(res)) == length(dims)) {
		aperm(res, order(dims))
	} else {
		res
	}	
}

dim(iapply(a, c(1,2), I))
dim(iapply(a, c(1,2), diff))

Hopefully this may be of use to someone else.  The function isn't
completely satisfying as apply doesn't deal nicely with functions that
return higher-dimensional arrays (eg.  apply(a, 1, I)) and reduces 1-D
arrays to vectors (eg. apply(a,3,sum)).  Any comments on the code
would be greatfully appreciated.

Hadley



From nw.galwey at ukonline.co.uk  Fri Jan 13 23:13:50 2006
From: nw.galwey at ukonline.co.uk (N.W.Galwey)
Date: Fri, 13 Jan 2006 22:13:50 -0000
Subject: [R] Syntax for linear mixed model
Message-ID: <E1ExXBL-00032r-00@smarthost3.mail.uk.easynet.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060113/3d733bbc/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Jan 13 23:16:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jan 2006 23:16:59 +0100
Subject: [R] for loop should check the looping index !!
In-Reply-To: <20060113215748.21313.qmail@web31410.mail.mud.yahoo.com>
References: <20060113215748.21313.qmail@web31410.mail.mud.yahoo.com>
Message-ID: <x2irsnai3o.fsf@turmalin.kubism.ku.dk>

johan Faux <johanfaux at yahoo.com> writes:

> Hello ,
>    
>   a<-c(1)
>   for(i in 2:length(a))
>       do.something with a[[i]]
>    
>   I get :
>   Error in a[[i]] : subscript out of bounds
>    
>   Am I missing something here?  Doesnt R check the value of i inside "for" and if the condition is not tru, dont do anything ????

Well,

> 2:1
[1] 2 1


The for loop as such has no idea what you're going to do with the
index variable, so will faithfully try to do.something with a[[2]]

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tplate at acm.org  Fri Jan 13 23:17:31 2006
From: tplate at acm.org (Tony Plate)
Date: Fri, 13 Jan 2006 15:17:31 -0700
Subject: [R] for loop should check the looping index !!
In-Reply-To: <20060113215748.21313.qmail@web31410.mail.mud.yahoo.com>
References: <20060113215748.21313.qmail@web31410.mail.mud.yahoo.com>
Message-ID: <43C826FB.90202@acm.org>

Yep, you missed the fact that 2:1 generates the sequence c(2,1).

Personally, I'd excuse you for missing this, as the documentation for 
seq says:

      The operator ':' and the 'seq(from, to)' form generate the
      sequence 'from, from+1, ..., to'.

Maybe I'm missing something, but I don't see anywhere on the help page 
for seq and ":" any mention of the fact the seq() generates a descending 
sequence if 'to' is less than 'from'.

In programming, *never* use a construct like 1:length(x) or 2:length(x), 
always using something like seq(1,len=length(x)) (or simply 
seq(len=length(x)), or seq(2, len=length(x)-1) or seq(along=x)[-1].

-- Tony Plate


johan Faux wrote:
> Hello ,
>    
>   a<-c(1)
>   for(i in 2:length(a))
>       do.something with a[[i]]
>    
>   I get :
>   Error in a[[i]] : subscript out of bounds
>    
>   Am I missing something here?  Doesnt R check the value of i inside "for" and if the condition is not tru, dont do anything ????
>    
>   thanks,
>   johan
> 
> 			
> ---------------------------------
> 
>  Got holiday prints? See all the ways to get quality prints in your hands ASAP.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Sat Jan 14 00:15:51 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 13 Jan 2006 15:15:51 -0800
Subject: [R] apply
In-Reply-To: <200601132124.k0DLOG3g006656@hertz.gene.com>
References: <200601132124.k0DLOG3g006656@hertz.gene.com>
Message-ID: <43C834A7.1040908@pdf.com>

In addition the error message explicitly says:

Error in optim(0.3, opt1, method = "BFGS") :
      objective function in optim evaluates to length 18 not 1

And from the objective function we see

opt1 <- function (x) {
   Z <- x + M
   c.hat <- (x/Z)*pop*(1-exp(-Z))
   y <- (catch - c.hat)^2
   return(y)
}

The last line should be replaced by:

sum(y)

rather than "return(y)". See ?optim, ?apply, and the reference Bert 
gives below.

HTH,

--sundar


Berton Gunter wrote:
> You are not calling apply() properly. Please read relevant reference
> material carefully. You might also wish to pick up one of the several good
> books on R (check CRAN website) -- I like V&R's S PROGRAMMING.
> 
> I did not go through your example in detail, but your apply() call should be
> of the form
> 
> apply(d,2,function(x)optim(x,...))
> 
> You may or may not get into scoping problems with the opt1 argument and have
> to pass it in explicitly -- I can't remember how things work with optim. 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>Guenther, Cameron
>>Sent: Friday, January 13, 2006 11:03 AM
>>To: R-help at r-project.org
>>Subject: [R] apply
>>
>>Hello,
>>I have a dataset d which is
>> >d
>>         pop     catch 
>>1   66462.01 10807.757 
>>2   87486.73 46257.885 
>>3   57211.64  9345.058 
>>4   71321.62  4892.868 
>>5  100024.89 27334.248 
>>6  104504.91 48535.092 
>>7   95295.51 39348.195 
>>8   93737.35 34343.489 
>>9   89375.05 28750.743 
>>10  95312.65 30755.064 
>>11 100888.17 55404.370 
>>12  84790.23 37751.074 
>>13  81129.82 29277.443 
>>14  69797.09 21500.398 
>>15  61690.34 18199.664 
>>16  60671.08 19349.051 
>>17  62852.57 16300.982 
>>18  90646.32 34793.148 
>>
>>And a function opt1:
>>
>>opt1 <- function (x) {
>>  Z <- x + M
>>  c.hat <- (x/Z)*pop*(1-exp(-Z))
>>  y <- (catch - c.hat)^2
>>  return(y)
>>  }
>>
>>And define M = 0.25 
>>For each row I want to return a value F that is a minimization of the
>>opt1 function
>>
>>I have tried many variations of:
>>d$F<-apply(d,2,optim(0.3,opt1,method="BFGS")
>>and even:
>>For (I in 1:length(pop))apply(d,2,optim(0.3,opt1,method="BFGS"))
>>
>>Every time I get the same error message
>>
>>Error in optim(0.3, opt1, method = "BFGS") : 
>>        objective function in optim evaluates to length 18 not 1
>>
>>So the apply function is returning 18 values of F which I want but the
>>function only wants to return 1 value.
>>
>>Any Suggestions.
>>Thanks,
>>Cam
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sat Jan 14 02:15:53 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jan 2006 20:15:53 -0500
Subject: [R] Variance-covariance by factor
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF04FE5889@s2-edm-r1.nrn.nrcan.gc.ca>
References: <F0E0B899CB43D5118D220002A55113CF04FE5889@s2-edm-r1.nrn.nrcan.gc.ca>
Message-ID: <971536df0601131715sf1a0646hecd8fab871d4d6dc@mail.gmail.com>

The cbind should be removed:

sim <- data.frame(S = s, D= d, H=h, L=l, W=w)
by(sim[,-1], sim$S, var)

On 1/13/06, Yang, Richard <dyang at nrcan.gc.ca> wrote:
>
> Dear all,
>
>        I have a data frame with one factor and four numeric variables and
> wish to obtain the var-cor matrix separately by factor. I tried by() and
> sapply() but getting nowhere. I understand this can be done by subsetting
> the dataframe, but there should have some sleek ways of doing it.
>
> Here is a simulated dataframe;
>
> s <- rep(c("A","B","C"), c(25,22,18))
>
> d <- c(rnorm(25,14,2.6),rnorm(22,15.2,2.8),rnorm(18,16.4,3.0))
> h <- c(rnorm(25,10,1.4),rnorm(22,11.2,1.8),rnorm(18,12.3,2.0))
> l <- c(rnorm(25,6.8,1.6), rnorm(22,7.0,1.7),rnorm(18,7.3,1.8))
> w <- c(rnorm(25,2.5,0.65),rnorm(22,2.6,0.7),rnorm(18,2.8,0.71))
>
> sim <- data.frame(cbind(S = s, D= d, H=h,L=l,W=w))
>
> > sim.var <- sapply(split(sim, sim$S), function(z) var(z))
> Error in var(z) : missing observations in cov/cor
> In addition: Warning message:
> NAs introduced by coercion
>
> > sim.var <- by(sim, sim$S, function(z) var(z))
> Error in var(z) : missing observations in cov/cor
> In addition: Warning message:
> NAs introduced by coercion
>
> Debug() and trac() the function got no further info.
>
> Because the error suggested missing data, I looked at the data frame and was
> surprised
>
> > str(sim)
> `data.frame':   65 obs. of  5 variables:
>  $ S: Factor w/ 3 levels "A","B","C": 1 1 1 1 1 1 1 1 1 1 ...
>  $ D: Factor w/ 65 levels "10.0860856437045",..: 51 12 21 11 8 15 57 44 19
> 60 ...
>  $ H: Factor w/ 65 levels "10.0345903489406",..: 17 2 4 52 6 21 29 9 62 10
> ...
>  $ L: Factor w/ 65 levels "10.3854663209663",..: 10 6 23 55 60 8 58 65 11 2
> ...
>  $ W: Factor w/ 65 levels "0.93902749732563",..: 38 13 33 12 22 39 47 31 36
> 53 ...
>
> Why the data.frame() converts numeric vectors d, h, l, w into factors?
>
> Any suggestions for 1) how to compute var-cor by factor in a data frame, and
> 2) why data.frame converts numeric variables into factors ?
>
> > sessionInfo()
> R version 2.2.0, 2005-10-06, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "base"
>
> Thanks,
>
> Richard  Yang
>
> Northern Forestry Centre   /    Centre de foresterie du Nord
> Canadian Forest Service    /    Service canadien des for??ts
> Natural Resources Canada   /    Ressources naturelles Canada
> 5320-122 Street            /    5320, rue 122
>
> Edmonton Alberta Canada
> T6H 3S5
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From spencer.graves at pdf.com  Sat Jan 14 03:15:15 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 13 Jan 2006 18:15:15 -0800
Subject: [R] panel data unit root tests
In-Reply-To: <1136584568.43bee778927e6@www2.helsinki.fi>
References: <1136584568.43bee778927e6@www2.helsinki.fi>
Message-ID: <43C85EB3.70109@pdf.com>

	  If replies to this post will no longer be useful for you, then please 
discregard this comment.  If not, I will start by saying that I could 
not understand enough of your question to respond directly. 
RSiteSearch("panel unit root") led me to an exchange we had following a 
related question you posted last October 
(ttp://finzi.psych.upenn.edu/R/Rhelp02a/archive/63545.html).  Did you 
try the nlme package as suggested there?  I've just now looked at that, 
and I confess I could not figure out how to do it in a few minutes.

	  Do you want to perform a unit root test for a particular panel data 
set you have?  Or do you want to develop software for a particular panel 
unit root test?  If the former, I suggest you prepare a very simple toy 
example trying to do something like this using lme with perhaps corAR1, 
then send this list a question on whether it is possible to do what you 
want with lme, asking how to take the next step with the toy example. 
If rather you want to develop software for a particular unit root test, 
then I suggest you at least provide a more complete citation than just 
"a la Arellano & Bond."  In either case, I also suggest you review the 
posting guide! "www.R-project.org/posting-guide.html" and try to make 
your post as easily understood as possible.  In general, I think that 
posts that are clear and succinct tend to get quicker, more useful 
answers.  Maybe I'm just dense, but I don't understand what you are 
asking.  For example, your code includes "print(levinlin(ws, year, id, 
lags = 3))".  What is the "levinlin" function?  RSiteSearch("levinlin") 
produced zero hits.

	  hope this helps.
	  spencer graves

jukka ruohonen wrote:

> When finally got some time to do some coding, I started and stopped right 
> after. The stationary test is a good starting point because it demonstrates 
> how we should be able to move the very basic R matrices. I have a real-
> world small N data set with 
> 
> rows:
> id(n=1)---t1---variable1
> ...
> id=(N=20)---T=21---variable1
> 
> Thus, a good test case. For first id I was considering something like this:
> 
> lag <- as.integer(lags)
> lags.p <- lags + 1
> id <- unique(group)
> id.l <- length(id)
> y.l <- length(y)
> yid.l <- length(y)/id.l
>   if (lag > yid.l -2) 
>         stop("\nlag too long for defined cross-sections.\n")
> 
> #for (i in id) {
>   lagy <- y[2:yid.l]
>       lagy.em <- embed(lagy, lags)
>   id.l <- length(id)
>   dy <- diff(y)[1:yid.l-1]
>       dy.em <- embed(dy, lags)
> #     }
> print(levinlin(ws, year, id, lags = 3))
> 
> Couldn't figure the loop over units out but with N = 1 the data 
> transformation seemed to work just fine. Now we should pool the new 
> variables within the panel and regress y over yt-1 and dy-t1 +....+ dy-t-j 
> with, say, BIC doing the job for d's (H0: y-1 ~ 0) for each in the panel. 
> 
> Now the above example puts the right-hand on columns, and if we are dealing 
> with panel models in general, we should store the new variables together 
> with dX's, which should then give clues to IV estimator with e.g. 
> orthogonal deviations, e.g. k <- y ~ yt-1 + x + as.factor(id)). So one 
> confusing part is the requirement of some big storage base for different 
> matrices doing the IV business with lags/levels - the amount of instruments 
> can be enormous with possibly calculation problems in a GMM dynamic panel 
> estimator a la Arellano & Bond. Therefore one should code the theoretically 
> relevant instruments beforehand with various transformation matrices. Thus, 
> should I start to study something that can be done with the newly added 
> SparseM package? 
> 
> Regards,
> 
> Jukka Ruohonen.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wasquith at austin.rr.com  Sat Jan 14 04:19:30 2006
From: wasquith at austin.rr.com (William H. Asquith)
Date: Fri, 13 Jan 2006 21:19:30 -0600
Subject: [R] Rd file--latex {array} help
Message-ID: <e0c8e805d43f8e950db28b8f9c7c4fd7@austin.rr.com>

I am trying to build the notation for combination as the function 
\code{choose} in a *.Rd file; but it is not working.
To reduce to essence

\deqn{  \left( \begin{array}{c}
            r-1   \\
            n
         \end{array} \right)
      }

I have double checked the latex logic (without \deqn{}) in LaTexIT on 
MacOSX so it seems fine.
I've read the R-ext.pdf and down some searching in the 
archives--stumped.

R CMD Rd2dvi --pdf

renders something like this

( r-1\n )

Advice?
Thanks.

Wil



From Scott.Waichler at pnl.gov  Sat Jan 14 06:24:31 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 13 Jan 2006 21:24:31 -0800
Subject: [R] change lattice panel background color
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0356456D@pnlmse35.pnl.gov>


I can't find a way to change just the panel background color in lattice.
I would like NA regions in levelplot() to appear black.  I've tried the
trellis.par.set() stuff, but it it makes the background of the whole
graphic black.

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at _ pnl.gov



From hb at maths.lth.se  Sat Jan 14 08:39:30 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 14 Jan 2006 18:39:30 +1100
Subject: [R] LaTeX slide show (Was: Re:  Taking code from packages)
In-Reply-To: <43C7A110.10808@stats.uwo.ca>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca>
Message-ID: <43C8AAB2.7030801@maths.lth.se>

Duncan Murdoch wrote:
> On 1/13/2006 2:04 AM, Ales Ziberna wrote:
> 
>>Hello!
>>
[snip]

> (I'm a little sensitive about dependencies now, since the LaTeX seminar 
> template I've used a few times no longer works.  It depends on too many 
> LaTeX packages, and someone, somewhere has introduced incompatibilities 
> in them.  Seems like I'll be forced to use Powerpoint or Impress.)

Try LaTeX Beamer!  It is the best thing that happend to LaTeX in a long 
time.  Simply beautiful, intuitive and very easy to use, and it's not 
yet another 'seminar' or 'prosper'.   Part of MikTeX now.  See 
http://latex-beamer.sourceforge.net/ for documentation, examples etc.

Cheers

Henrik

> Duncan Murdoch
> 

[snip]



From p.dalgaard at biostat.ku.dk  Sat Jan 14 11:17:18 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jan 2006 11:17:18 +0100
Subject: [R] how to obtain "par(ask=TRUE)" with trellis-plots
In-Reply-To: <eb555e660601110822s517f1840j6618997b3b90ac0a@mail.gmail.com>
References: <43C4FCF5.70001@anicca-vijja.de>
	<eb555e660601110822s517f1840j6618997b3b90ac0a@mail.gmail.com>
Message-ID: <x23bjr866p.fsf@turmalin.kubism.ku.dk>

Deepayan Sarkar <deepayan.sarkar at gmail.com> writes:

> On 1/11/06, Leo G????rtler <leog at anicca-vijja.de> wrote:
> > Dear alltogether,
> >
> > how can a delay like possible with par(ask=TRUE) be attained while using
> > trellis-plots within a loop or something like that?
> > the following draws each plot without waiting for a signal
> > (mouse-klick), so par() does not work for that:
> >
> > library(nlme)
> > for(i in 1:3)
> > {
> >   fitlme <- lme(Orthodont)
> >   par(ask=TRUE)                     # does not work with trellis....
> >   print( plot(augPred(fitlme)) )
> > }
> 
> See ?grid.prompt in the grid package. To use it you can either attach
> grid, or do
> 
> grid::grid.prompt(TRUE)

I happened to need this today (some example() sections are a bit
unhelpful without it) and the only way I figured it out was by recalling
this recent thread. Any chance of putting a suitable pointer in the
help pages for trellis.par.set and/or trellis.device? 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dieter.menne at menne-biomed.de  Sat Jan 14 12:29:16 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sat, 14 Jan 2006 11:29:16 +0000 (UTC)
Subject: [R] Syntax for linear mixed model
References: <E1ExXBL-00032r-00@smarthost3.mail.uk.easynet.net>
Message-ID: <loom.20060114T122419-428@post.gmane.org>

N.W.Galwey <nw.galwey <at> ukonline.co.uk> writes:

> 
> The syntax below works, and gives the expected results, in R version 2.0.0,
> provided that I have loaded packages Matrix, latticeExtra and lme4.
> However, I cannot get it to work in version 2.2.1.   Can anyone tell me how
> to fit this model in the more recent version?
> 
> 
> barleyprogeny.model1lme <- lme(yield_g_m2 ~ 1, 
> 
>    data = barleyprogeny.unbalanced, random = ~ 1|fline + fblock) 
> 

You must use the new style and call lmer when using package lme4 now. There is 
no full documentation on lmer yet since package lme4 is work in progress, but 
there are a few examples in the MEMSS package. 

However, I would suggest that for the rather simple model, you better don't 
load lme4 and Matrix, and use well-documented package nlme instead.

Dieter



From voodooochild at gmx.de  Sat Jan 14 16:08:43 2006
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Sat, 14 Jan 2006 16:08:43 +0100
Subject: [R] Different length of objects
Message-ID: <43C913FB.1070907@gmx.de>

Hello,

i got an warning message in the following code:

f<-1:100
t<-1:100
b<-100

ll2 <- function(b,f,t) {
  t<-cumsum(t)
  tn<-t[length(t)]
  i<-seq(along=f)
  s1<-(tn*exp(-b*tn)*sum(f[i]))/(1-exp(-b*tn))
  
s2<-sum((f[i]*(t[i]*exp(-b*t[i])-t[i-1]*exp(b*t[i-1])))/(exp(-b*t[i-1])-exp(-b*t[i])))
  s1-s2
}

ll2(b,f,t)

i think, the problem here is, that t[0] doesn't exist and so i got 
different length of objects. want can i do to avoid this error?
the assumption is that t[0] should be 0.

best regards
andreas



From voodooochild at gmx.de  Sat Jan 14 17:02:25 2006
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Sat, 14 Jan 2006 17:02:25 +0100
Subject: [R] Different length of objects
In-Reply-To: <644e1f320601140735q18bb6197u229a149de1e69d0a@mail.gmail.com>
References: <43C913FB.1070907@gmx.de>
	<644e1f320601140735q18bb6197u229a149de1e69d0a@mail.gmail.com>
Message-ID: <43C92091.7000200@gmx.de>

ok, thank you, in my problem i want to solve the following equation 
numericaly for b , t_n indicates the last value of t

(t_n*exp(-b*t_n)*sum_{i=1}^{n} f_i) / (1-exp(-b*t_n)) - (sum_{i=1}^{n} 
(f_i*(t_i*exp(-b*t_i)-t_{i-1}*exp(-b*t_{i-1}))) / 
(exp(-b*t_{i-1})-exp(-b*t_i)) )  = 0

b is then the mle.

jim holtman wrote:

> If you type
>  
> t[0]
>  
> you will get the value
>  
> numeric(0)
>  
> Which is a numeric vector of lenght 0; this is not the same a the 
> value of 0.
>  
> t[0] does not select any value.  If you expect this to happen in your 
> code, then check against it and assign whatever value you want:
>  
> ifelse(length(t[i]) == 0, 0, t[i])
>
>
>  
> On 1/14/06, *voodooochild at gmx.de <mailto:voodooochild at gmx.de>* 
> <voodooochild at gmx.de <mailto:voodooochild at gmx.de>> wrote:
>
>     Hello,
>
>     i got an warning message in the following code:
>
>     f<-1:100
>     t<-1:100
>     b<-100
>
>     ll2 <- function(b,f,t) {
>     t<-cumsum(t)
>     tn<-t[length(t)]
>     i<-seq(along=f)
>     s1<-(tn*exp(-b*tn)*sum(f[i]))/(1-exp(-b*tn))
>
>     s2<-sum((f[i]*(t[i]*exp(-b*t[i])-t[i-1]*exp(b*t[i-1])))/(exp(-b*t[i-1])-exp(-b*t[i])))
>
>     s1-s2
>     }
>
>     ll2(b,f,t)
>
>     i think, the problem here is, that t[0] doesn't exist and so i got
>     different length of objects. want can i do to avoid this error?
>     the assumption is that t[0] should be 0.
>
>     best regards
>     andreas
>
>     ______________________________________________
>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>     list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
>
>
>
>
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?



From murdoch at stats.uwo.ca  Sat Jan 14 17:39:31 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 14 Jan 2006 11:39:31 -0500
Subject: [R] LaTeX slide show (Was: Re:  Taking code from packages)
In-Reply-To: <43C8AAB2.7030801@maths.lth.se>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>	<43C7A110.10808@stats.uwo.ca>
	<43C8AAB2.7030801@maths.lth.se>
Message-ID: <43C92943.4090804@stats.uwo.ca>

On 1/14/2006 2:39 AM, Henrik Bengtsson wrote:
> Duncan Murdoch wrote:
>> On 1/13/2006 2:04 AM, Ales Ziberna wrote:
>>
>>> Hello!
>>>
> [snip]
> 
>> (I'm a little sensitive about dependencies now, since the LaTeX seminar 
>> template I've used a few times no longer works.  It depends on too many 
>> LaTeX packages, and someone, somewhere has introduced incompatibilities 
>> in them.  Seems like I'll be forced to use Powerpoint or Impress.)
> 
> Try LaTeX Beamer!  It is the best thing that happend to LaTeX in a long 
> time.  Simply beautiful, intuitive and very easy to use, and it's not 
> yet another 'seminar' or 'prosper'.   Part of MikTeX now.  See 
> http://latex-beamer.sourceforge.net/ for documentation, examples etc.

Thanks to Henrik and Stephen Eglen for this suggestion.  It does look 
nice (though the test presentation, beamerexample1.tex failed with

! Undefined control sequence.
<recently read> \rowcolors

l.937 \end{frame}

indicating some version incompatibility with what I've got installed, 
the simpler examples all seem to work and do indeed give nice output.)

Duncan Murdoch



From leog at anicca-vijja.de  Sat Jan 14 17:44:50 2006
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Sat, 14 Jan 2006 17:44:50 +0100
Subject: [R] lmer and handling heteroscedasticity
Message-ID: <43C92A82.2020806@anicca-vijja.de>

Dear altogether,

is it possible to integrate "weights" arguments within lmer to 
incorporate statements to handle heteroscedasticity as it is possible 
with lme?
I searched the R-archive but found nothing, insofer I assume it is not 
possible, but as lmer is under heavy develpoment, maybe something 
changed or is solved differently.

Thus my question:

While encountering heavy heteroscedasticity within data, lmer is not the 
right application to use, but use instead lme?

Thanks in advance for a short statement,

best ,
leo

-- 

email: leog at anicca-vijja.de
www: http://www.anicca-vijja.de/



From M.Anyadike-Danes at erini.ac.uk  Sat Jan 14 18:12:45 2006
From: M.Anyadike-Danes at erini.ac.uk (Michael Anyadike-Danes)
Date: Sat, 14 Jan 2006 17:12:45 -0000
Subject: [R] (no subject)
Message-ID: <C9328F0EEDC3BC439FDABD12060E910910A132@erini1.ERINI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060114/41ae645f/attachment.pl

From jkawczak at uncc.edu  Sat Jan 14 18:11:51 2006
From: jkawczak at uncc.edu (Janusz Kawczak)
Date: Sat, 14 Jan 2006 12:11:51 -0500 (EST)
Subject: [R] LaTeX slide show (Was: Re:  Taking code from packages)
In-Reply-To: <43C92943.4090804@stats.uwo.ca>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca>
	<43C8AAB2.7030801@maths.lth.se> <43C92943.4090804@stats.uwo.ca>
Message-ID: <Pine.GSO.4.55.0601141207001.10212@is-sm1.uncc.edu>

Duncan,

it seems that you did not update 'xcolor' package properly. That's where
\rowcolors is defined. In some instances, especially under MikTeX, the
automatic update fails to update xcolor.sty. So, I suggest you download
the latest xcolor package from the same place as the beamer package and
then manually run 'latex xcolor.ins' to regenerate xcolor.sty. Then move
the *.def and *.sty file to the proper places to gain full compatibility.

Janusz.

** Janusz Kawczak							**
** UNC at Charlotte, Department of Mathematics & Statistics, Room 345B	**
** 9201 University City Blvd          					**
** Charlotte, NC, 28223-0001, U.S.A.					**
** Tel.: (W) (704) 687-2566  Fax.: (704) 687-6415			**

"All truth passes through three stages. First, it is ridiculed, second it
is violently opposed, and third, it is accepted as self-evident."
-- Arthur Schopenhauer

On Sat, 14 Jan 2006, Duncan Murdoch wrote:

> On 1/14/2006 2:39 AM, Henrik Bengtsson wrote:
> > Duncan Murdoch wrote:
> >> On 1/13/2006 2:04 AM, Ales Ziberna wrote:
> >>
> >>> Hello!
> >>>
> > [snip]
> >
> >> (I'm a little sensitive about dependencies now, since the LaTeX seminar
> >> template I've used a few times no longer works.  It depends on too many
> >> LaTeX packages, and someone, somewhere has introduced incompatibilities
> >> in them.  Seems like I'll be forced to use Powerpoint or Impress.)
> >
> > Try LaTeX Beamer!  It is the best thing that happend to LaTeX in a long
> > time.  Simply beautiful, intuitive and very easy to use, and it's not
> > yet another 'seminar' or 'prosper'.   Part of MikTeX now.  See
> > http://latex-beamer.sourceforge.net/ for documentation, examples etc.
>
> Thanks to Henrik and Stephen Eglen for this suggestion.  It does look
> nice (though the test presentation, beamerexample1.tex failed with
>
> ! Undefined control sequence.
> <recently read> \rowcolors
>
> l.937 \end{frame}
>
> indicating some version incompatibility with what I've got installed,
> the simpler examples all seem to work and do indeed give nice output.)
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Sat Jan 14 18:17:54 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 14 Jan 2006 18:17:54 +0100
Subject: [R] problems installing lattice on Windows; was:  (no subject)
In-Reply-To: <C9328F0EEDC3BC439FDABD12060E910910A132@erini1.ERINI.local>
References: <C9328F0EEDC3BC439FDABD12060E910910A132@erini1.ERINI.local>
Message-ID: <43C93242.5070408@statistik.uni-dortmund.de>

Michael Anyadike-Danes wrote:

> I am having difficulty installing the package "lattice". 
> 
>  
> 
> First I tried downloading it from the CRAN site (in the normal way) :
> the message said it worked but when I typed library I got an error
> message ("there is no package").


lattice is shipped with R. You don't need to install it separately 
unless you want a more recent version in which case you simply can use 
update.packages().


> Second I tried installing it from the local zip. I have reproduced the
> result below.
> 
>  
> 
> utils:::menuInstallLocal()
> 
> package 'lattice' successfully unpacked and MD5 sums checked
> 
> Warning: cannot remove prior installation of package 'lattice'


So you had lattice loaded once you decided to update. This does not work 
under Windows as you have certainly already read in the R for Windows FAQs.

Start R without loading lattice and then update the package.


> updating HTML package descriptions
> 
> 
>>library(lattice)
> 
> 
> Error in library(lattice) : there is no package called 'lattice'
> 
>  
> 
> Have I missed something? 

Yes: reading the manuals and the posting guide (which also asks you to 
use a sensible subject line).
I assume this is R under Windows, and I hope a recent copy such as 
"R-2.2.1".

Uwe Ligges


>  
> 
> Apologies if it is something really obvious.
> 
>  
> 
> Michael Anyadike-Danes
> 
> Economic Research Institute of Northern Ireland
> 
> Floral Buildings
> 
> 2-14 East Bridge Street
> 
> Belfast BT1 3NQ
> 
> Tel:  (028) 90727362
> 
> Fax: (028) 90319003
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Sat Jan 14 18:43:43 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 14 Jan 2006 11:43:43 -0600
Subject: [R] change lattice panel background color
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0356456D@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0356456D@pnlmse35.pnl.gov>
Message-ID: <eb555e660601140943j678cd5a6gdf0ea9f2299dd188@mail.gmail.com>

On 1/13/06, Waichler, Scott R <Scott.Waichler at pnl.gov> wrote:
>
> I can't find a way to change just the panel background color in lattice.
> I would like NA regions in levelplot() to appear black.  I've tried the
> trellis.par.set() stuff, but it it makes the background of the whole
> graphic black.

Use panel.fill in a panel function, e.g. (untested)

panel = function(...) {
    panel.fill(col = "black")
    panel.levelplot(...)
}

-Deepayan



From dusa.adrian at gmail.com  Sat Jan 14 18:47:19 2006
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Sat, 14 Jan 2006 17:47:19 +0000 (UTC)
Subject: [R] R newbie example code question
References: <A8B87FDB74320349A9D1CC9021052A7646656E@exchange.psg.com>
	<XFMail.060110133037.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <loom.20060114T184227-307@post.gmane.org>

 <Ted.Harding <at> nessie.mcc.ac.uk> writes:
> [...]
> The solution I finally opted for, and still use,
> is based (in a Linux environment) on including
> the following code in your .Rprofile file:
> 
> .xthelp <- function() {
>     tdir <- tempdir()
>     pgr <- paste(tdir, "/pgr", sep="")
>     con <- file(pgr, "w")
>     cat("#! /bin/bash\n", file=con)
>     cat("export HLPFIL=`mktemp ", tdir, "/R_hlp.XXXXXX`\n",
>          sep="", file=con)
>     cat("cat > $HLPFIL\nxterm -e less $HLPFIL &\n", file=con)
>     close(con)
>     system(paste("chmod 755 ", pgr, sep=""))
>     options(pager=pgr)
> }
> .xthelp()
> rm(.xthelp)
> 
> (and it's also specific to the 'bash' shell because
> of the "#! /bin/bash\n", but you should be able to
> change this appropriately). The above was posted by
> Roger Bivand on 27 May.
> [...]

I also like the function, it's beautiful. I wonder if anyone could help me with
the correct syntax for my bash shell (I assume this is the problem) because I
get this error:

Error in rm(.xthelp) : cannot remove variables from base namespace

when starting R and when installing a new package. 
Thank you,
Adrian



From adi at roda.ro  Sat Jan 14 18:53:46 2006
From: adi at roda.ro (Adrian DUSA)
Date: Sat, 14 Jan 2006 19:53:46 +0200
Subject: [R] R newbie example code question
In-Reply-To: <XFMail.060113154553.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.060113154553.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200601141953.47025.adi@roda.ro>

On Friday 13 January 2006 17:45, Ted Harding wrote:
> On 13-Jan-06 Michael Friendly wrote:
> > Ted:
> >
> > Your .xthelp is extremely useful, help on Linux being otherwise
> > quite awkward to use since a pager in the same window make it hard
> > to cut/paste examples --- where 'more' or 'less' really means
> > 'instead of' :-)
>
> Glad you found it useful. I find it indispensable!
> For the record: this is not my code but Roger Bivand's,
> it being the one out of several suggestions on that thread
> which I decided to adopt. I still admire the neat way he
> wrapped it all up.
> [...snip...]

I also like the function very much, but I get an annoying error when starting 
R or when installing a new package:

Error in rm(.xthelp) : cannot remove variables from base namespace

I assume it has something to do with my bash shell, but I have no idea what to 
do. I run I inside Kubuntu 5.10 Breezy (compiled from source).

> R.Version()
$platform
[1] "i686-pc-linux-gnu"
$arch
[1] "i686"
$os
[1] "linux-gnu"
$system
[1] "i686, linux-gnu"
$status
[1] ""
$major
[1] "2"
$minor
[1] "2.1"
$year
[1] "2005"
$month
[1] "12"
$day
[1] "20"
$"svn rev"
[1] "36812"
$language
[1] "R"

Thank you,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From marcodoc75 at yahoo.com  Sat Jan 14 20:06:52 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Sat, 14 Jan 2006 11:06:52 -0800 (PST)
Subject: [R] better way to replace missing values with zero
In-Reply-To: <1db726800601131022sb2af6d0j9445bb94fa503315@mail.gmail.com>
Message-ID: <20060114190652.54722.qmail@web31311.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060114/c60a7c1c/attachment.pl

From hastie at stanford.edu  Sat Jan 14 22:11:51 2006
From: hastie at stanford.edu (Trevor Hastie)
Date: Sat, 14 Jan 2006 13:11:51 -0800
Subject: [R] Data Mining Course
Message-ID: <567EE126-A3BD-44C2-AFF9-176B02F40ABF@stanford.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060114/81b1d40c/attachment.pl

From spencer.graves at pdf.com  Sat Jan 14 22:24:57 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 Jan 2006 13:24:57 -0800
Subject: [R] glmmPQL and variance structure
In-Reply-To: <43BFA0E8.1090600@univ-fcomte.fr>
References: <43B17D9B.3080107@univ-fcomte.fr> <43B81DC4.9070806@pdf.com>
	<43BFA0E8.1090600@univ-fcomte.fr>
Message-ID: <43C96C29.6040908@pdf.com>

	  Thanks for providing a partially reproducible example.  I believe the 
error message you cite came from "lme".  I say this, because I modified 
your call to "glmmPQL2" to call lme and got the following:

 > library(nlme)
 > fit.lme <- lme(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          data = bacteria, weights=varPower(~1))
Error in unlist(x, recursive, use.names) :
	argument not a list

	  I consulted Pinheiro and Bates (2000) Mixed-Effects Models in S and 
S-Plus (Springer, sec. 5.2, p.211) to see that the syntax for "varPower" 
appears to be correct.  I removed "~" and it worked, mostly:

 > fit.lme <- lme(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          data = bacteria, weights=varPower(1))
Warning message:
- not meaningful for factors in: Ops.factor(y[revOrder], Fitted)

	  I got "an answer", though with a warning and not for the problem you 
want to solve.  However, I then made this modification to a call to my 
own modification of Venables and Ripley's glmmPQL and it worked:

 > fit. <- glmmPQL.(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          family = binomial, data = bacteria,
+          weights.lme=varPower(1))
iteration 1
iteration 2
iteration 3
 > fit.
Linear mixed-effects model fit by maximum likelihood
   Data: bacteria
   Log-likelihood: -541.0882
   Fixed: y ~ trt + I(week > 2)
     (Intercept)         trtdrug        trtdrug+ I(week > 2)TRUE
       2.7742329      -1.0852566      -0.5896635      -1.2682626

Random effects:
  Formula: ~1 | ID
          (Intercept) Residual
StdDev: 4.940885e-05 2.519018

Variance function:
  Structure: Power of variance covariate
  Formula: ~fitted(.)
  Parameter estimates:
     power
0.3926788
Number of Observations: 220
Number of Groups: 50
 >
	  This function "glmmPQL." adds an argument "weights.lme" to Venables 
and Ripley's "glmmPQL" and uses that in place of 
'quote(varFixed(~invwt))' when provided;  see below.

	  hope this helps.
	  spencer graves
glmmPQL. <-
function (fixed, random, family, data, correlation, weights,
     weights.lme, control, niter = 10, verbose = TRUE, ...)
{
     if (!require("nlme"))
         stop("package 'nlme' is essential")
     if (is.character(family))
         family <- get(family)
     if (is.function(family))
         family <- family()
     if (is.null(family$family)) {
         print(family)
         stop("'family' not recognized")
     }
     m <- mcall <- Call <- match.call()
     nm <- names(m)[-1]
     wts.lme <- mcall$weights.lme
     keep <- is.element(nm, c("weights", "data", "subset", "na.action"))
     for (i in nm[!keep]) m[[i]] <- NULL
     allvars <- if (is.list(random))
         allvars <- c(all.vars(fixed), names(random), unlist(lapply(random,
             function(x) all.vars(formula(x)))))
     else c(all.vars(fixed), all.vars(random))
     Terms <- if (missing(data))
         terms(fixed)
     else terms(fixed, data = data)
     off <- attr(Terms, "offset")
     if (length(off <- attr(Terms, "offset")))
         allvars <- c(allvars, as.character(attr(Terms, "variables"))[off +
             1])
     m$formula <- as.formula(paste("~", paste(allvars, collapse = "+")))
     environment(m$formula) <- environment(fixed)
     m$drop.unused.levels <- TRUE
     m[[1]] <- as.name("model.frame")
     mf <- eval.parent(m)
     off <- model.offset(mf)
     if (is.null(off))
         off <- 0
     w <- model.weights(mf)
     if (is.null(w))
         w <- rep(1, nrow(mf))
     mf$wts <- w
     fit0 <- glm(formula = fixed, family = family, data = mf,
         weights = wts, ...)
     w <- fit0$prior.weights
     eta <- fit0$linear.predictor
     zz <- eta + fit0$residuals - off
     wz <- fit0$weights
     fam <- family
     nm <- names(mcall)[-1]
     keep <- is.element(nm, c("fixed", "random", "data", "subset",
         "na.action", "control"))
     for (i in nm[!keep]) mcall[[i]] <- NULL
     fixed[[2]] <- quote(zz)
     mcall[["fixed"]] <- fixed
     mcall[[1]] <- as.name("lme")
     mcall$random <- random
     mcall$method <- "ML"
     if (!missing(correlation))
         mcall$correlation <- correlation
#   weights.lme
     {
      if(is.null(wts.lme))
        mcall$weights <- quote(varFixed(~invwt))
      else
        mcall$weights <- wts.lme
    }
     mf$zz <- zz
     mf$invwt <- 1/wz
     mcall$data <- mf
     for (i in 1:niter) {
         if (verbose)
             cat("iteration", i, "\n")
         fit <- eval(mcall)
         etaold <- eta
         eta <- fitted(fit) + off
         if (sum((eta - etaold)^2) < 1e-06 * sum(eta^2))
             break
         mu <- fam$linkinv(eta)
         mu.eta.val <- fam$mu.eta(eta)
         mf$zz <- eta + (fit0$y - mu)/mu.eta.val - off
         wz <- w * mu.eta.val^2/fam$variance(mu)
         mf$invwt <- 1/wz
         mcall$data <- mf
     }
     attributes(fit$logLik) <- NULL
     fit$call <- Call
     fit$family <- family
     oldClass(fit) <- c("glmmPQL", oldClass(fit))
     fit
}

	

Patrick Giraudoux wrote:
> Dear listers,
> 
> On the line of a last (unanswered) question about glmmPQL() of the 
> library MASS, I am still wondering if it is possible to pass a variance 
> structure object  to the call to lme() within the functions (e.g. 
> weights=varPower(1), etc...). The current weights argument of glmmPQL is 
> actually used for a call to glm -and not for lme). I have tried to go 
> through the code, and gathered that the variance structure passed to the 
> call to lme()  was:
> 
> mcall$weights <- quote(varFixed(~invwt))
> 
> and this cannot be modified by and argument of glmmPQL().
> 
> I have tried to modify the script a bit wildly  and changed varFixed 
> into VarPower(~1), in a glmmPQL2 function. I get the following error:
> 
>  > glmmPQL2(y ~ trt + I(week > 2), random = ~ 1 | ID,
> + family = binomial, data = bacteria)
> iteration 1
> Error in unlist(x, recursive, use.names) :
>         argument not a list
> 
> I get the same error whatever the change in variance structure on this line.
> 
> Beyond this I wonder why variance structure cannot be passed to lme via 
> glmmPQL...
> 
> Any idea?
> 
> Patrick Giraudoux
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Sat Jan 14 22:46:05 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 14 Jan 2006 15:46:05 -0600
Subject: [R] how to obtain "par(ask=TRUE)" with trellis-plots
In-Reply-To: <x23bjr866p.fsf@turmalin.kubism.ku.dk>
References: <43C4FCF5.70001@anicca-vijja.de>
	<eb555e660601110822s517f1840j6618997b3b90ac0a@mail.gmail.com>
	<x23bjr866p.fsf@turmalin.kubism.ku.dk>
Message-ID: <eb555e660601141346l49d33250mfd4ef6e07ddfe58a@mail.gmail.com>

On 14 Jan 2006 11:17:18 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Deepayan Sarkar <deepayan.sarkar at gmail.com> writes:

[...]

> > See ?grid.prompt in the grid package. To use it you can either attach
> > grid, or do
> >
> > grid::grid.prompt(TRUE)
>
> I happened to need this today (some example() sections are a bit
> unhelpful without it) and the only way I figured it out was by recalling
> this recent thread. Any chance of putting a suitable pointer in the
> help pages for trellis.par.set and/or trellis.device?

Will do.

Deepayan



From zhaosz at umd.umich.edu  Sat Jan 14 22:48:09 2006
From: zhaosz at umd.umich.edu (Shaozhong Zhao)
Date: Sat, 14 Jan 2006 16:48:09 -0500
Subject: [R] Can I call a R function from within C/C++ directly?
Message-ID: <200601141648091093041@umd.umich.edu>

Dear all R users????

	Can I call a R function from within C/C++ directly? I mean don't run R. Thank you!

????????????????????????????????
Regards,
Shaozhong Zhao
 				
zhaosz at umd.umich.edu
2006-01-13????????????????????????????????????????
????????????????????????????????
????????????????????????????????



From Yingfu.Xie at sekon.slu.se  Sat Jan 14 22:52:43 2006
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Sat, 14 Jan 2006 22:52:43 +0100
Subject: [R] Can I ask for the C code inside an R function using .C?
	(Thanks)
References: <BAY103-F2574D017E6ADC338B09ED9A6260@phx.gbl>
Message-ID: <CA871298CD1882459F7859BD08DC06E4C9C088@slumail.ad.slu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060114/887e5096/attachment.pl

From rbaer at atsu.edu  Sat Jan 14 23:30:32 2006
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Sat, 14 Jan 2006 16:30:32 -0600
Subject: [R] example(plot) question
Message-ID: <004001c6195a$21d31330$6401a8c0@ALKAID>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060114/5bc2e8a5/attachment.pl

From murdoch at stats.uwo.ca  Sat Jan 14 23:35:48 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 14 Jan 2006 17:35:48 -0500
Subject: [R] Can I call a R function from within C/C++ directly?
In-Reply-To: <200601141648091093041@umd.umich.edu>
References: <200601141648091093041@umd.umich.edu>
Message-ID: <43C97CC4.9090903@stats.uwo.ca>

On 1/14/2006 4:48 PM, Shaozhong Zhao wrote:
> Dear all R users????
> 
> 	Can I call a R function from within C/C++ directly? I mean don't run R. Thank you!

Yes.  See the R Extensions manual, in particular chapter 7, "Linking 
GUIs and other front ends to R".

Duncan Murdoch



From pensterfuzzer at yahoo.de  Sun Jan 15 00:16:46 2006
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 15 Jan 2006 00:16:46 +0100 (CET)
Subject: [R] R-help Digest, Vol 35, Issue 14
Message-ID: <20060114231646.8273.qmail@web25801.mail.ukl.yahoo.com>

Dear all,

Is anybody aware of a tutorial, introduction, overview
or alike  for cluster 
analysis with R? I have been searching for something
like that but it seems 
there are only a few rather specialized articles
around.

I would very much appreciate any hint.

Thanks a million,
   Werner


	

	
		
___________________________________________________________ 
Telefonate ohne weitere Kosten vom PC zum PC: http://messenger.yahoo.de



From spencer.graves at pdf.com  Sun Jan 15 01:32:47 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 Jan 2006 16:32:47 -0800
Subject: [R] panel data unit root tests
In-Reply-To: <1136584568.43bee778927e6@www2.helsinki.fi>
References: <1136584568.43bee778927e6@www2.helsinki.fi>
Message-ID: <43C9982F.6080601@pdf.com>

	  Can anyone help with questions on correlations in lme?  I provide 
below a self-contained toy example that I can't quite complete.

QUESTIONS:
1.  Is there a unit root test in R?  Below please find a failed
attempt to do this using "lme".
2.  Is there a corStruct class for first differences (unit root)?
"corAR1(1)" generates an error.
3.  How can store in another R object the estimated AR(1) coefficient 
from an lme object generated with 'lme(..., 
correlation=corAR1(form=~1|subj))'?
4.  Why am I getting small p-values in testing for a unit root when I
simulate an AR1(1)?

TOY EXAMPLE

nSubj <- 4; nObs <- 5; nSims <- 2
UnitRootSims <- array(NA, dim=c(nSims, 2),
       dimnames=list(NULL, c("AR1", "p.value")))
corAR1. <- corAR1(form=~1|subj)
corUnitRoot <- corAR1(1, form=~1|subj, fixed=TRUE)
# corAR1(1) not allowed.
corUnitRoot <- corAR1(0.99999, form=~1|subj, fixed=TRUE)

set.seed(123)
library(nlme)
for(i in 1:nSims){
   e.subj <- array(rnorm(nObs*nSubj), dim=c(nObs, nSubj),
       dimnames=list(paste("obs", 1:nObs, sep=""), subj))
   y.subj <- apply(e.subj, 2, cumsum)
   Dat <- data.frame(subj=rep(subj, each=nObs),
                   y=as.vector(y.subj))
   fitAR1 <- lme(y~1, data=Dat, random=~1|subj,
        correlation=corAR1., method="ML")
# UnitRootSims[i, 1] <- ??? How to extract the AR1 estimate?
   fitUnitRoot <- lme(y~1, data=Dat, random=~1|subj,
        correlation=corUnitRoot, method="ML")
   aovUnitRoot <- anova(fitAR1, fitUnitRoot)
   UnitRootSims[i, 2] <- aovUnitRoot[2, "p-value"]
}
 > UnitRootSims
      AR1      p.value
[1,]  NA 8.277712e-11
[2,]  NA 1.174823e-08
# Why are the p-values so small?  Shouldn't they be insignificant?

	  Thanks,
	  Spencer Graves

############################
	  If replies to this post will no longer be useful for you, then please
discregard this comment.  If not, I will start by saying that I could
not understand enough of your question to respond directly.
RSiteSearch("panel unit root") led me to an exchange we had following a
related question you posted last October
(ttp://finzi.psych.upenn.edu/R/Rhelp02a/archive/63545.html).  Did you
try the nlme package as suggested there?  I've just now looked at that,
and I confess I could not figure out how to do it in a few minutes.

	  Do you want to perform a unit root test for a particular panel data
set you have?  Or do you want to develop software for a particular panel
unit root test?  If the former, I suggest you prepare a very simple toy
example trying to do something like this using lme with perhaps corAR1,
then send this list a question on whether it is possible to do what you
want with lme, asking how to take the next step with the toy example.
If rather you want to develop software for a particular unit root test,
then I suggest you at least provide a more complete citation than just
"a la Arellano & Bond."  In either case, I also suggest you review the
posting guide! "www.R-project.org/posting-guide.html" and try to make
your post as easily understood as possible.  In general, I think that
posts that are clear and succinct tend to get quicker, more useful
answers.  Maybe I'm just dense, but I don't understand what you are
asking.  For example, your code includes "print(levinlin(ws, year, id,
lags = 3))".  What is the "levinlin" function?  RSiteSearch("levinlin")
produced zero hits.

	  hope this helps.
	  spencer graves

jukka ruohonen wrote:

> When finally got some time to do some coding, I started and stopped right 
> after. The stationary test is a good starting point because it demonstrates 
> how we should be able to move the very basic R matrices. I have a real-
> world small N data set with 
> 
> rows:
> id(n=1)---t1---variable1
> ...
> id=(N=20)---T=21---variable1
> 
> Thus, a good test case. For first id I was considering something like this:
> 
> lag <- as.integer(lags)
> lags.p <- lags + 1
> id <- unique(group)
> id.l <- length(id)
> y.l <- length(y)
> yid.l <- length(y)/id.l
>   if (lag > yid.l -2) 
>         stop("\nlag too long for defined cross-sections.\n")
> 
> #for (i in id) {
>   lagy <- y[2:yid.l]
>       lagy.em <- embed(lagy, lags)
>   id.l <- length(id)
>   dy <- diff(y)[1:yid.l-1]
>       dy.em <- embed(dy, lags)
> #     }
> print(levinlin(ws, year, id, lags = 3))
> 
> Couldn't figure the loop over units out but with N = 1 the data 
> transformation seemed to work just fine. Now we should pool the new 
> variables within the panel and regress y over yt-1 and dy-t1 +....+ dy-t-j 
> with, say, BIC doing the job for d's (H0: y-1 ~ 0) for each in the panel. 
> 
> Now the above example puts the right-hand on columns, and if we are dealing 
> with panel models in general, we should store the new variables together 
> with dX's, which should then give clues to IV estimator with e.g. 
> orthogonal deviations, e.g. k <- y ~ yt-1 + x + as.factor(id)). So one 
> confusing part is the requirement of some big storage base for different 
> matrices doing the IV business with lags/levels - the amount of instruments 
> can be enormous with possibly calculation problems in a GMM dynamic panel 
> estimator a la Arellano & Bond. Therefore one should code the theoretically 
> relevant instruments beforehand with various transformation matrices. Thus, 
> should I start to study something that can be done with the newly added 
> SparseM package? 
> 
> Regards,
> 
> Jukka Ruohonen.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sun Jan 15 02:59:06 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 Jan 2006 17:59:06 -0800
Subject: [R] glmmPQL and variance structure
In-Reply-To: <43BFA0E8.1090600@univ-fcomte.fr>
References: <43B17D9B.3080107@univ-fcomte.fr> <43B81DC4.9070806@pdf.com>
	<43BFA0E8.1090600@univ-fcomte.fr>
Message-ID: <43C9AC6A.3020301@pdf.com>

	  I just identified an error in my recent post on this subject:  There 
is a very good reason that Venables & Ripley's "glmmPQL" did NOT include 
an argument like the "weights.lme" in the "glmmPQL." included in my 
recent post:  Their function calls "glm" first and then provides weights 
computed by "glm" to "lme".  If you want other weights, you need to 
consider appropriately the weights computed by "glm".  It may be 
reasonable to make your other weights proportional to the "glm" weights, 
but it would not be smart to do as I suggested a few hours ago, which 
ignored the "glm" weights.

	  I hope this error doesn't seriously inconvenience you or anyone else 
who might read these comments.

	  spencer graves

####################################
	  Thanks for providing a partially reproducible example.  I believe the
error message you cite came from "lme".  I say this, because I modified
your call to "glmmPQL2" to call lme and got the following:

> library(nlme)
> fit.lme <- lme(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          data = bacteria, weights=varPower(~1))
Error in unlist(x, recursive, use.names) :
	argument not a list

	  I consulted Pinheiro and Bates (2000) Mixed-Effects Models in S and
S-Plus (Springer, sec. 5.2, p.211) to see that the syntax for "varPower"
appears to be correct.  I removed "~" and it worked, mostly:

> fit.lme <- lme(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          data = bacteria, weights=varPower(1))
Warning message:
- not meaningful for factors in: Ops.factor(y[revOrder], Fitted)

	  I got "an answer", though with a warning and not for the problem you
want to solve.  However, I then made this modification to a call to my
own modification of Venables and Ripley's glmmPQL and it worked:

> fit. <- glmmPQL.(y ~ trt + I(week > 2), random = ~ 1 | ID,
+          family = binomial, data = bacteria,
+          weights.lme=varPower(1))
iteration 1
iteration 2
iteration 3
> fit.
Linear mixed-effects model fit by maximum likelihood
   Data: bacteria
   Log-likelihood: -541.0882
   Fixed: y ~ trt + I(week > 2)
     (Intercept)         trtdrug        trtdrug+ I(week > 2)TRUE
       2.7742329      -1.0852566      -0.5896635      -1.2682626

Random effects:
  Formula: ~1 | ID
          (Intercept) Residual
StdDev: 4.940885e-05 2.519018

Variance function:
  Structure: Power of variance covariate
  Formula: ~fitted(.)
  Parameter estimates:
     power
0.3926788
Number of Observations: 220
Number of Groups: 50
>
	  This function "glmmPQL." adds an argument "weights.lme" to Venables
and Ripley's "glmmPQL" and uses that in place of
'quote(varFixed(~invwt))' when provided;  see below.

	  hope this helps.
	  spencer graves
glmmPQL. <-
function (fixed, random, family, data, correlation, weights,
     weights.lme, control, niter = 10, verbose = TRUE, ...)
{
     if (!require("nlme"))
         stop("package 'nlme' is essential")
     if (is.character(family))
         family <- get(family)
     if (is.function(family))
         family <- family()
     if (is.null(family$family)) {
         print(family)
         stop("'family' not recognized")
     }
     m <- mcall <- Call <- match.call()
     nm <- names(m)[-1]
     wts.lme <- mcall$weights.lme
     keep <- is.element(nm, c("weights", "data", "subset", "na.action"))
     for (i in nm[!keep]) m[[i]] <- NULL
     allvars <- if (is.list(random))
         allvars <- c(all.vars(fixed), names(random), unlist(lapply(random,
             function(x) all.vars(formula(x)))))
     else c(all.vars(fixed), all.vars(random))
     Terms <- if (missing(data))
         terms(fixed)
     else terms(fixed, data = data)
     off <- attr(Terms, "offset")
     if (length(off <- attr(Terms, "offset")))
         allvars <- c(allvars, as.character(attr(Terms, "variables"))[off +
             1])
     m$formula <- as.formula(paste("~", paste(allvars, collapse = "+")))
     environment(m$formula) <- environment(fixed)
     m$drop.unused.levels <- TRUE
     m[[1]] <- as.name("model.frame")
     mf <- eval.parent(m)
     off <- model.offset(mf)
     if (is.null(off))
         off <- 0
     w <- model.weights(mf)
     if (is.null(w))
         w <- rep(1, nrow(mf))
     mf$wts <- w
     fit0 <- glm(formula = fixed, family = family, data = mf,
         weights = wts, ...)
     w <- fit0$prior.weights
     eta <- fit0$linear.predictor
     zz <- eta + fit0$residuals - off
     wz <- fit0$weights
     fam <- family
     nm <- names(mcall)[-1]
     keep <- is.element(nm, c("fixed", "random", "data", "subset",
         "na.action", "control"))
     for (i in nm[!keep]) mcall[[i]] <- NULL
     fixed[[2]] <- quote(zz)
     mcall[["fixed"]] <- fixed
     mcall[[1]] <- as.name("lme")
     mcall$random <- random
     mcall$method <- "ML"
     if (!missing(correlation))
         mcall$correlation <- correlation
#   weights.lme
     {
      if(is.null(wts.lme))
        mcall$weights <- quote(varFixed(~invwt))
      else
        mcall$weights <- wts.lme
    }
     mf$zz <- zz
     mf$invwt <- 1/wz
     mcall$data <- mf
     for (i in 1:niter) {
         if (verbose)
             cat("iteration", i, "\n")
         fit <- eval(mcall)
         etaold <- eta
         eta <- fitted(fit) + off
         if (sum((eta - etaold)^2) < 1e-06 * sum(eta^2))
             break
         mu <- fam$linkinv(eta)
         mu.eta.val <- fam$mu.eta(eta)
         mf$zz <- eta + (fit0$y - mu)/mu.eta.val - off
         wz <- w * mu.eta.val^2/fam$variance(mu)
         mf$invwt <- 1/wz
         mcall$data <- mf
     }
     attributes(fit$logLik) <- NULL
     fit$call <- Call
     fit$family <- family
     oldClass(fit) <- c("glmmPQL", oldClass(fit))
     fit
}

	

Patrick Giraudoux wrote:
> Dear listers,
> 
> On the line of a last (unanswered) question about glmmPQL() of the 
> library MASS, I am still wondering if it is possible to pass a variance 
> structure object  to the call to lme() within the functions (e.g. 
> weights=varPower(1), etc...). The current weights argument of glmmPQL is 
> actually used for a call to glm -and not for lme). I have tried to go 
> through the code, and gathered that the variance structure passed to the 
> call to lme()  was:
> 
> mcall$weights <- quote(varFixed(~invwt))
> 
> and this cannot be modified by and argument of glmmPQL().
> 
> I have tried to modify the script a bit wildly  and changed varFixed 
> into VarPower(~1), in a glmmPQL2 function. I get the following error:
> 
>  > glmmPQL2(y ~ trt + I(week > 2), random = ~ 1 | ID,
> + family = binomial, data = bacteria)
> iteration 1
> Error in unlist(x, recursive, use.names) :
>         argument not a list
> 
> I get the same error whatever the change in variance structure on this line.
> 
> Beyond this I wonder why variance structure cannot be passed to lme via 
> glmmPQL...
> 
> Any idea?
> 
> Patrick Giraudoux
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sun Jan 15 06:13:37 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 05:13:37 +0000 (GMT)
Subject: [R] R-help Digest, Vol 35, Issue 14
In-Reply-To: <20060114231646.8273.qmail@web25801.mail.ukl.yahoo.com>
References: <20060114231646.8273.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0601150512420.28808@gannet.stats>

On Sun, 15 Jan 2006, Werner Wernersen wrote:

> Is anybody aware of a tutorial, introduction, overview
> or alike  for cluster
> analysis with R? I have been searching for something
> like that but it seems
> there are only a few rather specialized articles
> around.

Chapter 11 of MASS (the book discussed in the FAQ).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Jan 15 06:49:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 05:49:15 +0000 (GMT)
Subject: [R] example(plot) question
In-Reply-To: <004001c6195a$21d31330$6401a8c0@ALKAID>
References: <004001c6195a$21d31330$6401a8c0@ALKAID>
Message-ID: <Pine.LNX.4.61.0601150502080.28808@gannet.stats>

You can either open a graphics device with history

windows(record = TRUE)

or open a graphics device and set par(ask=TRUE) to be asked for 
confirmation after each plot (as most demos do).

Finally, if you always want graphics recorded on a windows() device, use

options(graphics.record = TRUE)

as documented in ?windows.  (I am not sure how you would know about plot 
history without knowing that was the relevant device: e.g. both the rw-FAQ 
and the Windows README tell you.)


On Sat, 14 Jan 2006, Robert W. Baer, Ph.D. wrote:

>> From Windows RGui, I tried:
> example(plot)
>
> This worked as expected except that the various plots write over top of 
> each other in the graphics window that is created.  I then discovered 
> that graphics history is not enabled during the example generation.
>
> I used the graphics window menu to enable the history and repeated 
> example(plot), but this raises a couple of questions:
>
> Is there a command line equivalent for activating plot history?  Is plot 
> history a par() or options() setting or is it intrinsic to an individual 
> graphics window?
>
> Is there an argument to example() that enables plot history and/or 
> wouldn't it make sense to have this activated by default whenever an 
> example() execution creates plots?
>
> I tried:
>
> help.search('plot record')
> help.search('plot history')
> help.search('history recording')
>
> without finding anything insightful to help with this topic, but I'm 
> just betting more than one of you knows what I should have typed or 
> where I should look<G>.
>
> Thanks,
>
> Rob
>
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.0
> year     2005
> month    10
> day      06
> svn rev  35749
> language R
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vivek.satsangi at gmail.com  Sun Jan 15 12:06:22 2006
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Sun, 15 Jan 2006 06:06:22 -0500
Subject: [R] / Operator not meaningful for factors
Message-ID: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>

Folks,
I have a very basic question. The solution eludes me perhaps because
of my own lack of creativity. I am not attaching a fully reproducible
session because the issue may well be becuase of the way the data file
is, and the data file is large (and I don't know whether I can legally
distribute it). If people can suggest things that might be wrong in my
data or the way that I am reading it, I would be most grateful.

I get the following error message in the session quoted at the end of
this email:
/ not meaningful for factors in: Ops.factor(BookValuePS, Price)

As you can see in that some session, I check that the two vectors
being divided are numeric. I also check that the divisor is not 0 at
any index. I also believe that this is not because of the NA's in the
data. My question is, what are other "problems" that can cause the /
operator to not be meaningful?

I did try some simple examples to try to get the same error. However, 
I am not sure how to put the same NA's that one  gets from
read.table() into a vector:
> a <- c(1, 2, 3, NA);
> a
[1]  1  2  3 NA
> b <- c( 1, 2, 3, 4);
> c <- b / a;
> b
[1] 1 2 3 4
> a <- c(1, 2, 3, );
> c <- b/a;
Warning message:
longer object length
        is not a multiple of shorter object length in: b/a


******** Quoted Session below ********
 > explainPriceSimplified <- read.table("combinedClean.csv",
+                            sep = ",", header=TRUE);
> attach(explainPriceSimplified);
> summary(explainPriceSimplified);
     Symbol           Date              Price            EPS          
   BookValuePS
 XL     :   98   Min.   :19870630   22     :   61   Min.   :-1.401e+05
  Min.   :-6.901e+05
 ZION   :   97   1st Qu.:19910930   26.5   :   61   1st Qu.: 4.650e-01
  1st Qu.: 3.892e+00
 YRCW   :   72   Median :19960331   27.5   :   58   Median : 1.060e+00
  Median : 7.882e+00
 AA     :   71   Mean   :19957688   30     :   58   Mean   :-1.534e+01
  Mean   : 1.515e+02
 ABS    :   71   3rd Qu.:20001231   25     :   56   3rd Qu.: 1.890e+00
  3rd Qu.: 1.444e+01
 ABT    :   71   Max.   :20041231   (Other):29561   Max.   : 5.309e+03
  Max.   : 3.366e+06
 (Other):29624                      NA's   :  249   NA's   : 2.460e+02
  NA's   : 4.760e+02
 FiscalQuarterRep    F12MRet
 2004/2F:  482    Min.   :-100.00
 2003/4F:  471    1st Qu.:  -8.82
 2004/1F:  470    Median :  10.57
 2004/3F:  470    Mean   :  13.36
 2003/3F:  464    3rd Qu.:  31.12
 2003/2F:  463    Max.   :4700.00
 (Other):27284    NA's   : 463.00
> mode(Price)
[1] "numeric"
> mode(EPS)
[1] "numeric"
> mode(BookValuePS)
[1] "numeric"
> BP <- BookValuePS / Price ;
Warning message:
/ not meaningful for factors in: Ops.factor(BookValuePS, Price)
> which(Price==0)
numeric(0)
>


--
-- Vivek Satsangi
Student, Rochester, NY USA



From sara at gmesintra.com  Sun Jan 15 12:12:21 2006
From: sara at gmesintra.com (Sara Mouro)
Date: Sun, 15 Jan 2006 11:12:21 -0000
Subject: [R] How do I put these functions "together"?
Message-ID: <200601151112.k0FBCOKe025338@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060115/25da95c8/attachment.pl

From ggrothendieck at gmail.com  Sun Jan 15 12:20:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 Jan 2006 06:20:33 -0500
Subject: [R] / Operator not meaningful for factors
In-Reply-To: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>
References: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>
Message-ID: <971536df0601150320w25795579yf2a856bb6a2495c0@mail.gmail.com>

But factors are numeric:

mode(factor(1:3)) # numeric

so the numerator or denominator are likely a factor.
Try using str and class rather than mode to investigate
this.


On 1/15/06, Vivek Satsangi <vivek.satsangi at gmail.com> wrote:
> Folks,
> I have a very basic question. The solution eludes me perhaps because
> of my own lack of creativity. I am not attaching a fully reproducible
> session because the issue may well be becuase of the way the data file
> is, and the data file is large (and I don't know whether I can legally
> distribute it). If people can suggest things that might be wrong in my
> data or the way that I am reading it, I would be most grateful.
>
> I get the following error message in the session quoted at the end of
> this email:
> / not meaningful for factors in: Ops.factor(BookValuePS, Price)
>
> As you can see in that some session, I check that the two vectors
> being divided are numeric. I also check that the divisor is not 0 at
> any index. I also believe that this is not because of the NA's in the
> data. My question is, what are other "problems" that can cause the /
> operator to not be meaningful?
>
> I did try some simple examples to try to get the same error. However,
> I am not sure how to put the same NA's that one  gets from
> read.table() into a vector:
> > a <- c(1, 2, 3, NA);
> > a
> [1]  1  2  3 NA
> > b <- c( 1, 2, 3, 4);
> > c <- b / a;
> > b
> [1] 1 2 3 4
> > a <- c(1, 2, 3, );
> > c <- b/a;
> Warning message:
> longer object length
>        is not a multiple of shorter object length in: b/a
>
>
> ******** Quoted Session below ********
>  > explainPriceSimplified <- read.table("combinedClean.csv",
> +                            sep = ",", header=TRUE);
> > attach(explainPriceSimplified);
> > summary(explainPriceSimplified);
>     Symbol           Date              Price            EPS
>   BookValuePS
>  XL     :   98   Min.   :19870630   22     :   61   Min.   :-1.401e+05
>  Min.   :-6.901e+05
>  ZION   :   97   1st Qu.:19910930   26.5   :   61   1st Qu.: 4.650e-01
>  1st Qu.: 3.892e+00
>  YRCW   :   72   Median :19960331   27.5   :   58   Median : 1.060e+00
>  Median : 7.882e+00
>  AA     :   71   Mean   :19957688   30     :   58   Mean   :-1.534e+01
>  Mean   : 1.515e+02
>  ABS    :   71   3rd Qu.:20001231   25     :   56   3rd Qu.: 1.890e+00
>  3rd Qu.: 1.444e+01
>  ABT    :   71   Max.   :20041231   (Other):29561   Max.   : 5.309e+03
>  Max.   : 3.366e+06
>  (Other):29624                      NA's   :  249   NA's   : 2.460e+02
>  NA's   : 4.760e+02
>  FiscalQuarterRep    F12MRet
>  2004/2F:  482    Min.   :-100.00
>  2003/4F:  471    1st Qu.:  -8.82
>  2004/1F:  470    Median :  10.57
>  2004/3F:  470    Mean   :  13.36
>  2003/3F:  464    3rd Qu.:  31.12
>  2003/2F:  463    Max.   :4700.00
>  (Other):27284    NA's   : 463.00
> > mode(Price)
> [1] "numeric"
> > mode(EPS)
> [1] "numeric"
> > mode(BookValuePS)
> [1] "numeric"
> > BP <- BookValuePS / Price ;
> Warning message:
> / not meaningful for factors in: Ops.factor(BookValuePS, Price)
> > which(Price==0)
> numeric(0)
> >
>
>
> --
> -- Vivek Satsangi
> Student, Rochester, NY USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sun Jan 15 13:19:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 12:19:54 +0000 (GMT)
Subject: [R] / Operator not meaningful for factors
In-Reply-To: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>
References: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601151206510.19165@gannet.stats>

The mode of a factor is numeric, so your test does not do what you think 
it does.

is.numeric() is the recommended test of a vector being numeric.  I have no 
idea where you got the idea that mode() was a useful test (perhaps you 
could give us the reference you used), but it rather rarely is (typeof is 
usually more informative).

>From the summary quoted, Price is clearly a factor.  Test it with 
is.factor.

On Sun, 15 Jan 2006, Vivek Satsangi wrote:

> Folks,
> I have a very basic question. The solution eludes me perhaps because
> of my own lack of creativity. I am not attaching a fully reproducible
> session because the issue may well be becuase of the way the data file
> is, and the data file is large (and I don't know whether I can legally
> distribute it). If people can suggest things that might be wrong in my
> data or the way that I am reading it, I would be most grateful.
>
> I get the following error message in the session quoted at the end of
> this email:
> / not meaningful for factors in: Ops.factor(BookValuePS, Price)
>
> As you can see in that some session, I check that the two vectors
> being divided are numeric.

(see the request above for your reference here)

> I also check that the divisor is not 0 at any index. I also believe that 
> this is not because of the NA's in the data. My question is, what are 
> other "problems" that can cause the / operator to not be meaningful?

Why not test for factor, since that is what the very helpful error message 
told you the problem was?

> I did try some simple examples to try to get the same error. However,
> I am not sure how to put the same NA's that one  gets from
> read.table() into a vector:
>> a <- c(1, 2, 3, NA);
>> a
> [1]  1  2  3 NA
>> b <- c( 1, 2, 3, 4);
>> c <- b / a;
>> b
> [1] 1 2 3 4
>> a <- c(1, 2, 3, );
>> c <- b/a;
> Warning message:
> longer object length
>        is not a multiple of shorter object length in: b/a
>
>
> ******** Quoted Session below ********
> > explainPriceSimplified <- read.table("combinedClean.csv",
> +                            sep = ",", header=TRUE);
>> attach(explainPriceSimplified);
>> summary(explainPriceSimplified);
>     Symbol           Date              Price            EPS
>   BookValuePS
> XL     :   98   Min.   :19870630   22     :   61   Min.   :-1.401e+05
>  Min.   :-6.901e+05
> ZION   :   97   1st Qu.:19910930   26.5   :   61   1st Qu.: 4.650e-01
>  1st Qu.: 3.892e+00
> YRCW   :   72   Median :19960331   27.5   :   58   Median : 1.060e+00
>  Median : 7.882e+00
> AA     :   71   Mean   :19957688   30     :   58   Mean   :-1.534e+01
>  Mean   : 1.515e+02
> ABS    :   71   3rd Qu.:20001231   25     :   56   3rd Qu.: 1.890e+00
>  3rd Qu.: 1.444e+01
> ABT    :   71   Max.   :20041231   (Other):29561   Max.   : 5.309e+03
>  Max.   : 3.366e+06
> (Other):29624                      NA's   :  249   NA's   : 2.460e+02
>  NA's   : 4.760e+02
> FiscalQuarterRep    F12MRet
> 2004/2F:  482    Min.   :-100.00
> 2003/4F:  471    1st Qu.:  -8.82
> 2004/1F:  470    Median :  10.57
> 2004/3F:  470    Mean   :  13.36
> 2003/3F:  464    3rd Qu.:  31.12
> 2003/2F:  463    Max.   :4700.00
> (Other):27284    NA's   : 463.00
>> mode(Price)
> [1] "numeric"
>> mode(EPS)
> [1] "numeric"
>> mode(BookValuePS)
> [1] "numeric"
>> BP <- BookValuePS / Price ;
> Warning message:
> / not meaningful for factors in: Ops.factor(BookValuePS, Price)
>> which(Price==0)
> numeric(0)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vivek.satsangi at gmail.com  Sun Jan 15 13:37:32 2006
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Sun, 15 Jan 2006 07:37:32 -0500
Subject: [R] / Operator not meaningful for factors
In-Reply-To: <Pine.LNX.4.61.0601151206510.19165@gannet.stats>
References: <bcb171920601150306t22920f5bkf3cc4dd22edeec09@mail.gmail.com>
	<Pine.LNX.4.61.0601151206510.19165@gannet.stats>
Message-ID: <bcb171920601150437y1b304af8w6df8b476ee4e1e3e@mail.gmail.com>

Sir,
I made the (incorrect, probably unjustified) deduction of using mode()
based on section 3.1 of "An Introduction to R". Since the write up
talks about the "mode" of an object, and using attr() did not work (it
gives some error saying that "mode of name must be character"), I
tried mode() and reached this incorrect conclusion.

I have had this confusion for a while now about the fact that
something is numeric AND it is a factor, since if it were just a
vector and not a factor, it would still be numeric, as in:
> a <- c (1, 2, 3);
> class(a);
[1] "numeric"

I'll try to think of a way to improve the explanation in "An
Introduction to R" so that the next person coming along does not fall
into the same pit.

Thank you for getting me unstuck,

Vivek

On 1/15/06, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> The mode of a factor is numeric, so your test does not do what you think
> it does.
>
> is.numeric() is the recommended test of a vector being numeric.  I have no
> idea where you got the idea that mode() was a useful test (perhaps you
> could give us the reference you used), but it rather rarely is (typeof is
> usually more informative).
>
> From the summary quoted, Price is clearly a factor.  Test it with
> is.factor.
>
> On Sun, 15 Jan 2006, Vivek Satsangi wrote:
>
> > Folks,
> > I have a very basic question. The solution eludes me perhaps because
> > of my own lack of creativity. I am not attaching a fully reproducible
> > session because the issue may well be becuase of the way the data file
> > is, and the data file is large (and I don't know whether I can legally
> > distribute it). If people can suggest things that might be wrong in my
> > data or the way that I am reading it, I would be most grateful.
> >
> > I get the following error message in the session quoted at the end of
> > this email:
> > / not meaningful for factors in: Ops.factor(BookValuePS, Price)
> >
> > As you can see in that some session, I check that the two vectors
> > being divided are numeric.
>
> (see the request above for your reference here)
>
> > I also check that the divisor is not 0 at any index. I also believe that
> > this is not because of the NA's in the data. My question is, what are
> > other "problems" that can cause the / operator to not be meaningful?
>
> Why not test for factor, since that is what the very helpful error message
> told you the problem was?
>
> > I did try some simple examples to try to get the same error. However,
> > I am not sure how to put the same NA's that one  gets from
> > read.table() into a vector:
> >> a <- c(1, 2, 3, NA);
> >> a
> > [1]  1  2  3 NA
> >> b <- c( 1, 2, 3, 4);
> >> c <- b / a;
> >> b
> > [1] 1 2 3 4
> >> a <- c(1, 2, 3, );
> >> c <- b/a;
> > Warning message:
> > longer object length
> >        is not a multiple of shorter object length in: b/a
> >
> >
> > ******** Quoted Session below ********
> > > explainPriceSimplified <- read.table("combinedClean.csv",
> > +                            sep = ",", header=TRUE);
> >> attach(explainPriceSimplified);
> >> summary(explainPriceSimplified);
> >     Symbol           Date              Price            EPS
> >   BookValuePS
> > XL     :   98   Min.   :19870630   22     :   61   Min.   :-1.401e+05
> >  Min.   :-6.901e+05
> > ZION   :   97   1st Qu.:19910930   26.5   :   61   1st Qu.: 4.650e-01
> >  1st Qu.: 3.892e+00
> > YRCW   :   72   Median :19960331   27.5   :   58   Median : 1.060e+00
> >  Median : 7.882e+00
> > AA     :   71   Mean   :19957688   30     :   58   Mean   :-1.534e+01
> >  Mean   : 1.515e+02
> > ABS    :   71   3rd Qu.:20001231   25     :   56   3rd Qu.: 1.890e+00
> >  3rd Qu.: 1.444e+01
> > ABT    :   71   Max.   :20041231   (Other):29561   Max.   : 5.309e+03
> >  Max.   : 3.366e+06
> > (Other):29624                      NA's   :  249   NA's   : 2.460e+02
> >  NA's   : 4.760e+02
> > FiscalQuarterRep    F12MRet
> > 2004/2F:  482    Min.   :-100.00
> > 2003/4F:  471    1st Qu.:  -8.82
> > 2004/1F:  470    Median :  10.57
> > 2004/3F:  470    Mean   :  13.36
> > 2003/3F:  464    3rd Qu.:  31.12
> > 2003/2F:  463    Max.   :4700.00
> > (Other):27284    NA's   : 463.00
> >> mode(Price)
> > [1] "numeric"
> >> mode(EPS)
> > [1] "numeric"
> >> mode(BookValuePS)
> > [1] "numeric"
> >> BP <- BookValuePS / Price ;
> > Warning message:
> > / not meaningful for factors in: Ops.factor(BookValuePS, Price)
> >> which(Price==0)
> > numeric(0)
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


--
-- Vivek Satsangi
Student, Rochester, NY USA



From roland.regoes at env.ethz.ch  Sun Jan 15 14:04:19 2006
From: roland.regoes at env.ethz.ch (Roland R Regoes)
Date: Sun, 15 Jan 2006 14:04:19 +0100
Subject: [R] problems with glm
Message-ID: <20060115130417.GA25815@uwis-cx-dock-1-0751.ethz.ch>

Dear R users,

     I am having some problems with glm. The first is an error message "subscript out of bounds". The second is the fact that reasonable starting values are not accepted by the function.

     To be more specific, here is an example:

> success <- c(13,12,11,14,14,11,13,11,12)
> failure <- c(0,0,0,0,0,0,0,2,2)
> predictor <- c(0,80*5^(0:7))
> glm(cbind(success,failure) ~ predictor + 0,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
Deviance = 3.348039 Iterations - 1 
Error: subscript out of bounds
In addition: Warning message:
step size truncated: out of bounds 
>

	The model with intercept yields:

> glm(cbind(success,failure) ~ predictor ,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))

Call:  glm(formula = cbind(success, failure) ~ predictor, family = binomial(link = "log"),      control = glm.control(epsilon = 1e-08, trace = FALSE, maxit = 50)) 

Coefficients:
(Intercept)    predictor  
 -5.830e-17   -4.000e-08  

Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
Null Deviance:	    12.08 
Residual Deviance: 2.889 	AIC: 11.8 
There were 33 warnings (use warnings() to see them)
> warnings()
1: step size truncated: out of bounds
...
31: step size truncated: out of bounds
32: algorithm stopped at boundary value in: glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,   ...
33: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,   ...
>

	Since the intercept in the above fit is fairly small I thought I could use -4e-8 as a reasonable starting value in a model without intercept. But to no avail:

> glm(cbind(success,failure) ~ predictor + 0, start=-4e-8,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
Error in glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  : 
	cannot find valid starting values: please specify some
>

	I am stuck here. Am I doing something wrong when specifying the starting value? I would appreciate any help. (I could not find anything relevant in the documentation of glm and the mailing list archives, but I did not read the source code of glm yet.)

Roland


PS: I am using R Version 2.2.0 (R Cocoa GUI 1.13 (1915)) on MacOSX 10.4.4

-- 
-----------------------------------------------------------------------
Roland Regoes
Theoretical Biology
Universitaetsstr. 16 
ETH Zentrum, CHN H76.1 
CH-8092 Zurich, Switzerland

tel: +41-44-632-6935
fax: +41-44-632-1271



From Bill.Venables at csiro.au  Sun Jan 15 14:16:09 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 16 Jan 2006 00:16:09 +1100
Subject: [R] problems with glm
Message-ID: <B998A44C8986644EA8029CFE6396A924546879@exqld2-bne.qld.csiro.au>

Most of your problems seem to come from 'link = "log"' whereas you
probably mean 'link = logit' (which is the default.  Hence:

##########################################
> success <- c(13,12,11,14,14,11,13,11,12)
> failure <- c(0,0,0,0,0,0,0,2,2)
> predictor <- c(0,80*5^(0:7))
> glm(cbind(success,failure) ~ predictor,
+           family = binomial, #(link="log"),
+           control = glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
Deviance = 7.621991 Iterations - 1 
Deviance = 6.970934 Iterations - 2 
Deviance = 6.941054 Iterations - 3 
Deviance = 6.940945 Iterations - 4 
Deviance = 6.940945 Iterations - 5 

Call:  glm(formula = cbind(success, failure) ~ predictor, family =
binomial,      control = glm.control(epsilon = 1e-08, trace = TRUE,
maxit = 50)) 

Coefficients:
(Intercept)    predictor  
  4.180e+00   -4.106e-07  

Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
Null Deviance:      12.08 
Residual Deviance: 6.941        AIC: 15.85 
> 

#######################################

Bill Venables, 
CMIS, CSIRO Laboratories, 
PO Box 120, Cleveland, Qld. 4163 
AUSTRALIA 
Office Phone (email preferred): +61 7 3826 7251 
Fax (if absolutely necessary):    +61 7 3826 7304 
Mobile (rarely used):                +61 4 1963 4642 
Home Phone:                          +61 7 3286 7700 
mailto:Bill.Venables at csiro.au 
http://www.cmis.csiro.au/bill.venables/ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland R Regoes
Sent: Sunday, 15 January 2006 11:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] problems with glm


Dear R users,

     I am having some problems with glm. The first is an error message
"subscript out of bounds". The second is the fact that reasonable
starting values are not accepted by the function.

     To be more specific, here is an example:

> success <- c(13,12,11,14,14,11,13,11,12)
> failure <- c(0,0,0,0,0,0,0,2,2)
> predictor <- c(0,80*5^(0:7))
> glm(cbind(success,failure) ~ predictor + 0,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
Deviance = 3.348039 Iterations - 1 
Error: subscript out of bounds
In addition: Warning message:
step size truncated: out of bounds 
>

	The model with intercept yields:

> glm(cbind(success,failure) ~ predictor ,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))

Call:  glm(formula = cbind(success, failure) ~ predictor, family =
binomial(link = "log"),      control = glm.control(epsilon = 1e-08,
trace = FALSE, maxit = 50)) 

Coefficients:
(Intercept)    predictor  
 -5.830e-17   -4.000e-08  

Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
Null Deviance:	    12.08 
Residual Deviance: 2.889 	AIC: 11.8 
There were 33 warnings (use warnings() to see them)
> warnings()
1: step size truncated: out of bounds
...
31: step size truncated: out of bounds
32: algorithm stopped at boundary value in: glm.fit(x = X, y = Y,
weights = weights, start = start, etastart = etastart,   ...
33: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X,
y = Y, weights = weights, start = start, etastart = etastart,   ...
>

	Since the intercept in the above fit is fairly small I thought I
could use -4e-8 as a reasonable starting value in a model without
intercept. But to no avail:

> glm(cbind(success,failure) ~ predictor + 0, start=-4e-8,
+           family=binomial(link="log"),
+           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
Error in glm.fit(x = X, y = Y, weights = weights, start = start,
etastart = etastart,  : 
	cannot find valid starting values: please specify some
>

	I am stuck here. Am I doing something wrong when specifying the
starting value? I would appreciate any help. (I could not find anything
relevant in the documentation of glm and the mailing list archives, but
I did not read the source code of glm yet.)

Roland


PS: I am using R Version 2.2.0 (R Cocoa GUI 1.13 (1915)) on MacOSX
10.4.4

-- 
-----------------------------------------------------------------------
Roland Regoes
Theoretical Biology
Universitaetsstr. 16 
ETH Zentrum, CHN H76.1 
CH-8092 Zurich, Switzerland

tel: +41-44-632-6935
fax: +41-44-632-1271

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andywongcw at gmail.com  Sun Jan 15 14:56:00 2006
From: andywongcw at gmail.com (Andy Wong)
Date: Sun, 15 Jan 2006 21:56:00 +0800
Subject: [R] Raw Data Input
Message-ID: <cafcabf80601150556g6355b737gfe10e1437c893be8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060115/7f6be225/attachment.pl

From roland.regoes at env.ethz.ch  Sun Jan 15 14:56:13 2006
From: roland.regoes at env.ethz.ch (Roland R Regoes)
Date: Sun, 15 Jan 2006 14:56:13 +0100
Subject: [R] problems with glm
In-Reply-To: <B998A44C8986644EA8029CFE6396A924546879@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924546879@exqld2-bne.qld.csiro.au>
Message-ID: <20060115135613.GA27570@uwis-cx-dock-1-0751.ethz.ch>

	I am sorry, I should have emphasized that: I really mean 'family = 
binomial(link="log")'. 

	I am fitting infection data. Failure and success correspond to being infected or not. The probability of success (ie, not being infected in my context) is derived from a population model describing the interaction between hosts and parasites: 
	P(not infected) = exp(-InfectionRate*ParasiteDose)
'ParasiteDose' corresponds to 'predictor'. I want to estimate the  infection rate. Hence I need 'binomial(link="log")' and must set intercept=0.

Roland



On Mon, Jan 16, 2006 at 12:16:09AM +1100, Bill.Venables at csiro.au wrote:
> Most of your problems seem to come from 'link = "log"' whereas you
> probably mean 'link = logit' (which is the default.  Hence:
> 
> ##########################################
> > success <- c(13,12,11,14,14,11,13,11,12)
> > failure <- c(0,0,0,0,0,0,0,2,2)
> > predictor <- c(0,80*5^(0:7))
> > glm(cbind(success,failure) ~ predictor,
> +           family = binomial, #(link="log"),
> +           control = glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
> Deviance = 7.621991 Iterations - 1 
> Deviance = 6.970934 Iterations - 2 
> Deviance = 6.941054 Iterations - 3 
> Deviance = 6.940945 Iterations - 4 
> Deviance = 6.940945 Iterations - 5 
> 
> Call:  glm(formula = cbind(success, failure) ~ predictor, family =
> binomial,      control = glm.control(epsilon = 1e-08, trace = TRUE,
> maxit = 50)) 
> 
> Coefficients:
> (Intercept)    predictor  
>   4.180e+00   -4.106e-07  
> 
> Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
> Null Deviance:      12.08 
> Residual Deviance: 6.941        AIC: 15.85 
> > 
> 
> #######################################
> 
> Bill Venables, 
> CMIS, CSIRO Laboratories, 
> PO Box 120, Cleveland, Qld. 4163 
> AUSTRALIA 
> Office Phone (email preferred): +61 7 3826 7251 
> Fax (if absolutely necessary):    +61 7 3826 7304 
> Mobile (rarely used):                +61 4 1963 4642 
> Home Phone:                          +61 7 3286 7700 
> mailto:Bill.Venables at csiro.au 
> http://www.cmis.csiro.au/bill.venables/ 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland R Regoes
> Sent: Sunday, 15 January 2006 11:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problems with glm
> 
> 
> Dear R users,
> 
>      I am having some problems with glm. The first is an error message
> "subscript out of bounds". The second is the fact that reasonable
> starting values are not accepted by the function.
> 
>      To be more specific, here is an example:
> 
> > success <- c(13,12,11,14,14,11,13,11,12)
> > failure <- c(0,0,0,0,0,0,0,2,2)
> > predictor <- c(0,80*5^(0:7))
> > glm(cbind(success,failure) ~ predictor + 0,
> +           family=binomial(link="log"),
> +           control=glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
> Deviance = 3.348039 Iterations - 1 
> Error: subscript out of bounds
> In addition: Warning message:
> step size truncated: out of bounds 
> >
> 
> 	The model with intercept yields:
> 
> > glm(cbind(success,failure) ~ predictor ,
> +           family=binomial(link="log"),
> +           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
> 
> Call:  glm(formula = cbind(success, failure) ~ predictor, family =
> binomial(link = "log"),      control = glm.control(epsilon = 1e-08,
> trace = FALSE, maxit = 50)) 
> 
> Coefficients:
> (Intercept)    predictor  
>  -5.830e-17   -4.000e-08  
> 
> Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
> Null Deviance:	    12.08 
> Residual Deviance: 2.889 	AIC: 11.8 
> There were 33 warnings (use warnings() to see them)
> > warnings()
> 1: step size truncated: out of bounds
> ...
> 31: step size truncated: out of bounds
> 32: algorithm stopped at boundary value in: glm.fit(x = X, y = Y,
> weights = weights, start = start, etastart = etastart,   ...
> 33: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X,
> y = Y, weights = weights, start = start, etastart = etastart,   ...
> >
> 
> 	Since the intercept in the above fit is fairly small I thought I
> could use -4e-8 as a reasonable starting value in a model without
> intercept. But to no avail:
> 
> > glm(cbind(success,failure) ~ predictor + 0, start=-4e-8,
> +           family=binomial(link="log"),
> +           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
> Error in glm.fit(x = X, y = Y, weights = weights, start = start,
> etastart = etastart,  : 
> 	cannot find valid starting values: please specify some
> >
> 
> 	I am stuck here. Am I doing something wrong when specifying the
> starting value? I would appreciate any help. (I could not find anything
> relevant in the documentation of glm and the mailing list archives, but
> I did not read the source code of glm yet.)
> 
> Roland
> 
> 
> PS: I am using R Version 2.2.0 (R Cocoa GUI 1.13 (1915)) on MacOSX
> 10.4.4
> 
> -- 
> -----------------------------------------------------------------------
> Roland Regoes
> Theoretical Biology
> Universitaetsstr. 16 
> ETH Zentrum, CHN H76.1 
> CH-8092 Zurich, Switzerland
> 
> tel: +41-44-632-6935
> fax: +41-44-632-1271
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
-----------------------------------------------------------------------
Roland Regoes
Theoretical Biology
Universitaetsstr. 16 
ETH Zentrum, CHN H76.1 
CH-8092 Zurich, Switzerland

tel: +41-44-632-6935
fax: +41-44-632-1271



From andrej.kastrin at siol.net  Sun Jan 15 15:32:37 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sun, 15 Jan 2006 15:32:37 +0100
Subject: [R] Equal length axis
Message-ID: <43CA5D05.5070409@siol.net>

Dear useRs,
I am having difficulty to plot graphics with mfrow command, where both 
axis are equal length. Below is sample code, which plots rectangles 
instead of squares:

par (mfrow=c(3,3))
qqnorm(a)
qqnorm(b)
...
..

Thanks in advance for any pointers or notes.



From ripley at stats.ox.ac.uk  Sun Jan 15 15:57:22 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 14:57:22 +0000 (GMT)
Subject: [R] problems with glm
In-Reply-To: <20060115135613.GA27570@uwis-cx-dock-1-0751.ethz.ch>
References: <B998A44C8986644EA8029CFE6396A924546879@exqld2-bne.qld.csiro.au>
	<20060115135613.GA27570@uwis-cx-dock-1-0751.ethz.ch>
Message-ID: <Pine.LNX.4.61.0601151430150.680@gannet.stats>

On Sun, 15 Jan 2006, Roland R Regoes wrote:

> 	I am sorry, I should have emphasized that: I really mean 'family =
> binomial(link="log")'.
>
> 	I am fitting infection data. Failure and success correspond to 
> being infected or not. The probability of success (ie, not being 
> infected in my context) is derived from a population model describing 
> the interaction between hosts and parasites:
> 	P(not infected) = exp(-InfectionRate*ParasiteDose)

> 'ParasiteDose' corresponds to 'predictor'. I want to estimate the 
> infection rate. Hence I need 'binomial(link="log")' and must set 
> intercept=0.

Then your model is not appropriate for your data.  The only real evidence 
is coming from the last two points which have approximately equal failure 
rate yet differ by a factor of 5 in the predictor.  Also, your model 
without intercept predicts probablilty one for the first case, and the 
log link in R does not allow probability zero.  If we drop that case
(which contributes nothing to the model) we get

success <- c(12,11,14,14,11,13,11,12)
failure <- c(0,0,0,0,0,0,2,2)
predictor <- 80*5^(0:7)
glm(cbind(success,failure) ~ predictor+0,
            family = binomial(link="log"),
            control = glm.control(epsilon=1e-8,trace=TRUE,maxit=50))

which works, albeit with an infinite null deviance (as the null model is 
inappropriate).


> On Mon, Jan 16, 2006 at 12:16:09AM +1100, Bill.Venables at csiro.au wrote:
>> Most of your problems seem to come from 'link = "log"' whereas you
>> probably mean 'link = logit' (which is the default.  Hence:
>>
>> ##########################################
>>> success <- c(13,12,11,14,14,11,13,11,12)
>>> failure <- c(0,0,0,0,0,0,0,2,2)
>>> predictor <- c(0,80*5^(0:7))
>>> glm(cbind(success,failure) ~ predictor,
>> +           family = binomial, #(link="log"),
>> +           control = glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
>> Deviance = 7.621991 Iterations - 1
>> Deviance = 6.970934 Iterations - 2
>> Deviance = 6.941054 Iterations - 3
>> Deviance = 6.940945 Iterations - 4
>> Deviance = 6.940945 Iterations - 5
>>
>> Call:  glm(formula = cbind(success, failure) ~ predictor, family =
>> binomial,      control = glm.control(epsilon = 1e-08, trace = TRUE,
>> maxit = 50))
>>
>> Coefficients:
>> (Intercept)    predictor
>>   4.180e+00   -4.106e-07
>>
>> Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
>> Null Deviance:      12.08
>> Residual Deviance: 6.941        AIC: 15.85

>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roland R Regoes
>> Sent: Sunday, 15 January 2006 11:04 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] problems with glm
>>
>>
>> Dear R users,
>>
>>      I am having some problems with glm. The first is an error message
>> "subscript out of bounds". The second is the fact that reasonable
>> starting values are not accepted by the function.
>>
>>      To be more specific, here is an example:
>>
>>> success <- c(13,12,11,14,14,11,13,11,12)
>>> failure <- c(0,0,0,0,0,0,0,2,2)
>>> predictor <- c(0,80*5^(0:7))
>>> glm(cbind(success,failure) ~ predictor + 0,
>> +           family=binomial(link="log"),
>> +           control=glm.control(epsilon=1e-8,trace=TRUE,maxit=50))
>> Deviance = 3.348039 Iterations - 1
>> Error: subscript out of bounds
>> In addition: Warning message:
>> step size truncated: out of bounds
>>>
>>
>> 	The model with intercept yields:
>>
>>> glm(cbind(success,failure) ~ predictor ,
>> +           family=binomial(link="log"),
>> +           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
>>
>> Call:  glm(formula = cbind(success, failure) ~ predictor, family =
>> binomial(link = "log"),      control = glm.control(epsilon = 1e-08,
>> trace = FALSE, maxit = 50))
>>
>> Coefficients:
>> (Intercept)    predictor
>>  -5.830e-17   -4.000e-08
>>
>> Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
>> Null Deviance:	    12.08
>> Residual Deviance: 2.889 	AIC: 11.8
>> There were 33 warnings (use warnings() to see them)
>>> warnings()
>> 1: step size truncated: out of bounds
>> ...
>> 31: step size truncated: out of bounds
>> 32: algorithm stopped at boundary value in: glm.fit(x = X, y = Y,
>> weights = weights, start = start, etastart = etastart,   ...
>> 33: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X,
>> y = Y, weights = weights, start = start, etastart = etastart,   ...
>>>
>>
>> 	Since the intercept in the above fit is fairly small I thought I
>> could use -4e-8 as a reasonable starting value in a model without
>> intercept. But to no avail:
>>
>>> glm(cbind(success,failure) ~ predictor + 0, start=-4e-8,
>> +           family=binomial(link="log"),
>> +           control=glm.control(epsilon=1e-8,trace=FALSE,maxit=50))
>> Error in glm.fit(x = X, y = Y, weights = weights, start = start,
>> etastart = etastart,  :
>> 	cannot find valid starting values: please specify some
>>>
>>
>> 	I am stuck here. Am I doing something wrong when specifying the
>> starting value? I would appreciate any help. (I could not find anything
>> relevant in the documentation of glm and the mailing list archives, but
>> I did not read the source code of glm yet.)
>>
>> Roland
>>
>>
>> PS: I am using R Version 2.2.0 (R Cocoa GUI 1.13 (1915)) on MacOSX
>> 10.4.4
>>
>> --
>> -----------------------------------------------------------------------
>> Roland Regoes
>> Theoretical Biology
>> Universitaetsstr. 16
>> ETH Zentrum, CHN H76.1
>> CH-8092 Zurich, Switzerland
>>
>> tel: +41-44-632-6935
>> fax: +41-44-632-1271
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> -- 
> -----------------------------------------------------------------------
> Roland Regoes
> Theoretical Biology
> Universitaetsstr. 16
> ETH Zentrum, CHN H76.1
> CH-8092 Zurich, Switzerland
>
> tel: +41-44-632-6935
> fax: +41-44-632-1271
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Jan 15 16:05:24 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jan 2006 16:05:24 +0100
Subject: [R] Raw Data Input
In-Reply-To: <cafcabf80601150556g6355b737gfe10e1437c893be8@mail.gmail.com>
References: <cafcabf80601150556g6355b737gfe10e1437c893be8@mail.gmail.com>
Message-ID: <x27j915y6j.fsf@turmalin.kubism.ku.dk>

Andy Wong <andywongcw at gmail.com> writes:

> I'm a new user of the R and have already downloaded the latest version of
> it.  Can anybody tells me how to input the raw data for analysis?  Thanks.

Depends on the format of "the raw data". 

Did you read Ch.7 of "An Introduction to R" which ships with R?
Otherwise do so (and the rest of that manual too, for that matter).

I had a somewhat expanded coverage of data input in my ISCB course
last year. You might get something out of browsing the handouts at

http://staff.pubhealth.ku.dk/~pd/iscb-2005/Basics-2x2.pdf

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sun Jan 15 16:25:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 15:25:06 +0000 (GMT)
Subject: [R] Raw Data Input
In-Reply-To: <x27j915y6j.fsf@turmalin.kubism.ku.dk>
References: <cafcabf80601150556g6355b737gfe10e1437c893be8@mail.gmail.com>
	<x27j915y6j.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0601151523050.6346@gannet.stats>

On Sun, 15 Jan 2006, Peter Dalgaard wrote:

> Andy Wong <andywongcw at gmail.com> writes:
>
>> I'm a new user of the R and have already downloaded the latest version of
>> it.  Can anybody tells me how to input the raw data for analysis?  Thanks.
>
> Depends on the format of "the raw data".
>
> Did you read Ch.7 of "An Introduction to R" which ships with R?
> Otherwise do so (and the rest of that manual too, for that matter).

Let alone the `R Data Import/Export Manual' which also ships with R and is 
much more comprehensive.

> I had a somewhat expanded coverage of data input in my ISCB course
> last year. You might get something out of browsing the handouts at
>
> http://staff.pubhealth.ku.dk/~pd/iscb-2005/Basics-2x2.pdf


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Jan 15 16:56:45 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jan 2006 16:56:45 +0100
Subject: [R] Raw Data Input
In-Reply-To: <Pine.LNX.4.61.0601151523050.6346@gannet.stats>
References: <cafcabf80601150556g6355b737gfe10e1437c893be8@mail.gmail.com>
	<x27j915y6j.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0601151523050.6346@gannet.stats>
Message-ID: <x23bjp5vsy.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Sun, 15 Jan 2006, Peter Dalgaard wrote:
> 
> > Andy Wong <andywongcw at gmail.com> writes:
> >
> >> I'm a new user of the R and have already downloaded the latest version of
> >> it.  Can anybody tells me how to input the raw data for analysis?  Thanks.
> >
> > Depends on the format of "the raw data".
> >
> > Did you read Ch.7 of "An Introduction to R" which ships with R?
> > Otherwise do so (and the rest of that manual too, for that matter).
> 
> Let alone the `R Data Import/Export Manual' which also ships with R
> and is much more comprehensive.

Right, of course. My brain seems to be parked elsewhere today...


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From andrej.kastrin at siol.net  Sun Jan 15 16:59:42 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sun, 15 Jan 2006 16:59:42 +0100
Subject: [R] Equal length axis
In-Reply-To: <43CA5D05.5070409@siol.net>
References: <43CA5D05.5070409@siol.net>
Message-ID: <43CA716E.6080408@siol.net>

Andrej Kastrin wrote:

>Dear useRs,
>I am having difficulty to plot graphics with mfrow command, where both 
>axis are equal length. Below is sample code, which plots rectangles 
>instead of squares:
>
>par (mfrow=c(3,3))
>qqnorm(a)
>qqnorm(b)
>...
>..
>
>Thanks in advance for any pointers or notes.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
I done it myself:
par(mfrow=c(3,3),pty="s")
etc...



From andrej.kastrin at siol.net  Sun Jan 15 19:22:18 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sun, 15 Jan 2006 19:22:18 +0100
Subject: [R] Multiple comparison and two-way ANOVA design
Message-ID: <43CA92DA.7030604@siol.net>

Dear useRs,

I'm working on multiple comparison design on two factor (2  3 levels) 
ANOVA. Each of the tests I have tried (Tukey, multcomp package) seem to 
do only with one factor at a time.


fm1 <- aov(breaks ~ wool * tension, data = warpbreaks)
tHSD <- TukeyHSD(fm1, "tension", ordered = FALSE)

$tension
          diff       lwr        upr     p adj
M-L -10.000000 -19.35342 -0.6465793 0.0336262
H-L -14.722222 -24.07564 -5.3688015 0.0011218
H-M  -4.722222 -14.07564  4.6311985 0.4474210

I'm interested in posthoc comparisons between various levels of factor 1 
and various levels of factor 2, both at the same time.
I know that is possible in SPSS, but... is there any R solution?

Cheers, Andrej



From ripley at stats.ox.ac.uk  Sun Jan 15 19:56:11 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 18:56:11 +0000 (GMT)
Subject: [R] Multiple comparison and two-way ANOVA design
In-Reply-To: <43CA92DA.7030604@siol.net>
References: <43CA92DA.7030604@siol.net>
Message-ID: <Pine.LNX.4.61.0601151848420.8441@gannet.stats>

Did you read the TukeyHSD help page?  You chose to have one factor, so you 
cannot blame R for your own choice.

Do be very careful that you understand what these so-called post hoc 
tests do (and also what the main effect of a factor in the presence of an 
interaction means, for in R it is not the same as in SPSS).

[It really isn't fair to send UTF-8 mail, and the formatting has gone 
wrong en route to me (if it was ever right).  Plain text and wrapped lines 
please.]

On Sun, 15 Jan 2006, Andrej Kastrin wrote:

> Dear useRs,
> I'm working on multiple comparison design on two factor (2 ? 3 levels) ANOVA. Each of the tests I have tried (Tukey, multcomp package) seem to do only with one factor at a time.
>
> fm1 <- aov(breaks ~ wool * tension, data = warpbreaks)tHSD <- TukeyHSD(fm1, "tension", ordered = FALSE)
> $tension          diff       lwr        upr     p adjM-L -10.000000 -19.35342 -0.6465793 0.0336262H-L -14.722222 -24.07564 -5.3688015 0.0011218H-M  -4.722222 -14.07564  4.6311985 0.4474210
> I'm interested in posthoc comparisons between various levels of factor 1 and various levels of factor 2, both at the same time.I know that is possible in SPSS, but... is there any R solution?
> Cheers, Andrej
> ______________________________________________R-help at stat.math.ethz.ch mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From tolga at coubros.com  Sun Jan 15 21:17:13 2006
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 15 Jan 2006 20:17:13 +0000
Subject: [R] Powell's Metod
Message-ID: <43CAADC9.5020104@coubros.com>

Folks,
Has anyone implemented Powell's Method for minimisation in R ?
Many thanks,
Tolga



From baron at psych.upenn.edu  Sun Jan 15 21:45:50 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 15 Jan 2006 15:45:50 -0500
Subject: [R] Firefox slide show with embedded SVG
Message-ID: <20060115204550.GA26650@psych.upenn.edu>

Now that T. Jake Luciani has created a working RSvgDevice package 
(with one function called devSVG), we can produce SVG output from 
ordinary plots, not necessarily just those made with grid.

Since Firefox and Mozilla support SVG (in recent versions), this
might be a good way to put figures in web pages in a way that can 
be easily scaled, and even (with tweaking) manipulated with
scripts.  (Note that gridSVG can also help here, although I
haven't tried it.)

I have now (finally) figured out how to get SVG into a slide
show.  I use Firefox for talks (and lectures).  I remove the
sidebar and all toolbars, and move the location window into the
one remaining bar, the one with "File" etc.  (I like this
anyway.)  Then I use F11 to toggle fullscreen.  The usual keys go 
from slide to slide (PgUp, PgDn).  There is an associated css
file that is required for this to work.

An example, with only one SVG image in it (in slide #12), is in
the notes for a couple of lectures I plan to start this week:

http://www.psych.upenn.edu/~baron/900/prob.xml

Unfortunately, this one does not take advantage of the
possibility of using units other than px, because I know it works 
on the classroom projector.  It was also not made with R.  Oh
well.  It does show how to embed the image.

The main trick here is to use xml, which requires very strict
coding: no upper case tags; all tags closed; etc.  (See the
example for how to do this.)  Firefox turns out to be great at
reporting xml errors, so this was not very frustrating to do.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From gynmeerut at indiatimes.com  Sun Jan 15 21:53:36 2006
From: gynmeerut at indiatimes.com (gynmeerut)
Date: Mon, 16 Jan 2006 02:23:36 +0530
Subject: [R] MLE
Message-ID: <200601152034.CAA30545@WS0005.indiatimes.com>

Dear All,
            Can somebody tell me how to do Maximum Likelihood Estimation in R for Non-linear function?
My function is non-linear and it has four parameters, only one explanatory variable.
If possible Please tell me the source so that I can write my own code for above.


Thanks,

GS



From voodooochild at gmx.de  Sun Jan 15 22:09:10 2006
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Sun, 15 Jan 2006 22:09:10 +0100
Subject: [R] invalid "mode" of argument  optimize
Message-ID: <43CAB9F6.5010404@gmx.de>

Hello everybody,

i have the following function, which i want to solve for b

i=1,..,n,  and n is the length of t and t is the last element of t, if 
you do cumsum() before.

(t_n*exp(-b*t_n)*sum_{i=1}^{n} f_i) / (1-exp(-b*t_n)) - (sum_{i=1}^{n} 
(f_i*(t_i*exp(-b*t_i)-t_{i-1}*exp(-b*t_{i-1}))) / 
(exp(-b*t_{i-1})-exp(-b*t_i)) )  = 0

i implemented this function in the following way

ll2<-function(b,f,t) {
  t<-cumsum(t)
  tn<-t[length(t)]
  i<-seq(along=f)
  s1<-tn*exp(-b*tn)*sum(f[i])
  s2<-(1-exp(-b*tn))
  i[length(i)+1]<-0
  i<-sort(i)
  f[length(f)+1]<-1
  t[length(t)+1]<-0
  t<-sort(t)
  s3<-f[2]*(t[2]*exp(-b*t[2])-t[1]*exp(-b*t[1]))
  s4<-exp(-b*t[1])-exp(-b*t[2])
  i<-i[3:length(i)]
  s5<-sum(f[i]*(t[i]*exp(-b*t[i])-t[i-1]*exp(-b*t[i-1])))
  s6<-sum(exp(-b*t[i-1])-exp(-b*t[i]))
  (s1/s2)-((s3/s4)+(s5/s6))
}

# i have the given data
f=c(1,1,1)
t=c(320,14390,9000)

# now i wanted to use optimize() to get the minimum of ll2() with the 
given data

xmin<-optimize(ll2,c(0,10000),tol=0.001,f=f,t=t)

and i got always the error "invalid "mode" of argument  optimize", but i 
don't now what is really wrong? Or is there any mistake in my 
implementation of the function?

best regards
Andreas



From ripley at stats.ox.ac.uk  Sun Jan 15 23:57:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jan 2006 22:57:01 +0000 (GMT)
Subject: [R] Powell's Metod
In-Reply-To: <43CAADC9.5020104@coubros.com>
References: <43CAADC9.5020104@coubros.com>
Message-ID: <Pine.LNX.4.61.0601152252180.10972@gannet.stats>

(Michael) Powell (of Harwell and Cambridge) invented several optimization 
methods.  You will need to give us a precise reference.

Powell is associated with earlier versions of the Nelder-Mead, CG and BFGS 
methods implemented in optim() but in each case different variants have 
best stood the course of time.

On Sun, 15 Jan 2006, Tolga Uzuner wrote:

> Has anyone implemented Powell's Method for minimisation in R ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Jan 16 00:01:43 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Jan 2006 15:01:43 -0800
Subject: [R] lmer with nested/nonnested groupings?
In-Reply-To: <43C157B2.3010800@stat.columbia.edu>
References: <43C157B2.3010800@stat.columbia.edu>
Message-ID: <43CAD457.6000804@pdf.com>

	RSiteSearch("lmer nested") produced 85 hits, the first of which looks 
to me like it would answer your question 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/61571.html):  Have you 
tried replacing "state" with "region:state" something like the following:

 > lmer (y ~ black*female + (1 | region:state) + (1 | region) + (1 | 
age) + (1 | edu) + (1 | age.edu), family=binomial(link="logit"))

	  But please read the earlier post, as makes cites other documents, 
e.g., the "MlmSoftRev" vignette, and contains useful comments from Doug 
Bates, among others.

	  hope this helps.
	  spencer graves

Andrew Gelman wrote:

> I'm trying to figure out how to use lmer to fit models with factors that 
> have some nesting and some non-nested groupings.  For example, in this 
> paper:
> http://www.stat.columbia.edu/~gelman/research/published/parkgelmanbafumi.pdf
> we have a logistic regression of survey respondents' political 
> preferences (1=Republican, 0=Democrat), regressing on sex, ethnicity, 
> state (51 states within 5 regions), 4 age categories, and 4 education 
> categories.  I'd like to include states (nested within regions), and 
> also age, education, and age x education.  (That is, 5 batches of 
> varying coefs:  50 states, 5 regions, 4 age categories, 4 education 
> categories, and 16 age x education categories.)  The age x education 
> factor is kinda tricky because it's connected both to age and to education.
> 
> I'm thinking of a model like this:
> 
> lmer (y ~ black*female + (1 | state) + (1 | region) + (1 | age) + (1 | 
> edu) + (1 | age.edu), family=binomial(link="logit"))
> 
> (Here, I'm thinking of age.edu as a variable with 16 levels.)
> 
> Anyway, it blows up when i try to put in these nested things.  I read 
> Doug Bates's article in R-news and there seems to be a  way of doing 
> nested groupings (unfortunately, I can't quite figure out how to do it), 
> but I don't see any references to situations such as age, edu, and age*edu .
> 
> For the article, we used Bugs, which is fine, but I'd like to see how 
> far I can take it using lmer.  I could kludge it by, for example, 
> including age, edu, and region as unmodeled factors:
> 
> lmer (y ~ black*female + (1 | state) + factor(region) + factor(age) + 
> factor(edu) + (1 | age.edu), family=binomial(link="logit"))
> 
> but I'd like to do the full multilevel version.
> 
> Thanks!
> Andrew
>



From tolga at coubros.com  Mon Jan 16 00:38:49 2006
From: tolga at coubros.com (Tolga Uzuner)
Date: Sun, 15 Jan 2006 23:38:49 +0000
Subject: [R] Powell's Metod
In-Reply-To: <Pine.LNX.4.61.0601152252180.10972@gannet.stats>
References: <43CAADC9.5020104@coubros.com>
	<Pine.LNX.4.61.0601152252180.10972@gannet.stats>
Message-ID: <43CADD09.4040003@coubros.com>

Ah, my apologies. I meant the method outlined in section 10.5 of 
Numerical Recipes using Powell's heuristic of discarding the direction 
of largest decrease. A nice twist would be if linmin (line 
minimization)  were to be implemented using the methods described in 
10.1-10.3, but that is just a nice-to-have.

 From what I understand, Nelder-Mead is the method in section 10.4 
(downhill simplex), CG is section 10.6, and BFGS is section 10.7.

Admittedly, optim is fantastic most of the time. However, I have a 
factor model over hundreds of elements, where I am trying to back out 
the factor distribution and it appears that the method in 10.5 might work.

Thanks in advance in any case,
Tolga

Prof Brian Ripley wrote:

> (Michael) Powell (of Harwell and Cambridge) invented several 
> optimization methods.  You will need to give us a precise reference.
>
> Powell is associated with earlier versions of the Nelder-Mead, CG and 
> BFGS methods implemented in optim() but in each case different 
> variants have best stood the course of time.
>
> On Sun, 15 Jan 2006, Tolga Uzuner wrote:
>
>> Has anyone implemented Powell's Method for minimisation in R ?
>
>



From dmbates at gmail.com  Mon Jan 16 01:38:29 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 15 Jan 2006 18:38:29 -0600
Subject: [R] lmer and handling heteroscedasticity
In-Reply-To: <43C92A82.2020806@anicca-vijja.de>
References: <43C92A82.2020806@anicca-vijja.de>
Message-ID: <40e66e0b0601151638v4c4aa643n88b0de73f47ffa3f@mail.gmail.com>

On 1/14/06, Leo G??rtler <leog at anicca-vijja.de> wrote:
> Dear altogether,
>
> is it possible to integrate "weights" arguments within lmer to
> incorporate statements to handle heteroscedasticity as it is possible
> with lme?
> I searched the R-archive but found nothing, insofer I assume it is not
> possible, but as lmer is under heavy develpoment, maybe something
> changed or is solved differently.
>
> Thus my question:
>
> While encountering heavy heteroscedasticity within data, lmer is not the
> right application to use, but use instead lme?

Yes.

> Thanks in advance for a short statement,

I hope that was short enough :-)

> best ,
> leo
>
> --
>
> email: leog at anicca-vijja.de
> www: http://www.anicca-vijja.de/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Mon Jan 16 03:40:40 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Jan 2006 18:40:40 -0800
Subject: [R] trouble with extraction/interpretation of variance
 structure para	meters from a model built using gnls and varConstPower
In-Reply-To: <567ACB2E39C83543B746F1AD7F5E5E04062E537F@wa-mb2-sea.amgen.com>
References: <567ACB2E39C83543B746F1AD7F5E5E04062E537F@wa-mb2-sea.amgen.com>
Message-ID: <43CB07A8.9080005@pdf.com>

	  How about this:

 > exp(coef(model3$modelStruct$varStruct)["const"])
     const
0.6551298

	  Does that answer the question about not understanding the connection 
between summary(model3) and coef(model3$modelStruct$varStruct)["const"]?

	  Regarding the question about R not being able to find 'coef.varFunc', 
I tried the following:
 > methods("coef")
<snip>
  coef.varFunc*
[37] coef.varIdent*        coef.varPower*

    Non-visible functions are asterisked	

	  Since "coef.varFunc" is "asterisked", I tried 
'getAnywhere(coef.varFunc)'.  I walked through this code line by line, 
until I found the following:

 > (val <- as.vector(model3$modelStruct$varStruct))
Variance function structure of class varConstPower representing
     const     power
0.6551298 0.8913665

	  Answer the questions?
	  spencer graves

Rand, Hugh wrote:

> I have been using gnls with the weights argument (and varConstPower) to
> specify a variance structure for curve fits. In attempting to extract the
> parameters for the variance model I am seeing results I don't understand.
> When I simply display the model (or use "summary" on the model), I get what
> seem like reasonable values for both "power" and "const". When I actually
> try to extract the values, I get the same number for the "power", but a
> different (and less sensible) value for "const".
> 
> The simplest example I can come up with that shows the problem is as
> follows: 
> 
> 
> #Set up data
> 
> x    = c(0,0,0,1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,7,7);
> y0   = c(.1,.1,.1, .5,.5,.5, 1,1,1, 2,2,2, 4,4,4, 7,7,7, 9,9,9, 10,10,10);
> yp   = c(0,.03,.05, 0,.05,.01, 0,.07,.03, .1,0,.2, .2,.1,0, .3,0,.1,
> 0,.3,.4, .3,.5,0);
> y    = y0 + 4*yp;
> data = data.frame(x=x,y=y);
> 
> #Run model
> 
> library(nlme)
> model3 = try(gnls(y ~
> SSfpl(x,A,B,xmid,scal),data=data,weights=varConstPower(const=1,power=0,form=
> ~fitted(.))));
> 
> #Examine results
> 
> model3;                                         #const = .6551298,   power =
> .8913665
> summary(model3);                                #const = .6551298,   power =
> .8913665               
> 
> coef(model3$modelStruct$varStruct)["power"];    #                    power =
> .8913665
> coef(model3$modelStruct$varStruct)["const"];    #const = -0.4229219 
> coef.varFunc(model3$modelStruct$varStruct);     #R can't seem to find this
> function, Splus can
> 
> 
> Any advice on what I am doing wrong would be appreciated.
> 
> Hugh Rand
> Senior Scientist
> Amgen
> rand at amgen.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Jan 16 03:49:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Jan 2006 18:49:44 -0800
Subject: [R] glmmPQL / "system is computationally singular"
In-Reply-To: <A02D1532-5494-4777-9A41-32C57A6DE701@gmail.com>
References: <A02D1532-5494-4777-9A41-32C57A6DE701@gmail.com>
Message-ID: <43CB09C8.60102@pdf.com>

	  Did you try "traceback()"?  What do you get?

	  I've had good luck with problems like this in listing the function 
then using "debug" to review while I walk throught the code line by line.

	  This may not be the issue here, but with "family=binomial", if the 
model being fit can achieve perfect separation, software of this type 
might generate error messages similar to what you describe.

	  hope this helps.
	  spencer graves

David Reitter wrote:

> Hi,
> 
> I'm having trouble with glmmPQL from the MASS package.
> I'm trying to fit a model with a binary response variable, two fixed  
> and two random variables (nested), with a sample of about 200,000  
> data points.
> 
> Unfortunately, I'm getting an error message that is difficult to  
> understand without knowing the internals of the glmmPQL function.
> 
> 
>>model <-  glmmPQL(primed ~ log(dist) * role , random = ~ dist |  
>>target.utt / prime.utt , family=binomial(link = "logit"),  
>>data=data.utts, niter=5, verbose = TRUE)
>>Loading required package: nlme
>>iteration 1
>>iteration 2
>>iteration 3
>>Error in solve.default(pdMatrix(a, fact = TRUE)) :
>>        system is computationally singular: reciprocal condition  
>>number = 8.65949e-32
>>In addition: Warning messages:
>>1: Singular precision matrix in level -1, block 4
>>2: Singular precision matrix in level -1, block 4
>>3: Singular precision matrix in level -1, block 4
>>4: Singular precision matrix in level -1, block 4
>>5: Singular precision matrix in level -1, block 4
> 
> 
> Any suggestions? Will a larger dataset (possible) solve the problem?
> 
> Thanks
> David
> 
> --
> David Reitter - ICCS/HCRC, Informatics, University of Edinburgh
> Blog: http://www.davids-world.com   Homepage: http://www.david- 
> reitter.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Jan 16 04:00:48 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Jan 2006 19:00:48 -0800
Subject: [R] Homogenic groups generation - Randomisation
In-Reply-To: <OFD98B0E84.3684A675-ONC12570F3.004E6A95-C12570F3.004F4D6E@fr.fournierpharma.com>
References: <OFD98B0E84.3684A675-ONC12570F3.004E6A95-C12570F3.004F4D6E@fr.fournierpharma.com>
Message-ID: <43CB0C60.6080804@pdf.com>

	  I'm sorry but I do not understand.  If you would still like help from 
this group, PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html", especially the part about 
providing a toy example that maybe doesn't quite work but helps readers 
understand what you mean by a "homogenic group".  RSiteSearch("Homogenic 
groups") found, "No document matching your query", and nothing striked 
me as relevant when I Googled for "homogenic group".  If you can pose 
your question in a way that does not require knowledge of your specific 
technical term, you increase your chances of getting help from someone 
who understands the concepts but not your particular jargon.

	  J'espere que ceci vous aidera.
	  spencer graves

a.menicacci at fr.fournierpharma.com wrote:

> 
> 
> 
> Dear R-users,
> 
> We expect to create N homogenic groups of n features from an
> experimentation including N*n mesures. The aim of this is to prevent from
> group effects. How to do that with R functionalities. Does anyone know any
> methodes enabling this ?
> 
> Best regards.
> 
> Alexandre MENICACCI
> Bioinformatics - FOURNIER PHARMA
> 50, rue de Dijon - 21121 Daix - FRANCE
> a.menicacci at fr.fournierpharma.com
> t??l : 03.80.44.76.17
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From adrian at maths.uwa.edu.au  Mon Jan 16 05:04:13 2006
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 16 Jan 2006 12:04:13 +0800
Subject: [R] envelopes of simulations
In-Reply-To: <20060112132009.BEFD018356C@asclepius.uwa.edu.au>
References: <20060112132009.BEFD018356C@asclepius.uwa.edu.au>
Message-ID: <17355.6973.545554.738288@maths.uwa.edu.au>


Sara Mouro writes:
 > Hello!
 > 
 > I am writing you because I could not plot the confidence envelopes for
 > functions Jest, Jcross, Jdot, Jmulti, and L, using the Spatstat package.

Enquiries about a package should be sent to the package maintainer
rather than R-help. 

 > I have already understood how to do that for Kest or Jest, that is:
 > JEnv <- plot(envelope(PPPData, Jest))
 > 	Where PPPData is my ppp object.
 > 
 > However, for Jcross I must specify the two marks I want to analyse.
 > That is, usually I would get the Jcross doing:
 > Jc <- Jcross(PPPData,"Aun","Qsu")
 > 	For marks "Aun" and "Qsu".

To do this, type
      envelope(PPPData, Jcross, i="Aun", j="Qsu")

Explanation:
	Looking at help(Jcross) we see that its formal syntax is 
		Jcross(X, i, j). 
	This means that when you call 
		Jcross(PPPData,"Aun","Qsu")
	the arguments are matched as 
		Jcross(X=PPPData,i="Aun",j="Qsu").

	So our problem is, how to pass the arguments 	i="Aun" and j="Qsu"
	to the function Jcross each time it is called by `envelope'.

	The help entry for 'envelope' mentions that `envelope' will accept 
	extra arguments "..." which are then passed to the function 'fun'.
	
	So, to pass the two argument values i="Aun" and j="Qsu",
	just type
	     envelope(PPPData, Jcross, i="Aun", j="Qsu")

 > For L function, I can make:
 > K <- Kest (PPPData, correction="isotropic")
 > plot (K, r-sqrt(iso/pi)~r)

	You can compute the envelope for the K-function
	and transform it afterwards. 

	To plot envelopes of the L function discrepancy
	   LD(r) = sqrt(K(r)/pi) - r,
	just type 
		E <- envelope(PPPData, Kest, correction="isotropic")
		plot(E, sqrt(./pi) -r ~ r)

Hope this helps.
regards
Adrian Baddeley



From par at hunter-gatherer.org  Mon Jan 16 08:58:38 2006
From: par at hunter-gatherer.org (Par Leijonhufvud)
Date: Mon, 16 Jan 2006 08:58:38 +0100
Subject: [R] Kite diagrams
Message-ID: <20060116075837.GC26757@absaroka.hunter-gatherer.org>

I teach biology, and would like to show the students how to use R for
some statistical assignments. One of those is to make a kite diagram
(for example as seen in
http://www.medinavalleycentre.org.uk/images/Bembri1.jpg). Is there any
way to create one using R? I did a help.search("kite") and looked on the
r-project HP with no luck. 

Previously when the course was taugh the students have either abused MS
Exel or drawn the diagrams by hand.

/Par

-- 
Par Leijonhufvud                               par at hunter-gatherer.org
Imagine a codebase billions of years old, neveer taken off line, never
maintained by professionals, and hacked on by every damn luser who came
along and felt like a bit of whopee.    -- Johan Larson



From cberry at tajo.ucsd.edu  Mon Jan 16 01:30:09 2006
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sun, 15 Jan 2006 16:30:09 -0800
Subject: [R] LaTeX slide show (Was: Re:  Taking code from packages)
In-Reply-To: <43C92943.4090804@stats.uwo.ca>
References: <000101c6180f$912bca20$a7fdfea9@TAMARA>
	<43C7A110.10808@stats.uwo.ca>
	<43C8AAB2.7030801@maths.lth.se> <43C92943.4090804@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0601151623350.6160@tajo.ucsd.edu>


I have a few notes on using beamer with Sweave here:

 	https://biostat.ucsd.edu/~cberry/beamer/

Particularly, getting verbatim material to display nicely took a bit of 
fiddling.

On Sat, 14 Jan 2006, Duncan Murdoch wrote:

> On 1/14/2006 2:39 AM, Henrik Bengtsson wrote:
>>  Duncan Murdoch wrote:
>> >  On 1/13/2006 2:04 AM, Ales Ziberna wrote:
>> > 
>> > >  Hello!
>> > >
>>  [snip]
>> 
>> >  (I'm a little sensitive about dependencies now, since the LaTeX seminar 
>> >  template I've used a few times no longer works.  It depends on too many 
>> >  LaTeX packages, and someone, somewhere has introduced incompatibilities 
>> >  in them.  Seems like I'll be forced to use Powerpoint or Impress.)
>>
>>  Try LaTeX Beamer!  It is the best thing that happend to LaTeX in a long
>>  time.  Simply beautiful, intuitive and very easy to use, and it's not yet
>>  another 'seminar' or 'prosper'.   Part of MikTeX now.  See
>>  http://latex-beamer.sourceforge.net/ for documentation, examples etc.
>
> Thanks to Henrik and Stephen Eglen for this suggestion.  It does look nice 
> (though the test presentation, beamerexample1.tex failed with
>
> ! Undefined control sequence.
> <recently read> \rowcolors
>
> l.937 \end{frame}
>
> indicating some version incompatibility with what I've got installed, the 
> simpler examples all seem to work and do indeed give nice output.)
>
> Duncan Murdoch
>
>
>
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717



From ales.ziberna at gmail.com  Mon Jan 16 10:18:16 2006
From: ales.ziberna at gmail.com (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Mon, 16 Jan 2006 10:18:16 +0100
Subject: [R] General partition search algorithm (local search,
	genetic algorithm, ...)
Message-ID: <002701c61a7d$c97d63f0$0200a8c0@TAMARA>

Dear R users!

I was wondering if there exists (in R) any general algorithm for finding
optimal partition (optimal allocation of n units into k groups or bins),
such as local search, genetic algorithm, tabu search, ...

By general I mean such that would find an (approximately) optimal partition
based on some user specified criterion function, that would be evaluated on
the data and partition.

It is especially essential that the algorithm does not require any unit
fitness values, only the value of criterion function (or fitness) for the
partition as a whole.

Thanks for any replies!

Best regards,
Ales Ziberna



From Achim.Zeileis at wu-wien.ac.at  Mon Jan 16 10:54:08 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 16 Jan 2006 10:54:08 +0100 (CET)
Subject: [R] R-help Digest, Vol 35, Issue 14
In-Reply-To: <20060114231646.8273.qmail@web25801.mail.ukl.yahoo.com>
References: <20060114231646.8273.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.58.0601161051100.16592@thorin.ci.tuwien.ac.at>

On Sun, 15 Jan 2006, Werner Wernersen wrote:

> Dear all,
>
> Is anybody aware of a tutorial, introduction, overview
> or alike  for cluster
> analysis with R? I have been searching for something
> like that but it seems
> there are only a few rather specialized articles
> around.

As an overview (rather than an introduction or tutorial), the Cluster task
view might be helpful to you:
  http://CRAN.R-project.org/src/contrib/Views/Cluster.html
Z

> I would very much appreciate any hint.
>
> Thanks a million,
>    Werner
>
>
>
>
>
>
> ___________________________________________________________
> Telefonate ohne weitere Kosten vom PC zum PC: http://messenger.yahoo.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Mon Jan 16 11:08:50 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 16 Jan 2006 10:08:50 +0000
Subject: [R] Kite diagrams
In-Reply-To: <20060116075837.GC26757@absaroka.hunter-gatherer.org>
References: <20060116075837.GC26757@absaroka.hunter-gatherer.org>
Message-ID: <43CB70B2.5060003@lancaster.ac.uk>

Par Leijonhufvud wrote:
> I teach biology, and would like to show the students how to use R for
> some statistical assignments. One of those is to make a kite diagram
> (for example as seen in
> http://www.medinavalleycentre.org.uk/images/Bembri1.jpg). Is there any
> way to create one using R? I did a help.search("kite") and looked on the
> r-project HP with no luck. 

  The joy of R is that of course there is a way to create these - you 
just have to write the code!

  The data are, I guess, on the X-axis a discrete set of distance 
points, (identical for each species?), and then for each species an 
abundance measurement - is this continuous or discretized, or does it 
only take the values shown on the key ('ACFOR' = Abundant, Common, 
Frequent, Occasional, Rare??). Looking at the kites I'd guess the data 
are numbers and nearly-continuous.

  Anyway, you can use plot() with type='n' to set out a blank plot with 
X-axis according to your distance scale and a Y-axis of something like 
1:Nspecies, then use the polygon() function to draw the little kites, 
making sure you dont draw anything between separated kites. This 
probably means several polygon() calls or sticking NA's in the coordinates.

  Adding the little cross-section of the shoreline at the top is 
possible too...

> Previously when the course was taugh the students have either abused MS
> Exel or drawn the diagrams by hand.

  Whereas now they can just rely on the goodwill of R-help to do it! :)

  Why do these interesting questions always seem to occur on a Monday 
morning when I really dont want to get on with the stuff I'm supposed to 
be doing....

Barry



From ford at signal.QinetiQ.com  Mon Jan 16 11:37:48 2006
From: ford at signal.QinetiQ.com (Ashley Ford)
Date: Mon, 16 Jan 2006 10:37:48 +0000
Subject: [R] Matrix package download problem
Message-ID: <43CB777C.3020100@signal.qinetiq.com>

Trying to install the Matrix package with  install.packages fails for me 
  on Linux, it is trying to fetch the wrong version 0.99-4, downloading 
the tgz which is 0.99-6 and using R CMD INSTALL works fine


The output from the failed install was

 > install.packages(c("Matrix"))
trying URL 'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
Error in download.file(url, destfile, method, mode = "wb") :
         cannot open URL 
'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
In addition: Warning message:
cannot open: HTTP status was '404 Not Found'
Warning in download.packages(pkgs, destdir = tmpd, available = 
available,  :
          download of package 'Matrix' failed
 > R.version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R



From par at hunter-gatherer.org  Mon Jan 16 11:41:29 2006
From: par at hunter-gatherer.org (Par Leijonhufvud)
Date: Mon, 16 Jan 2006 11:41:29 +0100
Subject: [R] Kite diagrams
In-Reply-To: <43CB70B2.5060003@lancaster.ac.uk>
References: <20060116075837.GC26757@absaroka.hunter-gatherer.org>
	<43CB70B2.5060003@lancaster.ac.uk>
Message-ID: <20060116104128.GG26757@absaroka.hunter-gatherer.org>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> [2006.01.16] wrote:

>  The joy of R is that of course there is a way to create these - you 
> just have to write the code!

Ok. time to start learning...

>  The data are, I guess, on the X-axis a discrete set of distance 
> points, (identical for each species?), and then for each species an 
> abundance measurement - is this continuous or discretized, or does it 
> only take the values shown on the key ('ACFOR' = Abundant, Common, 
> Frequent, Occasional, Rare??). Looking at the kites I'd guess the data 
> are numbers and nearly-continuous.

In our case I would expect them to mostly be numbers, as continous as
real world things actually get, but sometimes only ACFOR. That is, I need
to code it to handle both cases.

>  Anyway, you can use plot() with type='n' to set out a blank plot with 
> X-axis according to your distance scale and a Y-axis of something like 
> 1:Nspecies, then use the polygon() function to draw the little kites, 
> making sure you dont draw anything between separated kites. This 
> probably means several polygon() calls or sticking NA's in the coordinates.
> 
>  Adding the little cross-section of the shoreline at the top is 
> possible too...
> 
> >Previously when the course was taugh the students have either abused MS
> >Exel or drawn the diagrams by hand.
> 
>  Whereas now they can just rely on the goodwill of R-help to do it! :)

I *was* hoping for "use the kite package from CRAN, silly", but
pointers on how to code such a package myself is ok. Good practice, and
a way to repay R for all the use I've gotten from it.

Knowing students I not only need to make it work, but also write
drool-proof instructions for how to use it. :-)

>  Why do these interesting questions always seem to occur on a Monday 
> morning when I really dont want to get on with the stuff I'm supposed to 
> be doing....

Because the universe loves you and hates your employer. Or vice versa.

/Par

-- 
Par Leijonhufvud                               par at hunter-gatherer.org
`You know, there's a word for people who
 think that everyone is out to get them...'
`Yes! Perceptive!'           --Woody Allen



From ligges at statistik.uni-dortmund.de  Mon Jan 16 11:49:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Jan 2006 11:49:55 +0100
Subject: [R] Matrix package download problem
In-Reply-To: <43CB777C.3020100@signal.qinetiq.com>
References: <43CB777C.3020100@signal.qinetiq.com>
Message-ID: <43CB7A53.7010305@statistik.uni-dortmund.de>

Ashley Ford wrote:

> Trying to install the Matrix package with  install.packages fails for me 
>   on Linux, it is trying to fetch the wrong version 0.99-4, downloading 
> the tgz which is 0.99-6 and using R CMD INSTALL works fine
> 
> 
> The output from the failed install was
> 
>  > install.packages(c("Matrix"))
> trying URL 'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
> Error in download.file(url, destfile, method, mode = "wb") :
>          cannot open URL 

Looks like your repository information got not updated (the repositorty 
seems to be consistent). A temporary repository information is stored 
and should be refreshed after a R is restarted. How long is your R 
process already opened?
Hence: Please start a fresh R session and try again.

Uwe Ligges


> 'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
> Warning in download.packages(pkgs, destdir = tmpd, available = 
> available,  :
>           download of package 'Matrix' failed
>  > R.version
>           _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ng296 at cam.ac.uk  Mon Jan 16 11:48:30 2006
From: ng296 at cam.ac.uk (N. Goodacre)
Date: 16 Jan 2006 10:48:30 +0000
Subject: [R] R function for Gap statistic
Message-ID: <Prayer.1.0.16.0601161048300.11654@hermes-2.csi.cam.ac.uk>

Dear All,

I need to calculate the optimal number of clusters for a classification 
based on a large number of observations (tens of thousands). Thibshirani et 
al. proposed the gap statistic for this purpose. Is any R code or fucntion 
available for this? Any help would be appreciated, including suggestions 
about other alternatives for the selection of an optimal number of cluster 
from large datasets.

Thanks, 

 Norman Goodacre



From ripley at stats.ox.ac.uk  Mon Jan 16 11:53:48 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jan 2006 10:53:48 +0000 (GMT)
Subject: [R] Matrix package download problem
In-Reply-To: <43CB777C.3020100@signal.qinetiq.com>
References: <43CB777C.3020100@signal.qinetiq.com>
Message-ID: <Pine.LNX.4.61.0601161050260.22620@gannet.stats>

Try another mirror.

This looks like a caching problem between you and the mirror, as the entry 
in PACKAGES is correct on the mirror.  (The code tries to avoid cached 
copies, but not all caches cooperate.)

On Mon, 16 Jan 2006, Ashley Ford wrote:

> Trying to install the Matrix package with  install.packages fails for me
>  on Linux, it is trying to fetch the wrong version 0.99-4, downloading
> the tgz which is 0.99-6 and using R CMD INSTALL works fine
>
> The output from the failed install was
>
> > install.packages(c("Matrix"))
> trying URL 'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
> Error in download.file(url, destfile, method, mode = "wb") :
>         cannot open URL
> 'http://cran.uk.r-project.org/src/contrib/Matrix_0.99-4.tar.gz'
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found'
> Warning in download.packages(pkgs, destdir = tmpd, available =
> available,  :
>          download of package 'Matrix' failed
> > R.version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bonneu at cict.fr  Mon Jan 16 12:09:31 2006
From: bonneu at cict.fr (Florent Bonneu)
Date: Mon, 16 Jan 2006 12:09:31 +0100
Subject: [R] Problems of data processing
Message-ID: <43CB7EEB.3040902@cict.fr>

I have two problems for the data processing of my large data base (50000 rows). For example, a sample is as follows

Num <- c(1,2,3,4,4,4,5,5)
Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
X <- c(1,,2,3,3,3,6,6)
Y <- c(1,,9,7,7,7,8,8)

toto <- data.frame(Num,Date,Place,X,Y)

The first problem is to keep one line for each Num with the minimum date. I managed to do it with loops but i would like a solution without using loops. It will be better for my large data base.

The other one is to retrieve the coordinates ill-informed. For example, for the same place x1, Num=2 doesn't have X and Y. But, we have this information for Num=1.

The example data base must be like this

Num <- c(1,2,3,4,5)
Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", "1/1/04 9:05")
Place <- c("x1","x1","x3","x4","x5")
X <- c(1,1,2,3,6)
Y <- c(1,1,9,7,8)

toto <- data.frame(Num,Date,Place,X,Y)  


Somebody know how to do ?
Thanks.

Florent Bonneu
Laboratoire de Statistique et Probabilits
bureau 148  bt. 1R2
Universit Toulouse 3
118 route de Narbonne - 31062 Toulouse cedex 9
bonneu at cict.fr <mailto:bonneu at cict.fr>



From florent.baty at unibas.ch  Mon Jan 16 12:22:34 2006
From: florent.baty at unibas.ch (Florent Baty)
Date: Mon, 16 Jan 2006 12:22:34 +0100
Subject: [R] Snow Rmpi Heterogeneous Cluster
Message-ID: <43CB81FA.6080203@unibas.ch>

Dear R-users,

I am trying to make Snow and Rmpi working on an heterogenous cluster of 
linux computers. The master computer is built on a 64 bit architecture 
whereas all nodes are built on a 32 bit architecture. LAM/MPI was 
installed successfully on all machine. LAM boots correctly on the master 
computer and it recognizes all nodes defined in the lamhosts file. 
However, when starting R, launching the 'snow' package and making a 
cluster including several nodes, everything is blocked. It seems that 
Rmpi cannot be started in the nodes which have a different architecture. 
I have to do 'lamhalt' on the first node of the cluster so that R is 
stopped on the master computer. On the other hand, using 'snow' and 
'Rmpi' on an homogeneous cluster works pretty fine.

I read on the 'snow' documentation that for non-homogeneous clusters of 
computers different settings are needed.
On the master computer and on each nodes, I copied the script 
RunSnowNode to the /usr/local/bin directory and defined the R_SNOW_LIB 
and R_HOME_LIB variables in /etc/profile as follow:

R_SNOW_LIB="/usr/local/lib64/R/library"
export R_SNOW_LIB

R_HOME_LIB="/usr/local/lib64/R/library"
export R_HOME_LIB

for the 64 bit architecture and:

R_SNOW_LIB="/usr/local/lib/R/library"
export R_SNOW_LIB

R_HOME_LIB="/usr/local/lib/R/library"
export R_HOME_LIB

for the 32 bit architecture.

In addition the same version of R is installed on all computers (nodes + 
master). Nevertheless, the heterogeneous cluster is not working...

Does anyone can help me solving this problem?

Thanks a lot in advance.

Florent

-- 
--------------------------------------------------
		Dr Florent BATY
Pulmonary Gene Research, Universit??tsspital Basel
   Petersgraben 4, CH-4031 Basel, Switzerland
 tel: +41 61 265 57 27 - fax: +41 61 265 45 87



From jacques.veslot at cirad.fr  Mon Jan 16 13:06:50 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 16 Jan 2006 16:06:50 +0400
Subject: [R] Problems of data processing
In-Reply-To: <43CB7EEB.3040902@cict.fr>
References: <43CB7EEB.3040902@cict.fr>
Message-ID: <43CB8C5A.8020603@cirad.fr>

something wrong in X and Y definitions... but this could work:

do.call("rbind", lapply(split(toto, toto$Num),
    function(x) x[which.min(as.POSIXct(strptime(toto$Date, "%d/%m/%y 
%H:%M"))),]))

i don't understand the second query; do you want to keep the first line 
when there are several lines for the same place ?


Florent Bonneu a crit :

>I have two problems for the data processing of my large data base (50000 rows). For example, a sample is as follows
>
>Num <- c(1,2,3,4,4,4,5,5)
>Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
>Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
>X <- c(1,,2,3,3,3,6,6)
>Y <- c(1,,9,7,7,7,8,8)
>
>toto <- data.frame(Num,Date,Place,X,Y)
>
>The first problem is to keep one line for each Num with the minimum date. I managed to do it with loops but i would like a solution without using loops. It will be better for my large data base.
>
>The other one is to retrieve the coordinates ill-informed. For example, for the same place x1, Num=2 doesn't have X and Y. But, we have this information for Num=1.
>
>The example data base must be like this
>
>Num <- c(1,2,3,4,5)
>Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", "1/1/04 9:05")
>Place <- c("x1","x1","x3","x4","x5")
>X <- c(1,1,2,3,6)
>Y <- c(1,1,9,7,8)
>
>toto <- data.frame(Num,Date,Place,X,Y)  
>
>
>Somebody know how to do ?
>Thanks.
>
>Florent Bonneu
>Laboratoire de Statistique et Probabilits
>bureau 148  bt. 1R2
>Universit Toulouse 3
>118 route de Narbonne - 31062 Toulouse cedex 9
>bonneu at cict.fr <mailto:bonneu at cict.fr>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From cwc at biocentrum.dtu.dk  Mon Jan 16 13:51:59 2006
From: cwc at biocentrum.dtu.dk (Chia-Wen Chang - Cleo)
Date: Mon, 16 Jan 2006 13:51:59 +0100
Subject: [R] How to analysis Y98 chips using RankProd package?
Message-ID: <409B366CEC0FC34F9D7C1D73AD756E8A01B8FFD9@MAILSERVER.biocentrum.dtu.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/6f5bfd4a/attachment.pl

From bonneu at cict.fr  Mon Jan 16 14:04:58 2006
From: bonneu at cict.fr (Florent Bonneu)
Date: Mon, 16 Jan 2006 14:04:58 +0100
Subject: [R] Problems of data processing
In-Reply-To: <43CB8C5A.8020603@cirad.fr>
References: <43CB7EEB.3040902@cict.fr> <43CB8C5A.8020603@cirad.fr>
Message-ID: <43CB99FA.3000803@cict.fr>

Indeed,
X <- c(1,Na,2,3,3,3,6,6)
Y <- c(1,Na,9,7,7,7,8,8)

I want to obtain one line for each Num. It's not a problem if there are 
several lines for the same place, because my identifier is Num. I just 
want to get X and Y well-informed in an other line for the same place. 
For example, "Num=2" is at the place "x1", like "Num=1", but we don't 
have the coordinates X and Y for "Num=2".  Now, the same coordinates are 
well-informed for "Num=1", so i want to retrieve this coordinates in my 
line "Num=2" for my columns X and Y.



Jacques VESLOT wrote:

> something wrong in X and Y definitions... but this could work:
>
> do.call("rbind", lapply(split(toto, toto$Num),
>    function(x) x[which.min(as.POSIXct(strptime(toto$Date, "%d/%m/%y 
> %H:%M"))),]))
>
> i don't understand the second query; do you want to keep the first 
> line when there are several lines for the same place ?
>
>
> Florent Bonneu a crit :
>
>> I have two problems for the data processing of my large data base 
>> (50000 rows). For example, a sample is as follows
>>
>> Num <- c(1,2,3,4,4,4,5,5)
>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", 
>> "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
>> Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
>> X <- c(1,,2,3,3,3,6,6)
>> Y <- c(1,,9,7,7,7,8,8)
>>
>> toto <- data.frame(Num,Date,Place,X,Y)
>>
>> The first problem is to keep one line for each Num with the minimum 
>> date. I managed to do it with loops but i would like a solution 
>> without using loops. It will be better for my large data base.
>>
>> The other one is to retrieve the coordinates ill-informed. For 
>> example, for the same place x1, Num=2 doesn't have X and Y. But, we 
>> have this information for Num=1.
>>
>> The example data base must be like this
>>
>> Num <- c(1,2,3,4,5)
>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", 
>> "1/1/04 9:05")
>> Place <- c("x1","x1","x3","x4","x5")
>> X <- c(1,1,2,3,6)
>> Y <- c(1,1,9,7,8)
>>
>> toto <- data.frame(Num,Date,Place,X,Y) 
>>
>> Somebody know how to do ?
>> Thanks.
>>
>> Florent Bonneu
>> Laboratoire de Statistique et Probabilits
>> bureau 148  bt. 1R2
>> Universit Toulouse 3
>> 118 route de Narbonne - 31062 Toulouse cedex 9
>> bonneu at cict.fr <mailto:bonneu at cict.fr>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>  
>>
>
>
>



From katrin.bernath at wsl.ch  Mon Jan 16 14:15:14 2006
From: katrin.bernath at wsl.ch (Katrin Bernath)
Date: Mon, 16 Jan 2006 14:15:14 +0100
Subject: [R] Poisson and negative binomial models with truncation
Message-ID: <5.2.1.1.1.20060116134717.04247cc0@mail.wsl.ch>

I am fitting count data models with zero-truncated data.

Are there commands in R to adjust the Poisson model (glm(y~x, poisson))
and the negative binomial model (glm.nb(y~x)) for truncated distributions?

Thanks in advance!
Katrin Bernath


---------------------------------------------------------
Katrin Bernath
Eidg. Forschungsanstalt WSL
Abteilung ??konomie
Z??rcherstrasse 111
CH-8903 Birmensdorf

Telefon +41-44-739 25 46
Fax +41-44-739 25 88
katrin.bernath at wsl.ch
http://www.wsl.ch/economics



From ggrothendieck at gmail.com  Mon Jan 16 15:22:00 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 Jan 2006 09:22:00 -0500
Subject: [R] General partition search algorithm (local search,
	genetic algorithm, ...)
In-Reply-To: <002701c61a7d$c97d63f0$0200a8c0@TAMARA>
References: <002701c61a7d$c97d63f0$0200a8c0@TAMARA>
Message-ID: <971536df0601160622n409b2783q81184f80da3af998@mail.gmail.com>

You may be able to use linear programming.  Check out this thread:

   http://tolstoy.newcastle.edu.au/~rking/R/help/05/11/16009.html

On 1/16/06, Ale? ?iberna <ales.ziberna at gmail.com> wrote:
> Dear R users!
>
> I was wondering if there exists (in R) any general algorithm for finding
> optimal partition (optimal allocation of n units into k groups or bins),
> such as local search, genetic algorithm, tabu search, ...
>
> By general I mean such that would find an (approximately) optimal partition
> based on some user specified criterion function, that would be evaluated on
> the data and partition.
>
> It is especially essential that the algorithm does not require any unit
> fitness values, only the value of criterion function (or fitness) for the
> partition as a whole.
>
> Thanks for any replies!
>
> Best regards,
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jan 16 15:31:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jan 2006 14:31:14 +0000 (GMT)
Subject: [R] Powell's Metod
In-Reply-To: <43CADD09.4040003@coubros.com>
References: <43CAADC9.5020104@coubros.com>
	<Pine.LNX.4.61.0601152252180.10972@gannet.stats>
	<43CADD09.4040003@coubros.com>
Message-ID: <Pine.LNX.4.61.0601161420070.29302@gannet.stats>

I am not aware of an R implementation of that method.  However, if you 
meet the licensing conditions (which for my copy of NR do not allow me to 
use the code on a Unix machine even if I typed it in) you could interface 
to the NR C/C++ routines.

On Sun, 15 Jan 2006, Tolga Uzuner wrote:

> Ah, my apologies. I meant the method outlined in section 10.5 of
> Numerical Recipes using Powell's heuristic of discarding the direction
> of largest decrease. A nice twist would be if linmin (line
> minimization)  were to be implemented using the methods described in
> 10.1-10.3, but that is just a nice-to-have.
>
> From what I understand, Nelder-Mead is the method in section 10.4
> (downhill simplex), CG is section 10.6, and BFGS is section 10.7.
>
> Admittedly, optim is fantastic most of the time. However, I have a
> factor model over hundreds of elements, where I am trying to back out
> the factor distribution and it appears that the method in 10.5 might work.
>
> Thanks in advance in any case,
> Tolga
>
> Prof Brian Ripley wrote:
>
>> (Michael) Powell (of Harwell and Cambridge) invented several
>> optimization methods.  You will need to give us a precise reference.
>>
>> Powell is associated with earlier versions of the Nelder-Mead, CG and
>> BFGS methods implemented in optim() but in each case different
>> variants have best stood the course of time.
>>
>> On Sun, 15 Jan 2006, Tolga Uzuner wrote:
>>
>>> Has anyone implemented Powell's Method for minimisation in R ?
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rvaradhan at jhmi.edu  Mon Jan 16 15:45:02 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 16 Jan 2006 09:45:02 -0500
Subject: [R] Powell's Metod
In-Reply-To: <Pine.LNX.4.61.0601161420070.29302@gannet.stats>
Message-ID: <000901c61aab$6eb17050$7c94100a@win.ad.jhu.edu>

Tolga,

Have you considered an EM algorithm for your factor analysis problem?  A
reference for this is:

Rubin, D. and Thayer, D. (1982). EM algorithms for ML factor analysis.
Psychometrika, 47(1):69--76. 

Hope this is helpful,
Ravi.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
> Sent: Monday, January 16, 2006 9:31 AM
> To: Tolga Uzuner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Powell's Metod
> 
> I am not aware of an R implementation of that method.  However, if you
> meet the licensing conditions (which for my copy of NR do not allow me to
> use the code on a Unix machine even if I typed it in) you could interface
> to the NR C/C++ routines.
> 
> On Sun, 15 Jan 2006, Tolga Uzuner wrote:
> 
> > Ah, my apologies. I meant the method outlined in section 10.5 of
> > Numerical Recipes using Powell's heuristic of discarding the direction
> > of largest decrease. A nice twist would be if linmin (line
> > minimization)  were to be implemented using the methods described in
> > 10.1-10.3, but that is just a nice-to-have.
> >
> > From what I understand, Nelder-Mead is the method in section 10.4
> > (downhill simplex), CG is section 10.6, and BFGS is section 10.7.
> >
> > Admittedly, optim is fantastic most of the time. However, I have a
> > factor model over hundreds of elements, where I am trying to back out
> > the factor distribution and it appears that the method in 10.5 might
> work.
> >
> > Thanks in advance in any case,
> > Tolga
> >
> > Prof Brian Ripley wrote:
> >
> >> (Michael) Powell (of Harwell and Cambridge) invented several
> >> optimization methods.  You will need to give us a precise reference.
> >>
> >> Powell is associated with earlier versions of the Nelder-Mead, CG and
> >> BFGS methods implemented in optim() but in each case different
> >> variants have best stood the course of time.
> >>
> >> On Sun, 15 Jan 2006, Tolga Uzuner wrote:
> >>
> >>> Has anyone implemented Powell's Method for minimisation in R ?
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From tolga at coubros.com  Mon Jan 16 16:36:57 2006
From: tolga at coubros.com (tolga@coubros.com)
Date: Mon, 16 Jan 2006 15:36:57 +0000
Subject: [R] Powell's Metod
In-Reply-To: <000901c61aab$6eb17050$7c94100a@win.ad.jhu.edu>
References: <000901c61aab$6eb17050$7c94100a@win.ad.jhu.edu>
Message-ID: <1137425817.43cbbd998c7df@mail.coubros.com>

Many thanks, let me look,
Tolga

Quoting Ravi Varadhan <rvaradhan at jhmi.edu>:

> Tolga,
>
> Have you considered an EM algorithm for your factor analysis problem?  A
> reference for this is:
>
> Rubin, D. and Thayer, D. (1982). EM algorithms for ML factor analysis.
> Psychometrika, 47(1):69--76.
>
> Hope this is helpful,
> Ravi.
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
> > Sent: Monday, January 16, 2006 9:31 AM
> > To: Tolga Uzuner
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Powell's Metod
> >
> > I am not aware of an R implementation of that method.  However, if you
> > meet the licensing conditions (which for my copy of NR do not allow me to
> > use the code on a Unix machine even if I typed it in) you could interface
> > to the NR C/C++ routines.
> >
> > On Sun, 15 Jan 2006, Tolga Uzuner wrote:
> >
> > > Ah, my apologies. I meant the method outlined in section 10.5 of
> > > Numerical Recipes using Powell's heuristic of discarding the direction
> > > of largest decrease. A nice twist would be if linmin (line
> > > minimization)  were to be implemented using the methods described in
> > > 10.1-10.3, but that is just a nice-to-have.
> > >
> > > From what I understand, Nelder-Mead is the method in section 10.4
> > > (downhill simplex), CG is section 10.6, and BFGS is section 10.7.
> > >
> > > Admittedly, optim is fantastic most of the time. However, I have a
> > > factor model over hundreds of elements, where I am trying to back out
> > > the factor distribution and it appears that the method in 10.5 might
> > work.
> > >
> > > Thanks in advance in any case,
> > > Tolga
> > >
> > > Prof Brian Ripley wrote:
> > >
> > >> (Michael) Powell (of Harwell and Cambridge) invented several
> > >> optimization methods.  You will need to give us a precise reference.
> > >>
> > >> Powell is associated with earlier versions of the Nelder-Mead, CG and
> > >> BFGS methods implemented in optim() but in each case different
> > >> variants have best stood the course of time.
> > >>
> > >> On Sun, 15 Jan 2006, Tolga Uzuner wrote:
> > >>
> > >>> Has anyone implemented Powell's Method for minimisation in R ?
> > >>
> > >>
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> > >
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>
>



From whschreiber at onlinehome.de  Mon Jan 16 17:24:24 2006
From: whschreiber at onlinehome.de (Dr. Walter H. Schreiber)
Date: Mon, 16 Jan 2006 17:24:24 +0100
Subject: [R] Standardized beta-coefficients in regression
Message-ID: <200601161724.24677.whschreiber@onlinehome.de>

Hello list,

I am used to give a lot of attention to the standardized regression 
coefficients, which in SPSS are listed automatically. 

Is there alternative to running the last two lines in the following example to 
get all the information?

ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
summary( lm(ctl ~ trt) )
summary( lm(scale(ctl) ~ scale(trt)) )

Any hints are appreciated.

	Walter.

-- 
Dr. Walter H. Schreiber
Dept. of Psychology
University of Koblenz-Landau



From szlevine at nana.co.il  Mon Jan 16 17:26:23 2006
From: szlevine at nana.co.il (Stephen)
Date: Mon, 16 Jan 2006 18:26:23 +0200
Subject: [R] gplots
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD67C@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/aba57ba6/attachment.pl

From ggrothendieck at gmail.com  Mon Jan 16 17:37:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 Jan 2006 11:37:58 -0500
Subject: [R] Standardized beta-coefficients in regression
In-Reply-To: <200601161724.24677.whschreiber@onlinehome.de>
References: <200601161724.24677.whschreiber@onlinehome.de>
Message-ID: <971536df0601160837k54f937yeaee4e32194dbe5f@mail.gmail.com>

Try this, possibly with a better name:

f <- function(formula, ...) {
	print(summary(lm(formula, ...)))
	formula <- update(formula, scale(.) ~ scale(.))
	print(summary(lm(formula, ...)))
}

f(ctl ~ trt)


On 1/16/06, Dr. Walter H. Schreiber <whschreiber at onlinehome.de> wrote:
> Hello list,
>
> I am used to give a lot of attention to the standardized regression
> coefficients, which in SPSS are listed automatically.
>
> Is there alternative to running the last two lines in the following example to
> get all the information?
>
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> summary( lm(ctl ~ trt) )
> summary( lm(scale(ctl) ~ scale(trt)) )
>
> Any hints are appreciated.
>
>        Walter.
>
> --
> Dr. Walter H. Schreiber
> Dept. of Psychology
> University of Koblenz-Landau
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From a.menicacci at fr.fournierpharma.com  Mon Jan 16 18:05:45 2006
From: a.menicacci at fr.fournierpharma.com (a.menicacci@fr.fournierpharma.com)
Date: Mon, 16 Jan 2006 18:05:45 +0100
Subject: [R] Homogenous groups building - Randomisation
Message-ID: <OFC79C0E5D.97AC8F04-ONC12570F8.005CBCBF-C12570F8.005DE926@fr.fournierpharma.com>





Dear R-users,

We expect to form N homogeneous groups of n features from an
experimentation including N*n data.
The aim is to prevent group effects.
How to do that with R functionalitites ? Does anyone know any way enabling
this ?


Example :

100 patients are observed. 3 biochemical parameters are mesured for each
one (Red and white globules ans glycemia).

Patient          RG           RW          Gly
1              3.4      1.38         1.62
2              1.8      1.19     1.55
3              1.9      1.26     1.77
4              3.0      1.29     1.72
5              1.9      1.09     1.72
6              3.3      1.31     1.63
...                  ...             ...      ...


These are freakish data.

How to compute 10 "equivalent" groups ?
For example with closed parameters means between groups :

Group   RGmean      RWmean      Glymean
1     1.5               1.22              1.68
2     1.3         1.29              1.75
3           1.6               1.25              1.63
4     1.2               1.23              1.70
...   ...               ...               ...


Best regards.


Alexandre MENICACCI
Bioinformatics - FOURNIER PHARMA
50, rue de Dijon - 21121 Daix - FRANCE
a.menicacci at fr.fournierpharma.com
t??l : 03.80.44.76.17



From j.logsdon at quantex-research.com  Mon Jan 16 18:07:43 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 16 Jan 2006 17:07:43 +0000 (GMT)
Subject: [R] lme output
Message-ID: <Pine.LNX.4.10.10601161655220.229-100000@mercury.quantex>

I am trying to extract the solution from a simple lme calculation.

For example (the first 4 have a mean 0, sd 1):

> y<-c(-1.118,-.5,.5,1.118,10)
> gp<-factor(c(rep('one',4),'two'))
> res<-lme(y~1,rand=~1|gp) 
Linear mixed-effects model fit by REML
  Data: NULL 
  Log-restricted-likelihood: -8.67141
  Fixed: y ~ 1 
(Intercept) 
   4.962502 

Random effects:
 Formula: ~1 | gp
        (Intercept)  Residual
StdDev:    7.026737 0.9999747

Number of Observations: 5
Number of Groups: 2 

The problem is to get the random effects intercept - essentially the
between groups standard deviation - into a scalar.  I can't see anything
in the values from res.  The other values - fixed intercept and residual -
are clearly available - eg res$coeff$fixed is straight forward.

I would use lmer but this does not appear to return any values.

Can someone advise please?  I do recall having this problem some time ago
but that was with a much older version.  This is with R 2.2.1 and the
latest lme.

TIA


Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From ripley at stats.ox.ac.uk  Mon Jan 16 18:13:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jan 2006 17:13:02 +0000 (GMT)
Subject: [R] Standardized beta-coefficients in regression
In-Reply-To: <200601161724.24677.whschreiber@onlinehome.de>
References: <200601161724.24677.whschreiber@onlinehome.de>
Message-ID: <Pine.LNX.4.61.0601161703070.8880@gannet.stats>

On Mon, 16 Jan 2006, Dr. Walter H. Schreiber wrote:

> Hello list,
>
> I am used to give a lot of attention to the standardized regression
> coefficients, which in SPSS are listed automatically.

I do wonder why?  Most people I have encountered who do that are 
interpreting them in invalid ways.

> Is there alternative to running the last two lines in the following example to
> get all the information?

Yes, but why do you want one?  (You don't need summary, just coef, in the 
second line, and you also do not need an intercept.)  For a single 
regressor as here, just cor(ctl, trt).

> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> summary( lm(ctl ~ trt) )
> summary( lm(scale(ctl) ~ scale(trt)) )

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bhx5 at mevik.net  Mon Jan 16 14:59:21 2006
From: bhx5 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 16 Jan 2006 14:59:21 +0100
Subject: [R] [R-pkgs] New package lspls
Message-ID: <m0lkxgp93a.fsf@bar.nemo-project.org>

Dear R useRs,

A new package `lspls' is now available on CRAN.  It implements the
LS-PLS (least squares--partial least squares) regression method,
described in for instance

J??rgensen, K., Segtnan, V. H., Thyholt, K., N??s, T. (2004) A Comparison of
Methods for Analysing Regression Models with Both Spectral and Designed
Variables; Journal of Chemometrics 18(10), 451--464.

The current version of the package (0.1-0) should probably be
considered `alpha software'.  Nothing is cast in iron yet, and
especially the formula interface and internal structure are apt to
change in future versions.  The software should, however, be fully
usable in its present form.

`lspls' currently includes fit and cross-validation functions with
formula interfaces, a predict method, and plots of scores, loadings
and (R)MSEP values.

Suggestions, bug reports and other comments are very welcome.

-- 
Sincerely,
Bj??rn-Helge Mevik

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Antje.Schuele at komdat.com  Mon Jan 16 18:33:34 2006
From: Antje.Schuele at komdat.com (=?iso-8859-1?Q?Antje_Sch=FCle?=)
Date: Mon, 16 Jan 2006 18:33:34 +0100
Subject: [R] label of second y-axis in xyplot (lattice)
Message-ID: <F5076E7EAA58F448A0EEC05ADE2317BD0307E4@muc-exch001.munich.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/7ce392f3/attachment.pl

From andrej.kastrin at siol.net  Mon Jan 16 18:51:20 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Mon, 16 Jan 2006 18:51:20 +0100
Subject: [R] Problem with plot()
Message-ID: <43CBDD18.8080207@siol.net>

Dear R useRs,

I have a problem to add title to the following graphics;

Tukey=TukeyHSD(aov(CA~C), "C",ordered=TRUE))
plot (Tukey, main="My first graph")

actually, it draw a graph, but it also display:

parameter "main" could not be set in high-level plot() function.

If I execute:
plot(rnorm(1000),main="My first plot"), no error occur.

What's the difference in those two examples? Thanks in advance.



From Peter.Rossi at chicagogsb.edu  Mon Jan 16 19:02:50 2006
From: Peter.Rossi at chicagogsb.edu (Rossi, Peter E.)
Date: Mon, 16 Jan 2006 12:02:50 -0600
Subject: [R] run-time of an R function
Message-ID: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>


I have noticed dramatic differences in the run-time for the execution of
one of my functions depending on whether or not R was restarted.
Immediately
after restart of R GUI, exec time = 2.8 min. If I then repeat the
execution
of the function in the same R session, exec time = 7.1 min. Removing all
objects via rm(list=(all=TRUE)) and initiating gc (gc(reset=TRUE))
helps, but
only slightly (exec time = 5.0 min). 

any thoughts on why this happens? 

I realize that this is somewhat of a generic question given that I
haven't
provided the source code for the function.  However, the function
is very involved so that I think presenting it might violate the posting
guidelines. The function creates and recreates large list structures
and calls numerical functions on elements of these lists.  The list
structures
can be nested several levels deep, e.g. a list of lists.

Are there particular aspects of R function programming to watch out for
that can create this sort of problem?

I'm running windows R v 2.2.1

thanks!

peter r
 

................................
  Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 353, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637, USA
 Tel: (773) 702-7513   |   Fax: (773) 834-2081



From ligges at statistik.uni-dortmund.de  Mon Jan 16 19:26:50 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Jan 2006 19:26:50 +0100
Subject: [R] Problem with plot()
In-Reply-To: <43CBDD18.8080207@siol.net>
References: <43CBDD18.8080207@siol.net>
Message-ID: <43CBE56A.7080701@statistik.uni-dortmund.de>

Andrej Kastrin wrote:

> Dear R useRs,
> 
> I have a problem to add title to the following graphics;
> 
> Tukey=TukeyHSD(aov(CA~C), "C",ordered=TRUE))
> plot (Tukey, main="My first graph")
> 
> actually, it draw a graph, but it also display:
> 
> parameter "main" could not be set in high-level plot() function.
> 
> If I execute:
> plot(rnorm(1000),main="My first plot"), no error occur.
> 
> What's the difference in those two examples? Thanks in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


The title is hardcoded in plot.TukeyHSD().
Type
  plot.TukeyHSD
and see its code. Simply adapt it as you need ...

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Mon Jan 16 19:29:04 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Jan 2006 19:29:04 +0100
Subject: [R] run-time of an R function
In-Reply-To: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>
References: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>
Message-ID: <43CBE5F0.1010103@statistik.uni-dortmund.de>

Rossi, Peter E. wrote:

> I have noticed dramatic differences in the run-time for the execution of
> one of my functions depending on whether or not R was restarted.
> Immediately
> after restart of R GUI, exec time = 2.8 min. If I then repeat the
> execution
> of the function in the same R session, exec time = 7.1 min. Removing all
> objects via rm(list=(all=TRUE)) and initiating gc (gc(reset=TRUE))
> helps, but
> only slightly (exec time = 5.0 min). 
> 
> any thoughts on why this happens? 


Quite probably due to memory fragmentation.
Smaller objects and more RAM might help a little bit.

Uwe Ligges


> I realize that this is somewhat of a generic question given that I
> haven't
> provided the source code for the function.  However, the function
> is very involved so that I think presenting it might violate the posting
> guidelines. The function creates and recreates large list structures
> and calls numerical functions on elements of these lists.  The list
> structures
> can be nested several levels deep, e.g. a list of lists.
> 
> Are there particular aspects of R function programming to watch out for
> that can create this sort of problem?
> 
> I'm running windows R v 2.2.1
> 
> thanks!
> 
> peter r
>  
> 
> ................................
>   Peter E. Rossi
>  Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
>  Editor, Quantitative Marketing and Economics
>  Rm 353, Graduate School of Business, U of Chicago
>  5807 S. Woodlawn Ave, Chicago IL 60637, USA
>  Tel: (773) 702-7513   |   Fax: (773) 834-2081
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From asaguiar at spsconsultoria.com  Mon Jan 16 19:30:44 2006
From: asaguiar at spsconsultoria.com (Alexandre Santos Aguiar)
Date: Mon, 16 Jan 2006 16:30:44 -0200
Subject: [R] run-time of an R function
In-Reply-To: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>
References: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>
Message-ID: <200601161630.45354.asaguiar@spsconsultoria.com>

Em Seg 16 Jan 2006 16:02, Rossi, Peter E. escreveu:
> of the function in the same R session, exec time = 7.1 min. Removing all
> objects via rm(list=(all=TRUE)) and initiating gc (gc(reset=TRUE))
> helps, but
> only slightly (exec time = 5.0 min).

Hi,

I am currently using extensively a script with +250 lines that performed the 
opposite:

3min 26s as first run
2min 57s in the second run and with several objects in memory

My script performs calculations of bayesian probabilities over 11 variables of 
1033 observations, uses no special library, performs several disk writes to a 
networked nfs exported directory and does the clean up of all explicitly 
created objects.

BTW, I currently use R 2.2.1 and it seems much faster than version 1.9 that I 
used before.

-- 

          Alexandre Santos Aguiar, MD
- independent consultant for health research -
       R Botucatu, 591 cj 81 - 04037-005
            S??o Paulo - SP - Brazil
             tel +55-11-9320-2046
             fax +55-11-5549-8760
            www.spsconsultoria.com



From gerifalte28 at hotmail.com  Mon Jan 16 19:48:24 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 16 Jan 2006 18:48:24 +0000
Subject: [R] Poisson and negative binomial models with truncation
In-Reply-To: <5.2.1.1.1.20060116134717.04247cc0@mail.wsl.ch>
Message-ID: <BAY103-F109E1884F1A6BBF92CD3CBA61B0@phx.gbl>

Did you try RSiteSearch("zero-inflated")?

Francisco

>From: Katrin Bernath <katrin.bernath at wsl.ch>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Poisson and negative binomial models with truncation
>Date: Mon, 16 Jan 2006 14:15:14 +0100
>
>I am fitting count data models with zero-truncated data.
>
>Are there commands in R to adjust the Poisson model (glm(y~x, poisson))
>and the negative binomial model (glm.nb(y~x)) for truncated distributions?
>
>Thanks in advance!
>Katrin Bernath
>
>
>---------------------------------------------------------
>Katrin Bernath
>Eidg. Forschungsanstalt WSL
>Abteilung konomie
>Zrcherstrasse 111
>CH-8903 Birmensdorf
>
>Telefon +41-44-739 25 46
>Fax +41-44-739 25 88
>katrin.bernath at wsl.ch
>http://www.wsl.ch/economics
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From admin at biostatistic.de  Mon Jan 16 19:53:59 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Mon, 16 Jan 2006 19:53:59 +0100
Subject: [R] rsq.rpart not found ...
Message-ID: <43CBEBC7.7090209@biostatistic.de>

Hi to all
I tried  help.search("rsq") and found


Help files with alias or concept or title matching 'rsq.rpart' using
regular expression matching:

rsq.rpart(rpart)        Plots the Approximate R-Square for the
                        Different Splits



then I tried to use the function but I got the error object not found
must I load any package before? there is nothing in the helpfile (R 2.2.0)

Regards Knut



From admin at biostatistic.de  Mon Jan 16 20:00:20 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Mon, 16 Jan 2006 20:00:20 +0100
Subject: [R] problem solved
In-Reply-To: <43CBEBC7.7090209@biostatistic.de>
References: <43CBEBC7.7090209@biostatistic.de>
Message-ID: <43CBED44.5060109@biostatistic.de>

 >library(rpart)

Regards Knut



From admin at biostatistic.de  Mon Jan 16 20:35:03 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Mon, 16 Jan 2006 20:35:03 +0100
Subject: [R] rsq.rpart   is there any R-Square value like in SPSS?
Message-ID: <43CBF567.6050708@biostatistic.de>


I found in the fullrefman only the function rsq.rpart with the search tearm r-square or rsquare.

SPSS returnes the R Square an i am not albe to found the value in R

Maybe there is another name for it



Model Summary(d)

Model    R       R Square    Adjusted R Square    Std. Error of the Estimate

1       ,768(a)       ,589             ,584                        171,032



Regards Knut



From ripley at stats.ox.ac.uk  Mon Jan 16 20:35:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jan 2006 19:35:35 +0000 (GMT)
Subject: [R] run-time of an R function
In-Reply-To: <200601161630.45354.asaguiar@spsconsultoria.com>
References: <1E7B167439290641966EB161D4330798CE0CD2@GSBEX.gsb.uchicago.edu>
	<200601161630.45354.asaguiar@spsconsultoria.com>
Message-ID: <Pine.LNX.4.61.0601161929550.10450@gannet.stats>

On Mon, 16 Jan 2006, Alexandre Santos Aguiar wrote:

> Em Seg 16 Jan 2006 16:02, Rossi, Peter E. escreveu:
>> of the function in the same R session, exec time = 7.1 min. Removing all
>> objects via rm(list=(all=TRUE)) and initiating gc (gc(reset=TRUE))
>> helps, but
>> only slightly (exec time = 5.0 min).
>
> Hi,
>
> I am currently using extensively a script with +250 lines that performed the
> opposite:
>
> 3min 26s as first run
> 2min 57s in the second run and with several objects in memory

This sounds like of those cases where automatic tuning of the garbage 
collector is helping.  Setting the startup parameters (see ?Memory) to 
similar values to gc() reports at the end of the run will probably help.
You can get further information from gc.time().

Most often performance deteriorates during a session as Uwe Ligges points 
out (at least on a 32-bit system as he uses).

> My script performs calculations of bayesian probabilities over 11 variables of
> 1033 observations, uses no special library, performs several disk writes to a
> networked nfs exported directory and does the clean up of all explicitly
> created objects.
>
> BTW, I currently use R 2.2.1 and it seems much faster than version 1.9 that I
> used before.

Not surprising as we continually do performance tuning.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ccleland at optonline.net  Mon Jan 16 20:43:16 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 16 Jan 2006 14:43:16 -0500
Subject: [R] rsq.rpart   is there any R-Square value like in SPSS?
In-Reply-To: <43CBF567.6050708@biostatistic.de>
References: <43CBF567.6050708@biostatistic.de>
Message-ID: <43CBF754.8080102@optonline.net>

I guess you want the Multiple R-Squared for a linear model.  You can get 
that by using summary() on a linear model.  For example:

ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2,10,20, labels=c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)
summary(lm.D9)$r.squared
summary(lm.D9)$adj.r.squared

Knut Krueger wrote:
> I found in the fullrefman only the function rsq.rpart with the search tearm r-square or rsquare.
> 
> SPSS returnes the R Square an i am not albe to found the value in R
> 
> Maybe there is another name for it
> 
> 
> 
> Model Summary(d)
> 
> Model    R       R Square    Adjusted R Square    Std. Error of the Estimate
> 
> 1       ,768(a)       ,589             ,584                        171,032
> 
> 
> 
> Regards Knut
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Chang_Shen at progressive.com  Mon Jan 16 21:30:25 2006
From: Chang_Shen at progressive.com (Chang Shen)
Date: Mon, 16 Jan 2006 15:30:25 -0500
Subject: [R] new comer's  question
Message-ID: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>


I am new to R.  I try to search the web but could not find the answer so I
post it here asking for help.



I have a csv file looks like this: (between two ==== lines)
===========================

Machine Name,"Resource, Type","Resource, Sub-type","Resource,
Instance",Date,,Data ->,,,,,,
,0.041666667,,,,,,,,,,,

Time (HH:MM) ->,,,,,,0:00,0:15,0:30,0:45,1:00,1:15,1:30
SCINFR06,Cache,Copy Read Hits %,,10-Jan-06,Cache->Copy Read Hits
%,0.99,1,1,1,1,1,0.99
SCINFR06,Cache,Data Map Hits %,,10-Jan-06,Cache->Data Map Hits
%,1,1,1,1,1,1,1

Time (HH:MM) ->,,,,,,0:00,0:15,0:30,0:45,1:00,1:15,1:30
SCINFR06,LogicalDisk,% Disk Read Time,C:,10-Jan-06,LogicalDisk->% Disk Read
Time->C:,2.14,1.52,1.94,1.68,2.52,2.05,2.66
SCINFR06,LogicalDisk,% Disk Read Time,D:,10-Jan-06,LogicalDisk->% Disk Read
Time->D:,0.04,0,0,0.08,0,0,0
SCINFR06,LogicalDisk,% Disk Read
Time,HarddiskVolume1,10-Jan-06,LogicalDisk->% Disk Read
Time->HarddiskVolume1,0,0,0,0,0,0,0
SCINFR06,LogicalDisk,% Disk Read Time,_Total,10-Jan-06,LogicalDisk->% Disk
Read Time->_Total,0.72,0.51,0.65,0.59,0.84,0.68,0.89

==============================


First I load it by read.table call:

myArray <- read.table("c:/mydata.csv",sep=",");

After this,  the array element myArray[1,2] looks like this

>myArray[1,2]
[1] Resource, Type
Levels:  0.041666667 Cache LogicalDisk Resource, Type

Here are the questions:
(1) What does Levels mean?
(2) When I try to split the string "Resource, Type", which is myArray[1,2],
using function strsplit(), I got error:

> w<-strsplit(myArray[1,2],",")
Error in strsplit(x, as.character(split), as.logical(extended),
as.logical(fixed),  :
        non-character argument in strsplit()

Then I tried this:

> y<-myArray[1,2]
> y
[1] Resource, Type
Levels:  0.041666667 Cache LogicalDisk Resource, Type
> w<-strsplit(y,",")
Error in strsplit(x, as.character(split), as.logical(extended),
as.logical(fixed),  :
        non-character argument in strsplit()


But the following call does not cause any error.

>  y<-"Resource, Type"
> w<-strsplit(y,",")
> w
[[1]]
[1] "Resource" " Type"



what is wrong?


Thanks

Chang



From Knut-krueger at einthal.de  Mon Jan 16 21:39:31 2006
From: Knut-krueger at einthal.de (Knut Krueger)
Date: Mon, 16 Jan 2006 21:39:31 +0100
Subject: [R] rsq.rpart   is there any R-Square value like in SPSS?
In-Reply-To: <43CBF754.8080102@optonline.net>
References: <43CBF567.6050708@biostatistic.de> <43CBF754.8080102@optonline.net>
Message-ID: <43CC0483.3010302@einthal.de>



Chuck Cleland schrieb:

> I guess you want the Multiple R-Squared for a linear model.  You can 
> get that by using summary() on a linear model.  For example:
>
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> group <- gl(2,10,20, labels=c("Ctl","Trt"))
> weight <- c(ctl, trt)
> lm.D9 <- lm(weight ~ group)
> summary(lm.D9)
> summary(lm.D9)$r.squared
> summary(lm.D9)$adj.r.squared

sure .. I was complete on the wrong way because I used the search
function :-(
used the summary more than one time ....


Regards Knut



From admin at biostatistic.de  Mon Jan 16 21:43:05 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Mon, 16 Jan 2006 21:43:05 +0100
Subject: [R] new comer's  question
In-Reply-To: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
References: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
Message-ID: <43CC0559.3080000@biostatistic.de>



Chang Shen schrieb:

>
>I have a csv file looks like this: (between two ==== lines)
>===========================
>
>
>
>
>First I load it by read.table call:
>
>myArray <- read.table("c:/mydata.csv",sep=",");
>
>  
>
did you try read.csv("c:/mydata.csv",sep=",")

Regards Knut



From dimitrijoe at gmail.com  Mon Jan 16 22:29:43 2006
From: dimitrijoe at gmail.com (Dimitri Joe)
Date: Mon, 16 Jan 2006 19:29:43 -0200
Subject: [R] importing from Stata
Message-ID: <43CC1047.5020305@ipea.gov.br>

Hi,

I have a new job, and everyone here uses Stata. I won't give up on R, 
but I must learn better how to exchange data between the two softwares. 
I am now focusing on importing data from Stata to R, and I must confess 
that I am a bit disappointed with the read.dta function from the foreign 
package because IT typically happens that

(i) I get a big R file (for example, a 15Mb Stata file became a 42Mb R 
file; after cleanup.import() from the Hmisc package, it drooped to 35Mb, 
but that's still more than 2x the original Stata file) which, in turn, I 
suspect is due the fact that

(ii) factors are created using Stata labels as levels.

I wonder if

(i) there isn't a way of forcing each variable to be numeric or integer, 
maintaining it's original values (instead of "Stata labels" as "R 
levels"). Or,

(ii) some one has written another function/s to carry this task.

I'd appreciate any suggestions on how to import from Stata to R more 
efficiently.
Thanks in advance,

Dimitri



From dpowers at mail.la.utexas.edu  Mon Jan 16 22:39:57 2006
From: dpowers at mail.la.utexas.edu (Daniel A. Powers)
Date: Mon, 16 Jan 2006 15:39:57 -0600
Subject: [R] fitted values from lmer (lme4 0.98)
Message-ID: <43CC12AD.3080304@mail.la.utexas.edu>


-- R-List

Can someone tell me how to get fitted values etc. after fitting lmer?
for example, from lme, I can fit mod.1 <- lme(....) and get fitted values, coefficients, etc. in this way

mod.1$fitted[,1] or mod.1$fitted[,2] etc. 

It seems lmer uses "slots" that are unfamiliar to me.

Thanks.
Dan 
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Daniel A. Powers, Ph.D.  
Department of Sociology 
University of Texas at Austin                    
1 University Station A1700
Austin, TX  78712-0118
phone: 512-232-6335
fax:   512-471-1748
dpowers at mail.la.utexas.edu



From yen.lin.chia at intel.com  Mon Jan 16 22:53:44 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Mon, 16 Jan 2006 13:53:44 -0800
Subject: [R] singular convergence(7)?
Message-ID: <E305A4AFB7947540BC487567B5449BA80921BCB5@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/4b2244b2/attachment.pl

From pauljohn32 at gmail.com  Mon Jan 16 23:13:44 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 16 Jan 2006 16:13:44 -0600
Subject: [R] Current state of support for BUGS access for Linux users?
Message-ID: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>

Greetings:

I'm going to encourage some students to try Bayesian ideas for
hierarchical models.
I want to run the WinBUGS and R examples in Tony Lancaster's An
Introduction to Modern Bayesian Econometrics.  That features MS
Windows and "bugs" from R2WinBUGS.

Today, I want to ask how people are doing this in Linux? I have found
a plethora of possibilities, some of which are not quite ready, some
of which work only under MS Windows.  Right now I just want to know
"what actually works".

Here's where I stand now in Fedora Core 4 Linux.
1. OpenBUGS-2.1.1 runs in Linux.  I can run "linbugs" (the console
version similar to the old BUGS) and also I can run--under wine--the
newest version of "winbugs.exe" that is circulated with OpenBUGS.  As
far as I can tell, the graphical interface in wine/winbugs works in
almost all elements.  A few things seem not quite right in the GUI
(can't initialize more than one chain, difficult to specify variables
for monitoring), but it does work.

It is easier to install and work with OpenBUGS's version of
winbugs.exe than with Winbugs-1.4 because the Open version does not
have that annoying license registration and "winbugs.exe" is not
wrapped inside an installation script.   I'm a little confused about
WinBUGS versions because the BRugs documents
http://www.biostat.umn.edu/~brad/software/BRugs/BRugs_install.html
refer to WinBUGS-1.5, which refers to
http://www.biostat.umn.edu/~brad/software/BRugs/WinBUGS15.zip, which
can be downloaded without any of the registration steps, but WinBUGS15
is not mentioned in the WinBUGS site (where 1.4.1 appears to be the
newest).

Supposing I get the winbugs.exe question settled:

2. How to most dependably send jobs from R to "linbugs" or "winbugs.exe"?

The BRugs package is preferred?

For a long time, R2WinBUGS was Windows-only, but toward the end of
last fall I noticed that R2WinBUGS now does compile and install under
R in Linux.

however, its help still says:
SystemRequirements:   WinBUGS 1.4 on Windows

I'd appreciate any advice.

--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From ivowel at gmail.com  Mon Jan 16 23:34:51 2006
From: ivowel at gmail.com (ivo welch)
Date: Mon, 16 Jan 2006 17:34:51 -0500
Subject: [R] princomp() with missing values in panel data?
Message-ID: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/ae859748/attachment.pl

From rvaradhan at jhmi.edu  Mon Jan 16 23:36:14 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 16 Jan 2006 17:36:14 -0500
Subject: [R] Legends in xyplot
Message-ID: <001101c61aed$424b4bc0$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/3446ccdd/attachment.pl

From fhong at salk.edu  Mon Jan 16 23:29:37 2006
From: fhong at salk.edu (fhong@salk.edu)
Date: Mon, 16 Jan 2006 14:29:37 -0800 (PST)
Subject: [R] [BioC] How to analysis Y98 chips using RankProd package?
In-Reply-To: <409B366CEC0FC34F9D7C1D73AD756E8A01B8FFD9@MAILSERVER.biocentrum.dtu.dk
	>
References: <409B366CEC0FC34F9D7C1D73AD756E8A01B8FFD9@MAILSERVER.biocentrum.dtu.dk>
Message-ID: <2816.10.10.200.202.1137450577.squirrel@webmail3.salk.edu>


> Dear R and Bioconductor Helpers,
>
> I am using a package called RankProd under Bioconductor to analysis my
> Y98 (yeast) microarray data. I had no problem following the example in
> the vignette but got stocked when I tried to analyze my own data.
>
> When I tried to run the following command,
>
> RP.out <- RP(rrf.sub, rrf.cl.sub, gene.names = y98.gnames, rand = 123)
>
> I got the following response:
>
> Rank Product analysis for two-class case
>
> Starting 100 permutations...
> Computing pfp ..
> Error in "row.names<-.data.frame"(`*tmp*`, value = c("logical(0)",
> "logical(0)",  :
>         duplicate 'row.names' are not allowed
I would guess that your gene name y98.gnames has duplicates, meaning one
gene name appears more than once.  This is normal since genes might be
spotted more than once on one chip, I think I would change my program to
accomodate this. But for now, it would be fine if you can change your gene
name to avoid duplicates. I will let you know when after I checked my
program.

Fangxin



> What is the problem? Is there something wrong with my gene names file?
> How can I fix it?
>
> Thank you very much for your kind help.
>
>
> Best Regards,
>
> - Cleo
>
>
>
> Chia-Wen Cleo Chang
> PhD Student
> CMB, BioCentrum-DTU, Building 223, room 213
> Technical University of Denmark, DK-2800 Lyngby, Denmark
> Telephone: +45 4525 2997, Fax: +45 4588 4148
> E-mail: cwc at biocentrum.dtu.dk
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> Bioconductor mailing list
> Bioconductor at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/bioconductor
>
>


--------------------
Fangxin Hong  Ph.D.
Plant Biology Laboratory
The Salk Institute
10010 N. Torrey Pines Rd.
La Jolla, CA 92037
E-mail: fhong at salk.edu
(Phone): 858-453-4100 ext 1105



From kerryrekky at yahoo.com  Mon Jan 16 23:46:53 2006
From: kerryrekky at yahoo.com (Cunningham Kerry)
Date: Mon, 16 Jan 2006 14:46:53 -0800 (PST)
Subject: [R] looking for ENAR partner
Message-ID: <20060116224653.78762.qmail@web51807.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/1bab7c5a/attachment.pl

From liuwensui at gmail.com  Tue Jan 17 01:07:00 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 16 Jan 2006 19:07:00 -0500
Subject: [R] off topic: how is xlispstat used in the industry?
Message-ID: <1115a2b00601161607p60bda136w7103ce69aff43f1f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060116/aac2ee5a/attachment.pl

From flom at ndri.org  Tue Jan 17 01:21:21 2006
From: flom at ndri.org (Peter Flom)
Date: Mon, 16 Jan 2006 19:21:21 -0500
Subject: [R] Standardized beta-coefficients in regression
Message-ID: <43CBF232020000C900001F40@MAIL.NDRI.ORG>

On Mon, 16 Jan 2006, Dr. Walter H. Schreiber wrote:

> Hello list,
>
> I am used to give a lot of attention to the standardized regression
> coefficients, which in SPSS are listed automatically.

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk>  >>> replied

<<<
I do wonder why?  Most people I have encountered who do that are 
interpreting them in invalid ways.

and

Yes, but why do you want one?  (You don't need summary, just coef, in
the 
second line, and you also do not need an intercept.)  For a single 
regressor as here, just cor(ctl, trt).
>>>

One reason might be that the American Psychological Ass'n requres them
for it's regression tables.  As to why the APA requires them, I couldn't
say,
but require them they do.  


Peter



From ggrothendieck at gmail.com  Tue Jan 17 01:31:22 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 Jan 2006 19:31:22 -0500
Subject: [R] off topic: how is xlispstat used in the industry?
In-Reply-To: <1115a2b00601161607p60bda136w7103ce69aff43f1f@mail.gmail.com>
References: <1115a2b00601161607p60bda136w7103ce69aff43f1f@mail.gmail.com>
Message-ID: <971536df0601161631x7814ea4fmf173f2dbbe9d2341@mail.gmail.com>

On 1/16/06, Wensui Liu <liuwensui at gmail.com> wrote:
> I am sorry for this off-topic question.
>
> Just curious how xlispstat is used in the industry and what's it strengthen
> compared with other computing languages such as R or matlab?
>
> Thanks a lot.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fbuchins at wpahs.org  Tue Jan 17 02:23:47 2006
From: fbuchins at wpahs.org (Farrel Buchinsky)
Date: Mon, 16 Jan 2006 20:23:47 -0500
Subject: [R] Installing a package yet it will not work.
Message-ID: <6E520FA8688B6C4C91D24DF15FAEEA7B0FBC5789@wphnt1.wpahs.org>

I want R to read my Microsoft Access database or maybe even a Sybase
database. I installed RODBC or at least thought I did. Then I issued the
following command:

library(RODBC) 

And got

Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
        ReadItem: unknown type 241
In addition: Warning message:
package 'RODBC' was built under R version 2.3.0 
Error: package/namespace load failed for 'RODBC'

I am running R2.2.1 on Windows XP. Where does one begin to troubleshoot
this.

Farrel Buchinsky, MD --- Mobile (412) 779-1073
Pediatric Otolaryngologist
Allegheny General Hospital
Pittsburgh, PA 


**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From jfox at mcmaster.ca  Tue Jan 17 02:42:41 2006
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 16 Jan 2006 20:42:41 -0500
Subject: [R] importing from Stata
In-Reply-To: <43CC1047.5020305@ipea.gov.br>
Message-ID: <20060117014240.XTPT9608.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Dimitri,

I don't have a solution for your problem, but your comment about factor
levels isn't the source of the problem. Factors are stored as integer vector
with a "levels" attribute (try, e.g., unclassing the factor), so the level
names are not repeated.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitri Joe
> Sent: Monday, January 16, 2006 4:30 PM
> To: R-Help
> Subject: [R] importing from Stata
> 
> Hi,
> 
> I have a new job, and everyone here uses Stata. I won't give 
> up on R, but I must learn better how to exchange data between 
> the two softwares. 
> I am now focusing on importing data from Stata to R, and I 
> must confess that I am a bit disappointed with the read.dta 
> function from the foreign package because IT typically happens that
> 
> (i) I get a big R file (for example, a 15Mb Stata file became 
> a 42Mb R file; after cleanup.import() from the Hmisc package, 
> it drooped to 35Mb, but that's still more than 2x the 
> original Stata file) which, in turn, I suspect is due the fact that
> 
> (ii) factors are created using Stata labels as levels.
> 
> I wonder if
> 
> (i) there isn't a way of forcing each variable to be numeric 
> or integer, maintaining it's original values (instead of 
> "Stata labels" as "R levels"). Or,
> 
> (ii) some one has written another function/s to carry this task.
> 
> I'd appreciate any suggestions on how to import from Stata to 
> R more efficiently.
> Thanks in advance,
> 
> Dimitri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jan 17 02:47:20 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 Jan 2006 20:47:20 -0500
Subject: [R] Installing a package yet it will not work.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED70E@usctmx1106.merck.com>

Several people have reported problem with installed packages recently, and
several turned out to be caused by the use of the StatLib mirror site.
Please try installing from a different mirror site and see if that solves
the problem for you.

Andy

From: Farrel Buchinsky
> 
> I want R to read my Microsoft Access database or maybe even a Sybase
> database. I installed RODBC or at least thought I did. Then I 
> issued the
> following command:
> 
> library(RODBC) 
> 
> And got
> 
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
>         ReadItem: unknown type 241
> In addition: Warning message:
> package 'RODBC' was built under R version 2.3.0 
> Error: package/namespace load failed for 'RODBC'
> 
> I am running R2.2.1 on Windows XP. Where does one begin to 
> troubleshoot
> this.
> 
> Farrel Buchinsky, MD --- Mobile (412) 779-1073
> Pediatric Otolaryngologist
> Allegheny General Hospital
> Pittsburgh, PA 
> 
> 
> **********************************************************************
> This email and any files transmitted with it are 
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Tue Jan 17 04:57:18 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 16 Jan 2006 19:57:18 -0800
Subject: [R] first derivative of a time series
In-Reply-To: <1137150894.43c78bae59a4b@mymail.tcd.ie>
References: <1137150894.43c78bae59a4b@mymail.tcd.ie>
Message-ID: <43CC6B1E.3060704@pdf.com>

	  First, I have to say that R is great.  I'd never heard of "cyclone" 
before I read your post, but in only a couple of minutes, I was able to 
install it, try it out, and form a (perhaps erroneous) opinion thereof.

	  The function "coefFit" fits a polynomial of degree N-1 to a 
timeseries of degree N-1.  For most applications, this is a bad idea, 
because the higher order coefficents are largely noise -- and the 
greater the noise, the fewer sensible coefficients one can get.  In 
other words, the more noise, the more nonsense on will likely get from 
such a procedure.  As evidence, I present the following:

set.seed(1)
ts.values <- rnorm(11)

## first calculate coefficients to pass to coefDeriv
(ts.coef <- coefFit(ts.values))
 > (ts.coef <- coefFit(ts.values))
$coefs
  [1]   -0.8204684    2.1309962   39.9775982  -53.9342218 -254.8143434
  [6]  281.8953570  611.4776909 -476.4068559 -634.0601417  247.3838420
[11]  238.6823281           NA
<snip>

	  The absolute values of ts.coef$coefs increase monotonically up to the 
7th coefficient, beyond which the smallest coefficient in absolute value 
is over 200 -- estimated from 11 pseudo-random numbers drawn per N(0,1). 
  I think it is conservative to describe this as merely silly.

	  From what I know, "the first derivative of an original time series" 
is not anything that has a standard definition.  If you would like more 
help from this listserve, please tell us a bit more about the problem 
you are trying to solve, for which you think "the first derivative of an 
original time series" might be useful.

	  spencer graves
p.s.  The fact that you included an almost reprodicible example was 
critical in permitting me to reply.  Without that, the most constructive 
comment I might have been able to make might have been, "PLEASE do read 
the posting guide! 'www.R-project.org/posting-guide.html'."  With your 
future posts, it might help increase the utility and frequency of 
replies if you include a self-contained snippet of R code.  I almost 
gave up on my attempt to reply to your question, because I when I tried, 
'read.table("timeseries.1d")', I got, 'Error in file(file, "r") : unable 
to open connection'.  You might get better help if you make it easier 
for people to help you.

gj wrote:

> Hi,
> 
> I need to derive a time series that represents the first derivative of an
> original time series. The function coefDeriv in the cyclones package seemed to
> be the ticket, but I'm not sure if I am interpreting the output of the function
> correctly...or even using the function correctly.
> 
> This is a snipbit of what I've been trying:
> --------
> library(cyclones)
> 
> ## read in my 1-column of values, each line is a different time step
> ts.table <- read.table("timeseries.1d")
> ts.values <- ts.table[,1]
> 
> ## first calculate coefficients to pass to coefDeriv
> ts.coef <- coefFit(ts.values)
> 
> d.ts <- coefDeriv(ts.coef)
> -------
> 
> However, when I try to look at d.ts$y.deriv, assumning that this will plot the
> derivative I want, all the numbers are huge (on the order of e+02 to e+08) when
> my original time series ranged from 0 to 970. Am I going about this the wrong
> way? Or are there other tools that would lead me to estimating the derivative
> of a time series?
> 
> Thanks in advance for any help!
> g
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mikael.anderson at gmail.com  Tue Jan 17 05:32:47 2006
From: mikael.anderson at gmail.com (Mikael Anderson)
Date: Tue, 17 Jan 2006 15:32:47 +1100
Subject: [R] Dynamic load
Message-ID: <bdc992b40601162032q1a4b6882u1bb1922c63e9c75b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/0db59bb9/attachment.pl

From Augusto.Sanabria at ga.gov.au  Tue Jan 17 05:47:19 2006
From: Augusto.Sanabria at ga.gov.au (Augusto.Sanabria@ga.gov.au)
Date: Tue, 17 Jan 2006 15:47:19 +1100
Subject: [R] Calculation of daily max
Message-ID: <9707EBA615A57747A0668CECD4638A3009BBAE@mail.agso.gov.au>


Good day everyone.

I have a large dataset of 1 min wind speeds
covering 5 years.

How can I make an array of maximum daily values?

The vectors I have are: 'VDATE' with dates in format
'%Y-%m-%d' (like '1992-10-28') and 'WS' with wind speed data 
(same number of elements as VDATE).

I want an array with 2 columns: Max daily wind speed and
corresponding day.

Has anyone got an elegant way of doing that?
(My background is in C++ I would tend to use loops
but that is not very elegant in R, isn't it?)

Augusto



--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2609
Ph. (02) 6249-9155



From jacques.veslot at cirad.fr  Tue Jan 17 06:12:00 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 17 Jan 2006 09:12:00 +0400
Subject: [R] Problems of data processing
In-Reply-To: <43CB99FA.3000803@cict.fr>
References: <43CB7EEB.3040902@cict.fr> <43CB8C5A.8020603@cirad.fr>
	<43CB99FA.3000803@cict.fr>
Message-ID: <43CC7CA0.6000506@cirad.fr>

OK ! so try this:
merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T)


Florent Bonneu a crit :

> Indeed,
> X <- c(1,Na,2,3,3,3,6,6)
> Y <- c(1,Na,9,7,7,7,8,8)
>
> I want to obtain one line for each Num. It's not a problem if there 
> are several lines for the same place, because my identifier is Num. I 
> just want to get X and Y well-informed in an other line for the same 
> place. For example, "Num=2" is at the place "x1", like "Num=1", but we 
> don't have the coordinates X and Y for "Num=2".  Now, the same 
> coordinates are well-informed for "Num=1", so i want to retrieve this 
> coordinates in my line "Num=2" for my columns X and Y.
>
>
>
> Jacques VESLOT wrote:
>
>> something wrong in X and Y definitions... but this could work:
>>
>> do.call("rbind", lapply(split(toto, toto$Num),
>>    function(x) x[which.min(as.POSIXct(strptime(toto$Date, "%d/%m/%y 
>> %H:%M"))),]))
>>
>> i don't understand the second query; do you want to keep the first 
>> line when there are several lines for the same place ?
>>
>>
>> Florent Bonneu a crit :
>>
>>> I have two problems for the data processing of my large data base 
>>> (50000 rows). For example, a sample is as follows
>>>
>>> Num <- c(1,2,3,4,4,4,5,5)
>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", 
>>> "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
>>> Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
>>> X <- c(1,,2,3,3,3,6,6)
>>> Y <- c(1,,9,7,7,7,8,8)
>>>
>>> toto <- data.frame(Num,Date,Place,X,Y)
>>>
>>> The first problem is to keep one line for each Num with the 
>>> minimum date. I managed to do it with loops but i would like a 
>>> solution without using loops. It will be better for my large data base.
>>>
>>> The other one is to retrieve the coordinates ill-informed. For 
>>> example, for the same place x1, Num=2 doesn't have X and Y. But, 
>>> we have this information for Num=1.
>>>
>>> The example data base must be like this
>>>
>>> Num <- c(1,2,3,4,5)
>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 2:14", 
>>> "1/1/04 9:05")
>>> Place <- c("x1","x1","x3","x4","x5")
>>> X <- c(1,1,2,3,6)
>>> Y <- c(1,1,9,7,8)
>>>
>>> toto <- data.frame(Num,Date,Place,X,Y)
>>> Somebody know how to do ?
>>> Thanks.
>>>
>>> Florent Bonneu
>>> Laboratoire de Statistique et Probabilits
>>> bureau 148  bt. 1R2
>>> Universit Toulouse 3
>>> 118 route de Narbonne - 31062 Toulouse cedex 9
>>> bonneu at cict.fr <mailto:bonneu at cict.fr>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>  
>>>
>>
>>
>>
>



From ggrothendieck at gmail.com  Tue Jan 17 06:32:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 Jan 2006 00:32:02 -0500
Subject: [R] Calculation of daily max
In-Reply-To: <9707EBA615A57747A0668CECD4638A3009BBAE@mail.agso.gov.au>
References: <9707EBA615A57747A0668CECD4638A3009BBAE@mail.agso.gov.au>
Message-ID: <971536df0601162132k152321d4t862446801cd303ab@mail.gmail.com>

Assuming VDATE is a character vector this produces
a zoo time series object.

	library(zoo)
	z <- aggregate(zoo(WS), as.Date(VDATE), max)

coredata(z) and time(z) are the data vector of maximums and
corresponding times, respectively.

The R command:

               vignette("zoo")

gives an introduction to zoo.

Aside from zoo you could check out ?tapply, ?by and ?aggregate .



On 1/16/06, Augusto.Sanabria at ga.gov.au <Augusto.Sanabria at ga.gov.au> wrote:
>
> Good day everyone.
>
> I have a large dataset of 1 min wind speeds
> covering 5 years.
>
> How can I make an array of maximum daily values?
>
> The vectors I have are: 'VDATE' with dates in format
> '%Y-%m-%d' (like '1992-10-28') and 'WS' with wind speed data
> (same number of elements as VDATE).
>
> I want an array with 2 columns: Max daily wind speed and
> corresponding day.
>
> Has anyone got an elegant way of doing that?
> (My background is in C++ I would tend to use loops
> but that is not very elegant in R, isn't it?)
>
> Augusto
>
>
>
> --------------------------------------------
> Augusto Sanabria. MSc, PhD.
> Mathematical Modeller
> Risk Research Group
> Geospatial & Earth Monitoring Division
> Geoscience Australia (www.ga.gov.au)
> Cnr. Jerrabomberra Av. & Hindmarsh Dr.
> Symonston ACT 2609
> Ph. (02) 6249-9155
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Jan 17 06:50:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 05:50:09 +0000 (GMT)
Subject: [R] princomp() with missing values in panel data?
In-Reply-To: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>
References: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601170548040.17319@gannet.stats>

See ?na.exclude (on the same page as na.omit)

On Mon, 16 Jan 2006, ivo welch wrote:

> dear R wizards:  the good news is that I know how to omit missing
> observations and run a principal components analysis.
>
>  p= princomp( na.omit( dataset ) )
>  p$scores[ ,1]  # the first factor
>
> (where dataset contains missing values;  incidentally, princomp(retailsmall,
> na.action=na.omit) does not work for me, so I must be doing something wrong,
> here.)

See ?princomp: only the formula method has an na.action argument.

> the bad news is that I would like NA observations to be retained as
> NA, so that I can reinsert the factors into the data set:
>  dataset$first.factor = p$scores[,1]
> there must be an elegant way of doing this.  help appreciated.
>
> may I humbly suggest that in linear models, it would be intuitive if the
> default would be for NA's to be ignored in the model computations, and that
> the functions residuals and fitted (and similar, such as scores() ) to
> understand when a particular obs num should be NA?

There is no function scores().

> help, as always, appreciated.
>
> sincerely,
>
> /ivo welch
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From whschreiber at onlinehome.de  Tue Jan 17 07:03:23 2006
From: whschreiber at onlinehome.de (Dr. Walter H. Schreiber)
Date: Tue, 17 Jan 2006 07:03:23 +0100
Subject: [R] Standardized beta-coefficients in regression
In-Reply-To: <43CBF232020000C900001F40@MAIL.NDRI.ORG>
References: <43CBF232020000C900001F40@MAIL.NDRI.ORG>
Message-ID: <200601170703.24319.whschreiber@onlinehome.de>

I had intended not to answer since I had a question regarding R and not about 
justification of a specific statistic. But nonetheless:

(a) standardized betas are liberated of the original scale and facilitates 
reading off the relative influcence of a variable on the hyperplane. I find 
it easy to think of deviations in SD.

(b) If you do model building and follow up un the change of betas you soon 
relize that they are fine indicators and give hints about multicollinearity 
(yes, I know about vif(), although I prefer 1/vif(); I also know that I can 
check for intercorrrelations in advance).

(c) as Peter mentioned, APA requires it.

(d) to assist converts from Sxx[x] who might miss something.

Cheers,

      Walter.

Am Dienstag, 17. Januar 2006 01:21 schrieb Peter Flom:
> On Mon, 16 Jan 2006, Dr. Walter H. Schreiber wrote:
> > Hello list,
> >
> > I am used to give a lot of attention to the standardized regression
> > coefficients, which in SPSS are listed automatically.
> >
> >>> Prof Brian Ripley <ripley at stats.ox.ac.uk>  >>> replied
>
> <<<
> I do wonder why?  Most people I have encountered who do that are
> interpreting them in invalid ways.
>
> and
>
> Yes, but why do you want one?  (You don't need summary, just coef, in
> the
> second line, and you also do not need an intercept.)  For a single
> regressor as here, just cor(ctl, trt).
>
>
> One reason might be that the American Psychological Ass'n requres them
> for it's regression tables.  As to why the APA requires them, I couldn't
> say,
> but require them they do.
>
>
> Peter

-- 
Dr. Walter H. Schreiber



From ligges at statistik.uni-dortmund.de  Tue Jan 17 08:50:05 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 Jan 2006 08:50:05 +0100
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>
References: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>
Message-ID: <43CCA1AD.6070805@statistik.uni-dortmund.de>

Paul Johnson wrote:
> Greetings:
> 
> I'm going to encourage some students to try Bayesian ideas for
> hierarchical models.
> I want to run the WinBUGS and R examples in Tony Lancaster's An
> Introduction to Modern Bayesian Econometrics.  That features MS
> Windows and "bugs" from R2WinBUGS.
> 
> Today, I want to ask how people are doing this in Linux? I have found
> a plethora of possibilities, some of which are not quite ready, some
> of which work only under MS Windows.  Right now I just want to know
> "what actually works".
> 
> Here's where I stand now in Fedora Core 4 Linux.
> 1. OpenBUGS-2.1.1 runs in Linux.  I can run "linbugs" (the console
> version similar to the old BUGS) and also I can run--under wine--the
> newest version of "winbugs.exe" that is circulated with OpenBUGS.  As
> far as I can tell, the graphical interface in wine/winbugs works in
> almost all elements.  A few things seem not quite right in the GUI
> (can't initialize more than one chain, difficult to specify variables
> for monitoring), but it does work.
> 
> It is easier to install and work with OpenBUGS's version of
> winbugs.exe than with Winbugs-1.4 because the Open version does not
> have that annoying license registration and "winbugs.exe" is not
> wrapped inside an installation script.   I'm a little confused about
> WinBUGS versions because the BRugs documents
> http://www.biostat.umn.edu/~brad/software/BRugs/BRugs_install.html
> refer to WinBUGS-1.5, which refers to
> http://www.biostat.umn.edu/~brad/software/BRugs/WinBUGS15.zip, which
> can be downloaded without any of the registration steps, but WinBUGS15
> is not mentioned in the WinBUGS site (where 1.4.1 appears to be the
> newest).
> 
> Supposing I get the winbugs.exe question settled:
> 
> 2. How to most dependably send jobs from R to "linbugs" or "winbugs.exe"?
> 
> The BRugs package is preferred?
> 
> For a long time, R2WinBUGS was Windows-only, but toward the end of
> last fall I noticed that R2WinBUGS now does compile and install under
> R in Linux.
> 
> however, its help still says:
> SystemRequirements:   WinBUGS 1.4 on Windows
> 
> I'd appreciate any advice.

[resend to less recipients in order to save Martin's spare time to 
approve message;
CCing Andrew Thomas, Bob O'Hara and Sibylle Sturtz separately]




Re BUGS:
WinBUGS-1.5 never got really released, AFAIK - Andrew or Bob might want
to correct me. It has been renamed to OpenBUGS. The current version is
the GPL'ed OpenBUGS 2.1.1 available from
http://mathstat.helsinki.fi/openbugs/.

Re R packages:
- R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
can speak with WinBUGS under wine thanks to user contributions. But it
still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
Windows).
- BRugs contains the BRugs interface, R functions and the whole OpenBUGS
installation. Unfortunately, due to serious compiler problems, we were
not able to get a Linux version running using the interface. Hence it
was not possible to release any non-Windows version up to now.
I haven't tested BRugs under wine yet (in which case R has to run under
wine as well, of course) ... and I do not know if there are any serious
performance penalties.
Note that even in the long term, OpenBUGS will only run on x86 based
platforms.

Due to the much more flexibile interface, I prefer BRugs.

BTW: "Real programmers" won't consider R2WinBUGS to be an "interface" at
all - it might be useful, though. ;-)


Uwe Ligges



> --
> Paul E. Johnson
> Professor, Political Science
> 1541 Lilac Lane, Room 504
> University of Kansas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jan 17 08:58:08 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 Jan 2006 08:58:08 +0100
Subject: [R] Dynamic load
In-Reply-To: <bdc992b40601162032q1a4b6882u1bb1922c63e9c75b@mail.gmail.com>
References: <bdc992b40601162032q1a4b6882u1bb1922c63e9c75b@mail.gmail.com>
Message-ID: <43CCA390.1020602@statistik.uni-dortmund.de>

Mikael Anderson wrote:

> Hi,
> 
> I am trying to load a FORTRAN program which I have downloaded from netlib.
> Counting the number of dependencies, there are about 10 programs which have
> to be loaded. This is under SunOS 5.9 and R 2.2.1.  I have compiled the
> files with R221 CMD SHLIB *.f.
> 
> If I load these object files one by one with dyn.load()  I get an error like
> _j4save:reference symbol not found.


Why not compile them into one shared library?


> I have used S-Plus in the past 15 years and what I remember is that  in the
> old versions of S-Plus (3.4 maybe?) one could create  one single object file
> under UNIX and load them with dyn.load but I don't have access to those old
> manuals at the moment.  I am new to R, so I wonder what is the best way of

It might surprise many R-help posters, but R has manuals as well...

Uwe Ligges


> loading a FORTRAN file with all its dependencies to R.
> 
> /Mikael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ozric at web.de  Tue Jan 17 09:03:31 2006
From: ozric at web.de (Christian Schulz)
Date: Tue, 17 Jan 2006 09:03:31 +0100
Subject: [R] new comer's  question
In-Reply-To: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
References: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
Message-ID: <43CCA4D3.9020506@web.de>


>what is wrong?
>
>
>  
>
Your data is ihmo  not correct imported

myArray <- read.table("c:/mydata.csv",sep=",",na.strings="")
...and it seems you have using quotes in your csv file so you should 
play with the
additional parameter quotes in read.table.

Type ?read.table....

Then check your data i.e. with edit(myArray) to look that columns 
imported well.

regards, christian





>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ozric at web.de  Tue Jan 17 09:03:31 2006
From: ozric at web.de (Christian Schulz)
Date: Tue, 17 Jan 2006 09:03:31 +0100
Subject: [R] new comer's  question
In-Reply-To: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
References: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
Message-ID: <43CCA4D3.9020506@web.de>


>what is wrong?
>
>
>  
>
Your data is ihmo  not correct imported

myArray <- read.table("c:/mydata.csv",sep=",",na.strings="")
...and it seems you have using quotes in your csv file so you should 
play with the
additional parameter quotes in read.table.

Type ?read.table....

Then check your data i.e. with edit(myArray) to look that columns 
imported well.

regards, christian





>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ligges at statistik.uni-dortmund.de  Tue Jan 17 09:08:07 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 Jan 2006 09:08:07 +0100
Subject: [R] Legends in xyplot
In-Reply-To: <001101c61aed$424b4bc0$7c94100a@win.ad.jhu.edu>
References: <001101c61aed$424b4bc0$7c94100a@win.ad.jhu.edu>
Message-ID: <43CCA5E7.9040406@statistik.uni-dortmund.de>

See ?xyplot and its argument "legend".

Uwe Ligges




Ravi Varadhan wrote:

> Hi,
> 
>  
> 
> How can I add legends in the "xyplot" function, in the "lattice" library?
> Here is a simulation example:
> 
>  
> 
> x <- runif(90)
> 
> z <- sample(1:3, 90, rep=T)
> 
> y <- rnorm(90, mean = x^2 + z, sd=1)
> 
>  
> 
> library(lattice)
> 
> trellis.par.set(col.whitebg())
> 
> xyplot(y ~x, groups=as.factor(z), type = c('p', 'smooth'), span = .67)
> 
>  
> 
> Thanks in advance,
> 
> Ravi.
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From EISM at statoil.com  Tue Jan 17 10:45:46 2006
From: EISM at statoil.com (=?iso-8859-1?Q?Eivind_Sm=F8rgrav?=)
Date: Tue, 17 Jan 2006 10:45:46 +0100
Subject: [R] Kriging for d>3
Message-ID: <8AEE42D73E439C4AA476A3A66BC7BD8021FE65@ST-EXCL06.statoil.net>

Hi,

I'm looking for software that can perform kriging on systems with dimensionality higher than 3, say d=5. 
Are anyone aware of packages in R that can do this?

Thanks,

Eivind Sm??rgrav


-------------------------------------------------------------------
The information contained in this message may be CONFIDENTIAL and is
intended for the addressee only. Any unauthorised use, dissemination of the
information or copying of this message is prohibited. If you are not the
addressee, please notify the sender immediately by return e-mail and delete
this message.
Thank you.



From lasarczyk at gmail.com  Tue Jan 17 11:04:49 2006
From: lasarczyk at gmail.com (Christian Lasarczyk)
Date: Tue, 17 Jan 2006 11:04:49 +0100
Subject: [R] Kriging for d>3
In-Reply-To: <8AEE42D73E439C4AA476A3A66BC7BD8021FE65@ST-EXCL06.statoil.net>
References: <8AEE42D73E439C4AA476A3A66BC7BD8021FE65@ST-EXCL06.statoil.net>
Message-ID: <926ba8890601170204t3138245ds@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/fe968840/attachment.pl

From r.hankin at noc.soton.ac.uk  Tue Jan 17 11:06:38 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 17 Jan 2006 10:06:38 +0000
Subject: [R] Kriging for d>3
In-Reply-To: <8AEE42D73E439C4AA476A3A66BC7BD8021FE65@ST-EXCL06.statoil.net>
References: <8AEE42D73E439C4AA476A3A66BC7BD8021FE65@ST-EXCL06.statoil.net>
Message-ID: <D95839A4-4966-477B-A63D-EDD092B6B39E@soc.soton.ac.uk>

Hi

the BACCO bundle includes package "emulator", which is effectively
kriging in arbitrary dimensions.


best wishes


Robin





On 17 Jan 2006, at 09:45, Eivind Sm??rgrav wrote:

> Hi,
>
> I'm looking for software that can perform kriging on systems with  
> dimensionality higher than 3, say d=5.
> Are anyone aware of packages in R that can do this?
>
> Thanks,
>
> Eivind Sm??rgrav
>
>
> -------------------------------------------------------------------
> The information contained in this message may be CONFIDENTIAL and is
> intended for the addressee only. Any unauthorised use,  
> dissemination of the
> information or copying of this message is prohibited. If you are  
> not the
> addressee, please notify the sender immediately by return e-mail  
> and delete
> this message.
> Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From dieter.menne at menne-biomed.de  Tue Jan 17 11:08:41 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 17 Jan 2006 10:08:41 +0000 (UTC)
Subject: [R] lme output
References: <Pine.LNX.4.10.10601161655220.229-100000@mercury.quantex>
Message-ID: <loom.20060117T110726-648@post.gmane.org>

John Logsdon <j.logsdon <at> quantex-research.com> writes:

> 
> I am trying to extract the solution from a simple lme calculation.
> 
> > y<-c(-1.118,-.5,.5,1.118,10)
> > gp<-factor(c(rep('one',4),'two'))
> > res<-lme(y~1,rand=~1|gp) 
...
> Random effects:
>  Formula: ~1 | gp
>         (Intercept)  Residual
> StdDev:    7.026737 0.9999747
> 
> The problem is to get the random effects intercept - essentially the
> between groups standard deviation - into a scalar.  

VarCorr(res) 

will give you the matrix with the relevant numbers



From bonneu at cict.fr  Tue Jan 17 11:59:58 2006
From: bonneu at cict.fr (Florent Bonneu)
Date: Tue, 17 Jan 2006 11:59:58 +0100
Subject: [R] Problems of data processing
In-Reply-To: <43CC7CA0.6000506@cirad.fr>
References: <43CB7EEB.3040902@cict.fr> <43CB8C5A.8020603@cirad.fr>
	<43CB99FA.3000803@cict.fr> <43CC7CA0.6000506@cirad.fr>
Message-ID: <43CCCE2E.7040100@cict.fr>

Thank you very much for your help but I think there is an error for the 
answer to the first problem  I spent time on searching the solution but 
I failed to find it. I tried to put "which.max" instead of "which.min" 
but it doesn't work. I tried to do my best but i didn't have any idea to 
solve this problem.

An example :

Num <- c(1,2,4,3,4,4,5,5,5)
Date <- c("1/1/04 0:48","1/1/04 8:02", "1/1/04 1:55", "1/1/04 2:14", "1/1/04 1:19", "1/1/04 1:02", "1/1/04 11:15", "1/1/04 9:06", "1/1/04 10:32")
Place <- c("x1","x1","x4","x3","x4","x4","x5","x5","x5")
X <- c(1,NA,3,2,3,3,6,6,6)
Y <- c(1,NA,7,9,7,7,8,8,8)
toto <- data.frame(Num,Date,Place,X,Y)
toto[order(toto$Num,as.numeric(as.POSIXct(strptime(toto$Date, "%d/%m/%y %H:%M")))),] 

toto <- merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T) 

help <- do.call("rbind", lapply(split(toto, toto$Num),
   function(x) x[which.min(as.numeric(as.POSIXct(strptime(toto$Date, "%d/%m/%y %H:%M")))),]))
help

The solution must be

Num <- c(1,2,3,4,5)
Date <- c("1/1/04 0:48","1/1/04 8:02", "1/1/04 2:14", "1/1/04 1:02", "1/1/04 9:06")
Place <- c("x1","x1","x3","x4","x5")
X <- c(1,1,2,3,6)
Y <- c(1,1,9,7,8)
toto <- data.frame(Num,Date,Place,X,Y)


Any suggestion is welcome.

Florent Bonneu.



Jacques VESLOT wrote:

> OK ! so try this:
> merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T)
>
>
> Florent Bonneu a crit :
>
>> Indeed,
>> X <- c(1,Na,2,3,3,3,6,6)
>> Y <- c(1,Na,9,7,7,7,8,8)
>>
>> I want to obtain one line for each Num. It's not a problem if there 
>> are several lines for the same place, because my identifier is Num. I 
>> just want to get X and Y well-informed in an other line for the same 
>> place. For example, "Num=2" is at the place "x1", like "Num=1", but 
>> we don't have the coordinates X and Y for "Num=2".  Now, the same 
>> coordinates are well-informed for "Num=1", so i want to retrieve this 
>> coordinates in my line "Num=2" for my columns X and Y.
>>
>>
>>
>> Jacques VESLOT wrote:
>>
>>> something wrong in X and Y definitions... but this could work:
>>>
>>> do.call("rbind", lapply(split(toto, toto$Num),
>>>    function(x) x[which.min(as.POSIXct(strptime(toto$Date, "%d/%m/%y 
>>> %H:%M"))),]))
>>>
>>> i don't understand the second query; do you want to keep the first 
>>> line when there are several lines for the same place ?
>>>
>>>
>>> Florent Bonneu a crit :
>>>
>>>> I have two problems for the data processing of my large data base 
>>>> (50000 rows). For example, a sample is as follows
>>>>
>>>> Num <- c(1,2,3,4,4,4,5,5)
>>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 
>>>> 2:14", "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
>>>> Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
>>>> X <- c(1,,2,3,3,3,6,6)
>>>> Y <- c(1,,9,7,7,7,8,8)
>>>>
>>>> toto <- data.frame(Num,Date,Place,X,Y)
>>>>
>>>> The first problem is to keep one line for each Num with the 
>>>> minimum date. I managed to do it with loops but i would like a 
>>>> solution without using loops. It will be better for my large data 
>>>> base.
>>>>
>>>> The other one is to retrieve the coordinates ill-informed. For 
>>>> example, for the same place x1, Num=2 doesn't have X and Y. But, 
>>>> we have this information for Num=1.
>>>>
>>>> The example data base must be like this
>>>>
>>>> Num <- c(1,2,3,4,5)
>>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 
>>>> 2:14", "1/1/04 9:05")
>>>> Place <- c("x1","x1","x3","x4","x5")
>>>> X <- c(1,1,2,3,6)
>>>> Y <- c(1,1,9,7,8)
>>>>
>>>> toto <- data.frame(Num,Date,Place,X,Y)
>>>> Somebody know how to do ?
>>>> Thanks.
>>>>
>>>> Florent Bonneu
>>>> Laboratoire de Statistique et Probabilits
>>>> bureau 148  bt. 1R2
>>>> Universit Toulouse 3
>>>> 118 route de Narbonne - 31062 Toulouse cedex 9
>>>> bonneu at cict.fr <mailto:bonneu at cict.fr>
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>  
>>>>
>>>
>>>
>>>
>>
>
>
>



From hilmar.berger at imise.uni-leipzig.de  Tue Jan 17 12:05:43 2006
From: hilmar.berger at imise.uni-leipzig.de (Hilmar Berger)
Date: Tue, 17 Jan 2006 12:05:43 +0100
Subject: [R] Font size of axis labels
Message-ID: <dqij25$nu5$1@sea.gmane.org>

Hi all,

In R, it is not possible to set the font size of axis labels directly 
(AFAIK). Instead, scaling factors for the font chosen by the graphics 
device can be supplied. It appears that there is no constant font size 
for axis labels. My impression is that the axis label font size is 
scaled internally by R depending on the number of labels given for an axis.

In addition to the R-internal scaling  I need to adjust the axis label 
font size for label length and the size of other elements in the same 
layout.

In order to use sensitive values for those externally supplied scaling 
factors (cex) I would like to know how R scales axis font sizes internally.
Could someone point me to the method/file in the R-source code (R 2.1.1) 
where this scaling is done ?

Thanks,
Hilmar
-- 

Hilmar Berger
Studienkoordinator
Institut f??r medizinische Informatik, Statistik und Epidemiologie
Universit??t Leipzig
H??rtelstr. 16-18
D-04107 Leipzig

Tel. +49 341 97 16 101
Fax. +49 341 97 16 109
email: hilmar.berger at imise.uni-leipzig.de



From farewelld at Cardiff.ac.uk  Tue Jan 17 12:17:21 2006
From: farewelld at Cardiff.ac.uk (Daniel Farewell)
Date: Tue, 17 Jan 2006 11:17:21 +0000
Subject: [R] multiple GLMs with lmList in lme4
Message-ID: <s3ccd249.061@ZGRW21.uwcm.ac.uk>

I'd like to fit a GLM to each of a number of subsets of some data. The `family' argument to `lmList' (in lme4) has given me cause for optimism, but so far I've only been able to achieve linear model fits. For example

> df <- data.frame(gp = gp.temp <- factor(rep(1:3, each = 100)),
x = x.temp <- rnorm(300),
y = rpois(300, exp((-1:1)[gp.temp] + x.temp)))

Then a call to `glm' on the group 1 subset gives

> glm(y ~ x, family = poisson, data = df, subset = gp == 1)

Call:  glm(formula = y ~ x, family = poisson, data = df, subset = gp == 1) 

Coefficients:
(Intercept)            x  
    -1.0143       0.9726  

Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
Null Deviance:      138.5 
Residual Deviance: 82.76        AIC: 178.5

(the right answer) but `lmList' gives

> show(lmList(y ~ x | gp, family = poisson, data = df))
Call: lmList(formula = y ~ x | gp, data = df, family = poisson) 
Coefficients:
  (Intercept)         x
1   0.5560377 0.6362124
2   1.8431794 1.8541193
3   4.5773106 4.7871929

Degrees of freedom: 300 total; 294 residual
Residual standard error: 2.655714

which come from linear model fits, e.g.

> lm(y ~ x, data = df, subset = gp == 1)

Call:
lm(formula = y ~ x, data = df, subset = gp == 1)

Coefficients:
(Intercept)            x  
     0.5560       0.6362

Any suggestions as to why lmList matches the linear fits rather than the GLM fits would be greatly appreciated. I'm using R2.2.1 with lme version 0.98-1 in Windows XP.

Daniel Farewell
Cardiff University



From jacques.veslot at cirad.fr  Tue Jan 17 12:28:15 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 17 Jan 2006 15:28:15 +0400
Subject: [R] Problems of data processing
In-Reply-To: <43CCCE2E.7040100@cict.fr>
References: <43CB7EEB.3040902@cict.fr> <43CB8C5A.8020603@cirad.fr>
	<43CB99FA.3000803@cict.fr> <43CC7CA0.6000506@cirad.fr>
	<43CCCE2E.7040100@cict.fr>
Message-ID: <43CCD4CF.7050501@cirad.fr>

sorry, i let toto$Date in the function within lapply() instead of x$Date !

now, it works :

 > toto
  Num         Date Place  X  Y
1   1  1/1/04 0:48    x1  1  1
2   2  1/1/04 8:02    x1 NA NA
3   4  1/1/04 1:55    x4  3  7
4   3  1/1/04 2:14    x3  2  9
5   4  1/1/04 1:19    x4  3  7
6   4  1/1/04 1:02    x4  3  7
7   5 1/1/04 11:15    x5  6  8
8   5  1/1/04 9:06    x5  6  8
9   5 1/1/04 10:32    x5  6  8

 > toto <- do.call("rbind", lapply(split(toto, toto$Num),
+    function(x) x[which.min(as.POSIXct(strptime(x$Date, "%d/%m/%y 
%H:%M"))),]))

 > toto
  Num        Date Place  X  Y
1   1 1/1/04 0:48    x1  1  1
2   2 1/1/04 8:02    x1 NA NA
3   3 1/1/04 2:14    x3  2  9
4   4 1/1/04 1:02    x4  3  7
5   5 1/1/04 9:06    x5  6  8

 > toto <- merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T)
 > toto
  Place Num        Date X Y
1    x1   1 1/1/04 0:48 1 1
2    x1   2 1/1/04 8:02 1 1
3    x3   3 1/1/04 2:14 2 9
4    x4   4 1/1/04 1:02 3 7
5    x5   5 1/1/04 9:06 6 8



Florent Bonneu a crit :

> Thank you very much for your help but I think there is an error for 
> the answer to the first problem  I spent time on searching the 
> solution but I failed to find it. I tried to put "which.max" instead 
> of "which.min" but it doesn't work. I tried to do my best but i didn't 
> have any idea to solve this problem.
>
> An example :
>
> Num <- c(1,2,4,3,4,4,5,5,5)
> Date <- c("1/1/04 0:48","1/1/04 8:02", "1/1/04 1:55", "1/1/04 2:14", 
> "1/1/04 1:19", "1/1/04 1:02", "1/1/04 11:15", "1/1/04 9:06", "1/1/04 
> 10:32")
> Place <- c("x1","x1","x4","x3","x4","x4","x5","x5","x5")
> X <- c(1,NA,3,2,3,3,6,6,6)
> Y <- c(1,NA,7,9,7,7,8,8,8)
> toto <- data.frame(Num,Date,Place,X,Y)
> toto[order(toto$Num,as.numeric(as.POSIXct(strptime(toto$Date, 
> "%d/%m/%y %H:%M")))),]
> toto <- merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T)
> help <- do.call("rbind", lapply(split(toto, toto$Num),
>   function(x) x[which.min(as.numeric(as.POSIXct(strptime(toto$Date, 
> "%d/%m/%y %H:%M")))),]))
> help
>
> The solution must be
>
> Num <- c(1,2,3,4,5)
> Date <- c("1/1/04 0:48","1/1/04 8:02", "1/1/04 2:14", "1/1/04 1:02", 
> "1/1/04 9:06")
> Place <- c("x1","x1","x3","x4","x5")
> X <- c(1,1,2,3,6)
> Y <- c(1,1,9,7,8)
> toto <- data.frame(Num,Date,Place,X,Y)
>
>
> Any suggestion is welcome.
>
> Florent Bonneu.
>
>
>
> Jacques VESLOT wrote:
>
>> OK ! so try this:
>> merge(toto[1:3], unique(na.omit(toto[3:5])),by="Place",all.x=T)
>>
>>
>> Florent Bonneu a crit :
>>
>>> Indeed,
>>> X <- c(1,Na,2,3,3,3,6,6)
>>> Y <- c(1,Na,9,7,7,7,8,8)
>>>
>>> I want to obtain one line for each Num. It's not a problem if there 
>>> are several lines for the same place, because my identifier is Num. 
>>> I just want to get X and Y well-informed in an other line for the 
>>> same place. For example, "Num=2" is at the place "x1", like "Num=1", 
>>> but we don't have the coordinates X and Y for "Num=2".  Now, the 
>>> same coordinates are well-informed for "Num=1", so i want to 
>>> retrieve this coordinates in my line "Num=2" for my columns X and Y.
>>>
>>>
>>>
>>> Jacques VESLOT wrote:
>>>
>>>> something wrong in X and Y definitions... but this could work:
>>>>
>>>> do.call("rbind", lapply(split(toto, toto$Num),
>>>>    function(x) x[which.min(as.POSIXct(strptime(toto$Date, "%d/%m/%y 
>>>> %H:%M"))),]))
>>>>
>>>> i don't understand the second query; do you want to keep the first 
>>>> line when there are several lines for the same place ?
>>>>
>>>>
>>>> Florent Bonneu a crit :
>>>>
>>>>> I have two problems for the data processing of my large data base 
>>>>> (50000 rows). For example, a sample is as follows
>>>>>
>>>>> Num <- c(1,2,3,4,4,4,5,5)
>>>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 
>>>>> 2:14", "1/1/04 3:09", "1/1/04 8:02", "1/1/04 9:05", "1/1/04 9:06")
>>>>> Place <- c("x1","x1","x3","x4","x4","x4","x5","x5")
>>>>> X <- c(1,,2,3,3,3,6,6)
>>>>> Y <- c(1,,9,7,7,7,8,8)
>>>>>
>>>>> toto <- data.frame(Num,Date,Place,X,Y)
>>>>>
>>>>> The first problem is to keep one line for each Num with the 
>>>>> minimum date. I managed to do it with loops but i would like a 
>>>>> solution without using loops. It will be better for my large data 
>>>>> base.
>>>>>
>>>>> The other one is to retrieve the coordinates ill-informed. For 
>>>>> example, for the same place x1, Num=2 doesn't have X and Y. But, 
>>>>> we have this information for Num=1.
>>>>>
>>>>> The example data base must be like this
>>>>>
>>>>> Num <- c(1,2,3,4,5)
>>>>> Date <- c("1/1/04 0:48","1/1/04 1:52", "1/1/04 1:55", "1/1/04 
>>>>> 2:14", "1/1/04 9:05")
>>>>> Place <- c("x1","x1","x3","x4","x5")
>>>>> X <- c(1,1,2,3,6)
>>>>> Y <- c(1,1,9,7,8)
>>>>>
>>>>> toto <- data.frame(Num,Date,Place,X,Y)
>>>>> Somebody know how to do ?
>>>>> Thanks.
>>>>>
>>>>> Florent Bonneu
>>>>> Laboratoire de Statistique et Probabilits
>>>>> bureau 148  bt. 1R2
>>>>> Universit Toulouse 3
>>>>> 118 route de Narbonne - 31062 Toulouse cedex 9
>>>>> bonneu at cict.fr <mailto:bonneu at cict.fr>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! 
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>>  
>>>>>
>>>>
>>>>
>>>>
>>>
>>
>>
>>
>



From modono at utvinternet.com  Tue Jan 17 12:45:34 2006
From: modono at utvinternet.com (modono@utvinternet.com)
Date: Tue, 17 Jan 2006 11:45:34 GMT
Subject: [R] Perl array conversion in R to compare plots.
Message-ID: <43ccd8de.610.0@utvinternet.com>

Hello all,
I am using RSPerl package to display plots in R . I want to be able to compare
some data in boxplot and in histogram and in probplot ( from e1071 package)
- ( cumulative probability plot - normal) 

I have a nested arrray of data ... from perl like the following
data[0][0] = (1,2,3,.....) 
data[0][1] = ( 2,1,2,2,2,2,2)  - could be different lengths...
etc ( could be more two sets to compare...)
These get passed to the R engine as follows

x <- list(list(1,2,3,4),list(2,1,2,2,2,2,2))

I am trying to write a function that will perform boxplot on all this data but
I am not sure how to extract data properly and display


so far my function looks like 
compareplot <- function(x,xlab=par("xlab"))
{
     new.plot()
    for ( i in 1:length(x)) 
    {   
       # I think I have to do some assign and paste here"???
      xlist[i] <- unlist(x[[i]])
      args <- paste(args,xlist[i])
     }  
so I want my args to look something like xlist1,xlist2,.... 
boxplot(args)
hist(args) ...
probplot(args)
or should I do some data.frame conversion on all of the lists... and how would
I go about doing a comparison of data on probplot ( cumulative probability plot
) or histogram compare?   

Thanks for any help,
Mary



From blindglobe at gmail.com  Tue Jan 17 13:08:06 2006
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 17 Jan 2006 13:08:06 +0100
Subject: [R] xlispstat and R
Message-ID: <1abe3fa90601170408o25696dk187f6ee48c1303bf@mail.gmail.com>

> From: Wensui Liu <liuwensui at gmail.com>


> Just curious how xlispstat is used in the industry and what's it strengthen
> compared with other computing languages such as R or matlab?

Almost not at all, though there are a few holdouts.

On a related note, I've been doing some interesting things with a
branch of LispStat for CommonLisp.  It'll be more interesting when R
gets embedded (now it's back on topic).  Rumor has it that R is
embedded within SBCL, which makes for an interesting distributed
computing environment.

It's a nice system, it still works, it's got some reasonable (though
old) tools, numerically it needs a good bit of work and updating, but
for a 17-year old program, it works quite nicely.  Dynamic graphics
are comparable to GGobi's (more flexible, but less scalable).  Plus,
it truly works cross platform.

CommonLispStat is fast (SBCL and CMUCL are compiled, not interpreted,
commonlisps), but needs a bit of work with numerics (SBCL isn't
stable), and Graphics (CLISP doesn't quite like it yet;  though tk and
gtk2 supported), and decisions about object systems (the old
prototypes vs. CLOS) are still open, both being available at this
point.

Also back on topic, Duncan T-L had a nice embedding of R/XLispStat
that worked nicely, but XLIsp isn't a rapidly evolving language,
unlike the OSS common lisps.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).



From ripley at stats.ox.ac.uk  Tue Jan 17 13:20:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 12:20:04 +0000 (GMT)
Subject: [R] Font size of axis labels
In-Reply-To: <dqij25$nu5$1@sea.gmane.org>
References: <dqij25$nu5$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.61.0601171201180.9335@gannet.stats>

On Tue, 17 Jan 2006, Hilmar Berger wrote:

> In R, it is not possible to set the font size of axis labels directly
> (AFAIK).

Nor of anything else, since you are limited to the fonts available on the 
output device.

> Instead, scaling factors for the font chosen by the graphics
> device can be supplied. It appears that there is no constant font size
> for axis labels. My impression is that the axis label font size is
> scaled internally by R depending on the number of labels given for an axis.

Please can we have a demonstration of this: I have never seen it.

> In addition to the R-internal scaling  I need to adjust the axis label
> font size for label length and the size of other elements in the same
> layout.
>
> In order to use sensitive values for those externally supplied scaling
> factors (cex) I would like to know how R scales axis font sizes internally.
> Could someone point me to the method/file in the R-source code (R 2.1.1)
> where this scaling is done ?

I don't believe there is any scaling.  And 2.1.1 is not current.

The code used is in do_axis in src/main/plot.c.  The font size is selected 
at

     Rf_gpptr(dd)->cex = Rf_gpptr(dd)->cexbase * Rf_gpptr(dd)->cexaxis;

that is, just the base 'cex' multiplied by 'cex.axis' (as documented in 
?axis in current R).  So the target font size is

'pointsize' * 'cex' * 'cex.axis'

where the first term is set by the specific device, the second by 
par("cex") and the third either via par("cex.axis") or inline from the 
axis() call.  Devices will produce a nearby font size in a 
device-dependent fashion (both the postscript/PDF and windows families of 
devices round to the nearest big point: the story for X11 is much more 
complicated).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Tue Jan 17 14:31:30 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 17 Jan 2006 08:31:30 -0500
Subject: [R] RODBC sqlQuery question
Message-ID: <BFF25BE2.3D28%sdavis2@mail.nih.gov>

Why does this happen when I do a sqlQuery?

> sessionInfo()
Version 2.3.0 Under development (unstable) (2006-01-04 r36984)
powerpc-apple-darwin8.3.0

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"     

other attached packages:
  RODBC 
"1.1-5" 
> b <- sqlQuery(con,"select * from HartProbe where PROBE_ID='Hart19'")
SQLNumResultCols
SQLNumResultCols
SQLDescribeColW
SQLDescribeColW
SQLDescribeColW
SQLDescribeColW
SQLFetch
SQLFetch

I get output for each row returned.  In this case, the table has a million
rows, so I can't really select from it reasonably.  It doesn't look to me
like sqlQuery or sqlGetResults has a verbose option to turn off such output.

Thanks,
Sean



From obeeker at gmail.com  Tue Jan 17 14:33:36 2006
From: obeeker at gmail.com (=?gb2312?B?zqzN9Q==?=)
Date: Tue, 17 Jan 2006 21:33:36 +0800
Subject: [R] how can i locate the source code of a module quickly?
Message-ID: <43ccf240.6dec645c.51f2.ffff9501@mx.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/75b3d31f/attachment.pl

From murdoch at stats.uwo.ca  Tue Jan 17 15:01:47 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 17 Jan 2006 09:01:47 -0500
Subject: [R] how can i locate the source code of a module quickly?
In-Reply-To: <43ccf240.6dec645c.51f2.ffff9501@mx.gmail.com>
References: <43ccf240.6dec645c.51f2.ffff9501@mx.gmail.com>
Message-ID: <43CCF8CB.2090209@stats.uwo.ca>

On 1/17/2006 8:33 AM, obeeker at gmail.com wrote:
> I have  dowloaded the Source Code of R,and I want to know the process of
> chi-sqared test,but how can I found it?

Which function are you using?  Let's assume chisq.test.  Then the first 
thing to do is to type the function name:

 > chisq.test
function (x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)),
     rescale.p = FALSE, simulate.p.value = FALSE, B = 2000)
{
     DNAME <- deparse(substitute(x))

[ lots of skipped lines ]

     structure(list(statistic = STATISTIC, parameter = PARAMETER,
         p.value = PVAL, method = METHOD, data.name = DNAME, observed = x,
         expected = E, residuals = (x - E)/sqrt(E)), class = "htest")
}
<environment: namespace:stats>
 >

This is a deparsed version of the source; often it's good enough.  If 
not, it tells you that the function is in the stats package, so you can 
look for the original source (with comments, if you're lucky) in 
src/library/stats/R.

For some functions you get less information:

 > pchisq
function (q, df, ncp = 0, lower.tail = TRUE, log.p = FALSE)
{
     if (missing(ncp))
         .Internal(pchisq(q, df, lower.tail, log.p))
     else .Internal(pnchisq(q, df, ncp, lower.tail, log.p))
}
<environment: namespace:stats>

The .Internal() call tells you that this function is mostly implemented 
in C or Fortran code from the R source.  (.C() or .Fortran() or .Call() 
would tell you that the source is in the package src/library/stats/src 
directory).

To find its source, look in src/main/names.c.  There's a huge table 
there, containing these lines:

{"dchisq",	do_math2,	6,	11,	2+1,	{PP_FUNCALL, PREC_FN,	0}},
{"pchisq",	do_math2,	7,	11,	2+2,	{PP_FUNCALL, PREC_FN,	0}},
{"qchisq",	do_math2,	8,	11,	2+2,	{PP_FUNCALL, PREC_FN,	0}},

These tell you that the chisq functions are all implemented in a 
function called do_math2, with codes 6, 7, and 8 respectively.  (Further 
up in names.c is a comment describing the other columns.)

You need to do a grep or other search to find do_math2; it's in 
src/main/arithmetic.c.  You can follow the source from there.

Sometimes the source to the function won't tell you which package it is 
in (because the package doesn't have a namespace defined).  In that case 
the "getAnywhere" function can be useful, e.g.

 > getAnywhere("airmiles")
A single object matching 'airmiles' was found
It was found in the following places
   package:datasets
with value

Time Series:
Start = 1937
End = 1960
Frequency = 1
  [1]   412   480   683  1052  1385  1418  1634  2178  3362  5948  6109 
  5981
[13]  6753  8003 10566 12528 14760 16769 19819 22362 25340 25343 29269 30514

airmiles isn't a function, it's a dataset, and it is located in 
src/library/datasets/data/airmiles.R.

I hope this helps.

Duncan Murdoch



From danova_fr at hotmail.com  Tue Jan 17 15:03:52 2006
From: danova_fr at hotmail.com (david v)
Date: Tue, 17 Jan 2006 14:03:52 +0000
Subject: [R] Consensus dendogram help?
Message-ID: <BAY108-F3911F49C4DC420E680EB14971A0@phx.gbl>

Hello,
The follwing code generates 1000 dendograms from 1000 input binary matrices.
I dont't know how to generate a consensus dendogram from the 1000.
Can you help me ?

#code
library(ade4)
library(cluster)
library(stats)

for (i in 1:1000) {
#read each file
data<-read.table(i,header=FALSE,sep="\t",row.names=1)
dis<-dist.binary(data,method=5,diag=TRUE)
clust<-hclust(dd)
dend<-as.dendrogram(clust)
}
Thanks for your help.



From fbuchins at wpahs.org  Tue Jan 17 15:05:51 2006
From: fbuchins at wpahs.org (Farrel Buchinsky)
Date: Tue, 17 Jan 2006 09:05:51 -0500
Subject: [R] Installing a package yet it will not work.
Message-ID: <6E520FA8688B6C4C91D24DF15FAEEA7B0FBC57D5@wphnt1.wpahs.org>

Thank you Andy. It worked. 


Farrel Buchinsky, MD --- Mobile (412) 779-1073
Pediatric Otolaryngologist
Allegheny General Hospital
Pittsburgh, PA 


-----Original Message-----
From: "Liaw, Andy" <andy_liaw at merck.com> 
Sent: Monday, January 16, 2006 20:47
To: r-help at stat.math.ethz.ch; Farrel Buchinsky
Subject: RE: [R] Installing a package yet it will not work.

Several people have reported problem with installed packages recently, and
several turned out to be caused by the use of the StatLib mirror site.
Please try installing from a different mirror site and see if that solves
the problem for you.

Andy

From: Farrel Buchinsky
> 
> I want R to read my Microsoft Access database or maybe even a Sybase 
> database. I installed RODBC or at least thought I did. Then I issued 
> the following command:
> 
> library(RODBC)
> 
> And got
> 
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) : 
>         ReadItem: unknown type 241
> In addition: Warning message:
> package 'RODBC' was built under R version 2.3.0
> Error: package/namespace load failed for 'RODBC'
> 
> I am running R2.2.1 on Windows XP. Where does one begin to 
> troubleshoot this.
> 
> Farrel Buchinsky, MD --- Mobile (412) 779-1073 Pediatric 
> Otolaryngologist Allegheny General Hospital Pittsburgh, PA
> 
> 
> **********************************************************************
> This email and any files transmitted with it are 
> confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andrej.kastrin at siol.net  Tue Jan 17 15:30:58 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Tue, 17 Jan 2006 15:30:58 +0100
Subject: [R] For each element in vector do...
Message-ID: <43CCFFA2.5040209@siol.net>

Dear R useRs,

I have a vector with positive and negative numbers:
A=c(0,1,2,3,0,4,5)

Now if  i-th element in vector A is > 0, then i-th element in vector B 
is a+1
else i-th element in vector b=a (or 0)

vector A: 0 1 2 3 0 4 5
vector B: 0 2 3 4 0 5 6

What's the right way to do this. I still have some problems with for and 
if statements...

Cheers,  Andrej



From ccleland at optonline.net  Tue Jan 17 15:35:15 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 17 Jan 2006 09:35:15 -0500
Subject: [R] For each element in vector do...
In-Reply-To: <43CCFFA2.5040209@siol.net>
References: <43CCFFA2.5040209@siol.net>
Message-ID: <43CD00A3.50107@optonline.net>

B <- ifelse(A > 0, A + 1, A)

?ifelse

Andrej Kastrin wrote:
> Dear R useRs,
> 
> I have a vector with positive and negative numbers:
> A=c(0,1,2,3,0,4,5)
> 
> Now if  i-th element in vector A is > 0, then i-th element in vector B 
> is a+1
> else i-th element in vector b=a (or 0)
> 
> vector A: 0 1 2 3 0 4 5
> vector B: 0 2 3 4 0 5 6
> 
> What's the right way to do this. I still have some problems with for and 
> if statements...
> 
> Cheers,  Andrej
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 17 15:42:24 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 17 Jan 2006 15:42:24 +0100
Subject: [R] For each element in vector do...
References: <43CCFFA2.5040209@siol.net>
Message-ID: <017e01c61b74$3add3650$0540210a@www.domain>

one way is:

a <- c(0,1,2,3,0,4,5)
b <- ifelse(a > 0, a + 1, a)
b


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Andrej Kastrin" <andrej.kastrin at siol.net>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 17, 2006 3:30 PM
Subject: [R] For each element in vector do...


> Dear R useRs,
>
> I have a vector with positive and negative numbers:
> A=c(0,1,2,3,0,4,5)
>
> Now if  i-th element in vector A is > 0, then i-th element in vector 
> B
> is a+1
> else i-th element in vector b=a (or 0)
>
> vector A: 0 1 2 3 0 4 5
> vector B: 0 2 3 4 0 5 6
>
> What's the right way to do this. I still have some problems with for 
> and
> if statements...
>
> Cheers,  Andrej
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From B.Rowlingson at lancaster.ac.uk  Tue Jan 17 15:49:24 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 17 Jan 2006 14:49:24 +0000
Subject: [R] For each element in vector do...
In-Reply-To: <43CCFFA2.5040209@siol.net>
References: <43CCFFA2.5040209@siol.net>
Message-ID: <43CD03F4.6030100@lancaster.ac.uk>

Andrej Kastrin wrote:

> vector A: 0 1 2 3 0 4 5
> vector B: 0 2 3 4 0 5 6
> 
> What's the right way to do this. I still have some problems with for and 
> if statements...
>

  ?ifelse perhaps...

  > A
  [1] 0 1 2 3 0 4 5
  > B=ifelse(A>0,A+1,0)
  > B
  [1] 0 2 3 4 0 5 6

  does a sort of element-wise if-else thing. However it evaluates A+1 
for all elements. If you have something like:

  B = ifelse(A>0, slowFunction(A), 0)

  and not very many zeroes in A, you'll be doing a lot of slowFunction() 
work for nothing. In which case:

  B = numeric(length(A))
  B[A>0] = slowFunction(A[A>0])

  will only pass the necessary values of B to slowFunction() and put 
them into the right parts of B. B is initially all zeroes.

  It is left as an exercise to work out the better alternative to:

  ifelse(A>0, slowFunction(A), otherSlowFunction(A))

Barry



From goran.brostrom at gmail.com  Tue Jan 17 15:49:58 2006
From: goran.brostrom at gmail.com (=?UTF-8?Q?G=C3=B6ran_Brostr=C3=B6m?=)
Date: Tue, 17 Jan 2006 15:49:58 +0100
Subject: [R] Indentation in emacs
In-Reply-To: <m2y81lh5pj.fsf@fhcrc.org>
References: <148ed8180601120219xc554f16q5ab4dedbe3f55f34@mail.gmail.com>
	<m2y81lh5pj.fsf@fhcrc.org>
Message-ID: <148ed8180601170649m46699ce9m7437ba8bb2361918@mail.gmail.com>

On 1/12/06, Seth Falcon <sfalcon at fhcrc.org> wrote:
> On 12 Jan 2006, goran.brostrom at gmail.com wrote:
>
> > I'm using emacs-21.4 on debian unstable, together with the latest
> > ESS implementation. I try to change indentation to 4 by following
> > the advise in "R-exts": It results in the following lines in my
> > .emacs file:
> >
> > (custom-set-variables
> > ;; custom-set-variables was added by Custom -- don't edit or
> > ;; cut/paste it!  Your init file should contain only one such
> > ;; instance.
> > '(c-basic-offset 4)
> > '(c-default-style "bsd")
> > '(latin1-display t nil (latin1-disp)))
> > (custom-set-faces
> > ;; custom-set-faces was added by Custom -- don't edit or cut/paste it!
> > ;; Your init file should contain only one such instance.
> > )
> >
> > But it doesn't work with R code (with C code it works). So what is
> > missing?
>
> Try:
> (setq ess-indent-level 4)
>
> You may also be interested in the ESS mail list (a better place for
> such questions):
>  ESS-help at stat.math.ethz.ch mailing list
>  https://stat.ethz.ch/mailman/listinfo/ess-help

Thanks a lot. I asked here because it is discussed  in "R-exts". Still
(R-2.2.1) in an incomplete and misleading way, though.

Gran
>
>
> + seth
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Gran Brostrm



From tlumley at u.washington.edu  Tue Jan 17 16:05:29 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 17 Jan 2006 07:05:29 -0800 (PST)
Subject: [R] importing from Stata
In-Reply-To: <43CC1047.5020305@ipea.gov.br>
References: <43CC1047.5020305@ipea.gov.br>
Message-ID: <Pine.LNX.4.64.0601170702190.25991@homer22.u.washington.edu>

On Mon, 16 Jan 2006, Dimitri Joe wrote:

>
> (i) I get a big R file (for example, a 15Mb Stata file became a 42Mb R
> file; after cleanup.import() from the Hmisc package, it drooped to 35Mb,
> but that's still more than 2x the original Stata file) which, in turn, I
> suspect is due the fact that
>
> (ii) factors are created using Stata labels as levels.

Your suspicion is wrong.

A more likely explanation is that Stata uses single-precision floating 
point by default and can use 1-byte and 2-byte integers. R uses double 
precision floating point and four-byte integers.


> I wonder if
>
> (i) there isn't a way of forcing each variable to be numeric or integer,
> maintaining it's original values (instead of "Stata labels" as "R
> levels"). Or,

Yes. If you read the help page for read.dta() it tells you how.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ales.ziberna at gmail.com  Tue Jan 17 16:06:40 2006
From: ales.ziberna at gmail.com (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Tue, 17 Jan 2006 16:06:40 +0100
Subject: [R] Printing numerical matrices
Message-ID: <000301c61b77$be5111c0$a7fdfea9@TAMARA>

Dear useRs!

I athought this was a trival question, however I could not fin dan answer in
the help files for print, format or formatC. I would like to print a
numerical matrix so that all cells (all rows/colums) are printed:
a)	with the same number of decimal places (numbers after the decimal
separator)
b)	with the same number of numbers (regardless of  ".") 
c)	with the same number of characters (including "." if present, with
option for ignoring or not the "-" sign)) - if the last character would be
".", it is ommited.

Let say I have a matrix M
     [,1]        [,2]     [,3]       [,4] [,5]
[1,]    1 -0.87330578 14.72961  1.0885293  101
[2,]    2 -0.61406616 13.63638 -0.3124361  102 
[3,]    3 -0.03322147 15.15509 -1.6021408  103
[4,]    4  1.58780140 15.00857 -0.1908761  104
[5,]    5  0.75340104 14.80913  0.3210328  105

Which you can get (at least similar) by:
M<- matrix(rnorm(25),ncol=5,nrow=5)
M[,1]<-1:5
M[,3]<-M[,3]+15
M[,5]<-101:105


If I select the number of 3, I want to get:
a)
         [,1]   [,2]   [,3]   [,4]     [,5]
[1,]    1.000 -0.873 14.729  1.088  101.000
[2,]    2.000 -0.614 13.636 -0.312  102.000
[3,]    3.000 -0.033 15.155 -1.602  103.000
[4,]    4.000  1.588 15.009 -0.190  104.000
[5,]    5.000  0.753 14.809  0.321  105.000

b)
        [,1]  [,2] [,3]  [,4]  [,5]
[1,]    1.00 -0.87 14.7  1.09  101
[2,]    2.00 -0.61 13.6 -0.31  102
[3,]    3.00 -0.03 15.2 -1.60  103
[4,]    4.00  1.58 15.0 -0.19  104
[5,]    5.00  0.75 14.8  0.32  105

c) (ignoring "-" sign)
       [,1] [,2] [,3] [,4] [,5]
[1,]    1.0 -0.9   14  1.1  101
[2,]    2.0 -0.6   14 -0.3  102
[3,]    3.0 -0.0   15 -1.6  103
[4,]    4.0  1.6   15 -0.2  104
[5,]    5.0  0.8   15  0.3  105

c) (counting the  "-" sign)
       [,1] [,2] [,3] [,4] [,5]
[1,]    1.0   -1   14  1.1  101
[2,]    2.0   -1   14   -0  102
[3,]    3.0   -0   15   -2  103
[4,]    4.0  1.6   15   -0  104
[5,]    5.0  0.8   15  0.3  105


I would appreciate a solution for any of those options.

Best regards,
Ales



From ggrothendieck at gmail.com  Tue Jan 17 16:08:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 Jan 2006 10:08:10 -0500
Subject: [R] For each element in vector do...
In-Reply-To: <43CCFFA2.5040209@siol.net>
References: <43CCFFA2.5040209@siol.net>
Message-ID: <971536df0601170708lbea38ddr2ec69b80e1461c56@mail.gmail.com>

If addition to the ifelse solution already posted one could do this
since a logical expression used in a numeric context is regarded
as 1 for TRUE and 0 for FALSE.

B <- A + (A>0)


On 1/17/06, Andrej Kastrin <andrej.kastrin at siol.net> wrote:
> Dear R useRs,
>
> I have a vector with positive and negative numbers:
> A=c(0,1,2,3,0,4,5)
>
> Now if  i-th element in vector A is > 0, then i-th element in vector B
> is a+1
> else i-th element in vector b=a (or 0)
>
> vector A:
> vector B: 0 2 3 4 0 5 6
>
> What's the right way to do this. I still have some problems with for and
> if statements...
>
> Cheers,  Andrej
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Jan 17 16:15:11 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 17 Jan 2006 07:15:11 -0800 (PST)
Subject: [R] multiple GLMs with lmList in lme4
In-Reply-To: <s3ccd249.061@ZGRW21.uwcm.ac.uk>
References: <s3ccd249.061@ZGRW21.uwcm.ac.uk>
Message-ID: <Pine.LNX.4.64.0601170714270.25991@homer22.u.washington.edu>

On Tue, 17 Jan 2006, Daniel Farewell wrote:

> I'd like to fit a GLM to each of a number of subsets of some data. The 
> `family' argument to `lmList' (in lme4) has given me cause for optimism, 
> but so far I've only been able to achieve linear model fits. For example
>
>> df <- data.frame(gp = gp.temp <- factor(rep(1:3, each = 100)),
> x = x.temp <- rnorm(300),
> y = rpois(300, exp((-1:1)[gp.temp] + x.temp)))

Unless you are particularly attached to lmList() you might try by():

by(df,df$gp,function(subset) glm(y~x,family=poisson,data=subset))

 	-thomas


>
> Then a call to `glm' on the group 1 subset gives
>
>> glm(y ~ x, family = poisson, data = df, subset = gp == 1)
>
> Call:  glm(formula = y ~ x, family = poisson, data = df, subset = gp == 1)
>
> Coefficients:
> (Intercept)            x
>    -1.0143       0.9726
>
> Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
> Null Deviance:      138.5
> Residual Deviance: 82.76        AIC: 178.5
>
> (the right answer) but `lmList' gives
>
>> show(lmList(y ~ x | gp, family = poisson, data = df))
> Call: lmList(formula = y ~ x | gp, data = df, family = poisson)
> Coefficients:
>  (Intercept)         x
> 1   0.5560377 0.6362124
> 2   1.8431794 1.8541193
> 3   4.5773106 4.7871929
>
> Degrees of freedom: 300 total; 294 residual
> Residual standard error: 2.655714
>
> which come from linear model fits, e.g.
>
>> lm(y ~ x, data = df, subset = gp == 1)
>
> Call:
> lm(formula = y ~ x, data = df, subset = gp == 1)
>
> Coefficients:
> (Intercept)            x
>     0.5560       0.6362
>
> Any suggestions as to why lmList matches the linear fits rather than the GLM fits would be greatly appreciated. I'm using R2.2.1 with lme version 0.98-1 in Windows XP.
>
> Daniel Farewell
> Cardiff University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tom at maladmin.com  Tue Jan 17 11:31:53 2006
From: tom at maladmin.com (tom wright)
Date: Tue, 17 Jan 2006 05:31:53 -0500
Subject: [R] For each element in vector do...
In-Reply-To: <43CCFFA2.5040209@siol.net>
References: <43CCFFA2.5040209@siol.net>
Message-ID: <1137493913.4525.97.camel@localhost.localdomain>

> a<-c(0,1,2,3,0,4,5)
> b<-vector(length=length(a))
> b[a>0]<-a[a>0]+1
> b[a<=0]<-a[a<=0]
> b
[1] 0 2 3 4 0 5 6


On Tue, 2006-17-01 at 15:30 +0100, Andrej Kastrin wrote:
> Dear R useRs,
> 
> I have a vector with positive and negative numbers:
> A=c(0,1,2,3,0,4,5)
> 
> Now if  i-th element in vector A is > 0, then i-th element in vector B 
> is a+1
> else i-th element in vector b=a (or 0)
> 
> vector A: 0 1 2 3 0 4 5
> vector B: 0 2 3 4 0 5 6
> 
> What's the right way to do this. I still have some problems with for and 
> if statements...
> 
> Cheers,  Andrej
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rb.glists at gmail.com  Tue Jan 17 16:28:51 2006
From: rb.glists at gmail.com (Ronnie Babigumira)
Date: Tue, 17 Jan 2006 16:28:51 +0100
Subject: [R] importing from Stata
In-Reply-To: <Pine.LNX.4.64.0601170702190.25991@homer22.u.washington.edu>
References: <43CC1047.5020305@ipea.gov.br>
	<Pine.LNX.4.64.0601170702190.25991@homer22.u.washington.edu>
Message-ID: <43CD0D33.5040200@gmail.com>

To add onto an already clear explanation (a comment on precision in Stata). Indeed Stata stores all numbers as floats 
(also known as single precision or 4-byte reals). One way you could check this is to save a small subset of your data 
with all numbers as doubles in stata and see how that size of the new Stata file compares with the new file you create in R

(A section on this can be found in the Stata user manual 13.10)


Thomas Lumley wrote:
> On Mon, 16 Jan 2006, Dimitri Joe wrote:
> 
>>
>> (i) I get a big R file (for example, a 15Mb Stata file became a 42Mb R
>> file; after cleanup.import() from the Hmisc package, it drooped to 35Mb,
>> but that's still more than 2x the original Stata file) which, in turn, I
>> suspect is due the fact that
>>
>> (ii) factors are created using Stata labels as levels.
> 
> Your suspicion is wrong.
> 
> A more likely explanation is that Stata uses single-precision floating 
> point by default and can use 1-byte and 2-byte integers. R uses double 
> precision floating point and four-byte integers.
> 
> 
>> I wonder if
>>
>> (i) there isn't a way of forcing each variable to be numeric or integer,
>> maintaining it's original values (instead of "Stata labels" as "R
>> levels"). Or,
> 
> Yes. If you read the help page for read.dta() it tells you how.
> 
>  	-thomas
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rvaradhan at jhmi.edu  Tue Jan 17 16:30:54 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 17 Jan 2006 10:30:54 -0500
Subject: [R] Legends in xyplot
In-Reply-To: <43CCA5E7.9040406@statistik.uni-dortmund.de>
Message-ID: <001401c61b7b$01606620$7c94100a@win.ad.jhu.edu>

Hi Uwe,

I am aware of the "legend" option in xyplot, but I can't figure out how to
make it work.  In particular, I do not understand how to use "legend" along
with the "key" argument. For my example, I would like to have a legend box
inside the plotting frame to indicate the 3 different smoothed curves.  
Can someone give me a simple example or show how to do this for my example?

Thanks very much,
Ravi.

> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Tuesday, January 17, 2006 3:08 AM
> To: Ravi Varadhan
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Legends in xyplot
> 
> See ?xyplot and its argument "legend".
> 
> Uwe Ligges
> 
> 
> 
> 
> Ravi Varadhan wrote:
> 
> > Hi,
> >
> >
> >
> > How can I add legends in the "xyplot" function, in the "lattice"
> library?
> > Here is a simulation example:
> >
> >
> >
> > x <- runif(90)
> >
> > z <- sample(1:3, 90, rep=T)
> >
> > y <- rnorm(90, mean = x^2 + z, sd=1)
> >
> >
> >
> > library(lattice)
> >
> > trellis.par.set(col.whitebg())
> >
> > xyplot(y ~x, groups=as.factor(z), type = c('p', 'smooth'), span = .67)
> >
> >
> >
> > Thanks in advance,
> >
> > Ravi.
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From farewelld at Cardiff.ac.uk  Tue Jan 17 16:53:28 2006
From: farewelld at Cardiff.ac.uk (Daniel Farewell)
Date: Tue, 17 Jan 2006 15:53:28 +0000
Subject: [R] multiple GLMs with lmList in lme4
Message-ID: <s3cd1307.086@ZGRW21.uwcm.ac.uk>

Many thanks! That'll work great. It's always good to discover a new, general, function like by().

I would still be interested to know how the family argument to lmList() should be used.

Daniel

>>> Thomas Lumley <tlumley at u.washington.edu> 01/17/06 3:15 pm >>>
On Tue, 17 Jan 2006, Daniel Farewell wrote:

> I'd like to fit a GLM to each of a number of subsets of some data. The 
> `family' argument to `lmList' (in lme4) has given me cause for optimism, 
> but so far I've only been able to achieve linear model fits. For example
>
>> df <- data.frame(gp = gp.temp <- factor(rep(1:3, each = 100)),
> x = x.temp <- rnorm(300),
> y = rpois(300, exp((-1:1)[gp.temp] + x.temp)))

Unless you are particularly attached to lmList() you might try by():

by(df,df$gp,function(subset) glm(y~x,family=poisson,data=subset))

 	-thomas


>
> Then a call to `glm' on the group 1 subset gives
>
>> glm(y ~ x, family = poisson, data = df, subset = gp == 1)
>
> Call:  glm(formula = y ~ x, family = poisson, data = df, subset = gp == 1)
>
> Coefficients:
> (Intercept)            x
>    -1.0143       0.9726
>
> Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
> Null Deviance:      138.5
> Residual Deviance: 82.76        AIC: 178.5
>
> (the right answer) but `lmList' gives
>
>> show(lmList(y ~ x | gp, family = poisson, data = df))
> Call: lmList(formula = y ~ x | gp, data = df, family = poisson)
> Coefficients:
>  (Intercept)         x
> 1   0.5560377 0.6362124
> 2   1.8431794 1.8541193
> 3   4.5773106 4.7871929
>
> Degrees of freedom: 300 total; 294 residual
> Residual standard error: 2.655714
>
> which come from linear model fits, e.g.
>
>> lm(y ~ x, data = df, subset = gp == 1)
>
> Call:
> lm(formula = y ~ x, data = df, subset = gp == 1)
>
> Coefficients:
> (Intercept)            x
>     0.5560       0.6362
>
> Any suggestions as to why lmList matches the linear fits rather than the GLM fits would be greatly appreciated. I'm using R2.2.1 with lme version 0.98-1 in Windows XP.
>
> Daniel Farewell
> Cardiff University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ggrothendieck at gmail.com  Tue Jan 17 16:54:59 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 Jan 2006 10:54:59 -0500
Subject: [R] Printing numerical matrices
In-Reply-To: <000301c61b77$be5111c0$a7fdfea9@TAMARA>
References: <000301c61b77$be5111c0$a7fdfea9@TAMARA>
Message-ID: <971536df0601170754j405c6e10h8cac2ff6108fc282@mail.gmail.com>

Here are a couple of alternatives to try:

noquote(format(round(M,3)))

noquote(apply(round(M,3), 2, format))



On 1/17/06, Ale? ?iberna <ales.ziberna at gmail.com> wrote:
> Dear useRs!
>
> I athought this was a trival question, however I could not fin dan answer in
> the help files for print, format or formatC. I would like to print a
> numerical matrix so that all cells (all rows/colums) are printed:
> a)      with the same number of decimal places (numbers after the decimal
> separator)
> b)      with the same number of numbers (regardless of  ".")
> c)      with the same number of characters (including "." if present, with
> option for ignoring or not the "-" sign)) - if the last character would be
> ".", it is ommited.
>
> Let say I have a matrix M
>     [,1]        [,2]     [,3]       [,4] [,5]
> [1,]    1 -0.87330578 14.72961  1.0885293  101
> [2,]    2 -0.61406616 13.63638 -0.3124361  102
> [3,]    3 -0.03322147 15.15509 -1.6021408  103
> [4,]    4  1.58780140 15.00857 -0.1908761  104
> [5,]    5  0.75340104 14.80913  0.3210328  105
>
> Which you can get (at least similar) by:
> M<- matrix(rnorm(25),ncol=5,nrow=5)
> M[,1]<-1:5
> M[,3]<-M[,3]+15
> M[,5]<-101:105
>
>
> If I select the number of 3, I want to get:
> a)
>         [,1]   [,2]   [,3]   [,4]     [,5]
> [1,]    1.000 -0.873 14.729  1.088  101.000
> [2,]    2.000 -0.614 13.636 -0.312  102.000
> [3,]    3.000 -0.033 15.155 -1.602  103.000
> [4,]    4.000  1.588 15.009 -0.190  104.000
> [5,]    5.000  0.753 14.809  0.321  105.000
>
> b)
>        [,1]  [,2] [,3]  [,4]  [,5]
> [1,]    1.00 -0.87 14.7  1.09  101
> [2,]    2.00 -0.61 13.6 -0.31  102
> [3,]    3.00 -0.03 15.2 -1.60  103
> [4,]    4.00  1.58 15.0 -0.19  104
> [5,]    5.00  0.75 14.8  0.32  105
>
> c) (ignoring "-" sign)
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1.0 -0.9   14  1.1  101
> [2,]    2.0 -0.6   14 -0.3  102
> [3,]    3.0 -0.0   15 -1.6  103
> [4,]    4.0  1.6   15 -0.2  104
> [5,]    5.0  0.8   15  0.3  105
>
> c) (counting the  "-" sign)
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1.0   -1   14  1.1  101
> [2,]    2.0   -1   14   -0  102
> [3,]    3.0   -0   15   -2  103
> [4,]    4.0  1.6   15   -0  104
> [5,]    5.0  0.8   15  0.3  105
>
>
> I would appreciate a solution for any of those options.
>
> Best regards,
> Ales
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From olivier.eterradossi at ema.fr  Tue Jan 17 17:07:26 2006
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Tue, 17 Jan 2006 17:07:26 +0100
Subject: [R]  Kriging for d>3
Message-ID: <43CD163E.1080709@ema.fr>

Hello,
probably you should have a look to the "RandomFields" package.
Regards. Olivier

>------------------------------
>
>Message: 59
>Date: Tue, 17 Jan 2006 10:45:46 +0100
>From: Eivind Sm?rgrav <EISM at statoil.com>
>Subject: [R] Kriging for d>3
>To: <r-help at stat.math.ethz.ch>
>Message-ID:
>	<8AEE42D73E439C4AA476A3A66BC7BD8021FE65 at ST-EXCL06.statoil.net>
>Content-Type: text/plain;	charset="iso-8859-1"
>
>Hi,
>
>I'm looking for software that can perform kriging on systems with dimensionality higher than 3, say d=5. 
>Are anyone aware of packages in R that can do this?
>
>Thanks,
>
>Eivind Sm?rgrav
>

-- 
Olivier ETERRADOSSI
Ma??tre-Assistant
CMGD / Equipe "Propri??t??s Psycho-Sensorielles des Mat??riaux"
Ecole des Mines d'Al??s
H??lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel: +33 (0)5.59.30.54.25
fax: +33 (0)5.59.30.63.68
http://www.ema.fr



From remigijus.lapinskas at mif.vu.lt  Tue Jan 17 17:32:46 2006
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Tue, 17 Jan 2006 18:32:46 +0200
Subject: [R] survival with Weibull
Message-ID: <43CD1C2E.5010003@mif.vu.lt>

Hello,

I want to test if the Weibull distribution is appropriate for the
failure time. When trying to reproduce an example from MASS (the book,
Ch. 13.2), I type

library(survival)
library(MASS)
leuk.wei <- survreg( Surv(time)~ag+log(wbc),data=leuk)
ntimes <- leuk$time*exp(-leuk.wei$linear.predictors)
plot(survfit(Surv(ntimes)),log=T)

and get (almost) the same graph as in the book (which means that the
distribution is close to exponential). On the other hand, if I want to
test for the Weibull distribution, the recommended command

> plot(survfit(Surv(ntimes)),fun="cloglog")

ends in an error message:

Error in rep.default(2, n2 - 1) : invalid number of copies in rep()
In addition: Warning message:
2 x values <= 0 omitted from logarithmic plot in: xy.coords(x, y,
xlabel, ylabel, log)

I'd appreciate any hints,
Rem



From deepayan.sarkar at gmail.com  Tue Jan 17 17:46:59 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 17 Jan 2006 10:46:59 -0600
Subject: [R] Legends in xyplot
In-Reply-To: <001401c61b7b$01606620$7c94100a@win.ad.jhu.edu>
References: <43CCA5E7.9040406@statistik.uni-dortmund.de>
	<001401c61b7b$01606620$7c94100a@win.ad.jhu.edu>
Message-ID: <eb555e660601170846p5a6ece93m9059098233ed2148@mail.gmail.com>

On 1/17/06, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Hi Uwe,
>
> I am aware of the "legend" option in xyplot, but I can't figure out how to
> make it work.  In particular, I do not understand how to use "legend" along
> with the "key" argument. For my example, I would like to have a legend box
> inside the plotting frame to indicate the 3 different smoothed curves.
> Can someone give me a simple example or show how to do this for my example?

Here's a modified version of an example in demo(lattice):

xyplot(Petal.Length ~ Petal.Width, data = iris,
       groups = Species,
       type = c("p", "smooth"), span=.75,
       auto.key =
       list(title = "Iris Data",
            x = .15, y=.85, corner = c(0,1),
            border = TRUE, lines = TRUE))

-Deepayan



From leif at reflectivity.com  Tue Jan 17 17:50:39 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Tue, 17 Jan 2006 08:50:39 -0800
Subject: [R] Scientific notation in plots
Message-ID: <200601171650.k0HGohmd017524@hypatia.math.ethz.ch>

You could also write your numbers using SI suffixes.
Below is a function I use for this purpose.
The option "near" allows you to require that numbers between 0.001 and 1000 are written as decimal; i.e. "0.010" appears as "0.010" instead of "10.0m".

##
## num2SI converts numbers to SI suffixed numbers
##
num2SI<-function(num,digits=4,near=TRUE){
  power<-signif(log10(abs(num)),digits=6)
  power3=floor(power/3); power3[!is.finite(power3)]=0
  powers=c("y","z","a","f","p","n","u","m","","k","M","G","T","P")
  powerstr=powers[power3+9]
  newnum=signif(num/10^(power3*3),digits=digits)
  newnum[is.nan(newnum)]=0
  numstr=(paste(newnum,powerstr,sep=""))
  nearidx=near & (abs(num)>0.001) & (abs(num)<1000)
  nearidx[is.na(nearidx)]=FALSE
  if(sum(nearidx)) numstr[nearidx]=sprintf("%.*f",pmax(1,(digits-floor(power[nearidx])-1),na.rm=TRUE),num[nearidx])
  numstr[!is.finite(num)] = paste(num[!is.finite(num)])
 return(numstr)
}

Leif Kirschenbaum
Senior Yield Engineer
Reflectivity, Inc.
(408) 737-8100 x307
leif at reflectivity.com 

> Message: 6
> Date: Fri, 13 Jan 2006 12:49:54 +0100 (CET)
> From: Oddmund Nordg??rd <nood at ext.sir.no>
> Subject: [R] Scientific notation in plots
> To: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.56.0601131247300.25418 at dna.sir.no>
> Content-Type: TEXT/PLAIN;	charset=ISO-8859-1
> 
> 
> Is it possible to use scientific notation of numbers on the 
> axis of plots
> without using the xEy notation. That means: a beatiful 1x10^3 
> instead of 1E3.
> Logarithmic scale, in my case.
> 
> Thank you very much!
> 
> Oddmund
> 
> ******************************************
> 
>   Oddmund Nordg?rd
> 
>   Department of Haematology and Oncology
>   Stavanger University Hospital
>   P.O. Box 8100
>   4068 STAVANGER
>   Phone: 51 51 89 34
>   Email: nood at ext.sir.no



From pauljohn32 at gmail.com  Tue Jan 17 17:50:54 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Tue, 17 Jan 2006 10:50:54 -0600
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <43CCA1AD.6070805@statistik.uni-dortmund.de>
References: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>
	<43CCA1AD.6070805@statistik.uni-dortmund.de>
Message-ID: <13e802630601170850l38352147mb4be199b8b70e11f@mail.gmail.com>

Thanks, Uwe

that clears up why I can't make R2WinBUGs work with OpenBUGS and WinBUGS1.5 :)
Both work pretty good with Wine in a GUI.  I noticed that when I tried
"rbugs", it does succeed in starting WinBUGS GUI, but then nothing
happens. I'll get WinBUGS1.4 and see what happens.

In the meanwhile, I'm going to t ry to see what BRugs is good for. In
Linux, when I try to install BRugs, the install fails with an error
saying that, at the current time, BRugs works only in Windows.


* Installing *source* package 'BRugs' ...
Package 'BRugs' currently only works under Windows.\nIt is supposed to
work under Linux in future releases.

I'd like to stop that check and see what happens!  The way I read the
sourcecode from OpenBUGS and BRugs, I need to replace the windows dll
install and instead put in an so file (as in OpenBUGS).

If anybody has done this, please let me know of your experience.


On 1/17/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Paul Johnson wrote:
> > Greetings:
> >
> > I'm going to encourage some students to try Bayesian ideas for
> > hierarchical models.
> > I want to run the WinBUGS and R examples in Tony Lancaster's An
> > Introduction to Modern Bayesian Econometrics.  That features MS
> > Windows and "bugs" from R2WinBUGS.
> >
> > Today, I want to ask how people are doing this in Linux? I have found
> > a plethora of possibilities, some of which are not quite ready, some
> > of which work only under MS Windows.  Right now I just want to know
> > "what actually works".
> >
> > Here's where I stand now in Fedora Core 4 Linux.
> > 1. OpenBUGS-2.1.1 runs in Linux.  I can run "linbugs" (the console
> > version similar to the old BUGS) and also I can run--under wine--the
> > newest version of "winbugs.exe" that is circulated with OpenBUGS.  As
> > far as I can tell, the graphical interface in wine/winbugs works in
> > almost all elements.  A few things seem not quite right in the GUI
> > (can't initialize more than one chain, difficult to specify variables
> > for monitoring), but it does work.
> >
> > It is easier to install and work with OpenBUGS's version of
> > winbugs.exe than with Winbugs-1.4 because the Open version does not
> > have that annoying license registration and "winbugs.exe" is not
> > wrapped inside an installation script.   I'm a little confused about
> > WinBUGS versions because the BRugs documents
> > http://www.biostat.umn.edu/~brad/software/BRugs/BRugs_install.html
> > refer to WinBUGS-1.5, which refers to
> > http://www.biostat.umn.edu/~brad/software/BRugs/WinBUGS15.zip, which
> > can be downloaded without any of the registration steps, but WinBUGS15
> > is not mentioned in the WinBUGS site (where 1.4.1 appears to be the
> > newest).
> >
> > Supposing I get the winbugs.exe question settled:
> >
> > 2. How to most dependably send jobs from R to "linbugs" or "winbugs.exe"?
> >
> > The BRugs package is preferred?
> >
> > For a long time, R2WinBUGS was Windows-only, but toward the end of
> > last fall I noticed that R2WinBUGS now does compile and install under
> > R in Linux.
> >
> > however, its help still says:
> > SystemRequirements:   WinBUGS 1.4 on Windows
> >
> > I'd appreciate any advice.
>
> [resend to less recipients in order to save Martin's spare time to
> approve message;
> CCing Andrew Thomas, Bob O'Hara and Sibylle Sturtz separately]
>
>
>
>
> Re BUGS:
> WinBUGS-1.5 never got really released, AFAIK - Andrew or Bob might want
> to correct me. It has been renamed to OpenBUGS. The current version is
> the GPL'ed OpenBUGS 2.1.1 available from
> http://mathstat.helsinki.fi/openbugs/.
>
> Re R packages:
> - R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
> can speak with WinBUGS under wine thanks to user contributions. But it
> still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
> Windows).
> - BRugs contains the BRugs interface, R functions and the whole OpenBUGS
> installation. Unfortunately, due to serious compiler problems, we were
> not able to get a Linux version running using the interface. Hence it
> was not possible to release any non-Windows version up to now.
> I haven't tested BRugs under wine yet (in which case R has to run under
> wine as well, of course) ... and I do not know if there are any serious
> performance penalties.
> Note that even in the long term, OpenBUGS will only run on x86 based
> platforms.
>
> Due to the much more flexibile interface, I prefer BRugs.
>
> BTW: "Real programmers" won't consider R2WinBUGS to be an "interface" at
> all - it might be useful, though. ;-)
>
>
> Uwe Ligges
>
>
>
> > --
> > Paul E. Johnson
> > Professor, Political Science
> > 1541 Lilac Lane, Room 504
> > University of Kansas
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>


--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From aleszib2 at gmail.com  Tue Jan 17 18:07:25 2006
From: aleszib2 at gmail.com (Ales Ziberna)
Date: Tue, 17 Jan 2006 18:07:25 +0100
Subject: [R] Printing numerical matrices
In-Reply-To: <971536df0601170754j405c6e10h8cac2ff6108fc282@mail.gmail.com>
Message-ID: <001001c61b88$7e716260$a7fdfea9@TAMARA>

Thank you! 

That solves my a) problem!

Best,
Ales 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Tuesday, January 17, 2006 4:55 PM
To: Ale?? ??iberna
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Printing numerical matrices

Here are a couple of alternatives to try:

noquote(format(round(M,3)))

noquote(apply(round(M,3), 2, format))



On 1/17/06, Ale?? ??iberna <ales.ziberna at gmail.com> wrote:
> Dear useRs!
>
> I athought this was a trival question, however I could not fin dan 
> answer in the help files for print, format or formatC. I would like to 
> print a numerical matrix so that all cells (all rows/colums) are printed:
> a)      with the same number of decimal places (numbers after the decimal
> separator)
> b)      with the same number of numbers (regardless of  ".")
> c)      with the same number of characters (including "." if present, with
> option for ignoring or not the "-" sign)) - if the last character 
> would be ".", it is ommited.
>
> Let say I have a matrix M
>     [,1]        [,2]     [,3]       [,4] [,5]
> [1,]    1 -0.87330578 14.72961  1.0885293  101
> [2,]    2 -0.61406616 13.63638 -0.3124361  102
> [3,]    3 -0.03322147 15.15509 -1.6021408  103
> [4,]    4  1.58780140 15.00857 -0.1908761  104
> [5,]    5  0.75340104 14.80913  0.3210328  105
>
> Which you can get (at least similar) by:
> M<- matrix(rnorm(25),ncol=5,nrow=5)
> M[,1]<-1:5
> M[,3]<-M[,3]+15
> M[,5]<-101:105
>
>
> If I select the number of 3, I want to get:
> a)
>         [,1]   [,2]   [,3]   [,4]     [,5]
> [1,]    1.000 -0.873 14.729  1.088  101.000
> [2,]    2.000 -0.614 13.636 -0.312  102.000
> [3,]    3.000 -0.033 15.155 -1.602  103.000
> [4,]    4.000  1.588 15.009 -0.190  104.000
> [5,]    5.000  0.753 14.809  0.321  105.000
>
> b)
>        [,1]  [,2] [,3]  [,4]  [,5]
> [1,]    1.00 -0.87 14.7  1.09  101
> [2,]    2.00 -0.61 13.6 -0.31  102
> [3,]    3.00 -0.03 15.2 -1.60  103
> [4,]    4.00  1.58 15.0 -0.19  104
> [5,]    5.00  0.75 14.8  0.32  105
>
> c) (ignoring "-" sign)
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1.0 -0.9   14  1.1  101
> [2,]    2.0 -0.6   14 -0.3  102
> [3,]    3.0 -0.0   15 -1.6  103
> [4,]    4.0  1.6   15 -0.2  104
> [5,]    5.0  0.8   15  0.3  105
>
> c) (counting the  "-" sign)
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1.0   -1   14  1.1  101
> [2,]    2.0   -1   14   -0  102
> [3,]    3.0   -0   15   -2  103
> [4,]    4.0  1.6   15   -0  104
> [5,]    5.0  0.8   15  0.3  105
>
>
> I would appreciate a solution for any of those options.
>
> Best regards,
> Ales
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From c.beale at macaulay.ac.uk  Tue Jan 17 18:07:52 2006
From: c.beale at macaulay.ac.uk (Colin Beale)
Date: Tue, 17 Jan 2006 17:07:52 +0000
Subject: [R] removing loop in array recalculation
Message-ID: <s3cd247b.050@macaulay.ac.uk>

Hi,

I'm looking for a more elegant (and faster) solution to my current
problem than the code at the end. I'm sure there is one, but can't think
where to look - any pointers would be very welcome. The problem is one
of resampling within an array. This array consists of 0s, 1s and NAs.
For each level of dimension z, I would like to rewrite the array such
that it looks up the values in the matrix on either side (through a
variable number of cells) and determines if there is a 1 in any of them
- if so, the new value is 1, otherwise 0. This process is repeated
across the entire matrix: I will obviously end up with a lot more 1s in
the new array than I did before. The code at the end works, but is very
slow for large arrays (it needs package (magic) to work). Any
suggestions/pointers would be gratefully recieved

Colin

An example dataset could be:

size = 1               #determines how many values to sample around the
focal 
library (magic)     
set.seed(1)
A <- array (sample (c (0,0,0,1), 35, replace = T), dim = c (5,4,2))
    temp <- apad (apad (A, c (size,size,0)), c(size,size,0), post =
FALSE)  # pads array
    temp[,c (1, (4 + 2 * size)),] <- NA            # makes additional
rows/cols = NA
    temp[c (1, (5 + 2 * size)),,] <- NA

    for (y in 1: 5) {                               # recalculates 
within size
      for (x in 1: 4) {
        for (z in 1: 2) {
          A[y,x,z] <- ifelse (sum (temp[(y):(y+2 * size),(x):(x+2 *
size),z], na.rm = TRUE) == 0, 0, 1)
        }
      }
    }



From dmbates at gmail.com  Tue Jan 17 18:41:38 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 17 Jan 2006 11:41:38 -0600
Subject: [R] multiple GLMs with lmList in lme4
In-Reply-To: <s3cd1307.086@ZGRW21.uwcm.ac.uk>
References: <s3cd1307.086@ZGRW21.uwcm.ac.uk>
Message-ID: <40e66e0b0601170941rf86d2f3l6d4f0e02f954a7a3@mail.gmail.com>

On 1/17/06, Daniel Farewell <farewelld at cardiff.ac.uk> wrote:
> Many thanks! That'll work great. It's always good to discover a new, general, function like by().
>
> I would still be interested to know how the family argument to lmList() should be used.

As you have discovered the family argument has no effect in lmList()
from the current version of the lme4 package.   A new version of the
package with this fixed will be uploaded shortly.

> Daniel
>
> >>> Thomas Lumley <tlumley at u.washington.edu> 01/17/06 3:15 pm >>>
> On Tue, 17 Jan 2006, Daniel Farewell wrote:
>
> > I'd like to fit a GLM to each of a number of subsets of some data. The
> > `family' argument to `lmList' (in lme4) has given me cause for optimism,
> > but so far I've only been able to achieve linear model fits. For example
> >
> >> df <- data.frame(gp = gp.temp <- factor(rep(1:3, each = 100)),
> > x = x.temp <- rnorm(300),
> > y = rpois(300, exp((-1:1)[gp.temp] + x.temp)))
>
> Unless you are particularly attached to lmList() you might try by():
>
> by(df,df$gp,function(subset) glm(y~x,family=poisson,data=subset))
>
>         -thomas
>
>
> >
> > Then a call to `glm' on the group 1 subset gives
> >
> >> glm(y ~ x, family = poisson, data = df, subset = gp == 1)
> >
> > Call:  glm(formula = y ~ x, family = poisson, data = df, subset = gp == 1)
> >
> > Coefficients:
> > (Intercept)            x
> >    -1.0143       0.9726
> >
> > Degrees of Freedom: 99 Total (i.e. Null);  98 Residual
> > Null Deviance:      138.5
> > Residual Deviance: 82.76        AIC: 178.5
> >
> > (the right answer) but `lmList' gives
> >
> >> show(lmList(y ~ x | gp, family = poisson, data = df))
> > Call: lmList(formula = y ~ x | gp, data = df, family = poisson)
> > Coefficients:
> >  (Intercept)         x
> > 1   0.5560377 0.6362124
> > 2   1.8431794 1.8541193
> > 3   4.5773106 4.7871929
> >
> > Degrees of freedom: 300 total; 294 residual
> > Residual standard error: 2.655714
> >
> > which come from linear model fits, e.g.
> >
> >> lm(y ~ x, data = df, subset = gp == 1)
> >
> > Call:
> > lm(formula = y ~ x, data = df, subset = gp == 1)
> >
> > Coefficients:
> > (Intercept)            x
> >     0.5560       0.6362
> >
> > Any suggestions as to why lmList matches the linear fits rather than the GLM fits would be greatly appreciated. I'm using R2.2.1 with lme version 0.98-1 in Windows XP.
> >
> > Daniel Farewell
> > Cardiff University
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> Thomas Lumley                   Assoc. Professor, Biostatistics
> tlumley at u.washington.edu        University of Washington, Seattle
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From danova_fr at hotmail.com  Tue Jan 17 18:56:22 2006
From: danova_fr at hotmail.com (david v)
Date: Tue, 17 Jan 2006 17:56:22 +0000
Subject: [R] Cannot convert from phylo to hclust , error!!???
Message-ID: <BAY108-F254BC34E5751B09904DB9D971A0@phx.gbl>

Hello,
The following code does'nt work for me. The last command reports an error. I 
have created a consensus tree using the consensus comand from phylo but 
cannot manipulate the phylo object afterwards to create a dendogram , by 
transforming the phylo object into a hclust object and then into a dendogram 
??
Thanks for any help

library(ade4)
library(cluster)
library(stats)
library(ape)

data<-read.table(file="in.matrix",header=FALSE,sep="\t",row.names=1)
dis<-dist.binary(data,method=5,diag=TRUE)
clust<-hclust(dis)
tree1<-as.phylo(clust)
tree2<-as.phylo(clust)
tree3<-as.phylo(clust)
tree4<-as.phylo(clust)
tree5<-as.phylo(clust)
liste <-list(tree1,tree2,tree3,tree4,tree5)
cons<-consensus(liste,p=0.5)
cons_clust <-as.hclust(cons)
Error in "[<-"(`*tmp*`, i, value = numeric(0)) :
        nothing to replace

david



From ng296 at cam.ac.uk  Tue Jan 17 19:14:48 2006
From: ng296 at cam.ac.uk (N. Goodacre)
Date: 17 Jan 2006 18:14:48 +0000
Subject: [R] Clustering function
Message-ID: <Prayer.1.0.16.0601171814480.21551@hermes-2.csi.cam.ac.uk>

Dear mailing group,

  I have loaded an Excel file into R by calling it ".csv" and using the 
"read.csv" function in R. However then I want to use the (limma package 
specific, I believe) function "hclust", which clusters data in a tree 
dendrogram, by similarity. However, I receive the errors msg.s: 1) "missing 
observations in cov/cor" 2) "In addition: warning message: NAs introduced 
by coercion"

The excel file does have missing data slots. Do these tow messages mean 
that it has written "NA" where the missing slots were? Or is there a more 
fundamental error?

 thanks.

-Norman



From wmk at takomasoftware.com  Tue Jan 17 19:16:12 2006
From: wmk at takomasoftware.com (Bill Kules)
Date: Tue, 17 Jan 2006 13:16:12 -0500
Subject: [R] Newbie question on using friedman.test()
Message-ID: <200601171816.k0HIGH7w014075@dispatch.cs.umd.edu>

I am trying to use the friedman.test() on a data frame, d, but
I am receiving the following error message:


> d
   AW HS IAC WA
1   6  8   3  5
2   2  2   3  6
3   7  7   8  3
4   8  5   4  5
....
20  2  5   2  7
21  7  7   6  7
22  7  8   6  8
23  6  8   4  5
24  5  7   5  2
> friedman.test(d)
Error in any(is.na(groups)) : argument "groups" is missing, with no default

I think I just need to convert the data frame to a matrix, and then
friedman.test() will get the roups and blocks automatically.

Question 1) Is my understanding correct?

Question 2) What R function will convert the data frame to the matrix I
need?
I'm still figuring out the matrix functions, and I would appreciate any
pointers or examples.  The help() section is sometimes a bit terse...

Thanks in advance from a Newbie,
Bill

PS - to recreate the above data frame "d":
d <-
structure(list(AW = as.integer(c(6, 2, 7, 8, 8, 8, 7, 5, 3, 6, 
8, 7, 6, 4, 8, 7, 8, 7, 7, 2, 7, 7, 6, 5)), HS = as.integer(c(8, 
2, 7, 5, 4, 5, 7, 7, 2, 8, 4, 7, 8, 7, 6, 7, 5, 8, 8, 5, 7, 8, 
8, 7)), IAC = as.integer(c(3, 3, 8, 4, 7, 5, 8, 3, 4, 3, 7, 4, 
6, 5, 6, 7, 8, 6, 8, 2, 6, 6, 4, 5)), WA = as.integer(c(5, 6, 
3, 5, 3, 6, 7, 8, 3, 5, 6, 7, 7, 7, 7, 6, 4, 7, 8, 7, 7, 8, 5, 
2))), .Names = c("AW", "HS", "IAC", "WA"), row.names = c("1", 
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", 
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24"
), class = "data.frame")

========
Bill Kules
Principal, Takoma Software, Inc., Takoma Park, MD
  www.takomasoftware.com
Ph.D. Candidate, University of Maryland Human-Computer Interaction Lab
  www.cs.umd.edu/hcil

wmk at takomasoftware.com
(301) 405-2725 voice
(301) 891-7271 voice + voicemail
(301) 891-7273 fax
(301) 755-7982 mobile
========



From pensterfuzzer at yahoo.de  Tue Jan 17 19:22:43 2006
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Tue, 17 Jan 2006 19:22:43 +0100 (CET)
Subject: [R] Vector indices
Message-ID: <20060117182243.28779.qmail@web25812.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/00f8b4e1/attachment.pl

From alexandrarma at yahoo.com.br  Tue Jan 17 19:24:24 2006
From: alexandrarma at yahoo.com.br (Alexandra R. M. de Almeida)
Date: Tue, 17 Jan 2006 15:24:24 -0300 (ART)
Subject: [R] Regression with no-intercept
Message-ID: <20060117182424.51001.qmail@web33310.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/28234d31/attachment.pl

From thogiti at gmail.com  Tue Jan 17 19:27:55 2006
From: thogiti at gmail.com (Nagu)
Date: Tue, 17 Jan 2006 10:27:55 -0800
Subject: [R] CLuster analysis with only nominal variables
Message-ID: <21da85430601171027n12e75bf4tf522d742ce379bb0@mail.gmail.com>

Hi All,

I am wondering if there is any literature or any prior implementations
of cluster analysis for only nominal (categorical) variables for a
large dataset, apprx 20,000 rows with 15 variables.

I came across one or two such implementations, but they seem to assume
certain data distributions.

Thank you,
Nagu



From liuha at umdnj.edu  Tue Jan 17 19:35:01 2006
From: liuha at umdnj.edu (Hao Liu)
Date: Tue, 17 Jan 2006 13:35:01 -0500
Subject: [R] help with coxph() for multiple genes
Message-ID: <002c01c61b94$b9d1c500$2b99db82@Leomobile>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/3f2afcc0/attachment.pl

From liuha at umdnj.edu  Tue Jan 17 19:47:18 2006
From: liuha at umdnj.edu (Hao Liu)
Date: Tue, 17 Jan 2006 13:47:18 -0500
Subject: [R] help with parsing multiple coxph() results
Message-ID: <003901c61b96$71602ad0$2b99db82@Leomobile>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/a1ada641/attachment.pl

From william.simpson at drdc-rddc.gc.ca  Tue Jan 17 20:10:03 2006
From: william.simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Tue, 17 Jan 2006 14:10:03 -0500
Subject: [R] lme model specification
Message-ID: <1137525003.31018.3.camel@localhost.localdomain>

I have been asked to analyse the results of (what is to me) a very
complicated experiment.

The dependent measure is the estimated distance, which is measured as a
function of the actual distance. There are also several other IVs.

The plot of log estimated distance as a function of log distance is
linear. So in the rest of the analysis I will use logestimate and
logdistance.

My plan is to see how the other IVs affect the slope and intercept of
this linear relationship between logestimate and log distance.

What complicates everything is that each datum point is not independent.
Rather, many data points come from each subject.

So:
* Each subject gets many objects at many distances which he has to
estimate.
* Each subject repeats this experiment using 4 colours of LEDs.
* Each subject repeats this experiment on 4 different sessions.
* Half the subjects do this under starlight, half under moonlight.
* Half the subjects do it with feedback and half without.

So some of these variables are within subjects and some between. I think
lme is a good way to proceed. But I am hung up on how to specify the
model

fit<-lme(fixed=logestimate~logdistance*session*illum*feedback,
random=???|subject???, data=df1)

I am familiar with the steps of model building using lm(), exploring
different models etc, so I think I will be OK once I get the idea of
specifying the basic lme model.

I have Pinheiro and Bates (2000) here.

Thanks very much for any help

Bill Simpson



From dmbates at gmail.com  Tue Jan 17 20:17:57 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 17 Jan 2006 13:17:57 -0600
Subject: [R] fitted values from lmer (lme4 0.98)
In-Reply-To: <43CC12AD.3080304@mail.la.utexas.edu>
References: <43CC12AD.3080304@mail.la.utexas.edu>
Message-ID: <40e66e0b0601171117p2ed8220ege618bdceba84be46@mail.gmail.com>

On 1/16/06, Daniel A. Powers <dpowers at mail.la.utexas.edu> wrote:
>
> -- R-List
>
> Can someone tell me how to get fitted values etc. after fitting lmer?
> for example, from lme, I can fit mod.1 <- lme(....) and get fitted values, coefficients, etc. in this way
>
> mod.1$fitted[,1] or mod.1$fitted[,2] etc.
>
> It seems lmer uses "slots" that are unfamiliar to me.

The preferred way is to use  the extractor functions fitted, fixef,
ranef and coef.

Using the recently uploaded version 0.995-1 of the Matrix package we get

> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy
      AIC      BIC    logLik MLdeviance REMLdeviance
 1753.628 1769.593 -871.8141   1751.986     1743.628
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.090  24.7405
          Days         35.072   5.9221  0.066
 Residual             654.941  25.5918
# of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept) 251.4051     6.8246  36.838
Days         10.4673     1.5458   6.771

Correlation of Fixed Effects:
     (Intr)
Days -0.138
> fixef(fm1)
(Intercept)        Days
  251.40510    10.46729
> ranef(fm1)
An object of class $-1????lmer.ranef????
[[1]]
    (Intercept)        Days
308   2.2585636   9.1989720
309 -40.3985870  -8.6197013
310 -38.9602563  -5.4488780
330  23.6905071  -4.8143326
331  22.2602104  -3.0698958
332   9.0395288  -0.2721711
333  16.8404364  -0.2236253
334  -7.2325817   1.0745765
335  -0.3336930 -10.7521594
337  34.8903592   8.6282824
349 -25.2101185   1.1734156
350 -13.0699625   6.6142058
351   4.5778374  -3.0152575
352  20.8635979   3.5360123
369   3.2754538   0.8722165
370 -25.6128786   4.8224661
371   0.8070403  -0.9881552
372  12.3145428   1.2840291

> coef(fm1)
$Subject
    (Intercept)       Days
308    253.6637 19.6662580
309    211.0065  1.8475846
310    212.4448  5.0184079
330    275.0956  5.6529533
331    273.6653  7.3973901
332    260.4446 10.1951148
333    268.2455 10.2436606
334    244.1725 11.5418624
335    251.0714 -0.2848734
337    286.2955 19.0955683
349    226.1950 11.6407015
350    238.3351 17.0814918
351    255.9829  7.4520285
352    272.2687 14.0032983
369    254.6806 11.3395024
370    225.7922 15.2897520
371    252.2121  9.4791308
372    263.7196 11.7513151

> fitted(fm1)
  [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950 371.6612 391.3275
  [9] 410.9937 430.6600 211.0065 212.8541 214.7017 216.5493 218.3969 220.2444
 [17] 222.0920 223.9396 225.7872 227.6348 212.4448 217.4633 222.4817 227.5001
 [25] 232.5185 237.5369 242.5553 247.5737 252.5921 257.6105 275.0956 280.7486
 [33] 286.4015 292.0545 297.7074 303.3604 309.0133 314.6663 320.3192 325.9722
 [41] 273.6653 281.0627 288.4601 295.8575 303.2549 310.6523 318.0497 325.4470
 [49] 332.8444 340.2418 260.4446 270.6397 280.8349 291.0300 301.2251 311.4202
 [57] 321.6153 331.8104 342.0056 352.2007 268.2455 278.4892 288.7329 298.9765
 [65] 309.2202 319.4638 329.7075 339.9512 350.1948 360.4385 244.1725 255.7144
 [73] 267.2562 278.7981 290.3400 301.8818 313.4237 324.9656 336.5074 348.0493
 [81] 251.0714 250.7865 250.5017 250.2168 249.9319 249.6470 249.3622 249.0773
 [89] 248.7924 248.5076 286.2955 305.3910 324.4866 343.5822 362.6777 381.7733
 [97] 400.8689 419.9644 439.0600 458.1556 226.1950 237.8357 249.4764 261.1171
[105] 272.7578 284.3985 296.0392 307.6799 319.3206 330.9613 238.3351 255.4166
[113] 272.4981 289.5796 306.6611 323.7426 340.8241 357.9056 374.9871 392.0686
[121] 255.9829 263.4350 270.8870 278.3390 285.7911 293.2431 300.6951 308.1471
[129] 315.5992 323.0512 272.2687 286.2720 300.2753 314.2786 328.2819 342.2852
[137] 356.2885 370.2918 384.2951 398.2984 254.6806 266.0201 277.3596 288.6991
[145] 300.0386 311.3781 322.7176 334.0571 345.3966 356.7361 225.7922 241.0820
[153] 256.3717 271.6615 286.9512 302.2410 317.5307 332.8205 348.1102 363.4000
[161] 252.2121 261.6913 271.1704 280.6495 290.1287 299.6078 309.0869 318.5661
[169] 328.0452 337.5243 263.7196 275.4710 287.2223 298.9736 310.7249 322.4762
[177] 334.2275 345.9789 357.7302 369.4815

I hope this helps.

Doug Bates



From fredrik.bg.lundgren at bredband.net  Tue Jan 17 20:26:16 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Tue, 17 Jan 2006 20:26:16 +0100
Subject: [R] help with parsing multiple coxph() results
References: <003901c61b96$71602ad0$2b99db82@Leomobile>
Message-ID: <000c01c61b9b$e33f86f0$4a9d72d5@Larissa>

Hao,

I'm not sure but you have specified your modell as tautology. The 
formula below should be enough:

survtest <- coxph(Surv(fup_interval, endpoint) ~ geneid, data = 
pcc.primary.stg.3.cox)

HTH
Fredrik
----- Original Message ----- 
From: "Hao Liu" <liuha at umdnj.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 17, 2006 7:47 PM
Subject: [R] help with parsing multiple coxph() results


> Dear All:
>
> I have a question on using coxph for multiple genes:
>
> I have written code to loop through all 22283 genes in the Hgu-133A 
> and
> apply coxph on survival data.
>
> However, I don't know how to work with the result for each gene:
>
> survtest<-coxph(Surv(pcc.primary.stg.3.cox[,'fup_interval'],pcc.primary.stg.
> 3.cox[,'endpoint'])~pcc.primary.stg.3.cox[,'geneid'],pcc.primary.stg.3.cox)
>
> each time I tried to look at what is in survtest it gives me this:
>
> ============================================================================
> ==============
> coxph(formula = Surv(pcc.primary.stg.3.cox[, "fup_interval"],
>    pcc.primary.stg.3.cox[, "endpoint"]) ~ pcc.primary.stg.3.cox[,
>    "208181_at"], data = pcc.primary.stg.3.cox)
>
>
>                                      coef exp(coef) se(coef)     z 
> p
> pcc.primary.stg.3.cox[, "208181_at"] -1.87     0.154    0.688 -2.72 
> 0.0065
>
> Likelihood ratio test=8.56  on 1 df, p=0.00343  n= 48
> ============================================================================
> ===============
>
> What I wanted to do is to use a matrix to store each "survtest" 
> result, but
> it seems to me there is no data
> structure in R to store the result of coxph into a matrix. I got the
> following code to calculate a P value
> based on "survtest"
> =================================
> z<-survtest$coefficients/sqrt(surv$var)
> p<-2*(1-pnorm(abs(z)))
>
> then, what is the P value thus calculated?
> ===========================================
> The question I have are:
>
> 1. How do I access different parts of coxph result?
> 2. Is there a way to store multiple coxph results into a data 
> structure that
> can be efficiently accessed?
> 3. if I find a list of genes I am interested, are there efficient to 
> plot
> all of them based on the survial data?
>
> Thanks
> Hao Liu, Ph. D
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mbock at Environcorp.com  Tue Jan 17 20:37:13 2006
From: mbock at Environcorp.com (Mike Bock)
Date: Tue, 17 Jan 2006 13:37:13 -0600
Subject: [R] Cumulative Density Plots (Hmisc/lattice)
Message-ID: <EB693868E54B314483E8E28E8BAAC6EF042E8E@chisrv01.environchicago.environ.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/6c5807a3/attachment.pl

From liuha at umdnj.edu  Tue Jan 17 20:38:27 2006
From: liuha at umdnj.edu (Hao Liu)
Date: Tue, 17 Jan 2006 14:38:27 -0500
Subject: [R]  help with parsing multiple coxph() results
Message-ID: <004101c61b9d$968ee380$2b99db82@Leomobile>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/93fc1901/attachment.pl

From Chang_Shen at progressive.com  Tue Jan 17 20:45:52 2006
From: Chang_Shen at progressive.com (Chang Shen)
Date: Tue, 17 Jan 2006 14:45:52 -0500
Subject: [R] array question
In-Reply-To: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@LocalDomain>
Message-ID: <OF536BA5CA.219F731C-ON852570F9.006B8DDD-852570F9.006C921A@progressive.com>

Hi all,

I want to create an array of datetime.

If I have a datetime object dt

>dt <- strptime("10Jan2006 00:00:15", "%d%b%Y %H:%M:%S")
>dt
[1]"2006-01-10 00:00:15"

I want to make an array of dt, say 100 size.  I got those error.

[1] "2006-01-10 00:00:15"
> dtarray<-array(dt, dim=c(100));
Error in array(dt, dim = c(100)) : dim<- : dims [product 100] do not match
the length of object [9]


> dtarray<-array(, dim=c(100));
> dtarray[1]<-dt;
Warning message:
number of items to replace is not a multiple of replacement length
>


Any help?

Thanks



From deepayan.sarkar at gmail.com  Tue Jan 17 20:55:01 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 17 Jan 2006 13:55:01 -0600
Subject: [R] Cumulative Density Plots (Hmisc/lattice)
In-Reply-To: <EB693868E54B314483E8E28E8BAAC6EF042E8E@chisrv01.environchicago.environ.local>
References: <EB693868E54B314483E8E28E8BAAC6EF042E8E@chisrv01.environchicago.environ.local>
Message-ID: <eb555e660601171155ud0f2ef8v6b07208b65400907@mail.gmail.com>

On 1/17/06, Mike Bock <mbock at environcorp.com> wrote:
> I have been using the ECDF function in the Hmisc package to produce
> cumulative distribution function plots. The problem is that for small
> datasets the steps "look bad" (not my characterization but from the
> client). Is there a way to get the same information but smoothed? I have
> tried the densityplot (lattice), which gives a smoothed line, but this
> does not give the cumulative density.

Have you tried qqmath, perhaps with distribution=qunif? Except for the
transposition, the idea behind the plots are similar.

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From gregory.r.warnes at pfizer.com  Tue Jan 17 21:01:24 2006
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Tue, 17 Jan 2006 15:01:24 -0500
Subject: [R] gplots
Message-ID: <915D2D65A9986440A277AC5C98AA466F018637D9@groamrexm02.amer.pfizer.com>


The plotmeans() function uses the order of the factor levels.  To change the order in the plot, change the order of the factor levels.

For example:

> data(state)
> plotmeans(state.area ~ state.region)   

Plots the groups in the order

levels(state.region)

> levels(state.region)
[1] "Northeast"     "South"         "North Central" "West"         

You can change the order of the levels using the gdata:::reorder.factor function:

> state.region <- reorder.factor(state.region, c("Northeast", "North Central", "South" "West")),

Now, doing 

> plotmeans(state.area ~ state.region)   

will show the plot with the new order.

-Greg


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Stephen
> Sent: Monday, January 16, 2006 11:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] gplots
> 
> 
> Hi 
>  
> I am sure that this question has been asked before ... appologies in
> advance
>  
> This - which comes out very nicely - better than the commercial stuff.
> plotmeans (cdpy~Dodefordpy, Data = Dataset, connect = False, 
> minbar = 1,
> mean.labels = FALSE, col = "blue", barwidth = 1.5, barcol = "red",
> ci.label = FALSE, xlab="Onset", pch = 15, par(las =2)).
> 
> Only one snag I want to order the X axis by the way it is 
> sorted in the
> data not by the n size group categories.
> 
> The data are sorted by the Dodefordpy categories - not mumeric -
> confering to the way I'd like the x axis.
> 
> Hence how do I order the x axis not by n but by the way the file is
> ordered.
> 
>  
> 
> Thanks 
> 
> S
> 
>  
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    2.1            
> year     2005           
> month    12             
> day      20             
> svn rev  36812          
> language R              
> > 
> 
> 
> Nana Mail <http://mail.nana.co.il> - Get Your Free Personal 
> Outlook 2003
> Now
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From cb486 at cam.ac.uk  Tue Jan 17 21:02:11 2006
From: cb486 at cam.ac.uk (Becker Cordula)
Date: Tue, 17 Jan 2006 20:02:11 +0000
Subject: [R] log-linear analysis - calculate treatment effects
Message-ID: <D0F54E24-BB1C-484F-970B-278DEBE6E815@cam.ac.uk>

Dear all,

I have run a hierarchical log-linear analysis using loglin {stats}  
and came up with a specific model. loglin returns me the parameter  
estimates giving me an idea in which direction the observed  
frequencies deviate from the expected ones for my different factors.  
To assess the significance of any such effects I would need to  
calculate the ratio of the log-linear parameter estimate to its  
standard error (as explained in Howell: Statistical Methods for  
Psychology, 4th Edition, p. 637ff). Unfortunately, I have no idea and  
couldn't find out what this standard error refers to and how to  
calculate it or the ratio using R (well, the ratio is no problem if I  
have the standard error).

I would very much appreciate any help!

Thanks and best wishes, Cordula Becker

----------------------------------------------------
Dr. Cordula Becker

Department of Experimental Psychology
University of Cambridge
Downing Site
Cambridge  CB2 3EB
United Kingdom

Tel.: +44 (0)1223 339715
Email: cb486 at cam.ac.uk

http://vision.psychol.cam.ac.uk/cbecker/



From rreiss at sciences.com  Tue Jan 17 21:02:10 2006
From: rreiss at sciences.com (Richard Reiss)
Date: Tue, 17 Jan 2006 15:02:10 -0500
Subject: [R] Using values from a function with complex output
Message-ID: <24B7390A8EDF9440BF68F966B17B53E40413DB@sii.sciences.com>



I am using the intervals function with a gnls (example below).  How can
I use some of the coefficients below in another calculation?  Or asked
another way, how do I index these values?

Thanks.

Rick


intervals(yy$fit[[1]])
Approximate 95% confidence intervals

 Coefficients:
               lower        est.       upper
A.sexF   -0.06399732 -0.05269831 -0.04139930
A.sexM   -0.07062259 -0.05942172 -0.04822085
BMD.sexF -2.36908925 -2.31640171 -2.26371416
BMD.sexM -2.33309959 -2.27979391 -2.22648823
attr(,"label")
[1] "Coefficients:"

 Variance function:
           lower       est.       upper
power -0.7957477 -0.4322209 -0.06869406
attr(,"label")
[1] "Variance function:"

 Residual standard error:
     lower       est.      upper 
0.02189063 0.02635919 0.03173990

 
_________________________________
Richard Reiss, Sc.D.
Vice President
Sciences International, Inc.
1800 Diagonal Road, Suite 500
Alexandria, VA  22314
phone: (703) 684-0123
fax: (703) 684-2223
Visit our new website at www.sciences.com



From liuha at umdnj.edu  Tue Jan 17 21:03:57 2006
From: liuha at umdnj.edu (Hao Liu)
Date: Tue, 17 Jan 2006 15:03:57 -0500
Subject: [R] help with parsing multiple coxph() results
In-Reply-To: <000c01c61b9b$e33f86f0$4a9d72d5@Larissa>
Message-ID: <004b01c61ba1$26b46900$2b99db82@Leomobile>

Thanks, I can do calculatation for each gene, however what I want to do is
to fit the model on each and every gene, store their result and then be able
to access all of them, then filter on them, to do that:

1. I need to know how to access each part of the coxph result, like its
coefficient, exp(coef),P,Z, etc.
2. I need to be able to generate a graphical presentation of the filtered
genes, to show how their expression level is associated with survival, etc.

I just could not find any explanation as to how to check different part of a
coxph result, not to mention to store them in certain data structure, like
vectors Java or Perl, etc.

Best
Hao

-----Original Message-----
From: Fredrik Lundgren [mailto:fredrik.bg.lundgren at bredband.net] 
Sent: Tuesday, January 17, 2006 2:26 PM
To: Hao Liu
Cc: R-help
Subject: Re: [R] help with parsing multiple coxph() results

Hao,

I'm not sure but you have specified your modell as tautology. The formula
below should be enough:

survtest <- coxph(Surv(fup_interval, endpoint) ~ geneid, data =
pcc.primary.stg.3.cox)

HTH
Fredrik
----- Original Message -----
From: "Hao Liu" <liuha at umdnj.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 17, 2006 7:47 PM
Subject: [R] help with parsing multiple coxph() results


> Dear All:
>
> I have a question on using coxph for multiple genes:
>
> I have written code to loop through all 22283 genes in the Hgu-133A 
> and
> apply coxph on survival data.
>
> However, I don't know how to work with the result for each gene:
>
>
survtest<-coxph(Surv(pcc.primary.stg.3.cox[,'fup_interval'],pcc.primary.stg.
>
3.cox[,'endpoint'])~pcc.primary.stg.3.cox[,'geneid'],pcc.primary.stg.3.cox)
>
> each time I tried to look at what is in survtest it gives me this:
>
>
============================================================================
> ==============
> coxph(formula = Surv(pcc.primary.stg.3.cox[, "fup_interval"],
>    pcc.primary.stg.3.cox[, "endpoint"]) ~ pcc.primary.stg.3.cox[,
>    "208181_at"], data = pcc.primary.stg.3.cox)
>
>
>                                      coef exp(coef) se(coef)     z 
> p
> pcc.primary.stg.3.cox[, "208181_at"] -1.87     0.154    0.688 -2.72 
> 0.0065
>
> Likelihood ratio test=8.56  on 1 df, p=0.00343  n= 48
>
============================================================================
> ===============
>
> What I wanted to do is to use a matrix to store each "survtest" 
> result, but
> it seems to me there is no data
> structure in R to store the result of coxph into a matrix. I got the
> following code to calculate a P value
> based on "survtest"
> =================================
> z<-survtest$coefficients/sqrt(surv$var)
> p<-2*(1-pnorm(abs(z)))
>
> then, what is the P value thus calculated?
> ===========================================
> The question I have are:
>
> 1. How do I access different parts of coxph result?
> 2. Is there a way to store multiple coxph results into a data 
> structure that
> can be efficiently accessed?
> 3. if I find a list of genes I am interested, are there efficient to 
> plot
> all of them based on the survial data?
>
> Thanks
> Hao Liu, Ph. D
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From droberts at montana.edu  Tue Jan 17 21:08:23 2006
From: droberts at montana.edu (Dave Roberts)
Date: Tue, 17 Jan 2006 13:08:23 -0700
Subject: [R] Clustering function
In-Reply-To: <Prayer.1.0.16.0601171814480.21551@hermes-2.csi.cam.ac.uk>
References: <Prayer.1.0.16.0601171814480.21551@hermes-2.csi.cam.ac.uk>
Message-ID: <43CD4EB7.6070406@montana.edu>

Norman,

     You're missing a step.  You need to convert the data file into a 
'dist' object, which is either a distance or dissimilarity matrix.  This 
is typically done by function dist(), but may also be done by other 
functions which produce dist objects, like daisy() in package cluster, 
vegdist() in package vegan or dsvdis() in package labdsv.  You then give 
the dist object as an argument to the hclust() function, which I believe 
is core R.

     Alternatively, function agnes() in package cluster produces a 
classification directly from a data matrix.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460


N. Goodacre wrote:
> Dear mailing group,
> 
>   I have loaded an Excel file into R by calling it ".csv" and using the 
> "read.csv" function in R. However then I want to use the (limma package 
> specific, I believe) function "hclust", which clusters data in a tree 
> dendrogram, by similarity. However, I receive the errors msg.s: 1) "missing 
> observations in cov/cor" 2) "In addition: warning message: NAs introduced 
> by coercion"
> 
> The excel file does have missing data slots. Do these tow messages mean 
> that it has written "NA" where the missing slots were? Or is there a more 
> fundamental error?
> 
>  thanks.
> 
> -Norman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From cberry at tajo.ucsd.edu  Tue Jan 17 21:29:08 2006
From: cberry at tajo.ucsd.edu (Chuck Berry)
Date: Tue, 17 Jan 2006 20:29:08 +0000 (UTC)
Subject: [R] new comer's  question
References: <OF73268F31.D74782A4-ON852570F8.006E93DF-852570F8.0070A71C@progressive.com>
Message-ID: <loom.20060117T202717-58@post.gmane.org>

Chang Shen <Chang_Shen <at> progressive.com> writes:

> 
> 
> I am new to R.  I try to search the web but could not find the answer so I
> post it here asking for help.

Broad web searches may not be as helpful as following the advice in the posting
guide:

   http://www.R-project.org/posting-guide.html

Following the advice to "Do help.search("keyword") with different keywords (type
this at the R prompt)" would have led you to the help page for 'levels'. More on
this below.

> 
> I have a csv file looks like this: (between two ==== lines)
> ===========================
> 
> Machine Name,"Resource, Type","Resource, Sub-type","Resource,

The line wrapping here and below makes it harder for folks to help you!!

> Instance",Date,,Data ->,,,,,,
> ,0.041666667,,,,,,,,,,,
> 
> Time (HH:MM) ->,,,,,,0:00,0:15,0:30,0:45,1:00,1:15,1:30
> SCINFR06,Cache,Copy Read Hits %,,10-Jan-06,Cache->Copy Read Hits
> %,0.99,1,1,1,1,1,0.99
> SCINFR06,Cache,Data Map Hits %,,10-Jan-06,Cache->Data Map Hits
> %,1,1,1,1,1,1,1
> 
> Time (HH:MM) ->,,,,,,0:00,0:15,0:30,0:45,1:00,1:15,1:30
> SCINFR06,LogicalDisk,% Disk Read Time,C:,10-Jan-06,LogicalDisk->% Disk Read
> Time->C:,2.14,1.52,1.94,1.68,2.52,2.05,2.66
> SCINFR06,LogicalDisk,% Disk Read Time,D:,10-Jan-06,LogicalDisk->% Disk Read
> Time->D:,0.04,0,0,0.08,0,0,0
> SCINFR06,LogicalDisk,% Disk Read
> Time,HarddiskVolume1,10-Jan-06,LogicalDisk->% Disk Read
> Time->HarddiskVolume1,0,0,0,0,0,0,0
> SCINFR06,LogicalDisk,% Disk Read Time,_Total,10-Jan-06,LogicalDisk->% Disk
> Read Time->_Total,0.72,0.51,0.65,0.59,0.84,0.68,0.89
> 
> ==============================
> 
> First I load it by read.table call:
> 
> myArray <- read.table("c:/mydata.csv",sep=",");
> 

Was the first row intended as a 'header'. If so, 

 myArray <- read.table("c:/mydata.csv", sep="," , header=TRUE )

might be a better start.

FYI, 'myArray' is not an array:

R > is.array(myArray)
[1] FALSE

If this surprises you, you need to review the read.table help page and the "R
Data Import/Export Manual" Chapter 2.

> After this,  the array element myArray[1,2] looks like this
> 
> >myArray[1,2]
> [1] Resource, Type
> Levels:  0.041666667 Cache LogicalDisk Resource, Type
> 
> Here are the questions:
> (1) What does Levels mean?

It looks like the 'levels' attribute printed in a formaated style:

R > levels( myArray[,2] )
[1] ""               "0.041666667"    "Cache"          "LogicalDisk"   
[5] "Resource, Type"
 



> (2) When I try to split the string "Resource, Type", which is myArray[1,2],

myArray[1,2] is NOT a 'string'.

R > is.character( myArray[1,2] )
[1] FALSE

but it is a 'factor':

R > class( myArray[1,2] )
[1] "factor"


> using function strsplit(), I got error:
> 
> > w<-strsplit(myArray[1,2],",")
> Error in strsplit(x, as.character(split), as.logical(extended),
> as.logical(fixed),  :
>         non-character argument in strsplit()
> 
> Then I tried this:
> 
> > y<-myArray[1,2]
> > y
> [1] Resource, Type
> Levels:  0.041666667 Cache LogicalDisk Resource, Type
> > w<-strsplit(y,",")
> Error in strsplit(x, as.character(split), as.logical(extended),
> as.logical(fixed),  :
>         non-character argument in strsplit()
> 
> But the following call does not cause any error.
> 
> >  y<-"Resource, Type"
> > w<-strsplit(y,",")
> > w
> [[1]]
> [1] "Resource" " Type"
> 
> what is wrong?

You need to convert the 'factor' object to a 'character' object:

R > strsplit( as.character( myArray[1,2] ), "," )
[[1]]
[1] "Resource" " Type"   


> 
> Thanks
> 
> Chang
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From martin.julien.2 at courrier.uqam.ca  Tue Jan 17 21:33:57 2006
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Tue, 17 Jan 2006 15:33:57 -0500
Subject: [R] Anova problem with order of terms in model
In-Reply-To: <200505161009.j4GA2nAH006949@hypatia.math.ethz.ch>
Message-ID: <000001c61ba5$5a8817a0$5044d084@TAMIAS>

Hi 
I have a linear model and I want to tests whether the model terms are
significant
I used anova but F and P value depend on the order of the terms in the
model.
I have repeated the same analysis in another stat software and F and P value
did not differ with order of terms in the model.
Can anyone can explain what happen or what am I doing wrong ?

In R
> anova (lm(Y~X.1+X.2))
Analysis of Variance Table
Response: Y
		Df	Sum Sq		Mean Sq	F value		Pr(>F)
X.1		1	4.0351		4.0351		6.5187
0.01852
X.2		1	0.5903		0.5903		0.9537
0.33991
Residuals	21	12.9991		0.619

> anova (lm(Y~X.2+X.1))
Analysis of Variance Table
Response: Y
		Df	Sum Sq		Mean Sq	F value		Pr(>F)
X.2		1	1.9651		1.9651		3.1747
0.08926
X.1		1	2.6603		2.6603		4.2977
0.05066
Residuals	21	12.9991		0.619		

With another stat software (JMP)
Y~X.1+X.2
		Df	Sum Sq		Mean Sq	F value		Pr(>F)
X.1		1	2.6603		2.6603		4.2977
0.05066
X.2		1	0.5903		0.5903		0.9537
0.33991
Residuals	21	12.9991		0.619		

Y~X.2+X.1
		Df	Sum Sq		Mean Sq	F value		Pr(>F)
X.2		1	0.5903		0.5903		0.9537
0.33991
X.1		1	2.6603		2.6603		4.2977
0.05066
Residuals	21	12.9991		0.619		


---------------------------------------------------------------------------
Julien Martin

Canadian Research Chair in behavioural ecology
Universit?? du Qu??bec ?? Montr??al

Email: martin.julien.2 at courrier.uqam.ca



From marinak at uchicago.edu  Tue Jan 17 21:45:18 2006
From: marinak at uchicago.edu (Marina Niessner)
Date: Tue, 17 Jan 2006 14:45:18 -0600
Subject: [R] Bivariate Normal Variables and correlation
Message-ID: <200601172042.k0HKg0HQ002706@relay01.uchicago.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/c9b1384e/attachment.pl

From pzs6 at CDC.GOV  Tue Jan 17 21:41:26 2006
From: pzs6 at CDC.GOV (Smith, Phil)
Date: Tue, 17 Jan 2006 15:41:26 -0500
Subject: [R] Step.glm() question
Message-ID: <2554377D323C9A40BD69956059FD05490ED4529C@mnip1>

Hi:

I am using step.glm() as follows:

form1 	<- 	as.formula(haspdata ~ 1)
lg.mod1 	<- 	glm ( formula=form1, data=st.mtx,
family=binomial , na.action=na.omit ) 

upper		<-	as.formula( haspdata ~ (
c5+childnm+educ1+incpov1+marital+msa+racekid+racemom+sex+shotcard )^2)
lower		<-	as.formula(haspdata~1)
lst		<-	list( upper=upper , lower=lower )
form1a.step	<-	step(lg.mod1, scope=lst , trace=T ,
direction="both" )

I get the following error message in R: 

Error in factor.scope(ffac, list(add = fadd, drop = fdrop)) : 
        upper scope does not include model

Because I am trying to migrate from Splus to R, I ran that same code in
Splus and do not get an error.

Could someone please tell me:
	1) what I'm doing wrong in R, and
	2) what the distinction between R and Splus is in this case.

Thank you,

Philip J. Smith, PhD
Centers for Disease Control and Prevention
National Immunization Program
Immunization Services Division
MS E-32
1600 Clifton Road, NE
Atlanta, GA 30333
ph: 404 639 8729
fax: 404 639 3266
pzs6 at cdc.gov



From gunter.berton at gene.com  Tue Jan 17 21:54:17 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 17 Jan 2006 12:54:17 -0800
Subject: [R] Vector indices
In-Reply-To: <20060117182243.28779.qmail@web25812.mail.ukl.yahoo.com>
Message-ID: <200601172054.k0HKsHVK024082@ohm.gene.com>

As you apparently haven't received any answer yet ...

Assuming your "table" is a data frame (str() will tell you), I believe you
are confusing the row names with indices. Please read the docs on data frame
for details (?data.frame). Row names are character strings that can be
indexed as such:

> test<-data.frame(x=1:4,y=letters[1:4])
> row.names(test)
[1] "1" "2" "3" "4"

> test
  x y
1 1 a
2 2 b
3 3 c
4 4 d

> test[2,]
  x y
2 2 b

> test['2',]
  x y
2 2 b

> test[2,1]<-NA
> test
   x y
1  1 a
2 NA b
3  3 c
4  4 d

> test2<-na.omit(test)
> test2
  x y
1 1 a
3 3 c
4 4 d

> test2[2,]
  x y
3 3 c

> test2['2',]
    x    y
NA NA <NA>

> test2['3',]
  x y
3 3 c


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Werner 
> Wernersen
> Sent: Tuesday, January 17, 2006 10:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Vector indices
> 
> Hi,
>   
>   I am despairing of getting the indices for a vector:
>   First, I have a table from which I kick out a number of 
> rows with  na.omit. Next, I use this table for clustering 
> with kmeans and  cl$cluster contains my clusters. The 
> cl$cluster is a vector which still  contains my original 
> indices from the very beginning, a kind of like an  
> "associative" array. But how can I get a list of only these 
> indices?  When I use which() on the data it returns new 
> indices starting from 1  to N together with the "old" indices 
> but the "old" ones remain  unaccessible. 
>   
>   It would be great if someone could give me a hand with that.
>   
>   Thanks,
>     Werner
>   
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Tue Jan 17 22:00:38 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jan 2006 22:00:38 +0100
Subject: [R] fitted values from lmer (lme4 0.98)
In-Reply-To: <40e66e0b0601171117p2ed8220ege618bdceba84be46@mail.gmail.com>
References: <43CC12AD.3080304@mail.la.utexas.edu>
	<40e66e0b0601171117p2ed8220ege618bdceba84be46@mail.gmail.com>
Message-ID: <x2bqya36yx.fsf@turmalin.kubism.ku.dk>

Douglas Bates <dmbates at gmail.com> writes:

> On 1/16/06, Daniel A. Powers <dpowers at mail.la.utexas.edu> wrote:
> >
> > -- R-List
> >
> > Can someone tell me how to get fitted values etc. after fitting lmer?
> > for example, from lme, I can fit mod.1 <- lme(....) and get fitted values, coefficients, etc. in this way
> >
> > mod.1$fitted[,1] or mod.1$fitted[,2] etc.
> >
> > It seems lmer uses "slots" that are unfamiliar to me.
> 
> The preferred way is to use  the extractor functions fitted, fixef,
> ranef and coef.
> 
> Using the recently uploaded version 0.995-1 of the Matrix package we get

But, at least with 0.99-6 (sorry for falling way behind, but I'd
rather not upgrade just now...), fitted(fm1) are effectively BLUPs. If
you want to get the estimated mean values, you need something like

model.matrix(fm1 at terms,sleepstudy) %*% fixef(fm1)

I realize that since lmer models are not necessarily hierarchical, it
takes more than cloning the "level" argument from fitted.lme, but it
could be useful to at least have a "level=0" equivalent.
 
> > (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> Linear mixed-effects model fit by REML
> Formula: Reaction ~ Days + (Days | Subject)
>    Data: sleepstudy
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  1753.628 1769.593 -871.8141   1751.986     1743.628
> Random effects:
>  Groups   Name        Variance Std.Dev. Corr
>  Subject  (Intercept) 612.090  24.7405
>           Days         35.072   5.9221  0.066
>  Residual             654.941  25.5918
> # of obs: 180, groups: Subject, 18
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept) 251.4051     6.8246  36.838
> Days         10.4673     1.5458   6.771
> 
> Correlation of Fixed Effects:
>      (Intr)
> Days -0.138
> > fixef(fm1)
> (Intercept)        Days
>   251.40510    10.46729
> > ranef(fm1)
> An object of class $-1????lmer.ranef????
> [[1]]
>     (Intercept)        Days
> 308   2.2585636   9.1989720
> 309 -40.3985870  -8.6197013
> 310 -38.9602563  -5.4488780
> 330  23.6905071  -4.8143326
> 331  22.2602104  -3.0698958
> 332   9.0395288  -0.2721711
> 333  16.8404364  -0.2236253
> 334  -7.2325817   1.0745765
> 335  -0.3336930 -10.7521594
> 337  34.8903592   8.6282824
> 349 -25.2101185   1.1734156
> 350 -13.0699625   6.6142058
> 351   4.5778374  -3.0152575
> 352  20.8635979   3.5360123
> 369   3.2754538   0.8722165
> 370 -25.6128786   4.8224661
> 371   0.8070403  -0.9881552
> 372  12.3145428   1.2840291
> 
> > coef(fm1)
> $Subject
>     (Intercept)       Days
> 308    253.6637 19.6662580
> 309    211.0065  1.8475846
> 310    212.4448  5.0184079
> 330    275.0956  5.6529533
> 331    273.6653  7.3973901
> 332    260.4446 10.1951148
> 333    268.2455 10.2436606
> 334    244.1725 11.5418624
> 335    251.0714 -0.2848734
> 337    286.2955 19.0955683
> 349    226.1950 11.6407015
> 350    238.3351 17.0814918
> 351    255.9829  7.4520285
> 352    272.2687 14.0032983
> 369    254.6806 11.3395024
> 370    225.7922 15.2897520
> 371    252.2121  9.4791308
> 372    263.7196 11.7513151
> 
> > fitted(fm1)
>   [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950 371.6612 391.3275
>   [9] 410.9937 430.6600 211.0065 212.8541 214.7017 216.5493 218.3969 220.2444
>  [17] 222.0920 223.9396 225.7872 227.6348 212.4448 217.4633 222.4817 227.5001
>  [25] 232.5185 237.5369 242.5553 247.5737 252.5921 257.6105 275.0956 280.7486
>  [33] 286.4015 292.0545 297.7074 303.3604 309.0133 314.6663 320.3192 325.9722
>  [41] 273.6653 281.0627 288.4601 295.8575 303.2549 310.6523 318.0497 325.4470
>  [49] 332.8444 340.2418 260.4446 270.6397 280.8349 291.0300 301.2251 311.4202
>  [57] 321.6153 331.8104 342.0056 352.2007 268.2455 278.4892 288.7329 298.9765
>  [65] 309.2202 319.4638 329.7075 339.9512 350.1948 360.4385 244.1725 255.7144
>  [73] 267.2562 278.7981 290.3400 301.8818 313.4237 324.9656 336.5074 348.0493
>  [81] 251.0714 250.7865 250.5017 250.2168 249.9319 249.6470 249.3622 249.0773
>  [89] 248.7924 248.5076 286.2955 305.3910 324.4866 343.5822 362.6777 381.7733
>  [97] 400.8689 419.9644 439.0600 458.1556 226.1950 237.8357 249.4764 261.1171
> [105] 272.7578 284.3985 296.0392 307.6799 319.3206 330.9613 238.3351 255.4166
> [113] 272.4981 289.5796 306.6611 323.7426 340.8241 357.9056 374.9871 392.0686
> [121] 255.9829 263.4350 270.8870 278.3390 285.7911 293.2431 300.6951 308.1471
> [129] 315.5992 323.0512 272.2687 286.2720 300.2753 314.2786 328.2819 342.2852
> [137] 356.2885 370.2918 384.2951 398.2984 254.6806 266.0201 277.3596 288.6991
> [145] 300.0386 311.3781 322.7176 334.0571 345.3966 356.7361 225.7922 241.0820
> [153] 256.3717 271.6615 286.9512 302.2410 317.5307 332.8205 348.1102 363.4000
> [161] 252.2121 261.6913 271.1704 280.6495 290.1287 299.6078 309.0869 318.5661
> [169] 328.0452 337.5243 263.7196 275.4710 287.2223 298.9736 310.7249 322.4762
> [177] 334.2275 345.9789 357.7302 369.4815
> 
> I hope this helps.
> 
> Doug Bates
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Tue Jan 17 22:05:43 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 Jan 2006 22:05:43 +0100
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <13e802630601170850l38352147mb4be199b8b70e11f@mail.gmail.com>
References: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>	<43CCA1AD.6070805@statistik.uni-dortmund.de>
	<13e802630601170850l38352147mb4be199b8b70e11f@mail.gmail.com>
Message-ID: <43CD5C27.5090500@statistik.uni-dortmund.de>

Paul Johnson wrote:
> Thanks, Uwe
> 
> that clears up why I can't make R2WinBUGs work with OpenBUGS and WinBUGS1.5 :)
> Both work pretty good with Wine in a GUI.  I noticed that when I tried
> "rbugs", it does succeed in starting WinBUGS GUI, but then nothing
> happens. I'll get WinBUGS1.4 and see what happens.
> 
> In the meanwhile, I'm going to t ry to see what BRugs is good for. In
> Linux, when I try to install BRugs, the install fails with an error
> saying that, at the current time, BRugs works only in Windows.


Yes, I have added that particular line in order not to confuse users, 
and I thought I told you in my last message that it works only under 
Windows.


> * Installing *source* package 'BRugs' ...
> Package 'BRugs' currently only works under Windows.\nIt is supposed to
> work under Linux in future releases.
> 
> I'd like to stop that check and see what happens!  

OK, just remove the configure file.


> The way I read the
> sourcecode from OpenBUGS and BRugs, I need to replace the windows dll
> install and instead put in an so file (as in OpenBUGS).

Yes, it is already shipped, and the infrastructure in the package is 
ready (hopefully), but the brugs.so file does not work as expected.


> If anybody has done this, please let me know of your experience.

Yes, several tried, among them Andrew Thomas and Uwe Ligges, and then I 
invited Andrew Thomas to Dortmund and we tried together (I have to admit 
that I was clueless all the time and in fact Andrew tried).
Andrew's conclusion was that there is some compiler problem on Linux 
with the BlackBox framework (Component Pascal compiler from Oberon 
microsystems) in which WinBUGS/OpenBUGS is written in ...

If you get it to work, please let us know!

Uwe




> 
> On 1/17/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Paul Johnson wrote:
>>
>>>Greetings:
>>>
>>>I'm going to encourage some students to try Bayesian ideas for
>>>hierarchical models.
>>>I want to run the WinBUGS and R examples in Tony Lancaster's An
>>>Introduction to Modern Bayesian Econometrics.  That features MS
>>>Windows and "bugs" from R2WinBUGS.
>>>
>>>Today, I want to ask how people are doing this in Linux? I have found
>>>a plethora of possibilities, some of which are not quite ready, some
>>>of which work only under MS Windows.  Right now I just want to know
>>>"what actually works".
>>>
>>>Here's where I stand now in Fedora Core 4 Linux.
>>>1. OpenBUGS-2.1.1 runs in Linux.  I can run "linbugs" (the console
>>>version similar to the old BUGS) and also I can run--under wine--the
>>>newest version of "winbugs.exe" that is circulated with OpenBUGS.  As
>>>far as I can tell, the graphical interface in wine/winbugs works in
>>>almost all elements.  A few things seem not quite right in the GUI
>>>(can't initialize more than one chain, difficult to specify variables
>>>for monitoring), but it does work.
>>>
>>>It is easier to install and work with OpenBUGS's version of
>>>winbugs.exe than with Winbugs-1.4 because the Open version does not
>>>have that annoying license registration and "winbugs.exe" is not
>>>wrapped inside an installation script.   I'm a little confused about
>>>WinBUGS versions because the BRugs documents
>>>http://www.biostat.umn.edu/~brad/software/BRugs/BRugs_install.html
>>>refer to WinBUGS-1.5, which refers to
>>>http://www.biostat.umn.edu/~brad/software/BRugs/WinBUGS15.zip, which
>>>can be downloaded without any of the registration steps, but WinBUGS15
>>>is not mentioned in the WinBUGS site (where 1.4.1 appears to be the
>>>newest).
>>>
>>>Supposing I get the winbugs.exe question settled:
>>>
>>>2. How to most dependably send jobs from R to "linbugs" or "winbugs.exe"?
>>>
>>>The BRugs package is preferred?
>>>
>>>For a long time, R2WinBUGS was Windows-only, but toward the end of
>>>last fall I noticed that R2WinBUGS now does compile and install under
>>>R in Linux.
>>>
>>>however, its help still says:
>>>SystemRequirements:   WinBUGS 1.4 on Windows
>>>
>>>I'd appreciate any advice.
>>
>>[resend to less recipients in order to save Martin's spare time to
>>approve message;
>>CCing Andrew Thomas, Bob O'Hara and Sibylle Sturtz separately]
>>
>>
>>
>>
>>Re BUGS:
>>WinBUGS-1.5 never got really released, AFAIK - Andrew or Bob might want
>>to correct me. It has been renamed to OpenBUGS. The current version is
>>the GPL'ed OpenBUGS 2.1.1 available from
>>http://mathstat.helsinki.fi/openbugs/.
>>
>>Re R packages:
>>- R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
>>can speak with WinBUGS under wine thanks to user contributions. But it
>>still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
>>Windows).
>>- BRugs contains the BRugs interface, R functions and the whole OpenBUGS
>>installation. Unfortunately, due to serious compiler problems, we were
>>not able to get a Linux version running using the interface. Hence it
>>was not possible to release any non-Windows version up to now.
>>I haven't tested BRugs under wine yet (in which case R has to run under
>>wine as well, of course) ... and I do not know if there are any serious
>>performance penalties.
>>Note that even in the long term, OpenBUGS will only run on x86 based
>>platforms.
>>
>>Due to the much more flexibile interface, I prefer BRugs.
>>
>>BTW: "Real programmers" won't consider R2WinBUGS to be an "interface" at
>>all - it might be useful, though. ;-)
>>
>>
>>Uwe Ligges
>>
>>
>>
>>
>>>--
>>>Paul E. Johnson
>>>Professor, Political Science
>>>1541 Lilac Lane, Room 504
>>>University of Kansas
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
> 
> 
> --
> Paul E. Johnson
> Professor, Political Science
> 1541 Lilac Lane, Room 504
> University of Kansas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Tue Jan 17 22:12:31 2006
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 17 Jan 2006 16:12:31 -0500
Subject: [R] symbols function
Message-ID: <43CD5DBF.5000105@chass.utoronto.ca>

Hi

I do not get why the symbols function produces warnings when axes=F is 
added. The following example illustrate this

 > symbols(0,10, inches=T, circles=1, axes=F, xlab="", ylab="")
Warning message:
parameter "axes" could not be set in high-level plot() function


I augmented symbols and added the axes=F argument to the plot function 
inside the original symbols function. It works as expected, no warning 
message. I am just lost as to why the extra arguments in symbols (...) 
are not behaving as expected.


Jean



From gunter.berton at gene.com  Tue Jan 17 22:13:34 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 17 Jan 2006 13:13:34 -0800
Subject: [R] Anova problem with order of terms in model
In-Reply-To: <000001c61ba5$5a8817a0$5044d084@TAMIAS>
Message-ID: <200601172113.k0HLDZNZ012007@hertz.gene.com>

I suggest you consult a local statistician or read up on linear
models,perhaps by reading the relevant section of V&R's MASS. Also search
the R-Help archives for "Type III SS." Further comments below.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Julien
> Sent: Tuesday, January 17, 2006 12:34 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Anova problem with order of terms in model
> 
> Hi 
> I have a linear model and I want to tests whether the model terms are
> significant
> I used anova but F and P value depend on the order of the terms in the
> model.

Correct. Differennt hypotheses are being tested. R does **sequential** SS
(this preserves additivity/orthogonality of contrasts being tested). In a
non-orthogonal designs, these depend on order.  

> I have repeated the same analysis in another stat software 
> and F and P value
> did not differ with order of terms in the model.
> Can anyone can explain what happen or what am I doing wrong ?

And this "other software" is not doing a sequential SS, but rather what is
usually called Type III SS (google on "Type III SS" for more info).



From dmbates at gmail.com  Tue Jan 17 22:31:40 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 17 Jan 2006 15:31:40 -0600
Subject: [R] fitted values from lmer (lme4 0.98)
In-Reply-To: <x2bqya36yx.fsf@turmalin.kubism.ku.dk>
References: <43CC12AD.3080304@mail.la.utexas.edu>
	<40e66e0b0601171117p2ed8220ege618bdceba84be46@mail.gmail.com>
	<x2bqya36yx.fsf@turmalin.kubism.ku.dk>
Message-ID: <40e66e0b0601171331h77fc26d0kaf41feb0393608fb@mail.gmail.com>

On 17 Jan 2006 22:00:38 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Douglas Bates <dmbates at gmail.com> writes:
>
> > On 1/16/06, Daniel A. Powers <dpowers at mail.la.utexas.edu> wrote:
> > >
> > > -- R-List
> > >
> > > Can someone tell me how to get fitted values etc. after fitting lmer?
> > > for example, from lme, I can fit mod.1 <- lme(....) and get fitted values, coefficients, etc. in this way
> > >
> > > mod.1$fitted[,1] or mod.1$fitted[,2] etc.
> > >
> > > It seems lmer uses "slots" that are unfamiliar to me.
> >
> > The preferred way is to use  the extractor functions fitted, fixef,
> > ranef and coef.
> >
> > Using the recently uploaded version 0.995-1 of the Matrix package we get
>
> But, at least with 0.99-6 (sorry for falling way behind, but I'd
> rather not upgrade just now...), fitted(fm1) are effectively BLUPs. If
> you want to get the estimated mean values, you need something like
>
> model.matrix(fm1 at terms,sleepstudy) %*% fixef(fm1)
>
> I realize that since lmer models are not necessarily hierarchical, it
> takes more than cloning the "level" argument from fitted.lme, but it
> could be useful to at least have a "level=0" equivalent.

Would it help if there were an option in the fitted method to allow
for fixed-effects only versus fixed- and random-effects?  As you say,
because lmer models do not need to be hierarchical it is not obvious
what it would mean to include some but not all of the random effects
terms in the "fitted values".  However, it is easy and unambiguous to
define fitted values for the fixed-effects only.

Up until a few days ago there was an option to do this but then I
changed the calculation of the fitted values in an attempt to clean up
the code.  The calculation of the level = 0 fitted values in the new
representation of the fitted model is quite easy.  It is

fm1 at X %*% fixef(fm1)

(except for complications introduced by na.exclude)

>
> > > (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> > Linear mixed-effects model fit by REML
> > Formula: Reaction ~ Days + (Days | Subject)
> >    Data: sleepstudy
> >       AIC      BIC    logLik MLdeviance REMLdeviance
> >  1753.628 1769.593 -871.8141   1751.986     1743.628
> > Random effects:
> >  Groups   Name        Variance Std.Dev. Corr
> >  Subject  (Intercept) 612.090  24.7405
> >           Days         35.072   5.9221  0.066
> >  Residual             654.941  25.5918
> > # of obs: 180, groups: Subject, 18
> >
> > Fixed effects:
> >             Estimate Std. Error t value
> > (Intercept) 251.4051     6.8246  36.838
> > Days         10.4673     1.5458   6.771
> >
> > Correlation of Fixed Effects:
> >      (Intr)
> > Days -0.138
> > > fixef(fm1)
> > (Intercept)        Days
> >   251.40510    10.46729
> > > ranef(fm1)
> > An object of class $-1????lmer.ranef????
> > [[1]]
> >     (Intercept)        Days
> > 308   2.2585636   9.1989720
> > 309 -40.3985870  -8.6197013
> > 310 -38.9602563  -5.4488780
> > 330  23.6905071  -4.8143326
> > 331  22.2602104  -3.0698958
> > 332   9.0395288  -0.2721711
> > 333  16.8404364  -0.2236253
> > 334  -7.2325817   1.0745765
> > 335  -0.3336930 -10.7521594
> > 337  34.8903592   8.6282824
> > 349 -25.2101185   1.1734156
> > 350 -13.0699625   6.6142058
> > 351   4.5778374  -3.0152575
> > 352  20.8635979   3.5360123
> > 369   3.2754538   0.8722165
> > 370 -25.6128786   4.8224661
> > 371   0.8070403  -0.9881552
> > 372  12.3145428   1.2840291
> >
> > > coef(fm1)
> > $Subject
> >     (Intercept)       Days
> > 308    253.6637 19.6662580
> > 309    211.0065  1.8475846
> > 310    212.4448  5.0184079
> > 330    275.0956  5.6529533
> > 331    273.6653  7.3973901
> > 332    260.4446 10.1951148
> > 333    268.2455 10.2436606
> > 334    244.1725 11.5418624
> > 335    251.0714 -0.2848734
> > 337    286.2955 19.0955683
> > 349    226.1950 11.6407015
> > 350    238.3351 17.0814918
> > 351    255.9829  7.4520285
> > 352    272.2687 14.0032983
> > 369    254.6806 11.3395024
> > 370    225.7922 15.2897520
> > 371    252.2121  9.4791308
> > 372    263.7196 11.7513151
> >
> > > fitted(fm1)
> >   [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950 371.6612 391.3275
> >   [9] 410.9937 430.6600 211.0065 212.8541 214.7017 216.5493 218.3969 220.2444
> >  [17] 222.0920 223.9396 225.7872 227.6348 212.4448 217.4633 222.4817 227.5001
> >  [25] 232.5185 237.5369 242.5553 247.5737 252.5921 257.6105 275.0956 280.7486
> >  [33] 286.4015 292.0545 297.7074 303.3604 309.0133 314.6663 320.3192 325.9722
> >  [41] 273.6653 281.0627 288.4601 295.8575 303.2549 310.6523 318.0497 325.4470
> >  [49] 332.8444 340.2418 260.4446 270.6397 280.8349 291.0300 301.2251 311.4202
> >  [57] 321.6153 331.8104 342.0056 352.2007 268.2455 278.4892 288.7329 298.9765
> >  [65] 309.2202 319.4638 329.7075 339.9512 350.1948 360.4385 244.1725 255.7144
> >  [73] 267.2562 278.7981 290.3400 301.8818 313.4237 324.9656 336.5074 348.0493
> >  [81] 251.0714 250.7865 250.5017 250.2168 249.9319 249.6470 249.3622 249.0773
> >  [89] 248.7924 248.5076 286.2955 305.3910 324.4866 343.5822 362.6777 381.7733
> >  [97] 400.8689 419.9644 439.0600 458.1556 226.1950 237.8357 249.4764 261.1171
> > [105] 272.7578 284.3985 296.0392 307.6799 319.3206 330.9613 238.3351 255.4166
> > [113] 272.4981 289.5796 306.6611 323.7426 340.8241 357.9056 374.9871 392.0686
> > [121] 255.9829 263.4350 270.8870 278.3390 285.7911 293.2431 300.6951 308.1471
> > [129] 315.5992 323.0512 272.2687 286.2720 300.2753 314.2786 328.2819 342.2852
> > [137] 356.2885 370.2918 384.2951 398.2984 254.6806 266.0201 277.3596 288.6991
> > [145] 300.0386 311.3781 322.7176 334.0571 345.3966 356.7361 225.7922 241.0820
> > [153] 256.3717 271.6615 286.9512 302.2410 317.5307 332.8205 348.1102 363.4000
> > [161] 252.2121 261.6913 271.1704 280.6495 290.1287 299.6078 309.0869 318.5661
> > [169] 328.0452 337.5243 263.7196 275.4710 287.2223 298.9736 310.7249 322.4762
> > [177] 334.2275 345.9789 357.7302 369.4815
> >
> > I hope this helps.
> >
> > Doug Bates
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>



From tlumley at u.washington.edu  Tue Jan 17 22:37:36 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 17 Jan 2006 13:37:36 -0800 (PST)
Subject: [R] symbols function
In-Reply-To: <43CD5DBF.5000105@chass.utoronto.ca>
Message-ID: <Pine.LNX.4.43.0601171337360.7252@hymn06.u.washington.edu>

On Tue, 17 Jan 2006, Jean Eid wrote:

> Hi
>
> I do not get why the symbols function produces warnings when axes=F is
> added. The following example illustrate this
>
> > symbols(0,10, inches=T, circles=1, axes=F, xlab="", ylab="")
> Warning message:
> parameter "axes" could not be set in high-level plot() function
>
>
> I augmented symbols and added the axes=F argument to the plot function
> inside the original symbols function. It works as expected, no warning
> message. I am just lost as to why the extra arguments in symbols (...)
> are not behaving as expected.
>

The ... argument is also passed to .Internal, and presumably the code there gives the warning.

       -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From f.harrell at vanderbilt.edu  Tue Jan 17 22:57:40 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 17 Jan 2006 15:57:40 -0600
Subject: [R] Cumulative Density Plots (Hmisc/lattice)
In-Reply-To: <EB693868E54B314483E8E28E8BAAC6EF042E8E@chisrv01.environchicago.environ.local>
References: <EB693868E54B314483E8E28E8BAAC6EF042E8E@chisrv01.environchicago.environ.local>
Message-ID: <43CD6854.2000106@vanderbilt.edu>

Mike Bock wrote:
> I have been using the ECDF function in the Hmisc package to produce
> cumulative distribution function plots. The problem is that for small
> datasets the steps "look bad" (not my characterization but from the
> client). Is there a way to get the same information but smoothed? I have
> tried the densityplot (lattice), which gives a smoothed line, but this
> does not give the cumulative density. 
> 
> Michael Bock, PhD
> ENVIRON International Corporation
> 136 Commercial Street, Suite 402
> Portland, ME 04101
> phone: 207.347.4413
> fax: 207.347.4384

That will invalidate the definition of ECDF.  I've never had a client 
complain about it before.

FH


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From spencer.graves at pdf.com  Tue Jan 17 23:20:29 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 17 Jan 2006 14:20:29 -0800
Subject: [R] fitted values from lmer (lme4 0.98)
In-Reply-To: <40e66e0b0601171331h77fc26d0kaf41feb0393608fb@mail.gmail.com>
References: <43CC12AD.3080304@mail.la.utexas.edu>	<40e66e0b0601171117p2ed8220ege618bdceba84be46@mail.gmail.com>	<x2bqya36yx.fsf@turmalin.kubism.ku.dk>
	<40e66e0b0601171331h77fc26d0kaf41feb0393608fb@mail.gmail.com>
Message-ID: <43CD6DAD.9030504@pdf.com>

Hi, Doug:

	  I think it would help me to have an option like you just suggested -- 
with your comment below included in the help file for that option.

	  Thanks for all your hard work and creativity in this.

	  Best Wishes,
	  spencer graves

Douglas Bates wrote:

> On 17 Jan 2006 22:00:38 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> 
>>Douglas Bates <dmbates at gmail.com> writes:
>>
>>
>>>On 1/16/06, Daniel A. Powers <dpowers at mail.la.utexas.edu> wrote:
>>>
>>>>-- R-List
>>>>
>>>>Can someone tell me how to get fitted values etc. after fitting lmer?
>>>>for example, from lme, I can fit mod.1 <- lme(....) and get fitted values, coefficients, etc. in this way
>>>>
>>>>mod.1$fitted[,1] or mod.1$fitted[,2] etc.
>>>>
>>>>It seems lmer uses "slots" that are unfamiliar to me.
>>>
>>>The preferred way is to use  the extractor functions fitted, fixef,
>>>ranef and coef.
>>>
>>>Using the recently uploaded version 0.995-1 of the Matrix package we get
>>
>>But, at least with 0.99-6 (sorry for falling way behind, but I'd
>>rather not upgrade just now...), fitted(fm1) are effectively BLUPs. If
>>you want to get the estimated mean values, you need something like
>>
>>model.matrix(fm1 at terms,sleepstudy) %*% fixef(fm1)
>>
>>I realize that since lmer models are not necessarily hierarchical, it
>>takes more than cloning the "level" argument from fitted.lme, but it
>>could be useful to at least have a "level=0" equivalent.
> 
> 
> Would it help if there were an option in the fitted method to allow
> for fixed-effects only versus fixed- and random-effects?  As you say,
> because lmer models do not need to be hierarchical it is not obvious
> what it would mean to include some but not all of the random effects
> terms in the "fitted values".  However, it is easy and unambiguous to
> define fitted values for the fixed-effects only.
> 
> Up until a few days ago there was an option to do this but then I
> changed the calculation of the fitted values in an attempt to clean up
> the code.  The calculation of the level = 0 fitted values in the new
> representation of the fitted model is quite easy.  It is
> 
> fm1 at X %*% fixef(fm1)
> 
> (except for complications introduced by na.exclude)
> 
> 
>>>>(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
>>>
>>>Linear mixed-effects model fit by REML
>>>Formula: Reaction ~ Days + (Days | Subject)
>>>   Data: sleepstudy
>>>      AIC      BIC    logLik MLdeviance REMLdeviance
>>> 1753.628 1769.593 -871.8141   1751.986     1743.628
>>>Random effects:
>>> Groups   Name        Variance Std.Dev. Corr
>>> Subject  (Intercept) 612.090  24.7405
>>>          Days         35.072   5.9221  0.066
>>> Residual             654.941  25.5918
>>># of obs: 180, groups: Subject, 18
>>>
>>>Fixed effects:
>>>            Estimate Std. Error t value
>>>(Intercept) 251.4051     6.8246  36.838
>>>Days         10.4673     1.5458   6.771
>>>
>>>Correlation of Fixed Effects:
>>>     (Intr)
>>>Days -0.138
>>>
>>>>fixef(fm1)
>>>
>>>(Intercept)        Days
>>>  251.40510    10.46729
>>>
>>>>ranef(fm1)
>>>
>>>An object of class $-1????lmer.ranef????
>>>[[1]]
>>>    (Intercept)        Days
>>>308   2.2585636   9.1989720
>>>309 -40.3985870  -8.6197013
>>>310 -38.9602563  -5.4488780
>>>330  23.6905071  -4.8143326
>>>331  22.2602104  -3.0698958
>>>332   9.0395288  -0.2721711
>>>333  16.8404364  -0.2236253
>>>334  -7.2325817   1.0745765
>>>335  -0.3336930 -10.7521594
>>>337  34.8903592   8.6282824
>>>349 -25.2101185   1.1734156
>>>350 -13.0699625   6.6142058
>>>351   4.5778374  -3.0152575
>>>352  20.8635979   3.5360123
>>>369   3.2754538   0.8722165
>>>370 -25.6128786   4.8224661
>>>371   0.8070403  -0.9881552
>>>372  12.3145428   1.2840291
>>>
>>>
>>>>coef(fm1)
>>>
>>>$Subject
>>>    (Intercept)       Days
>>>308    253.6637 19.6662580
>>>309    211.0065  1.8475846
>>>310    212.4448  5.0184079
>>>330    275.0956  5.6529533
>>>331    273.6653  7.3973901
>>>332    260.4446 10.1951148
>>>333    268.2455 10.2436606
>>>334    244.1725 11.5418624
>>>335    251.0714 -0.2848734
>>>337    286.2955 19.0955683
>>>349    226.1950 11.6407015
>>>350    238.3351 17.0814918
>>>351    255.9829  7.4520285
>>>352    272.2687 14.0032983
>>>369    254.6806 11.3395024
>>>370    225.7922 15.2897520
>>>371    252.2121  9.4791308
>>>372    263.7196 11.7513151
>>>
>>>
>>>>fitted(fm1)
>>>
>>>  [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950 371.6612 391.3275
>>>  [9] 410.9937 430.6600 211.0065 212.8541 214.7017 216.5493 218.3969 220.2444
>>> [17] 222.0920 223.9396 225.7872 227.6348 212.4448 217.4633 222.4817 227.5001
>>> [25] 232.5185 237.5369 242.5553 247.5737 252.5921 257.6105 275.0956 280.7486
>>> [33] 286.4015 292.0545 297.7074 303.3604 309.0133 314.6663 320.3192 325.9722
>>> [41] 273.6653 281.0627 288.4601 295.8575 303.2549 310.6523 318.0497 325.4470
>>> [49] 332.8444 340.2418 260.4446 270.6397 280.8349 291.0300 301.2251 311.4202
>>> [57] 321.6153 331.8104 342.0056 352.2007 268.2455 278.4892 288.7329 298.9765
>>> [65] 309.2202 319.4638 329.7075 339.9512 350.1948 360.4385 244.1725 255.7144
>>> [73] 267.2562 278.7981 290.3400 301.8818 313.4237 324.9656 336.5074 348.0493
>>> [81] 251.0714 250.7865 250.5017 250.2168 249.9319 249.6470 249.3622 249.0773
>>> [89] 248.7924 248.5076 286.2955 305.3910 324.4866 343.5822 362.6777 381.7733
>>> [97] 400.8689 419.9644 439.0600 458.1556 226.1950 237.8357 249.4764 261.1171
>>>[105] 272.7578 284.3985 296.0392 307.6799 319.3206 330.9613 238.3351 255.4166
>>>[113] 272.4981 289.5796 306.6611 323.7426 340.8241 357.9056 374.9871 392.0686
>>>[121] 255.9829 263.4350 270.8870 278.3390 285.7911 293.2431 300.6951 308.1471
>>>[129] 315.5992 323.0512 272.2687 286.2720 300.2753 314.2786 328.2819 342.2852
>>>[137] 356.2885 370.2918 384.2951 398.2984 254.6806 266.0201 277.3596 288.6991
>>>[145] 300.0386 311.3781 322.7176 334.0571 345.3966 356.7361 225.7922 241.0820
>>>[153] 256.3717 271.6615 286.9512 302.2410 317.5307 332.8205 348.1102 363.4000
>>>[161] 252.2121 261.6913 271.1704 280.6495 290.1287 299.6078 309.0869 318.5661
>>>[169] 328.0452 337.5243 263.7196 275.4710 287.2223 298.9736 310.7249 322.4762
>>>[177] 334.2275 345.9789 357.7302 369.4815
>>>
>>>I hope this helps.
>>>
>>>Doug Bates
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>--
>>   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>>  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>> (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Tue Jan 17 23:27:00 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 17 Jan 2006 17:27:00 -0500
Subject: [R] Anova problem with order of terms in model
In-Reply-To: <000001c61ba5$5a8817a0$5044d084@TAMIAS>
Message-ID: <20060117222657.EBXQ23065.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Martin,

The anova() function computes sequential (sometimes called "type-I") tests;
I'm not sure what Jmp does by default, but either "type-II" or "type-III"
tests would produce these results for a model of this structure. See the
Anova() function in the car package for "type-II" and "III" tests.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Julien
> Sent: Tuesday, January 17, 2006 3:34 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Anova problem with order of terms in model
> 
> Hi
> I have a linear model and I want to tests whether the model 
> terms are significant I used anova but F and P value depend 
> on the order of the terms in the model.
> I have repeated the same analysis in another stat software 
> and F and P value did not differ with order of terms in the model.
> Can anyone can explain what happen or what am I doing wrong ?
> 
> In R
> > anova (lm(Y~X.1+X.2))
> Analysis of Variance Table
> Response: Y
> 		Df	Sum Sq		Mean Sq	F value		Pr(>F)
> X.1		1	4.0351		4.0351		6.5187
> 0.01852
> X.2		1	0.5903		0.5903		0.9537
> 0.33991
> Residuals	21	12.9991		0.619
> 
> > anova (lm(Y~X.2+X.1))
> Analysis of Variance Table
> Response: Y
> 		Df	Sum Sq		Mean Sq	F value		Pr(>F)
> X.2		1	1.9651		1.9651		3.1747
> 0.08926
> X.1		1	2.6603		2.6603		4.2977
> 0.05066
> Residuals	21	12.9991		0.619		
> 
> With another stat software (JMP)
> Y~X.1+X.2
> 		Df	Sum Sq		Mean Sq	F value		Pr(>F)
> X.1		1	2.6603		2.6603		4.2977
> 0.05066
> X.2		1	0.5903		0.5903		0.9537
> 0.33991
> Residuals	21	12.9991		0.619		
> 
> Y~X.2+X.1
> 		Df	Sum Sq		Mean Sq	F value		Pr(>F)
> X.2		1	0.5903		0.5903		0.9537
> 0.33991
> X.1		1	2.6603		2.6603		4.2977
> 0.05066
> Residuals	21	12.9991		0.619		
> 
> 
> --------------------------------------------------------------
> -------------
> Julien Martin
> 
> Canadian Research Chair in behavioural ecology Universit?? du 
> Qu??bec ?? Montr??al
> 
> Email: martin.julien.2 at courrier.uqam.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Tue Jan 17 23:30:18 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 17 Jan 2006 16:30:18 -0600
Subject: [R] label of second y-axis in xyplot (lattice)
In-Reply-To: <F5076E7EAA58F448A0EEC05ADE2317BD0307E4@muc-exch001.munich.komdat.intern>
References: <F5076E7EAA58F448A0EEC05ADE2317BD0307E4@muc-exch001.munich.komdat.intern>
Message-ID: <eb555e660601171430k40b46026s8930419fc659afa7@mail.gmail.com>

On 1/16/06, Antje Schle <Antje.Schuele at komdat.com> wrote:
> Dear group,
>
>
>
> First I provide you with an example, I found in the newsgroup. Then I'd like
> to explain my problem to you by means of the output.
>
>
>
> enviro <-
>     data.frame(Year = rep(2001:2002, each = 365),
>                Day = rep(1:365, 2),
>                Precip = pmax(0, rnorm(365 * 2)),
>                Temp = 2 + 0.2 * rnorm(365 * 2))
>
>
> xyplot(Precip + Temp ~ Day | Year, data=enviro,
>        layout = c(1, 2),
>        panel = panel.superpose.2,
>        type = c('h', 'l'))
>
>
>
> Now my question.
>
>
>
> How can I achieve different labels for two y-axes? (Here this is not the
> case, but the example should just visualize my question.)
>
> So here in this case I'd like to have two labels. On the "normal" y-side
> (axis2) the label = "Precip", on the other side (axis4) the label "Temp".

The design of xyplot etc doesn't permit this. One (fake) solution is
to add a legend:

xyplot(Precip + Temp ~ Day | Year, data=enviro,
       layout = c(1, 2),
       panel = panel.superpose.2,
       ylab = "Precip",
       legend = list(right =
       list(fun = grid::textGrob("Temp", rot = 90))),
       type = c('h', 'l'))

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From Seungho.Huh at sas.com  Tue Jan 17 23:25:28 2006
From: Seungho.Huh at sas.com (Seungho Huh)
Date: Tue, 17 Jan 2006 17:25:28 -0500
Subject: [R] have difficulty in installing packages
Message-ID: <BBB646166E3FCE46A5E9CE14B252558608E00F71@MERC27.na.sas.com>

Dear sir or ma'am,

I have difficulty in installing packages. When I click "Install package(s)..." in the Packages menu, it takes forever and it finally shows the following message: 

--- Please select a CRAN mirror for use in this session ---
Warning: unable to access index for repository http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2
Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.2
Error in install.packages(NULL, .libPaths()[1], dependencies = TRUE, type = type) : 
        no packages were specified

In fact, I visited http://www.r- project.org/, looked for some CRAN mirror sites, and decided to click the following site: http://www.ibiblio.org/pub/languages/R/CRAN/ However, the ckicking function simply did not work for this site and all the other CRAN mirror sites listed there. 
Is this a problem that I will need to get help from the help desk of my company? Or, has someone else had similar problems? Thanks for any advice on this.



From ripley at stats.ox.ac.uk  Wed Jan 18 00:00:06 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 23:00:06 +0000 (GMT)
Subject: [R] array question
In-Reply-To: <OF536BA5CA.219F731C-ON852570F9.006B8DDD-852570F9.006C921A@progressive.com>
References: <OF536BA5CA.219F731C-ON852570F9.006B8DDD-852570F9.006C921A@progressive.com>
Message-ID: <Pine.LNX.4.61.0601172258560.32358@gannet.stats>

Please use POSIXct and not POSIXlt objects, which are lists.

On Tue, 17 Jan 2006, Chang Shen wrote:

> Hi all,
>
> I want to create an array of datetime.
>
> If I have a datetime object dt
>
>> dt <- strptime("10Jan2006 00:00:15", "%d%b%Y %H:%M:%S")
>> dt
> [1]"2006-01-10 00:00:15"
>
> I want to make an array of dt, say 100 size.  I got those error.
>
> [1] "2006-01-10 00:00:15"
>> dtarray<-array(dt, dim=c(100));
> Error in array(dt, dim = c(100)) : dim<- : dims [product 100] do not match
> the length of object [9]
>
>
>> dtarray<-array(, dim=c(100));
>> dtarray[1]<-dt;
> Warning message:
> number of items to replace is not a multiple of replacement length
>>
>
>
> Any help?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 18 00:15:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 23:15:19 +0000 (GMT)
Subject: [R] symbols function
In-Reply-To: <Pine.LNX.4.43.0601171337360.7252@hymn06.u.washington.edu>
References: <Pine.LNX.4.43.0601171337360.7252@hymn06.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0601172306550.32358@gannet.stats>

On Tue, 17 Jan 2006, Thomas Lumley wrote:

> On Tue, 17 Jan 2006, Jean Eid wrote:
>
>> Hi
>>
>> I do not get why the symbols function produces warnings when axes=F is
>> added. The following example illustrate this
>>
>>> symbols(0,10, inches=T, circles=1, axes=F, xlab="", ylab="")
>> Warning message:
>> parameter "axes" could not be set in high-level plot() function
>>
>>
>> I augmented symbols and added the axes=F argument to the plot function
>> inside the original symbols function. It works as expected, no warning
>> message. I am just lost as to why the extra arguments in symbols (...)
>> are not behaving as expected.
>>
>
> The ... argument is also passed to .Internal, and presumably the code 
> there gives the warning.

Indeed.  axes=F is not in the allowed list

      ...: graphics parameters can also be passed to this function, as
           can the plot aspect ratio 'asp' (see 'plot.window').

People confuse 'axes' with the graphics parameters, but it is in fact an 
argument to plot.default.  (The corresponding graphics parameters
xaxt and yaxt do work.)  R-devel gives a more informative message:

> attach(trees)
> symbols(Height, Volume, circles = Girth/24, inches = FALSE, axes=F)
Warning message:
"axes" is not a graphical parameter in: symbols(x, y, type, data, inches, 
bg, fg, ...)

We do ask people to read the help pages before posting for a good reason: 
the information is usually there in a more complete and accurate form than 
people remember.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregor.gorjanc at gmail.com  Wed Jan 18 00:18:55 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Wed, 18 Jan 2006 00:18:55 +0100
Subject: [R] Current state of support for BUGS access for Linux users?
Message-ID: <43CD7B5F.9050804@bfro.uni-lj.si>

Hello!

> Re R packages:
> - R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
> can speak with WinBUGS under wine thanks to user contributions. But it
> still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
> Windows).

However Andrew Gelman, has added also support[1] for OpenBUGS so this 
might be also a good news for Linux if wine is used. Changelog can be 
found at [2].

[1]http://www.stat.columbia.edu/~gelman/bugsR/
[2]http://www.stat.columbia.edu/~gelman/bugsR/bugs.R

-- 
Lep pozdrav / With regards,
     Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si 

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888 

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
  you have no certainty until you try." Sophocles ~ 450 B.C.



From ripley at stats.ox.ac.uk  Wed Jan 18 00:25:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 23:25:53 +0000 (GMT)
Subject: [R] have difficulty in installing packages
In-Reply-To: <BBB646166E3FCE46A5E9CE14B252558608E00F71@MERC27.na.sas.com>
References: <BBB646166E3FCE46A5E9CE14B252558608E00F71@MERC27.na.sas.com>
Message-ID: <Pine.LNX.4.61.0601172319510.32358@gannet.stats>

You seem to be using Windows.  I do suggest you ask local advice, as the 
problem is almost certainly local your site.

You may need to set a proxy -- see the rw-FAQ.
You may need to set a policy to allow R through a firewall.
You may need permission to download at all, and indeed the trouble you are 
having with your unnamed browser suggests so.

On Tue, 17 Jan 2006, Seungho Huh wrote:

> Dear sir or ma'am,
>
> I have difficulty in installing packages. When I click "Install package(s)..." in the Packages menu, it takes forever and it finally shows the following message:
>
> --- Please select a CRAN mirror for use in this session ---
> Warning: unable to access index for repository http://www.ibiblio.org/pub/languages/R/CRAN/bin/windows/contrib/2.2
> Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.2
> Error in install.packages(NULL, .libPaths()[1], dependencies = TRUE, type = type) :
>        no packages were specified
>
> In fact, I visited http://www.r- project.org/, looked for some CRAN 
> mirror sites, and decided to click the following site: 
> http://www.ibiblio.org/pub/languages/R/CRAN/ However, the ckicking 
> function simply did not work for this site and all the other CRAN mirror 
> sites listed there. Is this a problem that I will need to get help from 
> the help desk of my company? Or, has someone else had similar problems? 
> Thanks for any advice on this.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 18 00:27:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 23:27:09 +0000 (GMT)
Subject: [R] log-linear analysis - calculate treatment effects
In-Reply-To: <D0F54E24-BB1C-484F-970B-278DEBE6E815@cam.ac.uk>
References: <D0F54E24-BB1C-484F-970B-278DEBE6E815@cam.ac.uk>
Message-ID: <Pine.LNX.4.61.0601172300561.32358@gannet.stats>

Use multinom or glm instead.  The IFP algorithm used in loglin is not 
designed to find parameter estimates let alone standard errors.

Fitting log-linear models is discussed in all good books on R/S, e.g.
MASS (see the FAQ for full details).


On Tue, 17 Jan 2006, Becker Cordula wrote:

> Dear all,
>
> I have run a hierarchical log-linear analysis using loglin {stats}
> and came up with a specific model. loglin returns me the parameter
> estimates giving me an idea in which direction the observed
> frequencies deviate from the expected ones for my different factors.
> To assess the significance of any such effects I would need to
> calculate the ratio of the log-linear parameter estimate to its
> standard error (as explained in Howell: Statistical Methods for
> Psychology, 4th Edition, p. 637ff). Unfortunately, I have no idea and
> couldn't find out what this standard error refers to and how to
> calculate it or the ratio using R (well, the ratio is no problem if I
> have the standard error).
>
> I would very much appreciate any help!
>
> Thanks and best wishes, Cordula Becker
>
> ----------------------------------------------------
> Dr. Cordula Becker
>
> Department of Experimental Psychology
> University of Cambridge
> Downing Site
> Cambridge  CB2 3EB
> United Kingdom
>
> Tel.: +44 (0)1223 339715
> Email: cb486 at cam.ac.uk
>
> http://vision.psychol.cam.ac.uk/cbecker/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 18 00:29:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jan 2006 23:29:26 +0000 (GMT)
Subject: [R] Step.glm() question
In-Reply-To: <2554377D323C9A40BD69956059FD05490ED4529C@mnip1>
References: <2554377D323C9A40BD69956059FD05490ED4529C@mnip1>
Message-ID: <Pine.LNX.4.61.0601172241160.32358@gannet.stats>

There is no step.glm in R!

I suggest that you forget about step.glm, and start afresh with step in R 
(which is a cut-down version of MASS's stepAIC rather than emulating 
step.glm in S). In particular, it avoids many of the pitfalls of step.glm.

Please do study the posting guide, and tell us your R version and provide 
a reproducible example.  Without the latter it is almost impossible to 
pinpoint the error (if indeed there is one using the current version of 
R).

You are using many unnecessary as.formula calls that make your code hard 
to read.  z ~ x+y is a formula and needs no coercion.


On Tue, 17 Jan 2006, Smith, Phil wrote:

> Hi:
>
> I am using step.glm() as follows:
>
> form1 	<- 	as.formula(haspdata ~ 1)
> lg.mod1 	<- 	glm ( formula=form1, data=st.mtx,
> family=binomial , na.action=na.omit )
>
> upper		<-	as.formula( haspdata ~ (
> c5+childnm+educ1+incpov1+marital+msa+racekid+racemom+sex+shotcard )^2)
> lower		<-	as.formula(haspdata~1)
> lst		<-	list( upper=upper , lower=lower )
> form1a.step	<-	step(lg.mod1, scope=lst , trace=T ,
> direction="both" )
>
> I get the following error message in R:
>
> Error in factor.scope(ffac, list(add = fadd, drop = fdrop)) :
>        upper scope does not include model
>
> Because I am trying to migrate from Splus to R, I ran that same code in
> Splus and do not get an error.
>
> Could someone please tell me:
> 	1) what I'm doing wrong in R, and
> 	2) what the distinction between R and Splus is in this case.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rbaer at atsu.edu  Wed Jan 18 00:48:20 2006
From: rbaer at atsu.edu (Robert Baer)
Date: Tue, 17 Jan 2006 17:48:20 -0600
Subject: [R] Newbie question on using friedman.test()
References: <200601171816.k0HIGH7w014075@dispatch.cs.umd.edu>
Message-ID: <017901c61bc0$7f4c85b0$a00c010a@BigBaer>

?friedman.test

Says:
Description:
Performs a Friedman rank sum test with unreplicated blocked data.

Usage:
friedman.test(y, ...)

  y: either a numeric vector of data values, or a data matrix.

So assuming your data, d, is unreplicated blocked data, perhaps"
> d=as.matrix(d)
> friedman.test(d)
or simply,
friedman.test(as.matrix(d))
???



____________________________
Robert W. Baer, Ph.D.
Associate Professor
Department of Physiology
A. T. Still University of Health Science
800 W. Jefferson St.
Kirksville, MO 63501-1497 USA
----- Original Message ----- 
From: "Bill Kules" <wmk at takomasoftware.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 17, 2006 12:16 PM
Subject: [R] Newbie question on using friedman.test()


> I am trying to use the friedman.test() on a data frame, d, but
> I am receiving the following error message:
>
>
> > d
>    AW HS IAC WA
> 1   6  8   3  5
> 2   2  2   3  6
> 3   7  7   8  3
> 4   8  5   4  5
> ....
> 20  2  5   2  7
> 21  7  7   6  7
> 22  7  8   6  8
> 23  6  8   4  5
> 24  5  7   5  2
> > friedman.test(d)
> Error in any(is.na(groups)) : argument "groups" is missing, with no
default
>
> I think I just need to convert the data frame to a matrix, and then
> friedman.test() will get the roups and blocks automatically.
>
> Question 1) Is my understanding correct?
>
> Question 2) What R function will convert the data frame to the matrix I
> need?
> I'm still figuring out the matrix functions, and I would appreciate any
> pointers or examples.  The help() section is sometimes a bit terse...
>
> Thanks in advance from a Newbie,
> Bill
>
> PS - to recreate the above data frame "d":
> d <-
> structure(list(AW = as.integer(c(6, 2, 7, 8, 8, 8, 7, 5, 3, 6,
> 8, 7, 6, 4, 8, 7, 8, 7, 7, 2, 7, 7, 6, 5)), HS = as.integer(c(8,
> 2, 7, 5, 4, 5, 7, 7, 2, 8, 4, 7, 8, 7, 6, 7, 5, 8, 8, 5, 7, 8,
> 8, 7)), IAC = as.integer(c(3, 3, 8, 4, 7, 5, 8, 3, 4, 3, 7, 4,
> 6, 5, 6, 7, 8, 6, 8, 2, 6, 6, 4, 5)), WA = as.integer(c(5, 6,
> 3, 5, 3, 6, 7, 8, 3, 5, 6, 7, 7, 7, 7, 6, 4, 7, 8, 7, 7, 8, 5,
> 2))), .Names = c("AW", "HS", "IAC", "WA"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24"
> ), class = "data.frame")
>
> ========
> Bill Kules
> Principal, Takoma Software, Inc., Takoma Park, MD
>   www.takomasoftware.com
> Ph.D. Candidate, University of Maryland Human-Computer Interaction Lab
>   www.cs.umd.edu/hcil
>
> wmk at takomasoftware.com
> (301) 405-2725 voice
> (301) 891-7271 voice + voicemail
> (301) 891-7273 fax
> (301) 755-7982 mobile
> ========
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From sourceforge at metrak.com  Wed Jan 18 02:20:20 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 18 Jan 2006 12:20:20 +1100
Subject: [R] Windows ESS for XEmacs - installer
Message-ID: <43CD97D4.1030205@metrak.com>

I mentioned this on the ESS list a little while ago, I made an installer 
  for ESS and XEmacs on Windows.

It worked for me but given my minimal knowledege of ESS and XEmacs, it 
might not be the right way to do it and may or may not work for you.

I hosted it and the source for the inno setup script at 
http://brewiki.org/XEmacsESS but would be only too pleased for it to 
move somewhere more appropriate and modifed as deemed necessary.

cheers



From spencer.graves at pdf.com  Wed Jan 18 03:11:07 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 17 Jan 2006 18:11:07 -0800
Subject: [R] find mean of a list of timeseries
In-Reply-To: <1137154015.4525.84.camel@localhost.localdomain>
References: <1137154015.4525.84.camel@localhost.localdomain>
Message-ID: <43CDA3BB.4070201@pdf.com>

	  I suggest you not worry about the loops.  A decade ago (e.g., with 
S-Plus 3 or 3.1), loops were a major problem.  Releases of S-Plus and R 
since then have made substantial improvements in loop computations.

	  My preferred solution to your problem, as I understand it, is as 
follows:

a<-ts(matrix(c(1,1,1,10,10,10,20,20,20),nrow=3),names=c('var1','var2','var3'))
b<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3'))
c<-ts(matrix(c(3,3,3,12,12,12,22,22,22),nrow=3),names=c('var1','var2','var3'))

data<-list(a,b,c)

mean.list <- function(object){
   n <- length(object)
   a1 <- object[[1]]
   if(n>1)for(i in 2:n){
     a1 <- a1+object[[i]]
   }
   a1/n
}

gc()
start.time <- proc.time()
mean.list(data)
(et <- proc.time()-start.time)

 > mean.list(data)
Time Series:
Start = 1
End = 3
Frequency = 1
   a1.a1.var1 a1.a1.var2 a1.a1.var3
1          2         11         21
2          2         11         21
3          2         11         21
 > (et <- proc.time()-start.time)
[1] 0.04 0.00 0.09   NA   NA

	  My attempt to avoid loops is the following:  	

apply.list <- function(object, FUN){
   dim.list <- sapply(object, dim)
   if(is.list(dim.list))
     stop("attributes of ", deparse(substitute(object)),
          " do not have the same number of dimensions.")
   allEqual <- apply(dim.list, 1, function(x)diff(range(x)))
   if(any(allEqual !=0))
     stop("attributes of ", deparse(substitute(object)),
          " don't all have the same dimensions")
   n <- length(object)
   Dim <- c(n, dim.list[,1])
#
   obj.array <- array(unlist(object), dim=Dim)
   k <- length(dim)
   apply(obj.array, 2:k, FUN)
}

 > start.time <- proc.time()
 > apply.list(data, mean)
      [,1] [,2] [,3]
[1,]    2    2    2
[2,]   11   11   11
[3,]   21   21   21
 > (et <- proc.time()-start.time)
[1] 0.00 0.00 0.07   NA   NA
	
	 This seems to run slightly faster on this miniscule data.  However the 
answer is transposed, and I don't want to take the time to understand 
and fix that problem.

	  hope this helps.
	  spencer graves

tom wright wrote:

> Can someone please give me a clue how to 're'write this so I dont need
> to use loops.
> 
> a<-ts(matrix(c(1,1,1,10,10,10,20,20,20),nrow=3),names=c('var1','var2','var3'))
> b<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3'))
> c<-ts(matrix(c(3,3,3,12,12,12,22,22,22),nrow=3),names=c('var1','var2','var3'))
> 
> data<-list(a,b,c)
> 
> I now want to find the means of all vectors var1,var2 and var3
> 
> i.e. I need to end up with a new time series with three data vectors
> (var1, var2 and var3)
> result<-ts(matrix(c(2,2,2,11,11,11,21,21,21),nrow=3),names=c('var1','var2','var3))
> 
> I think its the list thats throwing my use of apply, I might be wrong
> but what other data structure could I use?
> 
> Many thanks
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Augusto.Sanabria at ga.gov.au  Wed Jan 18 03:57:24 2006
From: Augusto.Sanabria at ga.gov.au (Augusto.Sanabria@ga.gov.au)
Date: Wed, 18 Jan 2006 13:57:24 +1100
Subject: [R] Calculation of daily max
Message-ID: <9707EBA615A57747A0668CECD4638A300168026D@mail.agso.gov.au>

Thank you Gabor, the 'aggregate' function
operating on zoo(WS) does the job beautifully!

'zoo' has a lot of other goodies too, great 
package (thanks for that too).

Cheers,

Augusto

--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2609
Ph. (02) 6249-9155
 
 


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Tuesday, 17 January 2006 4:32 PM
To: Sanabria Augusto
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Calculation of daily max


Assuming VDATE is a character vector this produces
a zoo time series object.

	library(zoo)
	z <- aggregate(zoo(WS), as.Date(VDATE), max)

coredata(z) and time(z) are the data vector of maximums and corresponding
times, respectively.

The R command:

               vignette("zoo")

gives an introduction to zoo.

Aside from zoo you could check out ?tapply, ?by and ?aggregate .



On 1/16/06, Augusto.Sanabria at ga.gov.au <Augusto.Sanabria at ga.gov.au> wrote:
>
> Good day everyone.
>
> I have a large dataset of 1 min wind speeds
> covering 5 years.
>
> How can I make an array of maximum daily values?
>
> The vectors I have are: 'VDATE' with dates in format '%Y-%m-%d' (like 
> '1992-10-28') and 'WS' with wind speed data (same number of elements 
> as VDATE).
>
> I want an array with 2 columns: Max daily wind speed and corresponding 
> day.
>
> Has anyone got an elegant way of doing that?
> (My background is in C++ I would tend to use loops
> but that is not very elegant in R, isn't it?)
>
> Augusto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From anthony at darrouzet-nardi.net  Wed Jan 18 04:05:18 2006
From: anthony at darrouzet-nardi.net (Anthony Darrouzet-Nardi)
Date: Tue, 17 Jan 2006 20:05:18 -0700
Subject: [R] Installing RMySQL on Mac OS X 10.4.4
Message-ID: <p06100500bff35edb7289@[128.138.122.213]>

When I try to install RMySQL from source on Mac OS X 10.4.4 (iMac G5 
2.1 Ghz), I get this...

[tcom122-223-dhcp:2.1.1/Resources/library] root# setenv PKG_CPPFLAGS 
/usr/local/mysql/lib
[tcom122-223-dhcp:2.1.1/Resources/library] root# setenv PKG_LIBS 
/usr/local/mysql/include
[tcom122-223-dhcp:2.1.1/Resources/library] root# R CMD INSTALL ~/dl/RMySQL/
* Installing *source* package 'RMySQL' ...
loading cache ./config.cache
checking how to run the C preprocessor... (cached) cc -E
checking for compress in -lz... (cached) yes
checking for getopt_long in -lc... (cached) yes
checking for mysql_init in -lmysqlclient... (cached) no
checking for mysql.h... (cached) no
creating ./config.status
creating src/Makevars
** libs
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include 
/usr/local/mysql/lib -I/usr/local/include   -fno-common  -g -O2 -c 
RS-DBI.c -o RS-DBI.o
gcc-3.3: cannot specify -o with -c or -S and multiple compilations
make: *** [RS-DBI.o] Error 1
ERROR: compilation failed for package 'RMySQL'
** Removing 
'/Library/Frameworks/R.framework/Versions/2.1.1/Resources/library/RMySQL'
** Restoring previous 
'/Library/Frameworks/R.framework/Versions/2.1.1/Resources/library/RMySQL'


Any suggestions for how to successfully install RMySQL on my system 
would be appreciated.

Thanks,

Anthony



From pauljohn32 at gmail.com  Wed Jan 18 06:40:29 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Tue, 17 Jan 2006 23:40:29 -0600
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <43CD5C27.5090500@statistik.uni-dortmund.de>
References: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>
	<43CCA1AD.6070805@statistik.uni-dortmund.de>
	<13e802630601170850l38352147mb4be199b8b70e11f@mail.gmail.com>
	<43CD5C27.5090500@statistik.uni-dortmund.de>
Message-ID: <13e802630601172140o671fac5esa49444dcc06ca49@mail.gmail.com>

Thanks!  Let me ask this question again.  Clearly, if you experts
can't make R talk to WinBUGS, then I can't either.  So, If "bugs()"
doesn't work, What is the next best thing?

With wine, I can run OpenBUGS and WinBUGS, but I cannot send jobs from
R to a BUGS program (still trying, some people say they have seen
rbugs work in Linux).

I want to follow along with the instructions In Prof Gelman's site for
R2WinBUGS.  OR the examples in the rbugs package.  I just noticed that
rbugs works by writing the data, model, inits, and so forth into text
files, along with a script that drives WinBUGS.  Although there is
something wrong with the script file that causes a fatal WinBUGS
crash, I have just manually started the GUI and pointed and clicked my
way to a working set of  BUGS estimates (see details on script flaw
and trap below).

That makes me feel a bit confident that I will be able to create
working BUGS sims and get results.  I need to learn how to bring into
R from CODA output.  In the WinBUGS output, I find a codaIndex.txt
file that looks promising.


Now, about efforts to run WinBUGS from within R. In case other people
are trying this, here is what I learned.

1. bugs() from R2WinBUGS to WinBUGS14 under wine-0.9.5 is not able to
start WinBUGS14.exe

I've just tried the newest example from
http://www.stat.columbia.edu/~gelman/bugsR/ with WinBUGS14 under wine.

WinBUGS never shows on the screen before the error appears:

> library(R2WinBUGS)
> schools <- read.table ("schools.dat", header=T)
> J <- nrow(schools)
> y <- schools$estimate
> sigma.y <- schools$sd
> data <- list ("J", "y", "sigma.y")
> inits <- function() {list (theta=rnorm(J,0,100), mu.theta=rnorm(1,0,100), sigma.theta=runif(1,0,100))}
> parameters <- c("theta", "mu.theta", "sigma.theta")
> schools.sim <- bugs (data,
+                      inits,
+                      parameters,
+                      "schools.bug",
+                      bugs.directory = "/usr/local/share/WinBUGS14/",
+                      n.chains=3,
+                      n.iter=1000,
+                      useWINE= T,
+                      WINE = "/usr/bin/wine"
+                      )
Loading required package: tools
Error in pmatch(x, table, duplicates.ok) :
	argument is not of mode character

Humphf!

I tried running this under debug, but can't understand the output. I
get through about 8 steps, when bugs.script() seems to cause the
error:
Browse[1]> n
debug: bugs.script(parameters.to.save, n.chains, n.iter, n.burnin, n.thin,
    bugs.directory, new.model.file, debug = debug, is.inits = !is.null(inits),
    bin = bin, DIC = DIC, useWINE = useWINE)
Browse[1]> n
Error in pmatch(x, table, duplicates.ok) :
	argument is not of mode character


2. With the "rbugs" package and WinBUGS14, I get a lot further.  From
fumbling around in this code, I can tell that rbugs writes text files
in the working directory and then tells WinBUGS to start and access
those files.  This code causes the WinBUGS GUI to pop up and it goes
pretty far. Using the exact same code as in the R2WinBUGS example
above, observe:


library(rbugs)
schools.sim <- rbugs ( data,
                      inits,
                      parameters,
                      "schools.bug",
                      n.burnin = 1000, n.chains=1, n.iter=1000,
                      workingDir="/home/pauljohn/.wine/fake_windows/temp",
                      bugs="/usr/local/share/WinBUGS14/WinBUGS14.exe",
                      bugsWorkingDir="z:/home/pauljohn/.wine/fake_windows/temp",
                      useWine=TRUE,
                      wine="/usr/bin/wine",
                      debug=F)

As I said, the WinBUGS14 window opens, the log file says

display(log)
check(z:/home/pauljohn/.wine/fake_windows/temp/model.txt)
model is syntactically correct
data(z:/home/pauljohn/.wine/fake_windows/temp/data.txt)
data loaded
compile(1)
model compiled
inits(1,z:/home/pauljohn/.wine/fake_windows/temp/init1.txt)
model is initialized
gen.inits()
command #Bugs:gen.inits cannot be executed (is greyed out)
beg(1001)
thin.updater(1)
set(theta)
set(mu.theta)
set(sigma.theta)
set(deviance)
update(1000)
dic.set()
update(0)
stats(*)
no monitor set
dic.stats()


However, after all that up pops a window called "TRAP"  (pasted next).
 I believe this is going wrong becuase the fourth-to-last line is
update(0).  If I manually change that to update(30000), then the
script does not crash.  So there's something wrong in rbugs in taking
"n.iter" into the script.  I'll try to follow that up.

Here's the TRAP window output.  I believe it is much ado about
nothing--update(0) is causing it.

undefined real result

 MonitorsSummary.StdMonitor.Mean   [000002FBH]
	.mean	REAL	2.121995790965272E-314
	.monitor	MonitorsSummary.StdMonitor	[0113F3F0H]
 DeviancePlugin.EstimatedMeans   [000003FBH]
	.cursor	GraphStochastic.List	[01145FE0H]
	.i	INTEGER	0
	.j	INTEGER	2143872776
	.list	GraphStochastic.List	[01145FE0H]
	.node	GraphStochastic.Node	[01044600H]
	.size	INTEGER	1
	.value	REAL	2.121995790965272E-314
 DevianceInterface.DICAll   [0000058EH]
	.dBar	REAL	3.486226116065809E+307
	.dBarTotal	REAL	3.486184325428448E+307
	.dHat	REAL	1.856490554558022E-303
	.dHatTotal	REAL	8.948601956767742E-317
	.dicTotal	REAL	1.342105535202697E-102
	.f	TextMappers.Formatter	Fields
	.i	INTEGER	2143868088
	.monitors	POINTER	[01145ED0H]
	.num	INTEGER	1
	.pDTotal	REAL	4.244199150914158E-313
	.res	INTEGER	0
 DevianceCmds.DIC   [0000012DH]
	.asc	INTEGER	104775
	.dsc	INTEGER	28575
	.f	TextMappers.Formatter	Fields
	.height	INTEGER	16821552
	.lines	INTEGER	553779732
	.res	INTEGER	0
	.t	TextModels.Model	[01145E50H]
	.title	ARRAY 256 OF CHAR	"DIC"
	.width	INTEGER	114300
 StdInterpreter.CallProc   [0000047AH]
	.a	BOOLEAN	FALSE
	.b	BOOLEAN	FALSE
	.c	BOOLEAN	FALSE
	.i	Meta.Item	Fields
	.imported	ARRAY 256 OF CHAR	""   ...
	.importing	ARRAY 256 OF CHAR	""   ...
	.mn	Meta.Name	"DevianceCmds"
	.mod	StdInterpreter.Ident	"DevianceCmds"
	.object	ARRAY 256 OF CHAR	""   ...
	.ok	BOOLEAN	TRUE
	.parType	INTEGER	3
	.pn	Meta.Name	"DIC"
	.proc	StdInterpreter.Ident	"DIC"   ...
	.res	INTEGER	0
	.v	StdInterpreter.ProcVal	Fields
	.vi	StdInterpreter.ProcIVal	Fields
	.vii	StdInterpreter.ProcIIVal	Fields
	.vr	StdInterpreter.ProcRVal	Fields
	.vri	StdInterpreter.ProcRIVal	Fields
	.vrii	StdInterpreter.ProcRIIVal	Fields
	.vrr	StdInterpreter.ProcRRVal	Fields
	.vrri	StdInterpreter.ProcRRIVal	Fields
	.vrrii	StdInterpreter.ProcRRIIVal	Fields
	.vrs	StdInterpreter.ProcRSVal	Fields
	.vrsi	StdInterpreter.ProcRSIVal	Fields
	.vrsii	StdInterpreter.ProcRSIIVal	Fields
	.vs	StdInterpreter.ProcSVal	Fields
	.vsi	StdInterpreter.ProcSIVal	Fields
	.vsii	StdInterpreter.ProcSIIVal	Fields
	.vsr	StdInterpreter.ProcSRVal	Fields
	.vsri	StdInterpreter.ProcSRIVal	Fields
	.vsrii	StdInterpreter.ProcSRIIVal	Fields
	.vss	StdInterpreter.ProcSSVal	Fields
	.vssi	StdInterpreter.ProcSSIVal	Fields
	.vssii	StdInterpreter.ProcSSIIVal	Fields
 StdInterpreter.Command   [0000131CH]
	.left	StdInterpreter.Ident	"DevianceCmds"
	.ptype	INTEGER	3
	.right	StdInterpreter.Ident	"DIC"   ...
 StdInterpreter.CallHook.Call   [00001441H]
	.ch	CHAR	0X
	.e	ARRAY 64 OF CHAR	""   ...
	.errorMsg	ARRAY 1 OF CHAR	""
	.f	ARRAY 64 OF CHAR	""   ...
	.g	ARRAY 64 OF CHAR	""   ...
	.hook	StdInterpreter.CallHook	[01060050H]
	.i	INTEGER	24
	.i0	INTEGER	0
	.i1	INTEGER	0
	.id	StdInterpreter.Ident	"DIC"   ...
	.par0	Dialog.String	""   ...
	.par1	Views.Title	""   ...
	.proc	ARRAY 240 OF CHAR	"DevianceCmds.DIC('DIC')"   ...
	.res	INTEGER	0
	.s0	Dialog.String	"DIC"
	.s1	Dialog.String	""   ...
	.type	INTEGER	3
	.x	INTEGER	0
 Dialog.Call   [00002FC8H]
	.errorMsg	ARRAY 1 OF CHAR	""
	.proc	ARRAY 240 OF CHAR	"DevianceCmds.DIC('DIC')"   ...
	.res	INTEGER	0
 BugsScript.Call   [00000130H]
	.bugsCommands	ARRAY 240 OF CHAR	"DevianceCmds.DIC('DIC')"   ...
	.i	INTEGER	23
	.item	Meta.Item	Fields
	.j	INTEGER	23
	.ok	BOOLEAN	FALSE
	.par	Dialog.Par	Fields
	.pos	INTEGER	-1
	.res	INTEGER	0
	.s	ARRAY 240 OF CHAR	"DevianceCmds.DIC('DIC')"   ...
	.scriptCommand	ARRAY 240 OF CHAR	"#Bugs:dic.stats"   ...
	.start	INTEGER	18
	.v	BugsScript.RECORD	Fields
 BugsScript.Action.Do   [0000062FH]
	.a	BugsScript.Action	[011D2120H]
	.argNum	INTEGER	0
	.bugsCommands	ARRAY 240 OF CHAR	"DevianceCmds.DIC('DIC')"   ...
	.p	ARRAY 3, 120 OF CHAR	Elements
	.s	BugsScanners.Scanner	Fields
	.scriptCommand	ARRAY 240 OF CHAR	"#Bugs:dic.stats"   ...
	.vectorName	BOOLEAN	FALSE
 Services.Exec   [00000136H]
	.a	Services.Action	[011D2120H]
	.t	POINTER	[21C90170H]
 Services.IterateOverActions   [000002F4H]
	.p	Services.Action	[011D2120H]
	.t	POINTER	NIL
	.time	LONGINT	3498
 Services.StdHook.Step   [0000034DH]
	.h	Services.StdHook	[0101E380H]
 HostWindows.Idle   [00004A86H]
	.focus	BOOLEAN	FALSE
	.tick	Controllers.TickMsg	Fields
	.w	HostWindows.Window	NIL
 HostMenus.TimerTick   [00003422H]
	.lParam	INTEGER	0
	.ops	Controllers.PollOpsMsg	Fields
	.wParam	INTEGER	1
	.wnd	INTEGER	65574
 Kernel.Try   [00003A61H]
	.a	INTEGER	65574
	.b	INTEGER	1
	.c	INTEGER	0
	.h	PROCEDURE	HostMenus.TimerTick
 HostMenus.ApplWinHandler   [00003841H]
	.Proc	PROCEDURE	NIL
	.hit	BOOLEAN	Undefined70
	.lParam	INTEGER	0
	.message	INTEGER	275
	.res	INTEGER	1180843108
	.s	ARRAY 256 OF SHORTCHAR	2X   ...
	.w	INTEGER	538416909
	.wParam	INTEGER	1
	.wnd	INTEGER	65574
<system>   (pc=465EDB29H,  fp=7FC8FB00H)
<system>   (pc=465EE418H,  fp=7FC8FB3CH)
<system>   (pc=465F1DF0H,  fp=7FC8FB80H)
<system>   (pc=465BD031H,  fp=7FC8FBB0H)
 HostMenus.Loop   [00003BDEH]
	.done	BOOLEAN	FALSE
	.f	SET	{0..5}
	.n	INTEGER	0
	.res	INTEGER	0
	.w	HostWindows.Window	NIL
 Kernel.Start   [00002B8CH]
	.code	PROCEDURE	HostMenus.Loop


3. Accessing OpenBUGS with wine from "rbugs" gets as far as opening
the OpenBUGS GUI, but nothing ever appears in the log file and there
are no computations going on (according to system monitors, anyway),
but everything in OpenBUGS just seems stuck.


pj


On 1/17/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Paul Johnson wrote:
> > Thanks, Uwe
> >
> > that clears up why I can't make R2WinBUGs work with OpenBUGS and WinBUGS1.5 :)
> > Both work pretty good with Wine in a GUI.  I noticed that when I tried
> > "rbugs", it does succeed in starting WinBUGS GUI, but then nothing
> > happens. I'll get WinBUGS1.4 and see what happens.
> >
> > In the meanwhile, I'm going to t ry to see what BRugs is good for. In
> > Linux, when I try to install BRugs, the install fails with an error
> > saying that, at the current time, BRugs works only in Windows.
>
>
> Yes, I have added that particular line in order not to confuse users,
> and I thought I told you in my last message that it works only under
> Windows.
>
>
> > * Installing *source* package 'BRugs' ...
> > Package 'BRugs' currently only works under Windows.\nIt is supposed to
> > work under Linux in future releases.
> >
> > I'd like to stop that check and see what happens!
>
> OK, just remove the configure file.
>
>
> > The way I read the
> > sourcecode from OpenBUGS and BRugs, I need to replace the windows dll
> > install and instead put in an so file (as in OpenBUGS).
>
> Yes, it is already shipped, and the infrastructure in the package is
> ready (hopefully), but the brugs.so file does not work as expected.
>
>
> > If anybody has done this, please let me know of your experience.
>
> Yes, several tried, among them Andrew Thomas and Uwe Ligges, and then I
> invited Andrew Thomas to Dortmund and we tried together (I have to admit
> that I was clueless all the time and in fact Andrew tried).
> Andrew's conclusion was that there is some compiler problem on Linux
> with the BlackBox framework (Component Pascal compiler from Oberon
> microsystems) in which WinBUGS/OpenBUGS is written in ...
>
> If you get it to work, please let us know!
>
> Uwe
>

--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From securebenji-general at yahoo.com  Wed Jan 18 07:02:49 2006
From: securebenji-general at yahoo.com (Ben Ridenhour)
Date: Tue, 17 Jan 2006 22:02:49 -0800 (PST)
Subject: [R] Bootstrapping help
Message-ID: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060117/296d389c/attachment.pl

From A.Robinson at ms.unimelb.edu.au  Wed Jan 18 07:20:55 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 18 Jan 2006 17:20:55 +1100
Subject: [R] Bootstrapping help
In-Reply-To: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
Message-ID: <20060118062055.GR75255@ms.unimelb.edu.au>

The first thing you are doing wrong is that you are not including a
copy of cs for us to see ;).

Based on what you have written, I speculate that cs does not use the
index correctly.  if so then a simple, although inefficient,
workaround is to rewrite cs:

cs <- function(dataframe, index) {
	dataframe <- dataframe[index,]
	...
}

Good luck, 

Andrew

On Tue, Jan 17, 2006 at 10:02:49PM -0800, Ben Ridenhour wrote:
> Hello,
>  I am new to using R and I am having problems get boot() to work properly.  Here is what I am trying to do:
>  
>  I have statistic called "cs".  cs takes a data matrix (154 x 5) and calculates 12 different scores for me.  cs outputs the data as a vector (12 x 1).  cs doesn't really use weights, per se, however I have included this as one of the 2 arguments cs can take.
>  
>  I try performing a bootstrap by issuing: 
>  myout<-boot(data, cs,R=999) 
>  I have tried other versions where I specify stype="w", etc...
>  
>  The problem I get is that the dataset does not seem to be resampled.  I end up with 999 replicates that have the exact same value of the output of cs.
>  
>  In the end I have something like
>  
>  Bootstrap Statistics :
>           original        bias    std. error
>  t1*   0.865122275  1.698641e-14           0
>  t2*  -0.005248414 -9.627715e-17           0
>  t3*  -0.052833740 -8.812395e-16           0
>  t4*   0.807040121  1.287859e-14           0
>  t5*   0.542082588 -9.103829e-15           0
>  t6*  -0.018617838 -7.285839e-17           0
>  t7*   0.006409704  1.422473e-16           0
>  t8*   0.529874453  8.104628e-15           0
>  t9*   0.074804390  2.359224e-16           0
>  t10* -0.007153634  1.301043e-16           0
>  t11* -0.018241243 -2.359224e-16           0
>  t12*  0.049409513 -1.200429e-15           0
>  
>  Clearly the bootstrap is not working.  What am I doing wrong?
>  
>  Thanks,
>  Ben
>  
>    
> -------------------------------
> Benjamin Ridenhour
> School of Biological Sciences
> Washigton State University
> P.O. Box 644236
> Pullman, WA 99164-4236
> Phone (509)335-7218
> -------------------------------- 
> "Nothing in biology makes sense except in the light of evolution."
> -T. Dobzhansky
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From pauljohn32 at gmail.com  Wed Jan 18 07:39:16 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Wed, 18 Jan 2006 00:39:16 -0600
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <13e802630601172140o671fac5esa49444dcc06ca49@mail.gmail.com>
References: <13e802630601161413g69e1e295sd9218b657aa3bd05@mail.gmail.com>
	<43CCA1AD.6070805@statistik.uni-dortmund.de>
	<13e802630601170850l38352147mb4be199b8b70e11f@mail.gmail.com>
	<43CD5C27.5090500@statistik.uni-dortmund.de>
	<13e802630601172140o671fac5esa49444dcc06ca49@mail.gmail.com>
Message-ID: <13e802630601172239o798e014en9ad4636019981c0c@mail.gmail.com>

Before I forget, found the working recipe for "rbugs".  My mistake
before was not
realzing that the n.iter value is total iterations, including burnin,
and so by setting
n.iter=1000 and n.burnin=1000, I was leaving 0 iterations for the updates.

### Paul Johnson 2006-01-18. This does work!
### Works in Fedora Core 4 Linux with wine-0.9.5 and WinBUGS-1.4.1
### (that's patched Winbugs 1.4).  The first part is from Andrew Gelman's
### web site http://www.stat.columbia.edu/~gelman/bugsR/, but is basically
### same as rbugs example code.



schools <- read.table ("schools.dat", header=T)
J <- nrow(schools)
y <- schools$estimate
sigma.y <- schools$sd
data <- list ("J", "y", "sigma.y")
inits <- function() {list (theta=rnorm(J,0,100),
mu.theta=rnorm(1,0,100), sigma.theta=runif(1,0,100))}
parameters <- c("theta", "mu.theta", "sigma.theta")

library(rbugs)


# Note that n.iter-n.burnin = N of observations collected in CODA
schools.sim <- rbugs ( data,
                      inits,
                      parameters,
                      "schools.bug",
                      n.chains = 1,
                      n.iter = 30000,
                      n.burnin = 1000,
                      n.thin = 1,
                      workingDir="/home/pauljohn/.wine/fake_windows/temp",
                      bugs="/usr/local/share/WinBUGS14/WinBUGS14.exe",
                      bugsWorkingDir="z:/home/pauljohn/.wine/fake_windows/temp",
                      useWine=TRUE,
                      wine="/usr/bin/wine",
                      debug=F)


myResults <- getBugsOutput(workingDir =
"/home/pauljohn/.wine/fake_windows/temp", n.chains=1)
# Creates an ordinary data frame:
str(myResults)




library(coda)
# Another way to retrieve text file data. using coda library
c1 <- read.coda("coda1.txt",
"/home/pauljohn/.wine/fake_windows/temp/codaIndex.txt")
# That creates a much more complicated mcmc data structure, however.
# But the output is pretty nice! And there are diagnostic functions
summary(c1)

heidel.diag(c1)

geweke.diag(c1)

### need more chains to run gelman.diag

###Note R2WinBUGS has a "read.bugs" command very similar. I suspect it
### work like a simple wrapper around the read.coda function

c2 <- read.bugs("/home/pauljohn/.wine/fake_windows/temp/coda1.txt")

heidel.diag(c2)

geweke.diag(c2)


pj

On 1/17/06, Paul Johnson <pauljohn32 at gmail.com> wrote:
> Thanks!  Let me ask this question again.  Clearly, if you experts
> can't make R talk to WinBUGS, then I can't either.  So, If "bugs()"
> doesn't work, What is the next best thing?
>
> With wine, I can run OpenBUGS and WinBUGS, but I cannot send jobs from
> R to a BUGS program (still trying, some people say they have seen
> rbugs work in Linux).
>
> I want to follow along with the instructions In Prof Gelman's site for
> R2WinBUGS.  OR the examples in the rbugs package.  I just noticed that
> rbugs works by writing the data, model, inits, and so forth into text
> files, along with a script that drives WinBUGS.  Although there is
> something wrong with the script file that causes a fatal WinBUGS
> crash, I have just manually started the GUI and pointed and clicked my
> way to a working set of  BUGS estimates (see details on script flaw
> and trap below).
>
> That makes me feel a bit confident that I will be able to create
> working BUGS sims and get results.  I need to learn how to bring into
> R from CODA output.  In the WinBUGS output, I find a codaIndex.txt
> file that looks promising.
>
>
> Now, about efforts to run WinBUGS from within R. In case other people
> are trying this, here is what I learned.
>
> 1. bugs() from R2WinBUGS to WinBUGS14 under wine-0.9.5 is not able to
> start WinBUGS14.exe
>
> I've just tried the newest example from
> http://www.stat.columbia.edu/~gelman/bugsR/ with WinBUGS14 under wine.
>
> WinBUGS never shows on the screen before the error appears:
>
> > library(R2WinBUGS)
> > schools <- read.table ("schools.dat", header=T)
> > J <- nrow(schools)
> > y <- schools$estimate
> > sigma.y <- schools$sd
> > data <- list ("J", "y", "sigma.y")
> > inits <- function() {list (theta=rnorm(J,0,100), mu.theta=rnorm(1,0,100), sigma.theta=runif(1,0,100))}
> > parameters <- c("theta", "mu.theta", "sigma.theta")
> > schools.sim <- bugs (data,
> +                      inits,
> +                      parameters,
> +                      "schools.bug",
> +                      bugs.directory = "/usr/local/share/WinBUGS14/",
> +                      n.chains=3,
> +                      n.iter=1000,
> +                      useWINE= T,
> +                      WINE = "/usr/bin/wine"
> +                      )
> Loading required package: tools
> Error in pmatch(x, table, duplicates.ok) :
>         argument is not of mode character
>
> Humphf!
>
> I tried running this under debug, but can't understand the output. I
> get through about 8 steps, when bugs.script() seems to cause the
> error:
> Browse[1]> n
> debug: bugs.script(parameters.to.save, n.chains, n.iter, n.burnin, n.thin,
>     bugs.directory, new.model.file, debug = debug, is.inits = !is.null(inits),
>     bin = bin, DIC = DIC, useWINE = useWINE)
> Browse[1]> n
> Error in pmatch(x, table, duplicates.ok) :
>         argument is not of mode character
>
>
> 2. With the "rbugs" package and WinBUGS14, I get a lot further.  From
> fumbling around in this code, I can tell that rbugs writes text files
> in the working directory and then tells WinBUGS to start and access
> those files.  This code causes the WinBUGS GUI to pop up and it goes
> pretty far. Using the exact same code as in the R2WinBUGS example
> above, observe:
>
>
> library(rbugs)
> schools.sim <- rbugs ( data,
>                       inits,
>                       parameters,
>                       "schools.bug",
>                       n.burnin = 1000, n.chains=1, n.iter=1000,
>                       workingDir="/home/pauljohn/.wine/fake_windows/temp",
>                       bugs="/usr/local/share/WinBUGS14/WinBUGS14.exe",
>                       bugsWorkingDir="z:/home/pauljohn/.wine/fake_windows/temp",
>                       useWine=TRUE,
>                       wine="/usr/bin/wine",
>                       debug=F)
>
> As I said, the WinBUGS14 window opens, the log file says
>
> display(log)
> check(z:/home/pauljohn/.wine/fake_windows/temp/model.txt)
> model is syntactically correct
> data(z:/home/pauljohn/.wine/fake_windows/temp/data.txt)
> data loaded
> compile(1)
> model compiled
> inits(1,z:/home/pauljohn/.wine/fake_windows/temp/init1.txt)
> model is initialized
> gen.inits()
> command #Bugs:gen.inits cannot be executed (is greyed out)
> beg(1001)
> thin.updater(1)
> set(theta)
> set(mu.theta)
> set(sigma.theta)
> set(deviance)
> update(1000)
> dic.set()
> update(0)
> stats(*)
> no monitor set
> dic.stats()
>
>
> However, after all that up pops a window called "TRAP"  (pasted next).
>  I believe this is going wrong becuase the fourth-to-last line is
> update(0).  If I manually change that to update(30000), then the
> script does not crash.  So there's something wrong in rbugs in taking
> "n.iter" into the script.  I'll try to follow that up.
>
> Here's the TRAP window output.  I believe it is much ado about
> nothing--update(0) is causing it.
>
> undefined real result
>
>  MonitorsSummary.StdMonitor.Mean   [000002FBH]
>         .mean   REAL    2.121995790965272E-314
>         .monitor        MonitorsSummary.StdMonitor      [0113F3F0H]
>  DeviancePlugin.EstimatedMeans   [000003FBH]
>         .cursor GraphStochastic.List    [01145FE0H]
>         .i      INTEGER 0
>         .j      INTEGER 2143872776
>         .list   GraphStochastic.List    [01145FE0H]
>         .node   GraphStochastic.Node    [01044600H]
>         .size   INTEGER 1
>         .value  REAL    2.121995790965272E-314
>  DevianceInterface.DICAll   [0000058EH]
>         .dBar   REAL    3.486226116065809E+307
>         .dBarTotal      REAL    3.486184325428448E+307
>         .dHat   REAL    1.856490554558022E-303
>         .dHatTotal      REAL    8.948601956767742E-317
>         .dicTotal       REAL    1.342105535202697E-102
>         .f      TextMappers.Formatter   Fields
>         .i      INTEGER 2143868088
>         .monitors       POINTER [01145ED0H]
>         .num    INTEGER 1
>         .pDTotal        REAL    4.244199150914158E-313
>         .res    INTEGER 0
>  DevianceCmds.DIC   [0000012DH]
>         .asc    INTEGER 104775
>         .dsc    INTEGER 28575
>         .f      TextMappers.Formatter   Fields
>         .height INTEGER 16821552
>         .lines  INTEGER 553779732
>         .res    INTEGER 0
>         .t      TextModels.Model        [01145E50H]
>         .title  ARRAY 256 OF CHAR       "DIC"
>         .width  INTEGER 114300
>  StdInterpreter.CallProc   [0000047AH]
>         .a      BOOLEAN FALSE
>         .b      BOOLEAN FALSE
>         .c      BOOLEAN FALSE
>         .i      Meta.Item       Fields
>         .imported       ARRAY 256 OF CHAR       ""   ...
>         .importing      ARRAY 256 OF CHAR       ""   ...
>         .mn     Meta.Name       "DevianceCmds"
>         .mod    StdInterpreter.Ident    "DevianceCmds"
>         .object ARRAY 256 OF CHAR       ""   ...
>         .ok     BOOLEAN TRUE
>         .parType        INTEGER 3
>         .pn     Meta.Name       "DIC"
>         .proc   StdInterpreter.Ident    "DIC"   ...
>         .res    INTEGER 0
>         .v      StdInterpreter.ProcVal  Fields
>         .vi     StdInterpreter.ProcIVal Fields
>         .vii    StdInterpreter.ProcIIVal        Fields
>         .vr     StdInterpreter.ProcRVal Fields
>         .vri    StdInterpreter.ProcRIVal        Fields
>         .vrii   StdInterpreter.ProcRIIVal       Fields
>         .vrr    StdInterpreter.ProcRRVal        Fields
>         .vrri   StdInterpreter.ProcRRIVal       Fields
>         .vrrii  StdInterpreter.ProcRRIIVal      Fields
>         .vrs    StdInterpreter.ProcRSVal        Fields
>         .vrsi   StdInterpreter.ProcRSIVal       Fields
>         .vrsii  StdInterpreter.ProcRSIIVal      Fields
>         .vs     StdInterpreter.ProcSVal Fields
>         .vsi    StdInterpreter.ProcSIVal        Fields
>         .vsii   StdInterpreter.ProcSIIVal       Fields
>         .vsr    StdInterpreter.ProcSRVal        Fields
>         .vsri   StdInterpreter.ProcSRIVal       Fields
>         .vsrii  StdInterpreter.ProcSRIIVal      Fields
>         .vss    StdInterpreter.ProcSSVal        Fields
>         .vssi   StdInterpreter.ProcSSIVal       Fields
>         .vssii  StdInterpreter.ProcSSIIVal      Fields
>  StdInterpreter.Command   [0000131CH]
>         .left   StdInterpreter.Ident    "DevianceCmds"
>         .ptype  INTEGER 3
>         .right  StdInterpreter.Ident    "DIC"   ...
>  StdInterpreter.CallHook.Call   [00001441H]
>         .ch     CHAR    0X
>         .e      ARRAY 64 OF CHAR        ""   ...
>         .errorMsg       ARRAY 1 OF CHAR ""
>         .f      ARRAY 64 OF CHAR        ""   ...
>         .g      ARRAY 64 OF CHAR        ""   ...
>         .hook   StdInterpreter.CallHook [01060050H]
>         .i      INTEGER 24
>         .i0     INTEGER 0
>         .i1     INTEGER 0
>         .id     StdInterpreter.Ident    "DIC"   ...
>         .par0   Dialog.String   ""   ...
>         .par1   Views.Title     ""   ...
>         .proc   ARRAY 240 OF CHAR       "DevianceCmds.DIC('DIC')"   ...
>         .res    INTEGER 0
>         .s0     Dialog.String   "DIC"
>         .s1     Dialog.String   ""   ...
>         .type   INTEGER 3
>         .x      INTEGER 0
>  Dialog.Call   [00002FC8H]
>         .errorMsg       ARRAY 1 OF CHAR ""
>         .proc   ARRAY 240 OF CHAR       "DevianceCmds.DIC('DIC')"   ...
>         .res    INTEGER 0
>  BugsScript.Call   [00000130H]
>         .bugsCommands   ARRAY 240 OF CHAR       "DevianceCmds.DIC('DIC')"   ...
>         .i      INTEGER 23
>         .item   Meta.Item       Fields
>         .j      INTEGER 23
>         .ok     BOOLEAN FALSE
>         .par    Dialog.Par      Fields
>         .pos    INTEGER -1
>         .res    INTEGER 0
>         .s      ARRAY 240 OF CHAR       "DevianceCmds.DIC('DIC')"   ...
>         .scriptCommand  ARRAY 240 OF CHAR       "#Bugs:dic.stats"   ...
>         .start  INTEGER 18
>         .v      BugsScript.RECORD       Fields
>  BugsScript.Action.Do   [0000062FH]
>         .a      BugsScript.Action       [011D2120H]
>         .argNum INTEGER 0
>         .bugsCommands   ARRAY 240 OF CHAR       "DevianceCmds.DIC('DIC')"   ...
>         .p      ARRAY 3, 120 OF CHAR    Elements
>         .s      BugsScanners.Scanner    Fields
>         .scriptCommand  ARRAY 240 OF CHAR       "#Bugs:dic.stats"   ...
>         .vectorName     BOOLEAN FALSE
>  Services.Exec   [00000136H]
>         .a      Services.Action [011D2120H]
>         .t      POINTER [21C90170H]
>  Services.IterateOverActions   [000002F4H]
>         .p      Services.Action [011D2120H]
>         .t      POINTER NIL
>         .time   LONGINT 3498
>  Services.StdHook.Step   [0000034DH]
>         .h      Services.StdHook        [0101E380H]
>  HostWindows.Idle   [00004A86H]
>         .focus  BOOLEAN FALSE
>         .tick   Controllers.TickMsg     Fields
>         .w      HostWindows.Window      NIL
>  HostMenus.TimerTick   [00003422H]
>         .lParam INTEGER 0
>         .ops    Controllers.PollOpsMsg  Fields
>         .wParam INTEGER 1
>         .wnd    INTEGER 65574
>  Kernel.Try   [00003A61H]
>         .a      INTEGER 65574
>         .b      INTEGER 1
>         .c      INTEGER 0
>         .h      PROCEDURE       HostMenus.TimerTick
>  HostMenus.ApplWinHandler   [00003841H]
>         .Proc   PROCEDURE       NIL
>         .hit    BOOLEAN Undefined70
>         .lParam INTEGER 0
>         .message        INTEGER 275
>         .res    INTEGER 1180843108
>         .s      ARRAY 256 OF SHORTCHAR  2X   ...
>         .w      INTEGER 538416909
>         .wParam INTEGER 1
>         .wnd    INTEGER 65574
> <system>   (pc=465EDB29H,  fp=7FC8FB00H)
> <system>   (pc=465EE418H,  fp=7FC8FB3CH)
> <system>   (pc=465F1DF0H,  fp=7FC8FB80H)
> <system>   (pc=465BD031H,  fp=7FC8FBB0H)
>  HostMenus.Loop   [00003BDEH]
>         .done   BOOLEAN FALSE
>         .f      SET     {0..5}
>         .n      INTEGER 0
>         .res    INTEGER 0
>         .w      HostWindows.Window      NIL
>  Kernel.Start   [00002B8CH]
>         .code   PROCEDURE       HostMenus.Loop
>
>
> 3. Accessing OpenBUGS with wine from "rbugs" gets as far as opening
> the OpenBUGS GUI, but nothing ever appears in the log file and there
> are no computations going on (according to system monitors, anyway),
> but everything in OpenBUGS just seems stuck.
>
>
> pj
>
>
> On 1/17/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > Paul Johnson wrote:
> > > Thanks, Uwe
> > >
> > > that clears up why I can't make R2WinBUGs work with OpenBUGS and WinBUGS1.5 :)
> > > Both work pretty good with Wine in a GUI.  I noticed that when I tried
> > > "rbugs", it does succeed in starting WinBUGS GUI, but then nothing
> > > happens. I'll get WinBUGS1.4 and see what happens.
> > >
> > > In the meanwhile, I'm going to t ry to see what BRugs is good for. In
> > > Linux, when I try to install BRugs, the install fails with an error
> > > saying that, at the current time, BRugs works only in Windows.
> >
> >
> > Yes, I have added that particular line in order not to confuse users,
> > and I thought I told you in my last message that it works only under
> > Windows.
> >
> >
> > > * Installing *source* package 'BRugs' ...
> > > Package 'BRugs' currently only works under Windows.\nIt is supposed to
> > > work under Linux in future releases.
> > >
> > > I'd like to stop that check and see what happens!
> >
> > OK, just remove the configure file.
> >
> >
> > > The way I read the
> > > sourcecode from OpenBUGS and BRugs, I need to replace the windows dll
> > > install and instead put in an so file (as in OpenBUGS).
> >
> > Yes, it is already shipped, and the infrastructure in the package is
> > ready (hopefully), but the brugs.so file does not work as expected.
> >
> >
> > > If anybody has done this, please let me know of your experience.
> >
> > Yes, several tried, among them Andrew Thomas and Uwe Ligges, and then I
> > invited Andrew Thomas to Dortmund and we tried together (I have to admit
> > that I was clueless all the time and in fact Andrew tried).
> > Andrew's conclusion was that there is some compiler problem on Linux
> > with the BlackBox framework (Component Pascal compiler from Oberon
> > microsystems) in which WinBUGS/OpenBUGS is written in ...
> >
> > If you get it to work, please let us know!
> >
> > Uwe
> >
>
> --
> Paul E. Johnson
> Professor, Political Science
> 1541 Lilac Lane, Room 504
> University of Kansas
>


--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From gregor.gorjanc at bfro.uni-lj.si  Wed Jan 18 07:44:15 2006
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Wed, 18 Jan 2006 07:44:15 +0100
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <13e802630601172002y9719470p7f5479ea9fd90adc@mail.gmail.com>
References: <43CD7B5F.9050804@bfro.uni-lj.si>
	<13e802630601172002y9719470p7f5479ea9fd90adc@mail.gmail.com>
Message-ID: <43CDE3BF.3070301@bfro.uni-lj.si>

Paul Johnson wrote:
> Do you mean to say that you have actually made OpenBUGS run with
> R2WinBUGS in Linux?
> 

No, I did not say this.

> Gelman's page seems to state that OpenBUGS support is brought in from
> BRugs, which is still Windows-only.

Well, Gelman changed his site a bit. Few days (weeks?) ago there was a
red text at the top of [1] stating that bugs.R was accomodated to work
with OpenBUGS. Since bugs.R was base for R2WinBUGS, I conclude that
BRugs was not involved here. Since then Gelman was active with his pages
and things changed/improved. I do not know what he has done to handle
also OpenBUGS. Perhaps Gelman and R2WinBUGS maintainers could tell us
and I hope that upstream (by Gelman) changes will find way into
R2WinBUGS package.

>>
>>>Re R packages:
>>>- R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
>>>can speak with WinBUGS under wine thanks to user contributions. But it
>>>still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
>>>Windows).
>>
>>However Andrew Gelman, has added also support[1] for OpenBUGS so this
>>might be also a good news for Linux if wine is used. Changelog can be
>>found at [2].
>>
>>[1]http://www.stat.columbia.edu/~gelman/bugsR/
>>[2]http://www.stat.columbia.edu/~gelman/bugsR/bugs.R

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From krcabrer at epm.net.co  Wed Jan 18 08:35:35 2006
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Wed, 18 Jan 2006 02:35:35 -0500
Subject: [R] Data frame index?
In-Reply-To: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
Message-ID: <43CDEFC7.5000805@epm.net.co>

Hi, R users:

I have a data.frame (not a matrix), I got a vector with the same length 
as the
number of records (rows) of the data frame, and each element of
that vector is the column number (in a specific range of columns) of the 
corresponding
record that I must set to zero.

How can I  do this without a "for" loop?

Thank you for your help.

Kenneth


From petr.pikal at precheza.cz  Wed Jan 18 09:01:29 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 18 Jan 2006 09:01:29 +0100
Subject: [R] For each element in vector do...
In-Reply-To: <1137493913.4525.97.camel@localhost.localdomain>
References: <43CCFFA2.5040209@siol.net>
Message-ID: <43CE03E9.15564.414162@localhost>

Hi

Or mabe easier in this case

b <- a+1-(a==0)


HTH
Petr


On 17 Jan 2006 at 5:31, tom wright wrote:

From:           	tom wright <tom at maladmin.com>
To:             	Andrej Kastrin <andrej.kastrin at siol.net>
Date sent:      	Tue, 17 Jan 2006 05:31:53 -0500
Copies to:      	r-help <r-help at stat.math.ethz.ch>
Subject:        	Re: [R] For each element in vector do...

> > a<-c(0,1,2,3,0,4,5)
> > b<-vector(length=length(a))
> > b[a>0]<-a[a>0]+1
> > b[a<=0]<-a[a<=0]
> > b
> [1] 0 2 3 4 0 5 6
> 
> 
> On Tue, 2006-17-01 at 15:30 +0100, Andrej Kastrin wrote:
> > Dear R useRs,
> > 
> > I have a vector with positive and negative numbers:
> > A=c(0,1,2,3,0,4,5)
> > 
> > Now if  i-th element in vector A is > 0, then i-th element in vector
> > B is a+1 else i-th element in vector b=a (or 0)
> > 
> > vector A: 0 1 2 3 0 4 5
> > vector B: 0 2 3 4 0 5 6
> > 
> > What's the right way to do this. I still have some problems with for
> > and if statements...
> > 
> > Cheers,  Andrej
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From jacques.veslot at cirad.fr  Wed Jan 18 09:09:47 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 18 Jan 2006 12:09:47 +0400
Subject: [R] Data frame index?
In-Reply-To: <43CDEFC7.5000805@epm.net.co>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
	<43CDEFC7.5000805@epm.net.co>
Message-ID: <43CDF7CB.7010806@cirad.fr>

try:

DF2 <- as.data.frame(matrix(vec, nr=nrow(DF),nc=ncol(DF))==
            matrix(1:ncol(DF),nr=nrow(DF),nc=ncol(DF),byrow=T))

DF3 <- data.frame(mapply(function(z,x,y) { x[y] <- 0 ; x },
   names(DF), DF, DF2, SIMPLIFY=F))

but there must be an easier way...


Kenneth Cabrera a ??crit :

> Hi, R users:
>
> I have a data.frame (not a matrix), I got a vector with the same 
> length as the
> number of records (rows) of the data frame, and each element of
> that vector is the column number (in a specific range of columns) of 
> the corresponding
> record that I must set to zero.
>
> How can I  do this without a "for" loop?
>
> Thank you for your help.
>
> Kenneth
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Wed Jan 18 09:50:13 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 18 Jan 2006 09:50:13 +0100
Subject: [R] Data frame index?
In-Reply-To: <43CDEFC7.5000805@epm.net.co>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
Message-ID: <43CE0F55.11558.6DDDCB@localhost>

Hi

eg. your data frame has 35 rows and 6 columns

a<-sample(1:6, 35, replace=T)
b<-1:35
vec<-rep(0,35*6)
vec[a+6*(b-1)]<-1

This shall do the replacement
your.d.f[matrix(vec,35,6, byrow=T)==1] <- 0

But I am not sure if it is quicker than a loop.

HTH
Petr


On 18 Jan 2006 at 2:35, Kenneth Cabrera wrote:

Date sent:      	Wed, 18 Jan 2006 02:35:35 -0500
From:           	Kenneth Cabrera <krcabrer at epm.net.co>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Data frame index?

> Hi, R users:
> 
> I have a data.frame (not a matrix), I got a vector with the same
> length as the number of records (rows) of the data frame, and each
> element of that vector is the column number (in a specific range of
> columns) of the corresponding record that I must set to zero.
> 
> How can I  do this without a "for" loop?
> 
> Thank you for your help.
> 
> Kenneth
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From hypan at scbit.org  Wed Jan 18 09:51:22 2006
From: hypan at scbit.org (Haiyan Pan)
Date: Wed, 18 Jan 2006 16:51:22 +0800
Subject: [R] compiling R on powerpc-ibm-aix5.1.0.0
Message-ID: <200601180851.k0I8pYOv000498@hypatia.math.ethz.ch>

r-help

   I am trying to compile R on an powerpc-IBM-AIX5.1.0.0  machine, Is R suitable to be used in this system? The R Installation and Administration document mentioned  rs6000-ibm-aix not powerpc-IBM-AIX5.1.0.0 .
  When I tried to compile R.2.2.0 in powerpc-IBM-AIX5.1.0.0  using the following steps:
(1) ./configure
   There is an error message :configure: error: --with-readline=yes (default) and headers/libs are not available
  
  So I used "./configure  --with-readline=no --prefix=/export/home/hypan/R/bin" instead, and configure is through.
  The following is information when finished configure
  
  R is now configured for powerpc-ibm-aix5.1.0.0

  Source directory:          .
  Installation directory:    /export/home/hypan/R/bin

  C compiler:                gcc -mno-fp-in-toc -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:        
  Additional capabilities:   PNG, JPEG, MBCS, NLS
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build info or html versions of the R manuals
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer

*** Warning: the GNU linker, at least up to release 2.9.1, is reported
*** to be unable to reliably create shared libraries on AIX.
*** Therefore, libtool is disabling shared libraries support.  If you
*** really care for shared libraries, you may want to modify your PATH
*** so that a non-GNU linker is found, and then restart.

(2) make
  make is failed, the error message is:
  gcc -Wl,-bdynamic -Wl,-bE:../../etc/R.exp -Wl,-bM:SRE -L/usr/local/lib -o R.bin  Rmain.o  CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o character.o coerce.o colors.o complex.o connections.o context.o cov.o cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o logic.o main.o mapply.o match.o memory.o model.o names.o objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o sysutils.o unique.o util.o version.o vfonts.o xxxpr.o ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a   -lg2c -lm -lgcc_s /usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3/libgcc.a -lg /lib/crt0.o ../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a ../extra/intl/libintl.a   -ldl -lm -lc -liconv

/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3/../../../../powerpc-ibm-aix5.1.0.0/bin/ld: target dynamic not found
collect2: ld returned 1 exit status
make: 1254-004 The error code from the last command is 1.
Stop.
make: 1254-004 The error code from the last command is 2.
Stop.
make: 1254-004 The error code from the last command is 1.
Stop.
make: 1254-004 The error code from the last command is 1.
Stop.


your help will be greatly appreciated. Thanks!

Haiyan

= = = = = = = = = = = = = = = = = = = = 
  
 Haiyan Pan
 hypan at scbit.org

 Tel: 021-64363311-123
 Shanghai Center for Bioinformatics Technology
 Floor 12th,100# QinZhou Road
 Shanghai,China,200235



From dimitris.rizopoulos at med.kuleuven.be  Wed Jan 18 09:58:41 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 18 Jan 2006 09:58:41 +0100
Subject: [R] Data frame index?
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
	<43CDEFC7.5000805@epm.net.co>
Message-ID: <010701c61c0d$61244f80$0540210a@www.domain>

you could try something like the following:

dat <- data.frame(matrix(rnorm(200), 20, 10))
index <- sample(10, 20, TRUE)
###############
mat.ind <- matrix(FALSE, nrow(dat), length(dat))
mat.ind[cbind(seq(along = index), index)] <- TRUE
dat[mat.ind] <- 0

index
dat


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Kenneth Cabrera" <krcabrer at epm.net.co>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 18, 2006 8:35 AM
Subject: [R] Data frame index?


> Hi, R users:
>
> I have a data.frame (not a matrix), I got a vector with the same 
> length
> as the
> number of records (rows) of the data frame, and each element of
> that vector is the column number (in a specific range of columns) of 
> the
> corresponding
> record that I must set to zero.
>
> How can I  do this without a "for" loop?
>
> Thank you for your help.
>
> Kenneth
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From rausch_robert at web.de  Wed Jan 18 10:15:49 2006
From: rausch_robert at web.de (Robert Michael Rausch)
Date: Wed, 18 Jan 2006 10:15:49 +0100
Subject: [R] Own Color Palette
Message-ID: <330527823@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/0d603843/attachment.pl

From Roger.Bivand at nhh.no  Wed Jan 18 11:03:45 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 18 Jan 2006 11:03:45 +0100 (CET)
Subject: [R] Own Color Palette
In-Reply-To: <330527823@web.de>
Message-ID: <Pine.LNX.4.44.0601181103140.28692-100000@reclus.nhh.no>

On Wed, 18 Jan 2006, Robert Michael Rausch wrote:

?colorRampPalette


> 
> Own Color Palette
> 
> Dear all,<?xml:namespace prefix = o ns = "urn:schemas-microsoft-com:office:office" /><o:p></o:p>
> 
> I would like to generate a contour-plot according to a master plot. The problem is that the rainbow-palette included in R does not answer this purpose. I need a darker blue, no turquoise, relatively less green, more yellow and more red. Haw can I adjust the rainbow? Alternatively: How can I generate my own palette with at least 100 colors with smooth transitions?<o:p></o:p>
> 
> Can anybody help me?<o:p></o:p>
> 
> Thanks a lot<o:p></o:p>
> 
> Robert<o:p></o:p>
> 
>  <o:p></o:p>
> 
>  <o:p></o:p>
> 
>  
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From assampryseley at yahoo.com  Wed Jan 18 11:42:23 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Wed, 18 Jan 2006 02:42:23 -0800 (PST)
Subject: [R] Help with mixed effects models
In-Reply-To: <mailman.11.1137495602.26845.r-help@stat.math.ethz.ch>
Message-ID: <20060118104223.97978.qmail@web37109.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/4af9b871/attachment.pl

From gelman at stat.columbia.edu  Wed Jan 18 11:57:43 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Wed, 18 Jan 2006 05:57:43 -0500
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <43CDE3BF.3070301@bfro.uni-lj.si>
References: <43CD7B5F.9050804@bfro.uni-lj.si>
	<13e802630601172002y9719470p7f5479ea9fd90adc@mail.gmail.com>
	<43CDE3BF.3070301@bfro.uni-lj.si>
Message-ID: <43CE1F27.9040003@stat.columbia.edu>

Hi all.  Sorry for any confusion.  At one point, close to a year ago I 
think, I put in the effort to get the bugs() function in R to work with 
OpneBUGS.  But at the time, I couldn't get OpenBUGS to do much.  After 
some effort I got it to work on some simple examples but I couldn't get 
it to work on some of my other examples (e.g., a mulitlevel logistic 
regression).  In the meantime, Uwe and Sibylle set up R2WinBUGS.  I 
still think that BRugs would be improved by an R front-end (basically, 
the bugs() function adapted to BRugs).  But I haven't tried to get 
OpenBUGS working for awhile so I don't know if it currently fits the 
models I'd like to fit.  It certainly wouldn't be difficult to extend 
the bugs() fuction to do the appropriate BRugs calls.

In a different direction, Jouni Kerman wrote a function to take output 
from chains of iterative simulation--not necessarily from Bugs, they 
could come from mcmcsamp() or just your own customized Gibbs or 
Metropolis program--and convert them into bugs objects, which is nice 
because then you can automatically monitor convergence and look at the 
inferences using the built-in plot() and print() options in R2WinBUGS.  
We'll try to put this in R2WinBUGS soon.

A key part of a bugs object is the way a vector or matrix of parameters 
with the same name is structured as a vector or matrix, not simply as 
part of a long vector of parameters (this can be clearly seen in the 
right half of the plot() output for a hiearchical model fit in Bugs, and 
also can be seen if you fit a bugs model and then do attach.bugs() and 
look at simulations of parameter vectors or arrays.

Gregor Gorjanc wrote:

>Paul Johnson wrote:
>  
>
>>Do you mean to say that you have actually made OpenBUGS run with
>>R2WinBUGS in Linux?
>>
>>    
>>
>
>No, I did not say this.
>
>  
>
>>Gelman's page seems to state that OpenBUGS support is brought in from
>>BRugs, which is still Windows-only.
>>    
>>
>
>Well, Gelman changed his site a bit. Few days (weeks?) ago there was a
>red text at the top of [1] stating that bugs.R was accomodated to work
>with OpenBUGS. Since bugs.R was base for R2WinBUGS, I conclude that
>BRugs was not involved here. Since then Gelman was active with his pages
>and things changed/improved. I do not know what he has done to handle
>also OpenBUGS. Perhaps Gelman and R2WinBUGS maintainers could tell us
>and I hope that upstream (by Gelman) changes will find way into
>R2WinBUGS package.
>
>  
>
>>>>Re R packages:
>>>>- R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
>>>>can speak with WinBUGS under wine thanks to user contributions. But it
>>>>still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
>>>>Windows).
>>>>        
>>>>
>>>However Andrew Gelman, has added also support[1] for OpenBUGS so this
>>>might be also a good news for Linux if wine is used. Changelog can be
>>>found at [2].
>>>
>>>[1]http://www.stat.columbia.edu/~gelman/bugsR/
>>>[2]http://www.stat.columbia.edu/~gelman/bugsR/bugs.R
>>>
-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman



From luqiang at inforsense.com  Wed Jan 18 12:18:23 2006
From: luqiang at inforsense.com (Lu Qiang)
Date: Wed, 18 Jan 2006 06:18:23 -0500
Subject: [R] compiling R on powerpc-ibm-aix5.1.0.0
Message-ID: <58B7378F4748404AB7133931B25E7CC872E627@ms10.mse5.exchange.ms>


Hi, 

As the message pointed out, the current linker cannot produce share library. 
However gcc used option "-bdynamic", is that for share library? I guess. Need to read its gcc manual. 

It might be caused by very slight difference of the gcc compiler in your AIX. 
Please seek for Zhao Wei's help. He has very good experience in operating UNIX-like system.

If the gcc in your AIX is with low version, not suitable, how to do? hmmm ... anyway, let zhaowei have a look firstly.

Regards,

Lu



-----Original Message-----
From: Haiyan Pan [mailto:hypan at scbit.org] 
Sent: 18 January 2006 08:51
To: r-help
Cc: Dan Yang; Lu Qiang
Subject: compiling R on powerpc-ibm-aix5.1.0.0

r-help?

   I am trying to compile R on an powerpc-IBM-AIX5.1.0.0  machine, Is R suitable to be used in this system? The R Installation and Administration document mentioned  rs6000-ibm-aix not powerpc-IBM-AIX5.1.0.0 .
  When I tried to compile R.2.2.0 in powerpc-IBM-AIX5.1.0.0  using the following steps:
(1) ./configure
   There is an error message :configure: error: --with-readline=yes (default) and headers/libs are not available
  
  So I used "./configure  --with-readline=no --prefix=/export/home/hypan/R/bin" instead, and configure is through.
  The following is information when finished configure
  
  R is now configured for powerpc-ibm-aix5.1.0.0

  Source directory:          .
  Installation directory:    /export/home/hypan/R/bin

  C compiler:                gcc -mno-fp-in-toc -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:        
  Additional capabilities:   PNG, JPEG, MBCS, NLS
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build info or html versions of the R manuals
configure: WARNING: I could not determine a browser
configure: WARNING: I could not determine a PDF viewer

*** Warning: the GNU linker, at least up to release 2.9.1, is reported
*** to be unable to reliably create shared libraries on AIX.
*** Therefore, libtool is disabling shared libraries support.  If you
*** really care for shared libraries, you may want to modify your PATH
*** so that a non-GNU linker is found, and then restart.

(2) make
  make is failed, the error message is:
  gcc -Wl,-bdynamic -Wl,-bE:../../etc/R.exp -Wl,-bM:SRE -L/usr/local/lib -o R.bin  Rmain.o  CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o character.o coerce.o colors.o complex.o connections.o context.o cov.o cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o logic.o main.o mapply.o match.o memory.o model.o names.o objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o sysutils.o unique.o util.o version.o vfonts.o xxxpr.o ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a   -lg2c -lm -lgcc_s /usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3/libgcc.a -lg /lib/crt0.o ../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a ../extra/intl/libintl.a   -ldl -lm -lc -liconv

/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3/../../../../powerpc-ibm-aix5.1.0.0/bin/ld: target dynamic not found
collect2: ld returned 1 exit status
make: 1254-004 The error code from the last command is 1.
Stop.
make: 1254-004 The error code from the last command is 2.
Stop.
make: 1254-004 The error code from the last command is 1.
Stop.
make: 1254-004 The error code from the last command is 1.
Stop.


your help will be greatly appreciated. Thanks!

Haiyan

= = = = = = = = = = = = = = = = = = = = 
  
 Haiyan Pan
 hypan at scbit.org

 Tel: 021-64363311-123
 Shanghai Center for Bioinformatics Technology
 Floor 12th,100# QinZhou Road
 Shanghai,China,200235



From sourceforge at metrak.com  Wed Jan 18 12:20:55 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Wed, 18 Jan 2006 22:20:55 +1100
Subject: [R] y axis text truncated
Message-ID: <43CE2497.2080202@metrak.com>

I have been trying to find which par settings can help me avoid 
truncated text at the bottom of the y axis in a mosaic plot (created 
when I plot a result of a 2d xtabs) without much success.  Using las=1 
has helped but the text (the "500+" level) is still cropped.

I get the same result on XP/2.2.0 and FC4/2.2.1.

Any tips would be appreciated.

# dput(foo.df)
 > foo.df = structure(list(vol1 = structure(c(1, 2, 3, 4, 5, 6, 1, 2, 3,
4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6
), .Label = c("<100", "101-250", "251-500", "501-750", "751-1000",
"1000+"), class = "factor"), vol2 = structure(c(1, 1, 1, 1, 1,
1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,
5, 5, 5, 5), .Label = c("<20", "20-50", "50-100", "100-500",
"500+"), class = "factor"), Freq = c(4, 3, 0, 0, 2, 0, 4, 3,
6, 4, 1, 2, 1, 3, 3, 4, 5, 2, 3, 1, 3, 2, 2, 12, 0, 0, 1, 0,
2, 4)), .Names = c("vol1", "vol2", "Freq"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28", "29", "30"), class = "data.frame")

 > xtabs(Freq ~ vol1 + vol2, data=foo.df)
           vol2
vol1       <20 20-50 50-100 100-500 500+
   <100       4     4      1       3    0
   101-250    3     3      3       1    0
   251-500    0     6      3       3    1
   501-750    0     4      4       2    0
   751-1000   2     1      5       2    2
   1000+      0     2      2      12    4

 > plot(xtabs(Freq ~ vol1 + vol2, data=foo.df))
 > plot(xtabs(Freq ~ vol1 + vol2, data=foo.df), las=1)



From ligges at statistik.uni-dortmund.de  Wed Jan 18 12:29:52 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 18 Jan 2006 12:29:52 +0100
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <43CE1F27.9040003@stat.columbia.edu>
References: <43CD7B5F.9050804@bfro.uni-lj.si>	<13e802630601172002y9719470p7f5479ea9fd90adc@mail.gmail.com>	<43CDE3BF.3070301@bfro.uni-lj.si>
	<43CE1F27.9040003@stat.columbia.edu>
Message-ID: <43CE26B0.3080204@statistik.uni-dortmund.de>

Andrew Gelman wrote:

> Hi all.  Sorry for any confusion.  At one point, close to a year ago I 
> think, I put in the effort to get the bugs() function in R to work with 
> OpneBUGS.  But at the time, I couldn't get OpenBUGS to do much.  After 
> some effort I got it to work on some simple examples but I couldn't get 
> it to work on some of my other examples (e.g., a mulitlevel logistic 
> regression).  In the meantime, Uwe and Sibylle set up R2WinBUGS.  I 
> still think that BRugs would be improved by an R front-end (basically, 
> the bugs() function adapted to BRugs).  But I haven't tried to get 

Andrew, let me add a few details:

The BRugs equivalent to your bugs() function is called BRugsFit() - it 
runs the whole stuff fo you with one function call. Some plots can be 
produced afterwards with a second call.

For Linux, what we really want is to get the OpenBUGS library (i.e. 
BRugs.so) run in Linux natively. All other stuff are ugly workarounds.

Since it looks not that promising to get a quick solution these days, it 
might make sense to communicate to some WinBUGS/OpenBUGS running under 
wine. Then, R2WinBUGS could be used for WinBUGS 1.4.x under wine and for 
OpenBUGS something similar to
  - communication structure of R2WinBUGS *and*
  - interface to the OpenBUGS command language
is desirable.

In principle, one could look at BRugs and take the relevant information 
originally passed through .C() calls. Instead of calling .C(), print the 
commands into text files and start OpenBUGS so that it processed the 
generated command files. Unfortunately, one cannot run "interactive" 
simulations that way and is limited to the functionality R2WinBUGS 
already provides.


> OpenBUGS working for awhile so I don't know if it currently fits the 
> models I'd like to fit.  It certainly wouldn't be difficult to extend 
> the bugs() fuction to do the appropriate BRugs calls.
> 
> In a different direction, Jouni Kerman wrote a function to take output 
> from chains of iterative simulation--not necessarily from Bugs, they 
> could come from mcmcsamp() or just your own customized Gibbs or 
> Metropolis program--and convert them into bugs objects, which is nice 
> because then you can automatically monitor convergence and look at the 
> inferences using the built-in plot() and print() options in R2WinBUGS.  
> We'll try to put this in R2WinBUGS soon.

In which case one could use the coda package.


> A key part of a bugs object is the way a vector or matrix of parameters 
> with the same name is structured as a vector or matrix, not simply as 
> part of a long vector of parameters (this can be clearly seen in the 
> right half of the plot() output for a hiearchical model fit in Bugs, and 
> also can be seen if you fit a bugs model and then do attach.bugs() and 
> look at simulations of parameter vectors or arrays.

Hope this is the case in all representations of similar objects.

Anyway, among build-in plot facilities, BRugs supports conversion to 
coda's mcmc objects.

Uwe



> Gregor Gorjanc wrote:
> 
> 
>>Paul Johnson wrote:
>> 
>>
>>
>>>Do you mean to say that you have actually made OpenBUGS run with
>>>R2WinBUGS in Linux?
>>>
>>>   
>>>
>>
>>No, I did not say this.
>>
>> 
>>
>>
>>>Gelman's page seems to state that OpenBUGS support is brought in from
>>>BRugs, which is still Windows-only.
>>>   
>>>
>>
>>Well, Gelman changed his site a bit. Few days (weeks?) ago there was a
>>red text at the top of [1] stating that bugs.R was accomodated to work
>>with OpenBUGS. Since bugs.R was base for R2WinBUGS, I conclude that
>>BRugs was not involved here. Since then Gelman was active with his pages
>>and things changed/improved. I do not know what he has done to handle
>>also OpenBUGS. Perhaps Gelman and R2WinBUGS maintainers could tell us
>>and I hope that upstream (by Gelman) changes will find way into
>>R2WinBUGS package.
>>
>> 
>>
>>
>>>>>Re R packages:
>>>>>- R2WinBUGS is compatible with WinBUGS-1.4.x only, its newest version
>>>>>can speak with WinBUGS under wine thanks to user contributions. But it
>>>>>still depends on WinBUGS-1.4.x, hence Windows only (considering wine as
>>>>>Windows).
>>>>>       
>>>>>
>>>>
>>>>However Andrew Gelman, has added also support[1] for OpenBUGS so this
>>>>might be also a good news for Linux if wine is used. Changelog can be
>>>>found at [2].
>>>>
>>>>[1]http://www.stat.columbia.edu/~gelman/bugsR/
>>>>[2]http://www.stat.columbia.edu/~gelman/bugsR/bugs.R
>>>>



From gelman at stat.columbia.edu  Wed Jan 18 12:37:32 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Wed, 18 Jan 2006 06:37:32 -0500
Subject: [R] Current state of support for BUGS access for Linux users?
In-Reply-To: <43CE26B0.3080204@statistik.uni-dortmund.de>
References: <43CD7B5F.9050804@bfro.uni-lj.si>	<13e802630601172002y9719470p7f5479ea9fd90adc@mail.gmail.com>	<43CDE3BF.3070301@bfro.uni-lj.si>
	<43CE1F27.9040003@stat.columbia.edu>
	<43CE26B0.3080204@statistik.uni-dortmund.de>
Message-ID: <43CE287C.3080704@stat.columbia.edu>

Uwe,

Does coda put parameters into arrays based on their names?  I had the 
impression that coda just considered parameters as one very long vector 
(as in the summary output inside the Bugs window), hence it would not 
make those nice graphs on the right side of the output from plot() 
applied to a Bugs object.  But if coda is the standard, then I suppose 
what's needed is to adapt Jouni's function to also convert coda objects 
into bugs objects?  Then any iterative simulation object could be 
displayed and accessed by parameters, not simply scalar components of 
parameters.  (Or maybe coda can do more than I realize, I don't know.)

Andrew

--

Uwe Ligges wrote:

>
> In which case one could use the coda package.
>
>
>> A key part of a bugs object is the way a vector or matrix of 
>> parameters with the same name is structured as a vector or matrix, 
>> not simply as part of a long vector of parameters (this can be 
>> clearly seen in the right half of the plot() output for a hiearchical 
>> model fit in Bugs, and also can be seen if you fit a bugs model and 
>> then do attach.bugs() and look at simulations of parameter vectors or 
>> arrays.
>
>
> Hope this is the case in all representations of similar objects.
>
> Anyway, among build-in plot facilities, BRugs supports conversion to 
> coda's mcmc objects.
>
> Uwe


-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman



From ripley at stats.ox.ac.uk  Wed Jan 18 12:48:46 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jan 2006 11:48:46 +0000 (GMT)
Subject: [R] y axis text truncated
In-Reply-To: <43CE2497.2080202@metrak.com>
References: <43CE2497.2080202@metrak.com>
Message-ID: <Pine.LNX.4.61.0601181148020.9235@gannet.stats>

Use par(xpd=TRUE).

On Wed, 18 Jan 2006, paul sorenson wrote:

> I have been trying to find which par settings can help me avoid
> truncated text at the bottom of the y axis in a mosaic plot (created
> when I plot a result of a 2d xtabs) without much success.  Using las=1
> has helped but the text (the "500+" level) is still cropped.
>
> I get the same result on XP/2.2.0 and FC4/2.2.1.
>
> Any tips would be appreciated.
>
> # dput(foo.df)
> > foo.df = structure(list(vol1 = structure(c(1, 2, 3, 4, 5, 6, 1, 2, 3,
> 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6
> ), .Label = c("<100", "101-250", "251-500", "501-750", "751-1000",
> "1000+"), class = "factor"), vol2 = structure(c(1, 1, 1, 1, 1,
> 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,
> 5, 5, 5, 5), .Label = c("<20", "20-50", "50-100", "100-500",
> "500+"), class = "factor"), Freq = c(4, 3, 0, 0, 2, 0, 4, 3,
> 6, 4, 1, 2, 1, 3, 3, 4, 5, 2, 3, 1, 3, 2, 2, 12, 0, 0, 1, 0,
> 2, 4)), .Names = c("vol1", "vol2", "Freq"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28", "29", "30"), class = "data.frame")
>
> > xtabs(Freq ~ vol1 + vol2, data=foo.df)
>           vol2
> vol1       <20 20-50 50-100 100-500 500+
>   <100       4     4      1       3    0
>   101-250    3     3      3       1    0
>   251-500    0     6      3       3    1
>   501-750    0     4      4       2    0
>   751-1000   2     1      5       2    2
>   1000+      0     2      2      12    4
>
> > plot(xtabs(Freq ~ vol1 + vol2, data=foo.df))
> > plot(xtabs(Freq ~ vol1 + vol2, data=foo.df), las=1)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From admin at biostatistic.de  Wed Jan 18 13:04:12 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 18 Jan 2006 13:04:12 +0100
Subject: [R] some EPS rotated in journal preview
Message-ID: <43CE2EBC.1020303@biostatistic.de>


I am trying to send a manuscript to a journal.
One of the figures build by R is in the right orientation and 4 are rotated clockwise 90 deg in the preview.

I used the right click save to PS option and I used the command line

postscript("c:/temp/fig04.eps",bg="transparent",onefile = TRUE  ,pointsize=20,paper = "letter",height=8,width=8,horizontal=FALSE,family = "Helvetica", font = "Helvetica")

I treed Horizontal=TRUE Ghostsript show the rotated image but not the preview from the journal. :-(


Is there anything to change that the - unknown system - of the journal will be forced to display the image in the right direction?

Regards Knut



From vivek.satsangi at gmail.com  Wed Jan 18 13:08:00 2006
From: vivek.satsangi at gmail.com (Vivek Satsangi)
Date: Wed, 18 Jan 2006 07:08:00 -0500
Subject: [R] Possible improvement in lm
Message-ID: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>

Folks,

I do a series of regressions (one for each quarter in the dataset) and
then go and extract the residuals from each stored lm object that is
returned as follows:

vResiduals <- as.vector(unlist(resid(lQuarterlyRegressions[[i]])));

Here lQuarterlyRegressions is a vector of objects returned by lm().

Next, I may go find outliers using identify() on a plot or do some
other analysis which tells me which row of the quarterly data I need
to take a closer look at.

However, if I try to match some point in one of the quarters that I
have with its residual, then I have to drop the points from my
"current Data" which have NA's for either the explanatory variables or
the explained, so that the vector or residuals and the data have the
same indexes.

This lead to some serious confusion/bugs for me, and I am wondering if
it might not be better for lm to put an NA into those rows where the
point was dropped because of NA's in the explanatory or explained
variables (currently it just returns nothing at that index). Ofcourse,
there might be some arguments against this idea, and I would be
interested to hear them.

Thank you for your time and attention,


-- Vivek Satsangi
Student, Rochester, NY USA



From murdoch at stats.uwo.ca  Wed Jan 18 13:11:41 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Jan 2006 07:11:41 -0500
Subject: [R] Data frame index?
In-Reply-To: <43CDEFC7.5000805@epm.net.co>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
	<43CDEFC7.5000805@epm.net.co>
Message-ID: <43CE307D.80706@stats.uwo.ca>

On 1/18/2006 2:35 AM, Kenneth Cabrera wrote:
> Hi, R users:
> 
> I have a data.frame (not a matrix), I got a vector with the same length 
> as the
> number of records (rows) of the data frame, and each element of
> that vector is the column number (in a specific range of columns) of the 
> corresponding
> record that I must set to zero.
> 
> How can I  do this without a "for" loop?

It sounds as though you've found that you can use two-column matrix 
indexing on a data frame for reading but not assigning.  You create a 
matrix where the first column is the row number, and the second column 
is the column number.  Then indexing by that selects those particular 
elements in order.

For instance, if you have named your vector of columns "cols", you'd do

my.data.frame[ cbind(1:rows, cols) ] <- 0

Here's an example:

 > df
    x y
1  1 a
2  1 a
3  1 a
4  1 a
5  1 a
6  1 a
7  1 a
8  1 a
9  1 a
10 1 a
 > df[cbind(1:4,c(1,2,1,2))]
[1] "1" "a" "1" "a"

But

 > df[cbind(1:4,c(1,2,1,2))] <- 0
Error in "[<-.data.frame"(`*tmp*`, cbind(1:4, c(1, 2, 1, 2)), value = 0) :
         only logical matrix subscripts are allowed in replacement

To get around this, construct the logical matrix using this method, then 
  use it as an index:

 > mat <- matrix(FALSE, 10, 2)
 > mat[cbind(1:4,c(1,2,1,2))] <- TRUE
 > df[mat] <- 0
Warning message:
invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, thisvar, 
value = 0)
 > df
    x    y
1  0    a
2  1 <NA>
3  0    a
4  1 <NA>
5  1    a
6  1    a
7  1    a
8  1    a
9  1    a
10 1    a

If your columns are all numeric, you won't get the warning I got.

Duncan Murdoch



From phgrosjean at sciviews.org  Wed Jan 18 13:12:21 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 18 Jan 2006 13:12:21 +0100
Subject: [R] R Wiki and R-sig-wikii
Message-ID: <43CE30A5.7060001@sciviews.org>

Hello all,

This is to announce the creation of R-sig-wiki, a new R SIG (Special 
Interest Group) mailing list dedicated to the elaboration and 
maintenance of a R Wiki. You can subscribe at: 
https://stat.ethz.ch/mailman/listinfo/r-sig-wiki. There is currently a 
prototype for a new R Wiki at http://www.sciviews.org/_rgui/wiki 
(temporary address). The main idea is to offer a site where users could 
collaborate in writting various kind of documentation for R. It mainly 
targets R beginners, but is left open for more advanced sections. Any R 
user interested in the setting up of this Wiki is warmly welcome to 
participate and to subscribe to R-sig-wiki. For the others, we will send 
an announcement for the final R Wiki on this list when it will be ready. 
Hereunder is a (rather long) summary of the discussion we had so far on 
this topic.
Best,

Philippe Grosjean

======================================================================
There is a proposition to create a R Wiki, stimulated essentially by two 
facts:

1) The traffic in R-Help is very high, with many "trivial" questions 
asked repeatedly. Obviously, searching in R-Help archives is not so 
obvious for some R users. Perhaps another presentation, like plain HTML 
pages would be fine. Since the building of these HTML pages should be a 
collaborative work, a Wiki seems to be a possible solution (recall that 
a Wiki is essentially a simple way to collaborate on writting Web pages; 
see Wikipedia definition at http://en.wikipedia.org/wiki/Wiki).

2) Some threads in R-Help are very long and difficult to follow in the 
form of a succession of emails. Again, a more structured presentation, 
allowed by HTML / Wiki pages is suggested as a possible solution.

There is already one attempt to build a Wiki by Detlef Steuer at: 
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl. Despite not much effort 
was put in this wiki, several people collaborated to it with time, but 
it appears that it was below the minimum required to make it fly as it 
should.

Being unaware of that Wiki, I proposed recently (beginning of January 
2006) a prototype of another R Wiki, just to explore if and how it could 
answer those two problems on R-Help. The prototype is at 
http://www.sciviews.org/_rgui/wiki (temporary address).

There is a couple of other Wikis dedicated to R floating around, like: 
http://www.okada.jp.org/RWiki/ (in Japanese) and 
http://learnserver.csd.univie.ac.at/rcomwiki dedicated to R(D)COM and 
RExcel essentially.

After a discussion, the R-Core Team decided to give support to one or 
several intiatives to make a R Wiki, with a big concern about the 
quality of information in the Wiki and how to keep it in phase with the 
rapid development of R. Here is the mail send by Martin Maechler:

========================================================================
Martin Maechler wrote:
 >We've had a small "review time" within R-core on this topic,
 >amd would like to state the following:
 >
 >------------------------------------------------------------------------
 > The R-core team welcomes proposals to develop an R-wiki.
 >
 >- We would consider linking a very small number of Wikis (ideally one)
 >  from www.r-project.org and offering an address in the r-project.org
 >  domain (such as 'wiki.r-project.org').
 >
 >- The core team has no support time to offer, and would be looking for
 >  a medium-term commitment from a maintainer team for the Wiki(s).
 >
 >- Suggestions for the R documentation would best be filtered through
 >  the Wiki maintainers, who could e.g. supply suggested patches during
 >  the alpha phase of an R release.
 >--------------------------------------------------------------------------
 >
 >Our main concerns have been about ensuring the quality of such extra
 >documentation projects, hence the 2nd point above.
 >Several of our more general, not mainly R, experiences have been
 >of outdated web pages which are continued to be used as
 >reference when their advice has long been superseded.
 >I think it's very important to try ensuring that this won't
 >happen with an R Wiki.
===========================================================================

After that announcement, several people started to discuss the structure 
and content of the Wiki (Tony Plate and Ben Bolker took the initiative 
to propose one structure on the experimental R Wiki at: 
http://www.sciviews.org/_rgui/wiki/doku.php?id=varia:organization_discussion, 
and I propose to leave it open until the end of February. At that time, 
we will implement the proposed structure as a "proof-of-concept".

Another topic is about how to allow discussion on the R documentation in 
the Wiki. This topic was fed by Frank Harrell Jr, Gabor Grothendieck and 
others and it leads to a trial on: 
http://www.sciviews.org/_rgui/wiki/doku.php?id=varia:test_include. 
Basically, all CRAN and Bioconductor .Rd files would be converted 
automatically to read-only Wiki pages (regularly updated) that are 
themselves included in  fully editables "discussion" pages. A link to 
latest svn versions of .Rd files for base packages was also suggested. 
Finally, access to the documentation online in Wiki format should be 
included as naturally as possible in R (proposal of a helpWiki() 
function), so that users would be easily and naturally redirected to the 
most up-to-date information (latest .Rd man page followed by Wiki 
additions), if their computer is connected to the Internet.

Another problem is: which Wiki engine to choose and which features to 
propose. Indeed, there are more than 30 different wiki engines, and they 
range from simplistic one (like Tipiwiki, for instance), to very 
elaborate ones (like MediaWiki, the Wikipedia engine; Trac, a complete 
solution to follow software development with bug tracking and 
collaborative documentation writting in Wiki format; or TWiki, a very 
capable, but rather complex Wiki engine used in many big companies). The 
Web site http://www.wikimatrix.org/ is excellent to compare most popular 
Wiki engines. Using this tool, and after a couple of trials, I settled 
on DokuWiki: both simple enough (PHP only, plain text files) and 
powerful (and also completelly customizable/expandable through plugins).

Currently, Detlef Steuer decides to set its ancestor R Wiki as read-only 
to the benefit of the DokuWiki prototype (the idea is to move pages to 
the new one and fuse the two R Wikis in a single one). Although I am not 
sure everybody is completelly convinced that the customized DokuWiki 
engine I porpose (more than 10 plugins added, syntax coloring for R code 
added to GeSHI, MathML rendering of LaTeX equations added) is the one to 
use, there seems to be little alternative proposed (after disucssions, 
Trac and TWiki were considered as valid alternatives, but they do now 
overcompete DokuWiki as a good compromize between simplicity and 
performance for documentation writting).

To feed the Wiki with high-quality material (more than "just" the few 
thousands of .Rd man pages of R and additional packages in CRAN and 
Bioconductor, sic!), I proposed to several authors of Web sites 
dedicated to R documentation to move to the Wiki. This will certainly 
ease to keep the information up-to-date, since everybody could 
collaborate in maintaining this material. I already got a positive 
answer from Paul Johnson for Rtips 
(http://www.ku.edu/~pauljohn/R/Rtips.html#1.1) and from James Wettenhall 
for R Tcltk Examples 
(http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/).

So, what next?
Well, I think we should not rush. We should take time to refine the Wiki 
engine, the Wiki server and the structure of the Wiki in order to 
optimize them for the intended use. So, I suggest we continue to 
experiment with the propotype for a month or two. Then, we will set up 
the definitve R Wiki site, hopefully with an address like 
http://wiki.r-project.org... (if possible)! So, currently, we are 
seeking for good ideas both about the customization of the wiki engine, 
about how to feed the .Rd files in it and about how to structure the site.

In a second phase, we will certainly need volunteers to help translate 
HTML pages from RTips, R Tcltk Examples, and possibly other Web sites to 
Wiki pages.

I hope I did not forgot something important in this summary?!
Best,

Philippe Grosjean
-- 
..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................



From steffen.katzner at mail.gwdg.de  Wed Jan 18 13:12:53 2006
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Wed, 18 Jan 2006 13:12:53 +0100
Subject: [R] Within-Subjects ANOVA & comparisons of individual means
Message-ID: <43CE30C5.5050206@mail.gwdg.de>

I am having problems with comparing individual means in a 
within-subjects ANOVA. From my understanding, TukeyHSD is not 
appropriate in this context. So I am trying to compute contrasts, as 
follows:

seven subjects participated in each of 6 conditions (intervals).

 > subject = factor(rep(c(1:7), each = 6))
 > interval = factor(rep(c(1:6), 7))

and here is the dependent variable:

 > dv = c(3.3868, 3.1068, 1.7261, 1.5415, 1.7356, 0.7538,
+ 2.5957, 1.5666, 1.1984, 1.2761, 1.0022, 0.8597,
+ 3.9819, 3.1506, 1.5824, 1.7400, 1.4248, 0.6519,
+ 2.2521, 1.5248, 1.1209, 1.2193, 1.1994, 2.0910,
+ 2.4661, 1.3863, 1.3591, 0.9163, 1.3976, 1.7471,
+ 3.2486, 1.9492, 2.4228, 1.1276, 1.2836, 0.9814,
+ 1.7148, 1.7278, 2.7433, 1.4924, 1.0992, 0.7821)

 > d = data.frame(subject, interval, dv)

next I'm defining a contrast matrix:

 > con = matrix(c(1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 
0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1), nrow=6, ncol=5, byrow=F)

 > con
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]   -1    1    0    0    0
[3,]    0   -1    1    0    0
[4,]    0    0   -1    1    0
[5,]    0    0    0   -1    1
[6,]    0    0    0    0   -1


 > contrasts(d$interval)=con

and then I'm doing the ANOVA

 > aovRes = aov(dv~interval+Error(subject/interval), data=d)

 > summary(aovRes)

Error: subject
           Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  6 2.48531 0.41422

Error: subject:interval
           Df  Sum Sq Mean Sq F value    Pr(>F)
interval   5 13.8174  2.7635  8.7178 3.417e-05 ***
Residuals 30  9.5098  0.3170
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

but if I want to look at the contrasts, something has gone wrong:

summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs 
i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))

Error in 1:object$rank : NA/NaN argument

 > aovRes$contrasts
NULL

Can anybody help?
Thank you very much,  -Steffen



From Heather.Turner at warwick.ac.uk  Wed Jan 18 13:09:16 2006
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Wed, 18 Jan 2006 12:09:16 +0000
Subject: [R] Loading of namespace on load of .Rdata (was strange
	behaviour	of load)
Message-ID: <s3ce2ffa.090@liberator.csv.warwick.ac.uk>

Last week Giovanni Parrinello posted a message asking why various packages were loaded when he loaded an .Rdata file. Brian Ripley replied saying he thought it was because the saved workspace contained a reference to the namespace of ipred. (Correspondence copied below).

This begs the question: how did the reference to the namespace of ipred come to be in the .Rdata file? Brian did say it is likely to be because the workspace contained object(s) saved with environment the namespace of ipred - but how would this come about?

In this case I think is because the .Rdata file contained an object whose *parent* environment was the namespace of ipred. Take the following example from ?bagging (having loaded ipred):

> data(BreastCancer)
> 
> mod <- bagging(Class ~ Cl.thickness + Cell.size
+                 + Cell.shape + Marg.adhesion   
+                 + Epith.c.size + Bare.nuclei   
+                 + Bl.cromatin + Normal.nucleoli
+                 + Mitoses, data=BreastCancer, coob=TRUE)
>
> environment(mod$mtrees[[1]]$btree$terms)
<environment: 024E8138>
>
> parent.env(environment(mod$mtrees[[1]]$btree$terms))
<environment: namespace:ipred>

This occurs because the terms object is taken from the model frame which was evaluated within the environment of a function from the ipred package (here ipred:::irpart).

Therefore I think the behaviour observed by Giovanni will only occur in unusual circumstances: when the workspace contains a formula object, a terms object, a function, or some other object with a non-NULL environment, which has been created in the environment of a packaged function. In particular, this would not always occur with a packaged model fitting function, e.g. (from ?loglm in MASS)

> library(MASS)
> minn38a <- array(0, c(3,4,7,2), lapply(minn38[, -5], levels))
> minn38a[data.matrix(minn38[,-5])] <- minn38$f
> fm <- loglm(~1 + 2 + 3 + 4, minn38a)  
> environment(fm$terms)
<environment: R_GlobalEnv>

in this case because the terms component is obtained from the formula, whose environment is .GlobalEnv.

So, I have two points on this (more for R-devel than R-help now)

1. There is a more general situation where it would be useful to load the namespace of a package after loading a saved workspace: when the workspace contains objects of a class for which special methods are required. E.g. if 'fm' from the example above were saved in a workspace, the namespace of MASS would not be loaded when the workspace was loaded into R. Thus unless MASS was loaded by the user, default methods would be used by summary(), print() etc rather than the specialised methods for objects of class "loglm".

Of course the user should quickly realise this, but there may be cases where the default method gives a convincing but incorrect or unhelpful result. An alternative would be to add an attribute to objects of class "loglm" (say), e.g. attr(loglmObject, ".Environment") <- environment(MASS)
so that the namespace would automatically be loaded when it is required. [In fact alternatives such as environment(loglmObject) <- environment(MASS) or loglmObject$anyoldthing <- environment(MASS) would work just as well, but perhaps the first suggestion is neatest.].

What do others think of this idea? Should it (or an equivalent idea) be encouraged amongst package writers?

2. In the case highlighted by Giovanni, the namespace of ipred was loaded, but the package was not. This would be fine, except that the packages on which ipred depends *were* loaded. This seems inconsistent. I guess as long as there are packages without namespaces though, this is the only way to proceed. Perhaps in the meantime, package authors should be encouraged to use importFrom() rather than import()? Or perhaps where packages do have namespaces, only the namespace should be loaded in such a case.

Heather

> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Date: 12 January 2006 08:21:35 GMT
> To: giovanni parrinello <parrinel at med.unibs.it>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Strange behaviour of load
>
> On Wed, 11 Jan 2006, giovanni parrinello wrote:
>
>> Dear All,
>> simetimes when I load an Rdata I get this message
>>
>> #######
>> Code:
>>
>> load('bladder1.RData')
>> Carico il pacchetto richiesto: rpart ( Bad traslastion: Load required 
>> package-...)
>> Carico il pacchetto richiesto: MASS
>> Carico il pacchetto richiesto: mlbench
>> Carico il pacchetto richiesto: survival
>> Carico il pacchetto richiesto: splines
>>
>> Carico il pacchetto richiesto: 'survival'
>>
>>
>>        The following object(s) are masked from package:Hmisc :
>>
>>         untangle.specials
>>
>> Carico il pacchetto richiesto: class
>> Carico il pacchetto richiesto: nnet
>> #########
>>
>> So  I have many unrequired packages loaded.
>> Any idea?
>
> They are required!  My guess is that you have object(s) saved with
> environment the namespace of some package, and loading that namespace 
> is
> pulling these in.  The only CRAN package which requires mlbench 
> appears to
> be ipred, and that requires all of those except splines, required by
> survival.
>
> So I believe you have been using ipred and have saved a reference to 
> its
> namespace.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Dr H Turner
Research Assistant
Dept. of Statistics
The University of Warwick
Coventry
CV4 7AL

Tel: 024 76575870
Url: www.warwick.ac.uk/go/heatherturner



From perfumedlizard at yahoo.co.uk  Wed Jan 18 13:27:57 2006
From: perfumedlizard at yahoo.co.uk (Norman Goodacre)
Date: Wed, 18 Jan 2006 12:27:57 +0000 (GMT)
Subject: [R] Coercing a list to integer?
Message-ID: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/217b8ea2/attachment.pl

From manuel_gutierrez_lopez at yahoo.es  Wed Jan 18 13:50:07 2006
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Wed, 18 Jan 2006 13:50:07 +0100 (CET)
Subject: [R] se.fit in predict.nls
Message-ID: <20060118125012.5045.qmail@web26115.mail.ukl.yahoo.com>

The option se.fit in predict.nls is currently ignored.
Is there any other function available to calculate the
error in the predictions?
Thanks,
Manuel


		
______________________________________________ 
LLama Gratis a cualquier PC del Mundo. 
Llamadas a fijos y m??viles desde 1 c??ntimo por minuto.



From jholtman at gmail.com  Wed Jan 18 14:10:51 2006
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jan 2006 08:10:51 -0500
Subject: [R] Coercing a list to integer?
In-Reply-To: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
References: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
Message-ID: <644e1f320601180510s4647af72k8e4421895a14e8a1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/a6888ff4/attachment.pl

From jacques.veslot at cirad.fr  Wed Jan 18 14:24:52 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 18 Jan 2006 17:24:52 +0400
Subject: [R] Coercing a list to integer?
In-Reply-To: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
References: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
Message-ID: <43CE41A4.7010507@cirad.fr>

i am not sure i clearly understood...

do you want to coerce a vector into a list of its elements ?! like this:
split(1:5,1:5)



Norman Goodacre a ??crit :

> Dear group,
>  
>    I am nearly beside myself. After an entire night spent on a  niggling little detail, I am no closer to to the truth. I loaded an  Excel file in .csv form into R. It apparentely loads as a list, but not  the kind of list you can use. Oh no, it converts into a list that  cannot be converted into an integer, numeric, or vector, only a matrix,  whihc is useless without integers.
>  
>    How can I get a list of the form [1] 1,2,3,4,5 into the form [1]  1 [2] 2 [3] 3 [4] 4 [5] 5? Depending on hwo you define a list,  apparentely, it goes one way or the other.
>  
>   x <- list(1:5) means you have [1] 1,2,3,4,5
>  y <- list(1,2,3,4,5) means you have [1] 1 [2] 2 [3] 3 [4] 4 [5] 5
>  
>  Can anyone help?#
>  
>  I woudl greatly appreciate it.
>  
>  Sincerely,
>  Norman Goodacre
>  
>
>		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ggrothendieck at gmail.com  Wed Jan 18 14:33:27 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 Jan 2006 08:33:27 -0500
Subject: [R] Possible improvement in lm
In-Reply-To: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>
References: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>
Message-ID: <971536df0601180533v2f9a126dw7387d766d6d2fde8@mail.gmail.com>

1. Try using

   lm(...whatever..., na.action = na.exclude)

2. Be sure to read the note on Using Time Series in ?lm

3. The dyn package will accept ts, irts, its and zoo class time series
and output time series for the residuals.  Just preface lm with dyn$.
e.g.

library(dyn)

# test data
set.seed(1)
x <- ts(1:10, start = 2000, freq = 4)
x[5] <- NA
y <- x + rnorm(10)

# regress series y against series x
y.lm <- dyn$lm(y ~ x)
resid(y.lm)  # note that residuals are a time series


On 1/18/06, Vivek Satsangi <vivek.satsangi at gmail.com> wrote:
> Folks,
>
> I do a series of regressions (one for each quarter in the dataset) and
> then go and extract the residuals from each stored lm object that is
> returned as follows:
>
> vResiduals <- as.vector(unlist(resid(lQuarterlyRegressions[[i]])));
>
> Here lQuarterlyRegressions is a vector of objects returned by lm().
>
> Next, I may go find outliers using identify() on a plot or do some
> other analysis which tells me which row of the quarterly data I need
> to take a closer look at.
>
> However, if I try to match some point in one of the quarters that I
> have with its residual, then I have to drop the points from my
> "current Data" which have NA's for either the explanatory variables or
> the explained, so that the vector or residuals and the data have the
> same indexes.
>
> This lead to some serious confusion/bugs for me, and I am wondering if
> it might not be better for lm to put an NA into those rows where the
> point was dropped because of NA's in the explanatory or explained
> variables (currently it just returns nothing at that index). Ofcourse,
> there might be some arguments against this idea, and I would be
> interested to hear them.
>
> Thank you for your time and attention,
>
>
> -- Vivek Satsangi
> Student, Rochester, NY USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Jan 18 14:45:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jan 2006 13:45:26 +0000 (GMT)
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE2EBC.1020303@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
Message-ID: <Pine.LNX.4.61.0601181333060.17938@gannet.stats>

The problem is a well-known one in viewers looking at whole pages,
especially PS -> PDF converters.  R figures are particularly vulnerable as 
they have text running both horizontally and vertically (with normal 
axes).

Please do follow exactly the advice on the postscript help page.

      The postscript produced for a single R plot is EPS (_Encapsulated
      PostScript_) compatible, and can be included into other documents,
      e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
      in this way you will probably want to set 'horizontal = FALSE,
      onefile = FALSE, paper = "special"'.

If you have done that, suggest to your publisher that they turn 
autorotation off.


On Wed, 18 Jan 2006, Knut Krueger wrote:

>
> I am trying to send a manuscript to a journal.

> One of the figures build by R is in the right orientation and 4 are 
> rotated clockwise 90 deg in the preview.

> I used the right click save to PS option and I used the command line
>
> postscript("c:/temp/fig04.eps",bg="transparent",onefile = TRUE 
> ,pointsize=20,paper = "letter",height=8,width=8,horizontal=FALSE,family 
> = "Helvetica", font = "Helvetica")
>
> I treed Horizontal=TRUE Ghostsript show the rotated image but not the 
> preview from the journal. :-(
>
>
> Is there anything to change that the - unknown system - of the journal 
> will be forced to display the image in the right direction?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jan 18 14:47:32 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 18 Jan 2006 14:47:32 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE2EBC.1020303@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
Message-ID: <43CE46F4.7050407@statistik.uni-dortmund.de>

Knut Krueger wrote:

> I am trying to send a manuscript to a journal.
> One of the figures build by R is in the right orientation and 4 are rotated clockwise 90 deg in the preview.
> 
> I used the right click save to PS option and I used the command line
> 
> postscript("c:/temp/fig04.eps",bg="transparent",onefile = TRUE  ,pointsize=20,paper = "letter",height=8,width=8,horizontal=FALSE,family = "Helvetica", font = "Helvetica")
>

Please use
   paper="special"
for files to be included in documents.


> I treed Horizontal=TRUE Ghostsript show the rotated image but not the preview from the journal. :-(
> 
> 
> Is there anything to change that the - unknown system - of the journal will be forced to display the image in the right direction?

Always hard to tell what an "unknown" system is doing. We do not know of 
any rotation problems with R graphics, hence please ask the poeple 
running that "unknown system".

Uwe Ligges



> Regards Knut
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan 18 14:51:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jan 2006 13:51:30 +0000 (GMT)
Subject: [R] Possible improvement in lm
In-Reply-To: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>
References: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601181346070.17938@gannet.stats>

It seems you are looking for na.action = na.exclude.

This is described in all good books on S/R (e.g. MASS4, p. 141) and also 
in an answer on this list already this week.

It is even on the help pages for residuals.lm and fitted.

On Wed, 18 Jan 2006, Vivek Satsangi wrote:

> Folks,
>
> I do a series of regressions (one for each quarter in the dataset) and
> then go and extract the residuals from each stored lm object that is
> returned as follows:
>
> vResiduals <- as.vector(unlist(resid(lQuarterlyRegressions[[i]])));
>
> Here lQuarterlyRegressions is a vector of objects returned by lm().
>
> Next, I may go find outliers using identify() on a plot or do some
> other analysis which tells me which row of the quarterly data I need
> to take a closer look at.
>
> However, if I try to match some point in one of the quarters that I
> have with its residual, then I have to drop the points from my
> "current Data" which have NA's for either the explanatory variables or
> the explained, so that the vector or residuals and the data have the
> same indexes.
>
> This lead to some serious confusion/bugs for me, and I am wondering if
> it might not be better for lm to put an NA into those rows where the
> point was dropped because of NA's in the explanatory or explained
> variables (currently it just returns nothing at that index). Ofcourse,
> there might be some arguments against this idea, and I would be
> interested to hear them.
>
> Thank you for your time and attention,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ivowel at gmail.com  Wed Jan 18 14:56:05 2006
From: ivowel at gmail.com (ivo welch)
Date: Wed, 18 Jan 2006 08:56:05 -0500
Subject: [R] princomp() with missing values in panel data?
In-Reply-To: <Pine.LNX.4.61.0601170548040.17319@gannet.stats>
References: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>
	<Pine.LNX.4.61.0601170548040.17319@gannet.stats>
Message-ID: <50d1c22d0601180556j59748affw62fe1c5df9ccf201@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/aae66074/attachment.pl

From charles.edwin.white at us.army.mil  Wed Jan 18 14:50:43 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Wed, 18 Jan 2006 08:50:43 -0500
Subject: [R] Documentation in R 2.2.1 for Windows of: rcmd build --docs=
Message-ID: <8BAEC5E546879B4FAA536200A292C614EBF0A0@AMEDMLNARMC135.amed.ds.army.mil>

I recommend that the allowable values for docs be included in the short
help output generated by the command 'rcmd build --help'. The html help
file for build does not contain this information (which is fine since
the HTML help system supports multiple operating systems). I did find
the options I needed in the html help for install. Given the voluminous
(... and inherently somewhat disconnected...) documentation of R, I'm
sure someone can point me to other locations where these options are
documented. However, since the options seem to be OS specific and there
is already help output available from the OS specific command, I
recommend that the options be included in the short help listing
generated in association with the command. Thanks for all of your hard
work.

Chuck



From olsson1 at gmail.com  Wed Jan 18 14:58:26 2006
From: olsson1 at gmail.com (P. Olsson)
Date: Wed, 18 Jan 2006 14:58:26 +0100
Subject: [R] negative predicted values in poisson glm
Message-ID: <e984efc60601180558n5de15467x@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/304c94f7/attachment.pl

From MSchwartz at mn.rr.com  Wed Jan 18 15:06:25 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 18 Jan 2006 08:06:25 -0600
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE2EBC.1020303@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
Message-ID: <1137593185.3451.47.camel@localhost.localdomain>

On Wed, 2006-01-18 at 13:04 +0100, Knut Krueger wrote:
> I am trying to send a manuscript to a journal.
> One of the figures build by R is in the right orientation and 4 are
> rotated clockwise 90 deg in the preview.
> 
> I used the right click save to PS option and I used the command line
> 
> postscript("c:/temp/fig04.eps",bg="transparent",onefile =
> TRUE  ,pointsize=20,paper =
> "letter",height=8,width=8,horizontal=FALSE,family = "Helvetica", font
> = "Helvetica")
> 
> I treed Horizontal=TRUE Ghostsript show the rotated image but not the
> preview from the journal. :-(
> 
> 
> Is there anything to change that the - unknown system - of the journal
> will be forced to display the image in the right direction?
> 
> Regards Knut

One of the first things to do is to use 'onefile = FALSE', 'horizontal =
FALSE' and paper = "special"'.

This is in the Details section of ?postscript, which provides guidance
on the creation of EPS files for inclusion in publications.

I would try that to see if that provides a more consistent formatting of
the plots.

You don't indicate what you are using to create the manuscript itself
(ie. Word, LaTeX,  or ?) to help us in considering other possibilities
(such as autorotation).

If the above does not help, please provide a reproducible example of the
plot code and what you are using for the manuscript.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Jan 18 15:10:52 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jan 2006 14:10:52 +0000 (GMT)
Subject: [R] Data frame index?
In-Reply-To: <43CE307D.80706@stats.uwo.ca>
References: <20060118060249.39518.qmail@web31810.mail.mud.yahoo.com>
	<43CDEFC7.5000805@epm.net.co> <43CE307D.80706@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0601181402440.17938@gannet.stats>

It's worth noting that there are quite a few for loops inside the code 
used by matrix indexing of data frames.

I think a single for-loop over the columns is as good as any, something 
like

DF <- data.frame(x=1, y=rep("a", 4), z = 3)
ind <- c(1,3,3,1) # only numeric cols
for(i in unique(ind)) DF[ind==i, i] <- 0
DF
   x y z
1 0 a 3
2 1 a 0
3 1 a 0
4 0 a 3


On Wed, 18 Jan 2006, Duncan Murdoch wrote:

> On 1/18/2006 2:35 AM, Kenneth Cabrera wrote:
>> Hi, R users:
>>
>> I have a data.frame (not a matrix), I got a vector with the same length
>> as the
>> number of records (rows) of the data frame, and each element of
>> that vector is the column number (in a specific range of columns) of the
>> corresponding
>> record that I must set to zero.
>>
>> How can I  do this without a "for" loop?
>
> It sounds as though you've found that you can use two-column matrix
> indexing on a data frame for reading but not assigning.  You create a
> matrix where the first column is the row number, and the second column
> is the column number.  Then indexing by that selects those particular
> elements in order.
>
> For instance, if you have named your vector of columns "cols", you'd do
>
> my.data.frame[ cbind(1:rows, cols) ] <- 0
>
> Here's an example:
>
> > df
>    x y
> 1  1 a
> 2  1 a
> 3  1 a
> 4  1 a
> 5  1 a
> 6  1 a
> 7  1 a
> 8  1 a
> 9  1 a
> 10 1 a
> > df[cbind(1:4,c(1,2,1,2))]
> [1] "1" "a" "1" "a"
>
> But
>
> > df[cbind(1:4,c(1,2,1,2))] <- 0
> Error in "[<-.data.frame"(`*tmp*`, cbind(1:4, c(1, 2, 1, 2)), value = 0) :
>         only logical matrix subscripts are allowed in replacement
>
> To get around this, construct the logical matrix using this method, then
>  use it as an index:
>
> > mat <- matrix(FALSE, 10, 2)
> > mat[cbind(1:4,c(1,2,1,2))] <- TRUE
> > df[mat] <- 0
> Warning message:
> invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, thisvar,
> value = 0)
> > df
>    x    y
> 1  0    a
> 2  1 <NA>
> 3  0    a
> 4  1 <NA>
> 5  1    a
> 6  1    a
> 7  1    a
> 8  1    a
> 9  1    a
> 10 1    a
>
> If your columns are all numeric, you won't get the warning I got.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at mn.rr.com  Wed Jan 18 15:18:32 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 18 Jan 2006 08:18:32 -0600
Subject: [R] Coercing a list to integer?
In-Reply-To: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
References: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
Message-ID: <1137593913.3451.56.camel@localhost.localdomain>

On Wed, 2006-01-18 at 12:27 +0000, Norman Goodacre wrote:
>  Dear group,
>   
>     I am nearly beside myself. After an entire night spent on a
> niggling little detail, I am no closer to to the truth. I loaded an
> Excel file in .csv form into R. It apparentely loads as a list, but
> not  the kind of list you can use. Oh no, it converts into a list that
> cannot be converted into an integer, numeric, or vector, only a
> matrix,  whihc is useless without integers.
>   
>     How can I get a list of the form [1] 1,2,3,4,5 into the form [1]
> 1 [2] 2 [3] 3 [4] 4 [5] 5? Depending on hwo you define a list,
> apparentely, it goes one way or the other.
>   
>    x <- list(1:5) means you have [1] 1,2,3,4,5
>   y <- list(1,2,3,4,5) means you have [1] 1 [2] 2 [3] 3 [4] 4 [5] 5
>   
>   Can anyone help?#
>   
>   I woudl greatly appreciate it.
>   
>   Sincerely,
>   Norman Goodacre
>   

Presuming that you used read.csv() or similar, the imported CSV object
should be a simple data frame.

It is not truly clear here what your problem is relative to how you want
to use the imported data.

The difference between:

> list(1:5)
[[1]]
[1] 1 2 3 4 5

and

> list(1,2,3,4,5)
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

[[4]]
[1] 4

[[5]]
[1] 5

is that in the first case, you have a list with one element, which is a
vector containing 5 elements:

> str(list(1:5))
List of 1
 $ : int [1:5] 1 2 3 4 5

whereas in the second case, you have a list with five elements, each of
which is a vector with one element:

> str(list(1,2,3,4,5))
List of 5
 $ : num 1
 $ : num 2
 $ : num 3
 $ : num 4
 $ : num 5


Note also the not so subtle difference where in the first case, the
result of the sequence 1:5 yields integers and in the second case, the
elements are doubles (numeric), which is the default data type in R.

Please provide additional details on what it is you are trying to do
here and we can attempt to offer more specific guidance.

HTH,

Marc Schwartz



From jacques.veslot at cirad.fr  Wed Jan 18 15:43:54 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 18 Jan 2006 18:43:54 +0400
Subject: [R] Coercing a list to integer?
In-Reply-To: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
References: <20060118122757.24613.qmail@web27312.mail.ukl.yahoo.com>
Message-ID: <43CE542A.5000207@cirad.fr>

of course unlist()/as.list() do that !!! (sorry, i am tired)

Norman Goodacre a ??crit :

> Dear group,
>  
>    I am nearly beside myself. After an entire night spent on a  niggling little detail, I am no closer to to the truth. I loaded an  Excel file in .csv form into R. It apparentely loads as a list, but not  the kind of list you can use. Oh no, it converts into a list that  cannot be converted into an integer, numeric, or vector, only a matrix,  whihc is useless without integers.
>  
>    How can I get a list of the form [1] 1,2,3,4,5 into the form [1]  1 [2] 2 [3] 3 [4] 4 [5] 5? Depending on hwo you define a list,  apparentely, it goes one way or the other.
>  
>   x <- list(1:5) means you have [1] 1,2,3,4,5
>  y <- list(1,2,3,4,5) means you have [1] 1 [2] 2 [3] 3 [4] 4 [5] 5
>  
>  Can anyone help?#
>  
>  I woudl greatly appreciate it.
>  
>  Sincerely,
>  Norman Goodacre
>  
>
>		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From j.logsdon at quantex-research.com  Wed Jan 18 15:51:16 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Wed, 18 Jan 2006 14:51:16 +0000 (GMT)
Subject: [R] Windows package upates
Message-ID: <Pine.LNX.4.10.10601181446280.27402-100000@quantex-research.co.uk>

Dear list

Having just started to use the Windows version, I am very impressed with
it's package handling as well as the gui.

So I tried to see what was due for update and packages such as Hmisc,
Matrix and others came up.

But when I had updated them - which took a few goes as something hung
between here and Bristol - I noticed that the default packages such as
nmle, MASS had disappeared.  I re-installed them but is this a glitch or a
feature?

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com



From admin at biostatistic.de  Wed Jan 18 16:15:11 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 18 Jan 2006 16:15:11 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE46F4.7050407@statistik.uni-dortmund.de>
References: <43CE2EBC.1020303@biostatistic.de>
	<43CE46F4.7050407@statistik.uni-dortmund.de>
Message-ID: <43CE5B7F.7040303@biostatistic.de>



Uwe Ligges schrieb:

>
> Always hard to tell what an "unknown" system is doing. We do not know 
> of any rotation problems with R graphics, hence please ask the poeple 
> running that "unknown system".

I tried to ask, they converted the files to rotated tiff (as a free 
service) but used old files from the first submission, and they did not 
give me any answer why one file is not  rotated the other 3 are rotated ...

Sorry that I could not give you any more hints, but I will ask them to 
switch off auto rotation.

Regards Knut



From tlumley at u.washington.edu  Wed Jan 18 16:17:41 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 Jan 2006 07:17:41 -0800 (PST)
Subject: [R] Own Color Palette
In-Reply-To: <330527823@web.de>
References: <330527823@web.de>
Message-ID: <Pine.LNX.4.64.0601180716200.26064@homer21.u.washington.edu>

On Wed, 18 Jan 2006, Robert Michael Rausch wrote:
> I would like to generate a contour-plot according to a master plot. The 
> problem is that the rainbow-palette included in R does not answer this 
> purpose. I need a darker blue, no turquoise, relatively less green, more 
> yellow and more red. Haw can I adjust the rainbow? Alternatively: How 
> can I generate my own palette with at least 100 colors with smooth 
> transitions?<o:p></o:p>

colorRampPalette  will interpolate within a list of colors.  You might 
look at the RColorBrewer package to find a list of colors to interpolate 
within.

 	-thomas



From tlumley at u.washington.edu  Wed Jan 18 16:28:48 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 Jan 2006 07:28:48 -0800 (PST)
Subject: [R] negative predicted values in poisson glm
In-Reply-To: <e984efc60601180558n5de15467x@mail.gmail.com>
References: <e984efc60601180558n5de15467x@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0601180728070.26064@homer21.u.washington.edu>

On Wed, 18 Jan 2006, P. Olsson wrote:

> Dear  R  helpers,
> running the following code of a glm model of the family poisson, gives
> predicted values < 0. Why?

Look at the help page for predict.glm, particularly the "type" argument. 
Then exponentiate the results.

 	-thomas


>
> library(MASS)
> library(stats)
> library(mvtnorm)
> library(pscl)
> data(bioChemists)
> poisson_glm <-  glm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists,
> family = poisson)
> predicted.values = predict(poisson_glm)
> range(predicted.values)
>
>
> Thank you in advance for any hints.
> Best regards,
> P. Olsson
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From admin at biostatistic.de  Wed Jan 18 16:38:26 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 18 Jan 2006 16:38:26 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <Pine.LNX.4.61.0601181333060.17938@gannet.stats>
References: <43CE2EBC.1020303@biostatistic.de>
	<Pine.LNX.4.61.0601181333060.17938@gannet.stats>
Message-ID: <43CE60F2.4070504@biostatistic.de>



Prof Brian Ripley schrieb:

> The problem is a well-known one in viewers looking at whole pages,
> especially PS -> PDF converters.  R figures are particularly 
> vulnerable as they have text running both horizontally and vertically 
> (with normal axes).
>
> Please do follow exactly the advice on the postscript help page.
>
>      The postscript produced for a single R plot is EPS (_Encapsulated
>      PostScript_) compatible, and can be included into other documents,
>      e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
>      in this way you will probably want to set 'horizontal = FALSE,
>      onefile = FALSE, paper = "special"'.
>
>>
>> postscript("c:/temp/fig04.eps",bg="transparent",onefile = TRUE 
>> ,pointsize=20,paper = 
>> "letter",height=8,width=8,horizontal=FALSE,family = "Helvetica", font 
>> = "Helvetica")
>
postscript("c:/temp/fig04.eps",bg="transparent",height=8,width=8,bg="transparent",pointsize=20,horizontal 
= FALSE,onefile = FALSE, paper = "special",family = "Helvetica", font = 
"Helvetica")

There is a error from  boxplot without ,height=8,width=8
error in plot.new() : Grafikr??nder zu gro?? ( margins to big)

but I am afraid that they have autorotation on. I will aks the journal 
to switch it off.

Regards Knut



From p.dalgaard at biostat.ku.dk  Wed Jan 18 16:52:33 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jan 2006 16:52:33 +0100
Subject: [R] negative predicted values in poisson glm
In-Reply-To: <e984efc60601180558n5de15467x@mail.gmail.com>
References: <e984efc60601180558n5de15467x@mail.gmail.com>
Message-ID: <x28xtdzg72.fsf@viggo.kubism.ku.dk>

"P. Olsson" <olsson1 at gmail.com> writes:

> Dear  R  helpers,
> running the following code of a glm model of the family poisson, gives
> predicted values < 0. Why?
> 
> library(MASS)
> library(stats)
> library(mvtnorm)
> library(pscl)
> data(bioChemists)
> poisson_glm <-  glm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists,
> family = poisson)
> predicted.values = predict(poisson_glm)
> range(predicted.values)
> 
> 
> Thank you in advance for any hints.


The prediction is on the link scale.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From admin at biostatistic.de  Wed Jan 18 17:09:17 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 18 Jan 2006 17:09:17 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <1137593185.3451.47.camel@localhost.localdomain>
References: <43CE2EBC.1020303@biostatistic.de>
	<1137593185.3451.47.camel@localhost.localdomain>
Message-ID: <43CE682D.6080209@biostatistic.de>



Marc Schwartz schrieb:

>On Wed, 2006-01-18 at 13:04 +0100, Knut Krueger wrote:
>  
>
>One of the first things to do is to use 'onefile = FALSE', 'horizontal =
>FALSE' and paper = "special"'.
>  
>
I am afraid the problem is on the journals side, because the "wrong" 
postscript line (with "letter" )is working
and I changed the postscript options between both examples no change in 
the behaviour of the pdf creator of the journal.

>This is in the Details section of ?postscript, which provides guidance
>on the creation of EPS files for inclusion in publications.
>
>I would try that to see if that provides a more consistent formatting of
>the plots.
>  
>

>You don't indicate what you are using to create the manuscript itself
>(ie. Word, LaTeX,  or ?) to help us in considering other possibilities
>(such as auto rotation).
>  
>
The journal collects the figures once by once after the manuscript in 
the original file format.
to solve the problem I could change it to tiff and submit it, but the 
tiff files are look ing not as good as eps files.

>If the above does not help, please provide a reproducible example of the
>plot code and what you are using for the manuscript.
>
for your information:
the same problem occurs if I plot the data to the graphic device and use 
the right button -> create postscript file
One is not rotated the others are rotated.

Hope the code helps without data:

colored.barlabels  
<-function(x.barplot,x.barcolors,x.xrowlist,cexaxis=0.7,x.thick=F)
{
    countmax<-length(x.xrowlist)
    if (countmax > 0)
    {
    x.labels<-c(1:countmax)
    for (count in 1:countmax)
    {     for(count2 in 1:countmax)x.labels[count2]<-""
        x.labels[count] <- x.xrowlist[count]
        axis(1, at=x.barplot, tick=x.thick, labels=x.labels, 
col.axis=x.barcolors[count], cex.axis=cexaxis)
   
    }
    }#end if  (countmax > 0)
    else "Message from colored.barlabel: Nothing to do"

}

postscript("c:/temp//fig04.ps",bg="transparent",onefile = TRUE 
,pointsize=20,paper = "letter",height=8,width=8,horizontal=FALSE,family 
= "Helvetica", font = "Helvetica")
barcolors <-  
c(boxcolor1_3,boxcolor1_3,boxcolor1_3,boxcolor4,boxcolor5,boxcolor6,boxcolor7,boxcolor8,boxcolor9,boxcolor10)
xrow <-c(mean1,mean2,mean8,mean71,mean72,mean78,mean79,mean85,mean86,mean92)

sdrow<-c(sd1,sd2,sd8,sd71,sd72,sd78,sd79,sd85,sd86,sd92)
ci.l<-xrow-sdrow/2
ci.h<-xrow+sdrow/2

xrowlist <- c("Day 1","Day 2","Day 3","Day 4","Day 5","Day 6","Day 
7","Day 8","Day 9","Day 10")
t.barplot<-barplot2(xrow,col = 
barcolors,cex.lab=1.2,plot.ci=TRUE,ci.l=ci.l,ci.u=ci.h,ylab="mean time 
till following in sec.")
barcolors <-  
c("black","black","black","black","black","black","black","black","black","black",)
colored.barlabels(t.barplot,barcolors,xrowlist,0.7,F)
dev.off()

this is the not rotated figure
ant the follwoing is rotated
outl.text <-function(boxdata,input = FALSE)
{
    countmax<-length(boxdata$out)
    if (countmax > 0)
    {
    for (count in 1:countmax)
    {     outl.name <-boxdata$out[count]
        if (input==TRUE) outl.name <- readline(paste("text for value 
boxplot Nr:",toString(boxdata$group[count])," - value: 
",toString(boxdata$out[count]),": "))
        text(boxdata$group[count], 
boxdata$out[count],pos=4,cex=0.8,outl.name)
    }
    }#end if  (countmax > 0)
    else "Message from outl.text: no outlier"

}

postscript("c:/r/anschluss/plots/fig02.ps",height=8,width=8,bg="transparent",pointsize=20,horizontal 
= FALSE,onefile = FALSE, paper = "special",family = "Helvetica", font = 
"Helvetica")

day1<-Day_1
day2<-Day_2
day3<-Day_8
boxplot.names <-c("day 1","day 2","day 3") # Vektor f??r namen erzeugen
 
boxdata <- boxplot(day1,day2,day3,
col = boxcolor,names=boxplot.names,ylab="time till following in 
sec.",cex.lab=size.x.lab,xlab="",cex.lab=size.y.lab
)
outl.text(boxdata,input=FALSE)

dev.off()





Regards Knut



From murdoch at stats.uwo.ca  Wed Jan 18 17:10:56 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Jan 2006 11:10:56 -0500
Subject: [R] Windows package upates
In-Reply-To: <Pine.LNX.4.10.10601181446280.27402-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10601181446280.27402-100000@quantex-research.co.uk>
Message-ID: <43CE6890.8030107@stats.uwo.ca>

On 1/18/2006 9:51 AM, John Logsdon wrote:
> Dear list
> 
> Having just started to use the Windows version, I am very impressed with
> it's package handling as well as the gui.
> 
> So I tried to see what was due for update and packages such as Hmisc,
> Matrix and others came up.
> 
> But when I had updated them - which took a few goes as something hung
> between here and Bristol - I noticed that the default packages such as
> nmle, MASS had disappeared.  I re-installed them but is this a glitch or a
> feature?

That's not supposed to happen; my guess is that it's related to those 
hangs you mention.  Do you think it tried to update those, but failed 
because of problems with the mirror?

Duncan Murdoch



From gunter.berton at gene.com  Wed Jan 18 17:13:27 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 18 Jan 2006 08:13:27 -0800
Subject: [R] Possible improvement in lm
In-Reply-To: <bcb171920601180408x2a0de3dse03f4036150df011@mail.gmail.com>
Message-ID: <200601181613.k0IGDRor025640@volta.gene.com>

I'm afraid you're a day late and a dollar short: see ?na.exclude.

lm() has been around longer than you have, maybe, and is thus pretty well
optimized. Not perfect, mind you, but I think it unlikely that "casual"
suggestions haven't already been considered.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivek Satsangi
> Sent: Wednesday, January 18, 2006 4:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Possible improvement in lm
> 
> Folks,
> 
> I do a series of regressions (one for each quarter in the dataset) and
> then go and extract the residuals from each stored lm object that is
> returned as follows:
> 
> vResiduals <- as.vector(unlist(resid(lQuarterlyRegressions[[i]])));
> 
> Here lQuarterlyRegressions is a vector of objects returned by lm().
> 
> Next, I may go find outliers using identify() on a plot or do some
> other analysis which tells me which row of the quarterly data I need
> to take a closer look at.
> 
> However, if I try to match some point in one of the quarters that I
> have with its residual, then I have to drop the points from my
> "current Data" which have NA's for either the explanatory variables or
> the explained, so that the vector or residuals and the data have the
> same indexes.
> 
> This lead to some serious confusion/bugs for me, and I am wondering if
> it might not be better for lm to put an NA into those rows where the
> point was dropped because of NA's in the explanatory or explained
> variables (currently it just returns nothing at that index). Ofcourse,
> there might be some arguments against this idea, and I would be
> interested to hear them.
> 
> Thank you for your time and attention,
> 
> 
> -- Vivek Satsangi
> Student, Rochester, NY USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From azzalini at stat.unipd.it  Wed Jan 18 17:23:32 2006
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Wed, 18 Jan 2006 17:23:32 +0100
Subject: [R] negative predicted values in poisson glm
In-Reply-To: <e984efc60601180558n5de15467x@mail.gmail.com>
References: <e984efc60601180558n5de15467x@mail.gmail.com>
Message-ID: <20060118172332.31ed85f5.azzalini@stat.unipd.it>

On Wed, 18 Jan 2006 14:58:26 +0100, P. Olsson wrote:

PO> Dear  R  helpers,
PO> running the following code of a glm model of the family poisson,
PO> gives predicted values < 0. Why?
PO> 
PO> library(MASS)
PO> library(stats)
PO> library(mvtnorm)
PO> library(pscl)
PO> data(bioChemists)
PO> poisson_glm <-  glm(art ~ fem + mar + kid5 + phd + ment, data =
PO> bioChemists, family = poisson)
PO> predicted.values = predict(poisson_glm)
PO> range(predicted.values)
PO> 


use
 predicted.values = predict(poisson_glm, type="response")

best wishes,

Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From bolker at ufl.edu  Wed Jan 18 17:21:29 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 18 Jan 2006 16:21:29 +0000 (UTC)
Subject: [R] negative predicted values in poisson glm
References: <e984efc60601180558n5de15467x@mail.gmail.com>
Message-ID: <loom.20060118T172001-809@post.gmane.org>

P. Olsson <olsson1 <at> gmail.com> writes:

> 
> Dear  R  helpers,
> running the following code of a glm model of the family poisson, gives
> predicted values < 0. Why?
> 

  Because by default predict.glm() gives answers on the 
link (log) scale.  predict(poisson_glm,type="response")
is probably what you're looking for.  See ?predict.glm.

  Ben Bolker



From mtommasi at unica.it  Wed Jan 18 17:32:44 2006
From: mtommasi at unica.it (Posta Univ. Cagliari)
Date: Wed, 18 Jan 2006 17:32:44 +0100
Subject: [R] linear contrasts with anova
Message-ID: <002b01c61c4c$cfb49b50$6402a8c0@tommasi9270>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/029d728d/attachment.pl

From yen.lin.chia at intel.com  Wed Jan 18 17:52:23 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Wed, 18 Jan 2006 08:52:23 -0800
Subject: [R] Influence measure + lme ?
Message-ID: <E305A4AFB7947540BC487567B5449BA809292FED@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/9b68a27e/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jan 18 17:56:53 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jan 2006 16:56:53 +0000 (GMT)
Subject: [R] Windows package upates
In-Reply-To: <Pine.LNX.4.10.10601181446280.27402-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10601181446280.27402-100000@quantex-research.co.uk>
Message-ID: <Pine.LNX.4.61.0601181654020.20151@gannet.stats>

On Wed, 18 Jan 2006, John Logsdon wrote:

> Dear list
>
> Having just started to use the Windows version, I am very impressed with
> it's package handling as well as the gui.
>
> So I tried to see what was due for update and packages such as Hmisc,
> Matrix and others came up.
>
> But when I had updated them - which took a few goes as something hung
> between here and Bristol - I noticed that the default packages such as
> nmle, MASS had disappeared.  I re-installed them but is this a glitch or a
> feature?

Were they amongst the updates (I suspect so)?  R tries hard to recover 
from failures to download, but it cannot do so if you kill it at a crucial 
time.  (For source-code installs, 2.3.0 will be even better at recovering 
as it will catch attempts to kill and clean up.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dong_lsh at hotmail.com  Wed Jan 18 18:14:18 2006
From: dong_lsh at hotmail.com (Lingsheng Dong)
Date: Wed, 18 Jan 2006 12:14:18 -0500
Subject: [R] Logistftest  to select diagnostic genes
Message-ID: <BAY103-F838E9431742656592CDAFE51D0@phx.gbl>

Hi, all,

Anyone has experience on Logistf package? I am using logistftest in Logistf 
package to selelct diagnosis genes. The result seems not the same as I 
expected.

I have 10 gene expression data for 27 tumor 1 and 11 tumor 0. I want to 
select the best one using Maximum likelihood ratio test in logistic 
regression model. This is the way my code works:
1.  Read in 10 genes as independent variables and tumor type (1 or 0) as 
dependent variable.
2   Fit in a 10 variable logistic model and calculate it's likelihood.
3   Loop through the 10 genes, each time take one gene out of the model and 
calculate the likelihood without the gene. And compare each new likelihood 
with the likelihood of  the  original original 10 variable model to get 
likelihood ratio test for each gene.

I guess gene 8 should have best discrimination power because it can totally 
seperate the two group tumors. But the test result shows the likelihood 
ratio for Ratio 8 is not the biggest.

Where am I wrong? The package is not good for this problem? But the package 
document says this package takes care of separation and small sample size by 
finite parameter estimates and profile penalized log likelihood. Or my 
assumption is wrong? I am totally lost.

Thank you very much for reading this email! Your suggestion or comments will 
be appreciated.



Lingsheng






The fear of the LORD is the beginning of wisdom, and knowledge of the Holy 
One is understanding.
--Proverbs 10:10



From steffen.katzner at mail.gwdg.de  Wed Jan 18 19:14:06 2006
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Wed, 18 Jan 2006 19:14:06 +0100
Subject: [R] linear contrasts with anova
Message-ID: <43CE856E.3040300@mail.gwdg.de>

group=factor(rep(c(0:2), each = 8))
ar = data.frame(group, dip)

con = matrix(c(1, -1, 0, 1, 0, -1), nrow=3, ncol=2, byrow=F)
contrasts(ar$group)=con

aovRes = aov(dip~group, ar)

 > summary.aov(aovRes, split=list(group = list("0 vs 1" = 1, "0 vs 3" = 2)))

                 Df Sum Sq Mean Sq  F value    Pr(>F)
group            2 919.10  459.55  57.3041 3.121e-09 ***
   group: 0 vs 1  1   2.10    2.10   0.2622     0.614
   group: 0 vs 3  1 917.00  917.00 114.3460 5.915e-10 ***
Residuals       21 168.41    8.02
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


I only don't know why it does not work with within-subject designs.
I have posted this question before, does anybody know?

-steffen






> I have some doubts about the validity of my procedure to estimeate linear contrasts ina a factorial design.
> For sake of semplicity, let's imagine a one way ANOVA with three levels. I am interested to test the significance of the difference between the first and third level (called here contrast C1) and between the first and the seconda level (called here contrast C2). I used the following procedure:
> 
> 
> ------------------- reading data from a text file -----------------------
> 
>> ar <-read.table("C:/Programmi/R/myworks/contrasti/cont1.txt",header=TRUE)
> 
>> ar
> 
>      CC GROUP
> 
> 1   3.0     0
> 
> 2   3.0     0
> 
> 3   4.0     0
> 
> 4   5.0     0
> 
> 5   6.0     0
> 
> 6   7.0     0
> 
> 7   3.0     0
> 
> 8   2.0     0
> 
> 9   1.0     1
> 
> 10  6.0     1
> 
> 11  5.0     1
> 
> 12  7.0     1
> 
> 13  2.0     1
> 
> 14  3.0     1
> 
> 15  1.5     1
> 
> 16  1.7     1
> 
> 17 17.0     2
> 
> 18 12.0     2
> 
> 19 15.0     2
> 
> 20 16.0     2
> 
> 21 12.0     2
> 
> 22 23.0     2
> 
> 23 19.0     2
> 
> 24 21.0     2
> 
>  
> 
> ------------------- creating a new array of data-----------------------
> 
>> ar<-data.frame(GROUP=factor(ar$GROUP),DIP=ar$CC)
> 
>> ar
> 
>    GROUP  DIP
> 
> 1      0  3.0
> 
> 2      0  3.0
> 
> 3      0  4.0
> 
> 4      0  5.0
> 
> 5      0  6.0
> 
> 6      0  7.0
> 
> 7      0  3.0
> 
> 8      0  2.0
> 
> 9      1  1.0
> 
> 10     1  6.0
> 
> 11     1  5.0
> 
> 12     1  7.0
> 
> 13     1  2.0
> 
> 14     1  3.0
> 
> 15     1  1.5
> 
> 16     1  1.7
> 
> 17     2 17.0
> 
> 18     2 12.0
> 
> 19     2 15.0
> 
> 20     2 16.0
> 
> 21     2 12.0
> 
> 22     2 23.0
> 
> 23     2 19.0
> 
> 24     2 21.0
> 
>  
> 
> ------------------- creating two dummy variables (C1 and C2) for linear contrasts-----------------------
> 
>> ar<-data.frame(GROUP=factor(ar$GROUP),C1=factor(ar$GROUP),C2=factor(ar$GROUP),DIP=ar$DIP)
> 
>> ar
> 
>    GROUP C1 C2  DIP
> 
> 1      0  0  0  3.0
> 
> 2      0  0  0  3.0
> 
> 3      0  0  0  4.0
> 
> 4      0  0  0  5.0
> 
> 5      0  0  0  6.0
> 
> 6      0  0  0  7.0
> 
> 7      0  0  0  3.0
> 
> 8      0  0  0  2.0
> 
> 9      1  1  1  1.0
> 
> 10     1  1  1  6.0
> 
> 11     1  1  1  5.0
> 
> 12     1  1  1  7.0
> 
> 13     1  1  1  2.0
> 
> 14     1  1  1  3.0
> 
> 15     1  1  1  1.5
> 
> 16     1  1  1  1.7
> 
> 17     2  2  2 17.0
> 
> 18     2  2  2 12.0
> 
> 19     2  2  2 15.0
> 
> 20     2  2  2 16.0
> 
> 21     2  2  2 12.0
> 
> 22     2  2  2 23.0
> 
> 23     2  2  2 19.0
> 
> 24     2  2  2 21.0
> 
>  
> 
> ------------------- selecting the contrast levels-----------------------
> 
>> ar$C1 <- C(ar$C1, c(1,0,-1), how.many = 1)
> 
>> ar$C2 <- C(ar$C2, c(1,-1,0), how.many = 1)
> 
>  
> 
>  
> 
> ------------------- contrast analysis of C2 -----------------------
> 
>> r.aov8 <-aov(DIP ~  C2 + GROUP , data = ar)
> 
>> anova(r.aov8)
> 
> Analysis of Variance Table
> 
>  
> 
> Response: DIP
> 
>           Df Sum Sq Mean Sq  F value    Pr(>F)    
> 
> C2         1   2.10    2.10   0.2622     0.614    
> 
> GROUP      1 917.00  917.00 114.3460 5.915e-10 ***
> 
> Residuals 21 168.41    8.02                       
> 
> ---
> 
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
>  
> 
> ------------------- contrast analysis of C1 -----------------------
> 
>> r.aov9 <-aov(DIP ~  C1 + GROUP , data = ar)
> 
>> anova(r.aov9)
> 
> Analysis of Variance Table
> 
>  
> 
> Response: DIP
> 
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> 
> C1         1 650.25  650.25  81.083 1.175e-08 ***
> 
> GROUP      1 268.85  268.85  33.525 9.532e-06 ***
> 
> Residuals 21 168.41    8.02                      
> 
> ---
> 
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
>  
> 
> ------------------- anova of the global design -----------------------
> 
>> r.aov10 <-aov(DIP ~  GROUP , data = ar)
> 
>> anova(r.aov10)
> 
> Analysis of Variance Table
> 
>  
> 
> Response: DIP
> 
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> 
> GROUP      2 919.10  459.55  57.304 3.121e-09 ***
> 
> Residuals 21 168.41    8.02                      
> 
> ---
> 
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> 
> 
> 
> 
> 
> 
> 
> 
> I would like to know if there is a more economic procedure with R to do linear contrasts.
> 
> Every comments will be well accepted.
> 
> 
> 
> Thank you very much and best regards
> 
> 
> 
> Marco Tommasi
> 
> 	[[alternative HTML version deleted]]
>



From joshuacgilbert at gmail.com  Wed Jan 18 19:45:07 2006
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Wed, 18 Jan 2006 13:45:07 -0500
Subject: [R] Help with plot.svm from e1071
Message-ID: <ef96daa30601181045g5b2973c7o7749d483671396d4@mail.gmail.com>

Hi.

I'm trying to plot a pair of intertwined spirals and an svm that
separates them. I'm having some trouble. Here's what I tried.

> library(mlbench)
> library(e1071)
Loading required package: class
> raw <- mlbench.spirals(200,2)
> spiral <- data.frame(class=as.factor(raw$classes), x=raw$x[,1], y=raw$x[,2])
> m <- svm(class~., data=spiral)
> plot(m, spiral)
Error in -x$index : invalid argument to unary operator

So we delve into e1071:::plot.svm. When I run the code in plot.svm
everything is fine up until
 points(formula, data = data[-x$index, ], pch = dataSymbol,
                  col = symbolPalette[colind[-x$index]])
That gives me the same error message, "Error in -x$index : invalid
argument to unary operator". The weird thing is that I can run either
of the those statements in isolation
data[-x$index, ]
symbolPalette[colind[-x$index]]
and neither gives me an error. I looked in the two points functions I
can see (points.default and points.formula) but neither calls x$index.

I was following along the documentation for plot.svm, which has a
simple example (that works)
     ## a simple example
     library(MASS)
     data(cats)
     m <- svm(Sex~., data = cats)
     plot(m, cats)

I don't see what the difference between their example and mine.

Can anyone help me?


Thank you,
Josh.



From beperron at wustl.edu  Wed Jan 18 19:46:56 2006
From: beperron at wustl.edu (Brian Perron)
Date: Wed, 18 Jan 2006 12:46:56 -0600
Subject: [R] ICC for Binary data
Message-ID: <84C59624B1B0204BBCB3B7DDF981AE63010AD099@GWB-PO.gwb.wustl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/c7ac1970/attachment.pl

From mschwartz at mn.rr.com  Wed Jan 18 20:16:42 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 18 Jan 2006 13:16:42 -0600
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE682D.6080209@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
	<1137593185.3451.47.camel@localhost.localdomain>
	<43CE682D.6080209@biostatistic.de>
Message-ID: <1137611803.4333.42.camel@localhost.localdomain>

On Wed, 2006-01-18 at 17:09 +0100, Knut Krueger wrote:
> 
> Marc Schwartz schrieb:
> 
> >On Wed, 2006-01-18 at 13:04 +0100, Knut Krueger wrote:
> >  
> >
> >One of the first things to do is to use 'onefile = FALSE', 'horizontal =
> >FALSE' and paper = "special"'.
> >  
> >
> I am afraid the problem is on the journals side, because the "wrong" 
> postscript line (with "letter" )is working
> and I changed the postscript options between both examples no change in 
> the behaviour of the pdf creator of the journal.
> 
> >This is in the Details section of ?postscript, which provides guidance
> >on the creation of EPS files for inclusion in publications.
> >
> >I would try that to see if that provides a more consistent formatting of
> >the plots.
> >  
> >
> 
> >You don't indicate what you are using to create the manuscript itself
> >(ie. Word, LaTeX,  or ?) to help us in considering other possibilities
> >(such as auto rotation).
> >  
> >
> The journal collects the figures once by once after the manuscript in 
> the original file format.
> to solve the problem I could change it to tiff and submit it, but the 
> tiff files are look ing not as good as eps files.
> 
> >If the above does not help, please provide a reproducible example of the
> >plot code and what you are using for the manuscript.
> >
> for your information:
> the same problem occurs if I plot the data to the graphic device and use 
> the right button -> create postscript file
> One is not rotated the others are rotated.
> 
> Hope the code helps without data:

<SNIP>

Unfortunately, it may have precluded my being able to replicate exactly
what you are seeing, despite being intimately familiar with one of the
functions (barplot2  ;-) that you are using.

I did create some data that would generally fit what you are doing in
both cases, however, probably because I am on Linux and you are on
Windows, where both device and GS differences may be problematic, I
could not get the second to be rotated relative to the page.

If you want, e-mail me (offlist) the rotated EPS file that is created in
the second case and I can use that to try to replicate the problem as
well as review the EPS code to see if something sticks out.

Under Linuxen, one can set:

  GS_OPTIONS="-dAutoRotatePages=/None"
  export GS_OPTIONS

which disables autorotation on the system processing the EPS graphics. I
would envision that there is a similar situation on Windows, using
either an environment variable or via the command line, depending upon
what your publisher is using for tools. A review of the Windows
documentation for GS would be helpful here.

Another possibility is that the rotation is being caused by some
confounding in the text in your second plot. GS will base the rotation
properties on the dominant text in the file. Given your plots, it may be
possible that some of the text label components in the second plot are
causing GS to perform the rotation. If this is the case, disabling the
autorotation as above should help.

The use of 'paper = "special"' is very helpful in general, as it enables
tight bounding boxes around the EPS graphic for inclusion in other
documents. I would highly recommend using that as your default.

One other quick comment, which is relative to the readability of your
code.  Strategically placed spaces (" ") and line breaks would help
tremendously. Especially since most e-mail clients will create line
breaks at 72 chars (or similar), which can make the flow of the code
difficult to review.

HTH,

Marc Schwartz



From adayf96 at ucla.edu  Wed Jan 18 20:18:07 2006
From: adayf96 at ucla.edu (Aaron Day)
Date: Wed, 18 Jan 2006 11:18:07 -0800
Subject: [R] Converting a Perl Array of Arrayrefs to an R array or matrix
	using	RS Perl
Message-ID: <20060118111807.fkpkr9mggs40o404@mail.ucla.edu>

Dear R/RS-Perl users,

I have a perl script in which I parse a large number of files and 
construct an array of arrayrefs from the data in the files.  I then 
pass that construct to R using the RS Perl interface.  I want to be 
able to use the construct as an R array or matrix so that I can use the 
R function colSums.

So far, I've tried constructing an R matrix with dummy values, and then 
populating each row of the new matrix with the nested Arrays of the 
passed construct. But that just converts the R matrix to type list, so 
I get an error when I try the function colSums.  I then tried 
converting each nested Array using as.numeric() before putting the 
values into the matrix.  But for some reason as.numeric doesn't convert 
the nested arrays to numeric, so I still get an error when I try 
colSums.  If I check the values (say matrix[1][1]) the correct values 
are in the correct locations, but it still won't perform colSums.

I've been hammering away on this for longer than I would like to admit. 
  Any help you could provide would be greatly appreciated.

Best regards,

Aaron



From dgk at bgs.ac.uk  Wed Jan 18 20:33:50 2006
From: dgk at bgs.ac.uk (David Kinniburgh)
Date: Wed, 18 Jan 2006 19:33:50 +0000
Subject: [R] Powell's unconstrained derivative-free nonlinear least
	squares	routine, VA05AD
Message-ID: <s3ce9833.060@wpo.nerc.ac.uk>

I have used Mike Powell's optimization routine (VA05AD) from the Harwell Subroutine Library (HSL) for more than 20 years. It is no exaggeration to say that it has helped make my career (thanks Mike). I recently learned that I am not alone in this respect - apparently it still has a loyal following in all sorts of fields!

It is an exceedingly fine piece of software - fast, reliable and easy to set up. On some early problems that I looked at, it was twice as fast as the equivalent NAG routine.  To quote from the manual,  "A hybrid method is used combining features from the Newton -Raphson, Steepest descent and Marquardt methods and calculating and maintaining an approximation to the first derivative matrix using the ideas of Broyden." 

Now that I have converted to R, I will miss my trusted friend. I have started using nls() but have not accumulated enough experience to compare the two. It would be great if VA05AD could be an option in there (algorithm="Powell"). To this end, I recently enquired of the custodians of the HSL whether it would be possible to make it freely available to the R community. The answer was basically 'yes'. Would anybody be willing to port it across? I am not sufficiently capable of doing this yet.

HSL Archive: http://www.cse.clrc.ac.uk/nag/hsl/contents.shtml 

===================
Dr D G Kinniburgh
British Geological Survey
Crowmarsh Gifford
Wallingford
OX10 8BB 
UK

Phone: 0044 1491 692293
Fax:      0044 1491 692345
email:   dgk_at_bgs.ac.uk



From securebenji-general at yahoo.com  Wed Jan 18 20:39:36 2006
From: securebenji-general at yahoo.com (Ben Ridenhour)
Date: Wed, 18 Jan 2006 11:39:36 -0800 (PST)
Subject: [R] Bootstrapping help
In-Reply-To: <20060118062055.GR75255@ms.unimelb.edu.au>
Message-ID: <20060118193936.19762.qmail@web31802.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/d4ea138a/attachment.pl

From joshuacgilbert at gmail.com  Wed Jan 18 20:41:35 2006
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Wed, 18 Jan 2006 14:41:35 -0500
Subject: [R]  Help with plot.svm from e1071
Message-ID: <ef96daa30601181141k76e37abj7952c2f56e16237f@mail.gmail.com>

Hi.

I'm trying to plot a pair of intertwined spirals and an svm that
separates them. I'm having some trouble. Here's what I tried.

> library(mlbench)
> library(e1071)
Loading required package: class
> raw <- mlbench.spirals(200,2)
> spiral <- data.frame(class=as.factor(raw$classes), x=raw$x[,1], y=raw$x[,2])
> m <- svm(class~., data=spiral)
> plot(m, spiral)
Error in -x$index : invalid argument to unary operator

So we delve into e1071:::plot.svm. When I run the code in plot.svm
everything is fine up until
 points(formula, data = data[-x$index, ], pch = dataSymbol,
                 col = symbolPalette[colind[-x$index]])
That gives me the same error message, "Error in -x$index : invalid
argument to unary operator". The weird thing is that I can run either
of the those statements in isolation
data[-x$index, ]
symbolPalette[colind[-x$index]]
and neither gives me an error. I looked in the two points functions I
can see (points.default and points.formula) but neither calls x$index.

I was following along the documentation for plot.svm, which has a
simple example (that works)
    ## a simple example
    library(MASS)
    data(cats)
    m <- svm(Sex~., data = cats)
    plot(m, cats)

I don't see what the difference between their example and mine.

Can anyone help me?


Thank you,
Josh.



From admin at biostatistic.de  Wed Jan 18 20:48:10 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Wed, 18 Jan 2006 20:48:10 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <1137611803.4333.42.camel@localhost.localdomain>
References: <43CE2EBC.1020303@biostatistic.de>	
	<1137593185.3451.47.camel@localhost.localdomain>	
	<43CE682D.6080209@biostatistic.de>
	<1137611803.4333.42.camel@localhost.localdomain>
Message-ID: <43CE9B7A.3000307@biostatistic.de>



Marc Schwartz (via MN) schrieb:

>
>Unfortunately, it may have precluded my being able to replicate exactly
>what you are seeing, despite being intimately familiar with one of the
>functions (barplot2  ;-) that you are using.
>
>I did create some data that would generally fit what you are doing in
>both cases, however, probably because I am on Linux and you are on
>Windows, where both device and GS differences may be problematic, I
>could not get the second to be rotated relative to the page.
>  
>
I think I described the problem not clearly or misunderstood your text.
There was no Figure rotated on my system - only in the PDF file from the 
jurnal.

>If you want, e-mail me (offlist) the rotated EPS file that is created in
>the second case and I can use that to try to replicate the problem as
>well as review the EPS code to see if something sticks out.
>
>
>Under Linuxen, one can set:
>
>  GS_OPTIONS="-dAutoRotatePages=/None"
>  export GS_OPTIONS ...........
>

I will try to findout this on windows system, but maybe I need some time.
Fist I must send the paper out to the journal. I will attach both Tiff 
for the reviewers and eps for printing
.

>One other quick comment, which is relative to the readability of your
>code.  Strategically placed spaces (" ") and line breaks would help
>tremendously. Especially since most e-mail clients will create line
>breaks at 72 chars (or similar), which can make the flow of the code
>difficult to review.
>

sure  I only used copy and paste from the sriptfile and did noth thought 
about the autowrap fo the e-mailclient.

thanks very much Knut



From mschwartz at mn.rr.com  Wed Jan 18 21:03:30 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 18 Jan 2006 14:03:30 -0600
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE9B7A.3000307@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
	<1137593185.3451.47.camel@localhost.localdomain>
	<43CE682D.6080209@biostatistic.de>
	<1137611803.4333.42.camel@localhost.localdomain>
	<43CE9B7A.3000307@biostatistic.de>
Message-ID: <1137614610.4333.49.camel@localhost.localdomain>

On Wed, 2006-01-18 at 20:48 +0100, Knut Krueger wrote:
> 
> Marc Schwartz (via MN) schrieb:
> 
> >
> >Unfortunately, it may have precluded my being able to replicate exactly
> >what you are seeing, despite being intimately familiar with one of the
> >functions (barplot2  ;-) that you are using.
> >
> >I did create some data that would generally fit what you are doing in
> >both cases, however, probably because I am on Linux and you are on
> >Windows, where both device and GS differences may be problematic, I
> >could not get the second to be rotated relative to the page.
> >  
> >
> I think I described the problem not clearly or misunderstood your text.
> There was no Figure rotated on my system - only in the PDF file from the 
> jurnal.

That is correct. The rotation occurs during the "post processing" of the
EPS file in the full document (or single page) when using latex+dvips
+ps2pdf (or similar tool chain). Not typically the standalone EPS file
itself.

Regards,

Marc



From m.betts at unb.ca  Wed Jan 18 21:37:42 2006
From: m.betts at unb.ca (Matthew Betts)
Date: Wed, 18 Jan 2006 16:37:42 -0400
Subject: [R] Breakpoints for multiple variables using Segmented
Message-ID: <200601182037.k0IKbmdB005883@mailserv.unb.ca>

Hi all,

I am using the package ?Segmented? to estimate logistic regression models
with unknown breakpoints (see Muggeo 2003 Statistics in Medicine
22:3055-3071). In the documentation it suggests that it might be possible to
include several variables with breakpoints in the same model: ?Z = a vector
or a matrix meaning the (continuous) explanatory variable(s) having
segmented relationships with the response?. However, the syntax for
including multiple ?Z? and ?psi? (?starting values for the break-point(s)?)
is not stated. Does anyone have any suggestions?

Here is an example of correct code for detecting single breakpoint:

model.seg<-segmented.glm(obj = model.glm, Z = predictor_variable, psi = 2 ,
it.max = 50)

Thanks very much for your help.


Matthew G. Betts, Ph.D.
NB??Cooperative Fish and Wildlife Research Unit
Faculty of Forestry and Environmental Management
University of New Brunswick??
UNB Tweedale Centre
Hugh John Flemming Forestry Complex
1350 Regent St., Fredericton, N.B.
E3C 2G6
(506) 447-3408
http://www.unb.ca/web/acwern/people/mbetts/mbetts.htm



From sasprog474 at yahoo.com  Wed Jan 18 21:40:35 2006
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Wed, 18 Jan 2006 12:40:35 -0800 (PST)
Subject: [R] Plotting an lme( ) object
Message-ID: <20060118204035.84687.qmail@web35907.mail.mud.yahoo.com>

R for Windows, version 2.2.  I am trying the following
code:

BLAH.lme4 <- lme(fixed = ...., data = ...., random =
....)
plot(BLAH.lme4, resid(.,type = "p") ~ fitted(.) |
GROUP, 
     id = 0.05, adj = -0.3, 
     idLabels = BLAH$VALUE,
     main = "Pearson Residuals vs. Fitted Values, by
Group")

When I use plot( ), it generates a nice plot. 
However, no 
matter what I use for "adj = ..." I cannot get the
observation
labels to shift up, down, left or right.  How can I do
this?
I am trying to follow along with the example in
Pinheiro and
Bates, page 176.

Thanks much,

    Greg



From ucecgxu at ucl.ac.uk  Wed Jan 18 22:47:22 2006
From: ucecgxu at ucl.ac.uk (ucecgxu@ucl.ac.uk)
Date: Wed, 18 Jan 2006 21:47:22 +0000
Subject: [R] r-help,
	how can i use my own distance matrix without using dist()
Message-ID: <20060118214722.rmhqm9xvy8ks080o@www.webmail.ucl.ac.uk>

Dear R-helpers,

i am a beginner of R and i am using cluster package to do hierarchical
clustering

i am wondering if i can use my own distance matrix to do the hierarchical
clustering without using dist() function.

if i have my own distance matrix, how can i ask hclust() function to recongnize
it( as the output of dist() function).

thank you very much and i looking forward to hearing from you.

Marshall



From andy_liaw at merck.com  Wed Jan 18 22:58:03 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 18 Jan 2006 16:58:03 -0500
Subject: [R] r-help,
 how can i use my own distance matrix without usin g dist()
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED718@usctmx1106.merck.com>

Use something like hclust(as.dist(mydist), ...) ought to work.

Andy

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ucecgxu at ucl.ac.uk
Sent: Wednesday, January 18, 2006 4:47 PM
To: r-help at stat.math.ethz.ch
Subject: [R] r-help, how can i use my own distance matrix without using
dist()


Dear R-helpers,

i am a beginner of R and i am using cluster package to do hierarchical
clustering

i am wondering if i can use my own distance matrix to do the hierarchical
clustering without using dist() function.

if i have my own distance matrix, how can i ask hclust() function to
recongnize
it( as the output of dist() function).

thank you very much and i looking forward to hearing from you.

Marshall

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From h.wickham at gmail.com  Wed Jan 18 23:29:49 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 18 Jan 2006 16:29:49 -0600
Subject: [R] do.call, missing arguments and [
Message-ID: <f8e6ff050601181429u6e674e1ay37728ebcb117660f@mail.gmail.com>

x <- array(1:30, c(4,5,3))
x[1,,]

How can I do the same thing with do.call?

do.call("[", list(x, 1)) == x[1]
do.call("[", list(x, 1, NULL, NULL)) == x[1, NULL, NULL]

I guess you can't, because of the special way that [ deals with arguments.

How can I index an array programmatically for arrays and indices of
varying dimensionality?

Thanks,

Hadley



From sasprog474474 at yahoo.com  Wed Jan 18 23:34:04 2006
From: sasprog474474 at yahoo.com (Greg Tarpinian)
Date: Wed, 18 Jan 2006 14:34:04 -0800 (PST)
Subject: [R] Plotting an lme( ) object
Message-ID: <20060118223404.53064.qmail@web37106.mail.mud.yahoo.com>

I apologize for the second posting, my other email address
died today.

I am using R for Windows, version 2.2.  Here is my code:

  plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | LOT, 
id = 0.05, adj = -0.3, idLabels = RO54F$VALUE,
				 main = "Pearson Residuals vs. Fitted Values, by Lot", between = list(x =
.5, y = .5))



From marcodoc75 at yahoo.com  Wed Jan 18 23:38:25 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Wed, 18 Jan 2006 14:38:25 -0800 (PST)
Subject: [R] r-help,
	how can i use my own distance matrix without using dist()
In-Reply-To: <20060118214722.rmhqm9xvy8ks080o@www.webmail.ucl.ac.uk>
Message-ID: <20060118223825.8093.qmail@web31305.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/72a43c58/attachment.pl

From sasprog474474 at yahoo.com  Wed Jan 18 23:37:27 2006
From: sasprog474474 at yahoo.com (Greg Tarpinian)
Date: Wed, 18 Jan 2006 14:37:27 -0800 (PST)
Subject: [R] Plotting an lme( ) object
Message-ID: <20060118223727.96381.qmail@web37107.mail.mud.yahoo.com>

I apologize for the second posting, my other email address
died today.

I am using R for Windows, version 2.2.  Here is my code:

  plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | GROUP, 
       id = 0.05, adj = -0.3, 
       idLabels = FOO$value,
       main = "Pearson Residuals vs. Fitted Values, by Group",
       between = list(x = .5, y = .5))

The plot looks fine, but the "adj = -0.3" option seems to have
no effect on the labels that are added to identify potential
outliers.  I would like to offset the FOO$value text that is
currently being displayed right on top of several pearson
residuals.  How can I do this?


Thanks much,

     Greg



From deepayan.sarkar at gmail.com  Thu Jan 19 00:31:40 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 18 Jan 2006 17:31:40 -0600
Subject: [R] do.call, missing arguments and [
In-Reply-To: <f8e6ff050601181429u6e674e1ay37728ebcb117660f@mail.gmail.com>
References: <f8e6ff050601181429u6e674e1ay37728ebcb117660f@mail.gmail.com>
Message-ID: <eb555e660601181531q58ecd556mbeeefbeca5caac04@mail.gmail.com>

On 1/18/06, hadley wickham <h.wickham at gmail.com> wrote:
> x <- array(1:30, c(4,5,3))
> x[1,,]
>
> How can I do the same thing with do.call?

do.call("[", list(x, 1, TRUE, TRUE))

seems to work for me.

Deepayan

>
> do.call("[", list(x, 1)) == x[1]
> do.call("[", list(x, 1, NULL, NULL)) == x[1, NULL, NULL]
>
> I guess you can't, because of the special way that [ deals with arguments.
>
> How can I index an array programmatically for arrays and indices of
> varying dimensionality?



From p.connolly at hortresearch.co.nz  Thu Jan 19 00:36:23 2006
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 19 Jan 2006 12:36:23 +1300
Subject: [R] Canonical Variance Analysis by any other name?
Message-ID: <20060118233623.GR18619@hortresearch.co.nz>

I've been asked about "Canonical Variance Analysis" (CVA).  I don't
see any reference to it searching the R site.  Does it go by other
names?

Genstat describes it thus:

Canonical variates analysis operates on a within-group sums of squares
and products matrix, calculated from a set of variates and factor that
specifies the grouping of units. It finds linear combinations of the
variates that maximize the ratio of between-group to within-group
variation, thereby giving functions that can be used to discriminate
between the groups.

It's probably not particularly difficult to do, so I suspect someone
has a package for doing it.  What other name might I search for?

Thnx

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From A.Robinson at ms.unimelb.edu.au  Thu Jan 19 00:45:15 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 19 Jan 2006 10:45:15 +1100
Subject: [R] Bootstrapping help
In-Reply-To: <20060118193936.19762.qmail@web31802.mail.mud.yahoo.com>
References: <20060118062055.GR75255@ms.unimelb.edu.au>
	<20060118193936.19762.qmail@web31802.mail.mud.yahoo.com>
Message-ID: <20060118234515.GA45344@ms.unimelb.edu.au>

Ben,

although I appended a smiley to my first note, the message was
serious.  If you don't show us what you're doing, we can't help you.
Please provide an example in which you:

1) generate a small dataframe similar in structure to yours
2) provide cs
3) show the boot statement that applies cs to the example dataframe.

Also, it seems that you are unfamiliar with the use of indexing and
datframes.  Please read the Introduction to R, carefully, it is freely
available on CRAN.  You have asked R to provide you with all the rows
that are numbered 1/sample size.R; since the row numbers are integers
there aren't any.

And, please say hello to Andrew Storfer and Melanie Murphy from me.

Andrew

On Wed, Jan 18, 2006 at 11:39:36AM -0800, Ben Ridenhour wrote:
> 
>    Andrew,
>    Thanks for the suggestion!  This seems to have fixed things.  I was
>    wondering if you could explain why this works and what was wrong.  If
>    I issue the command
>    >my.boot<-boot(dataframe,cs,R=999)
>    and in order to what effect the command you told me use has I then do
>    something like
>    >dataframe[my.boot$weights,]
>    my.boot$weights looks to be a vector where element is 1/sample size.R
>    reports that
>    [1] var1    var2   var3   var4 var5
>    <0 rows> (or 0-length row.names)
>     Which indicates to me that I then have a dataframe with no data in
>    it! (Am I wrong about this?)  What is going on here?  Why did this
>    work?  Sorry for the basic questions.
>    Ben

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From securebenji-general at yahoo.com  Thu Jan 19 01:35:47 2006
From: securebenji-general at yahoo.com (Ben Ridenhour)
Date: Wed, 18 Jan 2006 16:35:47 -0800 (PST)
Subject: [R] Bootstrapping help
In-Reply-To: <20060118234515.GA45344@ms.unimelb.edu.au>
Message-ID: <20060119003547.46684.qmail@web31807.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/5492e391/attachment.pl

From A.Robinson at ms.unimelb.edu.au  Thu Jan 19 02:07:27 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 19 Jan 2006 12:07:27 +1100
Subject: [R] Bootstrapping help
In-Reply-To: <20060119003547.46684.qmail@web31807.mail.mud.yahoo.com>
References: <20060118234515.GA45344@ms.unimelb.edu.au>
	<20060119003547.46684.qmail@web31807.mail.mud.yahoo.com>
Message-ID: <20060119010727.GG45344@ms.unimelb.edu.au>

Ben,

Ok, it's clear now, thanks.  Note that your boot call 

boot(mydata,cs,R=999)

does not specify an "stype" argument.  The boot help file notes that
the default value for stype is "i", which means that boot will pass an
index to the function, not a weight, regardless of whether you call it
w, i, or whatever. 

The index that boot sends to the function is then used to index the
dataframe, thus selecting rows randomly with replacement.  Previously
you passed the dataframe to the function, which did not alter it, so
it passed through undisturbed.  In this incarnation the data<-data[w,]
command provides you with the (pseudo-)random sample with replacement
of the data.

I hope that this clears up the confusion.

Cheers,

Andrew

ps it's always good to provide a brief bit of sample code when you ask
a question.  Also, let me recommend that you omit semi-colons and
space the code to make it easier to read.  Thus

cs <- function(data, w) { 
     data<-data[w, ] 
     ...



On Wed, Jan 18, 2006 at 04:35:47PM -0800, Ben Ridenhour wrote:
> 
>    Thanks for responding :) Again...
>    I understand how indexing works (basically as in any other programming
>    language), that is why I am so confused as to why that statement made
>    my bootstraping work!  It seems like, if anything, it would completely
>    screw up everything.
>    Here is the (now working) cs function after I amended it to what you
>    said to do (i.e. I added the _very confusing_ statement
>    data<-data[w,]):
>    >cs<-function(data, w){
>    data<-data[w,];
>    y<-data[1];
>    x1<-data[2];
>    x2<-data[3];
>    x3<-data[4];
>    z<-data[5];
>    c1<-x1*z;
>    c2<-x2*z;
>    c3<-x3*z;
>    X<-cbind(x1,x2,x3,z,c1,c2,c3);
>    regcoef<-lsfit(X,y)$coefficients;
>    bx1<-regcoef[[2]];
>    bx2<-regcoef[[3]];
>    bx3<-regcoef[[4]];
>    bz<-regcoef[[5]];
>    bc1<-regcoef[[6]];
>    bc2<-regcoef[[7]];
>    bc3<-regcoef[[8]];
>    fx<-bx1*x1+bx2*x2+bx3*x3;
>    gy<-bz*z;
>    hxy<-bc1*c1+bc2*c2+bc3*c3;
>    sfx1<-cov(fx,x1);
>    sfx2<-cov(fx,x2);
>    sfx3<-cov(fx,x3);
>    sgx1<-cov(gy,x1);
>    sgx2<-cov(gy,x2);
>    sgx3<-cov(gy,x3);
>    shx1<-cov(hxy,x1);
>    shx2<-cov(hxy,x2);
>    shx3<-cov(hxy,x3);
>    sTx1<-cov(y,x1);
>    sTx2<-cov(y,x2);
>    sTx3<-cov(y,x3);
>    dataout<-c(sfx1,sgx1,shx1,sTx1,sfx2,sgx2,shx2,sTx2,sfx3,sgx3,shx3,sTx3
>    );
>    dataout
>    }
>    An example data frame would be
>     >mydata<-data.frame(Y=rnorm(20,0,1),X1=rnorm(20,0,1),X2=rnorm(20,0,1)
>    ,X3=rnorm(20,0,1),Z=rnorm(20,0,1))
>    The  boot statement is
>    >boot(mydata,cs,R=999)
>    Why does this rather mysterious indexing statement "data<-data[w,]"
>    make the bootstrap work when it didn't beforehand?
>    Thanks,
>    Ben
>    ps. I'll tell Melanie and Storfer hello.
>    -------------------------------
>    Benjamin Ridenhour
>    School of Biological Sciences
>    Washigton State University
>    P.O. Box 644236
>    Pullman, WA 99164-4236
>    Phone (509)335-7218
>    --------------------------------
>    "Nothing in biology makes sense except in the light of evolution."
>    -T. Dobzhansky
>    http://www.ms.unimelb.edu.au

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
Email: a.robinson at ms.unimelb.edu.au         http://www.ms.unimelb.edu.au



From h.wickham at gmail.com  Thu Jan 19 02:38:54 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 18 Jan 2006 19:38:54 -0600
Subject: [R] do.call, missing arguments and [
In-Reply-To: <eb555e660601181531q58ecd556mbeeefbeca5caac04@mail.gmail.com>
References: <f8e6ff050601181429u6e674e1ay37728ebcb117660f@mail.gmail.com>
	<eb555e660601181531q58ecd556mbeeefbeca5caac04@mail.gmail.com>
Message-ID: <f8e6ff050601181738u24c2a9b6m5af46cb1cd03bb65@mail.gmail.com>

> do.call("[", list(x, 1, TRUE, TRUE))
>
> seems to work for me.

Oh! Of course.  I knew someone obvious was staring me in the face.

Thanks,

Hadley



From berwin at maths.uwa.edu.au  Thu Jan 19 02:55:00 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 19 Jan 2006 09:55:00 +0800
Subject: [R] Powell's unconstrained derivative-free nonlinear
	least	squares	routine, VA05AD
In-Reply-To: <s3ce9833.060@wpo.nerc.ac.uk>
References: <s3ce9833.060@wpo.nerc.ac.uk>
Message-ID: <17358.61812.307792.363462@bossiaea.maths.uwa.edu.au>

G'day David,

>>>>> "DK" == David Kinniburgh <dgk at bgs.ac.uk> writes:

    DK> I have used Mike Powell's optimization routine (VA05AD) from
    DK> the Harwell Subroutine Library (HSL) for more than 20 years.
    DK> [...]

    DK> Now that I have converted to R, I will miss my trusted
    DK> friend. I have started using nls() but have not accumulated
    DK> enough experience to compare the two. It would be great if
    DK> VA05AD could be an option in there (algorithm="Powell"). To
    DK> this end, I recently enquired of the custodians of the HSL
    DK> whether it would be possible to make it freely available to
    DK> the R community. The answer was basically 'yes'.  [...]
You better ask again. :-)

I presume VA05AD is in what is now called the HSL archive?  If it is
part of HSL 2004, then I don't see a way how it could be incorporated
into R given the information on their web site.  

Even for the HSL archive, it is stated at
http://hsl.rl.ac.uk/archive/hslarchive.html that

        "[...] HSL Archive codes are now available without charge to
        anyone, so long as they are not then incorporated into a
        commercial product; the latter is, of course, still permitted
        subject to a commercial licence. [...]"

which sounds as if it would be possible to interface (parts of) HSL
archive to R.  But then that URL goes on ans states:

        "Access to the Archive is by means of a short-lived individual
        password-controlled account. Potential users are asked for
        brief details of the use they intend to make of the package(s)
        they aim to download. Users are also asked to accept a
        conditions-of-use form, and are not permitted to divulge their
        userid and password to anyone else, nor to distribute any
        codes they obtain to a third party." 

IANAL, but as I understand the GPL, the last part of that sentence
will make it pretty impossible to incorporate into R (or distribute as
an R package) (parts of) the HSL archive routines.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From gelman at stat.columbia.edu  Thu Jan 19 04:00:53 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Wed, 18 Jan 2006 22:00:53 -0500
Subject: [R] mcmcsamp() in lmer
In-Reply-To: <43C40AFC.4040003@stat.columbia.edu>
References: <43C40AFC.4040003@stat.columbia.edu>
Message-ID: <43CF00E5.104@stat.columbia.edu>

I am working with lmer() in the latest release of Matrix, doing various 
things including writing a function called mcsamp() that acts as a 
wrapper for mcmcsamp() and automatically runs multiple chains, diagnoses 
convergence, and stores the result as a bugs object so it can be 
plotted.  I recognize that at this point, mcmcsamp() is somewhat of a 
placeholder (since it doesn't work on a lot of models) but I'm sure it 
will continue to be improved so I'd like to be able to work with it, as 
a starting point if necessary.

Anyway, I couldn't get mcmcsamp() to work with the saveb=TRUE option.  
Here's a simple example:

y <- 1:10
group <- rep (c(1,2), c(5,5))
M1 <- lmer (y ~ 1 + (1 | group))   # works fine
mcmcsamp (M1)                         # works fine
mcmcsamp (M1, saveb=TRUE)

This last gives an error message:

Error in "colnames<-"(`*tmp*`, value = c("(Intercept)", "log(sigma^2)",  :
        length of 'dimnames' [2] not equal to array extent

Thanks for your help.
Andrew

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From spencer.graves at pdf.com  Thu Jan 19 04:40:05 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jan 2006 19:40:05 -0800
Subject: [R] glmmPQL: Na/NaN/Inf in foreign function call
In-Reply-To: <0138E72D-22BE-40EB-A80F-5882321DDDB9@gmail.com>
References: <0138E72D-22BE-40EB-A80F-5882321DDDB9@gmail.com>
Message-ID: <43CF0A15.6090009@pdf.com>

	  I have not seen a reply to this post, and unfortunately, I'm not 
smart enough to help you.  If you would still like help from this 
listserve, please provide a terse, reproducible example that someone can 
in seconds copy from your email into R and presumably get the same error 
message you report.  If you have that, then anyone with interest in 
glmmPQL can try to help you.  Without that, your email must reach 
someone who has gotten that specific error message from glmmPQL with 
sufficient regularity that the error message has become solidly recorded 
in their brains.  Moreover, this person must also have the time and 
interest to reply.  Don't gamble on a long shot.  Play the odds.  > 
PLEASE do read the posting guide! "www.R-project.org/posting-guide.html".

	  spencer graves
David Reitter wrote:

> I'm using glmmPQL, and I still have a few problems with it.
> In addition to the issue reported earlier, I'm getting the following  
> error and I was wondering if there's something I can do about it.
> 
> 
> Error in logLik.reStruct(object, conLin) : Na/NaN/Inf in foreign  
> function call (arg 3)
> 
> ... Warnings:
> 1: Singular precistion matrix in level -1, block 4
> (...)
> 4: ""
> 
> The interaction terms are
> 
> primed ~ log(dist) * role
> random = ~ dist | target.utt / prime.utt
> 
> The family is binomial (logit).
> 
> I ensured that log(dist) is always [0; 1]. Role is a factor (binary).  
> target.utt and prime.utt are categories as well.
> 
> This is version 2.1.1 - is the latest version more reliable?
> 
> Thanks for any help you can give me.
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sjpruitt at ucsd.edu  Thu Jan 19 05:17:22 2006
From: sjpruitt at ucsd.edu (Seth Pruitt)
Date: Wed, 18 Jan 2006 20:17:22 -0800
Subject: [R] numericDeriv() giving a vector when multiple variables input
Message-ID: <ed55c06b0601182017p6cfb7bb8ia01c0852102946c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/8a6075b8/attachment.pl

From rahmank at frim.gov.my  Thu Jan 19 21:19:30 2006
From: rahmank at frim.gov.my (Abd Rahman Kassim)
Date: Thu, 19 Jan 2006 12:19:30 -0800
Subject: [R] Legend Outside Plot Dimension
Message-ID: <008d01c61d35$a72611c0$4202a8c0@DrAbRahmandt7>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/339f75f3/attachment.pl

From spencer.graves at pdf.com  Thu Jan 19 05:37:33 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jan 2006 20:37:33 -0800
Subject: [R] Homogenous groups building - Randomisation
In-Reply-To: <OFC79C0E5D.97AC8F04-ONC12570F8.005CBCBF-C12570F8.005DE926@fr.fournierpharma.com>
References: <OFC79C0E5D.97AC8F04-ONC12570F8.005CBCBF-C12570F8.005DE926@fr.fournierpharma.com>
Message-ID: <43CF178D.6020401@pdf.com>

	  I'm sorry, but I still do not understand, but I will make a guess: 
You have data on n characteristics from each of many patients, and you 
want to form N groups, with roughly k patients per group from the next 
roughly k*N patients who walk into your clinic.

	  If this is your problem, and if you have a "training set" of data you 
can use to define group selection criteria, I suggest you try various 
versions of cluster analysis, including "hclust" in the "stats" package 
and other functions described with "See also" in the "hclust" 
documentation.

	  If you don't have this and would still like help from this listserve, 
please think very carefully about what it would take for someone to 
produce a useful response to your question.  Je voudrais vous aidez, 
mais ce demand plus que j'ai.

	  Hope this helps,
	  spencer graves

a.menicacci at fr.fournierpharma.com wrote:

> 
> 
> 
> Dear R-users,
> 
> We expect to form N homogeneous groups of n features from an
> experimentation including N*n data.
> The aim is to prevent group effects.
> How to do that with R functionalitites ? Does anyone know any way enabling
> this ?
> 
> 
> Example :
> 
> 100 patients are observed. 3 biochemical parameters are mesured for each
> one (Red and white globules ans glycemia).
> 
> Patient          RG           RW          Gly
> 1              3.4      1.38         1.62
> 2              1.8      1.19     1.55
> 3              1.9      1.26     1.77
> 4              3.0      1.29     1.72
> 5              1.9      1.09     1.72
> 6              3.3      1.31     1.63
> ...                  ...             ...      ...
> 
> 
> These are freakish data.
> 
> How to compute 10 "equivalent" groups ?
> For example with closed parameters means between groups :
> 
> Group   RGmean      RWmean      Glymean
> 1     1.5               1.22              1.68
> 2     1.3         1.29              1.75
> 3           1.6               1.25              1.63
> 4     1.2               1.23              1.70
> ...   ...               ...               ...
> 
> 
> Best regards.
> 
> 
> Alexandre MENICACCI
> Bioinformatics - FOURNIER PHARMA
> 50, rue de Dijon - 21121 Daix - FRANCE
> a.menicacci at fr.fournierpharma.com
> t??l : 03.80.44.76.17
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From slacey at umich.edu  Thu Jan 19 05:38:09 2006
From: slacey at umich.edu (Steven Lacey)
Date: Wed, 18 Jan 2006 23:38:09 -0500
Subject: [R] creating objects with a slot of class formula, using new
Message-ID: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060118/564d797e/attachment.pl

From spencer.graves at pdf.com  Thu Jan 19 05:55:07 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jan 2006 20:55:07 -0800
Subject: [R] singular convergence(7)?
In-Reply-To: <E305A4AFB7947540BC487567B5449BA80921BCB5@scsmsx402.amr.corp.intel.com>
References: <E305A4AFB7947540BC487567B5449BA80921BCB5@scsmsx402.amr.corp.intel.com>
Message-ID: <43CF1BAB.1090608@pdf.com>

	  In general, "singular convergence" means you were trying to estimate 
k parameters when the data would support estimation k-1 or fewer.

	  Beyond this, RSiteSearch("singlular convergence") produce 53 hits for 
me just now, and RSiteSearch("lme singular convergence") produced 7.  If 
none of those answer your question and you would still like help from 
this listserve, please submit a terse, reproducible example consistent 
with "www.R-project.org/posting-guide.html".  You can increase your 
chances of getting a useful reply by making it easier for potential 
respondents to comment.  With a simple, terse, reproducible example, 
anyone interested in "lme" can copy your code into R and see your error 
message in seconds.  With a slightly greater effort, they might be able 
to tell you exactly what you want to know.  Without such a simple, 
reproducible example, you are throwing darts blindly, hoping one will 
find the bulls-eye of a target you only hope is there.

	  hope this helps.
	  spencer graves

Chia, Yen Lin wrote:

> Hi all,
> 
>  
> 
> I just wonder what singular convergence means. Thanks.
> 
>  
> 
> Yen Lin
> 
>  
> 
> Error in lme.formula(Data ~ 1, random = ~1 | Wafer/fie/loc, subset =
> Wafer ==  : 
> 
>         singular convergence (7)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jacques.veslot at cirad.fr  Thu Jan 19 06:15:45 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 19 Jan 2006 09:15:45 +0400
Subject: [R] Legend Outside Plot Dimension
In-Reply-To: <008d01c61d35$a72611c0$4202a8c0@DrAbRahmandt7>
References: <008d01c61d35$a72611c0$4202a8c0@DrAbRahmandt7>
Message-ID: <43CF2081.4030802@cirad.fr>

use "xpd" argument in par(), as follows:

 > ?par
 > par(xpd=T, mar=par()$mar+c(0,0,0,4))
 > plot(1,1)
 > legend(1.5,1,"point",pch=1)


Abd Rahman Kassim a ??crit :

>Dear All,
>
>I'm trying to attach a legend outside the plot (Inside plot OK), but failed. Any help is very much appreciated.
>
>Thanks.
>
>
>Abd. Rahman Kassim, PhD
>Forest Management & Ecology Program
>Forestry & Conservation Division
>Forest Research Institute Malaysia
>Kepong 52109 Selangor
>MALAYSIA
>
>*****************************************
>
>Checked by TrendMicro Interscan Messaging Security.
>For any enquiries, please contact FRIM IT Department.
>*****************************************
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ggrothendieck at gmail.com  Thu Jan 19 06:27:42 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jan 2006 00:27:42 -0500
Subject: [R] creating objects with a slot of class formula, using new
In-Reply-To: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
References: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
Message-ID: <971536df0601182127k1d341233qddba2084e609e4fa@mail.gmail.com>

Create a virtual class that can either be a formula or be NULL:

   setClassUnion("formulaOrNULL", c("formula", "NULL"))
   setClass("a",representation(b="list",c="formulaOrNULL"))
   new("a", b = list(7))

On 1/18/06, Steven Lacey <slacey at umich.edu> wrote:
> Hi,
>
> This works fine.
> >setClass("a",representation(b="list",c="list"))
> >new("a",b=list(7))
> >An object of class "a"
> Slot "b":
> [[1]]
> [1] 7
>
>
> Slot "c":
> list()
>
> But, now suppose you want a slot to accept an object of class formula...
> >setClass("a",representation(b="list",c="formula"))
> >new("a",b=list(7))
> >Error in validObject(.Object) : invalid class "a" object: invalid object
> for slot "c" in class "a": got class "NULL", should be or extend class
> "formula"
>
> Why can't new handle this? Why must the slot be defined?
>
> If I call new without any named arguments, it works fine
> > new("a")
> An object of class "a"
> Slot "b":
> list()
>
> Slot "c":
> NULL
>
> If I call new with only a formula, it works fine.
> > new("a",c=formula(x~y))
> An object of class "a"
> Slot "b":
> list()
>
> Slot "c":
> x ~ y
>
> How can I get R to do this?
> >setClass("a",representation(b="list",c="formula"))
> >new("a",b=list(7))
> An object of class "a"
> Slot "b":
> [[1]]
> [1] 7
>
> Slot "c":
> NULL
>
> Thanks,
> Steve
>
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.1
> year     2005
> month    06
> day      20
> language R
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fsaldan1 at gmail.com  Thu Jan 19 06:50:40 2006
From: fsaldan1 at gmail.com (Fernando Saldanha)
Date: Thu, 19 Jan 2006 00:50:40 -0500
Subject: [R] sapply on data frames - $X vs.["X"]
Message-ID: <10dee4690601182150r19bd0abfl539ccf1d53f1d0b9@mail.gmail.com>

I have a program where the following code works fine:

df.opt$Delta <- apply(df.opt, 1, function(x)
EuropeanOption(x["CallPutString"], x["UnderlyingPrice"],
x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
x["Vol"])[[2]])

However, the code below fails:

#df.opt$Delta <- apply(df.opt, 1, function(x)
EuropeanOption(x$CallPutString, #x["UnderlyingPrice"],
x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
#x["Vol"])[[2]])

Here df.opt is a data frame and European Option is a function in the
package RQuantLib.
The only difference between the two pieces of code is that a column of
the data frame is accessed using "$" in the first case and brackets in
the second case.

Sorry if this is a trivial question, but could somebody tell me the
reason for this behavior?

FS



From jacques.veslot at cirad.fr  Thu Jan 19 07:05:03 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 19 Jan 2006 10:05:03 +0400
Subject: [R] sapply on data frames - $X vs.["X"]
In-Reply-To: <10dee4690601182150r19bd0abfl539ccf1d53f1d0b9@mail.gmail.com>
References: <10dee4690601182150r19bd0abfl539ccf1d53f1d0b9@mail.gmail.com>
Message-ID: <43CF2C0F.4060702@cirad.fr>

with apply loop, x is a vector you are indexing in the first sample code 
with its names.
so it failed when you tried indexing it with "$" which is dedicated to 
data frames.
 

Fernando Saldanha a ??crit :

>I have a program where the following code works fine:
>
>df.opt$Delta <- apply(df.opt, 1, function(x)
>EuropeanOption(x["CallPutString"], x["UnderlyingPrice"],
>x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
>x["Vol"])[[2]])
>
>However, the code below fails:
>
>#df.opt$Delta <- apply(df.opt, 1, function(x)
>EuropeanOption(x$CallPutString, #x["UnderlyingPrice"],
>x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
>#x["Vol"])[[2]])
>
>Here df.opt is a data frame and European Option is a function in the
>package RQuantLib.
>The only difference between the two pieces of code is that a column of
>the data frame is accessed using "$" in the first case and brackets in
>the second case.
>
>Sorry if this is a trivial question, but could somebody tell me the
>reason for this behavior?
>
>FS
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ggrothendieck at gmail.com  Thu Jan 19 07:09:48 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jan 2006 01:09:48 -0500
Subject: [R] sapply on data frames - $X vs.["X"]
In-Reply-To: <10dee4690601182150r19bd0abfl539ccf1d53f1d0b9@mail.gmail.com>
References: <10dee4690601182150r19bd0abfl539ccf1d53f1d0b9@mail.gmail.com>
Message-ID: <971536df0601182209j5a068988v4f4f10c29c94d39c@mail.gmail.com>

$ can be used with data frames but not arrays and apply is
not sending a data frame to your function.  For
example, try this:

apply(df.opt, 1, is.data.frame)

By the way, in addition the difference you mention there
are a bunch of # marks in your second statement.

On 1/19/06, Fernando Saldanha <fsaldan1 at gmail.com> wrote:
> I have a program where the following code works fine:
>
> df.opt$Delta <- apply(df.opt, 1, function(x)
> EuropeanOption(x["CallPutString"], x["UnderlyingPrice"],
> x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
> x["Vol"])[[2]])
>
> However, the code below fails:
>
> #df.opt$Delta <- apply(df.opt, 1, function(x)
> EuropeanOption(x$CallPutString, #x["UnderlyingPrice"],
> x["StrikePrice"], 0, interest.rate, x["FractionalExpiration"],
> #x["Vol"])[[2]])
>
> Here df.opt is a data frame and European Option is a function in the
> package RQuantLib.
> The only difference between the two pieces of code is that a column of
> the data frame is accessed using "$" in the first case and brackets in
> the second case.
>
> Sorry if this is a trivial question, but could somebody tell me the
> reason for this behavior?
>
> FS
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sfalcon at fhcrc.org  Thu Jan 19 08:36:43 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 18 Jan 2006 23:36:43 -0800
Subject: [R] creating objects with a slot of class formula, using new
In-Reply-To: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
	(Steven Lacey's message of "Wed, 18 Jan 2006 23:38:09 -0500")
References: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
Message-ID: <m2d5iod5ys.fsf@fhcrc.org>

On 18 Jan 2006, slacey at umich.edu wrote:
> But, now suppose you want a slot to accept an object of class
> formula...
>> setClass("a",representation(b="list",c="formula"))
>> new("a",b=list(7)) Error in validObject(.Object) : invalid class
>> "a" object: invalid object
> for slot "c" in class "a": got class "NULL", should be or extend
> class "formula"
>
> Why can't new handle this? Why must the slot be defined? 

Because 'formula' is not a formal S4 class.  new("formula") doesn't
work.

And because isVirtualClass(getClass("formula")) returns TRUE, which I
find surprising since one can have an instance of a formula.

One workaround is to define a prototype:

setClass("a", representation(b="list", c="formula"), 
         prototype=prototype(c=formula(~1)))

+ seth



From rahmank at frim.gov.my  Fri Jan 20 00:42:19 2006
From: rahmank at frim.gov.my (Abd Rahman Kassim)
Date: Thu, 19 Jan 2006 15:42:19 -0800
Subject: [R] Legend Outside Plot Dimension
References: <008d01c61d35$a72611c0$4202a8c0@DrAbRahmandt7> 
	<43CF2081.4030802@cirad.fr>
Message-ID: <001701c61d51$fcb2f1a0$4202a8c0@DrAbRahmandt7>


Dear Jacques,

Thanks for the promt response.

Abd. Rahman
----- Original Message ----- 
From: "Jacques VESLOT" <jacques.veslot at cirad.fr>
To: "Abd Rahman Kassim" <rahmank at frim.gov.my>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 18, 2006 9:15 PM
Subject: Re: [R] Legend Outside Plot Dimension



use "xpd" argument in par(), as follows:

 > ?par
 > par(xpd=T, mar=par()$mar+c(0,0,0,4))
 > plot(1,1)
 > legend(1.5,1,"point",pch=1)


Abd Rahman Kassim a ??crit :

>Dear All,
>
>I'm trying to attach a legend outside the plot (Inside plot OK), but 
>failed. Any help is very much appreciated.
>
>Thanks.
>
>
>Abd. Rahman Kassim, PhD
>Forest Management & Ecology Program
>Forestry & Conservation Division
>Forest Research Institute Malaysia
>Kepong 52109 Selangor
>MALAYSIA
>
>*****************************************
>
>Checked by TrendMicro Interscan Messaging Security.
>For any enquiries, please contact FRIM IT Department.
>*****************************************
> [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>
>



*****************************************

Checked by TrendMicro Interscan Messaging Security.
For any enquiries, please contact FRIM IT Department.
***************************************** 


*****************************************

Checked by TrendMicro Interscan Messaging Security.
For any enquiries, please contact FRIM IT Department.



From r.hankin at noc.soton.ac.uk  Thu Jan 19 09:08:02 2006
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 19 Jan 2006 08:08:02 +0000
Subject: [R] [R-pkgs] new R package: UNTB
Message-ID: <1F37DFAA-7366-42F4-B116-B410CF994E5E@soc.soton.ac.uk>

Dear List

please find uploaded to CRAN a new R package for the simulation and  
analysis
of biodiversity.

The package implements simulations and diagnostics of ecological systems
that obey the Unified Neutral Theory of Biodiversity (untb).

The canonical reference is:


The Unified Neutral Theory of Biodiversity and Biogeography,
Stephen P. Hubbell, Princeton University Press, 2000.


enjoy



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From petr.pikal at precheza.cz  Thu Jan 19 09:11:50 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 19 Jan 2006 09:11:50 +0100
Subject: [R] Legend Outside Plot Dimension
In-Reply-To: <008d01c61d35$a72611c0$4202a8c0@DrAbRahmandt7>
Message-ID: <43CF57D6.8628.408BED@localhost>

Hi

I think you need to use par(xpd=TRUE). Try to search archives as 
similar question was answered few days ago.

HTH
Petr


On 19 Jan 2006 at 12:19, Abd Rahman Kassim wrote:

From:           	"Abd Rahman Kassim" <rahmank at frim.gov.my>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Thu, 19 Jan 2006 12:19:30 -0800
Subject:        	[R] Legend Outside Plot Dimension

> 
> Dear All,
> 
> I'm trying to attach a legend outside the plot (Inside plot OK), but
> failed. Any help is very much appreciated.
> 
> Thanks.
> 
> 
> Abd. Rahman Kassim, PhD
> Forest Management & Ecology Program
> Forestry & Conservation Division
> Forest Research Institute Malaysia
> Kepong 52109 Selangor
> MALAYSIA
> 
> *****************************************
> 
> Checked by TrendMicro Interscan Messaging Security.
> For any enquiries, please contact FRIM IT Department.
> *****************************************
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From maechler at stat.math.ethz.ch  Thu Jan 19 09:20:11 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 19 Jan 2006 09:20:11 +0100
Subject: [R] creating objects with a slot of class formula, using new
In-Reply-To: <971536df0601182127k1d341233qddba2084e609e4fa@mail.gmail.com>
References: <000001c61cb2$2938c2e0$6700a8c0@lsa.adsroot.itcs.umich.edu>
	<971536df0601182127k1d341233qddba2084e609e4fa@mail.gmail.com>
Message-ID: <17359.19387.459082.320467@stat.math.ethz.ch>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at gmail.com>
>>>>>     on Thu, 19 Jan 2006 00:27:42 -0500 writes:

    Gabor> Create a virtual class that can either be a formula or be NULL:
    > setClassUnion("formulaOrNULL", c("formula", "NULL"))
    > setClass("a",representation(b="list",c="formulaOrNULL"))
    > new("a", b = list(7))

that's one possibility, and probably closest to what Steve was
asking for,  however read on

    Gabor> On 1/18/06, Steven Lacey <slacey at umich.edu> wrote:
    >> Hi,
    >> 
    >> .....
    >> 
    >> But, now suppose you want a slot to accept an object of class formula...

    >> >setClass("a",representation(b="list",c="formula"))
    >> >new("a",b=list(7))
    >> >Error in validObject(.Object) : invalid class "a" object: invalid object
    >> for slot "c" in class "a": got class "NULL", should be or extend class
    >> "formula"
    >> 
    >> Why can't new handle this? Why must the slot be defined?

the 'c' slot need not be defined, but it must be initialized to
an object of class "formula"

    >> If I call new without any named arguments, it works fine
    >> > new("a")
    >> An object of class "a"
    >> Slot "b":
    >> list()
    >> 
    >> Slot "c":
    >> NULL

No, it does not "work fine"; it has created an *invalid* object,
since, for efficiency reasons,
the internal equivalent of  validObject()  is not called in all
cases of object creation {and that is good: basic object
creation should be fast}:

  > (aa <- new("a"))
  An object of class "a"
  Slot "b":
  list()

  Slot "c":
  NULL

  > validObject(aa)## gives an error as it should
  Error in validObject(aa) : invalid class "a" object: invalid object for slot "c" in class "a": got class "NULL", should be or extend class "formula"


    >> If I call new with only a formula, it works fine.
    >> > new("a",c=formula(x~y))
    >> An object of class "a"
    >> Slot "b":
    >> list()
    >> 
    >> Slot "c":
    >> x ~ y

yes, since 'c' now *is* a formula, and BTW,
even the simpler
     new("a", c = x~y)

is sufficient {recurring topic of yesterday:  'y ~ x'  *is* a formula
	       and does not need an as.formula(.) or formula(.)
	       around it !!}

    >> How can I get R to do this?
    >> >setClass("a",representation(b="list",c="formula"))
    >> >new("a",b=list(7))
    >> An object of class "a"
    >> Slot "b":
    >> [[1]]
    >> [1] 7

    >> Slot "c":
    >> NULL

you can't and shouldn't be able to: If slot 'c' is defined to be
a formula, it should be a formula and not NULL.
So you need to change the class definition.

Apart from Gabor's possibility which I consider a bit unelegant and
not ideal here, I'd strongly suggest you make use of the 
'prototype' argument when you define the class,
e.g. :

  > setClass("a", representation(b = "list", f = "formula"),
  +          prototype = prototype(b = list(), f = y ~ x))
  [1] "a"
  > (aa <- new("a"))
  An object of class "a"
  Slot "b":
  list()

  Slot "f":
  y ~ x

  > validObject(aa)
  [1] TRUE
  > 

--
Martin Maechler, ETH Zurich



From rahmank at frim.gov.my  Fri Jan 20 01:26:11 2006
From: rahmank at frim.gov.my (Abd Rahman Kassim)
Date: Thu, 19 Jan 2006 16:26:11 -0800
Subject: [R] Legend Outside Plot Dimension
References: <43CF57D6.8628.408BED@localhost>
Message-ID: <003d01c61d58$1d915870$4202a8c0@DrAbRahmandt7>


Dear Peter,

Thanks for your promt response. 

Abd. Rahman 
----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Abd Rahman Kassim" <rahmank at frim.gov.my>; <r-help at stat.math.ethz.ch>
Sent: Thursday, January 19, 2006 12:11 AM
Subject: Re: [R] Legend Outside Plot Dimension


> 
> Hi
> 
> I think you need to use par(xpd=TRUE). Try to search archives as 
> similar question was answered few days ago.
> 
> HTH
> Petr
> 
> 
> On 19 Jan 2006 at 12:19, Abd Rahman Kassim wrote:
> 
> From:           "Abd Rahman Kassim" <rahmank at frim.gov.my>
> To:             <r-help at stat.math.ethz.ch>
> Date sent:      Thu, 19 Jan 2006 12:19:30 -0800
> Subject:        [R] Legend Outside Plot Dimension
> 
>> 
>> Dear All,
>> 
>> I'm trying to attach a legend outside the plot (Inside plot OK), but
>> failed. Any help is very much appreciated.
>> 
>> Thanks.
>> 
>> 
>> Abd. Rahman Kassim, PhD
>> Forest Management & Ecology Program
>> Forestry & Conservation Division
>> Forest Research Institute Malaysia
>> Kepong 52109 Selangor
>> MALAYSIA
>> 
>> *****************************************
>> 
>> Checked by TrendMicro Interscan Messaging Security.
>> For any enquiries, please contact FRIM IT Department.
>> *****************************************
>>  [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 
> 
> *****************************************
> Outgoing mail is certified Virus Free.
> Checked by TrendMicro Interscan Messaging Security.
> For any enquiries, please contact FRIM IT Department.
> *****************************************



From ripley at stats.ox.ac.uk  Thu Jan 19 09:35:10 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 08:35:10 +0000 (GMT)
Subject: [R] Canonical Variance Analysis by any other name?
In-Reply-To: <20060118233623.GR18619@hortresearch.co.nz>
References: <20060118233623.GR18619@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.61.0601190828150.13565@gannet.stats>

Canonical variates or canonical variance?

Canonical variates in the sense of your quote is the same thing as 
multi-group LDA (the part of LDA not due to Fisher)

Another sense (e.g. http://en.wikipedia.org/wiki/Canonical_analysis)
is canonical correlation analysis (function cancor).  And indeed, LDA can 
be derived from CCA: see my PRNN book.

On Thu, 19 Jan 2006, Patrick Connolly wrote:

> I've been asked about "Canonical Variance Analysis" (CVA).  I don't
> see any reference to it searching the R site.  Does it go by other
> names?
>
> Genstat describes it thus:
>
> Canonical variates analysis operates on a within-group sums of squares
> and products matrix, calculated from a set of variates and factor that
> specifies the grouping of units. It finds linear combinations of the
> variates that maximize the ratio of between-group to within-group
> variation, thereby giving functions that can be used to discriminate
> between the groups.
>
> It's probably not particularly difficult to do, so I suspect someone
> has a package for doing it.  What other name might I search for?
>
> Thnx
>
> -- 
> Patrick Connolly
> HortResearch
> Mt Albert
> Auckland
> New Zealand
> Ph: +64-9 815 4200 x 7188
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
> I have the world`s largest collection of seashells. I keep it on all
> the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajayshah at mayin.org  Thu Jan 19 09:35:58 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Thu, 19 Jan 2006 14:05:58 +0530
Subject: [R] Tobit estimation?
Message-ID: <20060119083558.GA25376@lubyanka.local>

Folks,

Based on
  http://www.biostat.wustl.edu/archives/html/s-news/1999-06/msg00125.html

I thought I should experiment with using survreg() to estimate tobit
models.

I start by simulating a data frame with 100 observations from a tobit model

> x1 <- runif(100)
> x2 <- runif(100)*3
> ystar <- 2 + 3*x1 - 4*x2 + rnorm(100)*2
> y <- ystar
> censored <- ystar <= 0
> y[censored] <- 0
> D <- data.frame(y, x1, x2)
> head(D)
          y         x1        x2
1 0.0000000 0.86848630 2.6275703
2 0.0000000 0.88675832 1.7199261
3 2.7559349 0.38341782 0.6247869
4 0.0000000 0.02679007 2.4617981
5 2.2634588 0.96974450 0.4345950
6 0.6563741 0.92623096 2.4983289

> # Estimate it
> library(survival)
> tfit <- survreg(Surv(y, y>0, type='left') ~ x1 + x2,
                  data=D, dist='gaussian', link='identity')

It says:

  Error in survreg.control(...) : unused argument(s) (link ...)
  Execution halted

My competence on library(survival) is zero. Is it still the case that
it's possible to be clever and estimate the tobit model using
library(survival)?

I also saw the two-equation setup in the micEcon library. I haven't
yet understood when I would use that and when I would use a straight
estimation of a censored regression by MLE. Can someone shed light on
that? My situation is: Foreign investment on the Indian stock
market. Lots of firms have zero foreign investment. But many do have
foreign investment. I thought this is a natural tobit situation.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From admin at biostatistic.de  Thu Jan 19 09:57:53 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Thu, 19 Jan 2006 09:57:53 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <43CE2EBC.1020303@biostatistic.de>
References: <43CE2EBC.1020303@biostatistic.de>
Message-ID: <43CF5491.7000203@biostatistic.de>



Knut Krueger schrieb:

>I am trying to send a manuscript to a journal.
>One of the figures build by R is in the right orientation and 4 are rotated clockwise 90 deg in the preview.
>
>  
>
So I realized that they rotated the Paper not the figure. I did not 
realize it because I did not set the Arcrobat reader to a full size view. 
So I saw only the rotaded figure not the hole page.

maybe this is a better hint.

And the hint from the journal:

Please be advised that your eps figure file could have rotated in the built
PDF due to its size which is larger than the paper.

but I think that could not be the problem, because the figure is in the right rotation at the page.



Regards Knut



From knoblauch at lyon.inserm.fr  Thu Jan 19 11:57:14 2006
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 19 Jan 2006 09:57:14 -0100 (CET)
Subject: [R]  Tobit estimation?
Message-ID: <49770.194.57.165.22.1137661034.squirrel@webmail.lyon.inserm.fr>

If you look at ?survreg, you might notice that there is no link argument,
but that
reference is made to survreg.old that does contain a link argument.  Thus,

help(survreg,package=survival)
> x1 <- runif(100)
> x2 <- runif(100)*3
> ystar <- 2 + 3*x1 - 4*x2 + rnorm(100)*2
> y <- ystar
> censored <- ystar <= 0
> y[censored] <- 0
> D <- data.frame(y, x1, x2)
> head(D)
         y        x1        x2
1 0.000000 0.8381902 2.3445745
2 2.834993 0.6603959 0.4572142
3 2.322114 0.6683632 0.2619112
4 0.000000 0.3762860 1.8897217
5 1.977283 0.9713827 0.8079159
6 0.536224 0.8266868 1.1839211
> library(survival)
Loading required package: splines
> tfit <- survreg(Surv(y, y>0, type='left') ~ x1 + x2,
+                   data=D, dist='gaussian', link='identity')
Error in survreg.control(...) : unused argument(s) (link ...)
+
> tfit <- survreg.old(Surv(y, y>0, type='left') ~ x1 + x2,
+                   data=D, dist='gaussian', link='identity')
> tfit
Call:
survreg(formula = formula, data = data, dist = dist, scale = scale)

Coefficients:
(Intercept)          x1          x2
   1.594501    2.825190   -2.985390

Scale= 1.493047

Loglik(model)= -66.1   Loglik(intercept only)= -109.7
	Chisq= 87.28 on 2 degrees of freedom, p= 0
n= 100
>
-- 
Ken Knoblauch
Inserm U371
Cerveau et Vision
Dept. of Cognitive Neuroscience
18 avenue du Doyen L??pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.lyon.inserm.fr/371/



From trittihn at web.de  Thu Jan 19 10:38:18 2006
From: trittihn at web.de (Stefan Semmeling)
Date: Thu, 19 Jan 2006 10:38:18 +0100
Subject: [R] html excel file
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>

hallo,

i have a problem reading in the above mentioned kind of a file.
does anybody know an easy way how to read it in?
i can save it as a text file that looks like:

Datum des Fixings
Restlaufzeit

 
1 Jahr
2 Jahre
3 Jahre
4 Jahre
5 Jahre
6 Jahre
7 Jahre
8 Jahre
9 Jahre
10 Jahre
12 Jahre
15 Jahre
13.01.06
2.819
2.983
3.073
3.137
3.194
3.247
3.302
3.355
3.409
3.459
3.548
3.649
12.01.06
2.847
3.013
3.102
3.164
3.217
3.268
3.322
3.373
3.426
3.475
3.564
3.665
...

after skipping the first four lines i want to read it in while the first 12
lines are the heades
the next is the date followed by the singel values...(these lines are always
repeated)

it is supposed to look like 

          1    2    3    4    5    6    7    8    9    10    12    15
datum1    a    b    c    d    e    f    g    h    i    j	 k      l   
datum2    a    b    c    d    e    f    g    h    i    j     k      l   
...

i tried to read it in as a normal excel file but it didn??t work out the way
it was supposed to

thank you for helping!!!

stefan



From roebuck at mdanderson.org  Thu Jan 19 11:19:01 2006
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Thu, 19 Jan 2006 04:19:01 -0600 (CST)
Subject: [R] R Commenting Style
Message-ID: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>

I seem to remember reading somewhere about some style
guide regarding R the number of comment characters (#)
prior to the comment meaning something.

Anyone know to what I'm referring? Where?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From bioflash at gmail.com  Thu Jan 19 11:36:31 2006
From: bioflash at gmail.com (Vincent Deng)
Date: Thu, 19 Jan 2006 18:36:31 +0800
Subject: [R] packages about microarray analysis
Message-ID: <455343d90601190236l7a5d3b5bh73c75d6d93176d26@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/bfcba691/attachment.pl

From manuel_gutierrez_lopez at yahoo.es  Thu Jan 19 11:42:46 2006
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 19 Jan 2006 11:42:46 +0100 (CET)
Subject: [R] se.fit in predict.nls
Message-ID: <20060119104246.53621.qmail@web26105.mail.ukl.yahoo.com>

Sorry to be so persistent but I really need to get
some measure of the error in the predictions of my nls
model.
I've tried to find out what predict.nls does and I've
got down to 
MiModelo$m$predict
function (newdata = list(), qr = FALSE) 
{
    eval(form[[3]], as.list(newdata), env)
}
<environment: 0x88a076c>
But I can not find what is "form".
Any help, please.
Manuel


Manuel Gutierrez wrote:
> The option se.fit in predict.nls is currently
ignored.
> Is there any other function available to calculate
the
> error in the predictions?
> Thanks,
> Manuel
> 


		
______________________________________________ 
LLama Gratis a cualquier PC del Mundo. 
Llamadas a fijos y m??viles desde 1 c??ntimo por minuto.



From andrej.kastrin at siol.net  Thu Jan 19 11:43:10 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Thu, 19 Jan 2006 11:43:10 +0100
Subject: [R] packages about microarray analysis
In-Reply-To: <455343d90601190236l7a5d3b5bh73c75d6d93176d26@mail.gmail.com>
References: <455343d90601190236l7a5d3b5bh73c75d6d93176d26@mail.gmail.com>
Message-ID: <43CF6D3E.2040206@siol.net>

Vincent Deng wrote:

>Dear R-helpers,
>
>Can anybody suggest me some common packages for standard microarray
>analysis, either from CRAN or Bioconductor?
>
>
>Many thanks...
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
Look at: http://ihome.cuhk.edu.hk/~b400559/arraysoft_rpackages.html

Cheers, Andrej



From renaud.lancelot at gmail.com  Thu Jan 19 11:48:52 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Thu, 19 Jan 2006 11:48:52 +0100
Subject: [R] ICC for Binary data
In-Reply-To: <84C59624B1B0204BBCB3B7DDF981AE63010AD099@GWB-PO.gwb.wustl.edu>
References: <84C59624B1B0204BBCB3B7DDF981AE63010AD099@GWB-PO.gwb.wustl.edu>
Message-ID: <c2ee56800601190248x641eb2b6l@mail.gmail.com>

Brian,

Have a look at package aod on CRAN, and more specifically the
functions "icc", "donner" and "raoscott".

Best,

Renaud

2006/1/18, Brian Perron <beperron at wustl.edu>:
> Hello R users:
>
>
>
> I am fairly new to R and am trying to figure out how to compute an intraclass correlation (ICC) and/or design effect for binary data?  More specifically, I am trying to determine the amount of clustering in a data set - that is, whether certain treatment programs tend to work with more or less severe clients.  The outcome variable is dichotomous (low severity / high severity) and the grouping variable is the treatment program.  Any suggestions for some R code or the appropriate R package would be greatly appreciated.
>
>
>
> Thanks,
>
> Brian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>


--
Renaud LANCELOT
D??partement Elevage et M??decine V??t??rinaire (EMVT) du CIRAD
Directeur adjoint charg?? des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (B??t. B, Bur. 214)
34398 Montpellier Cedex 5 - France
T??l   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From ripley at stats.ox.ac.uk  Thu Jan 19 12:01:21 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 11:01:21 +0000 (GMT)
Subject: [R] se.fit in predict.nls
In-Reply-To: <20060119104246.53621.qmail@web26105.mail.ukl.yahoo.com>
References: <20060119104246.53621.qmail@web26105.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0601191052580.13565@gannet.stats>

I think you first need to understand how you would compute the standard 
errors.  Since the model is non-linear, it is hard to see how this could 
be done accurately.  If the parameters are estimated with negligible error 
compared to the non-linearity (and you can explore that since profiling 
gives you an idea of the variability), you can use local linearization to 
do this. If not, you can simulate.

On Thu, 19 Jan 2006, Manuel Gutierrez wrote:

> Sorry to be so persistent but I really need to get
> some measure of the error in the predictions of my nls
> model.

That is not what se.fit would tell you.  It gives you a measure of the 
variability in the fitted mean curve at one point.  If that is large 
compared to the variability about the true mean curve, you are likely to 
have trouble finding it (and the standard error may not an adequarte 
summary as it is likely to be an asymmetric distribution).

> I've tried to find out what predict.nls does and I've
> got down to
> MiModelo$m$predict
> function (newdata = list(), qr = FALSE)
> {
>    eval(form[[3]], as.list(newdata), env)
> }
> <environment: 0x88a076c>
> But I can not find what is "form".

You really do need to understand all the code, and lexical scoping.

> Any help, please.
> Manuel
>
>
> Manuel Gutierrez wrote:
>> The option se.fit in predict.nls is currently
> ignored.
>> Is there any other function available to calculate
> the
>> error in the predictions?
>> Thanks,
>> Manuel
>>
>
>
>
> ______________________________________________
> LLama Gratis a cualquier PC del Mundo.
> Llamadas a fijos y m?viles desde 1 c?ntimo por minuto.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From sdavis2 at mail.nih.gov  Thu Jan 19 12:16:49 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 19 Jan 2006 06:16:49 -0500
Subject: [R] packages about microarray analysis
In-Reply-To: <455343d90601190236l7a5d3b5bh73c75d6d93176d26@mail.gmail.com>
Message-ID: <BFF4DF51.3FDE%sdavis2@mail.nih.gov>




On 1/19/06 5:36 AM, "Vincent Deng" <bioflash at gmail.com> wrote:

> Dear R-helpers,
> 
> Can anybody suggest me some common packages for standard microarray
> analysis, either from CRAN or Bioconductor?

You did look at the Bioconductor project?  The vast majority of those
packages are for microarray analysis.  If you need more specific help, I
would suggest using the bioconductor mailing list.

Sean



From kevin.thorpe at utoronto.ca  Thu Jan 19 12:26:31 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 19 Jan 2006 06:26:31 -0500
Subject: [R] R Commenting Style
In-Reply-To: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>
Message-ID: <43CF7767.6010206@utoronto.ca>

I don't know if this is what you're referring to, but in ESS the number 
of octothorpes (#) affects the indentation in the emacs buffer. If I 
remember right, ### puts it at the beginning of the line, ## the current 
indentation level (eg. in a function) and # further right.

Paul Roebuck wrote:

>I seem to remember reading somewhere about some style
>guide regarding R the number of comment characters (#)
>prior to the comment meaning something.
>
>Anyone know to what I'm referring? Where?
>


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297



From ded at novonordisk.com  Thu Jan 19 12:35:03 2006
From: ded at novonordisk.com (DED (David George Edwards))
Date: Thu, 19 Jan 2006 12:35:03 +0100
Subject: [R] Minimizing mahalanobis distance to negative orthant
Message-ID: <40D3930AC1C8EA469E39536E5BC808352B9504@EXDKBA021.corp.novocorp.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/5e5037ac/attachment.pl

From dominik.heinzmann at math.unizh.ch  Thu Jan 19 13:01:16 2006
From: dominik.heinzmann at math.unizh.ch (Dominik Heinzmann)
Date: Thu, 19 Jan 2006 13:01:16 +0100
Subject: [R] empirical maximum likelihood estimation
Message-ID: <43CF7F8C.5050100@math.unizh.ch>

Dear R-users

Problem:

Given the following system of ordinary differential euqations

dM/dt = (-n)*M-h*M
dS/dt = n*M-h*S+u*R
dA/dt = h*S-q*A
dI/dt = q*A-p*I
dJ/dt = h*M-v*J
dR/dt=p*I+v*J-u*R

where M,S,A,I,J,R are state variables and n,h,u,q,p,v parameters.

I'm able to calculate the likelihood value based on the solutions 
M,S,A,I,J,R of the ODE's given the data, but without an explicit formula.

How can I now optimize the loglikelihood with respect to the parameter 
n,h,u,q,p,v? Is there any functions available in R for dealing with such 
empirical likelihood problems?

Thanks a lot for your support.

-- 
Dominik Heinzmann
Master of Science in Mathematics, EPFL
Ph.D. student in Biostatistics
Institute of Mathematics
University of Zurich

Winterthurerstrasse 190
CH-8057 Z??rich
Office: Y36L90

E-Mail  : dominik.heinzmann at math.unizh.ch
Phone   : +41-(0)44-635 5858
Fax     : +41-(0)1-63 55705
Homepage: http://www.math.unizh.ch/user/heinzmann



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 19 13:04:00 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 19 Jan 2006 13:04:00 +0100
Subject: [R] Tobit estimation?
In-Reply-To: <20060119083558.GA25376@lubyanka.local>
References: <20060119083558.GA25376@lubyanka.local>
Message-ID: <20060119130400.43bbdf14.Achim.Zeileis@wu-wien.ac.at>

On Thu, 19 Jan 2006 14:05:58 +0530 Ajay Narottam Shah wrote:

> Folks,
> 
> Based on
>   http://www.biostat.wustl.edu/archives/html/s-news/1999-06/msg00125.html
> 
> I thought I should experiment with using survreg() to estimate tobit
> models.

I've been working on a convenience interface to survreg() that makes it
particularly easy to fit tobit models re-using the survreg()
infrastructure. The package containing the code will hopefully be
release soon - anyone who wants a devel snapshot, please contact me
off-list.
Ajay, I'll send you the code in a separate mail.

Best,
Z

> I start by simulating a data frame with 100 observations from a tobit
> model
> 
> > x1 <- runif(100)
> > x2 <- runif(100)*3
> > ystar <- 2 + 3*x1 - 4*x2 + rnorm(100)*2
> > y <- ystar
> > censored <- ystar <= 0
> > y[censored] <- 0
> > D <- data.frame(y, x1, x2)
> > head(D)
>           y         x1        x2
> 1 0.0000000 0.86848630 2.6275703
> 2 0.0000000 0.88675832 1.7199261
> 3 2.7559349 0.38341782 0.6247869
> 4 0.0000000 0.02679007 2.4617981
> 5 2.2634588 0.96974450 0.4345950
> 6 0.6563741 0.92623096 2.4983289
> 
> > # Estimate it
> > library(survival)
> > tfit <- survreg(Surv(y, y>0, type='left') ~ x1 + x2,
>                   data=D, dist='gaussian', link='identity')
> 
> It says:
> 
>   Error in survreg.control(...) : unused argument(s) (link ...)
>   Execution halted
> 
> My competence on library(survival) is zero. Is it still the case that
> it's possible to be clever and estimate the tobit model using
> library(survival)?
> 
> I also saw the two-equation setup in the micEcon library. I haven't
> yet understood when I would use that and when I would use a straight
> estimation of a censored regression by MLE. Can someone shed light on
> that? My situation is: Foreign investment on the Indian stock
> market. Lots of firms have zero foreign investment. But many do have
> foreign investment. I thought this is a natural tobit situation.
> 
> -- 
> Ajay Shah
> http://www.mayin.org/ajayshah
> ajayshah at mayin.org
> http://ajayshahblog.blogspot.com <*(:-? - wizard who doesn't know the
> answer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Jan 19 13:10:26 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 12:10:26 +0000 (GMT)
Subject: [R] Minimizing mahalanobis distance to negative orthant
In-Reply-To: <40D3930AC1C8EA469E39536E5BC808352B9504@EXDKBA021.corp.novocorp.net>
References: <40D3930AC1C8EA469E39536E5BC808352B9504@EXDKBA021.corp.novocorp.net>
Message-ID: <Pine.LNX.4.61.0601191205080.21855@gannet.stats>

On Thu, 19 Jan 2006, DED (David George Edwards) wrote:

> I have the following problem: given x (px1) and S (pXp positive 
> definite), find y such that y_i<=0 (i=1..p) minimizing the mahalanobis 
> distance (x-y)'S^{-1}(x-y).
>
> Has anyone worked on this problem? Tips or R code would be appreciated.

If I read this correctly (some spaces would help) you want

y such that y <= 0 and y minimizes (x-y)'Q(x-y) for a symmetric pos def Q.

That is a quadratic program, so see package quadprog.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Thu Jan 19 13:13:32 2006
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 19 Jan 2006 13:13:32 +0100
Subject: [R] R Commenting Style
In-Reply-To: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>
Message-ID: <43CF826C.1090600@free.fr>

Le 19.01.2006 11:19, Paul Roebuck a ??crit :

>I seem to remember reading somewhere about some style
>guide regarding R the number of comment characters (#)
>prior to the comment meaning something.
>
>Anyone know to what I'm referring? Where?
>  
>
Hi,

If you edit your R code with eclipse + statET plugin 
(http://www.walware.de/goto/statet)
you can associate task tags to your comments, ie :
# TODO : bla bla
# FIXME : replace that for loop by some vectorized incantation
# BUG :
... you can create new task tags according to your taste ...

so that the comments are managed by a separate window. Pretty useful.

Romain


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From ripley at stats.ox.ac.uk  Thu Jan 19 13:38:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 12:38:12 +0000 (GMT)
Subject: [R] empirical maximum likelihood estimation
In-Reply-To: <43CF7F8C.5050100@math.unizh.ch>
References: <43CF7F8C.5050100@math.unizh.ch>
Message-ID: <Pine.LNX.4.61.0601191235280.22198@gannet.stats>

Look at optim, or mle in package stats4.

There are a lot of similar problems addressed in R: few real-world 
likelihoods have `an explicit formula'.  One quite similar example is 
ARIMA fitting.

On Thu, 19 Jan 2006, Dominik Heinzmann wrote:

> Dear R-users
>
> Problem:
>
> Given the following system of ordinary differential euqations
>
> dM/dt = (-n)*M-h*M
> dS/dt = n*M-h*S+u*R
> dA/dt = h*S-q*A
> dI/dt = q*A-p*I
> dJ/dt = h*M-v*J
> dR/dt=p*I+v*J-u*R
>
> where M,S,A,I,J,R are state variables and n,h,u,q,p,v parameters.
>
> I'm able to calculate the likelihood value based on the solutions
> M,S,A,I,J,R of the ODE's given the data, but without an explicit formula.
>
> How can I now optimize the loglikelihood with respect to the parameter
> n,h,u,q,p,v? Is there any functions available in R for dealing with such
> empirical likelihood problems?
>
> Thanks a lot for your support.
>
> -- 
> Dominik Heinzmann
> Master of Science in Mathematics, EPFL
> Ph.D. student in Biostatistics
> Institute of Mathematics
> University of Zurich
>
> Winterthurerstrasse 190
> CH-8057 Z?rich
> Office: Y36L90
>
> E-Mail  : dominik.heinzmann at math.unizh.ch
> Phone   : +41-(0)44-635 5858
> Fax     : +41-(0)1-63 55705
> Homepage: http://www.math.unizh.ch/user/heinzmann
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From henric.nilsson at statisticon.se  Thu Jan 19 13:42:00 2006
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 19 Jan 2006 13:42:00 +0100
Subject: [R] princomp() with missing values in panel data?
In-Reply-To: <50d1c22d0601180556j59748affw62fe1c5df9ccf201@mail.gmail.com>
References: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>	<Pine.LNX.4.61.0601170548040.17319@gannet.stats>
	<50d1c22d0601180556j59748affw62fe1c5df9ccf201@mail.gmail.com>
Message-ID: <43CF8918.4040209@statisticon.se>

ivo welch said the following on 2006-01-18 14:56:

> thank you.  I am still not sure how to get the scores in princomp, though:
> 
> ds= as.data.frame( cbind(rnorm(10),rnorm(10)) )
> names(ds)=c("x1","x2")
> ds[5,]=c(NA,NA)
> pc= princomp( formula = ~ ds$x1 + ds$x2, na.action=na.omit)
> ds$pc1 = pc$scores[,1]  #<-- error, scores has 9 obs, ds has 10 obs
> 
> is there an elegant method to do this, or do I need to learn how to operate

Prof Ripley told you how to do it: `na.action = na.exclude'.

> with pc$loadings?  (may I also humbly suggest that the default behavior or
> $scores should be to contain NA in row 5?)

Choosing sensible defaults in the case of NAs is a tricky business.

Personally, I'd like the default to be `na.fail', so that I don't miss 
out on NAs if I've been sloppy while screening the data. Genrally, just 
ignoring missings and analysing the data as if it were complete may lead 
to seriously biased results.

> Incidentally, R is a lot cleverer than I understand.  pc$loadings by itself

Sometimes R is almost too clever, and I end up feeling humliated when 
finding out that I'm the stupid one... ;-)


HTH,
Henric



> gives me wonderfully intuitive output, with names, text, different
> components---but I can still use p$loadings[,2].  I presume that the array
> operator on the "loadings" object is overloaded.  very nice.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Thu Jan 19 14:09:14 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 19 Jan 2006 14:09:14 +0100
Subject: [R] html excel file
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
Message-ID: <43CF9D8A.17286.150DDBB@localhost>

Hi
your question is a bit cryptic. What file you want to read?

xls or some exported file from Excel?

There is many different ways how to read data into R.

See
read.table, RODBC, R data Import Export Manual....

I personally use

in Excel 
select the data with header and
Ctrl-C

in R

myfile <- read.delim("clipboard")

HTH
Petr





On 19 Jan 2006 at 10:38, Stefan Semmeling wrote:

From:           	"Stefan Semmeling" <trittihn at web.de>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Thu, 19 Jan 2006 10:38:18 +0100
Subject:        	[R] html excel file

> hallo,
> 
> i have a problem reading in the above mentioned kind of a file.
> does anybody know an easy way how to read it in?
> i can save it as a text file that looks like:
> 
> Datum des Fixings
> Restlaufzeit
> 
> 
> 1 Jahr
> 2 Jahre
> 3 Jahre
> 4 Jahre
> 5 Jahre
> 6 Jahre
> 7 Jahre
> 8 Jahre
> 9 Jahre
> 10 Jahre
> 12 Jahre
> 15 Jahre
> 13.01.06
> 2.819
> 2.983
> 3.073
> 3.137
> 3.194
> 3.247
> 3.302
> 3.355
> 3.409
> 3.459
> 3.548
> 3.649
> 12.01.06
> 2.847
> 3.013
> 3.102
> 3.164
> 3.217
> 3.268
> 3.322
> 3.373
> 3.426
> 3.475
> 3.564
> 3.665
> ...
> 
> after skipping the first four lines i want to read it in while the
> first 12 lines are the heades the next is the date followed by the
> singel values...(these lines are always repeated)
> 
> it is supposed to look like 
> 
>           1    2    3    4    5    6    7    8    9    10    12    15
> datum1    a    b    c    d    e    f    g    h    i    j	 k      l  
> datum2    a    b    c    d    e    f    g    h    i    j     k      l 
>  ...
> 
> i tried to read it in as a normal excel file but it didn??t work out
> the way it was supposed to
> 
> thank you for helping!!!
> 
> stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From HStevens at MUOhio.edu  Thu Jan 19 14:13:12 2006
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Thu, 19 Jan 2006 08:13:12 -0500
Subject: [R] Can't load Matrix 0.995-1 for lme4
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614E6A5DA@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614E6A5DA@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <B7639FC6-BD01-466E-B840-BA001BFCF635@MUOhio.edu>

Dear useRs,
I can no longer run lme4 because I think I upgraded lme4 without  
being able to upgrade Matrix.

I am using:
Mac OS 10.4.4
R v. 2.2.1 (updated from 2.2.0 this morning)
lme4 v. 0.995-1

I tried to use the RGUI  to load the package source of Matrix 0.995-1  
and it seemed to be progressing quite well and then it failed  
providing this message:

make: *** [Matrix.so] Error 1
ERROR: compilation failed for package 'Matrix'

I could not find a binary of Matrix >=0.995-1 on any of the URL  
respositoires that I investigated (us,uk,at,au).

I would very grateful for any ideas or assistance.

Cheers,
Hank


Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From roger.bos at gmail.com  Thu Jan 19 14:24:27 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 19 Jan 2006 08:24:27 -0500
Subject: [R] html excel file
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
Message-ID: <1db726800601190524v96539a8kfc8c4d9da6caca2b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/fbb35882/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jan 19 14:36:31 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 13:36:31 +0000 (GMT)
Subject: [R] princomp() with missing values in panel data?
In-Reply-To: <43CF8918.4040209@statisticon.se>
References: <50d1c22d0601161434i6a38ba5aq18673c18a6ea6794@mail.gmail.com>
	<Pine.LNX.4.61.0601170548040.17319@gannet.stats>
	<50d1c22d0601180556j59748affw62fe1c5df9ccf201@mail.gmail.com>
	<43CF8918.4040209@statisticon.se>
Message-ID: <Pine.LNX.4.61.0601191330270.26106@gannet.stats>

On Thu, 19 Jan 2006, Henric Nilsson wrote:

> ivo welch said the following on 2006-01-18 14:56:
>
>> thank you.  I am still not sure how to get the scores in princomp, though:
>>
>> ds= as.data.frame( cbind(rnorm(10),rnorm(10)) )
>> names(ds)=c("x1","x2")
>> ds[5,]=c(NA,NA)
>> pc= princomp( formula = ~ ds$x1 + ds$x2, na.action=na.omit)
>> ds$pc1 = pc$scores[,1]  #<-- error, scores has 9 obs, ds has 10 obs
>>
>> is there an elegant method to do this, or do I need to learn how to operate
>
> Prof Ripley told you how to do it: `na.action = na.exclude'.
>
>> with pc$loadings?  (may I also humbly suggest that the default behavior or
>> $scores should be to contain NA in row 5?)
>
> Choosing sensible defaults in the case of NAs is a tricky business.
>
> Personally, I'd like the default to be `na.fail', so that I don't miss
> out on NAs if I've been sloppy while screening the data. Genrally, just
> ignoring missings and analysing the data as if it were complete may lead
> to seriously biased results.

I tend to agree (and so does S).  You can achieve this with 
options(na.action=na.fail), almost everywhere.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Jan 19 14:45:47 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jan 2006 14:45:47 +0100
Subject: [R] Can't load Matrix 0.995-1 for lme4
In-Reply-To: <B7639FC6-BD01-466E-B840-BA001BFCF635@MUOhio.edu>
References: <8BAEC5E546879B4FAA536200A292C614E6A5DA@AMEDMLNARMC135.amed.ds.army.mil>
	<B7639FC6-BD01-466E-B840-BA001BFCF635@MUOhio.edu>
Message-ID: <x2r774covo.fsf@viggo.kubism.ku.dk>

"Martin Henry H. Stevens" <HStevens at muohio.edu> writes:

> Dear useRs,
> I can no longer run lme4 because I think I upgraded lme4 without  
> being able to upgrade Matrix.
> 
> I am using:
> Mac OS 10.4.4
> R v. 2.2.1 (updated from 2.2.0 this morning)
> lme4 v. 0.995-1
> 
> I tried to use the RGUI  to load the package source of Matrix 0.995-1  
> and it seemed to be progressing quite well and then it failed  
> providing this message:
> 
> make: *** [Matrix.so] Error 1
> ERROR: compilation failed for package 'Matrix'

You're chopping too much of the output there. This gives no
information as to why it failed. Missing tools or compile error?

> I could not find a binary of Matrix >=0.995-1 on any of the URL  
> respositoires that I investigated (us,uk,at,au).

Someone needs to build it... Delays appear to be slightly larger for
Mac than for Windows (which is still 1 revision out of sync with the
sources, though).
 
> I would very grateful for any ideas or assistance.

You *did* read section 5.2 of the MacOSX FAQ, didn't you?

> Cheers,
> Hank
> 
> 
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
> 
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Jan 19 15:00:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jan 2006 15:00:59 +0100
Subject: [R] html excel file
In-Reply-To: <1db726800601190524v96539a8kfc8c4d9da6caca2b@mail.gmail.com>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
	<1db726800601190524v96539a8kfc8c4d9da6caca2b@mail.gmail.com>
Message-ID: <x2mzhsco6c.fsf@viggo.kubism.ku.dk>

roger bos <roger.bos at gmail.com> writes:

> Read ?scan very carefully and play with the settings.  What makes your file
> difficult is that it is multi line, meaning that the headings have carriage
> returns between them instead of being one line separated with spaces or tabs
> or commas.  Can you change the way the file is outputted?  If not, you will
> have to play with scan to get the file in.  Anything is possible, but it
> looks like it will be tricky.

I'd try reading the whole thing as a character vector 

x <- scan(...., what="")

then 

M <- matrix(x[-(1:3)], byrow=TRUE)

cn <- M[1,-1]
rn <- M[-1,1]
n <- M[-1,-1]
mode(n) <- "numeric"
dimnames(n) <- list(rn,cn)
 
and then (possibly) data.frame(n)

[Notice that this is only partially tested, so no guarantees]

> HTH,
> Roger
> 
> 
> 
> On 1/19/06, Stefan Semmeling <trittihn at web.de> wrote:
> >
> > hallo,
> >
> > i have a problem reading in the above mentioned kind of a file.
> > does anybody know an easy way how to read it in?
> > i can save it as a text file that looks like:
> >
> > Datum des Fixings
> > Restlaufzeit
> >
> >
> > 1 Jahr
> > 2 Jahre
> > 3 Jahre
> > 4 Jahre
> > 5 Jahre
> > 6 Jahre
> > 7 Jahre
> > 8 Jahre
> > 9 Jahre
> > 10 Jahre
> > 12 Jahre
> > 15 Jahre
> > 13.01.06
> > 2.819
> > 2.983
> > 3.073
> > 3.137
> > 3.194
> > 3.247
> > 3.302
> > 3.355
> > 3.409
> > 3.459
> > 3.548
> > 3.649
> > 12.01.06
> > 2.847
> > 3.013
> > 3.102
> > 3.164
> > 3.217
> > 3.268
> > 3.322
> > 3.373
> > 3.426
> > 3.475
> > 3.564
> > 3.665
> > ...
> >
> > after skipping the first four lines i want to read it in while the first
> > 12
> > lines are the heades
> > the next is the date followed by the singel values...(these lines are
> > always
> > repeated)
> >
> > it is supposed to look like
> >
> >          1    2    3    4    5    6    7    8    9    10    12    15
> > datum1    a    b    c    d    e    f    g    h    i    j         k      l
> > datum2    a    b    c    d    e    f    g    h    i    j     k      l
> > ...
> >
> > i tried to read it in as a normal excel file but it didn??t work out the
> > way
> > it was supposed to
> >
> > thank you for helping!!!
> >
> > stefan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From portnoy at supereva.it  Thu Jan 19 15:01:58 2006
From: portnoy at supereva.it (portnoy@supereva.it)
Date: Thu, 19 Jan 2006 15:01:58 +0100
Subject: [R]  obtaining ROC curve from Nearest Shrunken Centroids (pamr)
Message-ID: <43CF9BD6.9030604@supereva.it>

Hello,

I have binary labels from a nearest shrunken centroids prediction 
(package pamr). I need to obtain a ROC curve for this test. What is the 
easiest way to obtain one?

Paolo Sonego



From laura at env.leeds.ac.uk  Thu Jan 19 15:06:57 2006
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 19 Jan 2006 14:06:57 +0000 (GMT)
Subject: [R] Creating dipole field
Message-ID: <Pine.LNX.4.44.0601191404080.30620-100000@gw.env.leeds.ac.uk>

Hello,

I'm looking to perform some statistical analysis on some idealised data
cases, and was wondering if someone could point me in the direction of how
to create a simple dipole field within R? I basically require a
rectangular grid with a distinct high/low pressure dipole.

Any pointers appreciated, thank you!

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From rkoenker at uiuc.edu  Thu Jan 19 15:10:41 2006
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 19 Jan 2006 08:10:41 -0600
Subject: [R] Tobit estimation?
In-Reply-To: <20060119130400.43bbdf14.Achim.Zeileis@wu-wien.ac.at>
References: <20060119083558.GA25376@lubyanka.local>
	<20060119130400.43bbdf14.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1D9B0999-26BF-4E70-A160-CC603DE4F7B0@uiuc.edu>

For adventurous, but skeptical souls who lack faith in the usual
Gaussian tobit assumptions, I could mention that there is new
"fcen"  method for the quantreg rq() function that implements
Powell's tobit estimator using an algorithm of Bernd Fitzenberger.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jan 19, 2006, at 6:04 AM, Achim Zeileis wrote:

> On Thu, 19 Jan 2006 14:05:58 +0530 Ajay Narottam Shah wrote:
>
>> Folks,
>>
>> Based on
>>   http://www.biostat.wustl.edu/archives/html/s-news/1999-06/ 
>> msg00125.html
>>
>> I thought I should experiment with using survreg() to estimate tobit
>> models.
>
> I've been working on a convenience interface to survreg() that  
> makes it
> particularly easy to fit tobit models re-using the survreg()
> infrastructure. The package containing the code will hopefully be
> release soon - anyone who wants a devel snapshot, please contact me
> off-list.
> Ajay, I'll send you the code in a separate mail.
>
> Best,
> Z
>
>> I start by simulating a data frame with 100 observations from a tobit
>> model
>>
>>> x1 <- runif(100)
>>> x2 <- runif(100)*3
>>> ystar <- 2 + 3*x1 - 4*x2 + rnorm(100)*2
>>> y <- ystar
>>> censored <- ystar <= 0
>>> y[censored] <- 0
>>> D <- data.frame(y, x1, x2)
>>> head(D)
>>           y         x1        x2
>> 1 0.0000000 0.86848630 2.6275703
>> 2 0.0000000 0.88675832 1.7199261
>> 3 2.7559349 0.38341782 0.6247869
>> 4 0.0000000 0.02679007 2.4617981
>> 5 2.2634588 0.96974450 0.4345950
>> 6 0.6563741 0.92623096 2.4983289
>>
>>> # Estimate it
>>> library(survival)
>>> tfit <- survreg(Surv(y, y>0, type='left') ~ x1 + x2,
>>                   data=D, dist='gaussian', link='identity')
>>
>> It says:
>>
>>   Error in survreg.control(...) : unused argument(s) (link ...)
>>   Execution halted
>>
>> My competence on library(survival) is zero. Is it still the case that
>> it's possible to be clever and estimate the tobit model using
>> library(survival)?
>>
>> I also saw the two-equation setup in the micEcon library. I haven't
>> yet understood when I would use that and when I would use a straight
>> estimation of a censored regression by MLE. Can someone shed light on
>> that? My situation is: Foreign investment on the Indian stock
>> market. Lots of firms have zero foreign investment. But many do have
>> foreign investment. I thought this is a natural tobit situation.
>>
>> -- 
>> Ajay Shah
>> http://www.mayin.org/ajayshah
>> ajayshah at mayin.org
>> http://ajayshahblog.blogspot.com <*(:-? - wizard who doesn't know the
>> answer.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From petr.pikal at precheza.cz  Thu Jan 19 16:06:09 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 19 Jan 2006 16:06:09 +0100
Subject: [R] factor or not factor that is a question
Message-ID: <43CFB8F1.9024.1BBE5B2@localhost>

Dear all

I encountered strange problem with split factor. I tried to use

boxplot(split(aaaa, factor)) but I got an error message

> split(test$asar,kvartaly)
Error in split(x, f) : second argument must be a factor

> str(kvartaly)
 Factor w/ 8 levels "1Q.04","2Q.04",..: 1 1 1 1 1 1 1 1 1 1 ...
> str(test$asar)
 num [1:731] 16.8 16.6 16.6 16.4 16.7 ...
> is.factor(kvartaly)
[1] TRUE
****************************************

I used formula interface
boxplot(formula) which worked OK. 

What can be wrong with my factor kvartaly? Where to look? Why it is 
not accepted by split?

W2000
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    2.1            
year     2005           
month    12             
day      20             
svn rev  36812          
language R              
> 

Best regards.
Petr Pikal
petr.pikal at precheza.cz



From henric.nilsson at statisticon.se  Thu Jan 19 16:30:53 2006
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 19 Jan 2006 16:30:53 +0100
Subject: [R] Plotting an lme( ) object
In-Reply-To: <20060118223727.96381.qmail@web37107.mail.mud.yahoo.com>
References: <20060118223727.96381.qmail@web37107.mail.mud.yahoo.com>
Message-ID: <43CFB0AD.3050706@statisticon.se>

Greg Tarpinian said the following on 2006-01-18 23:37:

> I apologize for the second posting, my other email address
> died today.
> 
> I am using R for Windows, version 2.2.  Here is my code:
> 
>   plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | GROUP, 
>        id = 0.05, adj = -0.3, 
>        idLabels = FOO$value,
>        main = "Pearson Residuals vs. Fitted Values, by Group",
>        between = list(x = .5, y = .5))
> 
> The plot looks fine, but the "adj = -0.3" option seems to have
> no effect on the labels that are added to identify potential
> outliers.  I would like to offset the FOO$value text that is
> currently being displayed right on top of several pearson
> residuals.  How can I do this?

I just checked the code for `plot.lme', and it builds a panel function 
that uses the `adj' argument of the `ltext'. Unfortunately, according to 
the help page for `ltext' (from the Details section),

"... For 'ltext', only values 0, .5 and 1 for 'adj' have any effect."

It wouldn't categorize this as a bug in R, since R's `ltext' function 
behaves according to its documentation. But it sure is irritating.

I've CC:ed Deepayan Sarkar, maintainer of the lattice package, and 
hopefully he's able to advise or, even better, extend `ltext' to accept 
a wider range of `adj' values.


HTH,
Henric





> 
> 
> Thanks much,
> 
>      Greg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccatj at web.de  Thu Jan 19 16:55:37 2006
From: ccatj at web.de (Christian Jones)
Date: Thu, 19 Jan 2006 16:55:37 +0100
Subject: [R] aggregating variables with pca
Message-ID: <465083934@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/e6dffafa/attachment.pl

From zh107 at york.ac.uk  Thu Jan 19 17:02:40 2006
From: zh107 at york.ac.uk (Zhesi He)
Date: Thu, 19 Jan 2006 16:02:40 +0000
Subject: [R] how to select a node in dendrogram/tree
In-Reply-To: <20060118104223.97978.qmail@web37109.mail.mud.yahoo.com>
References: <20060118104223.97978.qmail@web37109.mail.mud.yahoo.com>
Message-ID: <9d748a6d208495de4e26e6d41216fe29@york.ac.uk>

Dear R-users,

Can someone help me select a node/ nodes in a dendrogram or tree, like 
colouring the ones that are selected?

Also, I'm quite interested in any kind of interactive expandable 
dendrogram (like showing different levels of details at a time). I have 
a pretty large dataset and I really want to interact with the graph.

Thanks indeed.

___________________________________________________

Zhesi He
Computational Biology Laboratory, University of York
York YO10 5YW, U.K.
Phone:  +44-(0)1904-328279
Email:  zh107 at york.ac.uk



From maechler at stat.math.ethz.ch  Thu Jan 19 17:20:53 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 19 Jan 2006 17:20:53 +0100
Subject: [R] R Commenting Style
In-Reply-To: <43CF7767.6010206@utoronto.ca>
References: <Pine.OSF.4.58.0601190408150.155040@wotan.mdacc.tmc.edu>
	<43CF7767.6010206@utoronto.ca>
Message-ID: <17359.48229.788976.323025@stat.math.ethz.ch>

>>>>> "Kevin" == Kevin E Thorpe <kevin.thorpe at utoronto.ca>
>>>>>     on Thu, 19 Jan 2006 06:26:31 -0500 writes:

    Kevin> I don't know if this is what you're referring to, but in ESS the number 
    Kevin> of octothorpes (#) affects the indentation in the emacs buffer. If I 
    Kevin> remember right, ### puts it at the beginning of the line, ## the current 
    Kevin> indentation level (eg. in a function) and # further right.

That's correct, thank you, Kevin,
and it's further supported by ESS in emacs,
e.g., if you comment a region (M-;) two  "#" are used
automatically.

There's also a little not well-known function
 M-x ess-fix-comments 
with documentation string
 "Fix ess-mode buffer so that single-line comments start with at least `##'."
and ess-fix-comments is implicitly called by the function
  M-x ess-MM-fix-src
which I use all the time when I get source files from some
people in order to ``clean them'' to my taste...

Note that a current (development) version of the ESS manual is
also only at the ESS website,
specifically,
    http://ess.r-project.org/Manual/ess.html
where the section on indenting (and the comments) is
    http://ess.r-project.org/Manual/ess.html#Indenting

I have exceptionally broken the rule of crossposting, and am
sending this to ESS-help mailing list as well, just to make sure
it'll be seen (and found) there as well.

Martin Maechler, ETH Zurich

    Kevin> Paul Roebuck wrote:

    >> I seem to remember reading somewhere about some style
    >> guide regarding R the number of comment characters (#)
    >> prior to the comment meaning something.
    >> 
    >> Anyone know to what I'm referring? Where?



From danova_fr at hotmail.com  Thu Jan 19 17:32:10 2006
From: danova_fr at hotmail.com (david v)
Date: Thu, 19 Jan 2006 16:32:10 +0000
Subject: [R] help on statistics and code
Message-ID: <BAY108-F3759288919931A39568FCD971C0@phx.gbl>

Hi,
I need some advise on statistics. I have written the code below. What i'm 
trying to do here is that i read 1000 distance matrices derived from the 
same original data set. Afterwards I wan to create a consensus matix out of 
those 1000 distance matrices to see the relevance of the original matrix.
Is that correct ?? should i be doing something else ?

library(stats)
library(ade4)
library(cluster)
library(ape)

nb_runs <- 1000
first_matrix=TRUE

#Reading 1000 distance matrices
for (i in 1:nb_runs)
{
infile="input_matrix"
infile<-paste(infile,i,sep="_")
cat("\nReading ",infile,"")
data<-read.table(infile,header=FALSE,sep="\t",row.names=1)


#
# First matrix we read
if (first_matrix)
{
first_matrix=FALSE
Mdist=dist.binary(data,method=1,diag=TRUE)
}
else {
#the other 999 matrices
dis<-dist.binary(data,method=1,diag=TRUE)
Mdist <- Mdist+dis
}
#Get the consensus matrix based on 1000 matrices
Matallp<-(Mdist/nb_runs )
clust<-hclust(Matall)
...
Is that correct ?? the code works properly.

Thanks for your help



From erich.neuwirth at univie.ac.at  Thu Jan 19 17:37:32 2006
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 19 Jan 2006 17:37:32 +0100 (CET)
Subject: [R] html excel file
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sM
	KAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAhSZy0zcp30KH/pKqSCi9sMKAAAAQAAAAVwgZSuHmn0ajPBXeQq7ZSwEAAAAA@web.de>
Message-ID: <14709.82.218.0.80.1137688652.squirrel@webmail.univie.ac.at>



On Do, 19.01.2006, 10:38, Stefan Semmeling wrote:
> hallo,
>
> i have a problem reading in the above mentioned kind of a file.

One way of transferring data between R and Excel is using the
RExcel package contained in R(D)COM server available from CRAN
in category Other.

The package rcom (available from CRAN) can turn R into a COM server
for Excel and with a little bit of VBA programming it is possible
to transfer data both ways also.



From joshuacgilbert at gmail.com  Thu Jan 19 17:56:41 2006
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Thu, 19 Jan 2006 11:56:41 -0500
Subject: [R]  Using svm.plot with mlbench.spirals.
Message-ID: <ef96daa30601190856h1ef6c178sa54689a976e3af68@mail.gmail.com>

Hi.

I'm trying to plot a pair of intertwined spirals and an svm that
separates them. I'm having some trouble. Here's what I tried.

> library(mlbench)
> library(e1071)
Loading required package: class
> raw <- mlbench.spirals(200,2)
> spiral <- data.frame(class=as.factor(raw$classes), x=raw$x[,1], y=raw$x[,2])
> m <- svm(class~., data=spiral)
> plot(m, spiral)
Error in -x$index : invalid argument to unary operator

So we delve into e1071:::plot.svm. When I run the code in plot.svm
everything is fine up until
 points(formula, data = data[-x$index, ], pch = dataSymbol,
                 col = symbolPalette[colind[-x$index]])
That gives me the same error message, "Error in -x$index : invalid
argument to unary operator". The weird thing is that I can run either
of the those statements in isolation
data[-x$index, ]
symbolPalette[colind[-x$index]]
and neither gives me an error. I looked in the two points functions I
can see (points.default and points.formula) but neither calls x$index.

I was following along the documentation for plot.svm, which has a
simple example (that works)
    ## a simple example
    library(MASS)
    data(cats)
    m <- svm(Sex~., data = cats)
    plot(m, cats)

I don't see what the difference between their example and mine.

Can anyone help me?


Thank you,
Josh.



From maechler at stat.math.ethz.ch  Thu Jan 19 17:59:19 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 19 Jan 2006 17:59:19 +0100
Subject: [R] empirical maximum likelihood estimation
In-Reply-To: <Pine.LNX.4.61.0601191235280.22198@gannet.stats>
References: <43CF7F8C.5050100@math.unizh.ch>
	<Pine.LNX.4.61.0601191235280.22198@gannet.stats>
Message-ID: <17359.50535.325840.55569@stat.math.ethz.ch>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Thu, 19 Jan 2006 12:38:12 +0000 (GMT) writes:

    BDR> Look at optim, or mle in package stats4.
    BDR> There are a lot of similar problems addressed in R: few real-world 
    BDR> likelihoods have `an explicit formula'.  One quite similar example is 
    BDR> ARIMA fitting.

Further note the package   'nlmeODE'
which efficiently addresses a problem very similar to yours.

Martin Maechler, ETH Zurich

    BDR> On Thu, 19 Jan 2006, Dominik Heinzmann wrote:

    >> Dear R-users
    >> 
    >> Problem:
    >> 
    >> Given the following system of ordinary differential euqations
    >> 
    >> dM/dt = (-n)*M-h*M
    >> dS/dt = n*M-h*S+u*R
    >> dA/dt = h*S-q*A
    >> dI/dt = q*A-p*I
    >> dJ/dt = h*M-v*J
    >> dR/dt=p*I+v*J-u*R
    >> 
    >> where M,S,A,I,J,R are state variables and n,h,u,q,p,v parameters.
    >> 
    >> I'm able to calculate the likelihood value based on the solutions
    >> M,S,A,I,J,R of the ODE's given the data, but without an explicit formula.
    >> 
    >> How can I now optimize the loglikelihood with respect to the parameter
    >> n,h,u,q,p,v? Is there any functions available in R for dealing with such
    >> empirical likelihood problems?
    >> 
    >> Thanks a lot for your support.
    >> 
    >> -- 
    >> Dominik Heinzmann
    >> Master of Science in Mathematics, EPFL
    >> Ph.D. student in Biostatistics
    >> Institute of Mathematics
    >> University of Zurich
    >> 
    >> Winterthurerstrasse 190
    >> CH-8057 Z??rich
    >> Office: Y36L90
    >> 
    >> E-Mail  : dominik.heinzmann at math.unizh.ch
    >> Phone   : +41-(0)44-635 5858
    >> Fax     : +41-(0)1-63 55705
    >> Homepage: http://www.math.unizh.ch/user/heinzmann
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> 

    BDR> -- 
    BDR> Brian D. Ripley,                  ripley at stats.ox.ac.uk
    BDR> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
    BDR> University of Oxford,             Tel:  +44 1865 272861 (self)
    BDR> 1 South Parks Road,                     +44 1865 272866 (PA)
    BDR> Oxford OX1 3TG, UK                Fax:  +44 1865 272595______________________________________________
    BDR> R-help at stat.math.ethz.ch mailing list
    BDR> https://stat.ethz.ch/mailman/listinfo/r-help
    BDR> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From martac21 at libero.it  Thu Jan 19 18:07:14 2006
From: martac21 at libero.it (Marta Colombo)
Date: Thu, 19 Jan 2006 18:07:14 +0100
Subject: [R] (no subject)
Message-ID: <ITCNK2$14E3176563AD482FF81389704FC3F746@libero.it>

Good evening,
I am Marta Colombo, student at Milan's Politecnico.
Thank you very much for your kindness, this mailing list is really useful.
 I am using the function kde2d for two-dimensional kernel density estimation and I'd like to know something more about this kind of density estimator. In particular I'd like to know: what bandwidth is used ?
Thank you in advance for your attention

Marta Colombo



From martac21 at libero.it  Thu Jan 19 18:07:33 2006
From: martac21 at libero.it (Marta Colombo)
Date: Thu, 19 Jan 2006 18:07:33 +0100
Subject: [R] function kde2d
Message-ID: <ITCNKL$EC240262E007C8EA0061ABE4E199689C@libero.it>

Good evening,
I am Marta Colombo, student at Milan's Politecnico.
Thank you very much for your kindness, this mailing list is really useful.
 I am using the function kde2d for two-dimensional kernel density estimation and I'd like to know something more about this kind of density estimator. In particular I'd like to know: what bandwidth is used ?
Thank you in advance for your attention

Marta Colombo



From epurdom at stanford.edu  Thu Jan 19 18:42:55 2006
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Thu, 19 Jan 2006 09:42:55 -0800
Subject: [R] Downloads -- possible bug?
Message-ID: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>

Hi,
When I go to the CRAN page to download a new version of R, there are not 
the same versions available depending on which mirror I pick. When I go to 
http://cran.cnr.berkeley.edu/, for example, I get 2.2.1, but if I go 
http://cran.stat.ucla.edu/ the option is 2.2.0 (I'm downloading the Windows 
base file). Neither refreshing nor clearing my cache changes it. For future 
reference, I'd like to know if this is a mistake or if different mirrors 
are just updated at different times?
Thanks,
Elizabeth Purdom



From ripley at stats.ox.ac.uk  Thu Jan 19 18:44:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jan 2006 17:44:34 +0000 (GMT)
Subject: [R] function kde2d
In-Reply-To: <ITCNKL$EC240262E007C8EA0061ABE4E199689C@libero.it>
References: <ITCNKL$EC240262E007C8EA0061ABE4E199689C@libero.it>
Message-ID: <Pine.LNX.4.61.0601191738310.2314@gannet.stats>

Do you mean the one in package MASS?  If so, you need to consult the 
reference on its help page (although the bandwidth is actually given on 
the help page).

It is courteous to credit other people's work in contributed packages.

The author of MASS::kde2d

On Thu, 19 Jan 2006, Marta Colombo wrote:

> Good evening,
> I am Marta Colombo, student at Milan's Politecnico.
> Thank you very much for your kindness, this mailing list is really useful.
> I am using the function kde2d for two-dimensional kernel density
estimation and I'd like to know something more about this kind of
density estimator. In particular I'd like to know: what bandwidth is used ?
> Thank you in advance for your attention
>
> Marta Colombo

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Jan 19 18:55:03 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jan 2006 18:55:03 +0100
Subject: [R] Downloads -- possible bug?
In-Reply-To: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>
Message-ID: <43CFD277.9070001@statistik.uni-dortmund.de>

Elizabeth Purdom wrote:

> Hi,
> When I go to the CRAN page to download a new version of R, there are not 
> the same versions available depending on which mirror I pick. When I go to 
> http://cran.cnr.berkeley.edu/, for example, I get 2.2.1, but if I go 
> http://cran.stat.ucla.edu/ the option is 2.2.0 (I'm downloading the Windows 
> base file). Neither refreshing nor clearing my cache changes it. For future 
> reference, I'd like to know if this is a mistake or if different mirrors 
> are just updated at different times?

Looks like http://cran.stat.ucla.edu/ has not been synced for a couple 
of months, since Kurt Hornik's *daily* check has the date 08-Oct-2005 on 
that mirror. Fritz?

Uwe


> Thanks,
> Elizabeth Purdom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Jan 19 19:00:35 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jan 2006 19:00:35 +0100
Subject: [R] Downloads -- possible bug?
In-Reply-To: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>
Message-ID: <x2slrkayik.fsf@viggo.kubism.ku.dk>

Elizabeth Purdom <epurdom at stanford.edu> writes:

> Hi,
> When I go to the CRAN page to download a new version of R, there are not 
> the same versions available depending on which mirror I pick. When I go to 
> http://cran.cnr.berkeley.edu/, for example, I get 2.2.1, but if I go 
> http://cran.stat.ucla.edu/ the option is 2.2.0 (I'm downloading the Windows 
> base file). Neither refreshing nor clearing my cache changes it. For future 
> reference, I'd like to know if this is a mistake or if different mirrors 
> are just updated at different times?
> Thanks,
> Elizabeth Purdom

It depends on the mirroring sites' policy and maintenance. 

UCLA apparently hasn't updated since October, which is quite extreme.
My guess would be that it is a local bug rather than deliberate
policy.



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ericpante at hotmail.com  Thu Jan 19 19:08:32 2006
From: ericpante at hotmail.com (Eric Pante)
Date: Thu, 19 Jan 2006 13:08:32 -0500
Subject: [R] matrix export
In-Reply-To: <mailman.11.1137668402.11016.r-help@stat.math.ethz.ch>
References: <mailman.11.1137668402.11016.r-help@stat.math.ethz.ch>
Message-ID: <BAY106-DAV5333FA5DE329CD98EB274BC1C0@phx.gbl>

Dear listers,

I need to export a distance matrix in the following format :

a 0.0 1.0 0.2 1.0 1.0
b 1.0 0.0 1.0 1.0 1.0
c 0.2 1.0 0.0 1.0 1.0
d 1.0 1.0 1.0 0.0 1.0
e 1.0 1.0 1.0 1.0 0.0

I tried write.matrix() from the MASS library, which gives:

a b c d e
0.0 1.0 0.2 1.0 1.0
1.0 0.0 1.0 1.0 1.0
0.2 1.0 0.0 1.0 1.0
1.0 1.0 1.0 0.0 1.0
1.0 1.0 1.0 1.0 0.0

Does anyone know of an R trick to change the location of the headers 
?or do I need to do this externally, through perl or bash ?

Thanks in advance for your help,
Eric


Eric Pante
----------------------------------------------------------------
College of Charleston, Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
----------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac



From dj at research.bell-labs.com  Thu Jan 19 19:12:33 2006
From: dj at research.bell-labs.com (David James)
Date: Thu, 19 Jan 2006 13:12:33 -0500
Subject: [R] [R-pkgs] RSQLite
Message-ID: <20060119181232.GB10546@jessie.research.bell-labs.com>

Version 0.4-1 of the RSQLite package has been uploaded to CRAN.
RSQLite embeds the SQLite engine in R (see http://www.sqlite.org)

Changes include:

* Fixed problems exporting/importing NA's 

* An new experimental dbWriteTable() method to create SQLite tables
  from simple files (delimited unquoted fields), e.g., 
     > dbWriteTable(con, "table_name", "file_name", ...)
     > methods?dbWriteTable           ## see documentation for details

* dbConnect() now accepts values for the "cache_size" and "synchronous" 
  to tune/improve SQLite performance (see methods?dbConnect for details).

Thanks to Seth Falcon, Ronggui Huang, and Charles Loboz for their
patches, suggestions, bug reports and testing.

-- 
David

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From I.Szentirmai at rug.nl  Thu Jan 19 19:43:35 2006
From: I.Szentirmai at rug.nl (I.Szentirmai)
Date: Thu, 19 Jan 2006 19:43:35 +0100
Subject: [R] gam
Message-ID: <web-14929175@mail3.rug.nl>

Dear R users,

I'm new to both R and to this list and would like to get 
advice on how to build generalized additive models in R. 
Based on the description of gam, which I found on the R 
website, I specified the following model:
model1<-gam(ST~s(MOWST1),family=binomial,data=strikes.S),
in which ST is my binary response variable and MOWST1 is a 
categorical independent variable.

I get the following error message:
Error in smooth.construct.tp.smooth.spec(object, data, 
knots) :
         NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning messages:
1: argument is not numeric or logical: returning NA in: 
mean.default(xx)
2: - not meaningful for factors in: Ops.factor(xx, 
shift[i])

I would greatly appreciate if someone could tell me what I 
did wrong. Can I use categorical independents in gam at 
all?

Many thanks,
Istvan



From kims9578 at uidaho.edu  Thu Jan 19 19:49:44 2006
From: kims9578 at uidaho.edu (Mark Kimsey)
Date: Thu, 19 Jan 2006 10:49:44 -0800
Subject: [R] Editing Partial Correlation Matrix
Message-ID: <f597a6692de8b.2de8bf597a669@uidaho.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/297576d6/attachment.pl

From ericpante at hotmail.com  Thu Jan 19 20:05:27 2006
From: ericpante at hotmail.com (Eric Pante)
Date: Thu, 19 Jan 2006 14:05:27 -0500
Subject: [R] matrix export
In-Reply-To: <200601191838.k0JIckVH019223@meitner.gene.com>
References: <200601191838.k0JIckVH019223@meitner.gene.com>
Message-ID: <BAY106-DAV17CD550D9EE5F89939B1EDBC1C0@phx.gbl>

Thanks, Bert

write.table won't do the trick, because distance matrices in R are 
presented as lower triangular matrices. write.table "cannot coerce 
class "dist" into a data.frame".

maybe there is a way to transform the lower triangular matrix into a 
full matrix, and then use write.table ?

cheers, Eric

On Jan 19, 2006, at 1:38 PM, Berton Gunter wrote:

> Just use write.table() from the base package with row.names=TRUE (the
> default).
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific 
> learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Pante
>> Sent: Thursday, January 19, 2006 10:09 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] matrix export
>>
>> Dear listers,
>>
>> I need to export a distance matrix in the following format :
>>
>> a 0.0 1.0 0.2 1.0 1.0
>> b 1.0 0.0 1.0 1.0 1.0
>> c 0.2 1.0 0.0 1.0 1.0
>> d 1.0 1.0 1.0 0.0 1.0
>> e 1.0 1.0 1.0 1.0 0.0
>>
>> I tried write.matrix() from the MASS library, which gives:
>>
>> a b c d e
>> 0.0 1.0 0.2 1.0 1.0
>> 1.0 0.0 1.0 1.0 1.0
>> 0.2 1.0 0.0 1.0 1.0
>> 1.0 1.0 1.0 0.0 1.0
>> 1.0 1.0 1.0 1.0 0.0
>>
>> Does anyone know of an R trick to change the location of the headers
>> ?or do I need to do this externally, through perl or bash ?
>>
>> Thanks in advance for your help,
>> Eric
>>
>>
>> Eric Pante
>> ----------------------------------------------------------------
>> College of Charleston, Grice Marine Laboratory
>> 205 Fort Johnson Road, Charleston SC 29412
>> ----------------------------------------------------------------
>>
>> 	"On ne force pas la curiosite, on l'eveille ..."
>> 	Daniel Pennac
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
>
Eric Pante
----------------------------------------------------------------
College of Charleston, Grice Marine Laboratory
205 Fort Johnson Road, Charleston SC 29412
----------------------------------------------------------------

	"On ne force pas la curiosite, on l'eveille ..."
	Daniel Pennac



From sdavis2 at mail.nih.gov  Thu Jan 19 20:09:47 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 19 Jan 2006 14:09:47 -0500
Subject: [R] matrix export
In-Reply-To: <BAY106-DAV17CD550D9EE5F89939B1EDBC1C0@phx.gbl>
Message-ID: <BFF54E2B.40C3%sdavis2@mail.nih.gov>




On 1/19/06 2:05 PM, "Eric Pante" <ericpante at hotmail.com> wrote:

> Thanks, Bert
> 
> write.table won't do the trick, because distance matrices in R are
> presented as lower triangular matrices. write.table "cannot coerce
> class "dist" into a data.frame".
> 
> maybe there is a way to transform the lower triangular matrix into a
> full matrix, and then use write.table ?

Did you try as.matrix()?

>From the dist() help page:

     'as.dist()' is a generic function.  Its default method handles
     objects inheriting from class '"dist"', or coercible to matrices
     using 'as.matrix()'.


Sean



From william.simpson at drdc-rddc.gc.ca  Thu Jan 19 20:44:31 2006
From: william.simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Thu, 19 Jan 2006 14:44:31 -0500
Subject: [R] change fitted line colour in lme() trellis plot?
Message-ID: <1137699872.31308.4.camel@localhost.localdomain>

If I used a groupedData object, if I do 
fit<-lme(blah)
then
plot(augPred(fit))
produces a nice trellis plot of the data along with the fitted lines

However I find that the lines and the data points are in the same colour
(light blue against a medium grey background). Is there a way to make
the lines in a different colour (e.g. black)? It would also be nice if
the line were plotted after the points so it is visible (I have a lot of
points and the line is obscured).

I have looked at par().

Thanks very much for any help.

Bill



From p.connolly at hortresearch.co.nz  Thu Jan 19 22:03:24 2006
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 20 Jan 2006 10:03:24 +1300
Subject: [R] Canonical Variance Analysis by any other name?
In-Reply-To: <Pine.LNX.4.61.0601190828150.13565@gannet.stats>
References: <20060118233623.GR18619@hortresearch.co.nz> 
	<Pine.LNX.4.61.0601190828150.13565@gannet.stats>
Message-ID: <20060119210324.GS18619@hortresearch.co.nz>

On Thu, 19-Jan-2006 at 08:35AM +0000, Prof Brian Ripley wrote:

|> Canonical variates or canonical variance?
|> 

It was canonical variance I was asked about and I wasn't careful
enough getting the Genstat quote to make sure I was referring to the
same thing.  It seems SAS uses the term Canonical Variance Analysis
and the reference mentioned for it is:

Dixon, W. H., and M. B. Brown [eds.]. 1979 Biomedical computer
programs P-series. University of California Press, Los Angeles, CA.

Does anyone know for sure if they are the same thing?  It appears to
me that someone misheard "variates" as "variance", but that might not
be the case.  Perhaps SAS likes to use different terminology.

Thank you Brian for the wikipedia link.  It doesn't have anything
relating to canonical variance analysis which seems to support my
suspicions.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From deepayan.sarkar at gmail.com  Thu Jan 19 22:40:12 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 19 Jan 2006 15:40:12 -0600
Subject: [R] Plotting an lme( ) object
In-Reply-To: <43CFB0AD.3050706@statisticon.se>
References: <20060118223727.96381.qmail@web37107.mail.mud.yahoo.com>
	<43CFB0AD.3050706@statisticon.se>
Message-ID: <eb555e660601191340q2769c7do30cffb94731c17f9@mail.gmail.com>

On 1/19/06, Henric Nilsson <henric.nilsson at statisticon.se> wrote:
> Greg Tarpinian said the following on 2006-01-18 23:37:
>
> > I apologize for the second posting, my other email address
> > died today.
> >
> > I am using R for Windows, version 2.2.  Here is my code:
> >
> >   plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | GROUP,
> >        id = 0.05, adj = -0.3,
> >        idLabels = FOO$value,
> >        main = "Pearson Residuals vs. Fitted Values, by Group",
> >        between = list(x = .5, y = .5))
> >
> > The plot looks fine, but the "adj = -0.3" option seems to have
> > no effect on the labels that are added to identify potential
> > outliers.  I would like to offset the FOO$value text that is
> > currently being displayed right on top of several pearson
> > residuals.  How can I do this?
>
> I just checked the code for `plot.lme', and it builds a panel function
> that uses the `adj' argument of the `ltext'. Unfortunately, according to
> the help page for `ltext' (from the Details section),
>
> "... For 'ltext', only values 0, .5 and 1 for 'adj' have any effect."
>
> It wouldn't categorize this as a bug in R, since R's `ltext' function
> behaves according to its documentation. But it sure is irritating.
>
> I've CC:ed Deepayan Sarkar, maintainer of the lattice package, and
> hopefully he's able to advise or, even better, extend `ltext' to accept
> a wider range of `adj' values.

The grid.text argument 'just' does support numeric values like 'adj'
now, but it didn't when ltext was originally written (or at least I
was under the impression that it didn't). I have updated ltext
accordingly, which in future releases of lattice should support other
values of adj. I wasn't planning any updates for R 2.2.x, but I can if
anyone needs this desperately enough.

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From arnab at myrealbox.com  Thu Jan 19 22:55:12 2006
From: arnab at myrealbox.com (Arnab mukherji)
Date: Thu, 19 Jan 2006 21:55:12 +0000
Subject: [R] Dynamic Programming in R
Message-ID: <1137707712.c7d31adcarnab@myrealbox.com>

Hi R users,

I am looking to numerically solve a dynamic program in the R environment. I was wondering if there were people out there who had expereinced success at using R for such applications. I'd rather continue in R than learn Mathlab.

A concern that has been cited that may discourage R use for solving dynamic programs is its memory handling abilities.  A senior researcher had a lot of trouble with R becuase on any given run it would eat up all the computers memory and need to start using the hard disk. Yet, the memory needed was not substantial - saving the worksapce, exiting and recalling would noticebly start of tthe progam at a much lower memory use, level and a quick deteroration in a few thousand iterations.

Is this a problem other people have come across? Perhaps, its a problem already fixed, since the researcher was working on this in 2002 (he claimed he had tried it on windows, mac, and unix versions to check). 

Thanks.

Arnab



From deepayan.sarkar at gmail.com  Thu Jan 19 23:18:18 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 19 Jan 2006 16:18:18 -0600
Subject: [R] change fitted line colour in lme() trellis plot?
In-Reply-To: <1137699872.31308.4.camel@localhost.localdomain>
References: <1137699872.31308.4.camel@localhost.localdomain>
Message-ID: <eb555e660601191418n6a3df532vd50c44542f28463@mail.gmail.com>

On 1/19/06, Bill Simpson <william.simpson at drdc-rddc.gc.ca> wrote:
> If I used a groupedData object, if I do
> fit<-lme(blah)
> then
> plot(augPred(fit))
> produces a nice trellis plot of the data along with the fitted lines
>
> However I find that the lines and the data points are in the same colour
> (light blue against a medium grey background). Is there a way to make
> the lines in a different colour (e.g. black)?

plot(augPred(fit), col.line = 'black')

> It would also be nice if
> the line were plotted after the points so it is visible (I have a lot of
> points and the line is obscured).

The order is hard-coded in the panel function:

> plot(augPred(fit))$panel
function (x, y, subscripts, groups, ...)
{
    if (grid)
        panel.grid()
    orig <- groups[subscripts] == "original"
    panel.xyplot(x[orig], y[orig], ...)
    panel.xyplot(x[!orig], y[!orig], ..., type = "l")
}

You can change it by supplying your own panel function, e.g.

plot(augPred(fit), col.line = "black",
     panel =
     function (x, y, subscripts, groups, ...) {
         panel.grid()
         orig <- groups[subscripts] == "original"
         panel.xyplot(x[orig], y[orig], ...)
         panel.xyplot(x[!orig], y[!orig], ..., type = "l")
     })

Deepayan



From deepayan.sarkar at gmail.com  Thu Jan 19 23:21:13 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 19 Jan 2006 16:21:13 -0600
Subject: [R] change fitted line colour in lme() trellis plot?
In-Reply-To: <eb555e660601191418n6a3df532vd50c44542f28463@mail.gmail.com>
References: <1137699872.31308.4.camel@localhost.localdomain>
	<eb555e660601191418n6a3df532vd50c44542f28463@mail.gmail.com>
Message-ID: <eb555e660601191421s20c7e44bt237cffbc28a9d663@mail.gmail.com>

On 1/19/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 1/19/06, Bill Simpson <william.simpson at drdc-rddc.gc.ca> wrote:
> > If I used a groupedData object, if I do
> > fit<-lme(blah)
> > then
> > plot(augPred(fit))
> > produces a nice trellis plot of the data along with the fitted lines
> >
> > However I find that the lines and the data points are in the same colour
> > (light blue against a medium grey background). Is there a way to make
> > the lines in a different colour (e.g. black)?
>
> plot(augPred(fit), col.line = 'black')
>
> > It would also be nice if
> > the line were plotted after the points so it is visible (I have a lot of
> > points and the line is obscured).
>
> The order is hard-coded in the panel function:
>
> > plot(augPred(fit))$panel
> function (x, y, subscripts, groups, ...)
> {
>     if (grid)
>         panel.grid()
>     orig <- groups[subscripts] == "original"
>     panel.xyplot(x[orig], y[orig], ...)
>     panel.xyplot(x[!orig], y[!orig], ..., type = "l")
> }

Actually, this is already what you want, so you shouldn't need to do anything.

Deepayan



From gunter.berton at gene.com  Thu Jan 19 23:41:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 19 Jan 2006 14:41:58 -0800
Subject: [R] Dynamic Programming in R
In-Reply-To: <1137707712.c7d31adcarnab@myrealbox.com>
Message-ID: <200601192241.k0JMfwwo011512@faraday.gene.com>

1. I have no experience in dynamic programming (so you may want to ignore
the rest of this message)

2. Your message is vague (to me) -- more specifics about what you want to do
might result in a better answer.

3. R has become **much** better at loop management since 2002

4. Nevertheless, there are undoubtedly still situations where R may require
an unacceptably large amount of memory overhead. Recursion is one, I
believe.

Usually the best advice is: try it and see.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Arnab mukherji
> Sent: Thursday, January 19, 2006 1:55 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Dynamic Programming in R
> 
> Hi R users,
> 
> I am looking to numerically solve a dynamic program in the R 
> environment. I was wondering if there were people out there 
> who had expereinced success at using R for such applications. 
> I'd rather continue in R than learn Mathlab.
> 
> A concern that has been cited that may discourage R use for 
> solving dynamic programs is its memory handling abilities.  A 
> senior researcher had a lot of trouble with R becuase on any 
> given run it would eat up all the computers memory and need 
> to start using the hard disk. Yet, the memory needed was not 
> substantial - saving the worksapce, exiting and recalling 
> would noticebly start of tthe progam at a much lower memory 
> use, level and a quick deteroration in a few thousand iterations.
> 
> Is this a problem other people have come across? Perhaps, its 
> a problem already fixed, since the researcher was working on 
> this in 2002 (he claimed he had tried it on windows, mac, and 
> unix versions to check). 
> 
> Thanks.
> 
> Arnab
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pinard at iro.umontreal.ca  Fri Jan 20 00:54:24 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Thu, 19 Jan 2006 18:54:24 -0500
Subject: [R] Dynamic Programming in R
In-Reply-To: <1137707712.c7d31adcarnab@myrealbox.com>
References: <1137707712.c7d31adcarnab@myrealbox.com>
Message-ID: <20060119235424.GA17735@alcyon.progiciels-bpi.ca>

[Arnab mukherji]

>A concern that has been cited that may discourage R use for solving
>dynamic programs is its memory handling abilities.

For a dynamic programming problem defined over N steps, one usually 
needs a N*N matrix, so problems should be tractable for N being "not too 
big".  In those I studied, CPU time usually was the scarse resource.
As extreme paths were known to be very unlikely, this (and memory as 
well) could be alleviated somehow by limiting the solution search into 
bands (more or less wide) following the diagonal of the solution matrix.  
I also had some success in splitting big problems into a sequence of 
smaller subproblems, and recursively: such approximations are likely not 
acceptable in the general case.

I would guess that most dynamic programming problems have their own 
specific artifacts and speed-up techniques, a universal solution might 
be uneasy.  Who knows (I'm not sure): R might well offer a powerful 
environment for building a dynamic programming framework.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From yen.lin.chia at intel.com  Fri Jan 20 01:42:00 2006
From: yen.lin.chia at intel.com (Chia, Yen Lin)
Date: Thu, 19 Jan 2006 16:42:00 -0800
Subject: [R] Loop through factors without changing to numerics
Message-ID: <E305A4AFB7947540BC487567B5449BA8092C406A@scsmsx402.amr.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060119/726f23b5/attachment.pl

From gerifalte28 at hotmail.com  Fri Jan 20 02:18:53 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 20 Jan 2006 01:18:53 +0000
Subject: [R] Loop through factors without changing to numerics
In-Reply-To: <E305A4AFB7947540BC487567B5449BA8092C406A@scsmsx402.amr.corp.intel.com>
Message-ID: <BAY103-F15D3C78A713CE27E922928A61F0@phx.gbl>

An example would have helped to give you a better answer.  You can use 
characters in the "seq" argument of the for loop.  i.e

x=letters[1:4]
x
[1] "a" "b" "c" "d"

for(i in x) {print(i)}
[1] "a"
[1] "b"
[1] "c"
[1] "d"


Is this what you were looking for?

Francisco


>From: "Chia, Yen Lin" <yen.lin.chia at intel.com>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Loop through factors without changing to numerics
>Date: Thu, 19 Jan 2006 16:42:00 -0800
>
>Hi all,
>
>
>
>If I want to write a for loop to loop through a set of factors, which
>are not coded in 1,2,3, for e.g in character, possible to write the for
>loop without changing  it to categorical variables?  I saw the manual
>mentions for loop will take a list, but I'm not sure how to create a
>list here.  Any input will be appreciated.  Thanks.
>
>
>
>Yen Lin
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Jan 20 02:32:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jan 2006 20:32:33 -0500
Subject: [R] Dynamic Programming in R
In-Reply-To: <1137707712.c7d31adcarnab@myrealbox.com>
References: <1137707712.c7d31adcarnab@myrealbox.com>
Message-ID: <971536df0601191732r7c2322c8y4025b7de31e0ec30@mail.gmail.com>

The lpSolve package can handle integer programming problems.

Here is an example of using dynamic programming in R from
first principles in another setting:

http://tolstoy.newcastle.edu.au/R/help/06/01/18845.html

On 1/19/06, Arnab mukherji <arnab at myrealbox.com> wrote:
> Hi R users,
>
> I am looking to numerically solve a dynamic program in the R environment. I was wondering if there were people out there who had expereinced success at using R for such applications. I'd rather continue in R than learn Mathlab.
>
> A concern that has been cited that may discourage R use for solving dynamic programs is its memory handling abilities.  A senior researcher had a lot of trouble with R becuase on any given run it would eat up all the computers memory and need to start using the hard disk. Yet, the memory needed was not substantial - saving the worksapce, exiting and recalling would noticebly start of tthe progam at a much lower memory use, level and a quick deteroration in a few thousand iterations.
>
> Is this a problem other people have come across? Perhaps, its a problem already fixed, since the researcher was working on this in 2002 (he claimed he had tried it on windows, mac, and unix versions to check).
>
> Thanks.
>
> Arnab
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sasprog474474 at yahoo.com  Fri Jan 20 02:39:29 2006
From: sasprog474474 at yahoo.com (Greg Tarpinian)
Date: Thu, 19 Jan 2006 17:39:29 -0800 (PST)
Subject: [R] Plotting an lme( ) object
In-Reply-To: <eb555e660601191340q2769c7do30cffb94731c17f9@mail.gmail.com>
Message-ID: <20060120013929.35299.qmail@web37104.mail.mud.yahoo.com>

Based on the suggestions, the following yielded 
satisfactory results:


plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | GROUP, 
     id = 0.05, adj = 1, 
     idLabels = paste( round(FOO$VALUE,1), 
                       rep(" ",length(FOO$VALUE)) ),
     main = "Pearson Residuals vs. Fitted Values, by GROUP",
     between = list(x = .5, y = .5))


Using paste() to hardcode an extra "space" provided me with
the necessary graphical offsets.

Regards,

     Greg



--- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 1/19/06, Henric Nilsson <henric.nilsson at statisticon.se> wrote:
> > Greg Tarpinian said the following on 2006-01-18 23:37:
> >
> > > I apologize for the second posting, my other email address
> > > died today.
> > >
> > > I am using R for Windows, version 2.2.  Here is my code:
> > >
> > >   plot(FOO.lme4, resid(.,type = "p") ~ fitted(.) | GROUP,
> > >        id = 0.05, adj = -0.3,
> > >        idLabels = FOO$value,
> > >        main = "Pearson Residuals vs. Fitted Values, by Group",
> > >        between = list(x = .5, y = .5))
> > >
> > > The plot looks fine, but the "adj = -0.3" option seems to have
> > > no effect on the labels that are added to identify potential
> > > outliers.  I would like to offset the FOO$value text that is
> > > currently being displayed right on top of several pearson
> > > residuals.  How can I do this?
> >
> > I just checked the code for `plot.lme', and it builds a panel function
> > that uses the `adj' argument of the `ltext'. Unfortunately, according to
> > the help page for `ltext' (from the Details section),
> >
> > "... For 'ltext', only values 0, .5 and 1 for 'adj' have any effect."
> >
> > It wouldn't categorize this as a bug in R, since R's `ltext' function
> > behaves according to its documentation. But it sure is irritating.
> >
> > I've CC:ed Deepayan Sarkar, maintainer of the lattice package, and
> > hopefully he's able to advise or, even better, extend `ltext' to accept
> > a wider range of `adj' values.
> 
> The grid.text argument 'just' does support numeric values like 'adj'
> now, but it didn't when ltext was originally written (or at least I
> was under the impression that it didn't). I have updated ltext
> accordingly, which in future releases of lattice should support other
> values of adj. I wasn't planning any updates for R 2.2.x, but I can if
> anyone needs this desperately enough.
> 
> Deepayan
> --
> http://www.stat.wisc.edu/~deepayan/
>



From steffen.katzner at mail.gwdg.de  Fri Jan 20 09:04:22 2006
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Fri, 20 Jan 2006 09:04:22 +0100
Subject: [R] Within-Subjects ANOVA & comparisons of individual means
Message-ID: <43D09986.4070604@mail.gwdg.de>

I am having problems with comparing individual means in a
within-subjects ANOVA. From my understanding, TukeyHSD is not
appropriate in this context. So I am trying to compute contrasts, as
follows:

seven subjects participated in each of 6 conditions (intervals).

> subject = factor(rep(c(1:7), each = 6))
> interval = factor(rep(c(1:6), 7))

and here is the dependent variable:

> dv = c(3.3868, 3.1068, 1.7261, 1.5415, 1.7356, 0.7538,
+ 2.5957, 1.5666, 1.1984, 1.2761, 1.0022, 0.8597,
+ 3.9819, 3.1506, 1.5824, 1.7400, 1.4248, 0.6519,
+ 2.2521, 1.5248, 1.1209, 1.2193, 1.1994, 2.0910,
+ 2.4661, 1.3863, 1.3591, 0.9163, 1.3976, 1.7471,
+ 3.2486, 1.9492, 2.4228, 1.1276, 1.2836, 0.9814,
+ 1.7148, 1.7278, 2.7433, 1.4924, 1.0992, 0.7821)

> d = data.frame(subject, interval, dv)

next I'm defining a contrast matrix:

> con = matrix(c(1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 
0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1), nrow=6, ncol=5, byrow=F)

> con
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]   -1    1    0    0    0
[3,]    0   -1    1    0    0
[4,]    0    0   -1    1    0
[5,]    0    0    0   -1    1
[6,]    0    0    0    0   -1


> contrasts(d$interval)=con

and then I'm doing the ANOVA

> aovRes = aov(dv~interval+Error(subject/interval), data=d)

> summary(aovRes)

Error: subject
           Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  6 2.48531 0.41422

Error: subject:interval
           Df  Sum Sq Mean Sq F value    Pr(>F)
interval   5 13.8174  2.7635  8.7178 3.417e-05 ***
Residuals 30  9.5098  0.3170
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



but if I want to look at the contrasts, something has gone wrong:


summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs
i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))

Error in 1:object$rank : NA/NaN argument

> aovRes$contrasts
NULL

Can anybody help?
Thank you very much,  -Steffen



From alfonso at slafuente.es  Fri Jan 20 09:40:20 2006
From: alfonso at slafuente.es (Alfonso M Sanchez-Lafuente)
Date: Fri, 20 Jan 2006 09:40:20 +0100
Subject: [R] Type III SS in generalized linear mixed models
Message-ID: <43D0A1F4.2060501@slafuente.es>

Good morning everybody,

I would like to get some advice on the following issue:

I am analyzing a dataset obtained from a study on plant visitation 
likelihood related to experimental herbivory treatments and composition 
and abundance of the pollinator assamblage.

The experimental set up was as follows:

In a number of plants (N=73), I replicated a herbivory treatment with 3 
levels and a control. I wanted to know if the experimental treatment is 
responsible for differences in flower visitation likelihood, in 
pollinator attraction and in the interaction bettween these variables in 
3 different populations. All treatments were replicated within plants.

Summarizing:

Treatment effect: 3 levels + 1 control
Pollinator effect: 4 levels (i.e., 4 pollinator groups)
Site effect: 3 levels (i.e, 3 populations)

The response variable is binomial: Flower visited vs. Flower not visited 
(data obtained from 852 individual censuses).

The model selected was:

glmmPQL(Visited ~ Treatm*Pollinator*Site, random=~1|PlantID, 
family=binomial)

I want to test the effect of each factor independently of the other 
factors first. I guess this is Type III SS.
In other words, I do not want to test the effect of Pollinator once I 
have accounted for the effect of Treatment, because the relative 
abundance of different pollinators in a given site and season is not 
dependent on the treatment applied. However, I can only obtain Tipe I SS 
with this model, and the Anova function in package car does not accept 
this type of model with random grouping factors... Any suggestions to 
analyze this type of models to get Type III SS in R ?

-- 

----------------------------------------------
Alfonso M. Sanchez-Lafuente
Departamento de Biologia Vegetal y Ecologia
Facultad de Biologia
Universidad de Sevilla
Avd. Reina Mercedes 9
E-41012, Sevilla, Spain
email: alfonso at slafuente.es / slafuente at us.es



From oehl_list at gmx.de  Fri Jan 20 11:15:26 2006
From: oehl_list at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Fri, 20 Jan 2006 11:15:26 +0100 (MET)
Subject: [R] Dynamic Programming in R
Message-ID: <20834.1137752126@www011.gmx.net>

Gunter,

> there are undoubtedly still situations where R may require an unacceptably
large amount of memory overhead. Recursion is one, I
believe.

One can avoid unacceptably large amount of memory overhead when doing
recursion in R: either by passing parameters "by reference" using package
ref or by directly using environments.

Best regards


Jens Oehlschl??gel

--



From ernesto.adorio at gmail.com  Fri Jan 20 11:21:40 2006
From: ernesto.adorio at gmail.com (Ernesto Adorio)
Date: Fri, 20 Jan 2006 18:21:40 +0800
Subject: [R] Passing variable arguments to functions
Message-ID: <5cec18c10601200221r3a78a3c9le3628300dafc8bef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/7364cafa/attachment.pl

From vmuggeo at dssm.unipa.it  Fri Jan 20 11:29:09 2006
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Fri, 20 Jan 2006 11:29:09 +0100
Subject: [R] Breakpoints for multiple variables using Segmented
In-Reply-To: <200601182037.k0IKbmdB005883@mailserv.unb.ca>
References: <200601182037.k0IKbmdB005883@mailserv.unb.ca>
Message-ID: <43D0BB75.40708@dssm.unipa.it>

Dear Matthew,
Currently segmented() performs multiple (say L>1) estimation of 
breakpoints in GLM, namely:

1)L breakpoints for the same variable x, e.g.:
segmented(obj.glm, Z=x, psi=c(psi1,psi2,psi3))

2)L breakpoints for L explanatory `segmented' variables, e.g.:
segmented(obj.glm, Z=cbind(x1,x2,x3), psi=c(psi1,psi2,psi3))
In this case it is assumed that each x has just one breakpoint (psi1 for 
x1, psi2 for x2, and so on). This is, actually , a limit - I know. 
Currently I have no time :-(, but sooner or later I should update the 
package..

Hope this helps,
vito



Matthew Betts wrote:
> Hi all,
> 
> I am using the package Segmented to estimate logistic regression models
> with unknown breakpoints (see Muggeo 2003 Statistics in Medicine
> 22:3055-3071). In the documentation it suggests that it might be possible to
> include several variables with breakpoints in the same model: Z = a vector
> or a matrix meaning the (continuous) explanatory variable(s) having
> segmented relationships with the response. However, the syntax for
> including multiple Z and psi (starting values for the break-point(s))
> is not stated. Does anyone have any suggestions?
> 
> Here is an example of correct code for detecting single breakpoint:
> 
> model.seg<-segmented.glm(obj = model.glm, Z = predictor_variable, psi = 2 ,
> it.max = 50)
> 
> Thanks very much for your help.
> 
> 
> Matthew G. Betts, Ph.D.
> NB Cooperative Fish and Wildlife Research Unit
> Faculty of Forestry and Environmental Management
> University of New Brunswick 
> UNB Tweedale Centre
> Hugh John Flemming Forestry Complex
> 1350 Regent St., Fredericton, N.B.
> E3C 2G6
> (506) 447-3408
> http://www.unb.ca/web/acwern/people/mbetts/mbetts.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From ripley at stats.ox.ac.uk  Fri Jan 20 11:43:04 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jan 2006 10:43:04 +0000 (GMT)
Subject: [R] Passing variable arguments to functions
In-Reply-To: <5cec18c10601200221r3a78a3c9le3628300dafc8bef@mail.gmail.com>
References: <5cec18c10601200221r3a78a3c9le3628300dafc8bef@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601201039310.9201@gannet.stats>

?do.call is a good place to start.  If parms is a vector, then

y <- do.call(func, as.list(parms))

should do the job.  (And if parms is a list, your code probably does not 
do what you want.)

For such ideas, see `S Programming' (referenced in the FAQ).


On Fri, 20 Jan 2006, Ernesto Adorio wrote:

> Hi,
>
> Is there another  way to pass arguments via a vector to arbitrary functions
> as in the following code example without using a series of if else
> statements?
>
> f <- test(func, x, parms, fargs1, fargs2, ...)
> {
>   # parms is a vector of parameters to func.
>   #  ... is for use by f, not by func.
>   n <- length(parms)
>   if (n == 0)         y <- func(x)
>  else if (n == 1)   y <- func(x, parms[1])
>  else if (n == 2)   y <- func(x, parms[1], parms[2])
>
> # remaining body of f follows.
> }
>
> Regards,
>
> Ernie
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Fri Jan 20 11:55:22 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 20 Jan 2006 10:55:22 +0000
Subject: [R] [Rd] Display an Image on a Plane
In-Reply-To: <loom.20060119T215128-758@post.gmane.org>
References: <85F883EB8B41D61181C80002A541DB960627544F@valcartierex.drdc-rddc.gc.ca>
	<loom.20060119T215128-758@post.gmane.org>
Message-ID: <43D0C19A.5010806@lancaster.ac.uk>

[more an R-help than R-dev thing]

Ben Bolker wrote:
> Labbe, Vincent (AEREX <Vincent.Labbe.AEREX <at> drdc-rddc.gc.ca> writes:

>>I am new to R and I would like to display an image on a plane in a 3D plot,
>>i.e. I would like to be able to specify a theta and a phi parameters like in
>>the function persp to display a 2D image on an inclined plane.

>    can't think of an easy way to do this: what do you mean by "image"
> exactly?  A bitmapped image from a file?  Or something like the
> output of image()?  If the latter, you may be able to cobble together something
> using the trans3d() function (i.e., manually recreating an image()
> by drawing colored squares, but transforming each of the to the 3D
> perspective).  If the former, you might be able to do something with
> the pixmap package ...
> 

  I think once you get into doing fancy visualisations like this then 
you may find a solution outside of R. The front runners are probably:

http://www.opendx.org/

http://mayavi.sourceforge.net/

http://www.llnl.gov/visit/

  Not sure of the R-integration possibilities here, but you'd probably 
have to export your data to a file and pick it up in the visualisation 
package. You'd also have to build your complete plot from scratch in the 
package if its really just an R persp() you want to add to. Most of 
these packages can do persp()-style plots without even *ahem* persp-iring.

  Some of these packages are scriptable from Python or Tcl, which could 
make for tighter integration with R. A full R 3d graphics device would 
be nice... Rgl could be there already! - 
http://rgl.neoscientists.org/Gallery.html

Barry



From henric.nilsson at statisticon.se  Fri Jan 20 12:06:07 2006
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Fri, 20 Jan 2006 12:06:07 +0100
Subject: [R] gam
In-Reply-To: <web-14929175@mail3.rug.nl>
References: <web-14929175@mail3.rug.nl>
Message-ID: <43D0C41F.6000901@statisticon.se>

I.Szentirmai said the following on 2006-01-19 19:43:

 > Dear R users,
 >
 > I'm new to both R and to this list and would like to get
 > advice on how to build generalized additive models in R.
 > Based on the description of gam, which I found on the R

Which `gam'? Note that R ships with package `mgcv' which has a `gam' 
function, but also package `gam' on CRAN has a `gam' function. 
(Furthermore, several other packages exists with functions that I'd 
categorize as GAM-fitters, e.g. SemiPar, assist, gss, gamlss, ...)

 > website, I specified the following model:
 > model1<-gam(ST~s(MOWST1),family=binomial,data=strikes.S),
 > in which ST is my binary response variable and MOWST1 is a
 > categorical independent variable.
 >
 > I get the following error message:
 > Error in smooth.construct.tp.smooth.spec(object, data,

 From this error message, I can however deduce that we're talking about 
the `mgcv::gam' function.

 > knots) :
 >          NA/NaN/Inf in foreign function call (arg 1)
 > In addition: Warning messages:
 > 1: argument is not numeric or logical: returning NA in:
 > mean.default(xx)
 > 2: - not meaningful for factors in: Ops.factor(xx,
 > shift[i])
 >
 > I would greatly appreciate if someone could tell me what I
 > did wrong. Can I use categorical independents in gam at
 > all?

It's not clear to me what you mean by this. Yes, you can use factors in gam:

gam(ST ~ MOWST1, family = binomial, data = strikes.S)

would work. But you tried smoothing a factor, which isn't supported (and 
to me it doesn't make any sense doing so).

Smoothing an ordered factor may make sense, but this is not supported 
(and you didn't try it, according to the error message above) by `mgcv'. 
  I was under the impression that the `gam' function in package `gam' 
should be able to do this, but I just tried it and was rewarded by the 
error message

"Error: 'codes' is defunct."

relating to the internals of `gam' using a defunct R function -- I've 
e-mailed Prof Hastie, maintainer of package `gam', about this.

Even if it worked, the `gam' package won't allow estimation of the 
degree of smoothness of the model terms as part of the fitting process. 
So if this is what you want in combination with ordered factors, you're 
probably out of luck. (You can always send Prof Wood, `mgcv' maintainer, 
a feature request.)


HTH,
Henric



From christian.bieli at unibas.ch  Fri Jan 20 12:14:00 2006
From: christian.bieli at unibas.ch (Christian Bieli)
Date: Fri, 20 Jan 2006 12:14:00 +0100
Subject: [R] assign object to list
Message-ID: <43D0C5F8.8040100@unibas.ch>

Dear all

I want to generate a list like this:

a <- data.frame(1:10)
attr(a,'myattribute') <- 'something'
b <- data.frame(11:20)
attr(b,'myattribute') <- 'anything'
mylist <- list(a,b)

Is there a way to place the dataframes into the list giving them the 
attribute at the same time?
I don't want to create all the dataframes in my workspace first. I tried 
it with parentheses {}, but it obviously did not work.

Thanks in advance.
Christian

-- 
Christian Bieli, project assistant
Institute of Social and Preventive Medicine
University of Basel, Switzerland
Steinengraben 49
CH-4051 Basel
Tel.: +41 61 270 22 12
Fax:  +41 61 270 22 25
christian.bieli at unibas.ch
www.unibas.ch/ispmbs



From Arne.Muller at sanofi-aventis.com  Fri Jan 20 12:19:21 2006
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Fri, 20 Jan 2006 12:19:21 +0100
Subject: [R] Dynamic Programming in R
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF4AC@CRBSMXSUSR04>

Hello,

I've implemented dynamic programming for aligning spectral data (usually 100 to 200 peaks in one spectrum, but some spectra contain > 5k peaks) entirely in R. As Fran??ois Pinard pointed out, the memory usage should be proportional to the n x n dynamic programming matrix, and I've not yet had any problems on my machine (R2.2.0 win2k, 1GB mem, 2GHz Intel PV), CPU seems to be the more problematic issue. 

I guess it all depends on how much data you have. You could split the dynamic programming matrix into chunks and calculate them in parallel on different machines (but the implementatino of finding the optiomal trace will probably get a bit difficult).

	kind regards,

	Arne

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Arnab mukherji
Sent: Thursday, January 19, 2006 22:55
To: r-help at stat.math.ethz.ch
Subject: [R] Dynamic Programming in R


Hi R users,

I am looking to numerically solve a dynamic program in the R environment. I was wondering if there were people out there who had expereinced success at using R for such applications. I'd rather continue in R than learn Mathlab.

A concern that has been cited that may discourage R use for solving dynamic programs is its memory handling abilities.  A senior researcher had a lot of trouble with R becuase on any given run it would eat up all the computers memory and need to start using the hard disk. Yet, the memory needed was not substantial - saving the worksapce, exiting and recalling would noticebly start of tthe progam at a much lower memory use, level and a quick deteroration in a few thousand iterations.

Is this a problem other people have come across? Perhaps, its a problem already fixed, since the researcher was working on this in 2002 (he claimed he had tried it on windows, mac, and unix versions to check). 

Thanks.

Arnab

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jan 20 13:10:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jan 2006 12:10:07 +0000 (GMT)
Subject: [R] assign object to list
In-Reply-To: <43D0C5F8.8040100@unibas.ch>
References: <43D0C5F8.8040100@unibas.ch>
Message-ID: <Pine.LNX.4.61.0601201207310.16612@gannet.stats>

On Fri, 20 Jan 2006, Christian Bieli wrote:

> Dear all
>
> I want to generate a list like this:
>
> a <- data.frame(1:10)
> attr(a,'myattribute') <- 'something'
> b <- data.frame(11:20)
> attr(b,'myattribute') <- 'anything'
> mylist <- list(a,b)
>
> Is there a way to place the dataframes into the list giving them the
> attribute at the same time?

Yes, the above worked.

> lapply(mylist, attributes)
> lapply(mylist, function(x) attr(x, "myattribute"))

show you they are there.

What may have confused you is that the print method for data frames does 
not show arbitrary attributes.

> I don't want to create all the dataframes in my workspace first. I tried
> it with parentheses {}, but it obviously did not work.
>
> Thanks in advance.
> Christian

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jacques.veslot at cirad.fr  Fri Jan 20 13:13:09 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 20 Jan 2006 16:13:09 +0400
Subject: [R] assign object to list
In-Reply-To: <43D0C5F8.8040100@unibas.ch>
References: <43D0C5F8.8040100@unibas.ch>
Message-ID: <43D0D3D5.5050903@cirad.fr>

if you have a list of data frames without the attribute and a 
vector/list of the values for this attributes:

mapply(function(x,y) {attr(x,"names") <- y ; x},
    list(data.frame(1:10), data.frame(11:20)), c("something", 
"anything"), SIMPLIFY=F)


Christian Bieli a ??crit :

>Dear all
>
>I want to generate a list like this:
>
>a <- data.frame(1:10)
>attr(a,'myattribute') <- 'something'
>b <- data.frame(11:20)
>attr(b,'myattribute') <- 'anything'
>mylist <- list(a,b)
>
>Is there a way to place the dataframes into the list giving them the 
>attribute at the same time?
>I don't want to create all the dataframes in my workspace first. I tried 
>it with parentheses {}, but it obviously did not work.
>
>Thanks in advance.
>Christian
>
>  
>



From william.simpson at drdc-rddc.gc.ca  Fri Jan 20 13:46:02 2006
From: william.simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Fri, 20 Jan 2006 07:46:02 -0500
Subject: [R] change fitted line colour in lme() trellis plot?
In-Reply-To: <eb555e660601191421s20c7e44bt237cffbc28a9d663@mail.gmail.com>
References: <1137699872.31308.4.camel@localhost.localdomain>
	<eb555e660601191418n6a3df532vd50c44542f28463@mail.gmail.com>
	<eb555e660601191421s20c7e44bt237cffbc28a9d663@mail.gmail.com>
Message-ID: <1137761162.32500.3.camel@localhost.localdomain>

On Thu, 2006-01-19 at 16:21 -0600, Deepayan Sarkar wrote:
> On 1/19/06, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > On 1/19/06, Bill Simpson <william.simpson at drdc-rddc.gc.ca> wrote:
> > > If I used a groupedData object, if I do
> > > fit<-lme(blah)
> > > then
> > > plot(augPred(fit))
> > > produces a nice trellis plot of the data along with the fitted lines
> > >
> > > However I find that the lines and the data points are in the same colour
> > > (light blue against a medium grey background). Is there a way to make
> > > the lines in a different colour (e.g. black)?
> >
> > plot(augPred(fit), col.line = 'black')
> >
> > > It would also be nice if
> > > the line were plotted after the points so it is visible (I have a lot of
> > > points and the line is obscured).
> >
> > The order is hard-coded in the panel function:
> >
> > > plot(augPred(fit))$panel
> > function (x, y, subscripts, groups, ...)
> > {
> >     if (grid)
> >         panel.grid()
> >     orig <- groups[subscripts] == "original"
> >     panel.xyplot(x[orig], y[orig], ...)
> >     panel.xyplot(x[!orig], y[!orig], ..., type = "l")
> > }
> 
> Actually, this is already what you want, so you shouldn't need to do anything.
Thanks very much Deepayan for your helpful replies.
The line in the plot is exactly how I want now (black and on top of the
points [by default, as you say -- I just couldn't see it because it was
same the colour as the points before])

Best wishes
Bill



From ripley at stats.ox.ac.uk  Fri Jan 20 13:58:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jan 2006 12:58:34 +0000 (GMT)
Subject: [R] assign object to list
In-Reply-To: <Pine.LNX.4.61.0601201207310.16612@gannet.stats>
References: <43D0C5F8.8040100@unibas.ch>
	<Pine.LNX.4.61.0601201207310.16612@gannet.stats>
Message-ID: <Pine.LNX.4.61.0601201256340.17045@gannet.stats>

On Fri, 20 Jan 2006, Prof Brian Ripley wrote:

> On Fri, 20 Jan 2006, Christian Bieli wrote:
>
>> Dear all
>>
>> I want to generate a list like this:
>>
>> a <- data.frame(1:10)
>> attr(a,'myattribute') <- 'something'
>> b <- data.frame(11:20)
>> attr(b,'myattribute') <- 'anything'
>> mylist <- list(a,b)
>>
>> Is there a way to place the dataframes into the list giving them the
>> attribute at the same time?
>
> Yes, the above worked.
>
>> lapply(mylist, attributes)
>> lapply(mylist, function(x) attr(x, "myattribute"))
>
> show you they are there.

Or perhaps you were looking for

list(structure(a, myattribute='something'),
      structure(b, myattribute='anything'))

?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From h.wickham at gmail.com  Fri Jan 20 15:06:43 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 20 Jan 2006 08:06:43 -0600
Subject: [R] Dynamic Programming in R
In-Reply-To: <20834.1137752126@www011.gmx.net>
References: <20834.1137752126@www011.gmx.net>
Message-ID: <f8e6ff050601200606g63d807e3h3fc8e1bc8abb8a74@mail.gmail.com>

> > there are undoubtedly still situations where R may require an unacceptably
> large amount of memory overhead. Recursion is one, I
> believe.
>
> One can avoid unacceptably large amount of memory overhead when doing
> recursion in R: either by passing parameters "by reference" using package
> ref or by directly using environments.

Or by unrolling tail recursion into a loop, of course.

Hadley



From gwgilc at wm.edu  Fri Jan 20 15:35:01 2006
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Fri, 20 Jan 2006 09:35:01 -0500
Subject: [R] PCA: eigen/princomp vs. svd/prcomp
Message-ID: <91436B7F-3840-414B-8FEE-A5885E08BC82@wm.edu>

I am using R 2.2.1 on OS X 10.4.4. I have a question that is partly  
about R but also about some differences in the loadings when doing  
principal components using eigen()/princomp() versus prcomp() . Here  
is the story:

I have a matrix of mean monthly temperatures for 26 sites in the  
northern and southern hemispheres (26 x 12). I am using PCA to reduce  
this to one or two variables that capture most of the annual  
temperature variation among these sites. I am particularly interested  
in a single vector that captures the overall annual differences among  
sites. The southern hemisphere sites are 6 months out of phase with  
the northern, in terms of seasons. So the first question is whether  
or not to rotate the southern hemisphere data so that Jan=July,  
Feb=Aug, etc. before PCA. The second question is whether or not to  
center and scale the data. My gut feeling is, no, as these are all  
temperatures and the differences in means and variances among months  
are important.

If I do PCA using eigen()/princomp() on the unrotated, unscaled, and  
uncentered data, the first PC explains about 60% of the variation and  
represents the difference in phase between the southern and northern  
hemispheres. The second PC represents mean temperature and explains  
about 35% of the variation.

If I use prcomp() on the unrotated, unscaled, and uncentered data,  
the first PC represents mean temperature and explains >90% of the  
variation, the second represents the seasonal phase difference and  
explains less than 5% of the variation. This surprised me, as  
intuitively I had expected the seasonal phase difference to fall out  
first, as it did using eigen(). If anyone has an explanation for  
this, I would love to hear it.

If I center the data, the two methods yield nearly identical results,  
with the first PC capturing the seasonal phase difference and the  
second the mean, explaining 60% and 30% of the variances  
respectively. My intuition (which often is wrong...) says that this  
is not the right way to do things in this case.

I love the result from prcomp() using the uncentered, unscaled data,  
but the loadings are so different from the eigenvectors. I am  
suspicious that something funky is going on here. Does not centering  
the data cause a problem with the math? I would appreciate any comments.

If I rotate the southern hemisphere data six months out of phase,  
then the first PC by either method represents mean temperature and  
the second captures the seasonal difference but again separates the  
northern and southern hemispheres. The variance explained by the  
first PC is about 75% using eigen() and 97% using prcomp(). On one  
hand, this seems like a sensible approach, however it is pretty  
manipulative of the data. March in Santiago probably is NOT the same  
as September in San Francisco, as is reflected in the second PC. But  
again the two methods yield very different amounts of variance  
explained. Why?

Any thoughts would be very much appreciated!

cheers, George

..................................................................
George W. Gilchrist                        Email #1: gwgilc at wm.edu
Department of Biology, Box 8795          Email #2: kitesci at cox.net
College of William & Mary                    Phone: (757) 221-7751
Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
http://gwgilc.people.wm.edu/



From MaximilianOtto at gmx.at  Fri Jan 20 16:24:16 2006
From: MaximilianOtto at gmx.at (Max Kauer)
Date: Fri, 20 Jan 2006 16:24:16 +0100 (MET)
Subject: [R] function for rowMedian?
Message-ID: <29740.1137770656@www062.gmx.net>

Hi
is anybody aware of a function to calculate a the median for specified
columns in a dataframe or matrix - so analogous to rowMeans?
Thanks
Max

--



From saverio.vicario at yale.edu  Fri Jan 20 16:30:44 2006
From: saverio.vicario at yale.edu (saverio vicario)
Date: Fri, 20 Jan 2006 10:30:44 -0500
Subject: [R] big difference in estimate between dmvnorm and dnorm, how come?
Message-ID: <p0601023fbff6ad2c04e3@[192.168.2.36]>

Dear R community,
I was trying to estimate density at point zero of a multivariate 
distribution (9 dimensions) and for this I was using a multinormal 
approximation and the function dmvnorm , gtools package.
To have a sense of the error I tried to look the mismatch between a 
unidimensional version of my distribution and estimate density at 
point zero with function density, dmvnorm and dnorm.
At my big surprise dmvnorm and dnorm give very different result and 
dmvnorm match even better the theoritical distribution than the 
function density. How come?

#sampling from triangular distribution
X<-runif(10000)- runif(10000)
X<-matrix(X,length(X),1)

#dnorm estimate
plot(-100:100/100, dnorm((-100:100)/100,mean(X),var(X)),col=2,type="l")
#kernel density estimate
lines(density(X))
#dmvnorm estimate
res<-rep(NA,201);for ( i in 
-100:100){res[i+101]<-dmvnorm(i/100,mean(X),var(X))}
lines(-100:100/100, res,col=3)
#Theoretic triangular distribution
lines(-1:1,c(0,1,0),col=4)



From spencer.graves at pdf.com  Fri Jan 20 16:38:26 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Jan 2006 07:38:26 -0800
Subject: [R] lme model specification
In-Reply-To: <1137525003.31018.3.camel@localhost.localdomain>
References: <1137525003.31018.3.camel@localhost.localdomain>
Message-ID: <43D103F2.7050905@pdf.com>

	  Does each subject get only one LED per session or all 4 LEDs?  This 
should be important regarding which models are estimaable.  In either 
case, might the following help you?

nSubj <- 8
nSess <- 4
nObsPerSess <- 3

library(nlme)
library(e1071)
P4 <- permutations(4)

LED <- letters[t(P4[permSubj,])]

set.seed(1)
permSubj <- sample(24, nSubj)
N <- nSubj*nSess*nObsPerSess
DF <- data.frame(
   Subject=rep(1:nSubj, each=nSess*nObsPerSess),
   illum=rep(c("star", "moon"), each=N/2),
   feedback=rep(c("yes", "no"), each=N/4, length=N),
   session=rep(1:nSess, each=nObsPerSess, nSubj),
   LED=rep(LED, each=nObsPerSess),
   Rep=rep(1:nObsPerSess, nSess*nSubj),
   logdistance=rep(1:nObsPerSess, nSess*nSubj),
   logestimate=rnorm(nSubj*nSess*nObsPerSess) )

fit <- lme(logestimate~logdistance*illum*feedback+LED,
   random=~1|Subject,
   correlation=corAR1(form=~Rep|Subject/session),
   data=DF)

	  spencer graves

Bill Simpson wrote:

> I have been asked to analyse the results of (what is to me) a very
> complicated experiment.
> 
> The dependent measure is the estimated distance, which is measured as a
> function of the actual distance. There are also several other IVs.
> 
> The plot of log estimated distance as a function of log distance is
> linear. So in the rest of the analysis I will use logestimate and
> logdistance.
> 
> My plan is to see how the other IVs affect the slope and intercept of
> this linear relationship between logestimate and log distance.
> 
> What complicates everything is that each datum point is not independent.
> Rather, many data points come from each subject.
> 
> So:
> * Each subject gets many objects at many distances which he has to
> estimate.
> * Each subject repeats this experiment using 4 colours of LEDs.
> * Each subject repeats this experiment on 4 different sessions.
> * Half the subjects do this under starlight, half under moonlight.
> * Half the subjects do it with feedback and half without.
> 
> So some of these variables are within subjects and some between. I think
> lme is a good way to proceed. But I am hung up on how to specify the
> model
> 
> fit<-lme(fixed=logestimate~logdistance*session*illum*feedback,
> random=???|subject???, data=df1)
> 
> I am familiar with the steps of model building using lm(), exploring
> different models etc, so I think I will be OK once I get the idea of
> specifying the basic lme model.
> 
> I have Pinheiro and Bates (2000) here.
> 
> Thanks very much for any help
> 
> Bill Simpson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Fri Jan 20 16:39:21 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 20 Jan 2006 09:39:21 -0600
Subject: [R] function for rowMedian?
In-Reply-To: <29740.1137770656@www062.gmx.net>
References: <29740.1137770656@www062.gmx.net>
Message-ID: <43D10429.1060004@pdf.com>



Max Kauer wrote:
> Hi
> is anybody aware of a function to calculate a the median for specified
> columns in a dataframe or matrix - so analogous to rowMeans?
> Thanks
> Max
> 
> --
> 

No equivalent in base R, but you could always use ?apply:

apply(x, 1, median)

assuming "x" is an array or is coercible to a array.

I have a C-function that does this as part of my personal R package (not 
available from CRAN). It has the same time savings as "rowMeans" has 
over apply(x, 1, mean). Contact me offline if you would like a copy.

HTH,

--sundar



From andy_liaw at merck.com  Fri Jan 20 16:40:03 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 Jan 2006 10:40:03 -0500
Subject: [R] function for rowMedian?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>

apply(x, 1, median) should do it.  If not, you need to explain why.

Andy

-----Original Message-----
From: Max Kauer
Sent: Friday, January 20, 2006 10:24 AM
To: r-help at stat.math.ethz.ch
Subject: [R] function for rowMedian?


Hi
is anybody aware of a function to calculate a the median for specified
columns in a dataframe or matrix - so analogous to rowMeans?
Thanks
Max

--

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bolker at ufl.edu  Fri Jan 20 16:49:55 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 20 Jan 2006 15:49:55 +0000 (UTC)
Subject: [R] big difference in estimate between dmvnorm and dnorm,
	how come?
References: <p0601023fbff6ad2c04e3@[192.168.2.36]>
Message-ID: <loom.20060120T164827-207@post.gmane.org>

saverio vicario <saverio.vicario <at> yale.edu> writes:

> 
> Dear R community,
> I was trying to estimate density at point zero of a multivariate 
> distribution (9 dimensions) and for this I was using a multinormal 
> approximation and the function dmvnorm , gtools package.
> To have a sense of the error I tried to look the mismatch between a 
> unidimensional version of my distribution and estimate density at 
> point zero with function density, dmvnorm and dnorm.
> At my big surprise dmvnorm and dnorm give very different result and 
> dmvnorm match even better the theoritical distribution than the 
> function density. How come?

  Because dnorm takes the standard deviation and dmvnorm (which
is in the mvtnorm package, not the gtools package) takes the
variance as an argument.  replace var(X) with sd(X) in your call to dnorm and
everything will make more sense.

  cheers
    Ben Bolker



From Friedrich.Leisch at tuwien.ac.at  Fri Jan 20 17:03:17 2006
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 20 Jan 2006 17:03:17 +0100
Subject: [R] Downloads -- possible bug?
In-Reply-To: <43CFD277.9070001@statistik.uni-dortmund.de>
References: <6.1.2.0.2.20060119093026.03f99d90@epurdom.pobox.stanford.edu>
	<43CFD277.9070001@statistik.uni-dortmund.de>
Message-ID: <17361.2501.514104.689638@celebrian.ci.tuwien.ac.at>

>>>>> On Thu, 19 Jan 2006 18:55:03 +0100,
>>>>> Uwe Ligges (UL) wrote:

  > Elizabeth Purdom wrote:
  >> Hi,
  >> When I go to the CRAN page to download a new version of R, there are not 
  >> the same versions available depending on which mirror I pick. When I go to 
  >> http://cran.cnr.berkeley.edu/, for example, I get 2.2.1, but if I go 
  >> http://cran.stat.ucla.edu/ the option is 2.2.0 (I'm downloading the Windows 
  >> base file). Neither refreshing nor clearing my cache changes it. For future 
  >> reference, I'd like to know if this is a mistake or if different mirrors 
  >> are just updated at different times?

  > Looks like http://cran.stat.ucla.edu/ has not been synced for a couple 
  > of months, since Kurt Hornik's *daily* check has the date 08-Oct-2005 on 
  > that mirror. Fritz?

Yes, I will contact the admins.

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From phgrosjean at sciviews.org  Fri Jan 20 17:10:40 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 20 Jan 2006 17:10:40 +0100
Subject: [R] function for rowMedian?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>
Message-ID: <43D10B80.1080606@sciviews.org>

Note that there is a confusion here: 1st dimension is row, 2nd dimension 
is column for matrix & data.frame.
So, if the question is about "rowMedian", you have:

 > rowMedian <- function(x, na.rm = FALSE)
 >     apply(x, 2, median, na.rm = na.rm)

Now, you ask for the "median for specified columns", which should be as 
Andy proposes you, or, if you really want a colMedian function:

 > colMedian <- function(x, na.rm = FALSE)
 >     apply(x, 1, median, na.rm = na.rm)

Best,

Philippe Grosjean

Liaw, Andy wrote:
> apply(x, 1, median) should do it.  If not, you need to explain why.
> 
> Andy
> 
> -----Original Message-----
> From: Max Kauer
> Sent: Friday, January 20, 2006 10:24 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] function for rowMedian?
> 
> 
> Hi
> is anybody aware of a function to calculate a the median for specified
> columns in a dataframe or matrix - so analogous to rowMeans?
> Thanks
> Max
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pauljohn32 at gmail.com  Fri Jan 20 17:27:53 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Fri, 20 Jan 2006 10:27:53 -0600
Subject: [R] cron job install/update problems: tcltk can't find display
	(installing e.g., pbatR)
Message-ID: <13e802630601200827i21003d99r2704b80a6f32420b@mail.gmail.com>

On Fedora Core Linux 4, I have a cron job that causes R to update all
packages and install new ones.  Lately, I notice in the log that some
packages fail to install.  These are ones that assume X is running. 
For example, the pbatR install requires tcltk to be loaded, and then
the install fails because in a cron job, there is no DISPLAY
environment.  I suppose the same happens if you try to install R
packages in the console, without X running?

Error output is pasted here.  I wonder if you can advise me whether
this is the kind of thing that can be fixed in the cron job or not. 
I've verified that pbatR does install interactively (because tcltk
does start).  If you think this is a pbatR-specific problem, i will
contact the author directly.  When I have the repos option set, the
interactive install does not cause any tcltk widgets to pop up, so I
wonder if it is really necessary.

* Installing *source* package 'pbatR' ...
** libs
g++ -I/usr/lib/R/include  -I/usr/local/include   -fPIC  -O2 -g -pipe
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m32 -march=i386 -mtune=pentium4
-fasynchronous-unwind-tables -c wait.cpp -o wait.o
wait.cpp: In function 'int runCommands()':
wait.cpp:77: warning: ignoring return value of 'int system(const
char*)', declared with attribute warn_unused_result
g++ -shared -L/usr/local/lib -o pbatR.so wait.o   -L/usr/lib/R/lib -lR
** R
** save image
Loading required package: survival
Loading required package: splines
Loading required package: tcltk
Loading Tcl/Tk interface ... Error in fun(...) : no display name and
no $DISPLAY environment variable
Error: .onLoad failed in 'loadNamespace' for 'tcltk'
Error: package 'tcltk' could not be loaded
Execution halted
ERROR: execution of package source for 'pbatR' failed
** Removing '/usr/lib/R/library/pbatR'
** Restoring previous '/usr/lib/R/library/pbatR'
--------------------------------------------------------------

Here's the R code that runs from the Cron job

# Paul Johnson <pauljohn at ku.edu> 2005-08-31
# This should update and then install all packages, except for
# ones I exclude because they don't work or we don't want them.


#options(repos = "http://lib.stat.cmu.edu/R/CRAN/")
options(repos = "http://cran.cnr.berkeley.edu/")



#failPackages is the "black list".
# Things that don't build cleanly with FC4, but I
#  hope it will build someday soon.
failPackages <-
c("deldir","frailtypack","fSeries","fCalendar","fExtremes","fPortfolio","hmm.discnp","knncat","labdsv","survrec",
"SciViews","RGrace","uroot","fMultivar","fOptions","gcmrec","rcom","Rlsf")

#list of all currently installed packages
installedPackages <- rownames (installed.packages() )

#do any installed packages need removal because they are on the blacklist?
needRemoval <-  installedPackages %in% failPackages

# remove any blacklisted packages if they are already installed.
if (sum(needRemoval) >0)   remove.packages(installedPackages[needRemoval] )


#update the ones you want to keep
update.packages(ask=F, checkBuilt=T)

#get list of all new packages on CRAN
theNew <- new.packages()


#do any of the new packages belong to the black list?
shouldFail <- theNew %in% failPackages

#install non blacklisted packages that are in theNew list
if (sum(!shouldFail) > 0) install.packages( theNew[!shouldFail],dependencies=T)






--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From singyee.ling at googlemail.com  Fri Jan 20 17:52:01 2006
From: singyee.ling at googlemail.com (singyee ling)
Date: Fri, 20 Jan 2006 16:52:01 +0000
Subject: [R] questions about coxph.detail
Message-ID: <ca33a9890601200852l743326a1o@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/10e91c8f/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jan 20 18:49:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jan 2006 17:49:01 +0000 (GMT)
Subject: [R] function for rowMedian?
In-Reply-To: <43D10B80.1080606@sciviews.org>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>
	<43D10B80.1080606@sciviews.org>
Message-ID: <Pine.LNX.4.61.0601201738490.29805@gannet.stats>

Interchange 1 and 2 (see ?apply: Andy was answering the subject line, not 
that in the body).

However, Max asked also for column medians of a data frame, for which use 
sapply(DF, median)

I didn't understand the question, and it seems I was not alone :)

On Fri, 20 Jan 2006, Philippe Grosjean wrote:

> Note that there is a confusion here: 1st dimension is row, 2nd dimension
> is column for matrix & data.frame.

And the dim arg to apply is the one you want the answer to have.

A <- matrix(runif(6), 2, 3)
apply(A, 1, median) # row medians
apply(A, 2, median) # col medians


> So, if the question is about "rowMedian", you have:
>
> > rowMedian <- function(x, na.rm = FALSE)
> >     apply(x, 2, median, na.rm = na.rm)
>
> Now, you ask for the "median for specified columns", which should be as
> Andy proposes you, or, if you really want a colMedian function:
>
> > colMedian <- function(x, na.rm = FALSE)
> >     apply(x, 1, median, na.rm = na.rm)
>
> Best,
>
> Philippe Grosjean
>
> Liaw, Andy wrote:
>> apply(x, 1, median) should do it.  If not, you need to explain why.
>>
>> Andy
>>
>> -----Original Message-----
>> From: Max Kauer
>> Sent: Friday, January 20, 2006 10:24 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] function for rowMedian?
>>
>>
>> Hi
>> is anybody aware of a function to calculate a the median for specified
>> columns in a dataframe or matrix - so analogous to rowMeans?
>> Thanks
>> Max

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Reinecke at consultic.com  Fri Jan 20 18:55:19 2006
From: Reinecke at consultic.com (Michael Reinecke)
Date: Fri, 20 Jan 2006 18:55:19 +0100
Subject: [R] Selecting data frame components by name - do you know a shorter
	way?
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/dc26b005/attachment.pl

From marcodoc75 at yahoo.com  Fri Jan 20 18:58:05 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Fri, 20 Jan 2006 09:58:05 -0800 (PST)
Subject: [R] big difference in estimate between dmvnorm and dnorm,
	how come?
In-Reply-To: <p0601023fbff6ad2c04e3@[192.168.2.36]>
Message-ID: <20060120175805.54740.qmail@web31311.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/ee3d1c41/attachment.pl

From dgrove at fhcrc.org  Fri Jan 20 19:00:05 2006
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 20 Jan 2006 10:00:05 -0800 (PST)
Subject: [R] Selecting data frame components by name - do you know a
 shorter	way?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
Message-ID: <Pine.LNX.4.61.0601200958240.1970@echidna.fhcrc.org>

So you want to create a subset of a data frame?
with components "name1" "name2" "name3" ... 

dframe[, c("name1","name2","name3",...)]   

will do that

Doug



On Fri, 20 Jan 2006, Michael Reinecke wrote:

> Hi! I suspect there must be an easy way to access components of a data frame by name, i.e. the input should look like "name1 name2 name3 ..." and the output be a data frame of those components with the corresponding names. I ?ve been trying for hours, but only found the long way to do it (which is not feasible, since I have lots of components to select):
> 
>  
> 
> dframe[names(dframe)=="name1" | dframe=="name2" | dframe=="name3"]
> 
>  
> 
> Do you know a shortcut?
> 
>  
> 
> Michael
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 

From ccleland at optonline.net  Fri Jan 20 19:02:28 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 20 Jan 2006 13:02:28 -0500
Subject: [R] Selecting data frame components by name - do you know a
 shorter way?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
Message-ID: <43D125B4.9080905@optonline.net>

?subset

subset(dframe, select=c("name1", "name2", "name3"))

Michael Reinecke wrote:
> Hi! I suspect there must be an easy way to access components of a data frame by name, i.e. the input should look like "name1 name2 name3 ..." and the output be a data frame of those components with the corresponding names. I ??ve been trying for hours, but only found the long way to do it (which is not feasible, since I have lots of components to select):
> 
>  
> 
> dframe[names(dframe)=="name1" | dframe=="name2" | dframe=="name3"]
> 
>  
> 
> Do you know a shortcut?
> 
>  
> 
> Michael
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From p.dalgaard at biostat.ku.dk  Fri Jan 20 19:09:11 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jan 2006 19:09:11 +0100
Subject: [R] cron job install/update problems: tcltk can't find display
	(installing e.g., pbatR)
In-Reply-To: <13e802630601200827i21003d99r2704b80a6f32420b@mail.gmail.com>
References: <13e802630601200827i21003d99r2704b80a6f32420b@mail.gmail.com>
Message-ID: <x2veweydo8.fsf@turmalin.kubism.ku.dk>

Paul Johnson <pauljohn32 at gmail.com> writes:

> On Fedora Core Linux 4, I have a cron job that causes R to update all
> packages and install new ones.  Lately, I notice in the log that some
> packages fail to install.  These are ones that assume X is running. 
> For example, the pbatR install requires tcltk to be loaded, and then
> the install fails because in a cron job, there is no DISPLAY
> environment.  I suppose the same happens if you try to install R
> packages in the console, without X running?
> 
> Error output is pasted here.  I wonder if you can advise me whether
> this is the kind of thing that can be fixed in the cron job or not. 
> I've verified that pbatR does install interactively (because tcltk
> does start).  If you think this is a pbatR-specific problem, i will
> contact the author directly.  When I have the repos option set, the
> interactive install does not cause any tcltk widgets to pop up, so I
> wonder if it is really necessary.

The procedure used by CRAN (e.g.) for the automated testing involves
running a virtual X server (Xvfb).

On FC4 you'll need to install the RPM xorg-x11-Xvfb-6.8.2-37.FC4.49.2,
start it as part of the cron job and point your DISPLAY variable at
it.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mschwartz at mn.rr.com  Fri Jan 20 19:17:32 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 20 Jan 2006 12:17:32 -0600
Subject: [R] Selecting data frame components by name - do you know
	a	shorter way?
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
Message-ID: <1137781052.4278.9.camel@localhost.localdomain>

On Fri, 2006-01-20 at 18:55 +0100, Michael Reinecke wrote:
> Hi! I suspect there must be an easy way to access components of a data
> frame by name, i.e. the input should look like "name1 name2 name3 ..."
> and the output be a data frame of those components with the
> corresponding names. I ve been trying for hours, but only found the
> long way to do it (which is not feasible, since I have lots of
> components to select):

> dframe[names(dframe)=="name1" | dframe=="name2" | dframe=="name3"]

> Do you know a shortcut?

> Michael


See ?subset:

  subset(dframe, select = c(name1, name2, name3))

Alternatively, if the number of columns to remove is less than the
number of columns to select, you can precede the column names with a "-"
as per standard indexing conventions. 

See the Details section and the examples in ?subset.

HTH,

Marc Schwartz



From roger.bos at gmail.com  Fri Jan 20 19:26:44 2006
From: roger.bos at gmail.com (roger bos)
Date: Fri, 20 Jan 2006 13:26:44 -0500
Subject: [R] Selecting data frame components by name - do you know a
	shorter way?
In-Reply-To: <43D125B4.9080905@optonline.net>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
	<43D125B4.9080905@optonline.net>
Message-ID: <1db726800601201026h6ad1b250t1b1fa2bcff4e7e8b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/044a5dba/attachment.pl

From andy_liaw at merck.com  Fri Jan 20 19:33:46 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 Jan 2006 13:33:46 -0500
Subject: [R] Selecting data frame components by name - do you know a s
 horter way?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED729@usctmx1106.merck.com>

Try:

dframe[c("name1", "name2", "name3")]

This works because a data frame is a list, with each variable the component
of the list (and corresponding name), so you can subset it (by variables,
not rows) like a list.

Andy

> From: Michael Reinecke
> 
> Hi! I suspect there must be an easy way to access components 
> of a data frame by name, i.e. the input should look like 
> "name1 name2 name3 ..." and the output be a data frame of 
> those components with the corresponding names. I ??ve been 
> trying for hours, but only found the long way to do it (which 
> is not feasible, since I have lots of components to select):
> 
>  
> 
> dframe[names(dframe)=="name1" | dframe=="name2" | dframe=="name3"]
> 
>  
> 
> Do you know a shortcut?
> 
>  
> 
> Michael
> 
> 
> 	[[alternative HTML version deleted]]
> 
>



From rchandler at forwild.umass.edu  Fri Jan 20 19:37:05 2006
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Fri, 20 Jan 2006 13:37:05 -0500
Subject: [R] abline() or predict.lm() when log="x"
Message-ID: <1137782224.43d12dd101aae@mail-www2.oit.umass.edu>

Hello,

I'm trying to plot a fitted lm() line on a plot when the one
explanatory variable is log transformed and log="x". I get different
lines using abline and predict.lm(). 

#Example
x <- 1:100
y <- rnorm(100)
plot(y ~ x, log="x")
abline(lm(y ~ log(x)))
lines(x, predict(lm(y ~ log(x))), lwd=2)

I'm sure I'm missing something but could someone tell me which line is
correct? Thanks.

Richard 



-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From admin at biostatistic.de  Fri Jan 20 19:53:30 2006
From: admin at biostatistic.de (Knut Krueger)
Date: Fri, 20 Jan 2006 19:53:30 +0100
Subject: [R] some EPS rotated in journal preview
In-Reply-To: <Pine.LNX.4.61.0601181333060.17938@gannet.stats>
References: <43CE2EBC.1020303@biostatistic.de>
	<Pine.LNX.4.61.0601181333060.17938@gannet.stats>
Message-ID: <43D131AA.3030702@biostatistic.de>



Prof. Brian Ripley schrieb:

> The problem is a well-known one in viewers looking at whole pages,
> especially PS -> PDF converters.  R figures are particularly 
> vulnerable as they have text running both horizontally and vertically 
> (with normal axes).
>
> Please do follow exactly the advice on the postscript help page.
>
>      The postscript produced for a single R plot is EPS (_Encapsulated
>      PostScript_) compatible, and can be included into other documents,
>      e.g., into LaTeX, using '\includegraphics{<filename>}'.  For use
>      in this way you will probably want to set 'horizontal = FALSE,
>      onefile = FALSE, paper = "special"'.
>
> If you have done that, suggest to your publisher that they turn auto 
> rotation off. 



There are the reproducible codes (from the help file). The first one 
with the long text is rotated the second is not rotated.
Seems that they rotate the page, if the text of the y-axes is longer 
than the text of the x-axes.
Are you able to see any other reason for the rotation.
I will contact the journal again, if there is no other reason especially 
of the R-code.

Regards
Knut Krueger.

postscript("c:/r/test/regline2.eps",horizontal = FALSE,onefile=FALSE, 
paper = "special" ,pointsize=20,
        height=8,width=8,family = "Helvetica", font = "Helvetica")
     data(Davis)
     attach(Davis)
     mod.M<-lm(repwt~weight, subset=sex=="M")
     mod.F<-lm(repwt~weight, subset=sex=="F")
     plot(weight, repwt, pch=c(1,2)[sex],ylab="aaaa bbbb cccc dddd eeee 
ffff",xlab="aaaa bbbb")

     reg.line(mod.M)
     reg.line(mod.F, lty=2)

dev.off()

postscript("c:/r/test/regline3.eps",horizontal = FALSE,onefile=FALSE, 
paper = "special" ,pointsize=20,
        height=8,width=8,family = "Helvetica", font = "Helvetica")
     data(Davis)
     attach(Davis)
     mod.M<-lm(repwt~weight, subset=sex=="M")
     mod.F<-lm(repwt~weight, subset=sex=="F")
     plot(weight, repwt, pch=c(1,2)[sex])

     reg.line(mod.M)
     reg.line(mod.F, lty=2)

dev.off()



From mschwartz at mn.rr.com  Fri Jan 20 19:55:25 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Fri, 20 Jan 2006 12:55:25 -0600
Subject: [R] Selecting data frame components by name - do you know
	a	shorter way?
In-Reply-To: <1db726800601201026h6ad1b250t1b1fa2bcff4e7e8b@mail.gmail.com>
References: <D1A363788EC8F946A56DAF95C0FBE7CF196AAC@sbs2003.CMI.local>
	<43D125B4.9080905@optonline.net>
	<1db726800601201026h6ad1b250t1b1fa2bcff4e7e8b@mail.gmail.com>
Message-ID: <1137783325.4278.21.camel@localhost.localdomain>

One of the distinct advantages of using subset() with a large number of
columns is that given it's support for standard vector indexing in the
'select' argument, you can do something like:

  subset(dframe, select = c(name1, name5:name235, name437))

Here, by using the syntax "name5:name235", you are selecting a large
number of sequential columns in the dataframe, without having to
enumerate each column name individually. Of course, you can specify
multiple such sequential groups as appropriate.

HTH,

Marc Schwartz

On Fri, 2006-01-20 at 13:26 -0500, roger bos wrote:
> One idea is to keep the variable names you want in a vector, say 'use' and
> and get the indices using the match function:
> 
> match(use, names(df)
> 
> where use <- c("item1","item2",...)
> 
> You can then subset the data frame as follows:
> 
> newdf <- df[, match(use, names(df)]
> 
> HTH,
> 
> Roger
> 
> 
> On 1/20/06, Chuck Cleland <ccleland at optonline.net> wrote:
> >
> > ?subset
> >
> > subset(dframe, select=c("name1", "name2", "name3"))
> >
> > Michael Reinecke wrote:
> > > Hi! I suspect there must be an easy way to access components of a data
> > frame by name, i.e. the input should look like "name1 name2 name3 ..." and
> > the output be a data frame of those components with the corresponding names.
> > I ve been trying for hours, but only found the long way to do it (which is
> > not feasible, since I have lots of components to select):
> > >
> > >
> > >
> > > dframe[names(dframe)=="name1" | dframe=="name2" | dframe=="name3"]
> > >
> > >
> > >
> > > Do you know a shortcut?
> > >
> > >
> > >
> > > Michael



From albert at zoology.ubc.ca  Fri Jan 20 20:26:51 2006
From: albert at zoology.ubc.ca (Arianne Albert)
Date: Fri, 20 Jan 2006 11:26:51 -0800
Subject: [R] indexing within a function
Message-ID: <4BAFDFBE-9BDC-490B-B44A-BD66D1F8DBE2@zoology.ubc.ca>

Hello all,

I've got a large set of data consisting of 2 continuous numerical  
variables, and 2 factors. I'm trying to write a function that will  
draw scatter plots of the 2 numerical variables for various  
combinations of the factors. The problem is that my function doesn't  
seem to understand what I want it to do even though the command works  
fine outside the function. Here is an example of what I'm trying to do:

 > time<-seq(1, 10, 1)
 > depth<-seq(10, 19, 1)
 > pop<-rep(1:5, times=2)
 > pop<-as.factor(pop)
 > id<-rep(c(6,7), times=5)
 > id<-as.factor(id)

 > mydf<-cbind(time,depth,pop,id)
 > mydf<-as.data.frame(mydf)
 > attach(mydf)

 > myfunc<-function(x, y){ #this is the type of function that's not  
working
+ 	xx<-time[pop=="x"&id=="y"]
+ 	print(xx)}

 > myfunc(1,6)
numeric(0)

But, if I just enter:
 >time[pop=="1"&id=="6"]
[1] 1

I get the right answer... Does anyone know why the indexing works  
outside the function, but not inside?

Thanks in advance,

Arianne Albert
_______________________
Arianne Albert (PhD candidate)
Zoology Dept, UBC
6270 University Blvd
Vancouver, BC, V6T 1Z4, Canada
Ph: 604-822-5966
email: albert at zoology.ubc.ca
http://www.zoology.ubc.ca/~albert



From gunter.berton at gene.com  Fri Jan 20 21:30:33 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 20 Jan 2006 12:30:33 -0800
Subject: [R] indexing within a function
In-Reply-To: <4BAFDFBE-9BDC-490B-B44A-BD66D1F8DBE2@zoology.ubc.ca>
Message-ID: <200601202030.k0KKUXw6013043@ohm.gene.com>

Hmmm.. no one's responded yet...

**Please** Read An Introduction to R and the R Language Definition to learn
how to write functions and specify arguments.

Lose the quotes around x and y in your function. x is a name. "x" is a
character.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Arianne Albert
> Sent: Friday, January 20, 2006 11:27 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] indexing within a function
> 
> Hello all,
> 
> I've got a large set of data consisting of 2 continuous numerical  
> variables, and 2 factors. I'm trying to write a function that will  
> draw scatter plots of the 2 numerical variables for various  
> combinations of the factors. The problem is that my function doesn't  
> seem to understand what I want it to do even though the 
> command works  
> fine outside the function. Here is an example of what I'm 
> trying to do:
> 
>  > time<-seq(1, 10, 1)
>  > depth<-seq(10, 19, 1)
>  > pop<-rep(1:5, times=2)
>  > pop<-as.factor(pop)
>  > id<-rep(c(6,7), times=5)
>  > id<-as.factor(id)
> 
>  > mydf<-cbind(time,depth,pop,id)
>  > mydf<-as.data.frame(mydf)
>  > attach(mydf)
> 
>  > myfunc<-function(x, y){ #this is the type of function that's not  
> working
> + 	xx<-time[pop=="x"&id=="y"]
> + 	print(xx)}
> 
>  > myfunc(1,6)
> numeric(0)
> 
> But, if I just enter:
>  >time[pop=="1"&id=="6"]
> [1] 1
> 
> I get the right answer... Does anyone know why the indexing works  
> outside the function, but not inside?
> 
> Thanks in advance,
> 
> Arianne Albert
> _______________________
> Arianne Albert (PhD candidate)
> Zoology Dept, UBC
> 6270 University Blvd
> Vancouver, BC, V6T 1Z4, Canada
> Ph: 604-822-5966
> email: albert at zoology.ubc.ca
> http://www.zoology.ubc.ca/~albert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From roberto.furlan at gmail.com  Fri Jan 20 21:36:45 2006
From: roberto.furlan at gmail.com (Roberto Furlan)
Date: Fri, 20 Jan 2006 20:36:45 -0000
Subject: [R] fractional factorial design in R
Message-ID: <EMEELGDEKHMIAKDGLCDCOEKFCJAA.roberto.furlan@gmail.com>

Hi,
i need to create a fractional factorial design sufficient to estimate the
main effects.
The factors may have any number of levels, let's say any number from 2 to 6.
I've tried to use the library conf.design , but i cannot figure out how to
write the code.
For example, what is the code for a design with 5 factors (2x3x3x5x2) and
only main effects not confounded?

thanks in advance!
Roberto Furlan
University of Turin, Italy

PS: i've already tried to send this email, sorry if you 've received it
again!

----------------------------------------
La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
188 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!



From p.dalgaard at biostat.ku.dk  Fri Jan 20 22:29:34 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jan 2006 22:29:34 +0100
Subject: [R] abline() or predict.lm() when log="x"
In-Reply-To: <1137782224.43d12dd101aae@mail-www2.oit.umass.edu>
References: <1137782224.43d12dd101aae@mail-www2.oit.umass.edu>
Message-ID: <x2r772y4e9.fsf@turmalin.kubism.ku.dk>

Richard Chandler <rchandler at forwild.umass.edu> writes:

> Hello,
> 
> I'm trying to plot a fitted lm() line on a plot when the one
> explanatory variable is log transformed and log="x". I get different
> lines using abline and predict.lm(). 
> 
> #Example
> x <- 1:100
> y <- rnorm(100)
> plot(y ~ x, log="x")
> abline(lm(y ~ log(x)))
> lines(x, predict(lm(y ~ log(x))), lwd=2)
> 
> I'm sure I'm missing something but could someone tell me which line is
> correct? Thanks.

Base 10 is what you're missing.

The latter form is agnostic with respect to base, the former is not
(since the fitted values are the same, but regression coefficients
differ). So you need to know to use abline(lm(y ~ log10(x))).

You don't really notice which kind of log is being used until you look
at par(usr) for a plot with logged axes.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From luk111111 at yahoo.com  Fri Jan 20 22:53:05 2006
From: luk111111 at yahoo.com (luk)
Date: Fri, 20 Jan 2006 13:53:05 -0800 (PST)
Subject: [R] Run R in background?
Message-ID: <20060120215305.44108.qmail@web30902.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/5d8cfe7b/attachment.pl

From gunter.berton at gene.com  Fri Jan 20 23:15:49 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 20 Jan 2006 14:15:49 -0800
Subject: [R] fractional factorial design in R
In-Reply-To: <EMEELGDEKHMIAKDGLCDCOEKFCJAA.roberto.furlan@gmail.com>
Message-ID: <200601202215.k0KMFnJx000355@meitner.gene.com>

I'm not sure what you mean by a "fractional factorial" design here. In
general, when dealing with complex designs of the sort you appear to be
thinking about, small orthogonal designs don't exist. You might wish to look
at the AlgDesign package to generate an efficient design to estimate only
main effects. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roberto Furlan
> Sent: Friday, January 20, 2006 12:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] fractional factorial design in R
> 
> Hi,
> i need to create a fractional factorial design sufficient to 
> estimate the
> main effects.
> The factors may have any number of levels, let's say any 
> number from 2 to 6.
> I've tried to use the library conf.design , but i cannot 
> figure out how to
> write the code.
> For example, what is the code for a design with 5 factors 
> (2x3x3x5x2) and
> only main effects not confounded?
> 
> thanks in advance!
> Roberto Furlan
> University of Turin, Italy
> 
> PS: i've already tried to send this email, sorry if you 've 
> received it
> again!
> 
> ----------------------------------------
> La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
> 188 messaggi contenenti spam sono stati bloccati con successo.
> Scarica gratuitamente SPAMfighter!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From llei at bccrc.ca  Fri Jan 20 23:28:31 2006
From: llei at bccrc.ca (Linda Lei)
Date: Fri, 20 Jan 2006 14:28:31 -0800
Subject: [R] command in survival package
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC8@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060120/7ba54e7b/attachment.pl

From rchandler at forwild.umass.edu  Fri Jan 20 23:30:18 2006
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Fri, 20 Jan 2006 17:30:18 -0500
Subject: [R] abline() or predict.lm() when log="x"
Message-ID: <1137796218.43d1647a76f69@mail-www2.oit.umass.edu>

Thanks for the reply though I don't think your suggestion worked. I 
have found a way to get the correct line though it is not 
convenient.  Here is a better example:

x <- 1:100
y <- 1:100
plot(y ~ x, log="x")

#The only way I can get the correct line is to drop the log():
abline(lm(y ~ x), untf=T, lwd=2) #or
lines(x, predict(lm(y ~ x)), col=2) 

#Neither of these work
abline(lm(y ~ log10(x))) #or
abline(lm(y ~ log10(x)), untf=T)

What I really would like to do is plot fitted lines and 95% 
confidence intervals using predict.lm, as in shown in the example, 
but when the predictor is log transformed and log="x". I can't figure 
out how to do this without removing the log() from the response part 
of the formula and this isn't helpful because I'm generally trying to 
give predict() a fitted object rather than a lm() formula. I still 
think I'm probably missing something simple but are there any other 
suggestions? Thanks.


Richard


Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:

> Richard Chandler <rchandler at forwild.umass.edu> writes:
> 
> > Hello,
> > 
> > I'm trying to plot a fitted lm() line on a plot when the one
> > explanatory variable is log transformed and log="x". I get
> different
> > lines using abline and predict.lm(). 
> > 
> > #Example
> > x <- 1:100
> > y <- rnorm(100)
> > plot(y ~ x, log="x")
> > abline(lm(y ~ log(x)))
> > lines(x, predict(lm(y ~ log(x))), lwd=2)
> > 
> > I'm sure I'm missing something but could someone tell me which
> line is
> > correct? Thanks.
> 
> Base 10 is what you're missing.
> 
> The latter form is agnostic with respect to base, the former is
> not
> (since the fitted values are the same, but regression coefficients
> differ). So you need to know to use abline(lm(y ~ log10(x))).
> 
> You don't really notice which kind of log is being used until you
> look
> at par(usr) for a plot with logged axes.
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5,
> Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 


-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237
----- End forwarded message -----


-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From gwgilc at wm.edu  Sat Jan 21 00:02:10 2006
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Fri, 20 Jan 2006 18:02:10 -0500
Subject: [R] PCA: eigen/princomp vs. svd/prcomp
Message-ID: <F921A630-DC9B-475B-85B9-59F3758D7B12@wm.edu>

I am using R 2.2.1 on OS X 10.4.4. I have a question that is partly  
about R but also about some differences in the loadings when doing  
principal components using eigen()/princomp() versus prcomp() . Here  
is the story:

I have a matrix of mean monthly temperatures for 26 sites in the  
northern and southern hemispheres (26 x 12). I am using PCA to reduce  
this to one or two variables that capture most of the annual  
temperature variation among these sites. I am particularly interested  
in a single vector that captures the overall annual differences among  
sites. The southern hemisphere sites are 6 months out of phase with  
the northern, in terms of seasons. So the first question is whether  
or not to rotate the southern hemisphere data so that Jan=July,  
Feb=Aug, etc. before PCA. The second question is whether or not to  
center and scale the data. My gut feeling is, no, as these are all  
temperatures and the differences in means and variances among months  
are important.

If I do PCA using eigen()/princomp() on the unrotated, unscaled, and  
uncentered data, the first PC explains about 60% of the variation and  
represents the difference in phase between the southern and northern  
hemispheres. The second PC represents mean temperature and explains  
about 35% of the variation.

If I use prcomp() on the unrotated, unscaled, and uncentered data,  
the first PC represents mean temperature and explains >90% of the  
variation, the second represents the seasonal phase difference and  
explains less than 5% of the variation. This surprised me, as  
intuitively I had expected the seasonal phase difference to fall out  
first, as it did using eigen(). If anyone has an explanation for  
this, I would love to hear it.

If I center the data, the two methods yield nearly identical results,  
with the first PC capturing the seasonal phase difference and the  
second the mean, explaining 60% and 30% of the variances  
respectively. My intuition (which often is wrong...) says that this  
is not the right way to do things in this case.

I love the result from prcomp() using the uncentered, unscaled data,  
but the loadings are so different from the eigenvectors. I am  
suspicious that something funky is going on here. Does not centering  
the data cause a problem with the math? I would appreciate any comments.

If I rotate the southern hemisphere data six months out of phase,  
then the first PC by either method represents mean temperature and  
the second captures the seasonal difference but again separates the  
northern and southern hemispheres. The variance explained by the  
first PC is about 75% using eigen() and 97% using prcomp(). On one  
hand, this seems like a sensible approach, however it is pretty  
manipulative of the data. March in Santiago probably is NOT the same  
as September in San Francisco, as is reflected in the second PC. But  
again the two methods yield very different amounts of variance  
explained. Why?

Any thoughts would be very much appreciated!

cheers, George

..................................................................
George W. Gilchrist                        Email #1: gwgilc at wm.edu
Department of Biology, Box 8795          Email #2: kitesci at cox.net
College of William & Mary                    Phone: (757) 221-7751
Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
http://gwgilc.people.wm.edu/



From tlumley at u.washington.edu  Sat Jan 21 00:27:53 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Jan 2006 15:27:53 -0800 (PST)
Subject: [R] command in survival package
In-Reply-To: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC8@crcmail1.BCCRC.CA>
References: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC8@crcmail1.BCCRC.CA>
Message-ID: <Pine.LNX.4.64.0601201524340.11114@homer22.u.washington.edu>

On Fri, 20 Jan 2006, Linda Lei wrote:

> Hi there,
>
>
>
> I have a question about one command sentence when I follow the example
> in the book of "Survival analysis in S":
>
> > aml1<-aml[aml$group==1]

If this is really what the book says you should probably complain to the 
author.  However, if it says
   aml1<-aml[aml$group==1,]
then you show type that instead.

It is hard to be sure, because you don't say where the `aml' data set 
comes from.  It can't be the one in the survival package as this doesn't 
have a variable called "group"

>
> Thus, I couldn't keep going on the next command:
>
> esf.fit<-survfit(Surv(aml1,status)~1).
>

This looks implausible as well. I would have expected something like
   esf.fit<-survfit(Surv(time,status)~1, data=aml1)


 	-thomas



From pinard at iro.umontreal.ca  Sat Jan 21 00:30:38 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 20 Jan 2006 18:30:38 -0500
Subject: [R] [Rd] Display an Image on a Plane
In-Reply-To: <43D0C19A.5010806@lancaster.ac.uk>
References: <85F883EB8B41D61181C80002A541DB960627544F@valcartierex.drdc-rddc.gc.ca>
	<loom.20060119T215128-758@post.gmane.org>
	<43D0C19A.5010806@lancaster.ac.uk>
Message-ID: <20060120233038.GA13540@alcyon.progiciels-bpi.ca>

[Barry Rowlingson]
>[Ben Bolker]
>> [Labbe, Vincent]

>>>I am new to R and I would like to display an image on a plane in a
>>>3D plot, i.e. I would like to be able to specify a theta and a phi
>>>parameters like in the function persp to display a 2D image on an
>>>inclined plane.

>> what do you mean by "image" exactly?

>I think once you get into doing fancy visualisations like this then you
>may find a solution outside of R. [good referrences deleted]

Bonjour, Vincent.

I'm not fully sure I understand your request, what I get is that you 
want to transform an image on a plane as if one was looking at it in 
space, from an angle.   If I had this problem, I would probably produce 
the image using regular R machinery for this like png() or postscript(), 
then interactively process the result within Gimp, using trapezoidal 
deformations (I think they call it "Perspective transformation").  For 
example, I used this simple trick in the following picture:

   http://pinard.progiciels-bpi.ca/plaisirs/dessins/cd-back.jpg

for the KWIC listing being part of the composition.  However, if 
I needed a precise phi and theta for transformations beyond what 
trans3d() can offer, I would likely use Python or R for computing the 
projection of the rectangle enclosing the image, than PIL (Python 
Imaging Library) for producing that precise trapezoidal deformation.  
Just sharing ideas, of course.  Much likely that if I knew R better, 
I would use it more fully -- but that's a tautology! :-)

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From naiara at mail.utexas.edu  Sat Jan 21 01:17:32 2006
From: naiara at mail.utexas.edu (Naiara S. Pinto)
Date: Fri, 20 Jan 2006 18:17:32 -0600 (CST)
Subject: [R] " 'x' must be numeric"
Message-ID: <40711.129.116.71.233.1137802652.squirrel@129.116.71.233>

Hello all,

I am importing data from a txt file and try to get a histogram, I get the
message: "Error in hist: 'x' must be numeric".
When I use mode R returns "List".
However when I use srt I get:
`data.frame':   456 obs. of  1 variable:
 $ V1: num  0.6344 0.4516 0.0968 0.7634 0.7957 ...
My file consists of one column only (no headers) and I can't figure out
why I am getting this error message. Why does this happen?

Thanks!

Naiara.

--------------------------------------------
Naiara S. Pinto
Ecology, Evolution and Behavior
1 University Station A6700
Austin, TX, 78712



From dgrove at fhcrc.org  Sat Jan 21 01:43:17 2006
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 20 Jan 2006 16:43:17 -0800 (PST)
Subject: [R] " 'x' must be numeric"
In-Reply-To: <40711.129.116.71.233.1137802652.squirrel@129.116.71.233>
References: <40711.129.116.71.233.1137802652.squirrel@129.116.71.233>
Message-ID: <Pine.LNX.4.61.0601201639200.11377@echidna.fhcrc.org>

It's much more helpful if you show the actual command you used.

Presumably you have a data frame 'd' and you've done

hist(d), and 'hist' has complained because d is not numeric,
d is a data frame that *contains* a numeric vector.

You need to give hist() that numeric vector, which you can do
in many ways, including: d$V1, d[,"V1"] and d[,1]

Doug


On Fri, 20 Jan 2006, Naiara S. Pinto wrote:

> Hello all,
> 
> I am importing data from a txt file and try to get a histogram, I get the
> message: "Error in hist: 'x' must be numeric".
> When I use mode R returns "List".
> However when I use srt I get:
> `data.frame':   456 obs. of  1 variable:
>  $ V1: num  0.6344 0.4516 0.0968 0.7634 0.7957 ...
> My file consists of one column only (no headers) and I can't figure out
> why I am getting this error message. Why does this happen?
> 
> Thanks!
> 
> Naiara.
> 
> --------------------------------------------
> Naiara S. Pinto
> Ecology, Evolution and Behavior
> 1 University Station A6700
> Austin, TX, 78712
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Sat Jan 21 01:49:02 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Fri, 20 Jan 2006 18:49:02 -0600
Subject: [R] command in survival package
In-Reply-To: <Pine.LNX.4.64.0601201524340.11114@homer22.u.washington.edu>
References: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC8@crcmail1.BCCRC.CA>
	<Pine.LNX.4.64.0601201524340.11114@homer22.u.washington.edu>
Message-ID: <1137804542.5428.2.camel@localhost.localdomain>

On Fri, 2006-01-20 at 15:27 -0800, Thomas Lumley wrote:
> On Fri, 20 Jan 2006, Linda Lei wrote:
> 
> > Hi there,
> >
> >
> >
> > I have a question about one command sentence when I follow the example
> > in the book of "Survival analysis in S":
> >
> > > aml1<-aml[aml$group==1]
> 
> If this is really what the book says you should probably complain to the 
> author.  However, if it says
>    aml1<-aml[aml$group==1,]
> then you show type that instead.
> 
> It is hard to be sure, because you don't say where the `aml' data set 
> comes from.  It can't be the one in the survival package as this doesn't 
> have a variable called "group"
> 
> >
> > Thus, I couldn't keep going on the next command:
> >
> > esf.fit<-survfit(Surv(aml1,status)~1).
> >
> 
> This looks implausible as well. I would have expected something like
>    esf.fit<-survfit(Surv(time,status)~1, data=aml1)


Correct on both accounts Thomas. The text in the book on page 26 appears
as Linda posted.

However, there is an errata document here:

http://www.crcpress.com/e_products/downloads/download.asp?cat_no=C4088

which shows the corrected text as:

aml1 <- aml[aml$group==1,] # Maintained group only

esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)


HTH,

Marc Schwartz



From andy_liaw at merck.com  Sat Jan 21 04:50:32 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 Jan 2006 22:50:32 -0500
Subject: [R] Run R in background?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED734@usctmx1106.merck.com>

Try R CMD BATCH --help at the shell prompt.  

Andy

From: luk
> 
> According to the manual, a R script can be run in as follows in linux.
>    
>   $ R
>   > source("xy.R")
>    
>   Does this mean we have to type "R" first, and then within 
> R, type: source("xy.R")?
>    
>   Is there other way to run xy.R, say 
>    
>   $ R CMD SOURCE xy.R
>    
>   Is there any way to run R in background?
>    
>   Luk
>    
> 
> 		
> ---------------------------------
> 
>  Photo Books. You design it and we'll bind it!
> 	[[alternative HTML version deleted]]
> 
>



From wasquith at austin.rr.com  Sat Jan 21 05:15:21 2006
From: wasquith at austin.rr.com (William H. Asquith)
Date: Fri, 20 Jan 2006 22:15:21 -0600
Subject: [R]  Latex problem in Rd file
Message-ID: <fcedf4f9f8413ff2a36ab0db524776db@austin.rr.com>

I am trying to build

\deqn{(  \begin{array}{c}
               k-1 \\
                j
          \end{array}  )
      }

To represent the combinatorial notation as in

   k-1
    j

but R is rendering PDF literally as follows

( k-1\j )

which is not correct. This is happening on Linux and MacOSX 
installations.

I have tried variations of the \deqn{} to no avail.
Any suggestions or point me to an existing package with an Rd file that 
has and latex array in it.

Thanks
-wa



From spencer.graves at pdf.com  Sat Jan 21 08:49:07 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Jan 2006 23:49:07 -0800
Subject: [R] Help with mixed effects models
In-Reply-To: <20060118104223.97978.qmail@web37109.mail.mud.yahoo.com>
References: <20060118104223.97978.qmail@web37109.mail.mud.yahoo.com>
Message-ID: <43D1E773.8050204@pdf.com>

	  To understand an error message like you reported, someone recently 
suggested using "traceback()".  That's worth trying.  If that is not 
adequate, I've had good luck using "debug" to trace through code line by 
line.  With "lme", however, it's not completely transparent how to do 
that, because the function consists of only one line, 
'UseMethod("lme")'.  The function 'methods("lme")' gives us the following:

[1] lme.formula      lme.groupedData* lme.lmList

This tells us that 'UseMethod("lme")' calls 'lme.formula' if the first 
argument is of class 'formula', which covers your example.  You can then 
walk through the code line by line until you find the line that produces 
your error message.  Then you can study that statement by itself, read 
its documentation and try different things until you figure out what you 
need to make it do what you want.

	  If you had provided a replicable example, as suggested in the posting 
guide (www.R-project.org/posting-guide.html), I might have been able to 
help more -- or someone else might have answered your question sooner.

	  hope this helps,
	  spencer graves

Pryseley Assam wrote:

> Dear R-users
>    
>   I have problems using lme
>    
>   The model i want to fit can be viewed as a two-level bivariate model 
>  
> Two-level bivariate: bivariate (S coded as -1,T coded as 1) endpoint within trial
>   OR
>   It can equivalently be considered as a three-level model.Three-level: endpoint within patient, patient within trial. 
>    
> My code tries to model the levels through a RANDOM statement and a within group correlation structure.
>  
> ----------------------------------------------------------------------------------------------------------
>   
> Now then, i used the following R code:
>   bm <- lme (outcome~ -1 + as.factor(endpoint)+ as.factor(endpoint):trt, data=datt,
>        random=~-1 + as.factor(endpoint) + as.factor(endpoint):trt |as.factor(Trial),
>        corr = corSymm(form~-1+as.factor(endpoint)|trial/subject))
>   
> I beleive the fixed effects part of the code is okay. My intention for the 
random effects part is to estimate an intercept and treatment effect for 
each
endpoint at the trial level. The correlation structure should produce a 
within
correlation matrix for the enpoints at the subject level.
>    
>   Thus the random effects matrix is 4 by 4 and the within correlation matrix is 2 by 2
>   When i run the code in R, i get the following error message
>    
>   "Error in Initialize.corSymm(X[[2]], ...) : 
>         Initial value for corSymm parameters of wrong dimension"
>    
>   I hope someone will correct my codes .
>    
>   Kind regards
>   Pryseley
>    
>    
>   
> 
>  
> 
> 		
> ---------------------------------
> 
>  Ring in the New Year with Photo Calendars. Add photos, events, holidays, whatever.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Sat Jan 21 09:33:20 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Jan 2006 09:33:20 +0100
Subject: [R] fractional factorial design in R
In-Reply-To: <200601202215.k0KMFnJx000355@meitner.gene.com>
References: <EMEELGDEKHMIAKDGLCDCOEKFCJAA.roberto.furlan@gmail.com>
	<200601202215.k0KMFnJx000355@meitner.gene.com>
Message-ID: <17361.61904.956693.537045@stat.math.ethz.ch>

>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Fri, 20 Jan 2006 14:15:49 -0800 writes:

    BertG> I'm not sure what you mean by a "fractional
    BertG> factorial" design here. 

it's a pretty well known term, in (at least some schools of)
classical analysis of variance, e.g. 
in the famous "Box, Hunter^2" (1987) book which had a 2nd
edition last year (Wiley), see also Doug Bates's nice account on
a talk about it,
  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/29768.html
and the R package  "BHH2"  which is devoted to the book (2nd Ed).

BHH2 has a function  ffDesMatrix()  for fractional factorial
designs, *and* in it's see also section a link to the
conf.design() function in the package of the same name which
allows to construct ff designs (via different specifications).
see e.g, http://finzi.psych.upenn.edu/R/library/BHH2/html/ffDesMatrix.html

    BertG> In general, when dealing with complex designs of the sort you appear to be thinking
    BertG> about, small orthogonal designs don't exist. You
    BertG> might wish to look at the AlgDesign package to
    BertG> generate an efficient design to estimate only main
    BertG> effects.

    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
 
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box
 
now, since you have this nice signature, maybe you should
get the above book? ;-) :-)
 

    >> -----Original Message----- From:
    >> r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    >> Roberto Furlan Sent: Friday, January 20, 2006 12:37 PM
    >> To: r-help at stat.math.ethz.ch Subject: [R] fractional
    >> factorial design in R
    >> 
    >> Hi, i need to create a fractional factorial design
    >> sufficient to estimate the main effects.  The factors may
    >> have any number of levels, let's say any number from 2 to
    >> 6.  I've tried to use the library conf.design , but i
    >> cannot figure out how to write the code.  For example,
    >> what is the code for a design with 5 factors (2x3x3x5x2)
    >> and only main effects not confounded?
    >> 
    >> thanks in advance!  Roberto Furlan University of Turin,
    >> Italy



From Soren.Hojsgaard at agrsci.dk  Sat Jan 21 12:15:12 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sat, 21 Jan 2006 12:15:12 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but that
	version is not available on CRAN
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>

I just updated the packages on my pc (windows xp). When loading lme4 I get
 
 > library(lme4)
Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'

'Matrix'  0.995-1 is indeed installed on my computer, but update.packages() does not capture a never version; and seemingly for good reasons: When looking at CRAN, the new version of Matrix is available - but only as a .tar.gz archive. Two questions
 
1) How can a new version of lme4 make its way to CRAN when the packages it depends on are not available (that is, in a zip-version)?
 
An answer could be: Buhh, you can just download the .tar.gz archive and then compile it yourself! Surely, that would be possible, but it can not be the intention...
 
2) When actually trying to compile Matrix, I get
 
lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
make[3]: *** [Matrix.dll] Error 1
make[2]: *** [srcDynlib] Error 2
make[1]: *** [all] Error 2
make: *** [pkg-Matrix] Error 2
*** Installation of Matrix failed ***
Removing 'c:/programs/R/current/library/Matrix'
Restoring previous 'c:/programs/R/current/library/Matrix'
 
Can anyone help me on what to do???
Thanks / S??ren



From p.dalgaard at biostat.ku.dk  Sat Jan 21 13:39:47 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jan 2006 13:39:47 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
	that version is not available on CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
Message-ID: <x264od3gbw.fsf@turmalin.kubism.ku.dk>

S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> I just updated the packages on my pc (windows xp). When loading lme4 I get
>  
>  > library(lme4)
> Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'
> 
> 'Matrix'  0.995-1 is indeed installed on my computer, but
> update.packages() does not capture a never version; and seemingly
> for good reasons: When looking at CRAN, the new version of Matrix is
> available - but only as a .tar.gz archive. 

Oh s.....

>Two questions
>  
> 1) How can a new version of lme4 make its way to CRAN when the packages it depends on are not available (that is, in a zip-version)?

Looks like a buglet in Uwe's scripts. He has a check error on Matrix
0.995-2, but presumably it was actually built and loaded by lme4
0.995-2...

And of course the previous version of lme4 goes into the bit bucket
immediately. 

lme4 0.995-1 file still exists on the DK mirror. I've put a copy for
you in the usual place, just in case.


  
> An answer could be: Buhh, you can just download the .tar.gz archive and then compile it yourself! Surely, that would be possible, but it can not be the intention...
>  
> 2) When actually trying to compile Matrix, I get
>  
> lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
> make[3]: *** [Matrix.dll] Error 1
> make[2]: *** [srcDynlib] Error 2
> make[1]: *** [all] Error 2
> make: *** [pkg-Matrix] Error 2
> *** Installation of Matrix failed ***
> Removing 'c:/programs/R/current/library/Matrix'
> Restoring previous 'c:/programs/R/current/library/Matrix'
>  
> Can anyone help me on what to do???

This is not the same error that Uwe had (that was a clean compile but
error in the check phase), so upgrading to the latest version of the
build tools could be required.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rchandler at forwild.umass.edu  Sat Jan 21 15:18:13 2006
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Sat, 21 Jan 2006 09:18:13 -0500
Subject: [R] abline() or predict.lm() when log="x"
In-Reply-To: <x2mzhqy0ti.fsf@turmalin.kubism.ku.dk>
References: <1137782224.43d12dd101aae@mail-www2.oit.umass.edu>
	<x2r772y4e9.fsf@turmalin.kubism.ku.dk>
	<1137796019.43d163b389b80@mail-www2.oit.umass.edu>
	<x2mzhqy0ti.fsf@turmalin.kubism.ku.dk>
Message-ID: <1137853093.43d242a5ea5de@mail-www2.oit.umass.edu>

Sorry that was a typo when I said 'resposnse'... I meant predictor. I 
want to fit lm(y ~ log(x)) and plot the line with confidence 
intervals on a log="x" plot so that I can see the real units of x 
rather than the log(x) units. I can't get the real line using 
predict.lm() without removing the log() from the formula. Thanks 
again.

Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:

> Richard Chandler <rchandler at forwild.umass.edu> writes:
> 
> > Thanks for the reply though I don't think your suggestion worked.
> I 
> > have found a way to get the correct line though it is not 
> > convenient.  
> > 
> > x <- 1:100
> > y <- 1:100
> > plot(y ~ x, log="x")
> > 
> > #The only way I can get the correct line is to drop the log():
> > abline(lm(y ~ x), untf=T, lwd=2) #or
> > lines(x, predict(lm(y ~ x)), col=2) 
> > 
> > #Neither of these work
> > abline(lm(y ~ log10(x))) #or
> > abline(lm(y ~ log10(x)), untf=T)
> > 
> > What I really would like to do is plot fitted lines and 95% 
> > confidence intervals using predict.lm, as in shown in the
> example, 
> > but when the predictor is log transformed and log="x". I can't
> figure 
> > out how to do this without removing the log() from the response
> part 
> > of the formula and this isn't helpful because I'm generally
> trying to 
> > give predict() a fitted object rather than a lm() formula. I
> still 
> > think I'm probably missing something simple but are there any
> other 
> > suggestions? Thanks.
> > 
> 
> First decide what you really want. I see log() hopping all over
> the
> place. Is it on the response or the predictor? Do you want a
> straight
> line on an x-logged plot or an x-logged plot of a straight line?
> Do
> you intend to fit y~x or y~log(x) ?
> 
> 
> 
> > Richard
> > 
> > 
> > Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:
> > 
> > > Richard Chandler <rchandler at forwild.umass.edu> writes:
> > > 
> > > > Hello,
> > > > 
> > > > I'm trying to plot a fitted lm() line on a plot when the one
> > > > explanatory variable is log transformed and log="x". I get
> > > different
> > > > lines using abline and predict.lm(). 
> > > > 
> > > > #Example
> > > > x <- 1:100
> > > > y <- rnorm(100)
> > > > plot(y ~ x, log="x")
> > > > abline(lm(y ~ log(x)))
> > > > lines(x, predict(lm(y ~ log(x))), lwd=2)
> > > > 
> > > > I'm sure I'm missing something but could someone tell me
> which
> > > line is
> > > > correct? Thanks.
> > > 
> > > Base 10 is what you're missing.
> > > 
> > > The latter form is agnostic with respect to base, the former
> is
> > > not
> > > (since the fitted values are the same, but regression
> coefficients
> > > differ). So you need to know to use abline(lm(y ~ log10(x))).
> > > 
> > > You don't really notice which kind of log is being used until
> you
> > > look
> > > at par(usr) for a plot with logged axes.
> > > 
> > > -- 
> > >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5,
> > > Entr.B
> > >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph.
> K
> > >  (*) \(*) -- University of Copenhagen   Denmark          Ph: 
> (+45)
> > > 35327918
> > > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX:
> (+45)
> > > 35327907
> > > 
> > 
> > 
> > -- 
> > Richard Chandler, M.S. candidate
> > Department of Natural Resources Conservation
> > UMass Amherst
> > (413)545-1237
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5,
> Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
> 35327907
> 


-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From ligges at statistik.uni-dortmund.de  Sat Jan 21 15:58:27 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Jan 2006 15:58:27 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <x264od3gbw.fsf@turmalin.kubism.ku.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
Message-ID: <43D24C13.90302@statistik.uni-dortmund.de>

Peter Dalgaard wrote:
> S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
> 
> 
>>I just updated the packages on my pc (windows xp). When loading lme4 I get
>> 
>> > library(lme4)
>>Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'
>>
>>'Matrix'  0.995-1 is indeed installed on my computer, but
>>update.packages() does not capture a never version; and seemingly
>>for good reasons: When looking at CRAN, the new version of Matrix is
>>available - but only as a .tar.gz archive. 
> 
> 
> Oh s.....
> 
> 
>>Two questions
>> 
>>1) How can a new version of lme4 make its way to CRAN when the packages it depends on are not available (that is, in a zip-version)?
> 
> 
> Looks like a buglet in Uwe's scripts. He has a check error on Matrix
> 0.995-2, but presumably it was actually built and loaded by lme4
> 0.995-2...
> 
> And of course the previous version of lme4 goes into the bit bucket
> immediately. 
> 
> lme4 0.995-1 file still exists on the DK mirror. I've put a copy for
> you in the usual place, just in case.
> 
> 
>   
> 
>>An answer could be: Buhh, you can just download the .tar.gz archive and then compile it yourself! Surely, that would be possible, but it can not be the intention...
>> 
>>2) When actually trying to compile Matrix, I get
>> 
>>lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
>>make[3]: *** [Matrix.dll] Error 1
>>make[2]: *** [srcDynlib] Error 2
>>make[1]: *** [all] Error 2
>>make: *** [pkg-Matrix] Error 2
>>*** Installation of Matrix failed ***
>>Removing 'c:/programs/R/current/library/Matrix'
>>Restoring previous 'c:/programs/R/current/library/Matrix'
>> 
>>Can anyone help me on what to do???
> 
> 
> This is not the same error that Uwe had (that was a clean compile but
> error in the check phase), so upgrading to the latest version of the
> build tools could be required.
> 

We (Doug, Martin and I) are trying hard to debug an error in Matrix. 
Martin and Doug have ideas for a bugfix and I am checking right now.

The latest version of Matrix that passed the checks was Matrix_0.995-1 
which is still available from 
http://cran.r-project.org/bin/windows/contrib/2.2/last/

The error was also present in Matrix_0.995-1, but in that case (and by 
chance) it did not cause a crash during the examples or tests (what 
happens for Matrix_0.995-2 under Windows).

Uwe Ligges



From Soren.Hojsgaard at agrsci.dk  Sat Jan 21 16:16:11 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sat, 21 Jan 2006 16:16:11 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
	that version is not available on CRAN
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
	<43D24C13.90302@statistik.uni-dortmund.de>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387815D@DJFPOST01.djf.agrsci.dk>


We (Doug, Martin and I) are trying hard to debug an error in Matrix.
Martin and Doug have ideas for a bugfix and I am checking right now.

The latest version of Matrix that passed the checks was Matrix_0.995-1
which is still available from
http://cran.r-project.org/bin/windows/contrib/2.2/last/

The error was also present in Matrix_0.995-1, but in that case (and by
chance) it did not cause a crash during the examples or tests (what
happens for Matrix_0.995-2 under Windows).

Uwe Ligges


----


Dear Uwe, 

Thats fine, but what puzzles me is that a new lme4-version can go on CRAN when it can not run under windows... ? 

Anyway, that sort of things happen... I've now downloaded the previous version of lme4 as a tar.gz file - and that works. I wonder if it would be an idea to have a function rollback('pkg-name') for that sort of situations???

Best regards

S??ren H??jsgaard

 

 

Moreover, now that I have updated to the latest release of lme4, I wonder how to get get a version of lme4 that actually works with the version of Matrix which is currently available.



From ligges at statistik.uni-dortmund.de  Sat Jan 21 16:28:50 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Jan 2006 16:28:50 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387815D@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
	<43D24C13.90302@statistik.uni-dortmund.de>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387815D@DJFPOST01.djf.agrsci.dk>
Message-ID: <43D25332.5040806@statistik.uni-dortmund.de>

S??ren H??jsgaard wrote:

> We (Doug, Martin and I) are trying hard to debug an error in Matrix. 
> Martin and Doug have ideas for a bugfix and I am checking right now.
> 
> The latest version of Matrix that passed the checks was
> Matrix_0.995-1 which is still available from 
> http://cran.r-project.org/bin/windows/contrib/2.2/last/
> 
> The error was also present in Matrix_0.995-1, but in that case (and
> by chance) it did not cause a crash during the examples or tests
> (what happens for Matrix_0.995-2 under Windows).
> 
> Uwe Ligges
> 
> 
> ----
> 
> 
> Dear Uwe,
> 
> Thats fine, but what puzzles me is that a new lme4-version can go on
> CRAN when it can not run under windows... ?

Yes, but implementing this would require an even higher computational 
effort to build and check Windows binary packages.

Since it looks like the Matrix problems are persisting, I will play back 
the "old" versions of both packages to CRAN shortly.

Uwe



> Anyway, that sort of things happen... I've now downloaded the
> previous version of lme4 as a tar.gz file - and that works. I wonder
> if it would be an idea to have a function rollback('pkg-name') for
> that sort of situations???
> 
> Best regards
> 
> S??ren H??jsgaard
> 
> 
> 
> 
> 
> Moreover, now that I have updated to the latest release of lme4, I
> wonder how to get get a version of lme4 that actually works with the
> version of Matrix which is currently available.
> 
> 
> 
> 
> 
>



From spencer.graves at pdf.com  Sat Jan 21 17:51:35 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jan 2006 08:51:35 -0800
Subject: [R] Influence measure + lme ?
In-Reply-To: <E305A4AFB7947540BC487567B5449BA809292FED@scsmsx402.amr.corp.intel.com>
References: <E305A4AFB7947540BC487567B5449BA809292FED@scsmsx402.amr.corp.intel.com>
Message-ID: <43D26697.5030900@pdf.com>

	  My search just now found no mention of Cook's distance or influenc 
measures.  The closest I found was an unanswered question on this from 
April 2003 (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/4797.html).

	  Beyond that, there is an excellent discussion of "Examining a Fitted 
Model" in Sec. 4.3 (pp. 174-197) of Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  Pinheiro and Bates 
decided NOT to include plots of Cook's distance among the many 
diagnostics they did provide.  However, 'plot(fit.lme)' plots 
'standardized residuals' vs. predicted or 'fitted values'.  Wouldn't 
points with large influence stand apart from the crowd in terms of 
'fitted value'?

	  Of course, there are many things other one could do to get at related 
information, including reading the code for 'influence' and 'lme', and 
figure out from that how to write an 'influence' method for an 'lme' 
object.  However, that is far beyond what you asked and what I have time 
now to disucss.

	  Hope this helps.
	  spencer graves
	
Chia, Yen Lin wrote:

> Hi all,
> 
>  
> 
> Does lme has function to compute the cook's distance or influence
> measure like lm?  I can't find them.  Thanks.  
> 
>  
> 
> Yen Lin
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From marcodoc75 at yahoo.com  Sat Jan 21 18:37:28 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Sat, 21 Jan 2006 09:37:28 -0800 (PST)
Subject: [R] function kde2d
In-Reply-To: <ITFNST$7B60898BC032662F106C6958DA471269@libero.it>
Message-ID: <20060121173728.1032.qmail@web31308.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060121/c2435ccf/attachment.pl

From fredrik.bg.lundgren at bredband.net  Sat Jan 21 18:49:31 2006
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sat, 21 Jan 2006 18:49:31 +0100
Subject: [R]  converting help files with Sd2Rd
Message-ID: <000401c61eb3$089b42d0$6e9d72d5@Larissa>

Dear list,

I would like to convert several .d files to several .Rd files
in cmd.exe (Win XP home):

R CMD Sd2Rd *.d
prints all files to the screen, I want the individual files saved to 
disk

R CMD Sd2Rd *.d > foo.Rd
puts them into one file, I want the individual files

R CMD Sd2Rd *.d > *.Rd
complains:
Wrong syntax for file name, directory name eller volyme etiquette.

R CMD Sd2Rd foo.d > foo.Rd
works OK


How can I transform several .d files to several  .Rd files with one 
command?

Best wishes Fredrik



From ligges at statistik.uni-dortmund.de  Sat Jan 21 19:03:48 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Jan 2006 19:03:48 +0100
Subject: [R] converting help files with Sd2Rd
In-Reply-To: <000401c61eb3$089b42d0$6e9d72d5@Larissa>
References: <000401c61eb3$089b42d0$6e9d72d5@Larissa>
Message-ID: <43D27784.4040807@statistik.uni-dortmund.de>

Fredrik Lundgren wrote:

> Dear list,
> 
> I would like to convert several .d files to several .Rd files
> in cmd.exe (Win XP home):
> 
> R CMD Sd2Rd *.d
> prints all files to the screen, I want the individual files saved to 
> disk
> 
> R CMD Sd2Rd *.d > foo.Rd
> puts them into one file, I want the individual files
> 
> R CMD Sd2Rd *.d > *.Rd
> complains:
> Wrong syntax for file name, directory name eller volyme etiquette.
> 
> R CMD Sd2Rd foo.d > foo.Rd
> works OK
> 
> 
> How can I transform several .d files to several  .Rd files with one 
> command?


Beside shell programming, you can also use R ...

## Start R and type:
setwd(path_to_.d_files)
f <- list.files(pattern="\\.d$")
fo <- paste(sapply(strsplit(f, "\\."), "[", 1), ".Rd", sep="")
for(i in seq(along=f))
   shell(paste("R CMD Sd2Rd", f[i], ">", fo[i]))
q("no")


Uwe Ligges




> Best wishes Fredrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Jan 21 19:46:30 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Jan 2006 18:46:30 +0000 (GMT)
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <x264od3gbw.fsf@turmalin.kubism.ku.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0601211837460.21296@gannet.stats>

On Sat, 21 Jan 2006, Peter Dalgaard wrote:

> S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
>
>> I just updated the packages on my pc (windows xp). When loading lme4 I get
>>
>> > library(lme4)
>> Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'

[...]

>> 2) When actually trying to compile Matrix, I get
>>
>> lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
>> make[3]: *** [Matrix.dll] Error 1
>> make[2]: *** [srcDynlib] Error 2
>> make[1]: *** [all] Error 2
>> make: *** [pkg-Matrix] Error 2
>> *** Installation of Matrix failed ***
>> Removing 'c:/programs/R/current/library/Matrix'
>> Restoring previous 'c:/programs/R/current/library/Matrix'
>>
>> Can anyone help me on what to do???

Your toolset is out of date (well out of date, I suspect).  Please do 
ensure you have the versions listed in the R 2.2.1 R-admin manual.  I 
think this issue is an old mingw-runtime, but have not checked.

I can build Matrix 0.995.2 on Windows and have once checked it under 
R-devel.  However, it segfaults in its checks for me on almost all 
platforms, not always in the same test and not even in the same place on 
repeated runs.  I think the maintainers need to consider testing under 
valgrind.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ggrothendieck at gmail.com  Sat Jan 21 20:26:43 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 Jan 2006 14:26:43 -0500
Subject: [R] converting help files with Sd2Rd
In-Reply-To: <43D27784.4040807@statistik.uni-dortmund.de>
References: <000401c61eb3$089b42d0$6e9d72d5@Larissa>
	<43D27784.4040807@statistik.uni-dortmund.de>
Message-ID: <971536df0601211126s7ca2b268x57071385a48be012@mail.gmail.com>

Here is a slightly shorter version:

## Start R and type:
setwd(path_to_.d_files)
pat <- "\\.d"
for(f in list.files(pattern = pat))
  shell(paste("R CMD Sd2Rd", f, ">", sub(pat, ".Rd", f)))
q("no")



On 1/21/06, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Fredrik Lundgren wrote:
>
> > Dear list,
> >
> > I would like to convert several .d files to several .Rd files
> > in cmd.exe (Win XP home):
> >
> > R CMD Sd2Rd *.d
> > prints all files to the screen, I want the individual files saved to
> > disk
> >
> > R CMD Sd2Rd *.d > foo.Rd
> > puts them into one file, I want the individual files
> >
> > R CMD Sd2Rd *.d > *.Rd
> > complains:
> > Wrong syntax for file name, directory name eller volyme etiquette.
> >
> > R CMD Sd2Rd foo.d > foo.Rd
> > works OK
> >
> >
> > How can I transform several .d files to several  .Rd files with one
> > command?
>
>
> Beside shell programming, you can also use R ...
>
> ## Start R and type:
> setwd(path_to_.d_files)
> f <- list.files(pattern="\\.d$")
> fo <- paste(sapply(strsplit(f, "\\."), "[", 1), ".Rd", sep="")
> for(i in seq(along=f))
>   shell(paste("R CMD Sd2Rd", f[i], ">", fo[i]))
> q("no")
>
>
> Uwe Ligges
>
>
>
>
> > Best wishes Fredrik
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Jan 21 21:31:58 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jan 2006 12:31:58 -0800
Subject: [R] mcmcsamp() in lmer
In-Reply-To: <43CF00E5.104@stat.columbia.edu>
References: <43C40AFC.4040003@stat.columbia.edu>
	<43CF00E5.104@stat.columbia.edu>
Message-ID: <43D29A3E.1010208@pdf.com>

Dear Prof. Gelman:

	  Thanks for providing such a simple, replicable example.  I got the 
same results you describe under Windows XP Pro with:
 > sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      lme4   lattice    Matrix
"0.995-1" "0.12-11" "0.995-1"

	  Unfortunately, I'm missing something in my attempts to move beyond 
this.  First, I tried "traceback()", which gave me a standard 
cryptogram, which I couldn't decypher.  Then I typed "mcmcsamp" and 
learned that it consists solely of a call to 
'standardGeneric("mcmcsamp")'.  The help file for 'standardGeneric' sent 
me to "?GenericFunctions", which sent me further to "showMethods", which 
  produced the following:

showMethods("mcmcsamp")

Function "mcmcsamp":
object = "mer"
object = "lmer"
     (inherited from object = "mer")

	  Then I confirmed that the object "M1" created by the "lmer" call 
indeed had class "lmer".  From there, I tried several things without 
success.  The help("GenericFunctions") file mentioned 'dumpMethod' and 
'dumpMethods'.

 > dumpMethods("mcmcsamp") # produced nothing I could find.
 > dumpMethods("mcmcsamp", file="mcmcsamp.R")
# produced a file named "mcmcsamp.R" of official length 0 KB
# containing nothing, as far as I could tell.
 > dumpMethods("mcmcsamp", file="mcmcsamp.R", signature="lmer")
# also generated an empty file.

 > dumpMethod("mcmcsamp", "lmer")
[1] "mcmcsamp.lmer.R"
# Produced a file 'mcmcsamp.lmer.R' in the working directory,
#which contained only the following:

setMethod("mcmcsamp", "lmer",
NULL
)

	  I also tried 'trace(mcmcsamp)' and 'trace("mcmcsamp", browser, exit = 
browser)' before running the function giving the error message.  Nothing 
I saw from that seemed useful.

	  I'd be much obliged to anyone who could help understand how I could 
diagnose this issue.

	  Best Wishes,
	  Spencer Graves	

Andrew Gelman wrote:

> I am working with lmer() in the latest release of Matrix, doing various 
> things including writing a function called mcsamp() that acts as a 
> wrapper for mcmcsamp() and automatically runs multiple chains, diagnoses 
> convergence, and stores the result as a bugs object so it can be 
> plotted.  I recognize that at this point, mcmcsamp() is somewhat of a 
> placeholder (since it doesn't work on a lot of models) but I'm sure it 
> will continue to be improved so I'd like to be able to work with it, as 
> a starting point if necessary.
> 
> Anyway, I couldn't get mcmcsamp() to work with the saveb=TRUE option.  
> Here's a simple example:
> 
> y <- 1:10
> group <- rep (c(1,2), c(5,5))
> M1 <- lmer (y ~ 1 + (1 | group))   # works fine
> mcmcsamp (M1)                         # works fine
> mcmcsamp (M1, saveb=TRUE)
> 
> This last gives an error message:
> 
> Error in "colnames<-"(`*tmp*`, value = c("(Intercept)", "log(sigma^2)",  :
>         length of 'dimnames' [2] not equal to array extent
> 
> Thanks for your help.
> Andrew
>



From dmbates at gmail.com  Sat Jan 21 23:13:37 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 21 Jan 2006 16:13:37 -0600
Subject: [R] mcmcsamp() in lmer
In-Reply-To: <43D29A3E.1010208@pdf.com>
References: <43C40AFC.4040003@stat.columbia.edu>
	<43CF00E5.104@stat.columbia.edu> <43D29A3E.1010208@pdf.com>
Message-ID: <40e66e0b0601211413v57d2befal724665cccf70a05b@mail.gmail.com>

On 1/21/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> Dear Prof. Gelman:
>
>           Thanks for providing such a simple, replicable example.  I got the
> same results you describe under Windows XP Pro with:
>  > sessionInfo()
> R version 2.2.1, 2005-12-20, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>       lme4   lattice    Matrix
> "0.995-1" "0.12-11" "0.995-1"
>
>           Unfortunately, I'm missing something in my attempts to move beyond
> this.  First, I tried "traceback()", which gave me a standard
> cryptogram, which I couldn't decypher.  Then I typed "mcmcsamp" and
> learned that it consists solely of a call to
> 'standardGeneric("mcmcsamp")'.  The help file for 'standardGeneric' sent
> me to "?GenericFunctions", which sent me further to "showMethods", which
>   produced the following:
>
> showMethods("mcmcsamp")
>
> Function "mcmcsamp":
> object = "mer"
> object = "lmer"
>      (inherited from object = "mer")
>
>           Then I confirmed that the object "M1" created by the "lmer" call
> indeed had class "lmer".  From there, I tried several things without
> success.  The help("GenericFunctions") file mentioned 'dumpMethod' and
> 'dumpMethods'.
>
>  > dumpMethods("mcmcsamp") # produced nothing I could find.
>  > dumpMethods("mcmcsamp", file="mcmcsamp.R")
> # produced a file named "mcmcsamp.R" of official length 0 KB
> # containing nothing, as far as I could tell.
>  > dumpMethods("mcmcsamp", file="mcmcsamp.R", signature="lmer")
> # also generated an empty file.
>
>  > dumpMethod("mcmcsamp", "lmer")
> [1] "mcmcsamp.lmer.R"
> # Produced a file 'mcmcsamp.lmer.R' in the working directory,
> #which contained only the following:
>
> setMethod("mcmcsamp", "lmer",
> NULL
> )
>
>           I also tried 'trace(mcmcsamp)' and 'trace("mcmcsamp", browser, exit =
> browser)' before running the function giving the error message.  Nothing
> I saw from that seemed useful.
>
>           I'd be much obliged to anyone who could help understand how I could
> diagnose this issue.

First you try

> showMethods("mcmcsamp", classes = "mer", includeDefs = TRUE)

Function "mcmcsamp":
object = "mer":
structure(function (object, n = 1, verbose = FALSE, ...)
{
    .local <- function (object, n = 1, verbose = FALSE, saveb = FALSE,
        trans = TRUE, ...)
    {
        family <- object at family
        lmm <- family$family == "gaussian" && family$link ==
            "identity"
        if (!lmm)
            stop("mcmcsamp for GLMMs not yet implemented in supernodal
representation")
        ans <- t(.Call("mer_MCMCsamp", object, saveb, n, trans,
            PACKAGE = "Matrix"))
        attr(ans, "mcpar") <- as.integer(c(1, n, 1))
        class(ans) <- "mcmc"
        glmer <- FALSE
        gnms <- names(object at flist)
        cnms <- object at cnames
        ff <- fixef(object)
        colnms <- c(names(ff), if (glmer) character(0) else "sigma^2",
            unlist(lapply(seq(along = gnms), function(i) abbrvNms(gnms[i],
                cnms[[i]]))))
        if (trans) {
            ptyp <- c(integer(length(ff)), if (glmer) integer(0) else 1:1,
                unlist(lapply(seq(along = gnms), function(i) {
                  k <- length(cnms[[i]])
                  rep(1:2, c(k, (k * (k - 1))/2))
                })))
            colnms[ptyp == 1] <- paste("log(", colnms[ptyp ==
                1], ")", sep = "")
            colnms[ptyp == 2] <- paste("atanh(", colnms[ptyp ==
                2], ")", sep = "")
        }
        colnames(ans) <- colnms
        ans
    }
    .local(object, n, verbose, ...)
}, class = structure("MethodDefinition", package = "methods"), target
= structure("mer", .Names = "object", class = structure("signature",
package = "methods")), defined = structure("mer", .Names = "object",
class = structure("signature", package = "methods")))

which tells you that the real work is being done inside a C function
called mer_mcmcsamp.  The sources for that function are in
Matrix/src/lmer.c from the source package.

I had code for the saveb = TRUE option in there but hadn't tested it
out yet.  I believe that Martin has enabled it in the sources for the
0.995-3 release.  That's the good news.  The bad news is that we are
still running tests on that version trying to find the cause of the
segfault on Windows.


>
>           Best Wishes,
>           Spencer Graves
>
> Andrew Gelman wrote:
>
> > I am working with lmer() in the latest release of Matrix, doing various
> > things including writing a function called mcsamp() that acts as a
> > wrapper for mcmcsamp() and automatically runs multiple chains, diagnoses
> > convergence, and stores the result as a bugs object so it can be
> > plotted.  I recognize that at this point, mcmcsamp() is somewhat of a
> > placeholder (since it doesn't work on a lot of models) but I'm sure it
> > will continue to be improved so I'd like to be able to work with it, as
> > a starting point if necessary.
> >
> > Anyway, I couldn't get mcmcsamp() to work with the saveb=TRUE option.
> > Here's a simple example:
> >
> > y <- 1:10
> > group <- rep (c(1,2), c(5,5))
> > M1 <- lmer (y ~ 1 + (1 | group))   # works fine
> > mcmcsamp (M1)                         # works fine
> > mcmcsamp (M1, saveb=TRUE)
> >
> > This last gives an error message:
> >
> > Error in "colnames<-"(`*tmp*`, value = c("(Intercept)", "log(sigma^2)",  :
> >         length of 'dimnames' [2] not equal to array extent
> >
> > Thanks for your help.
> > Andrew
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rchandler at forwild.umass.edu  Sat Jan 21 23:28:27 2006
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Sat, 21 Jan 2006 17:28:27 -0500
Subject: [R] abline() or predict.lm() when log="x"
In-Reply-To: <1137853093.43d242a5ea5de@mail-www2.oit.umass.edu>
References: <1137782224.43d12dd101aae@mail-www2.oit.umass.edu>
	<x2r772y4e9.fsf@turmalin.kubism.ku.dk>
	<1137796019.43d163b389b80@mail-www2.oit.umass.edu>
	<x2mzhqy0ti.fsf@turmalin.kubism.ku.dk>
	<1137853093.43d242a5ea5de@mail-www2.oit.umass.edu>
Message-ID: <1137882507.43d2b58b09770@mail-www2.oit.umass.edu>

Please ignore my last message, I've realized that Peter's first reply 
was all I needed...thanks.

Richard

Quoting Richard Chandler <rchandler at forwild.umass.edu>:

> Sorry that was a typo when I said 'resposnse'... I meant predictor.
> I 
> want to fit lm(y ~ log(x)) and plot the line with confidence 
> intervals on a log="x" plot so that I can see the real units of x 
> rather than the log(x) units. I can't get the real line using 
> predict.lm() without removing the log() from the formula. Thanks 
> again.
> 
> Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:
> 
> > Richard Chandler <rchandler at forwild.umass.edu> writes:
> > 
> > > Thanks for the reply though I don't think your suggestion
> worked.
> > I 
> > > have found a way to get the correct line though it is not 
> > > convenient.  
> > > 
> > > x <- 1:100
> > > y <- 1:100
> > > plot(y ~ x, log="x")
> > > 
> > > #The only way I can get the correct line is to drop the log():
> > > abline(lm(y ~ x), untf=T, lwd=2) #or
> > > lines(x, predict(lm(y ~ x)), col=2) 
> > > 
> > > #Neither of these work
> > > abline(lm(y ~ log10(x))) #or
> > > abline(lm(y ~ log10(x)), untf=T)
> > > 
> > > What I really would like to do is plot fitted lines and 95% 
> > > confidence intervals using predict.lm, as in shown in the
> > example, 
> > > but when the predictor is log transformed and log="x". I can't
> > figure 
> > > out how to do this without removing the log() from the
> response
> > part 
> > > of the formula and this isn't helpful because I'm generally
> > trying to 
> > > give predict() a fitted object rather than a lm() formula. I
> > still 
> > > think I'm probably missing something simple but are there any
> > other 
> > > suggestions? Thanks.
> > > 
> > 
> > First decide what you really want. I see log() hopping all over
> > the
> > place. Is it on the response or the predictor? Do you want a
> > straight
> > line on an x-logged plot or an x-logged plot of a straight line?
> > Do
> > you intend to fit y~x or y~log(x) ?
> > 
> > 
> > 
> > > Richard
> > > 
> > > 
> > > Quoting Peter Dalgaard <p.dalgaard at biostat.ku.dk>:
> > > 
> > > > Richard Chandler <rchandler at forwild.umass.edu> writes:
> > > > 
> > > > > Hello,
> > > > > 
> > > > > I'm trying to plot a fitted lm() line on a plot when the
> one
> > > > > explanatory variable is log transformed and log="x". I get
> > > > different
> > > > > lines using abline and predict.lm(). 
> > > > > 
> > > > > #Example
> > > > > x <- 1:100
> > > > > y <- rnorm(100)
> > > > > plot(y ~ x, log="x")
> > > > > abline(lm(y ~ log(x)))
> > > > > lines(x, predict(lm(y ~ log(x))), lwd=2)
> > > > > 
> > > > > I'm sure I'm missing something but could someone tell me
> > which
> > > > line is
> > > > > correct? Thanks.
> > > > 
> > > > Base 10 is what you're missing.
> > > > 
> > > > The latter form is agnostic with respect to base, the former
> > is
> > > > not
> > > > (since the fitted values are the same, but regression
> > coefficients
> > > > differ). So you need to know to use abline(lm(y ~
> log10(x))).
> > > > 
> > > > You don't really notice which kind of log is being used
> until
> > you
> > > > look
> > > > at par(usr) for a plot with logged axes.
> > > > 
> > > > -- 
> > > >    O__  ---- Peter Dalgaard             ??ster Farimagsgade
> 5,
> > > > Entr.B
> > > >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014
> Cph.
> > K
> > > >  (*) \(*) -- University of Copenhagen   Denmark          Ph:
> 
> > (+45)
> > > > 35327918
> > > > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                 
> FAX:
> > (+45)
> > > > 35327907
> > > > 
> > > 
> > > 
> > > -- 
> > > Richard Chandler, M.S. candidate
> > > Department of Natural Resources Conservation
> > > UMass Amherst
> > > (413)545-1237
> > > 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5,
> > Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph: 
> (+45)
> > 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX:
> (+45)
> > 35327907
> > 
> 
> 
> -- 
> Richard Chandler, M.S. candidate
> Department of Natural Resources Conservation
> UMass Amherst
> (413)545-1237


-- 
Richard Chandler, M.S. candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From spencer.graves at pdf.com  Sat Jan 21 23:35:43 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jan 2006 14:35:43 -0800
Subject: [R] mcmcsamp() in lmer
In-Reply-To: <40e66e0b0601211413v57d2befal724665cccf70a05b@mail.gmail.com>
References: <43C40AFC.4040003@stat.columbia.edu>	
	<43CF00E5.104@stat.columbia.edu> <43D29A3E.1010208@pdf.com>
	<40e66e0b0601211413v57d2befal724665cccf70a05b@mail.gmail.com>
Message-ID: <43D2B73F.9060808@pdf.com>

Hi, Doug:

	  Thanks for all your hard work and creativity with mixed-effects 
models -- and for your quick reply to my question about how to use 
'showMethods' to view the source.

	  Best Wishes,
	  spencer graves

Douglas Bates wrote:

> On 1/21/06, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>Dear Prof. Gelman:
>>
>>          Thanks for providing such a simple, replicable example.  I got the
>>same results you describe under Windows XP Pro with:
>> > sessionInfo()
>>R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>
>>attached base packages:
>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>>[7] "base"
>>
>>other attached packages:
>>      lme4   lattice    Matrix
>>"0.995-1" "0.12-11" "0.995-1"
>>
>>          Unfortunately, I'm missing something in my attempts to move beyond
>>this.  First, I tried "traceback()", which gave me a standard
>>cryptogram, which I couldn't decypher.  Then I typed "mcmcsamp" and
>>learned that it consists solely of a call to
>>'standardGeneric("mcmcsamp")'.  The help file for 'standardGeneric' sent
>>me to "?GenericFunctions", which sent me further to "showMethods", which
>>  produced the following:
>>
>>showMethods("mcmcsamp")
>>
>>Function "mcmcsamp":
>>object = "mer"
>>object = "lmer"
>>     (inherited from object = "mer")
>>
>>          Then I confirmed that the object "M1" created by the "lmer" call
>>indeed had class "lmer".  From there, I tried several things without
>>success.  The help("GenericFunctions") file mentioned 'dumpMethod' and
>>'dumpMethods'.
>>
>> > dumpMethods("mcmcsamp") # produced nothing I could find.
>> > dumpMethods("mcmcsamp", file="mcmcsamp.R")
>># produced a file named "mcmcsamp.R" of official length 0 KB
>># containing nothing, as far as I could tell.
>> > dumpMethods("mcmcsamp", file="mcmcsamp.R", signature="lmer")
>># also generated an empty file.
>>
>> > dumpMethod("mcmcsamp", "lmer")
>>[1] "mcmcsamp.lmer.R"
>># Produced a file 'mcmcsamp.lmer.R' in the working directory,
>>#which contained only the following:
>>
>>setMethod("mcmcsamp", "lmer",
>>NULL
>>)
>>
>>          I also tried 'trace(mcmcsamp)' and 'trace("mcmcsamp", browser, exit =
>>browser)' before running the function giving the error message.  Nothing
>>I saw from that seemed useful.
>>
>>          I'd be much obliged to anyone who could help understand how I could
>>diagnose this issue.
> 
> 
> First you try
> 
> 
>>showMethods("mcmcsamp", classes = "mer", includeDefs = TRUE)
> 
> 
> Function "mcmcsamp":
> object = "mer":
> structure(function (object, n = 1, verbose = FALSE, ...)
> {
>     .local <- function (object, n = 1, verbose = FALSE, saveb = FALSE,
>         trans = TRUE, ...)
>     {
>         family <- object at family
>         lmm <- family$family == "gaussian" && family$link ==
>             "identity"
>         if (!lmm)
>             stop("mcmcsamp for GLMMs not yet implemented in supernodal
> representation")
>         ans <- t(.Call("mer_MCMCsamp", object, saveb, n, trans,
>             PACKAGE = "Matrix"))
>         attr(ans, "mcpar") <- as.integer(c(1, n, 1))
>         class(ans) <- "mcmc"
>         glmer <- FALSE
>         gnms <- names(object at flist)
>         cnms <- object at cnames
>         ff <- fixef(object)
>         colnms <- c(names(ff), if (glmer) character(0) else "sigma^2",
>             unlist(lapply(seq(along = gnms), function(i) abbrvNms(gnms[i],
>                 cnms[[i]]))))
>         if (trans) {
>             ptyp <- c(integer(length(ff)), if (glmer) integer(0) else 1:1,
>                 unlist(lapply(seq(along = gnms), function(i) {
>                   k <- length(cnms[[i]])
>                   rep(1:2, c(k, (k * (k - 1))/2))
>                 })))
>             colnms[ptyp == 1] <- paste("log(", colnms[ptyp ==
>                 1], ")", sep = "")
>             colnms[ptyp == 2] <- paste("atanh(", colnms[ptyp ==
>                 2], ")", sep = "")
>         }
>         colnames(ans) <- colnms
>         ans
>     }
>     .local(object, n, verbose, ...)
> }, class = structure("MethodDefinition", package = "methods"), target
> = structure("mer", .Names = "object", class = structure("signature",
> package = "methods")), defined = structure("mer", .Names = "object",
> class = structure("signature", package = "methods")))
> 
> which tells you that the real work is being done inside a C function
> called mer_mcmcsamp.  The sources for that function are in
> Matrix/src/lmer.c from the source package.
> 
> I had code for the saveb = TRUE option in there but hadn't tested it
> out yet.  I believe that Martin has enabled it in the sources for the
> 0.995-3 release.  That's the good news.  The bad news is that we are
> still running tests on that version trying to find the cause of the
> segfault on Windows.
> 
> 
> 
>>          Best Wishes,
>>          Spencer Graves
>>
>>Andrew Gelman wrote:
>>
>>
>>>I am working with lmer() in the latest release of Matrix, doing various
>>>things including writing a function called mcsamp() that acts as a
>>>wrapper for mcmcsamp() and automatically runs multiple chains, diagnoses
>>>convergence, and stores the result as a bugs object so it can be
>>>plotted.  I recognize that at this point, mcmcsamp() is somewhat of a
>>>placeholder (since it doesn't work on a lot of models) but I'm sure it
>>>will continue to be improved so I'd like to be able to work with it, as
>>>a starting point if necessary.
>>>
>>>Anyway, I couldn't get mcmcsamp() to work with the saveb=TRUE option.
>>>Here's a simple example:
>>>
>>>y <- 1:10
>>>group <- rep (c(1,2), c(5,5))
>>>M1 <- lmer (y ~ 1 + (1 | group))   # works fine
>>>mcmcsamp (M1)                         # works fine
>>>mcmcsamp (M1, saveb=TRUE)
>>>
>>>This last gives an error message:
>>>
>>>Error in "colnames<-"(`*tmp*`, value = c("(Intercept)", "log(sigma^2)",  :
>>>        length of 'dimnames' [2] not equal to array extent
>>>
>>>Thanks for your help.
>>>Andrew
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>>



From maechler at stat.math.ethz.ch  Sat Jan 21 23:45:03 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Jan 2006 23:45:03 +0100
Subject: [R] mcmcsamp() in lmer
In-Reply-To: <40e66e0b0601211413v57d2befal724665cccf70a05b@mail.gmail.com>
References: <43C40AFC.4040003@stat.columbia.edu>
	<43CF00E5.104@stat.columbia.edu> <43D29A3E.1010208@pdf.com>
	<40e66e0b0601211413v57d2befal724665cccf70a05b@mail.gmail.com>
Message-ID: <17362.47471.308840.775983@stat.math.ethz.ch>

>>>>> "DougB" == Douglas Bates <dmbates at gmail.com>
>>>>>     on Sat, 21 Jan 2006 16:13:37 -0600 writes:

    DougB> On 1/21/06, Spencer Graves <spencer.graves at pdf.com>
    DougB> wrote:
    >> Dear Prof. Gelman:
    >> 
    >> Thanks for providing such a simple, replicable example.
    >> I got the same results you describe under Windows XP Pro
    >> with: > sessionInfo() R version 2.2.1, 2005-12-20,
    >> i386-pc-mingw32
    >> 
    >> attached base packages: [1] "methods" "stats" "graphics"
    >> "grDevices" "utils" "datasets" [7] "base"
    >> 
    >> other attached packages: lme4 lattice Matrix "0.995-1"
    >> "0.12-11" "0.995-1"
    >> 
    >> Unfortunately, I'm missing something in my attempts to
    >> move beyond this.  First, I tried "traceback()", which
    >> gave me a standard cryptogram, which I couldn't decypher.
    >> Then I typed "mcmcsamp" and learned that it consists
    >> solely of a call to 'standardGeneric("mcmcsamp")'.  The
    >> help file for 'standardGeneric' sent me to
    >> "?GenericFunctions", which sent me further to
    >> "showMethods", which produced the following:
    >> 
    >> showMethods("mcmcsamp")
    >> 
    >> Function "mcmcsamp": object = "mer" object = "lmer"
    >> (inherited from object = "mer")
    >> 
    >> Then I confirmed that the object "M1" created by the
    >> "lmer" call indeed had class "lmer".  From there, I tried
    >> several things without success.  The
    >> help("GenericFunctions") file mentioned 'dumpMethod' and
    >> 'dumpMethods'.
    >> 
    >> > dumpMethods("mcmcsamp") # produced nothing I could
    >> find.  > dumpMethods("mcmcsamp", file="mcmcsamp.R") #
    >> produced a file named "mcmcsamp.R" of official length 0
    >> KB # containing nothing, as far as I could tell.  >
    >> dumpMethods("mcmcsamp", file="mcmcsamp.R",
    >> signature="lmer") # also generated an empty file.
    >> 
    >> > dumpMethod("mcmcsamp", "lmer") [1] "mcmcsamp.lmer.R" #
    >> Produced a file 'mcmcsamp.lmer.R' in the working
    >> directory, #which contained only the following:
    >> 
    >> setMethod("mcmcsamp", "lmer", NULL )
    >> 
    >> I also tried 'trace(mcmcsamp)' and 'trace("mcmcsamp",
    >> browser, exit = browser)' before running the function
    >> giving the error message.  Nothing I saw from that seemed
    >> useful.
    >> 
    >> I'd be much obliged to anyone who could help understand
    >> how I could diagnose this issue.

    DougB> First you try

    >> showMethods("mcmcsamp", classes = "mer", includeDefs =
    >> TRUE)

    DougB> Function "mcmcsamp": object = "mer":
    DougB> structure(function (object, n = 1, verbose = FALSE,
    DougB> ...)  { .local <- function (object, n = 1, verbose =
    DougB> FALSE, saveb = FALSE, trans = TRUE, ...)  { family <-
    DougB> object at family lmm <- family$family == "gaussian" &&
    DougB> family$link == "identity" if (!lmm) stop("mcmcsamp
    DougB> for GLMMs not yet implemented in supernodal
    DougB> representation") ans <- t(.Call("mer_MCMCsamp",
    DougB> object, saveb, n, trans, PACKAGE = "Matrix"))
    DougB> attr(ans, "mcpar") <- as.integer(c(1, n, 1))
    DougB> class(ans) <- "mcmc" glmer <- FALSE gnms <-
    DougB> names(object at flist) cnms <- object at cnames ff <-
    DougB> fixef(object) colnms <- c(names(ff), if (glmer)
    DougB> character(0) else "sigma^2", unlist(lapply(seq(along
    DougB> = gnms), function(i) abbrvNms(gnms[i], cnms[[i]]))))
    DougB> if (trans) { ptyp <- c(integer(length(ff)), if
    DougB> (glmer) integer(0) else 1:1, unlist(lapply(seq(along
    DougB> = gnms), function(i) { k <- length(cnms[[i]])
    DougB> rep(1:2, c(k, (k * (k - 1))/2)) }))) colnms[ptyp ==
    DougB> 1] <- paste("log(", colnms[ptyp == 1], ")", sep = "")
    DougB> colnms[ptyp == 2] <- paste("atanh(", colnms[ptyp ==
    DougB> 2], ")", sep = "") } colnames(ans) <- colnms ans }
    DougB> .local(object, n, verbose, ...)  }, class =
    DougB> structure("MethodDefinition", package = "methods"),
    DougB> target = structure("mer", .Names = "object", class =
    DougB> structure("signature", package = "methods")), defined
    DougB> = structure("mer", .Names = "object", class =
    DougB> structure("signature", package = "methods")))

    DougB> which tells you that the real work is being done
    DougB> inside a C function called mer_mcmcsamp.  The sources
    DougB> for that function are in Matrix/src/lmer.c from the
    DougB> source package.

    DougB> I had code for the saveb = TRUE option in there but
    DougB> hadn't tested it out yet.  I believe that Martin has
    DougB> enabled it in the sources for the 0.995-3 release.

yes, indeed, that's been fixed (and tests/lmer.R now also has
a test case).

I hadn't got around to reply to Andrew's e-mail,..
Martin

    DougB> That's the good news.  The bad news is that we are
    DougB> still running tests on that version trying to find
    DougB> the cause of the segfault on Windows.


    >>  Best Wishes, Spencer Graves
    >> 
    >> Andrew Gelman wrote:
    >> 
    >> > I am working with lmer() in the latest release of
    >> Matrix, doing various > things including writing a
    >> function called mcsamp() that acts as a > wrapper for
    >> mcmcsamp() and automatically runs multiple chains,
    >> diagnoses > convergence, and stores the result as a bugs
    >> object so it can be > plotted.  I recognize that at this
    >> point, mcmcsamp() is somewhat of a > placeholder (since
    >> it doesn't work on a lot of models) but I'm sure it >
    >> will continue to be improved so I'd like to be able to
    >> work with it, as > a starting point if necessary.
    >> >
    >> > Anyway, I couldn't get mcmcsamp() to work with the
    >> saveb=TRUE option.  > Here's a simple example:
    >> >
    >> > y <- 1:10 > group <- rep (c(1,2), c(5,5)) > M1 <- lmer
    >> (y ~ 1 + (1 | group)) # works fine > mcmcsamp (M1) #
    >> works fine > mcmcsamp (M1, saveb=TRUE)
    >> >
    >> > This last gives an error message:
    >> >
    >> > Error in "colnames<-"(`*tmp*`, value = c("(Intercept)",
    >> "log(sigma^2)", : > length of 'dimnames' [2] not equal to
    >> array extent
    >> >
    >> > Thanks for your help.  > Andrew
    >> >
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >> read the posting guide!
    >> http://www.R-project.org/posting-guide.html
    >> 

    DougB> ______________________________________________
    DougB> R-help at stat.math.ethz.ch mailing list
    DougB> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    DougB> do read the posting guide!
    DougB> http://www.R-project.org/posting-guide.html



From nw.galwey at ukonline.co.uk  Sat Jan 21 23:52:47 2006
From: nw.galwey at ukonline.co.uk (N.W.Galwey)
Date: Sat, 21 Jan 2006 22:52:47 -0000
Subject: [R] Means from balanced incomplete block design
Message-ID: <E1F0Rbe-000Ltv-00@smarthost1.mail.uk.easynet.net>

The code below is intended to analyse a textbook example of a balanced
incomplete block design:

#
# Data taken from  pp. 219-230 in
# Cox, D.R. (1958) Planning of Experiments. John Wiley and Son, Inc. New
York. 308 pp.
#
day <- factor(rep(1:10, each = 3))
T <- factor(c("T4","T5","T1","T4","T2","T5","T2","T4","T1","T5",
   "T3","T1","T3","T4","T5","T2","T3","T1","T3","T1",
   "T4","T3","T5","T2","T2","T3","T4","T5","T1","T2"))
response <- c(4.43,3.16,1.40,5.09,1.81,4.54,3.91,6.02,3.32,4.66,
   3.09,3.56,3.66,2.81,4.66,1.60,2.13,1.31,4.26,3.86,
   5.87,2.57,3.06,3.45,3.31,5.10,5.42,5.53,4.46,3.94)
incomplt.blk.modelaov <- 
   aov(response ~ T + Error(day))
summary(incomplt.blk.modelaov)
model.tables(incomplt.blk.modelaov, type = "means", se = TRUE)

It gives the correct anova, but the attempt to extract the treatment means
using function model.tables() produces the following error:

Error in FUN(X[[1]], ...) : eff.aovlist: non-orthogonal contrasts would give
an incorrect answer

Is there a way to obtain these means, either with or without recovery of
inter-block information?   

(N.B. The means with recovery of inter-block information can be obtained
from lme() after loading package nlme, but I'd like to do this using an
anova approach rather than a mixed modelling approach if possible.)

Thank you in anticipation.

Nick Galwey



From ajayshah at mayin.org  Sun Jan 22 01:38:01 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 22 Jan 2006 06:08:01 +0530
Subject: [R] Making a markov transition matrix
Message-ID: <20060122003801.GA2783@lubyanka.local>

Folks,

I am holding a dataset where firms are observed for a fixed (and
small) set of years. The data is in "long" format - one record for one
firm for one point in time. A state variable is observed (a factor).

I wish to make a markov transition matrix about the time-series
evolution of that state variable. The code below does this. But it's
hardcoded to the specific years that I observe. How might one
generalise this and make a general function which does this? :-)

           -ans.



set.seed(1001)

# Raw data in long format --
raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
                  state=sample(1:3, 8, replace=TRUE)
                  )
# Shift to wide format --
fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
                   direction="wide")
# Now tediously build up records for an intermediate data structure
try <- rbind(
             data.frame(prev=fixedup$state.83, new=fixedup$state.84),
             data.frame(prev=fixedup$state.84, new=fixedup$state.85),
             data.frame(prev=fixedup$state.85, new=fixedup$state.86)
             )
# This is a bad method because it is hardcoded to the specific values
# of "year".
markov <- table(destination$prev.state, destination$new.state)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From jholtman at gmail.com  Sun Jan 22 02:15:51 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 21 Jan 2006 20:15:51 -0500
Subject: [R] Making a markov transition matrix
In-Reply-To: <20060122003801.GA2783@lubyanka.local>
References: <20060122003801.GA2783@lubyanka.local>
Message-ID: <644e1f320601211715j207565ffncf46d1e202e8ff57@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060121/008228f9/attachment.pl

From jholtman at gmail.com  Sun Jan 22 02:20:21 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 21 Jan 2006 20:20:21 -0500
Subject: [R] Making a markov transition matrix
In-Reply-To: <644e1f320601211715j207565ffncf46d1e202e8ff57@mail.gmail.com>
References: <20060122003801.GA2783@lubyanka.local>
	<644e1f320601211715j207565ffncf46d1e202e8ff57@mail.gmail.com>
Message-ID: <644e1f320601211720hba3cc35j25110fd9065e102c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060121/1edc3edc/attachment.pl

From Bill.Venables at csiro.au  Sun Jan 22 03:26:17 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 22 Jan 2006 13:26:17 +1100
Subject: [R] Making a markov transition matrix
Message-ID: <B998A44C8986644EA8029CFE6396A924546910@exqld2-bne.qld.csiro.au>

If you can be sure that there are no missing years within firms, I think
I would do it this way:

> raw <- raw[do.call("order", raw), ]  # not needed here

> raw01 <- subset(data.frame(raw[-nrow(raw), ], raw[-1, ]), name ==
name.1)
> with(raw01, table(state, state.1))

     state.1
state 1 2 3
    1 1 0 0
    2 0 2 1
    3 1 1 0

So what would the general function look like?  I suppose

transitionM <- function(name, year, state) {
  raw <- data.frame(name = name, year = year, state = state)
  raw <- raw[do.call("order", raw), ]  # needed in general
  raw01 <- subset(data.frame(raw[-nrow(raw), ], raw[-1, ]), name ==
name.1)
  with(raw01, table(state, state.1))
}

give it a burl:

> with(raw, transitionM(name, year, state))
     state.1
state 1 2 3
    1 1 0 0
    2 0 2 1
    3 1 1 0

(NB no 'for' loops.)  ezy peezy.

W.


Bill Venables, 
CMIS, CSIRO Laboratories, 
PO Box 120, Cleveland, Qld. 4163 
AUSTRALIA 
Office Phone (email preferred): +61 7 3826 7251 
Fax (if absolutely necessary):    +61 7 3826 7304 
Mobile (rarely used):                +61 4 1963 4642 
Home Phone:                          +61 7 3286 7700 
mailto:Bill.Venables at csiro.au 
http://www.cmis.csiro.au/bill.venables/ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jim holtman
Sent: Sunday, 22 January 2006 11:20 AM
To: Ajay Narottam Shah
Cc: R-help
Subject: Re: [R] Making a markov transition matrix


Ignore last reply.  I sent the wrong script.

> set.seed(1001)
>
> # Raw data in long format --
> raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
+                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
+                  state=sample(1:3, 8, replace=TRUE)
+                  )
> # Shift to wide format --
> fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
+                   direction="wide")
>
> trans <- as.matrix(fixedup)
> result <- NULL
> # loop through all the columns and build up the 'result'
> for (i in 2:(ncol(trans) - 1)){
+ result <- rbind(result, cbind(name=trans[,1], PREV=trans[,i],
NEXT=trans[,i+1]))
+ }
> result
  name PREV NEXT
1 "f1" "3"  "2"
5 "f2" "2"  "3"
1 "f1" "2"  "2"
5 "f2" "3"  "1"
1 "f1" "2"  "2"
5 "f2" "1"  "1"
>
> (markov <- table(result[,"PREV"], result[,"NEXT"]))

    1 2 3
  1 1 0 0
  2 0 2 1
  3 1 1 0



On 1/21/06, jim holtman <jholtman at gmail.com> wrote:
>
> Is this what you want:
>
>
> set.seed(1001)
>
> # Raw data in long format --
> raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
>                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
>                  state=sample(1:3, 8, replace=TRUE)
>                  )
> # Shift to wide format --
> fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
>                   direction="wide")
>
> trans <- as.matrix(fixedup)
> result <- NULL
> for (i in 2:(ncol(trans) - 1)){
>  result <- rbind(result, cbind(name=trans[,1], prev=trans[,i],
> next=trans[,i+1]))
> }
>
> result
>
> markov <- table(try$prev.state, try$new.state)
>
>
>
>
>
>  On 1/21/06, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> >
> > Folks,
> >
> > I am holding a dataset where firms are observed for a fixed (and
> > small) set of years. The data is in "long" format - one record for
one
> > firm for one point in time. A state variable is observed (a factor).
> >
> > I wish to make a markov transition matrix about the time-series
> > evolution of that state variable. The code below does this. But it's
> > hardcoded to the specific years that I observe. How might one
> > generalise this and make a general function which does this? :-)
> >
> >           -ans.
> >
> >
> >
> > set.seed(1001)
> >
> > # Raw data in long format --
> > raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
> >                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
> >                  state=sample(1:3, 8, replace=TRUE)
> >                  )
> > # Shift to wide format --
> > fixedup <- reshape(raw, timevar="year", idvar="name",
v.names="state",
> >                   direction="wide")
> > # Now tediously build up records for an intermediate data structure
> > try <- rbind(
> >             data.frame(prev=fixedup$state.83, new=fixedup$state.84),
> >             data.frame(prev=fixedup$state.84, new=fixedup$state.85),
> >             data.frame(prev=fixedup$state.85, new=fixedup$state.86)
> >             )
> > # This is a bad method because it is hardcoded to the specific
values
> > # of "year".
> > markov <- table(destination$prev.state, destination$new.state)
> >
> > --
> > Ajay Shah
http://www.mayin.org/ajayshah
> >
> > ajayshah at mayin.org
> > http://ajayshahblog.blogspot.com
> > <*(:-? - wizard who doesn't know the answer.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> >
http://www.R-project.org/posting-guide.html<http://www.r-project.org/pos
ting-guide.html>
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?




--
Jim Holtman
Cincinnati, OH
+1 513 247 0281

What the problem you are trying to solve?

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Sun Jan 22 04:38:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 Jan 2006 22:38:34 -0500
Subject: [R] Making a markov transition matrix
In-Reply-To: <20060122003801.GA2783@lubyanka.local>
References: <20060122003801.GA2783@lubyanka.local>
Message-ID: <971536df0601211938i3d1cce1l302ea77dea32ce5e@mail.gmail.com>

See:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/42934.html

On 1/21/06, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> Folks,
>
> I am holding a dataset where firms are observed for a fixed (and
> small) set of years. The data is in "long" format - one record for one
> firm for one point in time. A state variable is observed (a factor).
>
> I wish to make a markov transition matrix about the time-series
> evolution of that state variable. The code below does this. But it's
> hardcoded to the specific years that I observe. How might one
> generalise this and make a general function which does this? :-)
>
>           -ans.
>
>
>
> set.seed(1001)
>
> # Raw data in long format --
> raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
>                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
>                  state=sample(1:3, 8, replace=TRUE)
>                  )
> # Shift to wide format --
> fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
>                   direction="wide")
> # Now tediously build up records for an intermediate data structure
> try <- rbind(
>             data.frame(prev=fixedup$state.83, new=fixedup$state.84),
>             data.frame(prev=fixedup$state.84, new=fixedup$state.85),
>             data.frame(prev=fixedup$state.85, new=fixedup$state.86)
>             )
> # This is a bad method because it is hardcoded to the specific values
> # of "year".
> markov <- table(destination$prev.state, destination$new.state)
>
> --
> Ajay Shah                                      http://www.mayin.org/ajayshah
> ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ajayshah at mayin.org  Sun Jan 22 08:14:34 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Sun, 22 Jan 2006 12:44:34 +0530
Subject: [R] Making a markov transition matrix
In-Reply-To: <B998A44C8986644EA8029CFE6396A924546911@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924546911@exqld2-bne.qld.csiro.au>
Message-ID: <20060122071434.GU450@lubyanka.local>

On Sun, Jan 22, 2006 at 01:47:00PM +1100, Bill.Venables at csiro.au wrote:
> If this is a real problem, here is a slightly tidier version of the
> function I gave on R-help:
> 
> transitionM <- function(name, year, state) {
>   raw <- data.frame(name = name, state = state)[order(name, year), ]
>   raw01 <- subset(data.frame(raw[-nrow(raw), ], raw[-1, ]), 
>                         name == name.1)
>   with(raw01, table(state, state.1))
> }
> 
> Notice that this does assume there are 'no gaps' in the time series
> within firms, but it does not require that each firm have responses for
> the same set of years.
> 
> Estimating the transition probability matrix when there are gaps within
> firms is a more interesting problem, both statistically and, when you
> figure that out, computationally.

With help from Gabor, here's my best effort. It should work even if
there are gaps in the timeseries within firms, and it allows different
firms to have responses in different years. It is wrapped up as a
function which eats a data frame. Somebody should put this function
into Hmisc or gtools or something of the sort.

# Problem statement:
#
# You are holding a dataset where firms are observed for a fixed
# (and small) set of years. The data is in "long" format - one
# record for one firm for one point in time. A state variable is
# observed (a factor).
# You wish to make a markov transition matrix about the time-series
# evolution of that state variable.

set.seed(1001)

# Raw data in long format --
raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
                  state=sample(1:3, 8, replace=TRUE)
                  )

transition.probabilities <- function(D, timevar="year",
                                     idvar="name", statevar="state") {
  merged <- merge(D, cbind(nextt=D[,timevar] + 1, D),
	by.x = c(timevar, idvar), by.y = c("nextt", idvar))
  t(table(merged[, grep(statevar, names(merged), value = TRUE)]))
}

transition.probabilities(raw, timevar="year", idvar="name", statevar="state")

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From spencer.graves at pdf.com  Sun Jan 22 08:15:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jan 2006 23:15:42 -0800
Subject: [R] numericDeriv() giving a vector when multiple variables input
In-Reply-To: <ed55c06b0601182017p6cfb7bb8ia01c0852102946c8@mail.gmail.com>
References: <ed55c06b0601182017p6cfb7bb8ia01c0852102946c8@mail.gmail.com>
Message-ID: <43D3311E.20901@pdf.com>

	  I've never used "numericDeriv" before, but from reading the 
documentation and working throught the example, a little thought led me 
to the following example:

f <- function(g., A){
  A %*% g.
}
g <- function(x1, x2, x3, x4, G){
   G %*%c(x1, x2, x3, x4)
}
env234 <- new.env()
A <- array(1:6, dim=c(2,3))
G <- array(1:12, dim=c(3,4))
assign("A", A, env=env234)
assign("G", G, env=env234)
assign("x1", 1., env=env234)
assign("x2", 2., env=env234)
assign("x3", 3., env=env234)
assign("x4", 4., env=env234)
(AG <- numericDeriv(quote(f(g(x1, x2, x3, x4, G), A)),
            c("x1", "x2", "x3", "x4"), env234))
(AG. <- A%*%G)
(AG.%*%1:4)

	  This seemed to work for me and give me the correct answer.  If this 
is not enough, RSiteSearch("numericDeriv") gave me 115 hits;  many of 
these are not relevant to your question, but I would expect that some 
might be.

	  hope this helps.
	  spencer graves

Seth Pruitt wrote:

> R Help List --
> 
> I have defined two time-series-vector-valued-functions, let them be f and g,
> and want to find the numeric derivative of f with respect to the variable x
> where f depends on x through g:
> (d/dx)(f (g(x) )
> 
> Moreover, x is a vector
> 
> I tried this out the long way (naming every element of the x vector and then
> making the 'theta' argument in numericDeriv() the character vector of all
> these names) and the result is just one time series vector; I was hoping for
> a matrix. Also weirdly, if I instead make theta equal to just one of the
> named elements of x, I get the same time series vector; the same happens for
> any subset of the named elements
> 
> My call to numericDeriv looks like this (vphi acts as x,
> swz.kalman.vectoracts as g,
> decision.ts.vector acts as f):
> 
> ***
> numericDeriv(
> expr = decision.ts.vector(
>   a = swz.kalman.vector(
>     zeta1=zeta[1], u = Phi$u, phi = Phi$Phi,
>     vphi =
> c(varphi1,varphi2,varphi3,varphi4,varphi5,varphi6,varphi7,varphi8,varphi9,varphi10,
> 
> varphi11,varphi12,varphi13,varphi14,varphi15,varphi16,varphi17,varphi18,varphi19,
> 
> varphi20,varphi21,varphi22,varphi23,varphi24,varphi25,varphi26,varphi27,varphi28,
> 
> varphi29,varphi30,varphi31,varphi32,varphi33,varphi34,varphi35,varphi36,varphi37,
>     varphi38,varphi39,varphi40,varphi41,varphi42),
>     alpha.prior = specs$alpha.prior
>     ),
>   phi = Phi$Phi, lambda = specs$lambda, delta = specs$delta, pi.star =
> specs$pi.star, u.2star = specs$u.2star
>   ),
> theta =
> c("varphi1","varphi2","varphi3","varphi4","varphi5","varphi6","varphi7","varphi8","varphi9","varphi10",
> 
> "varphi11","varphi12","varphi13","varphi14","varphi15","varphi16","varphi17","varphi18","varphi19",
> 
> "varphi20","varphi21","varphi22","varphi23","varphi24","varphi25","varphi26","varphi27","varphi28",
> 
> "varphi29","varphi30","varphi31","varphi32","varphi33","varphi34","varphi35","varphi36","varphi37",
>     "varphi38","varphi39","varphi40","varphi41","varphi42")
> );
> ***
> 
> As you can see, it includes some calls to other objects that are lying
> around. Maybe from this email someone can tell me my mistake with how I've
> called things; otherwise, I'm happy to send along the .R files to whomever
> is so kind as offer guidance.
> 
> I appreciate your time,
> Seth
> 
> --
> Seth Pruitt
> Department of Economics
> University of California, San Diego
> sjpruitt at ucsd.edu
> http://dss.ucsd.edu/~sjpruitt
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Sun Jan 22 12:00:17 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 22 Jan 2006 12:00:17 +0100
Subject: [R] function for rowMedian?
In-Reply-To: <Pine.LNX.4.61.0601201738490.29805@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>	<43D10B80.1080606@sciviews.org>
	<Pine.LNX.4.61.0601201738490.29805@gannet.stats>
Message-ID: <43D365C1.50101@sciviews.org>

Prof Brian Ripley wrote:
> Interchange 1 and 2 (see ?apply: Andy was answering the subject line, not 
> that in the body).
> 
> However, Max asked also for column medians of a data frame, for which use 
> sapply(DF, median)

By the way, what is the preferred code for column-wise calculation on 
*all numeric* data in a data frame?
 > sapply(DF, median)
which considers the list and then simplifies the results down to a 
vector or matrix,... or
 > apply(DF, 2, median)
which first convert into a matrix and then do the calculation on it?
Best,

Philippe Grosjean


> I didn't understand the question, and it seems I was not alone :)
> 
> On Fri, 20 Jan 2006, Philippe Grosjean wrote:
> 
> 
>>Note that there is a confusion here: 1st dimension is row, 2nd dimension
>>is column for matrix & data.frame.
> 
> 
> And the dim arg to apply is the one you want the answer to have.
> 
> A <- matrix(runif(6), 2, 3)
> apply(A, 1, median) # row medians
> apply(A, 2, median) # col medians
> 
> 
> 
>>So, if the question is about "rowMedian", you have:
>>
>>
>>>rowMedian <- function(x, na.rm = FALSE)
>>>    apply(x, 2, median, na.rm = na.rm)
>>
>>Now, you ask for the "median for specified columns", which should be as
>>Andy proposes you, or, if you really want a colMedian function:
>>
>>
>>>colMedian <- function(x, na.rm = FALSE)
>>>    apply(x, 1, median, na.rm = na.rm)
>>
>>Best,
>>
>>Philippe Grosjean
>>
>>Liaw, Andy wrote:
>>
>>>apply(x, 1, median) should do it.  If not, you need to explain why.
>>>
>>>Andy
>>>
>>>-----Original Message-----
>>>From: Max Kauer
>>>Sent: Friday, January 20, 2006 10:24 AM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] function for rowMedian?
>>>
>>>
>>>Hi
>>>is anybody aware of a function to calculate a the median for specified
>>>columns in a dataframe or matrix - so analogous to rowMeans?
>>>Thanks
>>>Max
> 
>



From steffen.katzner at mail.gwdg.de  Sun Jan 22 12:28:35 2006
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Sun, 22 Jan 2006 12:28:35 +0100
Subject: [R] Error in 1:object$rank : NA/NaN argument
Message-ID: <43D36C63.7020204@mail.gwdg.de>

can anybody help with the following error message:

Error in 1:object$rank : NA/NaN argument

I get it with comparisons of single means in an ANOVA.
Example data below.

Thanks,  Steffen




seven subjects participated in each of 6 conditions (intervals).

> subject = factor(rep(c(1:7), each = 6))
> interval = factor(rep(c(1:6), 7))

and here is the dependent variable:

> dv = c(3.3868, 3.1068, 1.7261, 1.5415, 1.7356, 0.7538,
+ 2.5957, 1.5666, 1.1984, 1.2761, 1.0022, 0.8597,
+ 3.9819, 3.1506, 1.5824, 1.7400, 1.4248, 0.6519,
+ 2.2521, 1.5248, 1.1209, 1.2193, 1.1994, 2.0910,
+ 2.4661, 1.3863, 1.3591, 0.9163, 1.3976, 1.7471,
+ 3.2486, 1.9492, 2.4228, 1.1276, 1.2836, 0.9814,
+ 1.7148, 1.7278, 2.7433, 1.4924, 1.0992, 0.7821)

> d = data.frame(subject, interval, dv)

next I'm defining a contrast matrix:

> con = matrix(c(1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 
0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1), nrow=6, ncol=5, byrow=F)

> con
      [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]   -1    1    0    0    0
[3,]    0   -1    1    0    0
[4,]    0    0   -1    1    0
[5,]    0    0    0   -1    1
[6,]    0    0    0    0   -1


> contrasts(d$interval)=con

and then I'm doing the ANOVA

> aovRes = aov(dv~interval+Error(subject/interval), data=d)

> summary(aovRes)

Error: subject
            Df  Sum Sq Mean Sq F value Pr(>F)
Residuals  6 2.48531 0.41422

Error: subject:interval
            Df  Sum Sq Mean Sq F value    Pr(>F)
interval   5 13.8174  2.7635  8.7178 3.417e-05 ***
Residuals 30  9.5098  0.3170
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



but if I want to look at the contrasts, something has gone wrong:


summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs
i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))

Error in 1:object$rank : NA/NaN argument

> aovRes$contrasts
NULL



From ripley at stats.ox.ac.uk  Sun Jan 22 12:44:29 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 22 Jan 2006 11:44:29 +0000 (GMT)
Subject: [R] function for rowMedian?
In-Reply-To: <43D365C1.50101@sciviews.org>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED725@usctmx1106.merck.com>
	<43D10B80.1080606@sciviews.org>
	<Pine.LNX.4.61.0601201738490.29805@gannet.stats>
	<43D365C1.50101@sciviews.org>
Message-ID: <Pine.LNX.4.61.0601221137330.18482@gannet.stats>

On Sun, 22 Jan 2006, Philippe Grosjean wrote:

> Prof Brian Ripley wrote:
>> Interchange 1 and 2 (see ?apply: Andy was answering the subject line, not 
>> that in the body).
>> 
>> However, Max asked also for column medians of a data frame, for which use 
>> sapply(DF, median)
>
> By the way, what is the preferred code for column-wise calculation on *all 
> numeric* data in a data frame?
>> sapply(DF, median)
> which considers the list and then simplifies the results down to a vector or 
> matrix,... or
>> apply(DF, 2, median)
> which first convert into a matrix and then do the calculation on it?

The first.  apply(DF,2, ...) converts to a matrix, extracts each column 
and applies FUN to each (in a for loop) to form a list answer and then 
simplifies.

I did once try rewriting apply to call lapply(split()), but that was no 
faster (at the time) and used more memory.

> Best,
>
> Philippe Grosjean
>
>
>> I didn't understand the question, and it seems I was not alone :)
>> 
>> On Fri, 20 Jan 2006, Philippe Grosjean wrote:
>> 
>> 
>>> Note that there is a confusion here: 1st dimension is row, 2nd dimension
>>> is column for matrix & data.frame.
>> 
>> 
>> And the dim arg to apply is the one you want the answer to have.
>> 
>> A <- matrix(runif(6), 2, 3)
>> apply(A, 1, median) # row medians
>> apply(A, 2, median) # col medians
>> 
>> 
>> 
>>> So, if the question is about "rowMedian", you have:
>>> 
>>> 
>>>> rowMedian <- function(x, na.rm = FALSE)
>>>>    apply(x, 2, median, na.rm = na.rm)
>>> 
>>> Now, you ask for the "median for specified columns", which should be as
>>> Andy proposes you, or, if you really want a colMedian function:
>>> 
>>> 
>>>> colMedian <- function(x, na.rm = FALSE)
>>>>    apply(x, 1, median, na.rm = na.rm)
>>> 
>>> Best,
>>> 
>>> Philippe Grosjean
>>> 
>>> Liaw, Andy wrote:
>>> 
>>>> apply(x, 1, median) should do it.  If not, you need to explain why.
>>>> 
>>>> Andy
>>>> 
>>>> -----Original Message-----
>>>> From: Max Kauer
>>>> Sent: Friday, January 20, 2006 10:24 AM
>>>> To: r-help at stat.math.ethz.ch
>>>> Subject: [R] function for rowMedian?
>>>> 
>>>> 
>>>> Hi
>>>> is anybody aware of a function to calculate a the median for specified
>>>> columns in a dataframe or matrix - so analogous to rowMeans?
>>>> Thanks
>>>> Max
>> 
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bill.Venables at csiro.au  Sun Jan 22 12:51:09 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 22 Jan 2006 22:51:09 +1100
Subject: [R] Making a markov transition matrix
Message-ID: <B998A44C8986644EA8029CFE6396A924546919@exqld2-bne.qld.csiro.au>

That solution for the case 'with gaps' merely omits transitions where
the transition information is not for a single time step.  (Mine can be
modified for this as well - see below.)

But if you know that a firm went from state i in year y to state j in
year y+3, say, without knowing the intermediate states, that must tell
you something about the 1-step transition matrix as well.  How do you
use this information?

That's a much more difficult problem but you can do it using maximum
likelihood, e.g.  You think about how to calculate the likelihood
function - and then to optimise it.  This is getting a bit away from the
original 'programming trick' question, but it is an interesting problem
that occurs more often than I had realised.  I'd be interested in
knowing if anyone had done anything slick in this area.

Bill Venables.

-----Original Message-----
From: Ajay Narottam Shah [mailto:ajayshah at mayin.org] 
Sent: Sunday, 22 January 2006 5:15 PM
To: R-help
Cc: jholtman at gmail.com; Venables, Bill (CMIS, Cleveland)
Subject: Re: [R] Making a markov transition matrix


On Sun, Jan 22, 2006 at 01:47:00PM +1100, Bill.Venables at csiro.au wrote:
> If this is a real problem, here is a slightly tidier version of the
> function I gave on R-help:
> 
> transitionM <- function(name, year, state) {
>   raw <- data.frame(name = name, state = state)[order(name, year), ]
>   raw01 <- subset(data.frame(raw[-nrow(raw), ], raw[-1, ]), 
>                         name == name.1)
>   with(raw01, table(state, state.1))
> }

To modify this solution for the 'with gaps' case, omitting multiple step
transitions, you need to include the year in the 'raw' data frame and
then just change the subset condition to

			name == name.1 & year == year.1 - 1


> 
> Notice that this does assume there are 'no gaps' in the time series
> within firms, but it does not require that each firm have responses
for
> the same set of years.
> 
> Estimating the transition probability matrix when there are gaps
within
> firms is a more interesting problem, both statistically and, when you
> figure that out, computationally.

With help from Gabor, here's my best effort. It should work even if
there are gaps in the timeseries within firms, and it allows different
firms to have responses in different years. It is wrapped up as a
function which eats a data frame. Somebody should put this function
into Hmisc or gtools or something of the sort.

# Problem statement:
#
# You are holding a dataset where firms are observed for a fixed
# (and small) set of years. The data is in "long" format - one
# record for one firm for one point in time. A state variable is
# observed (a factor).
# You wish to make a markov transition matrix about the time-series
# evolution of that state variable.

set.seed(1001)

# Raw data in long format --
raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
                  state=sample(1:3, 8, replace=TRUE)
                  )

transition.probabilities <- function(D, timevar="year",
                                     idvar="name", statevar="state") {
  merged <- merge(D, cbind(nextt=D[,timevar] + 1, D),
	by.x = c(timevar, idvar), by.y = c("nextt", idvar))
  t(table(merged[, grep(statevar, names(merged), value = TRUE)]))
}

transition.probabilities(raw, timevar="year", idvar="name",
statevar="state")

-- 
Ajay Shah
http://www.mayin.org/ajayshah  
ajayshah at mayin.org
http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From perlenana at yahoo.fr  Sun Jan 22 12:55:43 2006
From: perlenana at yahoo.fr (Nadege Nana)
Date: Sun, 22 Jan 2006 12:55:43 +0100 (CET)
Subject: [R] Rank Product
Message-ID: <20060122115543.16749.qmail@web26208.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060122/3773a3d3/attachment.pl

From laura at env.leeds.ac.uk  Sun Jan 22 14:38:05 2006
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sun, 22 Jan 2006 13:38:05 +0000 (GMT)
Subject: [R] White Noise
Message-ID: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>

I'm wanting to create a series of near-identical matrices via the addition
of "white noise" to my starting matrix. Is there a function within R which
will allow me to do this?

Thank you

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From andywongcw at gmail.com  Sun Jan 22 15:54:37 2006
From: andywongcw at gmail.com (Andy Wong)
Date: Sun, 22 Jan 2006 22:54:37 +0800
Subject: [R] Application of R and MNP
Message-ID: <cafcabf80601220654t6805e0c2v1d807a8f8c64a87c@mail.gmail.com>

Dear all,

I'm a new user of R and MNP.  Would anybody advise me what I have done wrong
to the input or data.  When I add the R7, it said that there is an error
"SWP:singular matrix".  I can add up to R6 only.  Copy of the R console is
attached for your reference.

Thanks.

Andy
-------------- next part --------------
A non-text attachment was scrubbed...
Name: mywork.pdf
Type: application/pdf
Size: 32328 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20060122/60706d16/mywork.pdf

From jfox at mcmaster.ca  Sun Jan 22 16:08:44 2006
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 22 Jan 2006 10:08:44 -0500
Subject: [R] Application of R and MNP
In-Reply-To: <cafcabf80601220654t6805e0c2v1d807a8f8c64a87c@mail.gmail.com>
Message-ID: <20060122150843.IQVR8316.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy,

You appear to have 7 observations and 8 explanatory variables in the model.
You'd be in trouble even if this were a linear model fit by least squares.
Moreover, each observation has a different value of the response variable --
that, is the response has 7 levels. Did you really intend to do that?
Perhaps it would help to know what you want to do with these data.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andy Wong
> Sent: Sunday, January 22, 2006 9:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Application of R and MNP
> 
> Dear all,
> 
> I'm a new user of R and MNP.  Would anybody advise me what I 
> have done wrong to the input or data.  When I add the R7, it 
> said that there is an error "SWP:singular matrix".  I can add 
> up to R6 only.  Copy of the R console is attached for your reference.
> 
> Thanks.
> 
> Andy
>



From asaguiar at spsconsultoria.com  Sun Jan 22 16:50:29 2006
From: asaguiar at spsconsultoria.com (Alexandre Santos Aguiar)
Date: Sun, 22 Jan 2006 13:50:29 -0200
Subject: [R] Difference between two correlation coefficients
Message-ID: <200601221350.30630.asaguiar@spsconsultoria.com>

Hi,

I've been interested in testing differences between correlation coefficients 
and as a newcomer to R it occured to me building a library to perform such 
tests.
However, before enganging on the project I need to know if there are such 
library functions already available and if so which they are.

Thanks.

-- 

          Alexandre Santos Aguiar, MD
- independent consultant for health research -
       R Botucatu, 591 cj 81 - 04037-005
            S??o Paulo - SP - Brazil
             tel +55-11-9320-2046
             fax +55-11-5549-8760
            www.spsconsultoria.com



From jholtman at gmail.com  Sun Jan 22 17:24:24 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 22 Jan 2006 11:24:24 -0500
Subject: [R] White Noise
In-Reply-To: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>
Message-ID: <644e1f320601220824q178e2169ubcc32cb58e3caa86@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060122/51c85b19/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Sun Jan 22 17:54:18 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 22 Jan 2006 16:54:18 -0000 (GMT)
Subject: [R] White Noise
In-Reply-To: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>
Message-ID: <XFMail.060122165418.Ted.Harding@nessie.mcc.ac.uk>

On 22-Jan-06 Laura Quinn wrote:
> I'm wanting to create a series of near-identical matrices
> via the addition of "white noise" to my starting matrix.
> Is there a function within R which will allow me to do this?
> 
> Thank you
> 
> Laura Quinn

The short ansdwer is: Yes, just do it as you descibe!

For example:

  M<-matrix(c(2,4,6,1,3,5),nr=2)
  M
  #      [,1] [,2] [,3]
  # [1,]    2    6    3
  # [2,]    4    1    5

  M + 0.01*runif(6)
  #          [,1]     [,2]     [,3]
  # [1,] 2.009196 6.000863 3.009307
  # [2,] 4.000973 1.008916 5.006934

You have certainly added "white noise" since, considered
as a sequence, the elements of runif(6) are independent.

It gets more interesting, however, if you want the result
to be more constrained then you have stated in your query.

Ensuring that the expected result is equal to the starting
matrix, or that the added noise has a specified distribution
(e.g. gaussian) is easy: just change the "0.01*runif()".

But if you want to ensure that (e.g.) the elements of the
result are positive, or that the result (if square) is
positive definite, or is symmetric, or is a correlation
matrix, then some further thought would have to be given
to just how best to proceed. The resulting "tuned" procedure
would then be better wrapped in a function.

If you would, perhaps, state more fully what you want to
achieve it may be possible to suggest something.

I'm not, myself, aware of a function in R which is designed
to do this job under the sort of constraints above, or
others (and, if unconstrained, you hardly need a special
function for it).

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Jan-06                                       Time: 16:54:15
------------------------------ XFMail ------------------------------



From hkahra at gmail.com  Sun Jan 22 18:37:00 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Sun, 22 Jan 2006 19:37:00 +0200
Subject: [R] NAs in optim
Message-ID: <3d35a2ca0601220937x6a1b2420p3b7d3ee55185105f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060122/2d14cc4a/attachment.pl

From Pierre.Lapointe at nbf.ca  Sun Jan 22 18:51:11 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Sun, 22 Jan 2006 12:51:11 -0500
Subject: [R] Solve for x in Ax=B with vectors, not matrices
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35CAC@lbmsg002.fbn-nbf.local>

Hello R-helpers,

What I have: I am working with vectors not matrice:

#Basic equations
A <-c(-20,-9,-2)
x <-c(0.17,0.22,0.61)
B <- crossprod(A,x)

# R matrix multiplication works with vectors
A%*%x==B 	# Is true...

Question: If x is unknown and A and B are known, how do I solve for x in R?
solve(A,B) won't work because A is not a square matrix 

solve(A,B)
Error in solve.default(A, B) : 'A' (3 x 1) must be square

I understand that I might have many solutions but note that the sum of x is
1 and all x are positive (x are weightings in % of the total).

Regards,

Pierre Lapointe



*****************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}



From stratja at auburn.edu  Sun Jan 22 19:06:55 2006
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Sun, 22 Jan 2006 12:06:55 -0600
Subject: [R] regression with nestedness
Message-ID: <43D3755F020000F200005EBD@TMIA1.AUBURN.EDU>

Dear R-users,

I set up an experiment where I put up bluebird boxes across an
urbanization gradient.  I monitored these boxes and at some point I
pulled a feather from a chick and a friend used spectral properties
(rtot, a continuous var) to index chick health.  There is an effect of
sex that I would like to include but how would I set up a regression and
look at the effect of urbanization (purban, a continuous var)) on
feather properties of chicks within boxes.  

So the model should look something like rtot = sex + purban +
(chick)clutch

Also, when I plot purban against rtot using the plot function I get
boxplots but I would like to ignore the clutch and just plot each point.
 I've tried type = "p" but this has no effect.  

Thanks,

Jeff

 

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From Ted.Harding at nessie.mcc.ac.uk  Sun Jan 22 19:27:22 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 22 Jan 2006 18:27:22 -0000 (GMT)
Subject: [R] Solve for x in Ax=B with vectors, not matrices
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF35CAC@lbmsg002.fbn-nbf.local>
Message-ID: <XFMail.060122182722.Ted.Harding@nessie.mcc.ac.uk>

On 22-Jan-06 Lapointe, Pierre wrote:
> Hello R-helpers,
> 
> What I have: I am working with vectors not matrice:
> 
>#Basic equations
> A <-c(-20,-9,-2)
> x <-c(0.17,0.22,0.61)
> B <- crossprod(A,x)
> 
># R matrix multiplication works with vectors
> A%*%x==B      # Is true...
> 
> Question: If x is unknown and A and B are known,
> how do I solve for x in R?
> solve(A,B) won't work because A is not a square matrix 
> 
> solve(A,B)
> Error in solve.default(A, B) : 'A' (3 x 1) must be square
> 
> I understand that I might have many solutions but note
> that the sum of x is 1 and all x are positive (x are
> weightings in % of the total).

For the example you have given, in "classical" vector
algebra notation the equation is

  A.x = B   [ = -6.6 in this case ]

where A and x are two vectors.

Note the explanataion resulting from ?"%*%"

     Multiplies two matrices, if they are conformable.
     If one argument is a vector, it will be coerced
     to a either a row or column matrix to make the
     two arguments conformable. If both are vectors it
     will return the inner product.

If this is the interpretation you intend, and if the above
is a typical problem of yours, then if you divide by the
"lengths" of A and x you will get an equation

  V.y = cos(u)

where V (corresponding to A) and y (corresponding to x)
are unit vectors, and cos(u) corresponds to B.

Now you want to find solutions y from this equation.
You are in the first instance looking for all vectors
y which are at a fixed angle u in (0,pi) to the vector
V (the "elevation", if you like), which you can find by
choosing another angle v (the "azimuth", say) arbitrarily
in (0,2*pi). All such angles give a solution of V.y = cos(u),
and the endpoints of the vectors describe a circle.

You can get back to x by re-scaling, and now you want
the solutions such that the sum of the elements is 1
(which defines a plane). There are either two of these
(where the circle cuts the plane, or infinitely many
(if the circle in question lies in the plane).

Does this outline point in s useful direction?

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Jan-06                                       Time: 18:27:18
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Sun Jan 22 21:52:35 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 22 Jan 2006 12:52:35 -0800
Subject: [R] lme model specification
In-Reply-To: <1137525003.31018.3.camel@localhost.localdomain>
References: <1137525003.31018.3.camel@localhost.localdomain>
Message-ID: <43D3F093.5060809@pdf.com>

	  If you are now presenting subjects with pairs, have you considered 
multidimensional scaling and possibly "Bradley-Terry" models & 
extensions?  RSiteSearch("multidimensional scaling") and 
RSiteSearch("Bradley-Terry") both seemed to contain potentially useful 
information.

	  best wishes,
	  spencer graves

#################
Bill Simpson wrote:
 > Thanks very much Spencer for your helpful reply.
 >
 > On Fri, 2006-01-20 at 07:38 -0800, Spencer Graves wrote:
 >
 >>	  Does each subject get only one LED per session or all 4 LEDs?
 >
 > I simplified a bit. Each subject gets pairs of LEDs and needs to
 > estimate the distance between them. LED1 can be Red or Blue, and LED2
 > can be R or B -- so there are 4 combos. These are presented in random
 > order. So in one session yes each subject sees all combos of LEDs.
 >
 >
 >>  This
 >>should be important regarding which models are estimaable.  In either
 >>case, might the following help you?
 >>
 >>nSubj <- 8
 >>nSess <- 4
 >>nObsPerSess <- 3
 >>
 >>library(nlme)
 >>library(e1071)
 >>P4 <- permutations(4)
 >>
 >>LED <- letters[t(P4[permSubj,])]
 >>
 >>set.seed(1)
 >>permSubj <- sample(24, nSubj)
 >>N <- nSubj*nSess*nObsPerSess
 >>DF <- data.frame(
 >>   Subject=rep(1:nSubj, each=nSess*nObsPerSess),
 >>   illum=rep(c("star", "moon"), each=N/2),
 >>   feedback=rep(c("yes", "no"), each=N/4, length=N),
 >>   session=rep(1:nSess, each=nObsPerSess, nSubj),
 >>   LED=rep(LED, each=nObsPerSess),
 >>   Rep=rep(1:nObsPerSess, nSess*nSubj),
 >>   logdistance=rep(1:nObsPerSess, nSess*nSubj),
 >>   logestimate=rnorm(nSubj*nSess*nObsPerSess) )
 >>
 >>fit <- lme(logestimate~logdistance*illum*feedback+LED,
 >>   random=~1|Subject,
 >>   correlation=corAR1(form=~Rep|Subject/session),
 >>   data=DF)
 >
 > Thanks very much, I didn't know about the corAR1 statement.
 >
 > Best wishes
 > Bill



From p.murrell at auckland.ac.nz  Sun Jan 22 22:51:26 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 23 Jan 2006 10:51:26 +1300
Subject: [R] change lattice panel background color
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0356456D@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0356456D@pnlmse35.pnl.gov>
Message-ID: <43D3FE5E.5020908@stat.auckland.ac.nz>

Hi


Waichler, Scott R wrote:
> I can't find a way to change just the panel background color in lattice.
> I would like NA regions in levelplot() to appear black.  I've tried the
> trellis.par.set() stuff, but it it makes the background of the whole
> graphic black.


Here's an approach using a simple custom panel function (mind the wrap)...

# Slight modification of example from ?levelplot
require(stats)
attach(environmental)
ozo.m <- loess((ozone^(1/3)) ~ wind * temperature * radiation,
                parametric = c("radiation", "wind"), span = 1, degree = 2)
w.marginal <- seq(min(wind), max(wind), length = 50)
t.marginal <- seq(min(temperature), max(temperature), length = 50)
r.marginal <- seq(min(radiation), max(radiation), length = 4)
wtr.marginal <- list(wind = w.marginal, temperature = t.marginal,
                      radiation = r.marginal)
grid <- expand.grid(wtr.marginal)
grid[, "fit"] <- c(predict(ozo.m, grid))
grid[, "fit"][sample(1:10000, 100)] <- NA
library(grid)
levelplot(fit ~ wind * temperature | radiation, data = grid,
           cuts = 10, region = TRUE,
           xlab = "Wind Speed (mph)",
           ylab = "Temperature (F)",
           main = "Cube Root Ozone (cube root ppb)",
           panel=function(...) {
               grid.rect(gp=gpar(col=NA, fill="black"))
               panel.levelplot(...)
           })


Paul


> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at _ pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From pau.carre at gmail.com  Sun Jan 22 23:02:05 2006
From: pau.carre at gmail.com (pau carre)
Date: Sun, 22 Jan 2006 23:02:05 +0100
Subject: [R] Execute a list of R instructions from a file
Message-ID: <4b7300ee0601221402j5a6f75e5v@mail.gmail.com>

Hello, I want to do the following: First, generate an R code in a file
from an external program, then send the name of the file to R using
sockets, finally let R to open that file and to execute the R code.
I just wanted to know if there is any function in R that reads a file
an executes the R code inside it.

Something like:
file.txt:
state <- c("tas", "sa",  "qld", "nsw", "nsw", "nt",  "wa",  "wa", "qld", "vic")

R execution:
>readAndExecute(file.txt)

Thanks
Pau.



From csardi at rmki.kfki.hu  Sun Jan 22 23:13:37 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Sun, 22 Jan 2006 23:13:37 +0100
Subject: [R] Execute a list of R instructions from a file
In-Reply-To: <4b7300ee0601221402j5a6f75e5v@mail.gmail.com>
References: <4b7300ee0601221402j5a6f75e5v@mail.gmail.com>
Message-ID: <20060122221337.GB21332@localhost.localdomain>

source("file.txt")

or even:

source("http://my.server.com/my/r/commands")

Gabor

On Sun, Jan 22, 2006 at 11:02:05PM +0100, pau carre wrote:
> Hello, I want to do the following: First, generate an R code in a file
> from an external program, then send the name of the file to R using
> sockets, finally let R to open that file and to execute the R code.
> I just wanted to know if there is any function in R that reads a file
> an executes the R code inside it.
> 
> Something like:
> file.txt:
> state <- c("tas", "sa",  "qld", "nsw", "nsw", "nt",  "wa",  "wa", "qld", "vic")
> 
> R execution:
> >readAndExecute(file.txt)
> 
> Thanks
> Pau.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From ckjmaner at carolina.rr.com  Mon Jan 23 00:38:23 2006
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 22 Jan 2006 18:38:23 -0500
Subject: [R] Making a markov transition matrix
Message-ID: <028c01c61fac$efd97640$0401000a@Athena>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060122/31b96264/attachment.pl

From Soren.Hojsgaard at agrsci.dk  Mon Jan 23 00:57:17 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Mon, 23 Jan 2006 00:57:17 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
	that version is not available on CRAN
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
	<43D24C13.90302@statistik.uni-dortmund.de>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387815D@DJFPOST01.djf.agrsci.dk>
	<43D25332.5040806@statistik.uni-dortmund.de>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387815E@DJFPOST01.djf.agrsci.dk>

I see that Matrix, lme4 and SASmixed in zip format have been 'rolled back' to the previous versions - which I have now installed. Running 
 
library(SASmixed)
data(Demand)
example(Demand)
 
gives
....
 
Demand> fm1Demand <- lmer(log(d) ~ log(y) + log(rd) + log(rt) + 
    log(rs) + (1 | State) + (1 | Year), Demand)
Error: object "d" not found
Error in log(d) : unable to find the argument 'x' in selecting a method for function 'log'

- so it is not a complete 'roll back' - unless I've misunderstood something? Help will be appreciated...
 
/ S??ren

 

[...]


Yes, but implementing this would require an even higher computational
effort to build and check Windows binary packages.

Since it looks like the Matrix problems are persisting, I will play back
the "old" versions of both packages to CRAN shortly.

Uwe



> Anyway, that sort of things happen... I've now downloaded the
> previous version of lme4 as a tar.gz file - and that works. I wonder
> if it would be an idea to have a function rollback('pkg-name') for
> that sort of situations???
>
> Best regards
>
> S??ren H??jsgaard
>
>
>
>
>
> Moreover, now that I have updated to the latest release of lme4, I
> wonder how to get get a version of lme4 that actually works with the
> version of Matrix which is currently available.
>
>
>
>
>
>



From h.wickham at gmail.com  Mon Jan 23 04:42:24 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 22 Jan 2006 21:42:24 -0600
Subject: [R] formatC slow? (or how can I make this function faster?
Message-ID: <f8e6ff050601221942u20d3c475l6ef1fc7d39df9923@mail.gmail.com>

I'm trying to convert a matrix of capture occasions to format that an
external program can read.  The job is to basically take a row of
matrix, like

> smp[1,]
 [1] 1 1 0 1 1 1 0 0 0 0

and convert it to the equivalent string "1101110000"

I'm having problems doing this in a speedy way.  The simplest solution
(calc_history below, using apply, paste and collapse) takes about 2
seconds for a 10,000 x 10 matrix.   I thought perhaps paste might be
building up the string in an efficient manner, so I tried using matrix
multiplication and formatC (as in calc_history2).  This is about 25%
faster, but still seems slow.

smp <- matrix(rbinom(100000, 1, 0.5), nrow=10000)

calc_history <- function(smp) {
	apply(smp, 1, paste, collapse="")
}

calc_history <- function(smp) {
	mul <- 10 ^ ((ncol(smp)-1):0)
	as.vector(formatC(smp %*% mul, format="d", width=ncol(smp), flag=0))
}

system.time(calc_history(smp))
system.time(calc_history2(smp))

Any ideas for improvement?

Thanks,

Hadley



From j.logsdon at quantex-research.com  Mon Jan 23 06:52:46 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 23 Jan 2006 05:52:46 +0000 (GMT)
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.10.10601230550550.27402-100000@quantex-research.co.uk>

I've just hit this problem as well and as I am slumming it on XP at the
moment, I don't have the compilation tools to use the .tar.gz version.

I also notice that the numbering format has changed.  Is this intentional?

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Sat, 21 Jan 2006, S??ren H??jsgaard wrote:

> I just updated the packages on my pc (windows xp). When loading lme4 I get
>  
>  > library(lme4)
> Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'
> 
> 'Matrix'  0.995-1 is indeed installed on my computer, but update.packages() does not capture a never version; and seemingly for good reasons: When looking at CRAN, the new version of Matrix is available - but only as a .tar.gz archive. Two questions
>  
> 1) How can a new version of lme4 make its way to CRAN when the packages it depends on are not available (that is, in a zip-version)?
>  
> An answer could be: Buhh, you can just download the .tar.gz archive and then compile it yourself! Surely, that would be possible, but it can not be the intention...
>  
> 2) When actually trying to compile Matrix, I get
>  
> lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
> make[3]: *** [Matrix.dll] Error 1
> make[2]: *** [srcDynlib] Error 2
> make[1]: *** [all] Error 2
> make: *** [pkg-Matrix] Error 2
> *** Installation of Matrix failed ***
> Removing 'c:/programs/R/current/library/Matrix'
> Restoring previous 'c:/programs/R/current/library/Matrix'
>  
> Can anyone help me on what to do???
> Thanks / S??ren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From nsahoo at gmail.com  Mon Jan 23 07:52:53 2006
From: nsahoo at gmail.com (Nachiketa Sahoo)
Date: Mon, 23 Jan 2006 01:52:53 -0500
Subject: [R] will vectorization help in this case?
Message-ID: <62e8fe7d0601222252q398ed6f7q7570886d673d1b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/602e4ba9/attachment.pl

From nsahoo at gmail.com  Mon Jan 23 07:57:12 2006
From: nsahoo at gmail.com (Nachiketa Sahoo)
Date: Mon, 23 Jan 2006 01:57:12 -0500
Subject: [R] Fwd: will vectorization help in this case?
In-Reply-To: <62e8fe7d0601222252q398ed6f7q7570886d673d1b4@mail.gmail.com>
References: <62e8fe7d0601222252q398ed6f7q7570886d673d1b4@mail.gmail.com>
Message-ID: <62e8fe7d0601222257q66882d5ch313eec132cbcc6e9@mail.gmail.com>

Hi all,

The previous post had some html formatting and did not go through. So,
here it is again in plain text.

I am doing a array marginalization over one dimension in the following manner:
===========================================================
for(l in 1:L)
{
   flst=list(Pzu, Pzi, array(Puzu[,as.character(r[[l,"u"]])],
dim=dim(Puzu)[1], dimnames=dimnames(Puzu)[1]),
array(Pizi[,as.character(r[[l, "i"]])], dim=dim(Pizi)[1],
dimnames=dimnames(Pizi)[1]) );
    tempa = 1;
    for(j in 1:length(flst)) tempa = tempa %X% flst[[j]];

    tempPzu = apply(tempa, 1, sum);
    tPzu = tPzu + tempPzu;
    tPuzu[,as.character(r[[l,"u"]])] = tPuzu[,as.character
(r[[l,"u"]])] + tempPzu;

    tempPzi = apply(tempa, 2, sum);
    tPzi = tPzi + tempPzi;
    tPizi[,as.character(r[[l,"i"]])] =
tPizi[,as.character(r[[l,"i"]])] + tempPzi;

    for(j in 1:ncomp) tPrjzuzilst[[j]][,,r[[l,rbase+j]]] =
tPrjzuzilst[[j]][,,r[[l,rbase+j]]] + tempa;
}
===========================================================
My question involves the operation using  tempPzu and tempPzi. Is my
use of loop in this case to addup certain arrays together, too ..
dumb? Is there another way of doing such things by vectorization?

Thanks in advance,
Nachi



From ripley at stats.ox.ac.uk  Mon Jan 23 08:02:14 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jan 2006 07:02:14 +0000 (GMT)
Subject: [R] NAs in optim
In-Reply-To: <3d35a2ca0601220937x6a1b2420p3b7d3ee55185105f@mail.gmail.com>
References: <3d35a2ca0601220937x6a1b2420p3b7d3ee55185105f@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601230657480.30181@gannet.stats>

This is NAs in your own code, not in optim!  Please use a truthful 
subject.

Consider the use of sum(na.rm=TRUE) rather than your inner loop.

On Sun, 22 Jan 2006, Hannu Kahra wrote:

> Hi,
>
> I am trying to maximize a utility function using optim. I have a problem
> with optim, since some of the values in the caw, mom, me and btm matrixes in
> the code bellow are missing. Is there a handy way just to skip the missing
> values in the loop?
>
> g <- 5
> retp <- NULL
> object <- function (x)
> {
>    b1 <- x[1]
>    b2 <- x[2]
>    b3 <- x[3]
>    for(i in 1:nrow(ret)){
>        for(j in 1:ncol(ret)){
>        retp[i] <- (caw[i,j]+1/24*(b1*mom[i,j] + b2*me[i,j] +
> b3*btm[i,j]))*ret[i,j]
>        }
>    }
>    util <- (1+retp)^(1-g)/(1-g)
>    return(-sum(util))
> }
> result <- optim(c(2.0,-1.0,3.5
> ),object,control=list(maxit=500),hessian=TRUE,method="BFGS")
>
> Thank you in advance.
> Hannu Kahra
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan 23 08:44:15 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jan 2006 07:44:15 +0000 (GMT)
Subject: [R] formatC slow? (or how can I make this function faster?
In-Reply-To: <f8e6ff050601221942u20d3c475l6ef1fc7d39df9923@mail.gmail.com>
References: <f8e6ff050601221942u20d3c475l6ef1fc7d39df9923@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601230710500.30181@gannet.stats>

First, your timings seem slow: even my laptop is using 0.4 secs.  So the 
simple solution is to use a better computer.

I would just write such things in C.  The following runs in 0.01sec on my 
machine (timed by looping over it)

system.time(.Call("Cpaste", smp))

using

#include <R.h>
#include <Rinternals.h>

SEXP Cpaste(SEXP A)
{
     SEXP dims, ans;
     double *rA = REAL(A);
     int i, j, nr, nc;
     char buf[100], one[] = "1", zero[] = "0";

     dims = getAttrib(A, R_DimSymbol);
     nr = INTEGER(dims)[0]; nc = INTEGER(dims)[1];
     ans = allocVector(STRSXP, nr);
     for(i = 0; i < nr; i ++) {
 	buf[0] = '\0';
 	for(j = 0; j < nc; j++) strcat(buf, rA[i + nr*j] > 0 ? one : zero);
 	SET_STRING_ELT(ans, i, mkChar(buf));
     }
     return ans;
}

and perhaps that could be made more efficient by avoiding strcat but I 
would expect mkChar to be taking much of the time.


On Sun, 22 Jan 2006, hadley wickham wrote:

> I'm trying to convert a matrix of capture occasions to format that an
> external program can read.  The job is to basically take a row of
> matrix, like
>
>> smp[1,]
> [1] 1 1 0 1 1 1 0 0 0 0
>
> and convert it to the equivalent string "1101110000"
>
> I'm having problems doing this in a speedy way.  The simplest solution
> (calc_history below, using apply, paste and collapse) takes about 2
> seconds for a 10,000 x 10 matrix.   I thought perhaps paste might be
> building up the string in an efficient manner, so I tried using matrix
> multiplication and formatC (as in calc_history2).  This is about 25%
> faster, but still seems slow.
>
> smp <- matrix(rbinom(100000, 1, 0.5), nrow=10000)
>
> calc_history <- function(smp) {
> 	apply(smp, 1, paste, collapse="")
> }
>
> calc_history <- function(smp) {
> 	mul <- 10 ^ ((ncol(smp)-1):0)
> 	as.vector(formatC(smp %*% mul, format="d", width=ncol(smp), flag=0))
> }
>
> system.time(calc_history(smp))
> system.time(calc_history2(smp))
>
> Any ideas for improvement?
>
> Thanks,
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jan 23 08:51:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jan 2006 08:51:58 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387815E@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<x264od3gbw.fsf@turmalin.kubism.ku.dk>
	<43D24C13.90302@statistik.uni-dortmund.de>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387815D@DJFPOST01.djf.agrsci.dk>
	<43D25332.5040806@statistik.uni-dortmund.de>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387815E@DJFPOST01.djf.agrsci.dk>
Message-ID: <43D48B1E.4040801@statistik.uni-dortmund.de>

S??ren H??jsgaard wrote:

> I see that Matrix, lme4 and SASmixed in zip format have been 'rolled back' to the previous versions - which I have now installed. Running 
>  
> library(SASmixed)
> data(Demand)
> example(Demand)
>  
> gives
> ....
>  
> Demand> fm1Demand <- lmer(log(d) ~ log(y) + log(rd) + log(rt) + 
>     log(rs) + (1 | State) + (1 | Year), Demand)
> Error: object "d" not found
> Error in log(d) : unable to find the argument 'x' in selecting a method for function 'log'
> 
> - so it is not a complete 'roll back' - unless I've misunderstood something? Help will be appreciated...

Yes, I rolled back one version. This is another error in the former 
version. But since Matrix 0.995-3 is on its way to CRAN, there are 
hopefully completely working versions available within 24 hours.

Uwe Ligges


> / S??ren
> 
>  
> 
> [...]
> 
> 
> Yes, but implementing this would require an even higher computational
> effort to build and check Windows binary packages.
> 
> Since it looks like the Matrix problems are persisting, I will play back
> the "old" versions of both packages to CRAN shortly.
> 
> Uwe
> 
> 
> 
> 
>>Anyway, that sort of things happen... I've now downloaded the
>>previous version of lme4 as a tar.gz file - and that works. I wonder
>>if it would be an idea to have a function rollback('pkg-name') for
>>that sort of situations???
>>
>>Best regards
>>
>>S??ren H??jsgaard
>>
>>
>>
>>
>>
>>Moreover, now that I have updated to the latest release of lme4, I
>>wonder how to get get a version of lme4 that actually works with the
>>version of Matrix which is currently available.
>>
>>
>>
>>
>>
>>
> 
> 
>



From hkahra at gmail.com  Mon Jan 23 08:57:37 2006
From: hkahra at gmail.com (Hannu Kahra)
Date: Mon, 23 Jan 2006 09:57:37 +0200
Subject: [R] NAs in optim
In-Reply-To: <Pine.LNX.4.61.0601230657480.30181@gannet.stats>
References: <3d35a2ca0601220937x6a1b2420p3b7d3ee55185105f@mail.gmail.com>
	<Pine.LNX.4.61.0601230657480.30181@gannet.stats>
Message-ID: <3d35a2ca0601222357t6d6f9115h765618464d69a4e1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/0d2c2f49/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jan 23 09:04:26 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jan 2006 09:04:26 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <Pine.LNX.4.10.10601230550550.27402-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10601230550550.27402-100000@quantex-research.co.uk>
Message-ID: <43D48E0A.9030408@statistik.uni-dortmund.de>

John Logsdon wrote:

> I've just hit this problem as well and as I am slumming it on XP at the
> moment, I don't have the compilation tools to use the .tar.gz version.

So please follow that thread and note that people were already working 
on it for you during the whole weekend!
In this case, it does not help to have the tools. The Windows binary 
package maintainer has had his reasons not to publish the most recent 
Matrix version.

> I also notice that the numbering format has changed.  Is this intentional?

Yes.

Uwe Ligges


> Best wishes
> 
> John
> 
> John Logsdon                               "Try to make things as simple
> Quantex Research Ltd, Manchester UK         as possible but not simpler"
> j.logsdon at quantex-research.com              a.einstein at relativity.org
> +44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com
> 
> 
> On Sat, 21 Jan 2006, S??ren H??jsgaard wrote:
> 
> 
>>I just updated the packages on my pc (windows xp). When loading lme4 I get
>> 
>> > library(lme4)
>>Error: package 'Matrix' 0.995-1 was found, but >= 0.995.2 is required by 'lme4'
>>
>>'Matrix'  0.995-1 is indeed installed on my computer, but update.packages() does not capture a never version; and seemingly for good reasons: When looking at CRAN, the new version of Matrix is available - but only as a .tar.gz archive. Two questions
>> 
>>1) How can a new version of lme4 make its way to CRAN when the packages it depends on are not available (that is, in a zip-version)?
>> 
>>An answer could be: Buhh, you can just download the .tar.gz archive and then compile it yourself! Surely, that would be possible, but it can not be the intention...
>> 
>>2) When actually trying to compile Matrix, I get
>> 
>>lmer.o(.text+0x285f):lmer.c: undefined reference to `atanh'
>>make[3]: *** [Matrix.dll] Error 1
>>make[2]: *** [srcDynlib] Error 2
>>make[1]: *** [all] Error 2
>>make: *** [pkg-Matrix] Error 2
>>*** Installation of Matrix failed ***
>>Removing 'c:/programs/R/current/library/Matrix'
>>Restoring previous 'c:/programs/R/current/library/Matrix'
>> 
>>Can anyone help me on what to do???
>>Thanks / S??ren
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From detlef.steuer at hsu-hamburg.de  Mon Jan 23 09:06:35 2006
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Mon, 23 Jan 2006 09:06:35 +0100
Subject: [R] Run R in background?
In-Reply-To: <20060120215305.44108.qmail@web30902.mail.mud.yahoo.com>
References: <20060120215305.44108.qmail@web30902.mail.mud.yahoo.com>
Message-ID: <20060123090635.b4c06c3f.detlef.steuer@hsu-hamburg.de>

Hi,

look for BATCH in the manual. 
(The other friend is called "nohup" on unix)

Detlef

On Fri, 20 Jan 2006 13:53:05 -0800 (PST)
luk <luk111111 at yahoo.com> wrote:

> According to the manual, a R script can be run in as follows in linux.
>    
>   $ R
>   > source("xy.R")
>    
>   Does this mean we have to type "R" first, and then within R, type: source("xy.R")?
>    
>   Is there other way to run xy.R, say 
>    
>   $ R CMD SOURCE xy.R
>    
>   Is there any way to run R in background?
>    
>   Luk
>    
> 
> 		
> ---------------------------------
> 
>  Photo Books. You design it and well bind it!
> 	[[alternative HTML version deleted]]
> 
>



From finbref.2006 at gmail.com  Mon Jan 23 10:07:02 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Mon, 23 Jan 2006 10:07:02 +0100
Subject: [R] chm.help in windows
Message-ID: <d0f55a670601230107n2c542708w@mail.gmail.com>

options(chmhelp=TRUE)
help(package=fCalendar)

does not open teh windows help browser, but

help(CalendarData, package=fCalendar)

does. Why? A bug?
I use R 2.1.1 under Windows2000
Thomas



From david.meyer at wu-wien.ac.at  Fri Jan 20 04:03:42 2006
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 20 Jan 2006 04:03:42 +0100
Subject: [R]   Help with plot.svm from e1071
Message-ID: <20060120040342.696fbf84.david.meyer@wu-wien.ac.at>

Josh,

the problem here is that your code and mine refer to "x" and
non-standard evaluation happens in points(), looking up "x" in the
object supplied to "data". So your code will work when you are using,
e.g., "xx" instead of "x" in the data frame and the call to svm(). I
will fix this ASAP, thanks for pointing this out...

Cheers,

David.

----------

Hi.

I'm trying to plot a pair of intertwined spirals and an svm that
separates them. I'm having some trouble. Here's what I tried.

> library(mlbench)
> library(e1071)
Loading required package: class
> raw <- mlbench.spirals(200,2)
> spiral <- data.frame(class=as.factor(raw$classes), x=raw$x[,1], y=raw$x[,2])
> m <- svm(class~., data=spiral)
> plot(m, spiral)
Error in -x$index : invalid argument to unary operator

So we delve into e1071:::plot.svm. When I run the code in plot.svm
everything is fine up until
 points(formula, data = data[-x$index, ], pch = dataSymbol,
                 col = symbolPalette[colind[-x$index]])
That gives me the same error message, "Error in -x$index : invalid
argument to unary operator". The weird thing is that I can run either
of the those statements in isolation
data[-x$index, ]
symbolPalette[colind[-x$index]]
and neither gives me an error. I looked in the two points functions I


can see (points.default and points.formula) but neither calls x$index.

I was following along the documentation for plot.svm, which has a
simple example (that works)
    ## a simple example
    library(MASS)
    data(cats)
    m <- svm(Sex~., data = cats)
    plot(m, cats)

I don't see what the difference between their example and mine.


-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From ccatj at web.de  Mon Jan 23 10:16:30 2006
From: ccatj at web.de (Christian Jones)
Date: Mon, 23 Jan 2006 10:16:30 +0100
Subject: [R] FW:  aggregating variables with pca
Message-ID: <468475878@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/ccb5fcda/attachment.pl

From hb at maths.lth.se  Mon Jan 23 10:21:43 2006
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 23 Jan 2006 20:21:43 +1100
Subject: [R] chm.help in windows
In-Reply-To: <d0f55a670601230107n2c542708w@mail.gmail.com>
References: <d0f55a670601230107n2c542708w@mail.gmail.com>
Message-ID: <43D4A027.2010609@maths.lth.se>

Same in R v2.2.1 patched (2006-01-01 r36947) and Rv2.3.0 (2006-01-01 
r36947). /Henrik

Thomas Steiner wrote:
> options(chmhelp=TRUE)
> help(package=fCalendar)
> 
> does not open teh windows help browser, but
> 
> help(CalendarData, package=fCalendar)
> 
> does. Why? A bug?
> I use R 2.1.1 under Windows2000
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From I.Visser at uva.nl  Mon Jan 23 10:45:57 2006
From: I.Visser at uva.nl (Ingmar Visser)
Date: Mon, 23 Jan 2006 10:45:57 +0100
Subject: [R] Making a markov transition matrix
In-Reply-To: <028c01c61fac$efd97640$0401000a@Athena>
Message-ID: <BFFA6465.C7F2%I.Visser@uva.nl>

Ajay,
On a similar note, there are two other packages as well for similar models:
hmm.discnp for hidden Markov models of univariate discrete non-parametric
distributions and depmix for hidden Markov models for multivariate data, ie
gaussians, multinomials etc, and combinations of these
hth, ingmar

> 
> Ajay--you seem to have gotten your question answered regarding putting your
> dataframe in the correct format, etc.  If you haven't already, you might
> want to check out the MSM package for multi-state Markov and hidden Markov
> models in continuous time.  It's been quite useful for some of my work
> regarding estimating Markov chains/matrices and is actively maintained.
> 
> 
> Thanks,
> Charles
> 
> <<< you wrote >>>
> 
> Folks,
> 
> I am holding a dataset where firms are observed for a fixed (and
> small) set of years. The data is in "long" format - one record for one
> firm for one point in time. A state variable is observed (a factor).
> 
> I wish to make a markov transition matrix about the time-series
> evolution of that state variable. The code below does this. But it's
> hardcoded to the specific years that I observe. How might one
> generalise this and make a general function which does this? :-)
> 
>          -ans.
> 
> 
> 
> set.seed(1001)
> 
> # Raw data in long format --
> raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
>                 year=c(83,   84,  85,  86,  83,  84,  85,  86),
>                 state=sample(1:3, 8, replace=TRUE)
>                 )
> # Shift to wide format --
> fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
>                  direction="wide")
> # Now tediously build up records for an intermediate data structure
> try <- rbind(
>            data.frame(prev=fixedup$state.83, new=fixedup$state.84),
>            data.frame(prev=fixedup$state.84, new=fixedup$state.85),
>            data.frame(prev=fixedup$state.85, new=fixedup$state.86)
>            )
> # This is a bad method because it is hardcoded to the specific values
> # of "year".
> markov <- table(destination$prev.state, destination$new.state)
> 
> -- 
> Ajay Shah                                      http://www.mayin.org/ajayshah
> 
> ajayshah at mayin.org
> http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Mon Jan 23 10:55:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jan 2006 10:55:58 +0100
Subject: [R] chm.help in windows
In-Reply-To: <43D4A027.2010609@maths.lth.se>
References: <d0f55a670601230107n2c542708w@mail.gmail.com>
	<43D4A027.2010609@maths.lth.se>
Message-ID: <43D4A82E.2070800@statistik.uni-dortmund.de>

Henrik Bengtsson wrote:

> Same in R v2.2.1 patched (2006-01-01 r36947) and Rv2.3.0 (2006-01-01 
> r36947). /Henrik
> 
> Thomas Steiner wrote:
> 
>>options(chmhelp=TRUE)
>>help(package=fCalendar)
>>
>>does not open teh windows help browser, but
>>
>>help(CalendarData, package=fCalendar)
>>
>>does. Why? A bug?

Because help on a *package* (help(package=fCalendar)) shows a text 
representation containing well formatted information from the files 
DESCRIPTION and Index of a package. This is not converted to ANY other 
help format.

Help on a *function* (help(CalendarData, package=fCalendar)) CAN be 
converted to chm, and this happens for CRAN binary packages for Windows.

Uwe Ligges



>>I use R 2.1.1 under Windows2000
>>Thomas
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jan 23 11:04:19 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jan 2006 10:04:19 +0000 (GMT)
Subject: [R] chm.help in windows
In-Reply-To: <43D4A027.2010609@maths.lth.se>
References: <d0f55a670601230107n2c542708w@mail.gmail.com>
	<43D4A027.2010609@maths.lth.se>
Message-ID: <Pine.LNX.4.61.0601230955540.10706@gannet.stats>

Yes, and as documented.  `topic' is not an optional argument to help, so
help(package=) just gives you a hint about what you could have supplied.

On Mon, 23 Jan 2006, Henrik Bengtsson wrote:

> Same in R v2.2.1 patched (2006-01-01 r36947) and Rv2.3.0 (2006-01-01
> r36947). /Henrik
>
> Thomas Steiner wrote:
>> options(chmhelp=TRUE)
>> help(package=fCalendar)
>>
>> does not open teh windows help browser, but
                  ^^^ ?
I have no idea what the `windows help browser' is, and it is not a term
mentioned on the R help page.

>> help(CalendarData, package=fCalendar)
>>
>> does. Why? A bug?
>> I use R 2.1.1 under Windows2000
>> Thomas

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From D.Vonka at UvT.nl  Mon Jan 23 11:05:12 2006
From: D.Vonka at UvT.nl (David Vonka)
Date: Mon, 23 Jan 2006 11:05:12 +0100
Subject: [R] R-2.2.1 doesn't compile under cygwin/Win2000
Message-ID: <PNEPKDPOKLJPJBNFAPHJGEADCAAA.D.Vonka@UvT.nl>

Hello,

I've just downloaded the R-2.2.1 source and I'm trying to
compile it under cygwin (1.5.19-4). The configure script runs
ok, but make generates the following error after a while:

----------------------------------------------------------------------------
-----
al/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c zutil.c -o zutil.o
rm -f libz.a
ar cr libz.a adler32.o compress.o crc32.o deflate.o gzio.o infback.o
inffast.o inflate.o inftrees.o trees.o uncompr.o zutil.o
ranlib libz.a
make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/zlib'
make[3]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/zlib'
make[3]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
make[4]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
making xdr.d from xdr.c
making xdr_float.d from xdr_float.c
making xdr_mem.d from xdr_mem.c
making xdr_stdio.d from xdr_stdio.c
make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
make[4]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
gcc -I. -I. -I../../../src/include -I../../../src/include -I/usr/local/inclu
de -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c xdr.c -o xdr.o
In file included from xdr.c:59:
./rpc/types.h:63: error: conflicting types for 'malloc'
./rpc/types.h:63: error: conflicting types for 'malloc'
make[4]: *** [xdr.o] Error 1
make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
make[3]: *** [R] Error 2
make[3]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/Vonkad/R-2.2.1/src'
make: *** [R] Error 1
--------------------------------------------------------------------------

What could be the cause ?

Regards and thanks,
David Vonka



From szlevine at nana.co.il  Mon Jan 23 11:04:36 2006
From: szlevine at nana.co.il (Stephen)
Date: Mon, 23 Jan 2006 12:04:36 +0200
Subject: [R] Trees
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD67E@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/f438c0c3/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jan 23 11:26:27 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jan 2006 11:26:27 +0100
Subject: [R] R-2.2.1 doesn't compile under cygwin/Win2000
In-Reply-To: <PNEPKDPOKLJPJBNFAPHJGEADCAAA.D.Vonka@UvT.nl>
References: <PNEPKDPOKLJPJBNFAPHJGEADCAAA.D.Vonka@UvT.nl>
Message-ID: <43D4AF53.6090607@statistik.uni-dortmund.de>

David Vonka wrote:

> Hello,
> 
> I've just downloaded the R-2.2.1 source and I'm trying to
> compile it under cygwin (1.5.19-4). The configure script runs
> ok, but make generates the following error after a while:
> 
> ----------------------------------------------------------------------------
> -----
> al/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c zutil.c -o zutil.o
> rm -f libz.a
> ar cr libz.a adler32.o compress.o crc32.o deflate.o gzio.o infback.o
> inffast.o inflate.o inftrees.o trees.o uncompr.o zutil.o
> ranlib libz.a
> make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/zlib'
> make[3]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/zlib'
> make[3]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> make[4]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> making xdr.d from xdr.c
> making xdr_float.d from xdr_float.c
> making xdr_mem.d from xdr_mem.c
> making xdr_stdio.d from xdr_stdio.c
> make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> make[4]: Entering directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> gcc -I. -I. -I../../../src/include -I../../../src/include -I/usr/local/inclu
> de -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c xdr.c -o xdr.o
> In file included from xdr.c:59:
> ./rpc/types.h:63: error: conflicting types for 'malloc'
> ./rpc/types.h:63: error: conflicting types for 'malloc'
> make[4]: *** [xdr.o] Error 1
> make[4]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra/xdr'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/home/Vonkad/R-2.2.1/src/extra'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home/Vonkad/R-2.2.1/src'
> make: *** [R] Error 1
> --------------------------------------------------------------------------
> 
> What could be the cause ?

The cause is that you have not read the R Installation and 
Administration manual. It tells you which tools are required to build R 
under Windows. In particular, cygwin is unsupported.
For the user's convenience, a Windows binary version is also provided on 
CRAN mirrors.

Uwe Ligges



> Regards and thanks,
> David Vonka
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ajayshah at mayin.org  Mon Jan 23 11:58:18 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 23 Jan 2006 16:28:18 +0530
Subject: [R] Making a markov transition matrix - more progress
Message-ID: <20060123105818.GA12449@lubyanka.local>

I solved the problem in one more (and more elegant) way. So here's the
program again.

Where does R stand on the Anderson-Goodman test of 1957? I hunted
around and nobody seems to be doing this in R. Is it that there has
been much progress after 1957 and nobody uses it anymore?

# Problem statement:
#
# You are holding a dataset where firms are observed for a fixed
# (and small) set of years. The data is in "long" format - one
# record for one firm for one point in time. A state variable is
# observed (a factor).
# You wish to make a markov transition matrix about the time-series
# evolution of that state variable.

set.seed(1001)

# Raw data in long format --
raw <- data.frame(name=c("f1","f1","f1","f1","f2","f2","f2","f2"),
                  year=c(83,   84,  85,  86,  83,  84,  85,  86),
                  state=sample(1:3, 8, replace=TRUE)
                  )
# Shift to wide format --
fixedup <- reshape(raw, timevar="year", idvar="name", v.names="state",
                   direction="wide")
# Now tediously build up records for an intermediate data structure
tmp <- rbind(
             data.frame(prev=fixedup$state.83, new=fixedup$state.84),
             data.frame(prev=fixedup$state.84, new=fixedup$state.85),
             data.frame(prev=fixedup$state.85, new=fixedup$state.86)
             )
# This is a bad method because it is hardcoded to the specific values
# of "year".
markov <- table(tmp$prev, tmp$new)
markov

# Gabor's method --
transition.probabilities <- function(D, timevar="year",
                                     idvar="name", statevar="state") {
  stage1 <- merge(D, cbind(nextt=D[,timevar] + 1, D),
                  by.x=timevar, by.y="nextt")
  v1 <- paste(idvar,".x",sep="")
  v2 <- paste(idvar,".y",sep="")
  stage2 <- subset(stage1, stage1[,v1]==stage1[,v2])
  v1 <- paste(statevar,".x",sep="")
  v2 <- paste(statevar,".y",sep="")
  t(table(stage2[,v1], stage2[,v2]))
}

transition.probabilities(raw, timevar="year", idvar="name", statevar="state")

# The new and improved way --
library(msm)
statetable.msm(state, name, data=raw)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From buser at stat.math.ethz.ch  Mon Jan 23 13:25:33 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 23 Jan 2006 13:25:33 +0100
Subject: [R] linear contrasts with anova
In-Reply-To: <002b01c61c4c$cfb49b50$6402a8c0@tommasi9270>
References: <002b01c61c4c$cfb49b50$6402a8c0@tommasi9270>
Message-ID: <17364.52029.251153.507164@stat.math.ethz.ch>

Dear Marco

If you are interested in a comparison of a reference level
against each other level (in your case level 1 against level 2
and level 1 against level 3), you can use the summary.lm(),
since this is the default contrast (see ?contr.treatment)

ar <- data.frame(GROUP = factor(rep(1:3, each = 8)),
                 DIP = c(3.0, 3.0, 4.0, 5.0, 6.0, 7.0, 3.0, 2.0, 1.0, 6.0, 5.0,
                   7.0, 2.0, 3.0, 1.5, 1.7, 17.0, 12.0, 15.0, 16.0, 12.0, 23.0,
                   19.0, 21.0))


r.aov10 <- aov(DIP ~  GROUP, data = ar)
anova(r.aov10)
summary.lm(r.aov10)

As result you will get the comparison GROUP 2 against GROUP 1,
denoted by GROUP2 and the comparison GROUP 3 against GROUP 1,
denoted by GROUP3.

Be careful. In your example you include both GROUP and C1 or C2,
respectively in your model. This results in a over parameterized
model and you get a warning that not all coefficients have been
estimated, due to singularities.

It is possible to use other contrasts than contr.treatment,
contr.sum, contr.helmert, contr.poly, but then you have to
specify the correctly in the model.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Posta Univ. Cagliari writes:
 > I have some doubts about the validity of my procedure to estimeate linear contrasts ina a factorial design.
 > For sake of semplicity, let's imagine a one way ANOVA with three levels. I am interested to test the significance of the difference between the first and third level (called here contrast C1) and between the first and the seconda level (called here contrast C2). I used the following procedure:
 > 
 > 
 > ------------------- reading data from a text file -----------------------
 > 
 > > ar <-read.table("C:/Programmi/R/myworks/contrasti/cont1.txt",header=TRUE)
 > 
 > > ar
 > 
 >      CC GROUP
 > 
 > 1   3.0     0
 > 
 > 2   3.0     0
 > 
 > 3   4.0     0
 > 
 > 4   5.0     0
 > 
 > 5   6.0     0
 > 
 > 6   7.0     0
 > 
 > 7   3.0     0
 > 
 > 8   2.0     0
 > 
 > 9   1.0     1
 > 
 > 10  6.0     1
 > 
 > 11  5.0     1
 > 
 > 12  7.0     1
 > 
 > 13  2.0     1
 > 
 > 14  3.0     1
 > 
 > 15  1.5     1
 > 
 > 16  1.7     1
 > 
 > 17 17.0     2
 > 
 > 18 12.0     2
 > 
 > 19 15.0     2
 > 
 > 20 16.0     2
 > 
 > 21 12.0     2
 > 
 > 22 23.0     2
 > 
 > 23 19.0     2
 > 
 > 24 21.0     2
 > 
 >  
 > 
 > ------------------- creating a new array of data-----------------------
 > 
 > > ar<-data.frame(GROUP=factor(ar$GROUP),DIP=ar$CC)
 > 
 > > ar
 > 
 >    GROUP  DIP
 > 
 > 1      0  3.0
 > 
 > 2      0  3.0
 > 
 > 3      0  4.0
 > 
 > 4      0  5.0
 > 
 > 5      0  6.0
 > 
 > 6      0  7.0
 > 
 > 7      0  3.0
 > 
 > 8      0  2.0
 > 
 > 9      1  1.0
 > 
 > 10     1  6.0
 > 
 > 11     1  5.0
 > 
 > 12     1  7.0
 > 
 > 13     1  2.0
 > 
 > 14     1  3.0
 > 
 > 15     1  1.5
 > 
 > 16     1  1.7
 > 
 > 17     2 17.0
 > 
 > 18     2 12.0
 > 
 > 19     2 15.0
 > 
 > 20     2 16.0
 > 
 > 21     2 12.0
 > 
 > 22     2 23.0
 > 
 > 23     2 19.0
 > 
 > 24     2 21.0
 > 
 >  
 > 
 > ------------------- creating two dummy variables (C1 and C2) for linear contrasts-----------------------
 > 
 > > ar<-data.frame(GROUP=factor(ar$GROUP),C1=factor(ar$GROUP),C2=factor(ar$GROUP),DIP=ar$DIP)
 > 
 > > ar
 > 
 >    GROUP C1 C2  DIP
 > 
 > 1      0  0  0  3.0
 > 
 > 2      0  0  0  3.0
 > 
 > 3      0  0  0  4.0
 > 
 > 4      0  0  0  5.0
 > 
 > 5      0  0  0  6.0
 > 
 > 6      0  0  0  7.0
 > 
 > 7      0  0  0  3.0
 > 
 > 8      0  0  0  2.0
 > 
 > 9      1  1  1  1.0
 > 
 > 10     1  1  1  6.0
 > 
 > 11     1  1  1  5.0
 > 
 > 12     1  1  1  7.0
 > 
 > 13     1  1  1  2.0
 > 
 > 14     1  1  1  3.0
 > 
 > 15     1  1  1  1.5
 > 
 > 16     1  1  1  1.7
 > 
 > 17     2  2  2 17.0
 > 
 > 18     2  2  2 12.0
 > 
 > 19     2  2  2 15.0
 > 
 > 20     2  2  2 16.0
 > 
 > 21     2  2  2 12.0
 > 
 > 22     2  2  2 23.0
 > 
 > 23     2  2  2 19.0
 > 
 > 24     2  2  2 21.0
 > 
 >  
 > 
 > ------------------- selecting the contrast levels-----------------------
 > 
 > > ar$C1 <- C(ar$C1, c(1,0,-1), how.many = 1)
 > 
 > > ar$C2 <- C(ar$C2, c(1,-1,0), how.many = 1)
 > 
 >  
 > 
 >  
 > 
 > ------------------- contrast analysis of C2 -----------------------
 > 
 > > r.aov8 <-aov(DIP ~  C2 + GROUP , data = ar)
 > 
 > > anova(r.aov8)
 > 
 > Analysis of Variance Table
 > 
 >  
 > 
 > Response: DIP
 > 
 >           Df Sum Sq Mean Sq  F value    Pr(>F)    
 > 
 > C2         1   2.10    2.10   0.2622     0.614    
 > 
 > GROUP      1 917.00  917.00 114.3460 5.915e-10 ***
 > 
 > Residuals 21 168.41    8.02                       
 > 
 > ---
 > 
 > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
 > 
 >  
 > 
 > ------------------- contrast analysis of C1 -----------------------
 > 
 > > r.aov9 <-aov(DIP ~  C1 + GROUP , data = ar)
 > 
 > > anova(r.aov9)
 > 
 > Analysis of Variance Table
 > 
 >  
 > 
 > Response: DIP
 > 
 >           Df Sum Sq Mean Sq F value    Pr(>F)    
 > 
 > C1         1 650.25  650.25  81.083 1.175e-08 ***
 > 
 > GROUP      1 268.85  268.85  33.525 9.532e-06 ***
 > 
 > Residuals 21 168.41    8.02                      
 > 
 > ---
 > 
 > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
 > 
 >  
 > 
 > ------------------- anova of the global design -----------------------
 > 
 > > r.aov10 <-aov(DIP ~  GROUP , data = ar)
 > 
 > > anova(r.aov10)
 > 
 > Analysis of Variance Table
 > 
 >  
 > 
 > Response: DIP
 > 
 >           Df Sum Sq Mean Sq F value    Pr(>F)    
 > 
 > GROUP      2 919.10  459.55  57.304 3.121e-09 ***
 > 
 > Residuals 21 168.41    8.02                      
 > 
 > ---
 > 
 > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
 > 
 > 
 > 
 > 
 > 
 > 
 > 
 > 
 > 
 > I would like to know if there is a more economic procedure with R to do linear contrasts.
 > 
 > Every comments will be well accepted.
 > 
 > 
 > 
 > Thank you very much and best regards
 > 
 > 
 > 
 > Marco Tommasi
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hamstersquats at web.de  Mon Jan 23 13:24:53 2006
From: hamstersquats at web.de (Thomas Kaliwe)
Date: Mon, 23 Jan 2006 13:24:53 +0100
Subject: [R] Image Processing packages
Message-ID: <000201c62018$037ac800$0401a8c0@mouse>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/1430cd8b/attachment.pl

From hamstersquats at web.de  Mon Jan 23 13:48:05 2006
From: hamstersquats at web.de (Thomas Kaliwe)
Date: Mon, 23 Jan 2006 13:48:05 +0100
Subject: [R] Image Processing packages
Message-ID: <000701c6201b$41752580$0401a8c0@mouse>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/621c46a5/attachment.pl

From phgrosjean at sciviews.org  Mon Jan 23 14:07:45 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 23 Jan 2006 14:07:45 +0100
Subject: [R] Image Processing packages
In-Reply-To: <000201c62018$037ac800$0401a8c0@mouse>
References: <000201c62018$037ac800$0401a8c0@mouse>
Message-ID: <43D4D521.2080503@sciviews.org>

Hello,

There are a couple of things for image processing in R (look at pixmap 
and Rimage, for instance). However, R is *not* a good software for 
processing images (nor is Matlab, Octave, IDL: they use to have specific 
packages for image processing and analysis, but resulting applications 
are way to slow in comparison to dedicated software).

A good approach is to mix ImageJ and R, if you are looking for an Open 
Source solution. You could look at ZooImage for an example application 
using these two software (analysis of digital zooplankton images, see: 
http://www.sciviews.org/zooimage).

Best,

Philippe Grosjean


Thomas Kaliwe wrote:
> Hi,
>  
> I've been looking for Image Processing packages. Thresholding, Edge
> Filters, Dct, Segmentation, Restoration. I'm aware, that Octave, Matlab
> etc. would be a good address but then I'm missing the "statistical
> power"  of R. Does anybody know of packages, projects etc. Comments on
> wether the use of R for such matters is useful are welcome.
>  
> Greetings
>  
> Thomas Kaliwe
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From frymor at gmail.com  Mon Jan 23 14:19:13 2006
From: frymor at gmail.com (Assa Yeroslaviz)
Date: Mon, 23 Jan 2006 14:19:13 +0100
Subject: [R] design matrix and coefficients
Message-ID: <9086c4f40601230519g59f4c5a2l@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/f3cdcc3b/attachment.pl

From ked at nilu.no  Mon Jan 23 14:39:31 2006
From: ked at nilu.no (Kare Edvardsen)
Date: Mon, 23 Jan 2006 14:39:31 +0100
Subject: [R] Image Processing packages
In-Reply-To: <000701c6201b$41752580$0401a8c0@mouse>
References: <000701c6201b$41752580$0401a8c0@mouse>
Message-ID: <43D4DC93.2020807@nilu.no>

Hi,

Have a look at PerlDL. http://pdl.perl.org/index_en.html

It's fast and It's free. Also there's a mailing-list. Far from the 
activity here, but usually you get your answer.

Kare

Thomas Kaliwe wrote:
>  
>  
> -----Ursprngliche Nachricht-----
> Von: Thomas Kaliwe [mailto:hamstersquats at web.de] 
> Gesendet: Montag, 23. Januar 2006 13:25
> An: 'r-help at stat.math.ethz.ch'
> Betreff: Image Processing packages
>  
> Hi,
>  
> Ive been looking for Image Processing packages. Thresholding, Edge
> Filters, Dct, Segmentation, Restoration. Im aware, that Octave, Matlab
> etc. would be a good address but then Im missing the statistical
> power  of R. Does anybody know of packages, projects etc. Comments on
> wether the use of R for such matters is useful are welcome.
>  
> Greetings
>  
> Thomas Kaliwe
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
###########################################
Kare Edvardsen <kare.edvardsen at nilu.no>
Norwegian Institute for Air Research (NILU)
Polarmiljosenteret
NO-9296 Tromso       http://www.nilu.no
Swb. +47 77 75 03 75 Dir. +47 77 75 03 90
Fax. +47 77 75 03 76 Mob. +47 90 74 60 69
###########################################



From dmbates at gmail.com  Mon Jan 23 15:57:16 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 23 Jan 2006 08:57:16 -0600
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
	that version is not available on CRAN
In-Reply-To: <Pine.LNX.4.10.10601230550550.27402-100000@quantex-research.co.uk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC0387815A@DJFPOST01.djf.agrsci.dk>
	<Pine.LNX.4.10.10601230550550.27402-100000@quantex-research.co.uk>
Message-ID: <40e66e0b0601230657h3e5c7885t375f48d32c160468@mail.gmail.com>

On 1/22/06, John Logsdon <j.logsdon at quantex-research.com> wrote:
> I've just hit this problem as well and as I am slumming it on XP at the
> moment, I don't have the compilation tools to use the .tar.gz version.
>
> I also notice that the numbering format has changed.  Is this intentional?

Which numbering format?  If you mean the use of 0.995 rather than 0.99
it is because I keep finding changes that need to be made in the
underlying format for mixed-effects representations (i.e. mer objects)
before a 1.0 release.  The current sequence of numbers is 0.99 (99%),
0.995 (99 1/2%), 0.9975 (99 3/4%), 0.99875 (99 7/8%), ...

It is a pattern like, but not as inventive as, Donald Knuth's
numbering sequence for TeX releases.  It has the desirable property of
allowing for an infinite number of releases before you need to declare
the package to be at release 1.0

There will be at least a 0.9975 series of minor releases (I need to
add another two slots to the mer object).



From groemping at tfh-berlin.de  Mon Jan 23 16:19:03 2006
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Mon, 23 Jan 2006 16:19:03 +0100
Subject: [R]  fractional factorial design in R
Message-ID: <20060123145934.M31708@tfh-berlin.de>

>Roberto Furlan wrote:
>
>Hi, 
>i need to create a fractional factorial design sufficient to estimate the 
>main effects. 
>The factors may have any number of levels, let's say any number from 2 to 6. 
>I've tried to use the library conf.design , but i cannot figure out how to 
<write the code. 
>For example, what is the code for a design with 5 factors (2x3x3x5x2) and 
>only main effects not confounded?
>
>thanks in advance! 
>Roberto Furlan 
>University of Turin, Italy

Hi Roberto,

unfortunately, fractional factorial designs typically require all factors to 
have the same number of levels. Hence, your 2x3x3x5x2 example is not a simple 
special case of a fractional factorial design. 

There are some special plans for mixed level designs, but the conf.design 
function requires all factors to have the same number of levels, as you can 
also find in its help:
?????? p: The common number of levels for each factor.?? Must be a prime 
?????????????????? number.

ffDesMatrix from package BHH2 is even worse, since it requires all factors to 
have 2 levels:
    k: numeric. The number of 2-levels design factors in the
          designs. 

(Note that designs with two level-factors only can accommodate a few four-
level factors, by combining two main effects plus their interaction for the 
three degrees of freedom of the 4-level factor. For doing this, I would 
recommend to have someone plan the design who really knows what he/she is 
doing.)

Regards, Ulrike



From Vincent.Labbe.AEREX at drdc-rddc.gc.ca  Mon Jan 23 16:30:05 2006
From: Vincent.Labbe.AEREX at drdc-rddc.gc.ca (Labbe, Vincent (AEREX))
Date: Mon, 23 Jan 2006 10:30:05 -0500
Subject: [R] [Rd] Display an Image on a Plane
Message-ID: <85F883EB8B41D61181C80002A541DB9606275459@valcartierex.drdc-rddc.gc.ca>

Thanks to all for the answers.

I needed something quickly for a report so I used Gimp (as proposed by
Fran??ois Pinard) but I will look at the links given by Barry Rowlingson if I
want something more fancy. Thanks also to Ben Bolker.

Vincent


-----Original Message-----
From: Fran??ois Pinard [mailto:pinard at iro.umontreal.ca] 
Sent: Friday, January 20, 2006 6:31 PM
To: Barry Rowlingson
Cc: Ben Bolker; R-help; Labbe, Vincent (AEREX)
Subject: Re: [R] [Rd] Display an Image on a Plane

[Barry Rowlingson]
>[Ben Bolker]
>> [Labbe, Vincent]

>>>I am new to R and I would like to display an image on a plane in a
>>>3D plot, i.e. I would like to be able to specify a theta and a phi
>>>parameters like in the function persp to display a 2D image on an
>>>inclined plane.

>> what do you mean by "image" exactly?

>I think once you get into doing fancy visualisations like this then you
>may find a solution outside of R. [good referrences deleted]

Bonjour, Vincent.

I'm not fully sure I understand your request, what I get is that you 
want to transform an image on a plane as if one was looking at it in 
space, from an angle.   If I had this problem, I would probably produce 
the image using regular R machinery for this like png() or postscript(), 
then interactively process the result within Gimp, using trapezoidal 
deformations (I think they call it "Perspective transformation").  For 
example, I used this simple trick in the following picture:

   http://pinard.progiciels-bpi.ca/plaisirs/dessins/cd-back.jpg

for the KWIC listing being part of the composition.  However, if 
I needed a precise phi and theta for transformations beyond what 
trans3d() can offer, I would likely use Python or R for computing the 
projection of the rectangle enclosing the image, than PIL (Python 
Imaging Library) for producing that precise trapezoidal deformation.  
Just sharing ideas, of course.  Much likely that if I knew R better, 
I would use it more fully -- but that's a tautology! :-)

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From dieter.menne at menne-biomed.de  Mon Jan 23 16:41:09 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 23 Jan 2006 16:41:09 +0100
Subject: [R] Sweave: skip if figure exists
Message-ID: <LPEJLJACLINDNMBMFAFIIEPOCBAA.dieter.menne@menne-biomed.de>

Dear RWeavers,

to speed up report generation with Sweave, I am looking for an option
"skipifexists" for figures. A named figure chunk should not be executed if
the file to be produced already exists.

I know that this could be done with hooks, but I want to avoid to re-invent
this wheel.

Dieter



From Friedrich.Leisch at tuwien.ac.at  Mon Jan 23 16:49:19 2006
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Mon, 23 Jan 2006 16:49:19 +0100
Subject: [R] Sweave: skip if figure exists
In-Reply-To: <LPEJLJACLINDNMBMFAFIIEPOCBAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIIEPOCBAA.dieter.menne@menne-biomed.de>
Message-ID: <17364.64255.66954.139967@celebrian.ci.tuwien.ac.at>

>>>>> On Mon, 23 Jan 2006 16:41:09 +0100,
>>>>> Dieter Menne (DM) wrote:

  > Dear RWeavers,
  > to speed up report generation with Sweave, I am looking for an option
  > "skipifexists" for figures. A named figure chunk should not be executed if
  > the file to be produced already exists.

  > I know that this could be done with hooks, but I want to avoid to re-invent
  > this wheel.

I have plans to implement something like this for a very long time
now, but no code yet. Contributions are of course more than welcome.

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Jan 23 16:51:03 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 23 Jan 2006 16:51:03 +0100 (CET)
Subject: [R] Trees
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD67E@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD67E@NANAMAILBACK1.nanamail.co.il>
Message-ID: <Pine.LNX.4.51.0601231648560.24671@artemis.imbe.med.uni-erlangen.de>


On Mon, 23 Jan 2006, Stephen wrote:

> Hi.,
>
> I would like to conduct a CHAID tree analysis - Chi-square Automatic
> Interaction Detector.
>
> >From what I can make out from MASS, tree, rpart, and a quick search it
> isn't available
> as a package.
>
> Is that correct or have I missed it?

correct, it is not available out-of-the-box.

> Has anyone an available implementation of it?

if you aim at unbiased variable selection you might want to checkout the
`party' package, especially the `ctree' function.

Best,

Torsten

>
> Thanks
>
> Stephen
>
> Nana Mail <http://mail.nana.co.il> - Get Your Free Personal Outlook 2003
> Now
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From jerk_alert at hotmail.com  Mon Jan 23 16:55:58 2006
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Mon, 23 Jan 2006 15:55:58 +0000
Subject: [R] ordering a data frame to same order as a chr vector
Message-ID: <BAY101-F1776977B803CE8B3D5376FE8100@phx.gbl>

Hi all,

I've got a data frame that has an identical column to a chr vector. I would 
like to use the chr vector to order the rows of the data frame to be 
identical to the order in the chr vector (the contents of the chr vector are 
completely identical to one col of the data frame), but this is proving 
trickier than it sounds..

Any help would be much obliged,
-Ken



From jerk_alert at hotmail.com  Mon Jan 23 17:09:58 2006
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Mon, 23 Jan 2006 16:09:58 +0000
Subject: [R] ordering a data frame to same order as a chr vector
Message-ID: <BAY101-F328EACEB0B4FDC356E3B46E8100@phx.gbl>

Let me clarify the problem a bit further:

tv <- 1:11108

tdf <- data.frame(cbind(11108:1, 22216:11109))


In this example, what I would like to do is re-order the entire tdf data 
frame based on its first column (since the contents of tdf[,1] are identical 
to tv, just not necessarily in the same order in my problem, but here 
obviously the tdf data frame is already ordered the same as tv).

I would like to use the vector tv to give tdf the same exact order as tv, 
based on that first column of tdf which is identical to tv.

Thanks,
ken



From mtommasi at unica.it  Mon Jan 23 17:10:11 2006
From: mtommasi at unica.it (Posta Univ. Cagliari)
Date: Mon, 23 Jan 2006 17:10:11 +0100
Subject: [R] Creating an R package file
Message-ID: <023b01c62037$7dcdfea0$6402a8c0@tommasi9270>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/4da88d73/attachment.pl

From andy_liaw at merck.com  Mon Jan 23 17:10:34 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jan 2006 11:10:34 -0500
Subject: [R] ordering a data frame to same order as a chr vector
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED737@usctmx1106.merck.com>

This is an FAQ that's not in the official FAQ (I believe).  Do
RSiteSearch("sort.data.frame") in R and you'll see.

Andy

From: Ken Termiso
> 
> Hi all,
> 
> I've got a data frame that has an identical column to a chr 
> vector. I would 
> like to use the chr vector to order the rows of the data frame to be 
> identical to the order in the chr vector (the contents of the 
> chr vector are 
> completely identical to one col of the data frame), but this 
> is proving 
> trickier than it sounds..
> 
> Any help would be much obliged,
> -Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.be  Mon Jan 23 17:13:57 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 23 Jan 2006 17:13:57 +0100
Subject: [R] ordering a data frame to same order as a chr vector
References: <BAY101-F1776977B803CE8B3D5376FE8100@phx.gbl>
Message-ID: <014d01c62038$03a3c640$0540210a@www.domain>

probably you are looking for something like the following:

dat <- data.frame(chr = letters[1:20], x = rnorm(20), y = rnorm(20))
chr.vec <- sample(letters[1:20])
####################3
dat[match(chr.vec, dat$chr), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ken Termiso" <jerk_alert at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, January 23, 2006 4:55 PM
Subject: [R] ordering a data frame to same order as a chr vector


> Hi all,
>
> I've got a data frame that has an identical column to a chr vector. 
> I would
> like to use the chr vector to order the rows of the data frame to be
> identical to the order in the chr vector (the contents of the chr 
> vector are
> completely identical to one col of the data frame), but this is 
> proving
> trickier than it sounds..
>
> Any help would be much obliged,
> -Ken
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From stecalza at tiscali.it  Mon Jan 23 17:24:12 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Mon, 23 Jan 2006 17:24:12 +0100
Subject: [R] ordering a data frame to same order as a chr vector
In-Reply-To: <BAY101-F328EACEB0B4FDC356E3B46E8100@phx.gbl>
References: <BAY101-F328EACEB0B4FDC356E3B46E8100@phx.gbl>
Message-ID: <20060123162411.GA10681@med.unibs.it>

Hi,

if I got it right I'd suggest to use match (which would allow you to use 
also character vectors)

So

tdf[match(tv,tdf[,1]),]

will give you tdf in the same order as tv


HIH
Ste

On Mon, Jan 23, 2006 at 04:09:58PM +0000, Ken Termiso wrote:
<Ken>Let me clarify the problem a bit further:
<Ken>
<Ken>tv <- 1:11108
<Ken>
<Ken>tdf <- data.frame(cbind(11108:1, 22216:11109))
<Ken>
<Ken>
<Ken>In this example, what I would like to do is re-order the entire tdf data 
<Ken>frame based on its first column (since the contents of tdf[,1] are identical 
<Ken>to tv, just not necessarily in the same order in my problem, but here 
<Ken>obviously the tdf data frame is already ordered the same as tv).
<Ken>
<Ken>I would like to use the vector tv to give tdf the same exact order as tv, 
<Ken>based on that first column of tdf which is identical to tv.
<Ken>
<Ken>Thanks,
<Ken>ken
<Ken>
<Ken>______________________________________________
<Ken>R-help at stat.math.ethz.ch mailing list
<Ken>https://stat.ethz.ch/mailman/listinfo/r-help
<Ken>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From lists at revelle.net  Mon Jan 23 17:28:37 2006
From: lists at revelle.net (William Revelle)
Date: Mon, 23 Jan 2006 10:28:37 -0600
Subject: [R] White Noise
In-Reply-To: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0601221140470.6514-100000@gw.env.leeds.ac.uk>
Message-ID: <p0623091ebffab3c26d86@[165.124.165.234]>

At 1:38 PM +0000 1/22/06, Laura Quinn wrote:
>I'm wanting to create a series of near-identical matrices via the addition
>of "white noise" to my starting matrix. Is there a function within R which
>will allow me to do this?
>
>Thank you

If the starting matrix is symmetric, see mvrnorm in the MASS package.



-- 
William Revelle		http://pmc.psych.northwestern.edu/revelle.html   
Professor			http://personality-project.org/personality.html
Department of Psychology       http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/



From andy_liaw at merck.com  Mon Jan 23 17:28:50 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jan 2006 11:28:50 -0500
Subject: [R] Creating an R package file
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED738@usctmx1106.merck.com>

Well, you simply type

    R CMD INSTALL --build mypackage

or

    R CMD build --binary mypackage

in the parent directory of the source package.  If you're on Windows, you
need to have some tools installed to be able to do that (but you haven't
said what OS you're using).

Andy

From: Posta Univ. Cagliari
> 
> Dear R community,
> 
> I would like to create my own R package files, but I find 
> some problemm for R versions >1.9.
> 
> When in previous versions of R I could write a simple text 
> file, to have a functioning file package, now I found that is 
> neccessary to implement also binary copies of the file. I 
> cannot understand, reading from R manuals, how it is the 
> correct procedure to create these binary files.
> 
> Is there anybody that can help me on this issue?
> 
> Many thanks,
> 
> Marco Tommasi
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mtommasi at unica.it  Mon Jan 23 17:39:03 2006
From: mtommasi at unica.it (Posta Univ. Cagliari)
Date: Mon, 23 Jan 2006 17:39:03 +0100
Subject: [R] Creating an R package file
References: <023b01c62037$7dcdfea0$6402a8c0@tommasi9270>
Message-ID: <026f01c6203b$85819860$6402a8c0@tommasi9270>

>If you're on Windows, you
>need to have some tools installed to be able to do that (but you haven't
>said what OS you're using).

Yes, I forgo tto say that my OS is Windows.

Regards,

Marco Tommasi



From HerwigMeschke at t-online.de  Mon Jan 23 17:47:20 2006
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Mon, 23 Jan 2006 17:47:20 +0100
Subject: [R] R-help Digest, Vol 35, Issue 23
In-Reply-To: <mailman.10.1138014002.18577.r-help@stat.math.ethz.ch>
Message-ID: <43D516A8.23620.7339CD@localhost>

> summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs
> i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))
> 
try
class(aovRes) #-> aovlist !
summary.aovlist(aovRes, spit=...)
or simply
summary(aovRes, spit=...)

Hoping this helps,
Herwig

-- 
Dr. Herwig Meschke
Wissenschaftliche Beratung
Hagsbucher Weg 27
D-89150 Laichingen

phone +49 7333 210 417 / fax +49 7333 210 418
email HerwigMeschke at t-online.de



From ligges at statistik.uni-dortmund.de  Mon Jan 23 17:51:58 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jan 2006 17:51:58 +0100
Subject: [R] Creating an R package file
In-Reply-To: <023b01c62037$7dcdfea0$6402a8c0@tommasi9270>
References: <023b01c62037$7dcdfea0$6402a8c0@tommasi9270>
Message-ID: <43D509AE.9080804@statistik.uni-dortmund.de>

Posta Univ. Cagliari wrote:

> Dear R community,
> 
> I would like to create my own R package files, but I find some
> problemm for R versions >1.9.
> 
> When in previous versions of R I could write a simple text file, to
> have a functioning file package, now I found that is neccessary to
> implement also binary copies of the file. I cannot understand,
> reading from R manuals, how it is the correct procedure to create
> these binary files.
> 
> Is there anybody that can help me on this issue?


The manual "Writing R Extensions" explains how to write a source package.
The manual "R Installation and Administration" explains how to collect
the required tools (your mail header tells us you are on Windows) to perform
   R CMD INSTALL, R CMD check and R CMD build

Uwe Ligges



> Many thanks,
> 
> Marco Tommasi [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From tozim at gmx.de  Mon Jan 23 17:54:11 2006
From: tozim at gmx.de (kaoskrew.de)
Date: Mon, 23 Jan 2006 17:54:11 +0100
Subject: [R] lines() in heatmap()
Message-ID: <6bc7d02d44ebf202c02c370117b9bcd0@gmx.de>

Hello!

I looked for help through google and the help-files and spend several 
hours with trial and
error, but didn't find a correct way.

It's all about lines in a heatmap to separate different data block from 
each other to underline
the significance of the found clusters!

The heatmap is build like that:

heatmap(X, Rowv=NA, Colv=NA, symm=TRUE, cexRow=0.3, cexCol=0.3)

I switched the dendrograms off, because I don't want the values sorted 
but in the way, I
extracted them from the data.

I tried to enter the lines that way:

lines(c(14,14),c(0,187), col="green")

That should appear as a line parallel to the x-axis, but there was no 
line at all.
I also tried several other ways, which didn't work at all. (The Matrix 
contains
187 values and is symmetric)

Also the line should start and end at the borders of the heatmap, so if 
there is
a parameter to enter that, I would be glad, if you add it.

Another short question:
Does anybody have an idea, how I could switch the names of my data from 
the heatmaps
right side to it's left side?


Thanks for help already,
Tobias Zimmer



From maechler at stat.math.ethz.ch  Mon Jan 23 18:08:20 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 23 Jan 2006 18:08:20 +0100
Subject: [R] fractional factorial design in R
In-Reply-To: <17361.61904.956693.537045@stat.math.ethz.ch>
References: <EMEELGDEKHMIAKDGLCDCOEKFCJAA.roberto.furlan@gmail.com>
	<200601202215.k0KMFnJx000355@meitner.gene.com>
	<17361.61904.956693.537045@stat.math.ethz.ch>
Message-ID: <17365.3460.269522.419731@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Sat, 21 Jan 2006 09:33:20 +0100 writes:

>>>>> "BertG" == Berton Gunter <gunter.berton at gene.com>
>>>>>     on Fri, 20 Jan 2006 14:15:49 -0800 writes:

    BertG> I'm not sure what you mean by a "fractional
    BertG> factorial" design here. 
			     ^^^^
and Bert was right on target.
Please apologize for the following, which I'm now sure Bert
knows probably much better than me.

    MM> it's a pretty well known term, in (at least some schools of)
    MM> classical analysis of variance, e.g. 
    MM> in the famous "Box, Hunter^2" (1987) book which had a 2nd
    MM> edition last year (Wiley), see also Doug Bates's nice account on
    MM> a talk about it,
    MM> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/29768.html
    MM> and the R package  "BHH2"  which is devoted to the book (2nd Ed).

    MM> BHH2 has a function  ffDesMatrix()  for fractional factorial
    MM> designs, *and* in it's see also section a link to the
    MM> conf.design() function in the package of the same name which
    MM> allows to construct ff designs (via different specifications).
    MM> see e.g, http://finzi.psych.upenn.edu/R/library/BHH2/html/ffDesMatrix.html

    BertG> In general, when dealing with complex designs of the sort you appear to be thinking
    BertG> about, small orthogonal designs don't exist. You
    BertG> might wish to look at the AlgDesign package to
    BertG> generate an efficient design to estimate only main
    BertG> effects.

    BertG> -- Bert Gunter Genentech Non-Clinical Statistics
    BertG> South San Francisco, CA
 
    BertG> "The business of the statistician is to catalyze the
    BertG> scientific learning process."  - George E. P. Box
 
    MM> now, since you have this nice signature, maybe you should
    MM> get the above book? ;-) :-)
 
which was supposed to be something like a joke;  
given my misreading of Bert's comment, I'm sorry about it.

I have no good excuse for my blunder:
  It was Saturday morning, 
  I had slept well, 
  had had a shower, coffee and breakfast, ...  

Martin Maechler, ETH Zurich

    >>> -----Original Message----- From:
    >>> r-help-bounces at stat.math.ethz.ch
    >>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
    >>> Roberto Furlan Sent: Friday, January 20, 2006 12:37 PM
    >>> To: r-help at stat.math.ethz.ch Subject: [R] fractional
    >>> factorial design in R
    >>> 
    >>> Hi, i need to create a fractional factorial design
    >>> sufficient to estimate the main effects.  The factors may
    >>> have any number of levels, let's say any number from 2 to
    >>> 6.  I've tried to use the library conf.design , but i
    >>> cannot figure out how to write the code.  For example,
    >>> what is the code for a design with 5 factors (2x3x3x5x2)
    >>> and only main effects not confounded?
    >>> 
    >>> thanks in advance!  Roberto Furlan University of Turin,
    >>> Italy



From Paolo.Ghisletta at cig.unige.ch  Mon Jan 23 18:19:58 2006
From: Paolo.Ghisletta at cig.unige.ch (Paolo Ghisletta)
Date: Mon, 23 Jan 2006 18:19:58 +0100
Subject: [R] nlme in R v.2.2.1 and S-Plus v. 7.0
Message-ID: <43D5103E.3050907@cig.unige.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/ca9c017e/attachment.pl

From azzalini at stat.unipd.it  Mon Jan 23 18:21:18 2006
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 23 Jan 2006 18:21:18 +0100
Subject: [R] mutlivariate normal and t distributions
Message-ID: <20060123182118.4e5e3049.azzalini@stat.unipd.it>


Dear R-help list members,

I have created a package 'mnormt' with facilities for the multivariate
normal and t distributions. The core part is simply an interface to
Fortran routines by Alan Genz for computing the integral of two
densities over rectangular regions, using an adaptive integration
method. Other R functions compute densities and generate random
numbers.

The starting motivation to write it was the  need to have functions
which compute the distribution functions in a non-Monte Carlo form,
sinse this caused me problems when these probabilities are involved in
a minimization problem. For this reason, I could not make use of the
CRAN package 'mvtnorm'.  Exactly to avoid superposition with the CRAN
package, 'mnormt' is made available somehere else, in case other
people want to use it.  The package 'mnormt' is at 
   http://azzalini.stat.unipd.it/SN/Pkg-mnormt

As explained, this is not uploaded to CRAN just to avoid clash with
the existing package. However, if it is felt appropriate, I have no
objection to upload it to CRAN.

Best wishes,

Adelchi Azzalini
-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From Reinecke at consultic.com  Mon Jan 23 18:29:17 2006
From: Reinecke at consultic.com (Michael Reinecke)
Date: Mon, 23 Jan 2006 18:29:17 +0100
Subject: [R] Selecting data frame components by name - do you know
	ashorter way?
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF196B0C@sbs2003.CMI.local>

 
Thank you all very much! The subset function is great! I knew there must
be such a very convenient feature in R. I also learned a lot by studying
the other ways of selecting data frame components.

Greetings

Michael



From llei at bccrc.ca  Mon Jan 23 18:33:25 2006
From: llei at bccrc.ca (Linda Lei)
Date: Mon, 23 Jan 2006 09:33:25 -0800
Subject: [R] command in survival package
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC9@crcmail1.BCCRC.CA>

Thank you guys. 
But I tried the commands and I still get:

> aml1<-aml[aml$group==1,]
> aml1
[1] time   status x     
<0 rows> (or 0-length row.names)
> esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
Error in Surv(aml1$weeks, status) : Time variable is not numeric
In addition: Warning message:
is.na() applied to non-(list or vector) in: is.na(time)

which still looks confusing. Or are they should be applied in s-plus
instead of R?

Thanks a lot!
Linda


-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at mn.rr.com] 
Sent: Friday, January 20, 2006 4:49 PM
To: Thomas Lumley
Cc: Linda Lei; R-help at stat.math.ethz.ch
Subject: Re: [R] command in survival package

On Fri, 2006-01-20 at 15:27 -0800, Thomas Lumley wrote:
> On Fri, 20 Jan 2006, Linda Lei wrote:
> 
> > Hi there,
> >
> >
> >
> > I have a question about one command sentence when I follow the
example
> > in the book of "Survival analysis in S":
> >
> > > aml1<-aml[aml$group==1]
> 
> If this is really what the book says you should probably complain to
the 
> author.  However, if it says
>    aml1<-aml[aml$group==1,]
> then you show type that instead.
> 
> It is hard to be sure, because you don't say where the `aml' data set 
> comes from.  It can't be the one in the survival package as this
doesn't 
> have a variable called "group"
> 
> >
> > Thus, I couldn't keep going on the next command:
> >
> > esf.fit<-survfit(Surv(aml1,status)~1).
> >
> 
> This looks implausible as well. I would have expected something like
>    esf.fit<-survfit(Surv(time,status)~1, data=aml1)


Correct on both accounts Thomas. The text in the book on page 26 appears
as Linda posted.

However, there is an errata document here:

http://www.crcpress.com/e_products/downloads/download.asp?cat_no=C4088

which shows the corrected text as:

aml1 <- aml[aml$group==1,] # Maintained group only

esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)


HTH,

Marc Schwartz



From llei at bccrc.ca  Mon Jan 23 18:48:23 2006
From: llei at bccrc.ca (Linda Lei)
Date: Mon, 23 Jan 2006 09:48:23 -0800
Subject: [R] (no subject)
Message-ID: <90B06673D826C64E8ED8EEA6B6FDF8CAE72ACA@crcmail1.BCCRC.CA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/2c68619d/attachment.pl

From f.harrell at vanderbilt.edu  Mon Jan 23 18:46:52 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 23 Jan 2006 11:46:52 -0600
Subject: [R] Importing QIF files
Message-ID: <43D5168C.4080008@vanderbilt.edu>

Has anyone written R code for importing financial QIF files?  I know 
there is an OpenOffice plugin for importing these into a spreadsheet but 
I found that it omits some records.

Thanks,
Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From sputzele at mail.ru  Mon Jan 23 18:56:36 2006
From: sputzele at mail.ru (Baronin P. Storch von)
Date: Mon, 23 Jan 2006 20:56:36 +0300
Subject: [R] =?koi8-r?b?Q2FuIG9uZSB3cml0ZSBhIHByb2NlZHVyZSBpbiBSIGxpa2Ug?=
	=?koi8-r?b?Zm9yIGluc3RhbmNlIGluIE1hcGxlID8=?=
Message-ID: <E1F15vg-000Fxe-00.sputzele-mail-ru@f55.mail.ru>


Dear R-wizards!

I have been learning on my own how to use this fantastic program.. but I agree with some people that even with the manuals, the faq and so on.. when you are sitting fully alone.. progress can be ... slow... very slow indeed.. In fact sometimes, looking at the "solutions" provided by  some of you- I am just flabbergasted to the point that I couldn't figure out how to come up with them myself (sometimes I don't even understand them :-( )

 But after spending around three weeks on this, and starting to get fairly obsessed with it, I decided I shall ask for help,'cause i can't figure out in the documentation where I should look for this.

In Maple when I want to automatize something boring I write a procedure..
here I am not too sure.. how to bind together a few statements- most of which are functions... sounds like it would make up a personnal macro..
I am sure I am doing things in such a primitive way that the R-specialists will wince.. but that's how it goes with beginners!

Set up.

 I have 4 fairly large data base with unequal number of lines (around 1200-1500), but identical number of columns (162)-years.
 
for each column, I construct a data-frame with the corresponding column of DB1, then from DB2, .. DB4.

This yields a data.frame in which many data are NA- some are real NAs some others are because I have to take the max of the lines. In any case, the number of NAs of each of these 4 columns is not identical.

I extract (by sorting and creating 4 new vectors) 4 vectors of variable length
-the relevant and interesting data- to whom I wish to apply some standardized treatment: that is  normality with say Shapiro-Wilks, Levene, etc.. Kruskal-Wallis and probably other things.. 

I am not showing my tasks because I do not think that I want to bother the readers with this,, rather ask for general remarks ( you can provide examples of your own).

For each column I want to write the results in a table.. and append these resulta for each column.

I was fairly efficient at doing that for a particular column,

but then the simple thought of  apply this "list of tasks" 162 times.. makes me.. 
feel that there should be a way.... to speed up my execution.. (a loop)

However I have not been able to create a super "function" (or procedure) that could tie all these statements together in a sensible fashion.. because each time the data.frame created is generated by a function.. and somehow i still did not figure out how to write a function of functions 
and then maybe a loop do for all values of dates =1:162 this function..

(all the stuff I tried failed, because I was indexing objects that were also indexed.. I am vague.. but then retracing a 3 weeks of trials and errors errors errors errors ...\infty :-) is cumbersome)


1. Could anybody  give me suggestions where to look and maybe unveil the tricks of the function of functions .. 

ideally I would construct a loop executing a super function.. whose results would be dumped in a file (write.table) appending each time the result of the loop i..

but I was not able to construct that...
 should I make a wiser use of these "apply, tapply, sapply" marvels? I dunno.
 does something like for (i in 1:G){sapply(b(i),sw)} were b(i) is the dataframe for column i, and SW is a function (super function-procedure) make sense in R?

I see that there are some fully esoterical paragraphs on things that seem to be relevant in the manuals.. but.. esoterical.. I cannot make sense of them...  vicious circle..


Thank you in advance for any courageous that  would give me a hint..

 Christina



From spencer.graves at pdf.com  Mon Jan 23 19:31:42 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 23 Jan 2006 10:31:42 -0800
Subject: [R] mutlivariate normal and t distributions
In-Reply-To: <20060123182118.4e5e3049.azzalini@stat.unipd.it>
References: <20060123182118.4e5e3049.azzalini@stat.unipd.it>
Message-ID: <43D5210E.4070302@pdf.com>

	  Thanks for this.  I got stuck some months ago, because I needed to 
evaluate a certain special normal tail region in at least 10,000 
dimensional space, and "mvtnorm" could not give me adequate numerical 
precision above about 30 or 35 dimensions.  When I get time to return to 
that project, I will be particularly interested in looking at your code.

	  I would strongly encourage you to submit your package to CRAN. 
Otherwise it will be harder for people to learn about it and get it.  If 
your package contains functions or other objects with the same names as 
objects in other packages, that fact could create a conflict, though not 
insurmountable.  If you can make some effort to minimize the potential 
for name conflicts with other packages, that would be great.  However, 
even without that, I prefer accessibility.

	  Of course, from a user's perspective, it would be best if you and the 
'mvtnorm' maintainer (Torsten Hothorn) could develop a consensus on the 
circumstances under which each algorithm is best and then develop a 
combined package that makes it easier for users to access the best known 
method -- and to easily switch between methods, when it may not be clear 
which is best.  However, if you and Torsten don't have time & energy for 
that, I would still encourage you to submit your package to CRAN.

	  Best Wishes,
	  Spencer Graves

Adelchi Azzalini wrote:

> Dear R-help list members,
> 
> I have created a package 'mnormt' with facilities for the multivariate
> normal and t distributions. The core part is simply an interface to
> Fortran routines by Alan Genz for computing the integral of two
> densities over rectangular regions, using an adaptive integration
> method. Other R functions compute densities and generate random
> numbers.
> 
> The starting motivation to write it was the  need to have functions
> which compute the distribution functions in a non-Monte Carlo form,
> sinse this caused me problems when these probabilities are involved in
> a minimization problem. For this reason, I could not make use of the
> CRAN package 'mvtnorm'.  Exactly to avoid superposition with the CRAN
> package, 'mnormt' is made available somehere else, in case other
> people want to use it.  The package 'mnormt' is at 
>    http://azzalini.stat.unipd.it/SN/Pkg-mnormt
> 
> As explained, this is not uploaded to CRAN just to avoid clash with
> the existing package. However, if it is felt appropriate, I have no
> objection to upload it to CRAN.
> 
> Best wishes,
> 
> Adelchi Azzalini



From ruser2006 at yahoo.com  Mon Jan 23 18:49:40 2006
From: ruser2006 at yahoo.com (r user)
Date: Mon, 23 Jan 2006 09:49:40 -0800 (PST)
Subject: [R] Converting from a dataset to a single "column"
Message-ID: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>

I have a dataset of 3 ?columns? and 5 ?rows?.

temp<-data.frame(col1=c(5,10,14,56,7),col2=c(4,2,8,3,34),col3=c(28,4,52,34,67))

I wish to convert this to a single ?column?, with
column 1 on ?top? and column 3 on ?bottom?.

i.e.

5
10
14
56
7
4
2
8
3
34
28
4
52
34
67

Are there any functions that do this, and that will
work well on much larger datasets (e.g. 1000 rows,
6000 columns)?



From gunter.berton at gene.com  Mon Jan 23 19:46:01 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 Jan 2006 10:46:01 -0800
Subject: [R] Can one write a procedure in R like for instance in Maple ?
In-Reply-To: <E1F15vg-000Fxe-00.sputzele-mail-ru@f55.mail.ru>
Message-ID: <200601231846.k0NIk1qs022668@faraday.gene.com>

1. Have you read the "R LANGUAGE DEFINITION" in which writing functions is
extensively discussed?

2. One excellent book on R is S PROGRAMMING by Venables and Ripley. I
recommend it highly.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Baronin P. Storch von
> Sent: Monday, January 23, 2006 9:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Can one write a procedure in R like for instance 
> in Maple ?
> 
> 
> Dear R-wizards!
> 
> I have been learning on my own how to use this fantastic 
> program.. but I agree with some people that even with the 
> manuals, the faq and so on.. when you are sitting fully 
> alone.. progress can be ... slow... very slow indeed.. In 
> fact sometimes, looking at the "solutions" provided by  some 
> of you- I am just flabbergasted to the point that I couldn't 
> figure out how to come up with them myself (sometimes I don't 
> even understand them :-( )
> 
>  But after spending around three weeks on this, and starting 
> to get fairly obsessed with it, I decided I shall ask for 
> help,'cause i can't figure out in the documentation where I 
> should look for this.
> 
> In Maple when I want to automatize something boring I write a 
> procedure..
> here I am not too sure.. how to bind together a few 
> statements- most of which are functions... sounds like it 
> would make up a personnal macro..
> I am sure I am doing things in such a primitive way that the 
> R-specialists will wince.. but that's how it goes with beginners!
> 
> Set up.
> 
>  I have 4 fairly large data base with unequal number of lines 
> (around 1200-1500), but identical number of columns (162)-years.
>  
> for each column, I construct a data-frame with the 
> corresponding column of DB1, then from DB2, .. DB4.
> 
> This yields a data.frame in which many data are NA- some are 
> real NAs some others are because I have to take the max of 
> the lines. In any case, the number of NAs of each of these 4 
> columns is not identical.
> 
> I extract (by sorting and creating 4 new vectors) 4 vectors 
> of variable length
> -the relevant and interesting data- to whom I wish to apply 
> some standardized treatment: that is  normality with say 
> Shapiro-Wilks, Levene, etc.. Kruskal-Wallis and probably 
> other things.. 
> 
> I am not showing my tasks because I do not think that I want 
> to bother the readers with this,, rather ask for general 
> remarks ( you can provide examples of your own).
> 
> For each column I want to write the results in a table.. and 
> append these resulta for each column.
> 
> I was fairly efficient at doing that for a particular column,
> 
> but then the simple thought of  apply this "list of tasks" 
> 162 times.. makes me.. 
> feel that there should be a way.... to speed up my 
> execution.. (a loop)
> 
> However I have not been able to create a super "function" (or 
> procedure) that could tie all these statements together in a 
> sensible fashion.. because each time the data.frame created 
> is generated by a function.. and somehow i still did not 
> figure out how to write a function of functions 
> and then maybe a loop do for all values of dates =1:162 this 
> function..
> 
> (all the stuff I tried failed, because I was indexing objects 
> that were also indexed.. I am vague.. but then retracing a 3 
> weeks of trials and errors errors errors errors ...\infty :-) 
> is cumbersome)
> 
> 
> 1. Could anybody  give me suggestions where to look and maybe 
> unveil the tricks of the function of functions .. 
> 
> ideally I would construct a loop executing a super function.. 
> whose results would be dumped in a file (write.table) 
> appending each time the result of the loop i..
> 
> but I was not able to construct that...
>  should I make a wiser use of these "apply, tapply, sapply" 
> marvels? I dunno.
>  does something like for (i in 1:G){sapply(b(i),sw)} were 
> b(i) is the dataframe for column i, and SW is a function 
> (super function-procedure) make sense in R?
> 
> I see that there are some fully esoterical paragraphs on 
> things that seem to be relevant in the manuals.. but.. 
> esoterical.. I cannot make sense of them...  vicious circle..
> 
> 
> Thank you in advance for any courageous that  would give me a hint..
> 
>  Christina
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Mon Jan 23 19:52:12 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 23 Jan 2006 10:52:12 -0800 (PST)
Subject: [R] command in survival package
In-Reply-To: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC9@crcmail1.BCCRC.CA>
References: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC9@crcmail1.BCCRC.CA>
Message-ID: <Pine.LNX.4.64.0601231049480.32205@homer22.u.washington.edu>

On Mon, 23 Jan 2006, Linda Lei wrote:

> Thank you guys.
> But I tried the commands and I still get:
>
>> aml1<-aml[aml$group==1,]
>> aml1
> [1] time   status x
> <0 rows> (or 0-length row.names)
>> esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
> Error in Surv(aml1$weeks, status) : Time variable is not numeric
> In addition: Warning message:
> is.na() applied to non-(list or vector) in: is.na(time)
>
> which still looks confusing. Or are they should be applied in s-plus
> instead of R?

You *still* haven't said where the aml dataset comes from, but as I said 
before, it isn't the same as the one built in to R, so you need to load it 
somehow.  The one in R has columns called "time", "status" and "x"; you 
appear to want one with columns "weeks","status", and "group", and with 
"group" being numeric rather than factor.


 	-thomas



From p.dalgaard at biostat.ku.dk  Mon Jan 23 20:00:14 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jan 2006 20:00:14 +0100
Subject: [R] Converting from a dataset to a single "column"
In-Reply-To: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
References: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <x2r76y3h35.fsf@turmalin.kubism.ku.dk>

r user <ruser2006 at yahoo.com> writes:

> I have a dataset of 3 ?columns? and 5 ?rows?.
> 
> temp<-data.frame(col1=c(5,10,14,56,7),col2=c(4,2,8,3,34),col3=c(28,4,52,34,67))
> 
> I wish to convert this to a single ?column?, with
> column 1 on ?top? and column 3 on ?bottom?.
> 
> i.e.
> 
> 5
> 10
> 14
> 56
> 7
> 4
> 2
> 8
> 3
> 34
> 28
> 4
> 52
> 34
> 67
> 
> Are there any functions that do this, and that will
> work well on much larger datasets (e.g. 1000 rows,
> 6000 columns)?

unlist(temp,use.names=FALSE)

Did you really mean "column" (as opposed to vector?), if so,

matrix(unlist(temp,use.names=FALSE))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From pensterfuzzer at yahoo.de  Mon Jan 23 20:01:01 2006
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Mon, 23 Jan 2006 20:01:01 +0100 (CET)
Subject: [R] cluster plot for cmeans cluster result
Message-ID: <20060123190101.9079.qmail@web25801.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/19723b95/attachment.pl

From statistical.model at googlemail.com  Mon Jan 23 20:02:59 2006
From: statistical.model at googlemail.com (statistical.model@googlemail.com)
Date: Mon, 23 Jan 2006 19:02:59 -0000
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <20060123145934.M31708@tfh-berlin.de>
Message-ID: <EMEELGDEKHMIAKDGLCDCAEMACJAA.Statistical.model@gmail.com>


Hi! 
thanks for your response!

>unfortunately, fractional factorial designs typically require all factors
to 
have the same number of levels. Hence, your 2x3x3x5x2 example is not a
simple 
special case of a fractional factorial design. 
>There are some special plans for mixed level designs, but the conf.design 
function requires all factors to have the same number of levels, as you can 
also find in its help:
    p: The common number of levels for each factor.  Must be a prime 
          number.
>ffDesMatrix from package BHH2 is even worse, since it requires all factors
to 
have 2 levels:
    k: numeric. The number of 2-levels design factors in the
          designs. 


I've tried both ffDesMatrix and conf.design and i realized that they cannot
help me for the problem above.

At the moment, I am using an SPSS function (orthoplan) for my needs. It
provides factorial design with only main effects (small orthogonal designs).
Since SPSS is very expensive (in particular with this add-on), I would like
to use R as for all my other projects and research activity.

Do you know any other way to produce these designs in R?

Thanks in advance,

Roberto Furlan
University of Turin, Italy



----------------------------------------
La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
188 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!


From csardi at rmki.kfki.hu  Mon Jan 23 20:05:43 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Mon, 23 Jan 2006 20:05:43 +0100
Subject: [R] Converting from a dataset to a single "column"
In-Reply-To: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
References: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <20060123190543.GI28980@rmki.kfki.hu>

> temp<-data.frame(col1=c(5,10,14,56,7),col2=c(4,2,8,3,34),col3=c(28,4,52,34,67))
> temp
      col1 col2 col3
    1    5    4   28
    2   10    2    4
    3   14    8   52
    4   56    3   34
    5    7   34   67
> as.numeric(as.matrix(temp))
 [1]  5 10 14 56  7  4  2  8  3 34 28  4 52 34 67
	     
Whether this works well enough:
	     
> temp <- as.data.frame(matrix(1:(1000*6000), nr=1000))
> dim(temp)
  [1] 1000 6000
> date() ; num <- as.numeric(as.matrix(temp)) ; date()
  [1] "Mon Jan 23 19:40:25 2006"
  [1] "Mon Jan 23 19:40:25 2006"
> length(num)
  [1] 6000000
	     
Gabor
	     
On Mon, Jan 23, 2006 at 09:49:40AM -0800, r user wrote:
> I have a dataset of 3 ?columns? and 5 ?rows?.
> 
> temp<-data.frame(col1=c(5,10,14,56,7),col2=c(4,2,8,3,34),col3=c(28,4,52,34,67))
>
> I wish to convert this to a single ?column?, with
> column 1 on ?top? and column 3 on ?bottom?.
> 
> i.e.
> 
> 5
> 10
> 14
> 56
> 7
> 4
> 2
> 8
> 3
> 34
> 28
> 4
> 52
> 34
> 67
> 
> Are there any functions that do this, and that will
> work well on much larger datasets (e.g. 1000 rows,
> 6000 columns)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From jholtman at gmail.com  Mon Jan 23 19:59:30 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 23 Jan 2006 13:59:30 -0500
Subject: [R] Converting from a dataset to a single "column"
In-Reply-To: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
References: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <644e1f320601231059s66e6970frcbe80f0b16cc9297@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/e509ac08/attachment.pl

From phgrosjean at sciviews.org  Mon Jan 23 20:22:08 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 23 Jan 2006 20:22:08 +0100
Subject: [R] Image Processing packages
In-Reply-To: <1138037487.43d512efc05b6@webmail.mcgill.ca>
References: <000201c62018$037ac800$0401a8c0@mouse>
	<43D4D521.2080503@sciviews.org>
	<1138037487.43d512efc05b6@webmail.mcgill.ca>
Message-ID: <43D52CE0.3010503@sciviews.org>

r.ghezzo at staff.mcgill.ca wrote:
> Philippe, I am trying to get there but I keep on receiving a blank file after 5
> minutes, is there something wrong with the site?
> H.Ghezzo
> McGill University
> Montreal - Canada

Well, I am trying myself. Indded, it seems that my server is down, or 
very busy. I will look at this tomorrow.
Best,

Philippe Grosjean

> Quoting Philippe Grosjean <phgrosjean at sciviews.org>:
> 
> 
>>Hello,
>>
>>There are a couple of things for image processing in R (look at pixmap
>>and Rimage, for instance). However, R is *not* a good software for
>>processing images (nor is Matlab, Octave, IDL: they use to have specific
>>packages for image processing and analysis, but resulting applications
>>are way to slow in comparison to dedicated software).
>>
>>A good approach is to mix ImageJ and R, if you are looking for an Open
>>Source solution. You could look at ZooImage for an example application
>>using these two software (analysis of digital zooplankton images, see:
>>http://www.sciviews.org/zooimage).
>>
>>Best,
>>
>>Philippe Grosjean
>>
>>
>>Thomas Kaliwe wrote:
>>
>>>Hi,
>>>
>>>I've been looking for Image Processing packages. Thresholding, Edge
>>>Filters, Dct, Segmentation, Restoration. I'm aware, that Octave, Matlab
>>>etc. would be a good address but then I'm missing the "statistical
>>>power"  of R. Does anybody know of packages, projects etc. Comments on
>>>wether the use of R for such matters is useful are welcome.
>>>
>>>Greetings
>>>
>>>Thomas Kaliwe
>>>
>>>	[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> 
>



From gunter.berton at gene.com  Mon Jan 23 20:28:45 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 Jan 2006 11:28:45 -0800
Subject: [R] fractional factorial design in R
In-Reply-To: <EMEELGDEKHMIAKDGLCDCAEMACJAA.Statistical.model@gmail.com>
Message-ID: <200601231928.k0NJSjM4003371@meitner.gene.com>

> 
> At the moment, I am using an SPSS function (orthoplan) for my 
> needs. It provides factorial design with only main effects 
> (small orthogonal designs).
> Since SPSS is very expensive (in particular with this 
> add-on), I would like to use R as for all my other projects 
> and research activity.

Yes, you're right. For, say, a 3 x 5 design, one can do this in as few as 7
runs -- but only in general by some version of one-factor-at-a-time (OFAT)
designs, which are inefficient.  It is easy, via, say model.matrix() to
write a general function to produce these. But I think it's a bad idea; more
efiicient algorithmic designs are better, IMO, which is why I suggested
AlgDesign. You and others are free to disagree, of course.

Cheers,
Bert


> 
> Do you know any other way to produce these designs in R?
> 
> Thanks in advance,
> 
> Roberto Furlan
> University of Turin, Italy
> 
> 
> 
> ----------------------------------------
> La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
> 188 messaggi contenenti spam sono stati bloccati con successo.
> Scarica gratuitamente SPAMfighter!
> 
>



From j.logsdon at quantex-research.com  Mon Jan 23 20:30:58 2006
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 23 Jan 2006 19:30:58 +0000 (GMT)
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
 that version is not available on CRAN
In-Reply-To: <40e66e0b0601230657h3e5c7885t375f48d32c160468@mail.gmail.com>
Message-ID: <Pine.LNX.4.10.10601231917000.27402-100000@quantex-research.co.uk>

Doug and list

No, the change as reported on my XP PC was Matrix package 0.995-1 was
found but lme4 requires >= 0.995.2.

The format change was from a dash to a dot.  

Maybe a few more would spell SOS!

But it's the sort of thing where sometimes even the geniuses very close to
the work may read the string they think they wrote by gestalt.  As long as
it is what was intended and works, it doesn't matter.

But what about working in logits.  Each new mini-version would have a
constant amount of logit added, ultimately to be changed to the next
release...:-))

Best wishes to all

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


On Mon, 23 Jan 2006, Douglas Bates wrote:

> On 1/22/06, John Logsdon <j.logsdon at quantex-research.com> wrote:
> > I've just hit this problem as well and as I am slumming it on XP at the
> > moment, I don't have the compilation tools to use the .tar.gz version.
> >
> > I also notice that the numbering format has changed.  Is this intentional?
> 
> Which numbering format?  If you mean the use of 0.995 rather than 0.99
> it is because I keep finding changes that need to be made in the
> underlying format for mixed-effects representations (i.e. mer objects)
> before a 1.0 release.  The current sequence of numbers is 0.99 (99%),
> 0.995 (99 1/2%), 0.9975 (99 3/4%), 0.99875 (99 7/8%), ...
> 
> It is a pattern like, but not as inventive as, Donald Knuth's
> numbering sequence for TeX releases.  It has the desirable property of
> allowing for an infinite number of releases before you need to declare
> the package to be at release 1.0
> 
> There will be at least a 0.9975 series of minor releases (I need to
> add another two slots to the mer object).
>



From p.murrell at auckland.ac.nz  Mon Jan 23 20:33:39 2006
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 24 Jan 2006 08:33:39 +1300
Subject: [R] lines() in heatmap()
In-Reply-To: <6bc7d02d44ebf202c02c370117b9bcd0@gmx.de>
References: <6bc7d02d44ebf202c02c370117b9bcd0@gmx.de>
Message-ID: <43D52F93.7090801@stat.auckland.ac.nz>

Hi


kaoskrew.de wrote:
> Hello!
> 
> I looked for help through google and the help-files and spend several 
> hours with trial and
> error, but didn't find a correct way.
> 
> It's all about lines in a heatmap to separate different data block from 
> each other to underline
> the significance of the found clusters!
> 
> The heatmap is build like that:
> 
> heatmap(X, Rowv=NA, Colv=NA, symm=TRUE, cexRow=0.3, cexCol=0.3)
> 
> I switched the dendrograms off, because I don't want the values sorted 
> but in the way, I
> extracted them from the data.
> 
> I tried to enter the lines that way:
> 
> lines(c(14,14),c(0,187), col="green")
> 
> That should appear as a line parallel to the x-axis, but there was no 
> line at all.
> I also tried several other ways, which didn't work at all. (The Matrix 
> contains
> 187 values and is symmetric)


Take a look at the 'add.expr' argument to heatmap.  Here's a simple example:

x  <- as.matrix(mtcars)
rc <- rainbow(nrow(x), start=0, end=.3)
cc <- rainbow(ncol(x), start=0, end=.3)
hv <- heatmap(x, col = cm.colors(256), scale="column",
               RowSideColors = rc, ColSideColors = cc, margin=c(5,10),
               xlab = "specification variables", ylab= "Car Models",
               main = "heatmap(<Mtcars data>, ..., scale = \"column\")",
               # IMPORTANT BIT HERE
               add.expr = abline(h=5, v=2))

Paul


> Also the line should start and end at the borders of the heatmap, so if 
> there is
> a parameter to enter that, I would be glad, if you add it.
> 
> Another short question:
> Does anybody have an idea, how I could switch the names of my data from 
> the heatmaps
> right side to it's left side?
> 
> 
> Thanks for help already,
> Tobias Zimmer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From john.maindonald at anu.edu.au  Mon Jan 23 20:35:00 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 24 Jan 2006 06:35:00 +1100 (EST)
Subject: [R] In which application areas is R used?
In-Reply-To: <mailman.10.1137668402.11016.r-help@stat.math.ethz.ch>
References: <mailman.10.1137668402.11016.r-help@stat.math.ethz.ch>
Message-ID: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>

If anyone has a list of application areas where there is
extensive use of R, I'd like to hear of it. My current
short list is:

Bioinformatics
Epidemiology
Geophysics
Agriculture and crop science

John Maindonald
Mathematical Sciences Institute, Australian National University.
john.maindonald at anu.edu.au



From chrysopa at gmail.com  Mon Jan 23 20:38:33 2006
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Mon, 23 Jan 2006 17:38:33 -0200
Subject: [R] simple problem
Message-ID: <200601231738.33240.chrysopa@gmail.com>

Hi,

look this:

> summary(fam??lia)
   Anacardiaceae       Annonaceae      Bombacaceae     Cecropiaceae 
               2                4                1                3 
Chrysobalanaceae       Clusiaceae    Euphorbiacaea          Fabacea 
               1                1                4                3 
        Fabaceae   Flacourtiaceae      Humiriaceae    indeterminada 
              17                2                1                2 
       Lauraceae    Malpighiaceae        Meliaceae       Mimosaceae 
               4                4                6                1 
         Moracea         Moraceae        Myrtaceae    Nyctaginaceae 
               1                1                2                1 
       Olacaceae        Rubiaceae       Sapotaceae        Violaceae 
               2                1                3                1 

> summary(fam??lia)==17
   Anacardiaceae       Annonaceae      Bombacaceae     Cecropiaceae 
           FALSE            FALSE            FALSE            FALSE 
Chrysobalanaceae       Clusiaceae    Euphorbiacaea          Fabacea 
           FALSE            FALSE            FALSE            FALSE 
        Fabaceae   Flacourtiaceae      Humiriaceae    indeterminada 
            TRUE            FALSE            FALSE            FALSE 
       Lauraceae    Malpighiaceae        Meliaceae       Mimosaceae 
           FALSE            FALSE            FALSE            FALSE 
         Moracea         Moraceae        Myrtaceae    Nyctaginaceae 
           FALSE            FALSE            FALSE            FALSE 
       Olacaceae        Rubiaceae       Sapotaceae        Violaceae 
           FALSE            FALSE            FALSE            FALSE 

> fam??lia[summary(fam??lia)==17]
[1] Fabaceae  Rubiaceae Meliaceae
24 Levels: Anacardiaceae Annonaceae Bombacaceae ... Violaceae

Why it dont work? Only Fabaceae == 17.

Thanks
Ronaldo

--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From mschwartz at mn.rr.com  Mon Jan 23 20:45:05 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Mon, 23 Jan 2006 13:45:05 -0600
Subject: [R] command in survival package
In-Reply-To: <Pine.LNX.4.64.0601231049480.32205@homer22.u.washington.edu>
References: <90B06673D826C64E8ED8EEA6B6FDF8CAE72AC9@crcmail1.BCCRC.CA>
	<Pine.LNX.4.64.0601231049480.32205@homer22.u.washington.edu>
Message-ID: <1138045505.4314.71.camel@localhost.localdomain>

On Mon, 2006-01-23 at 10:52 -0800, Thomas Lumley wrote:
> On Mon, 23 Jan 2006, Linda Lei wrote:
> 
> > Thank you guys.
> > But I tried the commands and I still get:
> >
> >> aml1<-aml[aml$group==1,]
> >> aml1
> > [1] time   status x
> > <0 rows> (or 0-length row.names)
> >> esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
> > Error in Surv(aml1$weeks, status) : Time variable is not numeric
> > In addition: Warning message:
> > is.na() applied to non-(list or vector) in: is.na(time)
> >
> > which still looks confusing. Or are they should be applied in s-plus
> > instead of R?
> 
> You *still* haven't said where the aml dataset comes from, but as I said 
> before, it isn't the same as the one built in to R, so you need to load it 
> somehow.  The one in R has columns called "time", "status" and "x"; you 
> appear to want one with columns "weeks","status", and "group", and with 
> "group" being numeric rather than factor.
> 
> 
>  	-thomas


I don't have the book with me (it's at home), but I suspect that the
problem here is that the 'aml' dataset provided at the aforementioned
web site: 

  http://www.crcpress.com/e_products/downloads/download.asp?cat_no=C4088

which is to be used with the code under discussion, was more than likely
not imported into an R session by Linda.

Since one needs to do a:

  > library(survival) 

to utilize the R code examples, that loads that package's 'aml' dataset,
resulting in:

  > aml[aml$group==1,]
[1] time   status x     
<0 rows> (or 0-length row.names)

since there is no 'group' column in the built in 'aml' dataset as Thomas
points out.

There is such a structure to the authors' version of the dataset:

> str(read.table("aml.txt", header = TRUE))
`data.frame':	23 obs. of  3 variables:
 $ weeks : num  9 13 13 18 23 28 31 34 45 48 ...
 $ group : num  1 1 1 1 1 1 1 1 1 1 ...
 $ status: num  1 1 0 1 1 0 1 1 0 1 ...


Thus:

>  aml <- read.table("aml.txt", header = TRUE)

> (aml1 <- aml[aml$group==1,])
   weeks group status
1      9     1      1
2     13     1      1
3     13     1      0
4     18     1      1
5     23     1      1
6     28     1      0
7     31     1      1
8     34     1      1
9     45     1      0
10    48     1      1
11   161     1      0


This bring us to the still remaining error in the authors' survival
code, which is not fully corrected in the authors' online errata:

> esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
Error in Surv(aml1$weeks, status) : object "status" not found


Thus, something like one of the following should be used instead:

> esf.fit <- survfit(Surv(aml1$weeks, aml1$status) ~ 1)

> esf.fit <- with(aml1, survfit(Surv(weeks, status) ~ 1))


> summary(esf.fit)
Call: survfit(formula = Surv(aml1$weeks, aml1$status) ~ 1)

 time n.risk n.event survival std.err lower 95% CI upper 95% CI
    9     11       1    0.909  0.0867       0.7541        1.000
   13     10       1    0.818  0.1163       0.6192        1.000
   18      8       1    0.716  0.1397       0.4884        1.000
   23      7       1    0.614  0.1526       0.3769        0.999
   31      5       1    0.491  0.1642       0.2549        0.946
   34      4       1    0.368  0.1627       0.1549        0.875
   48      2       1    0.184  0.1535       0.0359        0.944


I'll pass on a note to the authors, whose e-mail addresses I have
located via Google.

HTH,

Marc Schwartz



From rxg218 at psu.edu  Mon Jan 23 20:56:34 2006
From: rxg218 at psu.edu (RAJARSHI GUHA)
Date: Mon, 23 Jan 2006 14:56:34 -0500 (EST)
Subject: [R] In which application areas is R used?
Message-ID: <200601231956.OAA08144@webmail3.cac.psu.edu>



On Tue, 24 Jan 2006 06:35:00 +1100, "John Maindonald" wrote:

> If anyone has a list of application areas where there is
> extensive use of R, I'd like to hear of it. My current
> short list is:
> 
> Bioinformatics
> Epidemiology
> Geophysics
> Agriculture and crop science

Cheminformatics and QSAR

Rajarshi Guha
<rxg218 at psu.edu>
<http://jijo.cjb.net>



From stubben at lanl.gov  Mon Jan 23 21:04:06 2006
From: stubben at lanl.gov (Chris Stubben)
Date: Mon, 23 Jan 2006 13:04:06 -0700
Subject: [R] Sample rows in data frame by subsets
Message-ID: <43D536B6.4080009@lanl.gov>

Hi,

I need to resample rows in a data frame by subsets

L3 <- LETTERS[1:3]
d <- data.frame(cbind(x=1, y=1:10), fac=sample(L3, 10, repl=TRUE))
    x  y fac
1  1  1   A
2  1  2   A
3  1  3   A
4  1  4   A
5  1  5   C
6  1  6   C
7  1  7   B
8  1  8   A
9  1  9   C
10 1 10   A

I have seen this used to sample rows with replacement

d[sample(nrow(d), replace=T), ]

     x  y fac
7   1  7   B
2   1  2   A
1   1  1   A
3   1  3   A
2.1 1  2   A
10  1 10   A
8   1  8   A
9   1  9   C
1.1 1  1   A
8.1 1  8   A


but I would like to sample based on the original number in fac

summary(d$fac)
A B C
6 1 3


rbind(subset(d, fac=="A")[sample(6, replace=T), ],
       subset(d, fac=="B")[sample(1, replace=T), ],
       subset(d, fac=="C")[sample(3, replace=T), ] )

     x  y fac
2   1  2   A
3   1  3   A
3.1 1  3   A
1   1  1   A
10  1 10   A
1.1 1  1   A
7   1  7   B
5   1  5   C
6   1  6   C
5.1 1  5   C


Is there an easy way to do this in one step or with a short function?  I 
have lots of dataframes to resample.

Thanks,

Chris


-- 
-----------------
Chris Stubben

Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545



From andy_liaw at merck.com  Mon Jan 23 21:15:46 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jan 2006 15:15:46 -0500
Subject: [R] simple problem
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED73A@usctmx1106.merck.com>

From: Ronaldo Reis-Jr.
> 
> Hi,
> 
> look this:
> 
> > summary(fam??lia)
>    Anacardiaceae       Annonaceae      Bombacaceae     Cecropiaceae 
>                2                4                1                3 
> Chrysobalanaceae       Clusiaceae    Euphorbiacaea          Fabacea 
>                1                1                4                3 
>         Fabaceae   Flacourtiaceae      Humiriaceae    indeterminada 
>               17                2                1                2 
>        Lauraceae    Malpighiaceae        Meliaceae       Mimosaceae 
>                4                4                6                1 
>          Moracea         Moraceae        Myrtaceae    Nyctaginaceae 
>                1                1                2                1 
>        Olacaceae        Rubiaceae       Sapotaceae        Violaceae 
>                2                1                3                1 
> 
> > summary(fam??lia)==17
>    Anacardiaceae       Annonaceae      Bombacaceae     Cecropiaceae 
>            FALSE            FALSE            FALSE            FALSE 
> Chrysobalanaceae       Clusiaceae    Euphorbiacaea          Fabacea 
>            FALSE            FALSE            FALSE            FALSE 
>         Fabaceae   Flacourtiaceae      Humiriaceae    indeterminada 
>             TRUE            FALSE            FALSE            FALSE 
>        Lauraceae    Malpighiaceae        Meliaceae       Mimosaceae 
>            FALSE            FALSE            FALSE            FALSE 
>          Moracea         Moraceae        Myrtaceae    Nyctaginaceae 
>            FALSE            FALSE            FALSE            FALSE 
>        Olacaceae        Rubiaceae       Sapotaceae        Violaceae 
>            FALSE            FALSE            FALSE            FALSE 
> 
> > fam??lia[summary(fam??lia)==17]
> [1] Fabaceae  Rubiaceae Meliaceae
> 24 Levels: Anacardiaceae Annonaceae Bombacaceae ... Violaceae
> 
> Why it dont work? Only Fabaceae == 17.

If you look at length(familia) and length(summary(familia) == 17) perhaps
you'll see the light...

Andy

 
> Thanks
> Ronaldo
> 
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36570-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-4007                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From statistical.model at googlemail.com  Mon Jan 23 21:19:56 2006
From: statistical.model at googlemail.com (statistical.model@googlemail.com)
Date: Mon, 23 Jan 2006 20:19:56 -0000
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <200601231928.k0NJSjM4003371@meitner.gene.com>
Message-ID: <EMEELGDEKHMIAKDGLCDCMEMCCJAA.Statistical.model@gmail.com>

> Yes, you're right. For, say, a 3 x 5 design, one can do this in as few as
7
runs -- but only in general by some version of one-factor-at-a-time (OFAT)
designs, which are inefficient.  It is easy, via, say model.matrix() to
write a general function to produce these. But I think it's a bad idea; more
efiicient algorithmic designs are better, IMO, which is why I suggested
AlgDesign. You and others are free to disagree, of course.

Hi Bert,
thanks for your suggestion.
However, let us say that i need a 2x2x2x3x3x3 design, which should not be
too hard.
I've loaded AlgDesign, and i am aware now that gen.factorial allows me to
create a full desing. But how to create a main-effects-only factorial design
(orthogonal)?
I am still not able to produce what i need. The function
model.matrix.formula is not very clear... :(

Could you please indicate which syntax should i use? I'd really appreciate
your help.

Thanks in advance,

Roberto Furlan
University of Turin, Italy


----------------------------------------
La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
188 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!


From stubben at lanl.gov  Mon Jan 23 21:27:19 2006
From: stubben at lanl.gov (stubben)
Date: Mon, 23 Jan 2006 13:27:19 -0700
Subject: [R]  In which application areas is R used?
Message-ID: <524c2dc30bc0bd3162f943584236ca97@lanl.gov>

Population biology

It's dominated by matlab, but I see R used more and more.

http://popstudies.stanford.edu/summer_course.html


Chris


-- 
-----------------
Chris Stubben

Los Alamos National Lab
BioScience Division
MS M888
Los Alamos, NM 87545



From andy_liaw at merck.com  Mon Jan 23 21:33:06 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jan 2006 15:33:06 -0500
Subject: [R] Can one write a procedure in R like for instance in Maple ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED73B@usctmx1106.merck.com>

Thomas Lumley has some notes that might be very helpful for you:

http://faculty.washington.edu/tlumley/Rcourse/

Deepayan Sarkar also has some notes that might be of interest:

http://www.cs.wisc.edu/~deepayan/SIBS2005/

Andy

From: Baronin P. Storch von
> 
> 
> Dear R-wizards!
> 
> I have been learning on my own how to use this fantastic 
> program.. but I agree with some people that even with the 
> manuals, the faq and so on.. when you are sitting fully 
> alone.. progress can be ... slow... very slow indeed.. In 
> fact sometimes, looking at the "solutions" provided by  some 
> of you- I am just flabbergasted to the point that I couldn't 
> figure out how to come up with them myself (sometimes I don't 
> even understand them :-( )
> 
>  But after spending around three weeks on this, and starting 
> to get fairly obsessed with it, I decided I shall ask for 
> help,'cause i can't figure out in the documentation where I 
> should look for this.
> 
> In Maple when I want to automatize something boring I write a 
> procedure..
> here I am not too sure.. how to bind together a few 
> statements- most of which are functions... sounds like it 
> would make up a personnal macro..
> I am sure I am doing things in such a primitive way that the 
> R-specialists will wince.. but that's how it goes with beginners!
> 
> Set up.
> 
>  I have 4 fairly large data base with unequal number of lines 
> (around 1200-1500), but identical number of columns (162)-years.
>  
> for each column, I construct a data-frame with the 
> corresponding column of DB1, then from DB2, .. DB4.
> 
> This yields a data.frame in which many data are NA- some are 
> real NAs some others are because I have to take the max of 
> the lines. In any case, the number of NAs of each of these 4 
> columns is not identical.
> 
> I extract (by sorting and creating 4 new vectors) 4 vectors 
> of variable length
> -the relevant and interesting data- to whom I wish to apply 
> some standardized treatment: that is  normality with say 
> Shapiro-Wilks, Levene, etc.. Kruskal-Wallis and probably 
> other things.. 
> 
> I am not showing my tasks because I do not think that I want 
> to bother the readers with this,, rather ask for general 
> remarks ( you can provide examples of your own).
> 
> For each column I want to write the results in a table.. and 
> append these resulta for each column.
> 
> I was fairly efficient at doing that for a particular column,
> 
> but then the simple thought of  apply this "list of tasks" 
> 162 times.. makes me.. 
> feel that there should be a way.... to speed up my 
> execution.. (a loop)
> 
> However I have not been able to create a super "function" (or 
> procedure) that could tie all these statements together in a 
> sensible fashion.. because each time the data.frame created 
> is generated by a function.. and somehow i still did not 
> figure out how to write a function of functions 
> and then maybe a loop do for all values of dates =1:162 this 
> function..
> 
> (all the stuff I tried failed, because I was indexing objects 
> that were also indexed.. I am vague.. but then retracing a 3 
> weeks of trials and errors errors errors errors ...\infty :-) 
> is cumbersome)
> 
> 
> 1. Could anybody  give me suggestions where to look and maybe 
> unveil the tricks of the function of functions .. 
> 
> ideally I would construct a loop executing a super function.. 
> whose results would be dumped in a file (write.table) 
> appending each time the result of the loop i..
> 
> but I was not able to construct that...
>  should I make a wiser use of these "apply, tapply, sapply" 
> marvels? I dunno.
>  does something like for (i in 1:G){sapply(b(i),sw)} were 
> b(i) is the dataframe for column i, and SW is a function 
> (super function-procedure) make sense in R?
> 
> I see that there are some fully esoterical paragraphs on 
> things that seem to be relevant in the manuals.. but.. 
> esoterical.. I cannot make sense of them...  vicious circle..
> 
> 
> Thank you in advance for any courageous that  would give me a hint..
> 
>  Christina
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lists at nabble.com  Mon Jan 23 21:44:49 2006
From: lists at nabble.com (pat_primate (sent by Nabble.com))
Date: Mon, 23 Jan 2006 12:44:49 -0800 (PST)
Subject: [R] Easy, Robust and Stable GUI???
Message-ID: <2543480.post@talk.nabble.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/03d9aa36/attachment.pl

From andy_liaw at merck.com  Mon Jan 23 21:48:02 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jan 2006 15:48:02 -0500
Subject: [R] Sample rows in data frame by subsets
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED73C@usctmx1106.merck.com>

Here's one way, if you want to do it in one command:

do.call("rbind", lapply(split(d, d$fac), function(x) x[sample(nrow(x),
nrow(x), replace=TRUE),]))

split() splits the data into a list of data frames, by d$fac.  The lapply()
call then returns the same list, with the components replaced with the
resample of the original components.  Then just rbind them together.

Andy

From: Chris Stubben
> 
> Hi,
> 
> I need to resample rows in a data frame by subsets
> 
> L3 <- LETTERS[1:3]
> d <- data.frame(cbind(x=1, y=1:10), fac=sample(L3, 10, repl=TRUE))
>     x  y fac
> 1  1  1   A
> 2  1  2   A
> 3  1  3   A
> 4  1  4   A
> 5  1  5   C
> 6  1  6   C
> 7  1  7   B
> 8  1  8   A
> 9  1  9   C
> 10 1 10   A
> 
> I have seen this used to sample rows with replacement
> 
> d[sample(nrow(d), replace=T), ]
> 
>      x  y fac
> 7   1  7   B
> 2   1  2   A
> 1   1  1   A
> 3   1  3   A
> 2.1 1  2   A
> 10  1 10   A
> 8   1  8   A
> 9   1  9   C
> 1.1 1  1   A
> 8.1 1  8   A
> 
> 
> but I would like to sample based on the original number in fac
> 
> summary(d$fac)
> A B C
> 6 1 3
> 
> 
> rbind(subset(d, fac=="A")[sample(6, replace=T), ],
>        subset(d, fac=="B")[sample(1, replace=T), ],
>        subset(d, fac=="C")[sample(3, replace=T), ] )
> 
>      x  y fac
> 2   1  2   A
> 3   1  3   A
> 3.1 1  3   A
> 1   1  1   A
> 10  1 10   A
> 1.1 1  1   A
> 7   1  7   B
> 5   1  5   C
> 6   1  6   C
> 5.1 1  5   C
> 
> 
> Is there an easy way to do this in one step or with a short 
> function?  I 
> have lots of dataframes to resample.
> 
> Thanks,
> 
> Chris
> 
> 
> -- 
> -----------------
> Chris Stubben
> 
> Los Alamos National Lab
> BioScience Division
> MS M888
> Los Alamos, NM 87545
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gunter.berton at gene.com  Mon Jan 23 21:48:58 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 Jan 2006 12:48:58 -0800
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <EMEELGDEKHMIAKDGLCDCMEMCCJAA.Statistical.model@gmail.com>
Message-ID: <200601232048.k0NKmw57009390@meitner.gene.com>

In general, a "main effects design" need not be orthogonal -- the main
effects merely need to be estimable. The trick is to estimate them with good
efficiency, etc. I think you need to consult a local statistician for help
to understand what these statistical concepts mean.

In your example you could cross the 2^(3-1) with the 3^(3-1) to produce an
orthogonal design to estimate main effects. But of course that's 72 runs,
which I don't think you would consider "small." As a previous poster
commented, there are orthogonal mixed level arrays ("Addleman", "Kempthorne"
"Youden" -designs are a couple of phrases to try googling on) which stem
from the 1960's. I doubt that, in general, they would satisfy your needs.

I have not used the AlgDesign package myself. I suggest you direct questions
about it to the author/maintainer, Bob Wheeler.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> statistical.model at googlemail.com
> Sent: Monday, January 23, 2006 12:20 PM
> To: Berton Gunter; statistical.model at googlemail.com; 
> r-help at stat.math.ethz.ch
> Subject: [R] R: fractional factorial design in R
> 
> > Yes, you're right. For, say, a 3 x 5 design, one can do 
> this in as few as
> 7
> runs -- but only in general by some version of 
> one-factor-at-a-time (OFAT)
> designs, which are inefficient.  It is easy, via, say 
> model.matrix() to
> write a general function to produce these. But I think it's a 
> bad idea; more
> efiicient algorithmic designs are better, IMO, which is why I 
> suggested
> AlgDesign. You and others are free to disagree, of course.
> 
> Hi Bert,
> thanks for your suggestion.
> However, let us say that i need a 2x2x2x3x3x3 design, which 
> should not be
> too hard.
> I've loaded AlgDesign, and i am aware now that gen.factorial 
> allows me to
> create a full desing. But how to create a main-effects-only 
> factorial design
> (orthogonal)?
> I am still not able to produce what i need. The function
> model.matrix.formula is not very clear... :(
> 
> Could you please indicate which syntax should i use? I'd 
> really appreciate
> your help.
> 
> Thanks in advance,
> 
> Roberto Furlan
> University of Turin, Italy
> 
> 
> ----------------------------------------
> La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
> 188 messaggi contenenti spam sono stati bloccati con successo.
> Scarica gratuitamente SPAMfighter!
> 
>



From groemping at tfh-berlin.de  Mon Jan 23 22:22:42 2006
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Mon, 23 Jan 2006 22:22:42 +0100
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <EMEELGDEKHMIAKDGLCDCAEMACJAA.Statistical.model@gmail.com>
References: <20060123145934.M31708@tfh-berlin.de>
	<EMEELGDEKHMIAKDGLCDCAEMACJAA.Statistical.model@gmail.com>
Message-ID: <20060123205756.M29231@tfh-berlin.de>

I think that there is an understandable wish to have the simple orthogonal 
plans (and be it only for non-experts to be able to analyse the results 
themselves). For mixed levels, there is e.g. the L36 that should be able to 
accomodate plans like 2x2x2x3x3x3. Unfortunately, R is not very strong in 
this arena.

If I had more time, I would think about writing a package on comfortably 
designing experiments supported e.g. by the catalogues of  Chen, J., Sun, 
D.X., and Wu, C.F.J. (1993). (A catalogue of two-level and three-level 
fractional factorial designs with small runs. International Statistical 
Review 61, 131-145.) Such a package should also provide the analysis
facilities for any design generated with it, once it has been enriched with 
observed data. (This is a bit different from the typical R spirit, where 
users are often required to be experts themselves.) If anyone is planning a 
project like this or wants to make a diploma student work on it I would be 
interested in contributing. 

For the moment, if you want to implement main effects plans of the orthogonal 
sort (e.g. a Taguchi-plan like the L36) you have to use books or tables 
published on the internet, if you don't want to use expensive software like 
SPSS - not very comfortable, but possible. For example, you can find the L36 -
 which would be able to accomodate your 2x2x2x3x3x3 - in 
http://www.itl.nist.gov/div898/handbook/pri/section3/pri33a.htm.

With kind regards,
Ulrike

>In general, a "main effects design" need not be orthogonal -- the main
>effects merely need to be estimable. The trick is to estimate them with good
>efficiency, etc. I think you need to consult a local statistician for help
>to understand what these statistical concepts mean.
>
>In your example you could cross the 2^(3-1) with the 3^(3-1) to produce an
>orthogonal design to estimate main effects. But of course that's 72 runs,
>which I don't think you would consider "small." As a previous poster
>commented, there are orthogonal mixed level arrays ("Addleman", "Kempthorne"
>"Youden" -designs are a couple of phrases to try googling on) which stem
>from the 1960's. I doubt that, in general, they would satisfy your needs.
>
>I have not used the AlgDesign package myself. I suggest you direct questions
>about it to the author/maintainer, Bob Wheeler.
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process." - George E. P. Box
>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> statistical.model at googlemail.com
> Sent: Monday, January 23, 2006 12:20 PM
> To: Berton Gunter; statistical.model at googlemail.com; 
> r-help at stat.math.ethz.ch
> Subject: [R] R: fractional factorial design in R
> 
> > Yes, you're right. For, say, a 3 x 5 design, one can do 
> this in as few as
> 7
> runs -- but only in general by some version of 
> one-factor-at-a-time (OFAT)
> designs, which are inefficient. It is easy, via, say 
> model.matrix() to
> write a general function to produce these. But I think it's a 
> bad idea; more
> efiicient algorithmic designs are better, IMO, which is why I 
> suggested
> AlgDesign. You and others are free to disagree, of course.
> 
> Hi Bert,
> thanks for your suggestion.
> However, let us say that i need a 2x2x2x3x3x3 design, which 
> should not be
> too hard.
> I've loaded AlgDesign, and i am aware now that gen.factorial 
> allows me to
> create a full desing. But how to create a main-effects-only 
> factorial design
> (orthogonal)?
> I am still not able to produce what i need. The function
> model.matrix.formula is not very clear... :(
> 
> Could you please indicate which syntax should i use? I'd 
> really appreciate
> your help.
> 
> Thanks in advance,
> 
> Roberto Furlan
> University of Turin, Italy
> 
> 
> ----------------------------------------
> La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
> 188 messaggi contenenti spam sono stati bloccati con successo.
> Scarica gratuitamente SPAMfighter!
> 
>



From marcodoc75 at yahoo.com  Mon Jan 23 22:35:35 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Mon, 23 Jan 2006 13:35:35 -0800 (PST)
Subject: [R] weighted likelihood for lme
Message-ID: <20060123213535.39673.qmail@web31306.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/d375003b/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Mon Jan 23 22:44:03 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 23 Jan 2006 22:44:03 +0100
Subject: [R] Easy, Robust and Stable GUI???
In-Reply-To: <2543480.post@talk.nabble.com>
References: <2543480.post@talk.nabble.com>
Message-ID: <20060123224403.3ee520c2.Achim.Zeileis@wu-wien.ac.at>

On Mon, 23 Jan 2006 12:44:49 -0800 (PST) pat_primate (sent by
Nabble.com) wrote:

> I know that this isn't really a R help question, but I am a
> psychology student at TRU (tru.ca) and my psych department is going
> to be switching statistical software in the near future.  I thought
> this might be a good oppertunity to advocate for open source if an
> acceptable option is available.  I have looked around a bit and R
> seems to be the most stable and mature (not to mention powerful) open
> source statistical program going.  The only downfall is that the
> school has been using spss for years and would demand a similarly
> user friendly GUI based statistical program to replace it.  I have
> looked at a few of the R guis and most of them look like they are
> just command line interfaces in pretty desktop windows and not really
> a gui like spss.  If anyone knows of any stable, userfriendly and
> robust guis for R that would be similar to using spss please let me
> know, as I would love for my school to start embracing open source
> software.

Have you looked at John Fox's "Rcmdr" package?
See
  http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/
and
  John Fox (2005). "The R Commander: A Basic-Statistics Graphical User
  Interface to R", Journal of Statistical Software, 14(9). URL
  http://www.jstatsoft.org/

Also look at http://www.sciviews.org/_rgui/ for an overview of other
GUI projects.

Best,
Z

> Thanks
> 
> Pat
> --
> View this message in context:
> http://www.nabble.com/Easy%2C-Robust-and-Stable-GUI--t982193.html#a2543480
> Sent from the R help forum at Nabble.com.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Jan 23 23:01:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jan 2006 23:01:59 +0100
Subject: [R] Latest revision of lme4 requires 'Matrix' >= 0.995.2 but
	that version is not available on CRAN
In-Reply-To: <Pine.LNX.4.10.10601231917000.27402-100000@quantex-research.co.uk>
References: <Pine.LNX.4.10.10601231917000.27402-100000@quantex-research.co.uk>
Message-ID: <x2mzhm38o8.fsf@turmalin.kubism.ku.dk>

John Logsdon <j.logsdon at quantex-research.com> writes:

> Doug and list
> 
> No, the change as reported on my XP PC was Matrix package 0.995-1 was
> found but lme4 requires >= 0.995.2.
> 
> The format change was from a dash to a dot.  

As far as I remember, the version comparison logic is
separator-agnostic, so it's purely a cosmetic issue.
 
> Maybe a few more would spell SOS!

Some of the users are already going ... --- ... / .... . .-.. .--.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From phgrosjean at sciviews.org  Mon Jan 23 23:17:58 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 23 Jan 2006 23:17:58 +0100
Subject: [R] In which application areas is R used?
In-Reply-To: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
References: <mailman.10.1137668402.11016.r-help@stat.math.ethz.ch>
	<1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
Message-ID: <43D55616.3090308@sciviews.org>

You could probably add:
- Fisheries modelling
- Oceanography

Although there is still resistance of other software there, R is gaining 
more and more users in these fields.
Best,

Philippe Grosjean

P.S.: if you need actual examples, just ask... ;-)

John Maindonald wrote:
> If anyone has a list of application areas where there is
> extensive use of R, I'd like to hear of it. My current
> short list is:
> 
> Bioinformatics
> Epidemiology
> Geophysics
> Agriculture and crop science
> 
> John Maindonald
> Mathematical Sciences Institute, Australian National University.
> john.maindonald at anu.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From kilian.plank at m-lehrstuhl.de  Mon Jan 23 23:30:02 2006
From: kilian.plank at m-lehrstuhl.de (Kilian Plank)
Date: Mon, 23 Jan 2006 23:30:02 +0100
Subject: [R] varphi symbol for ylab expression
Message-ID: <F4C605A5EE93EA418CC0C5747357D4344C9603@letterman.intranet.m-lehrstuhl.de>

Hi all,

it is possible to invoke certain graphical functions (e.g. curve)
with an expression argument, e.g. "ylab=expression(phi)". There are
some greek letters with a second script. For instance, in latex
two symbols do exist: phi and varphi. Is the second symbol also 
available in an expression()? If yes, how?

Kind regards,

Kilian



From gerifalte28 at hotmail.com  Mon Jan 23 23:40:05 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 23 Jan 2006 22:40:05 +0000
Subject: [R] In which application areas is R used?
In-Reply-To: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
Message-ID: <BAY103-F316AD5B24D3180A3C667B4A6100@phx.gbl>

If it hasn't been mentioned yet, and if you want to consider this as a 
separate discipline from the ones mentioned below, we also use it for 
simulation modeling and risk analysis.

Francisco


>From: "John Maindonald" <john.maindonald at anu.edu.au>
>To: r-help at stat.math.ethz.ch
>Subject: [R] In which application areas is R used?
>Date: Tue, 24 Jan 2006 06:35:00 +1100 (EST)
>
>If anyone has a list of application areas where there is
>extensive use of R, I'd like to hear of it. My current
>short list is:
>
>Bioinformatics
>Epidemiology
>Geophysics
>Agriculture and crop science
>
>John Maindonald
>Mathematical Sciences Institute, Australian National University.
>john.maindonald at anu.edu.au
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Mon Jan 23 23:43:26 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 23 Jan 2006 23:43:26 +0100
Subject: [R] Easy, Robust and Stable GUI???
In-Reply-To: <20060123224403.3ee520c2.Achim.Zeileis@wu-wien.ac.at>
References: <2543480.post@talk.nabble.com>
	<20060123224403.3ee520c2.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <43D55C0E.8040804@sciviews.org>

Yes, Rcmdr is certainly the closest match. Note, however, that there are 
good reasons not to replicate SPSS style of GUI. This is because driving 
an analysis using scripts has three major advantages:

- flexibility: a good, high-level programming language allows to do much 
more than even the best GUI. This is because, as soon as you have 
programmed a dialog box, you restrict possibilities to the couple of 
choices that you hardcoded in that dialog box. With R, the only limit is 
your imagination (if something does not exist, you can program it by 
yourself).

- traceability: with a GUI, how do you remember all the mouse clicks 
that led you to a given analysis? On the contrary, a script is an 
accurate record of all steps.

- comprehension: to construct a R script, you must understand the 
analysis. R forces to learn the logic behind *before* you can run that 
analysis. With a menu/dialog box interface, you can click everywhere and 
get results "by chance". Statistical packages with intuitive GUIs tend 
to stimulate lazyness: "here, come! Look like I am a sexy GUI! Look, I 
am sooo easy to use! And I can deliver you plenty of results, ... No, 
no, don't be afraid, you don't need to understand what you're doing. 
Just click me!".

So, all GUIs proposed for R are just ways to learn in an easier way how 
to write R scripts. Even R Commander tries hard to stimulate use of R 
scripts... and you should consider a lack of a "good GUI" for R as a 
sign of quality and nothing else.

Best,

Philippe Grosjean

Achim Zeileis wrote:
> On Mon, 23 Jan 2006 12:44:49 -0800 (PST) pat_primate (sent by
> Nabble.com) wrote:
> 
> 
>>I know that this isn't really a R help question, but I am a
>>psychology student at TRU (tru.ca) and my psych department is going
>>to be switching statistical software in the near future.  I thought
>>this might be a good oppertunity to advocate for open source if an
>>acceptable option is available.  I have looked around a bit and R
>>seems to be the most stable and mature (not to mention powerful) open
>>source statistical program going.  The only downfall is that the
>>school has been using spss for years and would demand a similarly
>>user friendly GUI based statistical program to replace it.  I have
>>looked at a few of the R guis and most of them look like they are
>>just command line interfaces in pretty desktop windows and not really
>>a gui like spss.  If anyone knows of any stable, userfriendly and
>>robust guis for R that would be similar to using spss please let me
>>know, as I would love for my school to start embracing open source
>>software.
> 
> 
> Have you looked at John Fox's "Rcmdr" package?
> See
>   http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/
> and
>   John Fox (2005). "The R Commander: A Basic-Statistics Graphical User
>   Interface to R", Journal of Statistical Software, 14(9). URL
>   http://www.jstatsoft.org/
> 
> Also look at http://www.sciviews.org/_rgui/ for an overview of other
> GUI projects.
> 
> Best,
> Z
> 
> 
>>Thanks
>>
>>Pat
>>--
>>View this message in context:
>>http://www.nabble.com/Easy%2C-Robust-and-Stable-GUI--t982193.html#a2543480
>>Sent from the R help forum at Nabble.com.
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Charles.Annis at StatisticalEngineering.com  Mon Jan 23 23:44:14 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 23 Jan 2006 17:44:14 -0500
Subject: [R] In which application areas is R used?
In-Reply-To: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
Message-ID: <035d01c6206e$898d5470$6600a8c0@DD4XFW31>

Please don't forget engineering!  (e.g. fatigue and reliability - censored
regression and survival; quantitative nondestructive evaluation - GLM)


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Maindonald
Sent: Monday, January 23, 2006 2:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] In which application areas is R used?

If anyone has a list of application areas where there is
extensive use of R, I'd like to hear of it. My current
short list is:

Bioinformatics
Epidemiology
Geophysics
Agriculture and crop science

John Maindonald
Mathematical Sciences Institute, Australian National University.
john.maindonald at anu.edu.au

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Mon Jan 23 23:45:09 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 Jan 2006 14:45:09 -0800
Subject: [R] In which application areas is R used?
In-Reply-To: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
Message-ID: <200601232245.k0NMj9mU021740@hertz.gene.com>

Define "extensive."

I think your answers depend on your definition. I know a bunch of folks in
pharmaceutical preclinical R&D who use R for all sorts of stuff (analysis
and visualization of tox and efficacy animal studies, dose/response
modeling, PK work, IC50 determination, stability data analysis, etc.). Is
"bunch" a majority? I strongly doubt that it's near. Is it 5%, 10%, 30% ??
Dunno. Excel is still the Big Boy in most of these arenas I would bet. But I
would also bet that there are at least 1 or 2 folks in dozens of companies
who use R in for these things.

Is there a subtext to your query? -- i.e. are you trying to make an argument
for something?

-- Bert 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Maindonald
> Sent: Monday, January 23, 2006 11:35 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] In which application areas is R used?
> 
> If anyone has a list of application areas where there is
> extensive use of R, I'd like to hear of it. My current
> short list is:
> 
> Bioinformatics
> Epidemiology
> Geophysics
> Agriculture and crop science
> 
> John Maindonald
> Mathematical Sciences Institute, Australian National University.
> john.maindonald at anu.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From kilian.plank at m-lehrstuhl.de  Tue Jan 24 00:46:27 2006
From: kilian.plank at m-lehrstuhl.de (Kilian Plank)
Date: Tue, 24 Jan 2006 00:46:27 +0100
Subject: [R] (no subject)
Message-ID: <F4C605A5EE93EA418CC0C5747357D4344980D3@letterman.intranet.m-lehrstuhl.de>

Dear list members,

I have to apologize. I overlooked a remark in the docs of
mathplot. My problem is resolved.

Kind regards,

Kilian



From jporzak at gmail.com  Tue Jan 24 00:44:48 2006
From: jporzak at gmail.com (Jim Porzak)
Date: Mon, 23 Jan 2006 15:44:48 -0800
Subject: [R] In which application areas is R used?
In-Reply-To: <1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
References: <mailman.10.1137668402.11016.r-help@stat.math.ethz.ch>
	<1199.202.89.153.228.1138044900.squirrel@sqmail.anu.edu.au>
Message-ID: <2a9c000c0601231544h650cddd3w3a968b317c6459a1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/c903cea6/attachment.pl

From ruser2006 at yahoo.com  Tue Jan 24 00:52:35 2006
From: ruser2006 at yahoo.com (r user)
Date: Mon, 23 Jan 2006 15:52:35 -0800 (PST)
Subject: [R] exporting dates into Microsoft SQL Server
Message-ID: <20060123235235.15318.qmail@web37005.mail.mud.yahoo.com>

I am running R 2.1.1 in a Windows XP environment.

I wish to use the sqlSave command to export a
dataframe into Microsoft SQL.

My dataframe is called temp and has 2 ?columns?,
?monthenddate? and ?value?.

Monthenddate is in 'POSIXct', format. (i.e. 'POSIXct',
format: chr  "1984-01-31" "1984-01-31" "1984-01-31"
"1984-01-31" ...).

How can I export this dataframe into SQL and have the
format in SQL by one of the ?standard? SQL date
formats?

I am using the following r code:

db <- odbcConnect("testserver")
sqlSave(db, temp)



From MSchwartz at mn.rr.com  Tue Jan 24 03:15:11 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 23 Jan 2006 20:15:11 -0600
Subject: [R] command in survival package
Message-ID: <1138068911.5978.45.camel@localhost.localdomain>

> On Mon, 2006-01-23 at 10:52 -0800, Thomas Lumley wrote:
> > On Mon, 23 Jan 2006, Linda Lei wrote:
> > 
> > > Thank you guys.
> > > But I tried the commands and I still get:
> > >
> > >> aml1<-aml[aml$group==1,]
> > >> aml1
> > > [1] time   status x
> > > <0 rows> (or 0-length row.names)
> > >> esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
> > > Error in Surv(aml1$weeks, status) : Time variable is not numeric
> > > In addition: Warning message:
> > > is.na() applied to non-(list or vector) in: is.na(time)
> > >
> > > which still looks confusing. Or are they should be applied in
> s-plus
> > > instead of R?
> > 
> > You *still* haven't said where the aml dataset comes from, but as I
> said 
> > before, it isn't the same as the one built in to R, so you need to
> load it 
> > somehow.  The one in R has columns called "time", "status" and "x";
> you 
> > appear to want one with columns "weeks","status", and "group", and
> with 
> > "group" being numeric rather than factor.
> > 
> > 
> >  	-thomas
> 
> 
> I don't have the book with me (it's at home), but I suspect that the
> problem here is that the 'aml' dataset provided at the aforementioned
> web site: 
> 
> 
> http://www.crcpress.com/e_products/downloads/download.asp?cat_no=C4088
> 
> which is to be used with the code under discussion, was more than
> likely
> not imported into an R session by Linda.
> 
> Since one needs to do a:
> 
>   > library(survival) 
> 
> to utilize the R code examples, that loads that package's 'aml'
> dataset,
> resulting in:
> 
>   > aml[aml$group==1,]
> [1] time   status x     
> <0 rows> (or 0-length row.names)
> 
> since there is no 'group' column in the built in 'aml' dataset as
> Thomas
> points out.
> 
> There is such a structure to the authors' version of the dataset:
> 
> > str(read.table("aml.txt", header = TRUE))
> `data.frame':	23 obs. of  3 variables:
>  $ weeks : num  9 13 13 18 23 28 31 34 45 48 ...
>  $ group : num  1 1 1 1 1 1 1 1 1 1 ...
>  $ status: num  1 1 0 1 1 0 1 1 0 1 ...
> 
> 
> Thus:
> 
> >  aml <- read.table("aml.txt", header = TRUE)
> 
> > (aml1 <- aml[aml$group==1,])
>    weeks group status
> 1      9     1      1
> 2     13     1      1
> 3     13     1      0
> 4     18     1      1
> 5     23     1      1
> 6     28     1      0
> 7     31     1      1
> 8     34     1      1
> 9     45     1      0
> 10    48     1      1
> 11   161     1      0
> 
> 
> This bring us to the still remaining error in the authors' survival
> code, which is not fully corrected in the authors' online errata:
> 
> > esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)
> Error in Surv(aml1$weeks, status) : object "status" not found
> 
> 
> Thus, something like one of the following should be used instead:
> 
> > esf.fit <- survfit(Surv(aml1$weeks, aml1$status) ~ 1)
> 
> > esf.fit <- with(aml1, survfit(Surv(weeks, status) ~ 1))

Hi all,

A quick follow up here. Prof. Kim, one of the authors for the book in
question, replied to my e-mail of earlier today.

There are two issues here:

1. The confounding of the two versions of the 'aml' data set, which has
been identified above. Linda needs to be sure to import the authors'
version of the data set to be able to follow along with the code
examples and exercises in the book. 



2. In the code that Linda posted above, in combination with my not
having the book at hand earlier today, there is a single critical line
of code missing that impacts my reply with respect to the final couple
of statements.

The three full lines of code should be:

  aml1 <- aml[aml$group==1,]

  status <- rep(1, 11)

  esf.fit <- survfit(Surv(aml1$weeks,status) ~ 1)


The middle line, creating the local vector 'status', is why the code
that appears corrected in the errata does in fact work and why my
corrections above are not required. 

Hope those clarifications help.

Marc Schwartz



From john.maindonald at anu.edu.au  Tue Jan 24 03:24:28 2006
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 24 Jan 2006 13:24:28 +1100 (EST)
Subject: [R] In which application areas is R used?
Message-ID: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>

In this context "extensive" might be use of R in at least maybe 2% or 5%
of the published analyses in the area, enough to make waves and stir
awareness.

The immediate subtext is the demand of a book publisher for a list of
journals to which a new edition of a certain book might be sent for
review, and for a list of conferences where it might be given exposure.
For myself, in the medium to longer term, I am more interested in other
subtexts such as you mention, to which the answer might have relevance.

I've wondered what support there'd be for starting a database of
bibliographic information on papers where R was used for the analysis.
Authors might supply the information, or readers of a paper suggest its
addition to the database. Once well populated, this would provide a useful
indication of the range of application areas and journals where R is
finding use.  [Or has someone, somewhere, already started such a
database?]

Finance and biostatistics are obvious areas that I'd omitted.  Other areas
drawn to my attention have been telephony and electronic networks, solid
state etc manufacturing, computer system performance, oceanography and
fisheries research, risk analysis, process engineering and marketing. (I
hope my summaries are acceptably accurate).  I'm not sure what force these
other respondents have given the word "extensive".
John Maindonald
Mathematical Sciences Institute
Australian National University.
john.maindonald at anu.edu.au


Berton Gunter wrote:
> Define "extensive."
>
> I think your answers depend on your definition. I know a bunch of folks
in pharmaceutical preclinical R&D who use R for all sorts of stuff 
(analysis and visualization of tox and efficacy animal studies,
dose/response modeling, PK work, IC50 determination, stability data 
analysis, etc.). Is "bunch" a majority? I strongly doubt that it's near.
Is it 5%, 10%, 30% ?? Dunno. Excel is still the Big Boy in most of  these
arenas I would bet. But I would also bet that there are at  least 1 or 2
folks in dozens of companies who use R in for these things.
>
> Is there a subtext to your query? -- i.e. are you trying to make an
argument for something?
>
> -- Bert
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.c



From spencer.graves at pdf.com  Tue Jan 24 04:55:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 23 Jan 2006 19:55:19 -0800
Subject: [R] In which application areas is R used?
In-Reply-To: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
References: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
Message-ID: <43D5A527.7050703@pdf.com>

	  Might it be reasonable to connect this with the recent thread on a 
possible "R Wiki"?  (See, e.g., "www.sciviews.org/_rgui/wiki/doku.php".) 
  Am I correct that anyone can add an entry to a Wikipedia?  If yes, we 
just need to invite people with manuscripts and publications using R to 
post an entry.

	  What do you think?
	  spencer graves

John Maindonald wrote:

> In this context "extensive" might be use of R in at least maybe 2% or 5%
> of the published analyses in the area, enough to make waves and stir
> awareness.
> 
> The immediate subtext is the demand of a book publisher for a list of
> journals to which a new edition of a certain book might be sent for
> review, and for a list of conferences where it might be given exposure.
> For myself, in the medium to longer term, I am more interested in other
> subtexts such as you mention, to which the answer might have relevance.
> 
> I've wondered what support there'd be for starting a database of
> bibliographic information on papers where R was used for the analysis.
> Authors might supply the information, or readers of a paper suggest its
> addition to the database. Once well populated, this would provide a useful
> indication of the range of application areas and journals where R is
> finding use.  [Or has someone, somewhere, already started such a
> database?]
> 
> Finance and biostatistics are obvious areas that I'd omitted.  Other areas
> drawn to my attention have been telephony and electronic networks, solid
> state etc manufacturing, computer system performance, oceanography and
> fisheries research, risk analysis, process engineering and marketing. (I
> hope my summaries are acceptably accurate).  I'm not sure what force these
> other respondents have given the word "extensive".
> John Maindonald
> Mathematical Sciences Institute
> Australian National University.
> john.maindonald at anu.edu.au
> 
> 
> Berton Gunter wrote:
> 
>>Define "extensive."
>>
>>I think your answers depend on your definition. I know a bunch of folks
> 
> in pharmaceutical preclinical R&D who use R for all sorts of stuff 
> (analysis and visualization of tox and efficacy animal studies,
> dose/response modeling, PK work, IC50 determination, stability data 
> analysis, etc.). Is "bunch" a majority? I strongly doubt that it's near.
> Is it 5%, 10%, 30% ?? Dunno. Excel is still the Big Boy in most of  these
> arenas I would bet. But I would also bet that there are at  least 1 or 2
> folks in dozens of companies who use R in for these things.
> 
>>Is there a subtext to your query? -- i.e. are you trying to make an
> 
> argument for something?
> 
>>-- Bert
>>
>>
>>
>>>-----Original Message-----
>>>From: r-help-bounces at stat.math.ethz.ch
>>>[mailto:r-help-bounces at stat.math.ethz.c
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rwheeler at echip.com  Tue Jan 24 04:55:16 2006
From: rwheeler at echip.com (Bob Wheeler)
Date: Mon, 23 Jan 2006 22:55:16 -0500
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <20060123205756.M29231@tfh-berlin.de>
References: <20060123145934.M31708@tfh-berlin.de>	<EMEELGDEKHMIAKDGLCDCAEMACJAA.Statistical.model@gmail.com>
	<20060123205756.M29231@tfh-berlin.de>
Message-ID: <43D5A524.2000802@echip.com>

If an orthogonal main effect plan exists for the number of trials you 
specify, optFederov() in AlgDesign will more than likely find it for 
you, since such a design should be an optimal design.

Ulrike Gr??mping wrote:
> I think that there is an understandable wish to have the simple orthogonal 
> plans (and be it only for non-experts to be able to analyse the results 
> themselves). For mixed levels, there is e.g. the L36 that should be able to 
> accomodate plans like 2x2x2x3x3x3. Unfortunately, R is not very strong in 
> this arena.
> 
> If I had more time, I would think about writing a package on comfortably 
> designing experiments supported e.g. by the catalogues of  Chen, J., Sun, 
> D.X., and Wu, C.F.J. (1993). (A catalogue of two-level and three-level 
> fractional factorial designs with small runs. International Statistical 
> Review 61, 131-145.) Such a package should also provide the analysis
> facilities for any design generated with it, once it has been enriched with 
> observed data. (This is a bit different from the typical R spirit, where 
> users are often required to be experts themselves.) If anyone is planning a 
> project like this or wants to make a diploma student work on it I would be 
> interested in contributing. 
> 
> For the moment, if you want to implement main effects plans of the orthogonal 
> sort (e.g. a Taguchi-plan like the L36) you have to use books or tables 
> published on the internet, if you don't want to use expensive software like 
> SPSS - not very comfortable, but possible. For example, you can find the L36 -
>  which would be able to accomodate your 2x2x2x3x3x3 - in 
> http://www.itl.nist.gov/div898/handbook/pri/section3/pri33a.htm.
> 
> With kind regards,
> Ulrike
> 
> 
>>In general, a "main effects design" need not be orthogonal -- the main
>>effects merely need to be estimable. The trick is to estimate them with good
>>efficiency, etc. I think you need to consult a local statistician for help
>>to understand what these statistical concepts mean.
>>
>>In your example you could cross the 2^(3-1) with the 3^(3-1) to produce an
>>orthogonal design to estimate main effects. But of course that's 72 runs,
>>which I don't think you would consider "small." As a previous poster
>>commented, there are orthogonal mixed level arrays ("Addleman", "Kempthorne"
>>"Youden" -designs are a couple of phrases to try googling on) which stem
> 
>>from the 1960's. I doubt that, in general, they would satisfy your needs.
> 
>>I have not used the AlgDesign package myself. I suggest you direct questions
>>about it to the author/maintainer, Bob Wheeler.
>>
>>-- Bert Gunter
>>Genentech Non-Clinical Statistics
>>South San Francisco, CA
>>
>>"The business of the statistician is to catalyze the scientific learning
>>process." - George E. P. Box
>>
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>statistical.model at googlemail.com
>>Sent: Monday, January 23, 2006 12:20 PM
>>To: Berton Gunter; statistical.model at googlemail.com; 
>>r-help at stat.math.ethz.ch
>>Subject: [R] R: fractional factorial design in R
>>
>>
>>>Yes, you're right. For, say, a 3 x 5 design, one can do 
>>
>>this in as few as
>>7
>>runs -- but only in general by some version of 
>>one-factor-at-a-time (OFAT)
>>designs, which are inefficient. It is easy, via, say 
>>model.matrix() to
>>write a general function to produce these. But I think it's a 
>>bad idea; more
>>efiicient algorithmic designs are better, IMO, which is why I 
>>suggested
>>AlgDesign. You and others are free to disagree, of course.
>>
>>Hi Bert,
>>thanks for your suggestion.
>>However, let us say that i need a 2x2x2x3x3x3 design, which 
>>should not be
>>too hard.
>>I've loaded AlgDesign, and i am aware now that gen.factorial 
>>allows me to
>>create a full desing. But how to create a main-effects-only 
>>factorial design
>>(orthogonal)?
>>I am still not able to produce what i need. The function
>>model.matrix.formula is not very clear... :(
>>
>>Could you please indicate which syntax should i use? I'd 
>>really appreciate
>>your help.
>>
>>Thanks in advance,
>>
>>Roberto Furlan
>>University of Turin, Italy
>>
>>
>>----------------------------------------
>>La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
>>188 messaggi contenenti spam sono stati bloccati con successo.
>>Scarica gratuitamente SPAMfighter!
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Bob Wheeler --- http://www.bobwheeler.com/
    ECHIP, Inc. --- Randomness comes in bunches.



From lisas at salford-systems.com  Tue Jan 24 05:54:09 2006
From: lisas at salford-systems.com (Lisa Solomon)
Date: Mon, 23 Jan 2006 20:54:09 -0800
Subject: [R] An Appreciation of Leo Breiman (1928-2005)
Message-ID: <43D5B2F1.1050106@salford-systems.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060123/c6f964dc/attachment.pl

From petr.pikal at precheza.cz  Tue Jan 24 07:57:51 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 24 Jan 2006 07:57:51 +0100
Subject: [R] Converting from a dataset to a single "column"
In-Reply-To: <20060123174940.65652.qmail@web37008.mail.mud.yahoo.com>
Message-ID: <43D5DDFF.14575.4E288B@localhost>

Hi

?stack and ?unlist are good starting points.

HTH
Petr


On 23 Jan 2006 at 9:49, r user wrote:

Date sent:      	Mon, 23 Jan 2006 09:49:40 -0800 (PST)
From:           	r user <ruser2006 at yahoo.com>
To:             	rhelp <r-help at stat.math.ethz.ch>
Subject:        	[R] Converting from a dataset to a single "column"

> I have a dataset of 3  columns  and 5  rows .
> 
> temp<-data.frame(col1=c(5,10,14,56,7),col2=c(4,2,8,3,34),col3=c(28,4,5
> 2,34,67))
> 
> I wish to convert this to a single  column , with
> column 1 on  top  and column 3 on  bottom .
> 
> i.e.
> 
> 5
> 10
> 14
> 56
> 7
> 4
> 2
> 8
> 3
> 34
> 28
> 4
> 52
> 34
> 67
> 
> Are there any functions that do this, and that will
> work well on much larger datasets (e.g. 1000 rows,
> 6000 columns)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From torsten at hothorn.de  Tue Jan 24 08:31:16 2006
From: torsten at hothorn.de (torsten@hothorn.de)
Date: Tue, 24 Jan 2006 08:31:16 +0100 (CET)
Subject: [R] mutlivariate normal and t distributions
In-Reply-To: <43D5210E.4070302@pdf.com>
References: <20060123182118.4e5e3049.azzalini@stat.unipd.it>
	<43D5210E.4070302@pdf.com>
Message-ID: <Pine.LNX.4.51.0601240828580.18349@artemis.imbe.med.uni-erlangen.de>


On Mon, 23 Jan 2006, Spencer Graves wrote:

> 	  Thanks for this.  I got stuck some months ago, because I needed to
> evaluate a certain special normal tail region in at least 10,000
> dimensional space, and "mvtnorm" could not give me adequate numerical
> precision above about 30 or 35 dimensions.  When I get time to return to
> that project, I will be particularly interested in looking at your code.
>
> 	  I would strongly encourage you to submit your package to CRAN.

yes, Adelchi, you should definitely submit the package to CRAN!

> Otherwise it will be harder for people to learn about it and get it.  If
> your package contains functions or other objects with the same names as
> objects in other packages, that fact could create a conflict, though not
> insurmountable.  If you can make some effort to minimize the potential
> for name conflicts with other packages, that would be great.  However,
> even without that, I prefer accessibility.
>
> 	  Of course, from a user's perspective, it would be best if you and the
> 'mvtnorm' maintainer (Torsten Hothorn) could develop a consensus on the
> circumstances under which each algorithm is best and then develop a
> combined package that makes it easier for users to access the best known
> method -- and to easily switch between methods, when it may not be clear
> which is best.  However, if you and Torsten don't have time & energy for
> that, I would still encourage you to submit your package to CRAN.
>

we already talked about such ideas but, for the time being, having two
packages should not be a big problem.

Best,

Torsten


> 	  Best Wishes,
> 	  Spencer Graves
>
> Adelchi Azzalini wrote:
>
> > Dear R-help list members,
> >
> > I have created a package 'mnormt' with facilities for the multivariate
> > normal and t distributions. The core part is simply an interface to
> > Fortran routines by Alan Genz for computing the integral of two
> > densities over rectangular regions, using an adaptive integration
> > method. Other R functions compute densities and generate random
> > numbers.
> >
> > The starting motivation to write it was the  need to have functions
> > which compute the distribution functions in a non-Monte Carlo form,
> > sinse this caused me problems when these probabilities are involved in
> > a minimization problem. For this reason, I could not make use of the
> > CRAN package 'mvtnorm'.  Exactly to avoid superposition with the CRAN
> > package, 'mnormt' is made available somehere else, in case other
> > people want to use it.  The package 'mnormt' is at
> >    http://azzalini.stat.unipd.it/SN/Pkg-mnormt
> >
> > As explained, this is not uploaded to CRAN just to avoid clash with
> > the existing package. However, if it is felt appropriate, I have no
> > objection to upload it to CRAN.
> >
> > Best wishes,
> >
> > Adelchi Azzalini
>



From ripley at stats.ox.ac.uk  Tue Jan 24 09:07:34 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jan 2006 08:07:34 +0000 (GMT)
Subject: [R] varphi symbol for ylab expression
In-Reply-To: <F4C605A5EE93EA418CC0C5747357D4344C9603@letterman.intranet.m-lehrstuhl.de>
References: <F4C605A5EE93EA418CC0C5747357D4344C9603@letterman.intranet.m-lehrstuhl.de>
Message-ID: <Pine.LNX.4.61.0601240759240.8466@gannet.stats>

On Mon, 23 Jan 2006, Kilian Plank wrote:

> it is possible to invoke certain graphical functions (e.g. curve)
> with an expression argument, e.g. "ylab=expression(phi)".

Well, that's a internal (to curve, via plot.default) call to title(), and 
the use of expressions in graphical labels is called 'plotmath'.

> There are some greek letters with a second script. For instance, in 
> latex two symbols do exist: phi and varphi. Is the second symbol also 
> available in an expression()? If yes, how?

?plotmath tells you 'phi1' (in current R 2.2.1).  The next version of R 
(2.3.0) will also allow varphi as a synonym.

Be careful though: TeX (not 'latex') names for these are not standard, and 
over e.g. 'varpi' decidely confusing.  R primarily uses the Unicode/Adobe 
names.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Keith.Chamberlain at colorado.edu  Tue Jan 24 09:10:44 2006
From: Keith.Chamberlain at colorado.edu (Keith Chamberlain)
Date: Tue, 24 Jan 2006 01:10:44 -0700
Subject: [R] Relating Spectral Density to Chi-Square distribution
Message-ID: <000001c620bd$ad0cc1c0$742b8a80@komelandpc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/4bb468d9/attachment.pl

From azzalini at stat.unipd.it  Tue Jan 24 09:52:37 2006
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 24 Jan 2006 09:52:37 +0100
Subject: [R] mutlivariate normal and t distributions
In-Reply-To: <Pine.LNX.4.51.0601240828580.18349@artemis.imbe.med.uni-erlangen.de>
References: <20060123182118.4e5e3049.azzalini@stat.unipd.it>
	<43D5210E.4070302@pdf.com>
	<Pine.LNX.4.51.0601240828580.18349@artemis.imbe.med.uni-erlangen.de>
Message-ID: <20060124095237.08309435.azzalini@stat.unipd.it>

On Tue, 24 Jan 2006 08:31:16 +0100 (CET), torsten at hothorn.de wrote:

TD> 
TD>  yes, Adelchi, you should definitely submit the package to CRAN!

thanks for the feedback

I have just now uploaded the 'mnormt' package to CRAN, i.e. 
 ftp.ci.tuwien.ac.at/incoming/
perhaps at a later stage we can find a way to 
merge 'mnormt' and 'mvtnorm'

best wishes

Adelchi

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit?? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/



From Keith.Chamberlain at colorado.edu  Tue Jan 24 10:57:16 2006
From: Keith.Chamberlain at colorado.edu (Keith Chamberlain)
Date: Tue, 24 Jan 2006 02:57:16 -0700
Subject: [R] spec.pgram() normalized too what?
Message-ID: <000501c620cc$8eeb7290$742b8a80@komelandpc>

Dear list,

What on earth is spec.pgram() normalized too? If you would like to skip my
proof as to why it's not normed too the mean squared or sum squared
amplitude of the discrete function a[], feel free too skip the rest of the
message. If it is, but you know why it's not exact in spec.pgram() when it
should be, skip the rest of this message. The issue I refer herein refers
only too a single univariate time series, and may not reflect the issues
encountered in 3 or more dimensions.

I've been using Numerical Recipes too confirm my own routines, because the
periodogram estimate is not normalized to the sum squared, nor the mean
squared of the original discrete function, a[]. I'd expect the
non-overlapped, and non-tapered version of the periodogram too sum to
EXACTLY too some normalization of the discrete signal a[], and the word
"estimate" should come into play only when making inferences about the real
signal 'a' that the discretely sampled signal came from. 

>a	<-	sin(2*pi*(0:127)/16)
>N	<-	length(a)	# 128
>PSD	<-	spec.pgram(a, spans=NULL, detrend=F)

## Sum Squared amplitude of a[t] on [0, 127].
>sum(abs(a)^2)
[1] 64

## Mean Squared amplitude of a[t] on [0, 127]
sum(a^2)/N
[1] 0.5

By Parseval's theorem, the integral of the one-sided PSD over zero and
positive frequencies should equal the mean squared or sum squared amplitude
of the discrete signal a[], assuming the PSD is normalized too the
mean-square or sum squared of the signal a[] respectively. 

## Integral of the PSD returned by spec.pgram() does not equal MS or SS of
a[]
>sum(PSD$spec)
[1] 32.28501

Since the integral over positive frequencies does not equal the mean or sum
squared of the signal, I did some digging. The documentation for
spec.pgram() states that only the power at positive frequencies is returned
'due to symmetry'. Press, et al (2002) make mention of this situation.

"Be warned that one occasionally sees PSDs defined without this factor two.
These, strictly speaking, are called two-sided power spectral densities, but
some books are not careful about stating whether one- or two-sided is to be
assumed" (2002, p. 504).

As a result, I infer that spec.pgram() is returning what Press, et al. would
call a two-sided PSD. Thus, the power spectrum returned by spec.pgram()
should be multiplied by 2 between (DC, nyquist) non-inclusive, and the mean
can be resolved separately as the DC component is not returned by
spec.pgram(), as:

## N/2 used here because the length of PSD$spec is N/2 rather than N/2 + 1
#  as it does not return a DC value.

>mean(a) + PSD$spec[floor(N/2)]  + 2 * sum(PSD$spec[2:floor(N/2)-1])
[1] 64.54347

This situation is closer, and indicates that the normalization is likely too
the sum squared amplitude of a[], but it is not EXACT, as I would expect.
But I also checked a manual PSD estimate just to show the power spectrum of
a single periodicity would actually match the mean or sum squared amplitude
of a[] (the discrete function) exactly.

>a.fft	<-	fft(a)

# Two-sided estimate normed to SS Amplitude
>1/N * sum(Mod(a.fft[1:floor(N/2)+1])^2)
[1] 32

# One-sided estimate normed to SS Amplitude. The first part gets the
# quantities across the nyquist range from 0 too fc. The next 2 lines
# take out the factor 2 from DC and nyquist since those 2 terms are not 
# doubled in the one-sided estimate.

>a.PSD			<-	1/N * 2 * Mod(a.fft[1:(floor(N/2)+1)])^2
>a.PSD[1]			<-	a.PSD[1]/2	#DC freq
>a.PSD[floor(N/2)+1]	<-	a.PSD[floor(N/2)+1]/2
>sum(a.PSD)
[1] 64

This proves that the integral of the one-sided PSD estimate of the discrete
function a[] is actually exactly the same as the sum squared amplitude of
a[], at least in this simple case. Likewise when the periodogram is
normalized to the mean-squared amplitude of a[],

>sum(a.PSD)/N	# MS amplitude of a[]
[1] 0.5

## So I don't have to dimension a.PSD[], I took twice the modulus squared
#  of f[0] too f[c] inclusive all at once, and took out the erroneous
#  factor 2 from f[1] and f[c] independently

>a.PSD			<-	1/(N^2) * 2 * Mod(a.fft[1:(floor(N/2)+1)])^2
>a.PSD[1]			<-	a.PSD[1]/2
>a.PSD[floor(N/2)+1]	<-	a.PSD[floor(N/2)+1]/2

Of note, is that I have yet to encounter a case where the normalized raw
periodogram didn't equal the quantity it was normalized too, even when
segmenting data into multiple PSD estimates, and taking their average at
each frequency. I have not applied a 'window function' (Welch, Hanning, etc)
yet, but the only case I encountered where it's not exact is with
overlapping segments, and that, I suspect, is because the first section of
the first window, and last section of the last window are only used once as
opposed to twice like all of the other data points. If the last segment
wrapped around too the first in order too form the last window, I'd expect
the norming to be exact again. I have yet to encounter an estimate returned
by spec.pgram() which actually did equal the norming conventions that I've
expected too see (MS or SS amplitude).

I have tried various combinations of parameters in spec.pgram() to turn off
tapering and spans, so that essentially there is no leakage correction of
the data so I could reproduce the MS and SS amplitudes exactly, just as I
did calculating them independently. Either that can not be done with
spec.pgram(), or spec.pgram() is normalized too something other than the sum
squared amplitude of the signal a[]. 

What does spec.pgram() normalize too? Perhaps the periodogram returned by
spec.pgram() is normalized to the variance or some other quantity? If so,
perhaps someone could indicate why that particular norming convention is
used? 

The reason that this is so important is that the harmonic content fit by
least squares used in the chi-square periodogram is normalized to the
variance (I think), and so too is the Lomb periodogram. Those periodograms
can be used to get p-values from chi-square and exponential distributions
respectively, too assess "important" frequencies, but they're very slow
transforms. The fast fourier transform and/or periodograms can be related
too both methods, for fast computation (albeit, especially for the Lomb
method, it's not a direct relationship), so if the justification for
relating the fft's to any particular method for fast computation depends
partly on what the data are normed too, then it is important to be very
explicit about what conventions are used in any given routine. That way any
transforms on fourier amplitudes or, in the case of a periodogram, powers -
can be translated too and from one form of estimate or another; or if they
cannot be translated, the reason would be evident in the stated conventions.
Thus, I should hope that if spec.pgram() is normed too the SS amplitude of
the discrete function, someone could communicate why it isn't exactly the SS
amplitude, or conversely what the norming convention is.

Thank you very much for your feedback,
KeithC. - an aspiring signal analysis guru & honorary enginerd
chamberk at colorado.edu
Psych Undergrad, CU Boulder
RE McNair Scholar

"Perhaps we're the reason they call it 'psycho'physics..."



From sskim.box at gmail.com  Tue Jan 24 11:24:42 2006
From: sskim.box at gmail.com (Sung Soo Kim)
Date: Tue, 24 Jan 2006 05:24:42 -0500
Subject: [R] language setting of the R Window in Windows XP Korean ed.
Message-ID: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/a053aad2/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jan 24 11:31:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 Jan 2006 11:31:55 +0100
Subject: [R] language setting of the R Window in Windows XP Korean ed.
In-Reply-To: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
References: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
Message-ID: <43D6021B.4070907@statistik.uni-dortmund.de>

Sung Soo Kim wrote:

>  I installed R in Windows XP Korean ed.
> The problem is that R window shows Korean menus, but they are broken.
> Even worse than that, Tinn-R cannot recognize the R window.
> 
> Question:
> How can I change the Korean menus to English menus?
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Please read the R for Windows FAQs as the posting guide mentioned above 
suggests!

One exclamation discussed therein is: "I want R in English (and not in 
French/Chinese/...)!"

Uwe Ligges



From finbref.2006 at gmail.com  Tue Jan 24 11:31:32 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Tue, 24 Jan 2006 11:31:32 +0100
Subject: [R] R and Eclipse?
Message-ID: <d0f55a670601240231q538b8acfu@mail.gmail.com>

I know that (for windows users) there exists an extension for WinEdt for R.
Is there an editor plugin for eclipse as well. Eclipse is free,
cross-plattform, well known, ...
I found nothing on the web, but it seems (to me) to be a good idea (as
there is a LaTeX-plugin for eclipse as well).
Thomas



From ripley at stats.ox.ac.uk  Tue Jan 24 11:31:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jan 2006 10:31:54 +0000 (GMT)
Subject: [R] spec.pgram() normalized too what?
In-Reply-To: <000501c620cc$8eeb7290$742b8a80@komelandpc>
References: <000501c620cc$8eeb7290$742b8a80@komelandpc>
Message-ID: <Pine.LNX.4.61.0601241018480.10099@gannet.stats>

First, please look up `too' in your dictionary.

Second, please study the references on the help page, which give the 
details.  That is what references are for!  The references will also
answer your question about the reference distribution.

The help page does not say it is `normalized' at all: it says it computes 
the peridogram, and you seem unaware of the definitions of the latter (and 
beware, there are more than one).

On Tue, 24 Jan 2006, Keith Chamberlain wrote:

> Dear list,
>
> What on earth is spec.pgram() normalized too? If you would like to skip my
> proof as to why it's not normed too the mean squared or sum squared
> amplitude of the discrete function a[], feel free too skip the rest of the
> message. If it is, but you know why it's not exact in spec.pgram() when it
> should be, skip the rest of this message. The issue I refer herein refers
> only too a single univariate time series, and may not reflect the issues
> encountered in 3 or more dimensions.

[...]

> Thank you very much for your feedback,
> KeithC. - an aspiring signal analysis guru & honorary enginerd
> chamberk at colorado.edu
> Psych Undergrad, CU Boulder
> RE McNair Scholar

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jan 24 11:44:20 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jan 2006 10:44:20 +0000 (GMT)
Subject: [R] language setting of the R Window in Windows XP Korean ed.
In-Reply-To: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
References: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601241032020.10099@gannet.stats>

On Tue, 24 Jan 2006, Sung Soo Kim wrote:

> I installed R in Windows XP Korean ed.
> The problem is that R window shows Korean menus, but they are broken.

What does that mean?  If there are problems with the Korean translations, 
please report them to the translator.

> Even worse than that, Tinn-R cannot recognize the R window.

Please report bugs in Tinn-R to its maintainer.

> Question:
> How can I change the Korean menus to English menus?

This is in the rw-FAQ, and in the R-admin manual.  Please do read the
documentation.


> Thank you.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do!  (and not send HTML mail).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Jan 24 11:45:44 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 Jan 2006 11:45:44 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <d0f55a670601240231q538b8acfu@mail.gmail.com>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>
Message-ID: <43D60558.7050204@statistik.uni-dortmund.de>

Thomas Steiner wrote:

> I know that (for windows users) there exists an extension for WinEdt for R.
> Is there an editor plugin for eclipse as well. Eclipse is free,
> cross-plattform, well known, ...
> I found nothing on the web, but it seems (to me) to be a good idea (as
> there is a LaTeX-plugin for eclipse as well).
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


  RSiteSearch("Eclipse")
and try the first hit ...

Uwe Ligges



From ros110 at gmail.com  Tue Jan 24 11:47:41 2006
From: ros110 at gmail.com (Richard van Wingerden)
Date: Tue, 24 Jan 2006 11:47:41 +0100
Subject: [R] No scientific notation in format
Message-ID: <a3e689eb0601240247n5700c035m9cf43f21e9550c53@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/5fac065c/attachment.pl

From Chris.Planet at gmx.de  Tue Jan 24 12:01:32 2006
From: Chris.Planet at gmx.de (Christian Hinz)
Date: Tue, 24 Jan 2006 12:01:32 +0100 (MET)
Subject: [R]  Is there no definition for global variables in R?
References: <6203.1131548723@www95.gmx.net>
Message-ID: <2112.1138100492@www065.gmx.net>

Hello @all R-Help-User.

I need a global variable in R. The variable ought to be known for every
functions and subfunctions.  It is only to comparison purposes of the
numeric algorithms. Is there a possibility?

please answer in german if possible.

thank you in advance.

Christian Hinz

-- 
*********************
Christian Hinz 
Wickrather Str. 230 
41236 M??nchengladbach 
02166/125369



From p.dalgaard at biostat.ku.dk  Tue Jan 24 12:06:20 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 12:06:20 +0100
Subject: [R] language setting of the R Window in Windows XP Korean ed.
In-Reply-To: <Pine.LNX.4.61.0601241032020.10099@gannet.stats>
References: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
	<Pine.LNX.4.61.0601241032020.10099@gannet.stats>
Message-ID: <x2mzhl6g2b.fsf@viggo.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > Even worse than that, Tinn-R cannot recognize the R window.
> 
> Please report bugs in Tinn-R to its maintainer.

I don't think it *is* a bug, just a consequence of the communication
methods and the fact that Tinn-R's maintainer doesn't speak Korean. An
enhancement request to the maintainer, with an offer to provide the
relevant translations, could be fruitful.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ozric at web.de  Tue Jan 24 12:17:58 2006
From: ozric at web.de (Christian Schulz)
Date: Tue, 24 Jan 2006 12:17:58 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <d0f55a670601240231q538b8acfu@mail.gmail.com>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>
Message-ID: <43D60CE6.8000808@web.de>

http://www.walware.de/;jsessionid=F31C76BA1E592CDA2C82456CFA37C4FD?mainURL=/rplugin.zip

regards, christian

>I know that (for windows users) there exists an extension for WinEdt for R.
>Is there an editor plugin for eclipse as well. Eclipse is free,
>cross-plattform, well known, ...
>I found nothing on the web, but it seems (to me) to be a good idea (as
>there is a LaTeX-plugin for eclipse as well).
>Thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From p.dalgaard at biostat.ku.dk  Tue Jan 24 13:16:41 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 13:16:41 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <43D60CE6.8000808@web.de>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>
	<43D60CE6.8000808@web.de>
Message-ID: <x2irs96ct2.fsf@viggo.kubism.ku.dk>

Christian Schulz <ozric at web.de> writes:

> http://www.walware.de/;jsessionid=F31C76BA1E592CDA2C82456CFA37C4FD?mainURL=/rplugin.zip

You mean

http://www.walware.de/goto/statet

However, last I checked that one, it didn't work with Fedora's
Eclipse (Windows only?), and everything was provided as class files,
no source available. So I gave up on it. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From andrej.kastrin at siol.net  Tue Jan 24 13:25:24 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Tue, 24 Jan 2006 13:25:24 +0100
Subject: [R] Distance between axis and x.lab
Message-ID: <43D61CB4.1050606@siol.net>

Dear R useRs,

what's the most elegant way to modify distance between x-axis and it's 
title. I didn't find any parameter to do that...

Thank's in advance,

cheers, Andrej



From ripley at stats.ox.ac.uk  Tue Jan 24 13:21:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jan 2006 12:21:40 +0000 (GMT)
Subject: [R] No scientific notation in format
In-Reply-To: <a3e689eb0601240247n5700c035m9cf43f21e9550c53@mail.gmail.com>
References: <a3e689eb0601240247n5700c035m9cf43f21e9550c53@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601241220140.18400@gannet.stats>

Did you miss the 'sci' argument to format()?

On Tue, 24 Jan 2006, Richard van Wingerden wrote:

> Hi
>
> I have a data.frame with the following numbers (first column are month
> numbers)
>
> 07,0,0,0,0.315444056314174,0,0,0,12.5827462764176,0.079194498691732,
> 0.0280828101707015,0,0.0695808222378877
> 08,0,0,105600,0.393061160316545,0,0,0,8.95551253153947,0.0880023174276553,
> 0.285714285714286,0,0.0669139911789158
> 09,0,0,0,0,12.5,0,0,13.5135887094281,0.0557531529154668,0,0,
> 0.0487526139182026
> 10,0,0,0.475889042117876,0,0,0,0,10.0845573786599,0.102517312169950,0,0,
> 0.153967527953265
>
> When I write this to csv (with write.csv) R makes this of it:
> 07,0,0,0.00000e+00,0.3154441, 0.0,0,0,12.582746,0.07919450,0.02808281,0,
> 0.06958082
> 08,0,0,1.05600e+05,0.3930612, 0.0,0,0, 8.955513,0.08800232,0.28571429,0,
> 0.06691399
> 09,0,0,0.00000e+00,0.0000000,12.5,0,0,13.513589,0.05575315,0.00000000,0,
> 0.04875261
> 10,0,0,4.75889e-01,0.0000000, 0.0,0,0,10.084557,0.10251731,0.00000000,0,
> 0.15396753
>
> I am using data_frame <- format(data_frame, trim = TRUE, digits = 3, nsmall
> = 4, width = 5) for format.
> And data_frame <- formatC(data_frame, digits = 3, width = 4, format = "f",
> mode = NULL) leads to the following error:
> Error in as.integer.default(floor(log10(abs(x + ifelse(x == 0, 1, 0))))) :
>        (list) object cannot be coerced to 'integer'
>
> How do I get rid of the 'e+00' notation?
> Thanks in advance for you help.
>
> Regards,
> Richard
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dominik.heier at uni-bielefeld.de  Tue Jan 24 13:32:11 2006
From: dominik.heier at uni-bielefeld.de (Dominik Heier)
Date: Tue, 24 Jan 2006 13:32:11 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <x2irs96ct2.fsf@viggo.kubism.ku.dk>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>
	<43D60CE6.8000808@web.de> <x2irs96ct2.fsf@viggo.kubism.ku.dk>
Message-ID: <1138105931.2375.20.camel@localhost.localdomain>

Am Dienstag, den 24.01.2006, 13:16 +0100 schrieb Peter Dalgaard:
> Christian Schulz <ozric at web.de> writes:
> 
> > http://www.walware.de/;jsessionid=F31C76BA1E592CDA2C82456CFA37C4FD?mainURL=/rplugin.zip
> 
> You mean
> 
> http://www.walware.de/goto/statet
> 
> However, last I checked that one, it didn't work with Fedora's
> Eclipse (Windows only?), and everything was provided as class files,
> no source available. So I gave up on it. 
> 

It works fine on my ubuntu system with eclipse-3.1.1 and java-1.5. (Java
1.4.x is not supported by the plug-in!)
But the author has problems to start R inside Eclipse on Unix/Linux
preventing the batch-modus. This makes things like graphics and tlctk
stuff unavailable from within Eclipse.

Any ideas?



From david.reitter at gmail.com  Tue Jan 24 13:45:01 2006
From: david.reitter at gmail.com (David Reitter)
Date: Tue, 24 Jan 2006 12:45:01 +0000
Subject: [R] Condor and R
Message-ID: <C047AB28-13E9-47D5-976F-B87B159BFC55@gmail.com>

Hi,
I was wondering if anyone has successfully linked R against the  
Condor libraries so that R can be run as a Condor job in the  
"standard" (not "vanilla") universe. The advantage of this would be  
that due to checkpointing, jobs can be suspended and transferred to  
another node. There is a good overview by Xianhong Xie here:

http://cran.r-project.org/doc/Rnews/Rnews_2005-2.pdf

Unfortunately, the article points out that some restrictions of  
Condor preclude us from running R in the standard universe. Does  
anyone know if this can be overcome?

Thanks
David

--
David Reitter - ICCS/HCRC, Informatics, University of Edinburgh



From p.dalgaard at biostat.ku.dk  Tue Jan 24 13:39:40 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 13:39:40 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <1138105931.2375.20.camel@localhost.localdomain>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>
	<43D60CE6.8000808@web.de> <x2irs96ct2.fsf@viggo.kubism.ku.dk>
	<1138105931.2375.20.camel@localhost.localdomain>
Message-ID: <x2acdl6bqr.fsf@viggo.kubism.ku.dk>

Dominik Heier <dominik.heier at uni-bielefeld.de> writes:

> It works fine on my ubuntu system with eclipse-3.1.1 and java-1.5. (Java
> 1.4.x is not supported by the plug-in!)
> But the author has problems to start R inside Eclipse on Unix/Linux
> preventing the batch-modus. This makes things like graphics and tlctk
> stuff unavailable from within Eclipse.
> 
> Any ideas?

Not this week....

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ozric at web.de  Tue Jan 24 13:57:03 2006
From: ozric at web.de (Christian Schulz)
Date: Tue, 24 Jan 2006 13:57:03 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <x2irs96ct2.fsf@viggo.kubism.ku.dk>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>	<43D60CE6.8000808@web.de>
	<x2irs96ct2.fsf@viggo.kubism.ku.dk>
Message-ID: <43D6241F.7010308@web.de>

Peter Dalgaard schrieb:

>Christian Schulz <ozric at web.de> writes:
>
>  
>
>>http://www.walware.de/;jsessionid=F31C76BA1E592CDA2C82456CFA37C4FD?mainURL=/rplugin.zip
>>    
>>
>
>You mean
>
>http://www.walware.de/goto/statet
>
>However, last I checked that one, it didn't work with Fedora's
>Eclipse (Windows only?), and everything was provided as class files,
>no source available. So I gave up on it. 
>
>  
>
uups a hectic copy and paste, but
my experience the same like peter's.

regards, christian



From sdavis2 at mail.nih.gov  Tue Jan 24 14:12:16 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 Jan 2006 08:12:16 -0500
Subject: [R] Basic graphics question
Message-ID: <BFFB91E0.4482%sdavis2@mail.nih.gov>

I have a toy example given here:

    par(fig=c(0,1,0,0.05))
    par(mar=c(0,0,0,0))
    par(plt=c(0,1,0,1))
    par(oma=c(0,0,0,0))
    par(ann=F)
    plot(c(0,1),c(0,1),type='n',xlab='',ylab='',main='')
    rect(0,0,1,1,col='gray75')

What parameter am I missing to make the gray rectangle use the entire figure
region?  I am trying to build a plot from pieces, but there is a little bit
of margin left by the code above that prohibits the pieces from fitting
together perfectly.

Thanks,
Sean



From Chris.Planet at gmx.de  Tue Jan 24 14:15:28 2006
From: Chris.Planet at gmx.de (Christian Hinz)
Date: Tue, 24 Jan 2006 14:15:28 +0100 (MET)
Subject: [R]   Very important! Global variables in R?
References: <2112.1138100492@www065.gmx.net>
Message-ID: <8137.1138108528@www065.gmx.net>

Hello @all R-Help-User.

I need urgently your assistance!!!
I need a global variable in R. The variable ought to be known for every
functions and subfunctions.  It is only to comparison purposes of the
numeric algorithms. Is there a possibility?

please answer in german if possible.

thank you in advance.

Christian Hinz

-- 
*********************
Christian Hinz 
Wickrather Str. 230 
41236 M??nchengladbach 
02166/125369  
*********************

-- 
*********************
Christian Hinz 
Wickrather Str. 230 
41236 M??nchengladbach 
02166/125369



From jfox at mcmaster.ca  Tue Jan 24 14:18:09 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 24 Jan 2006 08:18:09 -0500
Subject: [R] In which application areas is R used?
In-Reply-To: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
Message-ID: <20060124131808.WUVT16473.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear John,

By this definition, I'm confident that you could add sociology and political
science to your list.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Maindonald
> Sent: Monday, January 23, 2006 9:24 PM
> To: Berton Gunter
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] In which application areas is R used?
> 
> In this context "extensive" might be use of R in at least 
> maybe 2% or 5% of the published analyses in the area, enough 
> to make waves and stir awareness.
> 
> The immediate subtext is the demand of a book publisher for a 
> list of journals to which a new edition of a certain book 
> might be sent for review, and for a list of conferences where 
> it might be given exposure.
> For myself, in the medium to longer term, I am more 
> interested in other subtexts such as you mention, to which 
> the answer might have relevance.
> 
> I've wondered what support there'd be for starting a database 
> of bibliographic information on papers where R was used for 
> the analysis.
> Authors might supply the information, or readers of a paper 
> suggest its addition to the database. Once well populated, 
> this would provide a useful indication of the range of 
> application areas and journals where R is finding use.  [Or 
> has someone, somewhere, already started such a database?]
> 
> Finance and biostatistics are obvious areas that I'd omitted. 
>  Other areas drawn to my attention have been telephony and 
> electronic networks, solid state etc manufacturing, computer 
> system performance, oceanography and fisheries research, risk 
> analysis, process engineering and marketing. (I hope my 
> summaries are acceptably accurate).  I'm not sure what force 
> these other respondents have given the word "extensive".
> John Maindonald
> Mathematical Sciences Institute
> Australian National University.
> john.maindonald at anu.edu.au
> 
> 
> Berton Gunter wrote:
> > Define "extensive."
> >
> > I think your answers depend on your definition. I know a bunch of 
> > folks
> in pharmaceutical preclinical R&D who use R for all sorts of 
> stuff (analysis and visualization of tox and efficacy animal 
> studies, dose/response modeling, PK work, IC50 determination, 
> stability data analysis, etc.). Is "bunch" a majority? I 
> strongly doubt that it's near.
> Is it 5%, 10%, 30% ?? Dunno. Excel is still the Big Boy in 
> most of  these arenas I would bet. But I would also bet that 
> there are at  least 1 or 2 folks in dozens of companies who 
> use R in for these things.
> >
> > Is there a subtext to your query? -- i.e. are you trying to make an
> argument for something?
> >
> > -- Bert
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch 
> >> [mailto:r-help-bounces at stat.math.ethz.c
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From roger.bos at gmail.com  Tue Jan 24 14:21:08 2006
From: roger.bos at gmail.com (roger bos)
Date: Tue, 24 Jan 2006 08:21:08 -0500
Subject: [R] Easy, Robust and Stable GUI???
In-Reply-To: <43D55C0E.8040804@sciviews.org>
References: <2543480.post@talk.nabble.com>
	<20060123224403.3ee520c2.Achim.Zeileis@wu-wien.ac.at>
	<43D55C0E.8040804@sciviews.org>
Message-ID: <1db726800601240521i39982645g6fdcdd87a3da143f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/e10367b1/attachment.pl

From finbref.2006 at gmail.com  Tue Jan 24 14:21:18 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Tue, 24 Jan 2006 14:21:18 +0100
Subject: [R] column of a list
Message-ID: <d0f55a670601240521v799bfee4m@mail.gmail.com>

I have a list of (same type) lists. I want to retrieve the same
entries of all my objects in the outer list.
eg:
a<-list("2006-01-23"=list(r=5,s=c(7,12,12,11,4)),
"2006-01-24"=list(r=6,s=c(3,8,8,9,12)))
a[][["s"]]

gives NULL, but I am looking for all the s-vectors in order to plot
and compare them (e changes to f in a day).
Thomas



From MSchwartz at mn.rr.com  Tue Jan 24 14:23:17 2006
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 24 Jan 2006 07:23:17 -0600
Subject: [R] Distance between axis and x.lab
In-Reply-To: <43D61CB4.1050606@siol.net>
References: <43D61CB4.1050606@siol.net>
Message-ID: <1138108997.5978.59.camel@localhost.localdomain>

On Tue, 2006-01-24 at 13:25 +0100, Andrej Kastrin wrote:
> Dear R useRs,
> 
> what's the most elegant way to modify distance between x-axis and it's 
> title. I didn't find any parameter to do that...
> 
> Thank's in advance,
> 
> cheers, Andrej

If you just want to move the x axis label and no other annotation, the
easiest way is probably:

 # Draw a plot, set the x axis label to ""
 plot(1:10, xlab = "")

 # Now use mtext() to do the label and move it 
 # away from the axis by using 'line'
 mtext(side = 1, text = "Index", line = 4)


Compare the above to:

  plot(1:10)


Further flexibility is provided by the 'ann', 'axes', 'xaxt' and 'yaxt'
arguments to skip other components of the plot such as titles and axes.

See ?plot.default, ?title, ?axis, ?mtext and ?text for more information.

HTH,

Marc Schwartz



From sdavis2 at mail.nih.gov  Tue Jan 24 14:23:21 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 Jan 2006 08:23:21 -0500
Subject: [R] Very important! Global variables in R?
In-Reply-To: <8137.1138108528@www065.gmx.net>
Message-ID: <BFFB9479.448A%sdavis2@mail.nih.gov>




On 1/24/06 8:15 AM, "Christian Hinz" <Chris.Planet at gmx.de> wrote:

> Hello @all R-Help-User.
> 
> I need urgently your assistance!!!
> I need a global variable in R. The variable ought to be known for every
> functions and subfunctions.  It is only to comparison purposes of the
> numeric algorithms. Is there a possibility?
> 
> please answer in german if possible.

 a <- 1 

is in your workspace, so it is "global" in the sense that it is accessible
to functions.  Be sure to read

http://cran.r-project.org/doc/manuals/R-intro.html

for more information....

Sean



From ripley at stats.ox.ac.uk  Tue Jan 24 14:28:07 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jan 2006 13:28:07 +0000 (GMT)
Subject: [R] Basic graphics question
In-Reply-To: <BFFB91E0.4482%sdavis2@mail.nih.gov>
References: <BFFB91E0.4482%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0601241323200.19228@gannet.stats>

On Tue, 24 Jan 2006, Sean Davis wrote:

> I have a toy example given here:
>
>    par(fig=c(0,1,0,0.05))
>    par(mar=c(0,0,0,0))
>    par(plt=c(0,1,0,1))
>    par(oma=c(0,0,0,0))
>    par(ann=F)
>    plot(c(0,1),c(0,1),type='n',xlab='',ylab='',main='')
>    rect(0,0,1,1,col='gray75')
>
> What parameter am I missing to make the gray rectangle use the entire figure
> region?  I am trying to build a plot from pieces, but there is a little bit
> of margin left by the code above that prohibits the pieces from fitting
> together perfectly.

pars 'xaxs' and 'yaxs'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Tue Jan 24 14:32:09 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 Jan 2006 08:32:09 -0500
Subject: [R] Basic graphics question
In-Reply-To: <Pine.LNX.4.61.0601241323200.19228@gannet.stats>
Message-ID: <BFFB9689.4490%sdavis2@mail.nih.gov>




On 1/24/06 8:28 AM, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:

> On Tue, 24 Jan 2006, Sean Davis wrote:
> 
>> I have a toy example given here:
>> 
>>    par(fig=c(0,1,0,0.05))
>>    par(mar=c(0,0,0,0))
>>    par(plt=c(0,1,0,1))
>>    par(oma=c(0,0,0,0))
>>    par(ann=F)
>>    plot(c(0,1),c(0,1),type='n',xlab='',ylab='',main='')
>>    rect(0,0,1,1,col='gray75')
>> 
>> What parameter am I missing to make the gray rectangle use the entire figure
>> region?  I am trying to build a plot from pieces, but there is a little bit
>> of margin left by the code above that prohibits the pieces from fitting
>> together perfectly.
> 
> pars 'xaxs' and 'yaxs'.

Thanks.  That was what I was missing.

Sean



From sdavis2 at mail.nih.gov  Tue Jan 24 14:36:31 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 Jan 2006 08:36:31 -0500
Subject: [R] column of a list
In-Reply-To: <d0f55a670601240521v799bfee4m@mail.gmail.com>
Message-ID: <BFFB978F.4494%sdavis2@mail.nih.gov>




On 1/24/06 8:21 AM, "Thomas Steiner" <finbref.2006 at gmail.com> wrote:

> I have a list of (same type) lists. I want to retrieve the same
> entries of all my objects in the outer list.
> eg:
> a<-list("2006-01-23"=list(r=5,s=c(7,12,12,11,4)),
> "2006-01-24"=list(r=6,s=c(3,8,8,9,12)))
> a[][["s"]]
> 
> gives NULL, but I am looking for all the s-vectors in order to plot
> and compare them (e changes to f in a day).

This may not be the best solution, but:

 lapply(a,function(x) {x$s})

will get your answer, I think.

Sean



From csardi at rmki.kfki.hu  Tue Jan 24 14:38:48 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 24 Jan 2006 14:38:48 +0100
Subject: [R] column of a list
In-Reply-To: <d0f55a670601240521v799bfee4m@mail.gmail.com>
References: <d0f55a670601240521v799bfee4m@mail.gmail.com>
Message-ID: <20060124133847.GA7547@rmki.kfki.hu>

> lapply(a, "[[", "s")
$"2006-01-23"
[1]  7 12 12 11  4

$"2006-01-24"
[1]  3  8  8  9 12

This is what you need?

G.

On Tue, Jan 24, 2006 at 02:21:18PM +0100, Thomas Steiner wrote:
> I have a list of (same type) lists. I want to retrieve the same
> entries of all my objects in the outer list.
> eg:
> a<-list("2006-01-23"=list(r=5,s=c(7,12,12,11,4)),
> "2006-01-24"=list(r=6,s=c(3,8,8,9,12)))
> a[][["s"]]
> 
> gives NULL, but I am looking for all the s-vectors in order to plot
> and compare them (e changes to f in a day).
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 24 14:43:08 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 24 Jan 2006 14:43:08 +0100
Subject: [R] column of a list
References: <d0f55a670601240521v799bfee4m@mail.gmail.com>
Message-ID: <001e01c620ec$1c74dfb0$0540210a@www.domain>

probably you need:

sapply(a, "[[", "s")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Thomas Steiner" <finbref.2006 at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 24, 2006 2:21 PM
Subject: [R] column of a list


>I have a list of (same type) lists. I want to retrieve the same
> entries of all my objects in the outer list.
> eg:
> a<-list("2006-01-23"=list(r=5,s=c(7,12,12,11,4)),
> "2006-01-24"=list(r=6,s=c(3,8,8,9,12)))
> a[][["s"]]
>
> gives NULL, but I am looking for all the s-vectors in order to plot
> and compare them (e changes to f in a day).
> Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From jacques.veslot at cirad.fr  Tue Jan 24 14:44:31 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 24 Jan 2006 17:44:31 +0400
Subject: [R] column of a list
In-Reply-To: <d0f55a670601240521v799bfee4m@mail.gmail.com>
References: <d0f55a670601240521v799bfee4m@mail.gmail.com>
Message-ID: <43D62F3F.9010802@cirad.fr>

lapply(a,"[[","s")

Thomas Steiner a ??crit :

>I have a list of (same type) lists. I want to retrieve the same
>entries of all my objects in the outer list.
>eg:
>a<-list("2006-01-23"=list(r=5,s=c(7,12,12,11,4)),
>"2006-01-24"=list(r=6,s=c(3,8,8,9,12)))
>a[][["s"]]
>
>gives NULL, but I am looking for all the s-vectors in order to plot
>and compare them (e changes to f in a day).
>Thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Roger.Bivand at nhh.no  Tue Jan 24 14:51:37 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 24 Jan 2006 14:51:37 +0100 (CET)
Subject: [R] Very important! Global variables in R?
In-Reply-To: <BFFB9479.448A%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.44.0601241450280.17370-100000@reclus.nhh.no>

On Tue, 24 Jan 2006, Sean Davis wrote:

> 
> 
> 
> On 1/24/06 8:15 AM, "Christian Hinz" <Chris.Planet at gmx.de> wrote:
> 
> > Hello @all R-Help-User.
> > 
> > I need urgently your assistance!!!
> > I need a global variable in R. The variable ought to be known for every
> > functions and subfunctions.  It is only to comparison purposes of the
> > numeric algorithms. Is there a possibility?
> > 
> > please answer in german if possible.

And consider reading Uwe Ligges' book:

http://www.statistik.uni-dortmund.de/~ligges/PmitR/

for access to German-language information.


> 
>  a <- 1 
> 
> is in your workspace, so it is "global" in the sense that it is accessible
> to functions.  Be sure to read
> 
> http://cran.r-project.org/doc/manuals/R-intro.html
> 
> for more information....
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From pmilin at gmail.com  Tue Jan 24 14:53:43 2006
From: pmilin at gmail.com (Petar Milin)
Date: Tue, 24 Jan 2006 14:53:43 +0100
Subject: [R] =?windows-1252?q?Can=92t_find_X11_font_when_trying_to_produce?=
	=?windows-1252?q?_graph?=
Message-ID: <10d410b60601240553p7d4bda66ifdc160e4494fa2cb@mail.gmail.com>

I am using R 2.1.1 on i486-pc-gnu-linux (Ubuntu Breezy). Recently, I
bought new laptop HP nx8220 and installed Breezy with a few problems
starting X-Windows. Nevertheless, I followed some instruction, managed
to start X, and added R from the Ubuntu's repositories. When I try to
produce any graph (like plot, bwplot etc.), I receive an error
message:
can't find X11 font
Error in X11( ....
unable to start device X11
I wonder if it is related to the problem that Ubuntu complained of, or
it is just something internal to R. In any case, I would really
appreciate help on that matter.

Sincerely,
Petar Milin



From kubovy at virginia.edu  Tue Jan 24 15:05:04 2006
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 24 Jan 2006 09:05:04 -0500
Subject: [R] In which application areas is R used?
In-Reply-To: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
References: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
Message-ID: <1D18B55E-7CE8-43C7-83C5-8C4F7CA6FB80@virginia.edu>

On Jan 23, 2006, at 9:24 PM, John Maindonald wrote:

> In this context "extensive" might be use of R in at least maybe 2%  
> or 5%
> of the published analyses in the area, enough to make waves and stir
> awareness.

It's hard to estimate, but the number of researchers in psychology  
using R is growing. See, for example, http://finzi.psych.upenn.edu/ ,  
Jonathan Baron's site, and William Revelle's http://personality- 
project.org/r/r.guide.html .

> I've wondered what support there'd be for starting a database of
> bibliographic information on papers where R was used for the analysis.

Good idea.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/



From joro.kolev at gmail.com  Tue Jan 24 15:17:25 2006
From: joro.kolev at gmail.com (Gueorgui Kolev)
Date: Tue, 24 Jan 2006 15:17:25 +0100
Subject: [R] Can R handle medium and large size data sets?
Message-ID: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>

Dear R experts,

Is it true that R generally cannot handle  medium sized data sets(a
couple of hundreds of thousands observations) and threrefore large
date set(couple of millions of observations)?

I googled and I found lots of questions regarding this issue, but
curiously there were no straightforward answers what can be done to
make R capable of handling data.

Is there sth inherent in the structure of R that makes it impossible
to work with say 100 000observations and more? If it is so, is there
any hope that R can be fixed in the future?

My experience is rather limited---I tried to load a Stata data set of
about 150000observations(which Stata handles instantly) using the
library "foreign". After half an hour R was still "thinking" so I
stopped the attempts.

Thank you in advance,

Gueorgui Kolev

Department of Economics and Business
Universitat Pompeu Fabra



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 24 15:39:57 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 24 Jan 2006 14:39:57 -0000 (GMT)
Subject: [R] Easy, Robust and Stable GUI???
In-Reply-To: <1db726800601240521i39982645g6fdcdd87a3da143f@mail.gmail.com>
Message-ID: <XFMail.060124143957.Ted.Harding@nessie.mcc.ac.uk>

On 24-Jan-06 roger bos wrote:
> This highlights the hurdle we face in increasing R users.
> People often use in their career software they learned in
> school and if we can't get the schools to use R (even though
> it free) it makes it all the more difficult to increase R
> usage in business and elsewhere.  If a business person says
> he wants a GUI so he can do some quick analysis w/o learning
> programming, fine, but if teachers and students aren't willing
> to put in a few hours to learn the basics, well, that shows
> its going to be a hard sell.  Just my 2 cents.

Rome was not built in a day, as they say.

I think that the "GUI Question" in the use of sophisticated
(specifically, for us, statistical) software is embarking on
a period of transition.

It reminds me of my early days with Linux (starting nearly
15 years ago). Then, and for a considerable time thereafter
(up to 10 years, depending on what one expected), the dearth
of what people call "applications software" was considered
by many to be the "killer argument" against using Linux.

As someone once asked me "Yes, the idea of Linux is all very
well. But where's the database, where's the word-processor,
where's the spreadsheet?"

For my own work, building on earlier Unix experience, I got
by for a long time using octave (matlab look-alike) for the
computations, and groff (ex troff) for writing documents and
reports. This was the hard way, sleeves rolled up and greasy
fingers, of doing (very effectively) what (already) others
could apparently do much more easily using the applications
(Word & Excel and the like; but of course also TeX for the
sophisticated) even on Windows-3.1 -- but (except for TeX)
with far less capability and quality of finished product,
in my view.

But, especially over the last 5 years, the "applications
software" resources available to Linux have developed
enormously. Now, such old objections are not relevant.

I think the non-GUI approach is currently in a similar
situation that Linux was in 10-15 years ago. We who know
are aware that the command-line allows complete, flexible
and *judicious* exploitation of the full resources of a
package. As has already been pointed out, GUI users are
generally limited to what has been already planted in
the menus, and are also exposed to the risk of *injudicious*
point-and-click: as Philippe Grosjean suggestively put it:
"Just click me!" -- [should there be a tee-shirt to this
effect?]

But I feel confident that, as time passes, the difference
in quality between what gets achieved (in general) with a
GUI, and what gets achieved (in general) with command-lines,
will become perceived. Of course it is possible to achive
good results with a GUI, and bad ones with commands; but that's
my overall view of what the two approaches respectively tend
to encourage.

And (as has also been pointed out), to get a result at all
via the command line requires a certain basic awareness of
the information which must be supplied to a command before
it will work at all. (It is part of the "Gui concept" that
the command which it constructs "behind the scenes" will
be syntactically, and up to a point semantically, acceptable
to the underlying software engine).

While not wishing to discourage development of a GUI interface
to R which will allow judicious and flexible usage, I do feel
inclined to discourage "giving in to the GUI lobby", simply
because that is the only way that such folks know of using
software.

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Jan-06                                       Time: 14:39:50
------------------------------ XFMail ------------------------------



From phgrosjean at sciviews.org  Tue Jan 24 16:03:22 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 24 Jan 2006 16:03:22 +0100
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
References: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
Message-ID: <43D641BA.5050802@sciviews.org>

Hello,

This is not true that R cannot handle matrices of 100 000's 
observations... but:
- Importation (typically using read.table() and the like) "saturates" 
much faster. Solution: use scan() and fill a preallocated matrix, or 
better, use a database.

- Data frames are very nice objects, but if you handle only numeric 
data, do prefer matrices: they consume less memory. Also, avoid using 
row/column names for very large matrices/data frames.

- Finally, of course, your mileage varies greatly depending on the 
calculation you do on your data.

In general, the relatively widely admitted idea that R cannot handle 
large datasets originates from: using read.table() / data frames / non 
optimized code.

As an example, I can create a matrix of 150 000 observations (you don't 
tell us how many variables, so, I took 20 columns) filled with random 
numbers, and calculate the mean for each variable very easily. Here it is:

 > gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 168994  4.6     350000  9.4   350000  9.4
Vcells  62415  0.5     786432  6.0   290343  2.3
 > system.time(a <- matrix(runif(150000 * 20), ncol = 20))
[1] 0.48 0.05 0.55   NA   NA
 > # Just a little bit more than half a second to create a table of
 > # 3 millions entries filled with random numbers (P IV, 3Ghz, Win XP)
 > dim(a)
[1] 150000     20

 > system.time(print(colMeans(a)))
  [1] 0.4998859 0.5004760 0.4994155 0.5000711 0.5005029
  [6] 0.4999672 0.5003233 0.5000419 0.4997827 0.5004858
[11] 0.5004905 0.4993428 0.4991187 0.5000143 0.5016212
[16] 0.4988943 0.4990586 0.5009718 0.4997235 0.5001220
[1] 0.03 0.00 0.03   NA   NA
 > # 30 milliseconds to calculate the mean of all 20
 > # variables over 150 000 observations

 > gc()
           used (Mb) gc trigger (Mb) max used (Mb)
Ncells  169514  4.6     350000  9.4   350000  9.4
Vcells 3062785 23.4    9317558 71.1  9062793 69.2
 > # Less than 30 Mb used (with a peak at 80 Mb)

Isn't it manageable?
Best,

Philippe Grosjean

Gueorgui Kolev wrote:
> Dear R experts,
> 
> Is it true that R generally cannot handle  medium sized data sets(a
> couple of hundreds of thousands observations) and threrefore large
> date set(couple of millions of observations)?
> 
> I googled and I found lots of questions regarding this issue, but
> curiously there were no straightforward answers what can be done to
> make R capable of handling data.
> 
> Is there sth inherent in the structure of R that makes it impossible
> to work with say 100 000observations and more? If it is so, is there
> any hope that R can be fixed in the future?
> 
> My experience is rather limited---I tried to load a Stata data set of
> about 150000observations(which Stata handles instantly) using the
> library "foreign". After half an hour R was still "thinking" so I
> stopped the attempts.
> 
> Thank you in advance,
> 
> Gueorgui Kolev
> 
> Department of Economics and Business
> Universitat Pompeu Fabra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From francoisromain at free.fr  Tue Jan 24 16:16:39 2006
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 24 Jan 2006 16:16:39 +0100
Subject: [R] R and Eclipse?
In-Reply-To: <x2irs96ct2.fsf@viggo.kubism.ku.dk>
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>	<43D60CE6.8000808@web.de>
	<x2irs96ct2.fsf@viggo.kubism.ku.dk>
Message-ID: <43D644D7.50501@free.fr>

Le 24.01.2006 13:16, Peter Dalgaard a ??crit :

>Christian Schulz <ozric at web.de> writes:
>
>  
>
>>http://www.walware.de/;jsessionid=F31C76BA1E592CDA2C82456CFA37C4FD?mainURL=/rplugin.zip
>>    
>>
>
>You mean
>
>http://www.walware.de/goto/statet
>
>However, last I checked that one, it didn't work with Fedora's
>Eclipse (Windows only?), and everything was provided as class files,
>no source available. So I gave up on it. 
>  
>
Hi,

I'm running it with no problem with Fedora 3 and Eclipse 3.1.
Installation is quite simple with the eclipse update feature

Maybe you use gcj instead of sun java ?

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From dominik.heier at uni-bielefeld.de  Tue Jan 24 16:26:40 2006
From: dominik.heier at uni-bielefeld.de (Dominik Heier)
Date: Tue, 24 Jan 2006 16:26:40 +0100
Subject: [R] =?iso-8859-7?Q?Can=A2t?= find X11 font when trying to	produce
	graph
In-Reply-To: <10d410b60601240553p7d4bda66ifdc160e4494fa2cb@mail.gmail.com>
References: <10d410b60601240553p7d4bda66ifdc160e4494fa2cb@mail.gmail.com>
Message-ID: <1138116401.7887.13.camel@localhost.localdomain>

I don't think this is a R internal problem. It seems that some fonts are
missing that were not included in your X11 installation.
However, you could upgrade to R 2.2.1 and hope that the problem
disappears.

Do so by adding:
"deb http://cran.R-project.org/bin/linux/debian stable/"
to your /etc/apt/sources.list (or via synaptic) and reinstall/update R.

On my ubuntu-breezy system (no laptop, amd-k7) I never had any problems
with this configuration.

Hope this helps
Dominik

Am Dienstag, den 24.01.2006, 14:53 +0100 schrieb Petar Milin:
> I am using R 2.1.1 on i486-pc-gnu-linux (Ubuntu Breezy). Recently, I
> bought new laptop HP nx8220 and installed Breezy with a few problems
> starting X-Windows. Nevertheless, I followed some instruction, managed
> to start X, and added R from the Ubuntu's repositories. When I try to
> produce any graph (like plot, bwplot etc.), I receive an error
> message:
> can't find X11 font
> Error in X11( ....
> unable to start device X11
> I wonder if it is related to the problem that Ubuntu complained of, or
> it is just something internal to R. In any case, I would really
> appreciate help on that matter.
> 
> Sincerely,
> Petar Milin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From liuwensui at gmail.com  Tue Jan 24 16:51:09 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 24 Jan 2006 10:51:09 -0500
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
References: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
Message-ID: <1115a2b00601240751g779482e7m5e540e71c6a5433a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/2157c12c/attachment.pl

From tlumley at u.washington.edu  Tue Jan 24 17:06:58 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 24 Jan 2006 08:06:58 -0800 (PST)
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
References: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0601240750490.25749@homer22.u.washington.edu>

On Tue, 24 Jan 2006, Gueorgui Kolev wrote:

> Dear R experts,
>
> Is it true that R generally cannot handle  medium sized data sets(a
> couple of hundreds of thousands observations) and threrefore large
> date set(couple of millions of observations)?
>
> I googled and I found lots of questions regarding this issue, but
> curiously there were no straightforward answers what can be done to
> make R capable of handling data.

Because it depends on the situation.

> My experience is rather limited---I tried to load a Stata data set of
> about 150000observations(which Stata handles instantly) using the
> library "foreign". After half an hour R was still "thinking" so I
> stopped the attempts.

Like Stata, R prefers to store all the data in memory, but because of R's 
flexibility it takes more memory than Stata does, and for simple analyses 
is slower. For simple analyses Stata probably needs only 10-20% as much 
memory as R on a given data set.

If you have a 64-bit version of R it can handle quite large data sets, 
certainly millions of records.  On the other hand an ordinary PC might 
well start to slow down noticeably with a few tens of thousands of 
reasonably complex records.

Often it is not necessary to store all the data in memory at once, and 
there are database interfaces to make this easier.

R (and S before it) have generally assumed that increasing computer power 
will solve a lot of problems more easily than programming would, and have 
generally been correct.

If you want Stata, you know where to find it (and it's a good choice for 
many problems).


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From gunter.berton at gene.com  Tue Jan 24 17:06:47 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 24 Jan 2006 08:06:47 -0800
Subject: [R] In which application areas is R used?
In-Reply-To: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
Message-ID: <200601241606.k0OG6lrv026434@ohm.gene.com>

John:

> In this context "extensive" might be use of R in at least 
> maybe 2% or 5%
> of the published analyses in the area, enough to make waves and stir
         ^^^^^^^^^


So publication is the criterion by which you wish to define things?
Personally, I think it's rather narrow -- perhaps even myopic. More to the
point, it excludes a large portion of the industrial (maybe I should say,
non-academic -- though neither term is quite right) user base, who generally
do not publish (a large part of) their work. While I would consider my use
of R  (economically, certainly) important, by your definition it does not
count. So I suspect that your application list would narrow.


> awareness.
> 
> The immediate subtext is the demand of a book publisher for a list of
> journals to which a new edition of a certain book might be sent for
> review, and for a list of conferences where it might be given 
> exposure.
> For myself, in the medium to longer term, I am more 
> interested in other
> subtexts such as you mention, to which the answer might have 
> relevance.
> 
Your definition would likely also exclude quite a few industrial
pharmaceutical conferences where R users meet and, **even** occasionally
present (but not publish).

Cheers,
Bert

> I've wondered what support there'd be for starting a database of
> bibliographic information on papers where R was used for the analysis.
> Authors might supply the information, or readers of a paper 
> suggest its
> addition to the database. Once well populated, this would 
> provide a useful
> indication of the range of application areas and journals where R is
> finding use.  [Or has someone, somewhere, already started such a
> database?]
> 
> Finance and biostatistics are obvious areas that I'd omitted. 
>  Other areas
> drawn to my attention have been telephony and electronic 
> networks, solid
> state etc manufacturing, computer system performance, oceanography and
> fisheries research, risk analysis, process engineering and 
> marketing. (I
> hope my summaries are acceptably accurate).  I'm not sure 
> what force these
> other respondents have given the word "extensive".
> John Maindonald
> Mathematical Sciences Institute
> Australian National University.
> john.maindonald at anu.edu.au
> 
> 
> Berton Gunter wrote:
> > Define "extensive."
> >
> > I think your answers depend on your definition. I know a 
> bunch of folks
> in pharmaceutical preclinical R&D who use R for all sorts of stuff 
> (analysis and visualization of tox and efficacy animal studies,
> dose/response modeling, PK work, IC50 determination, stability data 
> analysis, etc.). Is "bunch" a majority? I strongly doubt that 
> it's near.
> Is it 5%, 10%, 30% ?? Dunno. Excel is still the Big Boy in 
> most of  these
> arenas I would bet. But I would also bet that there are at  
> least 1 or 2
> folks in dozens of companies who use R in for these things.
> >
> > Is there a subtext to your query? -- i.e. are you trying to make an
> argument for something?
> >
> > -- Bert
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.c
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From csardi at rmki.kfki.hu  Tue Jan 24 17:20:00 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Tue, 24 Jan 2006 17:20:00 +0100
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <1115a2b00601240751g779482e7m5e540e71c6a5433a@mail.gmail.com>
References: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
	<1115a2b00601240751g779482e7m5e540e71c6a5433a@mail.gmail.com>
Message-ID: <20060124162000.GI7547@rmki.kfki.hu>

Completely agree, i use R to analyze graphs with millions of vertices and
tens of millions of edges. (Of course this is a bit different than working
with data frames.)

I think your problem is that the foreign package parses/converts your data
file slowly, convert it to R format and it will be much faster. 
The conversion can be a pain, but ideally you have to do it only once.

Gabor

On Tue, Jan 24, 2006 at 10:51:09AM -0500, Wensui Liu wrote:
> my experience is that 100,000 shouldn't be a problem. of course, it also
> depends on your computer configuration.
> 
> On 1/24/06, Gueorgui Kolev <joro.kolev at gmail.com> wrote:
> >
> > Dear R experts,
> >
> > Is it true that R generally cannot handle  medium sized data sets(a
> > couple of hundreds of thousands observations) and threrefore large
> > date set(couple of millions of observations)?
> >
[...]

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From stratja at auburn.edu  Tue Jan 24 17:33:49 2006
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Tue, 24 Jan 2006 10:33:49 -0600
Subject: [R] nested ANCOVA: still confused
Message-ID: <43D6028D020000F20000619D@TMIA1.AUBURN.EDU>

Dear R-users,

I did some more research and I'm still not sure how to set up an ANCOVA
with nestedness.  Specifically I'm not sure how to express chicks nested
within boxes.  I will be getting Pinheiro & Bates (Mixed Effects Models
in S and S-Plus) but it will not arrive for another two weeks from our
interlibrary loan.

The goal is to determine if there are urbanization (purban) effects on
chick health (rtot) and if there are differences between sexes (sex) and
the effect of being in the same clutch (box).

The model is rtot = sex + purban + (chick)box.

I've loaded the package lme4.  And the code I have so far is

bb <- read.csv("C:\\eabl\\eabl_feather04.csv", header=TRUE)
bb$sex <- factor(bb$sex)
rtot.lme <- lme(bb$rtot~bb$sex, bb$purban|bb$chick/bb$box,
na.action=na.omit)

but this is not working.

Any suggestions would be greatly appreciated.

Thanks,

Jeff








****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja



From lance at quantumbioinc.com  Tue Jan 24 17:50:43 2006
From: lance at quantumbioinc.com (Lance Westerhoff)
Date: Tue, 24 Jan 2006 11:50:43 -0500
Subject: [R] R vs. Excel (R-squared)
Message-ID: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>

Hello All-

I found an inconsistency between the R-squared reported in Excel vs.  
that in R, and I am wondering which (if any) may be correct and if  
this is a known issue.  While it certainly wouldn't surprise me if  
Excel is just flat out wrong, I just want to make sure since the R- 
squared reported in R seems surprisingly high.  Please let me know if  
this is the wrong list.  Thanks!

To begin, I have a set of data points in which the y is the  
experimental number and x is the predicted value.  The Excel- 
generated graph (complete with R^2 and trend line) is provided at  
this link if you want to take a look:

http://www.quantumbioinc.com/downloads/public/excel.png

As you can see, the R-squared that is reported by Excel is -0.1005.   
Now when I bring the same data into R, I get an R-square of +0.9331  
(see below).  Being that I am new to R and semi-new to stats, is  
there a difference between "multiple R-squared" and R-squared that  
perhaps I am simply interpreting this wrong, or is this a known  
inconsistency between the two applications?  If so, which is  
correct?  Any insight would be greatly appreciated!


======================

 > # note: a is experimental and c is predicted
 > summary(lm(a~c-1))

Call:
lm(formula = a ~ c - 1)

Residuals:
     Min      1Q  Median      3Q     Max
-2987.6 -1126.6  -181.7   855.3  5602.8

Coefficients:
   Estimate Std. Error t value Pr(>|t|)
c  0.99999    0.01402   71.33   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 1423 on 365 degrees of freedom
Multiple R-Squared: 0.9331,	Adjusted R-squared: 0.9329
F-statistic:  5088 on 1 and 365 DF,  p-value: < 2.2e-16

 > version
          _
platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

======================


Thank you very much for your time!

-Lance
____________________
Lance M. Westerhoff, Ph.D.
General Manager
QuantumBio Inc.

WWW:    http://www.quantumbioinc.com
Email:    lance at quantumbioinc.com


"Safety is not the most important thing. I know this sounds like heresy,
but it is a truth that must be embraced in order to do exploration.
The most important thing is to actually go."  ~ James Cameron



From spencer.graves at pdf.com  Tue Jan 24 17:52:21 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 Jan 2006 08:52:21 -0800
Subject: [R] Within-Subjects ANOVA & comparisons of individual means
In-Reply-To: <43D09986.4070604@mail.gwdg.de>
References: <43D09986.4070604@mail.gwdg.de>
Message-ID: <43D65B45.7060704@pdf.com>

	  1.  Did you try "summary(aovRes, ...)" rather than 
"summary.aov(aovRes, ...)"?  From "summary.aov", I got the same error 
message you did, but from "summary(aovRes, ...)", I got something that 
looked like what you were expecting.

	  2.  To understand this, look at the code for "summary":  It consists 
essentially of 'UseMethod("summary")'.  To trace methods dispatch here, 
note that 'class(aovRes)' is a 2-vector:  c("aovlist", "listof").  When 
I requested 'methods("summary"), I got a long list, the first two were 
'summary.aov' and 'summary.aovlist'.  Since aovRes was NOT of class 
'aov' but instead of class 'aovlist', summary(aovRes, ...) uses 
'summary.aovlist'.  You got an error message, because summary.aov was 
expecting an argument of class 'aov', and 'aovRes' didn't have the 
structure it required.

	  3.  To find an attribute "contrasts" in aovRes, I requested 
'str(aovRes)' and searched for "contrasts".  I found it in two places:

attr(attr(aovRes, "error.qr")$qr, "contrasts")
attr(aovRes, "contrasts")

	  4.  I wondered why you said, "From my understanding, TukeyHSD is not 
appropriate in this context."  Then I discovered that TukeyHSD is 
officially a generic function with a method only for objects of class 
"aov".  Since aovRes is NOT of class "aov", it can't use that function.
However, it looks to me like the same algorithm could be used for what 
you want, e.g., by copying "TukeyHSD.aov" into a script file, changing 
the name to "TukeyHSD.aovlist" and then walking through the code line by 
line, and changing things as necessary to produce the desired results. 
If I wanted to do this, I might first check with the author of TukeyHSD 
(Douglas Bates).  He might know some reason why it is inappropriate or 
some other special concerns of which I'm unaware.  Or he might have a 
not-quite-fully debugged version someplace that could help you.

	  Best Wishes,
	  Spencer Graves

Steffen Katzner wrote:

> I am having problems with comparing individual means in a
> within-subjects ANOVA. From my understanding, TukeyHSD is not
> appropriate in this context. So I am trying to compute contrasts, as
> follows:
> 
> seven subjects participated in each of 6 conditions (intervals).
> 
>>subject = factor(rep(c(1:7), each = 6))
>>interval = factor(rep(c(1:6), 7))
> 
> and here is the dependent variable:
> 
>>dv = c(3.3868, 3.1068, 1.7261, 1.5415, 1.7356, 0.7538,
> 
> + 2.5957, 1.5666, 1.1984, 1.2761, 1.0022, 0.8597,
> + 3.9819, 3.1506, 1.5824, 1.7400, 1.4248, 0.6519,
> + 2.2521, 1.5248, 1.1209, 1.2193, 1.1994, 2.0910,
> + 2.4661, 1.3863, 1.3591, 0.9163, 1.3976, 1.7471,
> + 3.2486, 1.9492, 2.4228, 1.1276, 1.2836, 0.9814,
> + 1.7148, 1.7278, 2.7433, 1.4924, 1.0992, 0.7821)
> 
> 
>>d = data.frame(subject, interval, dv)
> 
> next I'm defining a contrast matrix:
> 
>>con = matrix(c(1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 
> 
> 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1), nrow=6, ncol=5, byrow=F)
> 
>>con
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    0    0    0    0
> [2,]   -1    1    0    0    0
> [3,]    0   -1    1    0    0
> [4,]    0    0   -1    1    0
> [5,]    0    0    0   -1    1
> [6,]    0    0    0    0   -1
> 
>>contrasts(d$interval)=con
> 
> and then I'm doing the ANOVA
> 
>>aovRes = aov(dv~interval+Error(subject/interval), data=d)
> 
>>summary(aovRes)
> 
> Error: subject
>            Df  Sum Sq Mean Sq F value Pr(>F)
> Residuals  6 2.48531 0.41422
> 
> Error: subject:interval
>            Df  Sum Sq Mean Sq F value    Pr(>F)
> interval   5 13.8174  2.7635  8.7178 3.417e-05 ***
> Residuals 30  9.5098  0.3170
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> but if I want to look at the contrasts, something has gone wrong:
> 
> summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs
> i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))
> 
> Error in 1:object$rank : NA/NaN argument
> 
>>aovRes$contrasts
> 
> NULL
> 
> Can anybody help?
> Thank you very much,  -Steffen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From liuwensui at gmail.com  Tue Jan 24 17:59:02 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 24 Jan 2006 11:59:02 -0500
Subject: [R] Easy, Robust and Stable GUI???
In-Reply-To: <2543480.post@talk.nabble.com>
References: <2543480.post@talk.nabble.com>
Message-ID: <1115a2b00601240859h261d09beofa7e0f8c1d5cbee0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/e6bd1cd7/attachment.pl

From fernandomayer at gmail.com  Tue Jan 24 18:00:05 2006
From: fernandomayer at gmail.com (Fernando Mayer)
Date: Tue, 24 Jan 2006 15:00:05 -0200
Subject: [R] =?ISO-8859-1?Q?Can=A2t_find_X11_font_when_tryi?=
	=?ISO-8859-1?Q?ng_to_produce_graph?=
In-Reply-To: <1138116401.7887.13.camel@localhost.localdomain>
References: <10d410b60601240553p7d4bda66ifdc160e4494fa2cb@mail.gmail.com>
	<1138116401.7887.13.camel@localhost.localdomain>
Message-ID: <43D65D15.8090502@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/33255736/attachment.pl

From Laetitia.Marisa at cgm.cnrs-gif.fr  Tue Jan 24 18:05:48 2006
From: Laetitia.Marisa at cgm.cnrs-gif.fr (Laetitia Marisa)
Date: Tue, 24 Jan 2006 18:05:48 +0100
Subject: [R] Number of replications of a term
Message-ID: <43D65E6C.3080408@cgm.cnrs-gif.fr>

Hello,

Is there a simple and fast function that returns a vector of the number 
of replications for each object of a vector ?
For example :
I have a vector of IDs :
ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")

 I want the function returns the following vector where each term is the 
number of replicates for the given id :
c( 1, 2, 2, 3,3,3,1 )

Of course I have a vector of more than 40 000 ID and the function I 
wrote (it orders my data and checks on ID:Name of the data if the next 
term is the same as the previous one (see below) ) is really slow 
(30minutes for 44290 terms). But I don't have time by now to write a C 
function.
Thanks a lot for your help,

Laetitia.



Here is the function I have written maybe I have done something not 
optimized :

repVector <- function(obj){

    # order IDName 
    ord <- gif.indexByIDName(obj)
    ordobj <- obj[ord,]

    nspots <- nrow(obj)
    # vector of spot replicates number           
    spotrep <- rep(NA, nspots )
   
    # function to get ID:Name for a given spot       
    spotidname <- function(ind){
            paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
    }

    spot <- 1   

    while( spot < nspots ){
        i<-1
        while( spotidname(spot) == spotidname(spot + i) ){            
       
            i <- i + 1
        }
       
        spotrep[spot : (spot + i-1)] <- i
        spot <- spot + i
        #cat("spot : ",spot,"\n")   
    }       
   
    obj$genes$spotrep <- spotrep[order(ord)]

    obj
               
}



From p.dalgaard at biostat.ku.dk  Tue Jan 24 18:08:15 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 18:08:15 +0100
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
Message-ID: <x23bjdk0zk.fsf@viggo.kubism.ku.dk>

Lance Westerhoff <lance at quantumbioinc.com> writes:

> Hello All-
> 
> I found an inconsistency between the R-squared reported in Excel vs.  
> that in R, and I am wondering which (if any) may be correct and if  
> this is a known issue.  While it certainly wouldn't surprise me if  
> Excel is just flat out wrong, I just want to make sure since the R- 
> squared reported in R seems surprisingly high.  Please let me know if  
> this is the wrong list.  Thanks!

Excel is flat out wrong. As the name implies, R-squared values cannot
be less than zero (adjusted R-squared can, but I wouldn't think
that is what Excel does).

R-squared is a bit odd in the zero intercept case because it
describes how much better the line describes data compared to a
horizontal line *at zero*. However, it doesn't really makes sense to
compare with a non-zero constant, because the models are not nested. 
 


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From NordlDJ at dshs.wa.gov  Tue Jan 24 18:11:25 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan)
Date: Tue, 24 Jan 2006 09:11:25 -0800
Subject: [R] R vs. Excel (R-squared)
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED824@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Lance Westerhoff
> Sent: Tuesday, January 24, 2006 8:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R vs. Excel (R-squared)
> 
> Hello All-
> 
> I found an inconsistency between the R-squared reported in Excel vs.
> that in R, and I am wondering which (if any) may be correct and if
> this is a known issue.  While it certainly wouldn't surprise me if
> Excel is just flat out wrong, I just want to make sure since the R-
> squared reported in R seems surprisingly high.  Please let me know if
> this is the wrong list.  Thanks!
> 
<<snip>>
> 
>  > # note: a is experimental and c is predicted
>  > summary(lm(a~c-1))
> 
> Call:
> lm(formula = a ~ c - 1)
> 
> Residuals:
>      Min      1Q  Median      3Q     Max
> -2987.6 -1126.6  -181.7   855.3  5602.8
> 
> Coefficients:
>    Estimate Std. Error t value Pr(>|t|)
> c  0.99999    0.01402   71.33   <2e-16 ***
> ---
> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
> 
> Residual standard error: 1423 on 365 degrees of freedom
> Multiple R-Squared: 0.9331,	Adjusted R-squared: 0.9329
> F-statistic:  5088 on 1 and 365 DF,  p-value: < 2.2e-16
> 
<<snip>>

> Thank you very much for your time!
> 
> -Lance
> ____________________
> Lance M. Westerhoff, Ph.D.
> General Manager
> QuantumBio Inc.
> 
> WWW:    http://www.quantumbioinc.com
> Email:    lance at quantumbioinc.com
> 
> 


Lance,

Did you force the regression through the origin in Excel, like you are doing
with your R code?  And why are you doing the regression without an intercept
in R?

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204



From Bernhard_Pfaff at fra.invesco.com  Tue Jan 24 18:14:19 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 24 Jan 2006 17:14:19 -0000
Subject: [R] R vs. Excel (R-squared)
Message-ID: <25D1C2585277D311B9A20000F6CCC71B077C03CB@DEFRAEX02>

Hello Lance,

this was discussed on the list lately, see:

http://tolstoy.newcastle.edu.au/~rking/R/help/06/01/18934.html 


Bernhard

-----Urspr??ngliche Nachricht-----
Von: Lance Westerhoff [mailto:lance at quantumbioinc.com] 
Gesendet: Dienstag, 24. Januar 2006 17:51
An: r-help at stat.math.ethz.ch
Betreff: [R] R vs. Excel (R-squared)

Hello All-

I found an inconsistency between the R-squared reported in Excel vs.  
that in R, and I am wondering which (if any) may be correct and if  
this is a known issue.  While it certainly wouldn't surprise me if  
Excel is just flat out wrong, I just want to make sure since the R- 
squared reported in R seems surprisingly high.  Please let me know if  
this is the wrong list.  Thanks!

To begin, I have a set of data points in which the y is the  
experimental number and x is the predicted value.  The Excel- 
generated graph (complete with R^2 and trend line) is provided at  
this link if you want to take a look:

http://www.quantumbioinc.com/downloads/public/excel.png

As you can see, the R-squared that is reported by Excel is -0.1005.   
Now when I bring the same data into R, I get an R-square of +0.9331  
(see below).  Being that I am new to R and semi-new to stats, is  
there a difference between "multiple R-squared" and R-squared that  
perhaps I am simply interpreting this wrong, or is this a known  
inconsistency between the two applications?  If so, which is  
correct?  Any insight would be greatly appreciated!


======================

 > # note: a is experimental and c is predicted
 > summary(lm(a~c-1))

Call:
lm(formula = a ~ c - 1)

Residuals:
     Min      1Q  Median      3Q     Max
-2987.6 -1126.6  -181.7   855.3  5602.8

Coefficients:
   Estimate Std. Error t value Pr(>|t|)
c  0.99999    0.01402   71.33   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1423 on 365 degrees of freedom
Multiple R-Squared: 0.9331,	Adjusted R-squared: 0.9329
F-statistic:  5088 on 1 and 365 DF,  p-value: < 2.2e-16

 > version
          _
platform powerpc-apple-darwin7.9.0
arch     powerpc
os       darwin7.9.0
system   powerpc, darwin7.9.0
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

======================


Thank you very much for your time!

-Lance
____________________
Lance M. Westerhoff, Ph.D.
General Manager
QuantumBio Inc.

WWW:    http://www.quantumbioinc.com
Email:    lance at quantumbioinc.com


"Safety is not the most important thing. I know this sounds like heresy,
but it is a truth that must be embraced in order to do exploration.
The most important thing is to actually go."  ~ James Cameron

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Jan 24 18:26:05 2006
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 24 Jan 2006 18:26:05 +0100 (CET)
Subject: [R] useR! 2006 - Submission Deadline 2006-02-28
Message-ID: <Pine.LNX.4.51.0601241825010.26623@artemis.imbe.med.uni-erlangen.de>



The submission deadline for `useR! 2006', the second R user conference to be
held in Vienna June 15-17 2006, is only four weeks ahead. Now is the perfect
time to submit abstracts for user-contributed sessions!

The sessions will be a platform to bring together R users, contributers,
package maintainers and developers in the S spirit that `users are developers'.
People from different fields will show us how they solve problems
with R in fascinating applications, including
  - Applied Statistics & Biostatistics
  - Bayesian Statistics
  - Bioinformatics
  - Econometrics & Finance
  - Machine Learning
  - Marketing
  - Robust Statistics
  - Spatial Statistics
  - Statistics in the Social and Political Sciences
  - Teaching
  - Visualization & Graphics
  - and many more.

We invite all R users to submit abstracts on topics presenting innovations or
exciting applications of R. A web page offering more information on the `useR!'
conference, abstract submission, registration and Vienna is available at

  http://www.R-project.org/useR-2006/

We will accept submissions until February 28, 2006.

Let the contributions roll in!


The organizing committee:

Torsten Hothorn, Achim Zeileis, David Meyer, Bettina Gruen,
Kurt Hornik and Friedrich Leisch



From B.Rowlingson at lancaster.ac.uk  Tue Jan 24 18:27:57 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 24 Jan 2006 17:27:57 +0000
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <43D6639D.4040105@lancaster.ac.uk>

Laetitia Marisa wrote:
> Hello,
> 
> Is there a simple and fast function that returns a vector of the number 
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> 
>  I want the function returns the following vector where each term is the 
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )

One-liner:

 > table(ids)[ids]
ids
ID1 ID2 ID2 ID3 ID3 ID3 ID5
   1   2   2   3   3   3   1

  'table(ids)' computes the counts, then the subscripting [ids] looks it 
all up.

  Now try it on your 40,000-long vector!

Barry



From roebuck at mdanderson.org  Tue Jan 24 18:30:45 2006
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Tue, 24 Jan 2006 11:30:45 -0600 (CST)
Subject: [R] [REQ] Equivalent INSTALL cmdline option
Message-ID: <Pine.OSF.4.58.0601241117070.493518@wotan.mdacc.tmc.edu>

Would R-Core entertain adding the more proper (-L) option
as an equivalent to the existing (-l) option for INSTALL
script so it would match other utilities (like cc) for
specifying directories?

$ R CMD INSTALL -L <install_dir> <package>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From Raquel.Granell at bristol.ac.uk  Tue Jan 24 18:31:56 2006
From: Raquel.Granell at bristol.ac.uk (R Granell, Medicine)
Date: Tue, 24 Jan 2006 17:31:56 +0000
Subject: [R] multiple histograms
Message-ID: <5F033D604C7B5538E43E6F20@alsp-stats6.alspac.bris.ac.uk>

Here comes a simple question:

Is there a way of obtaining more than one histogram in the graph-window so 
you can edit all the plots simultaneously?

and how about scatter plots?


Thanks in advance



From p.dalgaard at biostat.ku.dk  Tue Jan 24 18:32:25 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 18:32:25 +0100
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <x2y815ilau.fsf@viggo.kubism.ku.dk>

Laetitia Marisa <Laetitia.Marisa at cgm.cnrs-gif.fr> writes:

> Hello,
> 
> Is there a simple and fast function that returns a vector of the number 
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> 
>  I want the function returns the following vector where each term is the 
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )
> 
> Of course I have a vector of more than 40 000 ID and the function I 
> wrote (it orders my data and checks on ID:Name of the data if the next 
> term is the same as the previous one (see below) ) is really slow 
> (30minutes for 44290 terms). But I don't have time by now to write a C 
> function.
> Thanks a lot for your help,

Will this do it?

> table(ids)[ids]
ids
ID1 ID2 ID2 ID3 ID3 ID3 ID5
  1   2   2   3   3   3   1

Or (could be faster):

> f <- factor(ids,levels=unique(ids))
> as.vector(table(f))[f]
[1] 1 2 2 3 3 3 1


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ccleland at optonline.net  Tue Jan 24 18:32:50 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 24 Jan 2006 12:32:50 -0500
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <43D664C2.3010202@optonline.net>

Laetitia Marisa wrote:
> Hello,
> 
> Is there a simple and fast function that returns a vector of the number 
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> 
>  I want the function returns the following vector where each term is the 
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )

 > rep(as.vector(table(ids)), as.vector(table(ids)))
[1] 1 2 2 3 3 3 1

> Of course I have a vector of more than 40 000 ID and the function I 
> wrote (it orders my data and checks on ID:Name of the data if the next 
> term is the same as the previous one (see below) ) is really slow 
> (30minutes for 44290 terms). But I don't have time by now to write a C 
> function.
> Thanks a lot for your help,
> 
> Laetitia.
> 
> 
> 
> Here is the function I have written maybe I have done something not 
> optimized :
> 
> repVector <- function(obj){
> 
>     # order IDName 
>     ord <- gif.indexByIDName(obj)
>     ordobj <- obj[ord,]
> 
>     nspots <- nrow(obj)
>     # vector of spot replicates number           
>     spotrep <- rep(NA, nspots )
>    
>     # function to get ID:Name for a given spot       
>     spotidname <- function(ind){
>             paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
>     }
> 
>     spot <- 1   
> 
>     while( spot < nspots ){
>         i<-1
>         while( spotidname(spot) == spotidname(spot + i) ){            
>        
>             i <- i + 1
>         }
>        
>         spotrep[spot : (spot + i-1)] <- i
>         spot <- spot + i
>         #cat("spot : ",spot,"\n")   
>     }       
>    
>     obj$genes$spotrep <- spotrep[order(ord)]
> 
>     obj
>                
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From joseclaudio.faria at terra.com.br  Tue Jan 24 18:41:14 2006
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Tue, 24 Jan 2006 14:41:14 -0300
Subject: [R] linear contrasts with anova
In-Reply-To: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
Message-ID: <43D666BA.2080507@terra.com.br>

> Date: Mon, 23 Jan 2006 13:25:33 +0100
> From: Christoph Buser <buser at stat.math.ethz.ch>
> Subject: Re: [R] linear contrasts with anova
> To: "Posta Univ. Cagliari" <mtommasi at unica.it>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <17364.52029.251153.507164 at stat.math.ethz.ch>
> Content-Type: text/plain; charset=us-ascii
> 
> Dear Marco
> 
> If you are interested in a comparison of a reference level
> against each other level (in your case level 1 against level 2
> and level 1 against level 3), you can use the summary.lm(),
> since this is the default contrast (see ?contr.treatment)
> 
> ar <- data.frame(GROUP = factor(rep(1:3, each = 8)),
>                  DIP = c(3.0, 3.0, 4.0, 5.0, 6.0, 7.0, 3.0, 2.0, 1.0, 6.0, 5.0,
>                    7.0, 2.0, 3.0, 1.5, 1.7, 17.0, 12.0, 15.0, 16.0, 12.0, 23.0,
>                    19.0, 21.0))
> 
> 
> r.aov10 <- aov(DIP ~  GROUP, data = ar)
> anova(r.aov10)
> summary.lm(r.aov10)
> 
> As result you will get the comparison GROUP 2 against GROUP 1,
> denoted by GROUP2 and the comparison GROUP 3 against GROUP 1,
> denoted by GROUP3.
> 
> Be careful. In your example you include both GROUP and C1 or C2,
> respectively in your model. This results in a over parameterized
> model and you get a warning that not all coefficients have been
> estimated, due to singularities.
> 
> It is possible to use other contrasts than contr.treatment,
> contr.sum, contr.helmert, contr.poly, but then you have to
> specify the correctly in the model.
> 
> Regards,
> 
> Christoph Buser
> 
> --------------------------------------------------------------
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C13
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-44-632-4673		fax: 632-1228
> http://stat.ethz.ch/~buser/
> --------------------------------------------------------------

Dear Marco

Try also the the below:

# Loading packages
library(gplots)
library(gmodels)

# Testing the nature of dF
is.data.frame(dF)
is.factor(dF$GROUP)
is.numeric(dF$DIP)

#Plotting GROUPS
win.graph(w = 4, h = 5)
plotmeans(DIP ~ GROUP, data = dF, mean.labels = TRUE,
           digits = 3, col = 'blue', connect = FALSE,
           ylab = 'DIP', xlab = 'GROUP', pch='')

# Contrasts
attach(dF)
                              #1   2   3 GROUP
	cmat = rbind('1 vs. 3' = c( 1,  0, -1,),
	             '1 vs. 2' = c( 1, -1,  0,))

   av  = aov(DIP ~ GROUP, data = dF, contrasts = list(GROUP = 
make.contrasts(cmat)))
   sav = summary(av1, split = list(GROUP=list('1 vs. 3'=1,
                                             '1 vs. 2'=2)))
   sav
detach(dF)

# Another option

attach(dF)
                             #A   B   C
   fit.contrast(av, GROUP, c( 1,  0, -1)) # from gmodels
   fit.contrast(av, GROUP, c( 1, -1,  0))
detach(dF)

HTH, []s,
-- 
Jose Claudio Faria
Brasil/Bahia/Ilh??us/UESC/DCET
Estat??stica Experimental/Prof. Adjunto
mails:
    joseclaudio.faria at terra.com.br
    jc_faria at uesc.br
    jc_faria at uol.com.br



From andy_liaw at merck.com  Tue Jan 24 18:37:23 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Jan 2006 12:37:23 -0500
Subject: [R] Number of replications of a term
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED743@usctmx1106.merck.com>

This should work:

> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> table(ids)[ids]
ids
ID1 ID2 ID2 ID3 ID3 ID3 ID5 
  1   2   2   3   3   3   1 

Andy

From: Laetitia Marisa
> 
> Hello,
> 
> Is there a simple and fast function that returns a vector of 
> the number 
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> 
>  I want the function returns the following vector where each 
> term is the 
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )
> 
> Of course I have a vector of more than 40 000 ID and the function I 
> wrote (it orders my data and checks on ID:Name of the data if 
> the next 
> term is the same as the previous one (see below) ) is really slow 
> (30minutes for 44290 terms). But I don't have time by now to 
> write a C 
> function.
> Thanks a lot for your help,
> 
> Laetitia.
> 
> 
> 
> Here is the function I have written maybe I have done something not 
> optimized :
> 
> repVector <- function(obj){
> 
>     # order IDName 
>     ord <- gif.indexByIDName(obj)
>     ordobj <- obj[ord,]
> 
>     nspots <- nrow(obj)
>     # vector of spot replicates number           
>     spotrep <- rep(NA, nspots )
>    
>     # function to get ID:Name for a given spot       
>     spotidname <- function(ind){
>             paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
>     }
> 
>     spot <- 1   
> 
>     while( spot < nspots ){
>         i<-1
>         while( spotidname(spot) == spotidname(spot + i) ){            
>        
>             i <- i + 1
>         }
>        
>         spotrep[spot : (spot + i-1)] <- i
>         spot <- spot + i
>         #cat("spot : ",spot,"\n")   
>     }       
>    
>     obj$genes$spotrep <- spotrep[order(ord)]
> 
>     obj
>                
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From john.marsland at mac.com  Tue Jan 24 18:46:53 2006
From: john.marsland at mac.com (John Marsland)
Date: Tue, 24 Jan 2006 17:46:53 +0000 (UTC)
Subject: [R] R and Eclipse?
References: <d0f55a670601240231q538b8acfu@mail.gmail.com>	<43D60CE6.8000808@web.de>
	<x2irs96ct2.fsf@viggo.kubism.ku.dk> <43D644D7.50501@free.fr>
Message-ID: <loom.20060124T184526-636@post.gmane.org>

Romain Francois <francoisromain <at> free.fr> writes:
> 
> Maybe you use gcj instead of sun java ?
> 
You have to be running Java 1.5 or above - it says this some where on the STATET
website. 

I've been running this (and previous versions) on Windows for many months
now, but I've had problems on OS X relating to Apple's strong preference for
Java 1.4 and the only recently release Apple Java 1.5.

Regards,

John Marsland



From ggrothendieck at gmail.com  Tue Jan 24 18:51:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jan 2006 12:51:34 -0500
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <971536df0601240951m6746e661m82d79f4d5401c43f@mail.gmail.com>

Try this:

ave(as.numeric(factor(ds)), ds, FUN = length)

See ?ave for more info.

On 1/24/06, Laetitia Marisa <Laetitia.Marisa at cgm.cnrs-gif.fr> wrote:
> Hello,
>
> Is there a simple and fast function that returns a vector of the number
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
>
>  I want the function returns the following vector where each term is the
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )
>
> Of course I have a vector of more than 40 000 ID and the function I
> wrote (it orders my data and checks on ID:Name of the data if the next
> term is the same as the previous one (see below) ) is really slow
> (30minutes for 44290 terms). But I don't have time by now to write a C
> function.
> Thanks a lot for your help,
>
> Laetitia.
>
>
>
> Here is the function I have written maybe I have done something not
> optimized :
>
> repVector <- function(obj){
>
>    # order IDName
>    ord <- gif.indexByIDName(obj)
>    ordobj <- obj[ord,]
>
>    nspots <- nrow(obj)
>    # vector of spot replicates number
>    spotrep <- rep(NA, nspots )
>
>    # function to get ID:Name for a given spot
>    spotidname <- function(ind){
>            paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
>    }
>
>    spot <- 1
>
>    while( spot < nspots ){
>        i<-1
>        while( spotidname(spot) == spotidname(spot + i) ){
>
>            i <- i + 1
>        }
>
>        spotrep[spot : (spot + i-1)] <- i
>        spot <- spot + i
>        #cat("spot : ",spot,"\n")
>    }
>
>    obj$genes$spotrep <- spotrep[order(ord)]
>
>    obj
>
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Jan 24 19:01:45 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 24 Jan 2006 10:01:45 -0800 (PST)
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <Pine.LNX.4.64.0601241001390.17573@homer23.u.washington.edu>


table()

 	-thomas

On Tue, 24 Jan 2006, Laetitia Marisa wrote:

> Hello,
>
> Is there a simple and fast function that returns a vector of the number
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
>
> I want the function returns the following vector where each term is the
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )
>
> Of course I have a vector of more than 40 000 ID and the function I
> wrote (it orders my data and checks on ID:Name of the data if the next
> term is the same as the previous one (see below) ) is really slow
> (30minutes for 44290 terms). But I don't have time by now to write a C
> function.
> Thanks a lot for your help,
>
> Laetitia.
>
>
>
> Here is the function I have written maybe I have done something not
> optimized :
>
> repVector <- function(obj){
>
>    # order IDName
>    ord <- gif.indexByIDName(obj)
>    ordobj <- obj[ord,]
>
>    nspots <- nrow(obj)
>    # vector of spot replicates number
>    spotrep <- rep(NA, nspots )
>
>    # function to get ID:Name for a given spot
>    spotidname <- function(ind){
>            paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
>    }
>
>    spot <- 1
>
>    while( spot < nspots ){
>        i<-1
>        while( spotidname(spot) == spotidname(spot + i) ){
>
>            i <- i + 1
>        }
>
>        spotrep[spot : (spot + i-1)] <- i
>        spot <- spot + i
>        #cat("spot : ",spot,"\n")
>    }
>
>    obj$genes$spotrep <- spotrep[order(ord)]
>
>    obj
>
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jholtman at gmail.com  Tue Jan 24 19:04:48 2006
From: jholtman at gmail.com (jim holtman)
Date: Tue, 24 Jan 2006 13:04:48 -0500
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <644e1f320601241004p5199421ft5dbed2009e707d8f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/03acca9d/attachment.pl

From tlumley at u.washington.edu  Tue Jan 24 19:06:17 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 24 Jan 2006 10:06:17 -0800 (PST)
Subject: [R] Number of replications of a term
In-Reply-To: <43D65E6C.3080408@cgm.cnrs-gif.fr>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
Message-ID: <Pine.LNX.4.64.0601241002130.17573@homer23.u.washington.edu>


Ah. It's a bit more complicated than just table(), because you want the 
result to be the same length.

tt <- table(id)
tt[match(id,names(tt))]


 	-thomas

On Tue, 24 Jan 2006, Laetitia Marisa wrote:

> Hello,
>
> Is there a simple and fast function that returns a vector of the number
> of replications for each object of a vector ?
> For example :
> I have a vector of IDs :
> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
>
> I want the function returns the following vector where each term is the
> number of replicates for the given id :
> c( 1, 2, 2, 3,3,3,1 )
>
> Of course I have a vector of more than 40 000 ID and the function I
> wrote (it orders my data and checks on ID:Name of the data if the next
> term is the same as the previous one (see below) ) is really slow
> (30minutes for 44290 terms). But I don't have time by now to write a C
> function.
> Thanks a lot for your help,
>
> Laetitia.
>
>
>
> Here is the function I have written maybe I have done something not
> optimized :
>
> repVector <- function(obj){
>
>    # order IDName
>    ord <- gif.indexByIDName(obj)
>    ordobj <- obj[ord,]
>
>    nspots <- nrow(obj)
>    # vector of spot replicates number
>    spotrep <- rep(NA, nspots )
>
>    # function to get ID:Name for a given spot
>    spotidname <- function(ind){
>            paste(ordobj$genes[ind, c("ID","Name") ], collapse=":")
>    }
>
>    spot <- 1
>
>    while( spot < nspots ){
>        i<-1
>        while( spotidname(spot) == spotidname(spot + i) ){
>
>            i <- i + 1
>        }
>
>        spotrep[spot : (spot + i-1)] <- i
>        spot <- spot + i
>        #cat("spot : ",spot,"\n")
>    }
>
>    obj$genes$spotrep <- spotrep[order(ord)]
>
>    obj
>
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From gavin.simpson at ucl.ac.uk  Tue Jan 24 19:16:58 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 24 Jan 2006 18:16:58 +0000
Subject: [R] In which application areas is R used?
In-Reply-To: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
References: <2303.202.89.150.213.1138069468.squirrel@sqmail.anu.edu.au>
Message-ID: <1138126618.20223.18.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2006-01-24 at 13:24 +1100, John Maindonald wrote:
> In this context "extensive" might be use of R in at least maybe 2% or 5%
> of the published analyses in the area, enough to make waves and stir
> awareness.

Given that description, then ecology would more than likely make the
grade.

All the best,

Gav

> 
> The immediate subtext is the demand of a book publisher for a list of
> journals to which a new edition of a certain book might be sent for
> review, and for a list of conferences where it might be given exposure.
> For myself, in the medium to longer term, I am more interested in other
> subtexts such as you mention, to which the answer might have relevance.
> 
> I've wondered what support there'd be for starting a database of
> bibliographic information on papers where R was used for the analysis.
> Authors might supply the information, or readers of a paper suggest its
> addition to the database. Once well populated, this would provide a useful
> indication of the range of application areas and journals where R is
> finding use.  [Or has someone, somewhere, already started such a
> database?]
> 
> Finance and biostatistics are obvious areas that I'd omitted.  Other areas
> drawn to my attention have been telephony and electronic networks, solid
> state etc manufacturing, computer system performance, oceanography and
> fisheries research, risk analysis, process engineering and marketing. (I
> hope my summaries are acceptably accurate).  I'm not sure what force these
> other respondents have given the word "extensive".
> John Maindonald
> Mathematical Sciences Institute
> Australian National University.
> john.maindonald at anu.edu.au
> 
> 
> Berton Gunter wrote:
> > Define "extensive."
> >
> > I think your answers depend on your definition. I know a bunch of folks
> in pharmaceutical preclinical R&D who use R for all sorts of stuff 
> (analysis and visualization of tox and efficacy animal studies,
> dose/response modeling, PK work, IC50 determination, stability data 
> analysis, etc.). Is "bunch" a majority? I strongly doubt that it's near.
> Is it 5%, 10%, 30% ?? Dunno. Excel is still the Big Boy in most of  these
> arenas I would bet. But I would also bet that there are at  least 1 or 2
> folks in dozens of companies who use R in for these things.
> >
> > Is there a subtext to your query? -- i.e. are you trying to make an
> argument for something?
> >
> > -- Bert
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.c
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Eduardo.Garcia at uv.es  Tue Jan 24 19:25:57 2006
From: Eduardo.Garcia at uv.es (Eduardo.Garcia@uv.es)
Date: Tue, 24 Jan 2006 19:25:57 +0100 (CET)
Subject: [R] fitting generalized linear models using glmmPQL
Message-ID: <2333585778emoigar@uv.es>

Hi, I have tried to run the following (I know it's a huge data set but 
I tried to perform it with a 1 GB RAM computer):

library(foreign)
library(MASS)
library(nlme)
datos<-read.spss(file="c:\\Documents and 
Settings\\Administrador\\Escritorio\\datosfin.sav",to.data.frame=TRUE)
str(datos)

`data.frame':   1414 obs. of  5 variables:
 $ POB     : Factor w/ 6 levels "CHI","HOS","HYR",..: 1 1 1 1 1 1 1 1 
1 1 ...
 $ CLON    : num  1 1 1 1 1 1 1 1 2 2 ...
 $ TEMP    : Factor w/ 2 levels "20 C","25 C": 1 1 1 1 2 2 2 2 1 1 ...
 $ SALINITA: Factor w/ 2 levels "15 g/l","30 g/l": 1 1 2 2 1 1 2 2 1 
1 ...
 $ DE      : num  17 0 7 1 15 28 4 14 13 16 ...
 - attr(*, "variable.labels")= Named chr  "" "" "" "" ...
  ..- attr(*, "names")= chr  "POB" "CLON" "TEMP" "SALINITA" ...

datos$CLON<-as.factor(datos$CLON)

modelo1<-glmmPQL(DE ~ (POB/CLON)*TEMP*SALINITA, data = datos, random = 
~ 1|CLON, family = poisson)

And I have obtained the following:

Error: NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
step size truncated due to divergence 

This is the first time I've observed such a message and I have no idea 
about what does it mean. Is it possible that the process failed 
because of the size of the data set (180 levels of the CLON factor)? 
Or maybe is it a syntax problem?

Thank you in advance.


********************************
Eduardo Mois??s Garc??a Roger

Institut Cavanilles de Biodiversitat i Biologia
Evolutiva - ICBIBE.
Tel. +34963543664
Fax  +34963543670



From Laetitia.Marisa at cgm.cnrs-gif.fr  Tue Jan 24 19:30:42 2006
From: Laetitia.Marisa at cgm.cnrs-gif.fr (Laetitia Marisa)
Date: Tue, 24 Jan 2006 19:30:42 +0100
Subject: [R] Number of replications of a term
In-Reply-To: <x2y815ilau.fsf@viggo.kubism.ku.dk>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
	<x2y815ilau.fsf@viggo.kubism.ku.dk>
Message-ID: <43D67252.1070406@cgm.cnrs-gif.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/626acbc8/attachment.pl

From ggrothendieck at gmail.com  Tue Jan 24 19:29:26 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jan 2006 13:29:26 -0500
Subject: [R] Number of replications of a term
In-Reply-To: <43D6639D.4040105@lancaster.ac.uk>
References: <43D65E6C.3080408@cgm.cnrs-gif.fr>
	<43D6639D.4040105@lancaster.ac.uk>
Message-ID: <971536df0601241029x3e8d3df9g92927c7d611a9080@mail.gmail.com>

Nice.  I timed it and its much faster than mine too.

On 1/24/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> Laetitia Marisa wrote:
> > Hello,
> >
> > Is there a simple and fast function that returns a vector of the number
> > of replications for each object of a vector ?
> > For example :
> > I have a vector of IDs :
> > ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> >
> >  I want the function returns the following vector where each term is the
> > number of replicates for the given id :
> > c( 1, 2, 2, 3,3,3,1 )
>
> One-liner:
>
>  > table(ids)[ids]
> ids
> ID1 ID2 ID2 ID3 ID3 ID3 ID5
>   1   2   2   3   3   3   1
>
>  'table(ids)' computes the counts, then the subscripting [ids] looks it
> all up.
>
>  Now try it on your 40,000-long vector!
>
> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pburns at pburns.seanet.com  Tue Jan 24 19:45:46 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 24 Jan 2006 18:45:46 +0000
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
Message-ID: <43D675DA.30806@pburns.seanet.com>

I would think that a negative value for a statistic that takes
on non-negative values would be suspiciously low.  But
some people think I'm overly skeptical.

There may or may not be a discussion of this in one or more
of the links in 'Spreadsheet Addiction'.
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html

The executive summary is, "Don't do statistics in spreadsheets,
especially Excel."

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Lance Westerhoff wrote:

>Hello All-
>
>I found an inconsistency between the R-squared reported in Excel vs.  
>that in R, and I am wondering which (if any) may be correct and if  
>this is a known issue.  While it certainly wouldn't surprise me if  
>Excel is just flat out wrong, I just want to make sure since the R- 
>squared reported in R seems surprisingly high.  Please let me know if  
>this is the wrong list.  Thanks!
>
>To begin, I have a set of data points in which the y is the  
>experimental number and x is the predicted value.  The Excel- 
>generated graph (complete with R^2 and trend line) is provided at  
>this link if you want to take a look:
>
>http://www.quantumbioinc.com/downloads/public/excel.png
>
>As you can see, the R-squared that is reported by Excel is -0.1005.   
>Now when I bring the same data into R, I get an R-square of +0.9331  
>(see below).  Being that I am new to R and semi-new to stats, is  
>there a difference between "multiple R-squared" and R-squared that  
>perhaps I am simply interpreting this wrong, or is this a known  
>inconsistency between the two applications?  If so, which is  
>correct?  Any insight would be greatly appreciated!
>
>
>======================
>
> > # note: a is experimental and c is predicted
> > summary(lm(a~c-1))
>
>Call:
>lm(formula = a ~ c - 1)
>
>Residuals:
>     Min      1Q  Median      3Q     Max
>-2987.6 -1126.6  -181.7   855.3  5602.8
>
>Coefficients:
>   Estimate Std. Error t value Pr(>|t|)
>c  0.99999    0.01402   71.33   <2e-16 ***
>---
>Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>Residual standard error: 1423 on 365 degrees of freedom
>Multiple R-Squared: 0.9331,	Adjusted R-squared: 0.9329
>F-statistic:  5088 on 1 and 365 DF,  p-value: < 2.2e-16
>
> > version
>          _
>platform powerpc-apple-darwin7.9.0
>arch     powerpc
>os       darwin7.9.0
>system   powerpc, darwin7.9.0
>status
>major    2
>minor    2.1
>year     2005
>month    12
>day      20
>svn rev  36812
>language R
>
>======================
>
>
>Thank you very much for your time!
>
>-Lance
>____________________
>Lance M. Westerhoff, Ph.D.
>General Manager
>QuantumBio Inc.
>
>WWW:    http://www.quantumbioinc.com
>Email:    lance at quantumbioinc.com
>
>
>"Safety is not the most important thing. I know this sounds like heresy,
>but it is a truth that must be embraced in order to do exploration.
>The most important thing is to actually go."  ~ James Cameron
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From lance at quantumbioinc.com  Tue Jan 24 19:48:22 2006
From: lance at quantumbioinc.com (Lance Westerhoff)
Date: Tue, 24 Jan 2006 13:48:22 -0500
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <592E8923DB6EA348BE8E33FCAADEFFFC13EED824@dshs-exch2.dshs.wa.lcl>
References: <592E8923DB6EA348BE8E33FCAADEFFFC13EED824@dshs-exch2.dshs.wa.lcl>
Message-ID: <8DB36B8D-61B4-4A55-9A3B-677E6A44529D@quantumbioinc.com>


On Jan 24, 2006, at 12:11 PM, Nordlund, Dan wrote:
>
> Lance,
>
> Did you force the regression through the origin in Excel, like you  
> are doing
> with your R code?  And why are you doing the regression without an  
> intercept
> in R?
>
> Dan
>

Hi Dan-

The reason why the intercept is forced to be zero is because I would  
like to determine how well my prediction is compared to experiment.   
Therefore, the only point we really know is (0,0) - everything else  
is conjecture.  Both in the excel case and the R case, the intercept  
is forced to be zero.  In terms of your question about the regression  
without an intercept in R, I'm not sure what you mean.  Haven't I set  
the intercept to be zero?

Thanks!

-Lance



From lance at quantumbioinc.com  Tue Jan 24 19:48:19 2006
From: lance at quantumbioinc.com (Lance Westerhoff)
Date: Tue, 24 Jan 2006 13:48:19 -0500
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <x23bjdk0zk.fsf@viggo.kubism.ku.dk>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
	<x23bjdk0zk.fsf@viggo.kubism.ku.dk>
Message-ID: <24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>


Hi-

On Jan 24, 2006, at 12:08 PM, Peter Dalgaard wrote:

> Lance Westerhoff <lance at quantumbioinc.com> writes:
>
>> Hello All-
>>
>> I found an inconsistency between the R-squared reported in Excel vs.
>> that in R, and I am wondering which (if any) may be correct and if
>> this is a known issue.  While it certainly wouldn't surprise me if
>> Excel is just flat out wrong, I just want to make sure since the R-
>> squared reported in R seems surprisingly high.  Please let me know if
>> this is the wrong list.  Thanks!
>
> Excel is flat out wrong. As the name implies, R-squared values cannot
> be less than zero (adjusted R-squared can, but I wouldn't think
> that is what Excel does).

I had thought the same thing, but then I came across the following  
site which states: "Note that it is possible to get a negative R- 
square for equations that do not contain a constant term. If R-square  
is defined as the proportion of variance explained by the fit, and if  
the fit is actually worse than just fitting a horizontal line, then R- 
square is negative. In this case, R-square cannot be interpreted as  
the square of a correlation." Since

R^2 = 1 - (SSE/SST)

I guess you can have SSE > SST which would result in a R^2 of less  
then 1.0.  However, it still seems very strange which made me wonder  
what is going on in Excel needless to say!

http://www.mathworks.com/access/helpdesk/help/toolbox/curvefit/ 
ch_fitt9.html

-Lance



From p.dalgaard at biostat.ku.dk  Tue Jan 24 19:59:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2006 19:59:28 +0100
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
	<x23bjdk0zk.fsf@viggo.kubism.ku.dk>
	<24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>
Message-ID: <x2u0btih9r.fsf@viggo.kubism.ku.dk>

Lance Westerhoff <lance at quantumbioinc.com> writes:

> Hi-
> 
> On Jan 24, 2006, at 12:08 PM, Peter Dalgaard wrote:
> 
> > Lance Westerhoff <lance at quantumbioinc.com> writes:
> >
> >> Hello All-
> >>
> >> I found an inconsistency between the R-squared reported in Excel vs.
> >> that in R, and I am wondering which (if any) may be correct and if
> >> this is a known issue.  While it certainly wouldn't surprise me if
> >> Excel is just flat out wrong, I just want to make sure since the R-
> >> squared reported in R seems surprisingly high.  Please let me know if
> >> this is the wrong list.  Thanks!
> >
> > Excel is flat out wrong. As the name implies, R-squared values cannot
> > be less than zero (adjusted R-squared can, but I wouldn't think
> > that is what Excel does).
> 
> I had thought the same thing, but then I came across the following
> site which states: "Note that it is possible to get a negative R-
> square for equations that do not contain a constant term. If R-square
> is defined as the proportion of variance explained by the fit, and if
> the fit is actually worse than just fitting a horizontal line, then R-
> square is negative. In this case, R-square cannot be interpreted as
> the square of a correlation." Since
> 
> R^2 = 1 - (SSE/SST)
> 
> I guess you can have SSE > SST which would result in a R^2 of less
> then 1.0.  However, it still seems very strange which made me wonder
> what is going on in Excel needless to say!
> 
> http://www.mathworks.com/access/helpdesk/help/toolbox/curvefit/
> ch_fitt9.html

Well, one more program (Matlab) decided to do things contrary to
conventional statistics programs it seems....


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From olsson1 at gmail.com  Tue Jan 24 20:30:05 2006
From: olsson1 at gmail.com (P. Olsson)
Date: Tue, 24 Jan 2006 20:30:05 +0100
Subject: [R] non-finite finite-difference value[]
Message-ID: <e984efc60601241130i7a4b8007j@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/72f6e053/attachment.pl

From dmbates at gmail.com  Tue Jan 24 20:41:08 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 24 Jan 2006 13:41:08 -0600
Subject: [R] Condor and R
In-Reply-To: <C047AB28-13E9-47D5-976F-B87B159BFC55@gmail.com>
References: <C047AB28-13E9-47D5-976F-B87B159BFC55@gmail.com>
Message-ID: <40e66e0b0601241141k339e8561t3348d25bfa82173a@mail.gmail.com>

On 1/24/06, David Reitter <david.reitter at gmail.com> wrote:
> Hi,
> I was wondering if anyone has successfully linked R against the
> Condor libraries so that R can be run as a Condor job in the
> "standard" (not "vanilla") universe. The advantage of this would be
> that due to checkpointing, jobs can be suspended and transferred to
> another node. There is a good overview by Xianhong Xie here:
>
> http://cran.r-project.org/doc/Rnews/Rnews_2005-2.pdf
>
> Unfortunately, the article points out that some restrictions of
> Condor preclude us from running R in the standard universe. Does
> anyone know if this can be overcome?

I don't think it can currently be overcome and this is not likely to
change soon.  Condor doesn't like dynamic linking and loading for
programs in the standard universe and R more-or-less requires that. 
We reached the conclusion that changing either system to be compatible
with the other would require a lot of work.

Mike Redmond <redmond at stat.wisc.edu> may have more current information
about the possibility of running R in the standard Condor universe.



From redmond at stat.wisc.edu  Tue Jan 24 20:53:25 2006
From: redmond at stat.wisc.edu (Michael Redmond)
Date: Tue, 24 Jan 2006 13:53:25 -0600
Subject: [R] Condor and R
In-Reply-To: <40e66e0b0601241141k339e8561t3348d25bfa82173a@mail.gmail.com>
References: <C047AB28-13E9-47D5-976F-B87B159BFC55@gmail.com>
	<40e66e0b0601241141k339e8561t3348d25bfa82173a@mail.gmail.com>
Message-ID: <43D685B5.8090905@stat.wisc.edu>

David,

We have discussed proposing a joint project with the UW-Madison Condor 
group to develop capabilities in Condor and R that would allow a subset 
of batch applications of R to run in the Condor standard universe. To 
this point, we have not identified a funding source to support the 
development, so the project is on hold.

I am interested in identifying the potential impact of this type of 
development as justification in our proposal. If it would affect a large 
community of R users, there would be a better possibility to find the 
needed funding.

If there is a community of R users out there that would be interested in 
having R run in Condor standard universe, please direct comments my way 
and, based on interest, I will try to move the proposal forward.

Thanks
Mike Redmond
Director of Computing Services
Dept. of Statistics, UW-Madison
---
Douglas Bates wrote:
> On 1/24/06, David Reitter <david.reitter at gmail.com> wrote:
> 
>>Hi,
>>I was wondering if anyone has successfully linked R against the
>>Condor libraries so that R can be run as a Condor job in the
>>"standard" (not "vanilla") universe. The advantage of this would be
>>that due to checkpointing, jobs can be suspended and transferred to
>>another node. There is a good overview by Xianhong Xie here:
>>
>>http://cran.r-project.org/doc/Rnews/Rnews_2005-2.pdf
>>
>>Unfortunately, the article points out that some restrictions of
>>Condor preclude us from running R in the standard universe. Does
>>anyone know if this can be overcome?
> 
> 
> I don't think it can currently be overcome and this is not likely to
> change soon.  Condor doesn't like dynamic linking and loading for
> programs in the standard universe and R more-or-less requires that. 
> We reached the conclusion that changing either system to be compatible
> with the other would require a lot of work.
> 
> Mike Redmond <redmond at stat.wisc.edu> may have more current information
> about the possibility of running R in the standard Condor universe.



From HDoran at air.org  Tue Jan 24 21:04:21 2006
From: HDoran at air.org (Doran, Harold)
Date: Tue, 24 Jan 2006 15:04:21 -0500
Subject: [R] nested ANCOVA: still confused
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A017D3CCD@dc1ex3.air.org>

Dear Jeff:

I see the issues in your code and have provided what I think will solve
your problem. It is often much easier to get help on this list when you
provide a small bit of data that can be replicated and you state what
the error messages are that you are receiving. OK, with that said, here
is what I see. First, you do not need to use the syntax bb$sex in your
model, this can be significantly simplified. Second, you do not have a
random statement in your model.

Here is your original model:
lme(bb$rtot~bb$sex, bb$purban|bb$chick/bb$box, na.action=na.omit)

Here is what it should be:

lme(fixed = rtot~sex, random=~purban|chick/box, na.action=na.omit,
data=bb)

Notice there is a fixed and random call. You can simplify this as

lme(rtot~sex, random=~purban|chick/box, na.action=na.omit, bb)

Note, you can eliminate the "fixed=" portion but not the random
statement.

Last, if you want to do this in lmer, the newer function for mixed
models in the Matrix package, you would do

lmer(rtot~sex + (purban|box:chick) + (purban|box), na.action=na.omit,
data=bb)

Hope this helps.
Harold




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Stratford
Sent: Tuesday, January 24, 2006 11:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] nested ANCOVA: still confused

Dear R-users,

I did some more research and I'm still not sure how to set up an ANCOVA
with nestedness.  Specifically I'm not sure how to express chicks nested
within boxes.  I will be getting Pinheiro & Bates (Mixed Effects Models
in S and S-Plus) but it will not arrive for another two weeks from our
interlibrary loan.

The goal is to determine if there are urbanization (purban) effects on
chick health (rtot) and if there are differences between sexes (sex) and
the effect of being in the same clutch (box).

The model is rtot = sex + purban + (chick)box.

I've loaded the package lme4.  And the code I have so far is

bb <- read.csv("C:\\eabl\\eabl_feather04.csv", header=TRUE) bb$sex <-
factor(bb$sex) rtot.lme <- lme(bb$rtot~bb$sex,
bb$purban|bb$chick/bb$box,
na.action=na.omit)

but this is not working.

Any suggestions would be greatly appreciated.

Thanks,

Jeff








****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tmlammail at yahoo.com  Tue Jan 24 21:13:07 2006
From: tmlammail at yahoo.com (Martin Lam)
Date: Tue, 24 Jan 2006 12:13:07 -0800 (PST)
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
Message-ID: <20060124201307.1245.qmail@web34701.mail.mud.yahoo.com>

Dear Gueorgui,

> Is it true that R generally cannot handle  medium
> sized data sets(a
> couple of hundreds of thousands observations) and
> threrefore large
> date set(couple of millions of observations)?

It depends on what you want to do with the data sets.
Loading the data sets shouldn't be any problem I
think. But using the data sets for analysis using self
written R code can get (very) slow,  since R is an
interpreted language (correct me if I'm wrong). To
increase speed you will often need to experiment with
the R code. For example, what I've noticed is that
processing data sets as matrices works much faster
than data.frame(). Writing your code in C(++), compile
it and include it in your R code is often the best
way.

HTH,

Martin



From dmbates at gmail.com  Tue Jan 24 21:25:36 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 24 Jan 2006 14:25:36 -0600
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
	<x23bjdk0zk.fsf@viggo.kubism.ku.dk>
	<24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>
Message-ID: <40e66e0b0601241225n65e53a7bkb750388a1a880f3b@mail.gmail.com>

On 1/24/06, Lance Westerhoff <lance at quantumbioinc.com> wrote:
>
> Hi-
>
> On Jan 24, 2006, at 12:08 PM, Peter Dalgaard wrote:
>
> > Lance Westerhoff <lance at quantumbioinc.com> writes:
> >
> >> Hello All-
> >>
> >> I found an inconsistency between the R-squared reported in Excel vs.
> >> that in R, and I am wondering which (if any) may be correct and if
> >> this is a known issue.  While it certainly wouldn't surprise me if
> >> Excel is just flat out wrong, I just want to make sure since the R-
> >> squared reported in R seems surprisingly high.  Please let me know if
> >> this is the wrong list.  Thanks!
> >
> > Excel is flat out wrong. As the name implies, R-squared values cannot
> > be less than zero (adjusted R-squared can, but I wouldn't think
> > that is what Excel does).
>
> I had thought the same thing, but then I came across the following
> site which states: "Note that it is possible to get a negative R-
> square for equations that do not contain a constant term. If R-square
> is defined as the proportion of variance explained by the fit, and if
> the fit is actually worse than just fitting a horizontal line, then R-
> square is negative. In this case, R-square cannot be interpreted as
> the square of a correlation." Since
>
> R^2 = 1 - (SSE/SST)
>
> I guess you can have SSE > SST which would result in a R^2 of less
> then 1.0.  However, it still seems very strange which made me wonder
> what is going on in Excel needless to say!
>
> http://www.mathworks.com/access/helpdesk/help/toolbox/curvefit/
> ch_fitt9.html

This seems to be a case of using the wrong formula.  R^2 should
measure the amount of variation for which the given model accounts
relative to the amount of variation for which the *appropriate* null
model does not account.  If you have a constant or intercept term in a
linear model then the null model for comparison is one with the
intercept only.  If you have a linear model without an intercept term
then the appropriate null model for comparison is the model that
predicts all the responses as zero.  Thus SST, the "corrected" total
sum of squares, should be used when you have a model with an intercept
term but the uncorrected total sum of squares should be used when you
do not have an intercept term.

It is disappointing to see the MathWorks propagating such an
elementary misconception.



From ruser2006 at yahoo.com  Tue Jan 24 21:28:13 2006
From: ruser2006 at yahoo.com (r user)
Date: Tue, 24 Jan 2006 12:28:13 -0800 (PST)
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
Message-ID: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>

I am using R 2.1.1 in a Windows Xp environment.

I need to import a large database from Microsoft SQL
into R.

I am currently using the ?sqlQuery? function/command.

This works, but I sometimes run out of memory if my
database is too big, or it take quite a long time for
the data to import into R.

Is there a better way to bring a large SQL database
into R? 

IS there an efficient way to convert the data into R
format prior to bringing it into R? (E.g. directly
from Microsoft SQL?)



From efg at stowers-institute.org  Tue Jan 24 21:39:59 2006
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 24 Jan 2006 14:39:59 -0600
Subject: [R] Very important! Global variables in R?
References: <8137.1138108528@www065.gmx.net>
	<BFFB9479.448A%sdavis2@mail.nih.gov>
Message-ID: <dr63b5$dk8$1@sea.gmane.org>

"Sean Davis" <sdavis2 at mail.nih.gov> wrote in message
news:BFFB9479.448A%sdavis2 at mail.nih.gov...

> > I need urgently your assistance!!!
> > I need a global variable in R. The variable ought to be known for every
> > functions and subfunctions.  It is only to comparison purposes of the
> > numeric algorithms. Is there a possibility?
> >
> > please answer in german if possible.

Es tut mir leid.  Ich kann Deutsch nicht sprechen.

>
>  a <- 1

If you make that assignment inside a function, it won't be known outside the
funtion, so "a" is not really global here.

Global assignment
x <<- 5
?"<<-"

BUT, be aware of these cautions about the use of global variables:

"I wish <<- had never been invented, as it makes an esoteric
and dangerous feature of the language *seem* normal and reasonable.  If you
want to dumb down R/S into a macro language, this is the operator for you."
-- Bill.Venables, R-Help,  July 30, 2001

#thomas (tlumley), R-Help, Jan 24, 2001
This is one of the acceptable uses of <<-, as shown in
demo(scoping).  <<- is useful for modifying a variable that you know
exists in the enclosing environment. Most of the problems come from people
trying to use it to modify things in a parent environment or in the global
environment.

Also see Section 13.3 "Global Data" in "Code Complete 2"
(http://www.cc2e.com/) including:
- Common Problems with Global Data
- Use Global Data Only as a Last Resort
- How to Reduce the Risks of Using Global Data

efg



From statistical.model at googlemail.com  Tue Jan 24 21:46:07 2006
From: statistical.model at googlemail.com (statistical.model@googlemail.com)
Date: Tue, 24 Jan 2006 20:46:07 -0000
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <43D5A524.2000802@echip.com>
Message-ID: <EMEELGDEKHMIAKDGLCDCKENDCJAA.Statistical.model@gmail.com>

>If an orthogonal main effect plan exists for the number of trials you 
specify, optFederov() in AlgDesign will more than likely find it for 
you, since such a design should be an optimal design.

thanks very much to you and the others!!
I have written a little function, and now the syntax to obtain an orthogonal
design such as 2x4x3x2 is:


design.test <- gen.orthogonal.design(c(2,4,3,2))


gen.orthogonal.design <- function(listFactors){
	library(AlgDesign)
	FactorsNames<-c("A","B","C","D","E","F","G","H","J","K","L")
	numFactors<-length(listFactors)
	
dat<-gen.factorial(listFactors,center=FALSE,varNames=FactorsNames[1:numFacto
rs])
	desPB<-optFederov(~.,dat,nRepeats=20,approximate=TRUE)
	design<-desPB$design[,2:numFactors]
	cat("Minimum number of trials: ", fill=T, length(design[,1]),
append=T)
	#cor(design)
	return(design)
}


Roberto Furlan
University of Turin, Italy



----------------------------------------
La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
188 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!


From statistical.model at googlemail.com  Tue Jan 24 22:03:00 2006
From: statistical.model at googlemail.com (statistical.model@googlemail.com)
Date: Tue, 24 Jan 2006 21:03:00 -0000
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <43D5A524.2000802@echip.com>
Message-ID: <EMEELGDEKHMIAKDGLCDCGENECJAA.Statistical.model@gmail.com>

sorry, some small mistakes in the previuos syntax. This works!

design.test <- gen.orthogonal.design(c(2,4,3),numCards=16)
design.test

gen.orthogonal.design <- function(listFactors,numCards){
	library(AlgDesign)
	FactorsNames<-c("A","B","C","D","E","F","G","H","J","K","L")
	numFactors<-length(listFactors)
	
dat<-gen.factorial(listFactors,center=FALSE,varNames=FactorsNames[1:numFacto
rs])
	
desPB<-optFederov(~.,dat,nRepeats=20,approximate=FALSE,nTrials=numCards)
	design<-desPB$design#[,2:(numFactors+1)]
	cat("Number of trials: ", fill=T, length(design[,1]), append=T)
	print(cor(design))
	return(design)
}

However, it is necessary to run the function and guess numCards until the
correlation matrix is diagonal and all levels are selected for the final
design.
Any idea how to solve this problem without an iterative function?

Roberto Furlan
University of Turin, Italy

----------------------------------------
La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
188 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!


From roger.bos at gmail.com  Tue Jan 24 22:02:34 2006
From: roger.bos at gmail.com (roger bos)
Date: Tue, 24 Jan 2006 16:02:34 -0500
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
In-Reply-To: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
References: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
Message-ID: <1db726800601241302n47efeb46ga53d3bee8e918bdb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060124/c25e60d8/attachment.pl

From sdavis2 at mail.nih.gov  Tue Jan 24 22:19:13 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 24 Jan 2006 16:19:13 -0500
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
In-Reply-To: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
Message-ID: <BFFC0401.45AA%sdavis2@mail.nih.gov>




On 1/24/06 3:28 PM, "r user" <ruser2006 at yahoo.com> wrote:

> I am using R 2.1.1 in a Windows Xp environment.
> 
> I need to import a large database from Microsoft SQL
> into R.
> 
> I am currently using the ??sqlQuery?? function/command.
> 
> This works, but I sometimes run out of memory if my
> database is too big, or it take quite a long time for
> the data to import into R.
> 
> Is there a better way to bring a large SQL database
> into R? 

How are you using sqlQuery?  The power of SQL is that you can fetch records
on demand.  If you don't need this functionality, then you may want to just
dump to a text file from Microsoft SQL and read into R.

> IS there an efficient way to convert the data into R
> format prior to bringing it into R? (E.g. directly
> from Microsoft SQL?)

You could probably check into how Microsoft SQL dumps a table.

Also, if you haven't read this:

http://cran.r-project.org/doc/manuals/R-data.html

it is probably a good time to look at it--it is full of tips for doing data
import from various sources including SQL databases.

Sean



From naiara at mail.utexas.edu  Tue Jan 24 22:28:36 2006
From: naiara at mail.utexas.edu (Naiara S. Pinto)
Date: Tue, 24 Jan 2006 15:28:36 -0600 (CST)
Subject: [R] polr (MASS)
Message-ID: <55308.129.116.71.233.1138138116.squirrel@129.116.71.233>

Hello all,

I am trying to use polr (the ordered logistic model from MASS) but I am
getting the following error message:

Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf :
missing value where TRUE/FALSE needed

My response variable is a factor with 3 levels and I have 2 independent
variables. I am not sure if I guessed the starting parameters right, which
I imagine could be a source of error. Here's my code:

> polr(a~b+c, method="logistic", start=c(10, 0.06))

Does anyone have any ideas why I am getting this error message?

Thank you very much,

Naiara.

--------------------------------------------
Naiara S. Pinto
Ecology, Evolution and Behavior
1 University Station A6700
Austin, TX, 78712



From ray at mcs.vuw.ac.nz  Tue Jan 24 22:32:32 2006
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Wed, 25 Jan 2006 10:32:32 +1300 (NZDT)
Subject: [R] Number of replications of a term
Message-ID: <200601242132.k0OLWWW7015590@tahi.mcs.vuw.ac.nz>

There's an even faster one, which nobody seems to have mentioned yet:

rep(l <- rle(ids)$lengths, l)

Timing on my 2.8GHz NetBSD system shows:

> length(ids)
[1] 45150
> # Gabor:
> system.time(for (i in 1:100) ave(as.numeric(factor(ids)), ids, FUN =
length))
[1] 3.45 0.06 3.54 0.00 0.00
> # Barry (and others I think):
> system.time(for (i in 1:100) table(ids)[ids])
[1] 2.13 0.05 2.20 0.00 0.00
> Me:
> system.time(for (i in 1:100) rep(l <- rle(ids)$lengths, l))
[1] 1.60 0.00 1.62 0.00 0.00

Of course the difference between 21 milliseconds and 16 milliseconds is
not great, unless you are doing this a lot.

Ray Brownrigg

> From: Gabor Grothendieck <ggrothendieck at gmail.com>
> 
> Nice.  I timed it and its much faster than mine too.
> 
> On 1/24/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> > Laetitia Marisa wrote:
> > > Hello,
> > >
> > > Is there a simple and fast function that returns a vector of the number
> > > of replications for each object of a vector ?
> > > For example :
> > > I have a vector of IDs :
> > > ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> > >
> > >  I want the function returns the following vector where each term is the
> > > number of replicates for the given id :
> > > c( 1, 2, 2, 3,3,3,1 )
> >
> > One-liner:
> >
> >  > table(ids)[ids]
> > ids
> > ID1 ID2 ID2 ID3 ID3 ID3 ID5
> >   1   2   2   3   3   3   1
> >
> >  'table(ids)' computes the counts, then the subscripting [ids] looks it
> > all up.
> >
> >  Now try it on your 40,000-long vector!
> >
> > Barry



From NordlDJ at dshs.wa.gov  Tue Jan 24 22:50:29 2006
From: NordlDJ at dshs.wa.gov (Nordlund, Dan)
Date: Tue, 24 Jan 2006 13:50:29 -0800
Subject: [R] R vs. Excel (R-squared)
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC13EED826@dshs-exch2.dshs.wa.lcl>

> -----Original Message-----
> From: Lance Westerhoff [mailto:lance at quantumbioinc.com]
> Sent: Tuesday, January 24, 2006 10:48 AM
> To: Nordlund, Dan
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R vs. Excel (R-squared)
> 
> 
> On Jan 24, 2006, at 12:11 PM, Nordlund, Dan wrote:
> >
> > Lance,
> >
> > Did you force the regression through the origin in Excel, like you
> > are doing
> > with your R code?  And why are you doing the regression without an
> > intercept
> > in R?
> >
> > Dan
> >
> 
> Hi Dan-
> 
> The reason why the intercept is forced to be zero is because I would
> like to determine how well my prediction is compared to experiment.
> Therefore, the only point we really know is (0,0) - everything else
> is conjecture.  Both in the excel case and the R case, the intercept
> is forced to be zero.  In terms of your question about the regression
> without an intercept in R, I'm not sure what you mean.  Haven't I set
> the intercept to be zero?
> 
> Thanks!
> 
> -Lance
> 
Your model formula, a ~ c - 1, estimates a slope coefficient but removes the
column of 1's which would be used to estimate an intercept term (i.e., you
eliminated the intercept term).  This effectively forces the regression
through the origin.  So yes, you set the intercept to zero. 

You stated that "the only point we really know is (0,0)".  The reason I
asked about why you were forcing the regression through the intercept is
that *I* don't know that you know anything about what happens when c=0.  It
is possible that your measurement process, whatever that might be, has a
bias such that a is not equal to 0 when c=0.  Did you actually have any
points where the predicted value was 0?  Just something to think about.

Dan

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204



From Keith.Chamberlain at colorado.edu  Tue Jan 24 23:06:50 2006
From: Keith.Chamberlain at colorado.edu (Keith.Chamberlain@colorado.edu)
Date: Tue, 24 Jan 2006 15:06:50 -0700
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
Message-ID: <1138140410.43d6a4fa35e94@webmail.colorado.edu>

Dear Prof Ripley,

First of all, unless you are an english professor, then I do not think you have
any business policing language. I'm still very much a student, both in R, and
regarding signal analysis. My competence on the subject as compared too your
own level of expertise, or my spelling for that matter, may be a contension for
you, but it would have been better had you kept that opinion too yourself. There
are plenty of other reasons besides laziness or carelessness that people will
consistently error in language use, such as learning disorders, head injuries,
and/or vertigo.

On the contrary, I am aware of the definition of a periodogram, and I know what
the unnormalized periodogram in the data I presented looks like. Spec.pgram()
is actually normalized too something, because it's discrete integral is not
well above the SS amplitude of the signal it computed the periodogram for. In
other words, the powers are not in units of around 4,000, which the peak would
be if the units were merely the modulus squared of the Fourier coeficients of
the data I presented. Alas, the modulus squared of the Fourier coeficients IS
the TWO SIDED unnormalized periodogram, ranging from [-fc, fc] | fc=nyquist
critical frequency. The definition of the ONE SIDED periodogram IS the modulus
squared of the Fourier coeficients ranging over [0, fc], but since the function
is even, data points in (0, fc) non-inclusive, need to be multiplied by 2. Thus
is according too the "definition" given by Press, et al (1988, 1992, 2002, c.f.
cp 12 & 13). I'm assuming that R returns an FFT in the same layout as Press, et
al describe.

Press, et al. are also very clear about the existence of far too many ways of
normalizing the periodogram too document, which they stated before delving into
particularly how they normalized to the mean squared amplitude of the signal
that the periodogram was computed from. In the page before, and perhaps this is
where some of the confusion arises from, they document the calculations for MS
and SS amplitudes and "time integral squared amplitude" of the signal in the
"time" domain, not the frequency domain. The page after that, their example
only shows how to normalize a periodogram so its sum is equal too the MS
amplitude. In short, but starting from SS amplitude:

a). sum(a[index=(1:N) or t=(0:N-1)]^2) = SS amplitude calculated in time domain

b). 1/N * sum(Mod(fft[-fc:fc])^2) = two sided periodogram that sums too the SS
amplitude

c). Same as b but over the range [0, fc], and (0, fc) multiplied by 2 is the one
sided periodogram, also sums too the SS amplitude

For MS amplitude, the procedures are identical, only the time domain is divided
by N, and the frequency domain figures are divided by N^2 instead of N.

When the periodogram is in power per unit time, as in the above, so that the
power is interpretable at N/2+1 independent frequencies, it is a normalized
periodogram. spec.pgram() IS normalized, I just do not know what it's
normalized too because I can not seem to get spec.pgram to stop tapering (at
which point the normalization should be dead on, not just "close").

By the way, "normalized" does not automatically mean anything unless "to what"
is stated. I could normalize something arbitrarily to the number of tics on my
dogs back side, and still call it normed, or erroneously refer too it as
unnormed. If "normalized" is suposed to mean something specific, then I am
confident that more than 90% of undergraduates are not familiar with what the
term "should" mean. Stats and coding and using programs are a human endeavor.
This human seems to have made meaning out of terms differently than what those
who wrote the documentation seem to have intended. Only, I do not know where
the documentation or my understanding may have been missled (R docs, Numerical
Recipes, or any other source I looked at since I started).

Cheers,
KeithC.

First, please look up `too' in your dictionary.

Second, please study the references on the help page, which give the
details.  That is what references are for!  The references will also
answer your question about the reference distribution.

The help page does not say it is `normalized' at all: it says it computes
the peridogram, and you seem unaware of the definitions of the latter (and
beware, there are more than one).

On Tue, 24 Jan 2006, Keith Chamberlain wrote:



From ggruber at terminal.at  Tue Jan 24 23:11:13 2006
From: ggruber at terminal.at (Gottfried Gruber)
Date: Tue, 24 Jan 2006 23:11:13 +0100
Subject: [R] Linearize a Function
Message-ID: <200601242311.13472.ggruber@terminal.at>

hi,

i calculate the log-returns in return1 and i want to get the performance for 
the security. with only one security i have the following code

# create matrix to keep performance
return100=matrix(rep(100,length(return1)+1))
# matrix for the sum
z1=matrix(rep(0,length(return1)+1))
# suming up the returns from current index to start
for (i in 1:length(return1)) {z1[i+1]=sum(return1[c(1:i)]) }
#adding both matrices
return100=return100+z1*100

this works fine for a 1 x n matrix, but if i want the same for a n x m matrix 
i assume the above code will get time-consuming. is there a trick to 
linearize the for-loop or any other solution?

thanks for any solution & effort,
tia gg
-- 
---------------------------------------------------
Gottfried Gruber
mailto:gottfried.gruber at terminal.at
www: http://gogo.sehrsupa.net



From ggrothendieck at gmail.com  Tue Jan 24 23:30:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jan 2006 17:30:58 -0500
Subject: [R] Number of replications of a term
In-Reply-To: <200601242132.k0OLWWW7015590@tahi.mcs.vuw.ac.nz>
References: <200601242132.k0OLWWW7015590@tahi.mcs.vuw.ac.nz>
Message-ID: <971536df0601241430kad4daaep3568700f64faa8e7@mail.gmail.com>

Note that that assumes that all occurrences of a value
are contiguous.

On 1/24/06, Ray Brownrigg <ray at mcs.vuw.ac.nz> wrote:
> There's an even faster one, which nobody seems to have mentioned yet:
>
> rep(l <- rle(ids)$lengths, l)
>
> Timing on my 2.8GHz NetBSD system shows:
>
> > length(ids)
> [1] 45150
> > # Gabor:
> > system.time(for (i in 1:100) ave(as.numeric(factor(ids)), ids, FUN =
> length))
> [1] 3.45 0.06 3.54 0.00 0.00
> > # Barry (and others I think):
> > system.time(for (i in 1:100) table(ids)[ids])
> [1] 2.13 0.05 2.20 0.00 0.00
> > Me:
> > system.time(for (i in 1:100) rep(l <- rle(ids)$lengths, l))
> [1] 1.60 0.00 1.62 0.00 0.00
>
> Of course the difference between 21 milliseconds and 16 milliseconds is
> not great, unless you are doing this a lot.
>
> Ray Brownrigg
>
> > From: Gabor Grothendieck <ggrothendieck at gmail.com>
> >
> > Nice.  I timed it and its much faster than mine too.
> >
> > On 1/24/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> > > Laetitia Marisa wrote:
> > > > Hello,
> > > >
> > > > Is there a simple and fast function that returns a vector of the number
> > > > of replications for each object of a vector ?
> > > > For example :
> > > > I have a vector of IDs :
> > > > ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> > > >
> > > >  I want the function returns the following vector where each term is the
> > > > number of replicates for the given id :
> > > > c( 1, 2, 2, 3,3,3,1 )
> > >
> > > One-liner:
> > >
> > >  > table(ids)[ids]
> > > ids
> > > ID1 ID2 ID2 ID3 ID3 ID3 ID5
> > >   1   2   2   3   3   3   1
> > >
> > >  'table(ids)' computes the counts, then the subscripting [ids] looks it
> > > all up.
> > >
> > >  Now try it on your 40,000-long vector!
> > >
> > > Barry
>



From rwheeler at echip.com  Wed Jan 25 01:14:44 2006
From: rwheeler at echip.com (Bob Wheeler)
Date: Tue, 24 Jan 2006 19:14:44 -0500
Subject: [R] R:  fractional factorial design in R
In-Reply-To: <EMEELGDEKHMIAKDGLCDCGENECJAA.Statistical.model@gmail.com>
References: <EMEELGDEKHMIAKDGLCDCGENECJAA.Statistical.model@gmail.com>
Message-ID: <43D6C2F4.1010606@echip.com>

I think you need to add factors="all" to gen.factorial(), otherwise the 
model df will be less than what you expect.

gen.orthogonal.design(c(2,2,3,3,3,3,2,2),numCards=16)

statistical.model at googlemail.com wrote:
> sorry, some small mistakes in the previuos syntax. This works!
> 
> design.test <- gen.orthogonal.design(c(2,4,3),numCards=16)
> design.test
> 
> gen.orthogonal.design <- function(listFactors,numCards){
> 	library(AlgDesign)
> 	FactorsNames<-c("A","B","C","D","E","F","G","H","J","K","L")
> 	numFactors<-length(listFactors)
> 	
> dat<-gen.factorial(listFactors,center=FALSE,varNames=FactorsNames[1:numFacto
> rs])
> 	
> desPB<-optFederov(~.,dat,nRepeats=20,approximate=FALSE,nTrials=numCards)
> 	design<-desPB$design#[,2:(numFactors+1)]
> 	cat("Number of trials: ", fill=T, length(design[,1]), append=T)
> 	print(cor(design))
> 	return(design)
> }
> 
> However, it is necessary to run the function and guess numCards until the
> correlation matrix is diagonal and all levels are selected for the final
> design.
> Any idea how to solve this problem without an iterative function?
> 
> Roberto Furlan
> University of Turin, Italy
> 
> ----------------------------------------
> La mia Cartella di Posta in Arrivo ?? protetta con SPAMfighter
> 188 messaggi contenenti spam sono stati bloccati con successo.
> Scarica gratuitamente SPAMfighter!
> 

-- 
Bob Wheeler --- http://www.bobwheeler.com/
    ECHIP, Inc. --- Randomness comes in bunches.



From tlumley at u.washington.edu  Wed Jan 25 01:44:38 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 24 Jan 2006 16:44:38 -0800 (PST)
Subject: [R] Number of replications of a term
In-Reply-To: <200601242132.k0OLWWW7015590@tahi.mcs.vuw.ac.nz>
References: <200601242132.k0OLWWW7015590@tahi.mcs.vuw.ac.nz>
Message-ID: <Pine.LNX.4.64.0601241638590.17573@homer23.u.washington.edu>

On Wed, 25 Jan 2006, Ray Brownrigg wrote:

> There's an even faster one, which nobody seems to have mentioned yet:
>
> rep(l <- rle(ids)$lengths, l)

I considered this but it wasn't clear to me from the initial post that 
each ID occupied a contiguous section of the vector.

Also, lazy evaluation makes code like this
    rep(l <- rle(ids)$lengths, l)
a bit worrying. It relies on rep() using the first argument before it uses 
the second one.  In this case, clearly, it works, but it is not a style I 
would encourage and it's easy to construct functions where it fails.

 	-thomas



> Timing on my 2.8GHz NetBSD system shows:
>
>> length(ids)
> [1] 45150
>> # Gabor:
>> system.time(for (i in 1:100) ave(as.numeric(factor(ids)), ids, FUN =
> length))
> [1] 3.45 0.06 3.54 0.00 0.00
>> # Barry (and others I think):
>> system.time(for (i in 1:100) table(ids)[ids])
> [1] 2.13 0.05 2.20 0.00 0.00
>> Me:
>> system.time(for (i in 1:100) rep(l <- rle(ids)$lengths, l))
> [1] 1.60 0.00 1.62 0.00 0.00
>
> Of course the difference between 21 milliseconds and 16 milliseconds is
> not great, unless you are doing this a lot.
>
> Ray Brownrigg
>
>> From: Gabor Grothendieck <ggrothendieck at gmail.com>
>>
>> Nice.  I timed it and its much faster than mine too.
>>
>> On 1/24/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
>>> Laetitia Marisa wrote:
>>>> Hello,
>>>>
>>>> Is there a simple and fast function that returns a vector of the number
>>>> of replications for each object of a vector ?
>>>> For example :
>>>> I have a vector of IDs :
>>>> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
>>>>
>>>>  I want the function returns the following vector where each term is the
>>>> number of replicates for the given id :
>>>> c( 1, 2, 2, 3,3,3,1 )
>>>
>>> One-liner:
>>>
>>> > table(ids)[ids]
>>> ids
>>> ID1 ID2 ID2 ID3 ID3 ID3 ID5
>>>   1   2   2   3   3   3   1
>>>
>>>  'table(ids)' computes the counts, then the subscripting [ids] looks it
>>> all up.
>>>
>>>  Now try it on your 40,000-long vector!
>>>
>>> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From michael_bibo at health.qld.gov.au  Wed Jan 25 00:27:14 2006
From: michael_bibo at health.qld.gov.au (Michael Bibo)
Date: Tue, 24 Jan 2006 23:27:14 +0000 (UTC)
Subject: [R] Easy, Robust and Stable GUI???
References: <2543480.post@talk.nabble.com>
Message-ID: <loom.20060125T000228-671@post.gmane.org>

pat_primate (sent by Nabble.com <lists <at> nabble.com> writes:

> I know that this isn't really a R help question, but I am a psychology 
student at TRU (tru.ca) and my psych
> department is going to be switching statistical software in the near 
future.  I thought this might be a good
> oppertunity to advocate for open source if an acceptable option is 
available.  I have looked around a bit
> and R seems to be the most stable and mature (not to mention powerful) open 
source statistical program
> going.  The only downfall is that the school has been using spss for years 
and would demand a similarly user
> friendly GUI based statistical program to replace it.  I have looked at a 
few of the R guis and most of them
> look like they are just command line interfaces in pretty desktop windows 
and not really a gui like spss.  If
> anyone knows of any stable, userfriendly and robust guis for R that would be 
similar to using spss please
> let me know, as I would love for my scho
>  ol to start embracing open source software.


I too am trying to move my organisation from SPSS to R.  While it is well 
worth considering the various command line vs GUI arguments, I appreciate that 
an easy to use GUI is important when weaning oneself and others from a system 
such as SPSS.

I would suggest you have a look at R-Commander.  It is available simply as an 
R package from CRAN (Rcmdr).  It has a familiar, menu-driven interface for a 
good range of data manipulation and analysis tasks, but without losing the 
power of the command line option.  It is actively developed, and John Fox is 
very approachable.  Perhaps most importantly, it is extensible, so you can 
tailor it to suit, and add functions to the menu system.

Michael Bibo
Queensland Health
michael_bibo at health.qld.gov.au



From andy_liaw at merck.com  Wed Jan 25 02:01:01 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Jan 2006 20:01:01 -0500
Subject: [R] lazy evaluation (was RE:  Number of replications of a term)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED74A@usctmx1106.merck.com>

From: Thomas Lumley
> 
> On Wed, 25 Jan 2006, Ray Brownrigg wrote:
> 
> > There's an even faster one, which nobody seems to have 
> mentioned yet:
> >
> > rep(l <- rle(ids)$lengths, l)
> 
> I considered this but it wasn't clear to me from the initial 
> post that 
> each ID occupied a contiguous section of the vector.
> 
> Also, lazy evaluation makes code like this
>     rep(l <- rle(ids)$lengths, l)
> a bit worrying. It relies on rep() using the first argument 
> before it uses 
> the second one.  In this case, clearly, it works, but it is 
> not a style I 
> would encourage and it's easy to construct functions where it fails.

Indeed.  Here's a trivial example:

2: package BRmisc in options("defaultPackages") was not found 
> f <- function(x, y) {
+     print(y)
+     x + y
+ }
> f(a <- 3, a)
Error in print(y) : object "a" not found

Without the print(), the function would work just fine.

Andy
 
>  	-thomas
> 
> 
> 
> > Timing on my 2.8GHz NetBSD system shows:
> >
> >> length(ids)
> > [1] 45150
> >> # Gabor:
> >> system.time(for (i in 1:100) ave(as.numeric(factor(ids)), 
> ids, FUN =
> > length))
> > [1] 3.45 0.06 3.54 0.00 0.00
> >> # Barry (and others I think):
> >> system.time(for (i in 1:100) table(ids)[ids])
> > [1] 2.13 0.05 2.20 0.00 0.00
> >> Me:
> >> system.time(for (i in 1:100) rep(l <- rle(ids)$lengths, l))
> > [1] 1.60 0.00 1.62 0.00 0.00
> >
> > Of course the difference between 21 milliseconds and 16 
> milliseconds is
> > not great, unless you are doing this a lot.
> >
> > Ray Brownrigg
> >
> >> From: Gabor Grothendieck <ggrothendieck at gmail.com>
> >>
> >> Nice.  I timed it and its much faster than mine too.
> >>
> >> On 1/24/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> >>> Laetitia Marisa wrote:
> >>>> Hello,
> >>>>
> >>>> Is there a simple and fast function that returns a 
> vector of the number
> >>>> of replications for each object of a vector ?
> >>>> For example :
> >>>> I have a vector of IDs :
> >>>> ids <- c( "ID1", "ID2", "ID2", "ID3", "ID3","ID3", "ID5")
> >>>>
> >>>>  I want the function returns the following vector where 
> each term is the
> >>>> number of replicates for the given id :
> >>>> c( 1, 2, 2, 3,3,3,1 )
> >>>
> >>> One-liner:
> >>>
> >>> > table(ids)[ids]
> >>> ids
> >>> ID1 ID2 ID2 ID3 ID3 ID3 ID5
> >>>   1   2   2   3   3   3   1
> >>>
> >>>  'table(ids)' computes the counts, then the subscripting 
> [ids] looks it
> >>> all up.
> >>>
> >>>  Now try it on your 40,000-long vector!
> >>>
> >>> Barry
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Wed Jan 25 02:27:18 2006
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 24 Jan 2006 20:27:18 -0500
Subject: [R] polr (MASS)
In-Reply-To: <55308.129.116.71.233.1138138116.squirrel@129.116.71.233>
Message-ID: <20060125012717.ZMHI14963.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Naiara,

I believe that the problem is in your specification of start values: If the
two independent variables are numeric, then there are four parameters in the
model, not two--i.e., two regression coefficients and two thresholds (or
intercepts). But why do you find it necessary to specify the start values in
any event?

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Naiara S. Pinto
> Sent: Tuesday, January 24, 2006 4:29 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] polr (MASS)
> 
> Hello all,
> 
> I am trying to use polr (the ordered logistic model from 
> MASS) but I am getting the following error message:
> 
> Error in if (all(pr > 0)) -sum(wt * log(pr)) else Inf :
> missing value where TRUE/FALSE needed
> 
> My response variable is a factor with 3 levels and I have 2 
> independent variables. I am not sure if I guessed the 
> starting parameters right, which I imagine could be a source 
> of error. Here's my code:
> 
> > polr(a~b+c, method="logistic", start=c(10, 0.06))
> 
> Does anyone have any ideas why I am getting this error message?
> 
> Thank you very much,
> 
> Naiara.
> 
> --------------------------------------------
> Naiara S. Pinto
> Ecology, Evolution and Behavior
> 1 University Station A6700
> Austin, TX, 78712
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From stratja at auburn.edu  Wed Jan 25 02:57:49 2006
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Tue, 24 Jan 2006 19:57:49 -0600
Subject: [R] nested ANCOVA: still confused
Message-ID: <43D686BD020000F2000062A5@TMIA1.AUBURN.EDU>

R-users and Harold.

First, thanks for the advice;  I'm almost there.  

The code I'm using now is 

library(nlme)
bb <- read.csv("E:\\eabl_feather04.csv", header=TRUE)
bb$sexv <- factor(bb$sexv)
rtot.lme <- lme(fixed=rtot~sexv, random=~purban2|chick/box,
na.action=na.omit, data=bb)

A sample of the data looks like this 

box	chick	rtot	purban2	sexv
1	1	6333.51	0.026846	f
1	2	8710.884	0.026846	m
2	1	5810.007	0.161074	f
2	2	5524.33	0.161074	f
2	3	4824.474	0.161074	f
2	4	5617.641	0.161074	f
2	5	6761.724	0.161074	f
4	1	7569.673	0.208054	m
4	2	7877.081	0.208054	m
4	4	7455.55	0.208054	f
7	1	5408.287	0.436242	m
10	1	6991.727	0.14094	f
12	1	8590.207	0.134228	f
12	2	7536.747	0.134228	m
12	3	5145.342	0.134228	m
12	4	6853.628	0.134228	f
15	1	8048.717	0.033557	m
15	2	7062.196	0.033557	m
15	3	8165.953	0.033557	m
15	4	8348.58	0.033557	m
16	2	6534.775	0.751678	m
16	3	7468.827	0.751678	m
16	4	5907.338	0.751678	f
21	1	7761.983	0.221477	m
21	2	6634.115	0.221477	m
21	3	6982.923	0.221477	m
21	4	7464.075	0.221477	m
22	1	6756.733	0.281879	f
23	2	8231.496	0.134228	m

The error I'm getting is

Error in logLik.lmeStructInt(lmeSt, lmePars) : 
        Calloc could not allocate (590465568 of 8) memory
In addition: Warning messages:
1: Fewer observations than random effects in all level 2 groups in:
lme.formula(fixed = rtot ~ sexv, random = ~purban2 | chick/box,  
2: Reached total allocation of 382Mb: see help(memory.size) 

There's nothing "special" about chick 1, 2, etc.  These were simply the
order of the birds measured in each box so chick 1 in box 1 has nothing
to do with chick 1 in box 2.

Many thanks,

Jeff 

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja
****************************************
>>> "Doran, Harold" <HDoran at air.org> 01/24/06 2:04 PM >>>
Dear Jeff:

I see the issues in your code and have provided what I think will solve
your problem. It is often much easier to get help on this list when you
provide a small bit of data that can be replicated and you state what
the error messages are that you are receiving. OK, with that said, here
is what I see. First, you do not need to use the syntax bb$sex in your
model, this can be significantly simplified. Second, you do not have a
random statement in your model.

Here is your original model:
lme(bb$rtot~bb$sex, bb$purban|bb$chick/bb$box, na.action=na.omit)

Here is what it should be:

lme(fixed = rtot~sex, random=~purban|chick/box, na.action=na.omit,
data=bb)

Notice there is a fixed and random call. You can simplify this as

lme(rtot~sex, random=~purban|chick/box, na.action=na.omit, bb)

Note, you can eliminate the "fixed=" portion but not the random
statement.

Last, if you want to do this in lmer, the newer function for mixed
models in the Matrix package, you would do

lmer(rtot~sex + (purban|box:chick) + (purban|box), na.action=na.omit,
data=bb)

Hope this helps.
Harold




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Stratford
Sent: Tuesday, January 24, 2006 11:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] nested ANCOVA: still confused

Dear R-users,

I did some more research and I'm still not sure how to set up an ANCOVA
with nestedness.  Specifically I'm not sure how to express chicks nested
within boxes.  I will be getting Pinheiro & Bates (Mixed Effects Models
in S and S-Plus) but it will not arrive for another two weeks from our
interlibrary loan.

The goal is to determine if there are urbanization (purban) effects on
chick health (rtot) and if there are differences between sexes (sex) and
the effect of being in the same clutch (box).

The model is rtot = sex + purban + (chick)box.

I've loaded the package lme4.  And the code I have so far is

bb <- read.csv("C:\\eabl\\eabl_feather04.csv", header=TRUE) bb$sex <-
factor(bb$sex) rtot.lme <- lme(bb$rtot~bb$sex,
bb$purban|bb$chick/bb$box,
na.action=na.omit)

but this is not working.

Any suggestions would be greatly appreciated.

Thanks,

Jeff








****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Jan 25 03:01:46 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 Jan 2006 18:01:46 -0800
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <1138140410.43d6a4fa35e94@webmail.colorado.edu>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<1138140410.43d6a4fa35e94@webmail.colorado.edu>
Message-ID: <43D6DC0A.60700@pdf.com>

Dear Mr. Chamberlain:

	  You asked for free consulting, and as near as I can tell, you got 
pretty good advice.  Now you complain that you don't like the packaging. 
  If you can't stand the heat, get out of the kitchen.

	  Professor Brian Ripley has an international reputation based on solid 
contributions to human knowledge over many years.  He is an expert in 
statistical science, not diplomacy.  Professor Ripley has been 
incredibly generous in donating substantial portions of his time for 
many years both to help make R what it is today and to answering 
questions on this listserve.  I think he deserves a great deal of 
respect for not only the time he has devoted to this but to how much he 
has achieved with that time.

	  What would you like him to do as a result of your email?  Retire? 
Stop contributing to this listserve and to the R project more generally? 
  I sincerely hope he does not consider such.  It would be a great loss 
to humanity if he did.

	  Mr. Chamberlain, if English (or as Prof. Ripley might say, 
"American") is your mother tongue, then your deplorable lack of skill in 
its use raises serious questions about the standard of academic 
excellence at the University of Colorado, which I had previously thought 
was a great university and the finest Colorado had to offer.  Of course, 
if English is a second language for you, then I would not complain. 
Rather, I would be humbled and honored that you chose to meet the rest 
of the world in my native tongue.  Another question:  The web lists you 
as a senior in psychology.  Have you learned anything in your study of 
psychology?  I would think that psychology students should meet a much 
higher standard for social skills and communications than you have 
displayed today.  Would you like me to forward your correspondence to, 
say, the editor of the Flatiron News there in Boulder or Prof. W. Edward 
Craighead, the chair of the Psychology Dept., asking if a degree from 
the once-great University of Colorado is supposed to imply that the 
degree holder meets any standard for academic excellence in comportment 
and the use of language?

	  Sincerely,
	  Spencer Graves

Keith.Chamberlain at colorado.edu wrote:

> Dear Prof Ripley,
> 
> First of all, unless you are an english professor, then I do not think you have
> any business policing language. I'm still very much a student, both in R, and
> regarding signal analysis. My competence on the subject as compared too your
> own level of expertise, or my spelling for that matter, may be a contension for
> you, but it would have been better had you kept that opinion too yourself. There
> are plenty of other reasons besides laziness or carelessness that people will
> consistently error in language use, such as learning disorders, head injuries,
> and/or vertigo.
> 
> On the contrary, I am aware of the definition of a periodogram, and I know what
> the unnormalized periodogram in the data I presented looks like. Spec.pgram()
> is actually normalized too something, because it's discrete integral is not
> well above the SS amplitude of the signal it computed the periodogram for. In
> other words, the powers are not in units of around 4,000, which the peak would
> be if the units were merely the modulus squared of the Fourier coeficients of
> the data I presented. Alas, the modulus squared of the Fourier coeficients IS
> the TWO SIDED unnormalized periodogram, ranging from [-fc, fc] | fc=nyquist
> critical frequency. The definition of the ONE SIDED periodogram IS the modulus
> squared of the Fourier coeficients ranging over [0, fc], but since the function
> is even, data points in (0, fc) non-inclusive, need to be multiplied by 2. Thus
> is according too the "definition" given by Press, et al (1988, 1992, 2002, c.f.
> cp 12 & 13). I'm assuming that R returns an FFT in the same layout as Press, et
> al describe.
> 
> Press, et al. are also very clear about the existence of far too many ways of
> normalizing the periodogram too document, which they stated before delving into
> particularly how they normalized to the mean squared amplitude of the signal
> that the periodogram was computed from. In the page before, and perhaps this is
> where some of the confusion arises from, they document the calculations for MS
> and SS amplitudes and "time integral squared amplitude" of the signal in the
> "time" domain, not the frequency domain. The page after that, their example
> only shows how to normalize a periodogram so its sum is equal too the MS
> amplitude. In short, but starting from SS amplitude:
> 
> a). sum(a[index=(1:N) or t=(0:N-1)]^2) = SS amplitude calculated in time domain
> 
> b). 1/N * sum(Mod(fft[-fc:fc])^2) = two sided periodogram that sums too the SS
> amplitude
> 
> c). Same as b but over the range [0, fc], and (0, fc) multiplied by 2 is the one
> sided periodogram, also sums too the SS amplitude
> 
> For MS amplitude, the procedures are identical, only the time domain is divided
> by N, and the frequency domain figures are divided by N^2 instead of N.
> 
> When the periodogram is in power per unit time, as in the above, so that the
> power is interpretable at N/2+1 independent frequencies, it is a normalized
> periodogram. spec.pgram() IS normalized, I just do not know what it's
> normalized too because I can not seem to get spec.pgram to stop tapering (at
> which point the normalization should be dead on, not just "close").
> 
> By the way, "normalized" does not automatically mean anything unless "to what"
> is stated. I could normalize something arbitrarily to the number of tics on my
> dogs back side, and still call it normed, or erroneously refer too it as
> unnormed. If "normalized" is suposed to mean something specific, then I am
> confident that more than 90% of undergraduates are not familiar with what the
> term "should" mean. Stats and coding and using programs are a human endeavor.
> This human seems to have made meaning out of terms differently than what those
> who wrote the documentation seem to have intended. Only, I do not know where
> the documentation or my understanding may have been missled (R docs, Numerical
> Recipes, or any other source I looked at since I started).
> 
> Cheers,
> KeithC.
> 
> First, please look up `too' in your dictionary.
> 
> Second, please study the references on the help page, which give the
> details.  That is what references are for!  The references will also
> answer your question about the reference distribution.
> 
> The help page does not say it is `normalized' at all: it says it computes
> the peridogram, and you seem unaware of the definitions of the latter (and
> beware, there are more than one).
> 
> On Tue, 24 Jan 2006, Keith Chamberlain wrote:
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pinard at iro.umontreal.ca  Wed Jan 25 03:57:09 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Tue, 24 Jan 2006 21:57:09 -0500
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <1138140410.43d6a4fa35e94@webmail.colorado.edu>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<1138140410.43d6a4fa35e94@webmail.colorado.edu>
Message-ID: <20060125025709.GA26595@phenix.sram.qc.ca>

[Keith.Chamberlain at colorado.edu, addressing to Brian Ripley]

>First of all, unless you are an english professor, then I do not think
>you have any business policing language.

We all do mistakes (English or otherwise).  I'm very grateful that 
people forgive my own errors, and I try to be tolerant to others.  (Yet, 
it happens that people lacking good will ask for stronger reactions.)

This is the business of everybody, really, building a better community 
in every possible aspect, and the means for this go through interaction 
and collaboration.  Let's all be humble enough to ponder the criticism 
of others, improve ourselves, and so increase the value of our share.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From ggrothendieck at gmail.com  Wed Jan 25 05:36:32 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jan 2006 23:36:32 -0500
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <1138140410.43d6a4fa35e94@webmail.colorado.edu>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<1138140410.43d6a4fa35e94@webmail.colorado.edu>
Message-ID: <971536df0601242036w12070f6drc87825a50c4cd03b@mail.gmail.com>

Its not really you.  Its a fact of life that this list is inhabited by
some rather rude participants but everyone puts up with
them in the hope that they do have some useful remarks.
This has been discussed repeatedly on the list and there
is even a group of thought that feels it is a justifiable way
to keep the list volume under control.

On 1/24/06, Keith.Chamberlain at colorado.edu
<Keith.Chamberlain at colorado.edu> wrote:
> Dear Prof Ripley,
>
> First of all, unless you are an english professor, then I do not think you have
> any business policing language. I'm still very much a student, both in R, and
> regarding signal analysis. My competence on the subject as compared too your
> own level of expertise, or my spelling for that matter, may be a contension for
> you, but it would have been better had you kept that opinion too yourself. There
> are plenty of other reasons besides laziness or carelessness that people will
> consistently error in language use, such as learning disorders, head injuries,
> and/or vertigo.
>
> On the contrary, I am aware of the definition of a periodogram, and I know what
> the unnormalized periodogram in the data I presented looks like. Spec.pgram()
> is actually normalized too something, because it's discrete integral is not
> well above the SS amplitude of the signal it computed the periodogram for. In
> other words, the powers are not in units of around 4,000, which the peak would
> be if the units were merely the modulus squared of the Fourier coeficients of
> the data I presented. Alas, the modulus squared of the Fourier coeficients IS
> the TWO SIDED unnormalized periodogram, ranging from [-fc, fc] | fc=nyquist
> critical frequency. The definition of the ONE SIDED periodogram IS the modulus
> squared of the Fourier coeficients ranging over [0, fc], but since the function
> is even, data points in (0, fc) non-inclusive, need to be multiplied by 2. Thus
> is according too the "definition" given by Press, et al (1988, 1992, 2002, c.f.
> cp 12 & 13). I'm assuming that R returns an FFT in the same layout as Press, et
> al describe.
>
> Press, et al. are also very clear about the existence of far too many ways of
> normalizing the periodogram too document, which they stated before delving into
> particularly how they normalized to the mean squared amplitude of the signal
> that the periodogram was computed from. In the page before, and perhaps this is
> where some of the confusion arises from, they document the calculations for MS
> and SS amplitudes and "time integral squared amplitude" of the signal in the
> "time" domain, not the frequency domain. The page after that, their example
> only shows how to normalize a periodogram so its sum is equal too the MS
> amplitude. In short, but starting from SS amplitude:
>
> a). sum(a[index=(1:N) or t=(0:N-1)]^2) = SS amplitude calculated in time domain
>
> b). 1/N * sum(Mod(fft[-fc:fc])^2) = two sided periodogram that sums too the SS
> amplitude
>
> c). Same as b but over the range [0, fc], and (0, fc) multiplied by 2 is the one
> sided periodogram, also sums too the SS amplitude
>
> For MS amplitude, the procedures are identical, only the time domain is divided
> by N, and the frequency domain figures are divided by N^2 instead of N.
>
> When the periodogram is in power per unit time, as in the above, so that the
> power is interpretable at N/2+1 independent frequencies, it is a normalized
> periodogram. spec.pgram() IS normalized, I just do not know what it's
> normalized too because I can not seem to get spec.pgram to stop tapering (at
> which point the normalization should be dead on, not just "close").
>
> By the way, "normalized" does not automatically mean anything unless "to what"
> is stated. I could normalize something arbitrarily to the number of tics on my
> dogs back side, and still call it normed, or erroneously refer too it as
> unnormed. If "normalized" is suposed to mean something specific, then I am
> confident that more than 90% of undergraduates are not familiar with what the
> term "should" mean. Stats and coding and using programs are a human endeavor.
> This human seems to have made meaning out of terms differently than what those
> who wrote the documentation seem to have intended. Only, I do not know where
> the documentation or my understanding may have been missled (R docs, Numerical
> Recipes, or any other source I looked at since I started).
>
> Cheers,
> KeithC.
>
> First, please look up `too' in your dictionary.
>
> Second, please study the references on the help page, which give the
> details.  That is what references are for!  The references will also
> answer your question about the reference distribution.
>
> The help page does not say it is `normalized' at all: it says it computes
> the peridogram, and you seem unaware of the definitions of the latter (and
> beware, there are more than one).
>
> On Tue, 24 Jan 2006, Keith Chamberlain wrote:
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Wed Jan 25 07:50:44 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 25 Jan 2006 07:50:44 +0100
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>
Message-ID: <43D72DD4.28307.3CC3D8@localhost>

Hi

In model without intercept Rsqared is high.

Se e.g. Julian J. Faraway - Practical regression ....

Warning: R2 as defined here doesn?t make any sense if you do not have 
an intercept in your model. This
is because the denominator in the definition of R2 has a null model 
with an intercept in mind when the sum
of squares is calculated. Alternative definitions of R2 are possible 
when there is no intercept but the same
graphical intuition is not available and the R2?s obtained should not 
be compared to those for models with
an intercept. ***Beware of high R2?s reported from models without an 
intercept***.

HTH
Petr




On 24 Jan 2006 at 11:50, Lance Westerhoff wrote:

To:             	r-help at stat.math.ethz.ch
From:           	Lance Westerhoff <lance at quantumbioinc.com>
Date sent:      	Tue, 24 Jan 2006 11:50:43 -0500
Subject:        	[R] R vs. Excel (R-squared)

> Hello All-
> 
> I found an inconsistency between the R-squared reported in Excel vs. 
> that in R, and I am wondering which (if any) may be correct and if 
> this is a known issue.  While it certainly wouldn't surprise me if 
> Excel is just flat out wrong, I just want to make sure since the R-
> squared reported in R seems surprisingly high.  Please let me know if 
> this is the wrong list.  Thanks!
> 
> To begin, I have a set of data points in which the y is the  
> experimental number and x is the predicted value.  The Excel- 
> generated graph (complete with R^2 and trend line) is provided at 
> this link if you want to take a look:
> 
> http://www.quantumbioinc.com/downloads/public/excel.png
> 
> As you can see, the R-squared that is reported by Excel is -0.1005.  
> Now when I bring the same data into R, I get an R-square of +0.9331 
> (see below).  Being that I am new to R and semi-new to stats, is 
> there a difference between "multiple R-squared" and R-squared that 
> perhaps I am simply interpreting this wrong, or is this a known 
> inconsistency between the two applications?  If so, which is  correct?
>  Any insight would be greatly appreciated!
> 
> 
> ======================
> 
>  > # note: a is experimental and c is predicted
>  > summary(lm(a~c-1))
> 
> Call:
> lm(formula = a ~ c - 1)
> 
> Residuals:
>      Min      1Q  Median      3Q     Max
> -2987.6 -1126.6  -181.7   855.3  5602.8
> 
> Coefficients:
>    Estimate Std. Error t value Pr(>|t|)
> c  0.99999    0.01402   71.33   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 1423 on 365 degrees of freedom
> Multiple R-Squared: 0.9331,	Adjusted R-squared: 0.9329
> F-statistic:  5088 on 1 and 365 DF,  p-value: < 2.2e-16
> 
>  > version
>           _
> platform powerpc-apple-darwin7.9.0
> arch     powerpc
> os       darwin7.9.0
> system   powerpc, darwin7.9.0
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> ======================
> 
> 
> Thank you very much for your time!
> 
> -Lance
> ____________________
> Lance M. Westerhoff, Ph.D.
> General Manager
> QuantumBio Inc.
> 
> WWW:    http://www.quantumbioinc.com
> Email:    lance at quantumbioinc.com
> 
> 
> "Safety is not the most important thing. I know this sounds like
> heresy, but it is a truth that must be embraced in order to do
> exploration. The most important thing is to actually go."  ~ James
> Cameron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ccatj at web.de  Wed Jan 25 08:10:33 2006
From: ccatj at web.de (Christian Jones)
Date: Wed, 25 Jan 2006 08:10:33 +0100
Subject: [R] combining variables with PCA
Message-ID: <470484320@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/773e7667/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jan 25 08:31:25 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jan 2006 07:31:25 +0000 (GMT)
Subject: [R] non-finite finite-difference value[]
In-Reply-To: <e984efc60601241130i7a4b8007j@mail.gmail.com>
References: <e984efc60601241130i7a4b8007j@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601250723260.31969@gannet.stats>

On Tue, 24 Jan 2006, P. Olsson wrote:

> Dear R-helpers,
>
> running a zeroinflated model of the following type:
> zinb = zeroinfl(count=response ~., x = ~ . - response, z = ~. - response,
> dist = "negbin", data = t.data, trace = TRUE)
> generates the following message:
>
> Zero-Inflated Count Model
> Using logit to model zero vs non-zero
> Using Negative Binomial for counts
> dependent variable y:
> Y
>  0   1   2   3
> 359  52   7   3
> generating start values...done
> MLE begins...
> initial  value 262.883959
> error in optim(fn = llhfunc, par = stval, method = method, control =
> control,  :
>        non-finite finite-difference value [3]
>
>
> Short version of my question:
> what information does non-finite finite-difference value [3] give?
>
>
> Extended version of my question:
> Since zeroinfl() calls optim(), I assume, that an infinite value is
> generated during the iteration steps of the optimizing algorithm. So the
> value[3] gives me a hint to the step, when it happens?

No.  The gradient is a vector, which is (I assume from the optim call) 
being evaluated by finite differences.  It is the third element of the 
vector that is non-finite (infinite or NA or NaN).

The information is for the optim user, which in this case is zeroinfl,
and is probably best passed on to the author of that function with an 
example.

> Or expressed in a different way, are my data structured in a way, that
> finding the maximum in the response surface is highly sensitive of the
> initial value, used for the algorithm?
> And if this is correct, is there an easy way in R, to set another starting
> value that the one, which is generated automatically in zeroinfl() (I mean,
> just from the technical point of view, without considering the theoretical
> thrill, which will be my next problem, since it is not at all familiar to
> me.).
> Or last questions: how can I extract the function, generated by zeroinfl()
> and passed to optim()? Is there a possibility to visualize it, so that I get
> an idea, what is going on?

These are questions about zeroinfl.  That is not part of R, and we don't 
know what it is.  There is a function of that name in package pscl, 
but you have not mentioned that and there could be others.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From drewbrewit at yahoo.com  Wed Jan 25 08:35:23 2006
From: drewbrewit at yahoo.com (Drew)
Date: Tue, 24 Jan 2006 23:35:23 -0800 (PST)
Subject: [R] panel function with barchart (lattice)
Message-ID: <20060125073523.11613.qmail@web50908.mail.yahoo.com>

Folks at R help,

I can't quite get the panel function to work the way I
want within barchart.
I guess I'm still not understanding how to piece
together multiple panel
arguments, especially when "groups" is specified.

Example: I want to be able to add the value of "yield"
to each section of
each bar in this graph:

barchart(yield ~ variety | site, data = barley,
	groups = year,
	layout = c(1,6),
	stack=TRUE,
	ylab = "Barley Yield (bushels/acre)"
)

To do this, I add my panel function:

barchart(yield ~ variety | site, data = barley,
	groups = year,
	layout = c(1,6),
	stack=TRUE,
	ylab = "Barley Yield (bushels/acre)",

	panel = function(x,y,subscripts,groups,...){
		panel.barchart(x,y,...)
		ltext(x = x, y = y, label =
round(barley$yield[subscripts],1), cex=.8)
	}
)

Then I get the values to print on each bar (which is
what I want) but the
bars no longer stack to appropriate height, and I
cannot get the subsections
of each bar to be a different color. I've tried
numerous variations of
panel.barchart, panel.superpose, etc. using examples
from ?xyplot, but
nothing quite works or I get an error message.

Any help would be appreciated.

~Nick



From ripley at stats.ox.ac.uk  Wed Jan 25 08:47:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jan 2006 07:47:59 +0000 (GMT)
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
In-Reply-To: <1db726800601241302n47efeb46ga53d3bee8e918bdb@mail.gmail.com>
References: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
	<1db726800601241302n47efeb46ga53d3bee8e918bdb@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601250741310.32325@gannet.stats>

On Tue, 24 Jan 2006, roger bos wrote:

> This question comes up a number of times what most people will tell you is
> that even if you get all you data into R you won't be able to do anything
> with it.  By that I mean, you need about 3 or 4 times as much memory as the
> size of your data object because R will need to create copies of it.
>
> I can tell you what I do in case it helps.  I also have a SQL Server
> database and the good thing about having the data in that format is that you
> probably don't need all of the data all of the time. First of all, a windows
> machine can handle up to 4GB of RAM, but most software cannot use all of

Or even up to 32Gb of RAM.  But a single process is limited to 3Gb of user 
address space, and to 2Gb unless you tell the OS to allow more.

> it by default.  I have 4GB and I also use the windows binary, so that means
> that whenever I download a new version of R, I have to modify the header
> file to the it LARGEADDRESSAWARE.  Using this trick, I can load up my big
> matrix into R to the point where task manager shows that R is using about
> 1.7GB of memory.  Despite such large objects, I am able to do regressions
> and other things with the data, so I am quite happy.  If you need more
> details just let me know.

This is the default in the current R 2.2.1, but you also need to set the 
/3GB flag in Windows.  I found the limit was about 1.7Gb without doing 
that, more like 2.5Gb when I did.

The details are in the rw-FAQ Q2.9 (and vary by R version, so please 
consult that in the version of R you use).

>
>
>
>
>
>
>
> On 1/24/06, r user <ruser2006 at yahoo.com> wrote:
>>
>> I am using R 2.1.1 in a Windows Xp environment.
>>
>> I need to import a large database from Microsoft SQL
>> into R.
>>
>> I am currently using the "sqlQuery" function/command.
>>
>> This works, but I sometimes run out of memory if my
>> database is too big, or it take quite a long time for
>> the data to import into R.
>>
>> Is there a better way to bring a large SQL database
>> into R?
>>
>> IS there an efficient way to convert the data into R
>> format prior to bringing it into R? (E.g. directly
>> from Microsoft SQL?)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrej.kastrin at siol.net  Wed Jan 25 09:08:14 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Wed, 25 Jan 2006 09:08:14 +0100
Subject: [R] read.table problem
Message-ID: <43D731EE.9050903@siol.net>

Dear R useRs,

I have big (23000 rows), vertical bar delimited file:

e.g.
A00001|Text a,Text b, Text c|345
A00002|Text bla|456
...
..
.

Try using

A <- read.table('filename.txt', header=FALSE,sep='\|')
 
process stop at line 11975 with warning message:
number of items read is not a multiple of the number of columns

I have no problems with processing similar file, which is only 10000 
rows long?

Any suggestion what's the problem here. Thank's in advance.

Cheers, Andrej



From comtech.usa at gmail.com  Wed Jan 25 09:09:34 2006
From: comtech.usa at gmail.com (Michael)
Date: Wed, 25 Jan 2006 00:09:34 -0800
Subject: [R] reducing learning curves?
Message-ID: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/9ded6c3e/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Jan 25 09:56:23 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jan 2006 09:56:23 +0100
Subject: [R] read.table problem
In-Reply-To: <43D731EE.9050903@siol.net>
References: <43D731EE.9050903@siol.net>
Message-ID: <x2psmgd6tk.fsf@viggo.kubism.ku.dk>

Andrej Kastrin <andrej.kastrin at siol.net> writes:

> Dear R useRs,
> 
> I have big (23000 rows), vertical bar delimited file:
> 
> e.g.
> A00001|Text a,Text b, Text c|345
> A00002|Text bla|456
> ...
> ..
> .
> 
> Try using
> 
> A <- read.table('filename.txt', header=FALSE,sep='\|')
>  
> process stop at line 11975 with warning message:
> number of items read is not a multiple of the number of columns
> 
> I have no problems with processing similar file, which is only 10000 
> rows long?
> 
> Any suggestion what's the problem here. Thank's in advance.

Well, the most obvious suspect is the file...

You might try 

table(count.fields('filename.txt',sep='|'))

(There's no point in escaping the vertical bar) 

Also, beware of quote symbols in the file, and possibly consider using
fill=TRUE.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Wed Jan 25 10:16:36 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 25 Jan 2006 10:16:36 +0100
Subject: [R] Can R handle medium and large size data sets?
In-Reply-To: <20060124201307.1245.qmail@web34701.mail.mud.yahoo.com>
References: <a194dd3e0601240617h3b0a1327j2e30f88eb620f333@mail.gmail.com>
	<20060124201307.1245.qmail@web34701.mail.mud.yahoo.com>
Message-ID: <17367.16884.732189.511819@stat.math.ethz.ch>

>>>>> "Martin" == Martin Lam <tmlammail at yahoo.com>
>>>>>     on Tue, 24 Jan 2006 12:13:07 -0800 (PST) writes:

    Martin> Dear Gueorgui,

    >> Is it true that R generally cannot handle medium sized
    >> data sets(a couple of hundreds of thousands observations)
    >> and threrefore large date set(couple of millions of
    >> observations)?

    Martin> It depends on what you want to do with the data sets.
    Martin> Loading the data sets shouldn't be any problem I
    Martin> think. But using the data sets for analysis using self
    Martin> written R code can get (very) slow,  since R is an
    Martin> interpreted language (correct me if I'm wrong).

(Since you asked for it ;-) )
Yes, you are wrong to quite some extent (you are partially
right, too):  Of course one *can* write  ``self written R code''
that is very slow, and yes, we have seen such code more than
once.  However, 98% of the problems {never trust a statistic
unless you mad it up ... :-) :-) } are relatively easily
solvable very efficiently with R.
You are right that it is easier to write slow code in an
interpreted language than in a compiled one.
E.g., not making use of vectorized operations in R is one famous
recipe to produce slow code pretty successfully ...

    Martin>  To increase speed you will often need to experiment with
    Martin> the R code. For example, what I've noticed is that
    Martin> processing data sets as matrices works much faster
    Martin> than data.frame().

yes, indeed;  see also the other answers to Gueorgui's question.

    Martin> Writing your code in C(++), compile it and include
    Martin> it in your R code is often the best way.

    Martin> HTH,

    Martin> Martin



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan 25 11:06:41 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 25 Jan 2006 10:06:41 -0000 (GMT)
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <971536df0601242036w12070f6drc87825a50c4cd03b@mail.gmail.com>
Message-ID: <XFMail.060125100641.Ted.Harding@nessie.mcc.ac.uk>

I've been reluctant to step into this topic, but now
feel that it may be helpful to make a certain point.

On the internet, for the most part, the person behind
the email is invisible and intangible. It is therefore
possible, when someone puts their foot down, to stamp
inadvertently on someone else's already broken toes.

A friend of mine, very intelligent, very knowledgeable
and creative, very articulate, nevertheless when writing
uses spelling which can be a close approximation to
random, and some interesting variants of grammar and
vocabulary as well.

The reason: dyslexia.

While most of us hit the wrong keys at times (and when
we read back over what we've written tend to see what we
intended to write rather than what we did write), and
when backed against the wall would admit that we could
have got it right if we had paid better attention, there
are some people who can't help "getting it wrong".

But, on the internet, one cannot readily recognise who
they are (though in some cases, if one knows the signs,
one may guess).

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Jan-06                                       Time: 10:06:35
------------------------------ XFMail ------------------------------



From michael.kobl at gmx.net  Wed Jan 25 11:27:10 2006
From: michael.kobl at gmx.net (Michael Kobl)
Date: Wed, 25 Jan 2006 11:27:10 +0100
Subject: [R] documentation mistake
Message-ID: <000f01c62199$e6f1b170$5835990a@lrzmuenchen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/8b68d4a8/attachment.pl

From bonneu at cict.fr  Wed Jan 25 11:35:31 2006
From: bonneu at cict.fr (Florent Bonneu)
Date: Wed, 25 Jan 2006 11:35:31 +0100
Subject: [R] How to use rfm.test ? (Package MarkedPointProcess)
Message-ID: <43D75473.80200@cict.fr>

I would like to compute the MC test (rfm.test) available in the package MarkedPointProcess (for the data BITOEK for example) in order to test the
dependence between the marks and their locations. Why the syntax of rfm.test is false here? I have the message :


******************************
ML WARNING! Forbidden values! -- if there are too many warnings try narrower lower and upper bounds for the variables. -9.802347e-17 1364.372 [ 0 0.01808660 , 9.990131 1364.372 ]
******************************


The program :
-----
# Packages #
library(spatstat)

library(RandomFields)
library(adapt)
library(MarkedPointProcess)


# Program #
data(BITOEK)
win <- ripras(steigerwald$coord)
PointProcess <- ppp(x=steigerwald$coord[,1],y=steigerwald$coord[,2],window=win,marks=steigerwald$diam)
plot(PointProcess)


rfm.test(coord=steigerwald$coord,steigerwald$diam,MCrepetitions=19)
-----


regards.

Florent Bonneu
Laboratoire de Statistique et Probabilit??s
bureau 148  b??t. 1R2
Universit?? Toulouse 3
118 route de Narbonne - 31062 Toulouse cedex 9
bonneu at cict.fr



From matthieu.cornec at gmail.com  Wed Jan 25 11:38:04 2006
From: matthieu.cornec at gmail.com (Matthieu Cornec)
Date: Wed, 25 Jan 2006 11:38:04 +0100
Subject: [R] Question about Aggregate
Message-ID: <8a83e5000601250238h51210914ud4545cc8a9d3d93c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/6b139c4b/attachment.pl

From Bernhard_Pfaff at fra.invesco.com  Wed Jan 25 11:53:27 2006
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Wed, 25 Jan 2006 10:53:27 -0000
Subject: [R] reducing learning curves?
Message-ID: <25D1C2585277D311B9A20000F6CCC71B077C03CE@DEFRAEX02>

Hello Michael,

you might want to utilise Emacs/ESS. ESS provides auto-completion by using
<TAB> for a process buffer '*R*' and C-c<TAB> for a source file '*.R'
(ess-mode). 

As far as debugging is concerned, R offers:

?browser
?debug
?trace

for example. Additionally, there is a CRAN package named 'debug' available
and an article in RNews:
Mark Bravington. Debugging without (too many) tears. R News, 3(3):29-32,
December 2003, about it.

HTH,
Bernhard  



-----Urspr??ngliche Nachricht-----
Von: Michael [mailto:comtech.usa at gmail.com] 
Gesendet: Mittwoch, 25. Januar 2006 09:10
An: R-help at stat.math.ethz.ch
Betreff: [R] reducing learning curves?

Hi all,

I am really new to the R language. I am a long time Matlab and C++ user and
I was "forced" to learn R because I am taking a statistics class.

I am seeking to reduce the learning curve to as smooth as possible.

Are there any addon/plug-in features that can reduce the learning curve, for
example, the following features can be very helpful for new learners:

1. Matlab-like command line auto-completion: Matlab has huge amount of
command and nobody is able to remember them off the head. So a nice feature
of Matlab command line is that I just need to type the first a few letters
and then I press "TAB" key, there will be a list of possible commands
popping up so I just need to select one. This helps a lot in terms of
learning for new comers. A more advanced command auto-completion is Visual
C++-like, which is implemented in program editor. It helps a lot while doing
programming;

2. A good IDE editor with embedded inline debugger: can be as good as VC++,
but also can be as simple as Matlab's debugger, which can breakpoint and
trace line-by-line... the editor can do syntax correction, syntax check,
syntax highlighting, code formatting, etc.

Could you please recommend some good addon/plugins that have the above
features?

Could you please also suggest some tips/tools/tricks that can help me reduce
the learning curve?

Thank you very much!

Michael.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}



From bgytcc at leeds.ac.uk  Wed Jan 25 11:56:46 2006
From: bgytcc at leeds.ac.uk (Tom C Cameron)
Date: Wed, 25 Jan 2006 10:56:46 +0000
Subject: [R] Interpolating spline problems and akima
Message-ID: <1138186606.96f9f7fa2a122@webmail1.leeds.ac.uk>

Hi everyone

I was using spline to interpolate single or two consecutive missing data points
in time series. However, when it comes to longer gaps in the data the spline
function generate new data for both my known and unknown data (see below).

Aside from not understanding why this happens, I thought thought I might try
function "aspline" in library (akima). However, I cannot install or find this
library/package.

Has anyone any instructions to install this package or help with my problem?


Using this works....

> day<-c(1,2,3,4,5,6,7,8,9,10,11,12,13)
> datfile<-c(16,23,39,37,45,34,NA,3,7,8,15,20,21)
> for (i in 1:13) {
+ dat<-datfile
+ dat.ok <- !is.na(dat)
+ dat2<-dat[dat.ok]
+ day2<-day[dat.ok]
+
+ dat.interp<-spline(day2,dat2,length(day))
+ outt<-rep(NA,length(day))
+ for (j in 1:length(dat.interp$x)) {
+         outt[j]<-dat.interp$y[j]
+ } }

but this does not....?


> day<-c(1,2,3,4,5,6,7,8,9,10)
> datfile<-c(17,6,29,22,90,72,20,4,4,NA)
> for (i in 1:10) {
+ dat<-datfile
+ dat.ok <- !is.na(dat)
+ dat2<-dat[dat.ok]
+ day2<-day[dat.ok]
+
+ dat.interp<-spline(day2,dat2,length(day))
+ outl<-rep(NA,length(day))
+ for (j in 1:length(dat.interp$x)) {
+         outl[j]<-dat.interp$y[j]
+ } }
...........................................................................
Dr Tom C Cameron                        office: 0113 34 32837 (10.23 Miall)
Ecology & Evolution Res. Group.         lab: 0113 34 32884 (10.20 Miall)
School of Biological Sciences           Mobile: 07966160266
University of Leeds                     email: t.c.cameron at leeds.ac.uk
Leeds LS2 9JT
LS2 9JT



From finbref.2006 at gmail.com  Wed Jan 25 11:59:12 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 25 Jan 2006 11:59:12 +0100
Subject: [R] persp() and character labels for axis
Message-ID: <d0f55a670601250259g13479682r@mail.gmail.com>

I want to plot dates on the y-axis of a persp() plot.

persp(x=1:30,y=days,y=yields)
axis(2, 1:5, LETTERS[1:5])

does not work. On the mailinglist I found old messages, that said,
that text() does not apply (yet) for 3-d plots. And the same question
( http://www.r-project.org/nocvs/mail/r-help/2002/9009.html ) was not
answered. Perhaps this time? ;)



From ligges at statistik.uni-dortmund.de  Wed Jan 25 12:56:28 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 Jan 2006 12:56:28 +0100
Subject: [R] persp() and character labels for axis
In-Reply-To: <d0f55a670601250259g13479682r@mail.gmail.com>
References: <d0f55a670601250259g13479682r@mail.gmail.com>
Message-ID: <43D7676C.6050404@statistik.uni-dortmund.de>

Thomas Steiner wrote:

> I want to plot dates on the y-axis of a persp() plot.
> 
> persp(x=1:30,y=days,y=yields)
> axis(2, 1:5, LETTERS[1:5])
> 
> does not work. On the mailinglist I found old messages, that said,
> that text() does not apply (yet) for 3-d plots. And the same question

?persp points you to ?trans3d which is useful to calculate coordinates 
for calls to 2D functions such as text().

Uwe Ligges

> ( http://www.r-project.org/nocvs/mail/r-help/2002/9009.html ) was not
> answered. Perhaps this time? ;)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From schlath at hsu-hh.de  Wed Jan 25 11:59:57 2006
From: schlath at hsu-hh.de (Martin Schlather)
Date: Wed, 25 Jan 2006 11:59:57 +0100
Subject: [R] How to use rfm.test ? (Package MarkedPointProcess)
In-Reply-To: <43D75473.80200@cict.fr>
References: <43D75473.80200@cict.fr>
Message-ID: <43D75A2D.2010803@hsu-hh.de>

Dear Florent Bonneu,

the optim algorithm with parameter method="L-BFGS-B"
(used in rfm.test) does not stay always exactly within the
given bounds during the search of the optimum. This happens
more frequently when the bounds are too wide. Here. rfm.test
notices that -9.802347e-17 is less then the given lower bound
of value 0.
If the messages appear frequently it appeared to me that the
algorithms runs into boundary, local minima more frequently.

So, if the messages appear only a very few times, ignore
them. If they appear several times, use closer bounds.

Cheers,
Martin

Florent Bonneu wrote:
> I would like to compute the MC test (rfm.test) available in the
> package MarkedPointProcess (for the data BITOEK for example) in order
> to test the
> dependence between the marks and their locations. Why the syntax of
> rfm.test is false here? I have the message :
>
>
> ******************************
> ML WARNING! Forbidden values! -- if there are too many warnings try
> narrower lower and upper bounds for the variables. -9.802347e-17
> 1364.372 [ 0 0.01808660 , 9.990131 1364.372 ]
> ******************************
>
>
> The program :
> -----
> # Packages #
> library(spatstat)
>
> library(RandomFields)
> library(adapt)
> library(MarkedPointProcess)
>
>
> # Program #
> data(BITOEK)
> win <- ripras(steigerwald$coord)
> PointProcess <-
> ppp(x=steigerwald$coord[,1],y=steigerwald$coord[,2],window=win,marks=steigerwald$diam)
>
> plot(PointProcess)
>
>
> rfm.test(coord=steigerwald$coord,steigerwald$diam,MCrepetitions=19)
> -----
>
>
> regards.
>
> Florent Bonneu
> Laboratoire de Statistique et Probabilit??s
> bureau 148  b??t. 1R2
> Universit?? Toulouse 3
> 118 route de Narbonne - 31062 Toulouse cedex 9
> bonneu at cict.fr
>
>


-- 
Prof. Dr. habil. Martin Schlather
Helmut-Schmidt-Universit??t
Universit??t der Bundeswehr Hamburg
Fachbereich WOW
Holstenhofweg 85
D -- 22043 Hamburg
email: schlather at hsu-hh.de     http://www2.unibw-hamburg.de/schlath
phone: +49 (0)40 6541 2873     Fax : +49 (0)40 6541 2023



From HDoran at air.org  Wed Jan 25 13:37:01 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 25 Jan 2006 07:37:01 -0500
Subject: [R] nested ANCOVA: still confused
Message-ID: <F5ED48890E2ACB468D0F3A64989D335AC99154@dc1ex3.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/768defe4/attachment.pl

From ggrothendieck at gmail.com  Wed Jan 25 13:40:54 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jan 2006 07:40:54 -0500
Subject: [R] Question about Aggregate
In-Reply-To: <8a83e5000601250238h51210914ud4545cc8a9d3d93c@mail.gmail.com>
References: <8a83e5000601250238h51210914ud4545cc8a9d3d93c@mail.gmail.com>
Message-ID: <971536df0601250440md3f7195q4ac0c9e7edf9d9eb@mail.gmail.com>

Try this:

> library(zoo)
> z <- zooreg(c(12, 13, 12, 14, 16, 15), start = c(2001, 2), freq = 12)
> aggregate(z, trunc(4 * time(z))/4, mean)
2001(1) 2001(2) 2001(3)
   12.5    14.0    15.0


On 1/25/06, Matthieu Cornec <matthieu.cornec at gmail.com> wrote:
> hello,
>
> Suppose you a monthly series you want to aggregate at a quaterly frequency
> with the start and the end of your series in the middle of the quarter.
> For example
>  2001M2 2001M3 2001M4 2001M5 2001M6 2001M7
> 12                  13     12      14         16              15
>
> how can you get something like :
> 2001Q1 2001Q2 200Q3
> NA       14 NA
>
> or
>  2001Q1 2001Q2 200Q3
> 12.5 14 15
>
> Thanks in advance
>
> Matthieu
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From finbref.2006 at gmail.com  Wed Jan 25 13:43:48 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 25 Jan 2006 13:43:48 +0100
Subject: [R] persp() and character labels for axis
In-Reply-To: <43D7676C.6050404@statistik.uni-dortmund.de>
References: <d0f55a670601250259g13479682r@mail.gmail.com>
	<43D7676C.6050404@statistik.uni-dortmund.de>
Message-ID: <d0f55a670601250443y498eb40cy@mail.gmail.com>

> > I want to plot dates on the y-axis of a persp() plot.

> ?persp points you to ?trans3d which is useful to calculate coordinates
> for calls to 2D functions such as text().

?trans3d gives this:
No documentation for 'trans3d' in specified packages and libraries:
you could try 'help.search("trans3d")'

Even loading grDevices and graphics libraries did not help.
http://finzi.psych.upenn.edu/R/library/grDevices/html/trans3d.html
seems to be what I should look for, but there it says noting about
text either (I want to label axis! ,something like
t=1:10
plot(x=t,y=t^2,type="l")
axis(1,labels=LETTERS[1:5])
and more proper...)

Thomas



From ligges at statistik.uni-dortmund.de  Wed Jan 25 13:49:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 Jan 2006 13:49:20 +0100
Subject: [R] persp() and character labels for axis
In-Reply-To: <d0f55a670601250443y498eb40cy@mail.gmail.com>
References: <d0f55a670601250259g13479682r@mail.gmail.com>	
	<43D7676C.6050404@statistik.uni-dortmund.de>
	<d0f55a670601250443y498eb40cy@mail.gmail.com>
Message-ID: <43D773D0.5060702@statistik.uni-dortmund.de>

So your R is outdated. Please upgrade!

Uwe Ligges



Thomas Steiner wrote:

>>>I want to plot dates on the y-axis of a persp() plot.
> 
> 
>>?persp points you to ?trans3d which is useful to calculate coordinates
>>for calls to 2D functions such as text().
> 
> 
> ?trans3d gives this:
> No documentation for 'trans3d' in specified packages and libraries:
> you could try 'help.search("trans3d")'
> 
> Even loading grDevices and graphics libraries did not help.
> http://finzi.psych.upenn.edu/R/library/grDevices/html/trans3d.html
> seems to be what I should look for, but there it says noting about
> text either (I want to label axis! ,something like
> t=1:10
> plot(x=t,y=t^2,type="l")
> axis(1,labels=LETTERS[1:5])
> and more proper...)
> 
> Thomas



From roger.bos at gmail.com  Wed Jan 25 13:59:15 2006
From: roger.bos at gmail.com (roger bos)
Date: Wed, 25 Jan 2006 07:59:15 -0500
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
In-Reply-To: <Pine.LNX.4.61.0601250741310.32325@gannet.stats>
References: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
	<1db726800601241302n47efeb46ga53d3bee8e918bdb@mail.gmail.com>
	<Pine.LNX.4.61.0601250741310.32325@gannet.stats>
Message-ID: <1db726800601250459mb1015fk1bb59759f37dcfcb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/510a3278/attachment.pl

From drewbrewit at yahoo.com  Wed Jan 25 14:22:36 2006
From: drewbrewit at yahoo.com (Drew)
Date: Wed, 25 Jan 2006 05:22:36 -0800 (PST)
Subject: [R] reducing learning curves?
Message-ID: <20060125132236.19414.qmail@web50913.mail.yahoo.com>

In regards to text editors:

If you are a Unix user, I'd recommend Emacs (although
it has its own large
learning curve.) On Windows I use PSpad
(www.pspad.com) because it is easy
to use and learn and has some of the features you
request: syntax
highlighting, code completion, code builder, among
many other features I
find useful.

~Nick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Michael
Sent: Wednesday, January 25, 2006 12:10 AM
To: R-help at stat.math.ethz.ch
Subject: [R] reducing learning curves?


Hi all,

I am really new to the R language. I am a long time
Matlab and C++ user and
I was "forced" to learn R because I am taking a
statistics class.

I am seeking to reduce the learning curve to as smooth
as possible.

Are there any addon/plug-in features that can reduce
the learning curve, for
example, the following features can be very helpful
for new learners:

1. Matlab-like command line auto-completion: Matlab
has huge amount of
command and nobody is able to remember them off the
head. So a nice feature
of Matlab command line is that I just need to type the
first a few letters
and then I press "TAB" key, there will be a list of
possible commands
popping up so I just need to select one. This helps a
lot in terms of
learning for new comers. A more advanced command
auto-completion is Visual
C++-like, which is implemented in program editor. It
helps a lot while doing
programming;

2. A good IDE editor with embedded inline debugger:
can be as good as VC++,
but also can be as simple as Matlab's debugger, which
can breakpoint and
trace line-by-line... the editor can do syntax
correction, syntax check,
syntax highlighting, code formatting, etc.

Could you please recommend some good addon/plugins
that have the above
features?

Could you please also suggest some tips/tools/tricks
that can help me reduce
the learning curve?

Thank you very much!

Michael.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan 25 14:27:23 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jan 2006 13:27:23 +0000 (GMT)
Subject: [R] Within-Subjects ANOVA & comparisons of individual means
In-Reply-To: <43D65B45.7060704@pdf.com>
References: <43D09986.4070604@mail.gwdg.de> <43D65B45.7060704@pdf.com>
Message-ID: <Pine.LNX.4.61.0601241735240.15990@gannet.stats>

On Tue, 24 Jan 2006, Spencer Graves wrote:

> 	  1.  Did you try "summary(aovRes, ...)" rather than
> "summary.aov(aovRes, ...)"?  From "summary.aov", I got the same error
> message you did, but from "summary(aovRes, ...)", I got something that
> looked like what you were expecting.
>
> 	  2.  To understand this, look at the code for "summary":  It consists
> essentially of 'UseMethod("summary")'.  To trace methods dispatch here,
> note that 'class(aovRes)' is a 2-vector:  c("aovlist", "listof").  When
> I requested 'methods("summary"), I got a long list, the first two were
> 'summary.aov' and 'summary.aovlist'.  Since aovRes was NOT of class
> 'aov' but instead of class 'aovlist', summary(aovRes, ...) uses
> 'summary.aovlist'.  You got an error message, because summary.aov was
> expecting an argument of class 'aov', and 'aovRes' didn't have the
> structure it required.
>
> 	  3.  To find an attribute "contrasts" in aovRes, I requested
> 'str(aovRes)' and searched for "contrasts".  I found it in two places:
>
> attr(attr(aovRes, "error.qr")$qr, "contrasts")
> attr(aovRes, "contrasts")
>
> 	  4.  I wondered why you said, "From my understanding, TukeyHSD is not
> appropriate in this context."  Then I discovered that TukeyHSD is
> officially a generic function with a method only for objects of class
> "aov".  Since aovRes is NOT of class "aov", it can't use that function.
> However, it looks to me like the same algorithm could be used for what
> you want, e.g., by copying "TukeyHSD.aov" into a script file, changing
> the name to "TukeyHSD.aovlist" and then walking through the code line by
> line, and changing things as necessary to produce the desired results.
> If I wanted to do this, I might first check with the author of TukeyHSD
> (Douglas Bates).  He might know some reason why it is inappropriate or
> some other special concerns of which I'm unaware.  Or he might have a
> not-quite-fully debugged version someplace that could help you.

It is a lot more complicated than that.  You can probably adjust TukeyHSD 
to work on a single stratum of an "aovlist" object, but it is not so 
obvious that you can retrieve the information needed from the aovlist 
object so you may need to refit.  However, my reason for not pursuing that 
is more philosophical: these post hoc tests are intended to allow for 
multiple testing, and that would only make sense if you allow for all the 
tests done in all the strata.  But here there is only stratum, and that 
suggests the wrong analysis is being done.

As I understand this design, it is a randomized block design with subject 
as block and interval as treatment.  The classic analysis is a 
fixed-effect one (subject + interval), and TukeyHSD can be applied to 
that.  E.g.

> fm <- aov(dv ~ subject + interval)
> summary(fm)
             Df  Sum Sq Mean Sq F value    Pr(>F)
subject      6  2.4853  0.4142  1.3067    0.2845
interval     5 13.8174  2.7635  8.7178 3.417e-05 ***
Residuals   30  9.5098  0.3170

(should might look familiar)
> TukeyHSD(fm, "interval")
   Tukey multiple comparisons of means
     95% family-wise confidence level

Fit: aov(formula = dv ~ subject + interval)

$interval
           diff       lwr         upr     p adj
2-1 -0.7477000 -1.663060  0.16766002 0.1608612
3-1 -1.0704286 -1.985789 -0.15506855 0.0145883
4-1 -1.4761143 -2.391474 -0.56075426 0.0004035
5-1 -1.5005143 -2.415874 -0.58515426 0.0003225
6-1 -1.6827143 -2.598074 -0.76735426 0.0000601
3-2 -0.3227286 -1.238089  0.59263145 0.8884095
4-2 -0.7284143 -1.643774  0.18694574 0.1814407
5-2 -0.7528143 -1.668174  0.16254574 0.1557224
6-2 -0.9350143 -1.850374 -0.01965426 0.0430664
4-3 -0.4056857 -1.321046  0.50967431 0.7563642
5-3 -0.4300857 -1.345446  0.48527431 0.7095947
6-3 -0.6122857 -1.527646  0.30307431 0.3475861
5-4 -0.0244000 -0.939760  0.89096002 0.9999994
6-4 -0.2066000 -1.121960  0.70876002 0.9821152
6-5 -0.1822000 -1.097560  0.73316002 0.9898235

However, I find it suspicious that your treatment effects are in fact 
ordered, and suspect there is something more to the squeezed out here.
You appear to be defining difference contrasts without using contr.sdif 
(in MASS).  Once you have a single-stratum fit, a lot more tools become 
available (se.contrast is one).  But for starters

> fm2 <- lm(dv ~ subject+interval, contrasts=list(interval="contr.sdif"))
> summary(fm2)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)
...
interval2-1 -0.74770    0.30095  -2.484   0.0188 *
interval3-2 -0.32273    0.30095  -1.072   0.2921
interval4-3 -0.40569    0.30095  -1.348   0.1877
interval5-4 -0.02440    0.30095  -0.081   0.9359
interval6-5 -0.18220    0.30095  -0.605   0.5495

I hope that is enough food for thought (it is not an invitation for
further free consultancy).


> Steffen Katzner wrote:
>
>> I am having problems with comparing individual means in a
>> within-subjects ANOVA. From my understanding, TukeyHSD is not
>> appropriate in this context. So I am trying to compute contrasts, as
>> follows:
>>
>> seven subjects participated in each of 6 conditions (intervals).
>>
>>> subject = factor(rep(c(1:7), each = 6))
>>> interval = factor(rep(c(1:6), 7))
>>
>> and here is the dependent variable:
>>
>>> dv = c(3.3868, 3.1068, 1.7261, 1.5415, 1.7356, 0.7538,
>>
>> + 2.5957, 1.5666, 1.1984, 1.2761, 1.0022, 0.8597,
>> + 3.9819, 3.1506, 1.5824, 1.7400, 1.4248, 0.6519,
>> + 2.2521, 1.5248, 1.1209, 1.2193, 1.1994, 2.0910,
>> + 2.4661, 1.3863, 1.3591, 0.9163, 1.3976, 1.7471,
>> + 3.2486, 1.9492, 2.4228, 1.1276, 1.2836, 0.9814,
>> + 1.7148, 1.7278, 2.7433, 1.4924, 1.0992, 0.7821)
>>
>>
>>> d = data.frame(subject, interval, dv)
>>
>> next I'm defining a contrast matrix:
>>
>>> con = matrix(c(1, -1, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1, 0,
>>
>> 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 1, -1), nrow=6, ncol=5, byrow=F)
>>
>>> con
>>      [,1] [,2] [,3] [,4] [,5]
>> [1,]    1    0    0    0    0
>> [2,]   -1    1    0    0    0
>> [3,]    0   -1    1    0    0
>> [4,]    0    0   -1    1    0
>> [5,]    0    0    0   -1    1
>> [6,]    0    0    0    0   -1
>>
>>> contrasts(d$interval)=con
>>
>> and then I'm doing the ANOVA
>>
>>> aovRes = aov(dv~interval+Error(subject/interval), data=d)
>>
>>> summary(aovRes)
>>
>> Error: subject
>>            Df  Sum Sq Mean Sq F value Pr(>F)
>> Residuals  6 2.48531 0.41422
>>
>> Error: subject:interval
>>            Df  Sum Sq Mean Sq F value    Pr(>F)
>> interval   5 13.8174  2.7635  8.7178 3.417e-05 ***
>> Residuals 30  9.5098  0.3170
>> ---
>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
>>
>> but if I want to look at the contrasts, something has gone wrong:
>>
>> summary.aov(aovRes, split=list(interval = list("i1 vs i2" = 1, "i2 vs
>> i3" = 2, "i3 vs i4" = 3, "i4 vs i5" = 4, "i5 vs i6" = 5)))
>>
>> Error in 1:object$rank : NA/NaN argument
>>
>>> aovRes$contrasts
>>
>> NULL
>>
>> Can anybody help?
>> Thank you very much,  -Steffen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roger.bos at gmail.com  Wed Jan 25 14:32:59 2006
From: roger.bos at gmail.com (roger bos)
Date: Wed, 25 Jan 2006 08:32:59 -0500
Subject: [R] Is there no definition for global variables in R?
In-Reply-To: <2112.1138100492@www065.gmx.net>
References: <6203.1131548723@www95.gmx.net> <2112.1138100492@www065.gmx.net>
Message-ID: <1db726800601250532g4c057360td1ae55a201ceb6cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/ed4834a8/attachment.pl

From roger.bos at gmail.com  Wed Jan 25 15:10:04 2006
From: roger.bos at gmail.com (roger bos)
Date: Wed, 25 Jan 2006 09:10:04 -0500
Subject: [R] exporting dates into Microsoft SQL Server
In-Reply-To: <20060123235235.15318.qmail@web37005.mail.mud.yahoo.com>
References: <20060123235235.15318.qmail@web37005.mail.mud.yahoo.com>
Message-ID: <1db726800601250610k6fdcbf1clee0475e23ac01ab3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/25f2c15f/attachment.pl

From bonneu at cict.fr  Wed Jan 25 15:10:25 2006
From: bonneu at cict.fr (Florent Bonneu)
Date: Wed, 25 Jan 2006 15:10:25 +0100
Subject: [R] How to use rfm.test ? (Package MarkedPointProcess)
In-Reply-To: <43D75A2D.2010803@hsu-hh.de>
References: <43D75473.80200@cict.fr> <43D75A2D.2010803@hsu-hh.de>
Message-ID: <43D786D1.8030006@cict.fr>

I don't find where it's possible to configure "closer bounds" in the 
algorithm. I have obtained some results with

rfm.test(coord=steigerwald$coord,steigerwald$diam,MCrepetitions=19,n.hypo=10)

but I don't know how to interpret them. Where is the result of the MC test ?
Could you give me the syntax of rfm.test for this example (Steigerwald) 
that you have used in your paper "Detecting dependence between Marks and 
Locations of Marked Point Processes ?

regards.



Martin Schlather wrote:

>Dear Florent Bonneu,
>
>the optim algorithm with parameter method="L-BFGS-B"
>(used in rfm.test) does not stay always exactly within the
>given bounds during the search of the optimum. This happens
>more frequently when the bounds are too wide. Here. rfm.test
>notices that -9.802347e-17 is less then the given lower bound
>of value 0.
>If the messages appear frequently it appeared to me that the
>algorithms runs into boundary, local minima more frequently.
>
>So, if the messages appear only a very few times, ignore
>them. If they appear several times, use closer bounds.
>
>Cheers,
>Martin
>
>Florent Bonneu wrote:
>  
>
>>I would like to compute the MC test (rfm.test) available in the
>>package MarkedPointProcess (for the data BITOEK for example) in order
>>to test the
>>dependence between the marks and their locations. Why the syntax of
>>rfm.test is false here? I have the message :
>>
>>
>>******************************
>>ML WARNING! Forbidden values! -- if there are too many warnings try
>>narrower lower and upper bounds for the variables. -9.802347e-17
>>1364.372 [ 0 0.01808660 , 9.990131 1364.372 ]
>>******************************
>>
>>
>>The program :
>>-----
>># Packages #
>>library(spatstat)
>>
>>library(RandomFields)
>>library(adapt)
>>library(MarkedPointProcess)
>>
>>
>># Program #
>>data(BITOEK)
>>win <- ripras(steigerwald$coord)
>>PointProcess <-
>>ppp(x=steigerwald$coord[,1],y=steigerwald$coord[,2],window=win,marks=steigerwald$diam)
>>
>>plot(PointProcess)
>>
>>
>>rfm.test(coord=steigerwald$coord,steigerwald$diam,MCrepetitions=19)
>>-----
>>
>>
>>regards.
>>
>>Florent Bonneu
>>Laboratoire de Statistique et Probabilit??s
>>bureau 148  b??t. 1R2
>>Universit?? Toulouse 3
>>118 route de Narbonne - 31062 Toulouse cedex 9
>>bonneu at cict.fr
>>
>>
>>    
>>
>
>
>  
>



From broland at gwu.edu  Wed Jan 25 15:14:23 2006
From: broland at gwu.edu (Brent Roland)
Date: Wed, 25 Jan 2006 09:14:23 -0500
Subject: [R] Problems running R immediately after installing
Message-ID: <000a01c621b9$a48f29f0$c689a480@rolanddesktop>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/511e0b92/attachment.pl

From dirk.debecker at biw.kuleuven.be  Wed Jan 25 15:29:53 2006
From: dirk.debecker at biw.kuleuven.be (Dirk De Becker)
Date: Wed, 25 Jan 2006 15:29:53 +0100
Subject: [R] Savitsky Golay smoothing
Message-ID: <43D78B61.1030202@biw.kuleuven.be>

Hi,

I am trying to process some spectral data using R, and I would like to 
use Savitsky Golay smoothing.
Is this already implemented in R or one of the optional packages, or do 
I have to implement is myself?

Thanks in advance,

Dirk

-- 
Dirk De Becker
Work: Kasteelpark Arenberg 30
      3001 Heverlee
      phone: ++32(0)16/32.14.44
      fax: ++32(0)16/32.85.90
Home: Waversebaan 90
      3001 Heverlee
      phone: ++32(0)16/23.36.65
dirk.debecker at biw.kuleuven.be
mobile phone: ++32(0)498/51.19.86


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ggrothendieck at gmail.com  Wed Jan 25 15:47:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jan 2006 09:47:25 -0500
Subject: [R] Savitsky Golay smoothing
In-Reply-To: <43D78B61.1030202@biw.kuleuven.be>
References: <43D78B61.1030202@biw.kuleuven.be>
Message-ID: <971536df0601250647k60c3ae92tafc3698904936368@mail.gmail.com>

Try:

RSiteSearch("Savitzky-Golay")


On 1/25/06, Dirk De Becker <dirk.debecker at biw.kuleuven.be> wrote:
> Hi,
>
> I am trying to process some spectral data using R, and I would like to
> use Savitsky Golay smoothing.
> Is this already implemented in R or one of the optional packages, or do
> I have to implement is myself?
>
> Thanks in advance,
>
> Dirk
>
> --
> Dirk De Becker
> Work: Kasteelpark Arenberg 30
>      3001 Heverlee
>      phone: ++32(0)16/32.14.44
>      fax: ++32(0)16/32.85.90
> Home: Waversebaan 90
>      3001 Heverlee
>      phone: ++32(0)16/23.36.65
> dirk.debecker at biw.kuleuven.be
> mobile phone: ++32(0)498/51.19.86
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fbuchins at wpahs.org  Wed Jan 25 15:50:32 2006
From: fbuchins at wpahs.org (Farrel Buchinsky)
Date: Wed, 25 Jan 2006 09:50:32 -0500
Subject: [R] Unequal sample sizes when calculating power
Message-ID: <6E520FA8688B6C4C91D24DF15FAEEA7B0FF4E6C3@wphnt1.wpahs.org>

Power calculations two sample test for proportions is very useful. Is there
a way however, to get away from the two samples being of the same size. What
would happen if one had n=15 in the one sample and n=45 in the other sample.

Farrel Buchinsky, MD 
Pediatric Otolaryngologist
Allegheny General Hospital
Pittsburgh, PA 



**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From sell_mirage_ne at hotmail.com  Wed Jan 25 04:29:51 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Tue, 24 Jan 2006 21:29:51 -0600
Subject: [R] is.integer() function
Message-ID: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>

Hi R users
I have a simple question to ask.
Why am I getting FALSE on this

>is.integer(10)
[1] FALSE

10 is a integer number.

Thanks in advance.

M



From antoinelucas at libertysurf.fr  Wed Jan 25 10:00:28 2006
From: antoinelucas at libertysurf.fr (Antoine Lucas)
Date: Wed, 25 Jan 2006 10:00:28 +0100
Subject: [R] [R-pkgs] amap
Message-ID: <20060125100028.5d9cc7c7.antoinelucas@libertysurf.fr>

Dear R users,


Version 0.7-1 of the amap package has been uploaded to CRAN.

   Amap package includes standard hierarchical
   clustering and k-means. We optimize implementation
   (with a parallelized hierarchical clustering) and
   allow the possibility of using different distances like
   Eulidean or Spearman (rank-based metric).

   We implement a principal component analysis (with robusts methods).

News of version 0.7-1: a new method of clustering "optimal partiton"

A major advantage of the method is that the number K of classes 
is computed from the data, and ONLY from the data. 

-- 
Antoine Lucas
Centre de g??n??tique Mol??culaire, CNRS
91198 Gif sur Yvette Cedex
Tel: (33)1 69 82 38 89
Fax: (33)1 69 82 38 77

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From csardi at rmki.kfki.hu  Wed Jan 25 16:23:30 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 25 Jan 2006 16:23:30 +0100
Subject: [R] is.integer() function
In-Reply-To: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
Message-ID: <20060125152330.GD20850@rmki.kfki.hu>

Becaues is.integer shows the internal representation, which is not an
integer but a double (real number). Some functions create integer vectors,
for example the : notation:

> is.integer(1:10)
[1] TRUE

You can create an integer vector with the integer() function:

> integer(10)
 [1] 0 0 0 0 0 0 0 0 0 0
> is.integer(integer(10))
[1] TRUE

Gabor

On Tue, Jan 24, 2006 at 09:29:51PM -0600, Taka Matzmoto wrote:
> Hi R users
> I have a simple question to ask.
> Why am I getting FALSE on this
> 
> >is.integer(10)
> [1] FALSE
> 
> 10 is a integer number.
> 
> Thanks in advance.
> 
> M
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From tlumley at u.washington.edu  Wed Jan 25 16:33:01 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jan 2006 07:33:01 -0800 (PST)
Subject: [R] importing a VERY LARGE database from Microsoft SQL into R
In-Reply-To: <1db726800601250459mb1015fk1bb59759f37dcfcb@mail.gmail.com>
References: <20060124202813.29480.qmail@web37013.mail.mud.yahoo.com>
	<1db726800601241302n47efeb46ga53d3bee8e918bdb@mail.gmail.com>
	<Pine.LNX.4.61.0601250741310.32325@gannet.stats>
	<1db726800601250459mb1015fk1bb59759f37dcfcb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0601250731460.32491@homer24.u.washington.edu>

On Wed, 25 Jan 2006, roger bos wrote:
>
> Does anyone have code that keeps generating random data until the memory is
> full and then tells you how much memory was successfully used?  I could try
> writing it, but if someone has already done it, thats all the better!
>

In recent versions of R gc() reports the maximum memory used, so all you 
need to do is write the code to use lots of memory.

 	-thomas



From pburns at pburns.seanet.com  Wed Jan 25 16:33:29 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 25 Jan 2006 15:33:29 +0000
Subject: [R] is.integer() function
In-Reply-To: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
Message-ID: <43D79A49.2070200@pburns.seanet.com>

S Poetry page 125.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Taka Matzmoto wrote:

>Hi R users
>I have a simple question to ask.
>Why am I getting FALSE on this
>
>  
>
>>is.integer(10)
>>    
>>
>[1] FALSE
>
>10 is a integer number.
>
>Thanks in advance.
>
>M
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From petr.pikal at precheza.cz  Wed Jan 25 16:35:10 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 25 Jan 2006 16:35:10 +0100
Subject: [R] is.integer() function
In-Reply-To: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
Message-ID: <43D7A8BE.18103.21CF450@localhost>

Hi

Numbers are normaly stored as double so you has to declare it as an 
integer.

> str(10)
 num 10
> str(as.integer(10))
 int 10
>

HTH
Petr



On 24 Jan 2006 at 21:29, Taka Matzmoto wrote:

From:           	"Taka Matzmoto" <sell_mirage_ne at hotmail.com>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Tue, 24 Jan 2006 21:29:51 -0600
Subject:        	[R] is.integer() function

> Hi R users
> I have a simple question to ask.
> Why am I getting FALSE on this
> 
> >is.integer(10)
> [1] FALSE
> 
> 10 is a integer number.
> 
> Thanks in advance.
> 
> M
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From tlumley at u.washington.edu  Wed Jan 25 16:36:18 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jan 2006 07:36:18 -0800 (PST)
Subject: [R] is.integer() function
In-Reply-To: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
Message-ID: <Pine.LNX.4.64.0601250733590.32491@homer24.u.washington.edu>

On Tue, 24 Jan 2006, Taka Matzmoto wrote:

> Hi R users
> I have a simple question to ask.
> Why am I getting FALSE on this
>
>> is.integer(10)
> [1] FALSE
>
> 10 is a integer number.

is.integer() asks how a number is stored, not whether the number happens 
to be an integer.  Explicit numbers typed into R are stored as double 
precision even if they happen to be whole numbers.  This is historical but 
is also useful: the double precision type can hold integers up to 2^52 
exactly and the integer type can only hold integers up to 2^31.

 	-thomas



From B.Rowlingson at lancaster.ac.uk  Wed Jan 25 16:57:20 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 25 Jan 2006 15:57:20 +0000
Subject: [R] is.integer() function
In-Reply-To: <20060125152330.GD20850@rmki.kfki.hu>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
	<20060125152330.GD20850@rmki.kfki.hu>
Message-ID: <43D79FE0.5080706@lancaster.ac.uk>

Gabor Csardi wrote:
> Becaues is.integer shows the internal representation, which is not an
> integer but a double (real number). Some functions create integer vectors,

  Some functions that you might think create integer vectors and even 
seem to say they create integer vectors dont create integer vectors:

      'ceiling' takes a single numeric argument 'x' and returns a
      numeric vector containing the smallest integers not less than the
      corresponding elements of 'x'.

 > ceiling(0.5)
[1] 1

 > is.integer(ceiling(0.5))
[1] FALSE

 > is.integer(1:3)
[1] TRUE
 > is.integer(ceiling(1:3))
[1] FALSE


  This could possibly be a documentation problem, since ?ceiling is 
using 'integer' in the sense of 'whole number', whereas ?is.integer is 
concerned with internal representation (aka 'storage mode')....

  This seems to be an endless source of confusion to anyone who didn't 
start their programming days in Fortran, C, or assembly language (or 
other strongly-typed language, I guess).

Barry



From droberts at montana.edu  Wed Jan 25 17:02:32 2006
From: droberts at montana.edu (Dave Roberts)
Date: Wed, 25 Jan 2006 09:02:32 -0700
Subject: [R] combining variables with PCA
In-Reply-To: <470484320@web.de>
References: <470484320@web.de>
Message-ID: <43D7A118.9080901@montana.edu>

Christian,

     One of the arguments to prcomp() is retx, with a default value of 
TRUE.  As explained in the help file, if retx is TRUE the prcomp object 
reurned by the function contains the projection of the original data 
along the principal components (which many of us call "scores").  Thus

 > z <- prcomp(cbind(runif(20),runif(20)))
 > z$x
                PC1          PC2
  [1,] -0.176564769 -0.175010202
  [2,]  0.286746995 -0.464200150
  [3,] -0.010385993  0.415020704
  [4,]  0.002108131 -0.150929991
  [5,] -0.030974478  0.005799164
  [6,]  0.516400555  0.051731728
  [7,] -0.273545829  0.082611939
  [8,]  0.028956640 -0.095674198
  [9,] -0.343703128  0.148523268
[10,] -0.267409591  0.236258325
[11,] -0.362410088  0.278893388
[12,]  0.198170683 -0.282954098
[13,] -0.195901187  0.030789448
[14,]  0.544997090 -0.089702393
[15,] -0.361799979 -0.055526763
[16,]  0.117021002  0.362033198
[17,] -0.385818070 -0.459613063
[18,] -0.182413187 -0.218456938
[19,]  0.386595491  0.337704022
[20,]  0.509929714  0.042702611

You can use the columns of the x component of your prcomp object as the 
independent variable in your regression analysis or whatever.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460

Christian Jones wrote:
> hello R_team
> 
> having perfomed a PCA on my fitted model with the function:
> 
> data<- na.omit(dataset) 
> 
> data.pca<-prcomp(data,scale =TRUE),
> 
> I??ve decided to aggregate two variables that are highly correlated.
> 
> My first question is:
> 
> How can I combine the two variables into one new predictor?
> 
> and secondly:
> 
> How can I predict with the newly created variable in a new dataset? 
> 
> Guess I need the "predict" and "new data" command but I??m having problems with the syntax and the help function was not sufficient on this issue.
> 
> many thanks in advance
> 
> Christian
> 
> 
> 
>  
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


--



From phgrosjean at sciviews.org  Wed Jan 25 17:22:22 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 25 Jan 2006 17:22:22 +0100
Subject: [R] reducing learning curves?
In-Reply-To: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
Message-ID: <43D7A5BE.5090203@sciviews.org>

Hello,
If you work under Windows, you can find a lot of useful tools in 
SciViews-R (http://www.sciviews.org/SciViews-R) and Tinn-R 
(http://www.sciviews.org/Tinn-R). For instance, you have:
- syntax coloring,
- code completion,
- calltips (tips displaying the syntax of a function as you type it),
- object explorer with lots of useful shortcuts in the object's context 
menu,
- electronic reference cards,
- viewing and reproting features,
- etc...

For a nice, graphical, debugger, look at debug package (you have to 
install it from CRAN and load it using:
 > library(debug)
Then, try:
 > ?mtrace

Best,

Philippe Grosjean

Michael wrote:
> Hi all,
> 
> I am really new to the R language. I am a long time Matlab and C++ user and
> I was "forced" to learn R because I am taking a statistics class.
> 
> I am seeking to reduce the learning curve to as smooth as possible.
> 
> Are there any addon/plug-in features that can reduce the learning curve, for
> example, the following features can be very helpful for new learners:
> 
> 1. Matlab-like command line auto-completion: Matlab has huge amount of
> command and nobody is able to remember them off the head. So a nice feature
> of Matlab command line is that I just need to type the first a few letters
> and then I press "TAB" key, there will be a list of possible commands
> popping up so I just need to select one. This helps a lot in terms of
> learning for new comers. A more advanced command auto-completion is Visual
> C++-like, which is implemented in program editor. It helps a lot while doing
> programming;
> 
> 2. A good IDE editor with embedded inline debugger: can be as good as VC++,
> but also can be as simple as Matlab's debugger, which can breakpoint and
> trace line-by-line... the editor can do syntax correction, syntax check,
> syntax highlighting, code formatting, etc.
> 
> Could you please recommend some good addon/plugins that have the above
> features?
> 
> Could you please also suggest some tips/tools/tricks that can help me reduce
> the learning curve?
> 
> Thank you very much!
> 
> Michael.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Wed Jan 25 17:24:27 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jan 2006 17:24:27 +0100
Subject: [R] Unequal sample sizes when calculating power
In-Reply-To: <6E520FA8688B6C4C91D24DF15FAEEA7B0FF4E6C3@wphnt1.wpahs.org>
References: <6E520FA8688B6C4C91D24DF15FAEEA7B0FF4E6C3@wphnt1.wpahs.org>
Message-ID: <x2lkx4cm2s.fsf@viggo.kubism.ku.dk>

Farrel Buchinsky <fbuchins at wpahs.org> writes:

> Power calculations two sample test for proportions is very useful. Is there
> a way however, to get away from the two samples being of the same size. What
> would happen if one had n=15 in the one sample and n=45 in the other sample.

Take a look at

https://www.stat.math.ethz.ch/pipermail/r-devel/2003-September/027458.html

That's for t-tests, but similar modifications might be used for
power.prop.test 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From phgrosjean at sciviews.org  Wed Jan 25 17:24:55 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 25 Jan 2006 17:24:55 +0100
Subject: [R] read.table problem
In-Reply-To: <43D731EE.9050903@siol.net>
References: <43D731EE.9050903@siol.net>
Message-ID: <43D7A657.8070807@sciviews.org>

Hello,
Well... the error message is explicit enough: "number of items read is 
not a multiple of the number of columns" means that you do not have the 
right number of items around line 11975 (not the same number as in the 
11974 previous lines)! This is an error in you file.
Best,

Philippe Grosjean

Andrej Kastrin wrote:
> Dear R useRs,
> 
> I have big (23000 rows), vertical bar delimited file:
> 
> e.g.
> A00001|Text a,Text b, Text c|345
> A00002|Text bla|456
> ...
> ..
> .
> 
> Try using
> 
> A <- read.table('filename.txt', header=FALSE,sep='\|')
>  
> process stop at line 11975 with warning message:
> number of items read is not a multiple of the number of columns
> 
> I have no problems with processing similar file, which is only 10000 
> rows long?
> 
> Any suggestion what's the problem here. Thank's in advance.
> 
> Cheers, Andrej
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From cborges at iqm.unicamp.br  Wed Jan 25 17:32:58 2006
From: cborges at iqm.unicamp.br (Cleber N. Borges)
Date: Wed, 25 Jan 2006 14:32:58 -0200
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <40e66e0b0601241225n65e53a7bkb750388a1a880f3b@mail.gmail.com>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>	<x23bjdk0zk.fsf@viggo.kubism.ku.dk>	<24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>
	<40e66e0b0601241225n65e53a7bkb750388a1a880f3b@mail.gmail.com>
Message-ID: <43D7A83A.5080407@iqm.unicamp.br>




I was quite interested in this thread (discussion),
once that I am chemistry student and I work with Mixtures Designs that are
models without intercept.

I thought quite attention the follow afirmation:

' Thus SST, the "corrected" total
sum of squares, should be used when you have a model with an intercept
term but the uncorrected total sum of squares should be used when you
do not have an intercept term. ' (Douglas Bates)


I have as reference a book called:

"Experiments with Mixtures: Designs, Models, and the Analysis of Mixture 
Data"
second edition

John A. Cornell
(Professor of Statistics in University Of Florida)


In this book, pg 42: item 2.7 - THE ANALYSIS OF VARIANCE TABLE,
I have the model below:


y(x) = 11.7x1 + 9.4x2 + 16.4x3 + 19.0x1x2 + 11.4x1x3 - 9.6x2x3


with the follow ANOVA Table:


source of variation    D.F.    SS                    MS

Regression        p-1    SSR=\sum( y_{pred} - y_{mean} )^2    ssR/(p-1)

Residual        N-p    SSE=\sum( y_{exp} - y_{pred} )^2    ssE/(N-p)   

Total            N-1    SSE=\sum( y_{exp} - y_{mean} )^2


pred = predicted
exp = experimental

and in many others books.

I always see the ANOVA Table of Mixtures systems with SST, the 
"corrected" total
sum of squares ( N-1 degrees freedom ).



I would like to ask:

1) What is approach ( point view ) more adequate ?

2) Could someone indicate some reference about this subject ?


Thanks a lot.
Regards


Cleber N. Borges





############################ Dados
x1      x2      x3      y
1    0    0    11
1    0    0    12.4
0.5    0.5    0    15
0.5    0.5    0    14.8
0.5    0.5    0    16.1
0    1    0    8.8
0    1    0    10
0    0.5    0.5    10
0    0.5    0.5    9.7
0    0.5    0.5    11.8
0    0    1    16.8
0    0    1    16
0.5    0    0.5    17.7
0.5    0    0.5    16.4
0.5    0    0.5    16.6

############################## Model

d.lm <- lm( y ~ -1 + x1*x2*x3 - x1:x2:x3, data = Dados )



### Anova like in the book
d.aov <- aov( y ~  x1*x2*x3 - x1:x2:x3, data = Dados )
#### SSR (fitted Model) = 128.296



















Douglas Bates wrote:

>On 1/24/06, Lance Westerhoff <lance at quantumbioinc.com> wrote:
>  
>
>>Hi-
>>
>>On Jan 24, 2006, at 12:08 PM, Peter Dalgaard wrote:
>>
>>    
>>
>>>Lance Westerhoff <lance at quantumbioinc.com> writes:
>>>
>>>      
>>>
>>>>Hello All-
>>>>
>>>>I found an inconsistency between the R-squared reported in Excel vs.
>>>>that in R, and I am wondering which (if any) may be correct and if
>>>>this is a known issue.  While it certainly wouldn't surprise me if
>>>>Excel is just flat out wrong, I just want to make sure since the R-
>>>>squared reported in R seems surprisingly high.  Please let me know if
>>>>this is the wrong list.  Thanks!
>>>>        
>>>>
>>>Excel is flat out wrong. As the name implies, R-squared values cannot
>>>be less than zero (adjusted R-squared can, but I wouldn't think
>>>that is what Excel does).
>>>      
>>>
>>I had thought the same thing, but then I came across the following
>>site which states: "Note that it is possible to get a negative R-
>>square for equations that do not contain a constant term. If R-square
>>is defined as the proportion of variance explained by the fit, and if
>>the fit is actually worse than just fitting a horizontal line, then R-
>>square is negative. In this case, R-square cannot be interpreted as
>>the square of a correlation." Since
>>
>>R^2 = 1 - (SSE/SST)
>>
>>I guess you can have SSE > SST which would result in a R^2 of less
>>then 1.0.  However, it still seems very strange which made me wonder
>>what is going on in Excel needless to say!
>>
>>http://www.mathworks.com/access/helpdesk/help/toolbox/curvefit/
>>ch_fitt9.html
>>    
>>
>
>This seems to be a case of using the wrong formula.  R^2 should
>measure the amount of variation for which the given model accounts
>relative to the amount of variation for which the *appropriate* null
>model does not account.  If you have a constant or intercept term in a
>linear model then the null model for comparison is one with the
>intercept only.  If you have a linear model without an intercept term
>then the appropriate null model for comparison is the model that
>predicts all the responses as zero.  Thus SST, the "corrected" total
>sum of squares, should be used when you have a model with an intercept
>term but the uncorrected total sum of squares should be used when you
>do not have an intercept term.
>
>It is disappointing to see the MathWorks propagating such an
>elementary misconception.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>.
>
>  
>



From Guillaume.Emaresi at unil.ch  Wed Jan 25 17:32:40 2006
From: Guillaume.Emaresi at unil.ch (Guillaume Emaresi)
Date: Wed, 25 Jan 2006 17:32:40 +0100
Subject: [R] (no subject)
Message-ID: <1FF083E4-6F61-48C6-A884-A956552D0090@unil.ch>

hello, i'm doing a master thesis in biology, and i'm studying genetic  
differenciation between population. So i used genetic distances,  
called FST, defining as a proportion of variance between two  
population. Then i'd like to use this pairwise FST as a dependent  
variable in glm. But i'm not sure of the error's distribution to  
use... Gaussian or binomial?
do you have an idea?
thanks a lot!
guillaume emaresi



From mschwartz at mn.rr.com  Wed Jan 25 17:38:16 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Wed, 25 Jan 2006 10:38:16 -0600
Subject: [R] Unequal sample sizes when calculating power
In-Reply-To: <6E520FA8688B6C4C91D24DF15FAEEA7B0FF4E6C3@wphnt1.wpahs.org>
References: <6E520FA8688B6C4C91D24DF15FAEEA7B0FF4E6C3@wphnt1.wpahs.org>
Message-ID: <1138207096.4288.9.camel@localhost.localdomain>

On Wed, 2006-01-25 at 09:50 -0500, Farrel Buchinsky wrote:
> Power calculations two sample test for proportions is very useful. Is there
> a way however, to get away from the two samples being of the same size. What
> would happen if one had n=15 in the one sample and n=45 in the other sample.

See ?bpower and ?bpower.sim (along with the other functions on the same
help page) in Frank Harrell's 'Hmisc' package on CRAN.

HTH,

Marc Schwartz
 
P.S. Go Steelers!  :-)



From ggrothendieck at gmail.com  Wed Jan 25 17:40:55 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jan 2006 11:40:55 -0500
Subject: [R] reducing learning curves?
In-Reply-To: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
Message-ID: <971536df0601250840i4392ea64w6d93d227d5bb478c@mail.gmail.com>

Keeping this reference card handy might reduce it somewhat:

http://www.rpad.org/Rpad/Rpad-refcard.pdf


On 1/25/06, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> I am really new to the R language. I am a long time Matlab and C++ user and
> I was "forced" to learn R because I am taking a statistics class.
>
> I am seeking to reduce the learning curve to as smooth as possible.
>
> Are there any addon/plug-in features that can reduce the learning curve, for
> example, the following features can be very helpful for new learners:
>
> 1. Matlab-like command line auto-completion: Matlab has huge amount of
> command and nobody is able to remember them off the head. So a nice feature
> of Matlab command line is that I just need to type the first a few letters
> and then I press "TAB" key, there will be a list of possible commands
> popping up so I just need to select one. This helps a lot in terms of
> learning for new comers. A more advanced command auto-completion is Visual
> C++-like, which is implemented in program editor. It helps a lot while doing
> programming;
>
> 2. A good IDE editor with embedded inline debugger: can be as good as VC++,
> but also can be as simple as Matlab's debugger, which can breakpoint and
> trace line-by-line... the editor can do syntax correction, syntax check,
> syntax highlighting, code formatting, etc.
>
> Could you please recommend some good addon/plugins that have the above
> features?
>
> Could you please also suggest some tips/tools/tricks that can help me reduce
> the learning curve?
>
> Thank you very much!
>
> Michael.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pinard at iro.umontreal.ca  Wed Jan 25 15:34:50 2006
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 25 Jan 2006 09:34:50 -0500
Subject: [R] R-help Digest, Vol 35, Issue 24
In-Reply-To: <971536df0601242036w12070f6drc87825a50c4cd03b@mail.gmail.com>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<1138140410.43d6a4fa35e94@webmail.colorado.edu>
	<971536df0601242036w12070f6drc87825a50c4cd03b@mail.gmail.com>
Message-ID: <20060125143450.GA6799@phenix.sram.qc.ca>

[Gabor Grothendieck]

>[...] this list is inhabited by some rather rude participants but
>everyone puts up with them in the hope that they do have some useful
>remarks.

I've been witnessing this list for about one year, and also read *lots* 
of archived messages.  While it is true that a few members do not use 
white gloves, are rather fond on concise replies, and do express strong 
opinions at times, they never went overboard insulting people and always 
kept a reasonable measure, at least so far that I could see (yet who 
knows, outliers might happen! :-).

(*) Our whole society is a bit shy and shivers easily when opinions are 
expressed nowadays, I often observed than people quickly get insecure,
feel attacked, and overreact (by running away or starting a fight).

>there is even a group of thought that feels it is a justifiable way to
>keep the list volume under control.

This may work because of the starred paragraph above, that is, for wrong 
reasons.  Best is, and this often occurs on the R list, when everything 
(facts, opinions) is being shared efficiently, without useless arguing.  
Then, threads quickly fade out.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From juansan at dca.upv.es  Wed Jan 25 18:02:19 2006
From: juansan at dca.upv.es (=?iso-8859-1?Q?Juan_Pablo_S=E1nchez?=)
Date: Wed, 25 Jan 2006 18:02:19 +0100
Subject: [R] About lmer output
Message-ID: <000701c621d1$1a34d7b0$13662a9e@portatilJP>

Dear R users:
I am using lmer fo fit binomial data with a probit link function:

> fer_lmer_PQL<-lmer(fer ~ gae + ctipo + (1|perm) -1,  
+                family = binomial(link="probit"), 
+                method = 'PQL',
+                data = FERTILIDAD,
+                msVerbose= True)

The output look like this:
> fer_lmer_PQL
Generalized linear mixed model fit using PQL 
Formula: fer ~ gae + ctipo + (1 | perm) - 1 
   Data: FERTILIDAD 
 Family: binomial(probit link)
      AIC      BIC    logLik deviance
 2728.086 2918.104 -1332.043 2664.086
Random effects:
     Groups        Name    Variance    Std.Dev. 
       perm (Intercept)     0.28256     0.53156 
# of obs: 2802, groups: perm, 529

Estimated scale (compare to 1)  0.8958656 

My question is about the meaning of  "Estimated scale (compare to 1)  0.8958656 "

I think that the scale would be 0.28256+1.0, Isn??t it?

Thanks in advance,
Juan Pablo S??nchez



From voodooochild at gmx.de  Wed Jan 25 18:04:05 2006
From: voodooochild at gmx.de (voodooochild@gmx.de)
Date: Wed, 25 Jan 2006 18:04:05 +0100
Subject: [R] Log-Likelihood 3d-plot and contourplot / optim() starting values
Message-ID: <43D7AF85.3040807@gmx.de>

Hello,

i have coded the following loglikelihood-function

# Log-Likelihood-Funktion
loglik_jm<-function(N,phi,t) {
  n<-length(t)
  i<-seq(along=t)
  s1<-sum(log(N-(i-1)))
  s2<-phi*sum((N-(i-1))*t[i])
  n*log(phi)+s1-s2
}

# the data
t<-c(7,11,8,10,15,22,20,25,28,35)

# now i want to do a 3d-plot and a contourplot in order to see at which 
values of N and phi the loglikelihood function becomes zero.
# i do this in order to get an idea where the starting values for a 
optimization for the mle of N could be

# 3dplot and contourplot
phi<-seq(0,1,length=50)
N<-seq(101,110,length=50)
z<-outer(N,phi,loglik_jm(N,phi,t))
persp(phi,N,z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
contourplot(z~N*phi)

# but i get some error messages, i don't know why?


# if you are interested, the mle function for N is

ll2 <- function(N,t) {
  i<-seq(along=t)
  n<-length(t)
  s1<-sum(1/(N-(i-1)))
  s2<-n/(N-(1/sum(t[i]))*(sum((i-1)*t[i])))
  (s1-s2)
}

# you get this function as usual, if you set the loglikelihood equal 
zero and differentiate for N
# i take the squares of ll2 in order to get the minimum easier

ll3<-function(N,t) {
  ll2(N,t)^2
}

# then i do an iteration to get the mle of N

xmin<-optim(10,ll3,method="BFGS",control=list(reltol=.Machine$double.eps),t=t)

# the problem is that this method only works well, if the starting 
values for optim() are very close to the real value!
# so i got the idea with contourplot() and persp() to see where good 
starting values could be.

i would be very thankful if anyone could give me some advice!


regards
Andreas



From kjetilbrinchmannhalvorsen at gmail.com  Wed Jan 25 18:12:58 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 25 Jan 2006 17:12:58 -0000
Subject: [R] R vs. Excel (R-squared)
In-Reply-To: <43D7A83A.5080407@iqm.unicamp.br>
References: <8582410F-B9C3-482F-82D0-EBC71FA7A4FD@quantumbioinc.com>	<x23bjdk0zk.fsf@viggo.kubism.ku.dk>	<24D498FC-C92F-4598-B5B0-51929F1B076A@quantumbioinc.com>	<40e66e0b0601241225n65e53a7bkb750388a1a880f3b@mail.gmail.com>
	<43D7A83A.5080407@iqm.unicamp.br>
Message-ID: <4400902E.7020006@gmail.com>

Cleber N. Borges wrote:
> 
> 
> I was quite interested in this thread (discussion),
> once that I am chemistry student and I work with Mixtures Designs that are
> models without intercept.
> 
> I thought quite attention the follow afirmation:
> 
> ' Thus SST, the "corrected" total
> sum of squares, should be used when you have a model with an intercept
> term but the uncorrected total sum of squares should be used when you
> do not have an intercept term. ' (Douglas Bates)
> 
> 
> I have as reference a book called:
> 
> "Experiments with Mixtures: Designs, Models, and the Analysis of Mixture 
> Data"
> second edition
> 
> John A. Cornell
> (Professor of Statistics in University Of Florida)
> 
> 
> In this book, pg 42: item 2.7 - THE ANALYSIS OF VARIANCE TABLE,
> I have the model below:
> 
> 
> y(x) = 11.7x1 + 9.4x2 + 16.4x3 + 19.0x1x2 + 11.4x1x3 - 9.6x2x3
> 
> 
> with the follow ANOVA Table:
> 
> 
> source of variation    D.F.    SS                    MS
> 
> Regression        p-1    SSR=\sum( y_{pred} - y_{mean} )^2    ssR/(p-1)
> 
> Residual        N-p    SSE=\sum( y_{exp} - y_{pred} )^2    ssE/(N-p)   
> 
> Total            N-1    SSE=\sum( y_{exp} - y_{mean} )^2
> 
> 
> pred = predicted
> exp = experimental
> 
> and in many others books.
> 
> I always see the ANOVA Table of Mixtures systems with SST, the 
> "corrected" total
> sum of squares ( N-1 degrees freedom ).
> 
> 
> 
> I would like to ask:
> 
> 1) What is approach ( point view ) more adequate ?

With a mixture model, although you do not have a intercept term
directly in the model, it is there, occulted,as the sum of the design
variables representing the mixture is 1! So it is correct to use the
"corrected" sum of squares.

Kjetil

> 
> 2) Could someone indicate some reference about this subject ?
> 
> 
> Thanks a lot.
> Regards
> 
> 
> Cleber N. Borges
> 
> 
> 
> 
> 
> ############################ Dados
> x1      x2      x3      y
> 1    0    0    11
> 1    0    0    12.4
> 0.5    0.5    0    15
> 0.5    0.5    0    14.8
> 0.5    0.5    0    16.1
> 0    1    0    8.8
> 0    1    0    10
> 0    0.5    0.5    10
> 0    0.5    0.5    9.7
> 0    0.5    0.5    11.8
> 0    0    1    16.8
> 0    0    1    16
> 0.5    0    0.5    17.7
> 0.5    0    0.5    16.4
> 0.5    0    0.5    16.6
> 
> ############################## Model
> 
> d.lm <- lm( y ~ -1 + x1*x2*x3 - x1:x2:x3, data = Dados )
> 
> 
> 
> ### Anova like in the book
> d.aov <- aov( y ~  x1*x2*x3 - x1:x2:x3, data = Dados )
> #### SSR (fitted Model) = 128.296
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Douglas Bates wrote:
> 
>> On 1/24/06, Lance Westerhoff <lance at quantumbioinc.com> wrote:
>>  
>>
>>> Hi-
>>>
>>> On Jan 24, 2006, at 12:08 PM, Peter Dalgaard wrote:
>>>
>>>    
>>>
>>>> Lance Westerhoff <lance at quantumbioinc.com> writes:
>>>>
>>>>      
>>>>
>>>>> Hello All-
>>>>>
>>>>> I found an inconsistency between the R-squared reported in Excel vs.
>>>>> that in R, and I am wondering which (if any) may be correct and if
>>>>> this is a known issue.  While it certainly wouldn't surprise me if
>>>>> Excel is just flat out wrong, I just want to make sure since the R-
>>>>> squared reported in R seems surprisingly high.  Please let me know if
>>>>> this is the wrong list.  Thanks!
>>>>>        
>>>>>
>>>> Excel is flat out wrong. As the name implies, R-squared values cannot
>>>> be less than zero (adjusted R-squared can, but I wouldn't think
>>>> that is what Excel does).
>>>>      
>>>>
>>> I had thought the same thing, but then I came across the following
>>> site which states: "Note that it is possible to get a negative R-
>>> square for equations that do not contain a constant term. If R-square
>>> is defined as the proportion of variance explained by the fit, and if
>>> the fit is actually worse than just fitting a horizontal line, then R-
>>> square is negative. In this case, R-square cannot be interpreted as
>>> the square of a correlation." Since
>>>
>>> R^2 = 1 - (SSE/SST)
>>>
>>> I guess you can have SSE > SST which would result in a R^2 of less
>>> then 1.0.  However, it still seems very strange which made me wonder
>>> what is going on in Excel needless to say!
>>>
>>> http://www.mathworks.com/access/helpdesk/help/toolbox/curvefit/
>>> ch_fitt9.html
>>>    
>>>
>> This seems to be a case of using the wrong formula.  R^2 should
>> measure the amount of variation for which the given model accounts
>> relative to the amount of variation for which the *appropriate* null
>> model does not account.  If you have a constant or intercept term in a
>> linear model then the null model for comparison is one with the
>> intercept only.  If you have a linear model without an intercept term
>> then the appropriate null model for comparison is the model that
>> predicts all the responses as zero.  Thus SST, the "corrected" total
>> sum of squares, should be used when you have a model with an intercept
>> term but the uncorrected total sum of squares should be used when you
>> do not have an intercept term.
>>
>> It is disappointing to see the MathWorks propagating such an
>> elementary misconception.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>> .
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Jan 25 18:36:11 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 25 Jan 2006 12:36:11 -0500
Subject: [R] is.integer() function
In-Reply-To: <43D79FE0.5080706@lancaster.ac.uk>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>	<20060125152330.GD20850@rmki.kfki.hu>
	<43D79FE0.5080706@lancaster.ac.uk>
Message-ID: <43D7B70B.9080201@stats.uwo.ca>

On 1/25/2006 10:57 AM, Barry Rowlingson wrote:
> Gabor Csardi wrote:
>> Becaues is.integer shows the internal representation, which is not an
>> integer but a double (real number). Some functions create integer vectors,
> 
>   Some functions that you might think create integer vectors and even 
> seem to say they create integer vectors dont create integer vectors:
> 
>       'ceiling' takes a single numeric argument 'x' and returns a
>       numeric vector containing the smallest integers not less than the
>       corresponding elements of 'x'.
> 
>  > ceiling(0.5)
> [1] 1
> 
>  > is.integer(ceiling(0.5))
> [1] FALSE
> 
>  > is.integer(1:3)
> [1] TRUE
>  > is.integer(ceiling(1:3))
> [1] FALSE
> 
> 
>   This could possibly be a documentation problem, since ?ceiling is 
> using 'integer' in the sense of 'whole number', whereas ?is.integer is 
> concerned with internal representation (aka 'storage mode')....

Here "numeric vector" is being used in the R-specific technical sense as 
a vector of double precision values, so the documentor was trying hard 
to be precise.  The problem is that English also admits the 
interpretation in a non-technical sense as a vector of numbers.  I 
believe your country is to blame for the language. :-)

Duncan Murdoch

> 
>   This seems to be an endless source of confusion to anyone who didn't 
> start their programming days in Fortran, C, or assembly language (or 
> other strongly-typed language, I guess).
> 
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From br44114 at gmail.com  Wed Jan 25 18:47:35 2006
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 25 Jan 2006 12:47:35 -0500
Subject: [R] read.table problem
Message-ID: <8d5a36350601250947m88dde1chb10328d253bcd092@mail.gmail.com>

By the way, you might find this sed one-liner useful:
   sed -n '11981q;11970,11980p' filename.txt
It will print the offending line and its neighbors. If you're on
Windows you need to install Windows Services For Unix or Cygwin.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrej Kastrin
> Sent: Wednesday, January 25, 2006 3:08 AM
> To: r-help
> Subject: [R] read.table problem
>
> Dear R useRs,
>
> I have big (23000 rows), vertical bar delimited file:
>
> e.g.
> A00001|Text a,Text b, Text c|345
> A00002|Text bla|456
> ...
> ..
> .
>
> Try using
>
> A <- read.table('filename.txt', header=FALSE,sep='\|')
>
> process stop at line 11975 with warning message:
> number of items read is not a multiple of the number of columns
>
> I have no problems with processing similar file, which is only 10000
> rows long?
>
> Any suggestion what's the problem here. Thank's in advance.
>
> Cheers, Andrej
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Wed Jan 25 19:10:27 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 25 Jan 2006 18:10:27 +0000
Subject: [R] is.integer() function
In-Reply-To: <43D7B70B.9080201@stats.uwo.ca>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>	<20060125152330.GD20850@rmki.kfki.hu>
	<43D79FE0.5080706@lancaster.ac.uk> <43D7B70B.9080201@stats.uwo.ca>
Message-ID: <43D7BF13.3000201@lancaster.ac.uk>

Duncan Murdoch wrote:

> Here "numeric vector" is being used in the R-specific technical sense as 
> a vector of double precision values, so the documentor was trying hard 
> to be precise.  The problem is that English also admits the 
> interpretation in a non-technical sense as a vector of numbers.  I 
> believe your country is to blame for the language. :-)


  But can "numeric vector" in the "R-specific technical sense" also mean 
a vector of integer (representation) values? It passes is.numeric() and 
is.vector():

 > x=1:3
 > is.numeric(x)
[1] TRUE
 > is.vector(x)
[1] TRUE
 > is.integer(x)
[1] TRUE


  Unless by the "R-specific technical sense of 'numeric vector'" you 
dont mean something for which is.numeric() and is.vector() are both 
true. Which is perverse. But then large chunks of R are. Anyway, is this 
right:

is.RSpecificTechnicalSenseNumericVector=function(v){is.numeric(v) & 
is.double(v)}

Barry



From ggrothendieck at gmail.com  Wed Jan 25 19:29:35 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jan 2006 13:29:35 -0500
Subject: [R] is.integer() function
In-Reply-To: <43D7BF13.3000201@lancaster.ac.uk>
References: <BAY110-F4BD890C52DC7F38EB971DC7120@phx.gbl>
	<20060125152330.GD20850@rmki.kfki.hu>
	<43D79FE0.5080706@lancaster.ac.uk> <43D7B70B.9080201@stats.uwo.ca>
	<43D7BF13.3000201@lancaster.ac.uk>
Message-ID: <971536df0601251029g183cf147xd6c3bd4046c56377@mail.gmail.com>

My understanding is that a variable of _mode_ "numeric" can be
of _type_ "double" or _type_ "integer".    Just knowing that a variable
is numeric does not tell you which of these two subcategories it
falls in.

x <- 1:3
mode(x) # "numeric"
typeof(x) # "integer"
y <- 1.2
mode(y) # "numeric"
typeof(y) # "double"


On 1/25/06, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> Duncan Murdoch wrote:
>
> > Here "numeric vector" is being used in the R-specific technical sense as
> > a vector of double precision values, so the documentor was trying hard
> > to be precise.  The problem is that English also admits the
> > interpretation in a non-technical sense as a vector of numbers.  I
> > believe your country is to blame for the language. :-)
>
>
>  But can "numeric vector" in the "R-specific technical sense" also mean
> a vector of integer (representation) values? It passes is.numeric() and
> is.vector():
>
>  > x=1:3
>  > is.numeric(x)
> [1] TRUE
>  > is.vector(x)
> [1] TRUE
>  > is.integer(x)
> [1] TRUE
>
>
>  Unless by the "R-specific technical sense of 'numeric vector'" you
> dont mean something for which is.numeric() and is.vector() are both
> true. Which is perverse. But then large chunks of R are. Anyway, is this
> right:
>
> is.RSpecificTechnicalSenseNumericVector=function(v){is.numeric(v) &
> is.double(v)}
>
> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ana.quiterio at ine.pt  Wed Jan 25 18:32:35 2006
From: ana.quiterio at ine.pt (=?iso-8859-1?Q?Ana_Quit=E9rio?=)
Date: Wed, 25 Jan 2006 17:32:35 -0000
Subject: [R]  Question about fitting power
Message-ID: <E97312684A84D511BDD40002A50968D6070ACAAD@lxpobw01.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/61a1d1ea/attachment.pl

From andy_liaw at merck.com  Wed Jan 25 19:38:49 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Jan 2006 13:38:49 -0500
Subject: [R] Question about fitting power
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED754@usctmx1106.merck.com>

The two methods are fitting different models.  With lm(), the model is

  y = a * x^b * error

or, equivalently,

  ln(y) = ln(a) + b * ln(x) + ln(error)

With nls(), the model is

  y = a * x^b + error

Thus you will get two different estimates.

Andy

From: Ana Quit??rio
> 
> Hi R users
> 
>  
> 
> I'm trying to fit a model y=ax^b.
> 
> I know if I made ln(y)=ln(a)+bln(x)  this is a linear regression.
> 
>  
> 
> But I obtain differente results with nls() and lm()
> 
>  
> 
> My commands are:   nls(CV ~a*Est^b, data=limiares, start 
> =list(a=100,b=0),
> trace = TRUE)  for nonlinear regression
> 
>                         and :  lm(ln_CV~ln_Est, 
> data=limiares) for linear
> regression
> 
>  
> 
>  
> 
> Nonlinear regression model:   a=738.2238151  and    b=-0.3951013 
> 
>  
> 
> Linear regression:   Coefficients:
> 
>                                 Estimate    Std. Error    t value
> Pr(>|t|)    
> 
> (Intercept)  7.8570224  0.0103680   757.8   <2e-16 ***
> 
> ln_Est      -0.5279412  0.0008658  -609.8   <2e-16 ***
> 
>  
> 
>  
> 
> I think it should be   a=exp("(Intercept)  ") = 
> exp(7.8570224) = 2583.815
> and b=ln_Est
> 
>  
> 
> Probably I'm wrong, but why??
> 
>  
> 
>  
> 
> Thanks in advance.
> 
>  
> 
> Ana Quiterio
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ruser2006 at yahoo.com  Wed Jan 25 19:57:44 2006
From: ruser2006 at yahoo.com (r user)
Date: Wed, 25 Jan 2006 10:57:44 -0800 (PST)
Subject: [R] paste - eliminate spaces?
Message-ID: <20060125185744.46496.qmail@web37011.mail.mud.yahoo.com>

I am trying to combine the value of a variable and
text.

e.g.
I want ?test1?, with no spaces.

I try:

h=1
paste(?test?,1)

But get:
[1] "test 1"

(i.e. there is a space between ?test?? and ?1?)

Is there a way to eliminate the space?



From ruser2006 at yahoo.com  Wed Jan 25 20:00:17 2006
From: ruser2006 at yahoo.com (r user)
Date: Wed, 25 Jan 2006 11:00:17 -0800 (PST)
Subject: [R] paste - eliminate spaces?
Message-ID: <20060125190018.50349.qmail@web37012.mail.mud.yahoo.com>

I found the answer:

add sep="" to the paste command

paste('test',1,sep="")



--- r user <ruser2006 at yahoo.com> wrote:

> I am trying to combine the value of a variable and
> text.
> 
> e.g.
> I want ?test1?, with no spaces.
> 
> I try:
> 
> h=1
> paste(?test?,1)
> 
> But get:
> [1] "test 1"
> 
> (i.e. there is a space between ?test?? and ?1?)
> 
> Is there a way to eliminate the space?
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>



From HDoran at air.org  Wed Jan 25 20:02:00 2006
From: HDoran at air.org (Doran, Harold)
Date: Wed, 25 Jan 2006 14:02:00 -0500
Subject: [R] paste - eliminate spaces?
Message-ID: <F5ED48890E2ACB468D0F3A64989D335A017D3DE1@dc1ex3.air.org>

Try 

paste('test',1,sep='') 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of r user
Sent: Wednesday, January 25, 2006 1:58 PM
To: rhelp
Subject: [R] paste - eliminate spaces?

I am trying to combine the value of a variable and text.

e.g.
I want "test1", with no spaces.

I try:

h=1
paste('test',1)

But get:
[1] "test 1"

(i.e. there is a space between "test'" and "1")

Is there a way to eliminate the space?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jan 25 20:01:33 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Jan 2006 14:01:33 -0500
Subject: [R] paste - eliminate spaces?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED757@usctmx1106.merck.com>

I don't think the help page for paste is all that hard to read or
understand, is it?  Please read about the `sep' option (and note its
default).

Andy

From: r user
> 
> I am trying to combine the value of a variable and
> text.
> 
> e.g.
> I want "test1", with no spaces.
> 
> I try:
> 
> h=1
> paste('test',1)
> 
> But get:
> [1] "test 1"
> 
> (i.e. there is a space between "test'" and "1")
> 
> Is there a way to eliminate the space?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From phgrosjean at sciviews.org  Wed Jan 25 20:02:47 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 25 Jan 2006 20:02:47 +0100
Subject: [R] paste - eliminate spaces?
In-Reply-To: <20060125185744.46496.qmail@web37011.mail.mud.yahoo.com>
References: <20060125185744.46496.qmail@web37011.mail.mud.yahoo.com>
Message-ID: <43D7CB57.3040301@sciviews.org>

Just read more carefully the online help for paste (?paste).
You have:

sep: a character string to separate the terms.

and the default value for sep is " " (a space).
So, just use:

 > paste("test", 1, sep = "")

Best,

Philippe Grosjean

r user wrote:
> I am trying to combine the value of a variable and
> text.
> 
> e.g.
> I want ?test1?, with no spaces.
> 
> I try:
> 
> h=1
> paste(?test?,1)
> 
> But get:
> [1] "test 1"
> 
> (i.e. there is a space between ?test?? and ?1?)
> 
> Is there a way to eliminate the space?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From dmbates at gmail.com  Wed Jan 25 20:04:49 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 25 Jan 2006 13:04:49 -0600
Subject: [R] About lmer output
In-Reply-To: <000701c621d1$1a34d7b0$13662a9e@portatilJP>
References: <000701c621d1$1a34d7b0$13662a9e@portatilJP>
Message-ID: <40e66e0b0601251104rc8b67e6wb7e799d1bbb5377f@mail.gmail.com>

On 1/25/06, Juan Pablo S??nchez <juansan at dca.upv.es> wrote:
> Dear R users:
> I am using lmer fo fit binomial data with a probit link function:
>
> > fer_lmer_PQL<-lmer(fer ~ gae + ctipo + (1|perm) -1,
> +                family = binomial(link="probit"),
> +                method = 'PQL',
> +                data = FERTILIDAD,
> +                msVerbose= True)
>
> The output look like this:
> > fer_lmer_PQL
> Generalized linear mixed model fit using PQL
> Formula: fer ~ gae + ctipo + (1 | perm) - 1
>    Data: FERTILIDAD
>  Family: binomial(probit link)
>       AIC      BIC    logLik deviance
>  2728.086 2918.104 -1332.043 2664.086
> Random effects:
>      Groups        Name    Variance    Std.Dev.
>        perm (Intercept)     0.28256     0.53156
> # of obs: 2802, groups: perm, 529
>
> Estimated scale (compare to 1)  0.8958656
>
> My question is about the meaning of  "Estimated scale (compare to 1)  0.8958656 "
>
> I think that the scale would be 0.28256+1.0, Isn??t it?

The estimated scale is what would be the estimate of the scale
parameter in the GLM family if there was a scale parameter.  For the
binomial and Poisson families there is no scale parameter but the
Iteratively Reweighted Least Squares (IRLS) algorithm still produces
an estimate of one.  If the data are neither overdispersed nor
underdispersed then that estimate should be close to 1.

It can provide a diagnostic for the model.  A value that is
substantially different from 1 indicates model failure or
over-modeling the data.  I would say that the value of 0.896 is close
to an indication of underdispersion.  Frequently this is caused be
including random effects associated with groups that have few
observations in each group.



From RRoa at fisheries.gov.fk  Wed Jan 25 19:05:52 2006
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 25 Jan 2006 16:05:52 -0200
Subject: [R] Question about fitting power
Message-ID: <03DCBBA079F2324786E8715BE538968A3DC70A@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ana Quit??rio
> Sent:	Wednesday, January 25, 2006 3:33 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R]  Question about fitting power
> 
>From: Ana Quit??rio
> > 
> > Hi R users
> > 
> > I'm trying to fit a model y=ax^b.
> > 
> > I know if I made ln(y)=ln(a)+bln(x)  this is a linear regression.
> > 
> > But I obtain differente results with nls() and lm()
> > 
> > My commands are:
> nls(CV ~a*Est^b, data=limiares, start =list(a=100,b=0), trace = TRUE)  
> for nonlinear regression, and :  
> lm(ln_CV~ln_Est, data=limiares) 
> for linear regression
> > 
> > Nonlinear regression model:   a=738.2238151  and    b=-0.3951013 
> > Linear regression:   Coefficients:
> >                   Estimate    Std. Error    t value Pr(>|t|)    
> > (Intercept)  7.8570224  0.0103680   757.8   <2e-16 ***
> ln_Est      -0.5279412  0.0008658  -609.8   <2e-16 ***
> > 
> > I think it should be   a=exp("(Intercept)  ") = 
> > exp(7.8570224) = 2583.815 and b=ln_Est
> > 
> > Probably I'm wrong, but why??
> > 
> > Thanks in advance.
> 
> Ana Quiterio

In addition to what Andy said, maybe your nls() model is not natural because a power 
statistical model should produce multiplicative outcomes. An alternative is to try the 
nonlinear power model with multiplicative deviates assuming lognormality (which is
what the linear model does but without the need for back-transformation), with nlm(), 
for instance:

fn<-function(p){
+  CV_mod=p[1]*Est^p[2];
+  squdiff=(log(limiares$CV)-log(CV_mod))^2;
+  lik=(length(limiares$CV)/2)*log(sum(squdiff)/length(limiares$CV))
+  }
CV.lik<-nlm(fn,p=c(100,0),hessian=TRUE)
fec.lik
covmat<-solve(CV.lik$hessian)
covmat

Ruben



From singyee.ling at googlemail.com  Wed Jan 25 20:15:10 2006
From: singyee.ling at googlemail.com (singyee ling)
Date: Wed, 25 Jan 2006 19:15:10 +0000
Subject: [R] cox.zph
Message-ID: <ca33a9890601251115h64584b57q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/33457963/attachment.pl

From deepayan.sarkar at gmail.com  Wed Jan 25 20:30:22 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 25 Jan 2006 13:30:22 -0600
Subject: [R] panel function with barchart (lattice)
In-Reply-To: <20060125073523.11613.qmail@web50908.mail.yahoo.com>
References: <20060125073523.11613.qmail@web50908.mail.yahoo.com>
Message-ID: <eb555e660601251130y1cf8bd49h366a75c26baf0af1@mail.gmail.com>

On 1/25/06, Drew <drewbrewit at yahoo.com> wrote:
> Folks at R help,
>
> I can't quite get the panel function to work the way I
> want within barchart.
> I guess I'm still not understanding how to piece
> together multiple panel
> arguments, especially when "groups" is specified.
>
> Example: I want to be able to add the value of "yield"
> to each section of
> each bar in this graph:
>
> barchart(yield ~ variety | site, data = barley,
> 	groups = year,
> 	layout = c(1,6),
> 	stack=TRUE,
> 	ylab = "Barley Yield (bushels/acre)"
> )
>
> To do this, I add my panel function:
>
> barchart(yield ~ variety | site, data = barley,
> 	groups = year,
> 	layout = c(1,6),
> 	stack=TRUE,
> 	ylab = "Barley Yield (bushels/acre)",
>
> 	panel = function(x,y,subscripts,groups,...){
> 		panel.barchart(x,y,...)

Well, panel.barchart needs the subscripts and groups arguments to draw
stacked bar charts, and you are calling it without them.

> 		ltext(x = x, y = y, label =
> round(barley$yield[subscripts],1), cex=.8)

The y values will need to be accumulated. Have you looked at what
panel.barchart does?

> 	}
> )
>
> Then I get the values to print on each bar (which is
> what I want) but the
> bars no longer stack to appropriate height, and I
> cannot get the subsections
> of each bar to be a different color. I've tried
> numerous variations of
> panel.barchart, panel.superpose, etc. using examples
> from ?xyplot, but
> nothing quite works or I get an error message.
>
> Any help would be appreciated.
>
> ~Nick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


--
http://www.stat.wisc.edu/~deepayan/



From tlumley at u.washington.edu  Wed Jan 25 20:59:10 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jan 2006 11:59:10 -0800 (PST)
Subject: [R] cox.zph
In-Reply-To: <ca33a9890601251115h64584b57q@mail.gmail.com>
References: <ca33a9890601251115h64584b57q@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0601251157440.8017@homer23.u.washington.edu>

On Wed, 25 Jan 2006, singyee ling wrote:

> Dear R-users,
>
> I am sorry if this is obvious. I am testing the proportional hazard
> assumptions using cox.zph. If i am not wrong, a g(t) function must be
> assumed. Four possibilities available in R  are "km","identity" and "rank".
> may i know what functions of time are these transformation assuming?

It should be fairly clear what "identity", "log" and "rank" are:
   g(t)=t
   g(t)=log(t)
and the rank of the observations time, respectively.

"km" uses the Kaplan-Meier estimator of survival at time t.

 	-thomas



From Greg.Snow at intermountainmail.org  Wed Jan 25 21:10:30 2006
From: Greg.Snow at intermountainmail.org (Gregory Snow)
Date: Wed, 25 Jan 2006 13:10:30 -0700
Subject: [R] reducing learning curves?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB129FEE@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/7b8509f5/attachment.pl

From yang.x.qiu at gsk.com  Wed Jan 25 21:37:15 2006
From: yang.x.qiu at gsk.com (yang.x.qiu@gsk.com)
Date: Wed, 25 Jan 2006 15:37:15 -0500
Subject: [R] how to test robustness of correlation
Message-ID: <OF92FDDC6F.B543C988-ON85257101.006F5925-85257101.0071464E@gsk.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/a8ed2f7f/attachment.pl

From ksteiger at illumigen.com  Wed Jan 25 21:49:05 2006
From: ksteiger at illumigen.com (Kathryn V. Steiger)
Date: Wed, 25 Jan 2006 12:49:05 -0800
Subject: [R] a nice tool
Message-ID: <9ADD7726630D5443B3E182C2B9698421E0D4@cure.illumigen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060125/e29db561/attachment.pl

From gunter.berton at gene.com  Wed Jan 25 21:57:47 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 Jan 2006 12:57:47 -0800
Subject: [R] how to test robustness of correlation
In-Reply-To: <OF92FDDC6F.B543C988-ON85257101.006F5925-85257101.0071464E@gsk.com>
Message-ID: <200601252057.k0PKvlww013172@hertz.gene.com>

check out cov.rob() in MASS (among others, I'm sure). The procedure is far
more sophisticated than "outlier removal" or resampling (??). References are
given in the docs.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> yang.x.qiu at gsk.com
> Sent: Wednesday, January 25, 2006 12:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to test robustness of correlation
> 
> Hi, there:
> 
> As you all know, correlation is not a very robust procedure.  
> Sometimes 
> correlation could be driven by a few outliers. There are a 
> few ways to 
> improve the robustness of correlation (pearson correlation), 
> either by 
> outlier removal procedure, or resampling technique. 
> 
> I am wondering if there is any R package or R code that have 
> incorporated 
> outlier removal or resampling procedure in calculating correlation 
> coefficient. 
> 
> Your help is greatly appreciated. 
> 
> Thanks.
> Yang
> 
> Yang Qiu
> Integrated Data Analysis
> Cheminformatics at RTP
> GlaxoSmithKline
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From cberry at tajo.ucsd.edu  Wed Jan 25 23:15:52 2006
From: cberry at tajo.ucsd.edu (Chuck Berry)
Date: Wed, 25 Jan 2006 22:15:52 +0000 (UTC)
Subject: [R] reducing learning curves?
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
Message-ID: <loom.20060125T230900-945@post.gmane.org>

Michael <comtech.usa <at> gmail.com> writes:

> 
> Hi all,
> 
> I am really new to the R language. I am a long time Matlab and C++ user and
> I was "forced" to learn R because I am taking a statistics class.
> 
> I am seeking to reduce the learning curve to as smooth as possible.
> 

This cheatsheet might be helpful for a Matlab user:

  http://cran.us.r-project.org/doc/contrib/R-and-octave.txt

and 

  help.search("keyword")

and

  RSiteSearch("terms of intererst")

are very useful in finding tips.

[ rest deleted ]



From spencer.graves at pdf.com  Wed Jan 25 23:42:52 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jan 2006 14:42:52 -0800
Subject: [R] D(dnorm...)?
Message-ID: <43D7FEEC.30206@pdf.com>

	  Can someone help me understand the following:

 > D(expression(dnorm(x, mean)), "mean")
[1] 0
 > sessionInfo()

R version 2.2.1, 2005-12-20, i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

	  By my computations, this should be something like 
((mean-x)/sd^2)*dnorm(...).

	  Thanks for your help.
	  Spencer Graves



From gunter.berton at gene.com  Wed Jan 25 23:57:33 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 Jan 2006 14:57:33 -0800
Subject: [R] D(dnorm...)?
In-Reply-To: <43D7FEEC.30206@pdf.com>
Message-ID: <200601252257.k0PMvX6D006058@compton.gene.com>

dnorm() is an internal function, so I don't see how D (or deriv) can do
anything with it symbolically. Am I missing something?

-- Bert
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, January 25, 2006 2:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] D(dnorm...)?
> 
> 	  Can someone help me understand the following:
> 
>  > D(expression(dnorm(x, mean)), "mean")
> [1] 0
>  > sessionInfo()
> 
> R version 2.2.1, 2005-12-20, i386-pc-mingw32
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"   
>   "datasets"
> [7] "base"
> 
> 	  By my computations, this should be something like 
> ((mean-x)/sd^2)*dnorm(...).
> 
> 	  Thanks for your help.
> 	  Spencer Graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Thu Jan 26 00:05:24 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jan 2006 15:05:24 -0800
Subject: [R] D(dnorm...)?
In-Reply-To: <200601252257.k0PMvX6D006058@compton.gene.com>
References: <200601252257.k0PMvX6D006058@compton.gene.com>
Message-ID: <43D80434.2070709@pdf.com>

Hi, Bert:

	  I think I was too terse:  Why didn't I get an error message?  When I 
tried the same thing with "dpois", I got an error message:

 > D(expression(dpois(x, prob)), "mean")
Error in D(expression(dpois(x, prob)), "mean") :
	Function 'dpois' is not in the derivatives table

	  With "dnorm", I got "0"!.  Whe didn't I get an error message?

	  Thanks for the reply.
	  Spencer Graves

Berton Gunter wrote:

> dnorm() is an internal function, so I don't see how D (or deriv) can do
> anything with it symbolically. Am I missing something?
> 
> -- Bert
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Wednesday, January 25, 2006 2:43 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] D(dnorm...)?
>>
>>	  Can someone help me understand the following:
>>
>> > D(expression(dnorm(x, mean)), "mean")
>>[1] 0
>> > sessionInfo()
>>
>>R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>
>>attached base packages:
>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"   
>>  "datasets"
>>[7] "base"
>>
>>	  By my computations, this should be something like 
>>((mean-x)/sd^2)*dnorm(...).
>>
>>	  Thanks for your help.
>>	  Spencer Graves
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
>



From ripley at stats.ox.ac.uk  Thu Jan 26 00:25:27 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jan 2006 23:25:27 +0000 (GMT)
Subject: [R] D(dnorm...)?
In-Reply-To: <43D80434.2070709@pdf.com>
References: <200601252257.k0PMvX6D006058@compton.gene.com>
	<43D80434.2070709@pdf.com>
Message-ID: <Pine.LNX.4.61.0601252318050.30126@gannet.stats>

On Wed, 25 Jan 2006, Spencer Graves wrote:

> Hi, Bert:
>
> 	  I think I was too terse:  Why didn't I get an error message?  When I
> tried the same thing with "dpois", I got an error message:
>
> > D(expression(dpois(x, prob)), "mean")
> Error in D(expression(dpois(x, prob)), "mean") :
> 	Function 'dpois' is not in the derivatives table
>
> 	  With "dnorm", I got "0"!.  Whe didn't I get an error message?

Because dnorm _is_ in the derivatives table, as a function of x.  (So is 
pnorm.)   These are functions we write as \phi and \Phi in Statistics 101.

This is not particularly well documented in R, but it is in MASS4 pp. 
437-8.

It would be worth enhancing the help page with the list of known 
functions.  Long ago we talked about making it extensible, but nothing 
happened.


>
> 	  Thanks for the reply.
> 	  Spencer Graves
>
> Berton Gunter wrote:
>
>> dnorm() is an internal function, so I don't see how D (or deriv) can do
>> anything with it symbolically. Am I missing something?
>>
>> -- Bert
>>
>>
>>
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>> Sent: Wednesday, January 25, 2006 2:43 PM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] D(dnorm...)?
>>>
>>> 	  Can someone help me understand the following:
>>>
>>>> D(expression(dnorm(x, mean)), "mean")
>>> [1] 0
>>>> sessionInfo()
>>>
>>> R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>>
>>> attached base packages:
>>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
>>>  "datasets"
>>> [7] "base"
>>>
>>> 	  By my computations, this should be something like
>>> ((mean-x)/sd^2)*dnorm(...).
>>>
>>> 	  Thanks for your help.
>>> 	  Spencer Graves
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Jan 26 00:43:35 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jan 2006 15:43:35 -0800
Subject: [R] D(dnorm...)?
In-Reply-To: <Pine.LNX.4.61.0601252318050.30126@gannet.stats>
References: <200601252257.k0PMvX6D006058@compton.gene.com>
	<43D80434.2070709@pdf.com>
	<Pine.LNX.4.61.0601252318050.30126@gannet.stats>
Message-ID: <43D80D27.5020006@pdf.com>

Hi, Prof. Ripley:

	  Thanks for the explanation.  If I had read your book more carefully, 
I would not have needed this email exchange.

	  Thanks again,
	  Spencer Graves

Prof Brian Ripley wrote:

> On Wed, 25 Jan 2006, Spencer Graves wrote:
> 
>> Hi, Bert:
>>
>>       I think I was too terse:  Why didn't I get an error message?  
>> When I
>> tried the same thing with "dpois", I got an error message:
>>
>> > D(expression(dpois(x, prob)), "mean")
>> Error in D(expression(dpois(x, prob)), "mean") :
>>     Function 'dpois' is not in the derivatives table
>>
>>       With "dnorm", I got "0"!.  Whe didn't I get an error message?
> 
> 
> Because dnorm _is_ in the derivatives table, as a function of x.  (So is 
> pnorm.)   These are functions we write as \phi and \Phi in Statistics 101.
> 
> This is not particularly well documented in R, but it is in MASS4 pp. 
> 437-8.
> 
> It would be worth enhancing the help page with the list of known 
> functions.  Long ago we talked about making it extensible, but nothing 
> happened.
> 
> 
>>
>>       Thanks for the reply.
>>       Spencer Graves
>>
>> Berton Gunter wrote:
>>
>>> dnorm() is an internal function, so I don't see how D (or deriv) can do
>>> anything with it symbolically. Am I missing something?
>>>
>>> -- Bert
>>>
>>>
>>>
>>>
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>>> Sent: Wednesday, January 25, 2006 2:43 PM
>>>> To: r-help at stat.math.ethz.ch
>>>> Subject: [R] D(dnorm...)?
>>>>
>>>>       Can someone help me understand the following:
>>>>
>>>>> D(expression(dnorm(x, mean)), "mean")
>>>>
>>>> [1] 0
>>>>
>>>>> sessionInfo()
>>>>
>>>>
>>>> R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>>>
>>>> attached base packages:
>>>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
>>>>  "datasets"
>>>> [7] "base"
>>>>
>>>>       By my computations, this should be something like
>>>> ((mean-x)/sd^2)*dnorm(...).
>>>>
>>>>       Thanks for your help.
>>>>       Spencer Graves
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From mdd9 at cornell.edu  Thu Jan 26 00:56:24 2006
From: mdd9 at cornell.edu (Mark D'Ascenzo)
Date: Wed, 25 Jan 2006 18:56:24 -0500
Subject: [R] imbalanced classes
Message-ID: <40b755be0601251556m3ac1b775r5c63a259dd427a2@mail.gmail.com>

Hi Andy,

I know this topic has been discussed before on the R-help, but I was
wondering if you could offer some advice specific to my application.

I'm using the R random forest package to compare two classes of data,
the number of cases in each class relatively low, 28 in class 1 and 9
in class 2. I'd really like to use R environment to analyze this data,
however I'm finding it difficult to put much trust in the results of
my analysis.  As you've stated, the classwt variables do not do much,
and I've tried working with the cuttoff and sampsize variables as
well, with limited success in balancing error rates between the two
classes.

It was unclear to me how to use the cuttoff parameter correctly.  If
you have any recommendations here, it would be appreciated. 
Additionally with the sampsize variable, I have tried a few values,
for example setting sampsize = c(2, 6) and c(9, 3), etc.  It wasn't
clear to me if I should be sampling more from the larger class or the
other way around.

Lastly, I'm wondering if you are currently working or have plans to
release in the near future an R version of randomForest that is
equivalent to the FORTRAN rf5 package.  It works wonderfully for my
application, but getting data in and out of it, changing parameters,
compiling is just a pain, as I'm sure you agree.

Your thoughts would be greatly appreciated.

Kind regards,

Mark D'Ascenzo
Biomedical Engineering
Cornell University
Ithaca, NY 14853



From stratja at auburn.edu  Thu Jan 26 00:56:31 2006
From: stratja at auburn.edu (Jeffrey Stratford)
Date: Wed, 25 Jan 2006 17:56:31 -0600
Subject: [R] nested ANCOVA: still confused
Message-ID: <43D7BBCF020000F200006445@TMIA1.AUBURN.EDU>

Harold, Kingsford, and R-users,

I settled on using the lmer function.  I think the memory issue was more
a function of my poor coding than an actual memory problem.  I also
switched the label from "box" to "clutch" to avoid any potential
confusion with other functions. 

This coding seems to have worked:

> eabl <- lmer(rtot ~ sexv + (purban2|clutch), maxIter=1000, data=bb,
na.action=na.omit)

However, I have two remaining questions: (1)how concerned should I be
with the warning message below and (2) is there a way to invoke output
to get an estimate of the effect of purban2 (the proportion of urban
cover 200 m around a box) on feather color (rtot) and if there is a
difference between the sexes?   I used the summary function and it
doesn't tell me much (see output below). 

I'll read up mixed models when Pinheiro arrives but any suggestions for
diagnostics?  I'm going to repeat this study and expand it by doubling
or tripling the number of birds.  

Warning message:
nlminb returned message false convergence (8) 
 in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
1.49011611938477e-08,  

> summary(eabl)
Linear mixed-effects model fit by REML
Formula: rtot ~ sexv + (purban2 | clutch) 
   Data: bb 
      AIC      BIC    logLik MLdeviance REMLdeviance
 5164.284 6997.864 -2052.142   4128.792     4104.284
Random effects:
 Groups   Name               Variance  Std.Dev. Corr                    
                                                                        
      
 clutch   (Intercept)           502829   709.10                         
                                                                        
      
          purban20             1341990  1158.44 -0.477                  
                                                                        
      
          purban20.006711409   5683957  2384.10 -0.226  0.082           
                                                                        
      
          purban20.01342282    1772922  1331.51 -0.386  0.176  0.067  
.
.
.
.
.
# of obs: 235, groups: clutch, 74

Fixed effects:
            Estimate Std. Error t value
(Intercept)  5950.01     241.59  24.628
sexvm        1509.07     145.73  10.355

Correlation of Fixed Effects:
      (Intr)
sexvm -0.304

Thanks many time over,

Jeff

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja
****************************************
>>> "Doran, Harold" <HDoran at air.org> 01/25/06 6:37 AM >>>
OK, we're getting somewhere. First, it looks as though (by the error
message) that you have a big dataset. My first recommendation is to use
lmer instead of lme, you will see a significant benefit in terms of
computional speed.

For the model this would be

lmer(rtot ~ sexv +(purban|box:chick) + (purban|box), bb,
na.action=na.omit)

Now, you have run out of memory. I don't know what operating system you
are using, so go and see the appropriate FAQ for increasing memory for
your OS. 

Second, I made a mistake in my reply. Your random statement should be
random=~purban|box/chick denoting that chicks are nested in boxes, not
boxes nested in chicks, sorry about that.

Now, why is it that each chick within box has the same value for purban?
If this is so, why are you fitting that as a random effect? It seems not
to vary across individual chicks, right? It seems there is only an
effect of box and not an effect for chicks. Why not just fit a random
effect only for box such as:

rtot.lme <- lme(fixed=rtot~sexv, random=~purban2|box,
na.action=na.omit,bb)

or in lmer
lmer(rtot ~ sexv + (purban|box), bb, na.action=na.omit)

Harold
 


-----Original Message-----
From:	Jeffrey Stratford [mailto:stratja at auburn.edu]
Sent:	Tue 1/24/2006 8:57 PM
To:	Doran, Harold; r-help at stat.math.ethz.ch
Cc:	
Subject:	RE: [R] nested ANCOVA: still confused

R-users and Harold.

First, thanks for the advice;  I'm almost there.  

The code I'm using now is 

library(nlme)
bb <- read.csv("E:\\eabl_feather04.csv", header=TRUE)
bb$sexv <- factor(bb$sexv)
rtot.lme <- lme(fixed=rtot~sexv, random=~purban2|chick/box,
na.action=na.omit, data=bb)

A sample of the data looks like this 

box	chick	rtot	purban2	sexv
1	1	6333.51	0.026846	f
1	2	8710.884	0.026846	m
2	1	5810.007	0.161074	f
2	2	5524.33	0.161074	f
2	3	4824.474	0.161074	f
2	4	5617.641	0.161074	f
2	5	6761.724	0.161074	f
4	1	7569.673	0.208054	m
4	2	7877.081	0.208054	m
4	4	7455.55	0.208054	f
7	1	5408.287	0.436242	m
10	1	6991.727	0.14094	f
12	1	8590.207	0.134228	f
12	2	7536.747	0.134228	m
12	3	5145.342	0.134228	m
12	4	6853.628	0.134228	f
15	1	8048.717	0.033557	m
15	2	7062.196	0.033557	m
15	3	8165.953	0.033557	m
15	4	8348.58	0.033557	m
16	2	6534.775	0.751678	m
16	3	7468.827	0.751678	m
16	4	5907.338	0.751678	f
21	1	7761.983	0.221477	m
21	2	6634.115	0.221477	m
21	3	6982.923	0.221477	m
21	4	7464.075	0.221477	m
22	1	6756.733	0.281879	f
23	2	8231.496	0.134228	m

The error I'm getting is

Error in logLik.lmeStructInt(lmeSt, lmePars) : 
        Calloc could not allocate (590465568 of 8) memory
In addition: Warning messages:
1: Fewer observations than random effects in all level 2 groups in:
lme.formula(fixed = rtot ~ sexv, random = ~purban2 | chick/box,  
2: Reached total allocation of 382Mb: see help(memory.size) 

There's nothing "special" about chick 1, 2, etc.  These were simply the
order of the birds measured in each box so chick 1 in box 1 has nothing
to do with chick 1 in box 2.

Many thanks,

Jeff 

****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja
****************************************
>>> "Doran, Harold" <HDoran at air.org> 01/24/06 2:04 PM >>>
Dear Jeff:

I see the issues in your code and have provided what I think will solve
your problem. It is often much easier to get help on this list when you
provide a small bit of data that can be replicated and you state what
the error messages are that you are receiving. OK, with that said, here
is what I see. First, you do not need to use the syntax bb$sex in your
model, this can be significantly simplified. Second, you do not have a
random statement in your model.

Here is your original model:
lme(bb$rtot~bb$sex, bb$purban|bb$chick/bb$box, na.action=na.omit)

Here is what it should be:

lme(fixed = rtot~sex, random=~purban|chick/box, na.action=na.omit,
data=bb)

Notice there is a fixed and random call. You can simplify this as

lme(rtot~sex, random=~purban|chick/box, na.action=na.omit, bb)

Note, you can eliminate the "fixed=" portion but not the random
statement.

Last, if you want to do this in lmer, the newer function for mixed
models in the Matrix package, you would do

lmer(rtot~sex + (purban|box:chick) + (purban|box), na.action=na.omit,
data=bb)

Hope this helps.
Harold




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeffrey Stratford
Sent: Tuesday, January 24, 2006 11:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] nested ANCOVA: still confused

Dear R-users,

I did some more research and I'm still not sure how to set up an ANCOVA
with nestedness.  Specifically I'm not sure how to express chicks nested
within boxes.  I will be getting Pinheiro & Bates (Mixed Effects Models
in S and S-Plus) but it will not arrive for another two weeks from our
interlibrary loan.

The goal is to determine if there are urbanization (purban) effects on
chick health (rtot) and if there are differences between sexes (sex) and
the effect of being in the same clutch (box).

The model is rtot = sex + purban + (chick)box.

I've loaded the package lme4.  And the code I have so far is

bb <- read.csv("C:\\eabl\\eabl_feather04.csv", header=TRUE) bb$sex <-
factor(bb$sex) rtot.lme <- lme(bb$rtot~bb$sex,
bb$purban|bb$chick/bb$box,
na.action=na.omit)

but this is not working.

Any suggestions would be greatly appreciated.

Thanks,

Jeff








****************************************
Jeffrey A. Stratford, Ph.D.
Postdoctoral Associate
331 Funchess Hall
Department of Biological Sciences
Auburn University
Auburn, AL 36849
334-329-9198
FAX 334-844-9234
http://www.auburn.edu/~stratja

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Thu Jan 26 01:10:23 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 Jan 2006 16:10:23 -0800
Subject: [R] nested ANCOVA: still confused
In-Reply-To: <43D7BBCF020000F200006445@TMIA1.AUBURN.EDU>
Message-ID: <200601260010.k0Q0ANaK010583@hertz.gene.com>

Jeff:

> However, I have two remaining questions: (1)how concerned should I be
> with the warning message below and 

There was a definitive comment on this just a few days ago on the list
(search the archives), the gist of it was: **very concerned** . "False
convergence" means that you're not truly converged, the details for which
I've forgotten (sigh...). Anyway, this means that your parameter estimates
could be far from the correct minimized values = you could be in trouble. I
can't help you any more than that, but hopefully you'll get responses from
those with suitable expertise who can.

Cheers,
Bert


(2) is there a way to invoke output
> to get an estimate of the effect of purban2 (the proportion of urban
> cover 200 m around a box) on feather color (rtot) and if there is a
> difference between the sexes?   I used the summary function and it
> doesn't tell me much (see output below). 
> 
> I'll read up mixed models when Pinheiro arrives but any 
> suggestions for
> diagnostics?  I'm going to repeat this study and expand it by doubling
> or tripling the number of birds.  
> 
> Warning message:
> nlminb returned message false convergence (8) 
>  in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 200, tolerance =
> 1.49011611938477e-08,  
> 
> > summary(eabl)
> Linear mixed-effects model fit by REML
> Formula: rtot ~ sexv + (purban2 | clutch) 
>    Data: bb 
>       AIC      BIC    logLik MLdeviance REMLdeviance
>  5164.284 6997.864 -2052.142   4128.792     4104.284
> Random effects:
>  Groups   Name               Variance  Std.Dev. Corr          
>           
>                                                               
>           
>       
>  clutch   (Intercept)           502829   709.10               
>           
>                                                               
>           
>       
>           purban20             1341990  1158.44 -0.477        
>           
>                                                               
>           
>       
>           purban20.006711409   5683957  2384.10 -0.226  0.082 
>           
>                                                               
>           
>       
>           purban20.01342282    1772922  1331.51 -0.386  0.176  0.067  
> .
> .
> .
> .
> .
> # of obs: 235, groups: clutch, 74
> 
> Fixed effects:
>             Estimate Std. Error t value
> (Intercept)  5950.01     241.59  24.628
> sexvm        1509.07     145.73  10.355
> 
> Correlation of Fixed Effects:
>       (Intr)
> sexvm -0.304
> 
> Thanks many time over,
> 
> Jeff
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> ****************************************
> >>> "Doran, Harold" <HDoran at air.org> 01/25/06 6:37 AM >>>
> OK, we're getting somewhere. First, it looks as though (by the error
> message) that you have a big dataset. My first recommendation 
> is to use
> lmer instead of lme, you will see a significant benefit in terms of
> computional speed.
> 
> For the model this would be
> 
> lmer(rtot ~ sexv +(purban|box:chick) + (purban|box), bb,
> na.action=na.omit)
> 
> Now, you have run out of memory. I don't know what operating 
> system you
> are using, so go and see the appropriate FAQ for increasing memory for
> your OS. 
> 
> Second, I made a mistake in my reply. Your random statement should be
> random=~purban|box/chick denoting that chicks are nested in boxes, not
> boxes nested in chicks, sorry about that.
> 
> Now, why is it that each chick within box has the same value 
> for purban?
> If this is so, why are you fitting that as a random effect? 
> It seems not
> to vary across individual chicks, right? It seems there is only an
> effect of box and not an effect for chicks. Why not just fit a random
> effect only for box such as:
> 
> rtot.lme <- lme(fixed=rtot~sexv, random=~purban2|box,
> na.action=na.omit,bb)
> 
> or in lmer
> lmer(rtot ~ sexv + (purban|box), bb, na.action=na.omit)
> 
> Harold
>  
> 
> 
> -----Original Message-----
> From:	Jeffrey Stratford [mailto:stratja at auburn.edu]
> Sent:	Tue 1/24/2006 8:57 PM
> To:	Doran, Harold; r-help at stat.math.ethz.ch
> Cc:	
> Subject:	RE: [R] nested ANCOVA: still confused
> 
> R-users and Harold.
> 
> First, thanks for the advice;  I'm almost there.  
> 
> The code I'm using now is 
> 
> library(nlme)
> bb <- read.csv("E:\\eabl_feather04.csv", header=TRUE)
> bb$sexv <- factor(bb$sexv)
> rtot.lme <- lme(fixed=rtot~sexv, random=~purban2|chick/box,
> na.action=na.omit, data=bb)
> 
> A sample of the data looks like this 
> 
> box	chick	rtot	purban2	sexv
> 1	1	6333.51	0.026846	f
> 1	2	8710.884	0.026846	m
> 2	1	5810.007	0.161074	f
> 2	2	5524.33	0.161074	f
> 2	3	4824.474	0.161074	f
> 2	4	5617.641	0.161074	f
> 2	5	6761.724	0.161074	f
> 4	1	7569.673	0.208054	m
> 4	2	7877.081	0.208054	m
> 4	4	7455.55	0.208054	f
> 7	1	5408.287	0.436242	m
> 10	1	6991.727	0.14094	f
> 12	1	8590.207	0.134228	f
> 12	2	7536.747	0.134228	m
> 12	3	5145.342	0.134228	m
> 12	4	6853.628	0.134228	f
> 15	1	8048.717	0.033557	m
> 15	2	7062.196	0.033557	m
> 15	3	8165.953	0.033557	m
> 15	4	8348.58	0.033557	m
> 16	2	6534.775	0.751678	m
> 16	3	7468.827	0.751678	m
> 16	4	5907.338	0.751678	f
> 21	1	7761.983	0.221477	m
> 21	2	6634.115	0.221477	m
> 21	3	6982.923	0.221477	m
> 21	4	7464.075	0.221477	m
> 22	1	6756.733	0.281879	f
> 23	2	8231.496	0.134228	m
> 
> The error I'm getting is
> 
> Error in logLik.lmeStructInt(lmeSt, lmePars) : 
>         Calloc could not allocate (590465568 of 8) memory
> In addition: Warning messages:
> 1: Fewer observations than random effects in all level 2 groups in:
> lme.formula(fixed = rtot ~ sexv, random = ~purban2 | chick/box,  
> 2: Reached total allocation of 382Mb: see help(memory.size) 
> 
> There's nothing "special" about chick 1, 2, etc.  These were 
> simply the
> order of the birds measured in each box so chick 1 in box 1 
> has nothing
> to do with chick 1 in box 2.
> 
> Many thanks,
> 
> Jeff 
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> ****************************************
> >>> "Doran, Harold" <HDoran at air.org> 01/24/06 2:04 PM >>>
> Dear Jeff:
> 
> I see the issues in your code and have provided what I think 
> will solve
> your problem. It is often much easier to get help on this 
> list when you
> provide a small bit of data that can be replicated and you state what
> the error messages are that you are receiving. OK, with that 
> said, here
> is what I see. First, you do not need to use the syntax bb$sex in your
> model, this can be significantly simplified. Second, you do not have a
> random statement in your model.
> 
> Here is your original model:
> lme(bb$rtot~bb$sex, bb$purban|bb$chick/bb$box, na.action=na.omit)
> 
> Here is what it should be:
> 
> lme(fixed = rtot~sex, random=~purban|chick/box, na.action=na.omit,
> data=bb)
> 
> Notice there is a fixed and random call. You can simplify this as
> 
> lme(rtot~sex, random=~purban|chick/box, na.action=na.omit, bb)
> 
> Note, you can eliminate the "fixed=" portion but not the random
> statement.
> 
> Last, if you want to do this in lmer, the newer function for mixed
> models in the Matrix package, you would do
> 
> lmer(rtot~sex + (purban|box:chick) + (purban|box), na.action=na.omit,
> data=bb)
> 
> Hope this helps.
> Harold
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Jeffrey Stratford
> Sent: Tuesday, January 24, 2006 11:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] nested ANCOVA: still confused
> 
> Dear R-users,
> 
> I did some more research and I'm still not sure how to set up 
> an ANCOVA
> with nestedness.  Specifically I'm not sure how to express 
> chicks nested
> within boxes.  I will be getting Pinheiro & Bates (Mixed 
> Effects Models
> in S and S-Plus) but it will not arrive for another two weeks from our
> interlibrary loan.
> 
> The goal is to determine if there are urbanization (purban) effects on
> chick health (rtot) and if there are differences between 
> sexes (sex) and
> the effect of being in the same clutch (box).
> 
> The model is rtot = sex + purban + (chick)box.
> 
> I've loaded the package lme4.  And the code I have so far is
> 
> bb <- read.csv("C:\\eabl\\eabl_feather04.csv", header=TRUE) bb$sex <-
> factor(bb$sex) rtot.lme <- lme(bb$rtot~bb$sex,
> bb$purban|bb$chick/bb$box,
> na.action=na.omit)
> 
> but this is not working.
> 
> Any suggestions would be greatly appreciated.
> 
> Thanks,
> 
> Jeff
> 
> 
> 
> 
> 
> 
> 
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sasprog474474 at yahoo.com  Thu Jan 26 01:11:35 2006
From: sasprog474474 at yahoo.com (Greg Tarpinian)
Date: Wed, 25 Jan 2006 16:11:35 -0800 (PST)
Subject: [R] panel.xyplot :  incorrectly "connecting" points
Message-ID: <20060126001135.26349.qmail@web37105.mail.mud.yahoo.com>

R 2.2, WinXP.  I am having problems getting the right kind of
xyplot( ) to be generated.  The first of these works fine, but
doesn't overlay a reference grid (which I need):

xyplot(Y ~ X | Factor1, type = 'b', groups = GROUP, 
col = c(1,13), pch = c(16,6), lty = 1, lwd = 2, 
cex = 1.2, data = FOO.Frame, 
between = list(x = .5, y = .5),
scales = list(alternating = TRUE))



The second of these displays the grid as I need, but incorrectly
"connects" the points from _different_ GROUP values within each
panel.  I have made sure that GROUP is an ordered factor:

xyplot(PROB ~ MEAN | SD, data = SimProb,
groups = GROUP,							
between = list(x = .5, y = .5),
scales = list(alternating = TRUE),
panel = function(x, y, ...)
   {
    panel.grid(h=-1, v=-1, lwd = 1)
    panel.xyplot(x, y, type = 'b', col = 1,
                 lwd = 2, cex = 1.2, ...)
   }
   )
           			

I am sure that this sort of question has been asked before, and
I apologize for any redundancy.  I would appreciate any help.

Kind regards,

     Greg



From deepayan.sarkar at gmail.com  Thu Jan 26 01:35:12 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 25 Jan 2006 18:35:12 -0600
Subject: [R] panel.xyplot : incorrectly "connecting" points
In-Reply-To: <20060126001135.26349.qmail@web37105.mail.mud.yahoo.com>
References: <20060126001135.26349.qmail@web37105.mail.mud.yahoo.com>
Message-ID: <eb555e660601251635p389cde31x110c67c36a59587a@mail.gmail.com>

On 1/25/06, Greg Tarpinian <sasprog474474 at yahoo.com> wrote:
> R 2.2, WinXP.  I am having problems getting the right kind of
> xyplot( ) to be generated.  The first of these works fine, but
> doesn't overlay a reference grid (which I need):
>
> xyplot(Y ~ X | Factor1, type = 'b', groups = GROUP,
> col = c(1,13), pch = c(16,6), lty = 1, lwd = 2,
> cex = 1.2, data = FOO.Frame,
> between = list(x = .5, y = .5),
> scales = list(alternating = TRUE))

The simplest solution is to use

type = c('b', 'g')

> The second of these displays the grid as I need, but incorrectly
> "connects" the points from _different_ GROUP values within each
> panel.  I have made sure that GROUP is an ordered factor:
>
> xyplot(PROB ~ MEAN | SD, data = SimProb,
> groups = GROUP,							
> between = list(x = .5, y = .5),
> scales = list(alternating = TRUE),
> panel = function(x, y, ...)
>    {
>     panel.grid(h=-1, v=-1, lwd = 1)
>     panel.xyplot(x, y, type = 'b', col = 1,
>                  lwd = 2, cex = 1.2, ...)
>    }
>    )

This should work in the devel version of R (2.3.0 to be). The reason
it doesn't work in 2.2.x is that the default panel function when
'groups' in non-null is not panel.xyplot, but rather panel.superpose.
(This is admittedly confusing, hence the change for 2.3.0.)

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From spencer.graves at pdf.com  Thu Jan 26 02:05:35 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jan 2006 17:05:35 -0800
Subject: [R] regression with nestedness
In-Reply-To: <43D3755F020000F200005EBD@TMIA1.AUBURN.EDU>
References: <43D3755F020000F200005EBD@TMIA1.AUBURN.EDU>
Message-ID: <43D8205F.1040906@pdf.com>

	  Have you considered "lme" in library(nlme)?  The companion book 
Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer) is my favorite reference for this kind of thing.  From what I 
understand of your question, you should be able to find excellent 
answers in this book.  [You could also try "lmer" associated with the 
lme4 package.  However, many of the helper functions needed to do what 
you want are available for "lme" but not yet for "lmer";  "lmer" can do 
things that "lme" can not, but it doesn't sound like you need any of 
those extra features.  Even if you did, I would suggest you first ignore 
those issues and see what you can get from "lme".]

	  If this is not adequate and you would like more help from this list, 
please submit another post.  Before you do, however, PLEASE do read the 
posting guide! "www.R-project.org/posting-guide.html", especially the 
part about providing a brief but self-contained toy example illustrating 
what you tried to do and why it was not adequate.


Jeffrey Stratford wrote:

> Dear R-users,
> 
> I set up an experiment where I put up bluebird boxes across an
> urbanization gradient.  I monitored these boxes and at some point I
> pulled a feather from a chick and a friend used spectral properties
> (rtot, a continuous var) to index chick health.  There is an effect of
> sex that I would like to include but how would I set up a regression and
> look at the effect of urbanization (purban, a continuous var)) on
> feather properties of chicks within boxes.  
> 
> So the model should look something like rtot = sex + purban +
> (chick)clutch
> 
> Also, when I plot purban against rtot using the plot function I get
> boxplots but I would like to ignore the clutch and just plot each point.
>  I've tried type = "p" but this has no effect.  
> 
> Thanks,
> 
> Jeff
> 
>  
> 
> ****************************************
> Jeffrey A. Stratford, Ph.D.
> Postdoctoral Associate
> 331 Funchess Hall
> Department of Biological Sciences
> Auburn University
> Auburn, AL 36849
> 334-329-9198
> FAX 334-844-9234
> http://www.auburn.edu/~stratja
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From eliotmcintire at gmail.com  Thu Jan 26 02:12:07 2006
From: eliotmcintire at gmail.com (Eliot McIntire)
Date: Wed, 25 Jan 2006 18:12:07 -0700
Subject: [R] Partial Canonical Correlation
Message-ID: <e6cccb9b0601251712r475a83c0k9ac43fcdb6a3254b@mail.gmail.com>

Hello,

I am interested in doing a partial canonical correlation (identical to
the SAS function, Proc Cancorr with the Partial statement).

By this I mean, I have 3 sets of data, a vegetation matrix (columns of
abundances of species in rows of plots), an "environment" matrix
(columns of environmental variables in same rows of plots), and a
"space" matrix (x and y locations of each of the plots).  I would like
to look at the relationships between these multivariate groups, with
the space partialed out.

This would be somewhat analogous in my mind to a partial mantel test
(i.e., 3 data matrices), but would be somewhat more sophisticated than
the correlations between distance matrices of the partial mantel test.

In the r-help archives I have found similar things, partial
correlations, canonical correlations, but not partial canonical
correlations.

Does somebody know how to do this in R?

Thank you in advance,

Eliot

--
Eliot McIntire
Post Doctoral Fellow
Department of Ecosystems and Conservation Science
College of Forestry and Conservation
University of Montana, Missoula, MT 59812
406-243-5239
fax: 406-243-4557
emcintire at forestry.umt.edu



From dimitrijoe at ipea.gov.br  Thu Jan 26 02:24:14 2006
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Wed, 25 Jan 2006 23:24:14 -0200
Subject: [R] A function slightly different from diff()
Message-ID: <1138238654.43d824be195bd@webmail.ipea.gov.br>

Hi,

I wonder if the following function has already been implemented in (some) R
(package):

                summ <- function(x, lag=1) # x is a vector
                        {
                            n <- length(x)
                            x[(1+lag):n] + x[1:(n-lag)]
                        }

which (I think) the only difference from diff() is "+" instead of "-", for
differences=1.

Thank you,
Dimitri Szerman



From ggrothendieck at gmail.com  Thu Jan 26 02:59:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jan 2006 20:59:41 -0500
Subject: [R] A function slightly different from diff()
In-Reply-To: <1138238654.43d824be195bd@webmail.ipea.gov.br>
References: <1138238654.43d824be195bd@webmail.ipea.gov.br>
Message-ID: <971536df0601251759h1bfa67c8pdf443dd713b69111@mail.gmail.com>

This isn't a single function but its a simple expression:

x <- ts(1:10)  # test data
x + lag(x)



On 1/25/06, dimitrijoe at ipea.gov.br <dimitrijoe at ipea.gov.br> wrote:
> Hi,
>
> I wonder if the following function has already been implemented in (some) R
> (package):
>
>                summ <- function(x, lag=1) # x is a vector
>                        {
>                            n <- length(x)
>                            x[(1+lag):n] + x[1:(n-lag)]
>                        }
>
> which (I think) the only difference from diff() is "+" instead of "-", for
> differences=1.
>
> Thank you,
> Dimitri Szerman
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jan 26 03:00:51 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Jan 2006 21:00:51 -0500
Subject: [R] imbalanced classes
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED75E@usctmx1106.merck.com>

Mark,

I guess the message is meant for me (yet you sent it to R-help).

If you have 10 class A and 100 class B, not setting sampsize would cause a
random sample (with replacement) of 110 from the whole lot, which, of
course, would give you on the average 10 times more Bs than As in the
sample.  If you grow a tree on such a sample, it's not going to do so well
in predicting the As.  However, if you set sampsize=c(10, 10), then each
tree is grown on 10 randomly sampled As and 10 randomly sampled Bs, giving
the tree a much better chance of giving roughly similar error rates for
predicting As and Bs.  If setting the sampsize to be equal doesn't quite do
it, you can try setting it to the more extreme direction.

As to cutoff, in a two-class problem, it's the same as setting the
classification threshold to something other than 0.5.  E.g., if
cutoff=c(0.9, 0.1), then a case with 80% of the votes for class A would
still be classified as B, because .8/.9 < .2/.1.  Hope that's clear.

I do have to wonder, though, if you only have a total of 37 cases in the
data, how can you be sure the estimates of class error rates you get will
pan out on a larger test set?  I would think the variability on the estimate
of the class error rates is so high that it doesn't make too much sense to
try to balance them too much...  Just my $0.02.

I do plan on implementing the weighted RF (see the To Do part of rfNews()),
but don't hold your breath...

Cheers,
Andy

From: Mark D'Ascenzo
> 
> Hi Andy,
> 
> I know this topic has been discussed before on the R-help, but I was
> wondering if you could offer some advice specific to my application.
> 
> I'm using the R random forest package to compare two classes of data,
> the number of cases in each class relatively low, 28 in class 1 and 9
> in class 2. I'd really like to use R environment to analyze this data,
> however I'm finding it difficult to put much trust in the results of
> my analysis.  As you've stated, the classwt variables do not do much,
> and I've tried working with the cuttoff and sampsize variables as
> well, with limited success in balancing error rates between the two
> classes.
> 
> It was unclear to me how to use the cuttoff parameter correctly.  If
> you have any recommendations here, it would be appreciated. 
> Additionally with the sampsize variable, I have tried a few values,
> for example setting sampsize = c(2, 6) and c(9, 3), etc.  It wasn't
> clear to me if I should be sampling more from the larger class or the
> other way around.
> 
> Lastly, I'm wondering if you are currently working or have plans to
> release in the near future an R version of randomForest that is
> equivalent to the FORTRAN rf5 package.  It works wonderfully for my
> application, but getting data in and out of it, changing parameters,
> compiling is just a pain, as I'm sure you agree.
> 
> Your thoughts would be greatly appreciated.
> 
> Kind regards,
> 
> Mark D'Ascenzo
> Biomedical Engineering
> Cornell University
> Ithaca, NY 14853
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Bill.Venables at csiro.au  Thu Jan 26 04:39:12 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 26 Jan 2006 14:39:12 +1100
Subject: [R] D(dnorm...)?
Message-ID: <B998A44C8986644EA8029CFE6396A924546966@exqld2-bne.qld.csiro.au>

Yes Bert, this time you are missing something (unusually) ...

As Brian Ripley pointed out 'dnorm' is in the derivative table, *but*
only as a function of one variable.  So if you want to find the
derivative of 

dnorm(x, mean, sigma)

you have to write it as 1/sigma * dnorm((x - mu)/sigma).  Here is a
little example:

> D(Quote(pnorm((x-mu)/sigma)), "x")
dnorm((x - mu)/sigma) * (1/sigma)

> D(D(Quote(pnorm((x-mu)/sigma)), "x"), "mu")
(x - mu)/sigma * (dnorm((x - mu)/sigma) * (1/sigma)) * (1/sigma)

---

Like Brian, I recall the suggestion that we make D(...) extensible.  I
still think it is a good idea and worth considering.  Under one scheme
you would specify an object such as

Fnorm <- structure(quote(pnorm(x, mu, sigma)), 
	deriv = 
	list(x = Quote(dnorm(x, mu, sigma)/sigms),
		mu = Quote(-dnorm(x, mu, sigma)/sigma),
		sigma = Quote(-(x - mu)*dnorm(x, mu, sigma)/sigma^2),
	class = "dfunction")

ane write a generic "differentiate" function with a "dfunction" method
and "D" as the default.

I don't think it's quite that easy, but the plan is clear enough.

Bill.





-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
Sent: Thursday, 26 January 2006 8:58 AM
To: 'Spencer Graves'; r-help at stat.math.ethz.ch
Subject: Re: [R] D(dnorm...)?


dnorm() is an internal function, so I don't see how D (or deriv) can do
anything with it symbolically. Am I missing something?

-- Bert
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, January 25, 2006 2:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] D(dnorm...)?
> 
> 	  Can someone help me understand the following:
> 
>  > D(expression(dnorm(x, mean)), "mean")
> [1] 0
>  > sessionInfo()
> 
> R version 2.2.1, 2005-12-20, i386-pc-mingw32
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"   
>   "datasets"
> [7] "base"
> 
> 	  By my computations, this should be something like 
> ((mean-x)/sd^2)*dnorm(...).
> 
> 	  Thanks for your help.
> 	  Spencer Graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Jan 26 05:14:07 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jan 2006 20:14:07 -0800
Subject: [R] D(dnorm...)?
In-Reply-To: <B998A44C8986644EA8029CFE6396A924546966@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924546966@exqld2-bne.qld.csiro.au>
Message-ID: <43D84C8F.5070306@pdf.com>

Hello, Bill:

	  I'm not qualified to make this suggestion since I'm incapable of 
turning it into reality, but what about creating a link between R and 
one of the Mathematica clones like Yacas?  I can immagine that it could 
be substantially more difficult than linking R to other software like 
Excel, but ... .

	  Spencer Graves

Bill.Venables at csiro.au wrote:
> Yes Bert, this time you are missing something (unusually) ...
> 
> As Brian Ripley pointed out 'dnorm' is in the derivative table, *but*
> only as a function of one variable.  So if you want to find the
> derivative of 
> 
> dnorm(x, mean, sigma)
> 
> you have to write it as 1/sigma * dnorm((x - mu)/sigma).  Here is a
> little example:
> 
> 
>>D(Quote(pnorm((x-mu)/sigma)), "x")
> 
> dnorm((x - mu)/sigma) * (1/sigma)
> 
> 
>>D(D(Quote(pnorm((x-mu)/sigma)), "x"), "mu")
> 
> (x - mu)/sigma * (dnorm((x - mu)/sigma) * (1/sigma)) * (1/sigma)
> 
> ---
> 
> Like Brian, I recall the suggestion that we make D(...) extensible.  I
> still think it is a good idea and worth considering.  Under one scheme
> you would specify an object such as
> 
> Fnorm <- structure(quote(pnorm(x, mu, sigma)), 
> 	deriv = 
> 	list(x = Quote(dnorm(x, mu, sigma)/sigms),
> 		mu = Quote(-dnorm(x, mu, sigma)/sigma),
> 		sigma = Quote(-(x - mu)*dnorm(x, mu, sigma)/sigma^2),
> 	class = "dfunction")
> 
> ane write a generic "differentiate" function with a "dfunction" method
> and "D" as the default.
> 
> I don't think it's quite that easy, but the plan is clear enough.
> 
> Bill.
> 
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Thursday, 26 January 2006 8:58 AM
> To: 'Spencer Graves'; r-help at stat.math.ethz.ch
> Subject: Re: [R] D(dnorm...)?
> 
> 
> dnorm() is an internal function, so I don't see how D (or deriv) can do
> anything with it symbolically. Am I missing something?
> 
> -- Bert
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Wednesday, January 25, 2006 2:43 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] D(dnorm...)?
>>
>>	  Can someone help me understand the following:
>>
>> > D(expression(dnorm(x, mean)), "mean")
>>[1] 0
>> > sessionInfo()
>>
>>R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>
>>attached base packages:
>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"   
>>  "datasets"
>>[7] "base"
>>
>>	  By my computations, this should be something like 
>>((mean-x)/sd^2)*dnorm(...).
>>
>>	  Thanks for your help.
>>	  Spencer Graves
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Bill.Venables at csiro.au  Thu Jan 26 06:03:44 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 26 Jan 2006 16:03:44 +1100
Subject: [R] D(dnorm...)?
Message-ID: <B998A44C8986644EA8029CFE6396A924546967@exqld2-bne.qld.csiro.au>

Hi Spencer,

I think if you have a problem that needs a lot of symbolic manipulation
you are probably better off driving it directly from something like
Maple or Mathematica (I prefer maple, actually) than trying to drive it
from R.  It just gets too clumsy.  On the other hand it is very handy
having a simple differentiator available in R, like D(...) for small
jobs that are not worth taking to a big system like Maple.  The point I
was trying to make in the previous message was that with a little
thought it could be made a lot more convenient.  

This arose in connexion with a real problem.  We needed to differentiate
a pdf that had the normal density function in it, but was otherwise
quite simple and we had to hack the code in another system (not unlike
R, as it happens) to handle it.  The hack was quite small and it became
clear that with a slight change of design users would not need to do
hacks like this for simple extensions such as the one we needed.  As it
was a hack, we only put it in for the standard density and I suspect
that is the reason why even now the derivative tables in both R and the
other system (not unlike R) only handle normal density and distribution
funcitons in one variable.

I'm sort of avoiding your question, because I don't know how hard it
would be to link R with Yacas, either, but if you really wanted to go
that way I see that Yacas can be driven over the net via a Java applet.
Something like this might provide the simplest link, if not the most
efficient.  But note that Yacas uses the eccentric Mathematica notation,
where the functions are Capitalised, for example, as nouns are in
German.  That's a small bother you could do without, too.

Regards,
Bill Venables.


-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Thursday, 26 January 2006 2:14 PM
To: Venables, Bill (CMIS, Cleveland)
Cc: gunter.berton at gene.com; r-help at stat.math.ethz.ch;
ripley at stats.ox.ac.uk
Subject: Re: [R] D(dnorm...)?


Hello, Bill:

	  I'm not qualified to make this suggestion since I'm incapable
of 
turning it into reality, but what about creating a link between R and 
one of the Mathematica clones like Yacas?  I can immagine that it could 
be substantially more difficult than linking R to other software like 
Excel, but ... .

	  Spencer Graves

Bill.Venables at csiro.au wrote:
> Yes Bert, this time you are missing something (unusually) ...
> 
> As Brian Ripley pointed out 'dnorm' is in the derivative table, *but*
> only as a function of one variable.  So if you want to find the
> derivative of 
> 
> dnorm(x, mean, sigma)
> 
> you have to write it as 1/sigma * dnorm((x - mu)/sigma).  Here is a
> little example:
> 
> 
>>D(Quote(pnorm((x-mu)/sigma)), "x")
> 
> dnorm((x - mu)/sigma) * (1/sigma)
> 
> 
>>D(D(Quote(pnorm((x-mu)/sigma)), "x"), "mu")
> 
> (x - mu)/sigma * (dnorm((x - mu)/sigma) * (1/sigma)) * (1/sigma)
> 
> ---
> 
> Like Brian, I recall the suggestion that we make D(...) extensible.  I
> still think it is a good idea and worth considering.  Under one scheme
> you would specify an object such as
> 
> Fnorm <- structure(quote(pnorm(x, mu, sigma)), 
> 	deriv = 
> 	list(x = Quote(dnorm(x, mu, sigma)/sigms),
> 		mu = Quote(-dnorm(x, mu, sigma)/sigma),
> 		sigma = Quote(-(x - mu)*dnorm(x, mu, sigma)/sigma^2),
> 	class = "dfunction")
> 
> ane write a generic "differentiate" function with a "dfunction" method
> and "D" as the default.
> 
> I don't think it's quite that easy, but the plan is clear enough.
> 
> Bill.
> 
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Thursday, 26 January 2006 8:58 AM
> To: 'Spencer Graves'; r-help at stat.math.ethz.ch
> Subject: Re: [R] D(dnorm...)?
> 
> 
> dnorm() is an internal function, so I don't see how D (or deriv) can
do
> anything with it symbolically. Am I missing something?
> 
> -- Bert
>  
>  
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Wednesday, January 25, 2006 2:43 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] D(dnorm...)?
>>
>>	  Can someone help me understand the following:
>>
>> > D(expression(dnorm(x, mean)), "mean")
>>[1] 0
>> > sessionInfo()
>>
>>R version 2.2.1, 2005-12-20, i386-pc-mingw32
>>
>>attached base packages:
>>[1] "methods"   "stats"     "graphics"  "grDevices" "utils"   
>>  "datasets"
>>[7] "base"
>>
>>	  By my computations, this should be something like 
>>((mean-x)/sd^2)*dnorm(...).
>>
>>	  Thanks for your help.
>>	  Spencer Graves
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From sell_mirage_ne at hotmail.com  Thu Jan 26 06:31:01 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Wed, 25 Jan 2006 23:31:01 -0600
Subject: [R] question about "system" command
Message-ID: <BAY110-F13EFBEF686555F6F060C36C7150@phx.gbl>

Hi R Users
I am going to write a very short script for my small pilot simulation study.
I need to call a DOS program with different input data files in the middle 
of for loop.
There are 100 input data files (e.g., input001.dat, input002.dat, .... , 
input100.dat)

for (1 in i :100)
{
    system('"C:\\Program Files\\DOSPROGRAM\\RUN.exe" input001.dat')
}

I like to change the input data file name as for loop runs. How can I do 
that ?

Thanks in advance

TM



From maustin at amgen.com  Thu Jan 26 06:54:50 2006
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 25 Jan 2006 21:54:50 -0800
Subject: [R] question about "system" command
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD735@teal-exch.amgen.com>

Something like this.

See ?formatC, ?paste

for (i in 1:100)
{
    system(paste("C:\\Progra~1\\DOSPROGRAM\\RUN.exe input",
                  formatC(i, digits=2, flag='0'),
                  '.dat',
                  sep=''))
}

--Matt
Statistician
Amgen, Inc


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Taka Matzmoto
Sent: Wednesday, January 25, 2006 9:31 PM
To: r-help at stat.math.ethz.ch
Subject: [R] question about "system" command


Hi R Users
I am going to write a very short script for my small pilot simulation study.
I need to call a DOS program with different input data files in the middle 
of for loop.
There are 100 input data files (e.g., input001.dat, input002.dat, .... , 
input100.dat)

for (1 in i :100)
{
    system('"C:\\Program Files\\DOSPROGRAM\\RUN.exe" input001.dat')
}

I like to change the input data file name as for loop runs. How can I do 
that ?

Thanks in advance

TM

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jan 26 08:24:54 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jan 2006 07:24:54 +0000 (GMT)
Subject: [R] question about "system" command
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859109DAD735@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859109DAD735@teal-exch.amgen.com>
Message-ID: <Pine.LNX.4.61.0601260719370.2826@gannet.stats>

It is a little easier to use sprintf, as in

system(sprintf("%s input%03d.dat","C:\\Progra~1\\DOSPROGRAM\\RUN.exe", i))

With formatC it is not obvious why digits=2 is needed, but it would be 
more obvious why width=3 would work.


On Wed, 25 Jan 2006, Austin, Matt wrote:

> Something like this.
>
> See ?formatC, ?paste
>
> for (i in 1:100)
> {
>    system(paste("C:\\Progra~1\\DOSPROGRAM\\RUN.exe input",
>                  formatC(i, digits=2, flag='0'),
>                  '.dat',
>                  sep=''))
> }
>
> --Matt
> Statistician
> Amgen, Inc
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Taka Matzmoto
> Sent: Wednesday, January 25, 2006 9:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] question about "system" command
>
>
> Hi R Users
> I am going to write a very short script for my small pilot simulation study.
> I need to call a DOS program with different input data files in the middle
> of for loop.
> There are 100 input data files (e.g., input001.dat, input002.dat, .... ,
> input100.dat)
>
> for (1 in i :100)
> {
>    system('"C:\\Program Files\\DOSPROGRAM\\RUN.exe" input001.dat')
> }
>
> I like to change the input data file name as for loop runs. How can I do
> that ?
>
> Thanks in advance
>
> TM
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stefano.iacus at unimi.it  Thu Jan 26 08:36:49 2006
From: stefano.iacus at unimi.it (stefano iacus)
Date: Thu, 26 Jan 2006 08:36:49 +0100
Subject: [R] [R-SIG-Mac] Hist for different levels of a factor
In-Reply-To: <BFFD3997.5B04%s.charlat@ucl.ac.uk>
References: <BFFD3997.5B04%s.charlat@ucl.ac.uk>
Message-ID: <43E0958E-2BBA-46AD-803E-9472F8912B42@unimi.it>

The list of your interest is R-help not R-sig-mac
stefano

Il giorno 26/gen/06, alle ore 01:20, Sylvain Charlat ha scritto:

> Hi,
>
> Is there any simple way to get histogram for different levels of  
> factor?
>
> Say you have the following data set:
>
>  Island Sp.diam
>  Moorea 1.21
>  Moorea 1.27
>  Moorea 1.28
>  Moorea 1.22
>  Moorea 1.28
>  Rurutu 1.5
>  Rurutu 1.67
>  Rurutu 1.75
>  Rurutu 1.55
>  Rurutu 1.7
>  Rurutu 1.55
>  Rurutu 1.59
>  Rurutu 1.66
>  Rurutu 1.7
>
> Is there anything better than:
>
>> hist(Sp.diam[factor(Island)=="Moorea"],main="Island=Moorea")
>
> followed by
>
>> hist(Sp.diam[factor(Island)=="Rurutu"],main="Island=Rurutu")
>
> Sorry for this very stupid and basic question...
>
> Thanks for any help,
>
> Sylvain.
>
> ---------------------------------------------------------------
> Sylvain Charlat
> s.charlat at ucl.ac.uk / sylvaincharlat at yahoo.fr
> http://www.ucl.ac.uk/~ucbtghu/Sylvain.htm
> - Department of Biology, University College London
>   4 Stephenson Way, London, NW1 2HE, UK
> - Fieldwork address:
>   Gump Station, University of California Berkeley
>   BP 244 Maharepa, 98728 Moorea, French Polynesia
> - Phone (Moorea): (+689) 56 43 97 or 26 90 18 or 56 52 87
> - Fax (Moorea): (+689) 56 32 72
> - Phone / Fax (London): (+44)20 76795072 / +(44) 20 76795052
> ---------------------------------------------------------------
>
>
>
>
> 	
>
> 	
> 		
> ______________________________________________________________________ 
> _____
> Nouveau : t??l??phonez moins cher avec Yahoo! Messenger ! D??couvez  
> les tarifs exceptionnels pour appeler la France et l'international.
> T??l??chargez sur http://fr.messenger.yahoo.com
>
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac



From christian.hoffmann at wsl.ch  Thu Jan 26 08:51:54 2006
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 26 Jan 2006 08:51:54 +0100
Subject: [R] construct a bundle, subdirs do not exist?
In-Reply-To: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
Message-ID: <43D87F9A.2030303@wsl.ch>

Hi,

Sorry to bother, but I checked around and did not succed creating a 
bundle from six existing packages (which are checkable, installable, 
etc. individually). I carefully followed the procedure given in ch. 
1.1.5 Package bundles. However, I am getting

hoffmann at fluke:~/R/Sources >R CMD check cwhmisc
* checking for working latex ... OK
* using log directory '/home/woodstock/hoffmann/R/Sources/cwhmisc.Rcheck'
* using R version 2.2.1, 2005-12-20
* checking for file 'cwhmisc/DESCRIPTION' ... OK
* this is package '' version '1.0.0'
* checking if this is a source package ... OK

/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhmath,: does not exist
/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhstring,: does not exist
/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhstat,: does not exist
/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhplot,: does not exist
/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhprint,: does not exist
/usr/local/lib/R/bin/INSTALL: 
/home/woodstock/hoffmann/R/Sources/cwhmisc/cwhool: does not exist
/usr/local/lib/R/bin/INSTALL: null directory
  ERROR
Installation failed.

*But I have*

hoffmann at fluke:~/R/Sources/cwhmisc >ls -lFa
total 144
drwxr-xr-x   8 hoffmann wsl         1024 Jan 26 07:51 ./
drwxrwxrwx  26 hoffmann wsl         3072 Jan 26 08:29 ../
-rwxr--r--   1 hoffmann wsl          430 Jan 26 07:51 DESCRIPTION*
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:03 cwhmath/
drwxr-xr-x   5 hoffmann wsl         1024 Jan 26 07:04 cwhplot/
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhprint/
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhstat/
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhstring/
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:13 cwhtool/

i.e. the subdirectories are *present*!

And the packages with their

hoffmann at fluke:~/<1>Sources/cwhmisc/cwhmath >ls -lFa
total 96
drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:03 ./
drwxr-xr-x   8 hoffmann wsl         1024 Jan 26 07:51 ../
-rwxr--r--   1 hoffmann wsl          142 Jan 25 16:24 DESCRIPTION.in*
-rwxr--r--   1 hoffmann wsl         1121 Jan 20 14:58 INDEX*
drwxr-xr-x   3 hoffmann wsl         1024 Jan 25 16:21 R/
drwxr-xr-x   2 hoffmann wsl         1024 Jan 25 16:21 man/

hoffmann at fluke:~/<1>Sources/cwhmisc/cwhmath >cat DESCRIPTION.in
Package: CWHmath
Description: Miscellaneous mathematical functions for general use
Title: Miscellaneous mathematical functions for general use

are fully present.



Is this information enough to find the flaw in my bundle construction?

Thank for help
Christian

-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann

International Conference 5.-7.6.2006 Ekaterinburg Russia
"Climate changes and their impact on boreal and temperate forests"
http://ecoinf.uran.ru/conference/



From Bill.Venables at csiro.au  Thu Jan 26 08:56:50 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 26 Jan 2006 18:56:50 +1100
Subject: [R] D(dnorm...)?
Message-ID: <B998A44C8986644EA8029CFE6396A92454696F@exqld2-bne.qld.csiro.au>

While symbolic computation is handy, I actually think a more pressing
addition to R is some kind of automatic differentiation facility,
particularly 'reverse mode' AD, which can be spectacular.  There are
free tools available for it as well, though I don't know how well
developed they are.  See:

http://www-unix.mcs.anl.gov/autodiff/AD_Tools/

I admit this is not quite the same thing, but for statistical
computations this is, in my experience, the key thing you need.  (Well,
for frequentist estimation at any rate...)

There are commercial systems that use this idea already, of course.  Two
that I know of are 'ADMB' (and its associated 'ADMB-RE' for random
effects) estimation and of course the 'S-NUOPT' module for another
system not unlike R.

ADMB is, frankly, difficult to use but it performs so well and so
quickly once you get it going nothing else seems to come close to it.  I
has become almost a de-facto standard at the higher end of the fishery
stock assessment game, for example, where they are always fitting huge,
highly complex and very non-linear models.

Bill V.

-----Original Message-----
From: Berwin A Turlach [mailto:berwin at bossiaea.maths.uwa.edu.au] On
Behalf Of Berwin A Turlach
Sent: Thursday, 26 January 2006 4:50 PM
To: Spencer Graves
Cc: Venables, Bill (CMIS, Cleveland); r-help at stat.math.ethz.ch;
gunter.berton at gene.com; ripley at stats.ox.ac.uk
Subject: Re: [R] D(dnorm...)?


G'day Spencer,

>>>>> "SG" == Spencer Graves <spencer.graves at pdf.com> writes:

    SG> I'm not qualified to make this suggestion since I'm incapable
    SG> of turning it into reality, [...]
This statement applies to me too, nevertheless I would like to point
out the following GPL library:

        http://www.gnu.org/software/libmatheval/

I am wondering since some time how hard it would be to incorporate
that library into R and let it provide symbolic differentiation
capabilities for R.

Cheers,

        Berwin



From berwin at maths.uwa.edu.au  Thu Jan 26 07:50:16 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 26 Jan 2006 14:50:16 +0800
Subject: [R] D(dnorm...)?
In-Reply-To: <43D84C8F.5070306@pdf.com>
References: <B998A44C8986644EA8029CFE6396A924546966@exqld2-bne.qld.csiro.au>
	<43D84C8F.5070306@pdf.com>
Message-ID: <17368.28968.855149.110007@bossiaea.maths.uwa.edu.au>

G'day Spencer,

>>>>> "SG" == Spencer Graves <spencer.graves at pdf.com> writes:

    SG> I'm not qualified to make this suggestion since I'm incapable
    SG> of turning it into reality, [...]
This statement applies to me too, nevertheless I would like to point
out the following GPL library:

        http://www.gnu.org/software/libmatheval/

I am wondering since some time how hard it would be to incorporate
that library into R and let it provide symbolic differentiation
capabilities for R.

Cheers,

        Berwin



From petr.pikal at precheza.cz  Thu Jan 26 09:27:32 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 26 Jan 2006 09:27:32 +0100
Subject: [R] [R-SIG-Mac] Hist for different levels of a factor
In-Reply-To: <43E0958E-2BBA-46AD-803E-9472F8912B42@unimi.it>
References: <BFFD3997.5B04%s.charlat@ucl.ac.uk>
Message-ID: <43D89604.28051.999012@localhost>

Hi


On 26 Jan 2006 at 8:36, stefano iacus wrote:

From:           	stefano iacus <stefano.iacus at unimi.it>
Date sent:      	Thu, 26 Jan 2006 08:36:49 +0100
To:             	Sylvain Charlat <s.charlat at ucl.ac.uk>
Copies to:      	r-help at stat.math.ethz.ch, r-sig-mac <r-sig-mac at stat.math.ethz.ch>
Subject:        	Re: [R] [R-SIG-Mac] Hist for different levels of a factor

> The list of your interest is R-help not R-sig-mac
> stefano
> 
> Il giorno 26/gen/06, alle ore 01:20, Sylvain Charlat ha scritto:
> 
> > Hi,
> >
> > Is there any simple way to get histogram for different levels of 
> > factor?
> >
> > Say you have the following data set:
> >
> >  Island Sp.diam
> >  Moorea 1.21
> >  Moorea 1.27
> >  Moorea 1.28
> >  Moorea 1.22
> >  Moorea 1.28
> >  Rurutu 1.5
> >  Rurutu 1.67
> >  Rurutu 1.75
> >  Rurutu 1.55
> >  Rurutu 1.7
> >  Rurutu 1.55
> >  Rurutu 1.59
> >  Rurutu 1.66
> >  Rurutu 1.7
> >
> > Is there anything better than:
> >
> >> hist(Sp.diam[factor(Island)=="Moorea"],main="Island=Moorea")

It depends on how you want histograms to be plotted. 

e.g.
library(lattice)
histogram(~Sp.diam|Island)

is one possibility

HTH
Petr


> >
> > followed by
> >
> >> hist(Sp.diam[factor(Island)=="Rurutu"],main="Island=Rurutu")
> >
> > Sorry for this very stupid and basic question...
> >
> > Thanks for any help,
> >
> > Sylvain.
> >
> > ---------------------------------------------------------------
> > Sylvain Charlat s.charlat at ucl.ac.uk / sylvaincharlat at yahoo.fr
> > http://www.ucl.ac.uk/~ucbtghu/Sylvain.htm - Department of Biology,
> > University College London
> >   4 Stephenson Way, London, NW1 2HE, UK
> > - Fieldwork address:
> >   Gump Station, University of California Berkeley
> >   BP 244 Maharepa, 98728 Moorea, French Polynesia
> > - Phone (Moorea): (+689) 56 43 97 or 26 90 18 or 56 52 87
> > - Fax (Moorea): (+689) 56 32 72
> > - Phone / Fax (London): (+44)20 76795072 / +(44) 20 76795052
> > ---------------------------------------------------------------
> >
> >
> >
> >
> > 	
> >
> > 	
> > 		
> > ____________________________________________________________________
> > __ _____ Nouveau : t??l??phonez moins cher avec Yahoo! Messenger !
> > D??couvez  les tarifs exceptionnels pour appeler la France et
> > l'international. T??l??chargez sur http://fr.messenger.yahoo.com
> >
> > _______________________________________________
> > R-SIG-Mac mailing list
> > R-SIG-Mac at stat.math.ethz.ch
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mac
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From comtech.usa at gmail.com  Thu Jan 26 09:53:38 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 00:53:38 -0800
Subject: [R]  What's wrong with JGR?
Message-ID: <b1f16d9d0601260053j118932hd0733ba5410928ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/0c5bf451/attachment.pl

From comtech.usa at gmail.com  Thu Jan 26 10:11:23 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 01:11:23 -0800
Subject: [R]  What's wrong with Rcmdr?
Message-ID: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/6746f202/attachment.pl

From stecalza at tiscali.it  Thu Jan 26 10:23:12 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Thu, 26 Jan 2006 10:23:12 +0100
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
References: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
Message-ID: <20060126092312.GB4835@med.unibs.it>

Try using

Commander()

but, obviously, the first time.

HIH,
Ste

On Thu, Jan 26, 2006 at 01:11:23AM -0800, Michael wrote:
<Michael>Hi all,
<Michael>
<Michael>I successfully installed Rcmdr. And I type "library(Rcmdr)", nothing
<Michael>happened;
<Michael>
<Michael>or if I select menu item "load package" and select "Rcmdr", still nothing
<Michael>happened...
<Michael>
<Michael>Why didn't Rcmdr start?
<Michael>
<Michael>Very strangely, if I close the R console and restart R console, every first
<Michael>time I load "Rcmdr", it starts! But not second, third time...
<Michael>
<Michael>What's wrong with it?
<Michael>
<Michael>Thanks a lot!
<Michael>
<Michael>Michael.
<Michael>
<Michael>	[[alternative HTML version deleted]]
<Michael>
<Michael>______________________________________________
<Michael>R-help at stat.math.ethz.ch mailing list
<Michael>https://stat.ethz.ch/mailman/listinfo/r-help
<Michael>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Thu Jan 26 10:24:00 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Jan 2006 10:24:00 +0100
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
References: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
Message-ID: <43D89530.1010103@sciviews.org>

I am afraid that you must be a little bit more "verbose" on your 
problem. You do not provide enough information to spot your problem.
Best,

Philippe Grosjean

Michael wrote:
> Hi all,
> 
> I successfully installed Rcmdr. And I type "library(Rcmdr)", nothing
> happened;
> 
> or if I select menu item "load package" and select "Rcmdr", still nothing
> happened...
> 
> Why didn't Rcmdr start?
> 
> Very strangely, if I close the R console and restart R console, every first
> time I load "Rcmdr", it starts! But not second, third time...
> 
> What's wrong with it?
> 
> Thanks a lot!
> 
> Michael.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From comtech.usa at gmail.com  Thu Jan 26 10:32:01 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 01:32:01 -0800
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <20060126092312.GB4835@med.unibs.it>
References: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
	<20060126092312.GB4835@med.unibs.it>
Message-ID: <b1f16d9d0601260132l5c92fba7p200205958ae9acd4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/033368ee/attachment.pl

From comtech.usa at gmail.com  Thu Jan 26 10:47:11 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 01:47:11 -0800
Subject: [R]  Re: reducing learning curves?
In-Reply-To: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
Message-ID: <b1f16d9d0601260147s65c95eacjd5637b38af2ff327@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/5759ca29/attachment.pl

From stecalza at tiscali.it  Thu Jan 26 10:50:58 2006
From: stecalza at tiscali.it (Stefano Calza)
Date: Thu, 26 Jan 2006 10:50:58 +0100
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <b1f16d9d0601260132l5c92fba7p200205958ae9acd4@mail.gmail.com>
References: <b1f16d9d0601260111n918d0eet38fb9a42c2468419@mail.gmail.com>
	<20060126092312.GB4835@med.unibs.it>
	<b1f16d9d0601260132l5c92fba7p200205958ae9acd4@mail.gmail.com>
Message-ID: <20060126095058.GC4835@med.unibs.it>

Ok, let's see:

1) library(Rcmdr) -> RCommander should start (you say it does)
2) Close R Commander but NOT R.
3) To restart RCommander use Commander()

Is this what you are looking for?

HIH,
Ste



On Thu, Jan 26, 2006 at 01:32:01AM -0800, Michael wrote:
<Michael>> Commander()
<Michael>Error: couldn't find function "Commander"
<Michael>
<Michael>
<Michael>On 1/26/06, Stefano Calza <stecalza at tiscali.it> wrote:
<Michael>>
<Michael>> Try using
<Michael>>
<Michael>> Commander()
<Michael>>
<Michael>> but, obviously, the first time.
<Michael>>
<Michael>> HIH,
<Michael>> Ste
<Michael>>
<Michael>> On Thu, Jan 26, 2006 at 01:11:23AM -0800, Michael wrote:
<Michael>> <Michael>Hi all,
<Michael>> <Michael>
<Michael>> <Michael>I successfully installed Rcmdr. And I type "library(Rcmdr)",
<Michael>> nothing
<Michael>> <Michael>happened;
<Michael>> <Michael>
<Michael>> <Michael>or if I select menu item "load package" and select "Rcmdr", still
<Michael>> nothing
<Michael>> <Michael>happened...
<Michael>> <Michael>
<Michael>> <Michael>Why didn't Rcmdr start?
<Michael>> <Michael>
<Michael>> <Michael>Very strangely, if I close the R console and restart R console,
<Michael>> every first
<Michael>> <Michael>time I load "Rcmdr", it starts! But not second, third time...
<Michael>> <Michael>
<Michael>> <Michael>What's wrong with it?
<Michael>> <Michael>
<Michael>> <Michael>Thanks a lot!
<Michael>> <Michael>
<Michael>> <Michael>Michael.
<Michael>> <Michael>
<Michael>> <Michael>       [[alternative HTML version deleted]]
<Michael>> <Michael>
<Michael>> <Michael>______________________________________________
<Michael>> <Michael>R-help at stat.math.ethz.ch mailing list
<Michael>> <Michael>https://stat.ethz.ch/mailman/listinfo/r-help
<Michael>> <Michael>PLEASE do read the posting guide!
<Michael>> http://www.R-project.org/posting-guide.html
<Michael>>
<Michael>> ______________________________________________
<Michael>> R-help at stat.math.ethz.ch mailing list
<Michael>> https://stat.ethz.ch/mailman/listinfo/r-help
<Michael>> PLEASE do read the posting guide!
<Michael>> http://www.R-project.org/posting-guide.html
<Michael>>
<Michael>
<Michael>	[[alternative HTML version deleted]]
<Michael>
<Michael>______________________________________________
<Michael>R-help at stat.math.ethz.ch mailing list
<Michael>https://stat.ethz.ch/mailman/listinfo/r-help
<Michael>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sw283 at maths.bath.ac.uk  Thu Jan 26 11:15:10 2006
From: sw283 at maths.bath.ac.uk (Simon Wood)
Date: Thu, 26 Jan 2006 10:15:10 +0000 (GMT)
Subject: [R] gam
In-Reply-To: <web-14929175@mail3.rug.nl>
References: <web-14929175@mail3.rug.nl>
Message-ID: <Pine.LNX.4.56.0601260957150.2641@thoth.maths.bath.ac.uk>

> I'm new to both R and to this list and would like to get
> advice on how to build generalized additive models in R.
> Based on the description of gam, which I found on the R
> website, I specified the following model:
> model1<-gam(ST~s(MOWST1),family=binomial,data=strikes.S),
> in which ST is my binary response variable and MOWST1 is a
> categorical independent variable.
>
> I get the following error message:
> Error in smooth.construct.tp.smooth.spec(object, data,
> knots) :
>          NA/NaN/Inf in foreign function call (arg 1)

- I guess this should maybe get trapped a bit earlier, so that you get
a more informative warning.

- The basic problem is that gams are based around sums of smooth functions
of covariates. For the notion of smooth to be meaningful the covariates
have to live in a space where you have at least a notion of distance
between the covariates, since in some loose sense `smooth' means that
f(x_1) must be close to f(x_2) if x_1 and x_2 are close. For factors you
doen't generally have any notion of distance between the levels of a
factor. (e.g. if a factor has levels "brick", "sky" and "purple", how far
is it from "brick" to "purple"?)

- Even if a factor is naturally ordered (e.g. "small", "medium", "large"),
you would still have to decide on how to measure smoothness/wiggliness of
a function of the factor. For this reason, I think that it is actually
better to explicitly convert levels of an ordered factor into numeric
values on a scale that you think is appropriate, before using the ordered
factor as the covariate in a gam. In this way it's usually fairly easy to
get one of the mgcv built in smoother classes to use the notion of
smoothness that you think is appropriate: if not then it's not too hard to
add a smoother class, following the template provided in ?p.spline
(actually you could use this template to write a smoother class for
ordered catagorical predictors).

best,
Simon



From avilella at gmail.com  Thu Jan 26 11:13:34 2006
From: avilella at gmail.com (Albert Vilella)
Date: Thu, 26 Jan 2006 11:13:34 +0100
Subject: [R] list entries file into a list
Message-ID: <1138270414.13153.3.camel@localhost.localdomain>

Hi all,

I have a file of this kind:

entry0001:AB0032,CF32134,DF34334
entry0002:AB0033
entry0003:AB0032,CF32134,DF34334,DD343434,DD34222
entry0004:AB0032,CF32134

And I would like to read it into something like a hash, so that I can
then loop over it by keys and values.

I wonder which would be the best way to do that in R?

Thanks,

    Albert.



From phgrosjean at sciviews.org  Thu Jan 26 11:15:08 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Jan 2006 11:15:08 +0100
Subject: [R] reducing learning curves?
In-Reply-To: <b1f16d9d0601260147s65c95eacjd5637b38af2ff327@mail.gmail.com>
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
	<b1f16d9d0601260147s65c95eacjd5637b38af2ff327@mail.gmail.com>
Message-ID: <43D8A12C.3010903@sciviews.org>

Michael wrote:
> Hi all,
> 
> Are there any R addon/pluggins with the following feature:
> 
> (1) command history? even stores the command history many days ago? Like
> Matlab does?

See ?history, and also, menu entries File -> Load/Save history... in 
Rgui (you use R under windows, isn't it?). also, if you answer "Yes" at 
the question "Save workspace image?" when you quit R, the history of 
commands is saved and resored next time you start R from the same 
directory.

> (2) online help? for example, as I see it, the Rcmdr is a good companian for
> a newbie like me who just touched R for 1.5 days. I can use Rcmdr to guide
> me to learn the commands to use for data analysis. However, Rcmdr does not
> have online help reference for R commands. For example, I saw the command
> "abline" and I want to know how to use it, I have to copy/type it in
> R-console, and do "?abline" things, so that I can obtain help. But this is
> troublesome, does any editor offer online-help for commands, so I just need
> to hover my mouse on "abline" and then press F1 key then a help window will
> open automatically?

Well, it is not much work to type

 > ?abline

However, if you want a button that you can click to do so, then look at 
http://www.sciviews.org/Tinn-R. That editor is definitely for you!
Best,

Philippe Grosjean

> Thanks a lot!
> 
> On 1/25/06, Michael <comtech.usa at gmail.com> wrote:
> 
>>Hi all,
>>
>>I am really new to the R language. I am a long time Matlab and C++ user
>>and I was "forced" to learn R because I am taking a statistics class.
>>
>>I am seeking to reduce the learning curve to as smooth as possible.
>>
>>Are there any addon/plug-in features that can reduce the learning curve,
>>for example, the following features can be very helpful for new learners:
>>
>>1. Matlab-like command line auto-completion: Matlab has huge amount of
>>command and nobody is able to remember them off the head. So a nice feature
>>of Matlab command line is that I just need to type the first a few letters
>>and then I press "TAB" key, there will be a list of possible commands
>>popping up so I just need to select one. This helps a lot in terms of
>>learning for new comers. A more advanced command auto-completion is Visual
>>C++-like, which is implemented in program editor. It helps a lot while doing
>>programming;
>>
>>2. A good IDE editor with embedded inline debugger: can be as good as
>>VC++, but also can be as simple as Matlab's debugger, which can breakpoint
>>and trace line-by-line... the editor can do syntax correction, syntax check,
>>syntax highlighting, code formatting, etc.
>>
>>Could you please recommend some good addon/plugins that have the above
>>features?
>>
>>Could you please also suggest some tips/tools/tricks that can help me
>>reduce the learning curve?
>>
>>Thank you very much!
>>
>>Michael.
>>
>>
>>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ozric at web.de  Thu Jan 26 11:38:44 2006
From: ozric at web.de (Christian Schulz)
Date: Thu, 26 Jan 2006 11:38:44 +0100
Subject: [R] What's wrong with JGR?
In-Reply-To: <b1f16d9d0601260053j118932hd0733ba5410928ca@mail.gmail.com>
References: <b1f16d9d0601260053j118932hd0733ba5410928ca@mail.gmail.com>
Message-ID: <43D8A6B4.9000202@web.de>

Which R version and which os is yours?

For me it works fine.

Lade n??tiges Paket: grDevices
using Java Runtime version 1.5.0
using JAVA_HOME = C:\develope\jre
Lade n??tiges Paket: rJava
Lade n??tiges Paket: methods
using Java Runtime version 1.5.0
using JAVA_HOME = C:\develope\jre
 >

 > R.Version()
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "2.1"

$year
[1] "2005"

$month
[1] "12"

$day
[1] "20"

$"svn rev"
[1] "36812"

$language
[1] "R"



>Hi all,
>
>I downloaded and installed JGR. Then when I tried to load the package in R
>console, it generated the following error message:
>
>  
>
>>local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>>    
>>
>+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
>Loading required package: rJava
>using Java Runtime version 1.5.0
>using JAVA_HOME = C:\Program Files\Java\jre1.5.0_01
>Loading required package: JavaGD
>using Java Runtime version 1.5.0
>using JAVA_HOME = C:\Program Files\Java\jre1.5.0_01
>Error in .jcall("org/rosuda/JGR/JGR", "V", "setRHome", as.character(R.home()))
>:
>        RcallStaticMethod: cannot find specified class
>Error in library(pkg, character.only = TRUE) :
>        .First.lib failed for 'JGR'
>
>
>What's wrong?
>
>Thanks a lot!
>
>Michael.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From csardi at rmki.kfki.hu  Thu Jan 26 11:39:53 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 26 Jan 2006 11:39:53 +0100
Subject: [R] reducing learning curves?
In-Reply-To: <43D8A12C.3010903@sciviews.org>
References: <b1f16d9d0601250009g17b9f743gcd41fb3f50f997f8@mail.gmail.com>
	<b1f16d9d0601260147s65c95eacjd5637b38af2ff327@mail.gmail.com>
	<43D8A12C.3010903@sciviews.org>
Message-ID: <20060126103953.GC31597@rmki.kfki.hu>

> > On 1/25/06, Michael <comtech.usa at gmail.com> wrote:
> > 
> >>Hi all,
> >>
> >>I am really new to the R language. I am a long time Matlab and C++ user
> >>and I was "forced" to learn R because I am taking a statistics class.
> >>
> >>I am seeking to reduce the learning curve to as smooth as possible.
> >>
> >>Are there any addon/plug-in features that can reduce the learning curve,
> >>for example, the following features can be very helpful for new learners:
> >>
> >>1. Matlab-like command line auto-completion: Matlab has huge amount of
> >>command and nobody is able to remember them off the head. So a nice feature

You can use command name auto-completion if you use emacs and the ess mode
for running R. (I don't whether this was mentioned earlier (not following
the thread closely), sorry if it was.)

> >>2. A good IDE editor with embedded inline debugger: can be as good as
> >>VC++, but also can be as simple as Matlab's debugger, which can breakpoint
> >>and trace line-by-line... the editor can do syntax correction, syntax check,
> >>syntax highlighting, code formatting, etc.

emacs and ess mode does that for you as well, i'm not sure about the
debugger, but syntax highlight and indentation work great. 

For debugging try ?debug. 

> >>Could you please recommend some good addon/plugins that have the above
> >>features?
> >>
> >>Could you please also suggest some tips/tools/tricks that can help me
> >>reduce the learning curve?

Reading 'An Introduction to R' helps a lot. Not about editors though,
and this is right i think. Personally i hate ide's with built in editors,
because you have to learn all of them, and they behave just a bit
differently to annoy you. Using a single editor for all your editing tasks
(writing mail, html, R, C++ and matlab code) on the other hand can greatly
increase productivity. Just my 2 cents.

Gabor

> >>Thank you very much!
> >>
> >>Michael.
> >>
> >>
> >>
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From finbref.2006 at gmail.com  Thu Jan 26 12:03:53 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 26 Jan 2006 12:03:53 +0100
Subject: [R] persp() and character labels for axis
In-Reply-To: <43D773D0.5060702@statistik.uni-dortmund.de>
References: <d0f55a670601250259g13479682r@mail.gmail.com>
	<43D7676C.6050404@statistik.uni-dortmund.de>
	<d0f55a670601250443y498eb40cy@mail.gmail.com>
	<43D773D0.5060702@statistik.uni-dortmund.de>
Message-ID: <d0f55a670601260303y2ad2c07ej@mail.gmail.com>

I upgraded and it still does not work.
I want something like this for 3d-persp() plots:

days=c("2006-01-23","2006-01-24","2006-01-25","2006-01-26","2006-01-27","2006-01-28")
sq=(1:6)^2
plot(x=as.Date(days, format="%Y-%m-%d"),y=sq, type="l", main="What I
learn about R", sub=R.version.string)

sq3d=matrix(nrow = 4, ncol = 6)
for (i in 1:4) {
  sq3d[i,]=sq+i
}
persp(x=1:4,y=1:6,z=sq3d,theta = 30, phi = 30, expand = 0.5, col = "lightblue")

(but y=days....)

Thomas



From dieter.menne at menne-biomed.de  Thu Jan 26 12:28:55 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 26 Jan 2006 12:28:55 +0100
Subject: [R] How to make two side-by side trellis plots same size
Message-ID: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>

Dear Latticers,

I want to position two trellis plots of different forms 
side-by-side. The plot types are slightly different, 
aspect=1 required, but panels should look the same in 
both plots. Current workaround uses a guessed factor. 
Any way to improve this?

Dieter


library(lattice)
n1 = 20
# I cannot rbind df1 and df2, because the x-dimensions are
# different and must be scaled individually
df1 = data.frame(y=rnorm(4*n1),x=rep(1:n1,4),
   facA=rep(c("A","B"),each=2*n1),facB=rep(c("C","D"),each=n1))

# I add a dummy facB here, to make sure panels have same structure
df2 = data.frame(y=rnorm(2*n1),z=rep(100*(1:n1),2),
   facA=rep(c("A","B"),each=n1),facB="C")

# Note: aspect = 1 is required
p1 = xyplot(y~x|facA*facB,data=df1,main="Plot1",aspect=1,
  between=list(x=2))
p2 = xyplot(y~z|facA*facB,data=df2,main="Plot2",aspect=1,layout=c(1,2))

wi = 0.61 # this is trial-and-error
print(p1,position=c(0,0,wi,1),more=T)
print(p2,position=c(wi,0,1,1),more=F)



From amurta at ipimar.pt  Thu Jan 26 14:07:47 2006
From: amurta at ipimar.pt (Alberto Murta)
Date: Thu, 26 Jan 2006 13:07:47 +0000
Subject: [R] Automatic differentiation (was: Re:  D(dnorm...)?)
In-Reply-To: <B998A44C8986644EA8029CFE6396A92454696F@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A92454696F@exqld2-bne.qld.csiro.au>
Message-ID: <200601261307.47934.amurta@ipimar.pt>

On Thursday 26 January 2006 07:56, Bill.Venables at csiro.au wrote:
> While symbolic computation is handy, I actually think a more pressing
> addition to R is some kind of automatic differentiation facility,
> particularly 'reverse mode' AD, which can be spectacular.  There are
> free tools available for it as well, though I don't know how well
> developed they are.  See:
>
> http://www-unix.mcs.anl.gov/autodiff/AD_Tools/
>
> I admit this is not quite the same thing, but for statistical
> computations this is, in my experience, the key thing you need.  (Well,
> for frequentist estimation at any rate...)
>
> There are commercial systems that use this idea already, of course.  Two
> that I know of are 'ADMB' (and its associated 'ADMB-RE' for random
> effects) estimation and of course the 'S-NUOPT' module for another
> system not unlike R.
>
> ADMB is, frankly, difficult to use but it performs so well and so
> quickly once you get it going nothing else seems to come close to it.  I
> has become almost a de-facto standard at the higher end of the fishery
> stock assessment game, for example, where they are always fitting huge,
> highly complex and very non-linear models.
>
> Bill V.
>

I think AD Model Builder is mainly used for fisheries assessment in North 
America and, it seems, also in Australia. In Europe, R is still the de-facto 
standard for fisheries assessment. However, I'd like to support Bill 
Venables' suggestion. I've been resisting to adopt AD model builder, or to 
start using again that other system not unlike R, mainly because of the 
licence price and because I really like R as a tool for almost everything. 
But an AD function would really make a huge difference for my work. There are 
free tools that can be used to perform AD on C or Fortran code (e.g. 
http://www.autodiff.org). One of the difficulties to use them with R is the 
need to translate the R code into C of Fortran code, but probably there are 
many other problems that I'm not able to see.

Alberto
-- 
  Alberto G. Murta <amurta at ipimar.pt>
National Institute of Agriculture and Fisheries Research
Avenida de Brasilia 1449-006 Lisboa Portugal
Tel: +351 213027120 | Fax: +351 213015948



From ripley at stats.ox.ac.uk  Thu Jan 26 13:06:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jan 2006 12:06:40 +0000 (GMT)
Subject: [R] construct a bundle, subdirs do not exist?
In-Reply-To: <43D87F9A.2030303@wsl.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<43D87F9A.2030303@wsl.ch>
Message-ID: <Pine.LNX.4.61.0601261204010.11226@gannet.stats>

Notice the trailing commas.

The Contains: field is space-delimited (see the example) and I surmise you 
used commas.

You also seem to have a typo:  'cwhool' not 'cwhtool'.

On Thu, 26 Jan 2006, Christian Hoffmann wrote:

> Hi,
>
> Sorry to bother, but I checked around and did not succed creating a
> bundle from six existing packages (which are checkable, installable,
> etc. individually). I carefully followed the procedure given in ch.
> 1.1.5 Package bundles. However, I am getting
>
> hoffmann at fluke:~/R/Sources >R CMD check cwhmisc
> * checking for working latex ... OK
> * using log directory '/home/woodstock/hoffmann/R/Sources/cwhmisc.Rcheck'
> * using R version 2.2.1, 2005-12-20
> * checking for file 'cwhmisc/DESCRIPTION' ... OK
> * this is package '' version '1.0.0'
> * checking if this is a source package ... OK
>
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhmath,: does not exist
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhstring,: does not exist
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhstat,: does not exist
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhplot,: does not exist
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhprint,: does not exist
> /usr/local/lib/R/bin/INSTALL:
> /home/woodstock/hoffmann/R/Sources/cwhmisc/cwhool: does not exist
> /usr/local/lib/R/bin/INSTALL: null directory
>  ERROR
> Installation failed.
>
> *But I have*
>
> hoffmann at fluke:~/R/Sources/cwhmisc >ls -lFa
> total 144
> drwxr-xr-x   8 hoffmann wsl         1024 Jan 26 07:51 ./
> drwxrwxrwx  26 hoffmann wsl         3072 Jan 26 08:29 ../
> -rwxr--r--   1 hoffmann wsl          430 Jan 26 07:51 DESCRIPTION*
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:03 cwhmath/
> drwxr-xr-x   5 hoffmann wsl         1024 Jan 26 07:04 cwhplot/
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhprint/
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhstat/
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:04 cwhstring/
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:13 cwhtool/
>
> i.e. the subdirectories are *present*!
>
> And the packages with their
>
> hoffmann at fluke:~/<1>Sources/cwhmisc/cwhmath >ls -lFa
> total 96
> drwxr-xr-x   4 hoffmann wsl         1024 Jan 26 07:03 ./
> drwxr-xr-x   8 hoffmann wsl         1024 Jan 26 07:51 ../
> -rwxr--r--   1 hoffmann wsl          142 Jan 25 16:24 DESCRIPTION.in*
> -rwxr--r--   1 hoffmann wsl         1121 Jan 20 14:58 INDEX*
> drwxr-xr-x   3 hoffmann wsl         1024 Jan 25 16:21 R/
> drwxr-xr-x   2 hoffmann wsl         1024 Jan 25 16:21 man/
>
> hoffmann at fluke:~/<1>Sources/cwhmisc/cwhmath >cat DESCRIPTION.in
> Package: CWHmath
> Description: Miscellaneous mathematical functions for general use
> Title: Miscellaneous mathematical functions for general use
>
> are fully present.
>
>
>
> Is this information enough to find the flaw in my bundle construction?
>
> Thank for help
> Christian
>
> -- 
> Dr. Christian W. Hoffmann,
> Swiss Federal Research Institute WSL
> Mathematics + Statistical Computing
> Zuercherstrasse 111
> CH-8903 Birmensdorf, Switzerland
>
> Tel +41-44-7392-277  (office)   -111(exchange)
> Fax +41-44-7392-215  (fax)
> christian.hoffmann at wsl.ch
> http://www.wsl.ch/staff/christian.hoffmann
>
> International Conference 5.-7.6.2006 Ekaterinburg Russia
> "Climate changes and their impact on boreal and temperate forests"
> http://ecoinf.uran.ru/conference/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From finbref.2006 at gmail.com  Thu Jan 26 13:53:20 2006
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Thu, 26 Jan 2006 13:53:20 +0100
Subject: [R] persp() and character labels for axis
In-Reply-To: <d0f55a670601260303y2ad2c07ej@mail.gmail.com>
References: <d0f55a670601250259g13479682r@mail.gmail.com>
	<43D7676C.6050404@statistik.uni-dortmund.de>
	<d0f55a670601250443y498eb40cy@mail.gmail.com>
	<43D773D0.5060702@statistik.uni-dortmund.de>
	<d0f55a670601260303y2ad2c07ej@mail.gmail.com>
Message-ID: <d0f55a670601260453m73ab824p@mail.gmail.com>

> persp(x=1:4,y=1:6,z=sq3d,theta = 30, phi = 30, expand = 0.5, col = "lightblue")
>
> (but y=days....)

of course it should be something like
persp(y=as.Date(days, format="%Y-%m-%d"),x=1:4,z=sq3d, theta=30,
phi=30, expand=0.5, ticktype="detailed", col="seagreen")

But this does not what I expected (after my experience with plot
above). ticktype="detailed" is better, but by far not the solution.
How do I add date-lables to the x-Axis of a persp() plot?
Who can help? Thanks...
Thomas



From petr.pikal at precheza.cz  Thu Jan 26 14:02:00 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 26 Jan 2006 14:02:00 +0100
Subject: [R] reducing learning curves?
In-Reply-To: <43D8A12C.3010903@sciviews.org>
References: <b1f16d9d0601260147s65c95eacjd5637b38af2ff327@mail.gmail.com>
Message-ID: <43D8D658.26138.194E244@localhost>

Hi

On 26 Jan 2006 at 11:15, Philippe Grosjean wrote:

Date sent:      	Thu, 26 Jan 2006 11:15:08 +0100
From:           	Philippe Grosjean <phgrosjean at sciviews.org>
Organization:   	SciViews & UMH - EcoNum
To:             	Michael <comtech.usa at gmail.com>
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] reducing learning curves?

> Michael wrote:
> > Hi all,
> > 
> > Are there any R addon/pluggins with the following feature:
> > 
> > (1) command history? even stores the command history many days ago?
> > Like Matlab does?
> 
> See ?history, and also, menu entries File -> Load/Save history... in
> Rgui (you use R under windows, isn't it?). also, if you answer "Yes"
> at the question "Save workspace image?" when you quit R, the history
> of commands is saved and resored next time you start R from the same
> directory.

Beware that .Rhistory file has not unlimited number of commands, you 
can see only about 500 lines in default setting, however you can 
modify it.

> 
> > (2) online help? for example, as I see it, the Rcmdr is a good
> > companian for a newbie like me who just touched R for 1.5 days. I
> > can use Rcmdr to guide me to learn the commands to use for data
> > analysis. However, Rcmdr does not have online help reference for R
> > commands. For example, I saw the command "abline" and I want to know
> > how to use it, I have to copy/type it in R-console, and do "?abline"
> > things, so that I can obtain help. But this is troublesome, does any
> > editor offer online-help for commands, so I just need to hover my
> > mouse on "abline" and then press F1 key then a help window will open
> > automatically?

Beware of mouse elbow when perusing mouse too much.
If you want interactive help just open HTML help pages with your 
favourite browser.

Cheers
Petr

PS. It is good to have a copy of Paul Johnson's Rtips somewhere near 
when you start. Basic stuff and howtodo's is covered there.

> 
> Well, it is not much work to type
> 
>  > ?abline
> 
> However, if you want a button that you can click to do so, then look
> at http://www.sciviews.org/Tinn-R. That editor is definitely for you!
> Best,
> 
> Philippe Grosjean
> 
> > Thanks a lot!
> > 
> > On 1/25/06, Michael <comtech.usa at gmail.com> wrote:
> > 
> >>Hi all,
> >>
> >>I am really new to the R language. I am a long time Matlab and C++
> >>user and I was "forced" to learn R because I am taking a statistics
> >>class.
> >>
> >>I am seeking to reduce the learning curve to as smooth as possible.
> >>
> >>Are there any addon/plug-in features that can reduce the learning
> >>curve, for example, the following features can be very helpful for
> >>new learners:
> >>
> >>1. Matlab-like command line auto-completion: Matlab has huge amount
> >>of command and nobody is able to remember them off the head. So a
> >>nice feature of Matlab command line is that I just need to type the
> >>first a few letters and then I press "TAB" key, there will be a list
> >>of possible commands popping up so I just need to select one. This
> >>helps a lot in terms of learning for new comers. A more advanced
> >>command auto-completion is Visual C++-like, which is implemented in
> >>program editor. It helps a lot while doing programming;
> >>
> >>2. A good IDE editor with embedded inline debugger: can be as good
> >>as VC++, but also can be as simple as Matlab's debugger, which can
> >>breakpoint and trace line-by-line... the editor can do syntax
> >>correction, syntax check, syntax highlighting, code formatting, etc.
> >>
> >>Could you please recommend some good addon/plugins that have the
> >>above features?
> >>
> >>Could you please also suggest some tips/tools/tricks that can help
> >>me reduce the learning curve?
> >>
> >>Thank you very much!
> >>
> >>Michael.
> >>
> >>
> >>
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Thu Jan 26 14:14:41 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jan 2006 08:14:41 -0500
Subject: [R] list entries file into a list
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED761@usctmx1106.merck.com>

The following might be what you want (replace "clipboard" with your
filename):

> mylist <- strsplit(readLines("clipboard"), ":")
> nm <- sapply(mylist, "[", 1)
> mylist <- lapply(mylist, "[", -1)
> names(mylist) <- nm
> mylist <- lapply(mylist, function(s) strsplit(s, ",")[[1]])
> mylist
$entry0001
[1] "AB0032"  "CF32134" "DF34334"

$entry0002
[1] "AB0033"

$entry0003
[1] "AB0032"   "CF32134"  "DF34334"  "DD343434" "DD34222" 

$entry0004
[1] "AB0032"  "CF32134"

Andy

From: Albert Vilella
> 
> Hi all,
> 
> I have a file of this kind:
> 
> entry0001:AB0032,CF32134,DF34334
> entry0002:AB0033
> entry0003:AB0032,CF32134,DF34334,DD343434,DD34222
> entry0004:AB0032,CF32134
> 
> And I would like to read it into something like a hash, so that I can
> then loop over it by keys and values.
> 
> I wonder which would be the best way to do that in R?
> 
> Thanks,
> 
>     Albert.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dimitrijoe at ipea.gov.br  Thu Jan 26 14:19:22 2006
From: dimitrijoe at ipea.gov.br (dimitrijoe@ipea.gov.br)
Date: Thu, 26 Jan 2006 11:19:22 -0200
Subject: [R] efficiency with "%*%"
Message-ID: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>

Hi,

x and y are (numeric) vectors. I wonder if one of the following is more
efficient than the other:

x%*%y

or

sum(x*y)
?

Thanks,
Dimitri Szerman



From ligges at statistik.uni-dortmund.de  Thu Jan 26 14:34:25 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jan 2006 14:34:25 +0100
Subject: [R] efficiency with "%*%"
In-Reply-To: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>
References: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>
Message-ID: <43D8CFE1.9030800@statistik.uni-dortmund.de>

dimitrijoe at ipea.gov.br wrote:

> Hi,
> 
> x and y are (numeric) vectors. I wonder if one of the following is more
> efficient than the other:
> 
> x%*%y
> 
> or
> 
> sum(x*y)
> ?

I'd try

   x <- rnorm(1000000)
   y <- rnorm(1000000)
   system.time(x%*%y)
   system.time(sum(x*y))

and finally (hint, hint!):

   system.time(crossprod(x, y))

Uwe Ligges


> Thanks,
> Dimitri Szerman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From roger.bos at gmail.com  Thu Jan 26 15:02:59 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 26 Jan 2006 09:02:59 -0500
Subject: [R] maximizing available memory under windows XP
Message-ID: <1db726800601260602s7b083e01o8cc2b7da547e8183@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/62e26839/attachment.pl

From phgrosjean at sciviews.org  Thu Jan 26 15:40:38 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Jan 2006 15:40:38 +0100
Subject: [R] efficiency with "%*%"
In-Reply-To: <43D8CFE1.9030800@statistik.uni-dortmund.de>
References: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>
	<43D8CFE1.9030800@statistik.uni-dortmund.de>
Message-ID: <43D8DF66.8020604@sciviews.org>

Excellent, but...

 > x <- rnorm(1000000)
 > y <- rnorm(1000000)
 > system.time(x%*%y)
[1] 0.03 0.00 0.03   NA   NA
 > system.time(sum(x*y))
[1] 0.05 0.00 0.04   NA   NA
 > system.time(crossprod(x, y))
[1]  0  0  0 NA NA

So, to paraphrase a well-known contributor on this mailing list:
"Excellent! So, what did you decided to do during the next 30 
milliseconds you will save by using crossprod() instead of x%*%y?
(joke)

Best,

Philippe Grosjean

P.S.: Uwe, perhaps you should consider buying a faster computer, isn't 
it? :-()

Uwe Ligges wrote:
> dimitrijoe at ipea.gov.br wrote:
> 
> 
>>Hi,
>>
>>x and y are (numeric) vectors. I wonder if one of the following is more
>>efficient than the other:
>>
>>x%*%y
>>
>>or
>>
>>sum(x*y)
>>?
> 
> 
> I'd try
> 
>    x <- rnorm(1000000)
>    y <- rnorm(1000000)
>    system.time(x%*%y)
>    system.time(sum(x*y))
> 
> and finally (hint, hint!):
> 
>    system.time(crossprod(x, y))
> 
> Uwe Ligges
> 
> 
> 
>>Thanks,
>>Dimitri Szerman
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jan 26 15:55:55 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jan 2006 15:55:55 +0100
Subject: [R] efficiency with "%*%"
In-Reply-To: <43D8DF66.8020604@sciviews.org>
References: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>
	<43D8CFE1.9030800@statistik.uni-dortmund.de>
	<43D8DF66.8020604@sciviews.org>
Message-ID: <43D8E2FB.3020409@statistik.uni-dortmund.de>

Philippe Grosjean wrote:

> Excellent, but...
> 
>  > x <- rnorm(1000000)
>  > y <- rnorm(1000000)
>  > system.time(x%*%y)
> [1] 0.03 0.00 0.03   NA   NA
>  > system.time(sum(x*y))
> [1] 0.05 0.00 0.04   NA   NA
>  > system.time(crossprod(x, y))
> [1]  0  0  0 NA NA
> 
> So, to paraphrase a well-known contributor on this mailing list:
> "Excellent! So, what did you decided to do during the next 30 
> milliseconds you will save by using crossprod() instead of x%*%y?
> (joke)
> 
> Best,
> 
> Philippe Grosjean
> 
> P.S.: Uwe, perhaps you should consider buying a faster computer, isn't 
> it? :-()

Well, I use R, you know. It is even fast enough for my 5 year old 
laptop. For your super computer, please replace 1e6 by 1e8 in the 
example above. ;-)

Uwe


> Uwe Ligges wrote:
> 
>> dimitrijoe at ipea.gov.br wrote:
>>
>>
>>> Hi,
>>>
>>> x and y are (numeric) vectors. I wonder if one of the following is more
>>> efficient than the other:
>>>
>>> x%*%y
>>>
>>> or
>>>
>>> sum(x*y)
>>> ?
>>
>>
>>
>> I'd try
>>
>>    x <- rnorm(1000000)
>>    y <- rnorm(1000000)
>>    system.time(x%*%y)
>>    system.time(sum(x*y))
>>
>> and finally (hint, hint!):
>>
>>    system.time(crossprod(x, y))
>>
>> Uwe Ligges
>>
>>
>>
>>> Thanks,
>>> Dimitri Szerman
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>



From deepayan.sarkar at gmail.com  Thu Jan 26 16:07:12 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 26 Jan 2006 09:07:12 -0600
Subject: [R] How to make two side-by side trellis plots same size
In-Reply-To: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>
Message-ID: <eb555e660601260707g1c78be6cp195c047dcd08cea@mail.gmail.com>

On 1/26/06, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Dear Latticers,
>
> I want to position two trellis plots of different forms
> side-by-side. The plot types are slightly different,
> aspect=1 required, but panels should look the same in
> both plots. Current workaround uses a guessed factor.
> Any way to improve this?
>
> Dieter
>
>
> library(lattice)
> n1 = 20
> # I cannot rbind df1 and df2, because the x-dimensions are
> # different and must be scaled individually

If that's the only reason, I would suggest rbind-ing them and then use

scales = list(x = "free")

If you want the first two columns to have the same x-limits, you can specify

xlim = list(c(0,25), c(0,25), c(0, 2500))

etc. Otherwise, if you want the panels to have the same physical
dimensions, look at the panel.width and panel.height arguments in
?print.trellis. I can't think of any other way of guaranteeing it.

Deepayan

> df1 = data.frame(y=rnorm(4*n1),x=rep(1:n1,4),
>    facA=rep(c("A","B"),each=2*n1),facB=rep(c("C","D"),each=n1))
>
> # I add a dummy facB here, to make sure panels have same structure
> df2 = data.frame(y=rnorm(2*n1),z=rep(100*(1:n1),2),
>    facA=rep(c("A","B"),each=n1),facB="C")
>
> # Note: aspect = 1 is required
> p1 = xyplot(y~x|facA*facB,data=df1,main="Plot1",aspect=1,
>   between=list(x=2))
> p2 = xyplot(y~z|facA*facB,data=df2,main="Plot2",aspect=1,layout=c(1,2))
>
> wi = 0.61 # this is trial-and-error
> print(p1,position=c(0,0,wi,1),more=T)
> print(p2,position=c(wi,0,1,1),more=F)



From jfox at mcmaster.ca  Thu Jan 26 16:20:25 2006
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 26 Jan 2006 10:20:25 -0500
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <20060126092312.GB4835@med.unibs.it>
Message-ID: <20060126152026.PRYU5216.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Michael,

To elaborate slightly: You can't load the same package twice in an R
session. As mentioned, used Commander() to restart the Rcmdr GUI.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefano Calza
> Sent: Thursday, January 26, 2006 4:23 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] What's wrong with Rcmdr?
> 
> Try using
> 
> Commander()
> 
> but, obviously, the first time.
> 
> HIH,
> Ste
> 
> On Thu, Jan 26, 2006 at 01:11:23AM -0800, Michael wrote:
> <Michael>Hi all,
> <Michael>
> <Michael>I successfully installed Rcmdr. And I type 
> "library(Rcmdr)", nothing <Michael>happened; <Michael> 
> <Michael>or if I select menu item "load package" and select 
> "Rcmdr", still nothing <Michael>happened...
> <Michael>
> <Michael>Why didn't Rcmdr start?
> <Michael>
> <Michael>Very strangely, if I close the R console and restart 
> R console, every first <Michael>time I load "Rcmdr", it 
> starts! But not second, third time...
> <Michael>
> <Michael>What's wrong with it?
> <Michael>
> <Michael>Thanks a lot!
> <Michael>
> <Michael>Michael.
> <Michael>
> <Michael>	[[alternative HTML version deleted]]
> <Michael>
> <Michael>______________________________________________
> <Michael>R-help at stat.math.ethz.ch mailing list 
> <Michael>https://stat.ethz.ch/mailman/listinfo/r-help
> <Michael>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Thu Jan 26 16:39:25 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 26 Jan 2006 15:39:25 +0000 (UTC)
Subject: [R] How to make two side-by side trellis plots same size
References: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>
	<eb555e660601260707g1c78be6cp195c047dcd08cea@mail.gmail.com>
Message-ID: <loom.20060126T163704-547@post.gmane.org>

Thanks, Deepayan

Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:

> 
> If that's the only reason, I would suggest rbind-ing them and then use
> 
> scales = list(x = "free")

I probably will go for this.
 
> If you want the first two columns to have the same x-limits, you can specify
> 
> xlim = list(c(0,25), c(0,25), c(0, 2500))
> 
> etc. Otherwise, if you want the panels to have the same physical
> dimensions, look at the panel.width and panel.height arguments in
> ?print.trellis. I can't think of any other way of guaranteeing it.

Documentation said that using panel.width with non-standard aspect (which must 
be 1) leads to undefined effects.

Dieter



From ripley at stats.ox.ac.uk  Thu Jan 26 16:41:12 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jan 2006 15:41:12 +0000 (GMT)
Subject: [R] maximizing available memory under windows XP
In-Reply-To: <1db726800601260602s7b083e01o8cc2b7da547e8183@mail.gmail.com>
References: <1db726800601260602s7b083e01o8cc2b7da547e8183@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601261515460.25205@gannet.stats>

The problem is your test.  You are trying to re-allocate large objects, 
and memory fragmentation will take its toll, and be almost random in its 
effects.

Using the CRAN binary of 2.2.1 and

> for(i in 1:1000) assign(paste("r", i, sep="."), rnorm(1e6))
> gc()
             used   (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells    170480    4.6     350000    9.4    350000    9.4
Vcells 253062963 1930.8  253513169 1934.2 253063937 1930.8
> memory.size(max=T)
[1] 2037137408

I was able to get close to address space limit on my laptop (which does 
not have the /3GB switch set (*in boot.ini*, not in the binary)).

And that binary is marked:

[d:/R/R-2.2.1/bin]% dumpbin /HEADERS Rterm.exe
Microsoft (R) COFF Binary File Dumper Version 6.00.8447
Copyright (C) Microsoft Corp 1992-1998. All rights reserved.


Dump of file Rterm.exe

PE signature found

File Type: EXECUTABLE IMAGE

FILE HEADER VALUES
              14C machine (i386)
                6 number of sections
         43A80482 time date stamp Tue Dec 20 13:17:54 2005
                0 file pointer to symbol table
                0 number of symbols
               E0 size of optional header
              32F characteristics
                    Relocations stripped
                    Executable
                    Line numbers stripped
                    Symbols stripped
                    Application can handle large (>2GB) addresses
                    32 bit word machine
                    Debug information stripped


On Thu, 26 Jan 2006, roger bos wrote:

> I have always been using ebitbin to set the 3GB switch in the windows
> binary, but version 2.2.1 has this set as default (which I verified using
> dumpbin).  However, when I generate junk data to fill up my memory and read
> the memory usage using gc(), it seems that I am not getting as good results
> with 2.2.1 patched as I was with 2.2.0 after I edited the header.  Under R
> 2.2.0 I was able to use over 2GB and with R 2.2.1 patched I can access only
> 1GB.  Anyone have any suggestions that I can try.  My machine has 4GB and I
> have modified the Boot.ini file.
>
> Thanks,
>
> Roger
>
> Here is the gc() on 2.2.1 patched:
>> gc()
>           used  (Mb) gc trigger   (Mb)  max used   (Mb)
> Ncells   252021   6.8     467875   12.5    379294   10.2
> Vcells 71097226 542.5  140857919 1074.7 140597245 1072.7
>>
>
> Here is the gc() output on 2.2.0 after I edited the header:
>> gc()
>            used  (Mb) gc trigger   (Mb)  max used   (Mb)
> Ncells    174118   4.7     350000    9.4    350000    9.4
> Vcells 130065529 992.4  257820332 1967.1 257565551 1965.1
>>
>
> Here is the code that I used to fill my memory (nothing fancy):
> a <- diag(1000)
> b <-a
> for (i in 1:1000000) {
> a<- diag(1000)
> b <- rbind(b,a)
> }
> gc()
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alexandrarma at yahoo.com.br  Thu Jan 26 16:44:57 2006
From: alexandrarma at yahoo.com.br (Alexandra R. M. de Almeida)
Date: Thu, 26 Jan 2006 12:44:57 -0300 (ART)
Subject: [R] Using special characters
Message-ID: <20060126154457.67827.qmail@web33315.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/58274726/attachment.pl

From Hans.Skaug at mi.uib.no  Thu Jan 26 16:55:27 2006
From: Hans.Skaug at mi.uib.no (Hans Skaug)
Date: Thu, 26 Jan 2006 16:55:27 +0100
Subject: [R] Automatic differentiation (was: Re: D(dnorm...)?)
Message-ID: <5BCBA62ECB426A47AE66567CDF930F9864411A@HUGIN.uib.no>

Dear Alberto,

There are fisheries people also in Europe using AD Model Builder (Denmark and England for instance),
but you are probably right that it is more widespread in North America. There is also effort
going on where people try to make assessment models written in ADMB callable from R.

best regards,

hans

> think AD Model Builder is mainly used for fisheries assessment in North 
>America and, it seems, also in Australia. In Europe, R is still the de-facto 
>standard for fisheries assessment. However, I'd like to support Bill 
>Venables' suggestion. I've been resisting to adopt AD model builder, or to 
>start using again that other system not unlike R, mainly because of the 
>licence price and because I really like R as a tool for almost everything. 
>But an AD function would really make a huge difference for my work. There are 
>free tools that can be used to perform AD on C or Fortran code (e.g. 
>http://www.autodiff.org). One of the difficulties to use them with R is the 
>need to translate the R code into C of Fortran code, but probably there are 
>many other problems that I'm not able to see.
>
>Alberto
>-- 

_____________________________
Hans Julius Skaug

Department of Mathematics
University of Bergen
Johannes Brunsgate 12
5008 Bergen
Norway



From yang.x.qiu at gsk.com  Thu Jan 26 16:56:38 2006
From: yang.x.qiu at gsk.com (yang.x.qiu@gsk.com)
Date: Thu, 26 Jan 2006 10:56:38 -0500
Subject: [R] how to test robustness of correlation
In-Reply-To: <200601252057.k0PKvlww013172@hertz.gene.com>
Message-ID: <OF12275401.3FBD48B7-ON85257102.0056CCA6-85257102.00579591@gsk.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/1467d771/attachment.pl

From mschwartz at mn.rr.com  Thu Jan 26 16:57:21 2006
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 26 Jan 2006 09:57:21 -0600
Subject: [R] efficiency with "%*%"
In-Reply-To: <43D8E2FB.3020409@statistik.uni-dortmund.de>
References: <1138281562.43d8cc5a455fe@webmail.ipea.gov.br>
	<43D8CFE1.9030800@statistik.uni-dortmund.de>
	<43D8DF66.8020604@sciviews.org>
	<43D8E2FB.3020409@statistik.uni-dortmund.de>
Message-ID: <1138291042.4347.5.camel@localhost.localdomain>

Perhaps you guys should try to benchmark that test on an nVidia GPU?

;-)

Best regards,

Marc

On Thu, 2006-01-26 at 15:55 +0100, Uwe Ligges wrote:
> Philippe Grosjean wrote:
> 
> > Excellent, but...
> > 
> >  > x <- rnorm(1000000)
> >  > y <- rnorm(1000000)
> >  > system.time(x%*%y)
> > [1] 0.03 0.00 0.03   NA   NA
> >  > system.time(sum(x*y))
> > [1] 0.05 0.00 0.04   NA   NA
> >  > system.time(crossprod(x, y))
> > [1]  0  0  0 NA NA
> > 
> > So, to paraphrase a well-known contributor on this mailing list:
> > "Excellent! So, what did you decided to do during the next 30 
> > milliseconds you will save by using crossprod() instead of x%*%y?
> > (joke)
> > 
> > Best,
> > 
> > Philippe Grosjean
> > 
> > P.S.: Uwe, perhaps you should consider buying a faster computer, isn't 
> > it? :-()
> 
> Well, I use R, you know. It is even fast enough for my 5 year old 
> laptop. For your super computer, please replace 1e6 by 1e8 in the 
> example above. ;-)
> 
> Uwe
> 
> 
> > Uwe Ligges wrote:
> > 
> >> dimitrijoe at ipea.gov.br wrote:
> >>
> >>
> >>> Hi,
> >>>
> >>> x and y are (numeric) vectors. I wonder if one of the following is more
> >>> efficient than the other:
> >>>
> >>> x%*%y
> >>>
> >>> or
> >>>
> >>> sum(x*y)
> >>> ?
> >>
> >>
> >>
> >> I'd try
> >>
> >>    x <- rnorm(1000000)
> >>    y <- rnorm(1000000)
> >>    system.time(x%*%y)
> >>    system.time(sum(x*y))
> >>
> >> and finally (hint, hint!):
> >>
> >>    system.time(crossprod(x, y))
> >>
> >> Uwe Ligges
> >>
> >>
> >>
> >>> Thanks,
> >>> Dimitri Szerman
> >>>



From gunter.berton at gene.com  Thu Jan 26 17:12:09 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 Jan 2006 08:12:09 -0800
Subject: [R] how to test robustness of correlation
In-Reply-To: <OF12275401.3FBD48B7-ON85257102.0056CCA6-85257102.00579591@gsk.com>
Message-ID: <200601261612.k0QGC92Q018205@ohm.gene.com>

Below



> 
> Hi, Berton: 
> thanks for getting back to me. 
> 
> I played around cor.rob().  Yes, I can get a robust 
> correlation coefficient matrix based on mcd or mve outlier 
> detection methods.   
> 
> I have two further questions: 
> 
> 1) How do I get a p value of the robust r? 

A p-value for what? That r==0 ?

> 2) What I mean by resampling is "leave one out" procedure, to 
> get a confidence interval of r.  Do you know if there is any 
> package in R to do it?  I suppose I could code it myself,  
> but it is nice if there is already one. 
> 
> thanks. 
> Yang 

**An** answer to both is the same -- bootstrap it. "Leave one out" is not
resampling (/bootstrapping). It is usually referred to as "jackknifing," but
that uses more specific ways of doing things than the analogy implies.
Efron's little SIAM book on "The jackknife, bootstrap, etc. explains them
and their relationships in detail. It is trivial to bootstrap cor.rob in
base R using sample() (from the <x,y> **pairs** -- or n-tuples generally --
not the marginals separately ). If you insist on a package, "boot" is the
obvious one -- why did you not attempt to find it yourself? Either way,
expect it to take a while for a decent size resample (e.g. >1e4).

-- Bert



From deepayan.sarkar at gmail.com  Thu Jan 26 17:12:33 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 26 Jan 2006 10:12:33 -0600
Subject: [R] How to make two side-by side trellis plots same size
In-Reply-To: <loom.20060126T163704-547@post.gmane.org>
References: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>
	<eb555e660601260707g1c78be6cp195c047dcd08cea@mail.gmail.com>
	<loom.20060126T163704-547@post.gmane.org>
Message-ID: <eb555e660601260812s66101966w722f0252d84a0004@mail.gmail.com>

On 1/26/06, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Thanks, Deepayan
>
> Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:
>
> >
> > If that's the only reason, I would suggest rbind-ing them and then use
> >
> > scales = list(x = "free")
>
> I probably will go for this.
>
> > If you want the first two columns to have the same x-limits, you can
> specify
> >
> > xlim = list(c(0,25), c(0,25), c(0, 2500))
> >
> > etc. Otherwise, if you want the panels to have the same physical
> > dimensions, look at the panel.width and panel.height arguments in
> > ?print.trellis. I can't think of any other way of guaranteeing it.
>
> Documentation said that using panel.width with non-standard aspect (which
> must
> be 1) leads to undefined effects.

Yes, because panel.width and panel.height together define the aspect,
and will override whatever the 'aspect' argument says. It's fine to
have aspect="fill" (the default), or even 1 probably, and then control
the aspect using panel.width and panel.height.

Deepayan



From roger.bos at gmail.com  Thu Jan 26 17:25:23 2006
From: roger.bos at gmail.com (roger bos)
Date: Thu, 26 Jan 2006 11:25:23 -0500
Subject: [R] maximizing available memory under windows XP
In-Reply-To: <Pine.LNX.4.61.0601261515460.25205@gannet.stats>
References: <1db726800601260602s7b083e01o8cc2b7da547e8183@mail.gmail.com>
	<Pine.LNX.4.61.0601261515460.25205@gannet.stats>
Message-ID: <1db726800601260825r391fc155i208c2745c552cc93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/9d9f9075/attachment.pl

From Pierre.Lapointe at nbf.ca  Thu Jan 26 17:28:17 2006
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Thu, 26 Jan 2006 11:28:17 -0500
Subject: [R] XML Request in R: Pointers/examples needed
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35CD7@lbmsg002.fbn-nbf.local>

Hello R-helpers,

I'm new to XML. I have been using an application for some time, but now I
wish to automate my downloads with R. When I use the web interface of their
XML application, I'm able to read the response in R with the XML package.

My problem now is to send the requests directly from R.  Here below are two
XML request scripts that come from the applicaton help file.  My questions
are:

	1-	What type of scripts are those (first one is xml, but I'm
not sure about the second one)
	2-	How do I get them to work in R?  I'm a loss with the first
one.  As for the second one, I suspect I would have to use the rcom or
R(D)COM packages but I do not understand what I should do once I have
established the COM object.

		(example: 	library(rcom)
				comCreateObject("MSXML2.XMLHTTP.3.0")

Any pointers/examples would be appreciated.

#### Script #1 Logon Request##################################### 

<?xml version="1.0"?>
<ds:Requests
xmlns:ds="http://product.datastream.com/zappy/dsxml_1_0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://product.datastream.com/zappy/dsxml_1_0
http://product.datastream.com/zappy/dsxml_1_0.xsd">
<ds:Request xsi:type="ds:SessionRequest">
<ds:UserName>myusername</ds:UserName>
<ds:Password>mypassword</ds:Password>
</ds:Request>
</ds:Requests>

#### Script #2 Request in HTTP Header############################

<script>
<!--
function Exec()
{
var objRequest = new ActiveXObject( "MSXML2.XMLHTTP.3.0" );
var request = "<?xml version="1.0"?>";
requests += <ds:Requests
xmlns:ds=\"http://product.datastream.com/zappy/dsxml_1_0\"
xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"
xsi:schemaLocation=\"http://product.datastream.com/zappy/dsxml_1_0
http://product.datastream.com/zappy/dsxml_1_0.xsd\">
<ds:Request xsi:type=\"ds:SessionRequest\">
<ds:UserName>myusername</ds:UserName>
<ds:Password>mypassword</ds:Password>
</ds:Request>
</ds:Requests>";
objRequest.open("POST","http://product.datastream.com/zappy/acces
spoint.asp, false);
objRequest.setRequestHeader ("DSXMLAPIREQUEST", request);
objRequest.send("");
//Get the response XML.
divResponse.innerText = objRequest.responseText
}
-->
</script>
#################################################################	

Thank you

Pierre Lapointe


*****************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}



From gunter.berton at gene.com  Thu Jan 26 17:33:05 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 Jan 2006 08:33:05 -0800
Subject: [R] how to test robustness of correlation
In-Reply-To: <OF12275401.3FBD48B7-ON85257102.0056CCA6-85257102.00579591@gsk.com>
Message-ID: <200601261633.k0QGX56k026627@ohm.gene.com>

One more thing ... 


> I played around cor.rob().  Yes, I can get a robust correlation 
> coefficient matrix based on mcd or mve outlier detection methods. 
> 
> I have two further questions:
> 

You might call it semantics, but I prefer "resistant estimation" to "outlier
detection methods." I recognize that they are equivalent (any resistant
estimator can be used to identify "outliers"; any outlier detection method
leads to a resistant estimator on downweighting of outliers). However, I
consider the distinction important. "Outlier detection" suggests:

1) That "outlier" is a statistically well-defined concept; it isn't. The
implied dichotomy is a fiction (a dangerous one, IMO -- but many would
disagree).

2) That some sort of hypothesis testing procedure is used to "reject"
points. None is.

Rather, mve() and mcd() try to characterize the behavior of the "central"
mass of the distribution, using that characterization to weight the
informativeness of points outside that mass. A 1-D equivalent is MAD for
spread. This is a far cry from the bad old days of (sequential) "outlier
detection." These methods are crucially dependent on modern computer power
of course.

Cheers,

Bert



From ripley at stats.ox.ac.uk  Thu Jan 26 17:44:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jan 2006 16:44:09 +0000 (GMT)
Subject: [R] Using special characters
In-Reply-To: <20060126154457.67827.qmail@web33315.mail.mud.yahoo.com>
References: <20060126154457.67827.qmail@web33315.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0601261635150.26165@gannet.stats>

You haven't told us much, for example not your R version nor your exact 
platform (nor if this is a binary or self-compiled version of R) nor your 
locale.

And you (or something between you and me) have wrapped the R code onto 
just two lines.  You talk about saving as pdf, and ran a jpeg device, 
unless I missed something in those messed-up lines.

I am afraid there is just too much uncertainty here.  For current R, with 
the pdf() device and a Latin-1 or UTF-8 locale, this should work.  For a 
jpeg() device, it depends on the fonts you have installed in your X11 
server, but it usually works.

If you can explain whether you want pdf or jpeg and give the missing 
details (and any others you think might be relevant), people here may be 
able to help you further.

On Thu, 26 Jan 2006, Alexandra R. M. de Almeida wrote:

> Dear R users
>
> I'm having problems in putting special characters (like ?, ?, ? )  in my plots, as much in titles, as in axis names, as in legend...when I save them as a pdf document. They don't appear...
> I don't know if it is because I'm using a linux platform...
> The script is the following:
>
> library(grDevices) jpeg(file="Fronteira/FronteiraNova.jpeg", bg="white", quality=100,width = 600, height = 480) grafico.fe<-function(varFA,retFA,varC,retC,covRF,retRF,varD,retD) {    minx<-min(100*sqrt(varFA),100*varC,100*sqrt(covRF[1,1]),100*sqrt(covRF[2,2]),100*sqrt(covRF[3,3]),100*sqrt(covRF[4,4]),100*varD)    maxx<-max(100*sqrt(varFA),100*varC,100*sqrt(covRF[1,1]),100*sqrt(covRF[2,2]),100*sqrt(covRF[3,3]),100*sqrt(covRF[4,4]),100*varD)    miny<-min(100*retFA,retC,retRF,retD)    maxy<-max(100*retFA,retC,retRF,retD)    plot(100*sqrt(varFA),100*retFA,type="l",lwd=2,col=2,xlab="Risco %", ylab="Retorno % (anualizado)", xlim=c(minx-0.01,maxx), ylim=c(miny,maxy),main="Fronteira Eficiente")    points(100*varC,retC,pch=16,cex=0.6, col=3)    points(100*sqrt(covRF[1,1]),retRF[1],pch=16,cex=0.6,col=4)    points(100*sqrt(covRF[2,2]),retRF[2],pch=16,cex=0.6,col=5)    points(100*sqrt(covRF[3,3]),retRF[3],pch=16,cex=0.6,col=6)    points(100*sqrt(covRF[4,4]),retRF[4],pch=16,cex=0.6,col=7)
> points(100*varD,retD,pch=16,cex=0.6,col=1)   legend(c(0.91,1.11),c(17.97,18.156),c("Fronteira Eficiente"),lwd=2,cex=0.6, col=2)   legend(0.93,18.13,c("Consolidado","Brad","AB","It","Uni","Ot"),pch=16, bty="n", cex=0.6, col=c(3,4,5,6,7,1), ncol=1) } grafico.fe(varFA,retFA,varC,retC,covRF,retRF,varD,retD) dev.off()
>
> Do someome have a problem like this?
>
> Thanks
>
> Alexandra Almeida
>
>
>
>
>  Alexandra R. Mendes de Almeida
>
>
>
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From j.g.vanevert at student.tudelft.nl  Thu Jan 26 17:54:31 2006
From: j.g.vanevert at student.tudelft.nl (Joost van Evert)
Date: Thu, 26 Jan 2006 17:54:31 +0100
Subject: [R] how to compare glm parameter values
Message-ID: <1138294471.27418.10.camel@inpc93.et.tudelft.nl>

Hi list,

I have trained a logistic regression model on three diverse features.
Now I want to compare these values to get an idea of their influence on
the result. For most features the summary function returns p-values that
equal zero, which makes the p-values unusable.

I have the intuition it is impossible to compare the relevance of the
features based on their parameter values as their ranges are quite
different. 

Is it possible to compare the relevance of features on their parameters,
if their range differs? 

Regards,

Joost



From ggrothendieck at gmail.com  Thu Jan 26 18:05:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jan 2006 12:05:10 -0500
Subject: [R] how to test robustness of correlation
In-Reply-To: <OF92FDDC6F.B543C988-ON85257101.006F5925-85257101.0071464E@gsk.com>
References: <OF92FDDC6F.B543C988-ON85257101.006F5925-85257101.0071464E@gsk.com>
Message-ID: <971536df0601260905n4bc3dbfdub42d5444ca06bb69@mail.gmail.com>

The cor function can do spearman correlation using
method = "spearman" .

On 1/25/06, yang.x.qiu at gsk.com <yang.x.qiu at gsk.com> wrote:
> Hi, there:
>
> As you all know, correlation is not a very robust procedure.  Sometimes
> correlation could be driven by a few outliers. There are a few ways to
> improve the robustness of correlation (pearson correlation), either by
> outlier removal procedure, or resampling technique.
>
> I am wondering if there is any R package or R code that have incorporated
> outlier removal or resampling procedure in calculating correlation
> coefficient.
>
> Your help is greatly appreciated.
>
> Thanks.
> Yang
>
> Yang Qiu
> Integrated Data Analysis
> Cheminformatics at RTP
> GlaxoSmithKline
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From alexandrarma at yahoo.com.br  Thu Jan 26 18:27:43 2006
From: alexandrarma at yahoo.com.br (Alexandra R. M. de Almeida)
Date: Thu, 26 Jan 2006 14:27:43 -0300 (ART)
Subject: [R] Re-writing the problem of Using special characters
In-Reply-To: <Pine.LNX.4.61.0601261635150.26165@gannet.stats>
Message-ID: <20060126172743.65696.qmail@web33301.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/6d32fa84/attachment.pl

From gunter.berton at gene.com  Thu Jan 26 18:32:59 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 Jan 2006 09:32:59 -0800
Subject: [R] how to test robustness of correlation
In-Reply-To: <971536df0601260905n4bc3dbfdub42d5444ca06bb69@mail.gmail.com>
Message-ID: <200601261732.k0QHWxZ8022573@meitner.gene.com>

Gabor:

Contrary to popular belief, rank-based procedures are **not** resistant.

Example:

> x<-c(1:10,100);y<-c(1:10+rnorm(10,sd=.25),-100)
> cor(x,y)
[1] -0.9816899  ## awful

> cor(x,y,method='spearman')
[1] 0.5	## better

> require(MASS)
Loading required package: MASS
[1] TRUE

> cov.rob(cbind(x,y),cor=TRUE)
## ... bunch of output omitted

$cor
          x         y
x 1.0000000 0.9977734  ## best
y 0.9977734 1.0000000

## Look at the plot to see.

-- Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Thursday, January 26, 2006 9:05 AM
> To: yang.x.qiu at gsk.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] how to test robustness of correlation
> 
> The cor function can do spearman correlation using
> method = "spearman" .
> 
> On 1/25/06, yang.x.qiu at gsk.com <yang.x.qiu at gsk.com> wrote:
> > Hi, there:
> >
> > As you all know, correlation is not a very robust 
> procedure.  Sometimes
> > correlation could be driven by a few outliers. There are a 
> few ways to
> > improve the robustness of correlation (pearson 
> correlation), either by
> > outlier removal procedure, or resampling technique.
> >
> > I am wondering if there is any R package or R code that 
> have incorporated
> > outlier removal or resampling procedure in calculating correlation
> > coefficient.
> >
> > Your help is greatly appreciated.
> >
> > Thanks.
> > Yang
> >
> > Yang Qiu
> > Integrated Data Analysis
> > Cheminformatics at RTP
> > GlaxoSmithKline
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Thu Jan 26 18:34:13 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 26 Jan 2006 17:34:13 +0000 (UTC)
Subject: [R] How to make two side-by side trellis plots same size
References: <LPEJLJACLINDNMBMFAFICEAGCCAA.dieter.menne@menne-biomed.de>
	<eb555e660601260707g1c78be6cp195c047dcd08cea@mail.gmail.com>
	<loom.20060126T163704-547@post.gmane.org>
	<eb555e660601260812s66101966w722f0252d84a0004@mail.gmail.com>
Message-ID: <loom.20060126T183206-60@post.gmane.org>

Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:

> > Documentation said that using panel.width with non-standard aspect (which
> > must
> > be 1) leads to undefined effects.
> 
> Yes, because panel.width and panel.height together define the aspect,
> and will override whatever the 'aspect' argument says. It's fine to
> have aspect="fill" (the default), or even 1 probably, and then control
> the aspect using panel.width and panel.height.

Thanks, Deepayan,
I got it now: I have to set BOTH width and height, and aspect set before will 
effectively get overridden. I tried before to set panel.width only, assuming 
that panel.height would follow according to aspect. Here the working solution.

library(lattice)
n1 = 20
# I cannot rbind df1 and df2, because the x-dimensions are
# different and must be scaled individually
df1 = data.frame(y=rnorm(4*n1),x=rep(1:n1,4),
   facA=rep(c("A","B"),each=2*n1),facB=rep(c("C","D"),each=n1))

# I add a dummy facB here, to make sure panels have same structure
df2 = data.frame(y=rnorm(2*n1),z=rep(100*(1:n1),2),
   facA=rep(c("A","B"),each=n1),facB="C")

# Note: aspect = 1 is required later, but settings aspect here
# does not make sense when we work with panel.height later
p1 = xyplot(y~x|facA*facB,data=df1,main="Plot1",
  between=list(x=2))
p2 = xyplot(y~z|facA*facB,data=df2,main="Plot2",layout=c(1,2))

pw = list(x=3,units="cm")
print(p1,position=c(0,0,2/3,1),more=T,panel.width=pw,panel.height=pw)
print(p2,position=c(2/3,0,1,1),more=F,panel.width=pw,panel.height=pw)



From andy_liaw at merck.com  Thu Jan 26 18:48:27 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jan 2006 12:48:27 -0500
Subject: [R] how to compare glm parameter values
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED762@usctmx1106.merck.com>

It's inappropriate to use the p-values associated with the coefficients for
that purpose.  They tell you how sure you can be, based on the data you
have, that they are not equal to zero. 

Prof. Fox's `effects' package on CRAN would be a better place to look, I
think.  I believe there's an JSS article describing the package as well.

Andy

From: Joost van Evert
> 
> Hi list,
> 
> I have trained a logistic regression model on three diverse features.
> Now I want to compare these values to get an idea of their 
> influence on
> the result. For most features the summary function returns 
> p-values that
> equal zero, which makes the p-values unusable.
> 
> I have the intuition it is impossible to compare the relevance of the
> features based on their parameter values as their ranges are quite
> different. 
> 
> Is it possible to compare the relevance of features on their 
> parameters,
> if their range differs? 
> 
> Regards,
> 
> Joost
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lamkelj at yahoo.com  Thu Jan 26 19:18:10 2006
From: lamkelj at yahoo.com (Kel Lam)
Date: Thu, 26 Jan 2006 10:18:10 -0800 (PST)
Subject: [R] Clustering Question
Message-ID: <20060126181810.13220.qmail@web52712.mail.yahoo.com>

Hi group,

My case has N physicians with each seeing M patients. 
One physician could have seen a group of patients, or,
a patient could have been seen by multiple number of
physicians.  In order words, there are overlaps.  Now,
I have the following NxM matrix

              Patient#1 Patient#2 Patient#3 .......
Patient#m
Physician#1       1        0          1     .......   
 0
Physician#2       1        1          1     .......   
 1
Physician#3       0        1          0     .......   
 1
.                 .        .          .     .......   
 .       
.                 .        .          .     .......   
 .       
Physician#n       1        1          0     .......   
 0

"1" indicates previous encouter and "0" otherwise.  My
aim is to identify physician group practice based on
the common patients they see.  Any suggestion on which
R package would best serve this purpose?  Thank you so
much!

Regards,
Kelvin



From Dale_Steele at brown.EDU  Thu Jan 26 19:31:32 2006
From: Dale_Steele at brown.EDU (Dale Steele)
Date: Thu, 26 Jan 2006 13:31:32 -0500
Subject: [R] Data management problem: convert text string to matrix of 0's
 and 1's
Message-ID: <43D91584.2060307@brown.EDU>

I have a data management problem which exceeds my meager R programming 
skills and would greatly appreciate suggestions on how to proceed?  The 
data consists of a series of observation periods. Specific behaviors are 
recorded for each time period in the order each is observed.  Their are 
8 possible behaviors, coded as "i" "c" "s" "r" "v" "e" "p" "f".

The data looks like:
-->
icsrvepf
fpevrsci
ics
p

f
ic
<--

I would like to convert the about to a matrix of the form:

  i c s r v e p f
  1 1 1 1 1 1 1 1
  1 1 1 1 1 1 1 1
  1 1 1 0 0 0 0 0
  0 0 0 0 0 0 1 0
  0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 1
  1 1 0 0 0 0 0 0

Thanks.

Dale

Dale Steele, MD
Pediatric Emergency Medicine
Brown Medical School



From sell_mirage_ne at hotmail.com  Thu Jan 26 19:35:27 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Thu, 26 Jan 2006 12:35:27 -0600
Subject: [R] DOS command using "system"
Message-ID: <BAY110-F38C89E0D65883B7E155DF5C7150@phx.gbl>

HI R users
I have one question for using DOS command through "system"
I like to delete a file that is located at C:\Program 
Files\DOSPROGRAM\input.dat
I can use a DOS command "del" on Dos prompt like this

C:\Documents and Settings> del "C:\Program Files\DOSPROGRAM\input.dat"

to delete input.dat file.

When I try to do the same thing on R using "system" command

system('del "C:\Program Files\DOSPROGRAM\input.dat"')
or
system("del "C:\Program Files\DOSPROGRAM\input.dat"")
or
system(paste("del", "\"C:\\Program Files\\DOSPROGRAM\\input.dat\"",sep=" "))

All the three system commands did work

Could you help me to figure out ?

Thanks in advance

TM



From comtech.usa at gmail.com  Thu Jan 26 19:39:30 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 10:39:30 -0800
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <20060126152026.PRYU5216.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <20060126092312.GB4835@med.unibs.it>
	<20060126152026.PRYU5216.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <b1f16d9d0601261039r48332691pb06d210de930162c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/54fb5d12/attachment.pl

From tlumley at u.washington.edu  Thu Jan 26 19:40:24 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 Jan 2006 10:40:24 -0800 (PST)
Subject: [R] Data management problem: convert text string to matrix of
 0's and 1's
In-Reply-To: <43D91584.2060307@brown.EDU>
References: <43D91584.2060307@brown.EDU>
Message-ID: <Pine.LNX.4.64.0601261037050.27941@homer23.u.washington.edu>

On Thu, 26 Jan 2006, Dale Steele wrote:
> The data looks like:
> -->
> icsrvepf
> fpevrsci
> ics
> p
>
> f
> ic
> <--
>
> I would like to convert the about to a matrix of the form:
>
>  i c s r v e p f
>  1 1 1 1 1 1 1 1
>  1 1 1 1 1 1 1 1
>  1 1 1 0 0 0 0 0
>  0 0 0 0 0 0 1 0
>  0 0 0 0 0 0 0 0
>  0 0 0 0 0 0 0 1
>  1 1 0 0 0 0 0 0
>

One possibility is to use grep()
> a
[1] "icsrvepf" "fpevrsci" "p"        ""         "f"        "ic"
> grep("i",a)
[1] 1 2 6

so
> results<-matrix(0,nrow=length(a),ncol=length(behaviours))
> colnames(results)<-behaviours
> for(b in behaviours) results[grep(b,a),b]<-1
> results
      i c s r v e p f
[1,] 1 1 1 1 1 1 1 1
[2,] 1 1 1 1 1 1 1 1
[3,] 0 0 0 0 0 0 1 0
[4,] 0 0 0 0 0 0 0 0
[5,] 0 0 0 0 0 0 0 1
[6,] 1 1 0 0 0 0 0 0
>

 	-thomas



From comtech.usa at gmail.com  Thu Jan 26 19:41:19 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 10:41:19 -0800
Subject: [R] What's wrong with JGR?
In-Reply-To: <b1f16d9d0601261040x1e2c8bdal8db5e67c4c6f8628@mail.gmail.com>
References: <b1f16d9d0601260053j118932hd0733ba5410928ca@mail.gmail.com>
	<43D8A6B4.9000202@web.de>
	<b1f16d9d0601261040x1e2c8bdal8db5e67c4c6f8628@mail.gmail.com>
Message-ID: <b1f16d9d0601261041w1fe1742bg8da85941bc4fea5e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/ca4527cf/attachment.pl

From dylan.beaudette at gmail.com  Thu Jan 26 20:11:00 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Thu, 26 Jan 2006 11:11:00 -0800
Subject: [R] understanding patterns in categorical vs. continuous data
Message-ID: <200601261111.00692.dylan.beaudette@gmail.com>

Greetings,

I have a set of bivariate data: one variable (vegetation type) which is 
categorical, and one (computed annual insolation) which is continuous. 
Plotting veg_type ~ insolation produces a nice overview of the patterns that 
I can see in the source data. However, due to the large number of samples 
(1,000), and the apparent "spread" in the distribution of a single vegetation 
type over a range of insolation values- I having a hard time quantitatively 
describing the relationship between the two variables. 

Here is a link to a sample graph:
http://casoilresource.lawr.ucdavis.edu/drupal/node/162

Since the data along each vegetation type "line" is not a distribution in the 
traditional sense, I am having problems applying descriptive statistical 
methods. Conceptually, I would like to some how describe the variation with 
insolation, along each vegetation type "line".

Any guidance, or suggested reading material would be greatly appreciated.


-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From Jincai.Jiang at morganstanley.com  Thu Jan 26 20:08:53 2006
From: Jincai.Jiang at morganstanley.com (Jiang, Jincai (Institutional Securities Management))
Date: Thu, 26 Jan 2006 14:08:53 -0500
Subject: [R] (no subject)
Message-ID: <9D07362B75E05C4F920DB8DF8584A68B05642CC3@NYWEXMB31.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/e3f2d60a/attachment.pl

From gunter.berton at gene.com  Thu Jan 26 20:25:02 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 Jan 2006 11:25:02 -0800
Subject: [R] understanding patterns in categorical vs. continuous data
In-Reply-To: <200601261111.00692.dylan.beaudette@gmail.com>
Message-ID: <200601261925.k0QJP2vt001004@compton.gene.com>

UC Davis has a statistical department, I would suggest you get consulting
help from them. Do they have a consulting service?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dylan Beaudette
> Sent: Thursday, January 26, 2006 11:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] understanding patterns in categorical vs. continuous data
> 
> Greetings,
> 
> I have a set of bivariate data: one variable (vegetation 
> type) which is 
> categorical, and one (computed annual insolation) which is 
> continuous. 
> Plotting veg_type ~ insolation produces a nice overview of 
> the patterns that 
> I can see in the source data. However, due to the large 
> number of samples 
> (1,000), and the apparent "spread" in the distribution of a 
> single vegetation 
> type over a range of insolation values- I having a hard time 
> quantitatively 
> describing the relationship between the two variables. 
> 
> Here is a link to a sample graph:
> http://casoilresource.lawr.ucdavis.edu/drupal/node/162
> 
> Since the data along each vegetation type "line" is not a 
> distribution in the 
> traditional sense, I am having problems applying descriptive 
> statistical 
> methods. Conceptually, I would like to some how describe the 
> variation with 
> insolation, along each vegetation type "line".
> 
> Any guidance, or suggested reading material would be greatly 
> appreciated.
> 
> 
> -- 
> Dylan Beaudette
> Soils and Biogeochemistry Graduate Group
> University of California at Davis
> 530.754.7341
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu Jan 26 20:30:54 2006
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 26 Jan 2006 13:30:54 -0600
Subject: [R] (no subject)
In-Reply-To: <9D07362B75E05C4F920DB8DF8584A68B05642CC3@NYWEXMB31.msad.ms.com>
References: <9D07362B75E05C4F920DB8DF8584A68B05642CC3@NYWEXMB31.msad.ms.com>
Message-ID: <43D9236E.9050209@pdf.com>



Jiang, Jincai (Institutional Securities Management) wrote:
> I found that read.table() does not work well on lines that is longer
> than 236 bytes.
> Is this know problem? Is it under fixing?
>  
> Regards,
>  
> Jincai Jiang
> (Office) 212-761-3984

Hi, Jincai,

What does "not work well" mean? 236 bytes is only 236 characters and is 
by far a very short line of data. I routinely read files with 100-300 
columns and never had a problem with read.table.

Please supply a reproducible example. Also, test your assertion on other 
datasets to determine if something is wrong with the file you are 
reading. I suspect the data format is the issue and not read.table.

Finally, read the posting guide. Comments such as "does not work well" 
usually do not go over well on this list.

HTH,

--sundar



From andersn at hawaii.edu  Thu Jan 26 20:30:14 2006
From: andersn at hawaii.edu (Anders Nielsen)
Date: Thu, 26 Jan 2006 09:30:14 -1000
Subject: [R] Automatic differentiation (was: Re: D(dnorm...)?)
In-Reply-To: <5BCBA62ECB426A47AE66567CDF930F9864411A@HUGIN.uib.no>
References: <5BCBA62ECB426A47AE66567CDF930F9864411A@HUGIN.uib.no>
Message-ID: <200601260930.14640.anders.nielsen@hawaii.edu>

I can confirm that AD Model Builder is used at the Danish 
Institute for Fisheries Research, and by fisheries people in 
and all around the Pacific.

On a few occasions I have solved a likelihood optimization 
problem in AD Model Builder, and then wrapped the binary 
in an R-package, with data read in, graphics and such, for 
others to use, as the binaries can be freely distributed.

I can recommend this approach to anyone with problems 
where 'optim' is struggling.  

I would love to see AD in R, but I think it would be difficult 
to combine many of the things that makes R so wonderful 
and flexible to work with, like logical indexing, with AD, but 
I could be wrong. 

Anders.     
     



On Thursday 26 January 2006 05:55 am, Hans Skaug wrote:
> Dear Alberto,
>
> There are fisheries people also in Europe using AD Model Builder
> (Denmark and England for instance), but you are probably right that
> it is more widespread in North America. There is also effort going
> on where people try to make assessment models written in ADMB
> callable from R.
>
> best regards,
>
> hans
>
> > think AD Model Builder is mainly used for fisheries assessment in
> > North America and, it seems, also in Australia. In Europe, R is
> > still the de-facto standard for fisheries assessment. However,
> > I'd like to support Bill Venables' suggestion. I've been
> > resisting to adopt AD model builder, or to start using again that
> > other system not unlike R, mainly because of the licence price
> > and because I really like R as a tool for almost everything. But
> > an AD function would really make a huge difference for my work.
> > There are free tools that can be used to perform AD on C or
> > Fortran code (e.g. http://www.autodiff.org). One of the
> > difficulties to use them with R is the need to translate the R
> > code into C of Fortran code, but probably there are many other
> > problems that I'm not able to see.
> >
> >Alberto
> >--
>
> _____________________________
> Hans Julius Skaug
>
> Department of Mathematics
> University of Bergen
> Johannes Brunsgate 12
> 5008 Bergen
> Norway
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Eric.Kort at vai.org  Thu Jan 26 20:39:10 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Thu, 26 Jan 2006 14:39:10 -0500
Subject: [R] Image Processing packages
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC19D@VAIEXCH04.vai.org>

Thomas Kaliwe wrote:
> Hi,
>  
> I've been looking for Image Processing packages. Thresholding, Edge
> Filters, Dct, Segmentation, Restoration. I'm aware, that Octave,
Matlab
> etc. would be a good address but then I'm missing the "statistical
> power"  of R. Does anybody know of packages, projects etc. Comments on
> wether the use of R for such matters is useful are welcome.
>  

See also my package rtiff for reading tiff images.

I routinely do image analysis in R.  Yes, it is relatively slow compared
to dedicated solutions, but I like the smooth integration with the
associated statistical analysis and the ability to have a single script
that performs the image analysis and multiple files and subsequent
statistical analysis, and with modern computing equipment R is fast
enough for my purposes.  

I have a variety of standard image processing functions written in R,
but have yet to distribute them because most people choose not to
perform image analysis in R for the previously stated reasons.  

So in general I would agree that R is sub-optimal for image processing
(and this is certainly outside the realm of things R was intended to do
if I read the early mailing list archives correctly).  However, it can
be done and it might be desirable to do so from a work-flow perspective.

-Eric

> Greetings
>  
> Thomas Kaliwe
This email message, including any attachments, is for the so...{{dropped}}



From Markus.Preisetanz at clientvela.com  Thu Jan 26 20:48:29 2006
From: Markus.Preisetanz at clientvela.com (Markus Preisetanz)
Date: Thu, 26 Jan 2006 20:48:29 +0100
Subject: [R] =?iso-8859-1?q?cluster_analysis=3A_=22error_in_vector=28=22do?=
	=?iso-8859-1?q?uble=22=2C_length=29=3A_given_vector_size_is_too_bi?=
	=?iso-8859-1?q?g_=7BFehler_in_vector=28=22double=22=2C_length=29_?=
	=?iso-8859-1?q?=3A_angegebene_Vektorgr=F6=DFe_ist_zu_gro=DF=7D?=
Message-ID: <79799E69EA1DA246A51F983B5663BEA2CD3498@server2.hq.clientvela.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/21edef2b/attachment.pl

From kalar1 at wp.pl  Thu Jan 26 20:57:26 2006
From: kalar1 at wp.pl (Kuba)
Date: Thu, 26 Jan 2006 20:57:26 +0100
Subject: [R] boot & boot.ci
Message-ID: <43d929a6d5dc5@wp.pl>

Dear all, 
   I've a problem with bootstarp package... I want to bootstrap 
correlation ratio of some data. 
 my code is:
 f<-function(d) cov(d$x,d$y)/sqrt(var(x)*var(y)) 
 boot(d,f,R) 
 
 As a result I got an error: 
 Error in statistic(data, original, ...) : unused argument(s) (.. 
 .) 
 
 d is a data.frame containing two vectors with labels x and y. 
 
 Also, another thing I have problem with is generating the 
confidence interval using boot.ci 
 
 if I write: 
 d.boot<-boot(d,cor,R) 
 boot.ci(d.boot,conf=0.95,type="bca") 
 
 I get an error: 
 Error in bca.ci(boot.out, conf, index[1], L = L, t = t.o, t0 = 
 t0.o, h = h,  : 
         estimated adjustment 'a' is NA 
 
 Last question is how to generate a studentized intervals for 
 such data? 
 
 Thanks, 
 Kuba

Kuba

----------------------------------------------------
Cameron Diaz i Toni Collette w filmie SIOSTRY 
W kinach od 27 stycznia 2005 r. 
http://klik.wp.pl/?adr=http%3A%2F%2Fadv.reklama.wp.pl%2Fas%2Fsiostry.html&sid=641



From droberts at montana.edu  Thu Jan 26 20:48:55 2006
From: droberts at montana.edu (Dave Roberts)
Date: Thu, 26 Jan 2006 12:48:55 -0700
Subject: [R] understanding patterns in categorical vs. continuous data
In-Reply-To: <200601261111.00692.dylan.beaudette@gmail.com>
References: <200601261111.00692.dylan.beaudette@gmail.com>
Message-ID: <43D927A7.1090406@montana.edu>

You might prefer boxplot(insolation~veg_type) as a graphic.  That will 
give you quantiles.  To get the actual numeric values you could

for (i in levels(veg_type)) {
    print(i)
    quantile(insolation[veg_type==i])
}

see ?quantile for more help.

Dylan Beaudette wrote:
> Greetings,
> 
> I have a set of bivariate data: one variable (vegetation type) which is 
> categorical, and one (computed annual insolation) which is continuous. 
> Plotting veg_type ~ insolation produces a nice overview of the patterns that 
> I can see in the source data. However, due to the large number of samples 
> (1,000), and the apparent "spread" in the distribution of a single vegetation 
> type over a range of insolation values- I having a hard time quantitatively 
> describing the relationship between the two variables. 
> 
> Here is a link to a sample graph:
> http://casoilresource.lawr.ucdavis.edu/drupal/node/162
> 
> Since the data along each vegetation type "line" is not a distribution in the 
> traditional sense, I am having problems applying descriptive statistical 
> methods. Conceptually, I would like to some how describe the variation with 
> insolation, along each vegetation type "line".
> 
> Any guidance, or suggested reading material would be greatly appreciated.
> 
> 


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460



From ggrothendieck at gmail.com  Thu Jan 26 21:03:40 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jan 2006 15:03:40 -0500
Subject: [R] understanding patterns in categorical vs. continuous data
In-Reply-To: <200601261111.00692.dylan.beaudette@gmail.com>
References: <200601261111.00692.dylan.beaudette@gmail.com>
Message-ID: <971536df0601261203q3ef21112tba0f66d4354617d1@mail.gmail.com>

Would this do?

boxplot(Sepal.Length ~ Species, iris, horizontal = TRUE)
library(Hmisc)
summary(Sepal.Length ~ Species, iris, fun = summary)


On 1/26/06, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> Greetings,
>
> I have a set of bivariate data: one variable (vegetation type) which is
> categorical, and one (computed annual insolation) which is continuous.
> Plotting veg_type ~ insolation produces a nice overview of the patterns that
> I can see in the source data. However, due to the large number of samples
> (1,000), and the apparent "spread" in the distribution of a single vegetation
> type over a range of insolation values- I having a hard time quantitatively
> describing the relationship between the two variables.
>
> Here is a link to a sample graph:
> http://casoilresource.lawr.ucdavis.edu/drupal/node/162
>
> Since the data along each vegetation type "line" is not a distribution in the
> traditional sense, I am having problems applying descriptive statistical
> methods. Conceptually, I would like to some how describe the variation with
> insolation, along each vegetation type "line".
>
> Any guidance, or suggested reading material would be greatly appreciated.
>
>
> --
> Dylan Beaudette
> Soils and Biogeochemistry Graduate Group
> University of California at Davis
> 530.754.7341
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jholtman at gmail.com  Thu Jan 26 21:20:27 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 26 Jan 2006 15:20:27 -0500
Subject: [R] DOS command using "system"
In-Reply-To: <BAY110-F38C89E0D65883B7E155DF5C7150@phx.gbl>
References: <BAY110-F38C89E0D65883B7E155DF5C7150@phx.gbl>
Message-ID: <644e1f320601261220t20fd0ac8ra0fdbce6583f3a55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/a6e49c5d/attachment.pl

From jholtman at gmail.com  Thu Jan 26 21:42:18 2006
From: jholtman at gmail.com (jim holtman)
Date: Thu, 26 Jan 2006 15:42:18 -0500
Subject: [R] Data management problem: convert text string to matrix of
	0's and 1's
In-Reply-To: <Pine.LNX.4.64.0601261037050.27941@homer23.u.washington.edu>
References: <43D91584.2060307@brown.EDU>
	<Pine.LNX.4.64.0601261037050.27941@homer23.u.washington.edu>
Message-ID: <644e1f320601261242g48d8786dnab8a67515a2fb72f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/d0f6951a/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jan 26 22:01:31 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jan 2006 22:01:31 +0100
Subject: [R] DOS command using "system"
In-Reply-To: <644e1f320601261220t20fd0ac8ra0fdbce6583f3a55@mail.gmail.com>
References: <BAY110-F38C89E0D65883B7E155DF5C7150@phx.gbl>
	<644e1f320601261220t20fd0ac8ra0fdbce6583f3a55@mail.gmail.com>
Message-ID: <43D938AB.2050503@statistik.uni-dortmund.de>

jim holtman wrote:

> Why don't you use 'unlink':
> 
> unlink('c:/Program Files/DOSPROGRAM/input.dat')
> 
> If you really want to use 'del', then you have to invoke the command
> processor:
> 
> system('cmd /c del "c:\\Program Files\\DOSPROGRAM\\input.dat"')

See also ?shell

Uwe Ligges

> 
> On 1/26/06, Taka Matzmoto <sell_mirage_ne at hotmail.com> wrote:
> 
>>HI R users
>>I have one question for using DOS command through "system"
>>I like to delete a file that is located at C:\Program
>>Files\DOSPROGRAM\input.dat
>>I can use a DOS command "del" on Dos prompt like this
>>
>>C:\Documents and Settings> del "C:\Program Files\DOSPROGRAM\input.dat"
>>
>>to delete input.dat file.
>>
>>When I try to do the same thing on R using "system" command
>>
>>system('del "C:\Program Files\DOSPROGRAM\input.dat"')
>>or
>>system("del "C:\Program Files\DOSPROGRAM\input.dat"")
>>or
>>system(paste("del", "\"C:\\Program Files\\DOSPROGRAM\\input.dat\"",sep="
>>"))
>>
>>All the three system commands did work
>>
>>Could you help me to figure out ?
>>
>>Thanks in advance
>>
>>TM
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> 
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dgrove at fhcrc.org  Thu Jan 26 23:32:37 2006
From: dgrove at fhcrc.org (Douglas Grove)
Date: Thu, 26 Jan 2006 14:32:37 -0800 (PST)
Subject: [R] can I do this with read.table??
Message-ID: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>

Hi,

I'm trying to figure out if there's an automated way to get
read.table to read in my data and *not* convert the character
columns into anything, just leave them alone.  What I'm referring
to as 'character columns' are columns in the data that are quoted.
For columns of alphabetic strings (that aren't TRUE or FALSE) I can
suppress conversion to factor with as.is=TRUE, but what I'd like to
stop is the conversion of quoted numbers of the form "01","02",..., 
into numeric form.
 
By an 'automated way', I mean one that does not involve me having
to know which columns in the data are the ones I want kept as
they are.

This doesn't seem like an unreasonable thing to want to do.
After all, say I've got the data.frame:

  A <- data.frame(a=1:3, b=I(c("01","02","03")))

I can export this to a text file with the simple command

  write.table(A, "A.txt", sep="\t", row.names=FALSE, quote=TRUE)

but I cannot find an equally simple mechanism for reading this
data back in from A.txt that allows me to reconstruct my
data.frame 'A'.  Is this an unreasonable thing to expect?

Thanks,
Doug



From gescati at yahoo.com.ar  Thu Jan 26 23:42:21 2006
From: gescati at yahoo.com.ar (=?iso-8859-1?q?gabriela=20escati=20pe=F1aloza?=)
Date: Thu, 26 Jan 2006 22:42:21 +0000 (GMT)
Subject: [R] degrees freedom in nlme
Message-ID: <20060126224221.50334.qmail@web37108.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/418b5760/attachment.pl

From tlumley at u.washington.edu  Thu Jan 26 23:49:04 2006
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 Jan 2006 14:49:04 -0800 (PST)
Subject: [R] can I do this with read.table??
In-Reply-To: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>
References: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>
Message-ID: <Pine.LNX.4.64.0601261438560.27941@homer23.u.washington.edu>

On Thu, 26 Jan 2006, Douglas Grove wrote:

> Hi,
>
> I'm trying to figure out if there's an automated way to get
> read.table to read in my data and *not* convert the character
> columns into anything, just leave them alone.  What I'm referring
> to as 'character columns' are columns in the data that are quoted.
> For columns of alphabetic strings (that aren't TRUE or FALSE) I can
> suppress conversion to factor with as.is=TRUE, but what I'd like to
> stop is the conversion of quoted numbers of the form "01","02",...,
> into numeric form.

One approach is to use quote="" to specify that the quote marks are 
considered part of the string. Depending on what you are using the data 
for, you may then need to strip the quotes off, 
eg with sub().
> B<-read.table("a.txt",sep="\t",as.is=TRUE,header=TRUE,quote="")
> B
   X.a. X.b.
1    1 "01"
2    2 "02"
3    3 "03"
> sub("^\"(.*)\"$", "\\1", B[[2]])
[1] "01" "02" "03"

 	-thomas



From kjetilbrinchmannhalvorsen at gmail.com  Thu Jan 26 23:50:22 2006
From: kjetilbrinchmannhalvorsen at gmail.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 26 Jan 2006 22:50:22 -0000
Subject: [R] can I do this with read.table??
In-Reply-To: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>
References: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>
Message-ID: <440230C3.1030609@gmail.com>

Douglas Grove wrote:
> Hi,
> 
> I'm trying to figure out if there's an automated way to get
> read.table to read in my data and *not* convert the character
> columns into anything, just leave them alone.  What I'm referring

?Did you read the help page?
What about argument as.is=TRUE?
See also argument colClasses

Kjetil

> to as 'character columns' are columns in the data that are quoted.
> For columns of alphabetic strings (that aren't TRUE or FALSE) I can
> suppress conversion to factor with as.is=TRUE, but what I'd like to
> stop is the conversion of quoted numbers of the form "01","02",..., 
> into numeric form.
>  
> By an 'automated way', I mean one that does not involve me having
> to know which columns in the data are the ones I want kept as
> they are.
> 
> This doesn't seem like an unreasonable thing to want to do.
> After all, say I've got the data.frame:
> 
>   A <- data.frame(a=1:3, b=I(c("01","02","03")))
> 
> I can export this to a text file with the simple command
> 
>   write.table(A, "A.txt", sep="\t", row.names=FALSE, quote=TRUE)
> 
> but I cannot find an equally simple mechanism for reading this
> data back in from A.txt that allows me to reconstruct my
> data.frame 'A'.  Is this an unreasonable thing to expect?
> 
> Thanks,
> Doug
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dgrove at fhcrc.org  Thu Jan 26 23:57:17 2006
From: dgrove at fhcrc.org (Douglas Grove)
Date: Thu, 26 Jan 2006 14:57:17 -0800 (PST)
Subject: [R] can I do this with read.table??
In-Reply-To: <440230C3.1030609@gmail.com>
References: <Pine.LNX.4.61.0601261347210.9567@echidna.fhcrc.org>
	<440230C3.1030609@gmail.com>
Message-ID: <Pine.LNX.4.61.0601261454430.11071@echidna.fhcrc.org>

I did read the help page, very carefully.   

The colClasses argument can be used if I want
to stop and look through every data set to see
which column I need to protect.  But that's what I 
said that I don't want to do.

As for 'as.is', I wish it did what you suggest, but
it doesn't.  If one reads carefully, as.is protects
a character vector from converstion to a *factor*,
but not from conversion to numeric/logical.

Doug




On Sun, 26 Feb 2006, Kjetil Brinchmann Halvorsen wrote:

> Douglas Grove wrote:
> > Hi,
> > 
> > I'm trying to figure out if there's an automated way to get
> > read.table to read in my data and *not* convert the character
> > columns into anything, just leave them alone.  What I'm referring
> 
> ?Did you read the help page?
> What about argument as.is=TRUE?
> See also argument colClasses
> 
> Kjetil
> 
> > to as 'character columns' are columns in the data that are quoted.
> > For columns of alphabetic strings (that aren't TRUE or FALSE) I can
> > suppress conversion to factor with as.is=TRUE, but what I'd like to
> > stop is the conversion of quoted numbers of the form "01","02",..., into
> > numeric form.
> > 
> > By an 'automated way', I mean one that does not involve me having
> > to know which columns in the data are the ones I want kept as
> > they are.
> > 
> > This doesn't seem like an unreasonable thing to want to do.
> > After all, say I've got the data.frame:
> > 
> > A <- data.frame(a=1:3, b=I(c("01","02","03")))
> > 
> > I can export this to a text file with the simple command
> > 
> > write.table(A, "A.txt", sep="\t", row.names=FALSE, quote=TRUE)
> > 
> > but I cannot find an equally simple mechanism for reading this
> > data back in from A.txt that allows me to reconstruct my
> > data.frame 'A'.  Is this an unreasonable thing to expect?
> > 
> > Thanks,
> > Doug
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> 
>



From Meredith.Briggs at team.telstra.com  Fri Jan 27 00:15:57 2006
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 27 Jan 2006 10:15:57 +1100
Subject: [R] How do you convert this from S Plus to R - any help appreciated
	. thanks
Message-ID: <3B5823541A25D311B3B90008C7F905641D8A2DE7@ntmsg0092.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/22f82fd8/attachment.pl

From dsonneborn at ucdavis.edu  Fri Jan 27 00:46:28 2006
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Thu, 26 Jan 2006 15:46:28 -0800
Subject: [R] footnote in postscript lattice
Message-ID: <43D95F54.80609@yellow.ucdavis.edu>

I would like to add a footnote to this graph but do not see a "footnote" command in the package:lattice documentation. I would like to note the "span=.8"
 as the footnote.

postscript(file= ?C:/Documents and Settings/dsonneborn/My Documents/Slovak/output/pcb_tables/smooth_PCB_lines_four.ps?, bg=?transparent?, onefile=FALSE, pointsize=20,paper=?letter?, horizontal=TRUE, family=?Helvetica?,font=?Helvetica?)

xyplot(AWGT ~ lipid_adj_lpcb2_cent | malex*romanix, data=pcb_graph3,

       auto.key = list(lines = TRUE, points = TRUE), ylab=?Birth Weight?, xlab=?Lipid Adjusted PCB?,

       par.settings =

       list(superpose.symbol = list(col = colr, pch = plotchar),

            superpose.line = list(col = colr, pch = plotchar, lty = 1)),

        type=c("p", "smooth"), span=.8)

dev.off()
thanks

-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516



From ripley at stats.ox.ac.uk  Fri Jan 27 00:54:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jan 2006 23:54:45 +0000 (GMT)
Subject: [R] How do you convert this from S Plus to R - any help
 appreciated . thanks
In-Reply-To: <3B5823541A25D311B3B90008C7F905641D8A2DE7@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F905641D8A2DE7@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <Pine.LNX.4.61.0601262346130.908@gannet.stats>

There is an `R Data Import/Export' manual to guide you on how to do data 
export in R.  Please consult it.

I only know vaguely what format exportData(type="ASCII") uses, and it is 
probably easier to work out what you want and tailor write.table() to give 
that.  (I am not aware that the fine details of exportData are 
documented.)

On Fri, 27 Jan 2006, Briggs, Meredith M wrote:

> exportData(MU.Cost,paste("C:/RAUDSL/S",as.character(MU.Cost$Run.Id[1]),"
> .",as.character(MU.Cost$MU.Id[1]),".MU.PRICE.OUTPUT.txt",sep=""),append
> = FALSE,type="ASCII",quote=FALSE)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From KSecrist at buttecounty.net  Fri Jan 27 00:54:48 2006
From: KSecrist at buttecounty.net (Secrist, Kevin)
Date: Thu, 26 Jan 2006 15:54:48 -0800
Subject: [R] Concept Mapping of Qualitative Data
Message-ID: <5BADEF9F1A24CD4CB7778D1CEB3769E2886ACB@bc-mail4.bci.buttecounty.net>

Dear list,

I am a first time user of this list and a ultra-novice "R" user.  I am seeking the ability to graphically depict open-ended responses from a focus group determining data needs of  community mental health staff members.

These responses would be mapped based upon either Theme or concept or domain, then sorted by similar responses and grouped. Next we would like to use multidimensional scaling to structure the responses on a grid.  and then basically connect the dots of the themed responses into clusters.  

Is there any templates or coding out there for this type of analysis and graphing?

Thank you,

Kevin Secrist
Decision Support Unit
Butte County Behavioral Health 
CONFIDENTIALITY NOTICE:  This e-mail transmission, and any d...{{dropped}}



From btyner at gmail.com  Fri Jan 27 01:13:52 2006
From: btyner at gmail.com (Benjamin Tyner)
Date: Thu, 26 Jan 2006 19:13:52 -0500
Subject: [R] xyplot: making strip function aware of panel.number
Message-ID: <43D965C0.3030500@stat.purdue.edu>

In xyplot(), I'd like to supress the printing of the strip on certain 
panels. I thought I'd do this inside the strip function, but it seems 
that panel.number doesn't get passed to that. Any ideas?

Thanks,
Ben



From gerifalte28 at hotmail.com  Fri Jan 27 01:16:10 2006
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 27 Jan 2006 00:16:10 +0000
Subject: [R] footnote in postscript lattice
In-Reply-To: <43D95F54.80609@yellow.ucdavis.edu>
Message-ID: <BAY103-F36A9C9AECAECB54C9AC71DA6140@phx.gbl>

After you create your xyplot use
library(grid)
panel.text(grid.locator(),label="My label")

Cheers

Francisco

PS:  How is good ol' David these days?

>From: Dean Sonneborn <dsonneborn at ucdavis.edu>
>To: r-help at stat.math.ethz.ch
>Subject: [R] footnote in postscript lattice
>Date: Thu, 26 Jan 2006 15:46:28 -0800
>
>I would like to add a footnote to this graph but do not see a "footnote" 
>command in the package:lattice documentation. I would like to note the 
>"span=.8"
>  as the footnote.
>
>postscript(file= C:/Documents and Settings/dsonneborn/My 
>Documents/Slovak/output/pcb_tables/smooth_PCB_lines_four.ps, 
>bg=transparent, onefile=FALSE, pointsize=20,paper=letter, 
>horizontal=TRUE, family=Helvetica,font=Helvetica)
>
>xyplot(AWGT ~ lipid_adj_lpcb2_cent | malex*romanix, data=pcb_graph3,
>
>        auto.key = list(lines = TRUE, points = TRUE), ylab=Birth Weight, 
>xlab=Lipid Adjusted PCB,
>
>        par.settings =
>
>        list(superpose.symbol = list(col = colr, pch = plotchar),
>
>             superpose.line = list(col = colr, pch = plotchar, lty = 1)),
>
>         type=c("p", "smooth"), span=.8)
>
>dev.off()
>thanks
>
>--
>Dean Sonneborn, MS
>Programmer Analyst
>Department of Public Health Sciences
>University of California, Davis
>(530) 754-9516
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Fri Jan 27 01:34:48 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 26 Jan 2006 18:34:48 -0600
Subject: [R] xyplot: making strip function aware of panel.number
In-Reply-To: <43D965C0.3030500@stat.purdue.edu>
References: <43D965C0.3030500@stat.purdue.edu>
Message-ID: <eb555e660601261634k5fd90e5fq691c9ce0b05987f4@mail.gmail.com>

On 1/26/06, Benjamin Tyner <btyner at gmail.com> wrote:
> In xyplot(), I'd like to supress the printing of the strip on certain
> panels. I thought I'd do this inside the strip function, but it seems
> that panel.number doesn't get passed to that. Any ideas?

Well, strip has access to much more detailed conditioning information
(which.given, which.panel), why are they not enough? The panel.number
feature exists for exactly the opposite reason, namely that panel
functions traditionally didn't have the information on conditioning
variables that the strip function has.

Deepayan



From deepayan.sarkar at gmail.com  Fri Jan 27 01:36:10 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 26 Jan 2006 18:36:10 -0600
Subject: [R] footnote in postscript lattice
In-Reply-To: <BAY103-F36A9C9AECAECB54C9AC71DA6140@phx.gbl>
References: <43D95F54.80609@yellow.ucdavis.edu>
	<BAY103-F36A9C9AECAECB54C9AC71DA6140@phx.gbl>
Message-ID: <eb555e660601261636qafb1fe6r294bd4598f4b5e18@mail.gmail.com>

On 1/26/06, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> After you create your xyplot use
> library(grid)
> panel.text(grid.locator(),label="My label")

Also, look up the 'page' argument in ?xyplot.

Deepayan

>
> Cheers
>
> Francisco
>
> PS:  How is good ol' David these days?
>
> >From: Dean Sonneborn <dsonneborn at ucdavis.edu>
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] footnote in postscript lattice
> >Date: Thu, 26 Jan 2006 15:46:28 -0800
> >
> >I would like to add a footnote to this graph but do not see a "footnote"
> >command in the package:lattice documentation. I would like to note the
> >"span=.8"
> >  as the footnote.
> >
> >postscript(file= "C:/Documents and Settings/dsonneborn/My
> >Documents/Slovak/output/pcb_tables/smooth_PCB_lines_four.ps",
> >bg="transparent", onefile=FALSE, pointsize=20,paper="letter",
> >horizontal=TRUE, family="Helvetica",font="Helvetica")
> >
> >xyplot(AWGT ~ lipid_adj_lpcb2_cent | malex*romanix, data=pcb_graph3,
> >
> >        auto.key = list(lines = TRUE, points = TRUE), ylab="Birth Weight",
> >xlab="Lipid Adjusted PCB",
> >
> >        par.settings =
> >
> >        list(superpose.symbol = list(col = colr, pch = plotchar),
> >
> >             superpose.line = list(col = colr, pch = plotchar, lty = 1)),
> >
> >         type=c("p", "smooth"), span=.8)
> >
> >dev.off()
> >thanks
> >
> >--
> >Dean Sonneborn, MS
> >Programmer Analyst
> >Department of Public Health Sciences
> >University of California, Davis
> >(530) 754-9516



From duncan.mackay at flinders.edu.au  Fri Jan 27 02:16:11 2006
From: duncan.mackay at flinders.edu.au (Duncan Mackay)
Date: Fri, 27 Jan 2006 11:46:11 +1030
Subject: [R] Justification of dendrogram labels
Message-ID: <004b01c622df$42b2bfe0$70e66081@bio.flinders.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/93565280/attachment.pl

From debayan.datta at yale.edu  Fri Jan 27 03:14:50 2006
From: debayan.datta at yale.edu (Debayan Datta)
Date: Thu, 26 Jan 2006 21:14:50 -0500 (EST)
Subject: [R] question about density estimation with monotonic constraints
Message-ID: <Pine.LNX.4.63.0601262111440.22012@ares.its.yale.edu>

Hi,
   I have a 1-d vector x of values but it is rather noisy. I know a priori 
that the density function should be monotonically decreasing, but my data 
doesnt have that. Is there a way to transform the data in such a way so 
that the resultant density be monotonically decreasing?
Thanks
Debayan



From spencer.graves at pdf.com  Fri Jan 27 03:34:22 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 26 Jan 2006 18:34:22 -0800
Subject: [R] nlme in R v.2.2.1 and S-Plus v. 7.0
In-Reply-To: <43D5103E.3050907@cig.unige.ch>
References: <43D5103E.3050907@cig.unige.ch>
Message-ID: <43D986AE.3040203@pdf.com>

	  I see you got an error message from R.  Did you have both either the 
lme4 or the Matrix packages in the search path at the same time you ran 
nlme to get the result you got below?  If yes, please rerun with only 
nlme in the search path.  (This may not be necessary, but I always quite 
R and restart whenever I want to switch between nlme and lme4.)

	  If this is not the problem, I would encourage you to experient with 
smaller models and data sets to try to find the simplest "toy example" 
that still produces the error message you got from summary(nlme(...)), 
then submit that to this listserve.  I routinely copy reproducible 
examples into R to see if I get the same error message.  Whether I do or 
not, I think my comments are much more likely to be helpful than if I 
have to guess.  And if I'm guessing about an error message I can't 
generate myself at will, my comments may not be very helpful.

	  Regarding whether to use S-Plus 7 or R 2.2.1 for this problem, if R 
gives you an error message when S-Plus does not, doesn't that answer 
your question?  Moreover, the S-Plus logLik is higher than for R, which 
suggests that it must get closer to the actual maximum of the likelihood 
function.

	  If you'd like more help from this listserve, I suggest you PLEASE do 
read the posting guide! "www.R-project.org/posting-guide.html".  Doing 
so will on average tend to increase the speed and utility of comments 
you might receive, I believe.

	  hope this helps,
	  spencer graves

Paolo Ghisletta wrote:

> Dear R-Users,
> 
> I am comparing the nlme package in S-Plus (v. 7.0) and R (v. 2.2.1, nlme 
> package version 3.1-68.1; the lattice, Matrix, and lme4 have also just 
> been updated today, Jan. 23, 2006) on a PC (2.40 GHz Pentium 4 processor 
> and 1 GHz RAM) operating on Windows XP. I am using a real data set with 
> 1,191 units with at most 4 repeated measures per unit (data are 
> incomplete, unbalanced). I use the same code with the same starting 
> values for both programs and obtain slightly different results. I am 
> aware that at this stage my model is far from being well specified for 
> the given data. Nevertheless, I wonder whether one program is more 
> suited than the other to pursue my modeling.
> 
> Below I included the input + output code, first for S-Plus, than for R.
> 
> Many thanks and best regards,
> Paolo Ghisletta
> 
> ############
> #S-Plus
> #min=4, max=41
>  > logistic4.a <- nlme(jugs ~ 4 + 41 / (1 + xmid*exp( -scal*I(occ-1) + 
> u)), fixed=scal+xmid~1, random= u~1 |id, start=c(scal=.2, xmid=155), 
> data=jug, na.action=na.exclude, method="ML")
>  > summary(logistic4.a)
> Nonlinear mixed-effects model fit by maximum likelihood
>   Model: jugs ~ 4 + 41/(1 + xmid * exp( - scal * I(occ - 1) + u))
>  Data: jug
>        AIC     BIC    logLik
>   29595.62 29621.3 -14793.81
> 
> Random effects:
>  Formula: u ~ 1 | id
>                u Residual
> StdDev: 5.162391 3.718887
> 
> Fixed effects: scal + xmid ~ 1
>         Value Std.Error   DF  t-value p-value
> scal   4.9697    0.0823 3339 60.39508  <.0001
> xmid 683.5634  125.8509 3339  5.43153  <.0001
> 
> Standardized Within-Group Residuals:
>        Min         Q1          Med        Q3      Max
>  -10.66576 -0.5039498 0.0002772166 0.1226745 5.453209
> 
> Number of Observations: 4531
> Number of Groups: 1191
> 
> ############
> # R
>  > #min=4, max=41
>  > logistic4.a <- nlme(jugs ~ 4 + 41 / (1 + xmid*exp( -scal*I(occ-1)+ 
> u)), data=jug, fixed=scal+xmid~1, random= u~1 |id, start=c(scal=.2, 
> xmid=155), method="ML", na.action=na.exclude)
>  > summary(logistic4.a)
> Nonlinear mixed-effects model fit by maximum likelihood
>   Model: jugs ~ 4 + 41/(1 + xmid * exp(-scal * I(occ - 1) + u))
>  Data: jug
>        AIC      BIC    logLik
>   29678.11 29703.78 -14835.05
> 
> Random effects:
>  Formula: u ~ 1 | id
>                u Residual
> StdDev: 5.116542 3.767097
> 
> Fixed effects: scal + xmid ~ 1
>         Value Std.Error   DF  t-value p-value
> scal   4.9244   0.08121 3339 60.63763       0
> xmid 633.6956 115.37512 3339  5.49248       0
> Erreur dans dim(x) : aucun slot de nom "Dim" pour cet objet de la classe 
> "correlation"
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Jan 27 03:36:27 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jan 2006 21:36:27 -0500
Subject: =?iso-8859-1?Q?RE=3A_=5BR=5D_cluster_analysis=3A_=22error_in_v?=
	=?iso-8859-1?Q?ector=28=22double=22=2C_length=29=3A_given_vector_size_?=
	=?iso-8859-1?Q?is_too_big_=7BFehler_in_vector=28=22double=22=2C_length?=
	=?iso-8859-1?Q?=29_=3A_angegebene_Vektorgr=F6=DFe_ist_zu_gro=DF=7D?=
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED76B@usctmx1106.merck.com>

Let's do some simple calculation:  The dist object from a data set with
80000 cases would have

  80000 * (80000 - 1) / 2 

elements, each takes 8 bytes to be stored in double precision.  That's over
24GB if my arithmetic isn't too flaky.  You'd have a devil of a time trying
to do this on a 64-bit machine with 32GB RAM, let alone what you are using.
You'd have much better chance sticking with algorithms that do not require
storage of the (dis)similarity matrix.

Andy

From: Markus Preisetanz
> 
> Dear R Specialists,
> 
>  
> 
> when trying to cluster a data.frame with about 80.000 rows 
> and 25 columns I get the above error message. I tried hclust 
> (using dist), agnes (entering the data.frame directly) and 
> pam (entering the data.frame directly). What I actually do 
> not want to do is generate a random sample from the data.
> 
>  
> 
> The machine I run R on is a Windows 2000 Server (Pentium 4) 
> with 2 GB of RAM.
> 
>  
> 
> Does anybody know what to do?
> 
>  
> 
> Sincerely
> 
> ___________________
> 
> Markus Preisetanz
> 
> Consultant
> 
>  
> 
> Client Vela GmbH
> 
> Albert-Ro??haupter-Str. 32
> 
> 81369 M??nchen
> 
> fon:          +49 (0) 89 742 17-113
> 
> fax:          +49 (0) 89 742 17-150
> 
> mailto:markus.preisetanz at clientvela.com 
> <mailto:markus.preisetanz at clientvela.com> 
> 
> 
> 
> Diese E-Mail enth??lt vertrauliche und/oder rechtlich 
> gesch??tzte Informationen. Wenn Sie nicht der richtige 
> Adressat sind oder diese E-Mail irrt??mlich erhalten haben, 
> informieren Sie bitte sofort den Absender und vernichten Sie 
> diese Mail. Das unerlaubte Kopieren sowie die unbefugte 
> Weitergabe dieser E-Mail ist nicht gestattet.
> 
> This e-mail may contain confidential and/or privileged 
> infor...{{dropped}}
> 
>



From Manuel.A.Morales at williams.edu  Fri Jan 27 03:45:05 2006
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 26 Jan 2006 21:45:05 -0500
Subject: [R] Nesting Functions
Message-ID: <1138329906.2610.6.camel@localhost.localdomain>

Dear list members,

I'm looking for a way to write "nested" functions similar to the
function "Nest" or "NestList" in Mathematica.

E.g.,

f<-function(x) x+2*x

f(f(f(2)))

might instead be written as nest(f, 2, 3)

read as, nest function f 3 times with 2 as the initial value.

Thanks!

Manuel



From klebyn at yahoo.com.br  Fri Jan 27 04:00:04 2006
From: klebyn at yahoo.com.br (klebyn)
Date: Fri, 27 Jan 2006 01:00:04 -0200
Subject: [R] about lm restrictions...
Message-ID: <43D98CB4.4020202@yahoo.com.br>


Hello all R-users


_question 1_

I need to make a statistical model and respective ANOVA table
but I get distinct results for

the T-test (in summary(lm.object) function) and
the F-test (in   anova(lm.object) )

shouldn't this two approach give me the same result, i.e
to indicate the same significants terms in both tests???????

obs.

The system has two restrictions:
1) sum( x_i ) = 1
2) sum( z_j ) = 1



*output below*

_question 2_


Has I to considerate a SST in ANOVA table with:

1) N-2 d.f. because of 2 restrictions?
 or
2) N-1 d.f. because of 1 global restriction: sum( x ) + sum( z ) = 2 ?


I don't find any paper, book or another reference,
if someone may to indicate references for this type model (with 2 
restrictions),
I would be very grateful.


Thanks a lot.
Regards
 
 
Cleber N. Borges



###############################
#         OUTPUT
###############################


Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  15.5000     0.5270  29.409 2.97e-10 ***
z1:x1        -5.0000     0.7454  -6.708 8.77e-05 ***
z1:x2         0.5000     0.7454   0.671 0.519177   
z1:x3        -3.0000     0.7454  -4.025 0.002996 **
z2:x1        -6.0000     0.7454  -8.050 2.11e-05 ***
z2:x2        -5.0000     0.7454  -6.708 8.77e-05 ***
z2:x3        -4.5000     0.7454  -6.037 0.000193 ***
z3:x1         1.0000     0.7454   1.342 0.212580   
z3:x2         1.5000     0.7454   2.012 0.075029 . 
z3:x3             NA         NA      NA       NA   

Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)   
z1:x1      1 16.674  16.674 30.0125 0.0003910 ***
z1:x2      1 13.580  13.580 24.4446 0.0007977 ***
z1:x3      1  1.190   1.190  2.1429 0.1772677   
z2:x1      1 35.267  35.267 63.4800 2.287e-05 ***
z2:x2      1 32.400  32.400 58.3200 3.202e-05 ***
z2:x3      1 42.667  42.667 76.8000 1.061e-05 ***
z3:x1      1  0.083   0.083  0.1500 0.7075349   
z3:x2      1  2.250   2.250  4.0500 0.0750295 . 
Residuals  9  5.000   0.556                     
---





###############################
#         DATA
###############################

  z1 z2 z3 x1 x2 x3  y
  1  0  0  1  0  0 10
  1  0  0  0  1  0 15
  1  0  0  0  0  1 12
  0  1  0  1  0  0 10
  0  1  0  0  1  0 11
  0  1  0  0  0  1 11
  0  0  1  1  0  0 16
  0  0  1  0  1  0 17
  0  0  1  0  0  1 15
  1  0  0  1  0  0 11
  1  0  0  0  1  0 17
  1  0  0  0  0  1 13
  0  1  0  1  0  0  9
  0  1  0  0  1  0 10
  0  1  0  0  0  1 11
  0  0  1  1  0  0 17
  0  0  1  0  1  0 17
  0  0  1  0  0  1 16



###############################
#         CODE
###############################


 x = read.table(file("clipboard"),h=T)

## NOT a Scheff?? Model:
 
 x.lm <- lm( y ~ (z1+z2+z3):(x1+x2+x3), data=x)
 summary(x.lm)
 anova(x.lm)


## Scheff?? Model: <- IS CORRECT the analysis below?
 
 x.lm <- lm( y ~ -1 + (z1+z2+z3):(x1+x2+x3), data=x)
 summary(x.lm)

 x.aov <- aov( y ~  (z1+z2+z3):(x1+x2+x3), data=x)
 summary(x.aov)



From murdoch at stats.uwo.ca  Fri Jan 27 03:55:05 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 26 Jan 2006 21:55:05 -0500
Subject: [R] Nesting Functions
In-Reply-To: <1138329906.2610.6.camel@localhost.localdomain>
References: <1138329906.2610.6.camel@localhost.localdomain>
Message-ID: <43D98B89.1010905@stats.uwo.ca>

On 1/26/2006 9:45 PM, Manuel Morales wrote:
> Dear list members,
> 
> I'm looking for a way to write "nested" functions similar to the
> function "Nest" or "NestList" in Mathematica.
> 
> E.g.,
> 
> f<-function(x) x+2*x
> 
> f(f(f(2)))
> 
> might instead be written as nest(f, 2, 3)
> 
> read as, nest function f 3 times with 2 as the initial value.

It's easy enough using a for loop:

nest <- function(f, initial, reps) {
    result <- initial
    for (i in seq(len=reps)) result <- f(result)
    result
}

Duncan Murdoch



From andy_liaw at merck.com  Fri Jan 27 04:07:42 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jan 2006 22:07:42 -0500
Subject: [R] understanding patterns in categorical vs. continuous data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED76D@usctmx1106.merck.com>

From: Dave Roberts
> 
> You might prefer boxplot(insolation~veg_type) as a graphic.  
> That will 
> give you quantiles.  To get the actual numeric values you could
> 
> for (i in levels(veg_type)) {
>     print(i)
>     quantile(insolation[veg_type==i])
> }
> 
> see ?quantile for more help.

If you want the five-number summaries plotted in the boxplots, just look at
the returned object of boxplot():

> g <- factor(rep(1:3, 10))
> y <- rnorm(30)
> res <- boxplot(y ~ g)
> str(res)
List of 6
 $ stats: num [1:5, 1:3] -1.135 -0.757 -0.536  0.499  0.996 ...
 $ n    : num [1:3] 10 10 10
 $ conf : num [1:2, 1:3] -1.1639  0.0918 -0.5208  1.6546 -1.2487 ...
 $ out  : num(0) 
 $ group: num(0) 
 $ names: chr [1:3] "1" "2" "3"

If you just want to compute the summaries without the boxplots, use
fivenum():

> tapply(y, g, fivenum)
$"1"
[1] -1.1352456 -0.7571895 -0.5360496  0.4994445  0.9956749

$"2"
[1] -1.1408493 -0.3751730  0.5668747  1.8018146  2.0019303

$"3"
[1] -2.2309983 -0.9333305 -0.3402786  0.8849042  0.9833057

... and if you really want the quantiles, you can do that, too:

> tapply(y, g, quantile)
$"1"
        0%        25%        50%        75%       100% 
-1.1352456 -0.7391977 -0.5360496  0.3378861  0.9956749 

$"2"
        0%        25%        50%        75%       100% 
-1.1408493 -0.3039648  0.5668747  1.6669879  2.0019303 

$"3"
        0%        25%        50%        75%       100% 
-2.2309983 -0.8389260 -0.3402786  0.6746950  0.9833057 

... but note how the quartiles and hinges are not necessarily the same.

Andy
 
> Dylan Beaudette wrote:
> > Greetings,
> > 
> > I have a set of bivariate data: one variable (vegetation 
> type) which is 
> > categorical, and one (computed annual insolation) which is 
> continuous. 
> > Plotting veg_type ~ insolation produces a nice overview of 
> the patterns that 
> > I can see in the source data. However, due to the large 
> number of samples 
> > (1,000), and the apparent "spread" in the distribution of a 
> single vegetation 
> > type over a range of insolation values- I having a hard 
> time quantitatively 
> > describing the relationship between the two variables. 
> > 
> > Here is a link to a sample graph:
> > http://casoilresource.lawr.ucdavis.edu/drupal/node/162
> > 
> > Since the data along each vegetation type "line" is not a 
> distribution in the 
> > traditional sense, I am having problems applying 
> descriptive statistical 
> > methods. Conceptually, I would like to some how describe 
> the variation with 
> > insolation, along each vegetation type "line".
> > 
> > Any guidance, or suggested reading material would be 
> greatly appreciated.
> > 
> > 
> 
> 
> -- 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~
> David W. Roberts                                     office 
> 406-994-4548
> Professor and Head                                      FAX 
> 406-994-3190
> Department of Ecology                         email 
> droberts at montana.edu
> Montana State University
> Bozeman, MT 59717-3460
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Jan 27 04:22:06 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jan 2006 22:22:06 -0500
Subject: [R] about lm restrictions...
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED76E@usctmx1106.merck.com>



From: klebyn
> 
> Hello all R-users
> 
> 
> _question 1_
> 
> I need to make a statistical model and respective ANOVA table
> but I get distinct results for
> 
> the T-test (in summary(lm.object) function) and
> the F-test (in   anova(lm.object) )
> 
> shouldn't this two approach give me the same result, i.e
> to indicate the same significants terms in both tests???????

No, because they are not the same tests.  The t-tests in summary.lm() test
whether the coefficient is zero, when all other terms are present in the
model.  The F-tests in anova.lm() test the terms by sequentially adding them
into the model.  Here's an example:

> set.seed(1)
> d <- data.frame(x1=runif(20), x2=runif(20), y=rnorm(20))
> fm <- lm(y ~ ., d)
> summary(fm)$coef
              Estimate Std. Error    t value   Pr(>|t|)
(Intercept)  1.0187254  0.5534310  1.8407452 0.08318123
x1          -1.6914784  0.6377065 -2.6524404 0.01675543
x2          -0.1817831  0.6618875 -0.2746435 0.78689983
> anova(fm)
Analysis of Variance Table

Response: y
          Df  Sum Sq Mean Sq F value  Pr(>F)  
x1         1  4.2341  4.2341  7.0936 0.01638 *
x2         1  0.0450  0.0450  0.0754 0.78690  
Residuals 17 10.1472  0.5969                  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> anova(fm2 <- lm(y ~ x2 + x1, d))
Analysis of Variance Table

Response: y
          Df  Sum Sq Mean Sq F value  Pr(>F)  
x2         1  0.0797  0.0797  0.1336 0.71928  
x1         1  4.1994  4.1994  7.0354 0.01676 *
Residuals 17 10.1472  0.5969                  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Notice how the p-value for x1 in the last output matches that of the t-test:
because both are testing if the coefficient for x1 is 0 given that x2 is
already in the model.  (It's the same reason that the p-value for x2 in the
first anova() output matches that of the summary.lm(), but not the second
anova() output.)

I may be off, but I do not think the restrictions you mentioned have any
bearing on the analysis.  If x + z is restricted to something _for each
case_ then you do have to worry, but not the way you have it.  You can
choose the independent variables to take on any value you like (as in
designed experiments), so such restrictions should not matter.

Andy


 
> obs.
> 
> The system has two restrictions:
> 1) sum( x_i ) = 1
> 2) sum( z_j ) = 1
> 
> 
> 
> *output below*
> 
> _question 2_
> 
> 
> Has I to considerate a SST in ANOVA table with:
> 
> 1) N-2 d.f. because of 2 restrictions?
>  or
> 2) N-1 d.f. because of 1 global restriction: sum( x ) + sum( z ) = 2 ?
> 
> 
> I don't find any paper, book or another reference,
> if someone may to indicate references for this type model (with 2 
> restrictions),
> I would be very grateful.
> 
> 
> Thanks a lot.
> Regards
>  
>  
> Cleber N. Borges
> 
> 
> 
> ###############################
> #         OUTPUT
> ###############################
> 
> 
> Coefficients: (1 not defined because of singularities)
>             Estimate Std. Error t value Pr(>|t|)   
> (Intercept)  15.5000     0.5270  29.409 2.97e-10 ***
> z1:x1        -5.0000     0.7454  -6.708 8.77e-05 ***
> z1:x2         0.5000     0.7454   0.671 0.519177   
> z1:x3        -3.0000     0.7454  -4.025 0.002996 **
> z2:x1        -6.0000     0.7454  -8.050 2.11e-05 ***
> z2:x2        -5.0000     0.7454  -6.708 8.77e-05 ***
> z2:x3        -4.5000     0.7454  -6.037 0.000193 ***
> z3:x1         1.0000     0.7454   1.342 0.212580   
> z3:x2         1.5000     0.7454   2.012 0.075029 . 
> z3:x3             NA         NA      NA       NA   
> 
> Analysis of Variance Table
> 
> Response: y
>           Df Sum Sq Mean Sq F value    Pr(>F)   
> z1:x1      1 16.674  16.674 30.0125 0.0003910 ***
> z1:x2      1 13.580  13.580 24.4446 0.0007977 ***
> z1:x3      1  1.190   1.190  2.1429 0.1772677   
> z2:x1      1 35.267  35.267 63.4800 2.287e-05 ***
> z2:x2      1 32.400  32.400 58.3200 3.202e-05 ***
> z2:x3      1 42.667  42.667 76.8000 1.061e-05 ***
> z3:x1      1  0.083   0.083  0.1500 0.7075349   
> z3:x2      1  2.250   2.250  4.0500 0.0750295 . 
> Residuals  9  5.000   0.556                     
> ---
> 
> 
> 
> 
> 
> ###############################
> #         DATA
> ###############################
> 
>   z1 z2 z3 x1 x2 x3  y
>   1  0  0  1  0  0 10
>   1  0  0  0  1  0 15
>   1  0  0  0  0  1 12
>   0  1  0  1  0  0 10
>   0  1  0  0  1  0 11
>   0  1  0  0  0  1 11
>   0  0  1  1  0  0 16
>   0  0  1  0  1  0 17
>   0  0  1  0  0  1 15
>   1  0  0  1  0  0 11
>   1  0  0  0  1  0 17
>   1  0  0  0  0  1 13
>   0  1  0  1  0  0  9
>   0  1  0  0  1  0 10
>   0  1  0  0  0  1 11
>   0  0  1  1  0  0 17
>   0  0  1  0  1  0 17
>   0  0  1  0  0  1 16
> 
> 
> 
> ###############################
> #         CODE
> ###############################
> 
> 
>  x = read.table(file("clipboard"),h=T)
> 
> ## NOT a Scheff?? Model:
>  
>  x.lm <- lm( y ~ (z1+z2+z3):(x1+x2+x3), data=x)
>  summary(x.lm)
>  anova(x.lm)
> 
> 
> ## Scheff?? Model: <- IS CORRECT the analysis below?
>  
>  x.lm <- lm( y ~ -1 + (z1+z2+z3):(x1+x2+x3), data=x)
>  summary(x.lm)
> 
>  x.aov <- aov( y ~  (z1+z2+z3):(x1+x2+x3), data=x)
>  summary(x.aov)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mbeason at harrahs.com  Fri Jan 27 05:38:06 2006
From: mbeason at harrahs.com (Matthew Beason)
Date: Thu, 26 Jan 2006 20:38:06 -0800
Subject: [R] R compile on AIX 5.2
Message-ID: <E153C65077E0034E97A981C6CE26F1BD03814ACD@ENTWMAIL1A.harrahs.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/b33d0afb/attachment.pl

From sell_mirage_ne at hotmail.com  Fri Jan 27 05:52:48 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Thu, 26 Jan 2006 22:52:48 -0600
Subject: [R] generating random numbers from the logit-normal distribution ?
Message-ID: <BAY110-F5FB33B4BA5F87F3F3DFA4C7140@phx.gbl>

Hi R users

I like to generate random numbers from the logit-normal distribution.

Are there available simple commands  I can use?

I used to generate using

exp(rnorm(1000,0,1))/(1+exp(rnorm(1000,0,1)))

I am looking for a simple command such as rnorm().

Thanks in advance

TM



From Morten.Wennerberg at team.telstra.com  Fri Jan 27 05:58:56 2006
From: Morten.Wennerberg at team.telstra.com (Wennerberg, Morten)
Date: Fri, 27 Jan 2006 15:58:56 +1100
Subject: [R] Trimming / Removing leading and following blanks using
	Windows version
Message-ID: <572F3B113B14D311A2800008C72483491B4EF1CE@ntmsg0018.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/308d7c52/attachment.pl

From comtech.usa at gmail.com  Fri Jan 27 07:03:45 2006
From: comtech.usa at gmail.com (Michael)
Date: Thu, 26 Jan 2006 22:03:45 -0800
Subject: [R] why did not data.table work?
Message-ID: <b1f16d9d0601262203o1ca915ebk7fb36418bec8abb3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060126/5e52f572/attachment.pl

From ggrothendieck at gmail.com  Fri Jan 27 07:12:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jan 2006 01:12:09 -0500
Subject: [R] why did not data.table work?
In-Reply-To: <b1f16d9d0601262203o1ca915ebk7fb36418bec8abb3@mail.gmail.com>
References: <b1f16d9d0601262203o1ca915ebk7fb36418bec8abb3@mail.gmail.com>
Message-ID: <971536df0601262212l154864fci11ae02b912509de9@mail.gmail.com>

The error message seems pretty clear in this case.  It can't
find the file.  Try this instead:

x <- read.table(file.choose())

and then use the resulting GUI to locate the file.


On 1/27/06, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> I am wondering what's wrong with my following code?
>
> x=read.table('c:\\llll.txt');
>
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file 'c:\llll.txt', reason 'No such file or directory'
>
>
> what's wrong?
>
> Thanks
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sell_mirage_ne at hotmail.com  Fri Jan 27 07:12:37 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Fri, 27 Jan 2006 00:12:37 -0600
Subject: [R] [Q] extracting lower diagonal elements of a matrix
Message-ID: <BAY110-F28C3577A23976720046026C7140@phx.gbl>

Hi R users

I like to extract lower diagonal elements of a matrix in such a way like, 
data[1,2], data[1,3],
...., data[5,6] are extracted from a matrix called 'data'

This short script below is what I have written so far.

##########################################
data <- matrix(rnorm(36,0,1),nrow=6)
temp<-c()
for (i in 1:(nrow(data)-1))
{
    for (j in (i+1):nrow(data))
    {
        temp<-append(temp,data[j,i])
    }
}
##########################################

Is there any function for this?  or is there any elegant way to do this 
task?

Thanks in advance.

TM



From jacques.veslot at cirad.fr  Fri Jan 27 07:20:04 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 27 Jan 2006 10:20:04 +0400
Subject: [R] [Q] extracting lower diagonal elements of a matrix
In-Reply-To: <BAY110-F28C3577A23976720046026C7140@phx.gbl>
References: <BAY110-F28C3577A23976720046026C7140@phx.gbl>
Message-ID: <43D9BB94.4090207@cirad.fr>

try:
as.vector(as.dist(data))


Taka Matzmoto a ??crit :

>Hi R users
>
>I like to extract lower diagonal elements of a matrix in such a way like, 
>data[1,2], data[1,3],
>...., data[5,6] are extracted from a matrix called 'data'
>
>This short script below is what I have written so far.
>
>##########################################
>data <- matrix(rnorm(36,0,1),nrow=6)
>temp<-c()
>for (i in 1:(nrow(data)-1))
>{
>    for (j in (i+1):nrow(data))
>    {
>        temp<-append(temp,data[j,i])
>    }
>}
>##########################################
>
>Is there any function for this?  or is there any elegant way to do this 
>task?
>
>Thanks in advance.
>
>TM
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From christos at nuverabio.com  Fri Jan 27 07:22:20 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 27 Jan 2006 01:22:20 -0500
Subject: [R] [Q] extracting lower diagonal elements of a matrix
In-Reply-To: <BAY110-F28C3577A23976720046026C7140@phx.gbl>
Message-ID: <000301c6230a$0b5060e0$0202a8c0@headquarters>

Try

s <- matrix(rnorm(36,0,1),nrow=6)
s[col(s)<row(s)]

Courtesy of V&R.

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Taka Matzmoto
Sent: Friday, January 27, 2006 1:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] [Q] extracting lower diagonal elements of a matrix

Hi R users

I like to extract lower diagonal elements of a matrix in such a way like,
data[1,2], data[1,3], ...., data[5,6] are extracted from a matrix called
'data'

This short script below is what I have written so far.

##########################################
data <- matrix(rnorm(36,0,1),nrow=6)
temp<-c()
for (i in 1:(nrow(data)-1))
{
    for (j in (i+1):nrow(data))
    {
        temp<-append(temp,data[j,i])
    }
}
##########################################

Is there any function for this?  or is there any elegant way to do this
task?

Thanks in advance.

TM

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sell_mirage_ne at hotmail.com  Fri Jan 27 07:42:21 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Fri, 27 Jan 2006 00:42:21 -0600
Subject: [R] measuring computation time
Message-ID: <BAY110-F187686874CFC5B0F004E40C7140@phx.gbl>

Hi R users

Is there any function or command for measureing computation time?

For example, if I like to how long it takes to generate 100000 random 
numbers from a normal

distribution, Is there any command I can wrap up around "rnorm(100000,0,1)" 
and returns time

in a sec unit ?

Thanks

TM



From vincent at 7d4.com  Fri Jan 27 07:53:02 2006
From: vincent at 7d4.com (vincent@7d4.com)
Date: Fri, 27 Jan 2006 07:53:02 +0100
Subject: [R] [Q] extracting lower diagonal elements of a matrix
In-Reply-To: <BAY110-F28C3577A23976720046026C7140@phx.gbl>
References: <BAY110-F28C3577A23976720046026C7140@phx.gbl>
Message-ID: <43D9C34E.2090308@7d4.com>

?lower.tri
hih



From francoisromain at free.fr  Fri Jan 27 08:40:21 2006
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Fri, 27 Jan 2006 08:40:21 +0100
Subject: [R] generating random numbers from the logit-normal
	distribution ?
In-Reply-To: <BAY110-F5FB33B4BA5F87F3F3DFA4C7140@phx.gbl>
References: <BAY110-F5FB33B4BA5F87F3F3DFA4C7140@phx.gbl>
Message-ID: <1138347621.43d9ce6533e8e@imp5-g19.free.fr>

Selon Taka Matzmoto <sell_mirage_ne at hotmail.com>:

> Hi R users
>
> I like to generate random numbers from the logit-normal distribution.
>
> Are there available simple commands  I can use?
>
> I used to generate using
>
> exp(rnorm(1000,0,1))/(1+exp(rnorm(1000,0,1)))

> I am looking for a simple command such as rnorm().
>
> Thanks in advance
>
> TM
Hi,



Consider writing a function, you can even learn how to use ...

rlogitnormal <- function(...){
  x <- exp(rnorm(...))
  x / (1+x)
}

then

rlogitnormal(1000, 0, 1)

Romain



From francoisromain at free.fr  Fri Jan 27 08:44:22 2006
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Fri, 27 Jan 2006 08:44:22 +0100
Subject: [R] measuring computation time
In-Reply-To: <BAY110-F187686874CFC5B0F004E40C7140@phx.gbl>
References: <BAY110-F187686874CFC5B0F004E40C7140@phx.gbl>
Message-ID: <1138347862.43d9cf56d7600@imp5-g19.free.fr>

Selon Taka Matzmoto <sell_mirage_ne at hotmail.com>:

> Hi R users
>
> Is there any function or command for measureing computation time?
>
> For example, if I like to how long it takes to generate 100000 random
> numbers from a normal
>
> distribution, Is there any command I can wrap up around "rnorm(100000,0,1)"
> and returns time
>
> in a sec unit ?
>
> Thanks
>
> TM

Hint :

> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

there is

system.time(rnorm(100000,0,1))

RSiteSearch("time") would have told you that

Romain



From ripley at stats.ox.ac.uk  Fri Jan 27 08:45:05 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jan 2006 07:45:05 +0000 (GMT)
Subject: [R] generating random numbers from the logit-normal
 distribution ?
In-Reply-To: <BAY110-F5FB33B4BA5F87F3F3DFA4C7140@phx.gbl>
References: <BAY110-F5FB33B4BA5F87F3F3DFA4C7140@phx.gbl>
Message-ID: <Pine.LNX.4.61.0601270735260.7012@gannet.stats>

On Thu, 26 Jan 2006, Taka Matzmoto wrote:

> Hi R users
>
> I like to generate random numbers from the logit-normal distribution.

which is not a standard distribution.  I guess you mean a random variable 
whose logit is normal (but that is not the only meaning in use).

> Are there available simple commands  I can use?
>
> I used to generate using
>
> exp(rnorm(1000,0,1))/(1+exp(rnorm(1000,0,1)))

That would be incorrect.  I think you mean

> xx <- rnorm(1000, 0, 1)
> exp(xx)/(1+exp(xx))

> I am looking for a simple command such as rnorm().

> plogis(rnorm(1000))

And if that is not simple enough for you, you can write a function 
wrapper like

> rlgnorm <- function(...) plogis(rnorm(...))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jin.Li at csiro.au  Fri Jan 27 08:55:04 2006
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Fri, 27 Jan 2006 17:55:04 +1000
Subject: [R] about xyplot in lattice
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75152269@exqld1-ath.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/cdc96750/attachment.pl

From maechler at stat.math.ethz.ch  Fri Jan 27 09:07:46 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 27 Jan 2006 09:07:46 +0100
Subject: [R] Justification of dendrogram labels
In-Reply-To: <004b01c622df$42b2bfe0$70e66081@bio.flinders.edu.au>
References: <004b01c622df$42b2bfe0$70e66081@bio.flinders.edu.au>
Message-ID: <17369.54482.448423.927898@stat.math.ethz.ch>

>>>>> "DMackay" == Duncan Mackay <duncan.mackay at flinders.edu.au>
>>>>>     on Fri, 27 Jan 2006 11:46:11 +1030 writes:

    DMackay> Hi all,
    DMackay> Can someone tell me how to justify (right or left) the labels on the
    DMackay> branches of a dendrogram tree? I have produced a dendrogram via agnes and
    DMackay> plotted it with pltree. The dendrogram terminal branch labels seem to be
    DMackay> centre-justified by default and I was hoping to change this to left
    DMackay> justification. Thanks,

No, you can't when using pltree().  But you can by other means:

I just now realize how scarce  help(pltree) 
and help(pltree.twins) are on this --- and I will improve these help pages.
pltree() leads to the following call sequence:
  pltree() -> cluster:::pltree.twins -> stats:::plot.hclust()

and the plot.hclust() function, the S3 method for plot()ting  "hclust"
objects, delves into fast but rather inflexible C code.

This has been one of the reasons why the "dendrogram" (S3) class
had been introduced. These are more flexible and have a more
flexible plot method.
--> help(plot.dendrogram)
    and the "graphics show" you get from example(as.dendrogram).
    (and then study the 'Examples:' section of the help page
     more carefully).

To coerce agnes() results to "dendrogram", you need something like

  aa <- agnes(..........)

  dd <- as.dendrogram(as.hclust(aa))

  ## and now
  plot(dd)
  ## or 
  plot(*, ..........)

--
Martin Maechler, ETH Zurich



From comtech.usa at gmail.com  Fri Jan 27 09:14:19 2006
From: comtech.usa at gmail.com (Michael)
Date: Fri, 27 Jan 2006 00:14:19 -0800
Subject: [R] why did not data.table work?
In-Reply-To: <971536df0601262212l154864fci11ae02b912509de9@mail.gmail.com>
References: <b1f16d9d0601262203o1ca915ebk7fb36418bec8abb3@mail.gmail.com>
	<971536df0601262212l154864fci11ae02b912509de9@mail.gmail.com>
Message-ID: <b1f16d9d0601270014w2a95f402rfe974af234703ad9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/ea9235dc/attachment.pl

From maechler at stat.math.ethz.ch  Fri Jan 27 09:31:07 2006
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 27 Jan 2006 09:31:07 +0100
Subject: [R] cluster analysis for 80000 observations
In-Reply-To: <79799E69EA1DA246A51F983B5663BEA2CD3498@server2.hq.clientvela.net>
References: <79799E69EA1DA246A51F983B5663BEA2CD3498@server2.hq.clientvela.net>
Message-ID: <17369.55883.650729.172842@stat.math.ethz.ch>

>>>>> "Markus" == Markus Preisetanz <Markus.Preisetanz at clientvela.com>
>>>>>     on Thu, 26 Jan 2006 20:48:29 +0100 writes:

    Markus> Dear R Specialists,
    Markus> when trying to cluster a data.frame with about 80.000 rows and 25 columns I get the above error message. I tried hclust (using dist), agnes (entering the data.frame directly) and pam (entering the data.frame directly). What I actually do not want to do is generate a random sample from the data.

Currently all the above mentioned cluster methods work with
full distance / dissimilarity objects, even if only internally,
i.e. they store all d_{i,j} for  1 <= i < j <= n, i.e.  n(n-1)/2 values,
also each of them in double precision, i.e. 8 bytes.

So: no chance with the above functions and n=80'000

 Markus> The machine I run R on is a Windows 2000 Server (Pentium 4) with 2 GB of RAM.

If you would run an machine with a 64-bit version of OS and R
{typical case today: Linux on AMD Opteron}, you could go up
quite a bit higher than on your Windoze box,
{I vaguely remember I could do  'n = a few thousand' on our 
 dual opteron with 16 GBytes}, but 80'000 is definitely too
large.

OTOH, there is clara() in the cluster package, which has been
designed for such situations, 
	 CLARA:= [C]lustering [LAR]ge [A]pplications.
It is similar in spirit to pam(),
*does* cluster all 80'000 observations but does so by taking
sub samples to construct the medoids.
(and you can ask it to take many medium size subsamples, instead
 of just 5 small sized ones as it does by default).

Martin Maechler, ETH Zurich
maintainer of "cluster" package.



From tmlammail at yahoo.com  Fri Jan 27 09:58:44 2006
From: tmlammail at yahoo.com (Martin Lam)
Date: Fri, 27 Jan 2006 00:58:44 -0800 (PST)
Subject: [R] Data management problem: convert text string to matrix of
	0's and 1's
In-Reply-To: <43D91584.2060307@brown.EDU>
Message-ID: <20060127085844.61350.qmail@web34704.mail.mud.yahoo.com>

Hi Dale,

Unfortunately, you didn't say in what for format your
data is saved into, so I presume it's saved as a list
of strings. Perhaps there is a faster/better way, but
this should suffice if your datasize isn't enormous.

data = list()
data[1] = "icsrvepf"
data[2] = "fpevrsci"
data[3] = "ics"
data[4] = "p"
data[5] = ""
data[6] = "f"
data[7] = "ic"

names = as.character(c("i", "c", "s", "r", "v", "e",
"p", "f"))

mymatrix = matrix(0, nrow = 7, ncol = 8)
colnames(mymatrix) = names

for (i in 1:length(data)) {
  # split the string into separate characters
  chars = strsplit(data[[i]], split="")[[1]]

  mymatrix[i,which(names %in%chars)] = 1
}
mymatrix

HTH,

Martin Lam

--- Dale Steele <Dale_Steele at brown.EDU> wrote:

> I have a data management problem which exceeds my
> meager R programming 
> skills and would greatly appreciate suggestions on
> how to proceed?  The 
> data consists of a series of observation periods.
> Specific behaviors are 
> recorded for each time period in the order each is
> observed.  Their are 
> 8 possible behaviors, coded as "i" "c" "s" "r" "v"
> "e" "p" "f".
> 
> The data looks like:
> -->
> icsrvepf
> fpevrsci
> ics
> p
> 
> f
> ic
> <--
> 
> I would like to convert the about to a matrix of the
> form:
> 
>   i c s r v e p f
>   1 1 1 1 1 1 1 1
>   1 1 1 1 1 1 1 1
>   1 1 1 0 0 0 0 0
>   0 0 0 0 0 0 1 0
>   0 0 0 0 0 0 0 0
>   0 0 0 0 0 0 0 1
>   1 1 0 0 0 0 0 0
> 
> Thanks.
> 
> Dale
> 
> Dale Steele, MD
> Pediatric Emergency Medicine
> Brown Medical School
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Markus.Gesmann at lloyds.com  Fri Jan 27 10:01:32 2006
From: Markus.Gesmann at lloyds.com (Gesmann, Markus)
Date: Fri, 27 Jan 2006 09:01:32 +0000
Subject: [R] footnote in postscript lattice
Message-ID: <C3E3A3D81F4E0F438118DAA9722F12A9148A08@LNVCNTEXCH01.corp.lloydsnet>

Here is one example using the page option in xyplot:

library(lattice)
library(grid)
 add.footnote <- function(string="Hello World", col="grey",
lineheight=0.5, cex=0.7){
       grid.text(string,
      x=unit(1, "npc") - unit(1, "mm"),
      y=unit(1, "mm"), just=c("right", "bottom"),
      gp=gpar(col=col,lineheight=lineheight, cex=cex))
      }
      
 xyplot(1~1, page=function(n){ add.footnote()})


Markus Gesmann
FPMA
Lloyd's Market Analysis
Lloyd's * One Lime Street * London * EC3M 7HA
Telephone +44 (0)20 7327 6472
Facsimile +44 (0)20 7327 5718
http://www.lloyds.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dean Sonneborn
Sent: 26 January 2006 23:46
To: r-help at stat.math.ethz.ch
Subject: [R] footnote in postscript lattice


I would like to add a footnote to this graph but do not see a "footnote"
command in the package:lattice documentation. I would like to note the
"span=.8"
 as the footnote.

postscript(file= "C:/Documents and Settings/dsonneborn/My
Documents/Slovak/output/pcb_tables/smooth_PCB_lines_four.ps",
bg="transparent", onefile=FALSE, pointsize=20,paper="letter",
horizontal=TRUE, family="Helvetica",font="Helvetica")

xyplot(AWGT ~ lipid_adj_lpcb2_cent | malex*romanix, data=pcb_graph3,

       auto.key = list(lines = TRUE, points = TRUE), ylab="Birth
Weight", xlab="Lipid Adjusted PCB",

       par.settings =

       list(superpose.symbol = list(col = colr, pch = plotchar),

            superpose.line = list(col = colr, pch = plotchar, lty = 1)),

        type=c("p", "smooth"), span=.8)

dev.off()
thanks

-- 
Dean Sonneborn, MS
Programmer Analyst
Department of Public Health Sciences
University of California, Davis
(530) 754-9516

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

************LNSCNTMCS01***************************************************
The information in this E-Mail and in any attachments is CON...{{dropped}}



From phgrosjean at sciviews.org  Fri Jan 27 10:05:22 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 27 Jan 2006 10:05:22 +0100
Subject: [R] What's wrong with Rcmdr?
In-Reply-To: <b1f16d9d0601261039r48332691pb06d210de930162c@mail.gmail.com>
References: <20060126092312.GB4835@med.unibs.it>	<20060126152026.PRYU5216.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<b1f16d9d0601261039r48332691pb06d210de930162c@mail.gmail.com>
Message-ID: <43D9E252.2090506@sciviews.org>

Michael wrote:
> thanks everyone, another problem is I cannot load Rcmdr at all in SCIView...
> what might be the problem?

Not a problem, but a version incompatibility: SciViews is compiled for a 
given version of R and Rcmdr (the one on the Web site is for R 2.2.0 and 
Rcmdr 1.0-3... that is, not the latest ones). I still have to upload 
latest SciViews version compatible with both latest versions.
Best,

Philippe Grosjean

> On 1/26/06, John Fox <jfox at mcmaster.ca> wrote:
> 
>>Dear Michael,
>>
>>To elaborate slightly: You can't load the same package twice in an R
>>session. As mentioned, used Commander() to restart the Rcmdr GUI.
>>
>>I hope this helps,
>>John
>>
>>--------------------------------
>>John Fox
>>Department of Sociology
>>McMaster University
>>Hamilton, Ontario
>>Canada L8S 4M4
>>905-525-9140x23604
>>http://socserv.mcmaster.ca/jfox
>>--------------------------------
>>
>>
>>>-----Original Message-----
>>>From: r-help-bounces at stat.math.ethz.ch
>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefano Calza
>>>Sent: Thursday, January 26, 2006 4:23 AM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: Re: [R] What's wrong with Rcmdr?
>>>
>>>Try using
>>>
>>>Commander()
>>>
>>>but, obviously, the first time.
>>>
>>>HIH,
>>>Ste
>>>
>>>On Thu, Jan 26, 2006 at 01:11:23AM -0800, Michael wrote:
>>><Michael>Hi all,
>>><Michael>
>>><Michael>I successfully installed Rcmdr. And I type
>>>"library(Rcmdr)", nothing <Michael>happened; <Michael>
>>><Michael>or if I select menu item "load package" and select
>>>"Rcmdr", still nothing <Michael>happened...
>>><Michael>
>>><Michael>Why didn't Rcmdr start?
>>><Michael>
>>><Michael>Very strangely, if I close the R console and restart
>>>R console, every first <Michael>time I load "Rcmdr", it
>>>starts! But not second, third time...
>>><Michael>
>>><Michael>What's wrong with it?
>>><Michael>
>>><Michael>Thanks a lot!
>>><Michael>
>>><Michael>Michael.
>>><Michael>
>>><Michael>     [[alternative HTML version deleted]]
>>><Michael>
>>><Michael>______________________________________________
>>><Michael>R-help at stat.math.ethz.ch mailing list
>>><Michael>https://stat.ethz.ch/mailman/listinfo/r-help
>>><Michael>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From phgrosjean at sciviews.org  Fri Jan 27 10:44:11 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 27 Jan 2006 10:44:11 +0100
Subject: [R] [Q] extracting lower diagonal elements of a matrix
In-Reply-To: <000301c6230a$0b5060e0$0202a8c0@headquarters>
References: <000301c6230a$0b5060e0$0202a8c0@headquarters>
Message-ID: <43D9EB6B.1080109@sciviews.org>

Try:
 > data[lower.tri(data)]

(the same as data[col(data) < row(data)], but using the dedicated function).
Best,

Philippe Grosjean

Christos Hatzis wrote:
> Try
> 
> s <- matrix(rnorm(36,0,1),nrow=6)
> s[col(s)<row(s)]
> 
> Courtesy of V&R.
> 
> -Christos 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Taka Matzmoto
> Sent: Friday, January 27, 2006 1:13 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] [Q] extracting lower diagonal elements of a matrix
> 
> Hi R users
> 
> I like to extract lower diagonal elements of a matrix in such a way like,
> data[1,2], data[1,3], ...., data[5,6] are extracted from a matrix called
> 'data'
> 
> This short script below is what I have written so far.
> 
> ##########################################
> data <- matrix(rnorm(36,0,1),nrow=6)
> temp<-c()
> for (i in 1:(nrow(data)-1))
> {
>     for (j in (i+1):nrow(data))
>     {
>         temp<-append(temp,data[j,i])
>     }
> }
> ##########################################
> 
> Is there any function for this?  or is there any elegant way to do this
> task?
> 
> Thanks in advance.
> 
> TM
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From christian.hoffmann at wsl.ch  Fri Jan 27 10:54:11 2006
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Fri, 27 Jan 2006 10:54:11 +0100
Subject: [R] regular expressions, sub
In-Reply-To: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
Message-ID: <43D9EDC3.9000807@wsl.ch>

Hi,

I am trying to use sub, regexpr on expressions like

    log(D) ~ log(N)+I(log(N)^2)+log(t)

being a model specification.

The aim is to produce:

    "ln D ~ ln N + ln^2 N + ln t"

The variable names N, t may change, the number of terms too.

I succeded only partially, help on regular expressions is hard to 
understand for me, examples on my case are rare. The help page on R-help 
for grep etc. and "regular expressions"

What I am doing:

(f <- log(D) ~ log(N)+I(log(N)^2)+log(t))
(ft <- sub("","",f))   # creates string with parts of formula, how to do 
it simpler?
(fu <- paste(ft[c(2,1,3)],collapse=" "))  # converts to one string

Then I want to use \1 for backreferences something like

(fv <- sub("log( [:alpha:] N  )^ [:alpha:)","ln \\1^\\2",fu))

to change "log(g)^7" to "ln^7 g",

and to eliminate I(): sub("I(blabla)","\\1",fv)  # I(xxx) -> xxx

The special characters are making trouble, sub acceps "(", ")" only in 
pairs. Code for experimentation:

trysub <- function(s,t,e) {
ii<-0; for (i1 in c(TRUE,FALSE)) for (i2 in c(TRUE,FALSE)) for (i3 in 
c(TRUE,FALSE)) for (i4 in c(TRUE,FALSE)) 
print(paste(ii<-ii+1,ifelse(i1,"  "," ~"),"ext",ifelse(i2,"  "," 
~"),"perl",ifelse(i3,"  "," ~"),"fixed ",ifelse(i4,"  "," ~"),"useBytes: 
", try(sub(s,t,e, extended=i1, perl=i2, fixed=i3, 
useBytes=i4)),sep=""));invisible(0) }

trysub("I(log(N)^2)","ln n^2",fu) # A: desired result for cases 
5,6,13..16, the rest unsubstituted

trysub("log(","ln ",fu)           # B: no substitutions; errors for 
cases 1..4,7.. 12   # typical errors:
"3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement, 
x, ignore.case, useBytes) : \n\tinvalid regular expression 'log('\n"

trysub("log\(","ln ",fu)          # C: same as A

trysub("log\\(","ln ",fu)         # D: no substitutions; errors for 
cases 15,16        # typical errors:
"15 ~ext ~perl ~fixed   useBytes: Error in sub(pattern, replacement, x, 
ignore.case, extended, fixed, useBytes) : \n\tinvalid regular expression 
'log\\('\n"

trysub("log\\(([:alpha:]+)\\)","ln \1",fu) # no substitutions, no errors
# E: typical errors:
"3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement, 
x, ignore.case, useBytes) : \n\tinvalid regular expression 
'log\\(([:alpha:]+)\\)'\n"



Thanks for help
Christian

PS. The explanations in the documents
-- 
Dr. Christian W. Hoffmann,
Swiss Federal Research Institute WSL
Mathematics + Statistical Computing
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland

Tel +41-44-7392-277  (office)   -111(exchange)
Fax +41-44-7392-215  (fax)
christian.hoffmann at wsl.ch
http://www.wsl.ch/staff/christian.hoffmann

International Conference 5.-7.6.2006 Ekaterinburg Russia
"Climate changes and their impact on boreal and temperate forests"
http://ecoinf.uran.ru/conference/



From B.Rowlingson at lancaster.ac.uk  Fri Jan 27 10:57:10 2006
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 27 Jan 2006 09:57:10 +0000
Subject: [R] How do you convert this from S Plus to R - any help
 appreciated . thanks
In-Reply-To: <3B5823541A25D311B3B90008C7F905641D8A2DE7@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F905641D8A2DE7@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <43D9EE76.3030006@lancaster.ac.uk>

Briggs, Meredith M wrote:

> exportData(MU.Cost,paste("C:/RAUDSL/S",as.character(MU.Cost$Run.Id[1]),"
> .",as.character(MU.Cost$MU.Id[1]),".MU.PRICE.OUTPUT.txt",sep=""),append
> = FALSE,type="ASCII",quote=FALSE)

  Looks like perfectly good R to me.

  Except there's no exportData function. I assume this is an Splus 
function that R doesn't have, in which case telling us what it does 
might help. What does the Splus manual have to say about it?

I'm guessing R's write.table might be of use.

  Assuming its exportData that has you stuck - the other bits should 
allwork in R no problem, all it does is construct a path from parts of 
the MU.Cost object.

Barry



From msubianto at gmail.com  Fri Jan 27 11:39:25 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Fri, 27 Jan 2006 11:39:25 +0100
Subject: [R] How to convert decimals to fractions
Message-ID: <43D9F85D.3020601@gmail.com>

Dear all,
Are there any functions to convert decimals to fractions in R?
I have the result:
 >   summary(as.factor(complete.ID))
                  0 0.0133333333333333               0.04
               2256                488                230
0.0666666666666667 0.0933333333333333  0.106666666666667
               2342                310                726
  0.133333333333333  0.146666666666667               0.16
                179                750                163
  0.186666666666667                0.2  0.226666666666667
                194                180                 57
  0.253333333333333  0.266666666666667  0.293333333333333
                  1                 10                 40
               0.32  0.333333333333333  0.346666666666667
                 45                 61                117
  0.373333333333333
                 62
 >

How to convert something like this:

                  0               1/75               3/75
               2256                488                230
               5/75               7/75               8/75
               2342                310                726
              10/75              11/75              12/75
                179                750                163
              14/75              15/75              17/75
                194                180                 57
              19/75              20/75              22/75
                  1                 10                 40
              24/75              25/75              26/75
                 45                 61                117
              28/75
                 62

Any suggestions would be appreciated. Thanks you.
Best, Muhammad Subianto
PS.
I found this website: http://www.mindspring.com/~alanh/fracs.html



From ajayshah at mayin.org  Thu Jan 26 17:40:23 2006
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Thu, 26 Jan 2006 22:10:23 +0530
Subject: [R] Prediction when using orthogonal polynomials in regression
Message-ID: <20060126164023.GA21657@lubyanka.local>

Folks,

I'm doing fine with using orthogonal polynomials in a regression context:

  # We will deal with noisy data from the d.g.p. y = sin(x) + e
  x <- seq(0, 3.141592654, length.out=20)
  y <- sin(x) + 0.1*rnorm(10)
  d <- lm(y ~ poly(x, 4))
  plot(x, y, type="l"); lines(x, d$fitted.values, col="blue") # Fits great!
  all.equal(as.numeric(d$coefficients[1] + m %*% d$coefficients[2:5]),
            as.numeric(d$fitted.values))

What I would like to do now is to apply the estimated model to do
prediction for a new set of x points e.g.
  xnew <- seq(0,5,.5)

We know that the predicted values should be roughly sin(xnew). What I
don't know is: how do I use the object `d' to make predictions for
xnew?

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.



From ripley at stats.ox.ac.uk  Fri Jan 27 11:49:36 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jan 2006 10:49:36 +0000 (GMT)
Subject: [R] regular expressions, sub
In-Reply-To: <43D9EDC3.9000807@wsl.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<43D9EDC3.9000807@wsl.ch>
Message-ID: <Pine.LNX.4.61.0601271028200.8541@gannet.stats>

Note that [:alpha:] is a pre-defined character class and should only be 
used inside [].  And metacharacters need to be quoted.  See ?regexp.

> f <- log(D) ~ log(N)+I(log(N)^2)+log(t)
> f1 <- deparse(f)
> f1
[1] "log(D) ~ log(N) + I(log(N)^2) + log(t)"

Now we have a string.

(f2 <- gsub("I\\((.*)\\) ", "\\1 ", f1))
[1] "log(D) ~ log(N) + log(N)^2 + log(t)"
(f3 <- gsub("(?U)log\\((.*)\\)", "ln \\1", f2, perl=TRUE))
[1] "ln D ~ ln N + ln N^2 + ln t"
(f4 <- gsub("ln ([[:alpha:]])\\^([[:digit:]])", "ln^\\2 \\1", f3))
[1] "ln D ~ ln N + ln^2 N + ln t"

That should give you some ideas to be going on with.

On Fri, 27 Jan 2006, Christian Hoffmann wrote:

> Hi,
>
> I am trying to use sub, regexpr on expressions like
>
>    log(D) ~ log(N)+I(log(N)^2)+log(t)
>
> being a model specification.
>
> The aim is to produce:
>
>    "ln D ~ ln N + ln^2 N + ln t"
>
> The variable names N, t may change, the number of terms too.
>
> I succeded only partially, help on regular expressions is hard to
> understand for me, examples on my case are rare. The help page on R-help
> for grep etc. and "regular expressions"
>
> What I am doing:
>
> (f <- log(D) ~ log(N)+I(log(N)^2)+log(t))
> (ft <- sub("","",f))   # creates string with parts of formula, how to do
> it simpler?
> (fu <- paste(ft[c(2,1,3)],collapse=" "))  # converts to one string
>
> Then I want to use \1 for backreferences something like
>
> (fv <- sub("log( [:alpha:] N  )^ [:alpha:)","ln \\1^\\2",fu))
>
> to change "log(g)^7" to "ln^7 g",
>
> and to eliminate I(): sub("I(blabla)","\\1",fv)  # I(xxx) -> xxx
>
> The special characters are making trouble, sub acceps "(", ")" only in
> pairs.

>From ?regexp

   Any metacharacter with special meaning may be quoted by preceding it
   with a backslash.  The metacharacters are '. \ | ( ) [ { ^ $ * +  ?'.


> Code for experimentation:
>
> trysub <- function(s,t,e) {
> ii<-0; for (i1 in c(TRUE,FALSE)) for (i2 in c(TRUE,FALSE)) for (i3 in
> c(TRUE,FALSE)) for (i4 in c(TRUE,FALSE))
> print(paste(ii<-ii+1,ifelse(i1,"  "," ~"),"ext",ifelse(i2,"  ","
> ~"),"perl",ifelse(i3,"  "," ~"),"fixed ",ifelse(i4,"  "," ~"),"useBytes:
> ", try(sub(s,t,e, extended=i1, perl=i2, fixed=i3,
> useBytes=i4)),sep=""));invisible(0) }
>
> trysub("I(log(N)^2)","ln n^2",fu) # A: desired result for cases
> 5,6,13..16, the rest unsubstituted
>
> trysub("log(","ln ",fu)           # B: no substitutions; errors for
> cases 1..4,7.. 12   # typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement,
> x, ignore.case, useBytes) : \n\tinvalid regular expression 'log('\n"
>
> trysub("log\(","ln ",fu)          # C: same as A
>
> trysub("log\\(","ln ",fu)         # D: no substitutions; errors for
> cases 15,16        # typical errors:
> "15 ~ext ~perl ~fixed   useBytes: Error in sub(pattern, replacement, x,
> ignore.case, extended, fixed, useBytes) : \n\tinvalid regular expression
> 'log\\('\n"
>
> trysub("log\\(([:alpha:]+)\\)","ln \1",fu) # no substitutions, no errors
> # E: typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement,
> x, ignore.case, useBytes) : \n\tinvalid regular expression
> 'log\\(([:alpha:]+)\\)'\n"
>
>
>
> Thanks for help
> Christian
>
> PS. The explanations in the documents
> -- 
> Dr. Christian W. Hoffmann,
> Swiss Federal Research Institute WSL
> Mathematics + Statistical Computing
> Zuercherstrasse 111
> CH-8903 Birmensdorf, Switzerland
>
> Tel +41-44-7392-277  (office)   -111(exchange)
> Fax +41-44-7392-215  (fax)
> christian.hoffmann at wsl.ch
> http://www.wsl.ch/staff/christian.hoffmann
>
> International Conference 5.-7.6.2006 Ekaterinburg Russia
> "Climate changes and their impact on boreal and temperate forests"
> http://ecoinf.uran.ru/conference/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From berwin at maths.uwa.edu.au  Fri Jan 27 11:51:32 2006
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 27 Jan 2006 18:51:32 +0800
Subject: [R] How to convert decimals to fractions
In-Reply-To: <43D9F85D.3020601@gmail.com>
References: <43D9F85D.3020601@gmail.com>
Message-ID: <17369.64308.11647.715628@bossiaea.maths.uwa.edu.au>

G'day Muhammad,

>>>>> "MS" == Muhammad Subianto <msubianto at gmail.com> writes:

    MS> Are there any functions to convert decimals to fractions in R?
    MS> I have the result:
Something like:

> library(MASS)
> as.fractions(c(0, 0.0133333333333333,               0.04,
 0.0666666666666667, 0.0933333333333333,  0.106666666666667,
  0.133333333333333,  0.146666666666667,               0.16,
  0.186666666666667,                0.2,  0.226666666666667,
  0.253333333333333,  0.266666666666667,  0.293333333333333,
               0.32,  0.333333333333333,  0.346666666666667,
  0.373333333333333))

 [1]     0  1/75  1/25  1/15  7/75  8/75  2/15 11/75  4/25 14/75   1/5 17/75
[13] 19/75  4/15 22/75  8/25   1/3 26/75 28/75

?

Cheers,

        Berwin



From ripley at stats.ox.ac.uk  Fri Jan 27 11:53:09 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jan 2006 10:53:09 +0000 (GMT)
Subject: [R] How to convert decimals to fractions
In-Reply-To: <43D9F85D.3020601@gmail.com>
References: <43D9F85D.3020601@gmail.com>
Message-ID: <Pine.LNX.4.61.0601271050030.8541@gannet.stats>

library(MASS)
?fractions

help.search("fractions") gets you there.


On Fri, 27 Jan 2006, Muhammad Subianto wrote:

> Dear all,
> Are there any functions to convert decimals to fractions in R?
> I have the result:
> >   summary(as.factor(complete.ID))
>                  0 0.0133333333333333               0.04
>               2256                488                230
> 0.0666666666666667 0.0933333333333333  0.106666666666667
>               2342                310                726
>  0.133333333333333  0.146666666666667               0.16
>                179                750                163
>  0.186666666666667                0.2  0.226666666666667
>                194                180                 57
>  0.253333333333333  0.266666666666667  0.293333333333333
>                  1                 10                 40
>               0.32  0.333333333333333  0.346666666666667
>                 45                 61                117
>  0.373333333333333
>                 62
> >
>
> How to convert something like this:
>
>                  0               1/75               3/75
>               2256                488                230
>               5/75               7/75               8/75
>               2342                310                726
>              10/75              11/75              12/75
>                179                750                163
>              14/75              15/75              17/75
>                194                180                 57
>              19/75              20/75              22/75
>                  1                 10                 40
>              24/75              25/75              26/75
>                 45                 61                117
>              28/75
>                 62
>
> Any suggestions would be appreciated. Thanks you.
> Best, Muhammad Subianto
> PS.
> I found this website: http://www.mindspring.com/~alanh/fracs.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sourceforge at metrak.com  Fri Jan 27 12:05:42 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Fri, 27 Jan 2006 22:05:42 +1100
Subject: [R] updated r-help traffic plots
Message-ID: <43D9FE86.5010404@metrak.com>

I have updated some mail list traffic plots I created a while back 
however I wouldn't consider the data verified.  The reply stats are from 
"In-Reply-To" headers.

http://brewiki.org/tmp/r-help_traffic.png

The data set is http://brewiki.org/tmp/r-help.zip



From dimitris.rizopoulos at med.kuleuven.be  Fri Jan 27 12:08:27 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 27 Jan 2006 12:08:27 +0100
Subject: [R] How to convert decimals to fractions
References: <43D9F85D.3020601@gmail.com>
Message-ID: <010e01c62331$ffdab210$0540210a@www.domain>

if it happens to know the denominator, then a simple approach could 
be:

frac.fun <- function(x, den){
    dec <- seq(0, den) / den
    nams <- paste(seq(0, den), den, sep = "/")
    sapply(x, function(y) nams[which.min(abs(y - dec))])
}
#######################
frac.fun(c(0, 1, 0.8266667, .066666, 0.2666666), 75)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Muhammad Subianto" <msubianto at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Friday, January 27, 2006 11:39 AM
Subject: [R] How to convert decimals to fractions


> Dear all,
> Are there any functions to convert decimals to fractions in R?
> I have the result:
> >   summary(as.factor(complete.ID))
>                  0 0.0133333333333333               0.04
>               2256                488                230
> 0.0666666666666667 0.0933333333333333  0.106666666666667
>               2342                310                726
>  0.133333333333333  0.146666666666667               0.16
>                179                750                163
>  0.186666666666667                0.2  0.226666666666667
>                194                180                 57
>  0.253333333333333  0.266666666666667  0.293333333333333
>                  1                 10                 40
>               0.32  0.333333333333333  0.346666666666667
>                 45                 61                117
>  0.373333333333333
>                 62
> >
>
> How to convert something like this:
>
>                  0               1/75               3/75
>               2256                488                230
>               5/75               7/75               8/75
>               2342                310                726
>              10/75              11/75              12/75
>                179                750                163
>              14/75              15/75              17/75
>                194                180                 57
>              19/75              20/75              22/75
>                  1                 10                 40
>              24/75              25/75              26/75
>                 45                 61                117
>              28/75
>                 62
>
> Any suggestions would be appreciated. Thanks you.
> Best, Muhammad Subianto
> PS.
> I found this website: http://www.mindspring.com/~alanh/fracs.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From phgrosjean at sciviews.org  Fri Jan 27 12:14:25 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 27 Jan 2006 12:14:25 +0100
Subject: [R] regular expressions, sub
In-Reply-To: <43D9EDC3.9000807@wsl.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<43D9EDC3.9000807@wsl.ch>
Message-ID: <43DA0091.7010705@sciviews.org>

Hello,

Here is what I got after playing a little bit with your problem:

# First of all, if you prefer 'ln' instead of 'log', why not to define:
ln <- function(x) log(x)
ln2 <- function(x) log(x)^2
ln3 <- function(x) log(x)^3
ln4 <- function(x) log(x)^4
# ... as many function as powers you need

# Then, your formula is now closer to what you want
# which makes the whole code easier to read for you:

Form <- ln(D) ~ ln(N) + ln2(N) + ln(t) # Same as your original formula

# Here is the function to transform it in a more readable string:
formulaTransform <-
function(form, as.expression = FALSE) {
     if (!inherits(form, "formula"))
         stop("'form' must be a 'formula' object!")
	
     # Transform the formula into a string (is it a better way?)
     Res <- paste(as.character(form)[c(2, 1, 3)], collapse = " ")

     if (as.expression) { # Transform the formula in a nice expression
         # Change '~' into '=='
         Res <- sub("~", "%~~%", Res) # How to do '~' in an expression?
         # Eliminate brackets
         Res <- gsub("[(]([A-Za-z0-9._]*)[)]", " ~ \\1", Res)
         # Transform powers
         Res <- gsub("ln([2-9])", "ln^\\1", Res)
         Res <- eval(parse(text = Res))
     } else { # Make a nicer string
         # Eliminate brackets
         Res <- gsub("[(]([A-Za-z0-9._]*)[)]", " \\1", Res)
         # Transform powers
         Res <- gsub("ln([2-9])", "ln^\\1", Res)
     }

     # Return the result
     return(Res)
}

# Here is a nicer presentation as a string
formulaTransform(Form)

# Here is an even nicer presentation (creating an expression)
plot(1:3, type = "n")
text(2, 2, formulaTransform(Form, TRUE))

# The later form is really interesting when you use, for instance,
# greek letters for variables, or so...
Form2 <- ln(alpha) ~ ln(beta) + ln2(beta) + ln3(beta)

formulaTransform(Form2)
plot(1:3, type = "n")
text(2, 2, formulaTransform(Form2, TRUE))

# ... but this could be refined even more!

Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (
..............................................................

Christian Hoffmann wrote:
> Hi,
> 
> I am trying to use sub, regexpr on expressions like
> 
>     log(D) ~ log(N)+I(log(N)^2)+log(t)
> 
> being a model specification.
> 
> The aim is to produce:
> 
>     "ln D ~ ln N + ln^2 N + ln t"
> 
> The variable names N, t may change, the number of terms too.
> 
> I succeded only partially, help on regular expressions is hard to 
> understand for me, examples on my case are rare. The help page on R-help 
> for grep etc. and "regular expressions"
> 
> What I am doing:
> 
> (f <- log(D) ~ log(N)+I(log(N)^2)+log(t))
> (ft <- sub("","",f))   # creates string with parts of formula, how to do 
> it simpler?
> (fu <- paste(ft[c(2,1,3)],collapse=" "))  # converts to one string
> 
> Then I want to use \1 for backreferences something like
> 
> (fv <- sub("log( [:alpha:] N  )^ [:alpha:)","ln \\1^\\2",fu))
> 
> to change "log(g)^7" to "ln^7 g",
> 
> and to eliminate I(): sub("I(blabla)","\\1",fv)  # I(xxx) -> xxx
> 
> The special characters are making trouble, sub acceps "(", ")" only in 
> pairs. Code for experimentation:
> 
> trysub <- function(s,t,e) {
> ii<-0; for (i1 in c(TRUE,FALSE)) for (i2 in c(TRUE,FALSE)) for (i3 in 
> c(TRUE,FALSE)) for (i4 in c(TRUE,FALSE)) 
> print(paste(ii<-ii+1,ifelse(i1,"  "," ~"),"ext",ifelse(i2,"  "," 
> ~"),"perl",ifelse(i3,"  "," ~"),"fixed ",ifelse(i4,"  "," ~"),"useBytes: 
> ", try(sub(s,t,e, extended=i1, perl=i2, fixed=i3, 
> useBytes=i4)),sep=""));invisible(0) }
> 
> trysub("I(log(N)^2)","ln n^2",fu) # A: desired result for cases 
> 5,6,13..16, the rest unsubstituted
> 
> trysub("log(","ln ",fu)           # B: no substitutions; errors for 
> cases 1..4,7.. 12   # typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement, 
> x, ignore.case, useBytes) : \n\tinvalid regular expression 'log('\n"
> 
> trysub("log\(","ln ",fu)          # C: same as A
> 
> trysub("log\\(","ln ",fu)         # D: no substitutions; errors for 
> cases 15,16        # typical errors:
> "15 ~ext ~perl ~fixed   useBytes: Error in sub(pattern, replacement, x, 
> ignore.case, extended, fixed, useBytes) : \n\tinvalid regular expression 
> 'log\\('\n"
> 
> trysub("log\\(([:alpha:]+)\\)","ln \1",fu) # no substitutions, no errors
> # E: typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement, 
> x, ignore.case, useBytes) : \n\tinvalid regular expression 
> 'log\\(([:alpha:]+)\\)'\n"
> 
> 
> 
> Thanks for help
> Christian
> 
> PS. The explanations in the documents



From sourceforge at metrak.com  Fri Jan 27 12:16:19 2006
From: sourceforge at metrak.com (paul sorenson)
Date: Fri, 27 Jan 2006 22:16:19 +1100
Subject: [R] regular expressions, sub
In-Reply-To: <43D9EDC3.9000807@wsl.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<43D9EDC3.9000807@wsl.ch>
Message-ID: <43DA0103.1090105@metrak.com>

There are some interactive regex tools around.  I use a python one 
sometimes.  You just then have to be careful re escaping and the style 
of regular expressions used in the tool you worked with and the target 
environment.

Christian Hoffmann wrote:
> Hi,
> 
> I am trying to use sub, regexpr on expressions like
> 
>     log(D) ~ log(N)+I(log(N)^2)+log(t)
> 
> being a model specification.
> 
> The aim is to produce:
> 
>     "ln D ~ ln N + ln^2 N + ln t"
> 
> The variable names N, t may change, the number of terms too.
> 
> I succeded only partially, help on regular expressions is hard to 
> understand for me, examples on my case are rare. The help page on R-help 
> for grep etc. and "regular expressions"



From uctqmkt at ucl.ac.uk  Fri Jan 27 12:28:10 2006
From: uctqmkt at ucl.ac.uk (Mike Townsley)
Date: Fri, 27 Jan 2006 11:28:10 +0000
Subject: [R] monochrome mosaic plot in vcd package
Message-ID: <5.2.1.1.0.20060127103000.00981fc0@imap-server.ucl.ac.uk>

helpeRs,

I have a nice looking mosaic plot in an article to be published 
soon.  Sadly, the published version will be in black and white and so ruin 
the advantage of the default shading scheme of tiles.

What would readers suggest as an alternative shading scheme?  If I have a 
black-and-white shading scheme graduated according to suitable cutoffs I 
won't be able to tell positive from negative residuals.  The tile borders 
can be changed of course, but I'm uncertain that is will be clear enough 
for a reader.
Another option may be to use a fill pattern of sloping lines with different 
orientations for the sign and density for the magnitude.  The problem with 
this option is I wouldn't know where to start to incorporate into a legend.

The shading_binary function is no good as I would like the cells with 
residuals less than absolute 2 to be different from other cells.  How would 
readers of this list represent a mosaic plot so that it was easily 
interpretable in monochrome?

My data can be used as an example:


library(vcd)
library(MASS)

term.1 <- gl(2,1,8, labels = LETTERS[1:2])
term.2 <- gl(2,2,8, labels = LETTERS[3:4])
term.3 <- gl(2,4,8, labels = LETTERS[5:6])

cell.count <- c(72, 19, 5, 8, 117, 115, 81, 85)

mosaic(loglm(formula = cell.count ~ term.1 + term.2 + term.3),
        shade = TRUE, gp = shading_hcl, legend = TRUE,
        labeling_args = list(rot_labels = rep(0,4)),
        gp_args = list(lty = 1:2),legend_width = unit(0.2, "npc"))



------------------------------------------------------------
Dr Michael Townsley
Senior Research Fellow
Jill Dando Institute of Crime Science
University College London
Second Floor, Brook House
London, WC1E 7HN

Phone: 020 7679 0820
Fax: 020 7679 0828
Email: m.townsley at ucl.ac.uk



From Achim.Zeileis at wu-wien.ac.at  Fri Jan 27 12:31:41 2006
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 27 Jan 2006 12:31:41 +0100
Subject: [R] Prediction when using orthogonal polynomials in regression
In-Reply-To: <20060126164023.GA21657@lubyanka.local>
References: <20060126164023.GA21657@lubyanka.local>
Message-ID: <20060127123141.1cffd645.Achim.Zeileis@wu-wien.ac.at>

On Thu, 26 Jan 2006 22:10:23 +0530 Ajay Narottam Shah wrote:

> Folks,
> 
> I'm doing fine with using orthogonal polynomials in a regression
> context:
> 
>   # We will deal with noisy data from the d.g.p. y = sin(x) + e
>   x <- seq(0, 3.141592654, length.out=20)
>   y <- sin(x) + 0.1*rnorm(10)
>   d <- lm(y ~ poly(x, 4))
>   plot(x, y, type="l"); lines(x, d$fitted.values, col="blue")

fitted(d) is usually the preferred way of accessing the fitted values
(although equivalent in this particular case).

> great! all.equal(as.numeric(d$coefficients[1] + m %*% d$coefficients
> [2:5]), as.numeric(d$fitted.values))
> 
> What I would like to do now is to apply the estimated model to do
> prediction for a new set of x points e.g.
>   xnew <- seq(0,5,.5)
>
> We know that the predicted values should be roughly sin(xnew). What I
> don't know is: how do I use the object `d' to make predictions for
> xnew?

Use predict:
  predict(d, data.frame(x = xnew))
which is pretty evocative.

Best,
Z

> -- 
> Ajay Shah
> http://www.mayin.org/ajayshah
> ajayshah at mayin.org
> http://ajayshahblog.blogspot.com <*(:-? - wizard who doesn't know the
> answer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Markus.Preisetanz at clientvela.com  Fri Jan 27 12:46:43 2006
From: Markus.Preisetanz at clientvela.com (Markus Preisetanz)
Date: Fri, 27 Jan 2006 12:46:43 +0100
Subject: [R] Clustering Question
Message-ID: <79799E69EA1DA246A51F983B5663BEA2CE6584@server2.hq.clientvela.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/3ada1cf9/attachment.pl

From msubianto at gmail.com  Fri Jan 27 12:47:36 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Fri, 27 Jan 2006 12:47:36 +0100
Subject: [R] How to convert decimals to fractions
In-Reply-To: <Pine.LNX.4.61.0601271050030.8541@gannet.stats>
References: <43D9F85D.3020601@gmail.com>
	<Pine.LNX.4.61.0601271050030.8541@gannet.stats>
Message-ID: <43DA0858.4010304@gmail.com>

On this day 27/01/2006 11:51, Berwin A Turlach wrote:
 >> library(MASS)
 >> as.fractions(c(0, 0.0133333333333333,               0.04,
 >  0.0666666666666667, 0.0933333333333333,  0.106666666666667,
 >   0.133333333333333,  0.146666666666667,               0.16,
 >   0.186666666666667,                0.2,  0.226666666666667,
 >   0.253333333333333,  0.266666666666667,  0.293333333333333,
 >                0.32,  0.333333333333333,  0.346666666666667,
 >   0.373333333333333))
 >
 >  [1]     0  1/75  1/25  1/15  7/75  8/75  2/15 11/75  4/25 14/75 
1/5 17/75
 > [13] 19/75  4/15 22/75  8/25   1/3 26/75 28/75
 >

On this day 27/01/2006 11:53, Prof Brian Ripley wrote:
> library(MASS)
> ?fractions
> 
> help.search("fractions") gets you there.
> 


Many Thanks to Berwin A Turlach and Prof Brian Ripley for your suggestions.
 > ?fractions
 >
Best regards, Muhammad Subianto



From msubianto at gmail.com  Fri Jan 27 13:05:16 2006
From: msubianto at gmail.com (Muhammad Subianto)
Date: Fri, 27 Jan 2006 13:05:16 +0100
Subject: [R] How to convert decimals to fractions
In-Reply-To: <010e01c62331$ffdab210$0540210a@www.domain>
References: <43D9F85D.3020601@gmail.com>
	<010e01c62331$ffdab210$0540210a@www.domain>
Message-ID: <43DA0C7C.1090500@gmail.com>

Thanks you for your help.
Best wishes, Muhammad Subianto

On this day 27/01/2006 12:08, Dimitris Rizopoulos wrote:
> if it happens to know the denominator, then a simple approach could 
> be:
> 
> frac.fun <- function(x, den){
>     dec <- seq(0, den) / den
>     nams <- paste(seq(0, den), den, sep = "/")
>     sapply(x, function(y) nams[which.min(abs(y - dec))])
> }
> #######################
> frac.fun(c(0, 1, 0.8266667, .066666, 0.2666666), 75)
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://www.med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Muhammad Subianto" <msubianto at gmail.com>
> To: <R-help at stat.math.ethz.ch>
> Sent: Friday, January 27, 2006 11:39 AM
> Subject: [R] How to convert decimals to fractions
> 
> 
>> Dear all,
>> Are there any functions to convert decimals to fractions in R?
>> I have the result:
>>>   summary(as.factor(complete.ID))
>>                  0 0.0133333333333333               0.04
>>               2256                488                230
>> 0.0666666666666667 0.0933333333333333  0.106666666666667
>>               2342                310                726
>>  0.133333333333333  0.146666666666667               0.16
>>                179                750                163
>>  0.186666666666667                0.2  0.226666666666667
>>                194                180                 57
>>  0.253333333333333  0.266666666666667  0.293333333333333
>>                  1                 10                 40
>>               0.32  0.333333333333333  0.346666666666667
>>                 45                 61                117
>>  0.373333333333333
>>                 62
>> How to convert something like this:
>>
>>                  0               1/75               3/75
>>               2256                488                230
>>               5/75               7/75               8/75
>>               2342                310                726
>>              10/75              11/75              12/75
>>                179                750                163
>>              14/75              15/75              17/75
>>                194                180                 57
>>              19/75              20/75              22/75
>>                  1                 10                 40
>>              24/75              25/75              26/75
>>                 45                 61                117
>>              28/75
>>                 62
>>
>> Any suggestions would be appreciated. Thanks you.
>> Best, Muhammad Subianto
>> PS.
>> I found this website: http://www.mindspring.com/~alanh/fracs.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html



From Bill.Venables at csiro.au  Fri Jan 27 13:11:40 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Fri, 27 Jan 2006 23:11:40 +1100
Subject: [R] How do you convert this from S Plus to R - any help
	appreciated . thanks
Message-ID: <B998A44C8986644EA8029CFE6396A924546985@exqld2-bne.qld.csiro.au>

This particular call to exportData is probably equivalent in effect to
the R call:

write.table(MU.Cost, 
	file = paste("C:/RAUDSL/S",
			    as.character(MU.Cost$Run.Id[1]),
				".",as.character(MU.Cost$MU.Id[1]),
				".MU.PRICE.OUTPUT.txt", sep=""),
	append = FALSE)

as Barry sort of guesses, but in general exportData handles a wide
variety of data export facilities, including writing to odbc connexions,
so other uses of exportData would need the RODBC library in R and
functions like sqlSave or sqlUpdate.

write.table is also an S-PLUS function as well, but exportData is
generally a whole lot faster.

Bill Venables.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Barry Rowlingson
Sent: Friday, 27 January 2006 7:57 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] How do you convert this from S Plus to R - any help
appreciated . thanks


Briggs, Meredith M wrote:

>
exportData(MU.Cost,paste("C:/RAUDSL/S",as.character(MU.Cost$Run.Id[1]),"
>
.",as.character(MU.Cost$MU.Id[1]),".MU.PRICE.OUTPUT.txt",sep=""),append
> = FALSE,type="ASCII",quote=FALSE)

  Looks like perfectly good R to me.

  Except there's no exportData function. I assume this is an Splus 
function that R doesn't have, in which case telling us what it does 
might help. What does the Splus manual have to say about it?

I'm guessing R's write.table might be of use.

  Assuming its exportData that has you stuck - the other bits should 
allwork in R no problem, all it does is construct a path from parts of 
the MU.Cost object.

Barry

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chanchal at biochem.mpg.de  Fri Jan 27 13:23:57 2006
From: chanchal at biochem.mpg.de (Chanchal Kumar)
Date: Fri, 27 Jan 2006 13:23:57 +0100
Subject: [R] How to change the GUI language option
Message-ID: <512FDBA9F3D0C54E95CBAA9A616BD3A6010DE8F8@msx.w2k.biochem.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/f77168ad/attachment.pl

From mi2kelgrum at yahoo.com  Fri Jan 27 13:38:39 2006
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Fri, 27 Jan 2006 04:38:39 -0800 (PST)
Subject: [R] substituting an object not found
Message-ID: <20060127123839.9986.qmail@web60213.mail.yahoo.com>

Is there any function in R like

is.not.found(x, y)  

meaning if you can't find object x, then use object
y??


Mikkel Grum



From csardi at rmki.kfki.hu  Fri Jan 27 13:48:59 2006
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Fri, 27 Jan 2006 13:48:59 +0100
Subject: [R] substituting an object not found
In-Reply-To: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
References: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
Message-ID: <20060127124859.GB7662@rmki.kfki.hu>

> rm(list=ls())
> a <- 1
> ifelse(exists("b"), b, a)
[1] 1
> b <- 2
> ifelse(exists("b"), b, a)
[1] 2
> 

Gabor

On Fri, Jan 27, 2006 at 04:38:39AM -0800, Mikkel Grum wrote:
> Is there any function in R like
> 
> is.not.found(x, y)  
> 
> meaning if you can't find object x, then use object
> y??
> 
> 
> Mikkel Grum
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From ripley at stats.ox.ac.uk  Fri Jan 27 13:53:17 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jan 2006 12:53:17 +0000 (GMT)
Subject: [R] How to change the GUI language option
In-Reply-To: <512FDBA9F3D0C54E95CBAA9A616BD3A6010DE8F8@msx.w2k.biochem.mpg.de>
References: <512FDBA9F3D0C54E95CBAA9A616BD3A6010DE8F8@msx.w2k.biochem.mpg.de>
Message-ID: <Pine.LNX.4.61.0601271249420.8541@gannet.stats>

This is an FAQ from Windows users, who seem not to consult the rw-FAQ.
It is right there on the 'Help' menu and contains lots of useful 
information.

If perchance you are not using Windows, it is also in the R-admin manual
which INSTALL asked you to read before installation.

>   I installed R from one of the mirror sites (actually UK mirror site).
> Once installed the interface is in German. Can you please suggest how to
> change the R interface from German to English?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Fri Jan 27 14:03:09 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 27 Jan 2006 08:03:09 -0500
Subject: [R] substituting an object not found
In-Reply-To: <20060127124859.GB7662@rmki.kfki.hu>
References: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
	<20060127124859.GB7662@rmki.kfki.hu>
Message-ID: <43DA1A0D.7090108@stats.uwo.ca>

On 1/27/2006 7:48 AM, Gabor Csardi wrote:
>> rm(list=ls())
>> a <- 1
>> ifelse(exists("b"), b, a)
> [1] 1
>> b <- 2
>> ifelse(exists("b"), b, a)
> [1] 2

That's not quite right.  ifelse() is meant for vectors of conditions; 
you really want just plain old "if" here:

   if (exists("b")) b else a

For example, with no b in the workspace:

 > a <- 1:10
 > ifelse(exists("b"), b, a)
[1] 1
 > if (exists("b")) b else a
  [1]  1  2  3  4  5  6  7  8  9 10

Duncan Murdoch
> 
> Gabor
> 
> On Fri, Jan 27, 2006 at 04:38:39AM -0800, Mikkel Grum wrote:
>> Is there any function in R like
>>
>> is.not.found(x, y)  
>>
>> meaning if you can't find object x, then use object
>> y??
>>
>>
>> Mikkel Grum
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pburns at pburns.seanet.com  Fri Jan 27 14:27:24 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 27 Jan 2006 13:27:24 +0000
Subject: [R] draft of Comment on UCLA tech report
Message-ID: <43DA1FBC.9070901@pburns.seanet.com>

You may  recall that there was a discussion of a technical
report from the statistical consulting group at UCLA.

I have a draft of a comment on that report, which you
can get from
http://www.burns-stat.com/pages/Flotsam/uclaRcomment_draft1.pdf

I'm interested in comments: corrections, additions, deletions.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From bolker at ufl.edu  Fri Jan 27 14:45:16 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 27 Jan 2006 13:45:16 +0000 (UTC)
Subject: [R] why did not data.table work?
References: <b1f16d9d0601262203o1ca915ebk7fb36418bec8abb3@mail.gmail.com>
	<971536df0601262212l154864fci11ae02b912509de9@mail.gmail.com>
	<b1f16d9d0601270014w2a95f402rfe974af234703ad9@mail.gmail.com>
Message-ID: <loom.20060127T144206-555@post.gmane.org>

Michael <comtech.usa <at> gmail.com> writes:

> 
> yes, the error message was extremely clear; but what was also very clear was
> that the file definitely exists under c:\ directory...

what happens with 

file.exists('c:\\llll.txt')
?

 (presumably the same error)

  also try

 f=file.choose()

select the file in the browser and then see what R thinks
f is.

  My guess is that there's a hidden extension here somewhere ...

  Ben Bolker



From Stephan.Matthiesen at ed.ac.uk  Fri Jan 27 15:23:53 2006
From: Stephan.Matthiesen at ed.ac.uk (Stephan Matthiesen)
Date: Fri, 27 Jan 2006 14:23:53 +0000
Subject: [R] Image Processing packages
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC19D@VAIEXCH04.vai.org>
References: <CEA39A213F7F2E44A0DED9210BCD352FEDC19D@VAIEXCH04.vai.org>
Message-ID: <200601271423.53462.Stephan.Matthiesen@ed.ac.uk>

Hi,

Am Donnerstag, 26. Januar 2006 19:39 schrieb Kort, Eric:
> I have a variety of standard image processing functions written in R,
> but have yet to distribute them because most people choose not to
> perform image analysis in R for the previously stated reasons.

I encourage you to distribute them. I work with satellite imagery, where you 
need both statistical tools for data analysis and versatile image 
processing/visualization tools.

> So in general I would agree that R is sub-optimal for image processing
> (and this is certainly outside the realm of things R was intended to do
> if I read the early mailing list archives correctly).  However, it can
> be done and it might be desirable to do so from a work-flow perspective.

I agree. Actually, it seems to me that an increasing number of people use R 
for tasks that are not strictly statistical analyses, often as a replacement 
for Matlab.

Thanks for your good work!

Stephan



From ggrothendieck at gmail.com  Fri Jan 27 15:33:58 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jan 2006 09:33:58 -0500
Subject: [R] substituting an object not found
In-Reply-To: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
References: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
Message-ID: <971536df0601270633o37dd29cya1c315ba82475007@mail.gmail.com>

This does not answer your question directly but this
can relate to calling functions and also to inheritance
in object oriented systems.  The common thread in
1b and 2 is that if a variable is not found in the current
environment R will look into the parent environment so if we
give our variables the same name but put them in
different environments that have a parent/child relationship
then R will use the local variable if its present and the
variable of the same name from the parent if not.
There is also an example here (1a) that uses function args
but that requires one to be explicit.

1a. function args

f <- function(a = b) a
f(1) #1

b <- 2
f() # 2

1b. function free variables

f <- function() { a <- 10; a }
f()  # uses a in function f

a <- 20
g <- function() a
g()  # uses a from global environment

2. oo

In this example,
the parent is the global environment and the child is object
oo.

library(proto)
a <- 1
oo <- proto(f = function(.) .$a)
oo1f()  # 1   It has used a in global environment.

oo$a <- 2
oo$f()  # #2   It has used a in object oo.

On 1/27/06, Mikkel Grum <mi2kelgrum at yahoo.com> wrote:
> Is there any function in R like
>
> is.not.found(x, y)
>
> meaning if you can't find object x, then use object
> y??
>
>
> Mikkel Grum



From ggrothendieck at gmail.com  Fri Jan 27 15:46:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jan 2006 09:46:46 -0500
Subject: [R] Prediction when using orthogonal polynomials in regression
In-Reply-To: <20060126164023.GA21657@lubyanka.local>
References: <20060126164023.GA21657@lubyanka.local>
Message-ID: <971536df0601270646l5c1ee28el2e7ffa007278fbd7@mail.gmail.com>

On 1/26/06, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> Folks,
>
> I'm doing fine with using orthogonal polynomials in a regression context:
>
>  # We will deal with noisy data from the d.g.p. y = sin(x) + e
>  x <- seq(0, 3.141592654, length.out=20)

This has already been answered but note that pi is a built in variable
in R.

>  y <- sin(x) + 0.1*rnorm(10)
>  d <- lm(y ~ poly(x, 4))
>  plot(x, y, type="l"); lines(x, d$fitted.values, col="blue") # Fits great!
>  all.equal(as.numeric(d$coefficients[1] + m %*% d$coefficients[2:5]),
>            as.numeric(d$fitted.values))
>
> What I would like to do now is to apply the estimated model to do
> prediction for a new set of x points e.g.
>  xnew <- seq(0,5,.5)
>
> We know that the predicted values should be roughly sin(xnew). What I
> don't know is: how do I use the object `d' to make predictions for
> xnew?
>
> --
> Ajay Shah                                      http://www.mayin.org/ajayshah
> ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
> <*(:-? - wizard who doesn't know the answer.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bgytcc at leeds.ac.uk  Fri Jan 27 15:53:57 2006
From: bgytcc at leeds.ac.uk (Tom C Cameron)
Date: Fri, 27 Jan 2006 14:53:57 +0000
Subject: [R] How do I "normalise" a power spectral density analysis?
Message-ID: <1138373637.4afbee09782cc@webmail1.leeds.ac.uk>

Hi everyone

Can anyone tell me how I normalise a power spectral density (PSD) plot of a
periodical time-series. At present I get the graphical output of spectrum VS
frequency.

What I want to acheive is period VS spectrum? Are these the same things but the
x-axis scale needs transformed ?

Any help would be greatly appreciated

Tom
...........................................................................
Dr Tom C Cameron                        office: 0113 34 32837 (10.23 Miall)
Ecology & Evolution Res. Group.         lab: 0113 34 32884 (10.20 Miall)
School of Biological Sciences           Mobile: 07966160266
University of Leeds                     email: t.c.cameron at leeds.ac.uk
Leeds LS2 9JT
LS2 9JT



From sfalcon at fhcrc.org  Fri Jan 27 15:54:17 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 27 Jan 2006 06:54:17 -0800
Subject: [R] substituting an object not found
In-Reply-To: <20060127123839.9986.qmail@web60213.mail.yahoo.com> (Mikkel
	Grum's message of "Fri, 27 Jan 2006 04:38:39 -0800 (PST)")
References: <20060127123839.9986.qmail@web60213.mail.yahoo.com>
Message-ID: <m2r76tg1ra.fsf@ziti.local>

On 27 Jan 2006, mi2kelgrum at yahoo.com wrote:

> Is there any function in R like
>
> is.not.found(x, y)  
>
> meaning if you can't find object x, then use object
> y??

Along with exists(), you might find mget() useful since it allows you
to specify an ifnotfound value.

--
+ seth



From isaac_kohane at harvard.edu  Fri Jan 27 16:08:27 2006
From: isaac_kohane at harvard.edu (Isaac Kohane)
Date: Fri, 27 Jan 2006 10:08:27 -0500
Subject: [R] Previously compilation procedure on Mac OS X no longer works
Message-ID: <8507825D-32E3-4048-9C42-817C731CD069@harvard.edu>

Hi,

	I was a happy user of Peter Parks' package (see http://www.chip.org/ 
~ppark/Supplements/PNAS05/) and could compile it without error under  
Mac OS X 10.4. I then had a disk crash and had to re-install the  
developer tools and now I get hideous messages such as  the one  
below. I have tried installing an earlier version of Mac OS X  
developer tools. No joy, Could anyone put me out of my misery and  
compile  this code and post the binaries for mac os X so that I can  
just get back to work? Please? Or is there a surefire way to get the  
compilation process working again?

Thanks in advance.


-Zak



ld: warning -L: directory name (/usr/local/lib) does not exist
ld: can't locate file for: -lcc_dynamic
make: *** [sigPathway.so] Error 1
ERROR: compilation failed for package 'sigPathway'
** Removing '/Library/Frameworks/R.framework/Versions/2.2/Resources/ 
library/sigPathway'



From gescati at yahoo.com.ar  Fri Jan 27 16:04:32 2006
From: gescati at yahoo.com.ar (=?iso-8859-1?q?gabriela=20escati=20pe=F1aloza?=)
Date: Fri, 27 Jan 2006 15:04:32 +0000 (GMT)
Subject: [R] how calculation degrees freedom
Message-ID: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/ac958db3/attachment.pl

From Antigen_PEGASUS at cug1.umt.edu  Fri Jan 27 16:23:49 2006
From: Antigen_PEGASUS at cug1.umt.edu (Antigen_PEGASUS@cug1.umt.edu)
Date: 27 Jan 2006 08:23:49 -0700
Subject: [R] Antigen forwarded attachment
Message-ID: <PEGASUSOMyQlmLFQG10000000a9@pegasus.cfc.umt.edu>

The entire message "Re: [R] regular expressions, sub", originally sent to you by r-help-bounces at stat.math.ethz.ch (r-help-bounces at stat.math.ethz.ch), has been forwarded to you from the Antigen Quarantine area.
This message may have been re-scanned by Antigen and handled according to the appropriate scan job's settings.



<<Entire Message.eml>>
-------------- next part --------------
An embedded message was scrubbed...
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] regular expressions, sub
Date: Fri, 27 Jan 2006 10:49:36 +0000 (GMT)
Size: 8048
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/84003573/attachment.mht

From Roger.Bivand at nhh.no  Fri Jan 27 16:25:40 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 27 Jan 2006 16:25:40 +0100 (CET)
Subject: [R] Previously compilation procedure on Mac OS X no longer works
In-Reply-To: <8507825D-32E3-4048-9C42-817C731CD069@harvard.edu>
Message-ID: <Pine.LNX.4.44.0601271624530.3657-100000@reclus.nhh.no>

On Fri, 27 Jan 2006, Isaac Kohane wrote:

> Hi,
> 
> 	I was a happy user of Peter Parks' package (see http://www.chip.org/ 
> ~ppark/Supplements/PNAS05/) and could compile it without error under  
> Mac OS X 10.4. I then had a disk crash and had to re-install the  
> developer tools and now I get hideous messages such as  the one  
> below. I have tried installing an earlier version of Mac OS X  
> developer tools. No joy, Could anyone put me out of my misery and  
> compile  this code and post the binaries for mac os X so that I can  
> just get back to work? Please? Or is there a surefire way to get the  
> compilation process working again?

One source of information is:

http://wiki.urbanek.info/index.cgi?HomePage

and the R-sig-Mac list

> 
> Thanks in advance.
> 
> 
> -Zak
> 
> 
> 
> ld: warning -L: directory name (/usr/local/lib) does not exist
> ld: can't locate file for: -lcc_dynamic
> make: *** [sigPathway.so] Error 1
> ERROR: compilation failed for package 'sigPathway'
> ** Removing '/Library/Frameworks/R.framework/Versions/2.2/Resources/ 
> library/sigPathway'
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From rxg218 at psu.edu  Fri Jan 27 16:30:02 2006
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 27 Jan 2006 10:30:02 -0500
Subject: [R] identifying points on a clusplot
Message-ID: <1138375802.10078.11.camel@blue.chem.psu.edu>

Hi, I would like to identify a few points on a clusplot generated from a
fanny clustering.

The manpage for clusplot indicates that setting labels=5 allows me to
identify the points. However I only need to identify a subset.

I tried setting the 'Labels' attribute of my data matrix (23 rows) as:

> rownames(desc) <- NULL
> attr(desc, 'Labels') <- l # l is a vector with the labels
> attr(desc, 'Labels')
 [1] "27"  ""    ""    ""    ""    ""    "43"  ""    ""    ""    ""    "48"
[13] ""    ""    ""    ""    ""    ""    ""    ""    "169" "172" "177"
> clus <- fanny(desc, k=2)
> clusplot(clus, color=TRUE, lines=0, labels=5)

However, when I select points it use 1,2,3 ... 23 rather than the values
from the 'Label's attribute. Furthermore it forces me to select all the
points on the plot.

Is there a way to identify a subset of points on a clusplot, rather than
all the points?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A mathematician is a device for turning coffee into theorems.
-- P. Erdos



From Charles.Annis at StatisticalEngineering.com  Fri Jan 27 16:35:43 2006
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 27 Jan 2006 10:35:43 -0500
Subject: [R] Image Processing packages
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352FEDC19D@VAIEXCH04.vai.org>
Message-ID: <005201c62357$55f62740$6600a8c0@DD4XFW31>

Eric:

I use R to quantify the efficacy of ultrasonic inspections of metal
components (e.g. looking for nonmetallic inclusions in forgings) and use R
for image processing, but my methods have been rather a kluge.  I am
interested in your R functions, if you will make them available.
Unfortunately, making a package for CRAN is (in my opinion) WAY too hard on
Windows, and I've given up, but I hope that you do not.  I second Stephan
Matthiesen's recent suggestion that you make your image processing functions
available to fellow R users, if not on CRAN, then perhaps as ascii files
from your website.

Thanks.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kort, Eric
Sent: Thursday, January 26, 2006 2:39 PM
To: Thomas Kaliwe; r-help at stat.math.ethz.ch
Subject: Re: [R] Image Processing packages

Thomas Kaliwe wrote:
> Hi,
>  
> I've been looking for Image Processing packages. Thresholding, Edge
> Filters, Dct, Segmentation, Restoration. I'm aware, that Octave,
Matlab
> etc. would be a good address but then I'm missing the "statistical
> power"  of R. Does anybody know of packages, projects etc. Comments on
> wether the use of R for such matters is useful are welcome.
>  

See also my package rtiff for reading tiff images.

I routinely do image analysis in R.  Yes, it is relatively slow compared
to dedicated solutions, but I like the smooth integration with the
associated statistical analysis and the ability to have a single script
that performs the image analysis and multiple files and subsequent
statistical analysis, and with modern computing equipment R is fast
enough for my purposes.  

I have a variety of standard image processing functions written in R,
but have yet to distribute them because most people choose not to
perform image analysis in R for the previously stated reasons.  

So in general I would agree that R is sub-optimal for image processing
(and this is certainly outside the realm of things R was intended to do
if I read the early mailing list archives correctly).  However, it can
be done and it might be desirable to do so from a work-flow perspective.

-Eric

> Greetings
>  
> Thomas Kaliwe
This email message, including any attachments, is for the so...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From drewbrewit at yahoo.com  Fri Jan 27 16:41:54 2006
From: drewbrewit at yahoo.com (Drew)
Date: Fri, 27 Jan 2006 07:41:54 -0800 (PST)
Subject: [R] panel function with barchart (lattice)
Message-ID: <20060127154154.71747.qmail@web50907.mail.yahoo.com>

Deepayan, thank you for your help!!

After much trial and error (and re-reading the help
files), I was able to
come up with what I wanted by making sure the first
line of my panel
function looked like this:

		panel = function(y,x,...)


The complete set of code I ended up using:

barchart(yield ~ variety | site, data = barley,
        groups = year,
		layout = c(1,6),
		stack = TRUE,
		auto.key = list(points = FALSE, rectangles = TRUE,
space = "top"),
		scales = list(x = list(abbreviate = TRUE, minlength
= 5, rot = 45)),
		panel = function(y,x,...){
			panel.grid(h = -1, v = 0, col = "gray", lty =
"dotted")
			panel.barchart(x,y,...)
			panel.text(x,y,label = round(y,1),cex=.8)
	}
)

Thank you for your work on the lattice package!
~Nick



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Deepayan Sarkar
Sent: Wednesday, January 25, 2006 11:30 AM
To: Drew
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] panel function with barchart
(lattice)


On 1/25/06, Drew <drewbrewit at yahoo.com> wrote:
> Folks at R help,
>
> I can't quite get the panel function to work the way
I
> want within barchart.
> I guess I'm still not understanding how to piece
> together multiple panel
> arguments, especially when "groups" is specified.
>
> Example: I want to be able to add the value of
"yield"
> to each section of
> each bar in this graph:
>
> barchart(yield ~ variety | site, data = barley,
> 	groups = year,
> 	layout = c(1,6),
> 	stack=TRUE,
> 	ylab = "Barley Yield (bushels/acre)"
> )
>
> To do this, I add my panel function:
>
> barchart(yield ~ variety | site, data = barley,
> 	groups = year,
> 	layout = c(1,6),
> 	stack=TRUE,
> 	ylab = "Barley Yield (bushels/acre)",
>
> 	panel = function(x,y,subscripts,groups,...){
> 		panel.barchart(x,y,...)

Well, panel.barchart needs the subscripts and groups
arguments to draw
stacked bar charts, and you are calling it without
them.

> 		ltext(x = x, y = y, label =
> round(barley$yield[subscripts],1), cex=.8)

The y values will need to be accumulated. Have you
looked at what
panel.barchart does?

> 	}
> )
>
> Then I get the values to print on each bar (which is
> what I want) but the
> bars no longer stack to appropriate height, and I
> cannot get the subsections
> of each bar to be a different color. I've tried
> numerous variations of
> panel.barchart, panel.superpose, etc. using examples
> from ?xyplot, but
> nothing quite works or I get an error message.
>
> Any help would be appreciated.
>
> ~Nick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


--
http://www.stat.wisc.edu/~deepayan/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Eric.Kort at vai.org  Fri Jan 27 16:48:50 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Fri, 27 Jan 2006 10:48:50 -0500
Subject: [R] Image Processing packages
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC1A2@VAIEXCH04.vai.org>

Charles Annis, P.E. writes...
> Eric:
> 
> I use R to quantify the efficacy of ultrasonic inspections of metal
> components (e.g. looking for nonmetallic inclusions in forgings) and
use R
> for image processing, but my methods have been rather a kluge.  I am
> interested in your R functions, if you will make them available.
> Unfortunately, making a package for CRAN is (in my opinion) WAY too
hard
> on
> Windows, and I've given up, but I hope that you do not.  I second
Stephan
> Matthiesen's recent suggestion that you make your image processing
> functions
> available to fellow R users, if not on CRAN, then perhaps as ascii
files
> from your website.

I knew I was asking for trouble when I sent that post. =)

Ok, I will clean up my code and put together a package, hopefully in the
next couple of weeks.

-Eric

> 
> Thanks.
> 
> Charles Annis, P.E.
This email message, including any attachments, is for the so...{{dropped}}



From dmbates at gmail.com  Fri Jan 27 17:06:00 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 27 Jan 2006 10:06:00 -0600
Subject: [R] how calculation degrees freedom
In-Reply-To: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
Message-ID: <40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>

On 1/27/06, gabriela escati pe??aloza <gescati at yahoo.com.ar> wrote:
> Hi, I' m having a hard time understanding the computation of degrees of freedom

So do I and I'm one of the authors of the package :-)

> when runing nlme() on the following model:
>
>   > formula(my data.gd)
> dLt ~ Lt | ID
>
>   TasavB<- function(Lt, Linf, K) (K*(Linf-Lt))
>
>   my model.nlme <- nlme (dLt ~ TasavB(Lt, Linf, K),
>   data = my data.gd,
>   fixed = list(Linf ~ 1, K ~ 1),
>   start = list(fixed = c(70, 0.4)),
>   na.action= na.include, naPattern = ~!is.na(dLt))
>
>   > summary(my model.nlme)
> Nonlinear mixed-effects model fit by maximum likelihood
>   Model: dLt ~ TasavB(Lt, Linf, K)
>  Data: my data.gd
>        AIC      BIC    logLik
>   13015.63 13051.57 -6501.814
>   Random effects:
>  Formula: list(Linf ~ 1 , K ~ 1 )
>  Level: ID
>  Structure: General positive-definite
>             StdDev   Corr
>     Linf 7.3625291 Linf
>        K 0.0845886 -0.656
> Residual 1.6967358
>   Fixed effects: list(Linf + K ~ 1)
>         Value Std.Error   DF  t-value p-value
> Linf 69.32748 0.4187314  402 165.5655  <.0001
>    K  0.31424 0.0047690 2549  65.8917  <.0001
>   Standardized Within-Group Residuals:
>       Min         Q1         Med        Q3      Max
>  -3.98674 -0.5338083 -0.02783649 0.5261591 4.750609
>   Number of Observations: 2952
> Number of Groups: 403
> >
>
>   Why are the DF of Linf and K different? I would apreciate if you could point me to a reference

The algorithm is described in Pinheiro and Bates (2000) "Mixed-effects
Models in S and S-PLUS" published by Springer.  See section 2.4.2

I would point out that there is effectively no difference between a
t-distribution with 402 df and a t-distribution with 2549 df so the
actual number of degrees of freedom is irrelevant in this case.  All
you need to know is that it is "large".

I will defer to any of the "degrees of freedom police" who post to
this list to give you an explanation of why there should be different
degrees of freedom.  I have been studying mixed-effects models for
nearly 15 years and I still don't understand.

>   Note: I working with Splus 6.1. for Windows

Technically this email list is for questions about R.  There is
another list, s-news at biostat.wustl.edu, for questions about S-PLUS.

>
>
> Lic. Gabriela Escati Pe??aloza
> Biolog??a y Manejo de Recursos Acu??ticos
> Centro Nacional Patag??nico(CENPAT).
> CONICET
> Bvd. Brown s/n??.
> (U9120ACV)Pto. Madryn
> Chubut
> Argentina
>
> Tel: 54-2965/451301/451024/451375/45401 (Int:277)
> Fax: 54-29657451543
>
> ---------------------------------
>  1GB gratis, Antivirus y Antispam
>  Correo Yahoo!, el mejor correo web del mundo
>  Abr?? tu cuenta aqu??
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From hamstersquats at web.de  Fri Jan 27 18:23:17 2006
From: hamstersquats at web.de (Thomas Kaliwe)
Date: Fri, 27 Jan 2006 18:23:17 +0100
Subject: [R] Image Processing packages
In-Reply-To: <005201c62357$55f62740$6600a8c0@DD4XFW31>
Message-ID: <000c01c62366$5d2eeab0$0401a8c0@mouse>

Hi,

Is it possible that I could see some of your functions and/or results of
those functions(Eric, Charles, Stephan). It's more about that I'd like
to see what already has been accomplished and the way that was chosen
thus circumventing reinvention and getting an overview.


Cheers

Thomas

-----Urspr??ngliche Nachricht-----
Von: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Charles Annis,
P.E.
Gesendet: Freitag, 27. Januar 2006 16:36
An: 'Kort, Eric'
Cc: r-help at stat.math.ethz.ch; Stephan.Matthiesen at ed.ac.uk
Betreff: Re: [R] Image Processing packages

Eric:

I use R to quantify the efficacy of ultrasonic inspections of metal
components (e.g. looking for nonmetallic inclusions in forgings) and use
R
for image processing, but my methods have been rather a kluge.  I am
interested in your R functions, if you will make them available.
Unfortunately, making a package for CRAN is (in my opinion) WAY too hard
on
Windows, and I've given up, but I hope that you do not.  I second
Stephan
Matthiesen's recent suggestion that you make your image processing
functions
available to fellow R users, if not on CRAN, then perhaps as ascii files
from your website.

Thanks.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kort, Eric
Sent: Thursday, January 26, 2006 2:39 PM
To: Thomas Kaliwe; r-help at stat.math.ethz.ch
Subject: Re: [R] Image Processing packages

Thomas Kaliwe wrote:
> Hi,
>  
> I've been looking for Image Processing packages. Thresholding, Edge
> Filters, Dct, Segmentation, Restoration. I'm aware, that Octave,
Matlab
> etc. would be a good address but then I'm missing the "statistical
> power"  of R. Does anybody know of packages, projects etc. Comments on
> wether the use of R for such matters is useful are welcome.
>  

See also my package rtiff for reading tiff images.

I routinely do image analysis in R.  Yes, it is relatively slow compared
to dedicated solutions, but I like the smooth integration with the
associated statistical analysis and the ability to have a single script
that performs the image analysis and multiple files and subsequent
statistical analysis, and with modern computing equipment R is fast
enough for my purposes.  

I have a variety of standard image processing functions written in R,
but have yet to distribute them because most people choose not to
perform image analysis in R for the previously stated reasons.  

So in general I would agree that R is sub-optimal for image processing
(and this is certainly outside the realm of things R was intended to do
if I read the early mailing list archives correctly).  However, it can
be done and it might be desirable to do so from a work-flow perspective.

-Eric

> Greetings
>  
> Thomas Kaliwe
This email message, including any attachments, is for the\ s...{{dropped}}



From rduval at gmail.com  Fri Jan 27 18:50:27 2006
From: rduval at gmail.com (Robert Duval)
Date: Fri, 27 Jan 2006 12:50:27 -0500
Subject: [R] draft of Comment on UCLA tech report
In-Reply-To: <43DA1FBC.9070901@pburns.seanet.com>
References: <43DA1FBC.9070901@pburns.seanet.com>
Message-ID: <2b6e342f0601270950n54d0b848ia968a22fd5407e64@mail.gmail.com>

Being a Stata user in transition to R I have to say that it would be
fair to mention that data handling for large amounts of data might
take an extra step in R.

I understand that there are good reasons for R consuming more memory
(than Stata) when handling large datasets, but it is necessary to warn
the potential newcomers that they might require using MySQL or other
database managers if they use large datasets.

greetings
robert

On 1/27/06, Patrick Burns <pburns at pburns.seanet.com> wrote:
> You may  recall that there was a discussion of a technical
> report from the statistical consulting group at UCLA.
>
> I have a draft of a comment on that report, which you
> can get from
> http://www.burns-stat.com/pages/Flotsam/uclaRcomment_draft1.pdf
>
> I'm interested in comments: corrections, additions, deletions.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Eric.Kort at vai.org  Fri Jan 27 19:15:49 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Fri, 27 Jan 2006 13:15:49 -0500
Subject: [R] Image Processing packages
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352FEDC1A6@VAIEXCH04.vai.org>


> From: hamstersquats at web.de [mailto:hamstersquats at web.de]
> Hi,
> 
> Is it possible that I could see some of your functions and/or results
of
> those functions(Eric, Charles, Stephan). It's more about that I'd like
> to see what already has been accomplished and the way that was chosen
> thus circumventing reinvention and getting an overview.
> 

Yes.  I sent some examples to you off list (didn't want to clog the
mailing list with the images).

> 
> Cheers
> 
> Thomas
> 
This email message, including any attachments, is for the so...{{dropped}}



From avilella at gmail.com  Fri Jan 27 19:13:37 2006
From: avilella at gmail.com (Albert Vilella)
Date: Fri, 27 Jan 2006 19:13:37 +0100
Subject: [R] list entries file into a list
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED761@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED761@usctmx1106.merck.com>
Message-ID: <1138385617.25794.2.camel@localhost.localdomain>

This worked great but I'm a bit confused about how to access the names
(keys?) of a list in a loop, and why this is failing for me:

I want to cross the entries in "mylist" (like "entry0001") with the
columns of a dataframe "ginput":

$entry0001
> [1] "AB0032"  "CF32134" "DF34334"
> 
> $entry0002
> [1] "AB0033"
> 
> $entry0003
> [1] "AB0032"   "CF32134"  "DF34334"  "DD343434" "DD34222" 
> 
> $entry0004
> [1] "AB0032"  "CF32134"

> ginput
             mid    reffile       pa
1     myetag0001     AB0032 0.778270
2     myetag0002     AB0032 0.153093
3     myetag0003     AB0032 0.392175
4     myetag0004     AB0032 0.696303
5     myetag0005     AB0032 0.688537
6     myetag0006     AB0032 0.767044
7     myetag0001     AB0033 0.420000
8     myetag0002     AB0033 0.330333
9     myetag0003     AB0033 0.238013
10    myetag0004     AB0033 0.043213
...

The rough idea has been to do something like:

mynames = rep(0,length(names(mylist)))
mypas = rep(0,length(names(mylist)))
i = 0
for (entrylabel in names(mylist)) {
  mynames[i] = entrylabel
  mypas[i] =+ sum(ginput$pa[ginput$reffile %in% mylist$entrylabel])
  i = i+1
}

But mylist$entrylabel is not working inside the loop.
I checked that entrylabel returns:
[1] "entry0001"
And that mylist$entry0001 is returning the right list.

I would like to end up with something like:

dfpa = data.frame(dfnames=mynames,dfpas=mypas)
> dfpa
    dfnames       dfpas
1 entry0001    3.475422
2 entry0002    2.221355
...

I haven't been able to found my way with R lists, maybe because I'm
comparing them with perl's hashes.

Any idea of what I'm missing?

    Albert.




El dj 26 de 01 del 2006 a les 08:14 -0500, en/na Liaw, Andy va escriure:
> The following might be what you want (replace "clipboard" with your
> filename):
> 
> > mylist <- strsplit(readLines("clipboard"), ":")
> > nm <- sapply(mylist, "[", 1)
> > mylist <- lapply(mylist, "[", -1)
> > names(mylist) <- nm
> > mylist <- lapply(mylist, function(s) strsplit(s, ",")[[1]])
> > mylist
> $entry0001
> [1] "AB0032"  "CF32134" "DF34334"
> 
> $entry0002
> [1] "AB0033"
> 
> $entry0003
> [1] "AB0032"   "CF32134"  "DF34334"  "DD343434" "DD34222" 
> 
> $entry0004
> [1] "AB0032"  "CF32134"
> 
> Andy
> 
> From: Albert Vilella
> > 
> > Hi all,
> > 
> > I have a file of this kind:
> > 
> > entry0001:AB0032,CF32134,DF34334
> > entry0002:AB0033
> > entry0003:AB0032,CF32134,DF34334,DD343434,DD34222
> > entry0004:AB0032,CF32134
> > 
> > And I would like to read it into something like a hash, so that I can
> > then loop over it by keys and values.
> > 
> > I wonder which would be the best way to do that in R?
> > 
> > Thanks,
> > 
> >     Albert.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From joshuacgilbert at gmail.com  Fri Jan 27 19:25:27 2006
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Fri, 27 Jan 2006 13:25:27 -0500
Subject: [R]  Classifying Intertwined Spirals
Message-ID: <ef96daa30601271025o78bd103dqbe16a9d3dac00595@mail.gmail.com>

I'm using an SVM as I've seen a paper that reported extremely good
results. I'm not having such luck. I'm also interested in ideas for
other approaches to the problem that can also be applied to general
problems (no assuming that we're looking for spirals).

Here is my code:
library(mlbench)
library(e1071)
raw <- mlbench.spirals(194, 2)
spiral <- data.frame(class=as.factor(raw$classes), xx=raw$x[,1], y=raw$x[,2])
m <- svm(class~., data=spiral)
plot(m, spiral)

You'll note that I have two spirals with 97 points each and I'm using
a kernel with a radial basis: exp(-gamma*|u-v|^2).

You should be able to see a PNG of the resulting plot here:
http://www.flickr.com/photos/60118409 at N00/91835679/

The problem is that that's not good enough. I want a better fit. I
think I can get one, I just don't know how.

There's a paper on Proximal SVMs that claims a better result. To the
best of my knowledge, PSVMs should not outperform SVMs, they are
merely faster to compute. You can find the paper (with the picture of
their SVM) on citeseer:
http://citeseer.ifi.unizh.ch/cachedpage/515368/5
@misc{ fung-proximal,
  author = "G. Fung and O. Mangasarian",
  title = "Proximal support vector machine classifiers",
  text = "G. Fung and O. Mangasarian. Proximal support vector machine
classifiers.
    In F. P. D. Lee and R. Srikant, editors, KDD",
  url = "citeseer.ifi.unizh.ch/515368.html" }

I don't have much of a background in SVMs, I'm learning as I go, so
please don't hold back 'simple-minded' suggestions.

I'm also asking the authors, but I'm not expecting a reply from them.

There was a paper by Lang and Whitbrock in 1988 (Learning to Tell Two
Spirals Apart) that solved the problem with a neural network, but they
used a very specialized network architecture. I would say that
discovering such an architecture and then optimizing it would be very
time-intensive.

Thank you for any response.

Josh.



From Soren.Hojsgaard at agrsci.dk  Fri Jan 27 19:47:24 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 27 Jan 2006 19:47:24 +0100
Subject: [R] how calculation degrees freedom
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>

Degrees of freedom for mixed models is a delicate issue - except in certain orthogonal designs. 
 
However, I'll just point out that for lmer models, there is a simulate() function which can simulate data from a fitted model. simulate() is very fast - just like lmer(). So one way to "get around the problem" could be to evaluate the test statistic (e.g. -2 log Q) in an empirical distribution based on simulations under the model; that is to calculate a Monte Carlo p-value. It is fairly fast to and takes about 10 lines of code to program. 
 
Of course, Monte Carlo p-values have their problems, but the world is not perfect....
 
Along similar lines, I've noticed that the anova() function for lmer models now only reports the mean squares to go into the numerator but "nothing for the denominator" of an F-statistic; probably in recognition of the degree of freedom problem. It could be nice, however, if anova() produced even an approximate anova table which can be obtained from Wald tests. The anova function could then print that "these p-values are large sample ones and hence only approximate"...
 
Best regards
S??ren
 
 
 
 
 

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Douglas Bates
Sendt: fr 27-01-2006 17:06
Til: gabriela escati pe??aloza
Cc: R-help at stat.math.ethz.ch
Emne: Re: [R] how calculation degrees freedom



On 1/27/06, gabriela escati pe??aloza <gescati at yahoo.com.ar> wrote:
> Hi, I' m having a hard time understanding the computation of degrees of freedom

So do I and I'm one of the authors of the package :-)

> when runing nlme() on the following model:
>
>   > formula(my data.gd)
> dLt ~ Lt | ID
>
>   TasavB<- function(Lt, Linf, K) (K*(Linf-Lt))
>
>   my model.nlme <- nlme (dLt ~ TasavB(Lt, Linf, K),
>   data = my data.gd,
>   fixed = list(Linf ~ 1, K ~ 1),
>   start = list(fixed = c(70, 0.4)),
>   na.action= na.include, naPattern = ~!is.na(dLt))
>
>   > summary(my model.nlme)
> Nonlinear mixed-effects model fit by maximum likelihood
>   Model: dLt ~ TasavB(Lt, Linf, K)
>  Data: my data.gd
>        AIC      BIC    logLik
>   13015.63 13051.57 -6501.814
>   Random effects:
>  Formula: list(Linf ~ 1 , K ~ 1 )
>  Level: ID
>  Structure: General positive-definite
>             StdDev   Corr
>     Linf 7.3625291 Linf
>        K 0.0845886 -0.656
> Residual 1.6967358
>   Fixed effects: list(Linf + K ~ 1)
>         Value Std.Error   DF  t-value p-value
> Linf 69.32748 0.4187314  402 165.5655  <.0001
>    K  0.31424 0.0047690 2549  65.8917  <.0001
>   Standardized Within-Group Residuals:
>       Min         Q1         Med        Q3      Max
>  -3.98674 -0.5338083 -0.02783649 0.5261591 4.750609
>   Number of Observations: 2952
> Number of Groups: 403
> >
>
>   Why are the DF of Linf and K different? I would apreciate if you could point me to a reference

The algorithm is described in Pinheiro and Bates (2000) "Mixed-effects
Models in S and S-PLUS" published by Springer.  See section 2.4.2

I would point out that there is effectively no difference between a
t-distribution with 402 df and a t-distribution with 2549 df so the
actual number of degrees of freedom is irrelevant in this case.  All
you need to know is that it is "large".

I will defer to any of the "degrees of freedom police" who post to
this list to give you an explanation of why there should be different
degrees of freedom.  I have been studying mixed-effects models for
nearly 15 years and I still don't understand.

>   Note: I working with Splus 6.1. for Windows

Technically this email list is for questions about R.  There is
another list, s-news at biostat.wustl.edu, for questions about S-PLUS.

>
>
> Lic. Gabriela Escati Pe??aloza
> Biolog??a y Manejo de Recursos Acu??ticos
> Centro Nacional Patag??nico(CENPAT).
> CONICET
> Bvd. Brown s/n??.
> (U9120ACV)Pto. Madryn
> Chubut
> Argentina
>
> Tel: 54-2965/451301/451024/451375/45401 (Int:277)
> Fax: 54-29657451543
>
> ---------------------------------
>  1GB gratis, Antivirus y Antispam
>  Correo Yahoo!, el mejor correo web del mundo
>  Abr?? tu cuenta aqu??
>         [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sfalcon at fhcrc.org  Fri Jan 27 19:51:01 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 27 Jan 2006 10:51:01 -0800
Subject: [R] list entries file into a list
In-Reply-To: <1138385617.25794.2.camel@localhost.localdomain> (Albert
	Vilella's message of "Fri, 27 Jan 2006 19:13:37 +0100")
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED761@usctmx1106.merck.com>
	<1138385617.25794.2.camel@localhost.localdomain>
Message-ID: <m24q3pfqsq.fsf@ziti.local>

On 27 Jan 2006, avilella at gmail.com wrote:
> But mylist$entrylabel is not working inside the loop.

'$' doesn't evaluate its argument.  You want mylist[[entrylabel]].


> I haven't been able to found my way with R lists, maybe because I'm
> comparing them with perl's hashes.

R lists do have names, but they are not hashes.  You can have
duplicate names in different positions in a list.

You can get at list elements by name using $name and [[name]], but the
argument to $ has to be the actual name, not a var containing the
name.

You can also access elements by number using [[i]]

HTH,

+ seth



From ggrothendieck at gmail.com  Fri Jan 27 20:49:49 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jan 2006 14:49:49 -0500
Subject: [R] regular expressions, sub
In-Reply-To: <43D9EDC3.9000807@wsl.ch>
References: <mailman.9.1138100401.32594.r-help@stat.math.ethz.ch>
	<43D9EDC3.9000807@wsl.ch>
Message-ID: <971536df0601271149h6b74c545ncb1217d93bdad816@mail.gmail.com>

In this post:

	http://finzi.psych.upenn.edu/R/Rhelp02a/archive/30590.html

Thomas Lumley provided a function to traverse a formula recursively.
We can modify it as shown to transform ln(m)^n to ln^n(m) producing
proc2.  We then bundle everything up into proc3 which uses substitute
to translate log to ln and remove (, the calls proc2 to do the aforementioned
substitute and finally we use simple character processing to clean up the
rest.

Although this is substantially longer in terms of lines of code
we did not have to write many of them because proc2 is actually
just a modification of the code in the indicated post and the
character processing becomes extremely simple.  Also its more
powerful able to handle expressions like:

	log(D) ~ log(log(N)^2)^3




proc2 <-function(formula){
	process<-function(expr){
	    if (length(expr)==1)
	      return(expr)
	   if(length(expr)==2) {
	      expr[[2]] <- process(expr[[2]])
	      return(expr)
	   }
	   if ( expr[[1]]==as.name("^") && length(expr[[2]])==2 &&
		      expr[[2]][[1]] == as.name("ln") &&
		      class(idx <- expr[[3]]) == "numeric") {
		expr <- as.call(list(as.name(paste("ln",idx,sep = "^")),
		   expr[[2]][[2]]))
		expr[[2]] <- process(expr[[2]])
		return(expr)
	   }
	   expr[[2]]<-process(expr[[2]])
	   expr[[3]]<-process(expr[[3]])
	   return(expr)
	  }
   formula[[3]]<-process(formula[[3]])
   formula
}

proc3 <- function(f) {

	# replace log with ln
	result <- do.call("substitute", list(f, list(log = as.name("ln"))))

	# remove I
	result <- do.call("substitute", list(result, list(I = as.name("("))))

	# transform ln(m)^n to ln^n(m)
	result <- proc2(result)

	# now clean up using simple character substitutions
	result <- deparse(result)

	# ( -> space
	result <- gsub("[(]", " ", result)

	# remove " and )
	gsub("[\")]", "", result)
}

# tests

proc3( log(D) ~ log(N)+I(log(N)^2)+log(t) )     # "ln D ~ ln N +  ln^2 N + ln t"

proc3( log(D) ~ log(log(N)^2)^3)       # "ln D ~ ln^3 ln^2 N"



On 1/27/06, Christian Hoffmann <christian.hoffmann at wsl.ch> wrote:
> Hi,
>
> I am trying to use sub, regexpr on expressions like
>
>    log(D) ~ log(N)+I(log(N)^2)+log(t)
>
> being a model specification.
>
> The aim is to produce:
>
>    "ln D ~ ln N + ln^2 N + ln t"
>
> The variable names N, t may change, the number of terms too.
>
> I succeded only partially, help on regular expressions is hard to
> understand for me, examples on my case are rare. The help page on R-help
> for grep etc. and "regular expressions"
>
> What I am doing:
>
> (f <- log(D) ~ log(N)+I(log(N)^2)+log(t))
> (ft <- sub("","",f))   # creates string with parts of formula, how to do
> it simpler?
> (fu <- paste(ft[c(2,1,3)],collapse=" "))  # converts to one string
>
> Then I want to use \1 for backreferences something like
>
> (fv <- sub("log( [:alpha:] N  )^ [:alpha:)","ln \\1^\\2",fu))
>
> to change "log(g)^7" to "ln^7 g",
>
> and to eliminate I(): sub("I(blabla)","\\1",fv)  # I(xxx) -> xxx
>
> The special characters are making trouble, sub acceps "(", ")" only in
> pairs. Code for experimentation:
>
> trysub <- function(s,t,e) {
> ii<-0; for (i1 in c(TRUE,FALSE)) for (i2 in c(TRUE,FALSE)) for (i3 in
> c(TRUE,FALSE)) for (i4 in c(TRUE,FALSE))
> print(paste(ii<-ii+1,ifelse(i1,"  "," ~"),"ext",ifelse(i2,"  ","
> ~"),"perl",ifelse(i3,"  "," ~"),"fixed ",ifelse(i4,"  "," ~"),"useBytes:
> ", try(sub(s,t,e, extended=i1, perl=i2, fixed=i3,
> useBytes=i4)),sep=""));invisible(0) }
>
> trysub("I(log(N)^2)","ln n^2",fu) # A: desired result for cases
> 5,6,13..16, the rest unsubstituted
>
> trysub("log(","ln ",fu)           # B: no substitutions; errors for
> cases 1..4,7.. 12   # typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement,
> x, ignore.case, useBytes) : \n\tinvalid regular expression 'log('\n"
>
> trysub("log\(","ln ",fu)          # C: same as A
>
> trysub("log\\(","ln ",fu)         # D: no substitutions; errors for
> cases 15,16        # typical errors:
> "15 ~ext ~perl ~fixed   useBytes: Error in sub(pattern, replacement, x,
> ignore.case, extended, fixed, useBytes) : \n\tinvalid regular expression
> 'log\\('\n"
>
> trysub("log\\(([:alpha:]+)\\)","ln \1",fu) # no substitutions, no errors
> # E: typical errors:
> "3  ext  perl ~fixed   useBytes: Error in sub.perl(pattern, replacement,
> x, ignore.case, useBytes) : \n\tinvalid regular expression
> 'log\\(([:alpha:]+)\\)'\n"
>
>
>
> Thanks for help
> Christian
>
> PS. The explanations in the documents
> --
> Dr. Christian W. Hoffmann,
> Swiss Federal Research Institute WSL
> Mathematics + Statistical Computing
> Zuercherstrasse 111
> CH-8903 Birmensdorf, Switzerland
>
> Tel +41-44-7392-277  (office)   -111(exchange)
> Fax +41-44-7392-215  (fax)
> christian.hoffmann at wsl.ch
> http://www.wsl.ch/staff/christian.hoffmann
>
> International Conference 5.-7.6.2006 Ekaterinburg Russia
> "Climate changes and their impact on boreal and temperate forests"
> http://ecoinf.uran.ru/conference/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sskim.box at gmail.com  Fri Jan 27 21:09:07 2006
From: sskim.box at gmail.com (Sung Soo Kim)
Date: Fri, 27 Jan 2006 15:09:07 -0500
Subject: [R] language setting of the R Window in Windows XP Korean ed.
In-Reply-To: <x2mzhl6g2b.fsf@viggo.kubism.ku.dk>
References: <edc66abd0601240224w237b1d07sb181c6066335097b@mail.gmail.com>
	<Pine.LNX.4.61.0601241032020.10099@gannet.stats>
	<x2mzhl6g2b.fsf@viggo.kubism.ku.dk>
Message-ID: <edc66abd0601271209u5642c8b1hb3fab9b5b0108777@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/23853480/attachment.pl

From rkrishnan8216 at yahoo.com  Fri Jan 27 21:13:30 2006
From: rkrishnan8216 at yahoo.com (Krish Krishnan)
Date: Fri, 27 Jan 2006 12:13:30 -0800 (PST)
Subject: [R] Factor Analysis
Message-ID: <20060127201330.92214.qmail@web60911.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/1fc84078/attachment.pl

From luk111111 at yahoo.com  Fri Jan 27 21:21:13 2006
From: luk111111 at yahoo.com (luk)
Date: Fri, 27 Jan 2006 12:21:13 -0800 (PST)
Subject: [R] save trained randomForest model
Message-ID: <20060127202113.33364.qmail@web30909.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060127/84ae1031/attachment.pl

From andy_liaw at merck.com  Fri Jan 27 21:27:26 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 27 Jan 2006 15:27:26 -0500
Subject: [R] save trained randomForest model
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED77D@usctmx1106.merck.com>

Use save().

Andy

From: luk
> 
> I used the following command to train a randomForest model
>   train.rf <- randomForest(grp ~ ., data=tr, ntree=100, mtry=50)
>    
>   My question is how to save the trained model so that it can 
> be loaded later for testing new samples?
>    
>   Thanks,
>    
>   Luk
> 
> 		
> ---------------------------------
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Jan 27 21:48:58 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 27 Jan 2006 15:48:58 -0500
Subject: [R] Classifying Intertwined Spirals
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED77F@usctmx1106.merck.com>

You don't really expect SVM to give you good performance with no parameter
tuning at all, do you? 

Try:

m2 <- best.svm(class~., data=spiral, gamma=2^(-3:3), cost=2^(0:5))
plot(m2, spiral)

Andy

From: Joshua Gilbert
> 
> I'm using an SVM as I've seen a paper that reported extremely good
> results. I'm not having such luck. I'm also interested in ideas for
> other approaches to the problem that can also be applied to general
> problems (no assuming that we're looking for spirals).
> 
> Here is my code:
> library(mlbench)
> library(e1071)
> raw <- mlbench.spirals(194, 2)
> spiral <- data.frame(class=as.factor(raw$classes), 
> xx=raw$x[,1], y=raw$x[,2])
> m <- svm(class~., data=spiral)
> plot(m, spiral)
> 
> You'll note that I have two spirals with 97 points each and I'm using
> a kernel with a radial basis: exp(-gamma*|u-v|^2).
> 
> You should be able to see a PNG of the resulting plot here:
> http://www.flickr.com/photos/60118409 at N00/91835679/
> 
> The problem is that that's not good enough. I want a better fit. I
> think I can get one, I just don't know how.
> 
> There's a paper on Proximal SVMs that claims a better result. To the
> best of my knowledge, PSVMs should not outperform SVMs, they are
> merely faster to compute. You can find the paper (with the picture of
> their SVM) on citeseer:
> http://citeseer.ifi.unizh.ch/cachedpage/515368/5
> @misc{ fung-proximal,
>   author = "G. Fung and O. Mangasarian",
>   title = "Proximal support vector machine classifiers",
>   text = "G. Fung and O. Mangasarian. Proximal support vector machine
> classifiers.
>     In F. P. D. Lee and R. Srikant, editors, KDD",
>   url = "citeseer.ifi.unizh.ch/515368.html" }
> 
> I don't have much of a background in SVMs, I'm learning as I go, so
> please don't hold back 'simple-minded' suggestions.
> 
> I'm also asking the authors, but I'm not expecting a reply from them.
> 
> There was a paper by Lang and Whitbrock in 1988 (Learning to Tell Two
> Spirals Apart) that solved the problem with a neural network, but they
> used a very specialized network architecture. I would say that
> discovering such an architecture and then optimizing it would be very
> time-intensive.
> 
> Thank you for any response.
> 
> Josh.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Jan 27 21:50:01 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jan 2006 20:50:01 +0000 (GMT)
Subject: [R] Factor Analysis
In-Reply-To: <20060127201330.92214.qmail@web60911.mail.yahoo.com>
References: <20060127201330.92214.qmail@web60911.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0601272044520.29630@gannet.stats>

On Fri, 27 Jan 2006, Krish Krishnan wrote:

> I am very new to factor analysis as well as R.  I am trying to run a 
> factor analysis on the residual returns on common stock (residual to 
> some model) and trying to determine if there are any strong factors 
> remaining.  After running factanal, I can obtain the factor loadings but 
> how do I get the values of the factor returns themselves?  In other 
> words if the relationship is
>
> r = lambda * f
>
> I prrovide r, factanal estimates the lambdas (factor loadings).  But how do I get the
> f values? Am I looking at this too much from a multivariate regression angle?

Please consult the help page.  The model has an error term.  I don't know
what you mean by 'r': are these correlations or 'x'?

I think you are asking for the scores, which factanal optionally provides.

It is not clear from your description if PCA would not be more 
appropriate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ruser2006 at yahoo.com  Fri Jan 27 21:57:07 2006
From: ruser2006 at yahoo.com (r user)
Date: Fri, 27 Jan 2006 12:57:07 -0800 (PST)
Subject: [R] "Conditional" match?
Message-ID: <20060127205707.29521.qmail@web37004.mail.mud.yahoo.com>

I have two datasets, big and small.

s_date<-c(?2005-12-02?, ?2005-12-01?,
?2004-11-02?,?2002-10-05?,?2000-12-15?)
s_id<-c(?a?,?a?,?b?,?c?,?d?)

b_date<- c(?2005-12-31?, ?2005-12-31?,
?2004-12-31?,?2002-10-05?,?2001-10-31?,?1999-12-31?)

b_id<-c(?a?,?b?,?c?,?d?,?e?,?c?)

small<-data.frame(date_=as.Date(s_date),id=s_id)
big<-data.frame(date_=as.Date(b_date),id=b_id)

For each row in ?big?, I want to look for a match in
small where two conditions are met:

a.	big$id=small$id
b.	big$date_>=small$date

If  match is found, I wish to return the value of the
date.  If no match is found, I want NA.  

If more than 1 match is found, I wish to return the
match where small$date is greatest.

I?m thinking I might be able to do this using the
match function, and by sorting the ?small? dataset by
date_ in descending order.  

However, I do not know how to make the match
conditional on big$date_>=small$date_.

Any help is appreciated.



From dylan.beaudette at gmail.com  Fri Jan 27 22:09:03 2006
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Fri, 27 Jan 2006 13:09:03 -0800
Subject: [R] understanding patterns in categorical vs. continuous data
In-Reply-To: <971536df0601261203q3ef21112tba0f66d4354617d1@mail.gmail.com>
References: <200601261111.00692.dylan.beaudette@gmail.com>
	<971536df0601261203q3ef21112tba0f66d4354617d1@mail.gmail.com>
Message-ID: <200601271309.03948.dylan.beaudette@gmail.com>

Thanks to all for the helpful suggestions, I was able to get good start from 
there.

Cheers,

Dylan

On Thursday 26 January 2006 12:03 pm, Gabor Grothendieck wrote:
> Would this do?
>
> boxplot(Sepal.Length ~ Species, iris, horizontal = TRUE)
> library(Hmisc)
> summary(Sepal.Length ~ Species, iris, fun = summary)
>
> On 1/26/06, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> > Greetings,
> >
> > I have a set of bivariate data: one variable (vegetation type) which is
> > categorical, and one (computed annual insolation) which is continuous.
> > Plotting veg_type ~ insolation produces a nice overview of the patterns
> > that I can see in the source data. However, due to the large number of
> > samples (1,000), and the apparent "spread" in the distribution of a
> > single vegetation type over a range of insolation values- I having a hard
> > time quantitatively describing the relationship between the two
> > variables.
> >
> > Here is a link to a sample graph:
> > http://casoilresource.lawr.ucdavis.edu/drupal/node/162
> >
> > Since the data along each vegetation type "line" is not a distribution in
> > the traditional sense, I am having problems applying descriptive
> > statistical methods. Conceptually, I would like to some how describe the
> > variation with insolation, along each vegetation type "line".
> >
> > Any guidance, or suggested reading material would be greatly appreciated.
> >
> >
> > --
> > Dylan Beaudette
> > Soils and Biogeochemistry Graduate Group
> > University of California at Davis
> > 530.754.7341
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341



From webmaster at actu-cv.com  Fri Jan 27 22:01:01 2006
From: webmaster at actu-cv.com (webmaster@actu-cv.com)
Date: 27 Jan 2006 21:01:01 -0000
Subject: [R] Refused Mail: Mail En Retour:
Message-ID: <20060127210101.3344.qmail@ns30586.ovh.net>

-------------------------------------------------------------------------------
Ce message a ete cree automatiquement par MailContentFilter v0.2

Le message que vous avez envoye n'a pu etre delivre a tous ses destinataires
Ceci est du a un parametrage permanent du mail de destination

L'adresse suivante a refuse votre envoi: webmaster at actu-cv.com
Raison: Cette adresse n'accepte pas ce type de PIECE-JOINTE

Le sujet de votre message etait: Notice again

Merci de reemettre votre message, mais cette fois sans pieces-jointes

-------------------------------------------------------------------------------
This message was created automatically by MailContentFilter v0.2 software

A message that you sent could not be delivered to one or more of its recipients
This is a permanent error

The following address refused your post: webmaster at actu-cv.com
Reason: This address does not accept your ATTACHMENT type

The subject of your message was: Notice again

Please try again to send your message, but without any attachment



From charles.edwin.white at us.army.mil  Fri Jan 27 22:25:00 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Fri, 27 Jan 2006 16:25:00 -0500
Subject: [R] lme4_0.995-2/Matrix_0.995-4 upgrade introduces error messages
	(change management)
Message-ID: <8BAEC5E546879B4FAA536200A292C614F7EC3A@AMEDMLNARMC135.amed.ds.army.mil>

I'll address two issues. The first is today's error message and the other is change management for contributed packages on CRAN.

TODAY'S ERROR MESSAGE

I switched from the 0.995-1 versions of lme4 and Matrix to those referenced in the subject line this afternoon. Prior to using these packages on anything else, I applied them to code that 'worked' (provided numerical results with no error messages) under the previous set of packages. Since I can't provide the data, I realize this post may be of limited usefulness. Rightly or wrongly, I've regressed my R installation back to the 0.995-1 versions of lme4/Matrix... so I don't think that I continue to have a problem. 

R version 2.2.1, 2005-12-20, i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
[7] "base"     

other attached packages:
     lme4   lattice    Matrix 
"0.995-2" "0.12-11" "0.995-4"

> options(show.signif.stars=FALSE)
> m1a<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
+ family=binomial(link='probit'), method='Laplace')
Error in dev.resids(y, mu, weights) : argument wt must be a numeric vector of length 1 or length 219
> # probit doesn't converge
> m1b<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
+ family=binomial, method='Laplace')
Error in dev.resids(y, mu, weights) : argument wt must be a numeric vector of length 1 or length 219
> # logit is overdispersed
> m1<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
+ family=quasibinomial, method='Laplace')
Error in glm.fit(X, Y, weights = weights, offset = offset, family = family,  : 
	NAs in V(mu)
> m2<-lmer(cbind(prevented,control.count)~hour+(1|volunteer)+(1|date),
+ family=quasibinomial, method='Laplace')
Error in glm.fit(X, Y, weights = weights, offset = offset, family = family,  : 
	NAs in V(mu)

CHANGE MANAGEMENT

Does CRAN keep old versions of contributed packages someplace? If not, the strategy I've implemented today is to maintain my own repository of contributed packages that I use. Stuff happens and change management is good.

Chuck

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
301 319-9781
Personal/Professional Site:??
http://users.starpower.net/cwhite571/professional/



From ligges at statistik.uni-dortmund.de  Fri Jan 27 22:32:57 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 Jan 2006 22:32:57 +0100
Subject: [R] lme4_0.995-2/Matrix_0.995-4 upgrade introduces error
 messages (change management)
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614F7EC3A@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614F7EC3A@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <43DA9189.9060905@statistik.uni-dortmund.de>

White, Charles E WRAIR-Wash DC wrote:
> I'll address two issues. The first is today's error message and the other is change management for contributed packages on CRAN.
> 
> TODAY'S ERROR MESSAGE
> 
> I switched from the 0.995-1 versions of lme4 and Matrix to those referenced in the subject line this afternoon. Prior to using these packages on anything else, I applied them to code that 'worked' (provided numerical results with no error messages) under the previous set of packages. Since I can't provide the data, I realize this post may be of limited usefulness. Rightly or wrongly, I've regressed my R installation back to the 0.995-1 versions of lme4/Matrix... so I don't think that I continue to have a problem. 
> 
> R version 2.2.1, 2005-12-20, i386-pc-mingw32 
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets" 
> [7] "base"     
> 
> other attached packages:
>      lme4   lattice    Matrix 
> "0.995-2" "0.12-11" "0.995-4"
> 
> 
>>options(show.signif.stars=FALSE)
>>m1a<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
> 
> + family=binomial(link='probit'), method='Laplace')
> Error in dev.resids(y, mu, weights) : argument wt must be a numeric vector of length 1 or length 219
> 
>># probit doesn't converge
>>m1b<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
> 
> + family=binomial, method='Laplace')
> Error in dev.resids(y, mu, weights) : argument wt must be a numeric vector of length 1 or length 219
> 
>># logit is overdispersed
>>m1<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|date),
> 
> + family=quasibinomial, method='Laplace')
> Error in glm.fit(X, Y, weights = weights, offset = offset, family = family,  : 
> 	NAs in V(mu)
> 
>>m2<-lmer(cbind(prevented,control.count)~hour+(1|volunteer)+(1|date),
> 
> + family=quasibinomial, method='Laplace')
> Error in glm.fit(X, Y, weights = weights, offset = offset, family = family,  : 
> 	NAs in V(mu)
> 
> CHANGE MANAGEMENT
> 
> Does CRAN keep old versions of contributed packages someplace? If not, the strategy I've implemented today is to maintain my own repository of contributed packages that I use. Stuff happens and change management is good.


Yes, old packages are in
   CRAN/src/contrib/Archive/
You have to compile them from source yourself, though.

Uwe Ligges


> Chuck
> 
> Charles E. White, Senior Biostatistician, MS
> Walter Reed Army Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> 301 319-9781
> Personal/Professional Site: 
> http://users.starpower.net/cwhite571/professional/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From joshuacgilbert at gmail.com  Fri Jan 27 22:38:58 2006
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Fri, 27 Jan 2006 16:38:58 -0500
Subject: [R] Classifying Intertwined Spirals
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED77F@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED77F@usctmx1106.merck.com>
Message-ID: <ef96daa30601271338n3cb2055eifd0f335889553cc7@mail.gmail.com>

When armed with barely enough information to be dangerous you would be
amazed at what I am willing to believe.

Thank you for pointing out how foolish I was.


Josh.


On 1/27/06, Liaw, Andy <andy_liaw at merck.com> wrote:
> You don't really expect SVM to give you good performance with no parameter
> tuning at all, do you?
>
> Try:
>
> m2 <- best.svm(class~., data=spiral, gamma=2^(-3:3), cost=2^(0:5))
> plot(m2, spiral)
>
> Andy
>
> From: Joshua Gilbert
> >
> > I'm using an SVM as I've seen a paper that reported extremely good
> > results. I'm not having such luck. I'm also interested in ideas for
> > other approaches to the problem that can also be applied to general
> > problems (no assuming that we're looking for spirals).
> >
> > Here is my code:
> > library(mlbench)
> > library(e1071)
> > raw <- mlbench.spirals(194, 2)
> > spiral <- data.frame(class=as.factor(raw$classes),
> > xx=raw$x[,1], y=raw$x[,2])
> > m <- svm(class~., data=spiral)
> > plot(m, spiral)
> >
> > You'll note that I have two spirals with 97 points each and I'm using
> > a kernel with a radial basis: exp(-gamma*|u-v|^2).
> >
> > You should be able to see a PNG of the resulting plot here:
> > http://www.flickr.com/photos/60118409 at N00/91835679/
> >
> > The problem is that that's not good enough. I want a better fit. I
> > think I can get one, I just don't know how.
> >
> > There's a paper on Proximal SVMs that claims a better result. To the
> > best of my knowledge, PSVMs should not outperform SVMs, they are
> > merely faster to compute. You can find the paper (with the picture of
> > their SVM) on citeseer:
> > http://citeseer.ifi.unizh.ch/cachedpage/515368/5
> > @misc{ fung-proximal,
> >   author = "G. Fung and O. Mangasarian",
> >   title = "Proximal support vector machine classifiers",
> >   text = "G. Fung and O. Mangasarian. Proximal support vector machine
> > classifiers.
> >     In F. P. D. Lee and R. Srikant, editors, KDD",
> >   url = "citeseer.ifi.unizh.ch/515368.html" }
> >
> > I don't have much of a background in SVMs, I'm learning as I go, so
> > please don't hold back 'simple-minded' suggestions.
> >
> > I'm also asking the authors, but I'm not expecting a reply from them.
> >
> > There was a paper by Lang and Whitbrock in 1988 (Learning to Tell Two
> > Spirals Apart) that solved the problem with a neural network, but they
> > used a very specialized network architecture. I would say that
> > discovering such an architecture and then optimizing it would be very
> > time-intensive.
> >
> > Thank you for any response.
> >
> > Josh.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From marcodoc75 at yahoo.com  Fri Jan 27 22:47:03 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Fri, 27 Jan 2006 13:47:03 -0800 (PST)
Subject: [R] avoiding warning messages on the screen with 'lme'
Message-ID: <20060127214703.22996.qmail@web31302.mail.mud.yahoo.com>

Dear all,
does anyone know a command to shut 'lme' (nlme) up? :)

I have a loop

for(i in 1:M){
lme(..)
}

and for each "i" i get the warning message
>Fewer observations than random effects in all level 1
groups >in: ...

I know I'm using "fewer observations...", I just don't
want to see the message printed on the screen during
the loop.

Thanks,
Marco Geraci


> sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices"
"utils"     "datasets" 
[7] "base"     

other attached packages:
    nlme 
"3.1-66" 
>



From p.dalgaard at biostat.ku.dk  Fri Jan 27 23:08:28 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jan 2006 23:08:28 +0100
Subject: [R] how calculation degrees freedom
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
Message-ID: <x2acdh1fz7.fsf@turmalin.kubism.ku.dk>

S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> Along similar lines, I've noticed that the anova() function for lmer
> models now only reports the mean squares to go into the numerator
> but "nothing for the denominator" of an F-statistic; probably in
> recognition of the degree of freedom problem. It could be nice,
> however, if anova() produced even an approximate anova table which
> can be obtained from Wald tests. The anova function could then print
> that "these p-values are large sample ones and hence only
> approximate"...

I'm reasonably convinced by now that the relevant denominator is
always the residual variance, but it is happening via deep magic that
I don't quite understand... (and is quite counterintuitive to people
who are used to the traditional ANOVA decompositions in orthogonal
designs)

While we're on the subject: It would be desirable to have Wald tests
for specific terms rather than the "Type 1" (sorry, Bill) progressive
ANOVA table. Just like we already have in lme().



-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From goedman at mac.com  Fri Jan 27 23:19:42 2006
From: goedman at mac.com (Rob J Goedman)
Date: Fri, 27 Jan 2006 14:19:42 -0800
Subject: [R] Previously compilation procedure on Mac OS X no longer works
In-Reply-To: <Pine.LNX.4.44.0601271624530.3657-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0601271624530.3657-100000@reclus.nhh.no>
Message-ID: <8BE28BBA-A655-4B36-A0A9-2D32D8F43468@mac.com>

Isaac,

On Mac OS 10. 4.4 with Xcode 2.2 installed, after setting:

sudo gcc_select 3.3, I get:

Robs-Laptop:~ rob$ gcc_select
Current default compiler:
gcc version 3.3 20030304 (Apple Computer, Inc. build 1819)

Robs-Laptop:~ rob$ cd Downloads/
Robs-Laptop:~/Downloads rob$ R CMD INSTALL sigPathway_1.1-0.tar.gz

gzip: sigPathway_1.1-0.tar.gz: decompression OK, trailing garbage  
ignored
* Installing *source* package 'sigPathway' ...
** libs
gcc-3.3 -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/ 
include  -I/usr/local/include   -fno-common  -g -O2 -c sigPathway.c - 
o sigPathway.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib - 
o sigPathway.so sigPathway.o  -framework vecLib -lcc_dynamic -F/ 
Library/Frameworks/R.framework/.. -framework R
ld: warning multiple definitions of symbol _xerbla_
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/ 
vecLib.framework/Versions/A/libBLAS.dylib(single module) definition  
of _xerbla_
/Library/Frameworks/R.framework/../R.framework/R(print.lo) definition  
of _xerbla_
ld: warning multiple definitions of symbol _signgam
/Library/Frameworks/R.framework/../R.framework/R(lgamma.lo)  
definition of _signgam
/usr/lib/libSystem.dylib(gamma9.o) definition of _signgam
** R
** data
** help
 >>> Building/Updating help pages for package 'sigPathway'
      Formats: text html latex example
   GenesetsU133a                     text    html    latex
   MuscleData                        text    html    latex
   calcTNullFast                     text    html    latex
   calcTStatFast                     text    html    latex   example
   calculate.GSEA                    text    html    latex
   calculatePathwayStatistics        text    html    latex   example
   getPathwayStatistics              text    html    latex
   rankPathways                      text    html    latex
   runSigPathway                     text    html    latex   example
   selectGeneSets                    text    html    latex
** building package indices ...
* DONE (sigPathway)

and

 > library(sigPathway)
 > example(runSigPathway)

rnSgPt> data(MuscleData)

rnSgPt> sf <- apply(MuscleData, 2, mean, tr = 0.025)

... lots more output, seems to work.

R-Sig-Mac might be a better list to ask specific Mac questions.

Rob


> On Fri, 27 Jan 2006, Isaac Kohane wrote:
>
>> Hi,
>>
>> 	I was a happy user of Peter Parks' package (see http://www.chip.org/
>> ~ppark/Supplements/PNAS05/) and could compile it without error under
>> Mac OS X 10.4. I then had a disk crash and had to re-install the
>> developer tools and now I get hideous messages such as  the one
>> below. I have tried installing an earlier version of Mac OS X
>> developer tools. No joy, Could anyone put me out of my misery and
>> compile  this code and post the binaries for mac os X so that I can
>> just get back to work? Please? Or is there a surefire way to get the
>> compilation process working again?
>
> One source of information is:
>
> http://wiki.urbanek.info/index.cgi?HomePage
>
> and the R-sig-Mac list
>
>>
>> Thanks in advance.
>>
>>
>> -Zak
>>
>>
>>
>> ld: warning -L: directory name (/usr/local/lib) does not exist
>> ld: can't locate file for: -lcc_dynamic
>> make: *** [sigPathway.so] Error 1
>> ERROR: compilation failed for package 'sigPathway'
>> ** Removing '/Library/Frameworks/R.framework/Versions/2.2/Resources/
>> library/sigPathway'
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian  
> School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html



From andy_liaw at merck.com  Fri Jan 27 23:20:45 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 27 Jan 2006 17:20:45 -0500
Subject: [R] avoiding warning messages on the screen with 'lme'
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED780@usctmx1106.merck.com>

See the argument `warn' in ?options.

Andy

From: Marco Geraci
> 
> Dear all,
> does anyone know a command to shut 'lme' (nlme) up? :)
> 
> I have a loop
> 
> for(i in 1:M){
> lme(..)
> }
> 
> and for each "i" i get the warning message
> >Fewer observations than random effects in all level 1
> groups >in: ...
> 
> I know I'm using "fewer observations...", I just don't
> want to see the message printed on the screen during
> the loop.
> 
> Thanks,
> Marco Geraci
> 
> 
> > sessionInfo()
> R version 2.2.1, 2005-12-20, i386-pc-mingw32 
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices"
> "utils"     "datasets" 
> [7] "base"     
> 
> other attached packages:
>     nlme 
> "3.1-66" 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From eliotmcintire at gmail.com  Fri Jan 27 23:25:20 2006
From: eliotmcintire at gmail.com (Eliot McIntire)
Date: Fri, 27 Jan 2006 15:25:20 -0700
Subject: [R] Partial Canonical Correlation
Message-ID: <e6cccb9b0601271425w4c775dbctb9aa3ed52b2c853b@mail.gmail.com>

Hello,

I am interested in doing a partial canonical correlation (identical to
the SAS function, Proc Cancorr with the Partial statement).

By this I mean, I have 3 sets of data, a vegetation matrix (columns of
abundances of species in rows of plots), an "environment" matrix
(columns of environmental variables in same rows of plots), and a
"space" matrix (x and y locations of each of the plots).  I would like
to look at the relationships between these multivariate groups, with
the space partialed out.

This would be somewhat analogous in my mind to a partial mantel test
(i.e., 3 data matrices), but would be somewhat more sophisticated than
the correlations between distance matrices of the partial mantel test.

In the r-help archives I have found similar things, partial
correlations, canonical correlations, but not partial canonical
correlations.

Does somebody know how to do this in R?

Thank you in advance,

Eliot



--
Eliot McIntire
Post Doctoral Fellow
Department of Ecosystems and Conservation Science
College of Forestry and Conservation
University of Montana, Missoula, MT 59812
406-243-5239
fax: 406-243-4557
emcintire at forestry.umt.edu



From dmbates at gmail.com  Sat Jan 28 00:40:40 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 27 Jan 2006 17:40:40 -0600
Subject: [R] how calculation degrees freedom
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
Message-ID: <40e66e0b0601271540l541c881dk126eb4a04bd04f4d@mail.gmail.com>

On 1/27/06, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Degrees of freedom for mixed models is a delicate issue - except in certain orthogonal designs.
>
> However, I'll just point out that for lmer models, there is a simulate() function which can simulate data from a fitted model. simulate() is very fast - just like lmer(). So one way to "get around the problem" could be to evaluate the test statistic (e.g. -2 log Q) in an empirical distribution based on simulations under the model; that is to calculate a Monte Carlo p-value. It is fairly fast to and takes about 10 lines of code to program.
>
> Of course, Monte Carlo p-values have their problems, but the world is not perfect....

Another approach is to use mcmcsamp to derive a sample from the
posterior distribution of the parameters using Markov Chain Monte
Carlo sampling.  If you are interested in intervals rather than
p-values the HPDinterval function from the coda package can create
those.

> Along similar lines, I've noticed that the anova() function for lmer models now only reports the mean squares to go into the numerator but "nothing for the denominator" of an F-statistic; probably in recognition of the degree of freedom problem. It could be nice, however, if anova() produced even an approximate anova table which can be obtained from Wald tests. The anova function could then print that "these p-values are large sample ones and hence only approximate"...

I don't think the "degrees of freedom police" would find that to be a
suitable compromise. :-)

>
> Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Douglas Bates
> Sendt: fr 27-01-2006 17:06
> Til: gabriela escati pe??aloza
> Cc: R-help at stat.math.ethz.ch
> Emne: Re: [R] how calculation degrees freedom
>
>
>
> On 1/27/06, gabriela escati pe??aloza <gescati at yahoo.com.ar> wrote:
> > Hi, I' m having a hard time understanding the computation of degrees of freedom
>
> So do I and I'm one of the authors of the package :-)
>
> > when runing nlme() on the following model:
> >
> >   > formula(my data.gd)
> > dLt ~ Lt | ID
> >
> >   TasavB<- function(Lt, Linf, K) (K*(Linf-Lt))
> >
> >   my model.nlme <- nlme (dLt ~ TasavB(Lt, Linf, K),
> >   data = my data.gd,
> >   fixed = list(Linf ~ 1, K ~ 1),
> >   start = list(fixed = c(70, 0.4)),
> >   na.action= na.include, naPattern = ~!is.na(dLt))
> >
> >   > summary(my model.nlme)
> > Nonlinear mixed-effects model fit by maximum likelihood
> >   Model: dLt ~ TasavB(Lt, Linf, K)
> >  Data: my data.gd
> >        AIC      BIC    logLik
> >   13015.63 13051.57 -6501.814
> >   Random effects:
> >  Formula: list(Linf ~ 1 , K ~ 1 )
> >  Level: ID
> >  Structure: General positive-definite
> >             StdDev   Corr
> >     Linf 7.3625291 Linf
> >        K 0.0845886 -0.656
> > Residual 1.6967358
> >   Fixed effects: list(Linf + K ~ 1)
> >         Value Std.Error   DF  t-value p-value
> > Linf 69.32748 0.4187314  402 165.5655  <.0001
> >    K  0.31424 0.0047690 2549  65.8917  <.0001
> >   Standardized Within-Group Residuals:
> >       Min         Q1         Med        Q3      Max
> >  -3.98674 -0.5338083 -0.02783649 0.5261591 4.750609
> >   Number of Observations: 2952
> > Number of Groups: 403
> > >
> >
> >   Why are the DF of Linf and K different? I would apreciate if you could point me to a reference
>
> The algorithm is described in Pinheiro and Bates (2000) "Mixed-effects
> Models in S and S-PLUS" published by Springer.  See section 2.4.2
>
> I would point out that there is effectively no difference between a
> t-distribution with 402 df and a t-distribution with 2549 df so the
> actual number of degrees of freedom is irrelevant in this case.  All
> you need to know is that it is "large".
>
> I will defer to any of the "degrees of freedom police" who post to
> this list to give you an explanation of why there should be different
> degrees of freedom.  I have been studying mixed-effects models for
> nearly 15 years and I still don't understand.
>
> >   Note: I working with Splus 6.1. for Windows
>
> Technically this email list is for questions about R.  There is
> another list, s-news at biostat.wustl.edu, for questions about S-PLUS.
>
> >
> >
> > Lic. Gabriela Escati Pe??aloza
> > Biolog??a y Manejo de Recursos Acu??ticos
> > Centro Nacional Patag??nico(CENPAT).
> > CONICET
> > Bvd. Brown s/n??.
> > (U9120ACV)Pto. Madryn
> > Chubut
> > Argentina
> >
> > Tel: 54-2965/451301/451024/451375/45401 (Int:277)
> > Fax: 54-29657451543
> >
> > ---------------------------------
> >  1GB gratis, Antivirus y Antispam
> >  Correo Yahoo!, el mejor correo web del mundo
> >  Abr?? tu cuenta aqu??
> >         [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>



From dmbates at gmail.com  Sat Jan 28 00:52:45 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 27 Jan 2006 17:52:45 -0600
Subject: [R] how calculation degrees freedom
In-Reply-To: <x2acdh1fz7.fsf@turmalin.kubism.ku.dk>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
	<x2acdh1fz7.fsf@turmalin.kubism.ku.dk>
Message-ID: <40e66e0b0601271552x7e354188j8d7f6a7def769e14@mail.gmail.com>

On 27 Jan 2006 23:08:28 +0100, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> writes:
>
> > Along similar lines, I've noticed that the anova() function for lmer
> > models now only reports the mean squares to go into the numerator
> > but "nothing for the denominator" of an F-statistic; probably in
> > recognition of the degree of freedom problem. It could be nice,
> > however, if anova() produced even an approximate anova table which
> > can be obtained from Wald tests. The anova function could then print
> > that "these p-values are large sample ones and hence only
> > approximate"...
>
> I'm reasonably convinced by now that the relevant denominator is
> always the residual variance, but it is happening via deep magic that
> I don't quite understand... (and is quite counterintuitive to people
> who are used to the traditional ANOVA decompositions in orthogonal
> designs)

Not deep magic for you, Peter.  The slot called rXy in the fitted
model is analogous to the first p components of the "effects"
component in an lm model.  Cut it up according to the terms and sum
the squares.

> While we're on the subject: It would be desirable to have Wald tests
> for specific terms rather than the "Type 1" (sorry, Bill) progressive
> ANOVA table. Just like we already have in lme().

I think this is the point where I mention the Open Source nature of
project.  Sorry to say that it is not a priority for me right now.



From p.dalgaard at biostat.ku.dk  Sat Jan 28 01:12:59 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jan 2006 01:12:59 +0100
Subject: [R] how calculation degrees freedom
In-Reply-To: <40e66e0b0601271540l541c881dk126eb4a04bd04f4d@mail.gmail.com>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
	<40e66e0b0601271540l541c881dk126eb4a04bd04f4d@mail.gmail.com>
Message-ID: <x2oe1xfbw4.fsf@turmalin.kubism.ku.dk>

Douglas Bates <dmbates at gmail.com> writes:


> > Of course, Monte Carlo p-values have their problems, but the world
> > is not perfect....
> 
> Another approach is to use mcmcsamp to derive a sample from the
> posterior distribution of the parameters using Markov Chain Monte
> Carlo sampling.  If you are interested in intervals rather than
> p-values the HPDinterval function from the coda package can create
> those.
> 

We (S??ren and I) actually had a look at that, and it seems not to
solve the problem. Rather, mcmcsamp tends to reproduce the Wald style
inference (infinite DF) if you use a suitably vague prior.

It's a bit hard to understand clearly, but I think the crux is that
any Bayes inference only depends on data through the likelihood
function. The distribution of the likelihood never enters (the
hardcore Bayesian of course won't care). However, the nature of DF
corrections is that the LRT does not have its asymptotic distribution,
and mcmc has no way of picking that up.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ajroyal at ucalgary.ca  Sat Jan 28 01:27:39 2006
From: ajroyal at ucalgary.ca (Andrew)
Date: Fri, 27 Jan 2006 17:27:39 -0700
Subject: [R] Complex Matrix Exponentials.
Message-ID: <43DABA7B.2060200@ucalgary.ca>

Hello,

I was curious if there was a complex valued matrix exponential function 
available for R?  I have some Laplace transforms  of occupation times 
for a hidden Markov model.  The matrix exponential function in the msm 
package does not seem to handle complex values.  For example

 > MatrixExp(diag(1i,2))
     [,1] [,2]
[1,]    1    0
[2,]    0    1
Warning message:
imaginary parts discarded in coercion

Thanks in advance for your help,

Andrew Royal
University of Calgary



From p.dalgaard at biostat.ku.dk  Sat Jan 28 01:33:46 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jan 2006 01:33:46 +0100
Subject: [R] how calculation degrees freedom
In-Reply-To: <40e66e0b0601271552x7e354188j8d7f6a7def769e14@mail.gmail.com>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
	<x2acdh1fz7.fsf@turmalin.kubism.ku.dk>
	<40e66e0b0601271552x7e354188j8d7f6a7def769e14@mail.gmail.com>
Message-ID: <x2k6clfaxh.fsf@turmalin.kubism.ku.dk>

Douglas Bates <dmbates at gmail.com> writes:

> > I'm reasonably convinced by now that the relevant denominator is
> > always the residual variance, but it is happening via deep magic that
> > I don't quite understand... (and is quite counterintuitive to people
> > who are used to the traditional ANOVA decompositions in orthogonal
> > designs)
> 
> Not deep magic for you, Peter.  The slot called rXy in the fitted
> model is analogous to the first p components of the "effects"
> component in an lm model.  Cut it up according to the terms and sum
> the squares.

Yes, that's the bit I do understand. The magic bit is that at that
point, you have removed the effect of the Z, in a penalized fashion,
and somehow this manages to give the right thing.
 
Consider a balanced design with a number of observations for each
subject, and subjects divided into groups, subject effects considered
random. In traditional aov, you end up dividing the "group" SS by the
"subject" SS. However in lmer, you remove the subject effect (in a
BLUP sort of way) from everything and this gets the "group" SS
adjusted so that it matches the residual SS instead. I'm quite
prepared to believe it; I just don't think it is trivial.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From res90sx5 at verizon.net  Sat Jan 28 03:09:02 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Fri, 27 Jan 2006 18:09:02 -0800
Subject: [R] installation of R on Linux
Message-ID: <00e901c623af$cf5f8330$6401a8c0@main>

R-users,

I am new user of Linux (have been using Win XP Pro) and wanted to install R.  Since I am just beginning to learn Linux I was wondering, where in the directory structure do users of Linux usually install R?  Most of the instructions I have read simply say to untar the tarball where you want to install the program.  Any suggestions would be welcome as to an appropriate place.  I know I could get an rpm, but wanted to use this as a learning process for a variety of skills.  Currently working with SuSE 9.1

Thanks,

Dan Nordlund
Bothell, WA



From mmorales at williams.edu  Sat Jan 28 03:07:48 2006
From: mmorales at williams.edu (Manuel Morales)
Date: Fri, 27 Jan 2006 21:07:48 -0500
Subject: [R] Nesting Functions
In-Reply-To: <43D98B89.1010905@stats.uwo.ca>
References: <1138329906.2610.6.camel@localhost.localdomain>
	<43D98B89.1010905@stats.uwo.ca>
Message-ID: <1138414068.2877.7.camel@localhost.localdomain>

On Thu, 2006-01-26 at 21:55 -0500, Duncan Murdoch wrote:
> On 1/26/2006 9:45 PM, Manuel Morales wrote:
> > Dear list members,
> > 
> > I'm looking for a way to write "nested" functions similar to the
> > function "Nest" or "NestList" in Mathematica.
> > 
> > E.g.,
> > 
> > f<-function(x) x+2*x
> > 
> > f(f(f(2)))
> > 
> > might instead be written as nest(f, 2, 3)
> > 
> > read as, nest function f 3 times with 2 as the initial value.
> 
> It's easy enough using a for loop:
> 
> nest <- function(f, initial, reps) {
>     result <- initial
>     for (i in seq(len=reps)) result <- f(result)
>     result
> }
> 
> Duncan Murdoch
That works, thanks! But what if I want to apply the function to a set of
vectors.

init.values<-c(3,10,20)
rep.values<-c(0,1,2)

nest(f,init.values,rep.values) fails because only the first value is
used in a for loop. The following works, but it's clunky and doesn't
scale with variation in the number of reps.

nest.vectorize<-function(f, initial, reps)
    ifelse(reps==0,initial,
    ifelse(reps==1,f(initial),f(f(initial))))

nest.vectorize(f,init.values,rep.values)

Any suggestions?

Thanks again,

Manuel



From spencer.graves at pdf.com  Sat Jan 28 03:10:45 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 Jan 2006 18:10:45 -0800
Subject: [R] draft of Comment on UCLA tech report
In-Reply-To: <43DA1FBC.9070901@pburns.seanet.com>
References: <43DA1FBC.9070901@pburns.seanet.com>
Message-ID: <43DAD2A5.7050207@pdf.com>

Dear Patrick:

	  Thanks for doing this.

	  Just picking nits:  To end the second paragraph in Sec. 2, you say, 
"If your goal is to find what is in your data, then almost surely R will 
be the best tool for you sooner or later."  I think this is an 
overstatement.  R is for people who can't easily get what they want from 
standard packages, either because they can't afford it or because they 
want to explore data analysis possibilities beyond those offered by the 
package(s) available to them.  You clearly paint a more balanced view in 
the rest of the manuscript, and apart from this one comment, what you 
wrote sounds sensible to me.

	  Best Wishes,
	  Spencer Graves

Patrick Burns wrote:

> You may  recall that there was a discussion of a technical
> report from the statistical consulting group at UCLA.
> 
> I have a draft of a comment on that report, which you
> can get from
> http://www.burns-stat.com/pages/Flotsam/uclaRcomment_draft1.pdf
> 
> I'm interested in comments: corrections, additions, deletions.
> 
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Sat Jan 28 04:29:43 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 27 Jan 2006 22:29:43 -0500
Subject: [R] Nesting Functions
In-Reply-To: <1138414068.2877.7.camel@localhost.localdomain>
References: <1138329906.2610.6.camel@localhost.localdomain>	<43D98B89.1010905@stats.uwo.ca>
	<1138414068.2877.7.camel@localhost.localdomain>
Message-ID: <43DAE527.5030609@stats.uwo.ca>

On 1/27/2006 9:07 PM, Manuel Morales wrote:
> On Thu, 2006-01-26 at 21:55 -0500, Duncan Murdoch wrote:
>> On 1/26/2006 9:45 PM, Manuel Morales wrote:
>>> Dear list members,
>>>
>>> I'm looking for a way to write "nested" functions similar to the
>>> function "Nest" or "NestList" in Mathematica.
>>>
>>> E.g.,
>>>
>>> f<-function(x) x+2*x
>>>
>>> f(f(f(2)))
>>>
>>> might instead be written as nest(f, 2, 3)
>>>
>>> read as, nest function f 3 times with 2 as the initial value.
>> It's easy enough using a for loop:
>>
>> nest <- function(f, initial, reps) {
>>     result <- initial
>>     for (i in seq(len=reps)) result <- f(result)
>>     result
>> }
>>
>> Duncan Murdoch
> That works, thanks! But what if I want to apply the function to a set of
> vectors.
> 
> init.values<-c(3,10,20)
> rep.values<-c(0,1,2)
> 
> nest(f,init.values,rep.values) fails because only the first value is
> used in a for loop. The following works, but it's clunky and doesn't
> scale with variation in the number of reps.
> 
> nest.vectorize<-function(f, initial, reps)
>     ifelse(reps==0,initial,
>     ifelse(reps==1,f(initial),f(f(initial))))
> 
> nest.vectorize(f,init.values,rep.values)
> 
> Any suggestions?

Something like this would work:

nest <- function(f, initial, reps) {
      result <- initial
      for (i in seq(len=max(reps))) result <- ifelse(i <= reps,
					            f(result), result)
      result
  }



From ggrothendieck at gmail.com  Sat Jan 28 07:20:30 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 28 Jan 2006 01:20:30 -0500
Subject: [R] "Conditional" match?
In-Reply-To: <20060127205707.29521.qmail@web37004.mail.mud.yahoo.com>
References: <20060127205707.29521.qmail@web37004.mail.mud.yahoo.com>
Message-ID: <971536df0601272220l136abf5ftd3a3c7492ab51915@mail.gmail.com>

Try this:

merg <- merge(big, small, by = "id")
f <- function(x) {
	x$date_.y <- max(x$date_.y)
	x[x$date_.y >= x$date_.x, "date_.y"] <- NA
	x
}
do.call("rbind", by(merg, merg$date_.x, f))


On 1/27/06, r user <ruser2006 at yahoo.com> wrote:
> I have two datasets, big and small.
>
> s_date<-c('2005-12-02', '2005-12-01',
> '2004-11-02','2002-10-05','2000-12-15')
> s_id<-c('a','a','b','c','d')
>
> b_date<- c('2005-12-31', '2005-12-31',
> '2004-12-31','2002-10-05','2001-10-31','1999-12-31')
>
> b_id<-c('a','b','c','d','e','c')
>
> small<-data.frame(date_=as.Date(s_date),id=s_id)
> big<-data.frame(date_=as.Date(b_date),id=b_id)
>
> For each row in "big", I want to look for a match in
> small where two conditions are met:
>
> a.      big$id=small$id
> b.      big$date_>=small$date
>
> If  match is found, I wish to return the value of the
> date.  If no match is found, I want NA.
>
> If more than 1 match is found, I wish to return the
> match where small$date is greatest.
>
> I'm thinking I might be able to do this using the
> match function, and by sorting the "small" dataset by
> date_ in descending order.
>
> However, I do not know how to make the match
> conditional on big$date_>=small$date_.
>
> Any help is appreciated.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Jan 28 07:23:59 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 Jan 2006 22:23:59 -0800
Subject: [R] weighted likelihood for lme
In-Reply-To: <20060123213535.39673.qmail@web31306.mail.mud.yahoo.com>
References: <20060123213535.39673.qmail@web31306.mail.mud.yahoo.com>
Message-ID: <43DB0DFF.1000806@pdf.com>

	  Thank you for providing such a marvelous example.  I wish I could 
reward your dilligence with a simple, complete answer.  Unfortunately, 
the best I can offer at the moment is a guess and a reference.  First, I 
believe you are correct in that the "weights" argument describes "the 
within-group heteroscedasticity structure".  To specify between-group 
heterscadisticity, have you considered the following:

  foo <- Orthodont
  foo$w <- c(rep(1, 5*4), rep(0.5, 22*4))
 > lme(distance ~ 1, random = ~ w-1|Subject,
+     method="ML", data = foo)
Linear mixed-effects model fit by maximum likelihood
   Data: foo
   Log-likelihood: -258.8586
   Fixed: distance ~ 1
(Intercept)
     23.8834

Random effects:
  Formula: ~w - 1 | Subject
                w Residual
StdDev: 3.370796 2.233126

Number of Observations: 108
Number of Groups: 27

	  Second, have you consulted Pinheiro and Bates (2000) Mixed-Effects 
Models in S and S-Plus (Springer)?  If you have not already, I encourage 
you to spend some quality time with that book.  For me, this book helped 
transform "lme" from an inaequately documented and unusable black box 
into a simple, understandable, elegant tool.  I may have recommeded it 
to more people than any other single work over the past five years.

	  hope this helps.
	  spencer graves

Marco Geraci wrote:

>     Dear R users,
>   I'm trying to fit a simple random intercept model with a fixed intercept. 
>     Suppose I want to assign a weight w_i to the i-th contribute to the log-likelihood, i.e.
>    
>   w_i * logLik_i
>    
>   where logLik_i is the log-likelihood for the i-th subject.
>   I want to maximize the likelihood for N subjects
>    
>   Sum_i  {w_i * logLik_i}
>    
>   Here is a simple example to reproduce
>    
>   # require(nlme)
>     > foo <- Orthodont
> 
>>lme(distance ~ 1, random = ~ 1|Subject, method="ML", data = foo)
> 
> Linear mixed-effects model fit by maximum likelihood
>   Data: foo 
>   Log-likelihood: -257.7456
>   Fixed: distance ~ 1 
> (Intercept) 
>    24.02315 
>   Random effects:
>  Formula: ~1 | Subject
>         (Intercept) Residual
> StdDev:    1.888748 2.220312
>   Number of Observations: 108
> Number of Groups: 27 
> 
> Then I assign arbitrary weights, constant within the group. I want to give weight 1 to the first 5 subjects, and weight 0.5 to the others 22 (4 is the number of repeated measurements for each subject)
> 
>    
>   > foo$w <- c(rep(1, 5*4), rep(0.5, 22*4))
>    
>   Maybe I am missing something, but I believe that
>    
>   > lme(distance ~ 1, random = ~ 1|Subject, method="ML", data = foo, weight=~w)
>    
>   does not maximize the likelihood Sum_i  {w_i * logLik_i}, 
since 'weight' describes the with-in heteroscedasticity structure.
>   I think I need something like the option 'iweight' 
(importance weight) for the command 'xtreg' of Stata.
>    
>   Any suggestion for R?
>    
>   Thanks in advance,
>    
>   Marco Geraci
>    
>   >  sessionInfo()
> R version 2.2.1, 2005-12-20, i386-pc-mingw32 
>   attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"    
> [6] "datasets"  "base"     
>   other attached packages:
>     nlme 
> "3.1-66" 
> 
> 
> 
> 
> 
> 		
> ---------------------------------
>  
>  What are the most popular cars?  Find out at Yahoo! Autos
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan.sarkar at gmail.com  Sat Jan 28 07:33:14 2006
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 28 Jan 2006 00:33:14 -0600
Subject: [R] about xyplot in lattice
In-Reply-To: <2BEE99D7F6F1484EBDD1D22167385E75152269@exqld1-ath.nexus.csiro.au>
References: <2BEE99D7F6F1484EBDD1D22167385E75152269@exqld1-ath.nexus.csiro.au>
Message-ID: <eb555e660601272233m6d6144fbs2989272b24d3c716@mail.gmail.com>

On 1/27/06, Jin.Li at csiro.au <Jin.Li at csiro.au> wrote:
> Hi all,
>
>
>
> I am using xyplot (lattice) to generate a figure like Figure 4.18 in
> MASS4, but I have the following two questions (1) how to change the font
> of x(y)lab?

Something like

xyplot(...

       xlab = list("foo", font = 3),

       ...)

fontface and fontfamily may be specified in the list too, although the
documentation fails to mention this.

(2) how to plot the panels for each level of the conditional
> variable (a factor in my data set) in the order as occurred in the
> data.frame rather than in the order of the levels of the conditioning
> variable?

Not possible directly, you have to change the order somehow. The
easiest is to specify the order when creating the factor explicitly,
e.g.

newf <- factor(f, levels = unique(f))

This should work fine even if f is already a factor.

Deepayan
--
http://www.stat.wisc.edu/~deepayan/



From ripley at stats.ox.ac.uk  Sat Jan 28 08:49:23 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 28 Jan 2006 07:49:23 +0000 (GMT)
Subject: [R] installation of R on Linux
In-Reply-To: <00e901c623af$cf5f8330$6401a8c0@main>
References: <00e901c623af$cf5f8330$6401a8c0@main>
Message-ID: <Pine.LNX.4.61.0601280737550.8136@gannet.stats>

On Fri, 27 Jan 2006, Daniel Nordlund wrote:

> R-users,
>
> I am new user of Linux (have been using Win XP Pro) and wanted to 
> install R.  Since I am just beginning to learn Linux I was wondering, 
> where in the directory structure do users of Linux usually install R? 
> Most of the instructions I have read simply say to untar the tarball 
> where you want to install the program.  Any suggestions would be welcome 
> as to an appropriate place.  I know I could get an rpm, but wanted to 
> use this as a learning process for a variety of skills.  Currently 
> working with SuSE 9.1

There is a definitive set of instructions, in the file INSTALL in 
the tarball and at

https://svn.r-project.org/R/trunk/INSTALL

Unpacking and installing are separate operations.  There is more 
information in the R-admin manual (which you already have in a Windows 
version of R, and is also in the tarball).

What most of us do is to untar in any convenient place (I use ~/R), use 
configure, make, and then use 'make install' to >install< R.  This 
installs in /usr/local in the conventional subdirectories (and 
conventionally needs su to access).  Having installed, you can wipe out 
the unpacked version of the tarball.

So, in my example

cd ~/R
tar zxf R-2.2.1.tar.gz
cd R-2.2.1
configure
make
make info pdf
su
make install install-info install-pdf
[leave su shell]
cd ..
rm -rf R-2.2.1

Rehash and start R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From res90sx5 at verizon.net  Sat Jan 28 09:48:43 2006
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Sat, 28 Jan 2006 00:48:43 -0800
Subject: [R] installation of R on Linux
In-Reply-To: <Pine.LNX.4.61.0601280737550.8136@gannet.stats>
Message-ID: <00ed01c623e7$a4e95350$6401a8c0@main>

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Friday, January 27, 2006 11:49 PM
> To: Daniel Nordlund
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] installation of R on Linux
> 
> On Fri, 27 Jan 2006, Daniel Nordlund wrote:
> 
> > R-users,
> >
> > I am new user of Linux (have been using Win XP Pro) and wanted to
> > install R.  Since I am just beginning to learn Linux I was wondering,
> > where in the directory structure do users of Linux usually install R?
> > Most of the instructions I have read simply say to untar the tarball
> > where you want to install the program.  Any suggestions would be welcome
> > as to an appropriate place.  I know I could get an rpm, but wanted to
> > use this as a learning process for a variety of skills.  Currently
> > working with SuSE 9.1
> 
> There is a definitive set of instructions, in the file INSTALL in
> the tarball and at
> 
> https://svn.r-project.org/R/trunk/INSTALL
> 
> Unpacking and installing are separate operations.  There is more
> information in the R-admin manual (which you already have in a Windows
> version of R, and is also in the tarball).
> 
> What most of us do is to untar in any convenient place (I use ~/R), use
> configure, make, and then use 'make install' to >install< R.  This
> installs in /usr/local in the conventional subdirectories (and
> conventionally needs su to access).  Having installed, you can wipe out
> the unpacked version of the tarball.
> 
> So, in my example
> 
> cd ~/R
> tar zxf R-2.2.1.tar.gz
> cd R-2.2.1
> configure
> make
> make info pdf
> su
> make install install-info install-pdf
> [leave su shell]
> cd ..
> rm -rf R-2.2.1
> 
> Rehash and start R.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

Prof. Ripley,

Thanks for the example and the pointers to various locations for documentation.  As a new user of Linux (with minimal experience in using Unix-like systems), I am somewhat uncomfortable putting programs just anywhere since there seem to be default locations for where many system programs reside.  Your concrete example is very helpful.

Thanks again,

Daniel Nordlund
Bothell, WA  USA



From kevin.thorpe at utoronto.ca  Sat Jan 28 13:52:30 2006
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Sat, 28 Jan 2006 07:52:30 -0500
Subject: [R] installation of R on Linux
In-Reply-To: <00ed01c623e7$a4e95350$6401a8c0@main>
References: <00ed01c623e7$a4e95350$6401a8c0@main>
Message-ID: <43DB690E.4090708@utoronto.ca>

Daniel Nordlund wrote:
>>-----Original Message-----
>>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>>Sent: Friday, January 27, 2006 11:49 PM
>>To: Daniel Nordlund
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] installation of R on Linux
>>
>>On Fri, 27 Jan 2006, Daniel Nordlund wrote:
>>
>>
>>>R-users,
>>>
>>>I am new user of Linux (have been using Win XP Pro) and wanted to
>>>install R.  Since I am just beginning to learn Linux I was wondering,
>>>where in the directory structure do users of Linux usually install R?
>>>Most of the instructions I have read simply say to untar the tarball
>>>where you want to install the program.  Any suggestions would be welcome
>>>as to an appropriate place.  I know I could get an rpm, but wanted to
>>>use this as a learning process for a variety of skills.  Currently
>>>working with SuSE 9.1
>>
>>There is a definitive set of instructions, in the file INSTALL in
>>the tarball and at
>>
>>https://svn.r-project.org/R/trunk/INSTALL
>>
>>Unpacking and installing are separate operations.  There is more
>>information in the R-admin manual (which you already have in a Windows
>>version of R, and is also in the tarball).
>>
>>What most of us do is to untar in any convenient place (I use ~/R), use
>>configure, make, and then use 'make install' to >install< R.  This
>>installs in /usr/local in the conventional subdirectories (and
>>conventionally needs su to access).  Having installed, you can wipe out
>>the unpacked version of the tarball.
>>
>>So, in my example
>>
>>cd ~/R
>>tar zxf R-2.2.1.tar.gz
>>cd R-2.2.1
>>configure
>>make
>>make info pdf
>>su
>>make install install-info install-pdf
>>[leave su shell]
>>cd ..
>>rm -rf R-2.2.1
>>
>>Rehash and start R.
>>
>>--
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> Prof. Ripley,
> 
> Thanks for the example and the pointers to various locations for documentation.  As a new user of Linux (with minimal experience in using Unix-like systems), I am somewhat uncomfortable putting programs just anywhere since there seem to be default locations for where many system programs reside.  Your concrete example is very helpful.
> 
> Thanks again,
> 
> Daniel Nordlund
> Bothell, WA  USA
> 

One additional point.  I have often found it preferable to
run configure and make as a regular user and only run
'make install' as root.


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.946.3297



From laura at env.leeds.ac.uk  Sat Jan 28 14:55:52 2006
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sat, 28 Jan 2006 13:55:52 +0000 (GMT)
Subject: [R] Creating 3D Gaussian Plot
Message-ID: <Pine.LNX.4.44.0601281338010.3054-100000@gw.env.leeds.ac.uk>

Hello,

I requested help a couple of weeks ago creating a dipole field in R but
receieved no responses. Eventually I opted to create a 3d sinusoidal plot
and concatenate this with its inverse as a means for a "next best"
situation. It seems that this isn't sufficient for my needs and I'm really
after creating a continuous 3d gaussian mesh with a "positive" and
"negative" dipole.

Can anyone offer any pointers at all?

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From murdoch at stats.uwo.ca  Sat Jan 28 15:05:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 28 Jan 2006 09:05:01 -0500
Subject: [R] Creating 3D Gaussian Plot
In-Reply-To: <Pine.LNX.4.44.0601281338010.3054-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0601281338010.3054-100000@gw.env.leeds.ac.uk>
Message-ID: <43DB7A0D.1060502@stats.uwo.ca>

On 1/28/2006 8:55 AM, Laura Quinn wrote:
> Hello,
> 
> I requested help a couple of weeks ago creating a dipole field in R but
> receieved no responses. Eventually I opted to create a 3d sinusoidal plot
> and concatenate this with its inverse as a means for a "next best"
> situation. It seems that this isn't sufficient for my needs and I'm really
> after creating a continuous 3d gaussian mesh with a "positive" and
> "negative" dipole.

The names you're using don't mean anything to me; perhaps there just 
aren't enough atmospheric scientists on the list and that's why you 
didn't get any response.  If you don't get a response this time, you 
should describe what you want in basic terms, and/or point to examples 
of it on the web.

Duncan Murdoch

> 
> Can anyone offer any pointers at all?
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From laura at env.leeds.ac.uk  Sat Jan 28 15:27:41 2006
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sat, 28 Jan 2006 14:27:41 +0000 (GMT)
Subject: [R] Creating 3D Gaussian Plot
In-Reply-To: <43DB7A0D.1060502@stats.uwo.ca>
Message-ID: <Pine.LNX.4.44.0601281424160.3054-100000@gw.env.leeds.ac.uk>

My apologies.

With further apologies for the poor graphics, this link demonstrates the
sort of 3d mesh which I am hoping to replicate - I would like to be able
to replicate a number of these of varying intensity. Demonstrating
different levels of potential via the "steepness" of the slopes.

http://maxwell.ucdavis.edu/~electro/potential/images/steep.jpg

I then wish to pick a number of grid points at random from the output to
perform a further analysis upon.

I hope this makes things a little clearer!

Again, any help gratefully received, thank you.


Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Sat, 28 Jan 2006, Duncan Murdoch wrote:

> On 1/28/2006 8:55 AM, Laura Quinn wrote:
> > Hello,
> >
> > I requested help a couple of weeks ago creating a dipole field in R but
> > receieved no responses. Eventually I opted to create a 3d sinusoidal plot
> > and concatenate this with its inverse as a means for a "next best"
> > situation. It seems that this isn't sufficient for my needs and I'm really
> > after creating a continuous 3d gaussian mesh with a "positive" and
> > "negative" dipole.
>
> The names you're using don't mean anything to me; perhaps there just
> aren't enough atmospheric scientists on the list and that's why you
> didn't get any response.  If you don't get a response this time, you
> should describe what you want in basic terms, and/or point to examples
> of it on the web.
>
> Duncan Murdoch
>
> >
> > Can anyone offer any pointers at all?
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From alanc at umit.maine.edu  Sat Jan 28 15:42:30 2006
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Sat, 28 Jan 2006 09:42:30 -0500
Subject: [R] monochrome mosaic plot in vcd package
In-Reply-To: <mailman.6.1138446001.12044.r-help@stat.math.ethz.ch>
References: <mailman.6.1138446001.12044.r-help@stat.math.ethz.ch>
Message-ID: <fc.004c4d1925eec0d13b9aca008ab87bd1.25eec271@umit.maine.edu>

Michael,

How about using grayscale shading and setting the background color (the gaps between the tiles) to middle gray?

--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



r-help at stat.math.ethz.ch on Saturday, January 28, 2006 at 6:00 AM -0500 wrote:
>Content-Type: message/rfc822
>MIME-Version: 1.0
>
>From: Mike Townsley <uctqmkt at ucl.ac.uk>
>Precedence: list
>MIME-Version: 1.0
>To: r-help at stat.math.ethz.ch
>Date: Fri, 27 Jan 2006 11:28:10 +0000
>Message-ID: <5.2.1.1.0.20060127103000.00981fc0 at imap-server.ucl.ac.uk>
>Content-Type: text/plain; charset="us-ascii"; format=flowed
>Subject: [R] monochrome mosaic plot in vcd package
>Message: 5
>
>helpeRs,
>
>I have a nice looking mosaic plot in an article to be published 
>soon.  Sadly, the published version will be in black and white and so ruin 
>the advantage of the default shading scheme of tiles.
>
>What would readers suggest as an alternative shading scheme?  If I have a 
>black-and-white shading scheme graduated according to suitable cutoffs I 
>won't be able to tell positive from negative residuals.  The tile borders 
>can be changed of course, but I'm uncertain that is will be clear enough 
>for a reader.
>Another option may be to use a fill pattern of sloping lines with different 
>orientations for the sign and density for the magnitude.  The problem with 
>this option is I wouldn't know where to start to incorporate into a legend.
>
>The shading_binary function is no good as I would like the cells with 
>residuals less than absolute 2 to be different from other cells.  How would 
>readers of this list represent a mosaic plot so that it was easily 
>interpretable in monochrome?
>
>My data can be used as an example:
>
>
>library(vcd)
>library(MASS)
>
>term.1 <- gl(2,1,8, labels = LETTERS[1:2])
>term.2 <- gl(2,2,8, labels = LETTERS[3:4])
>term.3 <- gl(2,4,8, labels = LETTERS[5:6])
>
>cell.count <- c(72, 19, 5, 8, 117, 115, 81, 85)
>
>mosaic(loglm(formula = cell.count ~ term.1 + term.2 + term.3),
>        shade = TRUE, gp = shading_hcl, legend = TRUE,
>        labeling_args = list(rot_labels = rep(0,4)),
>        gp_args = list(lty = 1:2),legend_width = unit(0.2, "npc"))
>
>
>
>------------------------------------------------------------
>Dr Michael Townsley
>Senior Research Fellow
>Jill Dando Institute of Crime Science
>University College London
>Second Floor, Brook House
>London, WC1E 7HN
>
>Phone: 020 7679 0820
>Fax: 020 7679 0828
>Email: m.townsley at ucl.ac.uk
>



From theholts at care4free.net  Sat Jan 28 16:52:49 2006
From: theholts at care4free.net (Martin P. Holt)
Date: Sat, 28 Jan 2006 15:52:49 -0000
Subject: [R] Learning - Example programs
Message-ID: <007701c62422$e3f4b130$00d39156@MARTINSPC>

I'm working my way up the learning curve for R. A method of learning I find 
very effective is to work through an existing program. Are there any 
libraries or archives of R programs on the web ? If not, would this be a 
good idea for the R website ?
I hope this is not a FAQ: I have checked as far as I can.

Best Wishes,
Martin Holt



From edd at debian.org  Sat Jan 28 17:18:41 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 28 Jan 2006 10:18:41 -0600
Subject: [R] Learning - Example programs
In-Reply-To: <007701c62422$e3f4b130$00d39156@MARTINSPC>
References: <007701c62422$e3f4b130$00d39156@MARTINSPC>
Message-ID: <17371.39265.552101.362409@basebud.nulle.part>


On 28 January 2006 at 15:52, Martin P. Holt wrote:
| I'm working my way up the learning curve for R. A method of learning I find 
| very effective is to work through an existing program. Are there any 
| libraries or archives of R programs on the web ? If not, would this be a 

Well, you could try googleing for CRAN and its mirrors; each CRAN archive
contains over six hundred contributed packages all of which contain examples
for R. Each of which is only one command away from you for use and
inspection.

And your R installation itself has thousands of functions each with
examples. Try
	
	> help(example)
	> example(example)

R comes with six manuals that come with it, and a FAQ document.

| good idea for the R website ?
| I hope this is not a FAQ: I have checked as far as I can.

The FAQ lists several books on R.  CRAN and its mirrors host several free
books in pdf form.

Lastly, some Google scores:

	'R example'		140 million hits
	'R examples'		 78 million hits
	'R example code'	 63 million hits

Hope this helps, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From pcampbell at econ.bbk.ac.uk  Sat Jan 28 17:18:47 2006
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Sat, 28 Jan 2006 16:18:47 -0000
Subject: [R] Learning - Example programs
In-Reply-To: <007701c62422$e3f4b130$00d39156@MARTINSPC>
Message-ID: <NGECIFANPOJAGABBAEAPGEHAFIAA.pcampbell@econ.bbk.ac.uk>

Typing the function name at the prompt prints that body of the function.  By
working thrugh the steps in the boot function, helped me both understand the
way the bootstrap works and write better R code.

Phineas

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Martin P. Holt
Sent: Saturday, January 28, 2006 3:53 PM
To: r-help
Subject: [R] Learning - Example programs


I'm working my way up the learning curve for R. A method of learning I find
very effective is to work through an existing program. Are there any
libraries or archives of R programs on the web ? If not, would this be a
good idea for the R website ?
I hope this is not a FAQ: I have checked as far as I can.

Best Wishes,
Martin Holt

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Sat Jan 28 17:30:25 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 28 Jan 2006 11:30:25 -0500
Subject: [R] Creating 3D Gaussian Plot
In-Reply-To: <Pine.LNX.4.44.0601281447450.3054-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0601281447450.3054-100000@gw.env.leeds.ac.uk>
Message-ID: <43DB9C21.1020909@stats.uwo.ca>

On 1/28/2006 9:52 AM, Laura Quinn wrote:
> I'm working from a blank canvas! The perspective isn't key - the most
> important thing is getting the values for the grid mesh points, the method
> of plotting is less crucial. I was hoping there might be an inbuilt R
> function which would allow me to create the grid points by specifying
> amplitude/wavelength parameters but my searches have come up blank thus
> far. I'd happily knock up a FORTRAN routine to provide me with the coords
> for the gridpoints, but I can't figure out the underlying equation.

Supposing you want the peaks at (1,-1) and (-1,1), a reasonable equation 
might be

fn <- function(x, y, scale) 
dnorm(x,mean=1,sd=scale)*dnorm(y,mean=-1,sd=scale) - 
dnorm(x,mean=-1,sd=scale)*dnorm(y,mean=1,sd=scale)

Then you can plot it on a grid by

x <- seq(-4,4,len=100)
y <- seq(-4,4,len=100)
z <- outer(x,y,fn,scale=0.5)
persp(x,y,z,col="green",border=NA,shade=0.75)

You can play around with the arguments to persp to change the colours, 
rotate it, etc.  The scale argument controls how pointy the peaks are.

You might also want to look at the surface3d function in the rgl 
package.  It does scaling differently, so you'd probably want something like

bg3d("white")
surface3d(x,y,z*6,col="green")

which gives a surface like the one above, but you can rotate it using 
the mouse.

Duncan Murdoch

> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Sat, 28 Jan 2006, Duncan Murdoch wrote:
> 
>> On 1/28/2006 9:27 AM, Laura Quinn wrote:
>>> My apologies.
>>>
>>> With further apologies for the poor graphics, this link demonstrates the
>>> sort of 3d mesh which I am hoping to replicate - I would like to be able
>>> to replicate a number of these of varying intensity. Demonstrating
>>> different levels of potential via the "steepness" of the slopes.
>>>
>>> http://maxwell.ucdavis.edu/~electro/potential/images/steep.jpg
>>>
>>> I then wish to pick a number of grid points at random from the output to
>>> perform a further analysis upon.
>>>
>>> I hope this makes things a little clearer!
>>>
>>> Again, any help gratefully received, thank you.
>> That's helpful.  You can produce a graph like that using persp(),
>> provided you have already calculated the values at all the points on the
>> grid -- but it sounds as though you haven't got those yet.  What sort of
>> input do you have?
>>
>> Duncan Murdoch
>>>
>>> Laura Quinn
>>> Institute of Atmospheric Science
>>> School of Earth and Environment
>>> University of Leeds
>>> Leeds
>>> LS2 9JT
>>>
>>> tel: +44 113 343 1596
>>> fax: +44 113 343 6716
>>> mail: laura at env.leeds.ac.uk
>>>
>>> On Sat, 28 Jan 2006, Duncan Murdoch wrote:
>>>
>>>> On 1/28/2006 8:55 AM, Laura Quinn wrote:
>>>>> Hello,
>>>>>
>>>>> I requested help a couple of weeks ago creating a dipole field in R but
>>>>> receieved no responses. Eventually I opted to create a 3d sinusoidal plot
>>>>> and concatenate this with its inverse as a means for a "next best"
>>>>> situation. It seems that this isn't sufficient for my needs and I'm really
>>>>> after creating a continuous 3d gaussian mesh with a "positive" and
>>>>> "negative" dipole.
>>>> The names you're using don't mean anything to me; perhaps there just
>>>> aren't enough atmospheric scientists on the list and that's why you
>>>> didn't get any response.  If you don't get a response this time, you
>>>> should describe what you want in basic terms, and/or point to examples
>>>> of it on the web.
>>>>
>>>> Duncan Murdoch
>>>>
>>>>> Can anyone offer any pointers at all?
>>>>>
>>>>> Laura Quinn
>>>>> Institute of Atmospheric Science
>>>>> School of Earth and Environment
>>>>> University of Leeds
>>>>> Leeds
>>>>> LS2 9JT
>>>>>
>>>>> tel: +44 113 343 1596
>>>>> fax: +44 113 343 6716
>>>>> mail: laura at env.leeds.ac.uk
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>



From ggrothendieck at gmail.com  Sat Jan 28 17:57:47 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 28 Jan 2006 11:57:47 -0500
Subject: [R] Learning - Example programs
In-Reply-To: <007701c62422$e3f4b130$00d39156@MARTINSPC>
References: <007701c62422$e3f4b130$00d39156@MARTINSPC>
Message-ID: <971536df0601280857ld0610f3y2b2b7ec5dbbb40c1@mail.gmail.com>

In addition to what others have said go the R home page,
and under Documentation on the left hand side click on Other
and look at that plus from that page click on Contributed Documents
which is a link on that page.  Also this has lots of code:
http://www.ku.edu/~pauljohn/R/Rtips.html

On 1/28/06, Martin P. Holt <theholts at care4free.net> wrote:
> I'm working my way up the learning curve for R. A method of learning I find
> very effective is to work through an existing program. Are there any
> libraries or archives of R programs on the web ? If not, would this be a
> good idea for the R website ?
> I hope this is not a FAQ: I have checked as far as I can.
>
> Best Wishes,
> Martin Holt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gelman at stat.columbia.edu  Sat Jan 28 18:09:56 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sat, 28 Jan 2006 12:09:56 -0500
Subject: [R] yet another lmer question
Message-ID: <43DBA564.7080102@stat.columbia.edu>

I've been trying to keep track with lmer, and now I have a couple of 
questions with the latest version of Matrix (0.995-4).  I fit 2 very 
similar models, and the results are severely rounded in one case and 
rounded not at all in the other.

 > y <- 1:10
 > group <- rep (c(1,2), c(5,5))
 > M1 <- lmer (y ~ 1 + (1 | group))
 > coef(M1)
$group
  (Intercept)
1         3.1
2         7.9

 > x <- rep (c(1,2), c(3,7))
 > M2 <- lmer (y ~ 1 +  x + (1 + x | group))
 > coef(M2)
$group
  (Intercept)        x
1   -0.755102 2.755102
2    0.616483 3.640738

I can't figure out why everything is rounded for the first model but not 
for the second.  Also, mcmcsamp() works for M1 but not for M2:

 > mcmcsamp(M1)
     (Intercept) log(sigma^2) log(grop.(In))
[1,]    9.099073    0.5711817       3.246981
attr(,"mcpar")
[1] 1 1 1
attr(,"class")
[1] "mcmc"

 > mcmcsamp(M2)
Error: inconsistent degrees of freedom and dimension
Error in t(.Call("mer_MCMCsamp", object, saveb, n, trans, PACKAGE = 
"Matrix")) :
        unable to find the argument 'x' in selecting a method for 
function 't'


-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From spencer.graves at pdf.com  Sat Jan 28 18:39:29 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 28 Jan 2006 09:39:29 -0800
Subject: [R] fitting generalized linear models using glmmPQL
In-Reply-To: <2333585778emoigar@uv.es>
References: <2333585778emoigar@uv.es>
Message-ID: <43DBAC51.8040401@pdf.com>

	  I have not seen your particular error message often enough to be 
confident I know what caused it:
 >
 > Error: NA/NaN/Inf in foreign function call (arg 1)
 > In addition: Warning message:
 > step size truncated due to divergence 	

	  In this case, the "Warning" seems more informative to me than the 
actual "Error".  The function "glmmPQL" is an iterative algorithm.  The 
"warining says that "step size truncated due to divergence.  This kind 
of warning might occur when the iteration found a long, gently sloping 
plateau and tried to take a giant step to get beyond it.  The algorithm 
decided that the giant step was too large, and so tried to truncate it.

	  Since I don't have your data, I can only guess at what might have 
caused it.  My first guess is that you have the wrong model.  Have you 
considered the following:

modelo1a<-glmmPQL(DE ~ POB*TEMP*SALINITA, data = datos, random =
  ~ POB|CLON, family = poisson)

	  Putting nesting with the random effect in the 'fixed' model makes not 
sense to me and generates an explosion of the number of "fixed effect" 
parameters to be estimated.  This might cause your problem all by itself.

	  If that doesn't solve the problem, have you tried to get a separate 
fit for each level of "CLON" of the same fixed model, something like the 
following:

(CLON.count <- table(datos$CLON))
n.CLON <- length(CLON.count)
glm.fits <- vector(n.CLON, mode="list")
for(i in 1:n.CLON)
    glm.fits[[i]] <- try(glm(DE ~ POB*TEMP*SALINITA, data = 
datos[datos$CLON==names(CLON.count[i])))

	  Under certain circumstances, "glmmPQL" could still return a good 
answer even if "glm" returned an error for every level of CLON in this 
loop.  However, that's far from certain.

	  Also, have you tried changing the two "*" operators to "+" in the 
fixed formula?  This reduces the number of parameters to be estimated 
and might give you sensible results.

	  If you are NOT using the latest version of R with the latest versions 
of MASS and nlme, please upgrade before submitting another post.  And 
PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html", especialy the bit about 
providing a simple, self contained example.  I sometimes solve problems 
like this in the course of trying to prepare a simple example.

	  hope this helps.
	  spencer graves	

Eduardo.Garcia at uv.es wrote:

> Hi, I have tried to run the following (I know it's a huge data set but 
> I tried to perform it with a 1 GB RAM computer):
> 
> library(foreign)
> library(MASS)
> library(nlme)
> datos<-read.spss(file="c:\\Documents and 
> Settings\\Administrador\\Escritorio\\datosfin.sav",to.data.frame=TRUE)
> str(datos)
> 
> `data.frame':   1414 obs. of  5 variables:
>  $ POB     : Factor w/ 6 levels "CHI","HOS","HYR",..: 1 1 1 1 1 1 1 1 
> 1 1 ...
>  $ CLON    : num  1 1 1 1 1 1 1 1 2 2 ...
>  $ TEMP    : Factor w/ 2 levels "20 C","25 C": 1 1 1 1 2 2 2 2 1 1 ...
>  $ SALINITA: Factor w/ 2 levels "15 g/l","30 g/l": 1 1 2 2 1 1 2 2 1 
> 1 ...
>  $ DE      : num  17 0 7 1 15 28 4 14 13 16 ...
>  - attr(*, "variable.labels")= Named chr  "" "" "" "" ...
>   ..- attr(*, "names")= chr  "POB" "CLON" "TEMP" "SALINITA" ...
> 
> datos$CLON<-as.factor(datos$CLON)
> 
> modelo1<-glmmPQL(DE ~ (POB/CLON)*TEMP*SALINITA, data = datos, random = 
> ~ 1|CLON, family = poisson)
> 
> And I have obtained the following:
> 
> Error: NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> step size truncated due to divergence 
> 
> This is the first time I've observed such a message and I have no idea 
> about what does it mean. Is it possible that the process failed 
> because of the size of the data set (180 levels of the CLON factor)? 
> Or maybe is it a syntax problem?
> 
> Thank you in advance.
> 
> 
> ********************************
> Eduardo Mois??s Garc??a Roger
> 
> Institut Cavanilles de Biodiversitat i Biologia
> Evolutiva - ICBIBE.
> Tel. +34963543664
> Fax  +34963543670
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From laura at env.leeds.ac.uk  Sat Jan 28 19:27:54 2006
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sat, 28 Jan 2006 18:27:54 +0000 (GMT)
Subject: [R] Selecting Random Subset From Matrix - retaining indices
Message-ID: <Pine.LNX.4.44.0601281827120.8722-100000@gw.env.leeds.ac.uk>

Hello,

I was wondering whether there is a way to select random samples from a
data matrix, retaining the indexing for the rows and columns? I have
looked at using the sample() function. Applied directly to my matrix this
returns a vector of absolute values but the indices are lost, alternatively I can
select a random sample from a length equal to the number of elements in
the matrix and then translate each number into an element withing the
array but this seems to require a lot of work to ascertain the position
and value of each element. Is there a better way of performing this
operation?

Further to my earlier query I am hoping to pick a random selection of
grid points (with (x,y,z) coords) from a 3d map matrix.

Thanks in advance.

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From gregor.gorjanc at gmail.com  Sat Jan 28 19:54:07 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Sat, 28 Jan 2006 19:54:07 +0100
Subject: [R] draft of Comment on UCLA tech report
Message-ID: <43DBBDCF.8000009@bfro.uni-lj.si>

Hi!

I suggest that you look in fortune package and you could add some of
frotunes to this report. I think thay say a lot. I would just like to
add my story. I started stats with SAS and when I heard of R I, being a
open source fan, imidiatelly tried it. It was a real pain and I
abandoned that idea completely, although I really tried hard. Now I know
that my problem was wish to move to R, but not accepting its logic and
wish to do that in "one" day. Then I had to do a simple thing in SAS and
I realized that I do not know how to do it in SAS. Just a quick look in
R solved my problem and I took about one month of slow study and I am
not sorry for that. It really is important to change the approach. Of
course there are pros and cons, but I would say that one of the fortunes
that involve comparison of various software solutions and money tells it
all.

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From marcodoc75 at yahoo.com  Sat Jan 28 20:23:51 2006
From: marcodoc75 at yahoo.com (Marco Geraci)
Date: Sat, 28 Jan 2006 11:23:51 -0800 (PST)
Subject: [R] weighted likelihood for lme
In-Reply-To: <43DB0DFF.1000806@pdf.com>
Message-ID: <20060128192351.82747.qmail@web31311.mail.mud.yahoo.com>

--- Spencer Graves <spencer.graves at pdf.com> wrote:

> 	  Thank you for providing such a marvelous example.
>  I wish I could 
> reward your dilligence with a simple, complete
> answer.  Unfortunately, 
> the best I can offer at the moment is a guess and a
> reference.  First, I 
> believe you are correct in that the "weights"
> argument describes "the 
> within-group heteroscedasticity structure".  To
> specify between-group 
> heterscadisticity, have you considered the
> following:
> 
>   foo <- Orthodont
>   foo$w <- c(rep(1, 5*4), rep(0.5, 22*4))
>  > lme(distance ~ 1, random = ~ w-1|Subject,
> +     method="ML", data = foo)
> Linear mixed-effects model fit by maximum likelihood
>    Data: foo
>    Log-likelihood: -258.8586
>    Fixed: distance ~ 1
> (Intercept)
>      23.8834
> 
> Random effects:
>   Formula: ~w - 1 | Subject
>                 w Residual
> StdDev: 3.370796 2.233126
> 
> Number of Observations: 108
> Number of Groups: 27
> 

Nice! I haven't considered this alternative. As you
said, it's not a complete answer, but, I say, it's a
smart start.

> 	  Second, have you consulted Pinheiro and Bates
> (2000) Mixed-Effects 
> Models in S and S-Plus (Springer)?  If you have not
> already, I encourage 
> you to spend some quality time with that book.  For
> me, this book helped 
> transform "lme" from an inaequately documented and
> unusable black box 
> into a simple, understandable, elegant tool.  I may
> have recommeded it 
> to more people than any other single work over the
> past five years.

I know the book by Pinheiro and Bates and I do really
have to spend some time with it.

Thanks very much,
Marco

> 
> 	  hope this helps.
> 	  spencer graves
> 
> Marco Geraci wrote:
> 
> >     Dear R users,
> >   I'm trying to fit a simple random intercept
> model with a fixed intercept. 
> >     Suppose I want to assign a weight w_i to the
> i-th contribute to the log-likelihood, i.e.
> >    
> >   w_i * logLik_i
> >    
> >   where logLik_i is the log-likelihood for the
> i-th subject.
> >   I want to maximize the likelihood for N subjects
> >    
> >   Sum_i  {w_i * logLik_i}
> >    
> >   Here is a simple example to reproduce
> >    
> >   # require(nlme)
> >     > foo <- Orthodont
> > 
> >>lme(distance ~ 1, random = ~ 1|Subject,
> method="ML", data = foo)
> > 
> > Linear mixed-effects model fit by maximum
> likelihood
> >   Data: foo 
> >   Log-likelihood: -257.7456
> >   Fixed: distance ~ 1 
> > (Intercept) 
> >    24.02315 
> >   Random effects:
> >  Formula: ~1 | Subject
> >         (Intercept) Residual
> > StdDev:    1.888748 2.220312
> >   Number of Observations: 108
> > Number of Groups: 27 
> > 
> > Then I assign arbitrary weights, constant within
> the group. I want to give weight 1 to the first 5
> subjects, and weight 0.5 to the others 22 (4 is the
> number of repeated measurements for each subject)
> > 
> >    
> >   > foo$w <- c(rep(1, 5*4), rep(0.5, 22*4))
> >    
> >   Maybe I am missing something, but I believe that
> >    
> >   > lme(distance ~ 1, random = ~ 1|Subject,
> method="ML", data = foo, weight=~w)
> >    
> >   does not maximize the likelihood Sum_i  {w_i *
> logLik_i}, 
> since 'weight' describes the with-in
> heteroscedasticity structure.
> >   I think I need something like the option
> 'iweight' 
> (importance weight) for the command 'xtreg' of
> Stata.
> >    
> >   Any suggestion for R?
> >    
> >   Thanks in advance,
> >    
> >   Marco Geraci
> >    
> >   >  sessionInfo()
> > R version 2.2.1, 2005-12-20, i386-pc-mingw32 
> >   attached base packages:
> > [1] "methods"   "stats"     "graphics" 
> "grDevices" "utils"    
> > [6] "datasets"  "base"     
> >   other attached packages:
> >     nlme 
> > "3.1-66" 
> > 
> > 
> > 
> > 
> > 
> > 		
> > ---------------------------------
> >  
> >  What are the most popular cars?  Find out at
> Yahoo! Autos
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Sat Jan 28 20:52:25 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jan 2006 20:52:25 +0100
Subject: [R] yet another lmer question
In-Reply-To: <43DBA564.7080102@stat.columbia.edu>
References: <43DBA564.7080102@stat.columbia.edu>
Message-ID: <x2wtgk870m.fsf@turmalin.kubism.ku.dk>

Andrew Gelman <gelman at stat.columbia.edu> writes:

> I've been trying to keep track with lmer, and now I have a couple of 
> questions with the latest version of Matrix (0.995-4).  I fit 2 very 
> similar models, and the results are severely rounded in one case and 
> rounded not at all in the other.
> 
>  > y <- 1:10
>  > group <- rep (c(1,2), c(5,5))
>  > M1 <- lmer (y ~ 1 + (1 | group))
>  > coef(M1)
> $group
>   (Intercept)
> 1         3.1
> 2         7.9
> 
>  > x <- rep (c(1,2), c(3,7))
>  > M2 <- lmer (y ~ 1 +  x + (1 + x | group))
>  > coef(M2)
> $group
>   (Intercept)        x
> 1   -0.755102 2.755102
> 2    0.616483 3.640738
> 
> I can't figure out why everything is rounded for the first model but not 
> for the second.  Also, mcmcsamp() works for M1 but not for M2:


Well,

> dput(coef(M1)[[1]])
structure(list("(Intercept)" = c(3.10000000006436, 7.89999999993564
)), .Names = "(Intercept)", row.names = c("1", "2"), class =
"data.frame")
> c(3.10000000006436, 7.89999999993564)
[1] 3.1 7.9

I.e., if you pass fake data, sometimes you get *results* that can be
rounded to a few significant digits. R tries to get rid of trailing
zeros in its print routines.

 
>  > mcmcsamp(M1)
>      (Intercept) log(sigma^2) log(grop.(In))
 > [1,]    9.099073    0.5711817       3.246981
> attr(,"mcpar")
> [1] 1 1 1
> attr(,"class")
> [1] "mcmc"
> 
>  > mcmcsamp(M2)
> Error: inconsistent degrees of freedom and dimension
> Error in t(.Call("mer_MCMCsamp", object, saveb, n, trans, PACKAGE = 
> "Matrix")) :
>         unable to find the argument 'x' in selecting a method for 
> function 't'

Looks like a buglet, but

> x
 [1] 1 1 1 2 2 2 2 2 2 2
> group
 [1] 1 1 1 1 1 2 2 2 2 2

Effects of x can (seemingly?) only be detected within group 1. I.e.
the random variation of the effect of x is based on a sample of size
1, so I'm actually more surprised that you get a fit at all...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From gelman at stat.columbia.edu  Sat Jan 28 21:32:30 2006
From: gelman at stat.columbia.edu (Andrew Gelman)
Date: Sat, 28 Jan 2006 15:32:30 -0500
Subject: [R] yet another lmer question
In-Reply-To: <x2wtgk870m.fsf@turmalin.kubism.ku.dk>
References: <43DBA564.7080102@stat.columbia.edu>
	<x2wtgk870m.fsf@turmalin.kubism.ku.dk>
Message-ID: <43DBD4DE.3090202@stat.columbia.edu>

D'oh (on the rounding)!

But on mcmcsamp(), I'm still confused.  I changed the example slightly 
and got the same problem:

 > y <- (1:20)*pi
 > x <- (1:20)^2
 > group <- rep (1:2, each=10)
 > M1 <- lmer (y ~ 1 + (1 | group))
 > M2 <- lmer (y ~ 1 + x + (1 + x | group))
 > mcmcsamp (M1, saveb=TRUE)
 > mcmcsamp (M2, saveb=TRUE)
Error: inconsistent degrees of freedom and dimension
Error in t(.Call("mer_MCMCsamp", object, saveb, n, trans, PACKAGE = 
"Matrix")) :
        unable to find the argument 'x' in selecting a method for 
function 't'

It really should be able to work (actually, the earlier example should 
work too), but maybe it gets hung up when there are only two groups.  
Indeed, when I change 20 and 2 above to 30 and 3, it works fine.

So I guess, as a practical matter, this is fine.  The domain where 
lmer() excels is large datasets with many groups; conversely, Bugs works 
best with small datasets with few groups.  However, I do like to use 
lmer() as a starting point, so I hope that at some point it will fully 
work in the above example also.

Also, since I have you on the line, so to speak, I noticed that coef() 
gives estimated group-level coefficients, and ranef() gives these 
coefficients centered at zero:

 > coef(M2)
$group
  (Intercept)         x
1    6.885045 0.2701600
2   23.586015 0.1010110

 > ranef(M2)
An object of class "lmer.ranef"
[[1]]
  (Intercept)           x
1   -8.350485  0.08457451
2    8.350485 -0.08457451

I was just wondering:  is one or the other of these parameterizations 
preferred by Doug Bates et al.?  I wanted to know because we discuss 
lmer() in our book, and I'd like our examples to remain relevant after 
the book appears and lmer() continues to be developed.





Peter Dalgaard wrote:

>Andrew Gelman <gelman at stat.columbia.edu> writes:
>
>  
>
>>I've been trying to keep track with lmer, and now I have a couple of 
>>questions with the latest version of Matrix (0.995-4).  I fit 2 very 
>>similar models, and the results are severely rounded in one case and 
>>rounded not at all in the other.
>>
>> > y <- 1:10
>> > group <- rep (c(1,2), c(5,5))
>> > M1 <- lmer (y ~ 1 + (1 | group))
>> > coef(M1)
>>$group
>>  (Intercept)
>>1         3.1
>>2         7.9
>>
>> > x <- rep (c(1,2), c(3,7))
>> > M2 <- lmer (y ~ 1 +  x + (1 + x | group))
>> > coef(M2)
>>$group
>>  (Intercept)        x
>>1   -0.755102 2.755102
>>2    0.616483 3.640738
>>
>>I can't figure out why everything is rounded for the first model but not 
>>for the second.  Also, mcmcsamp() works for M1 but not for M2:
>>    
>>
>
>
>Well,
>
>  
>
>>dput(coef(M1)[[1]])
>>    
>>
>structure(list("(Intercept)" = c(3.10000000006436, 7.89999999993564
>)), .Names = "(Intercept)", row.names = c("1", "2"), class =
>"data.frame")
>  
>
>>c(3.10000000006436, 7.89999999993564)
>>    
>>
>[1] 3.1 7.9
>
>I.e., if you pass fake data, sometimes you get *results* that can be
>rounded to a few significant digits. R tries to get rid of trailing
>zeros in its print routines.
>
> 
>  
>
>> > mcmcsamp(M1)
>>     (Intercept) log(sigma^2) log(grop.(In))
>>    
>>
> > [1,]    9.099073    0.5711817       3.246981
>  
>
>>attr(,"mcpar")
>>[1] 1 1 1
>>attr(,"class")
>>[1] "mcmc"
>>
>> > mcmcsamp(M2)
>>Error: inconsistent degrees of freedom and dimension
>>Error in t(.Call("mer_MCMCsamp", object, saveb, n, trans, PACKAGE = 
>>"Matrix")) :
>>        unable to find the argument 'x' in selecting a method for 
>>function 't'
>>    
>>
>
>Looks like a buglet, but
>
>  
>
>>x
>>    
>>
> [1] 1 1 1 2 2 2 2 2 2 2
>  
>
>>group
>>    
>>
> [1] 1 1 1 1 1 2 2 2 2 2
>
>Effects of x can (seemingly?) only be detected within group 1. I.e.
>the random variation of the effect of x is based on a sample of size
>1, so I'm actually more surprised that you get a fit at all...
>
>  
>

-- 
Andrew Gelman
Professor, Department of Statistics
Professor, Department of Political Science
gelman at stat.columbia.edu
www.stat.columbia.edu/~gelman

Tues, Wed, Thurs:  
  Social Work Bldg (Amsterdam Ave at 122 St), Room 1016
  212-851-2142
Mon, Fri:
  International Affairs Bldg (Amsterdam Ave at 118 St), Room 711
  212-854-7075

Mailing address:
  1255 Amsterdam Ave, Room 1016
  Columbia University
  New York, NY 10027-5904
  212-851-2142
  (fax) 212-851-2164



From andrej.kastrin at siol.net  Sat Jan 28 21:54:34 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sat, 28 Jan 2006 21:54:34 +0100
Subject: [R] Regex question
Message-ID: <43DBDA0A.6000303@siol.net>

Dear R useRs,

is there any simple, build in function to match specific regular 
expression in data file and write it to a vector. I have the following 
text file:

*NEW RECORD
*ID-001
*AB-text

*NEW RECORD
*ID-002
*AB-text
etc.

Now I have to match all ID fields and print them to a vector:
001
002
etc.

I know that this is very simple with Perl or R-Perl interface, but if 
possible, I want to do that 'on the hard way'.

Cheers, Andrej



From Bill.Venables at csiro.au  Sat Jan 28 23:48:02 2006
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 29 Jan 2006 09:48:02 +1100
Subject: [R] Creating 3D Gaussian Plot
Message-ID: <B998A44C8986644EA8029CFE6396A92454698B@exqld2-bne.qld.csiro.au>

Getting a picture like this is pretty easy.  e.g.

x <- y <- seq(-5, 5, len = 200)
X <- expand.grid(x = x, y = y)
X <- transform(X, z = dnorm(x, -2.5)*dnorm(y) - dnorm(x, 2.5)*dnorm(y))
z <- matrix(X$z, nrow = 200)

persp(x, y, z, col = "lightgoldenrod", border = NA,
   theta = 30, phi = 15, ticktype = "detailed", 
   ltheta = -120, shade = 0.25)

You can vary things as you wish.  

I don't follow the remark about picking grid points at random for
analysis, though.  On simple, entirely deterministic things like this
wouldn't you just be analysing the randomness that you inject into it by
the choice process, effectively?

Bill Venables.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Quinn
Sent: Sunday, 29 January 2006 12:28 AM
To: Duncan Murdoch
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Creating 3D Gaussian Plot


My apologies.

With further apologies for the poor graphics, this link demonstrates the
sort of 3d mesh which I am hoping to replicate - I would like to be able
to replicate a number of these of varying intensity. Demonstrating
different levels of potential via the "steepness" of the slopes.

http://maxwell.ucdavis.edu/~electro/potential/images/steep.jpg

I then wish to pick a number of grid points at random from the output to
perform a further analysis upon.

I hope this makes things a little clearer!

Again, any help gratefully received, thank you.


Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Sat, 28 Jan 2006, Duncan Murdoch wrote:

> On 1/28/2006 8:55 AM, Laura Quinn wrote:
> > Hello,
> >
> > I requested help a couple of weeks ago creating a dipole field in R
but
> > receieved no responses. Eventually I opted to create a 3d sinusoidal
plot
> > and concatenate this with its inverse as a means for a "next best"
> > situation. It seems that this isn't sufficient for my needs and I'm
really
> > after creating a continuous 3d gaussian mesh with a "positive" and
> > "negative" dipole.
>
> The names you're using don't mean anything to me; perhaps there just
> aren't enough atmospheric scientists on the list and that's why you
> didn't get any response.  If you don't get a response this time, you
> should describe what you want in basic terms, and/or point to examples
> of it on the web.
>
> Duncan Murdoch
>
> >
> > Can anyone offer any pointers at all?
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jholtman at gmail.com  Sun Jan 29 00:04:46 2006
From: jholtman at gmail.com (jim holtman)
Date: Sat, 28 Jan 2006 18:04:46 -0500
Subject: [R] Regex question
In-Reply-To: <43DBDA0A.6000303@siol.net>
References: <43DBDA0A.6000303@siol.net>
Message-ID: <644e1f320601281504l71043ba8ga476e2a9c608244d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060128/2f151f6b/attachment.pl

From andrej.kastrin at siol.net  Sun Jan 29 00:28:30 2006
From: andrej.kastrin at siol.net (Andrej Kastrin)
Date: Sun, 29 Jan 2006 00:28:30 +0100
Subject: [R] Regex question
In-Reply-To: <644e1f320601281504l71043ba8ga476e2a9c608244d@mail.gmail.com>
References: <43DBDA0A.6000303@siol.net>
	<644e1f320601281504l71043ba8ga476e2a9c608244d@mail.gmail.com>
Message-ID: <43DBFE1E.1000706@siol.net>

jim holtman wrote:

> Is this what you want?
>  
> > result <- readLines('/tempxx.txt')
> > result
> [1] "*NEW RECORD" "*ID-001"     "*AB-text"    ""            "*NEW RECORD"
> [6] "*ID-002"     "*AB-text"  
> > result <- result[grep('^.ID-', result)] # select only ID lines
> > result
> [1] "*ID-001" "*ID-002"
> > sub('^.ID-', '', result)
> [1] "001" "002"
>
>
>  
> On 1/28/06, *Andrej Kastrin* <andrej.kastrin at siol.net 
> <mailto:andrej.kastrin at siol.net>> wrote:
>
>     Dear R useRs,
>
>     is there any simple, build in function to match specific regular
>     expression in data file and write it to a vector. I have the
>     following
>     text file:
>
>     *NEW RECORD
>     *ID-001
>     *AB-text
>
>     *NEW RECORD
>     *ID-002
>     *AB-text
>     etc.
>
>     Now I have to match all ID fields and print them to a vector:
>     001
>     002
>     etc.
>
>     I know that this is very simple with Perl or R-Perl interface, but if
>     possible, I want to do that 'on the hard way'.
>
>     Cheers, Andrej
>
>     ______________________________________________
>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>     list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
>
>
>
>
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve? 

I'm forever indepted to you for this.

Cheers, Andrej



From spencer.graves at pdf.com  Sun Jan 29 01:18:44 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 28 Jan 2006 16:18:44 -0800
Subject: [R] Linearize a Function
In-Reply-To: <200601242311.13472.ggruber@terminal.at>
References: <200601242311.13472.ggruber@terminal.at>
Message-ID: <43DC09E4.9080502@pdf.com>

	  1.  Have you looked at "cumsum"?

	  2.  What do you think you are computing when adding 100 to 
cumsum(log.returns)?  To compute cumulative returns in percent from 
log.returns [or cumusum(log.returns)], compute exp(log.returns) or 
expm1(log.returns) = (exp(log.returns)-1).  Similarly, to compute 
log.returns from simple.returns, compute log1p(simple.returns) = 
log(1+simple.returns) [making the obvious conversions between 
percentages and proportions].  Or am I missing something?

	  hope this helps,
	  spencer graves

Gottfried Gruber wrote:

> hi,
> 
> i calculate the log-returns in return1 and i want to get the performance for 
> the security. with only one security i have the following code
> 
> # create matrix to keep performance
> return100=matrix(rep(100,length(return1)+1))
> # matrix for the sum
> z1=matrix(rep(0,length(return1)+1))
> # suming up the returns from current index to start
> for (i in 1:length(return1)) {z1[i+1]=sum(return1[c(1:i)]) }
> #adding both matrices
> return100=return100+z1*100
> 
> this works fine for a 1 x n matrix, but if i want the same for a n x m matrix 
> i assume the above code will get time-consuming. is there a trick to 
> linearize the for-loop or any other solution?
> 
> thanks for any solution & effort,
> tia gg



From comtech.usa at gmail.com  Sun Jan 29 05:12:04 2006
From: comtech.usa at gmail.com (Michael)
Date: Sat, 28 Jan 2006 20:12:04 -0800
Subject: [R] What does this command "~" mean?
Message-ID: <b1f16d9d0601282012t4d1654d7s6084e2edb87496e6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060128/10d8704b/attachment.pl

From edd at debian.org  Sun Jan 29 05:45:19 2006
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 28 Jan 2006 22:45:19 -0600
Subject: [R] What does this command "~" mean?
In-Reply-To: <b1f16d9d0601282012t4d1654d7s6084e2edb87496e6@mail.gmail.com>
References: <b1f16d9d0601282012t4d1654d7s6084e2edb87496e6@mail.gmail.com>
Message-ID: <17372.18527.858599.170064@basebud.nulle.part>


On 28 January 2006 at 20:12, Michael wrote:
| I am reading books and tutorials about R.
| 
| I don't understand the following:
| 
| plot(salary~rank, data=salary)
| plot(Ozone~date, data=airquality)
| 
| I don't understand what does "~" here, 

If all else fails, you could consult the help system via either one of

	> help("~")
	> ?"~"

and within Emacs/ESS you even get to drop the quotes around ~.  In short, ~
stands between the left and right side of a model.  So what econometrics
books would call

	Y = X beta + epsilon

gets written here as

	Y ~ X

with the coefficient vector beta and errors epsilon being implied. 

| and how can plot() have a input
| argument called "data"... I have looked it up in "plot"'s help but I could
| not find about argument "data".

The 'R Intro' manual may be of help here.  In short

	> plot(Ozone~Day, data=airquality)

works because it tells plot that the columns Ozone and Day are part of the
data.frame airquality. The shorter plot(Ozone~Day) would fail unless you had
attach'ed airquality. Again, the 'R Intro' manual may help.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison



From sell_mirage_ne at hotmail.com  Sun Jan 29 06:54:28 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Sat, 28 Jan 2006 23:54:28 -0600
Subject: [R] extracting 'Z' value from a glm result
Message-ID: <BAY110-F265923E88D69E9AEC3B328C7160@phx.gbl>

Hello R users
I like to extract z values for x1 and x2. I know how to extract coefficents 
using model$coef
but I don't know how to extract z values for each of independent variable. I 
looked around
using names(model) but I couldn't find how to extract z values.

Any help would be appreciated.

Thanks

TM

#########################################################
>summary(model)

Call:
glm(formula = y ~ x1+ x2, family = binomial)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-2.1397  -1.2357   0.6875   0.8517   1.5743

Coefficients:
              Estimate     Std. Error   z value   Pr(>|z|)
(Intercept) -0.63930    1.13045  -0.566    0.572
x1              0.69956    0.09459   7.396 1.40e-13 ***
x2              1.51389    1.13212   1.337    0.181
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1214.9  on 999  degrees of freedom
Residual deviance: 1149.8  on 997  degrees of freedom
AIC: 1155.8

Number of Fisher Scoring iterations: 4
############################################################



From ccleland at optonline.net  Sun Jan 29 09:00:48 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 29 Jan 2006 03:00:48 -0500
Subject: [R] extracting 'Z' value from a glm result
In-Reply-To: <BAY110-F265923E88D69E9AEC3B328C7160@phx.gbl>
References: <BAY110-F265923E88D69E9AEC3B328C7160@phx.gbl>
Message-ID: <43DC7630.5050304@optonline.net>

summary(model)$coefficients[,3]

or

summary(model)$coefficients[-1,3]

For example:

 > counts <- c(18,17,15,20,10,20,25,13,12)
 >      outcome <- gl(3,1,9)
 >      treatment <- gl(3,3)
 > glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
 > summary(glm.D93)

Call:
glm(formula = counts ~ outcome + treatment, family = poisson())

Deviance Residuals:
        1         2         3         4         5         6         7
-0.67125   0.96272  -0.16965  -0.21999  -0.95552   1.04939   0.84715
        8         9
-0.09167  -0.96656

Coefficients:
               Estimate Std. Error  z value Pr(>|z|)
(Intercept)  3.045e+00  1.709e-01   17.815   <2e-16 ***
outcome2    -4.543e-01  2.022e-01   -2.247   0.0246 *
outcome3    -2.930e-01  1.927e-01   -1.520   0.1285
treatment2   8.717e-16  2.000e-01 4.36e-15   1.0000
treatment3   4.557e-16  2.000e-01 2.28e-15   1.0000
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

     Null deviance: 10.5814  on 8  degrees of freedom
Residual deviance:  5.1291  on 4  degrees of freedom
AIC: 56.761

Number of Fisher Scoring iterations: 4

 >      summary(glm.D93)$coefficients[,3]
   (Intercept)      outcome2      outcome3    treatment2    treatment3
  1.781478e+01 -2.246889e+00 -1.520097e+00  4.358442e-15  2.278668e-15
 >      summary(glm.D93)$coefficients[-1,3]
      outcome2      outcome3    treatment2    treatment3
-2.246889e+00 -1.520097e+00  4.358442e-15  2.278668e-15

Taka Matzmoto wrote:
> Hello R users
> I like to extract z values for x1 and x2. I know how to extract coefficents 
> using model$coef
> but I don't know how to extract z values for each of independent variable. I 
> looked around
> using names(model) but I couldn't find how to extract z values.
> 
> Any help would be appreciated.
> 
> Thanks
> 
> TM
> 
> #########################################################
>> summary(model)
> 
> Call:
> glm(formula = y ~ x1+ x2, family = binomial)
> 
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -2.1397  -1.2357   0.6875   0.8517   1.5743
> 
> Coefficients:
>               Estimate     Std. Error   z value   Pr(>|z|)
> (Intercept) -0.63930    1.13045  -0.566    0.572
> x1              0.69956    0.09459   7.396 1.40e-13 ***
> x2              1.51389    1.13212   1.337    0.181
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>     Null deviance: 1214.9  on 999  degrees of freedom
> Residual deviance: 1149.8  on 997  degrees of freedom
> AIC: 1155.8
> 
> Number of Fisher Scoring iterations: 4
> ############################################################
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From statistical.model at googlemail.com  Sun Jan 29 09:52:35 2006
From: statistical.model at googlemail.com (statistical.model@googlemail.com)
Date: Sun, 29 Jan 2006 08:52:35 -0000
Subject: [R] homogeneity index
In-Reply-To: <43DBFE1E.1000706@siol.net>
Message-ID: <EMEELGDEKHMIAKDGLCDCAEBACKAA.Statistical.model@gmail.com>

Hi,
i would like to have an homogeneity index to compare the homogeneity of
matrices like:

     1  2  3  4  5  6  7  8  9 10 11 12
  1     5 11 12  6 21 14 13  8 15 12 12
  2       12 12  9  8 11 14 15 12 17 18
  3          10 13 18 14 16  9 14  8 12
  4             12 14 11  7 17 12 14  9
  5                11 12 12  9 19 15 13
  6                    9  8 13 12 13  6
  7                      11 15  9 11 16
  8                         15 10 14 14
  9                            15  9 14
  10                               3 12
  11                                 13
  12
These are treatments in a factorial design. My aim is to get a summary index
for the balance property and to compare this score across different
matrices.

I would like to have a pure number in order to get a score non affected by
the number of elements in the matrix and their magnitude. I would like an
index ranging between 0 and 1.
What would you suggest? any index already written in R?

thanks in advance

Roberto Furlan
University of Torino



----------------------------------------
La mia Cartella di Posta in Arrivo e protetta con SPAMfighter
192 messaggi contenenti spam sono stati bloccati con successo.
Scarica gratuitamente SPAMfighter!



From ripley at stats.ox.ac.uk  Sun Jan 29 10:25:45 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 29 Jan 2006 09:25:45 +0000 (GMT)
Subject: [R] extracting 'Z' value from a glm result
In-Reply-To: <43DC7630.5050304@optonline.net>
References: <BAY110-F265923E88D69E9AEC3B328C7160@phx.gbl>
	<43DC7630.5050304@optonline.net>
Message-ID: <Pine.LNX.4.61.0601290918370.27578@gannet.stats>

Can I suggest rather using

> coef(summary(model))[, 3]

?  It has the same effect for glm fits, but is more widely applicable.

Using the standard extractor functions can avoid some nasty surprises:
for example, summary.nls() has no documentation in 2.2.1 and uses
'parameters' rather than 'coefficients'.

On Sun, 29 Jan 2006, Chuck Cleland wrote:

> summary(model)$coefficients[,3]
>
> or
>
> summary(model)$coefficients[-1,3]
>
> For example:
>
> > counts <- c(18,17,15,20,10,20,25,13,12)
> >      outcome <- gl(3,1,9)
> >      treatment <- gl(3,3)
> > glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
> > summary(glm.D93)
>
> Call:
> glm(formula = counts ~ outcome + treatment, family = poisson())
>
> Deviance Residuals:
>        1         2         3         4         5         6         7
> -0.67125   0.96272  -0.16965  -0.21999  -0.95552   1.04939   0.84715
>        8         9
> -0.09167  -0.96656
>
> Coefficients:
>               Estimate Std. Error  z value Pr(>|z|)
> (Intercept)  3.045e+00  1.709e-01   17.815   <2e-16 ***
> outcome2    -4.543e-01  2.022e-01   -2.247   0.0246 *
> outcome3    -2.930e-01  1.927e-01   -1.520   0.1285
> treatment2   8.717e-16  2.000e-01 4.36e-15   1.0000
> treatment3   4.557e-16  2.000e-01 2.28e-15   1.0000
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> (Dispersion parameter for poisson family taken to be 1)
>
>     Null deviance: 10.5814  on 8  degrees of freedom
> Residual deviance:  5.1291  on 4  degrees of freedom
> AIC: 56.761
>
> Number of Fisher Scoring iterations: 4
>
> >      summary(glm.D93)$coefficients[,3]
>   (Intercept)      outcome2      outcome3    treatment2    treatment3
>  1.781478e+01 -2.246889e+00 -1.520097e+00  4.358442e-15  2.278668e-15
> >      summary(glm.D93)$coefficients[-1,3]
>      outcome2      outcome3    treatment2    treatment3
> -2.246889e+00 -1.520097e+00  4.358442e-15  2.278668e-15
>
> Taka Matzmoto wrote:
>> Hello R users
>> I like to extract z values for x1 and x2. I know how to extract coefficents
>> using model$coef
>> but I don't know how to extract z values for each of independent variable. I
>> looked around
>> using names(model) but I couldn't find how to extract z values.
>>
>> Any help would be appreciated.
>>
>> Thanks
>>
>> TM
>>
>> #########################################################
>>> summary(model)
>>
>> Call:
>> glm(formula = y ~ x1+ x2, family = binomial)
>>
>> Deviance Residuals:
>>     Min       1Q   Median       3Q      Max
>> -2.1397  -1.2357   0.6875   0.8517   1.5743
>>
>> Coefficients:
>>               Estimate     Std. Error   z value   Pr(>|z|)
>> (Intercept) -0.63930    1.13045  -0.566    0.572
>> x1              0.69956    0.09459   7.396 1.40e-13 ***
>> x2              1.51389    1.13212   1.337    0.181
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> (Dispersion parameter for binomial family taken to be 1)
>>
>>     Null deviance: 1214.9  on 999  degrees of freedom
>> Residual deviance: 1149.8  on 997  degrees of freedom
>> AIC: 1155.8
>>
>> Number of Fisher Scoring iterations: 4
>> ############################################################
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Soren.Hojsgaard at agrsci.dk  Sun Jan 29 13:17:01 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sun, 29 Jan 2006 13:17:01 +0100
Subject: [R] how calculation degrees freedom
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com><40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com><C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk><40e66e0b0601271540l541c881dk126eb4a04bd04f4d@mail.gmail.com>
	<x2oe1xfbw4.fsf@turmalin.kubism.ku.dk>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03878181@DJFPOST01.djf.agrsci.dk>

In connection with calculating Monte Carlo p-values based on sampled data sets: The calculations involve something like
   update(lmer.model, data=newdata)
where newdata is a simulated dataset comming from simulate(lmer.model). I guess the update could be faster if one could supply the update function with the parameter estimates from the original fit of the lmer.model as starting values. Is this possible to achieve??
Best
S??ren

________________________________

Fra: pd at pubhealth.ku.dk p?? vegne af Peter Dalgaard
Sendt: l?? 28-01-2006 01:12
Til: Douglas Bates
Cc: S??ren H??jsgaard; R-help at stat.math.ethz.ch
Emne: Re: [R] how calculation degrees freedom



Douglas Bates <dmbates at gmail.com> writes:


> > Of course, Monte Carlo p-values have their problems, but the world
> > is not perfect....
>
> Another approach is to use mcmcsamp to derive a sample from the
> posterior distribution of the parameters using Markov Chain Monte
> Carlo sampling.  If you are interested in intervals rather than
> p-values the HPDinterval function from the coda package can create
> those.
>

We (S??ren and I) actually had a look at that, and it seems not to
solve the problem. Rather, mcmcsamp tends to reproduce the Wald style
inference (infinite DF) if you use a suitably vague prior.

It's a bit hard to understand clearly, but I think the crux is that
any Bayes inference only depends on data through the likelihood
function. The distribution of the likelihood never enters (the
hardcore Bayesian of course won't care). However, the nature of DF
corrections is that the LRT does not have its asymptotic distribution,
and mcmc has no way of picking that up.


--
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dieter.menne at menne-biomed.de  Sun Jan 29 15:31:14 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Sun, 29 Jan 2006 14:31:14 +0000 (UTC)
Subject: [R] Selecting Random Subset From Matrix - retaining indices
References: <Pine.LNX.4.44.0601281827120.8722-100000@gw.env.leeds.ac.uk>
Message-ID: <loom.20060129T152951-317@post.gmane.org>

Laura Quinn <laura <at> env.leeds.ac.uk> writes:

> I was wondering whether there is a way to select random samples from a
> data matrix, retaining the indexing for the rows and columns? 

Is this you are looking for? Dieter


# create data
n = 10
world = array(rnorm(n*n*n),c(n,n,n))
nsamples=12
# create indices
isample = matrix(sample(n,3*nsamples, replace=T),nrow=nsamples)
# use indexes to pick world samples
world[rbind(isample)]



From islandboy1982 at yahoo.com  Sun Jan 29 16:26:20 2006
From: islandboy1982 at yahoo.com (oliver wee)
Date: Sun, 29 Jan 2006 07:26:20 -0800 (PST)
Subject: [R] help with read.table() function
Message-ID: <20060129152620.65089.qmail@web33210.mail.mud.yahoo.com>

hello, I have just started using R for doing a project
in time series...

unfortunately, I am having trouble using the
read.table function for use in reading my data set.

This is what I'm getting:
I inputted:
data <-
read.table("D:/Oliver/Professional/Studies/Time Series
Analysis/spdc2693.data", header = TRUE)

I got:
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file 'D:/Oliver/Professional/Studies/Time
Series Analysis/spdc2693.data', reason 'No such file
or directory'

as I am just a novice programmer, I really would
appreciate help from you guys. Is there a need to
setpath in R, like in java or something like that...

I am using the windows version btw. 

I have also tried to put the file in the work
directory of R, so that I only typed 
data <- read.table("spdc2693.data", header = TRUE)
Again, it won't work, with the same error message.

I would appreciate any help. thanks again.



From francoisromain at free.fr  Sun Jan 29 16:52:04 2006
From: francoisromain at free.fr (Romain Francois)
Date: Sun, 29 Jan 2006 16:52:04 +0100
Subject: [R] help with read.table() function
In-Reply-To: <20060129152620.65089.qmail@web33210.mail.mud.yahoo.com>
References: <20060129152620.65089.qmail@web33210.mail.mud.yahoo.com>
Message-ID: <43DCE4A4.4090703@free.fr>

Le 29.01.2006 16:26, oliver wee a ??crit :

>hello, I have just started using R for doing a project
>in time series...
>
>unfortunately, I am having trouble using the
>read.table function for use in reading my data set.
>
>This is what I'm getting:
>I inputted:
>data <-
>read.table("D:/Oliver/Professional/Studies/Time Series
>Analysis/spdc2693.data", header = TRUE)
>
>I got:
>Error in file(file, "r") : unable to open connection
>In addition: Warning message:
>cannot open file 'D:/Oliver/Professional/Studies/Time
>Series Analysis/spdc2693.data', reason 'No such file
>or directory'
>
>as I am just a novice programmer, I really would
>appreciate help from you guys. Is there a need to
>setpath in R, like in java or something like that...
>
>I am using the windows version btw. 
>
>I have also tried to put the file in the work
>directory of R, so that I only typed 
>data <- read.table("spdc2693.data", header = TRUE)
>Again, it won't work, with the same error message.
>
>I would appreciate any help. thanks again.
>  
>
Hi, try :

read.table(file.choose(), header=TRUE)

and go to your file.
Also, you can look a ?setwd, ?getwd

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
mixmod 1.7 is released : http://www-math.univ-fcomte.fr/mixmod/index.php
+---------------------------------------------------------------+
| Romain FRANCOIS - http://francoisromain.free.fr               |
| Doctorant INRIA Futurs / EDF                                  |
+---------------------------------------------------------------+



From wpogoda at coba.usf.edu  Sun Jan 29 17:25:36 2006
From: wpogoda at coba.usf.edu (Pogoda, Wendy)
Date: Sun, 29 Jan 2006 11:25:36 -0500
Subject: [R] line numbers
Message-ID: <AC6230A5F7852248946CFC481739A95E37AD3F@tiki.fastmail.usf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/e4eb65ff/attachment.pl

From islandboy1982 at yahoo.com  Sun Jan 29 17:34:12 2006
From: islandboy1982 at yahoo.com (oliver wee)
Date: Sun, 29 Jan 2006 08:34:12 -0800 (PST)
Subject: [R] additional error on read.table function
Message-ID: <20060129163412.2714.qmail@web33205.mail.mud.yahoo.com>

hi,

thanks to the feedback of some people I was able to
solve my problem of reading data using the read.table
function by using the file.choose function inside the
method of the read.table function. 

Unfortunately, I encountered a new error message after
I chose my file.
After I unputted 
data = read.table(file.choose(), header = TRUE)

I got an error saying:

Error in scan(file = file, what = what, sep = sep,
quote = quote, dec = dec,  : 
        line 1 did not have 11 elements
In addition: Warning message:
incomplete final line found by readTableHeader on
'D:\Oliver\Professional\Studies\Time Series
Analysis\spdc2693.data.txt' 

my time series data looks like this...

------------
Standard and Poor's 500 Index closing values from 1926
to 1993.

  Date       Index
  260101     12.76
  260108     12.78
  260115     12.52
  260122     12.45
  260129     12.74
  260205     12.87
  260212     12.87
  260219     12.74
  260226     12.18
  260305     11.99
  260312     12.15
  260319     11.64
  260326     11.46
...
(and so on)
----------

Should I insert additional attributes besides header =
TRUE? 

thanks.



From ggrothendieck at gmail.com  Sun Jan 29 17:36:46 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 11:36:46 -0500
Subject: [R] line numbers
In-Reply-To: <AC6230A5F7852248946CFC481739A95E37AD3F@tiki.fastmail.usf.edu>
References: <AC6230A5F7852248946CFC481739A95E37AD3F@tiki.fastmail.usf.edu>
Message-ID: <971536df0601290836l321507cewae7e41bdd1370502@mail.gmail.com>

Check out ?sprintf, ?cat, ?noquote, ?format, ?formatC, ?print.

Also maybe ?sub, ?gsub, ?chartr, ?grep.

To view the code that prints summary(lm(...whatever...)), put this on
a line by itself:

  stats:::print.summary.lm

The summary statement mentioned above outputs an object of
class "summary.lm" and the S3 method to print summary.lm
objects is called print.summary.lm and that is contained in the
stats namespace.

On 1/29/06, Pogoda, Wendy <wpogoda at coba.usf.edu> wrote:
> I am using the sink function to save several results (i.e. values of many different variables) to an output file.  However, the output looks unattractive because it displays line numbers next to each new variable, it is difficult to remove the R variable name from the output, and I cannot print test without it using quotation marks.  The only way I have found around most of these issues is to create a data.frame such as:
>
> data.frame("Minimum BMD"=c(minimumBMD,minimumBMD.time,paste(100*(1-Climit.level),"%"),
>
> minimumBMD.ConfidenceLimit,
>
> if(CI.2sided)paste(minimumBMD.ConfidenceLimit.upper),paste(100*alpha.BMR,"%")),
>
> row.names=c("Minimum BMD",time.name,"Confidence Level","BMDL (Lower Conf. Limit)",
>
> if(CI.2sided)paste("BMDU (Upper Conf. Limit)"),"BMR Level"))
>
> However, the output will look like:
>                                Minimum.BMD
> Minimum BMD              0.0280273960783465
> Test Time                             28.56
> Confidence Level                       95 %
> BMDL (Lower Conf. Limit) 0.0178262627224170
> BMR Level                               5 %
>
> and I would rather not have the words "Minimum.BMD" in there at all.  Plus I would like to add addition lines of text and other values.  Is there a way I can format the output attractively?  I know this must be possible because many functions (e.g. summary(lm(y~x))) will display output nicely and include text.
>
> Thank you for your time.  If you would like to e-mail me directly, my e-mail address is wpogoda at coba.usf.edu.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Sun Jan 29 17:38:25 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 11:38:25 -0500
Subject: [R] additional error on read.table function
In-Reply-To: <20060129163412.2714.qmail@web33205.mail.mud.yahoo.com>
References: <20060129163412.2714.qmail@web33205.mail.mud.yahoo.com>
Message-ID: <971536df0601290838n46a82481kf31a1e37ec6073fa@mail.gmail.com>

You can use the skip= argument to skip over the indicated
number of lines of junk at the beginning of your file.  See
?read.table

On 1/29/06, oliver wee <islandboy1982 at yahoo.com> wrote:
> hi,
>
> thanks to the feedback of some people I was able to
> solve my problem of reading data using the read.table
> function by using the file.choose function inside the
> method of the read.table function.
>
> Unfortunately, I encountered a new error message after
> I chose my file.
> After I unputted
> data = read.table(file.choose(), header = TRUE)
>
> I got an error saying:
>
> Error in scan(file = file, what = what, sep = sep,
> quote = quote, dec = dec,  :
>        line 1 did not have 11 elements
> In addition: Warning message:
> incomplete final line found by readTableHeader on
> 'D:\Oliver\Professional\Studies\Time Series
> Analysis\spdc2693.data.txt'
>
> my time series data looks like this...
>
> ------------
> Standard and Poor's 500 Index closing values from 1926
> to 1993.
>
>  Date       Index
>  260101     12.76
>  260108     12.78
>  260115     12.52
>  260122     12.45
>  260129     12.74
>  260205     12.87
>  260212     12.87
>  260219     12.74
>  260226     12.18
>  260305     11.99
>  260312     12.15
>  260319     11.64
>  260326     11.46
> ...
> (and so on)
> ----------
>
> Should I insert additional attributes besides header =
> TRUE?
>
> thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Sun Jan 29 17:51:37 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jan 2006 17:51:37 +0100
Subject: [R] help with read.table() function
In-Reply-To: <43DCE4A4.4090703@free.fr>
References: <20060129152620.65089.qmail@web33210.mail.mud.yahoo.com>
	<43DCE4A4.4090703@free.fr>
Message-ID: <x2d5ibf04m.fsf@turmalin.kubism.ku.dk>

Romain Francois <francoisromain at free.fr> writes:

> Le 29.01.2006 16:26, oliver wee a ??crit :
> 
> >hello, I have just started using R for doing a project
> >in time series...
> >
> >unfortunately, I am having trouble using the
> >read.table function for use in reading my data set.
> >
> >This is what I'm getting:
> >I inputted:
> >data <-
> >read.table("D:/Oliver/Professional/Studies/Time Series
> >Analysis/spdc2693.data", header = TRUE)
> >
> >I got:
> >Error in file(file, "r") : unable to open connection
> >In addition: Warning message:
> >cannot open file 'D:/Oliver/Professional/Studies/Time
> >Series Analysis/spdc2693.data', reason 'No such file
> >or directory'
> >
> >as I am just a novice programmer, I really would
> >appreciate help from you guys. Is there a need to
> >setpath in R, like in java or something like that...
> >
> >I am using the windows version btw. 
> >
> >I have also tried to put the file in the work
> >directory of R, so that I only typed 
> >data <- read.table("spdc2693.data", header = TRUE)
> >Again, it won't work, with the same error message.
> >
> >I would appreciate any help. thanks again.
> >  
> >
> Hi, try :
> 
> read.table(file.choose(), header=TRUE)
> 
> and go to your file.
> Also, you can look a ?setwd, ?getwd

Right. Or just file.choose() and see what the OS thinks your file is
really called. The most common causes for symptoms like that are

(A) The file is "spcd2693.data"
(B) There's an extra extension which ever helpful Windows decided to
hide, as in "spdc2693.data.txt".


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Sun Jan 29 18:14:16 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 29 Jan 2006 12:14:16 -0500
Subject: [R] help with read.table() function
In-Reply-To: <20060129162850.73978.qmail@web33204.mail.mud.yahoo.com>
References: <20060129162850.73978.qmail@web33204.mail.mud.yahoo.com>
Message-ID: <43DCF7E8.2080900@stats.uwo.ca>

On 1/29/2006 11:28 AM, oliver wee wrote:
> hi, 
> 
> Sorry again to bother you, but I got the file.choose()
> to work. Thanks for the help there.
> 
> Unfortunately I encountered a new problem. After I
> selected the data, I got this error message:
> 
> Error in scan(file = file, what = what, sep = sep,
> quote = quote, dec = dec,  : 
>         line 1 did not have 11 elements
> In addition: Warning message:
> incomplete final line found by readTableHeader on
> 'D:\Oliver\Professional\Studies\Time Series
> Analysis\spdc2693.data.txt' 
> 
> my time series data looks like this...
> 
> ------------
> Standard and Poor's 500 Index closing values from 1926
> to 1993.
> 
>   Date       Index
>   260101     12.76
>   260108     12.78
>   260115     12.52
>   260122     12.45
>   260129     12.74
>   260205     12.87
>   260212     12.87
>   260219     12.74
>   260226     12.18
>   260305     11.99
>   260312     12.15
>   260319     11.64
>   260326     11.46
> ...
> (and so on)
> ----------
> 
> Should I insert additional attributes besides header =
> TRUE? 

Yes, you need to tell it to skip over the lines of the comment at the 
start of the file.  That looks like 3 lines (including the blank line), 
so add skip=3 to your read.table call.

Duncan Murdoch

> thanks.
> 
> 
> --- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> 
>> On 1/29/2006 10:26 AM, oliver wee wrote:
>>> hello, I have just started using R for doing a
>> project
>>> in time series...
>>>
>>> unfortunately, I am having trouble using the
>>> read.table function for use in reading my data
>> set.
>>> This is what I'm getting:
>>> I inputted:
>>> data <-
>>> read.table("D:/Oliver/Professional/Studies/Time
>> Series
>>> Analysis/spdc2693.data", header = TRUE)
>> Generally it's easier to use the dialogs to specify
>> the filename, e.g.
>>
>> read.table(file.choose(), header=TRUE)
>>
>> Then you shouldn't get the "no such file" message. 
>> If you do, you 
>> should check whether other programs (e.g. notepad)
>> can open the file. 
>> Maybe you don't have read permission?
>>
>> Duncan Murdoch
>>
>>> I got:
>>> Error in file(file, "r") : unable to open
>> connection
>>> In addition: Warning message:
>>> cannot open file
>> 'D:/Oliver/Professional/Studies/Time
>>> Series Analysis/spdc2693.data', reason 'No such
>> file
>>> or directory'
>>>
>>> as I am just a novice programmer, I really would
>>> appreciate help from you guys. Is there a need to
>>> setpath in R, like in java or something like
>> that...
>>> I am using the windows version btw. 
>>>
>>> I have also tried to put the file in the work
>>> directory of R, so that I only typed 
>>> data <- read.table("spdc2693.data", header = TRUE)
>>> Again, it won't work, with the same error message.
>>>
>>> I would appreciate any help. thanks again.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> __________________________________________________
> Do You Yahoo!?
> Tired of spam?  Yahoo! Mail has the best spam protection around 
> http://mail.yahoo.com



From ggrothendieck at gmail.com  Sun Jan 29 19:24:15 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 13:24:15 -0500
Subject: [R] help with read.table() function
In-Reply-To: <43DCF7E8.2080900@stats.uwo.ca>
References: <20060129162850.73978.qmail@web33204.mail.mud.yahoo.com>
	<43DCF7E8.2080900@stats.uwo.ca>
Message-ID: <971536df0601291024n57113e97t3cc938a78a11f495@mail.gmail.com>

Normally one expects stdin to be the default on command line
programs and something like file.choose to be the default on GUI
programs and this would break that expectation.

If there were a GUI version of read.table then that would reasonbly
have file.choose as the default.

On 1/29/06, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 1/29/2006 11:28 AM, oliver wee wrote:
> > hi,
> >
> > Sorry again to bother you, but I got the file.choose()
> > to work. Thanks for the help there.
> >
> > Unfortunately I encountered a new problem. After I
> > selected the data, I got this error message:
> >
> > Error in scan(file = file, what = what, sep = sep,
> > quote = quote, dec = dec,  :
> >         line 1 did not have 11 elements
> > In addition: Warning message:
> > incomplete final line found by readTableHeader on
> > 'D:\Oliver\Professional\Studies\Time Series
> > Analysis\spdc2693.data.txt'
> >
> > my time series data looks like this...
> >
> > ------------
> > Standard and Poor's 500 Index closing values from 1926
> > to 1993.
> >
> >   Date       Index
> >   260101     12.76
> >   260108     12.78
> >   260115     12.52
> >   260122     12.45
> >   260129     12.74
> >   260205     12.87
> >   260212     12.87
> >   260219     12.74
> >   260226     12.18
> >   260305     11.99
> >   260312     12.15
> >   260319     11.64
> >   260326     11.46
> > ...
> > (and so on)
> > ----------
> >
> > Should I insert additional attributes besides header =
> > TRUE?
>
> Yes, you need to tell it to skip over the lines of the comment at the
> start of the file.  That looks like 3 lines (including the blank line),
> so add skip=3 to your read.table call.
>
> Duncan Murdoch
>
> > thanks.
> >
> >
> > --- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> >
> >> On 1/29/2006 10:26 AM, oliver wee wrote:
> >>> hello, I have just started using R for doing a
> >> project
> >>> in time series...
> >>>
> >>> unfortunately, I am having trouble using the
> >>> read.table function for use in reading my data
> >> set.
> >>> This is what I'm getting:
> >>> I inputted:
> >>> data <-
> >>> read.table("D:/Oliver/Professional/Studies/Time
> >> Series
> >>> Analysis/spdc2693.data", header = TRUE)
> >> Generally it's easier to use the dialogs to specify
> >> the filename, e.g.
> >>
> >> read.table(file.choose(), header=TRUE)
> >>
> >> Then you shouldn't get the "no such file" message.
> >> If you do, you
> >> should check whether other programs (e.g. notepad)
> >> can open the file.
> >> Maybe you don't have read permission?
> >>
> >> Duncan Murdoch
> >>
> >>> I got:
> >>> Error in file(file, "r") : unable to open
> >> connection
> >>> In addition: Warning message:
> >>> cannot open file
> >> 'D:/Oliver/Professional/Studies/Time
> >>> Series Analysis/spdc2693.data', reason 'No such
> >> file
> >>> or directory'
> >>>
> >>> as I am just a novice programmer, I really would
> >>> appreciate help from you guys. Is there a need to
> >>> setpath in R, like in java or something like
> >> that...
> >>> I am using the windows version btw.
> >>>
> >>> I have also tried to put the file in the work
> >>> directory of R, so that I only typed
> >>> data <- read.table("spdc2693.data", header = TRUE)
> >>> Again, it won't work, with the same error message.
> >>>
> >>> I would appreciate any help. thanks again.
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> >
> > __________________________________________________
> > Do You Yahoo!?
> > Tired of spam?  Yahoo! Mail has the best spam protection around
> > http://mail.yahoo.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sun Jan 29 20:01:19 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 29 Jan 2006 11:01:19 -0800
Subject: [R] Question about Aggregate
In-Reply-To: <8a83e5000601250238h51210914ud4545cc8a9d3d93c@mail.gmail.com>
References: <8a83e5000601250238h51210914ud4545cc8a9d3d93c@mail.gmail.com>
Message-ID: <43DD10FF.1080501@pdf.com>

	  Did you try 'RSiteSearch("aggregate time series")'?  When I did this 
just now, the third hit was, 
"http://finzi.psych.upenn.edu/R/Rhelp02a/archive/55761.html", which 
might help you.  See in particular the "zoo" package and vignette, and 
especially the "aggregate.zoo" function, including the examples with the 
documentation.

	  If you'd like to submit another question to this listserver, PLEASE 
do read the posting guide! "www.R-project.org/posting-guide.html". 
People who follow more closely the procedure outlined there generally 
get better answers quicker, I believe.

	  hope this helps,
	  spencer graves

Matthieu Cornec wrote:

> hello,
> 
> Suppose you a monthly series you want to aggregate at a quaterly frequency
> with the start and the end of your series in the middle of the quarter.
> For example
>  2001M2 2001M3 2001M4 2001M5 2001M6 2001M7
> 12                  13     12      14         16              15
> 
> how can you get something like :
> 2001Q1 2001Q2 200Q3
> NA       14 NA
> 
> or
>  2001Q1 2001Q2 200Q3
> 12.5 14 15
> 
> Thanks in advance
> 
> Matthieu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From martin.chlond at btinternet.com  Sun Jan 29 21:59:30 2006
From: martin.chlond at btinternet.com (Martin)
Date: Sun, 29 Jan 2006 20:59:30 -0000
Subject: [R] Logit regression using MLE
Message-ID: <000001c62516$f6dcc6d0$2a9d9156@OWNER2TYZC0SV7>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/81c43934/attachment.pl

From Scott.Williams at petermac.org  Sun Jan 29 23:06:20 2006
From: Scott.Williams at petermac.org (Williams Scott)
Date: Mon, 30 Jan 2006 09:06:20 +1100
Subject: [R] actuarial prevalence plots
Message-ID: <46B75B4A4A45914ABB0901364EFF4A20261878@PMC-EMAIL.petermac.org.au>


Hi all,

I am trying to produce a series of plots showing the prevalence of a
condition, which is subject to censoring. In most cases the condition is
temporary and resolves with time. I would like to use the method of Pepe
et al Stat Med 1991; 413-421 - essentially the prevalence is the
Kaplan-Meier prob[having the condition at time t] - KM prob[recovery by
time t] (also divided by 1-KM[death by t], although death is not an
issue with this data).

I can easily produce the relevant actuarial data for either the
condition or recovery using survfit(eg survfit_cond$time ,
survfit_cond$surv, survfit_rec$time, survfit_rec$surv). I then have to
calculate (survfit_cond$surv-survfit_rec$surv) at each event time point.
Can anyone help me with an easy method to implement this? Or suggest an
easier method? I cant find a similar method after searching the
contributed packages (it doesn't appear to fit a recurrent events
problem). I have code for manual KM calculations, but the only method my
basic programming skills come up with seems tedious.

Thanks in advance

_____________________________

 

Dr. Scott Williams

Peter MacCallum Cancer Centre

Melbourne, Australia



From comtech.usa at gmail.com  Sun Jan 29 23:11:30 2006
From: comtech.usa at gmail.com (Michael)
Date: Sun, 29 Jan 2006 14:11:30 -0800
Subject: [R] SoS! How to predict new values using linear regression models?
Message-ID: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/139c3c0a/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Jan 29 23:17:32 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jan 2006 23:17:32 +0100
Subject: [R] Logit regression using MLE
In-Reply-To: <000001c62516$f6dcc6d0$2a9d9156@OWNER2TYZC0SV7>
References: <000001c62516$f6dcc6d0$2a9d9156@OWNER2TYZC0SV7>
Message-ID: <x24q3mfzlv.fsf@turmalin.kubism.ku.dk>

"Martin" <martin.chlond at btinternet.com> writes:

> I have used the following code to obtain a max likelihood estimator for
> a logit regression. The final command invokes ?optim? to obtain the
> parameter estimates. The code works OK but I want to use the ?mle?
> function in the ?stats4? package instead of directly calling ?optim?.
> Can someone please figure out the command to do this?
> 

You need to make sure that the loglikelihood you pass to mle is a
function of the parameters only, and you have to string out the
parameter vector:

ll <- eval(function(beta0=0,beta1=0)
             log.lo.like (c(beta0,beta1),Y,X),
           list(X=X,Y=Y))


summary(mle(ll))

glm(Y~X-1,family=binomial)

  
> 
> Thank you in advance.
> 
>  
> 
> Martin 
> 
>  
> 
> # mlelo.r - maximum likelihood estimation for logit regression
> 
>  
> 
> # log-likelihood function (returns negative to minimise)
> 
> log.lo.like <- function(beta,Y,X) {
> 
>   Fbetax   <- 1/(1+exp(-beta%*%t(X)))
> 
>   loglbeta <- -log(prod(Fbetax^Y*(1-Fbetax)^(1-Y)))
> 
> }
> 
>  
> 
> # data
> 
> Y <-  c(0,0,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,1)
> 
> X <-
> cbind(matrix(1,21,1),matrix(c(-48.5,24.4,82.8,-24.6,-31.6,91.0,52.1,-87.7,
> -17.0,-51.5,-90.7,65.5,-44.0,-7.0,51.6,32.4,-61.8,34.0,27.9,-72.9,49.9),
> 21,1))
> 
>  
> 
> # maximum likelihood estimator
> 
> mle.res <- optim(c(0,0),log.lo.like,Y=Y, X=X)
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Sun Jan 29 23:28:29 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 17:28:29 -0500
Subject: [R] SoS! How to predict new values using linear regression
	models?
In-Reply-To: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com>
References: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com>
Message-ID: <971536df0601291428t536b0297y1664c3d3a0a76e3d@mail.gmail.com>

Leaving aside the issue of whether linear regression is appropriate here,
do it like this where I have used the builtin iris data frame since I don't have
access to your ss:

iris.lm <- lm(as.numeric(Species) ~ Sepal.Length + Sepal.Width, iris)
predict(iris.lm, data.frame(Sepal.Length = 3, Sepal.Width = 2))

On 1/29/06, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> After trial and error by myself for a few hours, I decide to ask for your
> help.
>
> I have a training set which is a matrix of size 200 x 2, where the two
> columns denote each independent variable. I have 200 observations.
>
> -----------------
> ss=data.frame(trainingSet);
> result=lm(trainingClass~ss$X1+ss$X2);
> -----------------
>
> where trainingClass denotes the true classes of the training data.
>
> Now I want to apply the model to predict new data:
>
> -----------------
> > gg=predict(result, data.frame(X1=1, X2=2))
> Warning message:
> 'newdata' had 1 rows but variable(s) found have 200 rows
> -----------------
>
> That's to say, I provide a new data which is one observation of 2
> independent variables(1 row, two columns). I converted it into data frame.
>
> However, the R never gives me new predication value for this NEW ONE
> observation. Instead, it keeps giving me the above warning and keeps
> printing the fitted value for the 200 training samples...
>
> That's very bad.
>
> Please help me!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mobygeek at yahoo.com  Mon Jan 30 02:34:40 2006
From: mobygeek at yahoo.com (context grey)
Date: Sun, 29 Jan 2006 17:34:40 -0800 (PST)
Subject: [R] beginner Q:  hashtable or dictionary?
Message-ID: <20060130013440.2713.qmail@web51404.mail.yahoo.com>

Hi,

Is there something like a hashtable or (python)
dictionary in R/Splus?

(If not, is there a reason why it's not needed /
typical way to accomplish the same thing?)

Thank you



From jholtman at gmail.com  Mon Jan 30 02:38:49 2006
From: jholtman at gmail.com (jim holtman)
Date: Sun, 29 Jan 2006 20:38:49 -0500
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
Message-ID: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/8bf59523/attachment.pl

From hodgess at gator.dt.uh.edu  Mon Jan 30 03:42:21 2006
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sun, 29 Jan 2006 20:42:21 -0600
Subject: [R]  problem with mirror selection
Message-ID: <200601300242.k0U2gLuC005877@gator.dt.uh.edu>

Dear R People:

I'm having trouble with selecting a mirror to download packages.

When I select the location, R just sits.  When I try to cancel
the program, it never quits.

Has anyone else run into this, please?  It only happens on my
laptop, not my desktop.  My inclination is to say that it might be
a problem with a firewall, but I thought I would check here as
well.



R Version 2.2.1 Windows.

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From christos at nuverabio.com  Mon Jan 30 03:47:24 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Sun, 29 Jan 2006 21:47:24 -0500
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
Message-ID: <002c01c62547$83775760$0202a8c0@headquarters>

I was wondering if there is an easy way to extend this to implement a 2-D
hash, i.e. 2-way indexing? 

> x[["a"]][["b"]] <- "something"

Thanks.

Christos Hatzis

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jim holtman
Sent: Sunday, January 29, 2006 8:39 PM
To: context grey
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] beginner Q: hashtable or dictionary?

use a 'list':


> x <- list()
> x[['test']] <- 64
> x[['next one']] <- c(1,2,3,4)
> x
$test
[1] 64

$"next one"
[1] 1 2 3 4

> x[['test']]
[1] 64
>



On 1/29/06, context grey <mobygeek at yahoo.com> wrote:
>
> Hi,
>
> Is there something like a hashtable or (python) dictionary in R/Splus?
>
> (If not, is there a reason why it's not needed / typical way to 
> accomplish the same thing?)
>
> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



--
Jim Holtman
Cincinnati, OH
+1 513 247 0281

What the problem you are trying to solve?

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Mon Jan 30 04:32:40 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 22:32:40 -0500
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <002c01c62547$83775760$0202a8c0@headquarters>
References: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<002c01c62547$83775760$0202a8c0@headquarters>
Message-ID: <971536df0601291932l14d9c8e1nc7eb03ed95d54637@mail.gmail.com>

Make a list of lists:

L <- list(a = list(a = 1, b = 2), b = list(a = 3, b = 4))
L[["a"]][["b"]]
L$a$b

On 1/29/06, Christos Hatzis <christos at nuverabio.com> wrote:
> I was wondering if there is an easy way to extend this to implement a 2-D
> hash, i.e. 2-way indexing?
>
> > x[["a"]][["b"]] <- "something"
>
> Thanks.
>
> Christos Hatzis
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jim holtman
> Sent: Sunday, January 29, 2006 8:39 PM
> To: context grey
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] beginner Q: hashtable or dictionary?
>
> use a 'list':
>
>
> > x <- list()
> > x[['test']] <- 64
> > x[['next one']] <- c(1,2,3,4)
> > x
> $test
> [1] 64
>
> $"next one"
> [1] 1 2 3 4
>
> > x[['test']]
> [1] 64
> >
>
>
>
> On 1/29/06, context grey <mobygeek at yahoo.com> wrote:
> >
> > Hi,
> >
> > Is there something like a hashtable or (python) dictionary in R/Splus?
> >
> > (If not, is there a reason why it's not needed / typical way to
> > accomplish the same thing?)
> >
> > Thank you
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From h.wickham at gmail.com  Mon Jan 30 04:42:03 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 29 Jan 2006 21:42:03 -0600
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
Message-ID: <f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>

> use a 'list':

Is a list O(1) for setting and getting?

Hadley



From comtech.usa at gmail.com  Mon Jan 30 04:49:11 2006
From: comtech.usa at gmail.com (Michael)
Date: Sun, 29 Jan 2006 19:49:11 -0800
Subject: [R] How to add two different axis to one plot?
Message-ID: <b1f16d9d0601291949r23c9dd00ra336e8372c7b3a8d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/51a86263/attachment.pl

From ggrothendieck at gmail.com  Mon Jan 30 04:54:05 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 29 Jan 2006 22:54:05 -0500
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <971536df0601291932l14d9c8e1nc7eb03ed95d54637@mail.gmail.com>
References: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<002c01c62547$83775760$0202a8c0@headquarters>
	<971536df0601291932l14d9c8e1nc7eb03ed95d54637@mail.gmail.com>
Message-ID: <971536df0601291954k65693cffu3189b50b48229b3d@mail.gmail.com>

One could also use a matrix if appropriate:

> m <- matrix(1:4, 2, dimnames = list(c("a", "b"), c("a", "b")))
> m["a", "b"]
[1] 3

Also in the 1d case one could do this:

v <- c(a = 1, b = 2)
v[["a"]]

On 1/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Make a list of lists:
>
> L <- list(a = list(a = 1, b = 2), b = list(a = 3, b = 4))
> L[["a"]][["b"]]
> L$a$b
>
> On 1/29/06, Christos Hatzis <christos at nuverabio.com> wrote:
> > I was wondering if there is an easy way to extend this to implement a 2-D
> > hash, i.e. 2-way indexing?
> >
> > > x[["a"]][["b"]] <- "something"
> >
> > Thanks.
> >
> > Christos Hatzis
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jim holtman
> > Sent: Sunday, January 29, 2006 8:39 PM
> > To: context grey
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] beginner Q: hashtable or dictionary?
> >
> > use a 'list':
> >
> >
> > > x <- list()
> > > x[['test']] <- 64
> > > x[['next one']] <- c(1,2,3,4)
> > > x
> > $test
> > [1] 64
> >
> > $"next one"
> > [1] 1 2 3 4
> >
> > > x[['test']]
> > [1] 64
> > >
> >
> >
> >
> > On 1/29/06, context grey <mobygeek at yahoo.com> wrote:
> > >
> > > Hi,
> > >
> > > Is there something like a hashtable or (python) dictionary in R/Splus?
> > >
> > > (If not, is there a reason why it's not needed / typical way to
> > > accomplish the same thing?)
> > >
> > > Thank you
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 247 0281
> >
> > What the problem you are trying to solve?
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From A.Robinson at ms.unimelb.edu.au  Mon Jan 30 04:56:56 2006
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 30 Jan 2006 14:56:56 +1100 (EST)
Subject: [R] How to add two different axis to one plot?
In-Reply-To: <b1f16d9d0601291949r23c9dd00ra336e8372c7b3a8d@mail.gmail.com>
References: <b1f16d9d0601291949r23c9dd00ra336e8372c7b3a8d@mail.gmail.com>
Message-ID: <3309.24.61.193.81.1138593416.squirrel@webmail.ms.unimelb.edu.au>

Use the axis() command to add custom axes.

To "hold" the plot, insert

par(new=TRUE)

between the plot statements, thus:

plot(x1, y1, etc ...)
par(new=TRUE)
plot(x2, y2, etc ...)



Andrew

On Mon, January 30, 2006 2:49 pm, Michael said:
> Hi all,
>
> I need to put two different axis to one plot. On the top of the plot, I
> need
> to put one axis, with increments from left side to the right side; then at
> the bottom of the same plot, I need to put another axis, with increments
> from right side to the left side and showing a different unit. How do I do
> that?
>
> By the way, is there a "hold" command for plotting?
>
> If I first plot a picture, how to add another "plot" command onto it
> without
> erasing it?
>
> In matlab, it can be convinient done by "hold on" and "hold off"... and
> can
> I do it in R?
>
> Thanks a lot,
>
> Micheal
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>


Andrew Robinson
Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
Department of Mathematics and Statistics            Fax: +61-3-8344 4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au    Website: http://www.ms.unimelb.edu.au



From comtech.usa at gmail.com  Mon Jan 30 05:44:05 2006
From: comtech.usa at gmail.com (Michael)
Date: Sun, 29 Jan 2006 20:44:05 -0800
Subject: [R] Help! What does this R command mean?
Message-ID: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/d7251ff1/attachment.pl

From comtech.usa at gmail.com  Mon Jan 30 05:48:52 2006
From: comtech.usa at gmail.com (Michael)
Date: Sun, 29 Jan 2006 20:48:52 -0800
Subject: [R] How to add two different axis to one plot?
In-Reply-To: <3309.24.61.193.81.1138593416.squirrel@webmail.ms.unimelb.edu.au>
References: <b1f16d9d0601291949r23c9dd00ra336e8372c7b3a8d@mail.gmail.com>
	<3309.24.61.193.81.1138593416.squirrel@webmail.ms.unimelb.edu.au>
Message-ID: <b1f16d9d0601292048p46aa6bfav66ceea19538c1682@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/180ddf4e/attachment.pl

From maustin at amgen.com  Mon Jan 30 05:59:58 2006
From: maustin at amgen.com (Austin, Matt)
Date: Sun, 29 Jan 2006 20:59:58 -0800
Subject: [R] How to add two different axis to one plot?
Message-ID: <E7D5AB4811D20B489622AABA9C53859109DAD74E@teal-exch.amgen.com>

Using traditional graphics you probably want to look at

?lines, ?points, ?text . . . 

These allow you to add to your plot without calling new axes.

For custom axes:

plot(1:10, 1:10, axes=FALSE)
lines(1:10, 10:1)
points(1:10, runif(10, 2, 5), col=1, pch=21, bg=3, cex=3)
axis(1, at=1:10)
axis(3, at=1:10, lab=10:1)
axis(2)
box()

Look  at ?axis for details.

--Matt


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Michael
Sent: Sunday, January 29, 2006 8:49 PM
To: A.Robinson at ms.unimelb.edu.au
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] How to add two different axis to one plot?


No this is not good.

When I used par(new=TRUE), it does not adjust axis accordingly...

Thus two different axes end up overlap together.

For example, if my first plot has x-axis from 0 to 10, with ticks at 1, 3,
5, 7, 9;

and my second plot has x-axis from 2 to 12, with ticks at 2, 2.9, 5.5...

then the two plots have two different axes with ticks overlap together.

The visual appearance is very bad.

On 1/29/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
>
> Use the axis() command to add custom axes.
>
> To "hold" the plot, insert
>
> par(new=TRUE)
>
> between the plot statements, thus:
>
> plot(x1, y1, etc ...)
> par(new=TRUE)
> plot(x2, y2, etc ...)
>
>
>
> Andrew
>
> On Mon, January 30, 2006 2:49 pm, Michael said:
> > Hi all,
> >
> > I need to put two different axis to one plot. On the top of the plot, I
> > need
> > to put one axis, with increments from left side to the right side; then
> at
> > the bottom of the same plot, I need to put another axis, with increments
> > from right side to the left side and showing a different unit. How do I
> do
> > that?
> >
> > By the way, is there a "hold" command for plotting?
> >
> > If I first plot a picture, how to add another "plot" command onto it
> > without
> > erasing it?
> >
> > In matlab, it can be convinient done by "hold on" and "hold off"... and
> > can
> > I do it in R?
> >
> > Thanks a lot,
> >
> > Micheal
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> Andrew Robinson
> Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> Department of Mathematics and Statistics            Fax: +61-3-8344 4599
> University of Melbourne, VIC 3010 Australia
> Email: a.robinson at ms.unimelb.edu.au    Website:
> http://www.ms.unimelb.edu.au
>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From simonb at cres10.anu.edu.au  Mon Jan 30 06:03:53 2006
From: simonb at cres10.anu.edu.au (Simon Blomberg)
Date: Mon, 30 Jan 2006 16:03:53 +1100
Subject: [R] Help! What does this R command mean?
In-Reply-To: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
References: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
Message-ID: <43DD9E39.7050900@cres.anu.edu.au>

RTFM! Especially the Introduction to R tutorial. Type help.start() at 
the prompt. It should open your web browser. Choose "Introduction to R". 
Only then will you attain enlightenment.

Simon.

Michael wrote:
> Hi all,
>
> R is so difficult. I am so desperate.
>
> What does the ":" mean in the following statement?
>
> What does the "[, -1]" mean?
>
>   
>> # Leaps takes a design matrix as argument: throw away the intercept
>> # column or leaps will complain
>>
>> X <- model.matrix(lm(V ~ I + D + W +G:I + P + N, election.table))[,-1]
>>     
>
> Thanks a lot!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From ggrothendieck at gmail.com  Mon Jan 30 06:06:09 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 00:06:09 -0500
Subject: [R] Help! What does this R command mean?
In-Reply-To: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
References: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
Message-ID: <971536df0601292106m2db6de1el966d172acfef5da7@mail.gmail.com>

?":"
?"["

Also try
?"?"

Also
1. read the "An Introduction to R" manual.  On R Home page click on
Manuals under Documentation in the left hand column.
2. go to:
   http://cran.r-project.org/other-docs.html
and read your choice of documents.
3. print out this reference card and keep it handy:
  http://www.rpad.org/Rpad/R-refcard.pdf

On 1/29/06, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> R is so difficult. I am so desperate.
>
> What does the ":" mean in the following statement?
>
> What does the "[, -1]" mean?
>
> >
> > # Leaps takes a design matrix as argument: throw away the intercept
> > # column or leaps will complain
> >
> > X <- model.matrix(lm(V ~ I + D + W +G:I + P + N, election.table))[,-1]
>
> Thanks a lot!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Mon Jan 30 06:13:34 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 00:13:34 -0500
Subject: [R] How to add two different axis to one plot?
In-Reply-To: <b1f16d9d0601292048p46aa6bfav66ceea19538c1682@mail.gmail.com>
References: <b1f16d9d0601291949r23c9dd00ra336e8372c7b3a8d@mail.gmail.com>
	<3309.24.61.193.81.1138593416.squirrel@webmail.ms.unimelb.edu.au>
	<b1f16d9d0601292048p46aa6bfav66ceea19538c1682@mail.gmail.com>
Message-ID: <971536df0601292113p41887b70h5b007592dee5ff4f@mail.gmail.com>

See this thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/57707.html

On 1/29/06, Michael <comtech.usa at gmail.com> wrote:
> No this is not good.
>
> When I used par(new=TRUE), it does not adjust axis accordingly...
>
> Thus two different axes end up overlap together.
>
> For example, if my first plot has x-axis from 0 to 10, with ticks at 1, 3,
> 5, 7, 9;
>
> and my second plot has x-axis from 2 to 12, with ticks at 2, 2.9, 5.5...
>
> then the two plots have two different axes with ticks overlap together.
>
> The visual appearance is very bad.
>
> On 1/29/06, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> >
> > Use the axis() command to add custom axes.
> >
> > To "hold" the plot, insert
> >
> > par(new=TRUE)
> >
> > between the plot statements, thus:
> >
> > plot(x1, y1, etc ...)
> > par(new=TRUE)
> > plot(x2, y2, etc ...)
> >
> >
> >
> > Andrew
> >
> > On Mon, January 30, 2006 2:49 pm, Michael said:
> > > Hi all,
> > >
> > > I need to put two different axis to one plot. On the top of the plot, I
> > > need
> > > to put one axis, with increments from left side to the right side; then
> > at
> > > the bottom of the same plot, I need to put another axis, with increments
> > > from right side to the left side and showing a different unit. How do I
> > do
> > > that?
> > >
> > > By the way, is there a "hold" command for plotting?
> > >
> > > If I first plot a picture, how to add another "plot" command onto it
> > > without
> > > erasing it?
> > >
> > > In matlab, it can be convinient done by "hold on" and "hold off"... and
> > > can
> > > I do it in R?
> > >
> > > Thanks a lot,
> > >
> > > Micheal
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> >
> > Andrew Robinson
> > Senior Lecturer in Statistics                       Tel: +61-3-8344-9763
> > Department of Mathematics and Statistics            Fax: +61-3-8344 4599
> > University of Melbourne, VIC 3010 Australia
> > Email: a.robinson at ms.unimelb.edu.au    Website:
> > http://www.ms.unimelb.edu.au
> >
> >
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From epurdom at stanford.edu  Mon Jan 30 06:43:04 2006
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Sun, 29 Jan 2006 21:43:04 -0800
Subject: [R] 'all' inconsistent?
Message-ID: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>

Hello,
I came across the following behavior, which seems illogical to me. I don't 
know if it is a bug or if I'm missing something:

 > all(logical(0))
[1] TRUE
 > any(logical(0))
[1] FALSE
 > isTRUE(logical(0))
[1] FALSE

This actually came up in practice when I did something like
 > all( names(x) %in% vec )
as an error-handling, and I was hoping that it would work regardless of 
whether x had names or not. I can clearly work around it, but it seemed 
like strange behavior to me.
Thanks,
Elizabeth Purdom
R 2.2.1, Windows XP



From comtech.usa at gmail.com  Mon Jan 30 06:49:09 2006
From: comtech.usa at gmail.com (Michael)
Date: Sun, 29 Jan 2006 21:49:09 -0800
Subject: [R] Help! What does this R command mean?
In-Reply-To: <971536df0601292106m2db6de1el966d172acfef5da7@mail.gmail.com>
References: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
	<971536df0601292106m2db6de1el966d172acfef5da7@mail.gmail.com>
Message-ID: <b1f16d9d0601292149g19d435a2j353a4c306964777f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/16c7ad25/attachment.pl

From blomsp at ozemail.com.au  Mon Jan 30 06:54:03 2006
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Mon, 30 Jan 2006 16:54:03 +1100
Subject: [R] Help! What does this R command mean?
In-Reply-To: <b1f16d9d0601292149g19d435a2j353a4c306964777f@mail.gmail.com>
References: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>	<971536df0601292106m2db6de1el966d172acfef5da7@mail.gmail.com>
	<b1f16d9d0601292149g19d435a2j353a4c306964777f@mail.gmail.com>
Message-ID: <43DDA9FB.6050101@ozemail.com.au>

?formula

Michael wrote:
> After reading your pointers, I still don't understand that particular usage
> of ":":
>
> btw, the data after read.table():
>
>    Year      V  I  D W       G      P  N
> 1  1916 0.5168  1  1 0   2.229  4.252  3
> 2  1920 0.3612  1  0 1 -11.463 16.535  5
> 3  1924 0.4176 -1 -1 0  -3.872  5.161 10
>
> On 1/29/06, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>   
>> ?":"
>> ?"["
>>
>> Also try
>> ?"?"
>>
>> Also
>> 1. read the "An Introduction to R" manual.  On R Home page click on
>> Manuals under Documentation in the left hand column.
>> 2. go to:
>>    http://cran.r-project.org/other-docs.html
>> and read your choice of documents.
>> 3. print out this reference card and keep it handy:
>>   http://www.rpad.org/Rpad/R-refcard.pdf
>>
>> On 1/29/06, Michael <comtech.usa at gmail.com> wrote:
>>     
>>> Hi all,
>>>
>>> R is so difficult. I am so desperate.
>>>
>>> What does the ":" mean in the following statement?
>>>
>>> What does the "[, -1]" mean?
>>>
>>>       
>>>> # Leaps takes a design matrix as argument: throw away the intercept
>>>> # column or leaps will complain
>>>>
>>>> X <- model.matrix(lm(V ~ I + D + W +G:I + P + N, election.table))[,-1]
>>>>         
>>> Thanks a lot!
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>>       
>> http://www.R-project.org/posting-guide.html
>>     
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From ggrothendieck at gmail.com  Mon Jan 30 07:55:36 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 01:55:36 -0500
Subject: [R] 'all' inconsistent?
In-Reply-To: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
Message-ID: <971536df0601292255v74a69e50xd6e95c327956619c@mail.gmail.com>

I think the corresponding question was already discussed in
the context of sum and in terms of this question one wants:

all(x) && all(y) to equal all(c(x,y))

including the case where x or y has zero length.

On 1/30/06, Elizabeth Purdom <epurdom at stanford.edu> wrote:
> Hello,
> I came across the following behavior, which seems illogical to me. I don't
> know if it is a bug or if I'm missing something:
>
>  > all(logical(0))
> [1] TRUE
>  > any(logical(0))
> [1] FALSE
>  > isTRUE(logical(0))
> [1] FALSE
>
> This actually came up in practice when I did something like
>  > all( names(x) %in% vec )
> as an error-handling, and I was hoping that it would work regardless of
> whether x had names or not. I can clearly work around it, but it seemed
> like strange behavior to me.
> Thanks,
> Elizabeth Purdom
> R 2.2.1, Windows XP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jan 30 08:37:00 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jan 2006 07:37:00 +0000 (GMT)
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0601300723140.8419@gannet.stats>

On Sun, 29 Jan 2006, hadley wickham wrote:

>> use a 'list':
>
> Is a list O(1) for setting and getting?

Can you elaborate?  R is a vector language, and normally you create a list 
in one pass, and you can retrieve multiple elements at once.

Retrieving elements by name from a long vector (including a list) is very 
fast, as an internal hash table is used.  Does the following item from 
ONEWS answer your question?

     o	Indexing a vector by a character vector was slow if both the
 	vector and index were long (say 10,000).  Now hashing is used
 	and the time should be linear in the longer of the lengths
 	(but more memory is used).

Indexing by number is O(1) except where replacement causes the list vector 
to be copied.  There is always the option to use match() to convert to 
numeric indexing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregor.gorjanc at gmail.com  Mon Jan 30 08:46:21 2006
From: gregor.gorjanc at gmail.com (Gregor Gorjanc)
Date: Mon, 30 Jan 2006 08:46:21 +0100
Subject: [R] Is there anything like a write.fwf() or possibility to
 print a data.frame without rownames?
In-Reply-To: <971536df0511220509sb08a615h9bb3746ab6b211e5@mail.gmail.com>
References: <7FFEE688B57D7346BC6241C55900E730F31AC6@pollux.bfro.uni-lj.si>
	<971536df0511220509sb08a615h9bb3746ab6b211e5@mail.gmail.com>
Message-ID: <43DDC44D.1030703@bfro.uni-lj.si>

Hello!

Some time ago[1] I was aking about write.fwf or something similar. I
have now written a function[2] that does this job and it works
reasonably well. It is a bit slow, but works fine for me. Thanks to all,
who have helped me with this issue.

[1]https://stat.ethz.ch/pipermail/r-help/2005-November/081597.html
[2]http://www.bfro.uni-lj.si/MR/ggorjan/software/R/index.html

-- 
Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana     PhD student
Biotechnical Faculty
Zootechnical Department     URI: http://www.bfro.uni-lj.si/MR/ggorjan
Groblje 3                   mail: gregor.gorjanc <at> bfro.uni-lj.si

SI-1230 Domzale             tel: +386 (0)1 72 17 861
Slovenia, Europe            fax: +386 (0)1 72 17 888

----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From ripley at stats.ox.ac.uk  Mon Jan 30 08:52:59 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jan 2006 07:52:59 +0000 (GMT)
Subject: [R] 'all' inconsistent?
In-Reply-To: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
Message-ID: <Pine.LNX.4.61.0601300739520.8419@gannet.stats>

On Sun, 29 Jan 2006, Elizabeth Purdom wrote:

> I came across the following behavior, which seems illogical to me.

What did you expect and why?

> I don't know if it is a bug or if I'm missing something:
>
> > all(logical(0))
> [1] TRUE

All the values are true, all none of them.

> > any(logical(0))
> [1] FALSE

There are no true values here.

> > isTRUE(logical(0))
> [1] FALSE

This one I had to look up.  The help page says

    'isTRUE(x)' is an abbreviation of 'identical(TRUE,x)'.

so it means isTRUE() is true if and only if the result is a logical vector 
of length one and value TRUE (and with no attributes)
E.g.

> xx <- TRUE
> isTRUE(xx)
[1] TRUE
> names(xx) <- "a"
> isTRUE(xx)
[1] FALSE

That could use a little more explanation on the help page.

> This actually came up in practice when I did something like
> > all( names(x) %in% vec )
> as an error-handling, and I was hoping that it would work regardless of
> whether x had names or not.

Depends what `work' means here.  It is true if and only if all the names 
of 'x' are in 'vec', which is presumably not what you wanted.

> I can clearly work around it, but it seemed
> like strange behavior to me.
> Thanks,
> Elizabeth Purdom
> R 2.2.1, Windows XP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aitor_doctorado at yahoo.es  Sun Jan 29 12:32:02 2006
From: aitor_doctorado at yahoo.es (Aitor Mata Conde)
Date: Sun, 29 Jan 2006 12:32:02 +0100 (CET)
Subject: [R] textplot from gplots package
Message-ID: <20060129113202.23776.qmail@web30107.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060129/1380ea19/attachment.pl

From comtech.usa at gmail.com  Mon Jan 30 09:24:57 2006
From: comtech.usa at gmail.com (Michael)
Date: Mon, 30 Jan 2006 00:24:57 -0800
Subject: [R] matlab-like constant matrix initialization?
Message-ID: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/3b0195c2/attachment.pl

From edsberg at stud.ntnu.no  Mon Jan 30 09:39:15 2006
From: edsberg at stud.ntnu.no (Ole Edsberg)
Date: Mon, 30 Jan 2006 09:39:15 +0100
Subject: [R] Varying results of sammon(), for the same data set
Message-ID: <20060130083915.GA1332@stud.ntnu.no>

Hello,

I have a data set on which I run the sammon algorithm as follows:

library(MASS)
data = read.table('problemforr.dat')
y = cmdscale(data, add=TRUE)
s = sammon(data, y$points)

(In case it should be relevant, I make the data available at
http://idi.ntnu.no/~edsberg/problemforr.dat)

With R 2.2.1 on Debian Sid I always get one of two solutions (stress
1.74288 after 10 iterations or stress 1.33629 afer 9 iterations). I
always get the same result within the same R session, even if I read
the data again. With R 2.2.0 on SunOS 5.9 I always get the same result
(stress 0.13186 after 74 iterations).

I understand that the sammon algorithm is very sensitive to even tiny
variations in the starting point, but the observed behaviour seems
strange to me. Difference between machines could perhaps be explained
by floating point portability issues, but not difference on the same
machine, and not the fact that i get the same result within the same R
session.

I read in the documentation
(http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/sammon.html)
that "Further, since the configuration is only determined up to
rotations and reflections (by convention the centroid is at the
origin), the result can vary considerably from machine to machine."
This doesn't make sense to me. If the data and the algorithm is the
same, the result should be the same. What differences between machines
do they refer to here? Floating point issues?

I must admit that I am a beginner, both in R and in statistics. I'm
very curious about the cause of this strangeness. Does anybody have an
explanation?

Best Regards,

Ole Edsberg



From phgrosjean at sciviews.org  Mon Jan 30 09:52:42 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 30 Jan 2006 09:52:42 +0100
Subject: [R] matlab-like constant matrix initialization?
In-Reply-To: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
References: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
Message-ID: <43DDD3DA.8010600@sciviews.org>

Sorry Michael, but I don't understand your question.
If you want to intialize a constant matrix (there is not such thing in 
R, just create a numerical matrix and use it without changing its 
values), you just use matrix(). For help and arguments of the function, 
type:

 > ?matrix

Best,

Philippe Grosjean


Michael wrote:
> Hi all,
> 
> Suppose I have the following matrix which is a constant matrix I've copied
> from some other document:
> 
> 1.2  3.4 1.4 ...
> 2.3  3.7 2.6 ...
> ...
> 
> How do I make it into a matrix or array in R?
> 
> What is the fastest way of initializing a constant matrix with this
> copy/pasted values?
> 
> Thanks a lot!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From gchappi at gmail.com  Mon Jan 30 10:47:17 2006
From: gchappi at gmail.com (Hans-Peter)
Date: Mon, 30 Jan 2006 10:47:17 +0100
Subject: [R] matlab-like constant matrix initialization?
In-Reply-To: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
References: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
Message-ID: <47fce0650601300147jd003deg@mail.gmail.com>

> Suppose I have the following matrix which is a constant matrix I've copied
> from some other document:
>
> 1.2  3.4 1.4 ...
> 2.3  3.7 2.6 ...
> ...
> How do I make it into a matrix or array in R?
> What is the fastest way of initializing a constant matrix with this
> copy/pasted values?

you cannot just paste it, you have to adapt it either like this

x <- matrix( c( 1.2, 3.4, 1.4,
                      2.3, 3.7, 2.6 ),
                  nrow = 2, byrow = TRUE)

or like this:

x <- rbind( c( 1.2, 3.4, 1.4 ),
                c( 2.3, 3.7, 2.6 ) )

The second is closer to ML's x = [1,2 3.4 1.4;2.3 3.7 2.6] but the
first is probably the more popular/recommended approach. If it's a
large matrix that you don't want to adapt manually I think the only
way is to go via an ascii text file.

I see that you come from Matlab and that you have asked some rather
basic questions. I really recommend you, that you read the manuals (as
indicated). And do read them 2 or 3 times as they are much denser than
the Matlab manuals.

If you are not able to attend a course (which IMHO is the best way to
learn R) I'd buy a book.
(http://www.r-project.org/doc/bib/R-publications.html). Maybe the
"John Verzani. Using R for Introductory Statistics" or "Uwe Ligges.
Programmieren mit R" could help you.

Best regards,
Hans-Peter



From anil_rohilla at rediffmail.com  Mon Jan 30 10:28:40 2006
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 30 Jan 2006 09:28:40 -0000
Subject: [R] Filtering the time series
Message-ID: <20060130092840.17635.qmail@webmail24.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/d6d1dca4/attachment.pl

From adi at roda.ro  Mon Jan 30 12:34:40 2006
From: adi at roda.ro (Adrian DUSA)
Date: Mon, 30 Jan 2006 13:34:40 +0200
Subject: [R] yet another vectorization question
Message-ID: <200601301334.40440.adi@roda.ro>

Dear R-helpers,

I'm trying to develop a function which specifies all possible expressions that 
can be formed using a certain number of variables. For example, with three 
variables A, B and C we can have
- presence/absence of A; B and C
- presence/absence of combinations of two of them
- presence/absence of all three

    A   B   C
1   0
2   1
3       0
4       1
5           0
6           1
7   0   0
8   0   1
9   1   0
10  1   1
11  0       0
12  0       1
13  1       0
14  1       1
15      0   0
16      0   1
17      1   0
18      1   1
19  0   0   0
20  0   0   1
21  0   1   0
22  0   1   1
23  1   0   0
24  1   0   1
25  1   1   0
26  1   1   1

My function (pasted below) while producing the desired result, still needs 
some more vectorizing; in particular, I can't figure out how could one modify 
the element of a matrix using apply on a different matrix...
To produce the above outcome, I use:
> all.expr(LETTERS[1:3])

"all.expr" <-
function(column.names) {
    ncolumns <- length(column.names)
    return.matrix <- matrix(NA, nrow=(3^ncolumns - 1), ncol=ncolumns)
    colnames(return.matrix) <- column.names
    rownames(return.matrix) <- 1:nrow(return.matrix)
    start.row <- 1
    all.combn <- sapply(1:ncolumns, function(idx) {
                                        as.matrix(combn(ncolumns, idx))
                                    }, simplify=FALSE)
    for (j in 1:length(all.combn)) {
        idk <- all.combn[[j]]
        tt <- matrix(NA, ncol=nrow(idk), nrow=2^nrow(idk))
        for (i in 1:nrow(idk)) {
            tt[,i] <- c(rep(0, 2^(nrow(idk) - i)), rep(1, 2^(nrow(idk) - i)))
        }

        ## This is _slow_ part, where I don't know how to vectorize:
        for (k in 1:ncol(idk)) {
            end.row <- start.row + nrow(tt) - 1
            return.matrix[start.row:end.row, idk[ , k]] <- tt
            start.row <- end.row + 1
        }
        ## How can one modify "return.matrix" using apply on "idk"?
    }
        return.matrix[is.na(return.matrix)] <- ""
        return.matrix
    }
}

Thank you in advance,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From Raquel.Granell at bristol.ac.uk  Mon Jan 30 12:34:37 2006
From: Raquel.Granell at bristol.ac.uk (R Granell, Medicine)
Date: Mon, 30 Jan 2006 11:34:37 +0000
Subject: [R] multiple hystograms
Message-ID: <EDF44EE7E2B92B178C432884@alsp-stats6.alspac.bris.ac.uk>

Hello,

Here comes a simple question:

Is there a way of obtaining more than one histogram in the graph-window so
you can edit all the plots simultaneously?

and how about scatter plots?


Thanks in advance



From dusa.adrian at gmail.com  Mon Jan 30 12:42:16 2006
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Mon, 30 Jan 2006 11:42:16 +0000 (UTC)
Subject: [R] yet another vectorization question
References: <200601301334.40440.adi@roda.ro>
Message-ID: <loom.20060130T124012-434@post.gmane.org>

Adrian DUSA <adi <at> roda.ro> writes:
> 
> I'm trying to develop a function [...snip...]

Sorry for the traffic, I forgot to say that I'm using
library(combinat)
for the "combn" function...

Thank you,
Adrian



From jacques.veslot at cirad.fr  Mon Jan 30 12:57:16 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 30 Jan 2006 15:57:16 +0400
Subject: [R] yet another vectorization question
In-Reply-To: <200601301334.40440.adi@roda.ro>
References: <200601301334.40440.adi@roda.ro>
Message-ID: <43DDFF1C.1080504@cirad.fr>

this looks similar:
do.call(expand.grid,split(t(replicate(3,c(0,1,NA))),1:3))


Adrian DUSA a ??crit :

>Dear R-helpers,
>
>I'm trying to develop a function which specifies all possible expressions that 
>can be formed using a certain number of variables. For example, with three 
>variables A, B and C we can have
>- presence/absence of A; B and C
>- presence/absence of combinations of two of them
>- presence/absence of all three
>
>    A   B   C
>1   0
>2   1
>3       0
>4       1
>5           0
>6           1
>7   0   0
>8   0   1
>9   1   0
>10  1   1
>11  0       0
>12  0       1
>13  1       0
>14  1       1
>15      0   0
>16      0   1
>17      1   0
>18      1   1
>19  0   0   0
>20  0   0   1
>21  0   1   0
>22  0   1   1
>23  1   0   0
>24  1   0   1
>25  1   1   0
>26  1   1   1
>
>My function (pasted below) while producing the desired result, still needs 
>some more vectorizing; in particular, I can't figure out how could one modify 
>the element of a matrix using apply on a different matrix...
>To produce the above outcome, I use:
>  
>
>>all.expr(LETTERS[1:3])
>>    
>>
>
>"all.expr" <-
>function(column.names) {
>    ncolumns <- length(column.names)
>    return.matrix <- matrix(NA, nrow=(3^ncolumns - 1), ncol=ncolumns)
>    colnames(return.matrix) <- column.names
>    rownames(return.matrix) <- 1:nrow(return.matrix)
>    start.row <- 1
>    all.combn <- sapply(1:ncolumns, function(idx) {
>                                        as.matrix(combn(ncolumns, idx))
>                                    }, simplify=FALSE)
>    for (j in 1:length(all.combn)) {
>        idk <- all.combn[[j]]
>        tt <- matrix(NA, ncol=nrow(idk), nrow=2^nrow(idk))
>        for (i in 1:nrow(idk)) {
>            tt[,i] <- c(rep(0, 2^(nrow(idk) - i)), rep(1, 2^(nrow(idk) - i)))
>        }
>
>        ## This is _slow_ part, where I don't know how to vectorize:
>        for (k in 1:ncol(idk)) {
>            end.row <- start.row + nrow(tt) - 1
>            return.matrix[start.row:end.row, idk[ , k]] <- tt
>            start.row <- end.row + 1
>        }
>        ## How can one modify "return.matrix" using apply on "idk"?
>    }
>        return.matrix[is.na(return.matrix)] <- ""
>        return.matrix
>    }
>}
>
>Thank you in advance,
>Adrian
>
>  
>



From ales.ziberna at gmail.com  Mon Jan 30 13:17:06 2006
From: ales.ziberna at gmail.com (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Mon, 30 Jan 2006 13:17:06 +0100
Subject: [R] multiple hystograms
In-Reply-To: <EDF44EE7E2B92B178C432884@alsp-stats6.alspac.bris.ac.uk>
Message-ID: <06e101c62597$275ccb40$a7fdfea9@TAMARA>

See 
?par
Look for arguments 
mfrow and mfcol 

Hope this is what you are looking for,
Ales Ziberna

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of R Granell, Medicine
Sent: Monday, January 30, 2006 12:35 PM
To: r-help at stat.math.ethz.ch
Subject: [R] multiple hystograms

Hello,

Here comes a simple question:

Is there a way of obtaining more than one histogram in the graph-window so
you can edit all the plots simultaneously?

and how about scatter plots?


Thanks in advance

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Mon Jan 30 13:40:34 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 30 Jan 2006 13:40:34 +0100
Subject: [R] yet another vectorization question
In-Reply-To: <43DDFF1C.1080504@cirad.fr>
References: <200601301334.40440.adi@roda.ro> <43DDFF1C.1080504@cirad.fr>
Message-ID: <43DE0942.6060303@sciviews.org>

Hello,

Not exactly the same. By the way, why do you use do.call()? Couldn't you 
do simply:

expand.grid(split(t(replicate(3, c(0, 1, NA))), 1:3))

Best,

Philippe Grosjean


Jacques VESLOT wrote:
> this looks similar:
> do.call(expand.grid,split(t(replicate(3,c(0,1,NA))),1:3))
> 
> 
> Adrian DUSA a ??crit :
> 
> 
>>Dear R-helpers,
>>
>>I'm trying to develop a function which specifies all possible expressions that 
>>can be formed using a certain number of variables. For example, with three 
>>variables A, B and C we can have
>>- presence/absence of A; B and C
>>- presence/absence of combinations of two of them
>>- presence/absence of all three
>>
>>   A   B   C
>>1   0
>>2   1
>>3       0
>>4       1
>>5           0
>>6           1
>>7   0   0
>>8   0   1
>>9   1   0
>>10  1   1
>>11  0       0
>>12  0       1
>>13  1       0
>>14  1       1
>>15      0   0
>>16      0   1
>>17      1   0
>>18      1   1
>>19  0   0   0
>>20  0   0   1
>>21  0   1   0
>>22  0   1   1
>>23  1   0   0
>>24  1   0   1
>>25  1   1   0
>>26  1   1   1
>>
>>My function (pasted below) while producing the desired result, still needs 
>>some more vectorizing; in particular, I can't figure out how could one modify 
>>the element of a matrix using apply on a different matrix...
>>To produce the above outcome, I use:
>> 
>>
>>
>>>all.expr(LETTERS[1:3])
>>>   
>>>
>>
>>"all.expr" <-
>>function(column.names) {
>>   ncolumns <- length(column.names)
>>   return.matrix <- matrix(NA, nrow=(3^ncolumns - 1), ncol=ncolumns)
>>   colnames(return.matrix) <- column.names
>>   rownames(return.matrix) <- 1:nrow(return.matrix)
>>   start.row <- 1
>>   all.combn <- sapply(1:ncolumns, function(idx) {
>>                                       as.matrix(combn(ncolumns, idx))
>>                                   }, simplify=FALSE)
>>   for (j in 1:length(all.combn)) {
>>       idk <- all.combn[[j]]
>>       tt <- matrix(NA, ncol=nrow(idk), nrow=2^nrow(idk))
>>       for (i in 1:nrow(idk)) {
>>           tt[,i] <- c(rep(0, 2^(nrow(idk) - i)), rep(1, 2^(nrow(idk) - i)))
>>       }
>>
>>       ## This is _slow_ part, where I don't know how to vectorize:
>>       for (k in 1:ncol(idk)) {
>>           end.row <- start.row + nrow(tt) - 1
>>           return.matrix[start.row:end.row, idk[ , k]] <- tt
>>           start.row <- end.row + 1
>>       }
>>       ## How can one modify "return.matrix" using apply on "idk"?
>>   }
>>       return.matrix[is.na(return.matrix)] <- ""
>>       return.matrix
>>   }
>>}
>>
>>Thank you in advance,
>>Adrian
>>
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ales.ziberna at gmail.com  Mon Jan 30 13:53:22 2006
From: ales.ziberna at gmail.com (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Mon, 30 Jan 2006 13:53:22 +0100
Subject: [R] R on dual-core machines
Message-ID: <06e501c6259c$28121f40$a7fdfea9@TAMARA>

Dear expeRts!

I'm thinking of buying a new computer and am considering dual-core
processors, such as AMD Athlon64 X2. Since I'm not a computer expert, pleas
forgive me if some of my questions are silly.

First, am I correct that using a dual-core processor is (for R point of
view) the same as using a computer with two processors?

If that is true, the posts I found on the list imply that using such a
processor can usually bring significant improvements (in computational time)
only if the case where the core (C or sometimes R) is specially designed for
multiple processors (see comments below).

So based on these and other comments I can conclude that if I'm not prepared
(able) to make such modifications, I can aspect improvements only in this
two areas:
1.	If I am running two instances of R.
2.	If I'm running several other programs on the computer beside R, the
programs and R would run faster, since they would not "compete" for
processor time (so much)

Thanks in advance for any useful suggestions,
Ales Ziberna

P.S.: Useful posts on the list follow:



It depends on the usage pattern. If you run multiple CPU-bound processes in
parallel without too much coordination (parallel make is a good example,
simulations another), then you get close to double up from a dual. For a
single R process, you can get something like 40% improvement in large linear
algebra problems, using a threaded ATLAS.
For other problems the speedup is basically nil. There is some potential in
threading R or (much easier) some of its vector operations, but that is not
even on the drawing board at this stage.

------------------------------------------------------------------------

If you want to exploit multiple processors, you can write code (e.g., in C)
called from R (e.g., through .Call or .C) that performs parallel/threaded
computations in a thread-safe way (e.g., without calling back into R).
---
Another possibility is to replace the BLAS/LAPACK library with a thread-safe
version. This provides a boost to those R algorithms exploiting these
libraries.
---
An alternative is to do all of the parallelization within R using nice tools
like the snow package combined with Rmpi.  If your task is computationally
intensive on the R side, but not on the client, then parallelizing R code
may be the better way to go.  All depends on your application, I think.



From ligges at statistik.uni-dortmund.de  Mon Jan 30 14:03:10 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 Jan 2006 14:03:10 +0100
Subject: [R] R on dual-core machines
In-Reply-To: <06e501c6259c$28121f40$a7fdfea9@TAMARA>
References: <06e501c6259c$28121f40$a7fdfea9@TAMARA>
Message-ID: <43DE0E8E.2000709@statistik.uni-dortmund.de>

Yes, you are right, and you found some relevant posts.

Uwe Ligges


Ale? ?iberna wrote:

> Dear expeRts!
> 
> I'm thinking of buying a new computer and am considering dual-core
> processors, such as AMD Athlon64 X2. Since I'm not a computer expert, pleas
> forgive me if some of my questions are silly.
> 
> First, am I correct that using a dual-core processor is (for R point of
> view) the same as using a computer with two processors?
> 
> If that is true, the posts I found on the list imply that using such a
> processor can usually bring significant improvements (in computational time)
> only if the case where the core (C or sometimes R) is specially designed for
> multiple processors (see comments below).
> 
> So based on these and other comments I can conclude that if I'm not prepared
> (able) to make such modifications, I can aspect improvements only in this
> two areas:
> 1.	If I am running two instances of R.
> 2.	If I'm running several other programs on the computer beside R, the
> programs and R would run faster, since they would not "compete" for
> processor time (so much)
> 
> Thanks in advance for any useful suggestions,
> Ales Ziberna
> 
> P.S.: Useful posts on the list follow:
> 
> 
> 
> It depends on the usage pattern. If you run multiple CPU-bound processes in
> parallel without too much coordination (parallel make is a good example,
> simulations another), then you get close to double up from a dual. For a
> single R process, you can get something like 40% improvement in large linear
> algebra problems, using a threaded ATLAS.
> For other problems the speedup is basically nil. There is some potential in
> threading R or (much easier) some of its vector operations, but that is not
> even on the drawing board at this stage.
> 
> ------------------------------------------------------------------------
> 
> If you want to exploit multiple processors, you can write code (e.g., in C)
> called from R (e.g., through .Call or .C) that performs parallel/threaded
> computations in a thread-safe way (e.g., without calling back into R).
> ---
> Another possibility is to replace the BLAS/LAPACK library with a thread-safe
> version. This provides a boost to those R algorithms exploiting these
> libraries.
> ---
> An alternative is to do all of the parallelization within R using nice tools
> like the snow package combined with Rmpi.  If your task is computationally
> intensive on the R side, but not on the client, then parallelizing R code
> may be the better way to go.  All depends on your application, I think.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jan 30 14:06:40 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jan 2006 13:06:40 +0000 (GMT)
Subject: [R] Varying results of sammon(), for the same data set
In-Reply-To: <20060130083915.GA1332@stud.ntnu.no>
References: <20060130083915.GA1332@stud.ntnu.no>
Message-ID: <Pine.LNX.4.61.0601300903270.9795@gannet.stats>

On Mon, 30 Jan 2006, Ole Edsberg wrote:

> Hello,
>
> I have a data set on which I run the sammon algorithm as follows:
>
> library(MASS)
> data = read.table('problemforr.dat')

Hmm.  This is a data frame of 387 rows and 387 columns and Euclidean 
distance is used.  Squeezing 387 dims (and PCA shows these points as well 
spread in almost all those dimensions) to 2 is not a well-posed problem, 
and you should welcome the plurality of answers found.

> y = cmdscale(data, add=TRUE)
> s = sammon(data, y$points)
>
> (In case it should be relevant, I make the data available at
> http://idi.ntnu.no/~edsberg/problemforr.dat)
>
> With R 2.2.1 on Debian Sid I always get one of two solutions (stress
> 1.74288 after 10 iterations or stress 1.33629 afer 9 iterations). I
> always get the same result within the same R session, even if I read
> the data again. With R 2.2.0 on SunOS 5.9 I always get the same result
> (stress 0.13186 after 74 iterations).

Note that your subject line attributes this to sammon, but it could also 
be due to cmdscale.

On AMD64 Linux I get

> s = sammon(data, y$points)
Initial stress        : 2.21024
stress after  10 iters: 1.22268, magic = 0.092
stress after  20 iters: 0.48801, magic = 0.009
stress after  30 iters: 0.35007, magic = 0.020
stress after  40 iters: 0.24377, magic = 0.045
stress after  50 iters: 0.17343, magic = 0.021
stress after  60 iters: 0.14944, magic = 0.048
stress after  70 iters: 0.12810, magic = 0.022
stress after  80 iters: 0.12423, magic = 0.010
stress after  90 iters: 0.12191, magic = 0.118
stress after 100 iters: 0.11986, magic = 0.500

That large reduction in `magic' indicates the algorithm is having 
problems.  Without optimization (used for valgrind) I got the solution you 
quoted for Solaris 9.

However, on all four systems (AMD64 FC3 Linux, i686 FC3 Linux, Solaris and 
Windows) I tried the results were different between systems and repeatable 
by system.  I even ran under valgrind to be sure that no uninitialized 
areas were used (on FC3).

> I understand that the sammon algorithm is very sensitive to even tiny
> variations in the starting point, but the observed behaviour seems
> strange to me. Difference between machines could perhaps be explained
> by floating point portability issues, but not difference on the same
> machine, and not the fact that i get the same result within the same R
> session.

No, but then that is not reproducible, and has never been reported before. 
If for example different BLAS libraries get selected on different runs 
this would explain it.  Or it could be a Debian-Sid-specific bug in a 
shared library or compiler.

> I read in the documentation 
> (http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/sammon.html) 
> that "Further, since the configuration is only determined up to 
> rotations and reflections (by convention the centroid is at the origin), 
> the result can vary considerably from machine to machine." This doesn't 
> make sense to me.

Note that is addressing a separate issue.  For a given minimized stress 
there are multiple solutions which can be transformed into each other, and 
the help file is warning you of that.  There are also (in general) 
multiple local minima.

> If the data and the algorithm is the same, the result should be the 
> same.

Depending what you mean by 'algorithm', this is what the subject of 
numerical analysis is about.  I take it you are familiar with J. H. 
Wilkinson's classic work on the Algebraic Eigenvalue Problem?

> What differences between machines do they refer to here? Floating 
> point issues?

Any difference in the CPU/FPU or compiler or run-time environment 
(including all the dynamically linked support libraries).  Just changing 
the optimization level of the compiler changes the assembler-level 
algorithm used, and can often affect the answer of e.g. an eigenvalue 
calculation.  Rounding errors depend on whether (and when) 
extended-precision registers are used and the exact order of the 
calculations since computer arithmetic is not distributive.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From assampryseley at yahoo.com  Mon Jan 30 14:14:14 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Mon, 30 Jan 2006 05:14:14 -0800 (PST)
Subject: [R] Help with R: functions
In-Reply-To: <mailman.5.1138618802.26022.r-help@stat.math.ethz.ch>
Message-ID: <20060130131414.7979.qmail@web37110.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/6a20207e/attachment.pl

From andy_liaw at merck.com  Mon Jan 30 14:16:17 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 08:16:17 -0500
Subject: [R] problem with mirror selection
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED784@usctmx1106.merck.com>

You didn't say if you did the selection from the menu or the command line...

In any case, the menu `Packages' -> `Set CRAN Mirror...' runs the command
chooseCRANmirror(), which looks like

> chooseCRANmirror
function (graphics = TRUE) 
{
    if (!interactive()) 
        stop("cannot choose a CRAN mirror non-interactively")
    m <- read.csv(file.path(R.home("doc"), "CRAN_mirrors.csv"), 
        as.is = TRUE)
    res <- menu(m[, 1], graphics, "CRAN mirror")
    if (res > 0) {
        URL <- m[res, "URL"]
        repos <- getOption("repos")
        repos["CRAN"] <- gsub("/$", "", URL[1])
        options(repos = repos)
    }
    invisible()
}
<environment: namespace:utils>

As you can see, it simply sets the repos option, and never use the net.  So,
exactly how did you get the problem you described?

Andy


From: Erin Hodgess
> 
> Dear R People:
> 
> I'm having trouble with selecting a mirror to download packages.
> 
> When I select the location, R just sits.  When I try to cancel
> the program, it never quits.
> 
> Has anyone else run into this, please?  It only happens on my
> laptop, not my desktop.  My inclination is to say that it might be
> a problem with a firewall, but I thought I would check here as
> well.
> 
> 
> 
> R Version 2.2.1 Windows.
> 
> Thanks in advance!
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From petr.pikal at precheza.cz  Mon Jan 30 14:32:44 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 30 Jan 2006 14:32:44 +0100
Subject: [R] can I do this with read.table??
In-Reply-To: <Pine.LNX.4.61.0601261454430.11071@echidna.fhcrc.org>
References: <440230C3.1030609@gmail.com>
Message-ID: <43DE238C.15790.17CA521@localhost>

Hi

Well, colClesses can be used for what you want to do. See

> test<-read.table("c:/temp/test.txt", 
colClasses=c("character","numeric", "character", "factor"))
> str(test)
`data.frame':   10 obs. of  3 variables:
 $ doba : num  189 256 286 105 272 45 29.5 43 68.5 99
 $ otac : chr  "0.6" "0.6" "0.6" "1.2" ...
 $ sklon: Factor w/ 6 levels "10","110","120",..: 2 6 5 5 1 2 3 6 5 4
>

As you see you can change numeric variable to character or factor as 
you wish.

Do not forget!!!
          Note that 'colClasses' is specified per column (not per
          variable) and so includes the column of row names (if any). 

HTH
Petr




On 26 Jan 2006 at 14:57, Douglas Grove wrote:

Date sent:      	Thu, 26 Jan 2006 14:57:17 -0800 (PST)
From:           	Douglas Grove <dgrove at fhcrc.org>
To:             	Kjetil Brinchmann Halvorsen <kjetilbrinchmannhalvorsen at gmail.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] can I do this with read.table??

> I did read the help page, very carefully.   
> 
> The colClasses argument can be used if I want
> to stop and look through every data set to see
> which column I need to protect.  But that's what I 
> said that I don't want to do.
> 
> As for 'as.is', I wish it did what you suggest, but
> it doesn't.  If one reads carefully, as.is protects
> a character vector from converstion to a *factor*,
> but not from conversion to numeric/logical.
> 
> Doug
> 
> 
> 
> 
> On Sun, 26 Feb 2006, Kjetil Brinchmann Halvorsen wrote:
> 
> > Douglas Grove wrote:
> > > Hi,
> > > 
> > > I'm trying to figure out if there's an automated way to get
> > > read.table to read in my data and *not* convert the character
> > > columns into anything, just leave them alone.  What I'm referring
> > 
> > ?Did you read the help page?
> > What about argument as.is=TRUE?
> > See also argument colClasses
> > 
> > Kjetil
> > 
> > > to as 'character columns' are columns in the data that are quoted.
> > > For columns of alphabetic strings (that aren't TRUE or FALSE) I
> > > can suppress conversion to factor with as.is=TRUE, but what I'd
> > > like to stop is the conversion of quoted numbers of the form
> > > "01","02",..., into numeric form.
> > > 
> > > By an 'automated way', I mean one that does not involve me having
> > > to know which columns in the data are the ones I want kept as they
> > > are.
> > > 
> > > This doesn't seem like an unreasonable thing to want to do.
> > > After all, say I've got the data.frame:
> > > 
> > > A <- data.frame(a=1:3, b=I(c("01","02","03")))
> > > 
> > > I can export this to a text file with the simple command
> > > 
> > > write.table(A, "A.txt", sep="\t", row.names=FALSE, quote=TRUE)
> > > 
> > > but I cannot find an equally simple mechanism for reading this
> > > data back in from A.txt that allows me to reconstruct my
> > > data.frame 'A'.  Is this an unreasonable thing to expect?
> > > 
> > > Thanks,
> > > Doug
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Matthias.Templ at statistik.gv.at  Mon Jan 30 14:36:01 2006
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 30 Jan 2006 14:36:01 +0100
Subject: [R] Help with R: functions
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAE21@xchg1.statistik.local>


What is wrong with your first solution:

st <-function(x,y){
  ## y ... Response
  ## x ... terms
  rcc<-coef(lm(y ~ x))
  plot(x,y)
  abline(rcc[1],rcc[2])
  }
  st(dats$visual24,dats$visual52)

Or use attach:

st <-function(data,x,y){
  attach(data)
  rcc<-coef(lm(x~y))
  plot(x,y)
  abline(rcc[1],rcc[2])
  detach(data)
  }
  st(dats,visual24,visual52)
 
 Best,
Matthias

> -----Urspr??ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> Pryseley Assam
> Gesendet: Montag, 30. J??nner 2006 14:14
> An: r-help at stat.math.ethz.ch
> Betreff: [R] Help with R: functions
> 
> 
> 
> Hello R-users
>    
>   I am new to R and trying to write some functions. I have 
> problems writing functions that takes a data set as an 
> arguement and uses variables in the data. I illustrate my 
> problem with a small example below:
>    
>    sample data   #------------------
>   visual24<-rnorm(30,3,5)
>   visual52<-rt(30,7)
>   dats<- data.frame(cbind(visual24,visual52))
>   remove(visual24, visual52)
>    
>   # first code
>   #--------------
>   st <-function(data,x,y){
>   rcc<-coef(lm(y~x))
>   plot(x,y)
>   abline(rcc[1],rcc[2])
>   }
>   st(data=dats,x=dats$visual24,y=dats$visual52)
>    
>   This code works fine, but with such a code the data as an 
> arguement to the funtion is not necessary. 
>   However, i wish to write a function that reads the 
> variables from the data directly.
>   I tried using the function below but it does not work. 
>    
>   # second code
>   #------------------
>   st <-function(data,x,y){
>   rcc<-coef(lm(data$y~data$x))
>   plot(data$x,data$y)
>   abline(rcc[1],rcc[2])
>   }
>   st(dats,visual24,visual52)
>    
>   I wish to inquire if any one has an idea of what i need to 
> adjust in the function so that it works.
>   I believe that the referencing $x or $y in the function is 
> not doing the correct thing.
>    
>   Better still, will it be a problem if i code the functions 
> as in the first code above?
>   I mean given that they will be used to create a library
>    
>   Best regards
>   Pryseley
>    
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From phgrosjean at sciviews.org  Mon Jan 30 14:36:53 2006
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 30 Jan 2006 14:36:53 +0100
Subject: [R] Help with R: functions
In-Reply-To: <20060130131414.7979.qmail@web37110.mail.mud.yahoo.com>
References: <20060130131414.7979.qmail@web37110.mail.mud.yahoo.com>
Message-ID: <43DE1675.6080908@sciviews.org>

# Here it is (using the formula interface):
dats <- data.frame(visual24 = rnorm(30, 3, 5),
                    visual52 = rt(30, 7))

st <- function(formula, data, ...) {
	# Just use the formula to specify which variables to use
	rcc <- coef(lm(formula, data))
	# Make sure to keep only variable used in the data frame
	plot(data[ , rev(all.vars(formula))])
	# Draw the line. Note the ... that allows to change
	#lines features
	abline(coef = rcc, ...)
	# Return the coefficients invisibly
	return(invisible(rcc))
}

st(visual52 ~ visual24, data = dats)
# Change style and color of the line (thanks to '...')
st(visual52 ~ visual24, data = dats, lty = 2, col = "red")

Best,

Philippe Grosjean


Pryseley Assam wrote:
> Hello R-users
>    
>   I am new to R and trying to write some functions. I have problems writing functions that takes a data set as an arguement and uses variables in the data. I illustrate my problem with a small example below:
>    
>    sample data   #------------------
>   visual24<-rnorm(30,3,5)
>   visual52<-rt(30,7)
>   dats<- data.frame(cbind(visual24,visual52))
>   remove(visual24, visual52)
>    
>   # first code
>   #--------------
>   st <-function(data,x,y){
>   rcc<-coef(lm(y~x))
>   plot(x,y)
>   abline(rcc[1],rcc[2])
>   }
>   st(data=dats,x=dats$visual24,y=dats$visual52)
>    
>   This code works fine, but with such a code the data as an arguement to the funtion is not necessary. 
>   However, i wish to write a function that reads the variables from the data directly.
>   I tried using the function below but it does not work. 
>    
>   # second code
>   #------------------
>   st <-function(data,x,y){
>   rcc<-coef(lm(data$y~data$x))
>   plot(data$x,data$y)
>   abline(rcc[1],rcc[2])
>   }
>   st(dats,visual24,visual52)
>    
>   I wish to inquire if any one has an idea of what i need to adjust in the function so that it works.
>   I believe that the referencing $x or $y in the function is not doing the correct thing.
>    
>   Better still, will it be a problem if i code the functions as in the first code above?
>   I mean given that they will be used to create a library
>    
>   Best regards
>   Pryseley
>    
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From jacques.veslot at cirad.fr  Mon Jan 30 14:46:00 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 30 Jan 2006 17:46:00 +0400
Subject: [R] Help with R: functions
In-Reply-To: <20060130131414.7979.qmail@web37110.mail.mud.yahoo.com>
References: <20060130131414.7979.qmail@web37110.mail.mud.yahoo.com>
Message-ID: <43DE1898.1070500@cirad.fr>

st <- function(data, x, y){
    attach(data)
    rcc <- coef(lm(y~x))
    plot(x,y)
    abline(rcc)
    detach(data)}

st(data=dats, x=visual24, y=visual52)



Pryseley Assam a ??crit :

>Hello R-users
>   
>  I am new to R and trying to write some functions. I have problems writing functions that takes a data set as an arguement and uses variables in the data. I illustrate my problem with a small example below:
>   
>   sample data   #------------------
>  visual24<-rnorm(30,3,5)
>  visual52<-rt(30,7)
>  dats<- data.frame(cbind(visual24,visual52))
>  remove(visual24, visual52)
>   
>  # first code
>  #--------------
>  st <-function(data,x,y){
>  rcc<-coef(lm(y~x))
>  plot(x,y)
>  abline(rcc[1],rcc[2])
>  }
>  st(data=dats,x=dats$visual24,y=dats$visual52)
>   
>  This code works fine, but with such a code the data as an arguement to the funtion is not necessary. 
>  However, i wish to write a function that reads the variables from the data directly.
>  I tried using the function below but it does not work. 
>   
>  # second code
>  #------------------
>  st <-function(data,x,y){
>  rcc<-coef(lm(data$y~data$x))
>  plot(data$x,data$y)
>  abline(rcc[1],rcc[2])
>  }
>  st(dats,visual24,visual52)
>   
>  I wish to inquire if any one has an idea of what i need to adjust in the function so that it works.
>  I believe that the referencing $x or $y in the function is not doing the correct thing.
>   
>  Better still, will it be a problem if i code the functions as in the first code above?
>  I mean given that they will be used to create a library
>   
>  Best regards
>  Pryseley
>   
>
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From petr.pikal at precheza.cz  Mon Jan 30 14:50:00 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 30 Jan 2006 14:50:00 +0100
Subject: [R] SoS! How to predict new values using linear
	regression	models?
In-Reply-To: <971536df0601291428t536b0297y1664c3d3a0a76e3d@mail.gmail.com>
References: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com>
Message-ID: <43DE2798.16785.18C74FB@localhost>

Hi


On 29 Jan 2006 at 17:28, Gabor Grothendieck wrote:

Date sent:      	Sun, 29 Jan 2006 17:28:29 -0500
From:           	Gabor Grothendieck <ggrothendieck at gmail.com>
To:             	Michael <comtech.usa at gmail.com>
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] SoS! How to predict new values using linear regression
	models?

> Leaving aside the issue of whether linear regression is appropriate
> here, do it like this where I have used the builtin iris data frame
> since I don't have access to your ss:
> 
> iris.lm <- lm(as.numeric(Species) ~ Sepal.Length + Sepal.Width, iris)
> predict(iris.lm, data.frame(Sepal.Length = 3, Sepal.Width = 2))
> 
> On 1/29/06, Michael <comtech.usa at gmail.com> wrote:
> > Hi all,
> >
> > After trial and error by myself for a few hours, I decide to ask for
> > your help.
> >
> > I have a training set which is a matrix of size 200 x 2, where the
> > two columns denote each independent variable. I have 200
> > observations.
> >
> > -----------------
> > ss=data.frame(trainingSet);
> > result=lm(trainingClass~ss$X1+ss$X2);
                                          ^^^^    ^^^

As Gabor suggested, use data argument.

result=lm(trainingClass~X1+X2, data=ss)

and your predict shall work.

HTH
Petr

> > -----------------
> >
> > where trainingClass denotes the true classes of the training data.
> >
> > Now I want to apply the model to predict new data:
> >
> > -----------------
> > > gg=predict(result, data.frame(X1=1, X2=2))
> > Warning message:
> > 'newdata' had 1 rows but variable(s) found have 200 rows
> > -----------------
> >
> > That's to say, I provide a new data which is one observation of 2
> > independent variables(1 row, two columns). I converted it into data
> > frame.
> >
> > However, the R never gives me new predication value for this NEW ONE
> > observation. Instead, it keeps giving me the above warning and keeps
> > printing the fitted value for the 200 training samples...
> >
> > That's very bad.
> >
> > Please help me!
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From charles.edwin.white at us.army.mil  Mon Jan 30 15:03:32 2006
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Mon, 30 Jan 2006 09:03:32 -0500
Subject: [R] lme4_0.995-2/Matrix_0.995-4 upgrade introduces error
	messages (change management)
Message-ID: <8BAEC5E546879B4FAA536200A292C614F7ECF2@AMEDMLNARMC135.amed.ds.army.mil>

I hope I'm not making your life unnecessarily difficult. As I will
demonstrate below my signature, my original straight application of
lme4_0.995-2/Matrix_0.995-4 is failing without providing any
optimization information. For reference, I've provided optimization
output from lme4_0.995-1/Matrix_0.995-1. Including the lmer command
control=list(PQLmaxIt=0) or control=list(PQLmaxIt=10) produces exactly
the same error as when the commands are not included.

Chuck

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
301 319-9781

Personal/Professional Site: 
http://users.starpower.net/cwhite571/professional/ 

> sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" 
[7] "base"     

other attached packages:
     lme4   lattice    Matrix 
"0.995-2" "0.12-11" "0.995-4" 
>
m1<-lmer(cbind(Treat.Landed,Control.Landed)~Repellant+Hour.After.Applica
tion+(1|Volunteer)+(1|Date),
+ family=quasibinomial, method='Laplace')
Error in glm.fit(X, Y, weights = weights, offset = offset, family =
family,  : 
	NAs in V(mu)
########################################################################
##
> sessionInfo()
R version 2.2.1, 2005-12-20, i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" 
[7] "base"     

other attached packages:
     lme4   lattice    Matrix 
"0.995-1" "0.12-11" "0.995-1" 
>
m1<-lmer(cbind(Treat.Landed,Control.Landed)~Repellant+Hour.After.Applica
tion+(1|Volunteer)+(1|Date),
+ family=quasibinomial, method='Laplace')
  EM iterations
  0 1643.816 ( 273.179: -0.0214) ( 90.3079: -0.0282)
  1 1640.765 ( 366.319: -0.0102) ( 248.902:-0.00169)
  2 1640.187 ( 437.442:-0.00561) ( 278.171:-0.000393)
  3 1639.953 ( 489.655:-0.00341) ( 285.979:-0.000160)
  4 1639.846 ( 527.983:-0.00221) ( 289.280:-9.04e-005)
  5 1639.793 ( 556.263:-0.00150) ( 291.184:-5.89e-005)
  6 1639.767 ( 577.229:-0.00105) ( 292.439:-4.07e-005)
  7 1639.753 ( 592.835:-0.000748) ( 293.311:-2.89e-005)
  8 1639.745 ( 604.485:-0.000541) ( 293.934:-2.09e-005)
  9 1639.741 ( 613.202:-0.000395) ( 294.386:-1.53e-005)
 10 1639.739 ( 619.736:-0.000291) ( 294.717:-1.12e-005)
 11 1639.738 ( 624.639:-0.000216) ( 294.961:-8.32e-006)
 12 1639.737 ( 628.322:-0.000161) ( 295.142:-6.19e-006)
 13 1639.736 ( 631.090:-0.000120) ( 295.277:-4.62e-006)
 14 1639.736 ( 633.172:-8.97e-005) ( 295.378:-3.46e-006)
 15 1639.736 ( 634.739:-6.72e-005) ( 295.453:-2.59e-006)
  0      1639.74: 0.00157545 0.00338463
  1      1639.74: 0.00156384 0.00338453
  2      1639.74: 0.00156308 0.00338224
  3      1639.74: 0.00156394 0.00338218
  4      1639.74: 0.00156367 0.00338136
  5      1639.74: 0.00156374 0.00338222
  6      1639.74: 0.00156367 0.00338219
  7      1639.74: 0.00156371 0.00338211
  8      1639.74: 0.00156366 0.00338206
  9      1639.74: 0.00156370 0.00338199
 10      1639.74: 0.00156370 0.00338199
  EM iterations
  0 1601.856 ( 639.508: 0.00108) ( 295.684: 0.00140)
  1 1601.814 ( 620.816:0.000875) ( 267.871:0.000262)
  2 1601.802 ( 606.495:0.000663) ( 263.248:6.65e-005)
  0      1601.80: 0.00164882 0.00379870
  1      1601.79: 0.00181161 0.00380177
  2      1601.79: 0.00176152 0.00395670
  3      1601.79: 0.00174046 0.00388111
  4      1601.79: 0.00176162 0.00387505
  5      1601.79: 0.00175271 0.00385493
  6      1601.79: 0.00176027 0.00385121
  7      1601.79: 0.00175618 0.00385019
  8      1601.79: 0.00175420 0.00384200
  9      1601.79: 0.00175615 0.00384174
 10      1601.79: 0.00175577 0.00383981
 11      1601.79: 0.00175577 0.00383981
  EM iterations
  0 1608.593 ( 569.550:-0.000323) ( 260.429:-0.000384)
  1 1608.591 ( 574.148:-0.000245) ( 267.114:-6.56e-005)
  2 1608.590 ( 577.686:-0.000179) ( 268.289:-1.70e-005)
  0      1608.59: 0.00173104 0.00372732
  1      1608.59: 0.00168995 0.00372648
  2      1608.59: 0.00170234 0.00368730
  3      1608.59: 0.00170328 0.00372838
  4      1608.59: 0.00170194 0.00372450
  5      1608.59: 0.00170465 0.00372141
  6      1608.59: 0.00170173 0.00371852
  7      1608.59: 0.00170315 0.00371466
  8      1608.59: 0.00170267 0.00371666
  9      1608.59: 0.00170246 0.00371667
 10      1608.59: 0.00170255 0.00371648
  EM iterations
  0 1608.661 ( 587.354:-6.52e-006) ( 269.072:-3.50e-006)
  1 1608.661 ( 587.452:-4.87e-006) ( 269.135:-7.21e-007)
  2 1608.661 ( 587.525:-3.58e-006) ( 269.148:-2.53e-007)
  0      1608.66: 0.00170206 0.00371543
  1      1608.66: 0.00170148 0.00371542
  2      1608.66: 0.00170148 0.00371524
  3      1608.66: 0.00170148 0.00371524
  4      1608.66: 0.00170148 0.00371524
  EM iterations
  0 1608.660 ( 587.724:-1.09e-008) ( 269.162:5.68e-008)
  1 1608.660 ( 587.724:-5.92e-009) ( 269.161:8.25e-009)
  2 1608.660 ( 587.724:-4.02e-009) ( 269.161:1.07e-009)
  0      1608.66: 0.00170148 0.00371525
  1      1608.66: 0.00170148 0.00371525
  2      1608.66: 0.00170148 0.00371525
  EM iterations
  0 1608.660 ( 587.725:2.30e-010) ( 269.161:4.40e-010)
  1 1608.660 ( 587.725:1.83e-010) ( 269.161:7.32e-011)
  2 1608.660 ( 587.725:1.36e-010) ( 269.161:1.65e-011)
  0      1608.66: 0.00170148 0.00371525
  1      1608.66: 0.00170148 0.00371525
  0      11444.3: -1.57468 -0.114374 0.0891461 0.295675 0.322676
-0.0819240 0.0613226 -0.278625 0.252676 0.297048 0.00170148 0.00371525
  1      10461.4: -1.57468 -0.114375 0.0891456 0.295675 0.322677
-0.0819245 0.0613221 -0.278625 0.252676 0.297048 0.991395 0.146916
  2      10453.7: -1.57501 -0.118004 0.0914816 0.325860 0.316566
-0.101131 0.0995624 -0.273603 0.254018 0.290755 0.987977 0.148760
  3      10452.4: -1.57627 -0.106030 0.110693 0.344082 0.324971
-0.0605686 0.106017 -0.267820 0.245485 0.293816 0.976769 0.154694
  4      10451.5: -1.57797 -0.0856623 0.117621 0.334970 0.344039
-0.0620508 0.146502 -0.274762 0.257380 0.289187 0.968650 0.161734
  5      10450.2: -1.57831 -0.0912595 0.116721 0.344484 0.342080
-0.0541054 0.139780 -0.273456 0.253567 0.291741 0.966484 0.162502
  6      10450.1: -1.58093 -0.0960249 0.120939 0.348483 0.333461
-0.0497757 0.138781 -0.271218 0.250089 0.293405 0.960659 0.169695
  7      10449.8: -1.58338 -0.0947018 0.111567 0.349242 0.340198
-0.0491439 0.142989 -0.272130 0.253299 0.291556 0.953538 0.175865
  8      10449.7: -1.58601 -0.0918766 0.121701 0.342860 0.342149
-0.0469333 0.143516 -0.272566 0.251350 0.294516 0.946432 0.181555
  9      10449.6: -1.58943 -0.0910486 0.119831 0.352018 0.337230
-0.0454451 0.140744 -0.272584 0.256178 0.290521 0.939746 0.188275
 10      10449.5: -1.59166 -0.0935204 0.116089 0.350666 0.341477
-0.0510304 0.145357 -0.270167 0.247932 0.296975 0.933589 0.191757
 11      10449.4: -1.59447 -0.0957850 0.120865 0.343099 0.343630
-0.0473610 0.143548 -0.269864 0.255472 0.290163 0.927840 0.195228
 12      10449.1: -1.59658 -0.0901759 0.115450 0.350433 0.337106
-0.0458197 0.142501 -0.272300 0.252706 0.296086 0.921275 0.197706
 13      10449.0: -1.60106 -0.0990970 0.119617 0.350897 0.341253
-0.0521281 0.143267 -0.269335 0.253346 0.294103 0.914170 0.202652
 14      10448.9: -1.60360 -0.0884343 0.118272 0.344260 0.339332
-0.0487273 0.139916 -0.268830 0.255000 0.292972 0.906302 0.204724
 15      10448.8: -1.60708 -0.0952676 0.116544 0.350083 0.341797
-0.0438318 0.142868 -0.273987 0.255999 0.298785 0.898871 0.208172
 16      10448.6: -1.61004 -0.0936384 0.119330 0.347368 0.338683
-0.0502022 0.147287 -0.265043 0.253930 0.293356 0.891785 0.209803
 17      10448.4: -1.61572 -0.0922092 0.119692 0.348542 0.342165
-0.0453877 0.138443 -0.265999 0.256307 0.294703 0.883089 0.215247
 18      10448.4: -1.61897 -0.0915042 0.119826 0.346438 0.340360
-0.0538914 0.143168 -0.273599 0.260039 0.300747 0.876663 0.215492
 19      10448.1: -1.62170 -0.0959069 0.114654 0.350710 0.339778
-0.0497085 0.142583 -0.264006 0.254848 0.298294 0.869750 0.215881
 20      10448.0: -1.62425 -0.0925439 0.121244 0.342900 0.337405
-0.0463290 0.142340 -0.266442 0.261394 0.295969 0.861491 0.216977
 21      10447.8: -1.63033 -0.0931746 0.119288 0.346826 0.344990
-0.0511664 0.144844 -0.264393 0.258442 0.302008 0.853196 0.217943
 22      10447.7: -1.63145 -0.0916493 0.118219 0.352653 0.337064
-0.0455886 0.138990 -0.264261 0.263362 0.297954 0.845762 0.217530
 23      10447.4: -1.63584 -0.0963003 0.117473 0.344293 0.334583
-0.0471378 0.145940 -0.261730 0.260632 0.302341 0.838119 0.219307
 24      10447.3: -1.63779 -0.0939463 0.112834 0.349643 0.342489
-0.0464460 0.141193 -0.262403 0.264623 0.301302 0.828109 0.218743
 25      10447.1: -1.64064 -0.0902682 0.123597 0.349856 0.343063
-0.0531242 0.139255 -0.260173 0.263578 0.303512 0.820044 0.219590
 26      10446.8: -1.64322 -0.0935995 0.115354 0.350843 0.338844
-0.0474707 0.141749 -0.260037 0.264904 0.305199 0.809290 0.219342
 27      10446.6: -1.64619 -0.0945572 0.119069 0.342510 0.338353
-0.0456821 0.147251 -0.257505 0.267071 0.304463 0.798134 0.219737
 28      10446.3: -1.64999 -0.0947331 0.118412 0.349323 0.341641
-0.0476797 0.140415 -0.256079 0.268139 0.307440 0.786893 0.221035
 29      10446.2: -1.65084 -0.0886278 0.118513 0.347099 0.337841
-0.0542178 0.146136 -0.256303 0.268253 0.309310 0.775711 0.219820
 30      10445.8: -1.65368 -0.0922684 0.119903 0.347534 0.339643
-0.0467803 0.141972 -0.253972 0.271075 0.309212 0.763495 0.220808
 31      10445.7: -1.65432 -0.0934434 0.114649 0.351249 0.341609
-0.0511498 0.147058 -0.254179 0.267805 0.313519 0.751625 0.219946
 32      10445.3: -1.65655 -0.0926586 0.119584 0.348001 0.339997
-0.0492817 0.143213 -0.252695 0.272859 0.310855 0.738776 0.221235
 33      10445.1: -1.65724 -0.0930273 0.115031 0.351075 0.341181
-0.0465371 0.141509 -0.252119 0.268540 0.315546 0.725421 0.220247
 34      10444.7: -1.65927 -0.0927543 0.118641 0.348636 0.340453
-0.0497414 0.144647 -0.251144 0.273531 0.312746 0.711868 0.221569
 35      10444.4: -1.66014 -0.0927254 0.117734 0.349532 0.340062
-0.0444686 0.139415 -0.250910 0.272214 0.315254 0.697846 0.220846
 36      10441.9: -1.69136 -0.104869 0.114555 0.361175 0.357164
-0.0599348 0.140898 -0.232009 0.280566 0.334682 0.511023 0.248880
 37      10436.0: -1.70375 -0.0828958 0.116911 0.349359 0.350304
-0.0512119 0.156769 -0.216600 0.296166 0.352977 0.320550 0.255465
 38      10434.6: -1.70384 -0.0880833 0.121505 0.352662 0.345165
-0.0429543 0.146445 -0.218435 0.306303 0.345228 0.316458 0.254844
 39      10434.2: -1.70422 -0.0934200 0.122745 0.347648 0.341108
-0.0496339 0.146789 -0.218789 0.304552 0.349584 0.299201 0.252209
 40      10433.7: -1.70444 -0.0925859 0.115720 0.350862 0.340392
-0.0469562 0.141048 -0.216638 0.310348 0.347099 0.282092 0.249917
 41      10433.3: -1.70446 -0.0931099 0.118574 0.346919 0.339547
-0.0516847 0.143506 -0.213742 0.309185 0.352231 0.263546 0.246707
 42      10432.6: -1.70471 -0.0941059 0.113513 0.351775 0.338769
-0.0495017 0.139931 -0.208146 0.319561 0.356341 0.225173 0.237935
 43      10432.4: -1.70797 -0.0937163 0.121158 0.343195 0.343718
-0.0530556 0.140490 -0.193316 0.328469 0.374068 0.205060 0.213949
 44      10432.2: -1.71930 -0.0944377 0.117711 0.350098 0.336539
-0.0507862 0.141410 -0.175205 0.348005 0.387963 0.197098 0.190250
 45      10432.1: -1.72151 -0.0949748 0.116203 0.344533 0.342755
-0.0494401 0.137980 -0.173972 0.351927 0.392206 0.197160 0.187444
 46      10432.1: -1.72269 -0.0948574 0.117125 0.346960 0.339561
-0.0515517 0.139812 -0.171871 0.353255 0.393072 0.196681 0.187603
 47      10432.1: -1.72279 -0.0949262 0.117043 0.347170 0.339564
-0.0509965 0.139315 -0.171544 0.353063 0.393329 0.196471 0.187595
 48      10432.1: -1.72306 -0.0947929 0.117129 0.347058 0.339491
-0.0511342 0.139607 -0.171074 0.353431 0.393525 0.195959 0.187574
 49      10432.1: -1.72379 -0.0949714 0.116977 0.347264 0.339587
-0.0507304 0.139303 -0.170283 0.354277 0.394605 0.195653 0.187622
 50      10432.1: -1.73745 -0.0949212 0.117176 0.346877 0.338784
-0.0514425 0.139079 -0.154894 0.371388 0.410965 0.196248 0.187693
 51      10432.0: -1.74347 -0.0950768 0.116443 0.347248 0.340567
-0.0494151 0.141337 -0.145484 0.376406 0.418066 0.196017 0.187831
 52      10432.0: -1.74954 -0.0940385 0.118101 0.347600 0.340126
-0.0478509 0.143164 -0.139359 0.385729 0.424524 0.196150 0.187119
 53      10432.0: -1.75110 -0.0936333 0.117473 0.347457 0.340236
-0.0495394 0.140504 -0.138277 0.385992 0.426828 0.195005 0.188405
 54      10432.0: -1.75362 -0.0954679 0.116741 0.346808 0.339022
-0.0500229 0.140646 -0.136244 0.388144 0.427714 0.194715 0.189040
 55      10432.0: -1.75554 -0.0949921 0.117023 0.347058 0.339582
-0.0496439 0.141035 -0.134474 0.389739 0.430382 0.194467 0.186809
 56      10432.0: -1.75591 -0.0944204 0.117272 0.347378 0.339771
-0.0498677 0.140480 -0.133556 0.391030 0.430849 0.194460 0.186355
 57      10432.0: -1.75622 -0.0945095 0.117389 0.347319 0.339824
-0.0499044 0.140630 -0.132562 0.391734 0.432105 0.194408 0.187217
 58      10432.0: -1.75686 -0.0947003 0.117059 0.347200 0.339619
-0.0495625 0.140879 -0.131663 0.392896 0.432997 0.194223 0.187654
 59      10432.0: -1.75778 -0.0947472 0.117253 0.347206 0.339628
-0.0495707 0.140869 -0.130626 0.393636 0.433921 0.194453 0.186924
 60      10432.0: -1.75877 -0.0944993 0.117209 0.347352 0.339856
-0.0498309 0.140826 -0.129775 0.394661 0.434882 0.194274 0.186688
 61      10432.0: -1.75948 -0.0944810 0.117426 0.347291 0.339689
-0.0496809 0.140634 -0.128936 0.395592 0.435697 0.194265 0.187723
 62      10432.0: -1.75997 -0.0947607 0.117194 0.347261 0.339698
-0.0495161 0.140802 -0.127874 0.396307 0.436611 0.194448 0.186709
 63      10432.0: -1.75997 -0.0946737 0.117224 0.347231 0.339640
-0.0496096 0.140894 -0.127978 0.396412 0.436636 0.194408 0.186713
 64      10432.0: -1.76005 -0.0946805 0.117222 0.347294 0.339677
-0.0495418 0.140844 -0.127949 0.396448 0.436763 0.194315 0.186808
 65      10432.0: -1.76015 -0.0946524 0.117252 0.347272 0.339682
-0.0495778 0.140898 -0.127841 0.396564 0.436825 0.194276 0.186895
 66      10432.0: -1.76026 -0.0946463 0.117233 0.347280 0.339677
-0.0495408 0.140861 -0.127736 0.396679 0.436936 0.194229 0.186908
 67      10432.0: -1.76037 -0.0946545 0.117245 0.347265 0.339681
-0.0495587 0.140902 -0.127618 0.396777 0.437052 0.194202 0.186926
 68      10432.0: -1.76046 -0.0946495 0.117251 0.347297 0.339682
-0.0495406 0.140898 -0.127506 0.396928 0.437150 0.194192 0.186917
 69      10432.0: -1.76055 -0.0946422 0.117237 0.347264 0.339676
-0.0495445 0.140893 -0.127380 0.397023 0.437290 0.194186 0.186930
 70      10432.0: -1.76066 -0.0946661 0.117261 0.347292 0.339685
-0.0495541 0.140922 -0.127271 0.397146 0.437395 0.194167 0.186931
 71      10432.0: -1.76090 -0.0946560 0.117228 0.347271 0.339666
-0.0495355 0.140895 -0.127036 0.397390 0.437615 0.194173 0.186938
 72      10432.0: -1.76094 -0.0946313 0.117272 0.347291 0.339721
-0.0495014 0.140922 -0.126762 0.397597 0.437908 0.194168 0.187017
 73      10432.0: -1.76117 -0.0946427 0.117274 0.347272 0.339696
-0.0495377 0.140953 -0.126576 0.397825 0.438082 0.194202 0.186803
 74      10432.0: -1.76158 -0.0946616 0.117253 0.347316 0.339677
-0.0495179 0.140918 -0.126503 0.397935 0.438176 0.194094 0.186901
 75      10432.0: -1.76140 -0.0946345 0.117251 0.347299 0.339693
-0.0495395 0.140916 -0.126245 0.398162 0.438420 0.194118 0.186986
 76      10432.0: -1.76141 -0.0946363 0.117267 0.347278 0.339688
-0.0495374 0.140918 -0.126244 0.398163 0.438431 0.194115 0.186983
 77      10432.0: -1.76143 -0.0946396 0.117255 0.347286 0.339689
-0.0495327 0.140920 -0.126237 0.398173 0.438435 0.194114 0.186975
 78      10432.0: -1.76147 -0.0946394 0.117259 0.347277 0.339683
-0.0495314 0.140928 -0.126219 0.398188 0.438453 0.194112 0.186960
 79      10432.0: -1.76169 -0.0946427 0.117253 0.347281 0.339694
-0.0495116 0.140935 -0.126132 0.398263 0.438534 0.194134 0.186893
 80      10432.0: -1.76166 -0.0946452 0.117262 0.347287 0.339683
-0.0495108 0.140937 -0.125985 0.398428 0.438681 0.194128 0.186879
 81      10432.0: -1.76189 -0.0946304 0.117263 0.347296 0.339691
-0.0495325 0.140936 -0.125929 0.398483 0.438741 0.194137 0.186972
 82      10432.0: -1.76190 -0.0946413 0.117247 0.347281 0.339689
-0.0495284 0.140929 -0.125781 0.398617 0.438904 0.194099 0.186915
 83      10432.0: -1.76201 -0.0946397 0.117254 0.347278 0.339693
-0.0494968 0.140946 -0.125642 0.398758 0.439026 0.194118 0.186973
 84      10432.0: -1.76218 -0.0946350 0.117265 0.347288 0.339688
-0.0495205 0.140928 -0.125534 0.398871 0.439121 0.194086 0.186879
 85      10432.0: -1.76226 -0.0946434 0.117254 0.347285 0.339675
-0.0495004 0.140968 -0.125402 0.399021 0.439273 0.194094 0.186884
 86      10432.0: -1.76230 -0.0946341 0.117261 0.347287 0.339686
-0.0494978 0.140955 -0.125331 0.399077 0.439348 0.194119 0.186884
 87      10432.0: -1.76230 -0.0946341 0.117261 0.347287 0.339686
-0.0494978 0.140955 -0.125331 0.399077 0.439348 0.194119 0.186884
>

-----Original Message-----
From: Douglas Bates [mailto:dmbates at gmail.com] 
Sent: Friday, January 27, 2006 6:33 PM
To: White, Charles E WRAIR-Wash DC
Subject: Re: lme4_0.995-2/Matrix_0.995-4 upgrade introduces error
messages (change management)

Sorry to hear of the difficulties, Charles.

One thing to try is to turn on the verbose output so fit your models
after setting

options(verbose=TRUE)

Another thing that may be interesting to try is to go to optimization
of the Laplace approximation deviance directly without doing any PQL
iterations.

My theory has been that the PQL iterations help to stabilize the
optimization process but it appears that sometimes they do more harm
than good.

Can you let me know what the verbose output shows?  The thing to watch
for is what I call "ping-ponging" of the PQL iterations.  One set of
iterations converges to one optimum that determines weights that send
it to another optimum that determines weights that sends it back to
the original optimum.


On 1/27/06, White, Charles E WRAIR-Wash DC
<charles.edwin.white at us.army.mil> wrote:
> I'll address two issues. The first is today's error message and the
other is change management for contributed packages on CRAN.
>
> TODAY'S ERROR MESSAGE
>
> I switched from the 0.995-1 versions of lme4 and Matrix to those
referenced in the subject line this afternoon. Prior to using these
packages on anything else, I applied them to code that 'worked'
(provided numerical results with no error messages) under the previous
set of packages. Since I can't provide the data, I realize this post may
be of limited usefulness. Rightly or wrongly, I've regressed my R
installation back to the 0.995-1 versions of lme4/Matrix... so I don't
think that I continue to have a problem.
>
> R version 2.2.1, 2005-12-20, i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
> [7] "base"
>
> other attached packages:
>      lme4   lattice    Matrix
> "0.995-2" "0.12-11" "0.995-4"
>
> > options(show.signif.stars=FALSE)
> >
m1a<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1
|date),
> + family=binomial(link='probit'), method='Laplace')
> Error in dev.resids(y, mu, weights) : argument wt must be a numeric
vector of length 1 or length 219
> > # probit doesn't converge
> >
m1b<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1
|date),
> + family=binomial, method='Laplace')
> Error in dev.resids(y, mu, weights) : argument wt must be a numeric
vector of length 1 or length 219
> > # logit is overdispersed
> >
m1<-lmer(cbind(prevented,control.count)~repellant+hour+(1|volunteer)+(1|
date),
> + family=quasibinomial, method='Laplace')
> Error in glm.fit(X, Y, weights = weights, offset = offset, family =
family,  :
>         NAs in V(mu)
> > m2<-lmer(cbind(prevented,control.count)~hour+(1|volunteer)+(1|date),
> + family=quasibinomial, method='Laplace')
> Error in glm.fit(X, Y, weights = weights, offset = offset, family =
family,  :
>         NAs in V(mu)
>
> CHANGE MANAGEMENT
>
> Does CRAN keep old versions of contributed packages someplace? If not,
the strategy I've implemented today is to maintain my own repository of
contributed packages that I use. Stuff happens and change management is
good.
>
> Chuck
>
> Charles E. White, Senior Biostatistician, MS
> Walter Reed Army Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> 301 319-9781
> Personal/Professional Site:
> http://users.starpower.net/cwhite571/professional/
>
>



From andy_liaw at merck.com  Mon Jan 30 15:17:41 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 09:17:41 -0500
Subject: [R] Help with R: functions
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED786@usctmx1106.merck.com>

This might be a bit closer to what Pryseley wanted:

st <- function(dat, x, y) {
    f <- formula(substitute(y ~ x), env=environment(dat))
    plot(f, dat)
    abline(lm(f, dat))
}

Note that the variable names in the plot when tested on `dats' as Pryseley
created.

Andy

From: Jacques VESLOT
> 
> st <- function(data, x, y){
>     attach(data)
>     rcc <- coef(lm(y~x))
>     plot(x,y)
>     abline(rcc)
>     detach(data)}
> 
> st(data=dats, x=visual24, y=visual52)
> 
> 
> 
> Pryseley Assam a ??crit :
> 
> >Hello R-users
> >   
> >  I am new to R and trying to write some functions. I have 
> problems writing functions that takes a data set as an 
> arguement and uses variables in the data. I illustrate my 
> problem with a small example below:
> >   
> >   sample data   #------------------
> >  visual24<-rnorm(30,3,5)
> >  visual52<-rt(30,7)
> >  dats<- data.frame(cbind(visual24,visual52))
> >  remove(visual24, visual52)
> >   
> >  # first code
> >  #--------------
> >  st <-function(data,x,y){
> >  rcc<-coef(lm(y~x))
> >  plot(x,y)
> >  abline(rcc[1],rcc[2])
> >  }
> >  st(data=dats,x=dats$visual24,y=dats$visual52)
> >   
> >  This code works fine, but with such a code the data as an 
> arguement to the funtion is not necessary. 
> >  However, i wish to write a function that reads the 
> variables from the data directly.
> >  I tried using the function below but it does not work. 
> >   
> >  # second code
> >  #------------------
> >  st <-function(data,x,y){
> >  rcc<-coef(lm(data$y~data$x))
> >  plot(data$x,data$y)
> >  abline(rcc[1],rcc[2])
> >  }
> >  st(dats,visual24,visual52)
> >   
> >  I wish to inquire if any one has an idea of what i need to 
> adjust in the function so that it works.
> >  I believe that the referencing $x or $y in the function is 
> not doing the correct thing.
> >   
> >  Better still, will it be a problem if i code the functions 
> as in the first code above?
> >  I mean given that they will be used to create a library
> >   
> >  Best regards
> >  Pryseley
> >   
> >
> >
> >		
> >---------------------------------
> >
> >
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jacques.veslot at cirad.fr  Mon Jan 30 15:19:36 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Mon, 30 Jan 2006 18:19:36 +0400
Subject: [R] match() & seq()
In-Reply-To: <43DE2798.16785.18C74FB@localhost>
References: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com>
	<43DE2798.16785.18C74FB@localhost>
Message-ID: <43DE2078.8010304@cirad.fr>

sorry if it has already been discussed but i can't understand this:

 > seq(0.1,1,by=0.1)
 [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
 > match(0.1,seq(0.1,1,by=0.1))
[1] 1
 > match(0.2,seq(0.1,1,by=0.1))
[1] 2
 > match(0.3,seq(0.1,1,by=0.1))
[1] NA
 > match(0.4,seq(0.1,1,by=0.1))
[1] 4

 > R.version
         _             
platform i386-pc-mingw32
arch     i386          
os       mingw32       
system   i386, mingw32 
status                 
major    2             
minor    2.1           
year     2005          
month    12            
day      20            
svn rev  36812         
language R



From dmbates at gmail.com  Mon Jan 30 15:20:55 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 30 Jan 2006 08:20:55 -0600
Subject: [R] lme4_0.995-2/Matrix_0.995-4 upgrade introduces error
	messages (change management)
In-Reply-To: <8BAEC5E546879B4FAA536200A292C614F7ECF2@AMEDMLNARMC135.amed.ds.army.mil>
References: <8BAEC5E546879B4FAA536200A292C614F7ECF2@AMEDMLNARMC135.amed.ds.army.mil>
Message-ID: <40e66e0b0601300620x5c619d19obc49c6e7cd009331@mail.gmail.com>

Thanks Charles.

The error is occuring in a fit of a glm using the fixed-effects terms
only.  This fit provides the starting estimates for the fixed effects
in the GLMM model.  I changed the way that this fit is performed and
apparently didn't do it correctly.

I think it would be better if you and I corresponded off-list about this.

On 1/30/06, White, Charles E WRAIR-Wash DC
<charles.edwin.white at us.army.mil> wrote:
> I hope I'm not making your life unnecessarily difficult. As I will
> demonstrate below my signature, my original straight application of
> lme4_0.995-2/Matrix_0.995-4 is failing without providing any
> optimization information. For reference, I've provided optimization
> output from lme4_0.995-1/Matrix_0.995-1. Including the lmer command
> control=list(PQLmaxIt=0) or control=list(PQLmaxIt=10) produces exactly
> the same error as when the commands are not included.
>
> Chuck
>
> Charles E. White, Senior Biostatistician, MS
> Walter Reed Army Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> 301 319-9781



From andy_liaw at merck.com  Mon Jan 30 15:27:16 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 09:27:16 -0500
Subject: [R] match() & seq()
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED787@usctmx1106.merck.com>

Hope this is clear:

> x <- seq(0.1, 1, by=0.1)
> 0.3 == x[3]
[1] FALSE
> abs(0.3 - x[3])
[1] 5.551115e-17


Andy

From: Jacques VESLOT
> 
> sorry if it has already been discussed but i can't understand this:
> 
>  > seq(0.1,1,by=0.1)
>  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
>  > match(0.1,seq(0.1,1,by=0.1))
> [1] 1
>  > match(0.2,seq(0.1,1,by=0.1))
> [1] 2
>  > match(0.3,seq(0.1,1,by=0.1))
> [1] NA
>  > match(0.4,seq(0.1,1,by=0.1))
> [1] 4
> 
>  > R.version
>          _             
> platform i386-pc-mingw32
> arch     i386          
> os       mingw32       
> system   i386, mingw32 
> status                 
> major    2             
> minor    2.1           
> year     2005          
> month    12            
> day      20            
> svn rev  36812         
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.be  Mon Jan 30 15:35:44 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 30 Jan 2006 15:35:44 +0100
Subject: [R] match() & seq()
References: <b1f16d9d0601291411g54022abcp67813ce6d0155cbe@mail.gmail.com><43DE2798.16785.18C74FB@localhost>
	<43DE2078.8010304@cirad.fr>
Message-ID: <018101c625aa$73df2630$0540210a@www.domain>

the problem is in

0.3 == seq(0.1,1,by=0.1)[3]

versus

all.equal(0.3, seq(0.1,1,by=0.1)[3])

look at ?Comparison for more info.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jacques VESLOT" <jacques.veslot at cirad.fr>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, January 30, 2006 3:19 PM
Subject: [R] match() & seq()


> sorry if it has already been discussed but i can't understand this:
>
> > seq(0.1,1,by=0.1)
> [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
> > match(0.1,seq(0.1,1,by=0.1))
> [1] 1
> > match(0.2,seq(0.1,1,by=0.1))
> [1] 2
> > match(0.3,seq(0.1,1,by=0.1))
> [1] NA
> > match(0.4,seq(0.1,1,by=0.1))
> [1] 4
>
> > R.version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From ripley at stats.ox.ac.uk  Mon Jan 30 15:37:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jan 2006 14:37:35 +0000 (GMT)
Subject: [R] R on dual-core machines
In-Reply-To: <06e501c6259c$28121f40$a7fdfea9@TAMARA>
References: <06e501c6259c$28121f40$a7fdfea9@TAMARA>
Message-ID: <Pine.LNX.4.61.0601301328490.11920@gannet.stats>

On Mon, 30 Jan 2006, [iso-8859-2] Alea }iberna wrote:

> Dear expeRts!
>
> I'm thinking of buying a new computer and am considering dual-core
> processors, such as AMD Athlon64 X2. Since I'm not a computer expert, pleas
> forgive me if some of my questions are silly.
>
> First, am I correct that using a dual-core processor is (for R point of
> view) the same as using a computer with two processors?

Depends on the OS (R does not get to see at that level), but that's a fair 
presumption.  For example, our dual dual-core Opteron box is reported as 
having four processors by Linux.

> If that is true, the posts I found on the list imply that using such a
> processor can usually bring significant improvements (in computational time)
> only if the case where the core (C or sometimes R) is specially designed for
> multiple processors (see comments below).
>
> So based on these and other comments I can conclude that if I'm not prepared
> (able) to make such modifications, I can aspect improvements only in this
> two areas:
> 1.	If I am running two instances of R.
> 2.	If I'm running several other programs on the computer beside R, the
> programs and R would run faster, since they would not "compete" for
> processor time (so much)

Yes, but running multiple R instances can be very useful.

Our long-term experience with multiple-processor machines is that you do 
need to ensure you have adequate RAM and plenty of swap space, especially 
on OSes that do not handle out-of-swap gracefully.

>
> Thanks in advance for any useful suggestions,
> Ales Ziberna
>
> P.S.: Useful posts on the list follow:
>
>
>
> It depends on the usage pattern. If you run multiple CPU-bound processes in
> parallel without too much coordination (parallel make is a good example,
> simulations another), then you get close to double up from a dual. For a
> single R process, you can get something like 40% improvement in large linear
> algebra problems, using a threaded ATLAS.
> For other problems the speedup is basically nil. There is some potential in
> threading R or (much easier) some of its vector operations, but that is not
> even on the drawing board at this stage.
>
> ------------------------------------------------------------------------
>
> If you want to exploit multiple processors, you can write code (e.g., in C)
> called from R (e.g., through .Call or .C) that performs parallel/threaded
> computations in a thread-safe way (e.g., without calling back into R).
> ---
> Another possibility is to replace the BLAS/LAPACK library with a thread-safe
> version. This provides a boost to those R algorithms exploiting these
> libraries.

Replace `thread-safe' by `multi-threaded', as you do need the BLAS to 
use multiple threads itself (not be told to).  See the R-admin manul for 
how to do this with Linux versions of OS.

> ---
> An alternative is to do all of the parallelization within R using nice tools
> like the snow package combined with Rmpi.  If your task is computationally
> intensive on the R side, but not on the client, then parallelizing R code
> may be the better way to go.  All depends on your application, I think.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Mon Jan 30 15:44:10 2006
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Jan 2006 15:44:10 +0100 (CET)
Subject: [R] match() & seq()
In-Reply-To: <43DE2078.8010304@cirad.fr>
Message-ID: <Pine.LNX.4.44.0601301528200.4578-100000@reclus.nhh.no>

On Mon, 30 Jan 2006, Jacques VESLOT wrote:

> sorry if it has already been discussed but i can't understand this:
> 
>  > seq(0.1,1,by=0.1)
>  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
>  > match(0.1,seq(0.1,1,by=0.1))
> [1] 1
>  > match(0.2,seq(0.1,1,by=0.1))
> [1] 2
>  > match(0.3,seq(0.1,1,by=0.1))
> [1] NA
>  > match(0.4,seq(0.1,1,by=0.1))
> [1] 4

FAQ 7.31
http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

> print(seq(0.1,1,by=0.1), digits=20)
 [1] 0.10000000000000000555 0.20000000000000001110 0.30000000000000004441
 [4] 0.40000000000000002220 0.50000000000000000000 0.59999999999999997780
 [7] 0.70000000000000006661 0.80000000000000004441 0.90000000000000002220
[10] 1.00000000000000000000
> match(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9), seq(0.1,1,by=0.1))
[1]  1  2 NA  4  5  6 NA  8  9
> all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0), seq(0.1,1,by=0.1), 
+ tolerance = .Machine$double.eps ^ 2)
[1] "Mean relative  difference: 1.665335e-16"
> all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0), seq(0.1,1,by=0.1))
[1] TRUE


> 
>  > R.version
>          _             
> platform i386-pc-mingw32
> arch     i386          
> os       mingw32       
> system   i386, mingw32 
> status                 
> major    2             
> minor    2.1           
> year     2005          
> month    12            
> day      20            
> svn rev  36812         
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From sfalcon at fhcrc.org  Mon Jan 30 15:47:37 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 30 Jan 2006 06:47:37 -0800
Subject: [R] 'all' inconsistent?
In-Reply-To: <Pine.LNX.4.61.0601300739520.8419@gannet.stats> (Brian Ripley's
	message of "Mon, 30 Jan 2006 07:52:59 +0000 (GMT)")
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
Message-ID: <m24q3ldb7a.fsf@ziti.local>

On 29 Jan 2006, ripley at stats.ox.ac.uk wrote:

> On Sun, 29 Jan 2006, Elizabeth Purdom wrote:
>
>> I came across the following behavior, which seems illogical to me.
>
> What did you expect and why?
>
>> I don't know if it is a bug or if I'm missing something:
>>
>>> all(logical(0))
>> [1] TRUE
>
> All the values are true, all none of them.

I thought all the values are false, all none of them, because there
aren't any that are true:

any(logical(0))
[1] FALSE

I can see how someone might expect an error, or NA, or FALSE in the
above two cases.  It is harder for me to see when all(logical(0))
being TRUE would be useful.

glass-is-half-emptily-yours,

+ seth



From ggrothendieck at gmail.com  Mon Jan 30 15:51:28 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 09:51:28 -0500
Subject: [R] matlab-like constant matrix initialization?
In-Reply-To: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
References: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
Message-ID: <971536df0601300651o7f648a40h524e8b0ed33e7a4a@mail.gmail.com>

Do you mean they are in the clipboard in the format shown?
If that is the case then do this:

  as.matrix(read.table("clipboard"))


On 1/30/06, Michael <comtech.usa at gmail.com> wrote:
> Hi all,
>
> Suppose I have the following matrix which is a constant matrix I've copied
> from some other document:
>
> 1.2  3.4 1.4 ...
> 2.3  3.7 2.6 ...
> ...
>
> How do I make it into a matrix or array in R?
>
> What is the fastest way of initializing a constant matrix with this
> copy/pasted values?
>
> Thanks a lot!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sfalcon at fhcrc.org  Mon Jan 30 15:55:59 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 30 Jan 2006 06:55:59 -0800
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	(jim holtman's message of "Sun, 29 Jan 2006 20:38:49 -0500")
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
Message-ID: <m2zmldbw8w.fsf@ziti.local>

> On 1/29/06, context grey <mobygeek at yahoo.com> wrote:
>>
>> Hi,
>>
>> Is there something like a hashtable or (python)
>> dictionary in R/Splus?

On 29 Jan 2006, jholtman at gmail.com wrote:
> use a 'list':

Most of the time, a list will be what you want, but it has some
important differences from a Python dictionary.  In particular, one
can end up with duplicate keys.  Here is an example:

    h <- list()
    h[["foo"]] <- 1
    h[[2]] <- 2
    names(h) <- rep("foo", 2)
    h
      $foo
      [1] 1
      
      $foo
      [1] 2
    
    h[["foo"]]
      [1] 1

'environments' may be what you are looking for, see help for new.env().

    h <- new.env(hash=TRUE)

An important thing to keep in mind with environments, however, is that
they are an exception to the pass by value semantics of the language.
Environments are not copied when passed as function args.  This has
its uses, but can be confusing.


+ seth



From h.wickham at gmail.com  Mon Jan 30 16:09:31 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 30 Jan 2006 09:09:31 -0600
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <Pine.LNX.4.61.0601300723140.8419@gannet.stats>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>
	<Pine.LNX.4.61.0601300723140.8419@gannet.stats>
Message-ID: <f8e6ff050601300709p74895c07p929b6b0fbc2cc326@mail.gmail.com>

> > Is a list O(1) for setting and getting?
>
> Can you elaborate?  R is a vector language, and normally you create a list
> in one pass, and you can retrieve multiple elements at once.

When you use a hash table you expect it to be O(1) (on average) for
getting and setting values (conditional on having a good hash
function, a large enough table etc...).  (and thanks for the pointer
to that RNEWS item)

> Indexing by number is O(1) except where replacement causes the list vector
> to be copied.  There is always the option to use match() to convert to
> numeric indexing.

I read this to mean that setting a value in list will be O(n) (where n
is the length of the new list) - If you blindly expect a list to act
like a hash table you will be badly surprised.  If you are copying an
algorithm from another language, you will probably need to rethink the
way that updates work.  If however, you just want an object that can
be indexed by a string, then a list will be fine.

Regards,

Hadley



From GPetris at uark.edu  Mon Jan 30 16:12:01 2006
From: GPetris at uark.edu (Giovanni Petris)
Date: Mon, 30 Jan 2006 09:12:01 -0600 (CST)
Subject: [R] 'all' inconsistent?
In-Reply-To: <m24q3ldb7a.fsf@ziti.local> (message from Seth Falcon on Mon, 30
	Jan 2006 06:47:37 -0800)
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
	<m24q3ldb7a.fsf@ziti.local>
Message-ID: <200601301512.k0UFC1Wc029269@definetti.uark.edu>


> I thought all the values are false, all none of them, because there
> aren't any that are true:
> 
> any(logical(0))
> [1] FALSE
> 

This is for the same reason why a product over an empty set of
factors is 1, and a sum over an empty set of terms is 0.

GP

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From Bjorn.VanCampenhout at ua.ac.be  Mon Jan 30 16:17:15 2006
From: Bjorn.VanCampenhout at ua.ac.be (Van Campenhout Bjorn)
Date: Mon, 30 Jan 2006 16:17:15 +0100
Subject: [R] non linear 3SLS with constraints
Message-ID: <28F23EB76AB6184983FDF87CEC412EB410B8F9@xmail02.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/de460b8a/attachment.pl

From murdoch at stats.uwo.ca  Mon Jan 30 16:18:24 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 Jan 2006 10:18:24 -0500
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <m2zmldbw8w.fsf@ziti.local>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<m2zmldbw8w.fsf@ziti.local>
Message-ID: <43DE2E40.60709@stats.uwo.ca>

On 1/30/2006 9:55 AM, Seth Falcon wrote:
>> On 1/29/06, context grey <mobygeek at yahoo.com> wrote:
>>>
>>> Hi,
>>>
>>> Is there something like a hashtable or (python)
>>> dictionary in R/Splus?
> 
> On 29 Jan 2006, jholtman at gmail.com wrote:
>> use a 'list':
> 
> Most of the time, a list will be what you want, but it has some
> important differences from a Python dictionary.  In particular, one
> can end up with duplicate keys.  Here is an example:
> 
>     h <- list()
>     h[["foo"]] <- 1
>     h[[2]] <- 2
>     names(h) <- rep("foo", 2)
>     h
>       $foo
>       [1] 1
>       
>       $foo
>       [1] 2
>     
>     h[["foo"]]
>       [1] 1
> 
> 'environments' may be what you are looking for, see help for new.env().
> 
>     h <- new.env(hash=TRUE)
> 
> An important thing to keep in mind with environments, however, is that
> they are an exception to the pass by value semantics of the language.
> Environments are not copied when passed as function args.  This has
> its uses, but can be confusing.

Both lists and environments have other oddities, too.  For example, 
"mean" would be found in your environment above by functions like get(), 
since the base environment is its parent; "f" would be found in your 
list, since partial name matching is used in lists.  You can avoid the 
parent problem in R 2.2.x+ by setting the parent explicitly to emptyenv().

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Mon Jan 30 16:22:20 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 Jan 2006 16:22:20 +0100
Subject: [R] 'all' inconsistent?
In-Reply-To: <m24q3ldb7a.fsf@ziti.local>
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
	<m24q3ldb7a.fsf@ziti.local>
Message-ID: <43DE2F2C.4080703@statistik.uni-dortmund.de>

Seth Falcon wrote:

> On 29 Jan 2006, ripley at stats.ox.ac.uk wrote:
> 
> 
>>On Sun, 29 Jan 2006, Elizabeth Purdom wrote:
>>
>>
>>>I came across the following behavior, which seems illogical to me.
>>
>>What did you expect and why?
>>
>>
>>>I don't know if it is a bug or if I'm missing something:
>>>
>>>
>>>>all(logical(0))
>>>
>>>[1] TRUE
>>
>>All the values are true, all none of them.
> 
> 
> I thought all the values are false, all none of them, because there
> aren't any that are true:
> 
> any(logical(0))
> [1] FALSE
> 
> I can see how someone might expect an error, or NA, or FALSE in the
> above two cases.  It is harder for me to see when all(logical(0))
> being TRUE would be useful.

Current behaviour is consistent in so far that identical(all(x), 
!any(!x)) is TRUE and definition of any() is obvious.

Uwe


> 
> glass-is-half-emptily-yours,
> 
> + seth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Mon Jan 30 16:23:14 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 30 Jan 2006 15:23:14 +0000 (UTC)
Subject: [R] multiple hystograms
References: <EDF44EE7E2B92B178C432884@alsp-stats6.alspac.bris.ac.uk>
	<06e101c62597$275ccb40$a7fdfea9@TAMARA>
Message-ID: <loom.20060130T161903-894@post.gmane.org>

Ale iberna <ales.ziberna <at> gmail.com> writes:

> 
> ?par
> Look for arguments 
> mfrow and mfcol 
> 

Original Post
> [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of R Granell, 
Medicine
> 
> Is there a way of obtaining more than one histogram in the graph-window so
> you can edit all the plots simultaneously?
> 
> and how about scatter plots?

The word "edit" confuses me a bit. If should mean "view", Ale' showed one way, 
but I would suggest you better look at trellis plots nowadays. Check help for 
xyplot and histogram.

Dieter



From Camarda at demogr.mpg.de  Mon Jan 30 16:29:38 2006
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Mon, 30 Jan 2006 16:29:38 +0100
Subject: [R] Subsetting a matrix without for-loop
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/17617491/attachment.pl

From sfalcon at fhcrc.org  Mon Jan 30 16:49:05 2006
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 30 Jan 2006 07:49:05 -0800
Subject: [R] 'all' inconsistent?
In-Reply-To: <43DE2F2C.4080703@statistik.uni-dortmund.de> (Uwe Ligges's
	message of "Mon, 30 Jan 2006 16:22:20 +0100")
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
	<m24q3ldb7a.fsf@ziti.local>
	<43DE2F2C.4080703@statistik.uni-dortmund.de>
Message-ID: <m2irs1btse.fsf@ziti.local>

On 30 Jan 2006, ligges at statistik.uni-dortmund.de wrote:
> Current behaviour is consistent in so far that identical(all(x),
> !any(!x)) is TRUE and definition of any() is obvious.

That helps, thanks.  I'm not sure I've had enough coffee to continue,
but, for the set analogy I think we are saying:

logical(0) is the empty set {}.
Complement of {} is the universal set U.

Then !logical(0)  == !{} == U.  any(U) is TRUE, isn't it?  

I guess the real message is that you need to protect yourself by
testing for positive length first.

+ seth



From adi at roda.ro  Mon Jan 30 16:47:53 2006
From: adi at roda.ro (Adrian Dusa)
Date: Mon, 30 Jan 2006 17:47:53 +0200
Subject: [R] yet another vectorization question
In-Reply-To: <43DE0942.6060303@sciviews.org>
References: <200601301334.40440.adi@roda.ro> <43DDFF1C.1080504@cirad.fr>
	<43DE0942.6060303@sciviews.org>
Message-ID: <200601301747.53555.adi@roda.ro>

On Monday 30 January 2006 14:40, Philippe Grosjean wrote:
> Hello,
> Not exactly the same. By the way, why do you use do.call()? Couldn't you
> do simply:
> expand.grid(split(t(replicate(3, c(0, 1, NA))), 1:3))
> Best,
> Philippe Grosjean
>
> Jacques VESLOT wrote:
> > this looks similar:
> > do.call(expand.grid,split(t(replicate(3,c(0,1,NA))),1:3))

Sigh, what a pity. It is indeed not the same...
So close to a one-liner though.

I come back to my original question: is it possible to modify the content of a 
matrix, using apply on a different matrix?
In my original function, the slow part is:
## ...
for (k in 1:ncol(idk)) {
   end.row <- start.row + nrow(tt) - 1
   return.matrix[start.row:end.row, idk[ , k]] <- tt
   start.row <- end.row + 1
}
## ...

I'd like to use apply on the "idk" matrix (to get rid of the for loop) and 
write the contents of "tt" in the "result.matrix"...

Best,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From ligges at statistik.uni-dortmund.de  Mon Jan 30 16:58:48 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 30 Jan 2006 16:58:48 +0100
Subject: [R] 'all' inconsistent?
In-Reply-To: <m2irs1btse.fsf@ziti.local>
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>	<m24q3ldb7a.fsf@ziti.local>	<43DE2F2C.4080703@statistik.uni-dortmund.de>
	<m2irs1btse.fsf@ziti.local>
Message-ID: <43DE37B8.9010107@statistik.uni-dortmund.de>

Seth Falcon wrote:

> On 30 Jan 2006, ligges at statistik.uni-dortmund.de wrote:
> 
>>Current behaviour is consistent in so far that identical(all(x),
>>!any(!x)) is TRUE and definition of any() is obvious.
> 
> 
> That helps, thanks.  I'm not sure I've had enough coffee to continue,
> but, for the set analogy I think we are saying:
> 
> logical(0) is the empty set {}.
> Complement of {} is the universal set U.
> 
> Then !logical(0)  == !{} == U.  any(U) is TRUE, isn't it?  

Hmmm, "!" is for *logical* negation, and indeed identical(logical(0), 
!(logical(0))) is TRUE, hence my first statement holds.


> I guess the real message is that you need to protect yourself by
> testing for positive length first.

Yes, indeed.

Uwe



> + seth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From morlanes at cc.hut.fi  Mon Jan 30 17:16:13 2006
From: morlanes at cc.hut.fi (morlanes@cc.hut.fi)
Date: Mon, 30 Jan 2006 18:16:13 +0200 (EET)
Subject: [R] fExtreme packages
Message-ID: <1138637773.43de3bcd1cea8@webmail2.hut.fi>

 
Hello, 
 
I am a new user of R. I am trying to use the packages fBasics and fExtremes 
when i am running the examples I get few error. Could someone tell me what is 
happenig? Thank you beforehand. 
 
from Fbasics packages: 
 
 xmpfBasics() 
Error in file(file, "r") : unable to open connection 
In addition: Warning message: 
cannot open file '/usr/lib/R/library/fBasics/demoIndex'  
 
> source("fBasics-xmpTools") 
Error in file(file, "r", encoding = encoding) :  
	unable to open connection 
In addition: Warning message: 
cannot open file 'fBasics-xmpTools'  
 
from fExtremes: 
 
 data(danish) 
xmpExtremes("\nStart: Simulate a GPD Distributed Sample > ") 
 x = gpdSim(model = list(shape = 0.25, location = 0, scale = 1), n = 1000) 
Error in rgpd(n = n, xi = model$shape, mu = model$location, beta = 
model$scale) : unused argument(s) (mu ...) 
 
 fit = gpdFit(danish, threshold = 10, type = "mle") 
>  par(mfrow = c(1, 1)) 
>  gpdtailPlot(fit, main = "Danish Fire: GPD Tail Estimate", col = 
"steelblue4")  
Error in as.numeric(gpd.obj$upper.exceed) :  
	argument "gpd.obj" is missing, with no default



From ggrothendieck at gmail.com  Mon Jan 30 17:20:33 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 11:20:33 -0500
Subject: [R] Subsetting a matrix without for-loop
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>
Message-ID: <971536df0601300820t14367458ye7aadb417095bb1d@mail.gmail.com>

The result is linear in A so its a matter of finding the matrix to multiply it
by:

 matrix(c(rep(1,3), rep(0,7)), 3, 9, byrow = TRUE) %*% A

On 1/30/06, Camarda, Carlo Giovanni <Camarda at demogr.mpg.de> wrote:
> Dear R-users,
> I'm struggling in R in order to "squeeze" a matrix without using a
> for-loop.
> Although my case is a bit more complex, the following example should
> help you to understand what I would like to do, but without the slow
> for-loop.
> Thanks in advance,
> Carlo Giovanni Camarda
>
>
> A  <- matrix(1:54, ncol=6)      # my original matrix
> A.new <- matrix(nrow=3, ncol=6) # a new matrix which I'll fill
> # for-loop
> for(i in 1:nrow(A.new)){
>    B <- A[i:(i+2), ]   # selecting the rows
>    C <- apply(B,2,sum) # summing by columns
>    A.new[i,] <- C      # inserting in the new matrix
> }
>
>
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Jan 30 17:21:51 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 17:21:51 +0100
Subject: [R] 'all' inconsistent?
In-Reply-To: <m2irs1btse.fsf@ziti.local>
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
	<m24q3ldb7a.fsf@ziti.local>
	<43DE2F2C.4080703@statistik.uni-dortmund.de>
	<m2irs1btse.fsf@ziti.local>
Message-ID: <x2vew1bs9s.fsf@viggo.kubism.ku.dk>

Seth Falcon <sfalcon at fhcrc.org> writes:

> On 30 Jan 2006, ligges at statistik.uni-dortmund.de wrote:
> > Current behaviour is consistent in so far that identical(all(x),
> > !any(!x)) is TRUE and definition of any() is obvious.
> 
> That helps, thanks.  I'm not sure I've had enough coffee to continue,
> but, for the set analogy I think we are saying:
> 
> logical(0) is the empty set {}.
> Complement of {} is the universal set U.
> 
> Then !logical(0)  == !{} == U.  any(U) is TRUE, isn't it?  
> 
> I guess the real message is that you need to protect yourself by
> testing for positive length first.

This comes up repeatedly. Probably the most useful way of viewing
these empty sum/prod/any/all issues is that you want

sum(c(x,y)) == sum(x) + sum(y)
prod(c(x,y)) == prod(x) * prod(y)
any(c(x,y)) == any(x) | any(y)
all(c(x,y)) == all(x) & all(y)

even in the cases where x or y is empty, and of course the neutral
operations are 

adding 0
multiplying by 1
or'ing with FALSE
and'ing with TRUE

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Jesse.Whittington at pc.gc.ca  Mon Jan 30 17:22:45 2006
From: Jesse.Whittington at pc.gc.ca (Jesse.Whittington@pc.gc.ca)
Date: Mon, 30 Jan 2006 09:22:45 -0700
Subject: [R] Logistic regression model selection with
	overdispersed/autocorrelated data
Message-ID: <OF6C249F80.0076B321-ON87257106.0059AE18@pc.gc.ca>



I am creating habitat selection models for caribou and other species with
data collected from GPS collars.  In my current situation the radio-collars
recorded the locations of 30 caribou every 6 hours.  I am then comparing
resources used at caribou locations to random locations using logistic
regression (standard habitat analysis).

The data is therefore highly autocorrelated and this causes Type I error
two ways  small standard errors around beta-coefficients and
over-paramaterization during model selection.  Robust standard errors are
easily calculated by block-bootstrapping the data using animal as a
cluster with the Design library, however I havent found a satisfactory
solution for model selection.

A couple options are:
1.  Using QAIC where the deviance is divided by a variance inflation factor
(Burnham & Anderson).  However, this VIF can vary greatly depending on the
data set and the set of covariates used in the global model.
2.  Manual forward stepwise regression using both changes in deviance and
robust p-values for the beta-coefficients.

I have been looking for a solution to this problem for a couple years and
would appreciate any advice.

Jesse



From jholtman at gmail.com  Mon Jan 30 17:33:45 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jan 2006 11:33:45 -0500
Subject: [R] Subsetting a matrix without for-loop
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>
Message-ID: <644e1f320601300833u558a0f85j9440186bc973f0ab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/bf1516f1/attachment.pl

From ifloresc at stevens.edu  Mon Jan 30 17:32:16 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Mon, 30 Jan 2006 11:32:16 -0500
Subject: [R] Integer bit size and the modulus operator
Message-ID: <43DE3F90.5050706@stevens.edu>

I am a statistician and I come up to an interesting problem in 
cryptography. I would like to use R since there are some statistical 
procedures that I need to use.
However, I run into a problem when using the modulus operator %%.

I am using R 2.2.1 and when I calculate modulus for large numbers (that 
I need with my problem) R gives me warnings. For instance if one does:
a=1:40;
8^a %% 41
one obtains zeros which is not possible since 8 to any power is not a 
multiple of 41.
In addition when working with numbers larger that this and with the mod 
operator R crashes randomly.

I believe this is because R stores large integers as real numbers thus 
there may be lack of accuracy when applying the modulus operator and 
converting back to integers.

So my question is this: Is it possible to increase the size of memory 
used for storing integers? Say from 32 bits to 512 bits (Typical size of 
integers in cryptography).

Thank you, any help would be greatly appreciated.
Ionut Florescu



From Scott.Waichler at pnl.gov  Mon Jan 30 17:38:37 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Mon, 30 Jan 2006 08:38:37 -0800
Subject: [R] Timeliness of precompiled binaries--R-2.2.1 still not available
	as RPM
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>


R-2.2.1 is still not available for Redhat Linux as an RPM on CRAN.  It
is available as an SRPM.  Can someone fill me in on why it takes so long
to make RPMs available?  I would be happy to help make the RPMs for el4
and el3 if such help is needed.

Regards,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov



From itsme_410 at yahoo.com  Mon Jan 30 16:17:03 2006
From: itsme_410 at yahoo.com (Globe Trotter)
Date: Mon, 30 Jan 2006 07:17:03 -0800 (PST)
Subject: [R] OT: code for non-central t-density/cdf
Message-ID: <20060130151703.43870.qmail@web54506.mail.yahoo.com>

Hi,

This is not an R question, but can anyone please point me to C/Fortran (C
preferred) code which calculates the non-central t-density or the cdf?

Many thanks and best wishes!
GT



From pburns at pburns.seanet.com  Mon Jan 30 17:46:29 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 30 Jan 2006 16:46:29 +0000
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <f8e6ff050601300709p74895c07p929b6b0fbc2cc326@mail.gmail.com>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>	<f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>	<Pine.LNX.4.61.0601300723140.8419@gannet.stats>
	<f8e6ff050601300709p74895c07p929b6b0fbc2cc326@mail.gmail.com>
Message-ID: <43DE42E5.3000103@pburns.seanet.com>

hadley wickham wrote:

> I read this to mean that setting a value in list will be O(n) (where n
>
>is the length of the new list) - If you blindly expect a list to act
>like a hash table you will be badly surprised.  If you are copying an
>algorithm from another language, you will probably need to rethink the
>way that updates work.  If however, you just want an object that can
>be indexed by a string, then a list will be fine.
>

I would think that when translating from another language,
it is best to write it in R in the simplest way, which probably
means using a list.  Then if it turns out to be too slow, try doing
something fancy.  I suspect that speed improvements are
seldom necessary -- I can't believe how fast most computations
are these days.

I don't see that worrying about O(1) versus O(n) is likely to
get you very far.  Just try it out on a normal size problem,
and try it out on a very large problem and see if it is good
enough.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>
>
>
>  
>



From julie.bernauer at ibbmc.u-psud.fr  Mon Jan 30 17:50:14 2006
From: julie.bernauer at ibbmc.u-psud.fr (Julie Bernauer)
Date: Mon, 30 Jan 2006 17:50:14 +0100
Subject: [R] handling NA by mean replacement
Message-ID: <20060130175014.4iei8plb4g8sk8ck@webmail.crans.org>

Hello

I am sorry fuch such a stupid question. Suppose I have a table of data having a
lot of NAs and I want to replace those NAs by the mean of the column before NA
replacement. How is it possible to do that efficiently ?

Thanks in advance,

Julie

-- 
Julie Bernauer
Yeast Structural Genomics
http://www.genomics.eu.org



From h.wickham at gmail.com  Mon Jan 30 18:00:30 2006
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 30 Jan 2006 11:00:30 -0600
Subject: [R] beginner Q: hashtable or dictionary?
In-Reply-To: <43DE42E5.3000103@pburns.seanet.com>
References: <20060130013440.2713.qmail@web51404.mail.yahoo.com>
	<644e1f320601291738h3a3c7805x442d5fa06e7b4c93@mail.gmail.com>
	<f8e6ff050601291942l6338d482t1a320e1ebd37f54f@mail.gmail.com>
	<Pine.LNX.4.61.0601300723140.8419@gannet.stats>
	<f8e6ff050601300709p74895c07p929b6b0fbc2cc326@mail.gmail.com>
	<43DE42E5.3000103@pburns.seanet.com>
Message-ID: <f8e6ff050601300900k422870a8p16efa3781a4ea71e@mail.gmail.com>

> I would think that when translating from another language,
> it is best to write it in R in the simplest way, which probably
> means using a list.  Then if it turns out to be too slow, try doing
> something fancy.  I suspect that speed improvements are
> seldom necessary -- I can't believe how fast most computations
> are these days.

I totally agree.  However, if you are implementing an algorithm that
is O(n^2) when using a hash table that has O(1) updates, it is likely
to be at least O(n^3) when using a list with O(n) updates.  And
sometimes speed does matter!  (Although not as often as most people
think, and, of course, beware premature optimisation.)

Hadley



From gunter.berton at gene.com  Mon Jan 30 18:20:15 2006
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 30 Jan 2006 09:20:15 -0800
Subject: [R] handling NA by mean replacement
In-Reply-To: <20060130175014.4iei8plb4g8sk8ck@webmail.crans.org>
Message-ID: <200601301720.k0UHKFhN029749@volta.gene.com>

Lots of other folks will give you the simple answer (hint: ?'['  ?is.na)

Yours is one of those "iceberg" questions  -- 2/3 hidden underwater.

Two points:

Point 1: Generally you **don't have to do such replacement** as most of R's
functions have a na.rm or na.action argument (unfortunately, for historical
reasons, the argument names and meanings aren't consistent) that does
basically what you want anyway.

Point 2: Doing what you ask is probably a bad idea, as it creates mythical
degrees of freedom and biases results --> gives wrong statistical answers.

As a general matter, handling missing values "correctly" is a difficult
statistical issue that you may want to avoid if you can (R has plenty of
packages that can deal with it, but it requires background expertise).
Honestly, I'm not sure "if you can" makes any sense here (how do you know?),
but let's just say that I think your potential for mischief is reduced if
you use R's inbuilt arguments for ignoring missings rather than imputing
them naively.

Having said that, I believe that clustering procedures, for example, may not
permit this (but they have builtin missing imputation capabilities of their
own, do they not?), so you may have to impute. In this case, try to do so
wisely (e.g. via multiple imputation?). 

Perhaps this will stimulate real experts to offer you some advice. Good
luck.

Cheers,
Bert
 
Bert Gunter
Genentech

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julie Bernauer
> Sent: Monday, January 30, 2006 8:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] handling NA by mean replacement
> 
> Hello
> 
> I am sorry fuch such a stupid question. Suppose I have a 
> table of data having a
> lot of NAs and I want to replace those NAs by the mean of the 
> column before NA
> replacement. How is it possible to do that efficiently ?
> 
> Thanks in advance,
> 
> Julie
> 
> -- 
> Julie Bernauer
> Yeast Structural Genomics
> http://www.genomics.eu.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Mon Jan 30 18:26:10 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 12:26:10 -0500
Subject: [R] handling NA by mean replacement
In-Reply-To: <20060130175014.4iei8plb4g8sk8ck@webmail.crans.org>
References: <20060130175014.4iei8plb4g8sk8ck@webmail.crans.org>
Message-ID: <971536df0601300926u39858d40h2f5dfcfd1c50eca7@mail.gmail.com>

Don't know about efficiency but here is one way:

# test data
A  <- matrix(1:54, ncol=6)
A[3,3] <- A[6,6] <- A[5,6] <- NA

f <- function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)
apply(A, 2, f)

On 1/30/06, Julie Bernauer <julie.bernauer at ibbmc.u-psud.fr> wrote:
> Hello
>
> I am sorry fuch such a stupid question. Suppose I have a table of data having a
> lot of NAs and I want to replace those NAs by the mean of the column before NA
> replacement. How is it possible to do that efficiently ?
>
> Thanks in advance,
>
> Julie
>
> --
> Julie Bernauer
> Yeast Structural Genomics
> http://www.genomics.eu.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gavin.simpson at ucl.ac.uk  Mon Jan 30 18:37:03 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 30 Jan 2006 17:37:03 +0000
Subject: [R] Timeliness of precompiled binaries--R-2.2.1 still
	not	available as RPM
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>
Message-ID: <1138642623.4592.43.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-01-30 at 08:38 -0800, Waichler, Scott R wrote:
> R-2.2.1 is still not available for Redhat Linux as an RPM on CRAN.  It
> is available as an SRPM.  Can someone fill me in on why it takes so long
> to make RPMs available?  I would be happy to help make the RPMs for el4
> and el3 if such help is needed.

Because someone has to volunteer their time to create such things -
these are not automatically built by R-Core members. Martyn Plummer has
done this for as long as I've been using R...

...Mores the point, though, which CRAN mirror were you looking at? R
2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I could
be bothered to look at on the UK Bristol mirror for example.

G

> Regards,
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at_ pnl.gov

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From costas.magnuse at gmail.com  Mon Jan 30 18:41:11 2006
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Mon, 30 Jan 2006 19:41:11 +0200
Subject: [R] Loops that last for ever...
Message-ID: <30ddfdae0601300941r1639435bn@mail.gmail.com>

Hello, good morning or evening!...

After studying some of the examples at S-poetry Document, I tried to
implement some of the concepts in my R script, that intensively uses
looping constructs. However I did not manage any improvement.
My main problem is that I have a list of a lot of data e.g.:
> xs
[[1]]
[1]........................[1000]
[[2]]
[1]........................[840]
...
[[50]]
[1]........................[945]


Having a script with loops inside loops (for example in a Monte-Carlo
simulation) takes a lot of minutes before it is completed. Is there
another easier way to perform functions for each of the [[i]]  ? Using
probably apply? or constructing a specific function? or using the
so-called "vectorising" tricks?

One example could be the following,  that calculates the sums 1:5, 
2:6, 3:7,...,  for each of xs[[i]] :

xs <- lapply(1:500,  function(x) rnorm(1000))
totalsum <- list()
sums <- list()
first <- list()

for(i in 1:length(xs)) {
totalsum[i] <- sum(xs[[i]])
	for(j in 1:length(xs[[i]])) {
		if(j == 1) {
			sums[[i]] <- list()
			}
		if(j >= 5) {
			sums[[i]][j] <- sum(xs[[i]][(j-4):j])
			}
		}
}

Of course the functions I actually call are more complicated,
increasing the total time of calculations to a lot of minutes,...

<< 1 >>. How could I optimize (or better eliminate?...) the above
loop? Any other suggestions for my scripting habits?

Another problem that I am facing is that calculating a lot of lists
(>50), that contain results of various econometric tests of all the
variables, in the form of

   example.list[[i]] <- expression

demands more than 50 lines at the beginning of the script that
"initiate" the lists (e.g.
example.list.1 <- list()
example.list.2 <- list()
...
example.list.50 <- list()

<< 2 >>.    Is there a way to avoid that?


Thank you very very much in advance,

Constantine  Tsardounis



From p.dalgaard at biostat.ku.dk  Mon Jan 30 18:53:19 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 18:53:19 +0100
Subject: [R] Timeliness of precompiled binaries--R-2.2.1 still
	not	available as RPM
In-Reply-To: <1138642623.4592.43.camel@gsimpson.geog.ucl.ac.uk>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>
	<1138642623.4592.43.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <x2fyn5r4a8.fsf@turmalin.kubism.ku.dk>

Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:

> On Mon, 2006-01-30 at 08:38 -0800, Waichler, Scott R wrote:
> > R-2.2.1 is still not available for Redhat Linux as an RPM on CRAN.  It
> > is available as an SRPM.  Can someone fill me in on why it takes so long
> > to make RPMs available?  I would be happy to help make the RPMs for el4
> > and el3 if such help is needed.
> 
> Because someone has to volunteer their time to create such things -
> these are not automatically built by R-Core members. Martyn Plummer has
> done this for as long as I've been using R...
> 
> ...Mores the point, though, which CRAN mirror were you looking at? R
> 2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I could
> be bothered to look at on the UK Bristol mirror for example.

Not in EL4 on CRAN master, as far as I can see...

It's not only volunteer time that matters. They also need access to a
machine that runs the OS version in question, which I suspect could be
the issue here.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jholtman at gmail.com  Mon Jan 30 18:54:25 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jan 2006 12:54:25 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE3F90.5050706@stevens.edu>
References: <43DE3F90.5050706@stevens.edu>
Message-ID: <644e1f320601300954y59cb0ebbs2590bd6c65c6ab1e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/591e23e7/attachment.pl

From sdavis2 at mail.nih.gov  Mon Jan 30 18:56:44 2006
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 30 Jan 2006 12:56:44 -0500
Subject: [R] handling NA by mean replacement
In-Reply-To: <20060130175014.4iei8plb4g8sk8ck@webmail.crans.org>
Message-ID: <C003BD8C.4BE5%sdavis2@mail.nih.gov>

You might also want to look at the "impute" package on CRAN.

Sean


On 1/30/06 11:50 AM, "Julie Bernauer" <julie.bernauer at ibbmc.u-psud.fr>
wrote:

> Hello
> 
> I am sorry fuch such a stupid question. Suppose I have a table of data having
> a
> lot of NAs and I want to replace those NAs by the mean of the column before NA
> replacement. How is it possible to do that efficiently ?
> 
> Thanks in advance,
> 
> Julie



From gavin.simpson at ucl.ac.uk  Mon Jan 30 18:57:30 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 30 Jan 2006 17:57:30 +0000
Subject: [R] Timeliness of precompiled binaries--R-2.2.1
	still	not	available as RPM
In-Reply-To: <x2fyn5r4a8.fsf@turmalin.kubism.ku.dk>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>
	<1138642623.4592.43.camel@gsimpson.geog.ucl.ac.uk>
	<x2fyn5r4a8.fsf@turmalin.kubism.ku.dk>
Message-ID: <1138643850.4592.46.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-01-30 at 18:53 +0100, Peter Dalgaard wrote:
> Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:
<snip>
> > ...Mores the point, though, which CRAN mirror were you looking at? R
> > 2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I could
> > be bothered to look at on the UK Bristol mirror for example.
> 
> Not in EL4 on CRAN master, as far as I can see...

Time to visit the opticians me thinks...

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From adi at roda.ro  Mon Jan 30 19:09:55 2006
From: adi at roda.ro (Adrian Dusa)
Date: Mon, 30 Jan 2006 20:09:55 +0200
Subject: [R] yet another vectorization question
In-Reply-To: <43DE0942.6060303@sciviews.org>
References: <200601301334.40440.adi@roda.ro> <43DDFF1C.1080504@cirad.fr>
	<43DE0942.6060303@sciviews.org>
Message-ID: <200601302009.55504.adi@roda.ro>

On Monday 30 January 2006 14:40, Philippe Grosjean wrote:
> Hello,
> Not exactly the same. By the way, why do you use do.call()? Couldn't you
> do simply:
> expand.grid(split(t(replicate(3, c(0, 1, NA))), 1:3))

Just for the sake of it, the above can be even more simple with:

expand.grid(lapply(1:3, function(x) c(0, 1, NA)))

Best,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From Scott.Waichler at pnl.gov  Mon Jan 30 19:18:11 2006
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Mon, 30 Jan 2006 10:18:11 -0800
Subject: [R] Timeliness of precompiled binaries--R-2.2.1
 still	notavailable as RPM
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0378646B@pnlmse35.pnl.gov>

>...More the point, though, which CRAN mirror were you looking at? R
>2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I
could be 
>bothered to look at on the UK Bristol mirror for example.

I use this mirror:

http://cran.fhcrc.org/ 	Fred Hutchinson Cancer Research Center, Seattle,
WA

Is there something about the timeliness/completeness of mirrors that I
need to understand?

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov



From murdoch at stats.uwo.ca  Mon Jan 30 19:28:37 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 Jan 2006 13:28:37 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE3F90.5050706@stevens.edu>
References: <43DE3F90.5050706@stevens.edu>
Message-ID: <43DE5AD5.8070107@stats.uwo.ca>

On 1/30/2006 11:32 AM, Ionut Florescu wrote:
> I am a statistician and I come up to an interesting problem in 
> cryptography. I would like to use R since there are some statistical 
> procedures that I need to use.
> However, I run into a problem when using the modulus operator %%.
> 
> I am using R 2.2.1 and when I calculate modulus for large numbers (that 
> I need with my problem) R gives me warnings. For instance if one does:
> a=1:40;
> 8^a %% 41
> one obtains zeros which is not possible since 8 to any power is not a 
> multiple of 41.
> In addition when working with numbers larger that this and with the mod 
> operator R crashes randomly.

Could you keep a record of the random crashes, and see if you can make 
any of them repeatable?  R shouldn't crash.  If you can find a 
repeatable way to make it crash, then that's a bug that needs to be 
fixed.  (If it crashes at random it should still be fixed, but it's so 
much harder to fix that it's unlikely to happen unless the cases are 
ones that look likely to come up in normal situations.)


> 
> I believe this is because R stores large integers as real numbers thus 
> there may be lack of accuracy when applying the modulus operator and 
> converting back to integers.
> 
> So my question is this: Is it possible to increase the size of memory 
> used for storing integers? Say from 32 bits to 512 bits (Typical size of 
> integers in cryptography).

No, but there is at least one contributed package that does multiple 
precision integer arithmetic.  I can't remember the name of it right 
now, but Google should be able to find it for you...

Duncan Murdoch
> 
> Thank you, any help would be greatly appreciated.
> Ionut Florescu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rkrishnan8216 at yahoo.com  Mon Jan 30 19:35:47 2006
From: rkrishnan8216 at yahoo.com (Krish Krishnan)
Date: Mon, 30 Jan 2006 10:35:47 -0800 (PST)
Subject: [R] Warning message when returning multiple items
Message-ID: <20060130183547.87121.qmail@web60924.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/72b10c5d/attachment.pl

From andy_liaw at merck.com  Mon Jan 30 19:39:07 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 13:39:07 -0500
Subject: [R] Integer bit size and the modulus operator
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED788@usctmx1106.merck.com>

R is probably not the best tool for handling large integers...

AFAIK Python has the interesting feature of not only having a `long' integer
type (that can store arbitrarily large integers), but can convert a regular
4-byte int to the `long' type when necessary.  Perhaps that would be a
better tool for the purpose.

Andy

From: jim holtman
> 
> You have reached the maximum value that can be stored accurately in a
> floating point number.  That is what the error message is 
> telling you.  I
> get 21 warnings and this says that at 8^20 I am now 
> truncating digits in the
> variable.  You only have about 54 bits in the floating point 
> number and you
> exceed this about 8^19.
> 
> > a=1:40;
> > 8^a %% 41
>  [1]  8 23 20 37  9 31  2 16  5 40 33 18 21  4 32 10 39 25 36 
>  1  8 23 20
> 37  9 31  2 16  5 40 33
> [32] 18 21  4 32 10  0  0  0  0
> There were 21 warnings (use warnings() to see them)
> > warnings()
> Warning messages:
> 1: probable complete loss of accuracy in modulus
> 2: probable complete loss of accuracy in modulus
> 3: probable complete loss of accuracy in modulus
> 4: probable complete loss of accuracy in modulus
> 5: probable complete loss of accuracy in modulus
> 6: probable complete loss of accuracy in modulus
> 7: probable complete loss of accuracy in modulus
> 8: probable complete loss of accuracy in modulus
> 9: probable complete loss of accuracy in modulus
> 10: probable complete loss of accuracy in modulus
> 11: probable complete loss of accuracy in modulus
> 12: probable complete loss of accuracy in modulus
> 13: probable complete loss of accuracy in modulus
> 14: probable complete loss of accuracy in modulus
> 15: probable complete loss of accuracy in modulus
> 16: probable complete loss of accuracy in modulus
> 17: probable complete loss of accuracy in modulus
> 18: probable complete loss of accuracy in modulus
> 19: probable complete loss of accuracy in modulus
> 20: probable complete loss of accuracy in modulus
> 21: probable complete loss of accuracy in modulus
> >
> > 8^35
> [1] 4.056482e+31
> > 8^36
> [1] 3.245186e+32
> > 8^19
> [1] 1.441152e+17
> > 8^19%%41
> [1] 36
> > 8^20
> [1] 1.152922e+18
> > 8^20%%41
> [1] 1
> Warning message:
> probable complete loss of accuracy in modulus
> >
> 
> 
> 
> On 1/30/06, Ionut Florescu <ifloresc at stevens.edu> wrote:
> >
> > I am a statistician and I come up to an interesting problem in
> > cryptography. I would like to use R since there are some statistical
> > procedures that I need to use.
> > However, I run into a problem when using the modulus operator %%.
> >
> > I am using R 2.2.1 and when I calculate modulus for large 
> numbers (that
> > I need with my problem) R gives me warnings. For instance 
> if one does:
> > a=1:40;
> > 8^a %% 41
> > one obtains zeros which is not possible since 8 to any 
> power is not a
> > multiple of 41.
> > In addition when working with numbers larger that this and 
> with the mod
> > operator R crashes randomly.
> >
> > I believe this is because R stores large integers as real 
> numbers thus
> > there may be lack of accuracy when applying the modulus operator and
> > converting back to integers.
> >
> > So my question is this: Is it possible to increase the size 
> of memory
> > used for storing integers? Say from 32 bits to 512 bits 
> (Typical size of
> > integers in cryptography).
> >
> > Thank you, any help would be greatly appreciated.
> > Ionut Florescu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 
> 
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
> 
> What the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gavin.simpson at ucl.ac.uk  Mon Jan 30 19:40:42 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 30 Jan 2006 18:40:42 +0000
Subject: [R] Timeliness of precompiled
	binaries--R-2.2.1	still	notavailable as RPM
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0378646B@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A0378646B@pnlmse35.pnl.gov>
Message-ID: <1138646442.4592.53.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-01-30 at 10:18 -0800, Waichler, Scott R wrote:
> >...More the point, though, which CRAN mirror were you looking at? R
> >2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I
> could be 
> >bothered to look at on the UK Bristol mirror for example.
> 
> I use this mirror:
> 
> http://cran.fhcrc.org/ 	Fred Hutchinson Cancer Research Center, Seattle,
> WA
> 
> Is there something about the timeliness/completeness of mirrors that I
> need to understand?

Scott,

There can be problems with mirrors from time to time - indeed there have
been discussions on this list recently of one such case.

However, in this instance, my comments were based on me not noticing
that the rpm in the Redhat EL4 directory was still at 2.1.1 (where as
Martyn has updated all the other rpms to 2.2.1) and you not specifying
which flavour of Redhat you were enquiring about.

All the best,

G

> 
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler _at_ pnl.gov
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ifloresc at stevens.edu  Mon Jan 30 19:39:56 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Mon, 30 Jan 2006 13:39:56 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE5AD5.8070107@stats.uwo.ca>
References: <43DE3F90.5050706@stevens.edu> <43DE5AD5.8070107@stats.uwo.ca>
Message-ID: <43DE5D7C.30406@stevens.edu>

Thank you for the quick reply, I will look into the R packages.
For crashing R try this:

generator.zp=function(x,p)
{a=1:(p-1); b=x^a%%p;
if(all(b[1:(p-2)]!=1)&&(b[p-1]==1)){return(x, " Good ")}
else{return(x, " No Good, try another integer ")}
}

This checks if element x is a generator of the group Z_p. If you try 
this function for p = 41 and x various increasing values eventually it 
will crash R. That is what I meant by random, at first I started x=2,3 
so on, when I got to 8, R crashed. Now apparently I can get to 15. When 
I tried again I got to 20.

Ionut Florescu


Duncan Murdoch wrote:
> On 1/30/2006 11:32 AM, Ionut Florescu wrote:
>> I am a statistician and I come up to an interesting problem in 
>> cryptography. I would like to use R since there are some statistical 
>> procedures that I need to use.
>> However, I run into a problem when using the modulus operator %%.
>>
>> I am using R 2.2.1 and when I calculate modulus for large numbers 
>> (that I need with my problem) R gives me warnings. For instance if 
>> one does:
>> a=1:40;
>> 8^a %% 41
>> one obtains zeros which is not possible since 8 to any power is not a 
>> multiple of 41.
>> In addition when working with numbers larger that this and with the 
>> mod operator R crashes randomly.
>
> Could you keep a record of the random crashes, and see if you can make 
> any of them repeatable?  R shouldn't crash.  If you can find a 
> repeatable way to make it crash, then that's a bug that needs to be 
> fixed.  (If it crashes at random it should still be fixed, but it's so 
> much harder to fix that it's unlikely to happen unless the cases are 
> ones that look likely to come up in normal situations.)
>
>
>>
>> I believe this is because R stores large integers as real numbers 
>> thus there may be lack of accuracy when applying the modulus operator 
>> and converting back to integers.
>>
>> So my question is this: Is it possible to increase the size of memory 
>> used for storing integers? Say from 32 bits to 512 bits (Typical size 
>> of integers in cryptography).
>
> No, but there is at least one contributed package that does multiple 
> precision integer arithmetic.  I can't remember the name of it right 
> now, but Google should be able to find it for you...
>
> Duncan Murdoch
>>
>> Thank you, any help would be greatly appreciated.
>> Ionut Florescu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From andy_liaw at merck.com  Mon Jan 30 19:44:29 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 13:44:29 -0500
Subject: [R] OT: code for non-central t-density/cdf
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED789@usctmx1106.merck.com>

See:

http://www.biostat.wustl.edu/archives/html/s-news/2002-11/msg00079.html

The original source is from the Applied Statistics section on the StatLib. 

Andy

From: Globe Trotter
> 
> Hi,
> 
> This is not an R question, but can anyone please point me to 
> C/Fortran (C
> preferred) code which calculates the non-central t-density or the cdf?
> 
> Many thanks and best wishes!
> GT
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at gmail.com  Mon Jan 30 19:45:08 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 13:45:08 -0500
Subject: [R] Warning message when returning multiple items
In-Reply-To: <20060130183547.87121.qmail@web60924.mail.yahoo.com>
References: <20060130183547.87121.qmail@web60924.mail.yahoo.com>
Message-ID: <971536df0601301045q1bad0415k628edac3d39dcfc9@mail.gmail.com>

Return a list:

   f <- function(x) list(x = x, x.squared = x*x)


On 1/30/06, Krish Krishnan <rkrishnan8216 at yahoo.com> wrote:
>  In my function I am trying to return multiple computed items (separated by commas).  The function does what I need, but I get a warning message that multi-argument returns are deprecated.  Is this a warning I should heed, or is there a more elegant and warning free way of achieving the same end?  Thanks
>
>
>
> ---------------------------------
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Mon Jan 30 19:50:01 2006
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 Jan 2006 13:50:01 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE5D7C.30406@stevens.edu>
References: <43DE3F90.5050706@stevens.edu> <43DE5AD5.8070107@stats.uwo.ca>
	<43DE5D7C.30406@stevens.edu>
Message-ID: <43DE5FD9.2010702@stats.uwo.ca>

On 1/30/2006 1:39 PM, Ionut Florescu wrote:
> Thank you for the quick reply, I will look into the R packages.
> For crashing R try this:
> 
> generator.zp=function(x,p)
> {a=1:(p-1); b=x^a%%p;
> if(all(b[1:(p-2)]!=1)&&(b[p-1]==1)){return(x, " Good ")}
> else{return(x, " No Good, try another integer ")}
> }

Thanks, I can reproduce the crash using

for (x in 10:100) generator.zp(x, 41)

I'll see if I can track down what's going wrong.  By the way, you're not 
supposed to use two arguments to return():  that's not supposed to be 
allowed any more.  I'm somewhat surprised you don't get an error from 
it.  But that's not the cause of the crash.

Duncan Murdoch


> 
> This checks if element x is a generator of the group Z_p. If you try 
> this function for p = 41 and x various increasing values eventually it 
> will crash R. That is what I meant by random, at first I started x=2,3 
> so on, when I got to 8, R crashed. Now apparently I can get to 15. When 
> I tried again I got to 20.
> 
> Ionut Florescu
> 
> 
> Duncan Murdoch wrote:
>> On 1/30/2006 11:32 AM, Ionut Florescu wrote:
>>> I am a statistician and I come up to an interesting problem in 
>>> cryptography. I would like to use R since there are some statistical 
>>> procedures that I need to use.
>>> However, I run into a problem when using the modulus operator %%.
>>>
>>> I am using R 2.2.1 and when I calculate modulus for large numbers 
>>> (that I need with my problem) R gives me warnings. For instance if 
>>> one does:
>>> a=1:40;
>>> 8^a %% 41
>>> one obtains zeros which is not possible since 8 to any power is not a 
>>> multiple of 41.
>>> In addition when working with numbers larger that this and with the 
>>> mod operator R crashes randomly.
>>
>> Could you keep a record of the random crashes, and see if you can make 
>> any of them repeatable?  R shouldn't crash.  If you can find a 
>> repeatable way to make it crash, then that's a bug that needs to be 
>> fixed.  (If it crashes at random it should still be fixed, but it's so 
>> much harder to fix that it's unlikely to happen unless the cases are 
>> ones that look likely to come up in normal situations.)
>>
>>
>>>
>>> I believe this is because R stores large integers as real numbers 
>>> thus there may be lack of accuracy when applying the modulus operator 
>>> and converting back to integers.
>>>
>>> So my question is this: Is it possible to increase the size of memory 
>>> used for storing integers? Say from 32 bits to 512 bits (Typical size 
>>> of integers in cryptography).
>>
>> No, but there is at least one contributed package that does multiple 
>> precision integer arithmetic.  I can't remember the name of it right 
>> now, but Google should be able to find it for you...
>>
>> Duncan Murdoch
>>>
>>> Thank you, any help would be greatly appreciated.
>>> Ionut Florescu
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>



From gavin.simpson at ucl.ac.uk  Mon Jan 30 19:54:17 2006
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 30 Jan 2006 18:54:17 +0000
Subject: [R] Warning message when returning multiple items
In-Reply-To: <20060130183547.87121.qmail@web60924.mail.yahoo.com>
References: <20060130183547.87121.qmail@web60924.mail.yahoo.com>
Message-ID: <1138647257.4592.59.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2006-01-30 at 10:35 -0800, Krish Krishnan wrote:
>  In my function I am trying to return multiple computed items
> (separated by commas).  The function does what I need, but I get a
> warning message that multi-argument returns are deprecated.  Is this a
> warning I should heed, or is there a more elegant and warning free way
> of achieving the same end?  Thanks

foo <- function(x)
  {
     x5 <- x * 5
     rootx <- sqrt(x)
     squaredx <- x^2
     return(list(x5 = x5, rootx = rootx, squaredx = squaredx))
  }

> foo(9)
$x5
[1] 45

$rootx
[1] 3

$squaredx
[1] 81

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From jholtman at gmail.com  Mon Jan 30 19:59:28 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jan 2006 13:59:28 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE3F90.5050706@stevens.edu>
References: <43DE3F90.5050706@stevens.edu>
Message-ID: <644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/b453137b/attachment.pl

From andy_liaw at merck.com  Mon Jan 30 20:02:59 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 14:02:59 -0500
Subject: [R] Warning message when returning multiple items
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED78A@usctmx1106.merck.com>

For the last few versions of R (don't remember when the change happened
now), you need to explicitly wrap the objects in a list, instead of simply
having them in return().  I.e., instead of return(a=thing1, b=thing2), you
simply use list(a=thing1, b=thing2) as the last line of the function.

Andy

From: Krish Krishnan
> 
>  In my function I am trying to return multiple computed items 
> (separated by commas).  The function does what I need, but I 
> get a warning message that multi-argument returns are 
> deprecated.  Is this a warning I should heed, or is there a 
> more elegant and warning free way of achieving the same end?  Thanks
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gescati at yahoo.com.ar  Mon Jan 30 19:55:15 2006
From: gescati at yahoo.com.ar (=?iso-8859-1?q?gabriela=20escati=20pe=F1aloza?=)
Date: Mon, 30 Jan 2006 18:55:15 +0000 (GMT)
Subject: [R] degrees of freedom
Message-ID: <20060130185515.30748.qmail@web37105.mail.mud.yahoo.com>

Dear Dr. Bates,
Thank you very much for your response. I had consulted
the algorithm described in Pinheiro and Bates.
However, what I don't understand (among other things)
is why my two parameters appear to be estimated at
different grouping levels (based on the DF values).
Affect this different values of DF at the estimates
parameters? The estimates fixed effects were get at
the same level of grouping?
I apreciate any response.


Lic. Gabriela Escati Pe??aloza
Biolog??a y Manejo de Recursos Acu??ticos
Centro Nacional Patag??nico(CENPAT). 
CONICET
Bvd. Brown s/n??.
(U9120ACV)Pto. Madryn 
Chubut
Argentina

Tel: 54-2965/451301/451024/451375/45401 (Int:277)
Fax: 54-29657451543

__________________________________________________
Correo Yahoo!
Espacio para todos tus mensajes, antivirus y antispam ??gratis! 
??Abr?? tu cuenta ya! - http://correo.yahoo.com.ar



From jholtman at gmail.com  Mon Jan 30 20:11:24 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jan 2006 14:11:24 -0500
Subject: [R] Loops that last for ever...
In-Reply-To: <30ddfdae0601300941r1639435bn@mail.gmail.com>
References: <30ddfdae0601300941r1639435bn@mail.gmail.com>
Message-ID: <644e1f320601301111p6a8a49fdnc4e5a9c50a230102@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/43cd75d9/attachment.pl

From rkrishnan8216 at yahoo.com  Mon Jan 30 20:13:47 2006
From: rkrishnan8216 at yahoo.com (Krish Krishnan)
Date: Mon, 30 Jan 2006 11:13:47 -0800 (PST)
Subject: [R] Warning message when returning multiple items
Message-ID: <20060130191347.41361.qmail@web60922.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/6caefdd9/attachment.pl

From jholtman at gmail.com  Mon Jan 30 20:17:54 2006
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jan 2006 14:17:54 -0500
Subject: [R] Loops that last for ever...
In-Reply-To: <30ddfdae0601300941r1639435bn@mail.gmail.com>
References: <30ddfdae0601300941r1639435bn@mail.gmail.com>
Message-ID: <644e1f320601301117o2feed3a1g5fe5ff2aa389cc4c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/347c10df/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jan 30 20:27:32 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jan 2006 19:27:32 +0000 (GMT)
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE5D7C.30406@stevens.edu>
References: <43DE3F90.5050706@stevens.edu> <43DE5AD5.8070107@stats.uwo.ca>
	<43DE5D7C.30406@stevens.edu>
Message-ID: <Pine.LNX.4.61.0601301908020.21600@gannet.stats>

This is a double protection error in real_binary.  See the R-devel list 
for the details.  Now fixed.

On Mon, 30 Jan 2006, Ionut Florescu wrote:

> Thank you for the quick reply, I will look into the R packages.
> For crashing R try this:
>
> generator.zp=function(x,p)
> {a=1:(p-1); b=x^a%%p;
> if(all(b[1:(p-2)]!=1)&&(b[p-1]==1)){return(x, " Good ")}
> else{return(x, " No Good, try another integer ")}
> }
>
> This checks if element x is a generator of the group Z_p. If you try
> this function for p = 41 and x various increasing values eventually it
> will crash R. That is what I meant by random, at first I started x=2,3
> so on, when I got to 8, R crashed. Now apparently I can get to 15. When
> I tried again I got to 20.
>
> Ionut Florescu
>
>
> Duncan Murdoch wrote:
>> On 1/30/2006 11:32 AM, Ionut Florescu wrote:
>>> I am a statistician and I come up to an interesting problem in
>>> cryptography. I would like to use R since there are some statistical
>>> procedures that I need to use.
>>> However, I run into a problem when using the modulus operator %%.
>>>
>>> I am using R 2.2.1 and when I calculate modulus for large numbers
>>> (that I need with my problem) R gives me warnings. For instance if
>>> one does:
>>> a=1:40;
>>> 8^a %% 41
>>> one obtains zeros which is not possible since 8 to any power is not a
>>> multiple of 41.
>>> In addition when working with numbers larger that this and with the
>>> mod operator R crashes randomly.
>>
>> Could you keep a record of the random crashes, and see if you can make
>> any of them repeatable?  R shouldn't crash.  If you can find a
>> repeatable way to make it crash, then that's a bug that needs to be
>> fixed.  (If it crashes at random it should still be fixed, but it's so
>> much harder to fix that it's unlikely to happen unless the cases are
>> ones that look likely to come up in normal situations.)
>>
>>
>>>
>>> I believe this is because R stores large integers as real numbers
>>> thus there may be lack of accuracy when applying the modulus operator
>>> and converting back to integers.
>>>
>>> So my question is this: Is it possible to increase the size of memory
>>> used for storing integers? Say from 32 bits to 512 bits (Typical size
>>> of integers in cryptography).
>>
>> No, but there is at least one contributed package that does multiple
>> precision integer arithmetic.  I can't remember the name of it right
>> now, but Google should be able to find it for you...
>>
>> Duncan Murdoch
>>>
>>> Thank you, any help would be greatly appreciated.
>>> Ionut Florescu
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vasu.akkineni at gmail.com  Mon Jan 30 20:27:35 2006
From: vasu.akkineni at gmail.com (Vasundhara Akkineni)
Date: Mon, 30 Jan 2006 14:27:35 -0500
Subject: [R] Anova help
Message-ID: <3b67376c0601301127i67181325pd08c5fee1f46a1a1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/cc34e9d2/attachment.pl

From andy_liaw at merck.com  Mon Jan 30 20:40:30 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 14:40:30 -0500
Subject: [R] Loops that last for ever...
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED78B@usctmx1106.merck.com>

From: Constantine Tsardounis
> 
> Hello, good morning or evening!...
> 
> After studying some of the examples at S-poetry Document, I tried to
> implement some of the concepts in my R script, that intensively uses
> looping constructs. However I did not manage any improvement.
> My main problem is that I have a list of a lot of data e.g.:
> > xs
> [[1]]
> [1]........................[1000]
> [[2]]
> [1]........................[840]
> ...
> [[50]]
> [1]........................[945]
> 
> 
> Having a script with loops inside loops (for example in a Monte-Carlo
> simulation) takes a lot of minutes before it is completed. Is there
> another easier way to perform functions for each of the [[i]]  ? Using
> probably apply? or constructing a specific function? or using the
> so-called "vectorising" tricks?
> 
> One example could be the following,  that calculates the sums 1:5, 
> 2:6, 3:7,...,  for each of xs[[i]] :
> 
> xs <- lapply(1:500,  function(x) rnorm(1000))
> totalsum <- list()
> sums <- list()
> first <- list()
> 
> for(i in 1:length(xs)) {
> totalsum[i] <- sum(xs[[i]])
> 	for(j in 1:length(xs[[i]])) {
> 		if(j == 1) {
> 			sums[[i]] <- list()
> 			}
> 		if(j >= 5) {
> 			sums[[i]][j] <- sum(xs[[i]][(j-4):j])
> 			}
> 		}
> }

For this you want to vectorize the computation inside, eliminating the j
loop, then use lapply() if you like for the outer loop.  That saves you the
line to initialize the list.
 
> Of course the functions I actually call are more complicated,
> increasing the total time of calculations to a lot of minutes,...
> 
> << 1 >>. How could I optimize (or better eliminate?...) the above
> loop? Any other suggestions for my scripting habits?
> 
> Another problem that I am facing is that calculating a lot of lists
> (>50), that contain results of various econometric tests of all the
> variables, in the form of
> 
>    example.list[[i]] <- expression
> 
> demands more than 50 lines at the beginning of the script that
> "initiate" the lists (e.g.
> example.list.1 <- list()
> example.list.2 <- list()
> ...
> example.list.50 <- list()
> 
> << 2 >>.    Is there a way to avoid that?

Yes, by putting them all in one list.

Andy
 
> 
> Thank you very very much in advance,
> 
> Constantine  Tsardounis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pburns at pburns.seanet.com  Mon Jan 30 20:44:35 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 30 Jan 2006 19:44:35 +0000
Subject: [R] yet another vectorization question
In-Reply-To: <200601302009.55504.adi@roda.ro>
References: <200601301334.40440.adi@roda.ro>
	<43DDFF1C.1080504@cirad.fr>	<43DE0942.6060303@sciviews.org>
	<200601302009.55504.adi@roda.ro>
Message-ID: <43DE6CA3.4070208@pburns.seanet.com>

I tried to let this pass, but failed:

lapply(1:3, function(x) c(0, 1, NA))

might more clearly be written as

rep(list(c(0, 1, NA)), 3)



Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Adrian Dusa wrote:

>On Monday 30 January 2006 14:40, Philippe Grosjean wrote:
>  
>
>>Hello,
>>Not exactly the same. By the way, why do you use do.call()? Couldn't you
>>do simply:
>>expand.grid(split(t(replicate(3, c(0, 1, NA))), 1:3))
>>    
>>
>
>Just for the sake of it, the above can be even more simple with:
>
>expand.grid(lapply(1:3, function(x) c(0, 1, NA)))
>
>Best,
>Adrian
>
>  
>



From adi at roda.ro  Mon Jan 30 21:09:18 2006
From: adi at roda.ro (Adrian Dusa)
Date: Mon, 30 Jan 2006 22:09:18 +0200
Subject: [R] yet another vectorization question
In-Reply-To: <43DE6CA3.4070208@pburns.seanet.com>
References: <200601301334.40440.adi@roda.ro> <200601302009.55504.adi@roda.ro>
	<43DE6CA3.4070208@pburns.seanet.com>
Message-ID: <200601302209.18419.adi@roda.ro>

On Monday 30 January 2006 21:44, Patrick Burns wrote:
> I tried to let this pass, but failed:
>
> lapply(1:3, function(x) c(0, 1, NA))
>
> might more clearly be written as
>
> rep(list(c(0, 1, NA)), 3)

Indeed! Excellent, thanks :)

Hmm, I was just thinking perhaps my first example was too cluttered to spot an 
immediate solution.
With your permission, I came up with a simpler example (I hope I don't upset 
anybody being too persistent):

set.seed(5)
aa <- matrix(sample(10, 15, replace=T), ncol=5)
bb <- matrix(NA, ncol=10, nrow=5)
for (i in 1:ncol(aa)) bb[i, aa[, i]] <- c(0, 1, 0)

Is there any possibility to vectorize this "for" loop?
(sometimes I have hundreds of columns in the "aa" matrix)

Many big thanks in advance,
Adrian

-- 
Adrian DUSA
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From edsberg at stud.ntnu.no  Mon Jan 30 21:19:09 2006
From: edsberg at stud.ntnu.no (Ole Edsberg)
Date: Mon, 30 Jan 2006 21:19:09 +0100
Subject: [R] Varying results of sammon(), for the same data set
In-Reply-To: <Pine.LNX.4.61.0601300903270.9795@gannet.stats>
References: <20060130083915.GA1332@stud.ntnu.no>
	<Pine.LNX.4.61.0601300903270.9795@gannet.stats>
Message-ID: <20060130201909.GA6090@stud.ntnu.no>

Thanks a lot for the explanation and the advice! It looks like I
should find something else to do with the data. I'm afraid that my
knowledge of numerical analysis is very limited, and I haven't read
Wilkinson's book.

Best Regards,

Ole Edsberg



From leaflovesun at yahoo.ca  Mon Jan 30 21:31:14 2006
From: leaflovesun at yahoo.ca (Leaf Sun)
Date: Mon, 30 Jan 2006 13:31:14 -0700
Subject: [R] how to get the row name?
Message-ID: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>

Hi R-listers,

I have a simple question about a data frame.  

I sorted a data set by one of the variable in some condition (eg. X>=0),  the followed is part of the achieved. I was wondering how can I get the row name, i. e.   (1202,  2077 , 2328,  3341,...  ) and save them as a vector.     Thanks!

			  Tag  Species     X     Y Dbh3 Recr4 mort slope elevation aspect    SA   SR     dist1     dist2     dist3
1202    19103     316 856.0 430.3   21     4    1  9.87    151.42  60.08 25.38 1.02 0.2236068 0.7211103 1.3601471
2077    29893     316 935.4 482.7   28     4    1  5.66    137.28  13.86 25.14 1.01 0.6403124 0.8944272 1.0630146
2328    32989     316 910.7 301.5   12     4    1  8.07    137.69  86.16 25.26 1.01 0.3000000 1.2806248 1.3038405
3341    45198     316 975.2   2.4  144     4    1  2.95    121.10 173.60  0.00 0.00 0.5656854 1.2727922 1.3416408
...

Regards,

Leaf



From ccleland at optonline.net  Mon Jan 30 21:36:55 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 30 Jan 2006 15:36:55 -0500
Subject: [R] how to get the row name?
In-Reply-To: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
Message-ID: <43DE78E7.5040209@optonline.net>

rownames(subset(mydata, X >=0))

?rownames

Leaf Sun wrote:
> Hi R-listers,
> 
> I have a simple question about a data frame.  
> 
> I sorted a data set by one of the variable in some condition (eg. X>=0),  the followed is part of the achieved. I was wondering how can I get the row name, i. e.   (1202,  2077 , 2328,  3341,...  ) and save them as a vector.     Thanks!
> 
> 			  Tag  Species     X     Y Dbh3 Recr4 mort slope elevation aspect    SA   SR     dist1     dist2     dist3
> 1202    19103     316 856.0 430.3   21     4    1  9.87    151.42  60.08 25.38 1.02 0.2236068 0.7211103 1.3601471
> 2077    29893     316 935.4 482.7   28     4    1  5.66    137.28  13.86 25.14 1.01 0.6403124 0.8944272 1.0630146
> 2328    32989     316 910.7 301.5   12     4    1  8.07    137.69  86.16 25.26 1.01 0.3000000 1.2806248 1.3038405
> 3341    45198     316 975.2   2.4  144     4    1  2.95    121.10 173.60  0.00 0.00 0.5656854 1.2727922 1.3416408
> ...
> 
> Regards,
> 
> Leaf
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From p.connolly at hortresearch.co.nz  Mon Jan 30 21:51:16 2006
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 31 Jan 2006 09:51:16 +1300
Subject: [R] weights argument in the lmer function in lme4
Message-ID: <20060130205116.GV18619@hortresearch.co.nz>

I suspect the weights argument is not having any effect.

Package:              Matrix
Version:              0.995-2
Date:                 2006-01-19


Beginning with this:

Browse[1]>   resp.lmer <- lmer(SensSSC ~ Block + Season + (1 | Plot) + (1 | Ma) + (1 | Pa) + 
+     (1 | MaPa), weights = SensSSC.N, data = xx)

I group the output into a table with my ran.eff function and get this:

Browse[1]> ran.eff(resp.lmer)
          01     02     03     04     05     06     07   GCAf RankF
A     13.714 13.709 13.886 14.124 15.120 13.546 14.586  0.472     1
B     13.452     NA 13.426 13.632 14.439 13.512 13.713  0.069     3
C     13.922 13.770 14.353     NA 14.661 13.529 14.367  0.453     2
D         NA     NA 13.353     NA     NA     NA     NA -0.051     4
E     12.775 12.767 12.823 12.767 14.036 12.631 13.645 -0.495     6
F     13.043 13.338 12.641 12.977 13.848 12.425 13.530 -0.448     5
GCAm  -0.200 -0.169 -0.165 -0.103  0.736 -0.428  0.329     NA    NA
RankM  6.000  5.000  4.000  3.000  1.000  7.000  2.000     NA    NA


Despite any shortcomings in my ran.eff function, those values look
alright, but they're the same (to any number of decimal places) as I'd
get without a weights argument.  Just to check that the weights really
don't effect it, I tried using only the rows with a weight of 5
(almost 90% of the data) but it was substantially different.

Browse[1]>   resp.lmer5 <- lmer(SensSSC ~ Block + Season + (1 | Plot) + (1 | Ma) + (1 | Pa) + 
+     (1 | MaPa), subset = SensSSC.N == 5, data = xx)

Browse[1]> ran.eff(resp.lmer5)
          01     02     03     04     05     06     07   GCAf RankF
A     13.435 13.349 13.595 13.914 14.722 13.161 14.414  0.345     2
B     13.068     NA 13.110 13.447 14.121 13.296 13.637 -0.014     4
C     13.702 13.537 14.256     NA 14.371 13.575 14.247  0.469     1
D         NA     NA 13.276     NA     NA     NA     NA -0.001     3
E     12.717 12.659 12.786 12.719 13.642 12.659 13.556 -0.425     6
F     13.015 13.101 12.549 12.920 13.629 12.438 13.474 -0.374     5
GCAm  -0.210 -0.230 -0.146 -0.049  0.596 -0.353  0.391     NA    NA
RankM  5.000  6.000  4.000  3.000  1.000  7.000  2.000     NA    NA

That seems to indicate that weights cannot be readily ignored.

Has anyone had experience to indicate that the weights argument does
produce a difference, and so I should be looking somewhere else for
the reason why I'm getting such results?


TIA

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From davidr00 at hotmail.com  Mon Jan 30 21:51:54 2006
From: davidr00 at hotmail.com (David Randel)
Date: Mon, 30 Jan 2006 15:51:54 -0500
Subject: [R] Date Not Staying in Date Format
Message-ID: <BAY107-F67D36AB649EEE07832946C3090@phx.gbl>

I have a column in a data frame that has a class of "Date" and a mode of 
"numeric".  When I:

max(df$Date)

My output stays in Date format, i.e. "2006-01-03".

However, when I run the following statment:

tapply(df$Date, df$SomeFactor, max)

my output looks like this:  9129   9493   9861  10226  10591  10956  11320  
11687  12052  12417

The returned object is of mode "numeric" and class "array".  Each array 
element is of mode "numeric" and class "numeric".  I believe that this is 
the integer representation of my date.  I can't seem to convert it back to a 
date.

How do I get these to be intrepreted as a date instead of a number?

Thanks,
~Dave R.



From ifloresc at stevens.edu  Mon Jan 30 21:53:29 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Mon, 30 Jan 2006 15:53:29 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
References: <43DE3F90.5050706@stevens.edu>
	<644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
Message-ID: <43DE7CC9.60208@stevens.edu>

Thank you, I didn't notice that. I may have to come up with my own power 
function as well, slower but precise.


jim holtman wrote:
> The other thing that you have to be aware of is that 8^n is not 8 
> multiplied by itself n times.  You are probably using logs to compute 
> this.  Here is a sample of 8^(1:20). The value of 8^2 
> is 64.000000000000004 (not exactly an integer); roundoff errors are 
> apparent in the other values. 
>  
> > 8^(1:20)
>  [1] 8.0000000000000000e+00 6.4000000000000004e+01 
> 5.1200000000000001e+02 4.0960000000000001e+03
>  [5] 3.2768000000000002e+04 2.6214400000000002e+05 
> 2.0971519999999999e+06 1.6777215999999999e+07
>  [9] 1.3421772800000000e+08 1.0737418240000001e+09 
> 8.5899345920000005e+09 6.8719476736000003e+10
> [13] 5.4975581388799997e+11 4.3980465111039999e+12 
> 3.5184372088832001e+13 2.8147497671065600e+14
> [17] 2.2517998136852482e+15 1.8014398509481984e+16 
> 1.4411518807585588e+17 1.1529215046068471e+18
> >
>  
> On 1/30/06, *Ionut Florescu* <ifloresc at stevens.edu 
> <mailto:ifloresc at stevens.edu>> wrote:
>
>     I am a statistician and I come up to an interesting problem in
>     cryptography. I would like to use R since there are some statistical
>     procedures that I need to use.
>     However, I run into a problem when using the modulus operator %%.
>
>     I am using R 2.2.1 and when I calculate modulus for large numbers
>     (that
>     I need with my problem) R gives me warnings. For instance if one
>     does:
>     a=1:40;
>     8^a %% 41
>     one obtains zeros which is not possible since 8 to any power is not a
>     multiple of 41.
>     In addition when working with numbers larger that this and with
>     the mod
>     operator R crashes randomly.
>
>     I believe this is because R stores large integers as real numbers thus
>     there may be lack of accuracy when applying the modulus operator and
>     converting back to integers.
>
>     So my question is this: Is it possible to increase the size of memory
>     used for storing integers? Say from 32 bits to 512 bits (Typical
>     size of
>     integers in cryptography).
>
>     Thank you, any help would be greatly appreciated.
>     Ionut Florescu
>
>     ______________________________________________
>     R-help at stat.math.ethz.ch <mailto:R-help at stat.math.ethz.ch> mailing
>     list
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide!
>     http://www.R-project.org/posting-guide.html
>
>
>
>
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 247 0281
>
> What the problem you are trying to solve?



From asaguiar at spsconsultoria.com  Mon Jan 30 22:00:48 2006
From: asaguiar at spsconsultoria.com (Alexandre Santos Aguiar)
Date: Mon, 30 Jan 2006 19:00:48 -0200
Subject: [R] Glossay of available R functions
In-Reply-To: <43DE78E7.5040209@optonline.net>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net>
Message-ID: <200601301900.50123.asaguiar@spsconsultoria.com>

Em Seg 30 Jan 2006 18:36, Chuck Cleland escreveu:
> rownames(subset(mydata, X >=0))

Hi,

I am new to R and read this list to learn. It is amazing how frequently new  
functions pop in messages. Useful and timesaving functions like subset 
(above) must be documented somewhere.

Is there a glossary of functions?

-- 

          Alexandre Santos Aguiar, MD
- independent consultant for health research -
       R Botucatu, 591 cj 81 - 04037-005
            S??o Paulo - SP - Brazil
             tel +55-11-9320-2046
             fax +55-11-5549-8760
            www.spsconsultoria.com



From p.dalgaard at biostat.ku.dk  Mon Jan 30 22:02:03 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 22:02:03 +0100
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
References: <43DE3F90.5050706@stevens.edu>
	<644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
Message-ID: <x2bqxtqvjo.fsf@turmalin.kubism.ku.dk>

jim holtman <jholtman at gmail.com> writes:

> The other thing that you have to be aware of is that 8^n is not 8 multiplied
> by itself n times.  You are probably using logs to compute this.  Here is a
> sample of 8^(1:20). The value of 8^2 is 64.000000000000004 (not exactly an
> integer); roundoff errors are apparent in the other values.
> 
> > 8^(1:20)
>  [1] 8.0000000000000000e+00 6.4000000000000004e+01 5.1200000000000001e+02
> 4.0960000000000001e+03
>  [5] 3.2768000000000002e+04 2.6214400000000002e+05 2.0971519999999999e+06
> 1.6777215999999999e+07
>  [9] 1.3421772800000000e+08 1.0737418240000001e+09 8.5899345920000005e+09
> 6.8719476736000003e+10
> [13] 5.4975581388799997e+11 4.3980465111039999e+12 3.5184372088832001e+13
> 2.8147497671065600e+14
> [17] 2.2517998136852482e+15 1.8014398509481984e+16 1.4411518807585588e+17
> 1.1529215046068471e+18

This was resolved a few versions back as I recall it (seems to have
eluded the NEWS file?):

> options(digits=20)
> 8^(1:20)
 [1] 8.0000000000000000000e+00 6.4000000000000000000e+01
 [3] 5.1200000000000000000e+02 4.0960000000000000000e+03
 [5] 3.2768000000000000000e+04 2.6214400000000000000e+05
 [7] 2.0971520000000000000e+06 1.6777216000000000000e+07
 [9] 1.3421772800000000000e+08 1.0737418240000000000e+09
[11] 8.5899345920000000000e+09 6.8719476736000000000e+10
[13] 5.4975581388800000000e+11 4.3980465111040000000e+12
[15] 3.5184372088832000000e+13 2.8147497671065600000e+14
[17] 2.2517998136852480000e+15 1.8014398509481984000e+16
[19] 1.4411518807585587200e+17 1.1529215046068469760e+18

> 8^(1:20) %%1
 [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Warning messages:
1: probable complete loss of accuracy in modulus
2: probable complete loss of accuracy in modulus
3: probable complete loss of accuracy in modulus


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ifloresc at stevens.edu  Mon Jan 30 22:01:16 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Mon, 30 Jan 2006 16:01:16 -0500
Subject: [R] how to get the row name?
In-Reply-To: <43DE78E7.5040209@optonline.net>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net>
Message-ID: <43DE7E9C.6000103@stevens.edu>

I don't know about rownames but
x >= 0 gives you a vector of logical values True and false.
If then you do
c(1:length(x)) [x>=0]
this gives the positions where the true happened, meaning your vector of
values.



Chuck Cleland wrote:
> rownames(subset(mydata, X >=0))
>
> ?rownames
>
> Leaf Sun wrote:
>   
>> Hi R-listers,
>>
>> I have a simple question about a data frame.  
>>
>> I sorted a data set by one of the variable in some condition (eg. X>=0),  the followed is part of the achieved. I was wondering how can I get the row name, i. e.   (1202,  2077 , 2328,  3341,...  ) and save them as a vector.     Thanks!
>>
>> 			  Tag  Species     X     Y Dbh3 Recr4 mort slope elevation aspect    SA   SR     dist1     dist2     dist3
>> 1202    19103     316 856.0 430.3   21     4    1  9.87    151.42  60.08 25.38 1.02 0.2236068 0.7211103 1.3601471
>> 2077    29893     316 935.4 482.7   28     4    1  5.66    137.28  13.86 25.14 1.01 0.6403124 0.8944272 1.0630146
>> 2328    32989     316 910.7 301.5   12     4    1  8.07    137.69  86.16 25.26 1.01 0.3000000 1.2806248 1.3038405
>> 3341    45198     316 975.2   2.4  144     4    1  2.95    121.10 173.60  0.00 0.00 0.5656854 1.2727922 1.3416408
>> ...
>>
>> Regards,
>>
>> Leaf
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>     
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ifloresc at stevens.edu  Mon Jan 30 22:06:50 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Mon, 30 Jan 2006 16:06:50 -0500
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <x2bqxtqvjo.fsf@turmalin.kubism.ku.dk>
References: <43DE3F90.5050706@stevens.edu>
	<644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
	<x2bqxtqvjo.fsf@turmalin.kubism.ku.dk>
Message-ID: <43DE7FEA.8060703@stevens.edu>

Actually it does that in my 2.2.1 version as well:

 > options(digits=20)
 > 8^(1:20)
 [1] 8.0000000000000000e+00 6.4000000000000004e+01 5.1200000000000001e+02
 [4] 4.0960000000000001e+03 3.2768000000000002e+04 2.6214400000000002e+05
 [7] 2.0971519999999999e+06 1.6777215999999999e+07 1.3421772800000000e+08
[10] 1.0737418240000001e+09 8.5899345920000005e+09 6.8719476736000003e+10
[13] 5.4975581388799997e+11 4.3980465111039999e+12 3.5184372088832001e+13
[16] 2.8147497671065600e+14 2.2517998136852482e+15 1.8014398509481984e+16
[19] 1.4411518807585588e+17 1.1529215046068471e+18



Peter Dalgaard wrote:
> jim holtman <jholtman at gmail.com> writes:
>
>   
>> The other thing that you have to be aware of is that 8^n is not 8 multiplied
>> by itself n times.  You are probably using logs to compute this.  Here is a
>> sample of 8^(1:20). The value of 8^2 is 64.000000000000004 (not exactly an
>> integer); roundoff errors are apparent in the other values.
>>
>>     
>>> 8^(1:20)
>>>       
>>  [1] 8.0000000000000000e+00 6.4000000000000004e+01 5.1200000000000001e+02
>> 4.0960000000000001e+03
>>  [5] 3.2768000000000002e+04 2.6214400000000002e+05 2.0971519999999999e+06
>> 1.6777215999999999e+07
>>  [9] 1.3421772800000000e+08 1.0737418240000001e+09 8.5899345920000005e+09
>> 6.8719476736000003e+10
>> [13] 5.4975581388799997e+11 4.3980465111039999e+12 3.5184372088832001e+13
>> 2.8147497671065600e+14
>> [17] 2.2517998136852482e+15 1.8014398509481984e+16 1.4411518807585588e+17
>> 1.1529215046068471e+18
>>     
>
> This was resolved a few versions back as I recall it (seems to have
> eluded the NEWS file?):
>
>   
>> options(digits=20)
>> 8^(1:20)
>>     
>  [1] 8.0000000000000000000e+00 6.4000000000000000000e+01
>  [3] 5.1200000000000000000e+02 4.0960000000000000000e+03
>  [5] 3.2768000000000000000e+04 2.6214400000000000000e+05
>  [7] 2.0971520000000000000e+06 1.6777216000000000000e+07
>  [9] 1.3421772800000000000e+08 1.0737418240000000000e+09
> [11] 8.5899345920000000000e+09 6.8719476736000000000e+10
> [13] 5.4975581388800000000e+11 4.3980465111040000000e+12
> [15] 3.5184372088832000000e+13 2.8147497671065600000e+14
> [17] 2.2517998136852480000e+15 1.8014398509481984000e+16
> [19] 1.4411518807585587200e+17 1.1529215046068469760e+18
>
>   
>> 8^(1:20) %%1
>>     
>  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Warning messages:
> 1: probable complete loss of accuracy in modulus
> 2: probable complete loss of accuracy in modulus
> 3: probable complete loss of accuracy in modulus
>
>
>



From schmatthew at gmail.com  Mon Jan 30 22:09:49 2006
From: schmatthew at gmail.com (Matthew Scholz)
Date: Mon, 30 Jan 2006 14:09:49 -0700
Subject: [R] collating columns
Message-ID: <3035b8df0601301309o428947fdq88b9dcaed5aa182a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/af3cd02a/attachment.pl

From ccleland at optonline.net  Mon Jan 30 22:23:11 2006
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 30 Jan 2006 16:23:11 -0500
Subject: [R] Glossay of available R functions
In-Reply-To: <200601301900.50123.asaguiar@spsconsultoria.com>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net>
	<200601301900.50123.asaguiar@spsconsultoria.com>
Message-ID: <43DE83BF.9040908@optonline.net>

I am not sure what you mean by a glossary, but subset and rownames are 
both in the R Reference Index:

http://cran.r-project.org/doc/manuals/fullrefman.pdf

If you suspect there might be an R function for something but don't know 
what it is, help.search() can be useful.  For example:

help.search("row names")

Alexandre Santos Aguiar wrote:
> Em Seg 30 Jan 2006 18:36, Chuck Cleland escreveu:
>> rownames(subset(mydata, X >=0))
> 
> Hi,
> 
> I am new to R and read this list to learn. It is amazing how frequently new  
> functions pop in messages. Useful and timesaving functions like subset 
> (above) must be documented somewhere.
> 
> Is there a glossary of functions?
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From martin.julien.2 at courrier.uqam.ca  Mon Jan 30 22:22:04 2006
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Mon, 30 Jan 2006 16:22:04 -0500
Subject: [R] Type II SS for fixed-effect in mixed model
In-Reply-To: <mailman.4.1138618802.26022.r-help@stat.math.ethz.ch>
Message-ID: <000001c625e3$3bc97460$5044d084@TAMIAS>

Hi
In mixed-model with lme()
How can I obtain Type II SS or Type III SS for fixed effect?
Thanks 
Julien



From Michaell.Taylor at boxwoodmeans.com  Mon Jan 30 22:32:44 2006
From: Michaell.Taylor at boxwoodmeans.com (Michaell Taylor)
Date: Mon, 30 Jan 2006 15:32:44 -0600
Subject: [R] RMySQL install
Message-ID: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>


I am having trouble installing RMySQL on a clean install of Fedora Core 4 64 
bit on a dual dual core machine (that is, two dual core processors).  Seems 
like the LD_LIBRARY_PATH is incorrect, but I don't seem to have it quite 
right yet. 

There are a few mentions of this problem in google, but thus far none of the 
"fixes" and fixed my problem.  I've tried defining the LD_LIBRARY_PATH 
environment variable, and setting the PKG_CPPFLAGS, and  PKG_LIBS environment 
variables as well.  No luck so far.

(initially would not compile, but specification of the PKG_CPPFLAGS, and  
PKG_LIBS got the compile to complete without errors).  I still cannot load 
the library.  My error message is:

> library(RMySQL)
Loading required package: DBI
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library 
'/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
  /usr/lib64/R/library/RMySQL/libs/RMySQL.so: undefined symbol: 
mysql_field_count
Error in library(RMySQL) : .First.lib failed for 'RMySQL'
>

Trying to install the newest RMySQL on the newest R.

> version
         _
platform x86_64-redhat-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu
status
major    2
minor    2.1
year     2005
month    12
day      20
svn rev  36812
language R

*********************
I find libmysqlclient.so in the following location(s).

[root at BX mtaylor]# locate libmysqlclient.so
/usr/lib/mysql/libmysqlclient.so.14.0.0
/usr/lib/mysql/libmysqlclient.so.14
/usr/lib64/mysql/libmysqlclient.so.10.0.0
/usr/lib64/mysql/libmysqlclient.so.10
/usr/lib64/mysql/libmysqlclient.so.14.0.0
/usr/lib64/mysql/libmysqlclient.so.14
/usr/lib64/mysql/libmysqlclient.so
/usr/lib64/mysql3/mysql/libmysqlclient.so.10.0.0
/usr/lib64/mysql3/mysql/libmysqlclient.so.10
/usr/lib64/mysql3/mysql/libmysqlclient.so

**********************

I set the LD_LIBRARY_PATH according to these results, then double checked that 
it was set.  (I also set it to /usr/lib64/mysql3/mysql as another iteration)

[root at BX mtaylor]# printenv LD_LIBRARY_PATH
/usr/lib64/mysql3/

Just to be sure...

[root at BX mtaylor]# printenv PKG_CPPFLAGS
-I/usr/include/mysql

[root at BX mtaylor]# printenv PKG_LIBS
-L/usr/lib64/mysql

(also tried PKG_LIBS=-L/usr/lib64/mysql3/mysql and PKG_LIBS=-L/usr/lib/mysql)

I recompiled the package each time with R CMD INSTALL RMySQL_0.5-7.tar.gz, but 
I always get the same error message.  

******************
appears to install ....

[root at BX mtaylor]# R CMD INSTALL RMySQL_0.5-7.tar.gz
* Installing *source* package 'RMySQL' ...
creating cache ./config.cache
checking how to run the C preprocessor... cc -E
checking for compress in -lz... yes
checking for getopt_long in -lc... yes
checking for mysql_init in -lmysqlclient... no
checking for mysql.h... no
updating cache ./config.cache
creating ./config.status
creating src/Makevars
** libs
gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
RS-DBI.c -o RS-DBI.o
gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
RS-MySQL.c -o RS-MySQL.o
gcc -shared -L/usr/local/lib64 -o RMySQL.so RS-DBI.o RS-MySQL.o 
-L/usr/lib/mysql -lz  -L/usr/lib64/R/lib -lR
** R
** inst
** save image
.
snip
.
** building package indices ...
* DONE (RMySQL)


[root at BX mtaylor]# /sbin/ldconfig -v | grep mysql
/usr/lib/mysql:
        libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
        libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
/usr/lib64/mysql:
        libmysqlclient_r.so.10 -> libmysqlclient_r.so.10.0.0
        libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
        libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
        libmysqlclient.so.10 -> libmysqlclient.so.10.0.0

just to make sure the headers are actually in the specified location...

[root at BX mtaylor]# ls /usr/include/mysql
chardefs.h  m_ctype.h    my_dir.h     my_no_pthread.h  mysql_embed.h    
my_xml.h     rlshell.h      sslopt-longopts.h
errmsg.h    m_string.h   my_getopt.h  my_pthread.h     mysql.h          raid.h       
rltypedefs.h   sslopt-vars.h
history.h   my_alloc.h   my_global.h  my_semaphore.h   mysql_time.h     
readline.h   sql_common.h   tilde.h
keycache.h  my_config.h  my_list.h    mysql_com.h      mysql_version.h  
rlmbutil.h   sql_state.h    typelib.h
keymaps.h   my_dbug.h    my_net.h     mysqld_error.h   my_sys.h         
rlprivate.h  sslopt-case.h  xmalloc.h


There is a dial in here somewhere that I think I am turning the wrong 
direction - just can't see it.  Any experience with this one?


-- 
=======================================
Michaell Taylor, PhD.
Principal
Boxwood Means, Inc.
203.653.4100



From andy_liaw at merck.com  Mon Jan 30 22:33:56 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jan 2006 16:33:56 -0500
Subject: [R] collating columns
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED78D@usctmx1106.merck.com>

Here's one possible way (assuming the two data frames have the same number
of columns):

> d1 <- data.frame(A=1, B=2, C=3)
> d2 <- data.frame(X=1, Y=2, Z=3)
> res <- c(d1, d2)  # This cbind them and turn into a list.
> idx <- as.vector(matrix(1:(2 * ncol(d1)), 2, byrow=TRUE))
> idx
[1] 1 4 2 5 3 6
> as.data.frame(res[idx])
  A X B Y C Z
1 1 1 2 2 3 3

HTH,
Andy

From: Matthew Scholz
> 
> Dumb newbie question: I've searched the manual, R help and 
> the mailing list
> archives, but can't seem to find the answer to this simple 
> problem that I
> have. If I have a series of columns in a dataframe: (A, B, C 
> ...) and I want
> to merge them with another series of columns (Z,Y,X ...) such that the
> resulting data frame has columns A,Z,B,Y,C,X ..., how do I do 
> this? In other
> words, I'm trying to collate two column sets (for purposes of 
> writing a
> presentable file) rather than simply using cbind to add one 
> set of columns
> onto the end of another set.
> 
> Thanks in advance of your help.
> 
> Matt
> --
> Matt Scholz
> Senior Research Specialist
> Department of Plant Sciences
> University of Arizona
> (520) 621-1695
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Mon Jan 30 22:37:06 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 22:37:06 +0100
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <43DE7FEA.8060703@stevens.edu>
References: <43DE3F90.5050706@stevens.edu>
	<644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
	<x2bqxtqvjo.fsf@turmalin.kubism.ku.dk> <43DE7FEA.8060703@stevens.edu>
Message-ID: <x264o1qtx9.fsf@turmalin.kubism.ku.dk>

Ionut Florescu <ifloresc at stevens.edu> writes:

> Actually it does that in my 2.2.1 version as well:
> 
>  > options(digits=20)
>  > 8^(1:20)
>  [1] 8.0000000000000000e+00 6.4000000000000004e+01 5.1200000000000001e+02
>  [4] 4.0960000000000001e+03 3.2768000000000002e+04 2.6214400000000002e+05
>  [7] 2.0971519999999999e+06 1.6777215999999999e+07 1.3421772800000000e+08
> [10] 1.0737418240000001e+09 8.5899345920000005e+09 6.8719476736000003e+10
> [13] 5.4975581388799997e+11 4.3980465111039999e+12 3.5184372088832001e+13
> [16] 2.8147497671065600e+14 2.2517998136852482e+15 1.8014398509481984e+16
> [19] 1.4411518807585588e+17 1.1529215046068471e+18
> 

Hmm, then it is a platform dependency and my memory playing tricks on
me... The thing that got fixed was log2(8), perhaps?
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From antonio.fabio at gmail.com  Mon Jan 30 22:30:11 2006
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Mon, 30 Jan 2006 22:30:11 +0100
Subject: [R] how to get the row name?
In-Reply-To: <43DE7E9C.6000103@stevens.edu>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net> <43DE7E9C.6000103@stevens.edu>
Message-ID: <b0808fdc0601301330w698da81fj@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/ac474de2/attachment.pl

From f.harrell at vanderbilt.edu  Mon Jan 30 22:37:23 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 30 Jan 2006 15:37:23 -0600
Subject: [R] Logistic regression model selection
 with	overdispersed/autocorrelated data
In-Reply-To: <OF6C249F80.0076B321-ON87257106.0059AE18@pc.gc.ca>
References: <OF6C249F80.0076B321-ON87257106.0059AE18@pc.gc.ca>
Message-ID: <43DE8713.4020506@vanderbilt.edu>

Jesse.Whittington at pc.gc.ca wrote:
> 
> I am creating habitat selection models for caribou and other species with
> data collected from GPS collars.  In my current situation the radio-collars
> recorded the locations of 30 caribou every 6 hours.  I am then comparing
> resources used at caribou locations to random locations using logistic
> regression (standard habitat analysis).
> 
> The data is therefore highly autocorrelated and this causes Type I error
> two ways  small standard errors around beta-coefficients and
> over-paramaterization during model selection.  Robust standard errors are
> easily calculated by block-bootstrapping the data using animal as a
> cluster with the Design library, however I havent found a satisfactory
> solution for model selection.
> 
> A couple options are:
> 1.  Using QAIC where the deviance is divided by a variance inflation factor
> (Burnham & Anderson).  However, this VIF can vary greatly depending on the
> data set and the set of covariates used in the global model.
> 2.  Manual forward stepwise regression using both changes in deviance and
> robust p-values for the beta-coefficients.
> 
> I have been looking for a solution to this problem for a couple years and
> would appreciate any advice.
> 
> Jesse

If you must do non-subject-matter-driven model selection, look at the 
fastbw function in Design, which will use the cluster bootstrap variance 
matrix.

Frank

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Mon Jan 30 22:47:20 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 22:47:20 +0100
Subject: [R] Type II SS for fixed-effect in mixed model
In-Reply-To: <000001c625e3$3bc97460$5044d084@TAMIAS>
References: <000001c625e3$3bc97460$5044d084@TAMIAS>
Message-ID: <x21wypqtg7.fsf@turmalin.kubism.ku.dk>

"Martin Julien" <martin.julien.2 at courrier.uqam.ca> writes:

> Hi
> In mixed-model with lme()
> How can I obtain Type II SS or Type III SS for fixed effect?
> Thanks 
> Julien

You don't want that (look up suitable sermon by Bill Venables in the
archives). Single-term Wald tests, however, are available using the 
syntax

anova(myfit,"myfactor")

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From david.reitter at gmail.com  Mon Jan 30 23:01:33 2006
From: david.reitter at gmail.com (David Reitter)
Date: Mon, 30 Jan 2006 22:01:33 +0000
Subject: [R] predict.lme / nlmmPQL: "non-conformable arguments"
Message-ID: <2D31CED5-2507-4612-884D-FC81541D2526@gmail.com>

I'm trying to use "predict" with a linear mixed-effects logistic  
regression model fitted with nlmmPQL from the MASS library.
Unfortunately, I'm getting an error "non-conformable arguments" in  
predict.lme, and I would like to understand why.

I have used the same call to "predict" with "glm" models without  
problems. I assume I'm doing something wrong, but I have no idea what  
it is. If someone could help me (even by telling me how to trace this  
properly - is there an interactive tracer/debugger I can use?),  
that'd be fantastic.

Here's what I'm doing:

 > summary(model)
...
Random effects:
Formula: ~log(distance) | target.utt
...
Fixed effects: primed ~ log(distance) * role * source - log 
(distance):source
...

 >  x=10:500*0.1
 >  new <- data.frame(distance=x, role="r", source="m"  )

 > yp = predict(model,  newdata=new, type="response",  level=0)
Error in X %*% fixef(object) : non-conformable arguments


 > traceback()
4: predict.lme(object, newdata, level = level, na.action = na.action)
3: predict(object, newdata, level = level, na.action = na.action)
2: predict.glmmPQL(model, newdata = new, type = "response", level = 0)
1: predict(model, newdata = new, type = "response", level = 0)



From p.dalgaard at biostat.ku.dk  Mon Jan 30 23:15:12 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2006 23:15:12 +0100
Subject: [R] RMySQL install
In-Reply-To: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
References: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
Message-ID: <x2wtghpdlb.fsf@turmalin.kubism.ku.dk>

Michaell Taylor <Michaell.Taylor at boxwoodmeans.com> writes:

> I am having trouble installing RMySQL on a clean install of Fedora Core 4 64 
> bit on a dual dual core machine (that is, two dual core processors).  Seems 
> like the LD_LIBRARY_PATH is incorrect, but I don't seem to have it quite 
> right yet. 
> 
> There are a few mentions of this problem in google, but thus far none of the 
> "fixes" and fixed my problem.  I've tried defining the LD_LIBRARY_PATH 
> environment variable, and setting the PKG_CPPFLAGS, and  PKG_LIBS environment 
> variables as well.  No luck so far.
> 
> (initially would not compile, but specification of the PKG_CPPFLAGS, and  
> PKG_LIBS got the compile to complete without errors).  I still cannot load 
> the library.  My error message is:
> 
> > library(RMySQL)
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library 
> '/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
>   /usr/lib64/R/library/RMySQL/libs/RMySQL.so: undefined symbol: 
> mysql_field_count

Hmm. You seem to have pressed most of the buttons that I'd try.


If it keeps acting up, the RODBC package seems to install nicely on
FC4, once you have installed unixODBC and -devel.

> Error in library(RMySQL) : .First.lib failed for 'RMySQL'
> >
> 
> Trying to install the newest RMySQL on the newest R.
> 
> > version
>          _
> platform x86_64-redhat-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> *********************
> I find libmysqlclient.so in the following location(s).
> 
> [root at BX mtaylor]# locate libmysqlclient.so
> /usr/lib/mysql/libmysqlclient.so.14.0.0
> /usr/lib/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql/libmysqlclient.so.10
> /usr/lib64/mysql/libmysqlclient.so.14.0.0
> /usr/lib64/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10
> /usr/lib64/mysql3/mysql/libmysqlclient.so
> 
> **********************
> 
> I set the LD_LIBRARY_PATH according to these results, then double checked that 
> it was set.  (I also set it to /usr/lib64/mysql3/mysql as another iteration)
> 
> [root at BX mtaylor]# printenv LD_LIBRARY_PATH
> /usr/lib64/mysql3/
> 
> Just to be sure...
> 
> [root at BX mtaylor]# printenv PKG_CPPFLAGS
> -I/usr/include/mysql
> 
> [root at BX mtaylor]# printenv PKG_LIBS
> -L/usr/lib64/mysql
> 
> (also tried PKG_LIBS=-L/usr/lib64/mysql3/mysql and PKG_LIBS=-L/usr/lib/mysql)
> 
> I recompiled the package each time with R CMD INSTALL RMySQL_0.5-7.tar.gz, but 
> I always get the same error message.  
> 
> ******************
> appears to install ....
> 
> [root at BX mtaylor]# R CMD INSTALL RMySQL_0.5-7.tar.gz
> * Installing *source* package 'RMySQL' ...
> creating cache ./config.cache
> checking how to run the C preprocessor... cc -E
> checking for compress in -lz... yes
> checking for getopt_long in -lc... yes
> checking for mysql_init in -lmysqlclient... no
> checking for mysql.h... no
> updating cache ./config.cache
> creating ./config.status
> creating src/Makevars
> ** libs
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
> -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
> RS-DBI.c -o RS-DBI.o
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
> -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
> RS-MySQL.c -o RS-MySQL.o
> gcc -shared -L/usr/local/lib64 -o RMySQL.so RS-DBI.o RS-MySQL.o 
> -L/usr/lib/mysql -lz  -L/usr/lib64/R/lib -lR
> ** R
> ** inst
> ** save image
> .
> snip
> .
> ** building package indices ...
> * DONE (RMySQL)
> 
> 
> [root at BX mtaylor]# /sbin/ldconfig -v | grep mysql
> /usr/lib/mysql:
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
> /usr/lib64/mysql:
>         libmysqlclient_r.so.10 -> libmysqlclient_r.so.10.0.0
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
>         libmysqlclient.so.10 -> libmysqlclient.so.10.0.0
> 
> just to make sure the headers are actually in the specified location...
> 
> [root at BX mtaylor]# ls /usr/include/mysql
> chardefs.h  m_ctype.h    my_dir.h     my_no_pthread.h  mysql_embed.h    
> my_xml.h     rlshell.h      sslopt-longopts.h
> errmsg.h    m_string.h   my_getopt.h  my_pthread.h     mysql.h          raid.h       
> rltypedefs.h   sslopt-vars.h
> history.h   my_alloc.h   my_global.h  my_semaphore.h   mysql_time.h     
> readline.h   sql_common.h   tilde.h
> keycache.h  my_config.h  my_list.h    mysql_com.h      mysql_version.h  
> rlmbutil.h   sql_state.h    typelib.h
> keymaps.h   my_dbug.h    my_net.h     mysqld_error.h   my_sys.h         
> rlprivate.h  sslopt-case.h  xmalloc.h
> 
> 
> There is a dial in here somewhere that I think I am turning the wrong 
> direction - just can't see it.  Any experience with this one?
> 
> 
> -- 
> =======================================
> Michaell Taylor, PhD.
> Principal
> Boxwood Means, Inc.
> 203.653.4100
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From comtech.usa at gmail.com  Mon Jan 30 23:37:58 2006
From: comtech.usa at gmail.com (Michael)
Date: Mon, 30 Jan 2006 14:37:58 -0800
Subject: [R] matlab-like constant matrix initialization?
In-Reply-To: <971536df0601300651o7f648a40h524e8b0ed33e7a4a@mail.gmail.com>
References: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
	<971536df0601300651o7f648a40h524e8b0ed33e7a4a@mail.gmail.com>
Message-ID: <b1f16d9d0601301437x4be75ea6yd46420bdd3d45c9e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/6aed69af/attachment.pl

From comtech.usa at gmail.com  Mon Jan 30 23:38:52 2006
From: comtech.usa at gmail.com (Michael)
Date: Mon, 30 Jan 2006 14:38:52 -0800
Subject: [R] matlab-like constant matrix initialization?
In-Reply-To: <47fce0650601300147jd003deg@mail.gmail.com>
References: <b1f16d9d0601300024w8fb98f5qf4ba4bfb972271a6@mail.gmail.com>
	<47fce0650601300147jd003deg@mail.gmail.com>
Message-ID: <b1f16d9d0601301438i167da097w691d9f52f221039c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/1c07c282/attachment.pl

From nicolas.ferrari at m4x.org  Mon Jan 30 23:42:04 2006
From: nicolas.ferrari at m4x.org (Nicolas Ferrari)
Date: Mon, 30 Jan 2006 23:42:04 +0100
Subject: [R] testing the signifiance of variables in a robust regression
	with M-estimators
Message-ID: <000601c625ee$64deb940$0500a8c0@enzo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/28cb18e2/attachment.pl

From droberts at montana.edu  Mon Jan 30 23:38:05 2006
From: droberts at montana.edu (Dave Roberts)
Date: Mon, 30 Jan 2006 15:38:05 -0700
Subject: [R] collating columns
In-Reply-To: <3035b8df0601301309o428947fdq88b9dcaed5aa182a@mail.gmail.com>
References: <3035b8df0601301309o428947fdq88b9dcaed5aa182a@mail.gmail.com>
Message-ID: <43DE954D.4090808@montana.edu>

You can just use data.frame().  If (using your example) your dataframes 
are called first and second, your could

new <- dataframe(first$A,second$Z,first$B,second$Y,first$C,second$X...)

followed by

names(new) <- c('A','Z','B','Y','C','X')

If you have an enormous number of columns that's a pain, but it works. 
If they're both attached (and the names are unique) you can omit the 
"first$" and "second$"

If the dataframes are large, but the spacing is regular, you could do

new <- data.frame(first,second)
column <- c()
for (i in 1:ncol(first)) column <- c(column,i,i+ncol(first))
new <- new[,column]

that way you get columns interspersed in the desired order.

Dave R.
-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
David W. Roberts                                     office 406-994-4548
Professor and Head                                      FAX 406-994-3190
Department of Ecology                         email droberts at montana.edu
Montana State University
Bozeman, MT 59717-3460

Matthew Scholz wrote:
> Dumb newbie question: I've searched the manual, R help and the mailing list
> archives, but can't seem to find the answer to this simple problem that I
> have. If I have a series of columns in a dataframe: (A, B, C ...) and I want
> to merge them with another series of columns (Z,Y,X ...) such that the
> resulting data frame has columns A,Z,B,Y,C,X ..., how do I do this? In other
> words, I'm trying to collate two column sets (for purposes of writing a
> presentable file) rather than simply using cbind to add one set of columns
> onto the end of another set.
> 
> Thanks in advance of your help.
> 
> Matt
> --
> Matt Scholz
> Senior Research Specialist
> Department of Plant Sciences
> University of Arizona
> (520) 621-1695
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From dj at research.bell-labs.com  Mon Jan 30 23:45:48 2006
From: dj at research.bell-labs.com (David James)
Date: Mon, 30 Jan 2006 17:45:48 -0500
Subject: [R] RMySQL install
In-Reply-To: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
References: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
Message-ID: <20060130224547.GC779@jessie.research.bell-labs.com>

Michaell Taylor wrote:
> 
> I am having trouble installing RMySQL on a clean install of Fedora Core 4 64 
> bit on a dual dual core machine (that is, two dual core processors).  Seems 
> like the LD_LIBRARY_PATH is incorrect, but I don't seem to have it quite 
> right yet. 
> 
> There are a few mentions of this problem in google, but thus far none of the 
> "fixes" and fixed my problem.  I've tried defining the LD_LIBRARY_PATH 
> environment variable, and setting the PKG_CPPFLAGS, and  PKG_LIBS environment 
> variables as well.  No luck so far.
> 
> (initially would not compile, but specification of the PKG_CPPFLAGS, and  
> PKG_LIBS got the compile to complete without errors).  I still cannot load 
> the library.  My error message is:
> 
> > library(RMySQL)
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library 
> '/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
>   /usr/lib64/R/library/RMySQL/libs/RMySQL.so: undefined symbol: 
> mysql_field_count
> Error in library(RMySQL) : .First.lib failed for 'RMySQL'
> >
> 
> Trying to install the newest RMySQL on the newest R.
> 
> > version
>          _
> platform x86_64-redhat-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
> 
> *********************
> I find libmysqlclient.so in the following location(s).
> 
> [root at BX mtaylor]# locate libmysqlclient.so
> /usr/lib/mysql/libmysqlclient.so.14.0.0
> /usr/lib/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql/libmysqlclient.so.10
> /usr/lib64/mysql/libmysqlclient.so.14.0.0
> /usr/lib64/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10
> /usr/lib64/mysql3/mysql/libmysqlclient.so
> 
> **********************
> 
> I set the LD_LIBRARY_PATH according to these results, then double checked that 
> it was set.  (I also set it to /usr/lib64/mysql3/mysql as another iteration)
> 
> [root at BX mtaylor]# printenv LD_LIBRARY_PATH
> /usr/lib64/mysql3/
> 
> Just to be sure...
> 
> [root at BX mtaylor]# printenv PKG_CPPFLAGS
> -I/usr/include/mysql
> 
> [root at BX mtaylor]# printenv PKG_LIBS
> -L/usr/lib64/mysql
> 
> (also tried PKG_LIBS=-L/usr/lib64/mysql3/mysql and PKG_LIBS=-L/usr/lib/mysql)
> 
> I recompiled the package each time with R CMD INSTALL RMySQL_0.5-7.tar.gz, but 
> I always get the same error message.  
> 
> ******************
> appears to install ....
> 
> [root at BX mtaylor]# R CMD INSTALL RMySQL_0.5-7.tar.gz
> * Installing *source* package 'RMySQL' ...
> creating cache ./config.cache
> checking how to run the C preprocessor... cc -E
> checking for compress in -lz... yes
> checking for getopt_long in -lc... yes
> checking for mysql_init in -lmysqlclient... no
> checking for mysql.h... no
> updating cache ./config.cache
> creating ./config.status
> creating src/Makevars
> ** libs
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
> -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
> RS-DBI.c -o RS-DBI.o
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include   -fPIC  
> -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona -c 
> RS-MySQL.c -o RS-MySQL.o
> gcc -shared -L/usr/local/lib64 -o RMySQL.so RS-DBI.o RS-MySQL.o 
> -L/usr/lib/mysql -lz  -L/usr/lib64/R/lib -lR
> ** R
> ** inst
> ** save image
> .
> snip
> .
> ** building package indices ...
> * DONE (RMySQL)
> 
> 
> [root at BX mtaylor]# /sbin/ldconfig -v | grep mysql
> /usr/lib/mysql:
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
> /usr/lib64/mysql:
>         libmysqlclient_r.so.10 -> libmysqlclient_r.so.10.0.0
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
>         libmysqlclient.so.10 -> libmysqlclient.so.10.0.0
> 
> just to make sure the headers are actually in the specified location...
> 
> [root at BX mtaylor]# ls /usr/include/mysql
> chardefs.h  m_ctype.h    my_dir.h     my_no_pthread.h  mysql_embed.h    
> my_xml.h     rlshell.h      sslopt-longopts.h
> errmsg.h    m_string.h   my_getopt.h  my_pthread.h     mysql.h          raid.h       
> rltypedefs.h   sslopt-vars.h
> history.h   my_alloc.h   my_global.h  my_semaphore.h   mysql_time.h     
> readline.h   sql_common.h   tilde.h
> keycache.h  my_config.h  my_list.h    mysql_com.h      mysql_version.h  
> rlmbutil.h   sql_state.h    typelib.h
> keymaps.h   my_dbug.h    my_net.h     mysqld_error.h   my_sys.h         
> rlprivate.h  sslopt-case.h  xmalloc.h
> 
> 
> There is a dial in here somewhere that I think I am turning the wrong 
> direction - just can't see it.  Any experience with this one?
> 
> 
> -- 
> =======================================
> Michaell Taylor, PhD.
> Principal
> Boxwood Means, Inc.
> 203.653.4100
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

We did have similar problems, but it turned out to be a MySQL 
installation problem -- for unknown reasons (to me, at least) 
the symbolic link to libmysqlclient.so.14 was pointing nowhere.
After re-installing the client and devel RPMS things worked fine:

export PKG_LIBS='-L/usr/lib64/mysql  -lmysqlclient'
export PKG_CPPFLAGS=-I/usr/include/mysql

R CMD INSTALL -l /tmp/R RMySQL_0.5-7.tar.gz
...

> library(RMySQL, lib.loc="/tmp/R")
Loading required package: DBI
> sessionInfo()
R version 2.2.1, 2005-12-20, x86_64-unknown-linux-gnu

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils" "datasets"
[7] "base"

other attached packages:
RMySQL     DBI
"0.5-7" "0.1-9"


Hope this helps,

-- 
David



From pauljohn32 at gmail.com  Tue Jan 31 00:37:19 2006
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 30 Jan 2006 17:37:19 -0600
Subject: [R] Sweave: trouble controlling Design package startup output
Message-ID: <13e802630601301537x15f04e4es659a4c93be76dcb0@mail.gmail.com>

Hello, everybody:

I'm experimenting more with Sweave, R, and LyX. There's now an entry
in the LyX wiki on using R, so anybody can do it!

http://wiki.lyx.org/LyX/LyxWithRThroughSweave

Now I notice this frustrating thing.  I think I've done everything
possible to make the Design library start up without any fanfare:


<<echo=F,quiet=T,print=F>>=
library(Design, verbose=F)
@

But in the output there is still one page of banner information, which
starts like this:

Loading required package: Hmisc
Hmisc library by Frank E Harrell Jr
Type library(help= Hmisc ), ?Overview, or ?Hmisc.Overview )
to see overall documentation.
NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting. To get the old behavior of Hmisc type dropUnusedLevels().
  Attaching package: Hmisc      The following object(s) are masked
from package:stats :
[and so forth]

Can you tell me how to make it stop?

I've seen this kind of output with a couple of other packages over the
past 2 months, but I can't find the files that produced them, so I
can't cite the examples.  But I don't think it is confined to Design.

--
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas



From costas.magnuse at gmail.com  Tue Jan 31 00:50:56 2006
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Tue, 31 Jan 2006 01:50:56 +0200
Subject: [R] Loops that last for ever...
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED78B@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED78B@usctmx1106.merck.com>
Message-ID: <30ddfdae0601301550r21447b2eg@mail.gmail.com>

Thank you very very much for your responses...
How exactly do I vectorize?
> > One example could be the following,  that calculates the sums 1:5,
> > 2:6, 3:7,...,  for each of xs[[i]] :
> >
> > xs <- lapply(1:500,  function(x) rnorm(1000))
> > totalsum <- list()
> > sums <- list()
> > first <- list()
> >
> > for(i in 1:length(xs)) {
> > totalsum[i] <- sum(xs[[i]])
> >       for(j in 1:length(xs[[i]])) {
> >               if(j == 1) {
> >                       sums[[i]] <- list()
> >                       }
> >               if(j >= 5) {
> >                       sums[[i]][j] <- sum(xs[[i]][(j-4):j])
> >                       }
> >               }
> > }
>
> For this you want to vectorize the computation inside, eliminating the j
> loop, then use lapply() if you like for the outer loop.  That saves you the
> line to initialize the list.



From f.harrell at vanderbilt.edu  Tue Jan 31 00:53:32 2006
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 30 Jan 2006 17:53:32 -0600
Subject: [R] Sweave: trouble controlling Design package startup output
In-Reply-To: <13e802630601301537x15f04e4es659a4c93be76dcb0@mail.gmail.com>
References: <13e802630601301537x15f04e4es659a4c93be76dcb0@mail.gmail.com>
Message-ID: <43DEA6FC.4010705@vanderbilt.edu>

Paul Johnson wrote:
> Hello, everybody:
> 
> I'm experimenting more with Sweave, R, and LyX. There's now an entry
> in the LyX wiki on using R, so anybody can do it!
> 
> http://wiki.lyx.org/LyX/LyxWithRThroughSweave
> 
> Now I notice this frustrating thing.  I think I've done everything
> possible to make the Design library start up without any fanfare:
> 
> 
> <<echo=F,quiet=T,print=F>>=
> library(Design, verbose=F)
> @
> 
> But in the output there is still one page of banner information, which
> starts like this:
> 
> Loading required package: Hmisc
> Hmisc library by Frank E Harrell Jr
> Type library(help= Hmisc ), ?Overview, or ?Hmisc.Overview )
> to see overall documentation.
> NOTE:Hmisc no longer redefines [.factor to drop unused levels when
> subsetting. To get the old behavior of Hmisc type dropUnusedLevels().
>   Attaching package: Hmisc      The following object(s) are masked
> from package:stats :
> [and so forth]
> 
> Can you tell me how to make it stop?

options(Hverbose=FALSE) before library(Design)

Frank

> 
> I've seen this kind of output with a couple of other packages over the
> past 2 months, but I can't find the files that produced them, so I
> can't cite the examples.  But I don't think it is confined to Design.
> 
> --
> Paul E. Johnson
> Professor, Political Science
> 1541 Lilac Lane, Room 504
> University of Kansas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From costas.magnuse at gmail.com  Tue Jan 31 01:04:58 2006
From: costas.magnuse at gmail.com (Constantine Tsardounis)
Date: Tue, 31 Jan 2006 02:04:58 +0200
Subject: [R] Loops that last for ever...
In-Reply-To: <644e1f320601301117o2feed3a1g5fe5ff2aa389cc4c@mail.gmail.com>
References: <30ddfdae0601300941r1639435bn@mail.gmail.com>
	<644e1f320601301117o2feed3a1g5fe5ff2aa389cc4c@mail.gmail.com>
Message-ID: <30ddfdae0601301604o2c2a8094w@mail.gmail.com>

Thank you very much!!!  It works!.. I will try your philosophy to my script!...

xs <- lapply(1:500,  function(x) rnorm(1000))
result <- lapply(xs, function(.val){
    .total <- sum(.val)   # total sum
    .sums <- filter(.val, rep(1,5)) # sum 5 consective values
    list(total=.total, sum=.sums[-c(1,2,length(.sums)-1, length(.sums))])}
)
# create lists of sum and totals
total <- lapply(result, '[[', 'total')
sums <- lapply(result, '[[', 'sum')



From phawkins at connact.com  Tue Jan 31 00:38:39 2006
From: phawkins at connact.com (Patricia J. Hawkins)
Date: Mon, 30 Jan 2006 18:38:39 -0500
Subject: [R] Glossay of available R functions
In-Reply-To: <200601301900.50123.asaguiar@spsconsultoria.com> (Alexandre
	Santos Aguiar's message of "Mon, 30 Jan 2006 19:00:48 -0200")
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net>
	<200601301900.50123.asaguiar@spsconsultoria.com>
Message-ID: <wkvew1e16o.fsf@connact.com>

>>>>> "ASA" == Alexandre Santos Aguiar <asaguiar at spsconsultoria.com> writes:

ASA> I am new to R and read this list to learn. It is amazing how
ASA> frequently new functions pop in messages. Useful and timesaving
ASA> functions like subset (above) must be documented somewhere.

ASA> Is there a glossary of functions?

I'm also new to R, and was wondering the same thing.  Took a bunch of
tries, but if you run start.help() and then choose Packages, then
Base, you will get the list of functions.

As a newcomer, I hesitate to suggest this, but maybe there should be a
comment on the index page to that effect?

-- 
Patricia J. Hawkins
Hawkins Internet Applications
www.hawkinsia.com



From macq at llnl.gov  Tue Jan 31 01:28:45 2006
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 30 Jan 2006 16:28:45 -0800
Subject: [R] Date Not Staying in Date Format
In-Reply-To: <BAY107-F67D36AB649EEE07832946C3090@phx.gbl>
References: <BAY107-F67D36AB649EEE07832946C3090@phx.gbl>
Message-ID: <p06210210c0045f672b9e@[128.115.153.6]>

Try following this example:

>  foo <- Sys.time()
>  class(foo)
[1] "POSIXt"  "POSIXct"
>  ick <- as.numeric(foo)
>  ick
[1] 1138667199
>  class(ick)
[1] "numeric"
>  class(ick) <- class(foo)
>  ick
[1] "2006-01-30 16:26:39 PST"

-Don

At 3:51 PM -0500 1/30/06, David Randel wrote:
>I have a column in a data frame that has a class of "Date" and a mode of
>"numeric".  When I:
>
>max(df$Date)
>
>My output stays in Date format, i.e. "2006-01-03".
>
>However, when I run the following statment:
>
>tapply(df$Date, df$SomeFactor, max)
>
>my output looks like this:  9129   9493   9861  10226  10591  10956  11320 
>11687  12052  12417
>
>The returned object is of mode "numeric" and class "array".  Each array
>element is of mode "numeric" and class "numeric".  I believe that this is
>the integer representation of my date.  I can't seem to convert it back to a
>date.
>
>How do I get these to be intrepreted as a date instead of a number?
>
>Thanks,
>~Dave R.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Soren.Hojsgaard at agrsci.dk  Tue Jan 31 01:44:23 2006
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 31 Jan 2006 01:44:23 +0100
Subject: [R] lattice: combining panel.xyplot with panel.abline - is this
	possible?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC0387818C@DJFPOST01.djf.agrsci.dk>

Consider this data frame:
 
dat<-data.frame(id=gl(3,5),time=rep(1:5,3),cm1=rep(c(2,3,4),each=5),cm2=rep(c(2.5,3.5,4.5),each=5),y=rnorm(15))

> dat
   id time cm1 cm2          y
1   1    1   2 2.5 -1.0824549
2   1    2   2 2.5 -0.7784834
3   1    3   2 2.5 -1.7783560
4   1    4   2 2.5  0.5056637
5   1    5   2 2.5 -0.1967505
6   2    1   3 3.5  1.1643136
7   2    2   3 3.5 -1.4323765
8   2    3   3 3.5  0.6810645
9   2    4   3 3.5  1.1310363
10  2    5   3 3.5  1.5677158
11  3    1   4 4.5  1.6394222
12  3    2   4 4.5  1.2024161
13  3    3   4 4.5  0.1784666
14  3    4   4 4.5 -1.3260668
15  3    5   4 4.5 -0.2711263


I would like to plot y against time with a panel for each id, i.e. with something like 

xyplot(y~time|id, data=dat,type='l') or

xyplot(currCow.mean~currCow.cm|cowid, data=mergeData, type='l',
  panel=function(x,y,...){
    panel.xyplot(x,y)
  }
)


Then, I would additionally like to have two vertical lines in each panel, and these are defined by cm1 and cm2. My understanding of xyplot is that id will give a partitioning of data into subsets, so I expected to be able to get each subset into the panelfunction and from there I can extract cm1 and cm2 and call panel.abline(a,b)...

1) Is that possible? 
2) Are there other ways of achieving what I want?

Appologies if this is trivial...

Best regards

S??ren



From sasprog474474 at yahoo.com  Tue Jan 31 02:30:15 2006
From: sasprog474474 at yahoo.com (Greg Tarpinian)
Date: Mon, 30 Jan 2006 17:30:15 -0800 (PST)
Subject: [R] lme in R (WinXP) vs. Splus (HP UNIX)
Message-ID: <20060131013015.63957.qmail@web37110.mail.mud.yahoo.com>

R2.2 for WinXP, Splus 6.2.1 for HP 9000 Series, HP-UX 11.0.


I am trying to get a handle on why the same lme( ) code gives
such different answers.  My output makes me wonder if the 
fact that the UNIX box is 64 bits is the reason.  The estimated
random effects are identical, but the fixed effects are very
different.  Here is my R code and output, with some columns 
and rows deleted for space considerations:

FOO.lme1 <- lme(fixed = Y ~ X1*X2, 
random = ~ X2 | X3, data = FOO,
method = "REML",
control = list(maxIter = 200, msMaxIter = 200, 
tolerance = 1e-8, niterEM = 200,
msTol = 1e-8, msVerbose = TRUE, 
optimMethod = "Nelder-Mead"))

  0      203.991: 0.924323 -0.0884338 0.493897
  1      203.991: 0.924323 -0.0884338 0.493897

> summary(FOO.lme1)
Linear mixed-effects model fit by REML
 Data: FOO 
      AIC      BIC    logLik
  357.484 373.6868 -170.7420

Random effects:
 Formula: ~ X2 | X3
 Structure: General positive-definite, 
 Log-Cholesky parametrization
               StdDev   Corr  
(Intercept)    2.025868 (Intr)
X2             4.908517 -0.475
Residual       4.493171       

Fixed effects: Y ~ X1*X2
                              Value  p-value
(Intercept)                39.08067  0.0000
X1                         12.08700  0.0000
X2                          5.85417  0.1370
X1:X2                       0.80833  0.8299 

Number of Observations: 60
Number of Groups: 3


Here is my Splus code and output, similarly edited for
space considerations:

FOO.lme1 <- lme(fixed = Y ~ X1*X2, 
random = ~ X2 | X3, data = FOO,
method = "REML",
control = list(maxIter = 200, msMaxIter = 200, 
tolerance = 1e-8, niterEM = 200,
msTol = 1e-8, msVerbose = TRUE, 
optimMethod = "Nelder-Mead"))

Iteration:  0 ,  1  function calls, F=  205.3776 
Parameters: [1]  0.89525077  0.22549223 -0.05936197

Iteration:  1 ,  2  function calls, F=  205.3776 
Parameters: [1]  0.89525078  0.22549224 -0.05936187

> summary(FOO.lme1)
Linear mixed-effects model fit by REML
 Data: FOO 
       AIC      BIC    logLik 
  360.2566 376.4594 -172.1283

Random effects:
 Formula:  ~ X2 | X3
 Structure: General positive-definite
                 StdDev   Corr 
   (Intercept) 2.025861 (Inter
            X2 4.908618 -0.475
      Residual 4.493172       

Fixed effects: Y ~ X1* X2
                        Value  p-value 
         (Intercept) 45.12417  <.0001
               X1     6.04350  <.0001
               X2     6.25833  0.0709
            X1:X2     0.40417  0.8299

Number of Observations: 60
Number of Groups: 3


I have examined the help.start() HTML pages and the lmeScale( )
function looks helpful, but I don't know how to use it to tell
R to use a particular set of starting values for the params.
It would have been instructive to see what happens if the same
starting values are specified for both Splus and R....

Below is a sample dataset that is fairly close to the data
that I have actually been using (not specifically the data
used to obtain the output above). 

Any help would be greatly appreciated.


Kind Regards,

     Greg




Sample Data:
A,X3,X2,X1,Y
1,A,1.6,A,43
1,A,1.6,B,56
2,A,1.6,A,43
2,A,1.6,B,50
3,A,1.6,A,46
3,A,1.6,B,64
4,A,2.0,A,42
4,A,2.0,B,42
5,A,2.0,A,36
5,A,2.0,B,44
6,A,2.0,A,31
6,A,2.0,B,44
7,A,2.0,A,39
7,A,2.0,B,46
8,A,2.4,A,47
8,A,2.4,B,52
9,A,2.4,A,50
9,A,2.4,B,52
10,A,2.4,A,43
10,A,2.4,B,56
11,B,1.6,A,35
11,B,1.6,B,48
12,B,1.6,A,32
12,B,1.6,B,45
13,B,1.6,A,37
13,B,1.6,B,50
14,B,2.0,A,40
14,B,2.0,B,55
15,B,2.0,A,33
15,B,2.0,B,46
16,B,2.0,A,38
16,B,2.0,B,53
17,B,2.0,A,42
17,B,2.0,B,55
18,B,2.4,A,39
18,B,2.4,B,56
19,B,2.4,A,43
19,B,2.4,B,56
20,B,2.4,A,45
20,B,2.4,B,62
21,C,1.6,A,33
21,C,1.6,B,41
22,C,1.6,A,33
22,C,1.6,B,44
23,C,1.6,A,31
23,C,1.6,B,46
24,C,2.0,A,38
24,C,2.0,B,48
25,C,2.0,A,34
25,C,2.0,B,45
26,C,2.0,A,36
26,C,2.0,B,51
27,C,2.0,A,40
27,C,2.0,B,57
28,C,2.4,A,39
28,C,2.4,B,53
29,C,2.4,A,34
29,C,2.4,B,51
30,C,2.4,A,36
30,C,2.4,B,52



From spencer.graves at pdf.com  Tue Jan 31 02:45:58 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 Jan 2006 17:45:58 -0800
Subject: [R] How do I "normalise" a power spectral density analysis?
In-Reply-To: <1138373637.4afbee09782cc@webmail1.leeds.ac.uk>
References: <1138373637.4afbee09782cc@webmail1.leeds.ac.uk>
Message-ID: <43DEC156.2090900@pdf.com>

	  Since I have not seen a reply to this post, I will offer a comment, 
even though I have not used spectral analysis myself and therefore have 
you intuition about it.  First, from the definitions I read in the 
results from, e.g., RSiteSearch("time series power spectral density") 
[e.g., 
http://finzi.psych.upenn.edu/R/library/GeneTS/html/periodogram.html] and 
"spectral analysis" in Venables and Ripley (2002) Modern Applied 
Statistics with S (Springer), I see no reason why you couldn't plot the 
spectrum vs. the period rather than the frequency.  Someone else may 
help us understand why it is usually plotted vs. the frequency;  I'd 
guess that the standard plot looks more like the integrand in the 
standard Fourier inversion formula, but I'm not sure.

	  If you'd like more help from this listserve, you might briefly 
describe the problem you are trying to solve, why you think spectral 
analysis analysis should help, and include a toy example with some 
self-contained R code to illustrate what you tried and what you don't 
understand about it.  (And PLEASE do read the posting guide! 
"www.R-project.org/posting-guide.html".  Nothing is certain but 
following that posting guide will, I believe, tend to increase the speed 
and utility of response.)

	  hope this helps.
	  spencer graves

Tom C Cameron wrote:

> Hi everyone
> 
> Can anyone tell me how I normalise a power spectral density (PSD) plot of a
> periodical time-series. At present I get the graphical output of spectrum VS
> frequency.
> 
> What I want to acheive is period VS spectrum? Are these the same things but the
> x-axis scale needs transformed ?
> 
> Any help would be greatly appreciated
> 
> Tom
> ...........................................................................
> Dr Tom C Cameron                        office: 0113 34 32837 (10.23 Miall)
> Ecology & Evolution Res. Group.         lab: 0113 34 32884 (10.20 Miall)
> School of Biological Sciences           Mobile: 07966160266
> University of Leeds                     email: t.c.cameron at leeds.ac.uk
> Leeds LS2 9JT
> LS2 9JT
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Jan 31 03:01:41 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 21:01:41 -0500
Subject: [R] Date Not Staying in Date Format
In-Reply-To: <BAY107-F67D36AB649EEE07832946C3090@phx.gbl>
References: <BAY107-F67D36AB649EEE07832946C3090@phx.gbl>
Message-ID: <971536df0601301801s4e6b2461v2f8c4c34c6ea058b@mail.gmail.com>

Using this test data, say:

   dd <- Sys.Date() + 0:9
   fac <- gl(5,2)


Try this:

   do.call("c", tapply(dd, fac, max, simplify = FALSE))

or if a list result is ok then just

   tapply(dd, fac, max, simplify = FALSE)

Another possibility using the as.Date.numeric method in the zoo
package is:

  library(zoo)
  as.Date(tapply(dd, fac, max))

For more info on the internal representation of "Date" class objects see
the Help desk article in R News 4/1.


On 1/30/06, David Randel <davidr00 at hotmail.com> wrote:
> I have a column in a data frame that has a class of "Date" and a mode of
> "numeric".  When I:
>
> max(df$Date)
>
> My output stays in Date format, i.e. "2006-01-03".
>
> However, when I run the following statment:
>
> tapply(df$Date, df$SomeFactor, max)
>
> my output looks like this:  9129   9493   9861  10226  10591  10956  11320
> 11687  12052  12417
>
> The returned object is of mode "numeric" and class "array".  Each array
> element is of mode "numeric" and class "numeric".  I believe that this is
> the integer representation of my date.  I can't seem to convert it back to a
> date.
>
> How do I get these to be intrepreted as a date instead of a number?
>
> Thanks,
> ~Dave R.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From phawkins at connact.com  Tue Jan 31 04:37:41 2006
From: phawkins at connact.com (Patricia J. Hawkins)
Date: Mon, 30 Jan 2006 22:37:41 -0500
Subject: [R] yet another vectorization question
In-Reply-To: <200601302209.18419.adi@roda.ro> (Adrian Dusa's message of
	"Mon, 30 Jan 2006 22:09:18 +0200")
References: <200601301334.40440.adi@roda.ro> <200601302009.55504.adi@roda.ro>
	<43DE6CA3.4070208@pburns.seanet.com>
	<200601302209.18419.adi@roda.ro>
Message-ID: <wkmzhdcbju.fsf@connact.com>

>>>>> "AD" == Adrian Dusa <adi at roda.ro> writes:

AD> set.seed(5)
AD> aa <- matrix(sample(10, 15, replace=T), ncol=5)
AD> bb <- matrix(NA, ncol=10, nrow=5)
AD> for (i in 1:ncol(aa)) bb[i, aa[, i]] <- c(0, 1, 0)

AD> Is there any possibility to vectorize this "for" loop?
AD> (sometimes I have hundreds of columns in the "aa" matrix)

Well, coming from ignorance of R, I came up with the below.  However,
it means creating another vector that's the size of aa, so it's not
clear that it's a win:

#Problem:  Indexing bb correctly when vectorized
#Solution:  Add the following matrix to aa:
#     [,1] [,2] [,3] [,4] [,5]
#[1,]    0   10   20   30   40
#[2,]    0   10   20   30   40
#[3,]    0   10   20   30   40
#
# or its vector equivalent:
#
# rep(0:(ncol(aa)-1)*ncol(bb), each=nrow(aa))
# > [1]  0  0  0 10 10 10 20 20 20 30 30 30 40 40 40

bb <- matrix(1:50, ncol=10, nrow=5, byrow=TRUE)
bv <- as.vector(bb)
ai <- as.vector(aa) + rep(0:4*10, each=3)
bv[ai] <- c(0,1,0)
bb <- matrix(bv, ncol=10, nrow=5, byrow=TRUE)
bb

#which generalizes to:

bb <- matrix(1:50, ncol=10, nrow=5, byrow=TRUE)
bv <- as.vector(bb)
ai <- as.vector(aa) + rep((1:nrow(aa)-1)*10, each=3)
bv[ai] <- c(0,1,0)
bb <- matrix(bv, ncol=10, nrow=5, byrow=TRUE)
bb



-- 
Patricia J. Hawkins
Hawkins Internet Applications
www.hawkinsia.com



From vincent.goulet at act.ulaval.ca  Tue Jan 31 05:30:09 2006
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Mon, 30 Jan 2006 23:30:09 -0500
Subject: [R] Announce: Contributed Documentation
Message-ID: <200601302330.09737.vincent.goulet@act.ulaval.ca>

[Version fran??aise plus bas]

To the R community,

A quick word to announce the publication of my document "Introduction
?? la programmation en S". It is available in the French section of the
Contributed Documentation page of CRAN.

Many of the documents or books currently available on S-Plus and/or R
present the software in a statistical analysis context.  My document
rather focuses on learning the S programming language underlying the
statistical functions. [Not unlike the first three chapters of MASS,
to which I frequently refer.]  Numerical and statistical applications
(currently only optimization, basic regression and basic time series
analysis) are deferred to later chapters.

The text was developed as support for a course where students learn a
second programming language. Therefore, it contains a good number of
example scripts and exercises.

If there is any interest, I might prepare an English version of
the document.

The document is published under the GNU Free Documentation License.

In the hope the document may be useful,

--
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


===== ===== ===== ===== =====

?? la communaut?? francophone de R,

Un petit mot pour annoncer la publication de mon document
??Introduction ?? la programmation en S??. Il est disponible dans la
section ??French?? de la page ??Contributed Documentation?? de CRAN.

Il existe d??j?? de multiples ouvrages traitant de S-Plus ou R.  Dans la
majorit?? des cas, toutefois, ces deux logiciels sont pr??sent??s dans le
cadre d'applications statistiques sp??cifiques. Mon ouvrage se
concentre plut??t sur l'apprentissage du langage de programmation S,
sous-jacent aux diverses fonctions statistiques.

Le texte a ??t?? d??velopp?? comme support d'un cours dans lequel les
??tudiants doivent apprendre un second langage de programmation. On y
trouve donc de nombreux exemples et exercices.

Le document est publi?? sous la GNU Free Documentation License.

En esp??rant que le document s'av??rera utile,

-- 
  Vincent Goulet, Professeur agr??g??
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From spencer.graves at pdf.com  Tue Jan 31 05:36:05 2006
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 Jan 2006 20:36:05 -0800
Subject: [R] Complex Matrix Exponentials.
In-Reply-To: <43DABA7B.2060200@ucalgary.ca>
References: <43DABA7B.2060200@ucalgary.ca>
Message-ID: <43DEE935.8020006@pdf.com>

	  Thank you very much for providing a concrete, simple, reproducible 
example illustrating your question.  Without it, it would be much more 
difficult for me to understand your question and respond appropriately.

	  I know this is generally a very hard problem, and I've seen come 
discussion in the past year about some of the difficulties.  However, I 
just read the documentation, which led me to try the following:

 > MatrixExp(diag(1i,2), method="series")
                      [,1]                 [,2]
[1,] 0.5382509+0.8430744i 0.0000000+0.0000000i
[2,] 0.0000000+0.0000000i 0.5382509+0.8430744i
 > exp(1i)
[1] 0.5403023+0.841471i

	  In general, if A is a diagonal matrix, then exp(A) = 
diag(exp(diag(A))).  More generally, if you can compute the eigenvalues 
and eigenvectors of A, and if there are no repeated roots, then letting 
A = V %*% Lam %*% inv(V), we have exp(A) = V %*% exp(Lam) %*% inv(V). 
However, if A has repeated roots AND Lam is NOT diagonal, then I don't 
know what to do, and I don't know if R has a function that works for 
this case.

	  Clearly, your toy example is covered by my test case.  If this does 
NOT answer the more general question behind this, please let us know.

	  hope this helps.
	  spencer graves

Andrew wrote:

> Hello,
> 
> I was curious if there was a complex valued matrix exponential function 
> available for R?  I have some Laplace transforms  of occupation times 
> for a hidden Markov model.  The matrix exponential function in the msm 
> package does not seem to handle complex values.  For example
> 
>  > MatrixExp(diag(1i,2))
>      [,1] [,2]
> [1,]    1    0
> [2,]    0    1
> Warning message:
> imaginary parts discarded in coercion
> 
> Thanks in advance for your help,
> 
> Andrew Royal
> University of Calgary
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Jan 31 05:55:02 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jan 2006 23:55:02 -0500
Subject: [R] yet another vectorization question
In-Reply-To: <wkmzhdcbju.fsf@connact.com>
References: <200601301334.40440.adi@roda.ro> <200601302009.55504.adi@roda.ro>
	<43DE6CA3.4070208@pburns.seanet.com>
	<200601302209.18419.adi@roda.ro> <wkmzhdcbju.fsf@connact.com>
Message-ID: <971536df0601302055p6cf1e7c1r9c98eefb120ed308@mail.gmail.com>

On 1/30/06, Patricia J. Hawkins <phawkins at connact.com> wrote:
> >>>>> "AD" == Adrian Dusa <adi at roda.ro> writes:
>
> AD> set.seed(5)
> AD> aa <- matrix(sample(10, 15, replace=T), ncol=5)
> AD> bb <- matrix(NA, ncol=10, nrow=5)
> AD> for (i in 1:ncol(aa)) bb[i, aa[, i]] <- c(0, 1, 0)
>
> AD> Is there any possibility to vectorize this "for" loop?
> AD> (sometimes I have hundreds of columns in the "aa" matrix)
>
> Well, coming from ignorance of R, I came up with the below.  However,
> it means creating another vector that's the size of aa, so it's not
> clear that it's a win:
>
> #Problem:  Indexing bb correctly when vectorized
> #Solution:  Add the following matrix to aa:
> #     [,1] [,2] [,3] [,4] [,5]
> #[1,]    0   10   20   30   40
> #[2,]    0   10   20   30   40
> #[3,]    0   10   20   30   40
> #
> # or its vector equivalent:
> #
> # rep(0:(ncol(aa)-1)*ncol(bb), each=nrow(aa))
> # > [1]  0  0  0 10 10 10 20 20 20 30 30 30 40 40 40
>
> bb <- matrix(1:50, ncol=10, nrow=5, byrow=TRUE)
> bv <- as.vector(bb)
> ai <- as.vector(aa) + rep(0:4*10, each=3)
> bv[ai] <- c(0,1,0)
> bb <- matrix(bv, ncol=10, nrow=5, byrow=TRUE)
> bb
>
> #which generalizes to:
>
> bb <- matrix(1:50, ncol=10, nrow=5, byrow=TRUE)
> bv <- as.vector(bb)
> ai <- as.vector(aa) + rep((1:nrow(aa)-1)*10, each=3)
> bv[ai] <- c(0,1,0)
> bb <- matrix(bv, ncol=10, nrow=5, byrow=TRUE)
> bb

Try this:

bb <- matrix(NA, ncol=10, nrow=5)
bb[cbind(c(col(aa)), c(aa))] <- c(0,1,0)



From liuwensui at gmail.com  Tue Jan 31 05:58:44 2006
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 30 Jan 2006 23:58:44 -0500
Subject: [R] Announce: Contributed Documentation
In-Reply-To: <200601302330.09737.vincent.goulet@act.ulaval.ca>
References: <200601302330.09737.vincent.goulet@act.ulaval.ca>
Message-ID: <1115a2b00601302058t552237b9we86789d6deda7bd0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/3b955aec/attachment.pl

From reilly at stat.auckland.ac.nz  Tue Jan 31 07:23:27 2006
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Tue, 31 Jan 2006 19:23:27 +1300
Subject: [R] handling NA by mean replacement
In-Reply-To: <200601301720.k0UHKFhN029749@volta.gene.com>
References: <200601301720.k0UHKFhN029749@volta.gene.com>
Message-ID: <43DF025F.8040702@stat.auckland.ac.nz>


Here are a couple of documents that make much the same point (e.g. "mean
value imputation is not recommended"), and discuss several alternatives.

http://nces.ed.gov/statprog/2002/appendixb3.asp
http://www2.chass.ncsu.edu/garson/pa765/missing.htm

I think we'd need more information on the context to provide any real
advice. Another possible source of help is the Impute mailing list:
http://lists.utsouthwestern.edu/mailman/listinfo/impute

Cheers,
James
-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand

On 31/01/2006 6:20 a.m., Berton Gunter wrote:
> Lots of other folks will give you the simple answer (hint: ?'['  ?is.na)
> 
> Yours is one of those "iceberg" questions  -- 2/3 hidden underwater.
> 
> Two points:
> 
> Point 1: Generally you **don't have to do such replacement** as most of R's
> functions have a na.rm or na.action argument (unfortunately, for historical
> reasons, the argument names and meanings aren't consistent) that does
> basically what you want anyway.
> 
> Point 2: Doing what you ask is probably a bad idea, as it creates mythical
> degrees of freedom and biases results --> gives wrong statistical answers.
> 
> As a general matter, handling missing values "correctly" is a difficult
> statistical issue that you may want to avoid if you can (R has plenty of
> packages that can deal with it, but it requires background expertise).
> Honestly, I'm not sure "if you can" makes any sense here (how do you know?),
> but let's just say that I think your potential for mischief is reduced if
> you use R's inbuilt arguments for ignoring missings rather than imputing
> them naively.
> 
> Having said that, I believe that clustering procedures, for example, may not
> permit this (but they have builtin missing imputation capabilities of their
> own, do they not?), so you may have to impute. In this case, try to do so
> wisely (e.g. via multiple imputation?). 
> 
> Perhaps this will stimulate real experts to offer you some advice. Good
> luck.
> 
> Cheers,
> Bert
>  
> Bert Gunter
> Genentech
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Julie Bernauer
>> Sent: Monday, January 30, 2006 8:50 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] handling NA by mean replacement
>>
>> Hello
>>
>> I am sorry fuch such a stupid question. Suppose I have a 
>> table of data having a
>> lot of NAs and I want to replace those NAs by the mean of the 
>> column before NA
>> replacement. How is it possible to do that efficiently ?
>>
>> Thanks in advance,
>>
>> Julie
>>
>> -- 
>> Julie Bernauer
>> Yeast Structural Genomics
>> http://www.genomics.eu.org
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From comtech.usa at gmail.com  Tue Jan 31 08:01:04 2006
From: comtech.usa at gmail.com (Michael)
Date: Mon, 30 Jan 2006 23:01:04 -0800
Subject: [R] Sos! "Lines" doesn't add plots to an existing plot...
Message-ID: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060130/e615967a/attachment.pl

From FredeA.Togersen at agrsci.dk  Tue Jan 31 08:15:30 2006
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Tue, 31 Jan 2006 08:15:30 +0100
Subject: [R] lattice: combining panel.xyplot with panel.abline - is
	thispossible?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC03726604@DJFPOST01.djf.agrsci.dk>


Well, one way to do it is

xyplot(y~time|id, data=dat,type='l',
       panel=function(x,y,subscripts,...){
         panel.xyplot(x,y,subscripts,...)
         panel.abline(v=dat[subscripts,"cm1"])
         panel.abline(v=dat[subscripts,"cm2"])
  }
)

Since I don't know what the dataframe 'mergeData' is I have used 'dat'.

Best regards

Frede Aakmann T??gersen
Scientist


Danish Institute of Agricultural Sciences
Research Centre Foulum
Dept. of Genetics and Biotechnology
Blichers All?? 20, P.O. BOX 50
DK-8830 Tjele

Phone:   +45 8999 1900
Direct:  +45 8999 1878

E-mail:  FredeA.Togersen at agrsci.dk
Web:	   http://www.agrsci.org				

This email may contain information that is confidential.
Any use or publication of this email without written permission from DIAS is not allowed.
If you are not the intended recipient, please notify DIAS immediately and delete this email.

 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P?? vegne af S??ren H??jsgaard
> Sendt: 31. januar 2006 01:44
> Til: r-help at stat.math.ethz.ch
> Emne: [R] lattice: combining panel.xyplot with panel.abline - 
> is thispossible?
> 
> Consider this data frame:
>  
> dat<-data.frame(id=gl(3,5),time=rep(1:5,3),cm1=rep(c(2,3,4),ea
> ch=5),cm2=rep(c(2.5,3.5,4.5),each=5),y=rnorm(15))
> 
> > dat
>    id time cm1 cm2          y
> 1   1    1   2 2.5 -1.0824549
> 2   1    2   2 2.5 -0.7784834
> 3   1    3   2 2.5 -1.7783560
> 4   1    4   2 2.5  0.5056637
> 5   1    5   2 2.5 -0.1967505
> 6   2    1   3 3.5  1.1643136
> 7   2    2   3 3.5 -1.4323765
> 8   2    3   3 3.5  0.6810645
> 9   2    4   3 3.5  1.1310363
> 10  2    5   3 3.5  1.5677158
> 11  3    1   4 4.5  1.6394222
> 12  3    2   4 4.5  1.2024161
> 13  3    3   4 4.5  0.1784666
> 14  3    4   4 4.5 -1.3260668
> 15  3    5   4 4.5 -0.2711263
> 
> 
> I would like to plot y against time with a panel for each id, 
> i.e. with something like 
> 
> xyplot(y~time|id, data=dat,type='l') or
> 
> xyplot(currCow.mean~currCow.cm|cowid, data=mergeData, type='l',
>   panel=function(x,y,...){
>     panel.xyplot(x,y)
>   }
> )
> 
> 
> Then, I would additionally like to have two vertical lines in 
> each panel, and these are defined by cm1 and cm2. My 
> understanding of xyplot is that id will give a partitioning 
> of data into subsets, so I expected to be able to get each 
> subset into the panelfunction and from there I can extract 
> cm1 and cm2 and call panel.abline(a,b)...
> 
> 1) Is that possible? 
> 2) Are there other ways of achieving what I want?
> 
> Appologies if this is trivial...
> 
> Best regards
> 
> S??ren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jacques.veslot at cirad.fr  Tue Jan 31 08:21:20 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 31 Jan 2006 11:21:20 +0400
Subject: [R] match() & seq()
In-Reply-To: <Pine.LNX.4.44.0601301528200.4578-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0601301528200.4578-100000@reclus.nhh.no>
Message-ID: <43DF0FF0.7090404@cirad.fr>

Thanks Roger, Andy and Dimitris...
though i am familiar with this behaviour **in some cases**, i couldn't 
catch - yesterday evening - why it matched with 0.4, and not with 0.3; 
of course these numbers are not integers ! but i believed match() deals 
with such equalities.
i will have a look at the article mentionned in FAQ 7.31 since 
everything is not clear for me yet .
 > identical(0.4-0.3,0.1)
[1] FALSE
 > all.equal(0.4-0.3,0.1)
[1] TRUE
 
jacques


Roger Bivand a ??crit :

>On Mon, 30 Jan 2006, Jacques VESLOT wrote:
>
>  
>
>>sorry if it has already been discussed but i can't understand this:
>>
>> > seq(0.1,1,by=0.1)
>> [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
>> > match(0.1,seq(0.1,1,by=0.1))
>>[1] 1
>> > match(0.2,seq(0.1,1,by=0.1))
>>[1] 2
>> > match(0.3,seq(0.1,1,by=0.1))
>>[1] NA
>> > match(0.4,seq(0.1,1,by=0.1))
>>[1] 4
>>    
>>
>
>FAQ 7.31
>http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f
>
>  
>
>>print(seq(0.1,1,by=0.1), digits=20)
>>    
>>
> [1] 0.10000000000000000555 0.20000000000000001110 0.30000000000000004441
> [4] 0.40000000000000002220 0.50000000000000000000 0.59999999999999997780
> [7] 0.70000000000000006661 0.80000000000000004441 0.90000000000000002220
>[10] 1.00000000000000000000
>  
>
>>match(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9), seq(0.1,1,by=0.1))
>>    
>>
>[1]  1  2 NA  4  5  6 NA  8  9
>  
>
>>all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0), seq(0.1,1,by=0.1), 
>>    
>>
>+ tolerance = .Machine$double.eps ^ 2)
>[1] "Mean relative  difference: 1.665335e-16"
>  
>
>>all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0), seq(0.1,1,by=0.1))
>>    
>>
>[1] TRUE
>
>
>  
>
>> > R.version
>>         _             
>>platform i386-pc-mingw32
>>arch     i386          
>>os       mingw32       
>>system   i386, mingw32 
>>status                 
>>major    2             
>>minor    2.1           
>>year     2005          
>>month    12            
>>day      20            
>>svn rev  36812         
>>language R
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From dieter.menne at menne-biomed.de  Tue Jan 31 08:27:57 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 31 Jan 2006 07:27:57 +0000 (UTC)
Subject: [R] Sos! "Lines" doesn't add plots to an existing plot...
References: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>
Message-ID: <loom.20060131T082451-527@post.gmane.org>

Michael <comtech.usa <at> gmail.com> writes:

> > plot(testError, col="red")
> > lines(testVar, col="black")
> 
> Only one plot (the red one) appear on the Window, the black line did not
> appear...what's wrong?

We can only guess, because you did not supply the data. But I am quite sure, 
that the lines-data are outside the range set by plot, probable because it 
were "errors", i.e. differences to the original data. Try

plot(testVar, col="red")
lines(testError, col="black")

or use xlim as parameter of plot

Dieter



From b.rowlingson at lancaster.ac.uk  Tue Jan 31 08:28:44 2006
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 31 Jan 2006 07:28:44 +0000
Subject: [R] Sos! "Lines" doesn't add plots to an existing plot...
In-Reply-To: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>
References: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>
Message-ID: <43DF11AC.4080801@lancaster.ac.uk>

Michael wrote:

>>plot(testError, col="red")
>>lines(testVar, col="black")
> 
> 
> Only one plot (the red one) appear on the Window, the black line did not
> appear...what's wrong?

Hang on a second, just let me hack into your machine so I can find out 
what 'testError' and 'testVar' are....

I'm guessing that the lines for 'testVar' are outside the range of the 
plot formed by 'testError', but this is a guess and it could be a 
bazillion other things. We have no way of knowing unless you tell is the 
first things about testError and testVar.

For starters, try telling us what:

  str(testVar)
  str(testError)
  summary(testVar)
  summary(testError)

say.

Barry



From dieter.menne at menne-biomed.de  Tue Jan 31 08:33:30 2006
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 31 Jan 2006 07:33:30 +0000 (UTC)
Subject: [R] Sweave: trouble controlling Design package startup output
References: <13e802630601301537x15f04e4es659a4c93be76dcb0@mail.gmail.com>
Message-ID: <loom.20060131T083227-679@post.gmane.org>

Paul Johnson <pauljohn32 <at> gmail.com> writes:

 
> <<echo=F,quiet=T,print=F>>=
> library(Design, verbose=F)
>  <at> 
> 
> But in the output there is still one page of banner information, which
> starts like this:
> 
> Loading required package: Hmisc
> Hmisc library by Frank E Harrell Jr
> Type library(help= Hmisc ), ?Overview, or ?Hmisc.Overview )
> to see overall documentation.

As an alternative to Frank's suggestion, try

<<init,echo=FALSE, results=hide>>=
library(nlme)
library(lattice)
library(Hmisc)

Dieter



From ripley at stats.ox.ac.uk  Tue Jan 31 08:34:35 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jan 2006 07:34:35 +0000 (GMT)
Subject: [R] Integer bit size and the modulus operator
In-Reply-To: <x264o1qtx9.fsf@turmalin.kubism.ku.dk>
References: <43DE3F90.5050706@stevens.edu>
	<644e1f320601301059h7f33d086w41b649b183367867@mail.gmail.com>
	<x2bqxtqvjo.fsf@turmalin.kubism.ku.dk> <43DE7FEA.8060703@stevens.edu>
	<x264o1qtx9.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0601310725500.4187@gannet.stats>

On Mon, 30 Jan 2006, Peter Dalgaard wrote:

> Ionut Florescu <ifloresc at stevens.edu> writes:
>
>> Actually it does that in my 2.2.1 version as well:
>>
>> > options(digits=20)
>> > 8^(1:20)
>>  [1] 8.0000000000000000e+00 6.4000000000000004e+01 5.1200000000000001e+02
>>  [4] 4.0960000000000001e+03 3.2768000000000002e+04 2.6214400000000002e+05
>>  [7] 2.0971519999999999e+06 1.6777215999999999e+07 1.3421772800000000e+08
>> [10] 1.0737418240000001e+09 8.5899345920000005e+09 6.8719476736000003e+10
>> [13] 5.4975581388799997e+11 4.3980465111039999e+12 3.5184372088832001e+13
>> [16] 2.8147497671065600e+14 2.2517998136852482e+15 1.8014398509481984e+16
>> [19] 1.4411518807585588e+17 1.1529215046068471e+18
>>
>
> Hmm, then it is a platform dependency and my memory playing tricks on
> me... The thing that got fixed was log2(8), perhaps?

It is platform dependent, as it basically uses the system's pow() 
function.  That almost certainly does not use logs.

However, the results above are the result of printing digits that do not 
exist, and reflect the platforms printf() function.  They look like the 
results on Windows, where e.g.

> 8^7 - 2097152
[1] 0
> 8^(1:20) %% 1
  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Warning messages:
1: probable complete loss of accuracy in modulus
2: probable complete loss of accuracy in modulus
3: probable complete loss of accuracy in modulus

despite the discrepancies shown in the printout.

BTW, pow() on Windows was replaced a while back and is the same code as
glibc on ix86 Linux.

So two morals:

1) Do not ignore warnings
2) Distinguish between results and their printed representation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Tue Jan 31 08:38:42 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 31 Jan 2006 08:38:42 +0100
Subject: [R] Subsetting a matrix without for-loop
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6C0C3E4@HERMES.demogr.mpg.de>
Message-ID: <43DF2212.13515.2A377E@localhost>

Hi

> matrix(colSums(embed(A,3)[1:3,]),3,6, byrow=T)[3:1,]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    6   33   60   87  114  141
[2,]    9   36   63   90  117  144
[3,]   12   39   66   93  120  147

will do it. However I am not sure if it is quicker than your for 
loop.

HTH
Petr


On 30 Jan 2006 at 16:29, Camarda, Carlo Giovanni wrote:

Date sent:      	Mon, 30 Jan 2006 16:29:38 +0100
From:           	"Camarda, Carlo Giovanni" <Camarda at demogr.mpg.de>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] Subsetting a matrix without for-loop

> Dear R-users,
> I'm struggling in R in order to "squeeze" a matrix without using a
> for-loop. Although my case is a bit more complex, the following
> example should help you to understand what I would like to do, but
> without the slow for-loop. Thanks in advance, Carlo Giovanni Camarda
> 
> 
> A  <- matrix(1:54, ncol=6)      # my original matrix
> A.new <- matrix(nrow=3, ncol=6) # a new matrix which I'll fill
> # for-loop
> for(i in 1:nrow(A.new)){
>     B <- A[i:(i+2), ]   # selecting the rows
>     C <- apply(B,2,sum) # summing by columns
>     A.new[i,] <- C      # inserting in the new matrix
> }
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 31 08:54:06 2006
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 31 Jan 2006 07:54:06 -0000 (GMT)
Subject: [R] Sos! "Lines" doesn't add plots to an existing plot...
In-Reply-To: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>
Message-ID: <XFMail.060131075406.Ted.Harding@nessie.mcc.ac.uk>

On 31-Jan-06 Michael wrote:
> I am lost:
> 
>> plot(testError, col="red")
>> lines(testVar, col="black")
> 
> Only one plot (the red one) appear on the Window, the black line did
> not
> appear...what's wrong?
> 
> Thanks a lot!

Check the ranges of values of your two variables (in both x and y
directions in general, though x is probably not relevant to your
case since your implicit x-values are the indices of the y-values).

The first "plot" determines the ranges of values which will be
plotted, according to the range of values in the variables in
that plot, and these are held fixed when "lines" is called.

If, therefore, the y-values of "testVar" lie beyond the range
in the frame drawn by "plot", nothing will be plotted inside
that frame.

There are several possible ways of looking at this issue, but
they all come down finally to setting ylim=c(yMin,yMax)
and/or xlim=c(xMin,xMax) as arguments to the first "plot".
They vary in how you arrive at values for xMin, xMax, yMin, yMax.

For example, suppose you have daily temperatures for (in England)
the month of January (days 1-31 of the year) and July (182-212):

xJan<-(1:31)
yJan<-c( 2, 3, 6, 7, 5, 8,10,11, 8, 4,
        -1,-2, 0, 2, 6, 8, 9, 8, 6, 9,
        12,11, 9, 8, 5, 6, 8,10,11,10, 9)

xJul<-(182:212)
yJul<-c(17,20,21,20,19,22,25,26,28,27,
        25,24,22,25,26,23,19,20,19,22,
        20,23,25,24,21,19,22,25,28,26,27)

Now:

  plot(yJan,type="l",col="blue")
  lines(yJul,col="red")

exactly as you found: no red line! The plotted y-range is (-2,12),
and the yJul values lie outside this.

Next, note that the max value of yJul is 28, so:

  plot(yJan,type="l",col="blue",ylim=c(-2,28))
  lines(yJul,col="red")

so now you have the Jan and Jul temperatures for days 1:31 of
the month in each case.

But now:

  plot(xJan,yJan,type="l",col="blue",ylim=c(-2,28))
  lines(xJul,yJul,col="red")

and again the July plot is off the graph, because the x limits
were still set by the January x-values, and the July x-values
lie beyond this range. So, finally, noting that the x-values
range from 1 to 212 overall:

  plot(xJan,yJan,type="l",col="blue",xlim=c(1,212),ylim=c(-2,28))
  lines(xJul,yJul,col="red")

and now you have everything.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Jan-06                                       Time: 07:54:01
------------------------------ XFMail ------------------------------



From petr.pikal at precheza.cz  Tue Jan 31 08:59:03 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 31 Jan 2006 08:59:03 +0100
Subject: [R] Sos! "Lines" doesn't add plots to an existing plot...
In-Reply-To: <b1f16d9d0601302301j202d0cb6y9361ecc237568ec2@mail.gmail.com>
Message-ID: <43DF26D7.4205.3CCD3F@localhost>

Hi

most probably testError has different range then testVar, but only 
you know if it is true.

What says

str(testError) and str(testVar)

HTH
Petr


On 30 Jan 2006 at 23:01, Michael wrote:

Date sent:      	Mon, 30 Jan 2006 23:01:04 -0800
From:           	Michael <comtech.usa at gmail.com>
To:             	R-help at stat.math.ethz.ch
Subject:        	[R] Sos! "Lines" doesn't add plots to an existing plot...

> I am lost:
> 
> > plot(testError, col="red")
> > lines(testVar, col="black")
> 
> Only one plot (the red one) appear on the Window, the black line did
> not appear...what's wrong?
> 
> Thanks a lot!
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From renaud.lancelot at gmail.com  Tue Jan 31 09:02:26 2006
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Tue, 31 Jan 2006 09:02:26 +0100
Subject: [R] Logistic regression model selection with
	overdispersed/autocorrelated data
In-Reply-To: <OF6C249F80.0076B321-ON87257106.0059AE18@pc.gc.ca>
References: <OF6C249F80.0076B321-ON87257106.0059AE18@pc.gc.ca>
Message-ID: <c2ee56800601310002h9d019d6i@mail.gmail.com>

If you're not interested in fitting caribou-specific responses, you
can use beta-binomial logistic models. There are several package
available for this purpose on CRAN, among which aod. Because these
models are fitted using maximum-likelihood methods, you can use AIC
(or other information criteria) to compare different models.

Best,

Renaud

2006/1/30, Jesse.Whittington at pc.gc.ca <Jesse.Whittington at pc.gc.ca>:
>
>
> I am creating habitat selection models for caribou and other species with
> data collected from GPS collars.  In my current situation the radio-collars
> recorded the locations of 30 caribou every 6 hours.  I am then comparing
> resources used at caribou locations to random locations using logistic
> regression (standard habitat analysis).
>
> The data is therefore highly autocorrelated and this causes Type I error
> two ways  small standard errors around beta-coefficients and
> over-paramaterization during model selection.  Robust standard errors are
> easily calculated by block-bootstrapping the data using "animal" as a
> cluster with the Design library, however I haven't found a satisfactory
> solution for model selection.
>
> A couple options are:
> 1.  Using QAIC where the deviance is divided by a variance inflation factor
> (Burnham & Anderson).  However, this VIF can vary greatly depending on the
> data set and the set of covariates used in the global model.
> 2.  Manual forward stepwise regression using both changes in deviance and
> robust p-values for the beta-coefficients.
>
> I have been looking for a solution to this problem for a couple years and
> would appreciate any advice.
>
> Jesse
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


--
Renaud LANCELOT
Dpartement Elevage et Mdecine Vtrinaire (EMVT) du CIRAD
Directeur adjoint charg des affaires scientifiques

CIRAD, Animal Production and Veterinary Medicine Department
Deputy director for scientific affairs

Campus international de Baillarguet
TA 30 / B (Bt. B, Bur. 214)
34398 Montpellier Cedex 5 - France
Tl   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 39 04
Fax   +33 (0)4 67 59 37 95



From bhs2 at mevik.net  Tue Jan 31 09:07:33 2006
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 31 Jan 2006 09:07:33 +0100
Subject: [R] 'all' inconsistent?
In-Reply-To: <m24q3ldb7a.fsf@ziti.local> (Seth Falcon's message of "Mon, 30
	Jan 2006 06:47:37 -0800")
References: <6.1.2.0.2.20060129213316.03af9200@epurdom.pobox.stanford.edu>
	<Pine.LNX.4.61.0601300739520.8419@gannet.stats>
	<m24q3ldb7a.fsf@ziti.local>
Message-ID: <m0hd7kyg56.fsf@bar.nemo-project.org>

Seth Falcon wrote:

> On 29 Jan 2006, ripley at stats.ox.ac.uk wrote:
>
>> On Sun, 29 Jan 2006, Elizabeth Purdom wrote:
>>
>>> I came across the following behavior, which seems illogical to me.
>>
>> What did you expect and why?
>>
>>> I don't know if it is a bug or if I'm missing something:
>>>
>>>> all(logical(0))
>>> [1] TRUE
>>
>> All the values are true, all none of them.
>
> I thought all the values are false, all none of them, because there
> aren't any that are true:
>
> any(logical(0))
> [1] FALSE

But they are, all none of them:

> all(!logical(0))
[1] TRUE

:-)

And there aren't any FALSE values either:

> any(!logical(0))
[1] FALSE

so it is only logical that all none of them are TRUE.  I love the
empty set! :-)

-- 
Bj??rn-Helge Mevik



From petr.pikal at precheza.cz  Tue Jan 31 09:09:13 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 31 Jan 2006 09:09:13 +0100
Subject: [R] match() & seq()
In-Reply-To: <43DF0FF0.7090404@cirad.fr>
References: <Pine.LNX.4.44.0601301528200.4578-100000@reclus.nhh.no>
Message-ID: <43DF2939.11131.461B24@localhost>

Hi

?identical

The safe and reliable way to test two objects for being _exactly_
															^^^^^^^^^
     equal.  It returns 'TRUE' in this case, 'FALSE' in every other
     case.

?all.equal

     'all.equal(x,y)' is a utility to compare R objects 'x' and 'y'
     testing "near equality".  If they are different, comparison is
			^^^^^^^^^^^

     still made to some extent, and a report of the differences is
 
HTH
Petr





On 31 Jan 2006 at 11:21, Jacques VESLOT wrote:

Date sent:      	Tue, 31 Jan 2006 11:21:20 +0400
From:           	Jacques VESLOT <jacques.veslot at cirad.fr>
To:             	Roger.Bivand at nhh.no
Copies to:      	R-help at stat.math.ethz.ch
Subject:        	Re: [R] match() & seq()

> Thanks Roger, Andy and Dimitris...
> though i am familiar with this behaviour **in some cases**, i couldn't
> catch - yesterday evening - why it matched with 0.4, and not with 0.3;
> of course these numbers are not integers ! but i believed match()
> deals with such equalities. i will have a look at the article
> mentionned in FAQ 7.31 since everything is not clear for me yet .
>  > identical(0.4-0.3,0.1)
> [1] FALSE
>  > all.equal(0.4-0.3,0.1)
> [1] TRUE
> 
> jacques
> 
> 
> Roger Bivand a ??crit :
> 
> >On Mon, 30 Jan 2006, Jacques VESLOT wrote:
> >
> >  
> >
> >>sorry if it has already been discussed but i can't understand this:
> >>
> >> > seq(0.1,1,by=0.1)
> >> [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
> >> > match(0.1,seq(0.1,1,by=0.1))
> >>[1] 1
> >> > match(0.2,seq(0.1,1,by=0.1))
> >>[1] 2
> >> > match(0.3,seq(0.1,1,by=0.1))
> >>[1] NA
> >> > match(0.4,seq(0.1,1,by=0.1))
> >>[1] 4
> >>    
> >>
> >
> >FAQ 7.31
> >http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-
> >these-numbers-are-equal_003f
> >
> >  
> >
> >>print(seq(0.1,1,by=0.1), digits=20)
> >>    
> >>
> > [1] 0.10000000000000000555 0.20000000000000001110
> > 0.30000000000000004441 [4] 0.40000000000000002220
> > 0.50000000000000000000 0.59999999999999997780 [7]
> > 0.70000000000000006661 0.80000000000000004441 0.90000000000000002220
> >[10] 1.00000000000000000000
> >  
> >
> >>match(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9), seq(0.1,1,by=0.1))
> >>    
> >>
> >[1]  1  2 NA  4  5  6 NA  8  9
> >  
> >
> >>all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0),
> >>seq(0.1,1,by=0.1), 
> >>    
> >>
> >+ tolerance = .Machine$double.eps ^ 2)
> >[1] "Mean relative  difference: 1.665335e-16"
> >  
> >
> >>all.equal(c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0),
> >>seq(0.1,1,by=0.1))
> >>    
> >>
> >[1] TRUE
> >
> >
> >  
> >
> >> > R.version
> >>         _             
> >>platform i386-pc-mingw32
> >>arch     i386          
> >>os       mingw32       
> >>system   i386, mingw32 
> >>status                 
> >>major    2             
> >>minor    2.1           
> >>year     2005          
> >>month    12            
> >>day      20            
> >>svn rev  36812         
> >>language R
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Jan 31 09:10:08 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jan 2006 08:10:08 +0000 (GMT)
Subject: [R] Glossay of available R functions
In-Reply-To: <wkvew1e16o.fsf@connact.com>
References: <200601302031.k0UKVGob001618@hypatia.math.ethz.ch>
	<43DE78E7.5040209@optonline.net>
	<200601301900.50123.asaguiar@spsconsultoria.com>
	<wkvew1e16o.fsf@connact.com>
Message-ID: <Pine.LNX.4.61.0601310801310.4916@gannet.stats>

On Mon, 30 Jan 2006, Patricia J. Hawkins wrote:

>>>>>> "ASA" == Alexandre Santos Aguiar <asaguiar at spsconsultoria.com> writes:
>
> ASA> I am new to R and read this list to learn. It is amazing how
> ASA> frequently new functions pop in messages. Useful and timesaving
> ASA> functions like subset (above) must be documented somewhere.
>
> ASA> Is there a glossary of functions?
>
> I'm also new to R, and was wondering the same thing.  Took a bunch of
> tries, but if you run start.help() and then choose Packages, then
> Base, you will get the list of functions.

You get a list of objects (not just functions) in the base package.  You 
can also get a list by library(help=base).

However, that is far from all the functions available in base R.
As a quick check

as.matrix(sapply(search(), function(x) length(ls(x, all=TRUE))))

.GlobalEnv           0
package:methods    299
package:stats      497
package:graphics    79
package:grDevices   78
package:utils      152
package:datasets   103
Autoloads            1
package:base      1090

so it is less than half the objects loaded and visible in a default 
session.  And there are another 18 packages shipped with R.

Looking at a list of 2300 objects is daunting, and so we provide 
search facilities (including via the HTML pages).

> As a newcomer, I hesitate to suggest this, but maybe there should be a
> comment on the index page to that effect?

Which index page?  If you mean that given by help.start(), it is not a 
common request, and search is linked from there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sell_mirage_ne at hotmail.com  Tue Jan 31 09:41:30 2006
From: sell_mirage_ne at hotmail.com (Taka Matzmoto)
Date: Tue, 31 Jan 2006 02:41:30 -0600
Subject: [R] warnings in glm (logistic regression)
Message-ID: <BAY110-F711F1633978C8A8DD3FFEC7080@phx.gbl>

Hello R users

I ran more than 100 logistic regression analyses. Some of the analyses gave 
me this kind warning below.

###########################################################
Warning messages:
1: algorithm did not converge in: glm.fit(x = X, y = Y, weights = weights, 
start = start, etastart = etastart,   ...
2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y = 
Y, weights = weights, start = start, etastart = etastart,   ...
###########################################################

For those cases for which I got the warning messages, shouldn't I rely on 
coefficents ? It looks like I can still extract coefficients from R outputs

Are there any ways to avoid these warning messages ? or are these due to the 
problems with my data (e.g., perfect separation)

Any help or advice would be appreciated

Thank you

TM



From plummer at iarc.fr  Tue Jan 31 09:53:42 2006
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 31 Jan 2006 09:53:42 +0100
Subject: [R] Timeliness of precompiled binaries--R-2.2.1
	still	not	available as RPM
In-Reply-To: <x2fyn5r4a8.fsf@turmalin.kubism.ku.dk>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A036DD32F@pnlmse35.pnl.gov>
	<1138642623.4592.43.camel@gsimpson.geog.ucl.ac.uk>
	<x2fyn5r4a8.fsf@turmalin.kubism.ku.dk>
Message-ID: <1138697622.2875.7.camel@seurat.iarc.fr>

On Mon, 2006-01-30 at 18:53 +0100, Peter Dalgaard wrote:
> Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:
> 
> > On Mon, 2006-01-30 at 08:38 -0800, Waichler, Scott R wrote:
> > > R-2.2.1 is still not available for Redhat Linux as an RPM on CRAN.  It
> > > is available as an SRPM.  Can someone fill me in on why it takes so long
> > > to make RPMs available?  I would be happy to help make the RPMs for el4
> > > and el3 if such help is needed.
> > 
> > Because someone has to volunteer their time to create such things -
> > these are not automatically built by R-Core members. Martyn Plummer has
> > done this for as long as I've been using R...
> > 
> > ...Mores the point, though, which CRAN mirror were you looking at? R
> > 2.2.1 was in all the EL3/4 and FC3/4 i386 and x86_64 directories I could
> > be bothered to look at on the UK Bristol mirror for example.
> 
> Not in EL4 on CRAN master, as far as I can see...
> 
> It's not only volunteer time that matters. They also need access to a
> machine that runs the OS version in question, which I suspect could be
> the issue here.

What Peter said.  I do have a volunteer to create RHEL4 binaries but,
they needed to install RHEL4 on a new machine first. I sent a reminder
and I hope we shall have RHEL4 binaries on CRAN soon.

Martyn 



-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From bgytcc at leeds.ac.uk  Tue Jan 31 10:19:47 2006
From: bgytcc at leeds.ac.uk (Tom C Cameron)
Date: Tue, 31 Jan 2006 09:19:47 +0000
Subject: [R] How do I "normalise" a power spectral density analysis?
In-Reply-To: <43DEC156.2090900@pdf.com>
References: <1138373637.4afbee09782cc@webmail1.leeds.ac.uk>
	<43DEC156.2090900@pdf.com>
Message-ID: <1138699187.e3ff9501a3486@webmail1.leeds.ac.uk>

Hi Spencer, yes thanks it does help. In short I have resolved the issue.

In length,  not directly, as I have not found a way to ask R to actually plot
frequency vs. period directly, I have scoured the R help pages and Venebles &
Ripley but the call to R "spectrum()" or "periodogram()"  generates error
messages if I try to enhance or change the plot!

I did find that you can install the library packages "growth" and "rmutil" and
then use the function "pergram" that gives you the output of the PSD in a
matrix which I have then transformed to give me the period (1/frequency).

Thankyou very much for replying and for the advice

Best wishes

Tom
...........................................................................
Dr Tom C Cameron                        office: 0113 34 32837 (10.23 Miall)
Ecology & Evolution Res. Group.         lab: 0113 34 32884 (10.20 Miall)
School of Biological Sciences           Mobile: 07966160266
University of Leeds                     email: t.c.cameron at leeds.ac.uk
Leeds LS2 9JT
LS2 9JT



From mobygeek at yahoo.com  Tue Jan 31 10:30:51 2006
From: mobygeek at yahoo.com (context grey)
Date: Tue, 31 Jan 2006 01:30:51 -0800 (PST)
Subject: [R] classifier for histograms?
Message-ID: <20060131093051.44397.qmail@web51408.mail.yahoo.com>

Hi,

Apology for this question being off the topic (OT) of
R, though I expect 
this list might be the best place on the net to ask
this question.


In brief, the question is:  what classification
algorithm 
can one use if the features are  histograms?


I have a classification problem, and believe that
histograms
of the distribution of some values may be the best
"feature" to use.
To make the mail shorter, here's a simpler example
problem:

   Try to classify a person as e.g. drunk or not given
the histogram 
   of their driving speed.

In the training phase, we have a table whose rows
contain the driver, 
whether they are drunk, and a sample of driving speed.
 
>From this one can build separate histograms of driving
speed 
for drunk/non drunk.  
  (In my actual application, I have several such
histogram features, and they
are visibly different; they are also ranked now by
some analytic 
pdf-distance measures such as KL).

Now, how to classify... 

given a single speed, its probability can be evaluated
under the two classes,
but a single speed sample is not going to be reliable
in this problem.
Suppose instead that the _distribution_ of speeds is
sufficient 
to discriminate.  

We have a driver, and a distribution of their speeds
over time.  A histogram
can be built.    What to do with this histogram?...
Is there a standard classifier that can deal with this
situation?  

My thought(s):  
- the test histogram could be compared to each
of the training histograms with the Chi^2 measure - 
sum of squared Gaussian deviations, then get a
probability from this?
-  Alternately, consider training histograms with n
bins as points 
in N-dimensional space, use euclidean closeness in
this space.
This may not generalize to more than one such
histogram feature though....

Thanks for any thoughts.

(Also thanks for the replies to my recent question 
about hashtable/dictionary.)



From frymor at gmail.com  Tue Jan 31 10:34:49 2006
From: frymor at gmail.com (Assa Yeroslaviz)
Date: Tue, 31 Jan 2006 10:34:49 +0100
Subject: [R] changing the default repositories
Message-ID: <9086c4f40601310134p5de82c50r@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060131/f8296d04/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Jan 31 10:39:33 2006
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Jan 2006 10:39:33 +0100
Subject: [R] lme in R (WinXP) vs. Splus (HP UNIX)
In-Reply-To: <20060131013015.63957.qmail@web37110.mail.mud.yahoo.com>
References: <20060131013015.63957.qmail@web37110.mail.mud.yahoo.com>
Message-ID: <x264o020tm.fsf@viggo.kubism.ku.dk>

Greg Tarpinian <sasprog474474 at yahoo.com> writes:

> R2.2 for WinXP, Splus 6.2.1 for HP 9000 Series, HP-UX 11.0.
> 
> 
> I am trying to get a handle on why the same lme( ) code gives
> such different answers.  My output makes me wonder if the 
> fact that the UNIX box is 64 bits is the reason.  The estimated
> random effects are identical, but the fixed effects are very
> different.  Here is my R code and output, with some columns 
> and rows deleted for space considerations:
> 
> FOO.lme1 <- lme(fixed = Y ~ X1*X2, 
> random = ~ X2 | X3, data = FOO,
> method = "REML",
> control = list(maxIter = 200, msMaxIter = 200, 
> tolerance = 1e-8, niterEM = 200,
> msTol = 1e-8, msVerbose = TRUE, 
> optimMethod = "Nelder-Mead"))


Try

FOO.lme2 <- update(FOO.lme1,.~C(X1,"contr.helmert")*X2)
summary(FOO.lme2)

and see if enlightenment does not ensue.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Eric.Kort at vai.org  Tue Jan 31 03:18:57 2006
From: Eric.Kort at vai.org (Kort, Eric)
Date: Mon, 30 Jan 2006 21:18:57 -0500
Subject: [R] Image Processing packages
References: <005201c62357$55f62740$6600a8c0@DD4XFW31><000c01c62366$5d2eeab0$0401a8c0@mouse>
	<df01a8eb0601301548o32f56475g@mail.gmail.com>
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F6972D5@VAIEXCH04.vai.org>


Nice.  May I incorporate these into the collection I am packaging up for redistribution?

-Eric

-----Original Message-----
From: Vincent Zoonekynd [mailto:zoonek at gmail.com]
Sent: Mon 1/30/2006 6:48 PM
To: R-help at stat.math.ethz.ch
Cc: Charles.Annis at statisticalengineering.com; Kort, Eric; Stephan.Matthiesen at ed.ac.uk; Thomas Kaliwe
Subject: Re: [R] Image Processing packages
 
A couple of years ago, I was using R as a first step before
implementing image analysis algorithms on a portable biochip
reader. Here is the code I had written at the time, should
someone find it useful (it mainly contains "morphological
operations"):
  http://zoonek2.free.fr/UNIX/48_R/morphology.R

There are a few examples (commented in French) at the end of
  http://zoonek.free.fr/Ecrits/2004_BioRet.pdf.bz2

The code of those examples:
  http://zoonek2.free.fr/UNIX/48_R/Rapport_final_morphologie.Rnw

-- Vincent

This email message, including any attachments, is for the so...{{dropped}}



From herrdittmann at yahoo.co.uk  Tue Jan 31 00:03:16 2006
From: herrdittmann at yahoo.co.uk (B Dittmann)
Date: Tue, 31 Jan 2006 00:03:16 +0100
Subject: [R] Kolmogorov-Smirnov Test on R
Message-ID: <43DE9B34.7090005@yahoo.co.uk>

Hi,

just run the Kolmogorov-Smirnov test on R.
Is there any detailed documentation available for the options for the KS 
test, esp. with regard to the hypotheses.
The help file is rather "thin".

Many thanks.

Bernd



From Patrick.Kuss at unibas.ch  Tue Jan 31 11:10:02 2006
From: Patrick.Kuss at unibas.ch (Patrick Kuss)
Date: Tue, 31 Jan 2006 11:10:02 +0100
Subject: [R] lattice histogram finetuning
Message-ID: <1138702202.43df377a90fc4@webmail.unibas.ch>

Dear list,

I have some difficulties fine-tuning a lattice conditional histogram plot and
found little help in the documentation.
My dataset consists of plant flowering ages from 3 different altitudes and 3
successional sites at each altitudinal level.

My questions are:

How do I avoid that lattice automatically sorts the different $condlevels
alphabetically? I need:
hist$condlevels$succession <- list("early","mid","late")
 # not "early","late","mid"
hist$condlevels$site <- list("FU","SP","JU")

Additionally, how do I add a vertical line indicating the mean flowering age at
each panel?

And how do I assure breaks to happen for each year?
hist$panel.args.common$breaks


Happy for any suggestions

Patrick


# Flowering age at different altitudes and successional site

FU.e <- round(rnorm(20,mean=9,sd=1),dig=0)
FU.m <- round(rnorm(20,mean=12,sd=1),dig=0)
FU.l <- round(rnorm(20,mean=15,sd=1),dig=0)
SP.e <- round(rnorm(20,mean=8,sd=1),dig=0)
SP.m <- round(rnorm(20,mean=11,sd=1),dig=0)
SP.l <- round(rnorm(20,mean=14,sd=1),dig=0)
JU.e <- round(rnorm(20,mean=7,sd=1),dig=0)
JU.m <- round(rnorm(20,mean=10,sd=1),dig=0)
JU.l <- round(rnorm(20,mean=13,sd=1),dig=0)

age <- c(FU.e,FU.m,FU.l,SP.e,SP.m,SP.l,JU.e,JU.m,JU.l)
site <- rep(c("FU","SP","JU"),each=60)
succession <- rep(c("early","mid","late"),each=20,times=3)

CT.flow <- data.frame(site,succession,age)

library(lattice)
trellis.device(color=F)

hist <- histogram(~age|succession*site,data=CT.flow)
hist



--
Patrick Kuss
PhD-student
Institute of Botany
University of Basel
Sch??nbeinstr. 6
CH-4056 Basel
+41 61 267 2976



From ripley at stats.ox.ac.uk  Tue Jan 31 11:18:43 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jan 2006 10:18:43 +0000 (GMT)
Subject: [R] How do I "normalise" a power spectral density analysis?
In-Reply-To: <1138699187.e3ff9501a3486@webmail1.leeds.ac.uk>
References: <1138373637.4afbee09782cc@webmail1.leeds.ac.uk>
	<43DEC156.2090900@pdf.com>
	<1138699187.e3ff9501a3486@webmail1.leeds.ac.uk>
Message-ID: <Pine.LNX.4.61.0601311012260.17141@gannet.stats>

What's wrong with

z <- spec.pgram(ldeaths, plot = FALSE)
plot(1/z$freq, z$spec, log="y", xlab="period", ylab="power")

?


On Tue, 31 Jan 2006, Tom C Cameron wrote:

> Hi Spencer, yes thanks it does help. In short I have resolved the issue.
>
> In length, not directly, as I have not found a way to ask R to actually 
> plot frequency vs. period directly,

That's the 1/x function, so I am sure you can manage that.  I presume you 
want power vs period?

> I have scoured the R help pages and Venebles &
> Ripley but the call to R "spectrum()" or "periodogram()"  generates error
> messages if I try to enhance or change the plot!

There is no periodogram() function in R.


> I did find that you can install the library packages "growth" and "rmutil" and
> then use the function "pergram" that gives you the output of the PSD in a
> matrix which I have then transformed to give me the period (1/frequency).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From afiwa.kuzeawu at dialego.de  Tue Jan 31 11:32:43 2006
From: afiwa.kuzeawu at dialego.de (afiwa.kuzeawu@dialego.de)
Date: Tue, 31 Jan 2006 11:32:43 +0100
Subject: [R] TURF (total unreplicated reach and frequency) Analysis program
Message-ID: <OF6BA2FBF4.76AFC324-ONC1257107.0039C938@dialego.de>

Hi,

I am looking for information how to compute TURF-Analysis and with which
softwares this can be done.

Can you please help me?

Thank you in advance.

Best regards

Afiwa Kuzeawu
Project Manager

____________________________

76 Prozent der Verbraucher suchen bei Textilien
und Schuhen gezielt nach Sonderangeboten.
zur Studie


Dialego AG
Market Research Online
Friedrichstr. 69-71
52070 Aachen, Germany
+ 49-(0)241-97828-133 PHONE
+ 49-(0)241-97828-118 FAX
afiwa.kuzeawu at dialego.de
www.dialego.de


Dialego Inc., NY, USA
www.dialego.com



From vmuggeo at dssm.unipa.it  Tue Jan 31 11:47:57 2006
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Tue, 31 Jan 2006 11:47:57 +0100
Subject: [R] warnings in glm (logistic regression)
In-Reply-To: <BAY110-F711F1633978C8A8DD3FFEC7080@phx.gbl>
References: <BAY110-F711F1633978C8A8DD3FFEC7080@phx.gbl>
Message-ID: <43DF405D.3030202@dssm.unipa.it>

Hi,
very likely your data exhibit quasi-separation which cause (log)Lik to 
be monotone and thus ML estimate do not exist. However you can rely on 
point estimate and use LRT to test for its significance.
Or Better: have a look to brlr or logistf packages which bypass the 
monotone-likelihood problem by using penalized likelihood.

Best,
vito



Taka Matzmoto wrote:
> Hello R users
> 
> I ran more than 100 logistic regression analyses. Some of the analyses gave 
> me this kind warning below.
> 
> ###########################################################
> Warning messages:
> 1: algorithm did not converge in: glm.fit(x = X, y = Y, weights = weights, 
> start = start, etastart = etastart,   ...
> 2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y = 
> Y, weights = weights, start = start, etastart = etastart,   ...
> ###########################################################
> 
> For those cases for which I got the warning messages, shouldn't I rely on 
> coefficents ? It looks like I can still extract coefficients from R outputs
> 
> Are there any ways to avoid these warning messages ? or are these due to the 
> problems with my data (e.g., perfect separation)
> 
> Any help or advice would be appreciated
> 
> Thank you
> 
> TM
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit?? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From jacques.veslot at cirad.fr  Tue Jan 31 11:51:53 2006
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Tue, 31 Jan 2006 14:51:53 +0400
Subject: [R] lattice histogram finetuning
In-Reply-To: <1138702202.43df377a90fc4@webmail.unibas.ch>
References: <1138702202.43df377a90fc4@webmail.unibas.ch>
Message-ID: <43DF4149.1070303@cirad.fr>

it looks like a barchart rather than an histogram (?).
if it is, you can do something like that:

CT.flow$age <- factor(CT.flow$age)
CT.flow$succession <- 
factor(CT.flow$succession,levels=c("early","mid","late"))

ct <- expand.grid(  succession=levels(CT.flow$succession),
                    site=levels(CT.flow$site),
                    age=levels(CT.flow$age)

ct$eff <- as.vector(ftable(xtabs(~.,CT.flow)))

barchart(eff~age|site*succession,horiz=F,ct)



Patrick Kuss a ??crit :

>Dear list,
>
>I have some difficulties fine-tuning a lattice conditional histogram plot and
>found little help in the documentation.
>My dataset consists of plant flowering ages from 3 different altitudes and 3
>successional sites at each altitudinal level.
>
>My questions are:
>
>How do I avoid that lattice automatically sorts the different $condlevels
>alphabetically? I need:
>hist$condlevels$succession <- list("early","mid","late")
> # not "early","late","mid"
>hist$condlevels$site <- list("FU","SP","JU")
>
>Additionally, how do I add a vertical line indicating the mean flowering age at
>each panel?
>
>And how do I assure breaks to happen for each year?
>hist$panel.args.common$breaks
>
>
>Happy for any suggestions
>
>Patrick
>
>
># Flowering age at different altitudes and successional site
>
>FU.e <- round(rnorm(20,mean=9,sd=1),dig=0)
>FU.m <- round(rnorm(20,mean=12,sd=1),dig=0)
>FU.l <- round(rnorm(20,mean=15,sd=1),dig=0)
>SP.e <- round(rnorm(20,mean=8,sd=1),dig=0)
>SP.m <- round(rnorm(20,mean=11,sd=1),dig=0)
>SP.l <- round(rnorm(20,mean=14,sd=1),dig=0)
>JU.e <- round(rnorm(20,mean=7,sd=1),dig=0)
>JU.m <- round(rnorm(20,mean=10,sd=1),dig=0)
>JU.l <- round(rnorm(20,mean=13,sd=1),dig=0)
>
>age <- c(FU.e,FU.m,FU.l,SP.e,SP.m,SP.l,JU.e,JU.m,JU.l)
>site <- rep(c("FU","SP","JU"),each=60)
>succession <- rep(c("early","mid","late"),each=20,times=3)
>
>CT.flow <- data.frame(site,succession,age)
>
>library(lattice)
>trellis.device(color=F)
>
>hist <- histogram(~age|succession*site,data=CT.flow)
>hist
>
>
>
>--
>Patrick Kuss
>PhD-student
>Institute of Botany
>University of Basel
>Sch??nbeinstr. 6
>CH-4056 Basel
>+41 61 267 2976
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ligges at statistik.uni-dortmund.de  Tue Jan 31 12:03:24 2006
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jan 2006 12:03:24 +0100
Subject: [R] changing the default repositories
In-Reply-To: <9086c4f40601310134p5de82c50r@mail.gmail.com>
References: <9086c4f40601310134p5de82c50r@mail.gmail.com>
Message-ID: <43DF43FC.2040709@statistik.uni-dortmund.de>

Assa Yeroslaviz wrote:

> Hello,
> 
> Is it possible to change the default repositories?
> I've already changed the list in the repositories file under etc/
> this is how my repositories file under R/etc looks like:
> row name    menu_name    URL    default    source    win.binary
> mac.binary
> CRAN    CRAN    @CRAN@    TRUE    TRUE    TRUE    TRUE
> BioC    Bioconductor    http://www.bioconductor.org    TRUE    TRUE
> TRUE    FALSE
> CRANextra    CRAN (extras)    http://www.stats.ox.ac.uk/pub/RWin    TRUE
> FALSE    TRUE    FALSE
> Omegahat    Omegahat    http://www.omegahat.org/R    TRUE    TRUE    TRUE
> FALSE
> BioCcdf    BIOCcdf
> http://www.bioconductor.org/packages/data/annotation/1.7    TRUE    TRUE
> TRUE    FALSE
> BioCCourses    BioCcourses    http://www.bioconductor.org/repository/Courses
> TRUE    TRUE    TRUE    FALSE
> 
> every time i start R it shows me only the first two lines as default instead
> of all of them. Is there a possibility to change it in the Rprofile.site or
> in the repositories files, so R will always use the complete list as a
> default?

Yes, you are right, you can simply specify it in Rprofile.site as in:


options(repos = c(CRAN = "http://umfragen.sowi.uni-mainz.de/CRAN",
                   CRANextra = "http://www.stats.ox.ac.uk/pub/RWin"))

Uwe Ligges


> 
> Thx for any help.
> 
> Assa
> --
> Assa Yeroslaviz
> Loetzener Str. 15
> 51373 Leverkusen
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Tue Jan 31 12:04:39 2006
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 31 Jan 2006 12:04:39 +0100
Subject: [R] Type II SS for fixed-effect in mixed model
In-Reply-To: <000001c625e3$3bc97460$5044d084@TAMIAS>
References: <mailman.4.1138618802.26022.r-help@stat.math.ethz.ch>
	<000001c625e3$3bc97460$5044d084@TAMIAS>
Message-ID: <17375.17479.203285.839898@stat.math.ethz.ch>

Dear Julien

Have a look at the "type" argument in ?anova.lme

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Martin Julien writes:
 > Hi
 > In mixed-model with lme()
 > How can I obtain Type II SS or Type III SS for fixed effect?
 > Thanks 
 > Julien
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pburns at pburns.seanet.com  Tue Jan 31 12:28:43 2006
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 31 Jan 2006 11:28:43 +0000
Subject: [R] R relative to statistical packages
Message-ID: <43DF49EB.4040609@pburns.seanet.com>

The comment relating R to Stata, SAS and SPSS is now
on the Burns Statistics website at

http://www.burns-stat.com/pages/Tutor/R_relative_statpack.pdf

And also on the UCLA website where the original technical
report can be found as well.

http://www.ats.ucla.edu/stat/technicalreports/

Thank you to all who contributed comments.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From gescati at yahoo.com.ar  Tue Jan 31 13:15:31 2006
From: gescati at yahoo.com.ar (=?iso-8859-1?q?gabriela=20escati=20pe=F1aloza?=)
Date: Tue, 31 Jan 2006 12:15:31 +0000 (GMT)
Subject: [R] how calculation degrees freedom
In-Reply-To: <40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
Message-ID: <20060131121532.97055.qmail@web37103.mail.mud.yahoo.com>

Dear Dr. Bates,
Thank you very much for your response. 

 --- Douglas Bates <dmbates at gmail.com> escribi??:

> On 1/27/06, gabriela escati pe??aloza
> <gescati at yahoo.com.ar> wrote:
> > Hi, I' m having a hard time understanding the
> computation of degrees of freedom
> 
> So do I and I'm one of the authors of the package
> :-)
> 
> > when runing nlme() on the following model:
> >
> >   > formula(my data.gd)
> > dLt ~ Lt | ID
> >
> >   TasavB<- function(Lt, Linf, K) (K*(Linf-Lt))
> >
> >   my model.nlme <- nlme (dLt ~ TasavB(Lt, Linf,
> K),
> >   data = my data.gd,
> >   fixed = list(Linf ~ 1, K ~ 1),
> >   start = list(fixed = c(70, 0.4)),
> >   na.action= na.include, naPattern = ~!is.na(dLt))
> >
> >   > summary(my model.nlme)
> > Nonlinear mixed-effects model fit by maximum
> likelihood
> >   Model: dLt ~ TasavB(Lt, Linf, K)
> >  Data: my data.gd
> >        AIC      BIC    logLik
> >   13015.63 13051.57 -6501.814
> >   Random effects:
> >  Formula: list(Linf ~ 1 , K ~ 1 )
> >  Level: ID
> >  Structure: General positive-definite
> >             StdDev   Corr
> >     Linf 7.3625291 Linf
> >        K 0.0845886 -0.656
> > Residual 1.6967358
> >   Fixed effects: list(Linf + K ~ 1)
> >         Value Std.Error   DF  t-value p-value
> > Linf 69.32748 0.4187314  402 165.5655  <.0001
> >    K  0.31424 0.0047690 2549  65.8917  <.0001
> >   Standardized Within-Group Residuals:
> >       Min         Q1         Med        Q3     
> Max
> >  -3.98674 -0.5338083 -0.02783649 0.5261591
> 4.750609
> >   Number of Observations: 2952
> > Number of Groups: 403
> > >
> >
> >   Why are the DF of Linf and K different? I would
> apreciate if you could point me to a reference
> 
> The algorithm is described in Pinheiro and Bates
> (2000) "Mixed-effects
> Models in S and S-PLUS" published by Springer.  See
> section 2.4.2

I had consulted
the algorithm described in Pinheiro and Bates.

> 
> I would point out that there is effectively no
> difference between a
> t-distribution with 402 df and a t-distribution with
> 2549 df so the
> actual number of degrees of freedom is irrelevant in
> this case.  All
> you need to know is that it is "large".

However, what I don't understand (among other things)
is why my two parameters appear to be estimated at
different grouping levels (based on the DF values).
Affect this different values of DF at the estimates
parameters? The estimates fixed effects were get at
the same level of grouping?
> 
> I will defer to any of the "degrees of freedom
> police" who post to
> this list to give you an explanation of why there
> should be different
> degrees of freedom.  I have been studying
> mixed-effects models for
> nearly 15 years and I still don't understand.
> 
> >   Note: I working with Splus 6.1. for Windows
> 
> Technically this email list is for questions about
> R.  There is
> another list, s-news at biostat.wustl.edu, for
> questions about S-PLUS.


> I apreciate any response.


> > ---------------------------------
> >  1GB gratis, Antivirus y Antispam
> >  Correo Yahoo!, el mejor correo web del mundo
> >  Abr?? tu cuenta aqu??
> >         [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> 


Lic. Gabriela Escati Pe??aloza
Biolog??a y Manejo de Recursos Acu??ticos
Centro Nacional Patag??nico(CENPAT). 
CONICET
Bvd. Brown s/n??.
(U9120ACV)Pto. Madryn 
Chubut
Argentina

Tel: 54-2965/451301/451024/451375/45401 (Int:277)
Fax: 54-29657451543

__________________________________________________
Correo Yahoo!
Espacio para todos tus mensajes, antivirus y antispam ??gratis! 
??Abr?? tu cuenta ya! - http://correo.yahoo.com.ar



From strinz at freenet.de  Tue Jan 31 13:42:16 2006
From: strinz at freenet.de (strinz@freenet.de)
Date: Tue, 31 Jan 2006 13:42:16 +0100
Subject: [R] arules and frequent pattern tree
Message-ID: <E1F3ups-0003u3-HZ@www16.emo.freenet-rz.de>

Hello,

   package "arules" is very well suited for mining association rules,
   interfacing implementations of "apriori" and "eclat".

   What is not realised is an implementation of the Frequent Pattern Tree (Han, J., Pei, J. and Yiwen, Y. (2000).

   There are several free implementations available on the net.
   (e.g.: http://www.csc.liv.ac.uk/~frans/KDD/Software/FPgrowth/fpGrowth.html)

   I would appreciate any suggestions, how theese Algorithms could be interfaced with R,
   or how the textual output of the frequent items could be visually represented in a tree structure by means
   of which data structure ?) in R. 

   Thanks.   

Regards
Bj??rn


   



"Jetzt Handykosten senken mit klarmobil - 14 Ct./Min.! Hier klicken"
www.klarmobil.de/index.html?pid=73025



From petr.pikal at precheza.cz  Tue Jan 31 14:56:47 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 31 Jan 2006 14:56:47 +0100
Subject: [R] vector with specified values construction
Message-ID: <43DF7AAF.277.1845CA7@localhost>

Dear all

I want to have vector of length 24 which is filled by zeroes except 
that on position 

c(8,16,19,20) is 2
and
c(3,4,12,24) is 4
instead.
 
I am able to do

> (1:24)%in%c(8,16,19,20)*2
 [1] 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 2 0 0 0 0

and

> (1:24)%in%c(3,4,12,24)*4
 [1] 0 0 4 4 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4

but I can not find a suitable construction to end with vector
0 0 4 4 0 0 0 2 0 0 0 4 0 0 0 2 0 0 2 2 0 0 0 4

Can anybody help please?


Petr Pikal
petr.pikal at precheza.cz



From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 31 15:13:32 2006
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 31 Jan 2006 15:13:32 +0100
Subject: [R] vector with specified values construction
References: <43DF7AAF.277.1845CA7@localhost>
Message-ID: <01ca01c62670$84dce6e0$0540210a@www.domain>

consider the following:

x <- numeric(24)
x[c(8,16,19,20)] <- 2
x[c(3,4,12,24)] <- 4
x

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 31, 2006 2:56 PM
Subject: [R] vector with specified values construction


> Dear all
>
> I want to have vector of length 24 which is filled by zeroes except
> that on position
>
> c(8,16,19,20) is 2
> and
> c(3,4,12,24) is 4
> instead.
>
> I am able to do
>
>> (1:24)%in%c(8,16,19,20)*2
> [1] 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 2 0 0 0 0
>
> and
>
>> (1:24)%in%c(3,4,12,24)*4
> [1] 0 0 4 4 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4
>
> but I can not find a suitable construction to end with vector
> 0 0 4 4 0 0 0 2 0 0 0 4 0 0 0 2 0 0 2 2 0 0 0 4
>
> Can anybody help please?
>
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm



From christos at nuverabio.com  Tue Jan 31 15:27:42 2006
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 31 Jan 2006 09:27:42 -0500
Subject: [R] Glossay of available R functions
In-Reply-To: <Pine.LNX.4.61.0601310801310.4916@gannet.stats>
Message-ID: <001b01c62672$825d7d10$0e010a0a@headquarters>

Patricia,

If I understand correctly what you need, this is already available through
the main help page.  If you type help.start() to get to the html help, then
follow the link "Packages".  This will give you an index of all available
packages installed on your system (base + contributed).  Clicking on any of
the package names will give you a list of available functions in that
package.  This is a nice way to present a categorized index of several
thousand functions.  An additional advantage is that this index will be
updated if you install more packages on your system.

Hope this helps.

Christos Hatzis
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 2850
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 

  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Tuesday, January 31, 2006 3:10 AM
To: Patricia J. Hawkins
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Glossay of available R functions

On Mon, 30 Jan 2006, Patricia J. Hawkins wrote:

>>>>>> "ASA" == Alexandre Santos Aguiar <asaguiar at spsconsultoria.com>
writes:
>
> ASA> I am new to R and read this list to learn. It is amazing how 
> ASA> frequently new functions pop in messages. Useful and timesaving 
> ASA> functions like subset (above) must be documented somewhere.
>
> ASA> Is there a glossary of functions?
>
> I'm also new to R, and was wondering the same thing.  Took a bunch of 
> tries, but if you run start.help() and then choose Packages, then 
> Base, you will get the list of functions.

You get a list of objects (not just functions) in the base package.  You can
also get a list by library(help=base).

However, that is far from all the functions available in base R.
As a quick check

as.matrix(sapply(search(), function(x) length(ls(x, all=TRUE))))

.GlobalEnv           0
package:methods    299
package:stats      497
package:graphics    79
package:grDevices   78
package:utils      152
package:datasets   103
Autoloads            1
package:base      1090

so it is less than half the objects loaded and visible in a default session.
And there are another 18 packages shipped with R.

Looking at a list of 2300 objects is daunting, and so we provide search
facilities (including via the HTML pages).

> As a newcomer, I hesitate to suggest this, but maybe there should be a 
> comment on the index page to that effect?

Which index page?  If you mean that given by help.start(), it is not a
common request, and search is linked from there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From scruveil at genoscope.cns.fr  Tue Jan 31 15:30:41 2006
From: scruveil at genoscope.cns.fr (Stephane CRUVEILLER)
Date: Tue, 31 Jan 2006 15:30:41 +0100
Subject: [R] List of lists???
Message-ID: <200601311530.41705.scruveil@genoscope.cns.fr>

Hi,

I would like to perform computations on some variables belonging to the same 
dataframe. For instance my data frame has the following shape:
????????????????toto1??????toto2??????toto3??????toto4??????toto5
1 ????????????1??????????????2??????????????3??????????????4??????????????5
2??????????????6??????????????7??????????????8??????????????9??????????????10

I would like to perform the calculation on c("toto1","toto2","toto3") and then 
the same calculation on c("toto4","toto5"). Is there a way to tell R to do it 
using a loop (a list of lists)??

Stephane.
-- 
==========================================================
Stephane CRUVEILLER Ph. D.
Genoscope - Centre National de Sequencage
Atelier de Genomique Comparative
2, Rue Gaston Cremieux   CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr



From baron at psych.upenn.edu  Tue Jan 31 15:36:04 2006
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 31 Jan 2006 09:36:04 -0500
Subject: [R] Glossay of available R functions
In-Reply-To: <001b01c62672$825d7d10$0e010a0a@headquarters>
References: <Pine.LNX.4.61.0601310801310.4916@gannet.stats>
	<001b01c62672$825d7d10$0e010a0a@headquarters>
Message-ID: <20060131143604.GA12731@psych.upenn.edu>

In addition, the search page at

http://finzi.psych.upenn.edu

can search all functions of all CRAN packages.

This is also available through

RSiteSearch(string,restrict="functions").

See the help page for RSiteSearch.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Michaell.Taylor at boxwoodmeans.com  Tue Jan 31 15:36:16 2006
From: Michaell.Taylor at boxwoodmeans.com (Michaell Taylor)
Date: Tue, 31 Jan 2006 08:36:16 -0600
Subject: [R] RMySQL install
In-Reply-To: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
References: <200601301532.44995.Michaell.Taylor@boxwoodmeans.com>
Message-ID: <200601310836.17000.Michaell.Taylor@boxwoodmeans.com>


   
Received some excellent responses to my query.  The problem was with the 
mysql-devel libraries.  

I upgraded/installed  the mysql client/server/devel libraries to 4.1.16 and 
issued the "well known" fixes of :

export PKG_LIBS='-L/usr/lib64/mysql  -lmysqlclient'
export PKG_CPPFLAGS=-I/usr/include/mysql

R CMD INSTALL  RMySQL_0.5-7.tar.gz

and everything clicked on.

Thanks for the help.

Michaell

On Monday 30 January 2006 03:32 pm, Michaell Taylor wrote:
> I am having trouble installing RMySQL on a clean install of Fedora Core 4
> 64 bit on a dual dual core machine (that is, two dual core processors). 
> Seems like the LD_LIBRARY_PATH is incorrect, but I don't seem to have it
> quite right yet.
>
> There are a few mentions of this problem in google, but thus far none of
> the "fixes" and fixed my problem.  I've tried defining the LD_LIBRARY_PATH
> environment variable, and setting the PKG_CPPFLAGS, and  PKG_LIBS
> environment variables as well.  No luck so far.
>
> (initially would not compile, but specification of the PKG_CPPFLAGS, and
> PKG_LIBS got the compile to complete without errors).  I still cannot load
>
> the library.  My error message is:
> > library(RMySQL)
>
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
>   /usr/lib64/R/library/RMySQL/libs/RMySQL.so: undefined symbol:
> mysql_field_count
> Error in library(RMySQL) : .First.lib failed for 'RMySQL'
>
>
> Trying to install the newest RMySQL on the newest R.
>
> > version
>
>          _
> platform x86_64-redhat-linux-gnu
> arch     x86_64
> os       linux-gnu
> system   x86_64, linux-gnu
> status
> major    2
> minor    2.1
> year     2005
> month    12
> day      20
> svn rev  36812
> language R
>
> *********************
> I find libmysqlclient.so in the following location(s).
>
> [root at BX mtaylor]# locate libmysqlclient.so
> /usr/lib/mysql/libmysqlclient.so.14.0.0
> /usr/lib/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql/libmysqlclient.so.10
> /usr/lib64/mysql/libmysqlclient.so.14.0.0
> /usr/lib64/mysql/libmysqlclient.so.14
> /usr/lib64/mysql/libmysqlclient.so
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10.0.0
> /usr/lib64/mysql3/mysql/libmysqlclient.so.10
> /usr/lib64/mysql3/mysql/libmysqlclient.so
>
> **********************
>
> I set the LD_LIBRARY_PATH according to these results, then double checked
> that it was set.  (I also set it to /usr/lib64/mysql3/mysql as another
> iteration)
>
> [root at BX mtaylor]# printenv LD_LIBRARY_PATH
> /usr/lib64/mysql3/
>
> Just to be sure...
>
> [root at BX mtaylor]# printenv PKG_CPPFLAGS
> -I/usr/include/mysql
>
> [root at BX mtaylor]# printenv PKG_LIBS
> -L/usr/lib64/mysql
>
> (also tried PKG_LIBS=-L/usr/lib64/mysql3/mysql and
> PKG_LIBS=-L/usr/lib/mysql)
>
> I recompiled the package each time with R CMD INSTALL RMySQL_0.5-7.tar.gz,
> but I always get the same error message.
>
> ******************
> appears to install ....
>
> [root at BX mtaylor]# R CMD INSTALL RMySQL_0.5-7.tar.gz
> * Installing *source* package 'RMySQL' ...
> creating cache ./config.cache
> checking how to run the C preprocessor... cc -E
> checking for compress in -lz... yes
> checking for getopt_long in -lc... yes
> checking for mysql_init in -lmysqlclient... no
> checking for mysql.h... no
> updating cache ./config.cache
> creating ./config.status
> creating src/Makevars
> ** libs
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include  
> -fPIC -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona
> -c RS-DBI.c -o RS-DBI.o
> gcc -I/usr/lib64/R/include -I/usr/include/mysql -I/usr/local/include  
> -fPIC -O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -m64 -mtune=nocona
> -c RS-MySQL.c -o RS-MySQL.o
> gcc -shared -L/usr/local/lib64 -o RMySQL.so RS-DBI.o RS-MySQL.o
> -L/usr/lib/mysql -lz  -L/usr/lib64/R/lib -lR
> ** R
> ** inst
> ** save image
> .
> snip
> .
> ** building package indices ...
> * DONE (RMySQL)
>
>
> [root at BX mtaylor]# /sbin/ldconfig -v | grep mysql
> /usr/lib/mysql:
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
> /usr/lib64/mysql:
>         libmysqlclient_r.so.10 -> libmysqlclient_r.so.10.0.0
>         libmysqlclient.so.14 -> libmysqlclient.so.14.0.0
>         libmysqlclient_r.so.14 -> libmysqlclient_r.so.14.0.0
>         libmysqlclient.so.10 -> libmysqlclient.so.10.0.0
>
> just to make sure the headers are actually in the specified location...
>
> [root at BX mtaylor]# ls /usr/include/mysql
> chardefs.h  m_ctype.h    my_dir.h     my_no_pthread.h  mysql_embed.h
> my_xml.h     rlshell.h      sslopt-longopts.h
> errmsg.h    m_string.h   my_getopt.h  my_pthread.h     mysql.h         
> raid.h rltypedefs.h   sslopt-vars.h
> history.h   my_alloc.h   my_global.h  my_semaphore.h   mysql_time.h
> readline.h   sql_common.h   tilde.h
> keycache.h  my_config.h  my_list.h    mysql_com.h      mysql_version.h
> rlmbutil.h   sql_state.h    typelib.h
> keymaps.h   my_dbug.h    my_net.h     mysqld_error.h   my_sys.h
> rlprivate.h  sslopt-case.h  xmalloc.h
>
>
> There is a dial in here somewhere that I think I am turning the wrong
> direction - just can't see it.  Any experience with this one?

-- 
=======================================
Michaell Taylor, PhD.
Principal
Boxwood Means, Inc.
203.653.4100



From ggrothendieck at gmail.com  Tue Jan 31 15:50:07 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 Jan 2006 09:50:07 -0500
Subject: [R] List of lists???
In-Reply-To: <200601311530.41705.scruveil@genoscope.cns.fr>
References: <200601311530.41705.scruveil@genoscope.cns.fr>
Message-ID: <971536df0601310650o498b3f2ds9c446e600deb70da@mail.gmail.com>

Your post seems to be messed up but I will assume you have a
5 column data frame and the questino is how to run f on the first
three columns and separately on the last two.  I think the
easiest is just the following where I have used the builtin iris
data set where I have assumed that the operation you want
to perform is summary for purposes of example:

summary(iris[,1:3])
summary(iris[,4:5])

If this is not your real problem and the real problem has many more
divisions then try this where div is a factor that defines the division
of columns into sets:

div <- factor(c(1,1,1,2,2))
lapply(split(names(iris), div), function(n) summary(iris[n]))




On 1/31/06, Stephane CRUVEILLER <scruveil at genoscope.cns.fr> wrote:
> Hi,
>
> I would like to perform computations on some variables belonging to the same
> dataframe. For instance my data frame has the following shape:
> toto1toto2toto3toto4toto5
> 1 12345
> 2678910
>
> I would like to perform the calculation on c("toto1","toto2","toto3") and then
> the same calculation on c("toto4","toto5"). Is there a way to tell R to do it
> using 3 loop (a list of lists)??
>
> Stephane.
> --
> ==========================================================
> Stephane CRUVEILLER Ph. D.
> Genoscope - Centre National de Sequencage
> Atelier de Genomique Comparative
> 2, Rue Gaston Cremieux   CP 5706
> 91057 Evry Cedex - France
> Phone: +33 (0)1 60 87 84 58
> Fax: +33 (0)1 60 87 25 14
> EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Tue Jan 31 16:52:40 2006
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 31 Jan 2006 16:52:40 +0100
Subject: [R] vector with specified values construction
In-Reply-To: <43DF7AAF.277.1845CA7@localhost>
Message-ID: <43DF95D8.1745.1EE725E@localhost>

Thanks to all who responded. I definitly need some fresh air to clean 
my head. :-)

Petr

On 31 Jan 2006 at 14:56, Petr Pikal wrote:

From:           	"Petr Pikal" <petr.pikal at precheza.cz>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Tue, 31 Jan 2006 14:56:47 +0100
Priority:       	normal
Subject:        	[R] vector with specified values construction

> Dear all
> 
> I want to have vector of length 24 which is filled by zeroes except
> that on position 
> 
> c(8,16,19,20) is 2
> and
> c(3,4,12,24) is 4
> instead.
> 
> I am able to do
> 
> > (1:24)%in%c(8,16,19,20)*2
>  [1] 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 2 0 0 0 0
> 
> and
> 
> > (1:24)%in%c(3,4,12,24)*4
>  [1] 0 0 4 4 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 4
> 
> but I can not find a suitable construction to end with vector
> 0 0 4 4 0 0 0 2 0 0 0 4 0 0 0 2 0 0 2 2 0 0 0 4
> 
> Can anybody help please?
> 
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From rvaradhan at jhmi.edu  Tue Jan 31 16:55:19 2006
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 31 Jan 2006 10:55:19 -0500
Subject: [R] Multiple xyplots on the same page
Message-ID: <000001c6267e$bc9eb410$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060131/bc84d39e/attachment.pl

From Jesse.Whittington at pc.gc.ca  Tue Jan 31 17:09:00 2006
From: Jesse.Whittington at pc.gc.ca (Jesse.Whittington@pc.gc.ca)
Date: Tue, 31 Jan 2006 09:09:00 -0700
Subject: [R] Logistic regression model selection
 with	overdispersed/autocorrelated data
Message-ID: <OF9D00DA8C.1B4C9D92-ON87257107.0057715C@pc.gc.ca>





Jesse.Whittington at pc.gc.ca wrote:
>
> I am creating habitat selection models for caribou and other species with
> data collected from GPS collars.  In my current situation the
radio-collars
> recorded the locations of 30 caribou every 6 hours.  I am then comparing
> resources used at caribou locations to random locations using logistic
> regression (standard habitat analysis).
>
> The data is therefore highly autocorrelated and this causes Type I error
> two ways  small standard errors around beta-coefficients and
> over-paramaterization during model selection.  Robust standard errors are
> easily calculated by block-bootstrapping the data using animal as a
> cluster with the Design library, however I havent found a satisfactory
> solution for model selection.
>
> A couple options are:
> 1.  Using QAIC where the deviance is divided by a variance inflation
factor
> (Burnham & Anderson).  However, this VIF can vary greatly depending on
the
> data set and the set of covariates used in the global model.
> 2.  Manual forward stepwise regression using both changes in deviance and
> robust p-values for the beta-coefficients.
>
> I have been looking for a solution to this problem for a couple years and
> would appreciate any advice.
>
> Jesse

Frank E Harrell Jr wrote:

If you must do non-subject-matter-driven model selection, look at the
fastbw function in Design, which will use the cluster bootstrap variance
matrix.

Frank


Thanks for the tip.  I didn't know that the fastbw function could account
for the clustered variance.  For others, the code to run such a model from
the Design library would be:

model.1 <- lrm(y ~ x1+x2+x3+x4, data=data, x=T,y=T)          # create model
model.2 <- bootcov(model.1, cluster=data$animal, B=10000)    # calculate
robust variance matrix
fastbw(model.2)                                              # backward
step-wise selection.

Later we will examine individual caribou responses to trails
(subject-specific model selection).  For this we plan to use mixed effects
models (lmer).  Is this what you would also recommend?

I look forward to reading the new edition of your book when it is
published.

Jesse



From David.Brahm at geodecapital.com  Tue Jan 31 17:31:11 2006
From: David.Brahm at geodecapital.com (Brahm, David)
Date: Tue, 31 Jan 2006 11:31:11 -0500
Subject: [R] [R-pkgs]   sudoku
Message-ID: <4DD6F8B8782D584FABF50BF3A32B03D801A2BD0C@MSGBOSCLF2WIN.DMN1.FMR.COM>

sudoku_2.0 is now available on CRAN and mirrors.

Thanks to some terrific contributions and suggestions, especially by
new co-author Greg Snow, this version has a full GUI interface (native
on Windows, or using "tkrplot"), a puzzle generator, and a function to
fetch recently published puzzles.  The GUI (playSudoku) can act as an
editor to input a puzzle, then give you hints, correct errors, reveal
cells, or show the full solution.

See README for some work remaining to be done, including a uniqueness
test, "notes" in the GUI, and a scoring mechanism. 

-- David Brahm (brahm at alum.mit.edu)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From adi at roda.ro  Tue Jan 31 17:35:08 2006
From: adi at roda.ro (Adrian DUSA)
Date: Tue, 31 Jan 2006 18:35:08 +0200
Subject: [R] yet another vectorization question
In-Reply-To: <971536df0601302055p6cf1e7c1r9c98eefb120ed308@mail.gmail.com>
References: <200601301334.40440.adi@roda.ro> <wkmzhdcbju.fsf@connact.com>
	<971536df0601302055p6cf1e7c1r9c98eefb120ed308@mail.gmail.com>
Message-ID: <200601311835.09123.adi@roda.ro>

On Tuesday 31 January 2006 06:55, Gabor Grothendieck wrote:
> On 1/30/06, Patricia J. Hawkins <phawkins at connact.com> wrote:
> > [...snip...]
> >
> > #which generalizes to:
> >
> > bb <- matrix(1:50, ncol=10, nrow=5, byrow=TRUE)
> > bv <- as.vector(bb)
> > ai <- as.vector(aa) + rep((1:nrow(aa)-1)*10, each=3)
> > bv[ai] <- c(0,1,0)
> > bb <- matrix(bv, ncol=10, nrow=5, byrow=TRUE)
> > bb
>
> Try this:
>
> bb <- matrix(NA, ncol=10, nrow=5)
> bb[cbind(c(col(aa)), c(aa))] <- c(0,1,0)

Thank you very much both, I had a very good time exercising your solutions. 
The "col" fuction especially is useful (and insightful).
I wrote a working solution based on this type of matrix indexing, which is... 
unfortunately... _slower_ than the "for" loop, especially in large loops.
It seems that creating the necessary row and column indexes to cbind is much 
slower than copying chunks of data at certain columns:

> library(combinat) # for the combn function
> system.time(all.expr(LETTERS[1:12]))
[1] 6.12 0.39 6.54 0.00 0.00
> system.time(all.expr2(LETTERS[1:12]))
[1] 8.62 0.27 8.91 0.00 0.00

If anyone interested, I uploaded both functions here:
http://www.roda.ro/all.expr.R

Thank you,
Adrian

-- 
Adrian DUSA
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101



From assampryseley at yahoo.com  Tue Jan 31 17:48:11 2006
From: assampryseley at yahoo.com (Pryseley Assam)
Date: Tue, 31 Jan 2006 08:48:11 -0800 (PST)
Subject: [R] Help with R: functions
In-Reply-To: <43DE3B5B.2010005@pburns.seanet.com>
Message-ID: <20060131164811.84252.qmail@web37115.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20060131/09ca9577/attachment.pl

From bolker at ufl.edu  Tue Jan 31 18:11:21 2006
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 31 Jan 2006 17:11:21 +0000 (UTC)
Subject: [R] Kolmogorov-Smirnov Test on R
References: <43DE9B34.7090005@yahoo.co.uk>
Message-ID: <loom.20060131T180747-87@post.gmane.org>

B Dittmann <herrdittmann <at> yahoo.co.uk> writes:

> 
> Hi,
> 
> just run the Kolmogorov-Smirnov test on R.
> Is there any detailed documentation available for the options for the KS 
> test, esp. with regard to the hypotheses.
> The help file is rather "thin".
> 
> Many thanks.
> 
> Bernd
> 

   Hmmm.  Does looking in Conover (1971) as referenced on the
help page help?
   Can you be more specific about what you want to know?

  Ben



From Simon.Blanchet at giroq.ulaval.ca  Tue Jan 31 18:38:02 2006
From: Simon.Blanchet at giroq.ulaval.ca (Simon Blanchet)
Date: Tue, 31 Jan 2006 12:38:02 -0500
Subject: [R] Stepwise selection and F-enter anf F-remove values
Message-ID: <6.2.1.2.2.20060131075507.033b1848@hermes.ulaval.ca>

Hello,

I'm actually using the "Step" procedure in R for multiple regression analysis.
I'm using the stepwise selection which alternates between forward selection 
and backward elimination (direction "both" in the step procedure).
I would like to know which F-levels R is using to enter and then to remove 
variables?
I also would like which is the procedure to change such levels? For 
instance, how to use a critical/levels of 0.05/0.10?

Many thanks in advance!

Very sincerely,

Simon Blanchet

BLANCHET Simon
PhD student
Universit?? Laval - Qu??bec-Oc??an / CIRSA
Pavillon Alexandre-Vachon
Local 8022
Qu??bec (Qu??bec), Canada G1K 7P4
T??l??phone : (418) 656-2131 poste 8022
courriel : simon.blanchet at giroq.ulaval.ca



From dmbates at gmail.com  Tue Jan 31 18:52:55 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 31 Jan 2006 11:52:55 -0600
Subject: [R] how calculation degrees freedom
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC03878181@DJFPOST01.djf.agrsci.dk>
References: <20060127150432.86396.qmail@web37115.mail.mud.yahoo.com>
	<40e66e0b0601270806k4f2a30ew54e148dd98d81d1b@mail.gmail.com>
	<C83C5E3DEEE97E498B74729A33F6EAEC0387817D@DJFPOST01.djf.agrsci.dk>
	<40e66e0b0601271540l541c881dk126eb4a04bd04f4d@mail.gmail.com>
	<x2oe1xfbw4.fsf@turmalin.kubism.ku.dk>
	<C83C5E3DEEE97E498B74729A33F6EAEC03878181@DJFPOST01.djf.agrsci.dk>
Message-ID: <40e66e0b0601310952g414037e4n3f00b8c40a4912fe@mail.gmail.com>

On 1/29/06, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> In connection with calculating Monte Carlo p-values based on sampled data sets: The calculations involve something like
>    update(lmer.model, data=newdata)
> where newdata is a simulated dataset comming from simulate(lmer.model). I guess the update could be faster if one could supply the update function with the parameter estimates from the original fit of the lmer.model as starting values. Is this possible to achieve??

Possible - yes.  (See the quote in the fortunes package about "This is
R.  There is no if - only how.")

Roughly what one does is

 - Take a copy of the fitted model object as, say, "mer"
 - .Call("mer_update_y", mer, ynew, PACKAGE = "Matrix")
 - arrange for a suitable call to
   LMEoptimize(mer) <- controloptions

The last part is a little tricky in that "LMEoptimize<-" is hidden in
the Matrix namespace.

I'm happy to write a function to do this but my creativity is at a low
ebb and I would appreciate any suggestions for a suitable name and
calling sequence.  Even more welcome would be an existing generic
function for which something like this could be a method.

> Best
> S??ren
>
> ________________________________
>
> Fra: pd at pubhealth.ku.dk p?? vegne af Peter Dalgaard
> Sendt: l?? 28-01-2006 01:12
> Til: Douglas Bates
> Cc: S??ren H??jsgaard; R-help at stat.math.ethz.ch
> Emne: Re: [R] how calculation degrees freedom
>
>
>
> Douglas Bates <dmbates at gmail.com> writes:
>
>
> > > Of course, Monte Carlo p-values have their problems, but the world
> > > is not perfect....
> >
> > Another approach is to use mcmcsamp to derive a sample from the
> > posterior distribution of the parameters using Markov Chain Monte
> > Carlo sampling.  If you are interested in intervals rather than
> > p-values the HPDinterval function from the coda package can create
> > those.
> >
>
> We (S??ren and I) actually had a look at that, and it seems not to
> solve the problem. Rather, mcmcsamp tends to reproduce the Wald style
> inference (infinite DF) if you use a suitably vague prior.
>
> It's a bit hard to understand clearly, but I think the crux is that
> any Bayes inference only depends on data through the likelihood
> function. The distribution of the likelihood never enters (the
> hardcore Bayesian of course won't care). However, the nature of DF
> corrections is that the LRT does not have its asymptotic distribution,
> and mcmc has no way of picking that up.
>
>
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>
>
>



From ripley at stats.ox.ac.uk  Tue Jan 31 19:00:02 2006
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jan 2006 18:00:02 +0000 (GMT)
Subject: [R] Stepwise selection and F-enter anf F-remove values
In-Reply-To: <6.2.1.2.2.20060131075507.033b1848@hermes.ulaval.ca>
References: <6.2.1.2.2.20060131075507.033b1848@hermes.ulaval.ca>
Message-ID: <Pine.LNX.4.61.0601311755400.20031@gannet.stats>

If this is step() (not Step), it does not use tests. Please consult the 
help page which says

step                  package:stats                  R Documentation

Choose a model by AIC in a Stepwise Algorithm

Description:

      Select a formula-based model by AIC.
...

It seems these days we also have to point out that the help pages have 
references with the background information and even more detailed 
descriptions.


On Tue, 31 Jan 2006, Simon Blanchet wrote:

> Hello,
>
> I'm actually using the "Step" procedure in R for multiple regression analysis.
> I'm using the stepwise selection which alternates between forward selection
> and backward elimination (direction "both" in the step procedure).
> I would like to know which F-levels R is using to enter and then to remove
> variables?
> I also would like which is the procedure to change such levels? For
> instance, how to use a critical/levels of 0.05/0.10?
>
> Many thanks in advance!
>
> Very sincerely,
>
> Simon Blanchet

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
   ^^^^^^^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Jan 31 19:03:40 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 31 Jan 2006 13:03:40 -0500
Subject: [R] Multiple xyplots on the same page
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED792@usctmx1106.merck.com>

See ?print.trellis.

Andy

From: Ravi Varadhan
> 
> Hi,
> 
>  
> 
> I am using the "xyplot" function in the "lattice" package to generate
> multiple plots, but I would like to have them plotted on the 
> same page.  I
> would like to set something equivalent to the command:  
> par(mfrow=c(2,2)),
> in order that I can plot 4 xyplots on the same page.  How can 
> I do this in
> "xyplot"?
> 
> I am using R version 2.1.1 on Windows.
> 
>  
> 
> Thanks very much,
> 
> Ravi.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lutz.breitling at gmail.com  Tue Jan 31 19:22:52 2006
From: lutz.breitling at gmail.com (Lutz Ph. Breitling)
Date: Tue, 31 Jan 2006 18:22:52 +0000
Subject: [R] Mixed-effects models / heterogeneous covariances
Message-ID: <2e38a1c80601311022i2e1be92doa60b80b50b69eb0c@mail.gmail.com>

Dear R-list,

maybe someone can help me with the following mixed-effects models
problem, as I am unable to figure it out with the 'nlme-bible'.

I would like to fit (in R, obviously) a so-called animal model (google
e. g. "Heritability and genetic constraints of life-history" by Pettay
et al.) to estimate the variance component that is due to genetic
effects. The covariances of the genetic random effects between
observations are given by the different degrees of relatedness between
the individuals examined. (I find it difficult to explain, but Pettay
et al. describe it nicely in their methods section...)

Is there any straight-forward way to fit such a model with R? I first
thought I could handle it somehow with nlme's correlation structures,
but these within-group structures are quite a different thing, right?

Any suggestions would be highly appreciated-
Lutz

--
Lutz Ph. Breitling
University of Leeds/UK



From herrdittmann at yahoo.co.uk  Tue Jan 31 20:00:48 2006
From: herrdittmann at yahoo.co.uk (B Dittmann)
Date: Tue, 31 Jan 2006 20:00:48 +0100
Subject: [R] Kolmogorov-Smirnov Test - what are the exact alternative
	hypotheses?
Message-ID: <43DFB3E0.4020500@yahoo.co.uk>

Hi everyone,

I have performed the Kolmogorov-Smirnov test (ks.test) with R the first 
time.

What I am not sure about is the exact alternative hypotheses (H1) given 
any H0.

According to Conover (1971) Practical Nonparametric Statistics, chapter 
6, the following one-sample tests can be performed:

(1) Two-sided test

    H0: F(x) = F*(x)
    H1: F(x) =/= F*(x) [for at least one x]


(2) one-sided test

    (2.a): "less"
   
    H0: F(x) =< F*(x)
    H1: F(x) > F*(x) [for at least one x]


    (2.b): "greater"

    H0: F(x) >= F*(x)
    H1: F(x) < F*(x) [for at least one x]

with F(x) being the empirical distribution of sample data and F*(x) the 
hypothesised distribution. F*(x) requires full specification, which can 
be achieved by "fitdistr(..., F*(x)).


The KS Test on R tests a sample performs a 2-sided test as default mode.
The choice for "alternative" seems to allow for a choice of H0 different 
to the standard of two-sided.

Suppose one would chooses "less" or "greater" as "alternative". Does the 
KS test automatically set the correct H1?
Whenever I perform the test, the alternative hypothesis is not being 
stated, that is why I am not certain whether I would be correct to make 
this assumption.

Here is an example of the output for a test:
P*(x) is a lognormal distribution, fully specified with meanlog and sdlog.

=====================

 > ks.test(spread,plnorm, meanlog=2.359, sdlog=0.588, alternative = 
"greater")

        One-sample Kolmogorov-Smirnov test

data:  spread
D^+ = 0.035, p-value = 0.0462
alternative hypothesis: greater

Warning message:
cannot compute correct p-values with ties in: ks.test(spread, plnorm, 
meanlog = 2.359, sdlog = 0.588, alternative = "greater")

======================

In the above test I am testing one-sided, i.e. (2.b) above. Does ks.test 
automatically set H1 such that "F(x) < F*(x) for at least one x"?

Thank you for your help.

Bernd Dittmann



From debayan.datta at yale.edu  Tue Jan 31 20:07:24 2006
From: debayan.datta at yale.edu (Debayan Datta)
Date: Tue, 31 Jan 2006 14:07:24 -0500 (EST)
Subject: [R] Density estimation with monotonic constaints
Message-ID: <Pine.LNX.4.63.0601311404300.10007@argos.its.yale.edu>

Hi All,
   I have a sample x={x1,x2,..,xn} fom a distribution with density f. I 
wish to estimate the density. I know a priori that the density is 
monotonically decreasing. Is there a way to do this in R?
Thanks
Debayan



From orlowski at ebgm.jussieu.fr  Tue Jan 31 20:46:20 2006
From: orlowski at ebgm.jussieu.fr (Georges Orlowski)
Date: Tue, 31 Jan 2006 20:46:20 +0100 (CET)
Subject: [R] SVM question
Message-ID: <Pine.LNX.4.58.0601312031200.10519@bach.ebgm.jussieu.fr>

I'm running SVM from e1071 package on a data with ~150 columns (variables) 
and 50000 lines of data (it takes a bit of time) for radial kernel for 
different gamma and cost values. 

I get a very large models with at least 
30000 vectors and the prediction I get is not the best one. What does it 
mean and what could I do to ameliorate my model ?

Jerzy Orlowski



From david.reitter at gmail.com  Tue Jan 31 21:39:15 2006
From: david.reitter at gmail.com (David Reitter)
Date: Tue, 31 Jan 2006 20:39:15 +0000
Subject: [R] predict.lme / glmmPQL: "non-conformable arguments"
In-Reply-To: <2D31CED5-2507-4612-884D-FC81541D2526@gmail.com>
References: <2D31CED5-2507-4612-884D-FC81541D2526@gmail.com>
Message-ID: <52259C38-3BA8-4506-B3F8-A468A509093B@gmail.com>

  On 30 Jan 2006, at 22:01, David Reitter wrote:

> I'm trying to use "predict" with a linear mixed-effects logistic  
> regression model fitted with nlmmPQL from the MASS library.
> Unfortunately, I'm getting an error "non-conformable arguments" in  
> predict.lme, and I would like to understand why.

I'd like to add a bit of information. (Correction: I am talking about  
glmmPQL from the MASS library.)

Again, the error I'm getting is:

> > yp = predict(model,  newdata=new, type="response",  level=0)
> Error in X %*% fixef(object) : non-conformable arguments

I have ensured that I input a data frame in newdata with the fixed  
factors/predictors filled in (as factors with the correct level sets  
where appropriate).

Debugging this, I had a look at lme.R from the nlme library.
Specifically, line 1909:

if (maxQ == 0) {
     ## only population predictions
     val <- c(X %*% fixef(object))
     attr(val, "label") <- "Predicted values"
     return(val)
   }

the 'fixef' structure in my model looks like this (7 elements)

fixef(model)
                       (Intercept)                     log(distance)
                       -2.14560407                       -0.13207341
                             roler                     sourcemaptask
                       -0.58692474                       -0.93108113
               log(distance):roler log(distance):rolei:sourcemaptask
                        0.16449238                        0.06877369
log(distance):roler:sourcemaptask
                       -0.12278367


But the predict.lme function produces the following 6x5 matrix

   (Intercept) log(distance) roler sourcemaptask log 
(distance):roler    log(distance):roler:sourcemaptask
           1     0.0000000     1             0           0.0000000  0
          1     0.6931472     1             0           0.6931472  0
(...)

We're missing the coefficient for the 3-way interaction "log 
(distance):rolei:sourcemaptask", which is why we can't come up with  
the inner product of X and the fixed effects coefficients.

Is this an issue with predict.lme, and/or can I do something about it?

Thanks
D



From elvis at xlsolutions-corp.com  Tue Jan 31 21:39:24 2006
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 31 Jan 2006 13:39:24 -0700
Subject: [R] February course *** R/Splus Fundamentals and Programming
	Techniques
Message-ID: <20060131133923.9f08cc34deb45d78e54b3b5664e21546.fac4368363.wbe@email.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" in San Francisco: www.xlsolutions-corp.com/Rfund.htm

**** San Francisco,   February 16-17 
**** Seattle,            February 20-21 
**** Boston,            February 23-24 
**** New York,         February 27-28

Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From leif at reflectivity.com  Tue Jan 31 22:02:03 2006
From: leif at reflectivity.com (Leif Kirschenbaum)
Date: Tue, 31 Jan 2006 13:02:03 -0800
Subject: [R] How do I "normalise" a power spectral density
Message-ID: <200601312102.k0VL29fS003509@hypatia.math.ethz.ch>

I have done a fair bit of spectral analysis, and hadn't finished collecting my thoughts for a reply, so hadn't replied yet.

What exactly do you mean by normalize?
  I have not used the functons periodogram or spectrum, however from the description for periodogram it appears that it returns the spectral density, which is already normalized by frequency, so you don't have to worry about changing the appearance of your periodogram or power spectrum if you change the time intervals of your data. With a normal Fourier Transform, not only do you need to complex square the terms but you also need to divide by a normalizing factor to give power-per-frequency-bin, which the periodogram appears to do.

  However, if you look in various textbooks, the definition of the Fourier Transform (FT) varies from author to author in the magnitude of the prepended scaling factor. Since the periodogram is related to the FT (periodogram ultimately uses the function mvfft), without examination of the code for periodogram you cannot know the scaling factor, which is almost always one of the following:
  1.0
  1/2
  1/(2*pi)
  SQRT(1/2)
  SQRT(1/(2*pi))
  In fact, if you obtain an FT (or FFT or DFT) from a piece of electronics (say an electronic spectrum analyzer), the prepending factor can vary from manufacturer to manufacturer.
  Fortunately, there is a strict relationship between the variance of your signal and the integrated spectral density. If your time signal is x(t), the spectral density is S(f), and fc = frequency(x)/2 the Nyquist cutoff frequency, then this may be expressed as:

  variance(x(t)) = constant * {Integral from 0 to fc of S(f)}

In R-code: let x be your time series, and "constant" be the unknown scaling factor (1/2, 1/2pi, etc.)
  p <- periodogram(x)
  var(x) == constant * sum(p[[1]])/length(p[[1]])

Or:
  constant = sum(p[[1]])/length(p[[1]])/var(x)

and we find that the appropriate scaling constant is 1.0.

  As regards plotting versus period, the periodogram returns arrays of spectral amplitude and frequencies. The frequencies are in inverse units of the intervals of your time series. i.e. if your time series is 1-point per day, then the frequencies are in 1/day units. Thus if you wish to plot amplitude versus period in weeks you have a little math to do.
  I believe that plotting is usually versus frequency since most observers are interested in how things vary versus frequency: multiple evenly spaced peaks on a linear frequency scale indicates the presence of harmonics; this is not so simply seen in a plot versus period. ex. peaks of 10 Hz, 20 Hz, 30 Hz, 40 Hz,... in period would be at periods of 100 ms, 50 ms, 25 ms, 12.5 ms, and the peaks are not evenly spaced.
  Additionally, there are all kinds of typical responses versus frequency (1/f, 1/f^2) which are clearly seen in plots versus frequency as straight lines (log power vs log frequency), but would come out as curves in plots versus period.
  I can see how ecological studies may indeed be more interested in the periods.

  However, I would be wary of using the periodogram function, for if I calculate periodograms of the same sinewave but for differing lengths of the sample, the spectral density does not come out the same. All 4 of the plots produced by the code below should overlay, and yet as the time series becomes longer there appears to be an increasing offset of the magnitudes returned. (black-brown-blue-red)

x0<-ts(data=sin(2*pi*1.1*(1:1000)/10),frequency=10)
p0<-periodogram(x0)
var(x0)

x1<-ts(data=sin(2*pi*1.1*(1:10000)/10),frequency=10)
p1<-periodogram(x1)
var(x1)

x2<-ts(data=sin(2*pi*1.1*(1:100000)/10),frequency=10)
p2<-periodogram(x2)
var(x2)

x3<-ts(data=sin(2*pi*1.1*(1:1000000)/10),frequency=10)
p3<-periodogram(x3)
var(x3)

plot(p3[[2]],p3[[1]],col="red",type="p",pch=19,cex=0.05,log="y",
	xlim=c(0.1,0.12),ylim=c(1e-30,1e-15))
points(p2[[2]],p2[[1]],type="l",col="blue")
points(p1[[2]],p1[[1]],type="l",col="brown")
points(p0[[2]],p0[[1]],type="o",col="black")


> Message: 113
> Date: Mon, 30 Jan 2006 17:45:58 -0800
> From: Spencer Graves <spencer.graves at pdf.com>
> Subject: Re: [R] How do I "normalise" a power spectral density
> 	analysis?
> To: Tom C Cameron <bgytcc at leeds.ac.uk>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <43DEC156.2090900 at pdf.com>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> 	  Since I have not seen a reply to this post, I will 
> offer a comment, 
> even though I have not used spectral analysis myself and 
> therefore have 
> you intuition about it.  First, from the definitions I read in the 
> results from, e.g., RSiteSearch("time series power spectral density") 
> [e.g., 
> http://finzi.psych.upenn.edu/R/library/GeneTS/html/periodogram
> .html] and 
> "spectral analysis" in Venables and Ripley (2002) Modern Applied 
> Statistics with S (Springer), I see no reason why you 
> couldn't plot the 
> spectrum vs. the period rather than the frequency.  Someone else may 
> help us understand why it is usually plotted vs. the frequency;  I'd 
> guess that the standard plot looks more like the integrand in the 
> standard Fourier inversion formula, but I'm not sure.
> 
> 	  If you'd like more help from this listserve, you 
> might briefly 
> describe the problem you are trying to solve, why you think spectral 
> analysis analysis should help, and include a toy example with some 
> self-contained R code to illustrate what you tried and what you don't 
> understand about it.  (And PLEASE do read the posting guide! 
> "www.R-project.org/posting-guide.html".  Nothing is certain but 
> following that posting guide will, I believe, tend to 
> increase the speed 
> and utility of response.)
> 
> 	  hope this helps.
> 	  spencer graves
> 
> Tom C Cameron wrote:
> 
> > Hi everyone
> > 
> > Can anyone tell me how I normalise a power spectral density 
> (PSD) plot of a
> > periodical time-series. At present I get the graphical 
> output of spectrum VS
> > frequency.
> > 
> > What I want to acheive is period VS spectrum? Are these the 
> same things but the
> > x-axis scale needs transformed ?
> > 
> > Any help would be greatly appreciated
> > 
> > Tom
> > 
> ..............................................................
> .............
> > Dr Tom C Cameron                        office: 0113 34 
> 32837 (10.23 Miall)
> > Ecology & Evolution Res. Group.         lab: 0113 34 32884 
> (10.20 Miall)
> > School of Biological Sciences           Mobile: 07966160266
> > University of Leeds                     email: 
> t.c.cameron at leeds.ac.uk
> > Leeds LS2 9JT
> > LS2 9JT
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ifloresc at stevens.edu  Tue Jan 31 22:09:54 2006
From: ifloresc at stevens.edu (Ionut Florescu)
Date: Tue, 31 Jan 2006 16:09:54 -0500
Subject: [R] Help! What does this R command mean?
In-Reply-To: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
References: <b1f16d9d0601292044y37dabc37j94dabc39c6b3e311@mail.gmail.com>
Message-ID: <43DFD222.40504@stevens.edu>

a:b means - all the element in the vector from a to b
a[,-1] means for the matrix a keep all the rows but not the last or 
first -can't remember column.
When in doubt do what I do make a small matrix and apply the command see 
what it does.
After this:
 > a=matrix(c(1:9),3,3)
 > a[,-1]
     [,1] [,2]
[1,]    4    7
[2,]    5    8
[3,]    6    9

you see that -1 eliminates the first column.
I found the R manual useless myself.
The only thing useful is the search function in the html help. That has 
examples.

Ionut Florescu

Michael wrote:
> Hi all,
>
> R is so difficult. I am so desperate.
>
> What does the ":" mean in the following statement?
>
> What does the "[, -1]" mean?
>
>   
>> # Leaps takes a design matrix as argument: throw away the intercept
>> # column or leaps will complain
>>
>> X <- model.matrix(lm(V ~ I + D + W +G:I + P + N, election.table))[,-1]
>>     
>
> Thanks a lot!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ggrothendieck at gmail.com  Tue Jan 31 22:44:21 2006
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 Jan 2006 16:44:21 -0500
Subject: [R] How do I "normalise" a power spectral density
In-Reply-To: <200601312102.k0VL29fS003509@hypatia.math.ethz.ch>
References: <200601312102.k0VL29fS003509@hypatia.math.ethz.ch>
Message-ID: <971536df0601311344i1cc6428fu7a4a01e853d53829@mail.gmail.com>

This post has some details:

http://tolstoy.newcastle.edu.au/~rking/R/help/04/10/5581.html

On 1/31/06, Leif Kirschenbaum <leif at reflectivity.com> wrote:
> I have done a fair bit of spectral analysis, and hadn't finished collecting my thoughts for a reply, so hadn't replied yet.
>
> What exactly do you mean by normalize?
>  I have not used the functons periodogram or spectrum, however from the description for periodogram it appears that it returns the spectral density, which is already normalized by frequency, so you don't have to worry about changing the appearance of your periodogram or power spectrum if you change the time intervals of your data. With a normal Fourier Transform, not only do you need to complex square the terms but you also need to divide by a normalizing factor to give power-per-frequency-bin, which the periodogram appears to do.
>
>  However, if you look in various textbooks, the definition of the Fourier Transform (FT) varies from author to author in the magnitude of the prepended scaling factor. Since the periodogram is related to the FT (periodogram ultimately uses the function mvfft), without examination of the code for periodogram you cannot know the scaling factor, which is almost always one of the following:
>  1.0
>  1/2
>  1/(2*pi)
>  SQRT(1/2)
>  SQRT(1/(2*pi))
>  In fact, if you obtain an FT (or FFT or DFT) from a piece of electronics (say an electronic spectrum analyzer), the prepending factor can vary from manufacturer to manufacturer.
>  Fortunately, there is a strict relationship between the variance of your signal and the integrated spectral density. If your time signal is x(t), the spectral density is S(f), and fc = frequency(x)/2 the Nyquist cutoff frequency, then this may be expressed as:
>
>  variance(x(t)) = constant * {Integral from 0 to fc of S(f)}
>
> In R-code: let x be your time series, and "constant" be the unknown scaling factor (1/2, 1/2pi, etc.)
>  p <- periodogram(x)
>  var(x) == constant * sum(p[[1]])/length(p[[1]])
>
> Or:
>  constant = sum(p[[1]])/length(p[[1]])/var(x)
>
> and we find that the appropriate scaling constant is 1.0.
>
>  As regards plotting versus period, the periodogram returns arrays of spectral amplitude and frequencies. The frequencies are in inverse units of the intervals of your time series. i.e. if your time series is 1-point per day, then the frequencies are in 1/day units. Thus if you wish to plot amplitude versus period in weeks you have a little math to do.
>  I believe that plotting is usually versus frequency since most observers are interested in how things vary versus frequency: multiple evenly spaced peaks on a linear frequency scale indicates the presence of harmonics; this is not so simply seen in a plot versus period. ex. peaks of 10 Hz, 20 Hz, 30 Hz, 40 Hz,... in period would be at periods of 100 ms, 50 ms, 25 ms, 12.5 ms, and the peaks are not evenly spaced.
>  Additionally, there are all kinds of typical responses versus frequency (1/f, 1/f^2) which are clearly seen in plots versus frequency as straight lines (log power vs log frequency), but would come out as curves in plots versus period.
>  I can see how ecological studies may indeed be more interested in the periods.
>
>  However, I would be wary of using the periodogram function, for if I calculate periodograms of the same sinewave but for differing lengths of the sample, the spectral density does not come out the same. All 4 of the plots produced by the code below should overlay, and yet as the time series becomes longer there appears to be an increasing offset of the magnitudes returned. (black-brown-blue-red)
>
> x0<-ts(data=sin(2*pi*1.1*(1:1000)/10),frequency=10)
> p0<-periodogram(x0)
> var(x0)
>
> x1<-ts(data=sin(2*pi*1.1*(1:10000)/10),frequency=10)
> p1<-periodogram(x1)
> var(x1)
>
> x2<-ts(data=sin(2*pi*1.1*(1:100000)/10),frequency=10)
> p2<-periodogram(x2)
> var(x2)
>
> x3<-ts(data=sin(2*pi*1.1*(1:1000000)/10),frequency=10)
> p3<-periodogram(x3)
> var(x3)
>
> plot(p3[[2]],p3[[1]],col="red",type="p",pch=19,cex=0.05,log="y",
>        xlim=c(0.1,0.12),ylim=c(1e-30,1e-15))
> points(p2[[2]],p2[[1]],type="l",col="blue")
> points(p1[[2]],p1[[1]],type="l",col="brown")
> points(p0[[2]],p0[[1]],type="o",col="black")
>
>
> > Message: 113
> > Date: Mon, 30 Jan 2006 17:45:58 -0800
> > From: Spencer Graves <spencer.graves at pdf.com>
> > Subject: Re: [R] How do I "normalise" a power spectral density
> >       analysis?
> > To: Tom C Cameron <bgytcc at leeds.ac.uk>
> > Cc: r-help at stat.math.ethz.ch
> > Message-ID: <43DEC156.2090900 at pdf.com>
> > Content-Type: text/plain; charset=us-ascii; format=flowed
> >
> >         Since I have not seen a reply to this post, I will
> > offer a comment,
> > even though I have not used spectral analysis myself and
> > therefore have
> > you intuition about it.  First, from the definitions I read in the
> > results from, e.g., RSiteSearch("time series power spectral density")
> > [e.g.,
> > http://finzi.psych.upenn.edu/R/library/GeneTS/html/periodogram
> > .html] and
> > "spectral analysis" in Venables and Ripley (2002) Modern Applied
> > Statistics with S (Springer), I see no reason why you
> > couldn't plot the
> > spectrum vs. the period rather than the frequency.  Someone else may
> > help us understand why it is usually plotted vs. the frequency;  I'd
> > guess that the standard plot looks more like the integrand in the
> > standard Fourier inversion formula, but I'm not sure.
> >
> >         If you'd like more help from this listserve, you
> > might briefly
> > describe the problem you are trying to solve, why you think spectral
> > analysis analysis should help, and include a toy example with some
> > self-contained R code to illustrate what you tried and what you don't
> > understand about it.  (And PLEASE do read the posting guide!
> > "www.R-project.org/posting-guide.html".  Nothing is certain but
> > following that posting guide will, I believe, tend to
> > increase the speed
> > and utility of response.)
> >
> >         hope this helps.
> >         spencer graves
> >
> > Tom C Cameron wrote:
> >
> > > Hi everyone
> > >
> > > Can anyone tell me how I normalise a power spectral density
> > (PSD) plot of a
> > > periodical time-series. At present I get the graphical
> > output of spectrum VS
> > > frequency.
> > >
> > > What I want to acheive is period VS spectrum? Are these the
> > same things but the
> > > x-axis scale needs transformed ?
> > >
> > > Any help would be greatly appreciated
> > >
> > > Tom
> > >
> > ..............................................................
> > .............
> > > Dr Tom C Cameron                        office: 0113 34
> > 32837 (10.23 Miall)
> > > Ecology & Evolution Res. Group.         lab: 0113 34 32884
> > (10.20 Miall)
> > > School of Biological Sciences           Mobile: 07966160266
> > > University of Leeds                     email:
> > t.c.cameron at leeds.ac.uk
> > > Leeds LS2 9JT
> > > LS2 9JT
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jan 31 22:44:49 2006
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 31 Jan 2006 16:44:49 -0500
Subject: [R] Help! What does this R command mean?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFAFED79A@usctmx1106.merck.com>

":" in a formula is not the same as ":" otherwise!

Andy

From: Ionut Florescu
> 
> a:b means - all the element in the vector from a to b
> a[,-1] means for the matrix a keep all the rows but not the last or 
> first -can't remember column.
> When in doubt do what I do make a small matrix and apply the 
> command see 
> what it does.
> After this:
>  > a=matrix(c(1:9),3,3)
>  > a[,-1]
>      [,1] [,2]
> [1,]    4    7
> [2,]    5    8
> [3,]    6    9
> 
> you see that -1 eliminates the first column.
> I found the R manual useless myself.
> The only thing useful is the search function in the html 
> help. That has 
> examples.
> 
> Ionut Florescu
> 
> Michael wrote:
> > Hi all,
> >
> > R is so difficult. I am so desperate.
> >
> > What does the ":" mean in the following statement?
> >
> > What does the "[, -1]" mean?
> >
> >   
> >> # Leaps takes a design matrix as argument: throw away the intercept
> >> # column or leaps will complain
> >>
> >> X <- model.matrix(lm(V ~ I + D + W +G:I + P + N, 
> election.table))[,-1]
> >>     
> >
> > Thanks a lot!
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From moreyr at missouri.edu  Tue Jan 31 23:48:29 2006
From: moreyr at missouri.edu (Morey, Richard D (UMC-Student))
Date: Tue, 31 Jan 2006 16:48:29 -0600
Subject: [R] approximation to ln \Phi(x)
Message-ID: <88BEBEB4755E5D4BBEAE3A1ED85B1603022F698D@UM-EMAIL06.um.umsystem.edu>

I am using pnorm() with the log.p=T argument to get approximations to ln \Phi(x) and qnorm with the log.p=T argument to get estimates of \Phi^{-1}(exp(x)). What approximations are used in these two functions (I noticed in the source pnorm.c it doesn't look like Abramowitz and Stegen) and where can I find the citation?

Thanks,
Richard Morey



From dmbates at gmail.com  Tue Jan 31 23:56:08 2006
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 31 Jan 2006 16:56:08 -0600
Subject: [R] Mixed-effects models / heterogeneous covariances
In-Reply-To: <2e38a1c80601311022i2e1be92doa60b80b50b69eb0c@mail.gmail.com>
References: <2e38a1c80601311022i2e1be92doa60b80b50b69eb0c@mail.gmail.com>
Message-ID: <40e66e0b0601311456r57850ec9k1a9ca2164d46548a@mail.gmail.com>

On 1/31/06, Lutz Ph. Breitling <lutz.breitling at gmail.com> wrote:
> Dear R-list,
>
> maybe someone can help me with the following mixed-effects models
> problem, as I am unable to figure it out with the 'nlme-bible'.
>
> I would like to fit (in R, obviously) a so-called animal model (google
> e. g. "Heritability and genetic constraints of life-history" by Pettay
> et al.) to estimate the variance component that is due to genetic
> effects. The covariances of the genetic random effects between
> observations are given by the different degrees of relatedness between
> the individuals examined. (I find it difficult to explain, but Pettay
> et al. describe it nicely in their methods section...)
>
> Is there any straight-forward way to fit such a model with R? I first
> thought I could handle it somehow with nlme's correlation structures,
> but these within-group structures are quite a different thing, right?

Sorry to say, yes they are quite a different thing.

I am aware of  models like the animal model and the sire model in
animal breeding.  A student in our Animal Sciences Department, Ana
In??s V??zquez Saravia, is working with me on developing extensions to
the lmer function to handle such models.  The actual calculations are
not extraordinarily difficult - the difficulty is in deciding how to
specify the model and in massaging the data to convert the model
specification to model matrices.

The model specification for an lmer model assumes that each predicted
response is affected by one and only one random effects vector
associated with each of the grouping factors.  That is, the random
effects have only an instantaneous effect and there is no "carry-over"
of random effects from other levels of the grouping factor.  This is
not the case for the animal model or for the sire model.  A given
predicted response is affects by the random effects for each of the
ancestors of the animal on which the observation is made.  The "no
carry-over" assumption is also violated in longitudinal "value-added"
models for student achievement where the effect of a teacher in a
given year can carry over to subsequent years. J.R. Lockwood and
Harold Doran are very interested in these models.

All of these are important practical models but, as I said, it is
tricky to decide how to specify the model in these cases.



