From eairoldi at stat.cmu.edu  Sun Jun  1 00:15:08 2003
From: eairoldi at stat.cmu.edu (Edoardo M Airoldi)
Date: Sat, 31 May 2003 18:15:08 -0400 (EDT)
Subject: [R] logistic regression
Message-ID: <Pine.LNX.4.44.0305311735010.1797-100000@hydra4.stat.cmu.edu>

hi all,
 I am fitting a logistic regression model on binary data.  I care about 
the fitted probabilities, so I am not worried about infinite 
(or non-existent) MLEs.  I use:

> glm(Y~., data=X, weights=wgt, family=binomial(link=logit), maxit=250)

 I understand the three ways to fit model, and in my case Y is a factor,
one column 

> Y <- c(rep("A",679), rep("B",38))

  My question is about the weights.  I can use integer weights, which
makes more mathematical sense, and 

> wgt <- c(rep(1,679), rep(17,38)) 

or i can use

> wgt <- c(rep(38/679,679, rep(1,38))

which makes more sense for my problem, but the mathematic is weak as I am
using non integer successes in a bernoulli...  Since non-integer weights 
make more sense, AND the predictions of my model actually get better on 
the rare class.  I estimate the accuracy 'out of the bag' over 10000 
experiments to get

          | integer wgt          | non-int wgt
 -------- + -------------------- + --------------------
 accuracy | A = 94.9%  B = 82.3% | A = 94.7%  B = 83.3%
 std.dev. |      2.3%      15.4% |      2.6%      13.2%
 avg. AIC | 707                  | 124

 As I understand instead of augmenting the successes on the rare class, 
which I did not observe, I am sinply down-weighting the successes on the 
populus class.  The populations can be thought as equal, and only the 
sample sizes are unbalanced.
 I was hoping that the continuity of the Binomial for N in [0,1] ans X in 
[0,1] could guarantee me that my results still make sense, but I am not 
sure.  Any thoughts?  Thanks

Edo



From eairoldi at stat.cmu.edu  Sun Jun  1 00:21:54 2003
From: eairoldi at stat.cmu.edu (Edoardo M Airoldi)
Date: Sat, 31 May 2003 18:21:54 -0400 (EDT)
Subject: [R] logistic regression (weights)
Message-ID: <Pine.LNX.4.44.0305311817060.6137-100000@hydra4.stat.cmu.edu>

hi all,
 I am fitting a logistic regression model on binary data.  I care about 
the fitted probabilities, so I am not worried about infinite 
(or non-existent) MLEs.  I use:

> glm(Y~., data=X, weights=wgt, family=binomial(link=logit), maxit=250)

 I understand the three ways to fit model, and in my case Y is a factor,
one column 

> Y <- c(rep("A",679), rep("B",38))
> Y <- as.factor(Y)

  My question is about the weights.  I can use integer weights, which
makes more mathematical sense, and 

> wgt <- c(rep(1,679), rep(17,38)) 

or i can use

> wgt <- c(rep(38/679,679, rep(1,38))

which makes more sense for my problem, but the mathematic is weak as I am
using non integer successes in a bernoulli...  I estimate the accuracy
'out of the bag' over 10000 experiments to get

          | integer wgt          | non-int wgt
 -------- + -------------------- + --------------------
 accuracy | A = 94.9%  B = 82.3% | A = 94.7%  B = 83.3%
 std.dev. |      2.3%      15.4% |      2.6%      13.2%
 avg. AIC | 707                  | 124

 As I understand, non-integer weights are more respectful of what I
observe since instead of augmenting the successes on the rare class, which
I did not observe, they simply down-weight the successes on the populus
class.  The populations can be thought as equal, and only the sample sizes
are unbalanced.
 Predictions also look better, so I was hoping that the continuity of the
Binomial for N in [0,1] ans X in [0,1] could guarantee me that my results
still make sense, but I am not sure.  Any thoughts?
Thanks

Edo



From ggrothendieck at volcanomail.com  Sun Jun  1 01:38:43 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Sat, 31 May 2003 16:38:43 -0700 (PDT)
Subject: [R] function to populate a matrix based on a lookup to
	another matrix ?
Message-ID: <20030531233843.5E3BCABC5@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030531/6d693589/attachment.pl

From pfleonard at hotmail.com  Sun Jun  1 02:58:33 2003
From: pfleonard at hotmail.com (peter leonard)
Date: Sat, 31 May 2003 17:58:33 -0700
Subject: [R] function to populate a matrix based on a lookup to another
	matrix ?
Message-ID: <Law11-F36SIrwM4Id660005984d@hotmail.com>


That worked perfect. Thanks for the fast response.

Peter Leonard


>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: peter leonard <pfleonard at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] function to populate a matrix based on a lookup to another 
>matrix ?
>Date: Sat, 31 May 2003 17:58:27 +0200
>
>peter leonard wrote:
>>Hi,
>>
>>This is a beginner R question.
>>
>>I have a  4x4 matrix  named 'lookup' with the following values:
>>
>>            1         2               3          4
>>1 0.000000 2.828427 5.656854 8.485281
>>2 2.828427 0.000000 2.828427 5.656854
>>3 5.656854 2.828427 0.000000 2.828427
>>4 8.485281 5.656854 2.828427 0.000000
>>
>>I then create a new empty matrix named 'dd' with specfic row and col names 
>>:
>>
>>   1   3   4   3    3   1
>>1 NA NA NA NA NA NA
>>2 NA NA NA NA NA NA
>>3 NA NA NA NA NA NA
>>4 NA NA NA NA NA NA
>>3 NA NA NA NA NA NA
>>2 NA NA NA NA NA NA
>>1 NA NA NA NA NA NA
>>
>>I want to be able populate the cells in 'dd' using 'lookup' based on the 
>>specified rownames and colnames of 'dd'. For example, the cell in 'dd' 
>>where the rowname =2 and the colname = 1 should be assigned the value 
>>2.828427 .
>>
>>I've tried several ways of doing this with the apply function  but without 
>>success. I can do it with a for loop but I want to avoid that for 
>>efficiency reasons.
>>
>>After running the function, as an example, the first column of 'dd' should 
>>look like this :
>>
>>             1
>>1 0.000000000
>>2 2.828427125
>>3 5.656854249
>>4 8.485281374
>>3 5.656854249
>>2 2.828427125
>>1 0.000000000
>>
>>Can anyone please help me identify the required function or an alternative 
>>way of achieving the same result? Hopefully this is simple and I'm just 
>>not seeing it.
>>
>>Thanks
>>Peter
>>
>
>  dd <- lookup[as.numeric(rownames(dd)), as.numeric(colnames(dd))]
>and after that restoring dd's row- and colnames.
>
>Anyway, I guess you don't need to create "dd". Just calculate those 
>rownames (rn) and colnames (cn) as integers. Then the following works:
>
>  dd <- lookup[rn, cn]
>
>Uwe Ligges
>



From christoph.lehmann at gmx.ch  Sun Jun  1 09:24:04 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sun, 01 Jun 2003 07:24:04 -0000
Subject: [R] compositional data: percent values sum up to 1
Message-ID: <1054452185.1213.4.camel@christophl>

again, under another subject:
sorry, maybe an all too trivial question. But we have power data from J
frequency spectra and to have the same range for the data of all our
subjects, we just transformed them into % values, pseudo-code:

power[i,j]=power[i,j]/sum(power[i,1:J])

of course, now we have a perfect linear relationship in our x design-matrix,
since all power-values for each subject sum up to 1.

How shall we solve this problem: just eliminate one column of x, or
introduce a restriction which says exactly that our power data sum up to
1 for each subject?

Thanks a lot

Christoph
-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From chunlou at yahoo.com  Sun Jun  1 15:07:31 2003
From: chunlou at yahoo.com (Chunlou Yung)
Date: Sun, 1 Jun 2003 09:07:31 -0400
Subject: [R] RDCOMServer
Message-ID: <NCBBKDNFIKJKKCFELNNMMEKEDGAA.chunlou@yahoo.com>

Hi there,

I downloaded RDCOMServer and the related packages from Omegahat. Tried their
"R.Evaluator" example. But failed to invoke any method from the client side.
When I looked at R.Evaluator from OLE View (some Microsoft development tool
for browsing all the OLEs), I got the following error message from
R.Evaluator:

CoGetClassObject failed
Class not registered
REGDB_E_CLASSNOTREG ($80040154)

RDCOMServer's isRegistered method said R.Evaluator was registered. So,
what's wrong?

Thanks.



From tlumley at u.washington.edu  Sun Jun  1 17:42:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 1 Jun 2003 08:42:20 -0700 (PDT)
Subject: [R] parse on left hand side of R assignment
In-Reply-To: <3ED8DEEB.5090808@ku.edu>
Message-ID: <Pine.A41.4.44.0306010835510.101876-100000@homer07.u.washington.edu>

On Sat, 31 May 2003, Paul E. Johnson wrote:

> I keep finding myself in a situation where I want to calculate a
> variable name and then use it on the left hand side of an assignment.
> For example
>
> iteration <- 1
> varName <- paste("run",iteration,sep="")
> myList$parse(text=varName) <- aColumn
>

In this particular case you can just use

myList[[varName]]<-aColumn.

I don't think you can (easily) use assign, despite the suggestion -- it
doesn't dispatch assignment methods.

If you wanted to do something of this sort for which the above didn't work
you could also learn about substitute()
  eval(substitute(myList$newColumn<-aColumn),
     list(newColumn=as.name(varName)))
as an alternative to parse().

	-thomas



From tura at centroin.com.br  Sun Jun  1 18:17:03 2003
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sun, 01 Jun 2003 13:17:03 -0300
Subject: [R] Help with Temporal series
In-Reply-To: <1054452185.1213.4.camel@christophl>
Message-ID: <5.1.0.14.2.20030601065935.00a56090@centroin.com.br>



Good afternoon R-masters,      
      
I am with some doubts in the R, see the script below:

m<-c(69.6,67.3,75.6,74.3,64.7,60,65.7,62.5,66.5)
d<-c(11.6,15,17.8,18.3,11.2,11,4.6,5.8,7)
year<-c(1994,1995,1996,1997,1998,1999,2000,2001,2002)
male<-ts(m,start=c(1994))
death<-ts(d,start=c(1994))
data<-data.frame(year,death,male)
require(ts)
d100<-HoltWinters(data$death,gamma=0)
m100<-HoltWinters(data$male,gamma=0)
par(mfrow=c(3,1))
plot(d100,main="Death")
plot(m100,main="Male")
ccf(male,death)

    
I have 2 doubts:    
    
1 - How to I should interpret the third graph?    
2 - Has a hypothesis test to evaluate the cross-correlation it is significant in R?
Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil



From Timur.Elzhov at jinr.ru  Sun Jun  1 18:39:17 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Sun, 1 Jun 2003 20:39:17 +0400
Subject: [R] persp & colors
Message-ID: <20030601163917.GA19993@pcf004.jinr.ru>

Dear R experts!

I use image() & persp() functions for color plotting z(x,y)-type
graphics.  In image() colors correspond to z-values, that's what
I want.   OTOH, in persp() the col option means:

     col: the color(s) of the surface facets.  Transparent colours are
          ignored.  This is recycled to the (nx-1)(ny-1) facets.

but I'd like to persp()' colors behave like in image() function!

Could you help me to solve this problem?
Thank you very much.


--
WBR,
Timur.



From ligges at statistik.uni-dortmund.de  Sun Jun  1 18:53:55 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 01 Jun 2003 18:53:55 +0200
Subject: [R] persp & colors
In-Reply-To: <20030601163917.GA19993@pcf004.jinr.ru>
References: <20030601163917.GA19993@pcf004.jinr.ru>
Message-ID: <3EDA2FA3.8020207@statistik.uni-dortmund.de>

Timur Elzhov wrote:
> Dear R experts!
> 
> I use image() & persp() functions for color plotting z(x,y)-type
> graphics.  In image() colors correspond to z-values, that's what
> I want.   OTOH, in persp() the col option means:
> 
>      col: the color(s) of the surface facets.  Transparent colours are
>           ignored.  This is recycled to the (nx-1)(ny-1) facets.
> 
> but I'd like to persp()' colors behave like in image() function!
> 
> Could you help me to solve this problem?
> Thank you very much.
> 
> 
> --
> WBR,
> Timur.

That's not easy, because you have to redefine x, y and z values.

Simple example:

  x <- y <- 1:2
  z <- matrix(1:4, 2)
  image(x, y, z)    # OK, quite nice

but

  persp(x, y, z)

has only one facet. So the only way is to calculate the 9 values for x, 
y, and z to get the corners for the 4 facets in it.
That's easy for x and y, but can be impossible for z...

Uwe Ligges



From Timur.Elzhov at jinr.ru  Sun Jun  1 19:21:10 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Sun, 1 Jun 2003 21:21:10 +0400
Subject: [R] persp & colors
In-Reply-To: <3EDA2FA3.8020207@statistik.uni-dortmund.de>
References: <20030601163917.GA19993@pcf004.jinr.ru>
	<3EDA2FA3.8020207@statistik.uni-dortmund.de>
Message-ID: <20030601172109.GA20216@pcf004.jinr.ru>

On Sun, Jun 01, 2003 at 06:53:55PM +0200, Uwe Ligges wrote:

>> but I'd like to persp()' colors behave like in image() function!

> That's not easy, because you have to redefine x, y and z values.
> 
> Simple example:
> 
>  x <- y <- 1:2
>  z <- matrix(1:4, 2)
>  image(x, y, z)    # OK, quite nice
> 
> but
> 
>  persp(x, y, z)
> 
> has only one facet. So the only way is to calculate the 9 values for x, 
> y, and z to get the corners for the 4 facets in it.
> That's easy for x and y, but can be impossible for z...

OK, thank you for answer!
But, I saw that other mathematic frameworks (CERN ROOT for instance)
can plot 3D surfaces with colors corresponding to z-value.
Is there way to do this in R (with another functions/packages)?
It's not necessary to use _one_ color per facet, yes?.. :)


--
WBR,
Timur.



From ririzarr at jhsph.edu  Sun Jun  1 19:20:51 2003
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Sun, 01 Jun 2003 13:20:51 -0400 (EDT)
Subject: [R] integrate
Message-ID: <Pine.GSO.4.10.10306011213240.28951-100000@athena.biostat.jhsph.edu>

Im tryng to understand an error i get with integrate. this is 1.7.0 on
solaris 2.8.

##i am trying to approximate an integral of this function,
f<-function(b) exp(-(b-mu)^2/(2*tau2))/(p-exp(b))*10^6

##with 
tau2 <- .005;mu <- 7.96;p <- 2000
##from -inf to different upper limits. using
integrate(f,-Inf,log(p-exp(1)))
##i get the following error:
##Error in integrate(f, -Inf, log(p - exp(1))) : 
##	the integral is probably divergent

##whats confusing is that i only get this error when i use upper limit 
##in the range [log(p-2),log(p-1)]
##try this:
x <- seq(1,10,.1)
y <- sapply(x,function(i){
  x <- try(integrate(f,-Inf,log(p-i)),silent=TRUE)
  if(class(x)=="try-error") return(NA) else return(x$val)
})
x[is.na(y)]

##if i use stop=FALSE, the curve is interpolated nicely
plot(x,y)
y <- sapply(x,function(i)
  x <- integrate(f,-Inf,log(p-i),stop=FALSE)$val
)
lines(x,y)

##there doesnt appear to be anything strange about the function in 
##the 2-3 region.
x <- seq(1,5,.1)
xx <- log(p-x)
plot(x,f(xx))
abline(v=c(2,3))

##any ideas on why this is happening? 

##ps - same thing happens with:
##f <- function(b) exp(-(b-mu)^2/(2*tau2) - log(p-exp(b)))*10^6
##it doesnt with
##f<-function(b) exp(-(b-mu)^2/(2*tau2))/(p-exp(b))



From spencer.graves at pdf.com  Sun Jun  1 21:01:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 01 Jun 2003 12:01:47 -0700
Subject: [R] compositional data: percent values sum up to 1
References: <1054452185.1213.4.camel@christophl>
Message-ID: <3EDA4D9B.4050609@pdf.com>

What are you trying to do?  What I would do with this depends on many 
factors.

spencer graves

Christoph Lehmann wrote:
> again, under another subject:
> sorry, maybe an all too trivial question. But we have power data from J
> frequency spectra and to have the same range for the data of all our
> subjects, we just transformed them into % values, pseudo-code:
> 
> power[i,j]=power[i,j]/sum(power[i,1:J])
> 
> of course, now we have a perfect linear relationship in our x design-matrix,
> since all power-values for each subject sum up to 1.
> 
> How shall we solve this problem: just eliminate one column of x, or
> introduce a restriction which says exactly that our power data sum up to
> 1 for each subject?
> 
> Thanks a lot
> 
> Christoph



From mentus at gmx.de  Mon Jun  2 01:22:09 2003
From: mentus at gmx.de (Fernando Henrique Ferraz Pereira da Rosa)
Date: Mon, 2 Jun 2003 01:22:09 +0200 (MEST)
Subject: [R] Simulating a variable following an arbitrary distribution
Message-ID: <8531.1054509729@www61.gmx.net>

     Hi, I'd like to know if there's anything in R that could help me do
that. Let's suppose I have a density function of a random variable, for example
f(x) = (x^3)/4 0 < x < 2 and I would like to simulate it. For the common
distributions (exponencial, gamma, cauchy) there are the r-functions (rgamma,
rexp, runif, rcauchy, and so on).. But when the variable I want to simulate is
not one of those, how should I procede? I read some references on the subject
and saw that there are some algorithms that can do that, but I just wonder if
there is any implemented in R?

Thank you,

--



From edgar at cs.uprm.edu  Mon Jun  2 01:56:33 2003
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Sun, 1 Jun 2003 19:56:33 -0400 (EDT)
Subject: [R] Simulating a variable following an arbitrary distribution
In-Reply-To: <8531.1054509729@www61.gmx.net>
Message-ID: <Pine.GSO.4.33.0306011951320.7437-100000@cs.uprm.edu>

Hi,
Use the Inverse transformation method. See any basic Cbook in simulation
for instance Sheldon Ross's book.
Regards,
Edgar

On Mon, 2 Jun 2003, Fernando Henrique Ferraz Pereira da Rosa wrote:

>      Hi, I'd like to know if there's anything in R that could help me do
> that. Let's suppose I have a density function of a random variable, for example
> f(x) = (x^3)/4 0 < x < 2 and I would like to simulate it. For the common
> distributions (exponencial, gamma, cauchy) there are the r-functions (rgamma,
> rexp, runif, rcauchy, and so on).. But when the variable I want to simulate is
> not one of those, how should I procede? I read some references on the subject
> and saw that there are some algorithms that can do that, but I just wonder if
> there is any implemented in R?
>
> Thank you,
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Mon Jun  2 03:48:11 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 01 Jun 2003 18:48:11 -0700
Subject: [R] Simulating a variable following an arbitrary distribution
References: <Pine.GSO.4.33.0306011951320.7437-100000@cs.uprm.edu>
Message-ID: <3EDAACDB.8000205@pdf.com>

	  Your specific example is a scaled beta.  Therefore, "2*rbeta(1000, 4, 
1)" will generate 1000 random numbers according to that distribution. 
You can get the same distribution from "2*qbeta(runif(1000), 4, 1)".
	  We can generalize this last example to any case of interest.  Example:

df4 <- function(x)(x^3)/4
pf4 <- function(q)(q^4)/16
qf4 <- function(p)(16*p)^0.25
rf4 <- function(n)qf4(runif(n))

Then "rf4(1000)" will produce 1000 pseudo-random deviates following this 
distribution.

hth.  spencer graves

Edgar Acuna wrote:
> Hi,
> Use the Inverse transformation method. See any basic Cbook in simulation
> for instance Sheldon Ross's book.
> Regards,
> Edgar
> 
> On Mon, 2 Jun 2003, Fernando Henrique Ferraz Pereira da Rosa wrote:
> 
> 
>>     Hi, I'd like to know if there's anything in R that could help me do
>>that. Let's suppose I have a density function of a random variable, for example
>>f(x) = (x^3)/4 0 < x < 2 and I would like to simulate it. For the common
>>distributions (exponencial, gamma, cauchy) there are the r-functions (rgamma,
>>rexp, runif, rcauchy, and so on).. But when the variable I want to simulate is
>>not one of those, how should I procede? I read some references on the subject
>>and saw that there are some algorithms that can do that, but I just wonder if
>>there is any implemented in R?
>>
>>Thank you,
>>
>>--
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From support at skillsoft.com  Mon Jun  2 04:38:08 2003
From: support at skillsoft.com (support@skillsoft.com)
Date: Sun, 1 Jun 2003 23:38:08 -0300
Subject: [R] Re:   Re: Submited (004756-3463)  #DF/010989#
Message-ID: <OF71C75278.99176385-ON84256D39.000E7A47-84256D39.000E7A47@amr.smtf.ds>

This is an automated response.  SkillSoft Support has
received your message and this issue has been assigned
ticket number :  DF/010989

Please ensure that  #DF/010989#  appears in the subject
line of any further emails on this issue so that we can
identify the original issue, and you are not allocated
a new ticket number.

Thank You.



From jc at or.psychology.dal.ca  Mon Jun  2 04:53:08 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Sun, 1 Jun 2003 23:53:08 -0300
Subject: [R] extending subsets over another variable..
Message-ID: <574B969E-94A5-11D7-ACEF-000A9566473A@or.psychology.dal.ca>

Hi,
	I've been trying to learn and use R for data analysis, and so far it 
has be OK.  It is very slow with the aov command.  But, other than that 
things are generally OK.	Anyway, todays problem is one that I have 
looked in quite a few places to try to solve but to know avail.  I hope 
the list can help.
	I have my data organized in the appropriate way (i.e. a data point on 
each row and columns for each variable).  I want to divide my subjects 
up by a median.  However, I don't want the overall subject median but a 
subset, and then to split the entire subjects data on that subset.  I 
don't see a way to do that at all.  Any help would be appreciated.

to make things clearer, here is an example of how to split subject by 
median

ss<-split(s, ave(s$x, s$subj) > median(ave(s$x,s$subj)))

what I need to do is split by subject when s is further split by a sub 
factor.

TIA
John Christie



From xthua111 at sohu.com  Sun Jun  1 10:02:13 2003
From: xthua111 at sohu.com (=?GB2312?B?zOyyxQ==?=)
Date: Sun, 1 Jun 2003 16:02:13 +0800 (CST)
Subject: [R] want to write a package for R
Message-ID: <1685093.1054454533376.JavaMail.postfix@mx15.mail.sohu.com>

     hello,everyone!
     I have studied in univ. for two years.and my teacher have put out 
some stat. model .and i want to write it to a R package, and take 
it to my future paper.any advise? i learned something about SAS/stat.
and i have read the tutorial of R.
     now i can interactive with R,and
write R file.but i didn't have a contour of how to write a package.
R extension said package is consisted of DESCRIPTION,INDEX,subdirectories
,bundles.should i write all the file?
     thank you.



From christoph.lehmann at gmx.ch  Mon Jun  2 09:23:26 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 02 Jun 2003 09:23:26 +0200
Subject: [R] compositional data: percent values sum up to 1
In-Reply-To: <3EDA4D9B.4050609@pdf.com>
References: <1054452185.1213.4.camel@christophl> <3EDA4D9B.4050609@pdf.com>
Message-ID: <1054538606.4230.1.camel@christophl>

I want to do a logistic regression analysis, and to compare with, a
discriminant analysis. The mentioned power maps are my exogenous data,
the dependent variable (not mentioned so far) is a diagnosis
(ill/healthy)

thanks for the interest and the help

Christoph

On Sun, 2003-06-01 at 21:01, Spencer Graves wrote:
> What are you trying to do?  What I would do with this depends on many 
> factors.
> 
> spencer graves
> 
> Christoph Lehmann wrote:
> > again, under another subject:
> > sorry, maybe an all too trivial question. But we have power data from J
> > frequency spectra and to have the same range for the data of all our
> > subjects, we just transformed them into % values, pseudo-code:
> > 
> > power[i,j]=power[i,j]/sum(power[i,1:J])
> > 
> > of course, now we have a perfect linear relationship in our x design-matrix,
> > since all power-values for each subject sum up to 1.
> > 
> > How shall we solve this problem: just eliminate one column of x, or
> > introduce a restriction which says exactly that our power data sum up to
> > 1 for each subject?
> > 
> > Thanks a lot
> > 
> > Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From postmaster at intel.com  Mon Jun  2 09:48:06 2003
From: postmaster at intel.com (System Administrator)
Date: Mon, 2 Jun 2003 00:48:06 -0700 
Subject: [R] Undeliverable: Re: Submited (004756-3463)
Message-ID: <29E043268B05D611ACE200508BE06FEB1C1AD2D8@fmsmsx019.fm.intel.com>

Your message

  To:      ben.discoe at intel.com
  Subject: Re: Submited (004756-3463)
  Sent:    Mon, 2 Jun 2003 00:48:50 -0700

did not reach the following recipient(s):

ben.discoe at intel.com on Mon, 2 Jun 2003 00:48:02 -0700
    The recipient name is not recognized
    MSEXCH:IMS:Intel:Americas01:FMSMSX019 0 (000C05A6) Unknown Recipient



From otoomet at econ.dk  Mon Jun  2 09:46:17 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Mon, 2 Jun 2003 09:46:17 +0200
Subject: [R] want to write a package for R
In-Reply-To: <1685093.1054454533376.JavaMail.postfix@mx15.mail.sohu.com>
	(xthua111@sohu.com)
References: <1685093.1054454533376.JavaMail.postfix@mx15.mail.sohu.com>
Message-ID: <200306020746.h527kHF07935@punik.econ.au.dk>

Hello,

You should start creating a separate directory for your package
(e.g. foo).  As a minimum you need three things:

1) your R functions (as .R files in directory foo/R)

2) documentation of your functions in .Rd format (look "writing R
   extensions" and function prompt(), they must reside in foo/man directory)

3) The DESCRIPTION file (in the foo directory).  Everything else is
   made by R.

Now run "R CMD build foo" in the directory above (where the subdir foo
is lying).

It was pretty easy to do on linux, with windows you probably need
necessary tools as perl.  Otherwise, it should be similar.

Bundles are just collections of different packages, probably not for
you right now.

Perhaps it helps.

Ott

 | Date: Sun, 1 Jun 2003 16:02:13 +0800 (CST)
 | From: "????" <xthua111 at sohu.com>

 |      hello,everyone!
 |      I have studied in univ. for two years.and my teacher have put out 
 | some stat. model .and i want to write it to a R package, and take 
 | it to my future paper.any advise? i learned something about SAS/stat.
 | and i have read the tutorial of R.
 |      now i can interactive with R,and
 | write R file.but i didn't have a contour of how to write a package.
 | R extension said package is consisted of DESCRIPTION,INDEX,subdirectories
 | ,bundles.should i write all the file?
 |      thank you.



From ivonefig at ipimar.pt  Mon Jun  2 10:40:20 2003
From: ivonefig at ipimar.pt (Ivone Figueiredo)
Date: Mon, 2 Jun 2003 09:40:20 +0100
Subject: [R] Help - Curvature measures of nonlinearity
Message-ID: <000801c328e2$99fc6200$c9040a0a@Ivone1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030602/7a50aefe/attachment.pl

From whitemw at tiscali.co.uk  Mon Jun  2 11:46:32 2003
From: whitemw at tiscali.co.uk (Mike White)
Date: Mon, 2 Jun 2003 10:46:32 +0100
Subject: [R] Rounding problem R vs Excel
Message-ID: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030602/6737887b/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jun  2 08:41:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Jun 2003 08:41:52 +0200
Subject: [R] want to write a package for R
In-Reply-To: <1685093.1054454533376.JavaMail.postfix@mx15.mail.sohu.com>
References: <1685093.1054454533376.JavaMail.postfix@mx15.mail.sohu.com>
Message-ID: <3EDAF1B0.5010904@statistik.uni-dortmund.de>

Ìì²Å wrote:


Hello ??,


>      hello,everyone!
>      I have studied in univ. for two years.and my teacher have put out 
> some stat. model .and i want to write it to a R package, and take 
> it to my future paper.any advise? i learned something about SAS/stat.

SAS/stat, hmmm, not very interesting given you are going to write an *R*
package.


> and i have read the tutorial of R.
>      now i can interactive with R,and
> write R file.but i didn't have a contour of how to write a package.
> R extension said package is consisted of DESCRIPTION,INDEX,subdirectories
> ,bundles.should i write all the file?

Great, you already found the "Writing R Extensions" manual. Indeed, just
do what's descibed in there.

Uwe Ligges

PS: Please configure you mailtool to send plain text messages.



From vincent.stoliaroff at socgen.com  Mon Jun  2 12:22:37 2003
From: vincent.stoliaroff at socgen.com (vincent.stoliaroff@socgen.com)
Date: Mon, 2 Jun 2003 12:22:37 +0200
Subject: [R] authorized characters and symbols
Message-ID: <OF38E2B33E.1125BFDB-ONC1256D39.0038A11F@ges.marc.societe-generale.fr>

Hi R lovers

Obviously I cannot use the underscore "_ " character to name an object in R
Is there a special reason for that ?

I want to use it to rename a function
maybe the problem is due to the nature of the object I work on

thanks for any comments on that very little and not very bothering trouble





*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
titre de ce message s'il a ete altere, deforme ou falsifie.
				********
This message and any attachments (the "message") are confidentia... {{dropped}}



From ripley at stats.ox.ac.uk  Mon Jun  2 12:36:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 11:36:10 +0100 (BST)
Subject: [R] authorized characters and symbols
In-Reply-To: <OF38E2B33E.1125BFDB-ONC1256D39.0038A11F@ges.marc.societe-generale.fr>
Message-ID: <Pine.LNX.4.44.0306021131200.15780-100000@gannet.stats>

On Mon, 2 Jun 2003 vincent.stoliaroff at socgen.com wrote:

> Obviously I cannot use the underscore "_ " character to name an object 
> in R

Actually, you can but it is more trouble than it is worth as you will 
always have to quote the name, and often explicitly get it:

> "my_pi" <- pi
> get("my_pi")
[1] 3.141593

> Is there a special reason for that ?

Yes, it is allowed for assignment, but not for much longer.  Once
it is removed for assignment (1.8.0) it will be allowed as part of a name
after a decent interval.

> I want to use it to rename a function
> maybe the problem is due to the nature of the object I work on
> 
> thanks for any comments on that very little and not very bothering trouble

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From psplojaa at sc.ehu.es  Mon Jun  2 12:34:47 2003
From: psplojaa at sc.ehu.es (Alicia Lopez)
Date: Mon, 02 Jun 2003 12:34:47 +0200
Subject: [R] (no subject)
Message-ID: <3.0.6.32.20030602123447.00b7d158@mailin.sc.ehu.es>

I would be gratefulf if anybody can help me with this problem.  I have not
a experience with R 
language, actually this is my first job with it :

I have write some instructions for acomplish a simulation, and I have 36
conditions 
with 1000 iterations each one . The program runs without problems, but in
the 19 nth 
condition  gives and  error message (after running   some iterations of
this condition)   
is:

"Error in var(x, na.rm = na.rm) : missing observations in cov/cor"

Somebody would give me any clue about the origin of this error?
There is any probability  that the origin of the problem is  the scarcitie
of resources?
(  I don?t understand how is possible a programming error).

Any help will be welcome.




---------------------------------------------------
ALICIA L?PEZ J?UREGUI
 

   DEPARTAMENTO DE PSICOLOGIA SOCIAL Y METODOLOGIA
   FACULTAD DE PSICOLOGIA 
   Avda. de Tolosa, 70
   20018 DONOSTIA (SPAIN)

         TEL.: 943 / 018340   
         E-MAIL: psplojaa at sc.ehu.es



From christmann at statistik.uni-dortmund.de  Mon Jun  2 13:24:51 2003
From: christmann at statistik.uni-dortmund.de (Andreas Christmann)
Date: Mon, 02 Jun 2003 13:24:51 +0200
Subject: [R] Re: R-help Digest, Vol 3,
 Issue 29:  RE: Ordinal Data - regression trees & prop. odds
References: <200305301003.h4UA0nuE012844@hypatia.math.ethz.ch>
Message-ID: <3EDB3403.20505@statistik.uni-dortmund.de>

>    1. RE: Ordinal data - Regression Trees  & Proportional Odds
>       (Liaw, Andy)


> AFAIK there's no implementation (or description) of tree algorithm that
> handles ordinal response.  

Regression trees with an ordinal response variable can be computed with 
SPSS Answer Tree 3.0.

Andreas Christmann
-----------------------------------------------------------------------------
Andreas Christmann
University of Dortmund
Department of Statistics
44221 Dortmund
Germany
-----
Phone: +231 / 755 3180
Email: christmann at statistik.uni-dortmund.de
WWW: 
http://www.statistik.uni-dortmund.de/de/content/einrichtungen/lehrstuehle/datenanalyse.html



From christmann at statistik.uni-dortmund.de  Mon Jun  2 13:26:39 2003
From: christmann at statistik.uni-dortmund.de (Andreas Christmann)
Date: Mon, 02 Jun 2003 13:26:39 +0200
Subject: [R] RE: Ordinal data - Regression Trees  & Proportional Odds
Message-ID: <3EDB346F.8060409@statistik.uni-dortmund.de>

 >>> 1. RE: Ordinal data - Regression Trees  & Proportional Odds
       (Liaw, Andy)

 > AFAIK there's no implementation (or description) of tree algorithm
 > that handles ordinal response.
 >

Regression trees with an ordinal response variable can be computed with 
SPSS Answer Tree 3.0.

Andreas Christmann
-- 
-----------------------------------------------------------------------------
Andreas Christmann
University of Dortmund
Department of Statistics
44221 Dortmund
Germany
-----
Phone: +231 / 755 3180
Email: christmann at statistik.uni-dortmund.de
WWW: 
http://www.statistik.uni-dortmund.de/de/content/einrichtungen/lehrstuehle/datenanalyse.html



From bertola at fastmail.fm  Mon Jun  2 13:43:50 2003
From: bertola at fastmail.fm (Rafael Bertola)
Date: Mon, 02 Jun 2003 08:43:50 -0300
Subject: [R] compose a name in function
Message-ID: <20030602114350.A05CA356CB@www.fastmail.fm>

I write a function to plot some graphs and a gave for it one result of
lm() (reg) and a character (name). In  win.metafile("name-%02d.wmf",
pointsize = 14) i want compose de name of file with the string i pass to
the function. How i can make this?

plota.res.grava <- function(reg,name){

    win.metafile("name-%02d.wmf", pointsize = 14)
...
}

Thanks
R. Bertola
-- 
  
  bertola at fastmail.fm

-- 
http://www.fastmail.fm - And now for something completely different




From rs at bg.ioe.ac.uk  Mon Jun  2 13:55:26 2003
From: rs at bg.ioe.ac.uk (Ricardo Sabates)
Date: Mon, 2 Jun 2003 12:55:26 +0100
Subject: [R] dynamic ordered probit models?
Message-ID: <005601c328fd$e0ab5e70$2e265290@clsvenus.ioe.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030602/03a6ef90/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun  2 13:51:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 2 Jun 2003 12:51:38 +0100 (GMT Daylight Time)
Subject: [R] RE: Ordinal data - Regression Trees  & Proportional Odds
In-Reply-To: <3EDB346F.8060409@statistik.uni-dortmund.de>
Message-ID: <Pine.WNT.4.44.0306021243120.460-100000@gannet.stats.ox.ac.uk>

On Mon, 2 Jun 2003, Andreas Christmann wrote:

>  >>> 1. RE: Ordinal data - Regression Trees  & Proportional Odds
>        (Liaw, Andy)
>
>  > AFAIK there's no implementation (or description) of tree algorithm
>  > that handles ordinal response.
>  >
>
> Regression trees with an ordinal response variable can be computed with
> SPSS Answer Tree 3.0.

They *can* be handled by tree or rpart in R.

I think Andy's point was that there is no consensus as to the right way to
handle them:  certainly using the codes of categories works and may often
be reasonable, and treating ordinal responses as categorical is also very
often perfectly adequate.

Note that rpart is user-extensible, so it would be reasonably easy to write
an extension for a proportional-odds logistic regression model, if that is
thought appropriate (and it seems strange to me to impose such strong
structure on the model with such a general `linear predictor': POLR
models are often in my experience a poor reflection of real problems).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun  2 13:53:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 2 Jun 2003 12:53:29 +0100 (GMT Daylight Time)
Subject: [R] compose a name in function
In-Reply-To: <20030602114350.A05CA356CB@www.fastmail.fm>
Message-ID: <Pine.WNT.4.44.0306021251590.460-100000@gannet.stats.ox.ac.uk>

?paste

as in

win.metafile(paste(name, "%02d.wmf", sep="-"), pointsize = 14)

On Mon, 2 Jun 2003, Rafael Bertola wrote:

> I write a function to plot some graphs and a gave for it one result of
> lm() (reg) and a character (name). In  win.metafile("name-%02d.wmf",
> pointsize = 14) i want compose de name of file with the string i pass to
> the function. How i can make this?
>
> plota.res.grava <- function(reg,name){
>
>     win.metafile("name-%02d.wmf", pointsize = 14)
> ...
> }

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jun  2 14:01:36 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Jun 2003 14:01:36 +0200
Subject: [R] (no subject)
In-Reply-To: <3.0.6.32.20030602123447.00b7d158@mailin.sc.ehu.es>
References: <3.0.6.32.20030602123447.00b7d158@mailin.sc.ehu.es>
Message-ID: <3EDB3CA0.3090105@statistik.uni-dortmund.de>

Alicia Lopez wrote:

> I would be gratefulf if anybody can help me with this problem.  I have not
> a experience with R 
> language, actually this is my first job with it :
> 
> I have write some instructions for acomplish a simulation, and I have 36
> conditions 
> with 1000 iterations each one . The program runs without problems, but in
> the 19 nth 
> condition  gives and  error message (after running   some iterations of
> this condition)   
> is:
> 
> "Error in var(x, na.rm = na.rm) : missing observations in cov/cor"

So, presumably by former calculations, you got NAs in some data 
structure for which the variance is calculated.
You have to debug your code, if you don't know where the NAs come from.

Uwe Ligges


> Somebody would give me any clue about the origin of this error?
> There is any probability  that the origin of the problem is  the scarcitie
> of resources?
> (  I don?t understand how is possible a programming error).
> 
> Any help will be welcome.
> 
> 
> 
> 
> ---------------------------------------------------
> ALICIA L?PEZ J?UREGUI
>  
> 
>    DEPARTAMENTO DE PSICOLOGIA SOCIAL Y METODOLOGIA
>    FACULTAD DE PSICOLOGIA 
>    Avda. de Tolosa, 70
>    20018 DONOSTIA (SPAIN)
> 
>          TEL.: 943 / 018340   
>          E-MAIL: psplojaa at sc.ehu.es
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Jun  2 14:03:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Jun 2003 14:03:01 +0200
Subject: [R] compose a name in function
In-Reply-To: <20030602114350.A05CA356CB@www.fastmail.fm>
References: <20030602114350.A05CA356CB@www.fastmail.fm>
Message-ID: <3EDB3CF5.6080805@statistik.uni-dortmund.de>

Rafael Bertola wrote:

> I write a function to plot some graphs and a gave for it one result of
> lm() (reg) and a character (name). In  win.metafile("name-%02d.wmf",
> pointsize = 14) i want compose de name of file with the string i pass to
> the function. How i can make this?
> 
> plota.res.grava <- function(reg,name){
> 
>     win.metafile("name-%02d.wmf", pointsize = 14)
> ...
> }
> 
> Thanks
> R. Bertola

Use paste(), as in

  win.metafile(paste(name, "%02d.wmf", sep="-"), pointsize = 14)

Uwe Ligges



From dmurdoch at pair.com  Mon Jun  2 14:07:37 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 02 Jun 2003 08:07:37 -0400
Subject: [R] (no subject)
In-Reply-To: <3.0.6.32.20030602123447.00b7d158@mailin.sc.ehu.es>
References: <3.0.6.32.20030602123447.00b7d158@mailin.sc.ehu.es>
Message-ID: <p6fmdvsh4shv4bfj84aa0pefeqfd6ove32@4ax.com>

On Mon, 02 Jun 2003 12:34:47 +0200, you wrote:

>"Error in var(x, na.rm = na.rm) : missing observations in cov/cor"
>
>Somebody would give me any clue about the origin of this error?
>There is any probability  that the origin of the problem is  the scarcitie
>of resources?
>(  I don?t understand how is possible a programming error).

This is unlikely to be due to a scarcity of resources; that should
give a different error message.

What the message says is that at some point you (or a function you
call) are calling the var() function and passing it NA values where it
doesn't want them.  You can use the traceback() function after the
error to see where the call occurs. 

If the NA values should be removed, then you need to make sure that
na.rm = TRUE when this call occurs.  If the NA values shouldn't be
there, then you need to find why they are, and fix your code.

Duncan Murdoch



From administrator at vt.edu  Mon Jun  2 14:22:59 2003
From: administrator at vt.edu (administrator@vt.edu)
Date: Mon, 2 Jun 2003 08:22:59 -0400 (EDT)
Subject: [R] Virus Warning
Message-ID: <200306021222.BLZ48271@zidane.cc.vt.edu>


The message you emailed to kharrell at vt.edu, dated 06/02/03 08:22:59, contains the W32/Sobig-C virus in the movie.pif attachment. The action taken was: deleted the attachment.



From administrator at vt.edu  Mon Jun  2 14:25:23 2003
From: administrator at vt.edu (administrator@vt.edu)
Date: Mon, 2 Jun 2003 08:25:23 -0400 (EDT)
Subject: [R] Virus Warning
Message-ID: <200306021225.BFJ31216@vivi.cc.vt.edu>


The message you emailed to lajennin at vt.edu, dated 06/02/03 08:25:23, contains the W32/Sobig-C virus in the movie.pif attachment. The action taken was: deleted the attachment.



From Mailer-Daemon at ppsw.cam.ac.uk  Mon Jun  2 14:30:36 2003
From: Mailer-Daemon at ppsw.cam.ac.uk (Mail Delivery System)
Date: Mon, 02 Jun 2003 13:30:36 +0100
Subject: [R] Mail delivery failed: returning message to sender
Message-ID: <E19MoSS-0001SS-KC@gold.csi.cam.ac.uk>

This message was created automatically by mail delivery software.

A message that you sent could not be delivered to one or more of its
recipients. This is a permanent error. The following address(es) failed:

  pw10004 at cus.cam.ac.uk
    SMTP error from remote mailer after RCPT TO:<pw10004 at cus.cam.ac.uk>:
    host cus.cam.ac.uk [131.111.8.20]: 550 Unknown user - see http://www.cam.ac.uk/cs/email/bounce.html

------ This is a copy of the message, including all the headers. ------

Return-path: <r-announce at lists.r-project.org>
Received: from [134.151.78.134] (helo=PY-EFORD)
	by gold.csi.cam.ac.uk with esmtp (Exim 4.20)
	id 19MoSQ-0001S6-F5
	for pw10004 at cus.cam.ac.uk; Mon, 02 Jun 2003 13:30:34 +0100
From: <r-announce at lists.r-project.org>
To: <pw10004 at cus.cam.ac.uk>
Subject: Re: 45443-343556
Date: Mon, 2 Jun 2003 13:30:38 +0100
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="CSmtpMsgPart123X456_000_006B3C26"
Message-Id: <E19MoSQ-0001S6-F5 at gold.csi.cam.ac.uk>
X-Cam-ScannerAdmin: mail-scanner-support at ucs.cam.ac.uk
X-Cam-AntiVirus: Not scanned
X-Cam-SpamDetails: scanned, SpamAssassin (score=4.8, required 10,
	FORGED_MUA_OUTLOOK, MISSING_MIMEOLE, NO_REAL_NAME)
X-Cam-SpamScore: ssss

This is a multipart message in MIME format

--CSmtpMsgPart123X456_000_006B3C26
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: 7bit

Please see the attached file.
--CSmtpMsgPart123X456_000_006B3C26--



From C.J.Tuplin at liverpool.ac.uk  Mon Jun  2 14:47:57 2003
From: C.J.Tuplin at liverpool.ac.uk (C.J.Tuplin@liverpool.ac.uk)
Date: Mon, 02 Jun 2003 13:47:57 +0100
Subject: [R] Re: Your application
In-Reply-To: <E19MoRM-0001Zr-00@mailhub1.liv.ac.uk>
References: <E19MoRM-0001Zr-00@mailhub1.liv.ac.uk>
Message-ID: <18675483.1054561677@063024-18253r.liv.ac.uk>

I am at a loss to undestand this message since I have made no application 
to you (nor, for that matter, is there an attached file).  Please delete my 
address from all your records and advise me that this has been done.

--On 02 June 2003 13:29 +0100 r-help at lists.r-project.org wrote:

> Please see the attached file.



From D.Beare at marlab.ac.uk  Mon Jun  2 14:56:23 2003
From: D.Beare at marlab.ac.uk (Douglas Beare)
Date: Mon, 2 Jun 2003 13:56:23 +0100 
Subject: [R] legends on image plots
Message-ID: <67B92F9B2AFED611852500B0D0FE15097EAF61@mail3.marlab.ac.uk>

Hi,
Does anyone out there know how to add a legend when using the R-function
image?
Ie. is there something out there like the S+ function image.legend?
Doug.
Fisheries Research Services,
Marine Laboratory,
Victoria Road,
Torry,
Aberdeen, UK.
Tel. 44 (0) 1224 295314



From markhall at gol.com  Mon Jun  2 15:12:32 2003
From: markhall at gol.com (Mark Hall)
Date: Mon, 02 Jun 2003 22:12:32 +0900
Subject: [R] Re: Your application
In-Reply-To: <18675483.1054561677@063024-18253r.liv.ac.uk>
References: <E19MoRM-0001Zr-00@mailhub1.liv.ac.uk>
	<18675483.1054561677@063024-18253r.liv.ac.uk>
Message-ID: <20030602220440.A5ED.MARKHALL@gol.com>

It is one of the latest virus variants. If you have a windows operating system, check your 
virus software for updates.  I must have gotten this 10-15 times in the
past 3 days from one list or the other.  
-- 
 Mark Hall
Niigata Prefectural Museum of History

<>



From martinl at mathinfo.ens.univ-reims.fr  Mon Jun  2 15:17:06 2003
From: martinl at mathinfo.ens.univ-reims.fr (MARTIN  Ludovic)
Date: Mon, 02 Jun 2003 15:17:06 +0200
Subject: [R] Query:problem with the corStruct Classes.
Message-ID: <200306021413.h52EDQwR023349@tom.ens.univ-reims.fr>

Dear all,

In Pinheiro and bates'book, we fit an ARMA(1,1) model with:
>fm5Ovar.lme<-update(fm1Ovar.lme,corr=corARMA(p=1,q=1)
But the result is:
Error in "coef<-.corARMA"(*tmp*, value = c(62.3428455940029, 
95.0395441938099,  : 
        Coefficient matrix not invertible

There is also problems when we fit an exponential spatial correlation 
model for the within-group error:
fm3BW.lme<-update(fm2BW.lme,corr=corExp(form=~Time))
We obtain:
Error in recalc.corSpatial(object[[i]], conLin) : 
        NA/NaN/Inf in foreign function call (arg 1)

I would be grateful if anyone could help me.

Cordially,

Martin Ludovic.



From spencer.graves at pdf.com  Mon Jun  2 15:33:00 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 02 Jun 2003 06:33:00 -0700
Subject: [R] compositional data: percent values sum up to 1
References: <1054452185.1213.4.camel@christophl> <3EDA4D9B.4050609@pdf.com>
	<1054538606.4230.1.camel@christophl>
Message-ID: <3EDB520C.1000903@pdf.com>

"glm" will do multinomial logistic regression.  However, if J is large, 
I doubt if that will do what you want.  If it were my problem, I might 
feel a need to read the code for "glm" and modify it to do what I want. 
  Perhaps someone else can suggest something better.

hth.  spencer graves

Christoph Lehmann wrote:
> I want to do a logistic regression analysis, and to compare with, a
> discriminant analysis. The mentioned power maps are my exogenous data,
> the dependent variable (not mentioned so far) is a diagnosis
> (ill/healthy)
> 
> thanks for the interest and the help
> 
> Christoph
> 
> On Sun, 2003-06-01 at 21:01, Spencer Graves wrote:
> 
>>What are you trying to do?  What I would do with this depends on many 
>>factors.
>>
>>spencer graves
>>
>>Christoph Lehmann wrote:
>>
>>>again, under another subject:
>>>sorry, maybe an all too trivial question. But we have power data from J
>>>frequency spectra and to have the same range for the data of all our
>>>subjects, we just transformed them into % values, pseudo-code:
>>>
>>>power[i,j]=power[i,j]/sum(power[i,1:J])
>>>
>>>of course, now we have a perfect linear relationship in our x design-matrix,
>>>since all power-values for each subject sum up to 1.
>>>
>>>How shall we solve this problem: just eliminate one column of x, or
>>>introduce a restriction which says exactly that our power data sum up to
>>>1 for each subject?
>>>
>>>Thanks a lot
>>>
>>>Christoph
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Mon Jun  2 16:19:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Jun 2003 16:19:01 +0200
Subject: [R] legends on image plots
In-Reply-To: <67B92F9B2AFED611852500B0D0FE15097EAF61@mail3.marlab.ac.uk>
References: <67B92F9B2AFED611852500B0D0FE15097EAF61@mail3.marlab.ac.uk>
Message-ID: <3EDB5CD5.8030100@statistik.uni-dortmund.de>

Douglas Beare wrote:

> Hi,
> Does anyone out there know how to add a legend when using the R-function
> image?
> Ie. is there something out there like the S+ function image.legend?

What about legend() itself?

Uwe Ligges

> Doug.
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rvaradha at jhsph.edu  Mon Jun  2 16:43:07 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Mon, 02 Jun 2003 10:43:07 -0400
Subject: [R] Help - Curvature measures of nonlinearity
Message-ID: <6eff6e6f58d2.6f58d26eff6e@jhsph.edu>


How about "rms.curv" function in the MASS library?

Ravi.

----- Original Message -----
From: Ivone Figueiredo <ivonefig at ipimar.pt>
Date: Monday, June 2, 2003 4:40 am
Subject: [R] Help - Curvature measures of nonlinearity

> Dear colleagues,
> 
> Von Bertalanffy model is commonly adjust to data on fish length 
> (TL) and age (AGE) 
> TL= Linf*(1-exp(-K*(AGE-t0)). Linf, K and t0 are parameters of the 
> model.One main goal of the growth study is the comparison of 
> growth parameter estimates between sexes of the same species, or 
> estimates from different populations. 
> The realibility statistical tests normally applied are highly 
> dependent on the nonlinearity of the model used.
> 
> Are there any developed routines in R ? to:
> 1) calculate the maximum intrinsic curvature
> 2) maximum parameter affecting curvature 
> 3) plot the two dimensional cross sections of confidence regions 
> (for different alpha values) of the model parameters
> 
> Regards,
> Ivone Figueiredo 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From adi at roda.ro  Mon Jun  2 16:47:16 2003
From: adi at roda.ro (Adrian Dusa)
Date: Mon, 2 Jun 2003 17:47:16 +0300
Subject: [R] function for Browse
Message-ID: <000001c32915$df6c3ad0$c13afea9@RODAL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030602/9b88c5a1/attachment.pl

From jean.eid at utoronto.ca  Mon Jun  2 17:27:35 2003
From: jean.eid at utoronto.ca (Jean Eid)
Date: Mon, 2 Jun 2003 11:27:35 -0400
Subject: [R] data.frame building
Message-ID: <001901c3291b$7e0a83e0$d1b16480@ecograd.utoronto.ca>

Hi all,
I have two seperate questions that both deal with the way R handels data
sets:

First, I am trying to read a data set of 80M into R. I am using
read.table(). The file is a tab file and I have tested the function for a
small amount of lines. It seems to work fine (i.e. correct amount of columns
and rows). However, when I try to read the full data set, R outputs a memory
limit error and stops responding to any command even the gc()  and the
quit() commands. It says it reached maximum memory of 489MB (that is the
amount of RAM I have). I tried the to increase vsize and nsize but no luck.

My second question concerns building data frames in a loop. I know a while
back Prof. Ripley has suggested to construct a data frame of the right size
outside the loop and fill it within the loop. unfortunately, it is taken so
long (this is on P4 2.4 G 489M RAM). I am wondering if there was any other
way that is faster to do this job. I also do not jnow why it is faster to
construct columns in a data.frame than it is to do rows. i.e. I had to
construct 719 columns and it does not take any time to do so (each) however
if I say data[i,]<-data2[j,] it takes so long versus
data$temp<-data$a==data$b for example.


any help is greatly appreciated

Jean Eid



From ripley at stats.ox.ac.uk  Mon Jun  2 17:56:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 16:56:28 +0100 (BST)
Subject: [R] compositional data: percent values sum up to 1
In-Reply-To: <3EDB520C.1000903@pdf.com>
Message-ID: <Pine.LNX.4.44.0306021655350.21914-100000@gannet.stats>

On Mon, 2 Jun 2003, Spencer Graves wrote:

> "glm" will do multinomial logistic regression.  However, if J is large, 

Strictly, no, it will not as that is not a GLM.  glm() can only do it via 
surrogate Poisson models.  multinom in nnet(VR) will do multinomial 
logistic regression.

> I doubt if that will do what you want.  If it were my problem, I might 
> feel a need to read the code for "glm" and modify it to do what I want. 
>   Perhaps someone else can suggest something better.
> 
> hth.  spencer graves
> 
> Christoph Lehmann wrote:
> > I want to do a logistic regression analysis, and to compare with, a
> > discriminant analysis. The mentioned power maps are my exogenous data,
> > the dependent variable (not mentioned so far) is a diagnosis
> > (ill/healthy)
> > 
> > thanks for the interest and the help
> > 
> > Christoph
> > 
> > On Sun, 2003-06-01 at 21:01, Spencer Graves wrote:
> > 
> >>What are you trying to do?  What I would do with this depends on many 
> >>factors.
> >>
> >>spencer graves
> >>
> >>Christoph Lehmann wrote:
> >>
> >>>again, under another subject:
> >>>sorry, maybe an all too trivial question. But we have power data from J
> >>>frequency spectra and to have the same range for the data of all our
> >>>subjects, we just transformed them into % values, pseudo-code:
> >>>
> >>>power[i,j]=power[i,j]/sum(power[i,1:J])
> >>>
> >>>of course, now we have a perfect linear relationship in our x design-matrix,
> >>>since all power-values for each subject sum up to 1.
> >>>
> >>>How shall we solve this problem: just eliminate one column of x, or
> >>>introduce a restriction which says exactly that our power data sum up to
> >>>1 for each subject?
> >>>
> >>>Thanks a lot
> >>>
> >>>Christoph
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun  2 18:02:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 17:02:14 +0100 (BST)
Subject: [R] data.frame building
In-Reply-To: <001901c3291b$7e0a83e0$d1b16480@ecograd.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0306021659450.21914-100000@gannet.stats>

On Mon, 2 Jun 2003, Jean Eid wrote:

> I have two seperate questions that both deal with the way R handels data
> sets:
> 
> First, I am trying to read a data set of 80M into R. I am using
> read.table(). The file is a tab file and I have tested the function for a
> small amount of lines. It seems to work fine (i.e. correct amount of columns
> and rows). However, when I try to read the full data set, R outputs a memory
> limit error and stops responding to any command even the gc()  and the
> quit() commands. It says it reached maximum memory of 489MB (that is the
> amount of RAM I have). I tried the to increase vsize and nsize but no luck.

Have you read and used the hints in help(read.table) and the R Data
Import/Export Manual?  Alternatively, use scan() (as hinted there).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ltorgo at liacc.up.pt  Mon Jun  2 19:13:45 2003
From: ltorgo at liacc.up.pt (Luis Torgo)
Date: Mon, 2 Jun 2003 17:13:45 +0000
Subject: [R] Ploting rpart objects / namespace problems
Message-ID: <200306021713.45685.ltorgo@liacc.up.pt>

I've written a small adaptation of the text.rpart function of the rpart 
package to better suite my tree presentation needs (I basically left the code 
untouched, only changing some numbers relating to label positioning).

When I changed to version 1.7.0 this function stopped working printing the 
error :
    couldn't find function "rpartco"

As far as I understand this has to do with the new namespaces conventions and 
the package rpart not exporting "rpartco". Is there an obvious way of 
adapting my function so that I can call the function "rpartco" even if it is 
not exported from the package? If not, is there any way of going arround this 
without having to give up my self-tuned text.rpart ?

A second related question is the following: when I faced the problem mentioned 
above the first thing I done was trying to print the fucntion "text.rpart" to 
check whether something changed dramatically. I was unable to do that because 
of the same exporting reasons I guess. The only way I manage to see the code 
of "text.rpart" was through:
> get("text.rpart",envir=environment(rpart))

Is there an easiest way of seeing the code of this type of "hidden" functions?

Thanks,
Luis
-- 
Luis Torgo
    FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
    Machine Learning Group           Fax   : (+351) 22 600 36 54
    R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
    4150 PORTO   -  PORTUGAL         WWW   : http://www.liacc.up.pt/~ltorgo



From ripley at stats.ox.ac.uk  Mon Jun  2 18:28:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 17:28:44 +0100 (BST)
Subject: [R] Ploting rpart objects / namespace problems
In-Reply-To: <200306021713.45685.ltorgo@liacc.up.pt>
Message-ID: <Pine.LNX.4.44.0306021724490.21963-100000@gannet.stats>

On Mon, 2 Jun 2003, Luis Torgo wrote:

> I've written a small adaptation of the text.rpart function of the rpart 
> package to better suite my tree presentation needs (I basically left the code 
> untouched, only changing some numbers relating to label positioning).
> 
> When I changed to version 1.7.0 this function stopped working printing the 
> error :
>     couldn't find function "rpartco"
> 
> As far as I understand this has to do with the new namespaces conventions and 
> the package rpart not exporting "rpartco". Is there an obvious way of 
> adapting my function so that I can call the function "rpartco" even if it is 
> not exported from the package? If not, is there any way of going arround this 
> without having to give up my self-tuned text.rpart ?

getFromNamespace("rpartco", "rpart")

> A second related question is the following: when I faced the problem mentioned 
> above the first thing I done was trying to print the fucntion "text.rpart" to 
> check whether something changed dramatically. I was unable to do that because 
> of the same exporting reasons I guess. The only way I manage to see the code 
> of "text.rpart" was through:
> > get("text.rpart",envir=environment(rpart))
> 
> Is there an easiest way of seeing the code of this type of "hidden" functions?

See ?getS3method

These functions (and others) are mentioned in the NEWS file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dave at mirabellafunds.com  Mon Jun  2 18:37:50 2003
From: dave at mirabellafunds.com (David S. Khabie-Zeitoune)
Date: Mon, 2 Jun 2003 17:37:50 +0100
Subject: [R] test: please ignore this message
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012450@mail.internal.net>



From dave at mirabellafunds.com  Mon Jun  2 18:46:29 2003
From: dave at mirabellafunds.com (David S. Khabie-Zeitoune)
Date: Mon, 2 Jun 2003 17:46:29 +0100
Subject: [R] Help with factorized argument in solve.QP
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012451@mail.internal.net>

Hi
 
I'm having problems getting the "factorized" argument in solve.QP (part
of the quadprog library) to work as expected. The helpfile states that
when the factorized argument is set to TRUE, then the function requires
the inverse of a square-root factor of the Hessian instead of the
Hessian itself. That is, when factorized=TRUE, the Dmat argument should
be a matrix R^(-1), such that the Hessian of the objective function is
t(R) %*% R.
 
I modified the example in the helpfile slightly to test this out:
 
R          = matrix(rnorm(9),3,3)
R.inv      = solve(R)
Dmat       = t(R) %*% R
dvec       = c(0,5,0)
Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
bvec       = c(-8,2,0)
 
x1 	     = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=FALSE)
x2 	     = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=TRUE)
print(x1$solution)
print(x2$solution)

I would have expected that x1$solution and x2$solution were the same (or
numerically similar); however they are typically very different. Where
am I going wrong...?
 
Thanks
 
David



From p.dalgaard at biostat.ku.dk  Mon Jun  2 19:04:49 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 02 Jun 2003 17:04:49 -0000
Subject: [R] Help with factorized argument in solve.QP
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D012451@mail.internal.net>
References: <8D0F30FE2EB3314182D4A33F738BB19D012451@mail.internal.net>
Message-ID: <x2isroflok.fsf@biostat.ku.dk>

"David S. Khabie-Zeitoune" <dave at mirabellafunds.com> writes:

> I modified the example in the helpfile slightly to test this out:
>  
> R          = matrix(rnorm(9),3,3)
> R.inv      = solve(R)
> Dmat       = t(R) %*% R
> dvec       = c(0,5,0)
> Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
> bvec       = c(-8,2,0)
>  
> x1 	     = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=FALSE)
> x2 	     = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=TRUE)
> print(x1$solution)
> print(x2$solution)
> 
> I would have expected that x1$solution and x2$solution were the same (or
> numerically similar); however they are typically very different. Where
> am I going wrong...?

Hmmm. Could it be that it is assuming a *triangular* square root of
the matrix?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dave at evocapital.com  Mon Jun  2 19:11:06 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Mon, 2 Jun 2003 18:11:06 +0100
Subject: [R] Help with factorized argument in solve.QP
In-Reply-To: <x2isroflok.fsf@biostat.ku.dk>
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012453@mail.internal.net>

Yes -- this seems to be the case; the following example works as
expected. Thank you!

R          = matrix(rnorm(9),3,3)
R[lower.tri(R)] = 0
R.inv      = solve(R)
Dmat       = t(R) %*% R

dvec       = c(0,5,0)
Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
bvec       = c(-8,2,0)

x1 = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=FALSE)
x2 = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=TRUE)
print(x1$solution)
print(x2$solution)

-----Original Message-----
From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk] 
Sent: 02 June 2003 18:12
To: David S. Khabie-Zeitoune
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Help with factorized argument in solve.QP


"David S. Khabie-Zeitoune" <dave at mirabellafunds.com> writes:

> I modified the example in the helpfile slightly to test this out:
>  
> R          = matrix(rnorm(9),3,3)
> R.inv      = solve(R)
> Dmat       = t(R) %*% R
> dvec       = c(0,5,0)
> Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
> bvec       = c(-8,2,0)
>  
> x1 	     = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=FALSE)
> x2 	     = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=TRUE)
> print(x1$solution)
> print(x2$solution)
> 
> I would have expected that x1$solution and x2$solution were the same 
> (or numerically similar); however they are typically very different. 
> Where am I going wrong...?

Hmmm. Could it be that it is assuming a *triangular* square root of the
matrix?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dave at mirabellafunds.com  Mon Jun  2 19:11:07 2003
From: dave at mirabellafunds.com (David S. Khabie-Zeitoune)
Date: Mon, 2 Jun 2003 18:11:07 +0100
Subject: [R] Help with factorized argument in solve.QP
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012453@mail.internal.net>

Yes -- this seems to be the case; the following example works as
expected. Thank you!

R          = matrix(rnorm(9),3,3)
R[lower.tri(R)] = 0
R.inv      = solve(R)
Dmat       = t(R) %*% R

dvec       = c(0,5,0)
Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
bvec       = c(-8,2,0)

x1 = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=FALSE)
x2 = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
factorized=TRUE)
print(x1$solution)
print(x2$solution)

-----Original Message-----
From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk] 
Sent: 02 June 2003 18:12
To: David S. Khabie-Zeitoune
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Help with factorized argument in solve.QP


"David S. Khabie-Zeitoune" <dave at mirabellafunds.com> writes:

> I modified the example in the helpfile slightly to test this out:
>  
> R          = matrix(rnorm(9),3,3)
> R.inv      = solve(R)
> Dmat       = t(R) %*% R
> dvec       = c(0,5,0)
> Amat       = matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
> bvec       = c(-8,2,0)
>  
> x1 	     = solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=FALSE)
> x2 	     = solve.QP(Dmat=R.inv, dvec=dvec, Amat=Amat, bvec=bvec,
> factorized=TRUE)
> print(x1$solution)
> print(x2$solution)
> 
> I would have expected that x1$solution and x2$solution were the same 
> (or numerically similar); however they are typically very different. 
> Where am I going wrong...?

Hmmm. Could it be that it is assuming a *triangular* square root of the
matrix?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pauljohn at ku.edu  Mon Jun  2 19:19:32 2003
From: pauljohn at ku.edu (Paul E. Johnson)
Date: Mon, 02 Jun 2003 12:19:32 -0500
Subject: Rtips (was Re: [R] ? building a database with a the great examples
In-Reply-To: <p05200f03bafab75c5986@[128.40.218.142]>
References: <p05200f03bafab75c5986@[128.40.218.142]>
Message-ID: <3EDB8724.6050602@ku.edu>

Perhaps you want to start maintaining Rtips itself!  

I don't have as much time for it as I used to.  I still think it is 
valuable to have such a listing, but it is so hard to keep up to date 
with R and I'm only doing minimal work to keep it up to date.  I'm 
thinking of dropping altogether the section on packages because I just 
can't keep up with the creativity of the R community.  I did most of 
that list before Frank Harrell put up Hmisc and Design, you know.

The Rtips page is still up here:

http://lark.cc.ukans.edu/~pauljohn/R/statsRus.html

The FaqManager software is starting to show its age, but it still works. 
 If you want to be a contributor, I could assign for you a password on 
that server.  

Also, I think it will work if you go to this other name I have created 
for this same thing:

http://lark.cc.ukans.edu/~pauljohn/R/Rtips.html

Some users write and ask me why I did not contribute this doc to the R 
homepage, and I have no good answer except that in the beginning I did 
not have enough stuff to make it worthwhile.  Since the content of Rtips 
can change on a daily/weekly basis, it did not seem right to just make a 
snapshot and email it over. But I'm open to suggestions.

Frank Mattes wrote:

> Dear R help reader,
>
> I'm not an expert in R and are lerning a lot by reading the help 
> digest, which is sometimes difficult because the huge amount of data 
> posted. I have posted some questions before, and  are impressed how 
> quick I got a solution for my problem. Sometimes with quite different 
> suggestions. I was always wondering if my questions didn't come up 
> before. On the other site, it wasn't easy to search the help archive, 
> purely I didn't know how to formulate my problem.
> I'm wondering if we could not collect all the answers / examples in a 
> database -
> sorted in topics, like the help document "Rtips".
> I have no clue if this is possible to do nor how time consuming the 
> maintaining would be.
>
> This is just my view how the help list could be improved
>
> Yours
> Frank



-- 
Paul E. Johnson                       email: pauljohn at ukans.edu
Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66045                FAX: (785) 864-5700



From LowdenJK at bernstein.com  Mon Jun  2 19:58:49 2003
From: LowdenJK at bernstein.com (Lowden, James K)
Date: Mon, 2 Jun 2003 13:58:49 -0400
Subject: [R] Very slow startup on Win32
Message-ID: <0BC7BDCCBEAED51187340002B35CA0B507C5EACD@acnyexg02.beehive.com>

Hello, 

I installed R on Windows Terminal Server (a variant of NT) from the binary release.  This is my first encounter with R.  I have two questions.  

The initialization time is very long, several minutes between clicking on the "R 1.7.0" icon and getting to the ">" prompt.  During that time, the machine appears to be idle.  

When it finally comes ready, I'm presented with a warning that might be related:

	[...]
	Type `q()' to quit R.

	> x <- rnorm(50) 
	Warning message: 
	package nls in options("defaultPackages") was not found 

Given that the CPU is idle during the delay, it seems likely R is waiting for, and timing out on, some resource.  Can I perhaps restrict or display what happens at initialization?  It may be I don't need whatever it is that's causing the delay.  

FWIW, the machine is a dual-processor Pentium something @ 450 MHz w/ 512 MB RAM.  

The second question is simple:  How do I list the installed packages?  I haven't found the answer in the "Introduction to R" or the data reference.  I would just like to confirm that the ODBC package is included in the Win32 release binary.  

My next step is to hook up R to our database and compare its performance to SAS's.  Some of my colleagues had bad experiences with S some years ago, and are wary of using R for that reason.  

Thank you for your kind attention.  

--jkl

-----------------------------------------
The information contained in this transmission may contain privileged and confidential information and is intended only for the use of the person(s) named above. If you are not the intended recipient, or an employee or agent responsible for delivering this message to the intended recipient, any review, dissemination, distribution or duplication of this communication is strictly prohibited. If you are not the intended recipient, please contact the sender immediately by reply e-mail and destroy all copies of the original message. Please note that we do not accept account orders and/or instructions by e-mail, and therefore will not be responsible for carrying out such orders and/or instructions.



From r at difficulties.de  Mon Jun  2 22:11:51 2003
From: r at difficulties.de (Matthias Kirschner)
Date: Mon, 2 Jun 2003 22:11:51 +0200
Subject: [R] R summary (and quantiles)
In-Reply-To: <5.1.0.14.0.20030529174331.01e682a0@imap.rockefeller.edu>
References: <20030529225540.GA20871@kb.mbwg.de>
	<5.1.0.14.0.20030529174331.01e682a0@imap.rockefeller.edu>
Message-ID: <20030602201151.GA14157@kb.mbwg.de>

Hello Knut

> The solution is simple: Never use
> 
>         quantile or
>         summary
> 
> if you are interested in quantiles ;-)
Thanks a lot! 
Matze



From andy_liaw at merck.com  Mon Jun  2 19:58:47 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 02 Jun 2003 13:58:47 -0400
Subject: [R] compositional data: percent values sum up to 1
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FB8F@usrymx25.merck.com>

Eh?  The original message says it's the design matrix that is perfectly
collinear after the transformation, not the response.

I don't know much about this type of data, but seems like you could just fit
the model w/o intercept to eliminate the collinearity, no?  It's the
interpretation of the result that may be tricky, I think.

Andy


> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com]
> Sent: Monday, June 02, 2003 9:33 AM
> To: Christoph Lehmann
> Cc: Spencer Graves; r-help at stat.math.ethz.ch
> Subject: Re: [R] compositional data: percent values sum up to 1
> 
> 
> "glm" will do multinomial logistic regression.  However, if J 
> is large, 
> I doubt if that will do what you want.  If it were my 
> problem, I might 
> feel a need to read the code for "glm" and modify it to do 
> what I want. 
>   Perhaps someone else can suggest something better.
> 
> hth.  spencer graves
> 
> Christoph Lehmann wrote:
> > I want to do a logistic regression analysis, and to compare with, a
> > discriminant analysis. The mentioned power maps are my 
> exogenous data,
> > the dependent variable (not mentioned so far) is a diagnosis
> > (ill/healthy)
> > 
> > thanks for the interest and the help
> > 
> > Christoph
> > 
> > On Sun, 2003-06-01 at 21:01, Spencer Graves wrote:
> > 
> >>What are you trying to do?  What I would do with this 
> depends on many 
> >>factors.
> >>
> >>spencer graves
> >>
> >>Christoph Lehmann wrote:
> >>
> >>>again, under another subject:
> >>>sorry, maybe an all too trivial question. But we have 
> power data from J
> >>>frequency spectra and to have the same range for the data 
> of all our
> >>>subjects, we just transformed them into % values, pseudo-code:
> >>>
> >>>power[i,j]=power[i,j]/sum(power[i,1:J])
> >>>
> >>>of course, now we have a perfect linear relationship in 
> our x design-matrix,
> >>>since all power-values for each subject sum up to 1.
> >>>
> >>>How shall we solve this problem: just eliminate one column of x, or
> >>>introduce a restriction which says exactly that our power 
> data sum up to
> >>>1 for each subject?
> >>>
> >>>Thanks a lot
> >>>
> >>>Christoph
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Benjamin.STABLER at odot.state.or.us  Mon Jun  2 20:35:21 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Mon, 2 Jun 2003 11:35:21 -0700 
Subject: [R] Building an R package under Windows NT
Message-ID: <76A000A82289D411952F001083F9DD06047FE0E1@exsalem4-bu.odot.state.or.us>

I am trying to build a R 1.7 package under Windows NT.  I created the
DESCRIPTION file, the RD file and added the code to the R folder.  I also
downloaded and installed the Rtools package and have Perl 5.0.  I know that
Perl, Miktex, and gcc are working.  I also have my PATH variable set
correctly.  I can fake install my package by removing the *.R from the code
file, using Rcmd Rdconv to create the 00index.html file from my Rd file, and
copying the package folder to the library folder.  But I can't seem to get
Rcmd build to build a binary version of my package.  I can run build without
the "--binary" option and I get all my files in a taz.gz file.  

D:\>Rcmd build emme2
* checking for file 'emme2/DESCRIPTION' ... OK
* preparing 'emme2':
* checking whether 'INDEX' is up-to-date ... OK
* removing junk files
* building 'emme2_0.1.tar.gz'

But when I try to build a precompiled binary package I get the following
response:

D:\>Rcmd build --binary emme2
* checking for file 'emme2/DESCRIPTION' ... OK

make: *** [pkg-emme2] Error 255
*** Installation of emme2 failed ***

installing R.css in c:/TEMP/Rbuild.225

* building 'emme2_0.1.zip'
zip error: Invalid command arguments (no such option: X)


Any help would be appreciated.  Thanks.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From ripley at stats.ox.ac.uk  Mon Jun  2 20:56:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 19:56:22 +0100 (BST)
Subject: [R] Building an R package under Windows NT
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE0E1@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0306021949420.22479-100000@gannet.stats>

Try to install the package first, via

Rcmd INSTALL emme2

as a binary build involves an install and a direct install may be more
informative.

The message about zip suggests you don't have the right things first in 
your path, so please read the file readme.packages and cross check.
In particular, check that `zip -v' gives

Copyright (C) 1990-1999 Info-ZIP
Type 'zip "-L"' for software license.
This is Zip 2.3 (November 29th 1999), by Info-ZIP.
...

(and that version does have an X argument).

On Mon, 2 Jun 2003 Benjamin.STABLER at odot.state.or.us wrote:

> I am trying to build a R 1.7 package under Windows NT.  I created the
> DESCRIPTION file, the RD file and added the code to the R folder.  I also
> downloaded and installed the Rtools package and have Perl 5.0.  I know that
> Perl, Miktex, and gcc are working.  I also have my PATH variable set
> correctly.  I can fake install my package by removing the *.R from the code
> file, using Rcmd Rdconv to create the 00index.html file from my Rd file, and
> copying the package folder to the library folder.  But I can't seem to get
> Rcmd build to build a binary version of my package.  I can run build without
> the "--binary" option and I get all my files in a taz.gz file.  
> 
> D:\>Rcmd build emme2
> * checking for file 'emme2/DESCRIPTION' ... OK
> * preparing 'emme2':
> * checking whether 'INDEX' is up-to-date ... OK
> * removing junk files
> * building 'emme2_0.1.tar.gz'
> 
> But when I try to build a precompiled binary package I get the following
> response:
> 
> D:\>Rcmd build --binary emme2
> * checking for file 'emme2/DESCRIPTION' ... OK
> 
> make: *** [pkg-emme2] Error 255
> *** Installation of emme2 failed ***
> 
> installing R.css in c:/TEMP/Rbuild.225
> 
> * building 'emme2_0.1.zip'
> zip error: Invalid command arguments (no such option: X)
> 
> 
> Any help would be appreciated.  Thanks.
> 
> Benjamin Stabler
> Transportation Planning Analysis Unit
> Oregon Department of Transportation
> 555 13th Street NE, Suite 2
> Salem, OR 97301  Ph: 503-986-4104
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Jun  2 21:19:25 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 02 Jun 2003 12:19:25 -0700
Subject: [R] compositional data: percent values sum up to 1
References: <3A822319EB35174CA3714066D590DCD5C4FB8F@usrymx25.merck.com>
Message-ID: <3EDBA33D.8060900@pdf.com>

Hi, Christoph:

	  Andy Liaw's suggestion sounds sensible to me, though I don't have 
much experience with this kind of data either.

	  OTHER QUESTIONS:

	  * How big is J?  I'm guessing it might be quite large, but I don't 
know.

	  * Are the spectra relatively smooth?  I wonder if it might be 
appropriate to try to smooth the data some way preliminary to other 
analyses.

	  * How many observations do you have in each of "ill" and "healthy", 
especially relative to J?

	  I might try to do a principal components analysis (or "svd" if 
"princomp" bombed because of singular matrices) on the covariance matrix 
of the spectra.  Then I might want to test how different the spectra were.

hope this helps.  spencer graves

Liaw, Andy wrote:
> Eh?  The original message says it's the design matrix that is perfectly
> collinear after the transformation, not the response.
> 
> I don't know much about this type of data, but seems like you could just fit
> the model w/o intercept to eliminate the collinearity, no?  It's the
> interpretation of the result that may be tricky, I think.
> 
> Andy
> 
> 
> 
>>-----Original Message-----
>>From: Spencer Graves [mailto:spencer.graves at pdf.com]
>>Sent: Monday, June 02, 2003 9:33 AM
>>To: Christoph Lehmann
>>Cc: Spencer Graves; r-help at stat.math.ethz.ch
>>Subject: Re: [R] compositional data: percent values sum up to 1
>>
>>
>>"glm" will do multinomial logistic regression.  However, if J 
>>is large, 
>>I doubt if that will do what you want.  If it were my 
>>problem, I might 
>>feel a need to read the code for "glm" and modify it to do 
>>what I want. 
>>  Perhaps someone else can suggest something better.
>>
>>hth.  spencer graves
>>
>>Christoph Lehmann wrote:
>>
>>>I want to do a logistic regression analysis, and to compare with, a
>>>discriminant analysis. The mentioned power maps are my 
>>
>>exogenous data,
>>
>>>the dependent variable (not mentioned so far) is a diagnosis
>>>(ill/healthy)
>>>
>>>thanks for the interest and the help
>>>
>>>Christoph
>>>
>>>On Sun, 2003-06-01 at 21:01, Spencer Graves wrote:
>>>
>>>
>>>>What are you trying to do?  What I would do with this 
>>>
>>depends on many 
>>
>>>>factors.
>>>>
>>>>spencer graves
>>>>
>>>>Christoph Lehmann wrote:
>>>>
>>>>
>>>>>again, under another subject:
>>>>>sorry, maybe an all too trivial question. But we have 
>>>>
>>power data from J
>>
>>>>>frequency spectra and to have the same range for the data 
>>>>
>>of all our
>>
>>>>>subjects, we just transformed them into % values, pseudo-code:
>>>>>
>>>>>power[i,j]=power[i,j]/sum(power[i,1:J])
>>>>>
>>>>>of course, now we have a perfect linear relationship in 
>>>>
>>our x design-matrix,
>>
>>>>>since all power-values for each subject sum up to 1.
>>>>>
>>>>>How shall we solve this problem: just eliminate one column of x, or
>>>>>introduce a restriction which says exactly that our power 
>>>>
>>data sum up to
>>
>>>>>1 for each subject?
>>>>>
>>>>>Thanks a lot
>>>>>
>>>>>Christoph
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From murdoch at stats.uwo.ca  Mon Jun  2 21:28:59 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 Jun 2003 15:28:59 -0400
Subject: [R] Building an R package under Windows NT
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE0E1@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD06047FE0E1@exsalem4-bu.odot.state.or.us>
Message-ID: <7a9ndvcijevdi8h1id63e9mbs4nm142pl1@4ax.com>

On Mon, 2 Jun 2003 11:35:21 -0700 , you wrote in message
<76A000A82289D411952F001083F9DD06047FE0E1 at exsalem4-bu.odot.state.or.us>:

>
>But when I try to build a precompiled binary package I get the following
>response:
>
>D:\>Rcmd build --binary emme2
>* checking for file 'emme2/DESCRIPTION' ... OK
>
>make: *** [pkg-emme2] Error 255
>*** Installation of emme2 failed ***
>
>installing R.css in c:/TEMP/Rbuild.225
>
>* building 'emme2_0.1.zip'
>zip error: Invalid command arguments (no such option: X)

Looks as though you have an incompatible version of zip on your path
ahead of the one from Rtools.

Duncan Murdoch



From arrayprofile at yahoo.com  Mon Jun  2 21:29:29 2003
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 2 Jun 2003 12:29:29 -0700 (PDT)
Subject: [R] catch error/warning message in a loop
In-Reply-To: <3ED7F03E.3070809@pdf.com>
Message-ID: <20030602192929.24747.qmail@web41215.mail.yahoo.com>

cat can only catch the error message promptly, but
seems not able to catch the warning message promptly.
Can anyone point out how i can catch the warning
message in a loop?


--- Spencer Graves <spencer.graves at PDF.COM> wrote:
> 	  With situations like this, I often include
> something like "cat(i, 
> '')" in the loop (where "i" is the index of the
> loop).
> 
> 	  If I'm writing a function to be used by others, I
> might use "try", as 
> described, e.g. in Venables and Ripley (2000) S
> Programming (p. 48).
> 
> hth.  spencer graves
> 
> array chip wrote:
> > Hi, I am running cox regreesion (coxph) on a large
> > number of independent variables, one variable at a
> > time, using loop. At some point of the loop, the
> cox
> > regression stopped due to some errors. How can I
> know
> > at which variable the cox regression stopped so
> that I
> > can pinpoint the variable that causes the problem?
> I
> > guess this is not unique to cox regression, it is
> the
> > problem of catching the error/warning message
> promptly
> > in a loop.
> > 
> > tahnks
> > 
> > __________________________________
> > 

> to Outlook(TM).
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From spencer.graves at pdf.com  Mon Jun  2 21:44:40 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 02 Jun 2003 12:44:40 -0700
Subject: [R] catch error/warning message in a loop
References: <20030602192929.24747.qmail@web41215.mail.yahoo.com>
Message-ID: <3EDBA928.8080203@pdf.com>

	  ?warnings asserts, " `warnings' prints the top-level variable 
'last.warning' in a pleasing form."

	  Have you tried something like "cat(i, last.warning)"?

hth.  spencer graves

array chip wrote:
> cat can only catch the error message promptly, but
> seems not able to catch the warning message promptly.
> Can anyone point out how i can catch the warning
> message in a loop?
> 
> 
> --- Spencer Graves <spencer.graves at PDF.COM> wrote:
> 
>>	  With situations like this, I often include
>>something like "cat(i, 
>>'')" in the loop (where "i" is the index of the
>>loop).
>>
>>	  If I'm writing a function to be used by others, I
>>might use "try", as 
>>described, e.g. in Venables and Ripley (2000) S
>>Programming (p. 48).
>>
>>hth.  spencer graves
>>
>>array chip wrote:
>>
>>>Hi, I am running cox regreesion (coxph) on a large
>>>number of independent variables, one variable at a
>>>time, using loop. At some point of the loop, the
>>
>>cox
>>
>>>regression stopped due to some errors. How can I
>>
>>know
>>
>>>at which variable the cox regression stopped so
>>
>>that I
>>
>>>can pinpoint the variable that causes the problem?
>>
>>I
>>
>>>guess this is not unique to cox regression, it is
>>
>>the
>>
>>>problem of catching the error/warning message
>>
>>promptly
>>
>>>in a loop.
>>>
>>>tahnks
>>>
>>>__________________________________
>>>

>>
>>to Outlook(TM).
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>
>>
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>>
> 
> 
> __________________________________
> Do you Yahoo!?

> http://calendar.yahoo.com



From ripley at stats.ox.ac.uk  Mon Jun  2 21:45:08 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 20:45:08 +0100 (BST)
Subject: [R] catch error/warning message in a loop
In-Reply-To: <20030602192929.24747.qmail@web41215.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306022044410.22636-100000@gannet.stats>

On Mon, 2 Jun 2003, array chip wrote:

> cat can only catch the error message promptly, but
> seems not able to catch the warning message promptly.
> Can anyone point out how i can catch the warning
> message in a loop?

options(warn=1) or = 2

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From postmaster at localhost.localdomain  Mon Jun  2 22:08:26 2003
From: postmaster at localhost.localdomain (postmaster@localhost.localdomain)
Date: Mon, 02 Jun 2003 22:08:26 +0200
Subject: [R] Inflex scan report [0602220828420]
Message-ID: <200306022008.h52K8XA28464@localhost.localdomain>

Administrator Email Reply Address: postmaster
Email sent to:  info at winrar.it
Inflex ID: 0602220828420
Report Details -----------------------------------------------
AntiVirus Results...
Scanning /usr/local/inflex/tmp/inf_0602220828420/unpacked/*
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/_headers_
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/textfile0
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/textfile0/textfile0
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/textfile1
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/submited.pif
/usr/local/inflex/tmp/inf_0602220828420/unpacked/submited.pif
        Found the W32/Sobig.c at MM virus !!!
Scanning file /usr/local/inflex/tmp/inf_0602220828420/unpacked/textfile2

Summary report on /usr/local/inflex/tmp/inf_0602220828420/unpacked/*
File(s)
        Total files: ...........       6
        Clean: .................       5
        Possibly Infected: .....       1

File NAME/TYPE Scan Results


0602220828420  from:r-announce at hypatia.math.ethz.ch  to: info at winrar.itType scanning off.
Name scanning off.
Text scanning off.

END OF MESSAGE.

End.
.



From auto-filter at xtra.co.nz  Mon Jun  2 22:17:03 2003
From: auto-filter at xtra.co.nz (auto-filter@xtra.co.nz)
Date: Tue, 3 Jun 2003 08:17:03 +1200
Subject: [R] Virus Alert
Message-ID: <20030602201703.MEQM6898.mta202-rme.xtra.co.nz@localhost>

An attachment called (WORM_SOBIG.C) in an email that appears
to have been sent from your email address to (arrowcomp at xtra.co.nz)
contained the virus (WORM_SOBIG.C), which has been deleted.

If you do not believe you were the actual sender, the Klez virus is
likely to be the culprit. The Klez virus works by forging the 'From'
address inside the virus infected email, which means you can receive a
virus alert from Xtra even if you are not necessarily the actual sender.

Information on Xtra's anti-virus email filter:
http://xtra.co.nz/anti-virus

More on the Klez virus:
http://xtra.co.nz/help/0,,6156-1347943,00.html

Help with filtering anti-virus email alerts from Xtra:
http://xtra.co.nz/help/0,,6156-1656774,00.html

Help with removing a virus from your computer:
http://xtra.co.nz/help/0,,4128-544089,00.html

If you have any other questions, please forward this email along with
your enquiry to anti-virus at xtra.co.nz



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun  2 22:15:52 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 02 Jun 2003 21:15:52 +0100 (BST)
Subject: [R] Re: [Rd] RE: Approved
In-Reply-To: <x2wug4h6je.fsf@biostat.ku.dk>
Message-ID: <XFMail.030602195145.Ted.Harding@nessie.mcc.ac.uk>

On 02-Jun-03 Peter Dalgaard BSA wrote [to r-devel]:
> "BELL David J" <D.Bell at staffs.ac.uk> writes:
> 
>> No file attached??
> 
> Fairly well-known virus behaviour, but did you really get it from
> r-devel? (check headers for stat.math.ethz.ch)

There seem to be no instances of the actual virus-laden message in
either the r-devel or r-help archives.

This is a new variant (SoBig.C) of the SoBig virus which started
going the rounds a few weeks ago. That one was always "from"
support at microsoft.com, and it picked random "To:" address from
the user's [Windows] system. The new one does much the same, but
this time it also picks a random address to mail "from".

Hence those of you out there who subscribe to r-help or r-devel
and have been infected will from time to time send a virus attachment
as if "from" r-help or r-devel. When this hits a site that spots the
virus and bounces back an error message, the latter will be sent to the
apparent sender and so we will all get it -- without the attachment if
the site has done its job.

This new variant has been out since 31 May, and is not picked up by
virus-screening software capable of detecting the earlier version.
Only signature files updated since 31 May can do this, so you have
to update your virus signature files.

(Or migrate to Linux).

Best wishes to all,
Ted.
[PS the word is that this latest variant will expire on 8 June]



From mwgrant2001 at yahoo.com  Mon Jun  2 22:34:37 2003
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Mon, 2 Jun 2003 13:34:37 -0700 (PDT)
Subject: [R] persp & colors
In-Reply-To: <20030601172109.GA20216@pcf004.jinr.ru>
Message-ID: <20030602203437.15033.qmail@web20809.mail.yahoo.com>

Timur is this what you are looking for?

from Rtips at

http://lark.cc.ukans.edu/~pauljohn/R/statsRus.html#5.42

---------------------------------- Question: is it
possible to shade the 3d surface like a contour plot?
i.e. black for large z, white for small z, say 

Answer: 

# Create a simple surface f(x,y) = x^2 - y^2 

        nx <- 21
        ny <- 21
        x <- seq(-1, 1, length = nx)
        y <- seq(-1, 1, length = ny)
        z <- outer(x, y, function(x,y) x^2 - y^2)

        #  Average the values at the corner of each
facet
        #  and scale to a value in [0, 1].  We will
use this
        #  to select a gray for colouring the facet.

        hgt <- 0.25 * (z[-nx,-ny] + z[-1,-ny] +
z[-nx,-1] + z[-1,-1])
        hgt <- (hgt - min(hgt))/ (max(hgt) - min(hgt))

        #  Plot the surface with the specified facet
colours.

        persp(x, y, z, col = gray(1 - hgt), theta =
35)
        persp(x, y, z, col = cm.colors(10)[floor(9 *
hgt + 1)], theta = 35)

(from Ross Ihaka) 

Regards,
Michael Grant

--- Timur Elzhov <Timur.Elzhov at jinr.ru> wrote:
> On Sun, Jun 01, 2003 at 06:53:55PM +0200, Uwe Ligges
> wrote:
> 
> >> but I'd like to persp()' colors behave like in
> image() function!
> 
> > That's not easy, because you have to redefine x, y
> and z values.
> > 
> > Simple example:
> > 
> >  x <- y <- 1:2
> >  z <- matrix(1:4, 2)
> >  image(x, y, z)    # OK, quite nice
> > 
> > but
> > 
> >  persp(x, y, z)
> > 
> > has only one facet. So the only way is to
> calculate the 9 values for x, 
> > y, and z to get the corners for the 4 facets in
> it.
> > That's easy for x and y, but can be impossible for
> z...
> 
> OK, thank you for answer!
> But, I saw that other mathematic frameworks (CERN
> ROOT for instance)
> can plot 3D surfaces with colors corresponding to
> z-value.
> Is there way to do this in R (with another
> functions/packages)?
> It's not necessary to use _one_ color per facet,
> yes?.. :)
> 
> 
> --
> WBR,
> Timur.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at stat.ucla.edu  Mon Jun  2 22:36:54 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 02 Jun 2003 15:36:54 -0500
Subject: [R] legends on image plots
In-Reply-To: <67B92F9B2AFED611852500B0D0FE15097EAF61@mail3.marlab.ac.uk>
References: <67B92F9B2AFED611852500B0D0FE15097EAF61@mail3.marlab.ac.uk>
Message-ID: <3EDBB566.2020104@stat.ucla.edu>

You can try using filled.contour() or maybe levelplot() in the `lattice' 
package.

-roger

Douglas Beare wrote:

>Hi,
>Does anyone out there know how to add a legend when using the R-function
>image?
>Ie. is there something out there like the S+ function image.legend?
>Doug.
>Fisheries Research Services,
>Marine Laboratory,
>Victoria Road,
>Torry,
>Aberdeen, UK.
>Tel. 44 (0) 1224 295314
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From Benjamin.STABLER at odot.state.or.us  Mon Jun  2 23:08:55 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Mon, 2 Jun 2003 14:08:55 -0700 
Subject: [R] Building an R package under Windows NT
Message-ID: <76A000A82289D411952F001083F9DD06047FE0E3@exsalem4-bu.odot.state.or.us>

Thanks for the suggestions.  

1) I fixed the zip.exe PATH issue.
2) I removed unnecessary quotes around C:\Program Files\R
3) I ran Rcmd install emme2 with the following result:

---------- Making package emme2 ------------
  adding build stamp to DESCRIPTION
  installing R files
 175373 [main] sh 352 proc_subproc: Couldn't duplicate my handle<0xA4> for
pid 1736008448, Win32 error 6
  installing man source files
  33006 [main] sh 280 proc_subproc: Couldn't duplicate my handle<0xA0> for
pid 1736008448, Win32 error 6
  installing indices
make[1]: *** [indices] Error 255
make: *** [pkg-emme2] Error 2
*** Installation of emme2 failed ***


4) I ran Rcmd build --binary emme2 with the following results:


D:\>Rcmd build --binary emme2
* checking for file 'emme2/DESCRIPTION' ... OK

---------- Making package emme2 ------------
  adding build stamp to DESCRIPTION
  installing R files
 166100 [main] sh 310 proc_subproc: Couldn't duplicate my handle<0xA4> for
pid 1667855360, Win32 error 6
  installing man source files
  40843 [main] sh 294 proc_subproc: Couldn't duplicate my handle<0xA0> for
pid 1667855360, Win32 error 6
  installing indices
make[1]: *** [indices] Error 255
make: *** [pkg-emme2] Error 2
*** Installation of emme2 failed ***

installing R.css in c:/TEMP/Rbuild.308

* building 'emme2_0.1.zip'
  adding: emme2/ (stored 0%)
  adding: emme2/CONTENTS (deflated 35%)
  adding: emme2/DESCRIPTION (deflated 38%)
  adding: emme2/INDEX (deflated 26%)
  adding: emme2/man/ (stored 0%)
  adding: emme2/man/emme2.Rd (deflated 73%)
  adding: emme2/Meta/ (stored 0%)
  adding: emme2/Meta/Rd.rds (deflated 63%)
  adding: emme2/R/ (stored 0%)
  adding: emme2/R/emme2 (deflated 81%)

5) I ran Rcmd check emme2 with the following results:

D:\>Rcmd check emme2
* checking for working latex ... OK
* using log directory 'D://emme2.Rcheck'
* checking for file 'emme2/DESCRIPTION' ... OK
* checking if this is a source package ... OK

---------- Making package emme2 ------------
  adding build stamp to DESCRIPTION
  installing R files
 167548 [main] sh 73 proc_subproc: Couldn't duplicate my handle<0xA0> for
pid 1667855360, Win32 error 6
  installing man source files
  33391 [main] sh 337 proc_subproc: Couldn't duplicate my handle<0xA0> for
pid 1667855360, Win32 error 6
  installing indices
make[1]: *** [indices] Error 255
make: *** [pkg-emme2] Error 2
*** Installation of emme2 failed ***

installing R.css in D:/emme2.Rcheck

* checking package directory ... OK
* checking DESCRIPTION meta-information ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking generic/method consistency ... WARNING
plot:
  function(x, ...)
plot.links:
  function(link.data, nodes, centroids, ...)

* checking for assignment functions with final arg not named 'value' ... OK
* checking Rd files ... OK
* checking for undocumented objects ... ERROR
Error in undoc(package = "emme2") : directory 'D://emme2.Rcheck/emme2'
contains no help index


Thus I am not doing something correctly.  I was able to install the package
via the "install from local zip file" option in RGui though.  But I can't
search for any of my functions and the 00Index.html file was not created.
Do I need to create 00Index.html with Rdconv?  I thought maybe I should use
"Rcmd build --binary --docs=html emme2" but the "--docs=html" option is not
working for me.  The html is less important to me than the make working
correctly (atleast I am guessing so).  Any ideas as to why the make portion
of the check/build is failing?  Thanks.

Ben Stabler

>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Monday, June 02, 2003 11:56 AM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Building an R package under Windows NT
>
>
>Try to install the package first, via
>
>Rcmd INSTALL emme2
>
>as a binary build involves an install and a direct install may be more
>informative.
>
>The message about zip suggests you don't have the right things 
>first in 
>your path, so please read the file readme.packages and cross check.
>In particular, check that `zip -v' gives
>
>Copyright (C) 1990-1999 Info-ZIP
>Type 'zip "-L"' for software license.
>This is Zip 2.3 (November 29th 1999), by Info-ZIP.
>...
>
>(and that version does have an X argument).
>
>On Mon, 2 Jun 2003 Benjamin.STABLER at odot.state.or.us wrote:
>
>> I am trying to build a R 1.7 package under Windows NT.  I created the
>> DESCRIPTION file, the RD file and added the code to the R 
>folder.  I also
>> downloaded and installed the Rtools package and have Perl 
>5.0.  I know that
>> Perl, Miktex, and gcc are working.  I also have my PATH variable set
>> correctly.  I can fake install my package by removing the 
>*.R from the code
>> file, using Rcmd Rdconv to create the 00index.html file from 
>my Rd file, and
>> copying the package folder to the library folder.  But I 
>can't seem to get
>> Rcmd build to build a binary version of my package.  I can 
>run build without
>> the "--binary" option and I get all my files in a taz.gz file.  
>> 
>> D:\>Rcmd build emme2
>> * checking for file 'emme2/DESCRIPTION' ... OK
>> * preparing 'emme2':
>> * checking whether 'INDEX' is up-to-date ... OK
>> * removing junk files
>> * building 'emme2_0.1.tar.gz'
>> 
>> But when I try to build a precompiled binary package I get 
>the following
>> response:
>> 
>> D:\>Rcmd build --binary emme2
>> * checking for file 'emme2/DESCRIPTION' ... OK
>> 
>> make: *** [pkg-emme2] Error 255
>> *** Installation of emme2 failed ***
>> 
>> installing R.css in c:/TEMP/Rbuild.225
>> 
>> * building 'emme2_0.1.zip'
>> zip error: Invalid command arguments (no such option: X)
>> 
>> 
>> Any help would be appreciated.  Thanks.
>> 
>> Benjamin Stabler
>> Transportation Planning Analysis Unit
>> Oregon Department of Transportation
>> 555 13th Street NE, Suite 2
>> Salem, OR 97301  Ph: 503-986-4104
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From LowdenJK at bernstein.com  Mon Jun  2 23:08:50 2003
From: LowdenJK at bernstein.com (Lowden, James K)
Date: Mon, 2 Jun 2003 17:08:50 -0400
Subject: [R] Very slow startup on Win32
Message-ID: <0BC7BDCCBEAED51187340002B35CA0B507C5EACF@acnyexg02.beehive.com>

[Duncan Murdoch advised me offlist.  Thanks for your help, Duncan.  I tried to reply directly but your spam filter doesn't like my corporate email setup.]

I turned off all the initialization options I could, and set --verbose.  It seems like each step is slow, rather than racing to one point and stopping for a long time.  

The machine is behind a firewall with an http proxy, and I haven't found out how (if possible) to tell R about it.  Does that matter at startup time?  

Trying to add a package yields:

> local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
unable to connect to 'cran.r-project.org' on port 80.
Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
        cannot open URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'

even though that URL works fine in IE.  I understand I should be able to use Rcmd to install packages, but its perl scripts appear to be missing:

$ rcmd  --version
Can't open perl script "D:\Programs\R\rw1070/bin/--version": No such file or directory

Among the commands listed in in the appendix, the only installed script is rw1070\bin\BATCH.  

At the moment, it seems:

1.  I cannot install packages via Rgui, because I don't have nonproxied access to the internet.  
2.  I cannot install packages via Rcmd.

What to do?  

Thanks very much.  

--jkl

-----------------------------------------
The information contained in this transmission may contain privileged and confidential information and is intended only for the use of the person(s) named above. If you are not the intended recipient, or an employee or agent responsible for delivering this message to the intended recipient, any review, dissemination, distribution or duplication of this communication is strictly prohibited. If you are not the intended recipient, please contact the sender immediately by reply e-mail and destroy all copies of the original message. Please note that we do not accept account orders and/or instructions by e-mail, and therefore will not be responsible for carrying out such orders and/or instructions.



From LowdenJK at bernstein.com  Mon Jun  2 23:18:42 2003
From: LowdenJK at bernstein.com (Lowden, James K)
Date: Mon, 2 Jun 2003 17:18:42 -0400
Subject: [R] Very slow startup on Win32
Message-ID: <0BC7BDCCBEAED51187340002B35CA0B507C5EAD0@acnyexg02.beehive.com>

> From: Lowden, James K [mailto:LowdenJK at bernstein.com]
> Sent: June 2, 2003 5:09 PM
>
> At the moment, it seems:
> 
> 1.  I cannot install packages via Rgui, because I don't have 
> nonproxied access to the internet.  
> 2.  I cannot install packages via Rcmd.
> 
> What to do?  

Never mind.  A little RTFM time found the "install from a zip file" option.  

I'm still stuck wrt the start up time, though.  

thanks.

--jkl
-----------------------------------------
The information contained in this transmission may contain privileged and confidential information and is intended only for the use of the person(s) named above. If you are not the intended recipient, or an employee or agent responsible for delivering this message to the intended recipient, any review, dissemination, distribution or duplication of this communication is strictly prohibited. If you are not the intended recipient, please contact the sender immediately by reply e-mail and destroy all copies of the original message. Please note that we do not accept account orders and/or instructions by e-mail, and therefore will not be responsible for carrying out such orders and/or instructions.



From abunn at montana.edu  Mon Jun  2 23:20:50 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 02 Jun 2003 21:20:50 -0000
Subject: [R] panel function problem with ps
Message-ID: <1054588804.12517.26.camel@facet.msu.montana.edu>

Hi, I have an elaborate coplot for which I had to write my very first
panel statement. It looks fine on the screen (in an x11 device) but does
not go to postscript correctly. Not even close actually. This is the
only one of my r scripts that hasn't run perfectly under linux and I'm
generally very pleased at how smoothly the transistion has been from R
under windows to ESS-style R.

However, I've made this plot under windows and saved it just fine and am
ready to admit a problem with my panel function. I'm just not sure what
it is. 

The coplot is a double conditioning plot and I'm adding a spline to each
panel. So, I'm trying to use the subscripts argument.

#~~~~~~~~~~~~~~~~~~~~~~~~~~~
postscript("foo.ps")
coplot(x.var ~ y.var | m.var * n.var,
       given.v = list(m.var.overlap, n.var.overlap),
       subscripts = T, aspect = "xy",
       panel = function(x, y, subscripts, ...){

       #Add points
       points(x, y, col = "gray50")

       #Make and draw a spline in each panel
       spline.model <- smooth.spline(y.var[subscripts],
                                     x.var[subscripts])
       thefits <- predict(spline.model, y.var)$y
       sp <- spline(x, thefits[subscripts])

       #Add lines
       lines(sp$x, sp$y, col = "black", lwd = 4)
       })
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Any help appreciated.

-Andy



From edd at debian.org  Mon Jun  2 23:29:57 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 2 Jun 2003 16:29:57 -0500
Subject: [R] Very slow startup on Win32
In-Reply-To: <0BC7BDCCBEAED51187340002B35CA0B507C5EACF@acnyexg02.beehive.com>
References: <0BC7BDCCBEAED51187340002B35CA0B507C5EACF@acnyexg02.beehive.com>
Message-ID: <20030602212957.GA800@sonny.eddelbuettel.com>

On Mon, Jun 02, 2003 at 05:08:50PM -0400, Lowden, James K wrote:
> The machine is behind a firewall with an http proxy, and I haven't found out how (if possible) to tell R about it.  Does that matter at startup time?  
> 
> Trying to add a package yields:
> 
> > local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> unable to connect to 'cran.r-project.org' on port 80.
> Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
>         cannot open URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> 
> even though that URL works fine in IE.  I understand I should be able to use Rcmd to install packages, but its perl scripts appear to be missing:

Ther error fairly obvious: cannot open URL, and you even state that your
proxied.

> What to do?  

Search the mailing list archives, this gets discussed at least once a month.
There are two options, one invoing the internet2.dll, the other involving
wget. You get to pick one.

Hth, Dirk


-- 
Don't drink and derive. Alcohol and analysis don't mix.



From ripley at stats.ox.ac.uk  Mon Jun  2 23:36:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jun 2003 22:36:09 +0100 (BST)
Subject: [R] Building an R package under Windows NT
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE0E3@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0306022231451.22952-100000@gannet.stats>

On Mon, 2 Jun 2003 Benjamin.STABLER at odot.state.or.us wrote:

> Thanks for the suggestions.  
> 
> 1) I fixed the zip.exe PATH issue.
> 2) I removed unnecessary quotes around C:\Program Files\R
> 3) I ran Rcmd install emme2 with the following result:
> 
> ---------- Making package emme2 ------------
>   adding build stamp to DESCRIPTION
>   installing R files
>  175373 [main] sh 352 proc_subproc: Couldn't duplicate my handle<0xA4> for
> pid 1736008448, Win32 error 6
>   installing man source files
>   33006 [main] sh 280 proc_subproc: Couldn't duplicate my handle<0xA0> for
> pid 1736008448, Win32 error 6
>   installing indices
> make[1]: *** [indices] Error 255
> make: *** [pkg-emme2] Error 2
> *** Installation of emme2 failed ***

[Lots of duplicates of these messages removed.]

Do check that you *really* have the right versions of the tools, as this 
is yet another symptom of incorrect versions.

> Thus I am not doing something correctly.  I was able to install the package
> via the "install from local zip file" option in RGui though.  But I can't
> search for any of my functions and the 00Index.html file was not created.
> Do I need to create 00Index.html with Rdconv?  I thought maybe I should use
> "Rcmd build --binary --docs=html emme2" but the "--docs=html" option is not
> working for me.  The html is less important to me than the make working

Given that it is nowhere documented to work, why do you expect it to?  
Try Rcmd INSTALL --help to see the valid values of TYPE.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From support at skillsoft.com  Tue Jun  3 00:51:05 2003
From: support at skillsoft.com (support@skillsoft.com)
Date: Mon, 2 Jun 2003 19:51:05 -0300
Subject: [R] Re:   Approved  #DF/021859#
Message-ID: <OFEA494974.A9D22863-ON84256D39.007D86D6-84256D39.007D86D7@amr.smtf.ds>

This is an automated response.  SkillSoft Support has
received your message and this issue has been assigned
ticket number :  DF/021859

Please ensure that  #DF/021859#  appears in the subject
line of any further emails on this issue so that we can
identify the original issue, and you are not allocated
a new ticket number.

Thank You.



From NAVMSE-HTPRSRV2 at pwcwireless.com  Tue Jun  3 01:17:04 2003
From: NAVMSE-HTPRSRV2 at pwcwireless.com (NAVMSE-HTPRSRV2)
Date: Mon, 2 Jun 2003 18:17:04 -0500
Subject: [R] Norton AntiVirus detected and quarantined a virus in a message
	you sent.
Message-ID: <69427C6E10F3E04BA5C52F249FFBA4FA2ADDBB@htprsrv2.pwcwireless.com>

Recipient of the infected attachment:  HTPRSRV2, First Storage Group\Private Information Store (HTPRSRV2), Bruce Bassett/Inbox
Subject of the message:  Re: Screensaver
One or more attachments were quarantined.
  Attachment documents.pif was Quarantined for the following reasons:
    Virus W32.Sobig.C at mm was found.



From dmurdoch at pair.com  Tue Jun  3 02:09:21 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 02 Jun 2003 20:09:21 -0400
Subject: [R] Building an R package under Windows NT
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE0E3@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD06047FE0E3@exsalem4-bu.odot.state.or.us>
Message-ID: <k3ondv80lsp0q0dd6an0pdqkgeh8g81a04@4ax.com>

On Mon, 2 Jun 2003 14:08:55 -0700 , you wrote:

>Thanks for the suggestions.  
>
>1) I fixed the zip.exe PATH issue.
>2) I removed unnecessary quotes around C:\Program Files\R
>3) I ran Rcmd install emme2 with the following result:

Where is emme2?  You shouldn't keep the source in the
$RHOME/library/emme2 directory, which is where it will be installed.

>---------- Making package emme2 ------------
>  adding build stamp to DESCRIPTION
>  installing R files
> 175373 [main] sh 352 proc_subproc: Couldn't duplicate my handle<0xA4> for
>pid 1736008448, Win32 error 6

This is an error in sh.exe.  Are you getting the one from the R
toolset, or possibly another one?

Duncan Murdoch



From hedderik at cmu.edu  Tue Jun  3 02:50:20 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Mon, 2 Jun 2003 20:50:20 -0400
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
Message-ID: <5A1B0066-955D-11D7-9356-000393678426@cmu.edu>

Does this script do what you want?

cround <- function(x,digits=0) {
   a <- ifelse(x>0,.5,-.5)
   if (digits==0) {
     floor(x+a)
   } else {
     m <- 10^digits
     floor(x*m+a)/m
   }
}

 > cround(1.4535,1)
[1] 1.5
 > cround(1.4535,2)
[1] 1.45
 > cround(1.4535,3)
[1] 1.454
 > cround(1.4535,4)
[1] 1.4535


   - Hedderik.



From Simon.Gatehouse at csiro.au  Tue Jun  3 02:59:56 2003
From: Simon.Gatehouse at csiro.au (Simon.Gatehouse@csiro.au)
Date: Tue, 3 Jun 2003 10:59:56 +1000 
Subject: [R] winMenuAdd misbehaving?
Message-ID: <FFE02AF26875734B82A728403821CB2EFB04B0@asp-RI.riverside.CSIRO.AU>

Microsoft Windows R users,

I am operating Windows 2000 (build2195) with R1.7.  It may be pertinent that
I am using a dual head screen with the initial RGui filing the entire area
of both screens.

When starting R I use the .First() function to add menu items to the RGui
interface using winMenuAdd() and winMenuAddItem().  The menus do not display
until the RGui window is physically manipulated in some way. In R1.6.2 the
menus were immediately available without the need to manipulate the window.
In all other ways the menus function as previously.

Questions;
1) does anyone else get the same behaviour?

2) If this is intended for some reason, and not a bug, then is there a way
of invoking the update of the menu bar from within .First() or perhaps using
command line arguement?  

Perhaps I'm missing something obvious. Any suggestions are most welcome.

Regards
Simon Gatehouse

***********************************                                  
CSIRO Exploration and Mining,
Newbigin Close off Julius Ave
North Ryde, NSW
 
Mail:      PO Box 136, North Ryde
           NSW 1670, Australia
Phone:     61 (2) 9490 8677
Fax:       61 (2) 9490 8921
Mobile:    61  0407 130 635 
E-mail:    simon.gatehouse at csiro.au
Web Page:  http://www.csiro.au/



From dmurdoch at pair.com  Tue Jun  3 03:40:41 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 02 Jun 2003 21:40:41 -0400
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
Message-ID: <amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>

On Mon, 2 Jun 2003 20:50:20 -0400, you wrote:

>Does this script do what you want?
>
>cround <- function(x,digits=0) {
>   a <- ifelse(x>0,.5,-.5)
>   if (digits==0) {
>     floor(x+a)
>   } else {
>     m <- 10^digits
>     floor(x*m+a)/m
>   }
>}

No, the problem is that R uses binary formats, and some numbers aren't
representable there.  So for example,

> cround(0.145,2)
[1] 0.14

because 0.145 isn't representable exactly, and is actually being
represented as 0.14999999999 or something similar.

Duncan Murdoch



From dmurdoch at pair.com  Tue Jun  3 03:51:48 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 02 Jun 2003 21:51:48 -0400
Subject: [R] winMenuAdd misbehaving?
In-Reply-To: <FFE02AF26875734B82A728403821CB2EFB04B0@asp-RI.riverside.CSIRO.AU>
References: <FFE02AF26875734B82A728403821CB2EFB04B0@asp-RI.riverside.CSIRO.AU>
Message-ID: <l6vndv8mi5r560vbmfr98mmpa4vo5652t5@4ax.com>

On Tue, 3 Jun 2003 10:59:56 +1000 , you wrote:

>Microsoft Windows R users,
>
>I am operating Windows 2000 (build2195) with R1.7.  It may be pertinent that
>I am using a dual head screen with the initial RGui filing the entire area
>of both screens.
>
>When starting R I use the .First() function to add menu items to the RGui
>interface using winMenuAdd() and winMenuAddItem().  The menus do not display
>until the RGui window is physically manipulated in some way. 

This is bug 2817; it should be fixed in the current beta version of
1.7.1; please give it a try.  You can download it from
<http://www.stats.uwo.ca/faculty/murdoch/software/r-devel>.

Duncan Murdoch



From c.wilcox at uq.edu.au  Tue Jun  3 06:25:54 2003
From: c.wilcox at uq.edu.au (Chris Wilcox)
Date: Tue, 03 Jun 2003 14:25:54 +1000
Subject: [R] GLM(...,start,...) question
Message-ID: <BB026071.4D5A%c.wilcox@uq.edu.au>

I am trying to use the glm.nb proceedure in the new MASS library.  I have
count data (of snails) taken from a number of habitats (artesian springs).
I want to model the abundances, including an effect of the organic content
of the sediment.  When I try to fit a model like:

nb.model.p <- glm.nb(abundance~Organics.per.wt)

I get the error message:

Error: no valid set of coefficients has been found:please supply starting
values

I looked in the help for the glm proceedure and found the "start()"
statement for specifying coefficients.  However, I could not find much
documentation on what should be in start.  I searched the R website and also
didn't find much detail on start().  On a lark I tried the following, just
to see if I had the syntax correct:

nb.model.p <- glm.nb(abundance~Organics.per.wt,start=(1,1))

which I did not.  I had a look in Venables and Ripley, but could not find an
example there either.  So if anyone can point me in the direction of the
proper documentation or provide any helpful input I would greatly appreciate
it.

Thanks,

Chris Wilcox




Dr. Chris Wilcox
Research Fellow
Department of Zoology and Entomology
University of Queensland
St. Lucia 4072  QLD
Australia
email: c.wilcox at uq.edu.au
tel: 61.7.3365.1686
fax: 61.7.3365.1655
website: www.uq.edu.au/~uqcwilco

"In God we trust all others must bring data"  W. Edwards Deming



From Simon.Gatehouse at csiro.au  Tue Jun  3 06:27:52 2003
From: Simon.Gatehouse at csiro.au (Simon.Gatehouse@csiro.au)
Date: Tue, 3 Jun 2003 14:27:52 +1000 
Subject: [R] winMenuAdd misbehaving?
Message-ID: <FFE02AF26875734B82A728403821CB2EFB04B5@asp-RI.riverside.CSIRO.AU>

R1.7.1beta seems to be fine.
Many Thanks Duncan
SimonG

-----Original Message-----
From: Duncan Murdoch [mailto:dmurdoch at pair.com]
Sent: Tuesday, June 03, 2003 11:52 AM
To: Simon.Gatehouse at csiro.au
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] winMenuAdd misbehaving?


On Tue, 3 Jun 2003 10:59:56 +1000 , you wrote:

>Microsoft Windows R users,
>
>I am operating Windows 2000 (build2195) with R1.7.  It may be pertinent
that
>I am using a dual head screen with the initial RGui filing the entire area
>of both screens.
>
>When starting R I use the .First() function to add menu items to the RGui
>interface using winMenuAdd() and winMenuAddItem().  The menus do not
display
>until the RGui window is physically manipulated in some way. 

This is bug 2817; it should be fixed in the current beta version of
1.7.1; please give it a try.  You can download it from
<http://www.stats.uwo.ca/faculty/murdoch/software/r-devel>.

Duncan Murdoch



From c.wilcox at uq.edu.au  Tue Jun  3 06:39:27 2003
From: c.wilcox at uq.edu.au (Chris Wilcox)
Date: Tue, 03 Jun 2003 14:39:27 +1000
Subject: [R] GLM(...,start,...) question - disregard please
Message-ID: <BB02639F.4D5B%c.wilcox@uq.edu.au>

Sorry to take everyone's time.  I found a negative value that was causing
the problem.


Chris Wilcox


Dr. Chris Wilcox
Research Fellow
Department of Zoology and Entomology
University of Queensland
St. Lucia 4072  QLD
Australia
email: c.wilcox at uq.edu.au
tel: 61.7.3365.1686
fax: 61.7.3365.1655
website: www.uq.edu.au/~uqcwilco

"In God we trust all others must bring data"  W. Edwards Deming



From cmsclaud at uga.edu  Tue Jun  3 09:05:02 2003
From: cmsclaud at uga.edu (Claude Rubinson)
Date: Tue, 3 Jun 2003 00:05:02 -0700
Subject: Rtips (was Re: [R] ? building a database with a the great examples
In-Reply-To: <3EDB8724.6050602@ku.edu>
References: <p05200f03bafab75c5986@[128.40.218.142]> <3EDB8724.6050602@ku.edu>
Message-ID: <20030603070502.GA14264@wagner>

On Mon, Jun 02, 2003 at 12:19:32PM -0500, Paul E. Johnson wrote:
> Perhaps you want to start maintaining Rtips itself!

> I'm thinking of dropping altogether the section on packages because
> I just can't keep up with the creativity of the R community.

I'm wondering if there's demand/interest in a cookbook-style site
along the lines of ActiveState/O'Reilly's Perl and Python cookbook
sites.  (If you're not familiar with the cookbooks, they're databases
of user-supplied "recipes" illustrating various programming tips and
techniques.)  The advantage of the cookbooks are that, since users
supply the content, they take much of the demand off of the
maintainer(s).

I figure that a cookbook would probably be welcome (after all, who
doesn't like more information?).  So, I guess the real question is if
there are enough people who would be willing to contribute recipes on
an ongoing basis?  The R community is certainly smaller than that of
Perl or Python.

Thoughts?

Claude



From kurt.sys at UGent.be  Tue Jun  3 09:17:47 2003
From: kurt.sys at UGent.be (Kurt Sys)
Date: Tue, 3 Jun 2003 09:17:47 +0200
Subject: [R] Virus Alert
In-Reply-To: <20030602201703.MEQM6898.mta202-rme.xtra.co.nz@localhost>
References: <20030602201703.MEQM6898.mta202-rme.xtra.co.nz@localhost>
Message-ID: <16092.19355.394328.908482@ksys.rug.ac.be>

Hello,


just to be sure: I already got a few messages like the one below,
about virus Sobig.C. They seem to be send to
r-help at stat.math.ethz.ch. I hope everybody on the list gets them, or
something is very wrong with my system. I checked for virusses (using
both f-prot and antivir), but couldn't find any. Moveover, as far as I
know, a Linux system can't be infected by Sobig C.

So, I wonder: is everyone on the list getting these messages? is it my
system, or somebody else's? if it's mine, why is it not found (I have
recent definition files)? if it's not, can I found out who's system is
infected?


Kurt.




--
Mail from auto-filter at xtra.co.nz
sent on Tuesday June 3 2003 at 08:17 (GMT+1200):

   An attachment called (WORM_SOBIG.C) in an email that appears to
   have been sent from your email address to (arrowcomp at xtra.co.nz)
   contained the virus (WORM_SOBIG.C), which has been deleted.
   
   If you do not believe you were the actual sender, the Klez virus is
   likely to be the culprit. The Klez virus works by forging the
   'From' address inside the virus infected email, which means you can
   receive a virus alert from Xtra even if you are not necessarily the
   actual sender.
   
   Information on Xtra's anti-virus email filter:
   http://xtra.co.nz/anti-virus
   
   More on the Klez virus:
   http://xtra.co.nz/help/0,,6156-1347943,00.html
   
   Help with filtering anti-virus email alerts from Xtra:
   http://xtra.co.nz/help/0,,6156-1656774,00.html
   
   Help with removing a virus from your computer:
   http://xtra.co.nz/help/0,,4128-544089,00.html
   
   If you have any other questions, please forward this email along
   with your enquiry to anti-virus at xtra.co.nz



From p.dalgaard at biostat.ku.dk  Tue Jun  3 09:40:35 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 03 Jun 2003 07:40:35 -0000
Subject: [R] Virus Alert
In-Reply-To: <16092.19355.394328.908482@ksys.rug.ac.be>
References: <20030602201703.MEQM6898.mta202-rme.xtra.co.nz@localhost>
	<16092.19355.394328.908482@ksys.rug.ac.be>
Message-ID: <x2n0gzbnzp.fsf@biostat.ku.dk>

Kurt Sys <kurt.sys at UGent.be> writes:

> Hello,
> 
> 
> just to be sure: I already got a few messages like the one below,
> about virus Sobig.C. They seem to be send to
> r-help at stat.math.ethz.ch. I hope everybody on the list gets them, or
> something is very wrong with my system. I checked for virusses (using
> both f-prot and antivir), but couldn't find any. Moveover, as far as I
> know, a Linux system can't be infected by Sobig C.
> 
> So, I wonder: is everyone on the list getting these messages? is it my
> system, or somebody else's? if it's mine, why is it not found (I have
> recent definition files)? if it's not, can I found out who's system is
> infected?

I think we've seen that one before and quite a few others over the
last few days. 

It's really difficult to tell who the culprit is because all we're
seeing are autoreplies from someones system to an infected mail that
purported to be from r-help.

The original message headers might be able to tell us who is infected
(which may or may not be someone on the list, depending on where the
virus is grabbing its From: headers from).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From henric.nilsson at statisticon.se  Tue Jun  3 10:22:15 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Tue, 03 Jun 2003 10:22:15 +0200
Subject: [R] gam questions
Message-ID: <5.2.1.1.0.20030603091708.03e92720@pop3.norton.antivirus>

Dear all,

I'm a fairly new R user having two questions regarding gam:

1. The prediction example on p. 38 in the mgcv manual. In order to get 
predictions based on the original data set, by leaving out the 'newdata' 
argument ("newd" in the example), I get an error message

"Warning message: the condition has length > 1 and only the first element 
will be used in: if (object$dim == 0) m <- 0 else m <- length(object$sp)"

I suspected that it had somthing to do with the data not being attached, 
but when fitting a gam with an attached data set I got the same error. Why?


2. I've fitted a glm y~a+x+a:x, where a is a 3 level factor and x is a 
continuous covariate. If I want to fit a similar gam model, is it correct 
to fit y~a+s(x)+s(x,by=a.1)+s(x,by=a.2)+s(x,by=a.3), where a.1--a.3 are 
dummy variables representing each level of the factor? Or is the s(x) term 
redundant?

Any hints are greatly appreciated.

Best wishes,
Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From mikewhite.diu at tiscali.co.uk  Tue Jun  3 10:49:44 2003
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Tue, 3 Jun 2003 09:49:44 +0100
Subject: [R] Rounding problem R vs Excel
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer><5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
Message-ID: <00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>

Duncan
If the numbers are not represently exactly how does R resolve problems like
the one below? Is there something that needs to be set up in the R
environment like the number of significant figures?

> x<-4.145*100+0.5
> x
[1] 415
> floor(x)
[1] 414
> as.integer(x)
[1] 414
> trunc(x)
[1] 414

Mike White
----- Original Message -----
From: "Duncan Murdoch" <dmurdoch at pair.com>
To: "Hedderik van Rijn" <hedderik at cmu.edu>
Cc: <R-help at stat.math.ethz.ch>
Sent: Tuesday, June 03, 2003 2:40 AM
Subject: Re: [R] Rounding problem R vs Excel


> On Mon, 2 Jun 2003 20:50:20 -0400, you wrote:
>
> >Does this script do what you want?
> >
> >cround <- function(x,digits=0) {
> >   a <- ifelse(x>0,.5,-.5)
> >   if (digits==0) {
> >     floor(x+a)
> >   } else {
> >     m <- 10^digits
> >     floor(x*m+a)/m
> >   }
> >}
>
> No, the problem is that R uses binary formats, and some numbers aren't
> representable there.  So for example,
>
> > cround(0.145,2)
> [1] 0.14
>
> because 0.145 isn't representable exactly, and is actually being
> represented as 0.14999999999 or something similar.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From temiz at deprem.gov.tr  Tue Jun  3 10:54:02 2003
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 03 Jun 2003 11:54:02 +0300
Subject: [R] coefficient of logistic regression
Message-ID: <3EDC622A.5060201@deprem.gov.tr>

Hello

in logistic regression,
I want to know that it is possible to get probability values of each 
predictors by
using following formula for each predictor one by one (keeping constant 
the others)
 <<< exp(coef)/(1+exp(coef)) >>>

thanks in advance

Ahmet Temiz


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From Detlef.Steuer at unibw-hamburg.de  Tue Jun  3 11:00:46 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Tue, 03 Jun 2003 11:00:46 +0200 (CEST)
Subject: Rtips (was Re: [R] ? building a database with a the great examples
In-Reply-To: <3EDB8724.6050602@ku.edu>
Message-ID: <XFMail.20030603110046.steuer@unibw-hamburg.de>


On 02-Jun-2003 Paul E. Johnson wrote:
> Perhaps you want to start maintaining Rtips itself!  

Perhaps it is time to start a wiki for R?
For those not familiar with the idea of wikis look here:
http://www.wikipedia.org/ (incredible wiki encyclopedia)
http://www.wikipedia.org/wiki/WikiWiki (for a description of the mechanisms of
wikiwikis)

I just did a quick hack to set one up:
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

Any comments welcome!

If the community uses the site I promise to do what I can to keep it 
running.

Detlef

> 
> I don't have as much time for it as I used to.  I still think it is 
> valuable to have such a listing, but it is so hard to keep up to date 
> with R and I'm only doing minimal work to keep it up to date.  I'm 
> thinking of dropping altogether the section on packages because I just 
> can't keep up with the creativity of the R community.  I did most of 
> that list before Frank Harrell put up Hmisc and Design, you know.
> 
> The Rtips page is still up here:
> 
> http://lark.cc.ukans.edu/~pauljohn/R/statsRus.html
> 
> The FaqManager software is starting to show its age, but it still works. 
>  If you want to be a contributor, I could assign for you a password on 
> that server.  
> 
> Also, I think it will work if you go to this other name I have created 
> for this same thing:
> 
> http://lark.cc.ukans.edu/~pauljohn/R/Rtips.html
> 
> Some users write and ask me why I did not contribute this doc to the R 
> homepage, and I have no good answer except that in the beginning I did 
> not have enough stuff to make it worthwhile.  Since the content of Rtips 
> can change on a daily/weekly basis, it did not seem right to just make a 
> snapshot and email it over. But I'm open to suggestions.
> 
> Frank Mattes wrote:
> 
>> Dear R help reader,
>>
>> I'm not an expert in R and are lerning a lot by reading the help 
>> digest, which is sometimes difficult because the huge amount of data 
>> posted. I have posted some questions before, and  are impressed how 
>> quick I got a solution for my problem. Sometimes with quite different 
>> suggestions. I was always wondering if my questions didn't come up 
>> before. On the other site, it wasn't easy to search the help archive, 
>> purely I didn't know how to formulate my problem.
>> I'm wondering if we could not collect all the answers / examples in a 
>> database -
>> sorted in topics, like the help document "Rtips".
>> I have no clue if this is possible to do nor how time consuming the 
>> maintaining would be.
>>
>> This is just my view how the help list could be improved
>>
>> Yours
>> Frank
> 
> 
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ukans.edu
> Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66045                FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

"There is no way to peace, peace is the way." -- Ghandi

Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From ripley at stats.ox.ac.uk  Tue Jun  3 11:07:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 10:07:16 +0100 (BST)
Subject: [R] gam questions
In-Reply-To: <5.2.1.1.0.20030603091708.03e92720@pop3.norton.antivirus>
Message-ID: <Pine.LNX.4.44.0306031002180.7851-100000@gannet.stats>

On Tue, 3 Jun 2003, Henric Nilsson wrote:

> I'm a fairly new R user having two questions regarding gam:
> 
> 1. The prediction example on p. 38 in the mgcv manual. 

That's not very helpful: pagination of manuals depends on the paper size, 
for example.

> In order to get 
> predictions based on the original data set, by leaving out the 'newdata' 
> argument ("newd" in the example), I get an error message
> 
> "Warning message: the condition has length > 1 and only the first element 
> will be used in: if (object$dim == 0) m <- 0 else m <- length(object$sp)"
> 
> I suspected that it had somthing to do with the data not being attached, 
> but when fitting a gam with an attached data set I got the same error. Why?

That is a warning not an error!  It was a bug in mgcv a while back.
Do you have the latest version of mgcv (and of R, for that matter)?
I am not seeing this any more.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun  3 11:13:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 10:13:28 +0100 (BST)
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
Message-ID: <Pine.LNX.4.44.0306031008100.7851-100000@gannet.stats>

On Tue, 3 Jun 2003, Mike White wrote:

> If the numbers are not represently exactly how does R resolve problems like
> the one below? Is there something that needs to be set up in the R

What problem exactly?

> environment like the number of significant figures?

Printing is controlled by options("digits"): is that what you had in mind?

> > x<-4.145*100+0.5
> > x
> [1] 415

but
> print(x, digits=16)
[1] 414.9999999999999
> x - 415  # which would be exact if x were integer
[1] -5.684342e-14

> > floor(x)
> [1] 414
> > as.integer(x)
> [1] 414
> > trunc(x)
> [1] 414

All as expected as 414 < x < 415.

Don't confuse the printed representation of an object with the object 
itself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From simon at stats.gla.ac.uk  Tue Jun  3 11:43:36 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 3 Jun 2003 10:43:36 +0100 (BST)
Subject: [R] gam questions
In-Reply-To: <5.2.1.1.0.20030603091708.03e92720@pop3.norton.antivirus>
Message-ID: <Pine.SOL.3.96.1030603103727.11065D-100000@moon.stats.gla.ac.uk>

> 2. I've fitted a glm y~a+x+a:x, where a is a 3 level factor and x is a 
> continuous covariate. If I want to fit a similar gam model, is it correct 
> to fit y~a+s(x)+s(x,by=a.1)+s(x,by=a.2)+s(x,by=a.3), where a.1--a.3 are 
> dummy variables representing each level of the factor? Or is the s(x) term 
> redundant?
- yes the s(x) term is redundant and in the current mgcv version will
likely cause spectacular nonsense as a result of lack of identifiability
in the smooth part of the model (mgcv 0.9 will cope with this when
released, but it's still much better to use an identifiable smooth
model). 

Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From ripley at stats.ox.ac.uk  Tue Jun  3 11:45:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 10:45:16 +0100 (BST)
Subject: [R] Packages for Windows
Message-ID: <Pine.LNX.4.44.0306031037020.7951-100000@gannet.stats>

We are very grateful that Uwe Ligges has taken over the production of the
pre-compiled Windows versions of CRAN packages.  This has been mainly
automated, and only packages that build out-of-the-box will be available
from CRAN.  A few others (SJava, XML, gstat, netCDF, xgobi) are available
from http://www.stats.ox.ac.uk/pub/RWin.

Running update.packages() (or updating packages from the RGui menu)  will
update packages for the last month or so.  Be aware that the bundles VR
and dse cannot be updated this way in rw1070 because of a bug:  you need
to download the zip files and manually unzip them in rw1070/library.


The packages built by Uwe are currently on the Vienna master and will
propagate around CRAN shortly.  They should automatically reflect new or 
updated packages on CRAN (once the mirroring glitches are sorted out).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.lehmann at gmx.ch  Tue Jun  3 11:58:39 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 03 Jun 2003 11:58:39 +0200
Subject: [R] lda: how to get the eigenvalues
Message-ID: <1054634319.18425.16.camel@christophl>

Dear R-users
How can I get the eigenvalues out of an lda analysis?

thanks a lot

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ripley at stats.ox.ac.uk  Tue Jun  3 12:13:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 11:13:26 +0100 (BST)
Subject: [R] lda: how to get the eigenvalues
In-Reply-To: <1054634319.18425.16.camel@christophl>
Message-ID: <Pine.LNX.4.44.0306031111070.8036-100000@gannet.stats>

On Tue, 3 Jun 2003, Christoph Lehmann wrote:

> How can I get the eigenvalues out of an lda analysis?

It uses singular values not eigenvalues: see ?lda for a description of the
output, and the print method for one way to use them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From henric.nilsson at statisticon.se  Tue Jun  3 12:26:42 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Tue, 03 Jun 2003 12:26:42 +0200
Subject: [R] gam questions
In-Reply-To: <Pine.LNX.4.44.0306031002180.7851-100000@gannet.stats>
References: <5.2.1.1.0.20030603091708.03e92720@pop3.norton.antivirus>
Message-ID: <5.2.1.1.0.20030603111551.03f51920@pop3.norton.antivirus>

At 10:07 2003-06-03 +0100, Prof Brian Ripley wrote:

> > 1. The prediction example on p. 38 in the mgcv manual.
>That's not very helpful: pagination of manuals depends on the paper size,
>for example.

It's the gam.predict example. Instead of pred<-predict.gam(b,newd) I tried 
pred<-predict.gam(b).

>Do you have the latest version of mgcv (and of R, for that matter)?

I'm running R 1.7.0 under Windows 2000 with mgcv 0.8-8.

Best,
Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From dray at biomserv.univ-lyon1.fr  Tue Jun  3 12:31:17 2003
From: dray at biomserv.univ-lyon1.fr (Stephane Dray)
Date: Tue, 3 Jun 2003 12:31:17 +0200
Subject: [R] lda: how to get the eigenvalues
In-Reply-To: <Pine.LNX.4.44.0306031111070.8036-100000@gannet.stats>
References: <Pine.LNX.4.44.0306031111070.8036-100000@gannet.stats>
Message-ID: <a05010405bb0228ffbc9b@[134.214.32.69]>

>On Tue, 3 Jun 2003, Christoph Lehmann wrote:
>
>>  How can I get the eigenvalues out of an lda analysis?
>
>It uses singular values not eigenvalues: see ?lda for a description of the
>output, and the print method for one way to use them.

the function discrimin ofthe ade4 package performs discriminat 
analysis with eigen and so produces eigenvalues ($eig)
-- 
St?phane DRAY
---------------------------------------------------------------
Biom?trie et Biologie ?volutive - Equipe "?cologie Statistique"
Universite Lyon 1 - Bat 711 - 69622 Villeurbanne CEDEX - France

Tel : 04 72 43 27 56			   Fax : 04 72 43 13 88
       04 72 43 27 57 	   E-mail : dray at biomserv.univ-lyon1.fr 
---------------------------------------------------------------
Web                            http://www.steph280.freesurf.fr/



From jfox at mcmaster.ca  Tue Jun  3 12:56:13 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 03 Jun 2003 06:56:13 -0400
Subject: [R] coefficient of logistic regression
In-Reply-To: <3EDC622A.5060201@deprem.gov.tr>
Message-ID: <5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>

At 11:54 AM 6/3/2003 +0300, orkun wrote:

>in logistic regression,
>I want to know that it is possible to get probability values of each 
>predictors by
>using following formula for each predictor one by one (keeping constant 
>the others)
><<< exp(coef)/(1+exp(coef)) >>>

Dear Ahmet,

This will almost surely give you nonsense, since it produces a fitted 
probability ignoring the constant in the model (assuming that there is 
one), setting other predictors to 0 and the predictor in question to 1. 
What is it that you want to do?

I hope that this helps,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ripley at stats.ox.ac.uk  Tue Jun  3 12:57:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 11:57:22 +0100 (BST)
Subject: [R] gam questions
In-Reply-To: <5.2.1.1.0.20030603111551.03f51920@pop3.norton.antivirus>
Message-ID: <Pine.LNX.4.44.0306031153310.25867-100000@gannet.stats>

On Tue, 3 Jun 2003, Henric Nilsson wrote:

> At 10:07 2003-06-03 +0100, Prof Brian Ripley wrote:
> 
> > > 1. The prediction example on p. 38 in the mgcv manual.
> >That's not very helpful: pagination of manuals depends on the paper size,
> >for example.
> 
> It's the gam.predict example. Instead of pred<-predict.gam(b,newd) I tried 
> pred<-predict.gam(b).
> 
> >Do you have the latest version of mgcv (and of R, for that matter)?
> 
> I'm running R 1.7.0 under Windows 2000 with mgcv 0.8-8.
(Yes, that is current AFAIK.)

The example works.  If you alter it as you state, there is a warning
(only), and that is a bug in mgcv. object$dim == 0 makes no sense as it is 
> b$dim
[1] 1 1 1 1

Either all() or any() is needed, but Simon will have to tell you which.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From simon at stats.gla.ac.uk  Tue Jun  3 13:44:34 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 3 Jun 2003 12:44:34 +0100 (BST)
Subject: [R] gam questions
In-Reply-To: <5.2.1.1.0.20030603111551.03f51920@pop3.norton.antivirus>
Message-ID: <Pine.SOL.3.96.1030603124257.11065F-100000@moon.stats.gla.ac.uk>

> It's the gam.predict example. Instead of pred<-predict.gam(b,newd) I tried 
> pred<-predict.gam(b).
- Ok, thanks - this is a bug I missed, I'll fix it. The results should be
unaffected, though. Simon



From Ted.Harding at nessie.mcc.ac.uk  Tue Jun  3 12:38:40 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 03 Jun 2003 11:38:40 +0100 (BST)
Subject: [R] Virus Alert
In-Reply-To: <x2n0gzbnzp.fsf@biostat.ku.dk>
Message-ID: <XFMail.030603113840.Ted.Harding@nessie.mcc.ac.uk>

On 03-Jun-03 Peter Dalgaard BSA wrote:
> It's really difficult to tell who the culprit is because all we're
> seeing are autoreplies from someones system to an infected mail that
> purported to be from r-help.
> 
> The original message headers might be able to tell us who is infected
> (which may or may not be someone on the list, depending on where the
> virus is grabbing its From: headers from).

I don't think there's much mileage in trying to trace origins. The only
clue you're going to get from the autoreply is who it was originally
"To:", and if you do receive an "original" directly (as I have done quite
a few times) then also who it was "From:"; both addresses are faked by
the Sobig.C virus, being harvested from email addresses found on the
originating system. So such messages limit the field to people who have
these addresses in their system. It's possible that you may just manage to
guess who it is from this information, but in general this still leaves
far too big a field of possibilities.

Have a look, for instance, at the appropriate entries
  --> Sobig
  --> Sobig.A (Sobig)
  --> Sobig.B (Palyh)
  --> Sobig.C
under
  http://www.datafellows.com/v-descs/s.shtml
to learn about the modus operandi of the various versions of Sobig.

The "Message-Id:" header is unlikely to be much help either: While mailer
software originating a message is supposed to insert such a header at the
time, these viruses generally don't; and if a mail arrives at a mail-hub
without a "Message-Id:" then the mail-hub will insert its own.

The "helo=..." is useless: this is faked at the time of sending during
the SMTP dialogue that the virus initiates itself (bypassing the user's
own mail-transfer system).

The only thing I can suggest is for Windows users on the list to grab
the latest virus updates for their anti-virus software, and check their
own systems.

And in reassurance to Kurt Sys: A Linux system will not be vulnerable to
this virus since it can only get its teeth into a Windows system. The
message you got (and quoted to the list) you received from R-help (as
I did), since xtra.co.nz thought it had received a virus from r-help, and
replied to the list.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 03-Jun-03                                       Time: 11:38:40
------------------------------ XFMail ------------------------------



From temiz at deprem.gov.tr  Tue Jun  3 14:06:23 2003
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 03 Jun 2003 15:06:23 +0300
Subject: [R] coefficient of logistic regression
In-Reply-To: <5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
References: <5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
Message-ID: <3EDC8F3F.90504@deprem.gov.tr>

John Fox wrote:

> At 11:54 AM 6/3/2003 +0300, orkun wrote:
>
>> in logistic regression,
>> I want to know that it is possible to get probability values of each 
>> predictors by
>> using following formula for each predictor one by one (keeping 
>> constant the others)
>> <<< exp(coef)/(1+exp(coef)) >>>
>
>
> Dear Ahmet,
>
> This will almost surely give you nonsense, since it produces a fitted 
> probability ignoring the constant in the model (assuming that there is 
> one), setting other predictors to 0 and the predictor in question to 
> 1. What is it that you want to do?
>
> I hope that this helps,
>  John
>
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>
>
>
thank you

Say, I just want to find each predictor's particular effect on dependent 
variables.
Actual model is to prepare landslide susceptibility map on GIS. So  I 
want to know
what the effect as probability value comes from each predictor. For 
instane what is the effect
of  slope on landslide susceptibility. Should I keep others constant ?

kind regards


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From ida.scheel at nr.no  Tue Jun  3 14:19:14 2003
From: ida.scheel at nr.no (Ida Scheel)
Date: Tue, 03 Jun 2003 14:19:14 +0200
Subject: [R] (no subject)
Message-ID: <3EDC9242.6020404@nr.no>

Hei,

I am trying to fit an ANOVA-model by lm. I get the error-message

Error in lm.fit(x, y, offset = offset, ...) :
    negative length vectors are not allowed

which I don't understand. My data looks fine, but one factor has 
extremely many levels. Does anyone have a tip?

Best regards
Ida Scheel



From christoph.lehmann at gmx.ch  Tue Jun  3 14:23:13 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 03 Jun 2003 14:23:13 +0200
Subject: [R] lda: how to get the eigenvalues
In-Reply-To: <Pine.LNX.4.44.0306031157350.25867-100000@gannet.stats>
References: <Pine.LNX.4.44.0306031157350.25867-100000@gannet.stats>
Message-ID: <1054642993.18425.55.camel@christophl>

let's compare lda and discrimin (ade4) using the iris data:


with lda I get:
> lda1 <- lda(iris[,1:4],iris[,5])
> lda1$svd
[1] 48.642644  4.579983

with discrimin:
> discrimin1 <- discrimin(dudi.pca(iris[,1:4],scan=F),iris[,5],scan=F)
> discrimin1
eigen values: 0.9699 0.222


so where and how is the relationship?

thanks

christoph



On Tue, 2003-06-03 at 13:01, Prof Brian Ripley wrote:
> On Tue, 3 Jun 2003, Stephane Dray wrote:
> 
> > >On Tue, 3 Jun 2003, Christoph Lehmann wrote:
> > >
> > >>  How can I get the eigenvalues out of an lda analysis?
> > >
> > >It uses singular values not eigenvalues: see ?lda for a description of the
> > >output, and the print method for one way to use them.
> > 
> > the function discrimin ofthe ade4 package performs discriminat 
> > analysis with eigen and so produces eigenvalues ($eig)
> 
> For those for whom squaring is too difficult, that is.
> Why recommend software using an inferior algorithm to avoid squaring?
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From p.dalgaard at biostat.ku.dk  Tue Jun  3 14:29:32 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 03 Jun 2003 12:29:32 -0000
Subject: [R] (no subject)
In-Reply-To: <3EDC9242.6020404@nr.no>
References: <3EDC9242.6020404@nr.no>
Message-ID: <x2el2b9w1x.fsf@biostat.ku.dk>

Ida Scheel <ida.scheel at nr.no> writes:

> Hei,
> 
> I am trying to fit an ANOVA-model by lm. I get the error-message
> 
> Error in lm.fit(x, y, offset = offset, ...) :
>     negative length vectors are not allowed
> 
> which I don't understand. My data looks fine, but one factor has
> extremely many levels. Does anyone have a tip?

Not really. It sounds like it might be a bug. How many levels? Can you
generate a simple example (possibly with simulated data) showing the
same behaviour?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ida.scheel at nr.no  Tue Jun  3 15:17:46 2003
From: ida.scheel at nr.no (Ida Scheel)
Date: Tue, 03 Jun 2003 15:17:46 +0200
Subject: [R] (no subject)
References: <3EDC9242.6020404@nr.no> <x2el2b9w1x.fsf@biostat.ku.dk>
Message-ID: <3EDC9FFA.7060101@nr.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030603/d67c9535/attachment.pl

From rwatkins at cornerstonelp.com  Tue Jun  3 15:29:28 2003
From: rwatkins at cornerstonelp.com (rwatkins@cornerstonelp.com)
Date: Tue, 3 Jun 2003 08:29:28 -0500
Subject: [R] tseries "adf.test"
Message-ID: <NDEKIJPPGJCIKBNEDOKOCEFECCAA.rwatkins@cornerstonelp.com>

I have a question regarding the adf.test command in the tseries library.

	I have a vector of time series observations (2265 daily log prices for the
OEX to be exact).  I also have this same data in first-differenced form.  I
want to test both vectors individually for staionarity with an Augmented
Dickey-Fuller test.  I noticed when I use the adf.test command from the
tseries library, the general regression command used incorporates a constant
and a linear trend -- (trend "order" of 1, I presume).  My specific
questions are as follows: (1) is it possible to alter the function to use a
regression that does not incluse a linear trend? , because (2) it seems to
me that I do not need to "detrend" if I've already taken first differences.

	Thanks in advance for your assistance.
Rick



From murdoch at stats.uwo.ca  Tue Jun  3 15:32:11 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Jun 2003 09:32:11 -0400
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer><5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
Message-ID: <np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>

On Tue, 3 Jun 2003 09:49:44 +0100, you wrote in message
<00b701c329ad$14ef6460$ad002850 at FSSFQCV7BGDVED>:

>Duncan
>If the numbers are not represently exactly how does R resolve problems like
>the one below? Is there something that needs to be set up in the R
>environment like the number of significant figures?
>
>> x<-4.145*100+0.5
>> x
>[1] 415
>> floor(x)
>[1] 414

R doesn't do anything to resolve this problem; it's just the way the
IEEE standard floating point formats work.  In Excel 97, 4.145*100+0.5
is exactly equal to 415; I would guess this is either because they use
a binary coded decimal format instead of the IEEE floating point
types, or they round results internally in some way.  R doesn't
support BCD formats, and doesn't do tricky rounding behind your back.
You get what you ask for.

If you want the calculation above to give you exactly 415, the
standard workaround in languages without BCD formats is to work in
some decimal multiple of the actual numbers you're interested in, e.g.
10000.  Then you would store 4.145 as 41450, multiply by 1000000 (i.e.
100*10000) and divide by 10000 to give 4145000, and add 5000, to give
4150000.  All of these numbers are exactly representable in double
precision floating point types, because they are all integers with
fewer than 53 bits in their binary representations.  

Doing this means you need to change the definitions of *, /, ^, and
lots of other low level functions, but + and - work in the usual way.
It might be an interesting project to write a package that does all of
this.

Duncan Murdoch



From roger at ysidro.econ.uiuc.edu  Tue Jun  3 16:00:32 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Tue, 3 Jun 2003 09:00:32 -0500 (CDT)
Subject: [R] (no subject)
In-Reply-To: <3EDC9FFA.7060101@nr.no>
Message-ID: <Pine.SOL.4.30.0306030847050.12571-100000@ysidro.econ.uiuc.edu>

Although not immediately relevant to the present inquiry this might still be
an opportune moment to mention again an item that is at the top of
my R-wish-listr:

The SparseM function slm (for sparse lm) is well suited for problems like this in which
the design matrix is quite sparse, so it would be great to have a version of
model.matrix that would return a matrix in one of the formats of SparseM.  I've
hesitated to dig into this having looked a bit at the C, but if some brave soul
were looking for a nice well-defined summer amusement...

One can handle this "by hand" in specific instances, but it would be great to have
a more automated way to do this via the formula approach.

I think it is not uncommon in large regression problems that this would signficantly
expand the range of applications that could be handled by R, especially on smaller
machines.  Regression/anova problems need only store the non-zero elements of X
and computational effort on such problems should grow roughly proportionally
to the number of these non-zero elements.

Roger

url:	www.econ.uiuc.edu	Roger Koenker		Dept. of Economics UCL,
email	rkoenker at uiuc.edu	Department of Economics Drayton House,
vox: 	217-333-4558		University of Illinois	30 Gordon St,
fax:   	217-244-6678		Champaign, IL 61820	London,WC1H 0AX, UK
							vox:	020-7679-5838

On Tue, 3 Jun 2003, Ida Scheel wrote:

> It is over 3000 levels. I have enough RAM to do it, and I have run
> smaller examples (smaller datasets) with the same code which works.
>
> Peter Dalgaard BSA wrote:
>
> >Ida Scheel <ida.scheel at nr.no> writes:
> >
> >
> >
> >>Hei,
> >>
> >>I am trying to fit an ANOVA-model by lm. I get the error-message
> >>
> >>Error in lm.fit(x, y, offset = offset, ...) :
> >>    negative length vectors are not allowed
> >>
> >>which I don't understand. My data looks fine, but one factor has
> >>extremely many levels. Does anyone have a tip?
> >>
> >>
> >
> >Not really. It sounds like it might be a bug. How many levels? Can you
> >generate a simple example (possibly with simulated data) showing the
> >same behaviour?
> >
> >
> >
>
>
> 	[[alternate HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From pauljohn at ku.edu  Tue Jun  3 16:23:30 2003
From: pauljohn at ku.edu (Paul E. Johnson)
Date: Tue, 03 Jun 2003 09:23:30 -0500
Subject: Rtips (was Re: [R] ? building a database with a the great examples
In-Reply-To: <XFMail.20030603110046.steuer@unibw-hamburg.de>
References: <XFMail.20030603110046.steuer@unibw-hamburg.de>
Message-ID: <3EDCAF62.5040807@ku.edu>

I think there is a lot of merit in this idea.  I think there is a big 
question about "authentication" and protection of Wikis from vandalism.

I've set up Wikis for other projects that I started after Rtips.  I have 
not seen your Wiki software before,  but it looks pretty nice.  I see it 
does have diff support, so old pages can be restored, yes?  But it 
doesn't authenticate users, which causes me some concern.  (I understand 
the Wiki philosophy that we should not be concerned about 
authentication, but I've never bought into it all the way).

I have a TWiki site here:

http://www.ku.edu/cgiwrap/pauljohn/twiki/view

This one I hacked up special to use authentication on some pages so that 
people have to log in before they can edit.

I had not realized before I looked at your page that Wiki 
implementations are customized for document format.  For page sections, 
your Wiki uses

= aHeading =  

but Twiki uses

---+ aHeading

That's kindof a bummer.


Detlef Steuer wrote:

>On 02-Jun-2003 Paul E. Johnson wrote:
>  
>
>>Perhaps you want to start maintaining Rtips itself!  
>>    
>>
>
>Perhaps it is time to start a wiki for R?
>For those not familiar with the idea of wikis look here:
>http://www.wikipedia.org/ (incredible wiki encyclopedia)
>http://www.wikipedia.org/wiki/WikiWiki (for a description of the mechanisms of
>wikiwikis)
>
>I just did a quick hack to set one up:
>http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>
>Any comments welcome!
>
>If the community uses the site I promise to do what I can to keep it 
>running.
>
>Detlef
>  
>


-- 
Paul E. Johnson                       email: pauljohn at ukans.edu
Dept. of Political Science            http://lark.cc.ukans.edu/~pauljohn
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66045                FAX: (785) 864-5700



From D.Beare at marlab.ac.uk  Tue Jun  3 16:31:12 2003
From: D.Beare at marlab.ac.uk (Douglas Beare)
Date: Tue, 3 Jun 2003 15:31:12 +0100 
Subject: [R] S+ style implementation of GAM for R?
Message-ID: <67B92F9B2AFED611852500B0D0FE15097EAF6A@mail3.marlab.ac.uk>

Hi,
I've got the R library "mgcv" for GAM written by Simon Wood which works well
in many instances.  However, over the years I
got attached to the S+ implementation of GAM which allows loess smoothing in
more than 1 dimension as well as spline smoothing.
Has anyone ported the S+ GAM library to R?
Regards,
Doug Beare.
Fisheries Research Services,
Marine Laboratory,
Victoria Road,
Torry,
Aberdeen, UK.
Tel. 44 (0) 1224 295314



From Bernhard.Pfaff at drkw.com  Tue Jun  3 16:34:33 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 3 Jun 2003 16:34:33 +0200
Subject: [R] tseries "adf.test"
Message-ID: <18D602BD42B7E24EB810D6454A58DB90047303DC@ibfftce505.is.de.dresdnerkb.com>

I have a question regarding the adf.test command in the tseries library.

	I have a vector of time series observations (2265 daily log prices
for the
OEX to be exact).  I also have this same data in first-differenced form.  I
want to test both vectors individually for staionarity with an Augmented
Dickey-Fuller test.  I noticed when I use the adf.test command from the
tseries library, the general regression command used incorporates a constant
and a linear trend -- (trend "order" of 1, I presume).  My specific
questions are as follows: (1) is it possible to alter the function to use a
regression that does not incluse a linear trend? , because (2) it seems to
me that I do not need to "detrend" if I've already taken first differences.

	Thanks in advance for your assistance.
Rick


Hello Rick,

you might find the following link useful:

http://www.econ.uiuc.edu/~econ472/tutorial9.html

Pls note, that one typically follows a testing strategy in order to infer
the characteristics of the time series in question (pure random walk, random
with drift or random walk with drift and deterministic trend). The F-type
test statistics (denoted by phi1, phi2, phi3 in the literature) can be
calculated by making use of anova() and checking against the relevant
critical values of these test statistics.

HTH,
Bernhard


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From simon at stats.gla.ac.uk  Tue Jun  3 17:08:21 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 3 Jun 2003 16:08:21 +0100 (BST)
Subject: [R] S+ style implementation of GAM for R?
In-Reply-To: <67B92F9B2AFED611852500B0D0FE15097EAF6A@mail3.marlab.ac.uk>
Message-ID: <Pine.SOL.3.96.1030603160505.7048A-100000@moon.stats.gla.ac.uk>

> I've got the R library "mgcv" for GAM written by Simon Wood which works well
> in many instances.  However, over the years I
> got attached to the S+ implementation of GAM which allows loess smoothing in
> more than 1 dimension as well as spline smoothing.
> Has anyone ported the S+ GAM library to R?

- I've not come across a port (but note that mgcv does allow
multidimensional smooths, albeit spline based and not loess). 
best, Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From Laurie.Sindlinger at noaa.gov  Tue Jun  3 18:13:17 2003
From: Laurie.Sindlinger at noaa.gov (Laurie Sindlinger)
Date: Tue, 03 Jun 2003 12:13:17 -0400
Subject: [R] libraries in .First
Message-ID: <3EDCC91D.2010104@noaa.gov>

Dear all,

    I have a question regarding the .First function. I have included 
help.start() and several libraries in my .First as:

.First <- function() {
help.start(browser = "netscape7")
library(lattice)
library(modreg)
library(splines)
library(MASS)
library(maps)}

The libraries maps and splines do not seem to be available when I start 
R, but I have found if I change the order of the libraries in .First, 
these libraries may be available and others may not. Has anyone had a 
similar problem, or could anyone offer some suggestions? Thank you!

Laurie Sindlinger



From f.grignola at att.net  Tue Jun  3 18:23:20 2003
From: f.grignola at att.net (f.grignola@att.net)
Date: Tue, 03 Jun 2003 16:23:20 +0000
Subject: [R] X11 not available
Message-ID: <200306031623.h53GNKu9019622@hypatia.math.ethz.ch>

Hi,

I just installed R in a Linux box. Unfortunately, I don't have root access and
had to install it in my home directory.
The software runs fine, but I cannot make it print graphics to the screen. If I
type X11 I get: X11 is not available (though it is working for other software).
I noticed that it prints graphs to a default ps file.
Any suggestions about how to get around this?

Many thanks,

FG



From f.mattes at ucl.ac.uk  Tue Jun  3 18:31:01 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Tue, 3 Jun 2003 17:31:01 +0100
Subject: Rtips (was Re: [R] ? building a database with a the great
	/cookbook
In-Reply-To: <200306031017.h53A9auC025218@hypatia.math.ethz.ch>
References: <200306031017.h53A9auC025218@hypatia.math.ethz.ch>
Message-ID: <p05200f00bb027a438a66@[128.40.218.142]>

For  me as a beginner a cookbook would be welcome. so many great code 
examples are posted into the help list, but finding these is quite 
difficult. I think it comes all down to the problem who is compiling 
/ contributing and can judge what should go in.
In this respect, Detlef Steuer (suggestion , might be a solution 
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome.



Frank

>
>Message: 67
>Date: Tue, 3 Jun 2003 00:05:02 -0700
>From: Claude Rubinson <cmsclaud at uga.edu>
>Subject: Re: Rtips (was Re: [R] ? building a database with a the great
>
>I'm wondering if there's demand/interest in a cookbook-style site
>along the lines of ActiveState/O'Reilly's Perl and Python cookbook
>sites.  (If you're not familiar with the cookbooks, they're databases
>of user-supplied "recipes" illustrating various programming tips and
>techniques.)  The advantage of the cookbooks are that, since users
>supply the content, they take much of the demand off of the
>maintainer(s).
>
>I figure that a cookbook would probably be welcome (after all, who
>doesn't like more information?).  So, I guess the real question is if
>there are enough people who would be willing to contribute recipes on
>an ongoing basis?  The R community is certainly smaller than that of
>Perl or Python.
>
>Thoughts?
>
>Claude

-- 
Frank Mattes, MD			e-mail:	f.mattes at ucl.ac.uk
Department of Virology			fax	0044(0)207 8302854
Royal Free Hospital and			tel	0044(0)207 8302997
University College Medical School
London



From Benjamin.STABLER at odot.state.or.us  Tue Jun  3 18:35:50 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 3 Jun 2003 09:35:50 -0700 
Subject: [R] Building an R package under Windows NT
Message-ID: <76A000A82289D411952F001083F9DD06047FE0E9@exsalem4-bu.odot.state.or.us>

The version of Perl I have is:

  This is perl, version 5.005_02 built for MSWin32-x86-object

  Copyright 1987-1998, Larry Wall

  Binary build 506 provided by ActiveState Tool Corp.
http://www.ActiveState.com
  Built 15:40:37 Oct 27 1998

It was installed by my IS department some time ago.  I will try installing
Perl 5.8 and see if that works.

Since "Rcmd build -h" does not print a description of the acceptable values
for "--docs=", I thought I would look for something similar.  The
"--docs=TYPE" option for build and the "--type=TYPE" for Rdconv looked
related so I thought I would give it a try.  Without knowing what the
correct values for "--docs=TYPE" are, how am I suppose to know that
"--docs=TYPE" is not the same as "--type=TYPE" for specifing what types of
help documents to create?  I don't think it is a stretch to see how these
two could be confused.


>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Tuesday, June 03, 2003 12:48 AM
>To: STABLER Benjamin
>Subject: RE: [R] Building an R package under Windows NT
>
>
>On Mon, 2 Jun 2003 Benjamin.STABLER at odot.state.or.us wrote:
>
>> Professor Ripley,
>> 
>> I just downloaded and installed the most current tools from your site
>> (http://www.stats.ox.ac.uk/pub/Rtools/), I have version 
>5.005_02 of Perl,
>> and I still get the same error.  Do you think Perl 5.8 would 
>*fix* this
>> problem?
>
>I have no idea, but I do know that correcting all of *your* 
>errors would 
>solve the problem.  As that version of Perl was AFAIK never 
>released for 
>Windows (and has not been available for several years if it 
>was), I think 
>you need to try to follow the instructions *exactly* (which 
>you have not 
>done re Perl, which says
>
>  The Windows port of perl5, available via
>  http://www.activestate.com/Products/ActivePerl/.
>  BEWARE: you do need the *Windows* port and not the Cygwin one.
>
>).
> 
>> I didn't think the docs=html option would work as I got the 
>html option from
>> the Rdconv type=TYPE help since the build help does not list 
>the options.  I
>> didn't think to look at the install help.
>
>Why do you think --type (sic) takes the same values as --docs ?
>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Tue Jun  3 18:35:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 17:35:58 +0100 (BST)
Subject: [R] libraries in .First
In-Reply-To: <3EDCC91D.2010104@noaa.gov>
Message-ID: <Pine.LNX.4.44.0306031730540.8385-100000@gannet.stats>

If this is R 1.7.0, the preferred mechanism is options("defaultPackages"), 
as in the following (from the examples in .First)

     # Example of Rprofile.site
     local({
       old <- getOption("defaultPackages")
       options(defaultPackages = c(old, "MASS"))
     })

.First is run too early to be loading packages.

Oh, and the preferred way to set the browser is environmental variable
R_BROWSER, usually in R_HOME/etc/Renviron{.site}.

On Tue, 3 Jun 2003, Laurie Sindlinger wrote:

> Dear all,
> 
>     I have a question regarding the .First function. I have included 
> help.start() and several libraries in my .First as:
> 
> .First <- function() {
> help.start(browser = "netscape7")
> library(lattice)
> library(modreg)
> library(splines)
> library(MASS)
> library(maps)}
> 
> The libraries maps and splines do not seem to be available when I start 
> R, but I have found if I change the order of the libraries in .First, 
> these libraries may be available and others may not. Has anyone had a 
> similar problem, or could anyone offer some suggestions? Thank you!
> 
> Laurie Sindlinger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Benjamin.STABLER at odot.state.or.us  Tue Jun  3 18:37:00 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 3 Jun 2003 09:37:00 -0700 
Subject: [R] Building an R package under Windows NT
Message-ID: <76A000A82289D411952F001083F9DD06047FE0EA@exsalem4-bu.odot.state.or.us>

The emme2 package is not in the $RHOME/library directory.  My sh.exe is the
most current one from the tools available at
http://www.stats.ox.ac.uk/pub/Rtools/.    I searched for sh.exe and that is
only one I've got on my system.


>-----Original Message-----
>From: Duncan Murdoch [mailto:dmurdoch at pair.com]
>Sent: Monday, June 02, 2003 5:09 PM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Building an R package under Windows NT
>
>
>On Mon, 2 Jun 2003 14:08:55 -0700 , you wrote:
>
>>Thanks for the suggestions.  
>>
>>1) I fixed the zip.exe PATH issue.
>>2) I removed unnecessary quotes around C:\Program Files\R
>>3) I ran Rcmd install emme2 with the following result:
>
>Where is emme2?  You shouldn't keep the source in the
>$RHOME/library/emme2 directory, which is where it will be installed.
>
>>---------- Making package emme2 ------------
>>  adding build stamp to DESCRIPTION
>>  installing R files
>> 175373 [main] sh 352 proc_subproc: Couldn't duplicate my 
>handle<0xA4> for
>>pid 1736008448, Win32 error 6
>
>This is an error in sh.exe.  Are you getting the one from the R
>toolset, or possibly another one?
>
>Duncan Murdoch
>



From lsilva at fc.up.pt  Tue Jun  3 18:59:50 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Tue, 3 Jun 2003 17:59:50 +0100
Subject: [R] kmeans
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEA54@MAIL.fc.up.pt>

Dear helpers
 
I was working with kmeans from package mva and found some strange situations. When I run several times the kmeans algorithm with the same dataset I get the same partition. I simulated a little example with 6 observations and run kmeans giving the centers and making just one iteration. I expected that the algorithm just allocated the observations to the nearest center but think this is not the result that I get...
 
Here are the simulated data
 
> dados<-matrix(c(-1,0,2,2.5,7,9,0,3,0,6,1,4),6,2)
> dados
     [,1] [,2]
[1,] -1.0    0
[2,]  0.0    3
[3,]  2.0    0
[4,]  2.5    6
[5,]  7.0    1
[6,]  9.0    4
> plot(dados)
> dados<-matrix(c(-1,0,2,2.5,7,9,0,5,0,6,1,4),6,2)
> plot(dados)
> A<-kmeans(dados,dados[c(3,4),],1)
> A
$cluster
[1] 1 1 1 1 2 2
$centers
   [,1] [,2]
1 0.875 2.75
2 8.000 2.50
$withinss
[1] 38.9375  6.5000
$size
[1] 4 2
 
 
Any hints?
 
Thanks a lot 
 
Luis Silva



From enzmann at kfn.uni-hannover.de  Tue Jun  3 19:10:05 2003
From: enzmann at kfn.uni-hannover.de (Dirk Enzmann)
Date: Tue, 03 Jun 2003 19:10:05 +0200
Subject: [R] Update VR_7.1-6
Message-ID: <3EDCD66D.D349EA70@kfn.uni-hannover.de>

The update of VR by downloading VR_7.1-6.zip and using install.packages
(from local zip files) fails with the following error message:

Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file `VR/DESCRIPTION'

Other packages can be installed without problems, except of dse_2003.4-1
with a similar error message.

Why?

Operating System: Windows NT (4.0)
R Version: R 1.7.0

*************************************************
Dr. Dirk Enzmann
Criminological Research Institute of Lower Saxony
Luetzerodestr. 9
D-30161 Hannover
Germany

phone: +49-511-348.36.32
fax:   +49-511-348.36.10
email: ENZMANN at KFN.uni-hannover.de

http://www.kfn.de



From tlumley at u.washington.edu  Tue Jun  3 19:21:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jun 2003 10:21:50 -0700 (PDT)
Subject: [R] Update VR_7.1-6
In-Reply-To: <3EDCD66D.D349EA70@kfn.uni-hannover.de>
Message-ID: <Pine.A41.4.44.0306031020270.23688-100000@homer13.u.washington.edu>

On Tue, 3 Jun 2003, Dirk Enzmann wrote:

> The update of VR by downloading VR_7.1-6.zip and using install.packages
> (from local zip files) fails with the following error message:
>
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `VR/DESCRIPTION'
>
> Other packages can be installed without problems, except of dse_2003.4-1
> with a similar error message.
>

Because there is a bug in the handling of package bundles in R 1.7.0.  VR
and dse are bundles. R1.7.1 will be coming out soon and fixes this. Or you
can manually unzip the bundle in the appropriate place in the library
directory.

	-thomas



From gavin.simpson at ucl.ac.uk  Tue Jun  3 19:30:19 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 03 Jun 2003 18:30:19 +0100
Subject: [R] Update VR_7.1-6
In-Reply-To: <3EDCD66D.D349EA70@kfn.uni-hannover.de>
References: <3EDCD66D.D349EA70@kfn.uni-hannover.de>
Message-ID: <3EDCDB2B.3090600@ucl.ac.uk>

Hi Dirk,

dse and VR are bundles or packages.  There is a bug in rw1070 in the way 
it installs bundles.

Download the zip file directly and unzip it to the library directory in 
wherever R is installed on your system.

This has been discussed recently (this morning) on the list.  Search the 
archives to read more.

G

Dirk Enzmann wrote:
> The update of VR by downloading VR_7.1-6.zip and using install.packages
> (from local zip files) fails with the following error message:
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `VR/DESCRIPTION'
> 
> Other packages can be installed without problems, except of dse_2003.4-1
> with a similar error message.
> 
> Why?
> 
> Operating System: Windows NT (4.0)
> R Version: R 1.7.0
> 
> *************************************************
> Dr. Dirk Enzmann
> Criminological Research Institute of Lower Saxony
> Luetzerodestr. 9
> D-30161 Hannover
> Germany
> 
> phone: +49-511-348.36.32
> fax:   +49-511-348.36.10
> email: ENZMANN at KFN.uni-hannover.de
> 
> http://www.kfn.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ripley at stats.ox.ac.uk  Tue Jun  3 19:30:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 18:30:31 +0100 (BST)
Subject: [R] Update VR_7.1-6
In-Reply-To: <3EDCD66D.D349EA70@kfn.uni-hannover.de>
Message-ID: <Pine.LNX.4.44.0306031824180.8601-100000@gannet.stats>

I have explained that on R-help earlier today (and when VR_7.1-4 was 
announced).  Please look back a few hours in the R-help postings.

On Tue, 3 Jun 2003, Dirk Enzmann wrote:

> The update of VR by downloading VR_7.1-6.zip and using install.packages
> (from local zip files) fails with the following error message:
> 
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `VR/DESCRIPTION'
> 
> Other packages can be installed without problems, except of dse_2003.4-1
> with a similar error message.
> 
> Why?

Because VR and dse are bundles and someone broke the installation of
bundles in R 1.7.0 on Windows.

You can download and unzip the zip files in rw1070/library.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun  3 19:38:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jun 2003 18:38:17 +0100 (BST)
Subject: [R] kmeans
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEA54@MAIL.fc.up.pt>
Message-ID: <Pine.LNX.4.44.0306031830570.8601-100000@gannet.stats>

On Tue, 3 Jun 2003, Luis Miguel Almeida da Silva wrote:

>  I was working with kmeans from package mva and found some strange
> situations. When I run several times the kmeans algorithm with the same
> dataset I get the same partition. 

Why does that surprise you?

> I simulated a little example with 6
> observations and run kmeans giving the centers and making just one
> iteration. I expected that the algorithm just allocated the observations
> to the nearest center but think this is not the result that I get...

That's not what the documentation says it does:

     The data given by `x' is clustered by the k-means algorithm. When
     this terminates, all cluster centres are at the mean of their
     Voronoi sets (the set of data points which are nearest to the
     cluster centre).

which is true in your example.  It has run one iteration of re-allocation; 
as you can see by reading the source code or the reference.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tblackw at umich.edu  Tue Jun  3 22:02:32 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 3 Jun 2003 16:02:32 -0400 (EDT)
Subject: [R] coefficient of logistic regression
In-Reply-To: <3EDC622A.5060201@deprem.gov.tr>
Message-ID: <Pine.SOL.4.44.0306031535190.15996-100000@rygar.gpcc.itd.umich.edu>

Ahmet  -

In a logistic regression model, fitted probabilities make
sense for individual cases (rows in the data set), as well
as for future cases (predictions) for which no outcome
(success or failure) has been observed yet.  Fitted
probabilities are calculated from the matrix formula:

  Pr[success]  =  exp( X %*% beta) / (1 + exp( X %*% beta)

where  X  is an [n x (p+1)] matrix, containing all p predictor
variables as columns, preceded by a column of 1s for the
intercept, and  beta  is the [(p+1) x 1] vector of logistic
regression coefficients.

One can interpret the sign and the magnitude of an individual
regression coeffient by saying that an increase of 1 unit in
predictor variable [i] will increase or decrease the odds of
success by a multiplier of  exp(beta[i]).  When  beta[i] > 0
the odds increase, because  exp(beta[i]) > 1,  and when
beta[i] < 0  the odds decrease, because  exp(beta[i]) < 1.

I hope this explanation helps.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 3 Jun 2003, orkun wrote:

> Hello
>
> in logistic regression,
> I want to know that it is possible to get probability values of each
> predictors by
> using following formula for each predictor one by one (keeping constant
> the others)
>  <<< exp(coef)/(1+exp(coef)) >>>
>
> thanks in advance
> Ahmet Temiz



From ligges at statistik.uni-dortmund.de  Tue Jun  3 22:04:54 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 03 Jun 2003 22:04:54 +0200
Subject: [R] rw1070 and package bundles; was: Update VR_7.1-6
In-Reply-To: <Pine.LNX.4.44.0306031824180.8601-100000@gannet.stats>
References: <Pine.LNX.4.44.0306031824180.8601-100000@gannet.stats>
Message-ID: <3EDCFF66.5040803@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:
> I have explained that on R-help earlier today (and when VR_7.1-4 was 
> announced).  Please look back a few hours in the R-help postings.
> 
> On Tue, 3 Jun 2003, Dirk Enzmann wrote:
> 
> 
>>The update of VR by downloading VR_7.1-6.zip and using install.packages
>>(from local zip files) fails with the following error message:
>>
>>Error in file(file, "r") : unable to open connection
>>In addition: Warning message:
>>cannot open file `VR/DESCRIPTION'
>>
>>Other packages can be installed without problems, except of dse_2003.4-1
>>with a similar error message.
>>
>>Why?
> 
> 
> Because VR and dse are bundles and someone broke the installation of
> bundles in R 1.7.0 on Windows.
> 
> You can download and unzip the zip files in rw1070/library.


The CRAN maintainers also got first messages from users who didn't know 
about this bug. In order to avoid a huge amount of further questions and 
bug reports related to these problems, I decided to move the two package 
bundles from CRAN/bin/windows/contrib/1.7 to a subfolder until R-1.7.1 
will be released.
Please read CRAN/bin/windows/contrib/1.7/ReadMe for details.

The change presumably will show up on CRAN master within 24 hours.

Uwe Ligges



From Benjamin.STABLER at odot.state.or.us  Tue Jun  3 23:24:31 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 3 Jun 2003 14:24:31 -0700 
Subject: [R] Building an R package under Windows NT
Message-ID: <76A000A82289D411952F001083F9DD06047FE0F1@exsalem4-bu.odot.state.or.us>

I finally figured out the problem.  I went ahead and installed Perl 5.8 but
that didn't do it.  The problem was that an existing program's directory was
earlier in the PATH and so one (or more) of the components of the "Rcmd
INSTALL" was an older version.  It wasn't zip since I renamed the old zip.
Anyway, thanks for your help.

Regards,
Ben Stabler

>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Tuesday, June 03, 2003 12:48 AM
>To: STABLER Benjamin
>Subject: RE: [R] Building an R package under Windows NT
>
>
>On Mon, 2 Jun 2003 Benjamin.STABLER at odot.state.or.us wrote:
>
>> Professor Ripley,
>> 
>> I just downloaded and installed the most current tools from your site
>> (http://www.stats.ox.ac.uk/pub/Rtools/), I have version 
>5.005_02 of Perl,
>> and I still get the same error.  Do you think Perl 5.8 would 
>*fix* this
>> problem?
>
>I have no idea, but I do know that correcting all of *your* 
>errors would 
>solve the problem.  As that version of Perl was AFAIK never 
>released for 
>Windows (and has not been available for several years if it 
>was), I think 
>you need to try to follow the instructions *exactly* (which 
>you have not 
>done re Perl, which says
>
>  The Windows port of perl5, available via
>  http://www.activestate.com/Products/ActivePerl/.
>  BEWARE: you do need the *Windows* port and not the Cygwin one.
>
>).
> 
>> I didn't think the docs=html option would work as I got the 
>html option from
>> the Rdconv type=TYPE help since the build help does not list 
>the options.  I
>> didn't think to look at the install help.
>
>Why do you think --type (sic) takes the same values as --docs ?
>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From gilda.garibotti at hci.utah.edu  Tue Jun  3 23:27:15 2003
From: gilda.garibotti at hci.utah.edu (Gilda Garibotti)
Date: Tue, 3 Jun 2003 15:27:15 -0600
Subject: [R] (no subject)
Message-ID: <F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>

Hi,
I would like to know if it is possible to get printed output while a loop is taking place.
Example:
for(i in 1:10){
 print(i)
 some long process
}

This will print the values of i only after the loop is finished, what I would like is to 
see them when the process enters the i-th iteration to keep track of how the 
program is running.

Thank you,
Gilda



From p.dalgaard at biostat.ku.dk  Wed Jun  4 00:08:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 03 Jun 2003 22:08:38 -0000
Subject: [R] (no subject)
In-Reply-To: <F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>
References: <F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>
Message-ID: <x265nmkds1.fsf@biostat.ku.dk>

"Gilda Garibotti" <gilda.garibotti at hci.utah.edu> writes:

> Hi,
> I would like to know if it is possible to get printed output while a loop is taking place.
> Example:
> for(i in 1:10){
>  print(i)
>  some long process
> }
> 
> This will print the values of i only after the loop is finished, what I would like is to 
> see them when the process enters the i-th iteration to keep track of how the 
> program is running.

Windows, right? (This is system dependent) There's a menu item
entitled "Buffer output" or something to that effect. Turn it off and
print() calls display immediately. Lengthy output becomes slower,
though. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Wed Jun  4 00:57:36 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 03 Jun 2003 18:57:36 -0400
Subject: [R] coefficient of logistic regression
In-Reply-To: <3EDC8F3F.90504@deprem.gov.tr>
References: <5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
Message-ID: <5.1.0.14.2.20030603184955.01ea1440@mcmail.cis.mcmaster.ca>

Dear Ahmet,

Sorry for the slow response, but I've been busy all today, coincidentally 
teaching a workshop on logistic regression.

Tom Blackwell sent you a useful suggestion for interpreting coefficients on 
the odds scale. If you want to trace out the partial relationship of the 
fitted probability of response to a particular predictor holding others 
constant, you can set the other predictors to typical values and let the 
predictor in question vary over its range, transforming the fitted log-odds 
to the probability scale.

You may be interested in my effects package (on CRAN or at 
<http://socserv.socsci.mcmaster.ca/jfox/Misc/effects/index.html>), which 
makes these kinds of displays for linear and generalized-linear models, 
including those with interactions.

Regards,
  John

At 03:06 PM 6/3/2003 +0300, orkun wrote:
>John Fox wrote:
>
>>At 11:54 AM 6/3/2003 +0300, orkun wrote:
>>
>>>in logistic regression,
>>>I want to know that it is possible to get probability values of each 
>>>predictors by
>>>using following formula for each predictor one by one (keeping constant 
>>>the others)
>>><<< exp(coef)/(1+exp(coef)) >>>
>>
>>
>>Dear Ahmet,
>>
>>This will almost surely give you nonsense, since it produces a fitted 
>>probability ignoring the constant in the model (assuming that there is 
>>one), setting other predictors to 0 and the predictor in question to 1. 
>>What is it that you want to do?
>>
>>I hope that this helps,
>>  John
>>
>>
>thank you
>
>Say, I just want to find each predictor's particular effect on dependent 
>variables.
>Actual model is to prepare landslide susceptibility map on GIS. So  I want 
>to know
>what the effect as probability value comes from each predictor. For 
>instane what is the effect
>of  slope on landslide susceptibility. Should I keep others constant ?
>
>kind regards




-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ross at biostat.ucsf.edu  Wed Jun  4 01:14:09 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 03 Jun 2003 23:14:09 -0000
Subject: [R] Question about looking up names
Message-ID: <1054681987.15581.25.camel@epibiosun115-4>

I think I now understand how R looks up names.  Could anyone tell me if
I have this right?

First it looks up the nested environments created by lexical scoping.
Then, if it gets to the top (.GlobalEnv) it also looks through the list
of things that have been "attach"ed.

It never looks in the call stack unless you explicitly ask it to, or
mess with the environment frames.

The reason I ask is that it's not entirely clear to me from the "R
Language Definition" how these 3 search spaces (environments/lexical
scoping; call stack/dynamic scoping; attach/search list) are related. 
For example the discussion of 3.5.3 ("the call stack") observes that
dynamic scoping "contradicts the default scoping rules in R".  I spent
some time trying to figure out how it could do both, before deciding it
doesn't.  I suppose the implicit corollary of the contradiction referred
to in 3.5.3--"so we don't do that and you must intervene to achieve
dynamic scoping"--was obvious to the authors.  It just wasn't obvious to
me.  Since I'm still not sure, I thought I'd check.

Thanks.
-- 
Ross Boylan                                      wk: (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           hm: (415) 550-1062
University of California, San Francisco
San Francisco, CA 94143-0840



From tlumley at u.washington.edu  Wed Jun  4 01:30:44 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Jun 2003 16:30:44 -0700 (PDT)
Subject: [R] Question about looking up names
In-Reply-To: <1054681987.15581.25.camel@epibiosun115-4>
Message-ID: <Pine.A41.4.44.0306031626490.23688-100000@homer13.u.washington.edu>

On 3 Jun 2003, Ross Boylan wrote:

> I think I now understand how R looks up names.  Could anyone tell me if
> I have this right?
>
> First it looks up the nested environments created by lexical scoping.
> Then, if it gets to the top (.GlobalEnv) it also looks through the list
> of things that have been "attach"ed.
>
> It never looks in the call stack unless you explicitly ask it to, or
> mess with the environment frames.
>
> The reason I ask is that it's not entirely clear to me from the "R
> Language Definition" how these 3 search spaces (environments/lexical
> scoping; call stack/dynamic scoping; attach/search list) are related.
> For example the discussion of 3.5.3 ("the call stack") observes that
> dynamic scoping "contradicts the default scoping rules in R".  I spent
> some time trying to figure out how it could do both, before deciding it
> doesn't.  I suppose the implicit corollary of the contradiction referred
> to in 3.5.3--"so we don't do that and you must intervene to achieve
> dynamic scoping"--was obvious to the authors.  It just wasn't obvious to
> me.  Since I'm still not sure, I thought I'd check.
>

Yes.


	-thomas



From paul_bivand at blueyonder.co.uk  Wed Jun  4 01:42:29 2003
From: paul_bivand at blueyonder.co.uk (Paul)
Date: Wed, 04 Jun 2003 00:42:29 +0100
Subject: [R] Logistic regression problem: propensity score matching
Message-ID: <3EDD3265.4080405@blueyonder.co.uk>

Hello all.

I am doing one part of an evaluation of a mandatory welfare-to-work 
programme in the UK.
As with all evaluations, the problem is to determine what would have 
happened if the initiative had not taken place.
In our case, we have a number of pilot areas and no possibility of 
random assignment.
Therefore we have been given control areas.
My problem is to select for survey individuals in the control areas who 
match as closely as possible the randomly selected sample of action area 
participants.
As I understand the methodology, the procedure is to run a logistic 
regression to determine the odds of a case being in the sample, across 
both action and control areas, and then choose for control sample the 
control area individual whose odds of being in the sample are closest to 
an actual sample member.

So far, I have following the multinomial logistic regression example in 
Fox's Companion to Applied Regression.
Firstly, I would like to know if the predict() is producing odds ratios 
(or probabilities) for being in the sample, which is what I am aiming 
for. Secondly, how do I get rownames (my unique identifier) into the 
output from predict() - my input may be faulty somehow and the wrong 
rownames being picked up - as I need to export back to database to sort 
and match in names, addresses and phone numbers for my selected samples.

My code is as follows:
londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
rownames=ORCID)
attach(londonpsm)
mod.multinom <- multinom(sample ~ AGE + DISABLED + GENDER + ETHCODE + 
NDYPTOT + NDLTUTOT + LOPTYPE)
lonoutput <- predict(mod.multinom, sample, type='probs')
london2 <- data.frame(lonoutput)

The Logistic regression seems to work, although summary() says the it is 
not a matrix.
The output looks like odds ratios, but I would like to know whether this 
is so.

Thank you
Paul Bivand



From ross at biostat.ucsf.edu  Wed Jun  4 01:43:55 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 03 Jun 2003 23:43:55 -0000
Subject: [R] Question about looking up names
In-Reply-To: <20030603193434.F93@jimmy.harvard.edu>
References: <1054681987.15581.25.camel@epibiosun115-4>
	<20030603193434.F93@jimmy.harvard.edu>
Message-ID: <1054683788.15581.38.camel@epibiosun115-4>

On Tue, 2003-06-03 at 16:34, Robert Gentleman wrote:

> Also, note that you can get the effect of lexical scope by doing
> things like
> 
Do you mean "you can get the effects of dynamic scope...."?

>  f<- function(x) x+y
>  e1 <- new.env()
>  assign("y", 10, env=e1)
>  environment(f) <- e1
> 
> #now like lexical scope; you can futz with f's environment, assigning,
> # modifying as you like
> 


P.S. Thanks to everyone who responded.  So fast!



From Symantec_AntiVirus_mskavs1 at mskcc.org  Wed Jun  4 03:30:40 2003
From: Symantec_AntiVirus_mskavs1 at mskcc.org (Symantec_AntiVirus_mskavs1@mskcc.org)
Date: Tue, 03 Jun 2003 21:30:40 -0400
Subject: [R] Your Message Contained a Potential Virus
Message-ID: <200306040138.h541c3uA014569@hypatia.math.ethz.ch>

Text: This is an automated message. Please read it carefully.

You should know that your recent email message detailed below, to Memorial Sloan-Kettering Cancer Center, was identified as potentially containing a virus.

From: r-announce at lists.r-project.org
To: mitran at mskcc.org


If we have been able to repair your message, it has been delivered.  If we have not, it has been blocked and the recipient(s) of your email have been informed that the email has been blocked.

Please note that as a matter of policy certain attachment types are blocked by default based on the file extension.  If you have a legitimate file to send which is being blocked, please consider renaming the extension or enclosing it within a ZIP file.

Thank You


Viruses found:

--- Scan information follows ---

Result: Virus Detected
Virus Name: W32.Sobig.C at mm
File Attachment: screensaver.scr
Attachment Status: deleted

--- Original message information follows ---

From: <r-announce at lists.r-project.org>
To: <mitran at mskcc.org>
Date: Tue, 3 Jun 2003 21:38:00 --0400
Subject: Re: Application
Received: (from HZHANG2-LAP [24.74.137.211])
 by mskavs1.mskcc.org (SAVSMTP 3.1.1.32) with SMTP id M2003060321303704373
 for <mitran at mskcc.org>; Tue, 03 Jun 2003 21:30:38 -0400



From MSchwartz at medanalytics.com  Wed Jun  4 07:24:28 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 04 Jun 2003 05:24:28 -0000
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
	<np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
Message-ID: <1054704248.4122.291.camel@localhost>

On Tue, 2003-06-03 at 08:32, Duncan Murdoch wrote: 
> On Tue, 3 Jun 2003 09:49:44 +0100, you wrote in message
> <00b701c329ad$14ef6460$ad002850 at FSSFQCV7BGDVED>:
> 
> >Duncan
> >If the numbers are not represently exactly how does R resolve problems like
> >the one below? Is there something that needs to be set up in the R
> >environment like the number of significant figures?
> >
> >> x<-4.145*100+0.5
> >> x
> >[1] 415
> >> floor(x)
> >[1] 414
> 
> R doesn't do anything to resolve this problem; it's just the way the
> IEEE standard floating point formats work.  In Excel 97, 4.145*100+0.5
> is exactly equal to 415; I would guess this is either because they use
> a binary coded decimal format instead of the IEEE floating point
> types, or they round results internally in some way.  R doesn't
> support BCD formats, and doesn't do tricky rounding behind your back.
> You get what you ask for.
> 
> If you want the calculation above to give you exactly 415, the
> standard workaround in languages without BCD formats is to work in
> some decimal multiple of the actual numbers you're interested in, e.g.
> 10000.  Then you would store 4.145 as 41450, multiply by 1000000 (i.e.
> 100*10000) and divide by 10000 to give 4145000, and add 5000, to give
> 4150000.  All of these numbers are exactly representable in double
> precision floating point types, because they are all integers with
> fewer than 53 bits in their binary representations.  
> 
> Doing this means you need to change the definitions of *, /, ^, and
> lots of other low level functions, but + and - work in the usual way.
> It might be an interesting project to write a package that does all of
> this.
> 
> Duncan Murdoch


In Excel, the IEEE standard (754) is used to internally represent
floats. A MS-KB article on this is here:

http://support.microsoft.com/default.aspx?scid=kb;[LN];214118

Another, more detailed, is here:

http://support.microsoft.com/default.aspx?scid=kb;EN-US;78113


What is curious about this situation, and apropos to Prof. Ripley's
comments about the difference between internal representation, rounding
and displayed values, is the following information. Note how the results
of cell calculations differ between Excel, OpenOffice.org Calc and
Gnumeric. In each case, I use a format setting of 20 digits after the
decimal place with scientific notation. This is best read with a fixed
width font.


OOo Calc 1.0.2 and 1.1 Beta2:

Cell Formula          Value
= 4.145 * 100 + 0.5   4.15000000000000000000E+02
= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
=(0.5 - 0.4 - 0.1)    0.00000000000000000000E+00


Excel 2002 (XP):

Cell Formula          Value
= 4.145 * 100 + 0.5   4.15000000000000000000E+02
= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
=(0.5 - 0.4 - 0.1)    -2.77555756156289000000E-17


Gnumeric 1.0.12:

Cell Formula          Value
= 4.145 * 100 + 0.5   +4.14999999999999943157E+02
= 0.5 - 0.4 - 0.1     -2.77555756156289135106E-17
*Gnumeric does not appear to allow the surrounding parens.


For comparison, R 1.7.1 Beta under RH 9 and WinXP:

> print(4.145 * 100 + 0.5, digits = 20)
[1] 414.99999999999994
> formatC(4.145 * 100 + 0.5, format = "E", digits = 20)
[1] "4.14999999999999943157E+02"

> print(0.5 - 0.4 - 0.1, digits = 20)
[1] -2.775557561562891e-17
> formatC(0.5 - 0.4 - 0.1, format = "E", digits = 20)
[1] "-2.77555756156289135106E-17"


What is interesting is the change in the displayed value in Excel when
the second formula is surrounded by parens (which I found purely by
accident). This would suggest that there may be something going on in
the parsing of the cell formula that affects the calculation and
displayed value. Also note the precision of the resultant number.

Presuming that each of the spreadsheet programs are using IEEE standard
internal representation, there are clearly differences in the way in
which each visually displays the values, both by default and when
explicitly formatted.


Using the following cell formula:

= 1.333 + 1.225 - 1.333 - 1.225

there is an indication in the second MS-KB article above, that Excel 97
introduced an "optimization" dealing with results near zero. "The
example above when performed in Excel 97 and later correctly displays 0
or 0.000000000000000E+00 in scientific notation." 

whereas 

"Rather than displaying 0, Excel 95 displays -2.22044604925031E-16."

The terms "optimization" and "correctly displays" are an interesting
choice of words.


I have a post to one of the OOo forums regarding my inability to
replicate the IEEE precision issues in Calc under any circumstances
using the three formulas and any numeric formatting options. It may be
that the OOo folks copied the MS Excel "optimization" with no override.


FYI...the IEEE has a reference site for the standard here:

http://grouper.ieee.org/groups/754/


HTH,

Marc Schwartz



From otoomet at econ.dk  Wed Jun  4 07:53:46 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Wed, 4 Jun 2003 07:53:46 +0200
Subject: [R] X11 not available
In-Reply-To: <200306031623.h53GNKu9019622@hypatia.math.ethz.ch>
	(f.grignola@att.net)
References: <200306031623.h53GNKu9019622@hypatia.math.ethz.ch>
Message-ID: <200306040553.h545rkJ09463@punik.econ.au.dk>

Hello,

 | From: f.grignola at att.net
 | Date: Tue, 03 Jun 2003 16:23:20 +0000
 | 
 | Hi,
 | 
 | I just installed R in a Linux box. Unfortunately, I don't have root access and
 | had to install it in my home directory.
 | The software runs fine, but I cannot make it print graphics to the screen. If I
 | type X11 I get: X11 is not available (though it is working for other software).
 | I noticed that it prints graphs to a default ps file.
 | Any suggestions about how to get around this?
 | 
 | Many thanks,
 | 
 | FG

perhaps you have compiled it without x11 support.  The reason could be
missing XFree86-devel.rpm or something similar.  Have you checked whad did
the configuration script say?

Just a suggestion

Ott



From p.dalgaard at biostat.ku.dk  Wed Jun  4 08:04:39 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 04 Jun 2003 06:04:39 -0000
Subject: [R] Your Message Contained a Potential Virus
In-Reply-To: <200306040138.h541c3uA014569@hypatia.math.ethz.ch>
References: <200306040138.h541c3uA014569@hypatia.math.ethz.ch>
Message-ID: <x2y90i5q26.fsf@biostat.ku.dk>

Symantec_AntiVirus_mskavs1 at mskcc.org writes:

> Result: Virus Detected
> Virus Name: W32.Sobig.C at mm
> File Attachment: screensaver.scr
> Attachment Status: deleted
> 
> --- Original message information follows ---
> 
> From: <r-announce at lists.r-project.org>
> To: <mitran at mskcc.org>
> Date: Tue, 3 Jun 2003 21:38:00 --0400
> Subject: Re: Application
> Received: (from HZHANG2-LAP [24.74.137.211])
>  by mskavs1.mskcc.org (SAVSMTP 3.1.1.32) with SMTP id M2003060321303704373
>  for <mitran at mskcc.org>; Tue, 03 Jun 2003 21:30:38 -0400

Grrr.... Well they should go talk to mr./ms. Zhang about the virus on
his/her laptop shouldn't they? In any case, as far as I understand the
mail standards, delivery errors should go to the contents of the
Sender: field, not From:, exactly to avoid bothering entire mailing
lists.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ozric at web.de  Wed Jun  4 08:20:19 2003
From: ozric at web.de (Christian Schulz)
Date: Wed, 4 Jun 2003 08:20:19 +0200
Subject: [R] plot rpart tree's from list object
Message-ID: <000f01c32a61$60810b90$d200a8c0@pc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030604/afd7cd21/attachment.pl

From P.Lemmens at nici.kun.nl  Wed Jun  4 08:23:21 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 04 Jun 2003 08:23:21 +0200
Subject: [R] (no subject)
In-Reply-To: <x265nmkds1.fsf@biostat.ku.dk>
References: <F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>
	<x265nmkds1.fsf@biostat.ku.dk>
Message-ID: <667968.1054715001@lemmens.socsci.kun.nl>

Hoi Peter,

--On woensdag 4 juni 2003 0:16 +0200 Peter Dalgaard BSA 
<p.dalgaard at biostat.ku.dk> wrote:

> "Gilda Garibotti" <gilda.garibotti at hci.utah.edu> writes:
>
>> Hi,
>> I would like to know if it is possible to get printed output while a
>> loop is taking place. Example:
>> for(i in 1:10){
>>  print(i)
>>  some long process
>> }
>>
>> This will print the values of i only after the loop is finished, what I
>> would like is to  see them when the process enters the i-th iteration to
>> keep track of how the  program is running.
>
> Windows, right? (This is system dependent) There's a menu item
> entitled "Buffer output" or something to that effect. Turn it off and
> print() calls display immediately. Lengthy output becomes slower,
> though.
>
If you don't want to depend on you (or other people) turning of the 
buffering, use something like

cat("this or that"); flush.console.


regards,
Paul


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From ripley at stats.ox.ac.uk  Wed Jun  4 08:45:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jun 2003 07:45:18 +0100 (BST)
Subject: [R] Logistic regression problem: propensity score matching
In-Reply-To: <3EDD3265.4080405@blueyonder.co.uk>
Message-ID: <Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>

1) Why are you using multinom when this is not a multinomial logistic 
regression?  You could just use a binomial glm.

2) The second argument to predict() is `newdata'.  `sample' is an R 
function, so what did you mean to have there?  I think the predictions 
should be a named vector if `sample' is a data frame.

3) There are many more examples of such things (and more explanation) in 
Venables & Ripley's MASS (the book).

On Wed, 4 Jun 2003, Paul Bivand wrote:

> I am doing one part of an evaluation of a mandatory welfare-to-work 
> programme in the UK.
> As with all evaluations, the problem is to determine what would have 
> happened if the initiative had not taken place.
> In our case, we have a number of pilot areas and no possibility of 
> random assignment.
> Therefore we have been given control areas.
> My problem is to select for survey individuals in the control areas who 
> match as closely as possible the randomly selected sample of action area 
> participants.
> As I understand the methodology, the procedure is to run a logistic 
> regression to determine the odds of a case being in the sample, across 
> both action and control areas, and then choose for control sample the 
> control area individual whose odds of being in the sample are closest to 
> an actual sample member.
> 
> So far, I have following the multinomial logistic regression example in 
> Fox's Companion to Applied Regression.
> Firstly, I would like to know if the predict() is producing odds ratios 
> (or probabilities) for being in the sample, which is what I am aiming 
> for. 

You asked for `probs', so you got probabilities.

> Secondly, how do I get rownames (my unique identifier) into the 
> output from predict() - my input may be faulty somehow and the wrong 
> rownames being picked up - as I need to export back to database to sort 
> and match in names, addresses and phone numbers for my selected samples.
> 
> My code is as follows:
> londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
> rownames=ORCID)
> attach(londonpsm)
> mod.multinom <- multinom(sample ~ AGE + DISABLED + GENDER + ETHCODE + 
> NDYPTOT + NDLTUTOT + LOPTYPE)
> lonoutput <- predict(mod.multinom, sample, type='probs')
> london2 <- data.frame(lonoutput)
> 
> The Logistic regression seems to work, although summary() says the it is 
> not a matrix.

what is `it'?

> The output looks like odds ratios, but I would like to know whether this 
> is so.

No.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Detlef.Steuer at unibw-hamburg.de  Wed Jun  4 09:28:12 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Wed, 04 Jun 2003 09:28:12 +0200 (CEST)
Subject: Rtips (was Re: [R] ? building a database with a the great examples
In-Reply-To: <3EDCAF62.5040807@ku.edu>
Message-ID: <XFMail.20030604092812.steuer@unibw-hamburg.de>


On 03-Jun-2003 Paul E. Johnson wrote:
> I think there is a lot of merit in this idea.  I think there is a big 
> question about "authentication" and protection of Wikis from vandalism.

Yes. But I`ll do my daily backups, so major vandalism wouldn't be such a
problem. (I hope.)
Personally I don't like to have a list of people who are allowed to edit the
pages. (and I dont like to keep such a list current ...)
Under "preferences" you can choose to give yourself an identity, but that's not
for authentication.

> 
> I've set up Wikis for other projects that I started after Rtips.  I have 
> not seen your Wiki software before,  but it looks pretty nice.  I see it 
> does have diff support, so old pages can be restored, yes?  

Yes! It keeps the diffs for quite some time. So if anyone realizes minor, i.e.
pagewise, vandalism content can be rebuild.

> But it 
> doesn't authenticate users, which causes me some concern.  (I understand 
> the Wiki philosophy that we should not be concerned about 
> authentication, but I've never bought into it all the way).

I think the R community is growing very fast. The work to give
passwords to interested people frightens me. And I would never ask
for such a password personally. I'm willing to write some short note _now_, but
not if I have to wait a day or so to get access. Authentication fits if you
have a well defined group you expect to add contents. 

My position therefore: give it a try. If vandalism turns out to be a problem,
I'll have to think about it.

> 
> I have a TWiki site here:
> 
> http://www.ku.edu/cgiwrap/pauljohn/twiki/view
> 
> This one I hacked up special to use authentication on some pages so that 
> people have to log in before they can edit.
> 
> I had not realized before I looked at your page that Wiki 
> implementations are customized for document format.  For page sections, 
> your Wiki uses
> 
> = aHeading =  
> 
> but Twiki uses
> 
> ---+ aHeading
> 
> That's kindof a bummer.

Yes. I don't understand the authors of wikis, too. I just chose a recommended
 one. If there is anything serious against UseModWiki, _now_ would be the
time to switch. UseMod is very easy to set up, an needs little resources.
These two points are very important for me. 


detlef



From Patrik.Waldmann at djingis.se  Wed Jun  4 11:18:52 2003
From: Patrik.Waldmann at djingis.se (Patrik Waldmann)
Date: Wed, 4 Jun 2003 11:18:52 +0200
Subject: [R] Mode of MCMC chain
Message-ID: <000801c32a7a$50f4c0e0$a110a8c0@djingisob7lo8t>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030604/c550169f/attachment.pl

From Tom.Mulholland at health.wa.gov.au  Wed Jun  4 11:26:56 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Wed, 4 Jun 2003 17:26:56 +0800 
Subject: [R] Strip location and grid colour in Lattice
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56F9E@nt207mesep.health.wa.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030604/33887f05/attachment.pl

From christian.hoffmann at wsl.ch  Wed Jun  4 12:13:40 2003
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Wed, 04 Jun 2003 12:13:40 +0200
Subject: [R] (no subject)
In-Reply-To: <9481265.1054723814@lemmens.socsci.kun.nl>
References: <5.1.0.14.2.20030604103541.0269e5f8@mail.wsl.ch>
	<5.1.0.14.2.20030604083742.025b1e10@mail.wsl.ch>
	<x265nmkds1.fsf@biostat.ku.dk>
	<F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>
	<x265nmkds1.fsf@biostat.ku.dk>
	<5.1.0.14.2.20030604083742.025b1e10@mail.wsl.ch>
	<5.1.0.14.2.20030604103541.0269e5f8@mail.wsl.ch>
Message-ID: <5.1.0.14.2.20030604121043.0274b7c8@mail.wsl.ch>

Hi everybody,

I finally hope to reach the person who started the thread "[R] (no 
subject)".  The innermost level of text was my original request.

Kind regards
Christian

At 10:50 2003-06-04 +0200, you wrote:
>Hoi Christian,
>
>--On woensdag 4 juni 2003 10:36 +0200 Christian Hoffmann 
><christian.hoffmann at wsl.ch> wrote:
>
>>Hi Paul
>>
>>At 08:44 2003-06-04 +0200, you wrote:
>>>Hoi Christian,
>>>
>>>--On woensdag 4 juni 2003 8:39 +0200 Christian Hoffmann
>>><christian.hoffmann at wsl.ch> wrote:
>>>>
>>>>Please avoid "no subject". There might be people (like me, when I am in
>>>>a hurry or in a bad mood) who just delete such messages. It would be a
>>>>pity to miss interesting information, yet.



>>>Please avoid prematurely emailing somebody who did not originate a
>>>thread  that is under your current scrutiny!
>>
>>No offence meant, but...
>>How could I find out this person if I already deleted so many messages of
>>that kind?
>No offence either, but why mail me, and not the person who sent you the 
>first mail (in this current thread i.e.) without the subject? IMHO either 
>be consistent and mail everybody, or mail nobody!?
>
>regards,
>Paul
>
>
>
>--
>Paul Lemmens
>NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
>Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
>NL-6525 HR Nijmegen                                              X
>The Netherlands                                                 / \
>Phonenumber    +31-24-3612648
>Fax            +31-24-3616066
>
>

Dr.sc.math.Christian W. Hoffmann
Mathematics and Statistical Computing
Landscape Dynamics and Spatial Development
Swiss Federal Research Institute WSL
Zuercherstrasse 111
CH-8903 Birmensdorf, Switzerland
phone: ++41-1-739 22 77    fax: ++41-1-739 22 15
e-mail: christian.hoffmann at wsl.ch
www: http://www.wsl.ch/staff/christian.hoffmann/
- Coordinator of cooperation WSL - UFU Ekaterinburg/Russia -
Please avoid sending me Word or PowerPoint attachments.
See http://www.fsf.org/philosophy/no-word-attachments.html
Dangers of .doc, .rtf: http://www.heise.de/newsticker/data/jk-27.01.02-001/



From temiz at deprem.gov.tr  Wed Jun  4 13:26:25 2003
From: temiz at deprem.gov.tr (orkun)
Date: Wed, 04 Jun 2003 14:26:25 +0300
Subject: [R] predict.glm(glm.ob,type="terms")
Message-ID: <3EDDD761.60208@deprem.gov.tr>

hello

pgeo<-predict.glm(glm.ob,type="resp") works fine.

But I need to get predictions values in terms of each factor variables.
pgeo<-predict.glm(glm.ob,type="terms")
gives "Error in rep(1/n,n) %*% model.matrix(object): non conformable
arguments"


Could anyone tell me why ?

Ahmet Temiz
Turkey


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From tblackw at umich.edu  Wed Jun  4 14:02:11 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 4 Jun 2003 08:02:11 -0400 (EDT)
Subject: [R] (no subject)
In-Reply-To: <F062093E456F8C4A9566C5CA59726C6E1972F3@EMAIL.hci.utah.edu>
Message-ID: <Pine.SOL.4.44.0306040801190.21132-100000@mspacman.gpcc.itd.umich.edu>


I think cat() will print immediately from inside a loop.

-  tom blackwell  -  u mihigan medical school  -  ann arbor  -

On Tue, 3 Jun 2003, Gilda Garibotti wrote:

> Hi,
> I would like to know if it is possible to get printed output while a loop is taking place.
> Example:
> for(i in 1:10){
>  print(i)
>  some long process
> }
>
> This will print the values of i only after the loop is finished, what I would like is to
> see them when the process enters the i-th iteration to keep track of how the
> program is running.
>
> Thank you,
> Gilda
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From temiz at deprem.gov.tr  Wed Jun  4 14:04:06 2003
From: temiz at deprem.gov.tr (orkun)
Date: Wed, 04 Jun 2003 15:04:06 +0300
Subject: [R] coefficient of logistic regression
In-Reply-To: <5.1.0.14.2.20030603184955.01ea1440@mcmail.cis.mcmaster.ca>
References: <5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603184955.01ea1440@mcmail.cis.mcmaster.ca>
Message-ID: <3EDDE036.2010206@deprem.gov.tr>

John Fox wrote:

> Dear Ahmet,
>
> Sorry for the slow response, but I've been busy all today, 
> coincidentally teaching a workshop on logistic regression.
>
> Tom Blackwell sent you a useful suggestion for interpreting 
> coefficients on the odds scale. If you want to trace out the partial 
> relationship of the fitted probability of response to a particular 
> predictor holding others constant, you can set the other predictors to 
> typical values and let the predictor in question vary over its range, 
> transforming the fitted log-odds to the probability scale.
>
> You may be interested in my effects package (on CRAN or at 
> <http://socserv.socsci.mcmaster.ca/jfox/Misc/effects/index.html>), 
> which makes these kinds of displays for linear and generalized-linear 
> models, including those with interactions.
>
> Regards,
>  John
>
> At 03:06 PM 6/3/2003 +0300, orkun wrote:
>
>> John Fox wrote:
>>
>>> At 11:54 AM 6/3/2003 +0300, orkun wrote:
>>>
>>>> in logistic regression,
>>>> I want to know that it is possible to get probability values of 
>>>> each predictors by
>>>> using following formula for each predictor one by one (keeping 
>>>> constant the others)
>>>> <<< exp(coef)/(1+exp(coef)) >>>
>>>
>>>
>>>
>>> Dear Ahmet,
>>>
>>> This will almost surely give you nonsense, since it produces a 
>>> fitted probability ignoring the constant in the model (assuming that 
>>> there is one), setting other predictors to 0 and the predictor in 
>>> question to 1. What is it that you want to do?
>>>
>>> I hope that this helps,
>>>  John
>>>
>>>
>> thank you
>>
>> Say, I just want to find each predictor's particular effect on 
>> dependent variables.
>> Actual model is to prepare landslide susceptibility map on GIS. So  I 
>> want to know
>> what the effect as probability value comes from each predictor. For 
>> instane what is the effect
>> of  slope on landslide susceptibility. Should I keep others constant ?
>>
>> kind regards
>
>
>
>
>
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>
>
Dear Mr. Fox

thank you very much all.

Because of related to your answer. I ask you directly if you don't mind
I studied several ways after my email.
I wonder whether pgeo<-predict.glm(glm.ob,type="terms")
gives same result with probability value I asked before.
I tried on it. But it gives "Error in rep(1/n,n) %*% 
model.matrix(object): non conformable
arguments" .

By the way , your teaching notes is available on the internet ?


cordially

can y



______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From temiz at deprem.gov.tr  Wed Jun  4 14:08:35 2003
From: temiz at deprem.gov.tr (orkun)
Date: Wed, 04 Jun 2003 15:08:35 +0300
Subject: [R] coefficient of logistic regression
In-Reply-To: <Pine.SOL.4.44.0306031535190.15996-100000@rygar.gpcc.itd.umich.edu>
References: <Pine.SOL.4.44.0306031535190.15996-100000@rygar.gpcc.itd.umich.edu>
Message-ID: <3EDDE143.2090609@deprem.gov.tr>

Thomas W Blackwell wrote:

>Ahmet  -
>
>In a logistic regression model, fitted probabilities make
>sense for individual cases (rows in the data set), as well
>as for future cases (predictions) for which no outcome
>(success or failure) has been observed yet.  Fitted
>probabilities are calculated from the matrix formula:
>
>  Pr[success]  =  exp( X %*% beta) / (1 + exp( X %*% beta)
>
>where  X  is an [n x (p+1)] matrix, containing all p predictor
>variables as columns, preceded by a column of 1s for the
>intercept, and  beta  is the [(p+1) x 1] vector of logistic
>regression coefficients.
>
>One can interpret the sign and the magnitude of an individual
>regression coeffient by saying that an increase of 1 unit in
>predictor variable [i] will increase or decrease the odds of
>success by a multiplier of  exp(beta[i]).  When  beta[i] > 0
>the odds increase, because  exp(beta[i]) > 1,  and when
>beta[i] < 0  the odds decrease, because  exp(beta[i]) < 1.
>
>I hope this explanation helps.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>On Tue, 3 Jun 2003, orkun wrote:
>
>  
>
>>Hello
>>
>>in logistic regression,
>>I want to know that it is possible to get probability values of each
>>predictors by
>>using following formula for each predictor one by one (keeping constant
>>the others)
>> <<< exp(coef)/(1+exp(coef)) >>>
>>
>>thanks in advance
>>Ahmet Temiz
>>    
>>
>
>
>
>  
>
Dear Mr. Fox

thank you very much all.

So,  using the formula -exp(coef)/(1+exp(coef))- for getting probability 
of each predictor is correct.

Because of related to your answer. I ask you directly if you don't mind
I studied several ways after my email.
I wonder whether pgeo<-predict.glm(glm.ob,type="terms")
gives same result with probability value I asked before.
I tried on it. But it gives "Error in rep(1/n,n) %*% 
model.matrix(object): non conformable
arguments" .

could you tell me why ?


cordially



______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From p.pagel at gsf.de  Wed Jun  4 14:04:46 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 4 Jun 2003 14:04:46 +0200
Subject: [R] convert factor to numeric
Message-ID: <20030604120446.GA5875@porcupine.gsf.de>


	Hi R-experts!

Every once in a while I need to convert a factor to a vector of numeric
values. as.numeric(myfactor) of course returns a nice numeric vector of
the indexes of the levels which is usually not what I had in mind:

> v <- c(25, 3.78, 16.5, 37, 109)
> f <- factor(v)
> f
[1] 25   3.78 16.5 37   109
Levels: 3.78 16.5 25 37 109
> as.numeric(f)
[1] 3 1 2 4 5
>

What I really want is a function "unfactor" that returns v:
> unfactor(f)
[1]  25.00   3.78  16.50  37.00 109.00

Of course I could use something like

> as.numeric(levels(f)[as.integer(f)])

But I just can't believe there is no R function to do this in a more
readable way. Actually, the behaviour of as.numeric() doesn't strike me
as very intuitive. I'm sure it has been implemented that way for a
reason - but what is it?

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg
Germany



From dmurdoch at pair.com  Wed Jun  4 14:33:37 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 04 Jun 2003 08:33:37 -0400
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <1054704248.4122.291.camel@localhost>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
	<np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
	<1054704248.4122.291.camel@localhost>
Message-ID: <7iprdv4vq020ltvuq2sivscc3mqie2mr5l@4ax.com>

On 04 Jun 2003 00:24:08 -0500, you wrote:

>Excel 2002 (XP):
>
>Cell Formula          Value
>= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
>=(0.5 - 0.4 - 0.1)    -2.77555756156289000000E-17
 ...
>What is interesting is the change in the displayed value in Excel when
>the second formula is surrounded by parens (which I found purely by
>accident). This would suggest that there may be something going on in
>the parsing of the cell formula that affects the calculation and
>displayed value. 

"Interesting"?  I'd say "horrifying".  When (expr) does not evaluate
the same as expr, what can you trust?

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Jun  4 14:46:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jun 2003 13:46:32 +0100 (BST)
Subject: [R] (no subject)
In-Reply-To: <Pine.SOL.4.44.0306040801190.21132-100000@mspacman.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0306041344570.4793-100000@gannet.stats>

On Wed, 4 Jun 2003, Thomas W Blackwell wrote:

> 
> I think cat() will print immediately from inside a loop.

So will print(), but Gilda is probably on Windows and hasn't read the
rw-FAQ Q6.3.

FAQs are useful things!

> On Tue, 3 Jun 2003, Gilda Garibotti wrote:
> 
> > I would like to know if it is possible to get printed output while a loop is taking place.
> > Example:
> > for(i in 1:10){
> >  print(i)
> >  some long process
> > }
> >
> > This will print the values of i only after the loop is finished, what I would like is to
> > see them when the process enters the i-th iteration to keep track of how the
> > program is running.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun  4 14:55:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jun 2003 13:55:03 +0100 (BST)
Subject: [R] convert factor to numeric
In-Reply-To: <20030604120446.GA5875@porcupine.gsf.de>
Message-ID: <Pine.LNX.4.44.0306041352260.4793-100000@gannet.stats>

See the FAQ, Q7.12.

On Wed, 4 Jun 2003, Philipp Pagel wrote:

> Every once in a while I need to convert a factor to a vector of numeric
> values. as.numeric(myfactor) of course returns a nice numeric vector of
> the indexes of the levels which is usually not what I had in mind:
...

It's done that way because that is how it is defined to work in S, and 
lots of code relies on it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paulda at BATTELLE.ORG  Wed Jun  4 15:09:59 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Wed, 04 Jun 2003 09:09:59 -0400
Subject: [R] Rounding problem R vs Excel
Message-ID: <940250A9EB37A24CBE28D858EF077749136DC3@ws-bco-mse3.milky-way.battelle.org>

I don't have the reference, but a biologist friend of mine
once showed me a refereed journal article that purported
to demonstrate numerical errors made by MSExcel.  This 
would have been Excel97 or Excel2000... In any case, the
journal's scope was biological in nature and the article 
was of interest since Excel is heavily used in that community.

-david paul

-----Original Message-----
From: Duncan Murdoch [mailto:dmurdoch at pair.com] 
Sent: Wednesday, June 04, 2003 8:34 AM
To: MSchwartz at medanalytics.com
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Rounding problem R vs Excel


On 04 Jun 2003 00:24:08 -0500, you wrote:

>Excel 2002 (XP):
>
>Cell Formula          Value
>= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
>=(0.5 - 0.4 - 0.1)    -2.77555756156289000000E-17
 ...
>What is interesting is the change in the displayed value in Excel when 
>the second formula is surrounded by parens (which I found purely by 
>accident). This would suggest that there may be something going on in 
>the parsing of the cell formula that affects the calculation and 
>displayed value.

"Interesting"?  I'd say "horrifying".  When (expr) does not evaluate the
same as expr, what can you trust?

Duncan Murdoch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Wed Jun  4 15:14:13 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 4 Jun 2003 09:14:13 -0400 (EDT)
Subject: [R] Mode of MCMC chain
In-Reply-To: <000801c32a7a$50f4c0e0$a110a8c0@djingisob7lo8t>
Message-ID: <Pine.SOL.4.44.0306040859060.21132-100000@mspacman.gpcc.itd.umich.edu>

Patrik  -

I interpret your question as "statistical mode" rather than
"storage mode", etc. and I ask you to think about whether
this question is even well-defined.  Statistical mode, in the
sense of the most frequent value, is well-defined for samples
from a distribution on a discrete set, and it is well-defined
for a continuous density, relative to a specified measure ...
but for a SAMPLE from a continuous distribution, I think the
concept of "mode" is only defined via a (parametric or non-
parametric) estimate for an underlying smooth density.  The
exact criteria for "smooth" in this context want some careful
thought, as well.

So, with that as preamble, I think the answer to your question
is the function  density().

Others may be much more familiar with the recent statistical
literature in this area than I am.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 4 Jun 2003, Patrik Waldmann wrote:

> are there any functions in R for estimation of the mode of a MCMC-chain?
>
> Patrik Waldmann



From lehmann at puk.unibe.ch  Wed Jun  4 15:21:52 2003
From: lehmann at puk.unibe.ch (Christoph Lehmann)
Date: Wed, 04 Jun 2003 15:21:52 +0200
Subject: [R] simple matrix question
Message-ID: <1054732912.2712.1.camel@christophl>

what is the easiest way to get from 
> x
     x1 x2
[1,]  2  3
[2,]  3  2
[3,]  1  3
[4,]  1  4

the matrix xbar

> xbar1
       x1 x2
[1,] 1.75  3
[2,] 1.75  3
[3,] 1.75  3
[4,] 1.75  3

with the mean of the columns of x as values?

thanks

christoph

-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry



From P.Lemmens at nici.kun.nl  Wed Jun  4 15:28:17 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 04 Jun 2003 15:28:17 +0200
Subject: [R] simple matrix question
In-Reply-To: <1054732912.2712.1.camel@christophl>
References: <1054732912.2712.1.camel@christophl>
Message-ID: <26170609.1054740497@lemmens.socsci.kun.nl>

Hoi Christoph,

--On woensdag 4 juni 2003 15:21 +0200 Christoph Lehmann 
<lehmann at puk.unibe.ch> wrote:

> what is the easiest way to get from
> x
>      x1 x2
> [1,]  2  3
> [2,]  3  2
> [3,]  1  3
> [4,]  1  4
>
> xbar1
>        x1 x2
> [1,] 1.75  3
> [2,] 1.75  3
> [3,] 1.75  3
> [4,] 1.75  3
>
> with the mean of the columns of x as values?
>
xbar1 <- tapply(as.vector(x), gl(2,4), mean);



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From Alexander.Ploner at mep.ki.se  Wed Jun  4 15:40:09 2003
From: Alexander.Ploner at mep.ki.se (Alexander Ploner)
Date: Wed, 04 Jun 2003 15:40:09 +0200
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <940250A9EB37A24CBE28D858EF077749136DC3@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <JJEBKFOIJAKFKJKPEKIFCEHMCCAA.Alexander.Ploner@mep.ki.se>

> I don't have the reference, but a biologist friend of mine
> once showed me a refereed journal article that purported
> to demonstrate numerical errors made by MSExcel.  This
> would have been Excel97 or Excel2000... In any case, the
> journal's scope was biological in nature and the article
> was of interest since Excel is heavily used in that community.

@ARTICLE{McCull99,
   AUTHOR = {McCullough, B. and Wilson, B.},
   TITLE = {On the Accuracy of Statistical Procedures in Microsoft Excel
97},
   JOURNAL = "Comp. Stat. \& Data Anal.",
   VOLUME  = 31,
   NUMBER  = 1,
   YEAR    = 1999,
   PAGES   = {27-37}
}

I seem to remember that there was a follow-up article (note?) in CSDA
regarding the persistence of the problems in Excel 2000.

alex

Alexander.Ploner at mep.ki.se
Phone: 46-8-524-82329
Fax  : 46-8-314975
Medical Epidemiology & Biostatistics
Karolinska Institutet,
P.O. Box 281, SE-171 77 Stockholm



From p.dalgaard at biostat.ku.dk  Wed Jun  4 15:42:01 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 04 Jun 2003 13:42:01 -0000
Subject: [R] convert factor to numeric
In-Reply-To: <20030604120446.GA5875@porcupine.gsf.de>
References: <20030604120446.GA5875@porcupine.gsf.de>
Message-ID: <x27k820x6r.fsf@biostat.ku.dk>

Philipp Pagel <p.pagel at gsf.de> writes:

> But I just can't believe there is no R function to do this in a more
> readable way. Actually, the behaviour of as.numeric() doesn't strike me
> as very intuitive. I'm sure it has been implemented that way for a
> reason - but what is it?

One reason is S compatibility, as Brian pointed out. But there is also
the point that you can *always* convert a factor to its underlying
integer values, but only *sometimes* convert the level names. Generally
we prefer code that fails more rarely...

as.numeric(as.character(f)) works too, although not quite as
efficiently as as.numeric(levels(f))[f].

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rolf at math.unb.ca  Wed Jun  4 15:43:28 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 4 Jun 2003 10:43:28 -0300 (ADT)
Subject: [R] Rounding problem R vs Excel.
Message-ID: <200306041343.h54DhSe1023553@erdos.math.unb.ca>

David A. Paul wrote:

> I don't have the reference, but a biologist friend of mine once
> showed me a refereed journal article that purported to demonstrate
> numerical errors made by MSExcel.  This would have been Excel97 or
> Excel2000... In any case, the journal's scope was biological in
> nature and the article was of interest since Excel is heavily used in
> that community.

An excellent source of information about the incompetence of Excel
as a statistical package can be found on Jonathan Cryer's web
page at

	http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf

Some incredible howlers from their ``help'' pages are given, as
are a number of useful references.  (None from the biological
literature however.  If David Paul could dig up the reference
mentioned by his biologist friend, it would be useful.)

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From MSchwartz at medanalytics.com  Wed Jun  4 15:53:56 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 04 Jun 2003 13:53:56 -0000
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <940250A9EB37A24CBE28D858EF077749136DC3@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF077749136DC3@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <1054734816.6724.38.camel@localhost>

On Wed, 2003-06-04 at 08:09, Paul, David A wrote:
> I don't have the reference, but a biologist friend of mine
> once showed me a refereed journal article that purported
> to demonstrate numerical errors made by MSExcel.  This 
> would have been Excel97 or Excel2000... In any case, the
> journal's scope was biological in nature and the article 
> was of interest since Excel is heavily used in that community.
> 
> -david paul


There is a series of articles here:

http://www.stat.uni-muenchen.de/~knuesel/elv/accuracy.html


In addition, there are additional references on Excel specifically:


On the accuracy of statistical procedures in Microsoft Excel 2000 and
Excel XP
B.D. McCullough and B. Wilson, (2002), Computational Statistics & Data
Analysis, 40, pp 713 - 721
http://www.elsevier.com/gej-ng/10/15/38/85/51/28/abstract.html


On the accuracy of statistical procedures in Microsoft Excel ?97
B.D. McCullough and B. Wilson, (1999), Computational Statistics & Data
Analysis, 31, pp 27-37
http://www.elsevier.com/gej-ng/10/15/38/37/25/27/article.pdf


Problems with using Microsoft Excel for statistics
J.D. Cryer, (2001), presented at the Joint Statistical Meetings,
American Statistical Association, 2001, Atlanta Georgia
at http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf


Use of Excel for statistical analysis
Neil Cox, (2000), AgResearch Ruakura
at http://www.agresearch.cri.nz/Science/Statistics/exceluse1.htm


Using Excel for statistical data analysis
Eva Goldwater, (1999), Univ. of Massachusetts Office of Information
Technology
http://www.umass.edu/acco/statistics/handout/excel.html


Statistical analysis using Microsoft Excel 
Jeffrey Simonoff, (2002)
at http://www.stern.nyu.edu/~jsimonof/classes/1305/pdf/excelreg.pdf


Testing the Intrinsic Functions of Excel
National Physical Laboratory, UK
http://www.npl.co.uk/ssfm/ssfm1/validate/testing/excel.html



There are also some general articles on several stats applications by
McCullough.

http://www.amstat.org/publications/tas/mccull-1.pdf
http://www.amstat.org/publications/tas/mccull.pdf


It has been some time since I looked at many of these papers, but if my
memory is correct, in general, not much has changed in Excel since "97".
However, from McCullough's most recent article:

"The problems that rendered Excel 97 unfit for use as a statistical
package have not been fixed in either Excel 2000 or Excel 2002 (also
called "Excel XP"). Microsoft attempted to fix errors in the standard
normal random number generator and the inverse normal function, and in
the former case actually made the problem worse."


Many of the above articles have an overlap on references, some
published, some are online resources or lecture notes.


HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Wed Jun  4 15:56:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 04 Jun 2003 13:56:53 -0000
Subject: [R] simple matrix question
In-Reply-To: <26170609.1054740497@lemmens.socsci.kun.nl>
References: <1054732912.2712.1.camel@christophl>
	<26170609.1054740497@lemmens.socsci.kun.nl>
Message-ID: <x2y90iym4o.fsf@biostat.ku.dk>

Paul Lemmens <P.Lemmens at nici.kun.nl> writes:

> Hoi Christoph,
> 
> --On woensdag 4 juni 2003 15:21 +0200 Christoph Lehmann
> <lehmann at puk.unibe.ch> wrote:
> 
> > what is the easiest way to get from
> > x
> >      x1 x2
> > [1,]  2  3
> > [2,]  3  2
> > [3,]  1  3
> > [4,]  1  4
> >
> > xbar1
> >        x1 x2
> > [1,] 1.75  3
> > [2,] 1.75  3
> > [3,] 1.75  3
> > [4,] 1.75  3
> >
> > with the mean of the columns of x as values?
> >
> xbar1 <- tapply(as.vector(x), gl(2,4), mean);

No. I think you're missing the point (it gives a 2-vector rather than
a 4x2 matrix, and apply(x,2,mean) is quicker for that). Try

sweep(x,2,apply(x,2,mean),function(x,y)y)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rwatkins at cornerstonelp.com  Wed Jun  4 16:44:40 2003
From: rwatkins at cornerstonelp.com (rwatkins@cornerstonelp.com)
Date: Wed, 4 Jun 2003 09:44:40 -0500
Subject: [R] Error Using dwtest
Message-ID: <NDEKIJPPGJCIKBNEDOKOAEFLCCAA.rwatkins@cornerstonelp.com>

Hello all-
	I have two time series, Index1stdiff and Comps1stdiff.  I regressed the
first on the second and R returned the summary stats I expected.  Then I
looked at and plotted the residuals.  I then wanted to assess
autocorrelation characteristics and tried to run a Durbin-Watson using:

	library(lmtest)
      dwtest(formula=Index1stdiff~Comps1stdiff,alternative=c("greater"))

	I am getting the following error:

	Error in solve.default(crossprod(X), tol = tol) :
        Lapack routine dgesv: system is exactly singular

Can anyone assess why my attempt crashes?  I tried this earlier on what I
thought was similar data and was returned an answer for "d" and a "p-value".

Thanks in advance for your assistance.
Rick



From suzi_k at 012.net.il  Wed Jun  4 17:50:39 2003
From: suzi_k at 012.net.il (Suzi)
Date: Wed, 4 Jun 2003 17:50:39 +0200
Subject: [R] Re: Your application
References: <20030602092120.EMTQ8220.fep3@IENTSTU50>
Message-ID: <001d01c32ab1$2a0d5280$2b5ffea9@pc1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030604/0a79c2f1/attachment.pl

From bavorak at klobouk.fsv.cvut.cz  Wed Jun  4 16:57:06 2003
From: bavorak at klobouk.fsv.cvut.cz (bavorak@klobouk.fsv.cvut.cz)
Date: Wed, 4 Jun 2003 16:57:06 +0200 (CEST)
Subject: [R] combined graph
Message-ID: <Pine.LNX.4.43.0306041648240.15549-100001@klobouk.fsv.cvut.cz>

I need to create graph composed of different plots (bar + area). See
attachment, I am sending an example of intended design.
Is it possible to plot one set as fitted line and add another one in the
column shape, as it is shown on the example? It is intended because of
hydrological convence where totals have been depicted as bars and
processes as lines or areas.

Thanks,

Tomas Bayer



From henric.nilsson at statisticon.se  Wed Jun  4 17:01:05 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 04 Jun 2003 17:01:05 +0200
Subject: [R] gam()
Message-ID: <5.2.1.1.0.20030603165357.03ee4828@pop3.norton.antivirus>

Dear all,

I've now spent a couple of days trying to learn R and, in particular, the 
gam() function, and I now have a few questions and reflections regarding 
the latter. Maybe these things are implemented in some way that I'm not yet 
aware of or have perhaps been decided by the R community to not be what's 
wanted. Of course, my lack of complete theoretical understanding of what 
mgcv really does may also show...

1. When fitting models where a factor interacts with a smooth term, say 
y~a+s(x,by=a.1)+s(x,by=a.2), I noticed that the rug in the plot of each of 
the smooth terms is identical. I expected the rug in the plot of e.g. 
s(x,by=a.1) to only include those x for which a.1=1 to be able to judge if 
observations of x where a.1=1 are sparse in any region. Also, it would be 
really if nice the "by=..." was included in the output of the plot.gam() 
and the "Approximate significance of smooth terms:" part of the summary.gam().

2. John Fox has modified anova.glm() into anova.gam() 
(http://www.socsci.mcmaster.ca/jfox/Books/Companion/nonparametric-regression.txt) 
for comparison of two or more fitted models based on the difference between 
residual deviances. Indiscriminate use of such a procedure shouldn't 
perhaps be encouraged, but I think that many users expect it to be part of 
the mgcv package since this model selection idea is covered in several 
texts and also implemented in S-plus (and may be OK for truly nested 
models). And even if it's been decided that this functionality is not 
wanted in mgcv, perhaps another function comparing several models by the 
GCV/UBRE score and other useful statistics can be implemented?

3. Some authors [1, 2] suggests pointwise estimation of odds ratios and 
corresponding confidence intervals based on the smooth terms in a GAM. 
Maybe something for mgcv?
[1] Figueiras, A. & Cadarso-Su?rez C. (2001) "Application of Nonparametric 
Models for calculating Odds Ratios and Their Confidence Intervals for 
Continuous Exposures", American Journal of Epidemiology, 154(3), 264-275.
[2] Saez, M., Cadarso-Su?rez C. & Figueiras, A. (2003) "np.OR: an S-Plus 
function for pointwise nonparametric estimation of odds-ratios of 
continuous predictors", Computer Methods and Programs in Biomedicine, 71, 
175-179.

4. For each purely parametric covariate a t-test is produced; I'd like to 
have something like S-plus' anova.gam() to get an overall test. (Perhaps 
with the addition of a choice between Type I and Type III tests, but I 
guess that may be controversial). Is it possible?

//Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From ggrothendieck at volcanomail.com  Wed Jun  4 17:10:15 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Wed, 4 Jun 2003 08:10:15 -0700 (PDT)
Subject: [R] simple matrix question
Message-ID: <20030604151015.45F6A40A8@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030604/6e0f2575/attachment.pl

From otoomet at econ.dk  Wed Jun  4 17:14:25 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Wed, 4 Jun 2003 17:14:25 +0200
Subject: [R] Rounding problem R vs Excel.
In-Reply-To: <200306041343.h54DhSe1023553@erdos.math.unb.ca> (message from
	Rolf Turner on Wed, 4 Jun 2003 10:43:28 -0300 (ADT))
References: <200306041343.h54DhSe1023553@erdos.math.unb.ca>
Message-ID: <200306041514.h54FEP809747@punik.econ.au.dk>

Hej,

just about excel -- excel 97 includ a kind of flying game (or what
ever it should be called).  There was a landscape you could fly above,
and in the middle of it there were a screen with different names
(authors?).  You had to put the cursor on a particular cell and click
a particular icon, AFAIR, in order to start the game.

I have not hear about it later but I have tried it myself.


That program is just full of surprises.

Ott



From tblackw at umich.edu  Wed Jun  4 17:15:04 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 4 Jun 2003 11:15:04 -0400 (EDT)
Subject: [R] plot rpart tree's from list object
In-Reply-To: <000f01c32a61$60810b90$d200a8c0@pc>
Message-ID: <Pine.SOL.4.44.0306041039240.11104-100000@mspacman.gpcc.itd.umich.edu>

Christian  -

I am looking at the call to  post()  in the code fragment below,
and I wonder whether all of the arguments that are supplied will
be matched correctly.  I am also reading  help("post.rpart").

The first argument is the tree, and that should be matched okay.
The second argument in the help is named "title.".  It's unusual
to include a period explicitly at the end of an argument name,
and I wonder whether maybe it's needed in order to distinguish
this use of "title" from that in some other function ?

However, the second example at the bottom of the help page seems
to succeed in spite of using "title" without the dot, and it passes
the arguments in the same order as shown in your example ...

Hmmmm.   There are three experiments I would try, if this were
my problem - and NO guarantees that any of them will solve it:

(1) Pass arguments in exactly the order they appear in the
post.rpart() function definition, and use the dot in "title.".
The order is:  tree, title., filename, digits, pretty, use.n, ... .
(This is from R 1.6.2.  Type  post.rpart  (no parentheses) at
the command line prompt to see the function definition in the
version you are using.

(2) Try typing  graphics.off()  at the command line prompt
after the loop has finished.  This is needed in order to get
the last plot into the output file if you have used the
function  postscript()  from base R.

(3) Observe that all 18 filenames generated by  paste() will
be the same.  I think that  paste("Tree","i",sep=".ps")  will
produce  "Tree.psi"  every time.  The i is not interpreted
because it is in quotes.  So each new plot overwrites the
previous one.  An alternative might be:
paste("Tree", i, "ps", sep=".")  giving eg  "Tree.18.ps".

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -


On Wed, 4 Jun 2003, Christian Schulz wrote:

> i want the post plot's from a rpart list object with
> 18 tree's , getting no error - but getting no files,too?
> Perhaps i should using assign!?
>
> for (i in 1:length(treeList)) {
> post(treeList[[i]],filename=paste("Tree","i",sep=".ps"), title="Arbeitszufriedenheit",
> digits=getOption("digits") - 0,use.n=TRUE)
> }
>
> many thanks for help,
> christian



From tmurph6 at po-box.mcgill.ca  Wed Jun  4 17:16:01 2003
From: tmurph6 at po-box.mcgill.ca (Tanya Murphy)
Date: Wed, 4 Jun 2003 11:16:01 -0400
Subject: [R] Estimates for proportions
Message-ID: <3F4B97FA@webmail.mcgill.ca>

Hello,

I need to get a point estimate and SD for a proportion, but the subjects' data 
are not binary---they are proportions (of doses received). That is, I have a 
proportion for each subject. In the past I have analysed these data as a 
continuous (normal) variable, but I really don't want CIs over 100%. This 
seems like basic stuff, but I don't remember learning it and it's proving 
difficult to find (in medical statistics texts, anyway). Any pointers would be 
greatly appreciated!

Thanks!

Tanya Murphy
Dept. Epidemiology
McGill University



From tlumley at u.washington.edu  Wed Jun  4 17:24:35 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 4 Jun 2003 08:24:35 -0700 (PDT)
Subject: [R] convert factor to numeric
In-Reply-To: <Pine.LNX.4.44.0306041352260.4793-100000@gannet.stats>
Message-ID: <Pine.A41.4.44.0306040817070.78220-100000@homer36.u.washington.edu>

On Wed, 4 Jun 2003, Prof Brian Ripley wrote:

> See the FAQ, Q7.12.
>
> On Wed, 4 Jun 2003, Philipp Pagel wrote:
>
> > Every once in a while I need to convert a factor to a vector of numeric
> > values. as.numeric(myfactor) of course returns a nice numeric vector of
> > the indexes of the levels which is usually not what I had in mind:
> ...
>
> It's done that way because that is how it is defined to work in S, and
> lots of code relies on it.
>

There's also a consistency problem.  Suppose you have a factor
with levels(*)
   3163 2006 98195 98103  OX13DP

You really don't want to define as.numeric() so that its value for the
first four of these depends on whether the fifth is present.  So either
as.numeric() has to be NA for anything that can't be coerced to a number
or it needs something like its current definition.


	-thomas


(*) I'm not just being difficult.  These are postcodes I have had over the
past ten years or so.



From grassi at psico.univ.trieste.it  Wed Jun  4 17:31:11 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Wed, 4 Jun 2003 17:31:11 +0200 (MEST)
Subject: [R] 3d scatter plot
Message-ID: <200306041531.RAA04545@server.psico.univ.trieste.it>

how can i draw a 3d scatter plot? thank you.



From spencer.graves at pdf.com  Wed Jun  4 17:42:33 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 04 Jun 2003 08:42:33 -0700
Subject: [R] Estimates for proportions
References: <3F4B97FA@webmail.mcgill.ca>
Message-ID: <3EDE1369.8050807@pdf.com>

	  1.  If none of the numbers are 0 or 1, I might try a logit 
transformation log(p/(1-p)).  Then I'd make a normal probability plot of 
the transformed variables to check the transformation.  If that seemed 
OK., then I'd do the computations on logit space and back transform the 
result.

	  2.  If some of the numbers are 0 or 1, I'd shrink everything from 0 
and 1 using p0 = (c0+(1-2*c0)*p), then log(p0/(1-p0)).

	  Have you considered this?

hth.  spencer graves

Tanya Murphy wrote:
> Hello,
> 
> I need to get a point estimate and SD for a proportion, but the subjects' data 
> are not binary---they are proportions (of doses received). That is, I have a 
> proportion for each subject. In the past I have analysed these data as a 
> continuous (normal) variable, but I really don't want CIs over 100%. This 
> seems like basic stuff, but I don't remember learning it and it's proving 
> difficult to find (in medical statistics texts, anyway). Any pointers would be 
> greatly appreciated!
> 
> Thanks!
> 
> Tanya Murphy
> Dept. Epidemiology
> McGill University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From grassi at psico.univ.trieste.it  Wed Jun  4 17:46:50 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Wed, 4 Jun 2003 17:46:50 +0200 (MEST)
Subject: [R] cook distance
Message-ID: <200306041546.RAA04762@server.psico.univ.trieste.it>

where is the cook-distance in R? i can't find it!please 
help me!



From jccorrea at perseus.unalmed.edu.co  Wed Jun  4 17:56:26 2003
From: jccorrea at perseus.unalmed.edu.co (Juan Carlos Correa Morales)
Date: Wed, 4 Jun 2003 10:56:26 -0500 (Etc/GMT)
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <940250A9EB37A24CBE28D858EF077749136DC3@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <Pine.GSO.4.44.0306041055130.19883-100000@perseus.unalmed.edu.co>

Hi: Below you find two articles about Excel.

Knusel, L. (1998) On the Accuracy of Statistical Distributions in
Microsoft Excel 97. Computational Statistics & Data Analysis. Vol. 26, No.
3, pp. 375-377

McCullough, B.D. y Wilson, D. (1999) On the Accuracy of Statistical
Procedures in Microsoft Excel 97. Computational Statistics & Data
Analysis. Vol. 31, No. 1, pp. 27-37



On Wed, 4 Jun 2003, Paul, David  A wrote:

> I don't have the reference, but a biologist friend of mine
> once showed me a refereed journal article that purported
> to demonstrate numerical errors made by MSExcel.  This
> would have been Excel97 or Excel2000... In any case, the
> journal's scope was biological in nature and the article
> was of interest since Excel is heavily used in that community.
>
> -david paul
>
> -----Original Message-----
> From: Duncan Murdoch [mailto:dmurdoch at pair.com]
> Sent: Wednesday, June 04, 2003 8:34 AM
> To: MSchwartz at medanalytics.com
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Rounding problem R vs Excel
>
>
> On 04 Jun 2003 00:24:08 -0500, you wrote:
>
> >Excel 2002 (XP):
> >
> >Cell Formula          Value
> >= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
> >=(0.5 - 0.4 - 0.1)    -2.77555756156289000000E-17
>  ...
> >What is interesting is the change in the displayed value in Excel when
> >the second formula is surrounded by parens (which I found purely by
> >accident). This would suggest that there may be something going on in
> >the parsing of the cell formula that affects the calculation and
> >displayed value.
>
> "Interesting"?  I'd say "horrifying".  When (expr) does not evaluate the
> same as expr, what can you trust?
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kmw at rockefeller.edu  Wed Jun  4 18:03:56 2003
From: kmw at rockefeller.edu (Knut M. Wittkowski)
Date: Wed, 04 Jun 2003 12:03:56 -0400
Subject: [R] Rounding problem R vs Excel.
In-Reply-To: <200306041343.h54DhSe1023553@erdos.math.unb.ca>
Message-ID: <5.1.0.14.0.20030604120020.0286b9c8@imap.rockefeller.edu>

At 10:43 2003-06-04 -0300, you wrote:
>David A. Paul wrote:
>
> > I don't have the reference, but a biologist friend of mine once
> > showed me a refereed journal article that purported to demonstrate
> > numerical errors made by MSExcel.  This would have been Excel97 or
> > Excel2000... In any case, the journal's scope was biological in
> > nature and the article was of interest since Excel is heavily used in
> > that community.

Here is another reference on Excel "features":

http://www.rucares.org/Course/Excel.ppv

Note that this presentation runs with PowerPoint 97 and the viewer in more 
recent versions, but that more recent versions of PowerPoint are not really 
compatible.


Knut M. Wittkowski, PhD,DSc
------------------------------------------
The Rockefeller University, GCRC
1230 York Ave #121B, Box 322, NY,NY 10021
+1(212)327-7175, +1(212)327-8450 (Fax)
kmw at rockefeller.edu
http://www.rucares.org/statist/



From ernesto at ipimar.pt  Wed Jun  4 18:07:18 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 04 Jun 2003 16:07:18 -0000
Subject: [R] cook distance
In-Reply-To: <200306041546.RAA04762@server.psico.univ.trieste.it>
References: <200306041546.RAA04762@server.psico.univ.trieste.it>
Message-ID: <1054742802.1245.112.camel@gandalf.ipimar.pt>

On Wed, 2003-06-04 at 16:46, Michele Grassi wrote:
> where is the cook-distance in R? i can't find it!please 
> help me!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Hi

Try 

?cooks.distance

it's a part of the influence.measures functions.

Regards

EJ



From p.dalgaard at biostat.ku.dk  Wed Jun  4 18:21:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 04 Jun 2003 16:21:30 -0000
Subject: [R] cook distance
In-Reply-To: <200306041546.RAA04762@server.psico.univ.trieste.it>
References: <200306041546.RAA04762@server.psico.univ.trieste.it>
Message-ID: <x2ptlt6c2l.fsf@biostat.ku.dk>

Michele Grassi <grassi at psico.univ.trieste.it> writes:

> where is the cook-distance in R? i can't find it!please 
> help me!

> apropos("cook")
[1] "cooks.distance"     "cooks.distance.glm" "cooks.distance.lm"


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tmurph6 at po-box.mcgill.ca  Wed Jun  4 18:42:16 2003
From: tmurph6 at po-box.mcgill.ca (Tanya Murphy)
Date: Wed, 4 Jun 2003 12:42:16 -0400
Subject: [R] Estimates for proportions
Message-ID: <3F4C78EE@webmail.mcgill.ca>

Thanks Spencer!

That's what I was looking for. My data are not at all normal and neither are 
the transformed values because The majority are at an upper limit that is not 
1 (these are grouped data abstracted from a paper). There's nothing that can 
be done about that, but it was good to compare the CIs for the original and 
back-transformed values. The SD for the transformed disitribution was huge 
compared to the one from the original values, though. Is this normal?

Tanya



From gavin.simpson at ucl.ac.uk  Wed Jun  4 18:43:38 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 04 Jun 2003 17:43:38 +0100
Subject: [R] cook distance
In-Reply-To: <200306041546.RAA04762@server.psico.univ.trieste.it>
References: <200306041546.RAA04762@server.psico.univ.trieste.it>
Message-ID: <3EDE21BA.3040106@ucl.ac.uk>

Michele,

Did you use the search tools in R?

Within R:

 > apropos("cooks")
[1] "cooks.distance"     "cooks.distance.glm" "cooks.distance.lm"

and a similar search using the html help facility returned a single 
entry for the functions influence.measures() in base, under which the 
above three functions are documented.

HTH

Gav

Michele Grassi wrote:
> where is the cook-distance in R? i can't find it!please 
> help me!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From gavin.simpson at ucl.ac.uk  Wed Jun  4 18:49:14 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 04 Jun 2003 17:49:14 +0100
Subject: [R] 3d scatter plot
In-Reply-To: <200306041531.RAA04545@server.psico.univ.trieste.it>
References: <200306041531.RAA04545@server.psico.univ.trieste.it>
Message-ID: <3EDE230A.6000300@ucl.ac.uk>

Michele

get the scatterplot3d package by Uwe Ligges off CRAN:

scatterplot3d: 3D Scatter Plot

     Plots a three dimensional (3D) point cloud.
     Version:	0.3-13
     Depends:	R (>= 1.1.0)
     Date:	2003-01-16
     etc...

Gav

Michele Grassi wrote:
> how can i draw a 3d scatter plot? thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From deepayan at stat.wisc.edu  Wed Jun  4 19:18:09 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 4 Jun 2003 12:18:09 -0500
Subject: [R] 3d scatter plot
In-Reply-To: <200306041531.RAA04545@server.psico.univ.trieste.it>
References: <200306041531.RAA04545@server.psico.univ.trieste.it>
Message-ID: <200306041218.10222.deepayan@stat.wisc.edu>


scatterplot3d in package scatterplot3d
cloud in package lattice

On Wednesday 04 June 2003 10:31, Michele Grassi wrote:
> how can i draw a 3d scatter plot? thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sdfrost at ucsd.edu  Wed Jun  4 19:40:29 2003
From: sdfrost at ucsd.edu (Simon Frost)
Date: Wed, 04 Jun 2003 10:40:29 -0700
Subject: [R] Crash with R1.7.0 + Windows XP Professional
Message-ID: <5.1.0.14.0.20030604103751.0238d090@popmail.ucsd.edu>

Dear R-Help,

I've had some problems installing the R 1.7.0 binary on Windows XP 
Professional. If I launch R from the Start Menu or the Desktop, R crashes. 
However, if I launch by double-clicking on an .RData file, R loads up just 
fine. I've never had any problems with previous versions/OS. Has anyone 
else encountered this problem?

Thanks!
Simon



From deepayan at stat.wisc.edu  Wed Jun  4 20:11:33 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 4 Jun 2003 13:11:33 -0500
Subject: [R] Strip location and grid colour in Lattice
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D56F9E@nt207mesep.health.wa.gov.au>
References: <74E242B6968AA0469B632C5A3EFC1EFD03D56F9E@nt207mesep.health.wa.gov.au>
Message-ID: <200306041311.33629.deepayan@stat.wisc.edu>

On Wednesday 04 June 2003 04:26, Mulholland, Tom wrote:
> I am probably missing something quite obvious, but any help would be
> appreciated. I am continually getting people misreading the lattice plots
> because they are expecting the strip (with the factor names in them) to be
> below the graph. Is there anyway of achieving this.

No. (At least none that is easy.)

> Secondly, from a more personal note I find the grid formed by the axes to
> be a bit overpowering and would like to make it a little less bold by
> changing it to a grey of some kind. I can't see that the scales options
> have anythig in their that I could use. I can change the label colours and
> tick marks, but then I draw a blank.

It's not very clear to me what you want. If you want to change the colors, 
that should be doable. What exactly is the problem ? Do you want something 
more ?

> While I'm on a role, I find that quite often I have to resort to the at and
> label sections of the scales function to get my tickmarks looking OK. This
> seems to be when  am producing line graphs with one of the scales being a
> date (POSIXct). What is not clear to me is if all POSIXct variables are the
> same. The xyplot doco indicates that the at co-ordinates should be native
> co-ordinates. Can anyone point me to where in the voluminous documentation
> one looks to understand what this means. I have found that on some
> occasions the co-ordinates are in seconds (as the documentation on POSIXct
> states, but this afternoon I found that the values seemed to be in years.
> Which wasn't a problem other than I wish I could understand what was
> actually happening.

AFAIK, all POSIXct variables should be the 'same', as you say, being the 
number of seconds since the epoch. They have a particular class, but 
otherwise they are no different from numeric variables.

Older versions of lattice treated these as numeric (which usually were very 
very large values), and hence put horrible labels. Recent versions try to do 
something decent when they identify POSIXct variables, but they are not very 
good at it. So you _will_ need to adjust them by hand most of the time. 
Hopefully things would improve in the future. 

I'm not familiar enough with these things to understand your year problem, 
though.

Deepayan



From ripley at stats.ox.ac.uk  Wed Jun  4 20:40:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jun 2003 19:40:23 +0100 (BST)
Subject: [R] Crash with R1.7.0 + Windows XP Professional
In-Reply-To: <5.1.0.14.0.20030604103751.0238d090@popmail.ucsd.edu>
Message-ID: <Pine.LNX.4.44.0306041938280.7318-100000@gannet.stats>

Sounds as if you have a corrupt .RData file in your default working 
directory for R.  Try renaming it, or otherwise starting with --vanilla.

On Wed, 4 Jun 2003, Simon Frost wrote:

> I've had some problems installing the R 1.7.0 binary on Windows XP 
> Professional. 

*Installing* it?  You only describe problems running it -- have you told 
us the whole story?

> If I launch R from the Start Menu or the Desktop, R crashes. 
> However, if I launch by double-clicking on an .RData file, R loads up just 
> fine. I've never had any problems with previous versions/OS. Has anyone 
> else encountered this problem?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Wed Jun  4 20:49:36 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jun 2003 14:49:36 -0400
Subject: [R] Crash with R1.7.0 + Windows XP Professional
In-Reply-To: <5.1.0.14.0.20030604103751.0238d090@popmail.ucsd.edu>
References: <5.1.0.14.0.20030604103751.0238d090@popmail.ucsd.edu>
Message-ID: <refsdv44h1hmeppsr84jjo2jh4f7341gi2@4ax.com>

On Wed, 04 Jun 2003 10:40:29 -0700, you wrote in message
<5.1.0.14.0.20030604103751.0238d090 at popmail.ucsd.edu>:

>Dear R-Help,
>
>I've had some problems installing the R 1.7.0 binary on Windows XP 
>Professional. If I launch R from the Start Menu or the Desktop, R crashes. 
>However, if I launch by double-clicking on an .RData file, R loads up just 
>fine. I've never had any problems with previous versions/OS. Has anyone 
>else encountered this problem?

It sounds to me as though it is trying to automatically load something
it doesn't like.  See the "Invoking R under Windows" section of the
"An Introduction to R" manual for the loading sequence, and try
playing around with options and/or deleting files to see if that fixes
it.  You can get to the extreme of loading nothing at all by using the
--vanilla option on the command line; if that crashes, then there's
some other problem.

The other piece of advice is to please try out the 1.7.1 beta.  This
might be a bug that has already been fixed.  You can get a recent
build from 

< http://www.stats.uwo.ca/faculty/murdoch/software/r-devel>

Duncan Murdoch



From andy_liaw at merck.com  Wed Jun  4 20:48:11 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 04 Jun 2003 14:48:11 -0400
Subject: [R] Strip location and grid colour in Lattice
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBA6@usrymx25.merck.com>

> From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
> 
> On Wednesday 04 June 2003 04:26, Mulholland, Tom wrote:
> > I am probably missing something quite obvious, but any help would be
> > appreciated. I am continually getting people misreading the 
> lattice plots
> > because they are expecting the strip (with the factor names 
> in them) to be
> > below the graph. Is there anyway of achieving this.
> 
> No. (At least none that is easy.)

A possibly easier solution is to introduce gaps from row to row, thus
leaving no confusion as to which panel the strip is associated with.  My
impression is that gaps can be added at least between columns.  Not sure
about rows.

Andy

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From deepayan at stat.wisc.edu  Wed Jun  4 21:18:46 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 4 Jun 2003 14:18:46 -0500
Subject: [R] Strip location and grid colour in Lattice
In-Reply-To: <3A822319EB35174CA3714066D590DCD5C4FBA6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD5C4FBA6@usrymx25.merck.com>
Message-ID: <200306041418.46124.deepayan@stat.wisc.edu>

On Wednesday 04 June 2003 13:48, Liaw, Andy wrote:
> > From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
> >
> > On Wednesday 04 June 2003 04:26, Mulholland, Tom wrote:
> > > I am probably missing something quite obvious, but any help would be
> > > appreciated. I am continually getting people misreading the
> > lattice plots
> > > because they are expecting the strip (with the factor names
> > in them) to be
> > > below the graph. Is there anyway of achieving this.
> >
> > No. (At least none that is easy.)
>
> A possibly easier solution is to introduce gaps from row to row, thus
> leaving no confusion as to which panel the strip is associated with.  My
> impression is that gaps can be added at least between columns.  Not sure
> about rows.

Yes, that sounds like a good idea. Both columns and rows are possible, with 

between = list(x = 1, y = 1)   etc

Deepayan



From NOTESSMTP1/ASCAP at ascap.com  Wed Jun  4 21:24:34 2003
From: NOTESSMTP1/ASCAP at ascap.com (NOTESSMTP1/ASCAP@ascap.com)
Date: Wed, 4 Jun 2003 15:24:34 -0400
Subject: [R] NAV detected a virus in a document you authored.
Message-ID: <OF50DDDDD1.0B64D9E6-ON85256D3B.006A9EC9@ascap.com>

Please contact your system administrator.


The scanned document was QUARANTINED.


Virus Information:
The attachment documents.pif contained the virus W32.Sobig.C at mm and could
NOT be repaired.

              -------------- A . S . C . A . P.--------------
This message, including any attachments, is intended solely for the person
or entity to which it is addressed and may contain information that is
legally privileged, confidential or otherwise protected from disclosure.
If you are not the intended recipient, please contact sender immediately by
reply email and destroy all copies.



From paul_bivand at blueyonder.co.uk  Wed Jun  4 21:41:59 2003
From: paul_bivand at blueyonder.co.uk (Paul)
Date: Wed, 04 Jun 2003 20:41:59 +0100
Subject: [R] Logistic regression problem: propensity score matching
In-Reply-To: <Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
References: <Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
Message-ID: <3EDE4B87.3030801@blueyonder.co.uk>

Thanks for your reply.

I am using logistic regression because my response variable is 
categorical - and this seems to be recommended in the literature (by 
Heckman, Smith and others).

The response variable is named sample because that is what it is - I am 
new to R so haven't quite got into habits of naming using Title Case.

Having selected a sample from the action area, randomly, the aim is to 
find people to survey who, if they had been in the action area, would 
have had as close odds of being in the samle as those actually selected.

Therefore I select a sample using sample(), write that out back to the 
access database as a new table, then (from R) run a query which creates 
a dataset comprising all those in the action area who could have been 
selected, and the control area group, and read this back in to R (using 
as many characteristics as possible except area) before undertaking the 
logistic regression. sample can take the values 0 (not in sample) or 1 
(n sample).

The aim is to find the odds of being in the sample (by characteristics) 
which is the Propensity Score, and then match action to control using 
Propensity Score Matching.

I have MASS but was unable to locate logistic regression, which I was 
advised was the standard method for my problem.

Thanks again.

Prof Brian Ripley wrote:

>1) Why are you using multinom when this is not a multinomial logistic 
>regression?  You could just use a binomial glm.
>
>2) The second argument to predict() is `newdata'.  `sample' is an R 
>function, so what did you mean to have there?  I think the predictions 
>should be a named vector if `sample' is a data frame.
>
>3) There are many more examples of such things (and more explanation) in 
>Venables & Ripley's MASS (the book).
>
>On Wed, 4 Jun 2003, Paul Bivand wrote:
>
>  
>
>>I am doing one part of an evaluation of a mandatory welfare-to-work 
>>programme in the UK.
>>As with all evaluations, the problem is to determine what would have 
>>happened if the initiative had not taken place.
>>In our case, we have a number of pilot areas and no possibility of 
>>random assignment.
>>Therefore we have been given control areas.
>>My problem is to select for survey individuals in the control areas who 
>>match as closely as possible the randomly selected sample of action area 
>>participants.
>>As I understand the methodology, the procedure is to run a logistic 
>>regression to determine the odds of a case being in the sample, across 
>>both action and control areas, and then choose for control sample the 
>>control area individual whose odds of being in the sample are closest to 
>>an actual sample member.
>>
>>So far, I have following the multinomial logistic regression example in 
>>Fox's Companion to Applied Regression.
>>Firstly, I would like to know if the predict() is producing odds ratios 
>>(or probabilities) for being in the sample, which is what I am aiming 
>>for. 
>>    
>>
>
>You asked for `probs', so you got probabilities.
>
>  
>
>>Secondly, how do I get rownames (my unique identifier) into the 
>>output from predict() - my input may be faulty somehow and the wrong 
>>rownames being picked up - as I need to export back to database to sort 
>>and match in names, addresses and phone numbers for my selected samples.
>>
>>My code is as follows:
>>londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
>>rownames=ORCID)
>>attach(londonpsm)
>>mod.multinom <- multinom(sample ~ AGE + DISABLED + GENDER + ETHCODE + 
>>NDYPTOT + NDLTUTOT + LOPTYPE)
>>lonoutput <- predict(mod.multinom, sample, type='probs')
>>london2 <- data.frame(lonoutput)
>>
>>The Logistic regression seems to work, although summary() says the it is 
>>not a matrix.
>>    
>>
>
>what is `it'?
>
>  
>
>>The output looks like odds ratios, but I would like to know whether this 
>>is so.
>>    
>>
>
>No.
>
>  
>



From gwgilc at wm.edu  Wed Jun  4 22:05:11 2003
From: gwgilc at wm.edu (George W. Gilchrist)
Date: Wed, 04 Jun 2003 16:05:11 -0400
Subject: [R] Looking for R analogue of S-plus function ms()
Message-ID: <BB03C937.1AF5%gwgilc@wm.edu>

In S-Plus, there is a function ms() [minimum sums] that estimates parameters
in a multivariate model and returns, along with the estimates, a
loglikelihood score. It's very handy, but I have not found an R version yet.
Does anyone know if such a thing exists? Thanks.

Cheers, George

==================================================================
George W. Gilchrist                        Email #1: gwgilc at wm.edu
Department of Biology, Box 8795          Email #2: kitesci at cox.net
College of William & Mary                    Phone: (757) 221-7751
Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
http://gwgilc.people.wm.edu/



From sundar.dorai-raj at pdf.com  Wed Jun  4 23:14:59 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 04 Jun 2003 16:14:59 -0500
Subject: [R] Looking for R analogue of S-plus function ms()
References: <BB03C937.1AF5%gwgilc@wm.edu>
Message-ID: <3EDE6153.8080804@pdf.com>

Perhaps ?optim or ?optimize will suffice.

George W. Gilchrist wrote:
> In S-Plus, there is a function ms() [minimum sums] that estimates parameters
> in a multivariate model and returns, along with the estimates, a
> loglikelihood score. It's very handy, but I have not found an R version yet.
> Does anyone know if such a thing exists? Thanks.
> 
> Cheers, George
> 
> ==================================================================
> George W. Gilchrist                        Email #1: gwgilc at wm.edu
> Department of Biology, Box 8795          Email #2: kitesci at cox.net
> College of William & Mary                    Phone: (757) 221-7751
> Williamsburg, VA 23187-8795                    Fax: (757) 221-6483
> http://gwgilc.people.wm.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From arrayprofile at yahoo.com  Wed Jun  4 23:18:14 2003
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 4 Jun 2003 14:18:14 -0700 (PDT)
Subject: [R] R packages in S-Plus?
In-Reply-To: <3EDE230A.6000300@ucl.ac.uk>
Message-ID: <20030604211814.58380.qmail@web41209.mail.yahoo.com>

Hi, is it possible to use the packages from CRAN in
SPlus? and how to do it if yes?



From Benjamin.STABLER at odot.state.or.us  Thu Jun  5 01:48:35 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 4 Jun 2003 16:48:35 -0700 
Subject: [R] Package upload to CRAN
Message-ID: <76A000A82289D411952F001083F9DD06047FE102@exsalem4-bu.odot.state.or.us>

I want upload two packages to CRAN, but I can't seem to connect to
ftp://cran.r-project.org/incoming.  Do I need a username and password? 

Thanks.  
Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From tmulholl at bigpond.net.au  Thu Jun  5 01:45:43 2003
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Thu, 05 Jun 2003 07:45:43 +0800
Subject: [R] Strip location and grid colour in Lattice
In-Reply-To: <200306041311.33629.deepayan@stat.wisc.edu>
Message-ID: <000401c32af3$6a2476e0$0202a8c0@WorkGroup>

I'm not sure of the correct term, but in essence it is the equivalent to the
frame in the basic plots or might be described as the bounding box to each
plot. That is when you look at 16 plots on a page these frames (boxes) make
up a grid.


-----Original Message-----
> Secondly, from a more personal note I find the grid formed by the axes to
> be a bit overpowering and would like to make it a little less bold by
> changing it to a grey of some kind. I can't see that the scales options
> have anythig in their that I could use. I can change the label colours and
> tick marks, but then I draw a blank.

It's not very clear to me what you want. If you want to change the colors,
that should be doable. What exactly is the problem ? Do you want something
more ?


Deepayan


I think the "between = list(x = 1, y = 1)   etc" will work nicely as an
immediate solution.

Thank you Andy and Deepayan for your help on the first question. I think
Deepyan's answer to my ramble about POSIXct in lattice gives me all the
information I required. There's probably as much finger (brain) trouble on
my part, as there is anything else.


______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Jun  5 08:30:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 07:30:32 +0100 (BST)
Subject: [R] R packages in S-Plus?
In-Reply-To: <20030604211814.58380.qmail@web41209.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306050726220.8534-100000@gannet.stats>

On Wed, 4 Jun 2003, array chip wrote:

> Hi, is it possible to use the packages from CRAN in
> SPlus? and how to do it if yes?

It's going to be possible only if the packages use the common subset of 
the S language implemented in both R and S-PLUS: and the porter (you) will 
have to know how to check that.  It also depends on which version of 
S-PLUS.  But in brief

R CMD Rdconv can convert the help files to Ssgml, and less successfully to 
Sd.

Any C files may need conversion to S headers.

The R files may work.

After that you install in S-PLUS in the usual way.

For more details see `S Programming' and its on-line complements.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Jun  5 08:33:11 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 08:33:11 +0200
Subject: [R] Package upload to CRAN
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE102@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD06047FE102@exsalem4-bu.odot.state.or.us>
Message-ID: <3EDEE427.3020305@statistik.uni-dortmund.de>

Benjamin.STABLER at odot.state.or.us wrote:

> I want upload two packages to CRAN, but I can't seem to connect to
> ftp://cran.r-project.org/incoming.  Do I need a username and password? 

1) I can connect.
2) There's anonymous login on CRAN/incoming.

Uwe Ligges

> Thanks.  
> Benjamin Stabler
> Transportation Planning Analysis Unit
> Oregon Department of Transportation
> 555 13th Street NE, Suite 2
> Salem, OR 97301  Ph: 503-986-4104
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ozric at web.de  Thu Jun  5 08:57:25 2003
From: ozric at web.de (Christian Schulz)
Date: Thu, 5 Jun 2003 08:57:25 +0200
Subject: [R] plot rpart tree's from list object
References: <Pine.LNX.4.44.0306041634520.5137-100000@gannet.stats>
Message-ID: <006901c32b2f$ba0bd220$d200a8c0@pc>

....many Thanks!
For another lazzy typist's - this works  fine.

for (i in 1:length(tList)) {
post(tList[[i]],paste("Tree", i, "ps", sep="."), title="title",
digits=getOption("digits") - 0,use.n=TRUE)
graphics.off()
}

christian


> Yes, you need dev.off() inside the loop and to name the files differently.
>
> It's title. to avoid complaints about masking functions of the same name,
> in long-ago versions of S3 and R.
>
> On Wed, 4 Jun 2003, Thomas W Blackwell wrote:
>
> > Christian  -
> >
> > I am looking at the call to  post()  in the code fragment below,
> > and I wonder whether all of the arguments that are supplied will
> > be matched correctly.  I am also reading  help("post.rpart").
> >
> > The first argument is the tree, and that should be matched okay.
> > The second argument in the help is named "title.".  It's unusual
> > to include a period explicitly at the end of an argument name,
> > and I wonder whether maybe it's needed in order to distinguish
> > this use of "title" from that in some other function ?
> >
> > However, the second example at the bottom of the help page seems
> > to succeed in spite of using "title" without the dot, and it passes
> > the arguments in the same order as shown in your example ...
> >
> > Hmmmm.   There are three experiments I would try, if this were
> > my problem - and NO guarantees that any of them will solve it:
> >
> > (1) Pass arguments in exactly the order they appear in the
> > post.rpart() function definition, and use the dot in "title.".
> > The order is:  tree, title., filename, digits, pretty, use.n, ... .
> > (This is from R 1.6.2.  Type  post.rpart  (no parentheses) at
> > the command line prompt to see the function definition in the
> > version you are using.
> >
> > (2) Try typing  graphics.off()  at the command line prompt
> > after the loop has finished.  This is needed in order to get
> > the last plot into the output file if you have used the
> > function  postscript()  from base R.
> >
> > (3) Observe that all 18 filenames generated by  paste() will
> > be the same.  I think that  paste("Tree","i",sep=".ps")  will
> > produce  "Tree.psi"  every time.  The i is not interpreted
> > because it is in quotes.  So each new plot overwrites the
> > previous one.  An alternative might be:
> > paste("Tree", i, "ps", sep=".")  giving eg  "Tree.18.ps".
> >
> > HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> >
> > On Wed, 4 Jun 2003, Christian Schulz wrote:
> >
> > > i want the post plot's from a rpart list object with
> > > 18 tree's , getting no error - but getting no files,too?
> > > Perhaps i should using assign!?
> > >
> > > for (i in 1:length(treeList)) {
> > > post(treeList[[i]],filename=paste("Tree","i",sep=".ps"),
title="Arbeitszufriedenheit",
> > > digits=getOption("digits") - 0,use.n=TRUE)
> > > }
> > >
> > > many thanks for help,
> > > christian
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From Tom.Mulholland at health.wa.gov.au  Thu Jun  5 09:57:38 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Thu, 5 Jun 2003 15:57:38 +0800 
Subject: [R] Strip location and grid colour in Lattice
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FA0@nt207mesep.health.wa.gov.au>

I am not using panel.grid directly. It appears to use the reference.line
component of the lattice.theme. Since I have tried changing all obvious
comonents in the lattice.theme (including reference.line) I assume I am
correct in this assertion.

I probably used the wrong word, but I wasn't sure what else to call it. It
is the grid around each plot and strip. It dominates the trellis ...(that's
what I should have called it). How do you change the colour of the trellis? 


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Wednesday, 4 June 2003 6:26 PM

What grid?  You put it there, and you can change its colour.  It comes from
calling panel.grid, and arg(panel.grid) will hint to you how to change this.



From ripley at stats.ox.ac.uk  Thu Jun  5 10:19:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 09:19:59 +0100 (BST)
Subject: box colours in lattice (was RE: [R] Strip location and grid colour
	in Lattice)
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D56FA0@nt207mesep.health.wa.gov.au>
Message-ID: <Pine.LNX.4.44.0306050910180.14832-100000@gannet.stats>

If you mean the rectangular boxes surrounding the panels and strips, they 
are drawn by grid.rect() and not under the control of lattice.

If so, it is *not* a grid but a series of boxes, and they don't always
align perfectly, as example(xyplot) shows on my screen (and if you
had some space between the plots this would be clearer to you).

You could alter the grid.text calls in print.trellis to change the colour 
....

On Thu, 5 Jun 2003, Mulholland, Tom wrote:

> I am not using panel.grid directly. It appears to use the reference.line
> component of the lattice.theme. Since I have tried changing all obvious
> comonents in the lattice.theme (including reference.line) I assume I am
> correct in this assertion.
> 
> I probably used the wrong word, but I wasn't sure what else to call it. It
> is the grid around each plot and strip. It dominates the trellis ...(that's
> what I should have called it). How do you change the colour of the trellis? 
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Wednesday, 4 June 2003 6:26 PM
> 
> What grid?  You put it there, and you can change its colour.  It comes from
> calling panel.grid, and arg(panel.grid) will hint to you how to change this.
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From smap at ucar.edu  Thu Jun  5 10:52:32 2003
From: smap at ucar.edu (SMTP incoming)
Date: Thu,  5 Jun 2003 02:52:32 -0600 (MDT)
Subject: [R] DANGEROUS E-MAIL ATTACHMENT ALERT
Message-ID: <20030605085232.1806BDABA5@mscan4.ucar.edu>

============================================================================
It is known that viruses now often forge who sent them, so the message
this warning is about may or may not have actually been sent by your system.
============================================================================

    At Thu Jun  5 02:52:31 MDT 2003, the e-mail "sma018799"

    To: <<sstein at unavco.ucar.edu>> 
    From: <<r-help at lists.r-project.org>> 

    was quarantined due to a suspicious attachment:

filename="approved.pif

The trigger was 30:	name="approved.pif" 
33:	filename="approved.pif



From deepayan at stat.wisc.edu  Thu Jun  5 10:58:49 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 5 Jun 2003 03:58:49 -0500
Subject: box colours in lattice (was RE: [R] Strip location and grid
	colour in Lattice)
In-Reply-To: <Pine.LNX.4.44.0306050910180.14832-100000@gannet.stats>
References: <Pine.LNX.4.44.0306050910180.14832-100000@gannet.stats>
Message-ID: <200306050358.49929.deepayan@stat.wisc.edu>


I'm still not completely sure what you are after, but if indeed you want to 
change the color of the rectangle borders, how about this ?:

x <- rnorm(100)
y <- rnorm(100)
a <- factor(sample(1:4, 100, rep = T))
 
lset(list(axis.line = list(col = "grey")))
xyplot(y ~ x | a)

(Make sure you have a recent version of lattice.)

Deepayan

On Thursday 05 June 2003 03:19, Prof Brian Ripley wrote:
> If you mean the rectangular boxes surrounding the panels and strips, they
> are drawn by grid.rect() and not under the control of lattice.
>
> If so, it is *not* a grid but a series of boxes, and they don't always
> align perfectly, as example(xyplot) shows on my screen (and if you
> had some space between the plots this would be clearer to you).
>
> You could alter the grid.text calls in print.trellis to change the colour
> ....
>
> On Thu, 5 Jun 2003, Mulholland, Tom wrote:
> > I am not using panel.grid directly. It appears to use the reference.line
> > component of the lattice.theme. Since I have tried changing all obvious
> > comonents in the lattice.theme (including reference.line) I assume I am
> > correct in this assertion.
> >
> > I probably used the wrong word, but I wasn't sure what else to call it.
> > It is the grid around each plot and strip. It dominates the trellis
> > ...(that's what I should have called it). How do you change the colour of
> > the trellis?
> >
> >
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: Wednesday, 4 June 2003 6:26 PM
> >
> > What grid?  You put it there, and you can change its colour.  It comes
> > from calling panel.grid, and arg(panel.grid) will hint to you how to
> > change this.



From ernesto at ipimar.pt  Thu Jun  5 13:41:30 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 05 Jun 2003 11:41:30 -0000
Subject: [R] scales in xyplot doesn't seem to work for x axis
Message-ID: <1054813269.1242.15.camel@gandalf.ipimar.pt>

Hi

I'm doing a xyplot and I wand to reduce the number of tick marks in the
x axis. My x axis are month and I want to reduce the 12 tick marks to 4.
I used the scales argument but it doesn't seem to work, althougth it
works on y axis if I use scales=list(tick.number=4).

xyplot(land~mes|porto+arte,data=hom.land.mpa[hom.land.mpa$arte!="pseine",],type="l",scales=list(x=list(tick.number=4)))

Regards

EJ

-- 
Ernesto Jardim <ernesto at ipimar.pt>
Marine Biologist
IPIMAR, Lisboa, Portugal
SuSE Linux 8.1;R 1.7.0;LyX 1.3.2;Gnome 2.2.2;OO1.1Beta



From drosadi at server.eos.tuwien.ac.at  Thu Jun  5 13:53:35 2003
From: drosadi at server.eos.tuwien.ac.at (Dedi Rosadi)
Date: Thu, 05 Jun 2003 13:53:35 +0200
Subject: [R] estimation stable distribution parameters
Message-ID: <5.2.1.1.0.20030605134008.009f03b0@server.eos.tuwien.ac.at>

Hi,
I am wondering whether anyone of you already implemented in R or S+  the 
methods of estimation the parameters of stable distribution. If there is, i 
will be happy to get the copy of the codes.

Regards
Dedi



From zeileis at ci.tuwien.ac.at  Thu Jun  5 13:54:50 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Thu, 5 Jun 2003 13:54:50 +0200
Subject: [R] Error Using dwtest
In-Reply-To: <NDEKIJPPGJCIKBNEDOKOAEFLCCAA.rwatkins@cornerstonelp.com>
References: <NDEKIJPPGJCIKBNEDOKOAEFLCCAA.rwatkins@cornerstonelp.com>
Message-ID: <200306051154.h55BsoSC012361@thorin.ci.tuwien.ac.at>

On Wednesday 04 June 2003 16:44, rwatkins at cornerstonelp.com wrote:

> Hello all-
> 	I have two time series, Index1stdiff and Comps1stdiff.  I regressed
> the first on the second and R returned the summary stats I expected.
>  Then I looked at and plotted the residuals.  I then wanted to
> assess autocorrelation characteristics and tried to run a
> Durbin-Watson using:
>
> 	library(lmtest)
>      
> dwtest(formula=Index1stdiff~Comps1stdiff,alternative=c("greater"))
>
> 	I am getting the following error:
>
> 	Error in solve.default(crossprod(X), tol = tol) :
>         Lapack routine dgesv: system is exactly singular
>
> Can anyone assess why my attempt crashes?  I tried this earlier on
> what I thought was similar data and was returned an answer for "d"
> and a "p-value".

It means that the cross product X'X of your model matrix X, which is 
computed by

  X <- model.matrix(formula, data = data)

is exactly singular and thus cannot be inverted. In such a case 
dwtest() cannot compute the p value. You might want to try 
durbin.watson() in the package car which uses a different approach for 
computing the p value.
Best,
Z

> Thanks in advance for your assistance.
> Rick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From maechler at stat.math.ethz.ch  Thu Jun  5 14:23:25 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Jun 2003 14:23:25 +0200
Subject: [R] estimation stable distribution parameters
In-Reply-To: <5.2.1.1.0.20030605134008.009f03b0@server.eos.tuwien.ac.at>
References: <5.2.1.1.0.20030605134008.009f03b0@server.eos.tuwien.ac.at>
Message-ID: <16095.13885.829283.875498@gargle.gargle.HOWL>

>>>>> "Dedi" == Dedi Rosadi <drosadi at server.eos.tuwien.ac.at>
>>>>>     on Thu, 05 Jun 2003 13:53:35 +0200 writes:

    Dedi> I am wondering whether anyone of you already
    Dedi> implemented in R or S+ the methods of estimation the
    Dedi> parameters of stable distribution. If there is, i will
    Dedi> be happy to get the copy of the codes.

The "stable" package available from Jim Lindsey's  "repository"
is said to do even more.

--> http://cran.r-project.org/ -> Software "Other" -- bottom of page

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Friedrich.Leisch at ci.tuwien.ac.at  Thu Jun  5 14:32:52 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Thu, 5 Jun 2003 14:32:52 +0200
Subject: [R] R News Volume 3/1
Message-ID: <16095.14452.682830.688153@galadriel.ci.tuwien.ac.at>

We have published the 2003/1 issue of R News on

        http://cran.R-project.org/doc/Rnews

where you can download the newsletter as PDF or Postscript file. It
will propagate to the CRAN mirrors within a day or two.


Contents of this issue:

Name Space Management for R
Converting Packages to S4
The genetics Package
Variance Inflation Factors    13
Building Microsoft Windows Versions of R and R packages under Intel Linux
Analysing Survey Data in R
Computational Gains Using RPVM on a Beowulf Cluster
R Help Desk  
Book Reviews   
Changes in R 1.7.0 
Changes on CRAN 
Crossword 
Recent Events


For the editorial board,
Fritz Leisch



From emb7 at st-andrews.ac.uk  Thu Jun  5 14:45:24 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Thu, 05 Jun 2003 13:45:24 +0100
Subject: [R] Regression slopes
Message-ID: <5.1.0.14.0.20030605125903.01e52b78@gatty.st-and.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030605/1a69289b/attachment.pl

From v_bill_pikounis at merck.com  Thu Jun  5 14:52:30 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Thu, 05 Jun 2003 08:52:30 -0400
Subject: [R] estimation stable distribution parameters
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F662343@usrymx18.merck.com>

Dedi,
There is a very useful package called "stable" by Philippe Lambert and Jim
Lindsey at

http://alpha.luc.ac.be/~jlindsey/rcode.html

Look down that page to the "Probability functions and generalized regression
models for stable distributions" identifier.  And if you have not read it
already, I highly recommend the paper

Lambert, P. and Lindsey, J.K. (1999) Analysing financial returns using
regression models based on non-symmetric stable distributions. Applied
Statistics 48, 409-424. 

It helps out nicely with understanding the parameterization, etc.

I have had good experiences using this package under Linux, but not so well
under Windows.

Hope that helps.

Bill


----------------------------------------
Bill Pikounis, Ph.D.
Biometrics Research Department
Merck Research Laboratories

> -----Original Message-----
> From: Dedi Rosadi [mailto:drosadi at server.eos.tuwien.ac.at]
> Sent: Thursday, June 05, 2003 7:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] estimation stable distribution parameters
> 
> 
> Hi,
> I am wondering whether anyone of you already implemented in R 
> or S+  the 
> methods of estimation the parameters of stable distribution. 
> If there is, i 
> will be happy to get the copy of the codes.
> 
> Regards
> Dedi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From ripley at stats.ox.ac.uk  Thu Jun  5 15:08:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 14:08:18 +0100 (BST)
Subject: [R] Regression slopes
In-Reply-To: <5.1.0.14.0.20030605125903.01e52b78@gatty.st-and.ac.uk>
Message-ID: <Pine.LNX.4.44.0306051402270.1215-100000@gannet.stats>

No modification needed.  Fit either of

lm(y-x ~ x + z)
lm(y ~ x + z + offset(x))

and the t-test in the summary will be a test of the coefficient of x being 
one.

You can also use such models to do an anova against a modle with unit 
coefficient.

On Thu, 5 Jun 2003, Martin Biuw wrote:

> Sorry if this is an obvious one, but is there a simple way to modify the lm 
> function to test whether a slope coefficient is significantly different 
> from 1 instead of different from 0?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Thu Jun  5 15:13:24 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 05 Jun 2003 08:13:24 -0500
Subject: [R] Regression slopes
References: <5.1.0.14.0.20030605125903.01e52b78@gatty.st-and.ac.uk>
Message-ID: <3EDF41F4.9020602@pdf.com>



Martin Biuw wrote:
> Hi,
> Sorry if this is an obvious one, but is there a simple way to modify the lm 
> function to test whether a slope coefficient is significantly different 
> from 1 instead of different from 0?
> 
> Thanks,
> 

There might be an easier way, but the brute force method would be:

R> set.seed(1)
R> x <- data.frame(x = 1:10, y = 1:10 + rnorm(10))
R> lm.x <- lm(y ~ x, data = x)
R> coef.x <- summary(lm.x)$coef
R> # t-statistic comparing slope to 1
R> t.x <- (coef.x["x","Estimate"]-1)/coef.x["x","Std. Error"]
R> # p-value
R> 2 * pt(abs(t.x), lm.x$df, lower = FALSE)
[1] 0.5559868

Hope this helps,

Sundar



From lvillan at cec.uchile.cl  Thu Jun  5 15:21:09 2003
From: lvillan at cec.uchile.cl (=?iso-8859-1?Q?Le=F3n?= =?iso-8859-1?Q?_Vill=E1n?=)
Date: Thu, 05 Jun 2003 09:21:09 -0400
Subject: [R] Re: Approved
In-Reply-To: <200306050909.h5599EsL018404@ing.uchile.cl>
Message-ID: <5.2.0.9.0.20030605091912.00b4bdc0@mail.cec.uchile.cl>


I will appreciate you explain me the reason for sending me this e-mail, and
the purpose of the attached file (aplication.pi).

Best regards

Leon Villan


At 11:11 a.m. 05/06/2003 +0200, you wrote:
>Please see the attached file.



From th50 at leicester.ac.uk  Thu Jun  5 15:21:51 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 5 Jun 2003 14:21:51 +0100
Subject: [R] Error when creating layouts with partly filled pages within
	lattice
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B656@saffron.cfs.le.ac.uk>

Dear all,

Please take my apologies if that has already been asked
 - at least I couldn't find it in the archives.

When trying to specify a layout within library lattice,
i.e. using xyplot, I get an error when the prepanel
function tries to subscript the automatically generated
x.limits. This seems to appear if the last page wouldn't
be filled completely, i.e. there would be space left
for more panels. Please see session snippet for a simple
example.

Am I missing something? Any hints highly appreciated.

Cheers

Thomas

---

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.0            
year     2003           
month    04             
day      16             
language R              
> n<-900
> my.data<-data.frame(x=1:n,y=rnorm(n))
> my.shingle<-shingle(my.data$x,cbind(0:8*100,1:9*100))
> xyplot(y~x|my.shingle,data=my.data,as.table=T,scales=list(x=list(relation="sliced")),layout=c(1,2))
> xyplot(y~x|my.shingle,data=my.data,as.table=T,scales=list(x=list(relation="sliced")),layout=c(1,3))
> xyplot(y~x|my.shingle,data=my.data,as.table=T,scales=list(x=list(relation="sliced")),layout=c(1,4))
Error in x.limits[[i]] : subscript out of bounds
> traceback()
2: limits.and.aspect(prepanel.default.xyplot, prepanel = prepanel, 
       have.xlim = have.xlim, xlim = xlim, have.ylim = have.ylim, 
       ylim = ylim, x.relation = foo$x.scales$relation, y.relation = foo$y.scales$relation, 
       panel.args.common = foo$panel.args.common, panel.args = foo$panel.args, 
       aspect = aspect, nplots = nplots)
1: xyplot(y ~ x | my.shingle, data = my.data, as.table = T, scales = list(x = list(relation = "sliced")), 
       layout = c(1, 4))

---

P.S. Allow me to take the opportunity to give big thanks
to Deepayan Sarkar for writing and maintaining this 
brilliant package!

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From mailinglist_wegmann at web.de  Thu Jun  5 17:40:52 2003
From: mailinglist_wegmann at web.de (Wegmann (LIST))
Date: Thu, 5 Jun 2003 15:40:52 +0000
Subject: [R] ridge regression
Message-ID: <200306051540.52046.mailinglist_wegmann@web.de>

Hello R-user

I want to compute a multiple regression but I would to include a check for 
collinearity of the variables. Therefore I would like to use a ridge 
regression. 
I tried lm.ridge() but I don't know yet how to get p-values (single Pr() and p 
of the whole model) out of this model. Can anybody tell me how to get a 
similar output like the summary(lm(...)) output? Or if there is another way 
(e.g. subcommands of lm() ) to include a correction for collinearity. 

I hope I was precise enough and included all necessary information otherwise I 
can add some more infos. 

thanks in advance, Cheers Martin



From nolwenn.lemeur at nantes.inserm.fr  Thu Jun  5 15:47:35 2003
From: nolwenn.lemeur at nantes.inserm.fr (Nolwenn Le Meur)
Date: Thu, 5 Jun 2003 15:47:35 +0200
Subject: [R] Logical vectors
Message-ID: <LMEBLNBEKKODLAONNGJMIEHCCBAA.nolwenn.lemeur@nantes.inserm.fr>

Hi everybody,

just a quick question that drives me crazy:
Is it possible to "join" 2 logical vectors ?

i.e.
x<-4
ind <- 1:19200
pin <- c(0, rep(400, 48) * (1:48))

ind1<-((pin[j] + 1) <= ind) & (ind <= pin[j + 2])
ind2<-((pin[j+x] + 1) <= ind) & (ind <= pin[j+x + 2])

ind2 and ind1 give the right TRUE index

#but I would like something like that

ind3<-((pin[j] + 1) <= ind) & (ind <= pin[j + 2]) & ((pin[j+x] + 1) <= ind)
& (ind <= pin[j+x + 2])

#but it doesn't work !

Help, thanks
Nolwenn

********************************************
Nolwenn Le Meur
INSERM U533
Facult? de m?decine
1, rue Gaston Veil
44035 Nantes Cedex 1
France

Tel: (+33)-2-40-41-29-86 (office)
     (+33)-2-40-41-28-44 (secretary)
Fax: (+33)-2-40-41-29-50
mail: nolwenn.lemeur at nantes.inserm.fr



From Ted.Harding at nessie.mcc.ac.uk  Thu Jun  5 15:27:19 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 05 Jun 2003 14:27:19 +0100 (BST)
Subject: [R] Max sig figs as well as Min in print?
Message-ID: <XFMail.030605141223.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

Consider the following example (artificial, but it illustates the point):

  > r2<-sqrt(2)
  > x<-2-r2*r2

  > print(c(pi,sqrt(pi)),digits=5)
  [1] 3.1416 1.7725


  > print(c(pi,sqrt(pi),x),digits=5)
  [1]  3.1416e+00  1.7725e+00 -4.4409e-16

whereas I would prefer

  [1] 3.1416 1.7725 0.0000

which could be achieved by

  > print(c(pi,sqrt(pi),round(x)),digits=5)
  [1] 3.1416 1.7725 0.0000

if need be.

The above of course is due to the definition of "digits" as the miniumum
number of significant figures to print. In order to avoid the behaviour of
the second "print" and make it like the last line, one would need to be
able to set somthing like "maxdigits=5" as well as "digits=5".

Is anything of this sort possible (apart from anticipating the situation
and wrapping relevant computations in "round")?

Thanks, and best wishes,
Ted.



From Timur.Elzhov at jinr.ru  Thu Jun  5 16:00:35 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Thu, 5 Jun 2003 18:00:35 +0400
Subject: [R] persp(), par() and axis()
Message-ID: <20030605140035.GA26781@pcf004.jinr.ru>

Dear R experts,

On explanation of persp() parameters the last item is:

     ...: additional graphical parameters (see `par').

However, setting the `tcl' parameter has no any effect.
I guess that axes are added to persp() in somewhat freakish
way, and have nothing in common with axis() function.

I found the very useful trans3d() function in persp() help
page, and I'd like to use it with persp() + axis(), but I
didn't find how to draw axis() in arbitrary place & direction :(

So, I want to customize axes in persp(). Could anybody help me
solve this problem?
Thanks a lot!


--
WBR,
Timur.



From fharrell at virginia.edu  Thu Jun  5 16:01:42 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 5 Jun 2003 10:01:42 -0400
Subject: [R] ridge regression
In-Reply-To: <200306051540.52046.mailinglist_wegmann@web.de>
References: <200306051540.52046.mailinglist_wegmann@web.de>
Message-ID: <20030605100142.5eec9740.fharrell@virginia.edu>

On Thu, 5 Jun 2003 15:40:52 +0000
"Wegmann (LIST)" <mailinglist_wegmann at web.de> wrote:

> Hello R-user
> 
> I want to compute a multiple regression but I would to include a check for 
> collinearity of the variables. Therefore I would like to use a ridge 
> regression. 
> I tried lm.ridge() but I don't know yet how to get p-values (single Pr() and p 
> of the whole model) out of this model. Can anybody tell me how to get a 
> similar output like the summary(lm(...)) output? Or if there is another way 
> (e.g. subcommands of lm() ) to include a correction for collinearity. 
> 
> I hope I was precise enough and included all necessary information otherwise I 
> can add some more infos. 
> 
> thanks in advance, Cheers Martin

This doesn't really answer your question but the Design packages's ols function is another way to handle penalized least squares.  ols has advantages if you want to differentially penalize different types of terms in the model or if you have any categorical predictors.  Ordinary ridge regression does not correctly scale such variables in my opinion.

The anova method for ols fits 'works' when you penalize the model but there is some controversy over whether we should be testing biased coefficients.  Some believe that hypothesis tests should be done using the unpenalized model.  That brings up other ways to handle collinearity: test groups of variables in combination so they don't compete with each other, or collapse them into summary scores (e.g., principal components) before putting them in the model. 

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From tmulholl at bigpond.net.au  Thu Jun  5 16:07:33 2003
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Thu, 5 Jun 2003 22:07:33 +0800
Subject: box colours in lattice (was RE: [R] Strip location and grid
	colourin Lattice)
In-Reply-To: <Pine.LNX.4.44.0306050910180.14832-100000@gannet.stats>
Message-ID: <000001c32b6b$d006afa0$0202a8c0@WorkGroup>

Despite my assertion that I had checked all the lattice theme components, I
was in error. Having made the appropriate changes in print.trellis and
succeded in what I wanted to do, I noticed obvious trellis.par.get calls and
followed them up. In the end what I required did not take much at all. This
amended xyplot example shows roughly what I was trying to get to.

require(lattice)
SetColGrey <- function(x=NULL)
{
lset(list(background = list(col = "transparent"),
add.text=list(col="grey50",cex=1.3),
axis.line=list(col="grey95"),
regions = list(col = c("navy","steelblue2")),
strip.shingle = list(col = c("grey75")),
strip.background = list(col = c("white"))))
}

SetColGrey()
data(barley)
     barchart(yield ~ variety | site, data = barley,
              groups = year, layout = c(1,6),
              ylab = list("Barley Yield (bushels/acre)",col="grey"),
              scales = list(col="grey25",x = list(abbreviate = TRUE,
                            minlength = 5)),
	        between=list(y=1.5))

Thank you everyone for your help, it has been very much appreciated.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
Sent: Thursday, 5 June 2003 4:20
To: Mulholland, Tom
Cc: (r-help at stat.math.ethz.ch)
Subject: box colours in lattice (was RE: [R] Strip location and grid
colourin Lattice)


If you mean the rectangular boxes surrounding the panels and strips, they
are drawn by grid.rect() and not under the control of lattice.

If so, it is *not* a grid but a series of boxes, and they don't always
align perfectly, as example(xyplot) shows on my screen (and if you
had some space between the plots this would be clearer to you).

You could alter the grid.text calls in print.trellis to change the colour
....

On Thu, 5 Jun 2003, Mulholland, Tom wrote:

> I am not using panel.grid directly. It appears to use the reference.line
> component of the lattice.theme. Since I have tried changing all obvious
> comonents in the lattice.theme (including reference.line) I assume I am
> correct in this assertion.
>
> I probably used the wrong word, but I wasn't sure what else to call it. It
> is the grid around each plot and strip. It dominates the trellis
...(that's
> what I should have called it). How do you change the colour of the
trellis?
>
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Wednesday, 4 June 2003 6:26 PM
>
> What grid?  You put it there, and you can change its colour.  It comes
from
> calling panel.grid, and arg(panel.grid) will hint to you how to change
this.
>
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From anna at ptolemy.arc.nasa.gov  Thu Jun  5 16:20:11 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Thu, 5 Jun 2003 07:20:11 -0700
Subject: Fwd: Re: [R] legend() with option adj=1
Message-ID: <200306050720.11135.anna@ptolemy.arc.nasa.gov>

Is there a simpler way then the solution to the one that was posted here?  I'm 
not very proficient with legend, and I don't understand this solution.   All 
I have is two or more lines on one plot that I want to put a legend on and I 
can't figure out how to do it from the examples.  Can you give a very simple 
example? It does not have to be fancy!!  I have never worked with a package 
where the legend was not automatic.



----------  Forwarded Message  ----------

Subject: Re: [R] legend() with option adj=1
Date: Wed, 21 May 2003 09:19:11 +0200
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
To: Jerome Asselin <jerome at hivnet.ubc.ca>
Cc: r-help at stat.math.ethz.ch

Jerome Asselin wrote:
> Hi there,
>
> I want to justify to right the text of my legend. Consider this short
> reproducable example.
>
> x <- 1:5
> y1 <- 1/x
> y2 <- 2/x
> plot(rep(x,2),c(y1,y2),type="n",xlab="x",ylab="y")
> lines(x,y1)
> lines(x,y2,lty=2)
> legend(5,2,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1)
> legend(5,1.5,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1,adj=1)
>
> Now, I would like to right-justify the text of the legend. As you can see,
> the option adj=1 does not give satisfactory results.
>
> Is this a bug or is there an easy way that I'm missing?
>
> Thanks,
> Jerome

Works, e.g., with the following little trick:

  x <- 1:5
  y1 <- 1/x
  y2 <- 2/x
  plot(rep(x,2),c(y1,y2),type="n",xlab="x",ylab="y")
  lines(x,y1)
  lines(x,y2,lty=2)
  temp <- legend(5, 2, legend = c(" ", " "),
    text.width = strwidth("1,000,000"), lty = 1:2, xjust = 1, yjust = 1)
  text(temp$rect$left + temp$rect$w, temp$text$y,
     c("1,000", "1,000,000"), pos=2)

See ?legend for details, in particular the returned value.

Uwe Ligges

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From vincent.stoliaroff at socgen.com  Thu Jun  5 16:24:09 2003
From: vincent.stoliaroff at socgen.com (vincent.stoliaroff@socgen.com)
Date: Thu, 5 Jun 2003 16:24:09 +0200
Subject: [R] counting missing values
Message-ID: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>

Hello R lovers
I have written a little cute function to count the number of missing value
per row in a matrix and return the percentage of missing value

it takes a lot of time to run with a 1000 rows matrix

I'd like to know if there is a function already implemented to count the
number of occurence of a given values in a vector


For information,
here is the function
count<-0
for (i in 1:nrow(Matrix))
      {
      for (j in 1:ncol(Matrix)) {if (is.na(Matrix[i,j])) count<-count+1}
      Result[i,1]<-((count/(ncol(Matrix)))*100);
      count<-0
      }
Result

thanks for any help
Vincent




*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
titre de ce message s'il a ete altere, deforme ou falsifie.
				********
This message and any attachments (the "message") are confidentia... {{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Jun  5 16:33:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 16:33:05 +0200
Subject: Fwd: Re: [R] legend() with option adj=1
In-Reply-To: <200306050720.11135.anna@ptolemy.arc.nasa.gov>
References: <200306050720.11135.anna@ptolemy.arc.nasa.gov>
Message-ID: <3EDF54A1.8010505@statistik.uni-dortmund.de>

Anna H. Pryor wrote:

> Is there a simpler way then the solution to the one that was posted here?  I'm 
> not very proficient with legend, and I don't understand this solution.   All 
> I have is two or more lines on one plot that I want to put a legend on and I 
> can't figure out how to do it from the examples.  Can you give a very simple 
> example? It does not have to be fancy!!  I have never worked with a package 
> where the legend was not automatic.


Hmm. The simple solution is to use legend() as is, see ?legend for details.
The solution given below was intended for the specific question on right 
justified legend text.

Uwe Ligges

> 
> 
> ----------  Forwarded Message  ----------
> 
> Subject: Re: [R] legend() with option adj=1
> Date: Wed, 21 May 2003 09:19:11 +0200
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> To: Jerome Asselin <jerome at hivnet.ubc.ca>
> Cc: r-help at stat.math.ethz.ch
> 
> Jerome Asselin wrote:
> 
>>Hi there,
>>
>>I want to justify to right the text of my legend. Consider this short
>>reproducable example.
>>
>>x <- 1:5
>>y1 <- 1/x
>>y2 <- 2/x
>>plot(rep(x,2),c(y1,y2),type="n",xlab="x",ylab="y")
>>lines(x,y1)
>>lines(x,y2,lty=2)
>>legend(5,2,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1)
>>legend(5,1.5,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1,adj=1)
>>
>>Now, I would like to right-justify the text of the legend. As you can see,
>>the option adj=1 does not give satisfactory results.
>>
>>Is this a bug or is there an easy way that I'm missing?
>>
>>Thanks,
>>Jerome
> 
> 
> Works, e.g., with the following little trick:
> 
>   x <- 1:5
>   y1 <- 1/x
>   y2 <- 2/x
>   plot(rep(x,2),c(y1,y2),type="n",xlab="x",ylab="y")
>   lines(x,y1)
>   lines(x,y2,lty=2)
>   temp <- legend(5, 2, legend = c(" ", " "),
>     text.width = strwidth("1,000,000"), lty = 1:2, xjust = 1, yjust = 1)
>   text(temp$rect$left + temp$rect$w, temp$text$y,
>      c("1,000", "1,000,000"), pos=2)
> 
> See ?legend for details, in particular the returned value.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Jun  5 16:35:51 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 16:35:51 +0200
Subject: [R] counting missing values
In-Reply-To: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>
References: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>
Message-ID: <3EDF5547.2010709@statistik.uni-dortmund.de>

vincent.stoliaroff at socgen.com wrote:

> Hello R lovers
> I have written a little cute function to count the number of missing value
> per row in a matrix and return the percentage of missing value
> 
> it takes a lot of time to run with a 1000 rows matrix
> 
> I'd like to know if there is a function already implemented to count the
> number of occurence of a given values in a vector
> 
> 
> For information,
> here is the function
> count<-0
> for (i in 1:nrow(Matrix))
>       {
>       for (j in 1:ncol(Matrix)) {if (is.na(Matrix[i,j])) count<-count+1}
>       Result[i,1]<-((count/(ncol(Matrix)))*100);
>       count<-0
>       }
> Result
> 
> thanks for any help
> Vincent

Well, it's pretty easy to do it:

apply(Matrix, 1, function(x) sum(is.na(x))) / ncol(Matrix) * 100

Uwe Ligges



From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Jun  5 16:37:23 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 5 Jun 2003 16:37:23 +0200 (CEST)
Subject: [R] counting missing values
In-Reply-To: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>
References: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>
Message-ID: <Pine.LNX.4.51.0306051635000.23530@artemis.imbe.med.uni-erlangen.de>

> Hello R lovers
> I have written a little cute function to count the number of missing value
> per row in a matrix and return the percentage of missing value
>

R> M <- matrix(rnorm(25), ncol=5)
R> diag(M) <- NA
R> M[2,3] <- NA
R> apply(M, 2, function(x) sum(is.na(x)))/ncol(M)
[1] 0.2 0.2 0.4 0.2 0.2

the percentage of missing values per row...

Torsten

> it takes a lot of time to run with a 1000 rows matrix
>
> I'd like to know if there is a function already implemented to count the
> number of occurence of a given values in a vector
>
>
> For information,
> here is the function
> count<-0
> for (i in 1:nrow(Matrix))
>       {
>       for (j in 1:ncol(Matrix)) {if (is.na(Matrix[i,j])) count<-count+1}
>       Result[i,1]<-((count/(ncol(Matrix)))*100);
>       count<-0
>       }
> Result
>
> thanks for any help
> Vincent
>
>
>
>
> *************************************************************************
> Ce message et toutes les pieces jointes (ci-apres le "message") sont
> confidentiels et etablis a l'intention exclusive de ses destinataires.
> Toute utilisation ou diffusion non autorisee est interdite.
> Tout message electronique est susceptible d'alteration.
> La SOCIETE GENERALE et ses filiales declinent toute responsabilite au
> titre de ce message s'il a ete altere, deforme ou falsifie.
> 				********
> This message and any attachments (the "message") are confidentia... {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From th50 at leicester.ac.uk  Thu Jun  5 16:37:52 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 5 Jun 2003 15:37:52 +0100
Subject: [R] counting missing values
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B657@saffron.cfs.le.ac.uk>

Dear Vincent

> I'd like to know if there is a function already implemented 
> to count the
> number of occurence of a given values in a vector

Does

my.matrix<-cbind(c(0,1,NA),c(NA,1,NA))
count.NA<-function(the.matrix){
  result<-t(apply(the.matrix,1,function(x,n){
    m<-sum(is.na(x))
    c(m,m/n)
  },n=dim(the.matrix)[2]))
  dimnames(result)<-list(dimnames(the.matrix)[[1]],c("row count","proportion"))
  result
}
count.NA(my.matrix)

do what you were looking for? Hope that helps.

Cheers

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From am.power at ucc.ie  Thu Jun  5 16:44:19 2003
From: am.power at ucc.ie (Power, Anne Marie)
Date: Thu, 5 Jun 2003 15:44:19 +0100 
Subject: [R] lda
Message-ID: <49AB9D0C6521D84ABD017BF83CDF44C4051B6D43@xch1.ucc.ie>

Hello

I have been using r to classify fish into groups using the lda formula and
predict(object,)$class.  I am having problems finding an output that
simultaneously shows me classification function coefficients for predictors
and groups (such as is given in spss under classify:linear discriminant for
example). 
I have tried coef.lda which shows linear discriminant functions for
predictors but not groups.  Meanwhile predict.lda only gives the case by 
case posterior probabilities for each group and doesn't show predictors. 
Can anyone please advise?

Anne



From grassi at psico.univ.trieste.it  Thu Jun  5 16:37:44 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Thu, 5 Jun 2003 16:37:44 +0200 (MEST)
Subject: [R] visualize regression plane
Message-ID: <200306051437.QAA27764@server.psico.univ.trieste.it>

Hi,
how can i visualize a regression plane in my 3d 
scatterplot?



From ligges at statistik.uni-dortmund.de  Thu Jun  5 16:49:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 16:49:13 +0200
Subject: [R] Logical vectors
In-Reply-To: <LMEBLNBEKKODLAONNGJMIEHCCBAA.nolwenn.lemeur@nantes.inserm.fr>
References: <LMEBLNBEKKODLAONNGJMIEHCCBAA.nolwenn.lemeur@nantes.inserm.fr>
Message-ID: <3EDF5869.2080308@statistik.uni-dortmund.de>

Nolwenn Le Meur wrote:
> Hi everybody,
> 
> just a quick question that drives me crazy:
> Is it possible to "join" 2 logical vectors ?
> 
> i.e.
> x<-4
> ind <- 1:19200
> pin <- c(0, rep(400, 48) * (1:48))
> 
> ind1<-((pin[j] + 1) <= ind) & (ind <= pin[j + 2])
> ind2<-((pin[j+x] + 1) <= ind) & (ind <= pin[j+x + 2])
> 
> ind2 and ind1 give the right TRUE index
> 
> #but I would like something like that
> 
> ind3<-((pin[j] + 1) <= ind) & (ind <= pin[j + 2]) & ((pin[j+x] + 1) <= ind)
> & (ind <= pin[j+x + 2])
> 
> #but it doesn't work !

What does this mean ("doesn't work")?
I cannot reproduce your example since j is unknown.

Uwe Ligges



From fharrell at virginia.edu  Thu Jun  5 16:52:13 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 5 Jun 2003 10:52:13 -0400
Subject: Fwd: Re: [R] legend() with option adj=1
In-Reply-To: <200306050720.11135.anna@ptolemy.arc.nasa.gov>
References: <200306050720.11135.anna@ptolemy.arc.nasa.gov>
Message-ID: <20030605105213.22c50c9a.fharrell@virginia.edu>

On Thu, 5 Jun 2003 07:20:11 -0700
"Anna  H. Pryor" <anna at ptolemy.arc.nasa.gov> wrote:

> Is there a simpler way then the solution to the one that was posted here?  I'm 
> not very proficient with legend, and I don't understand this solution.   All 
> I have is two or more lines on one plot that I want to put a legend on and I 
> can't figure out how to do it from the examples.  Can you give a very simple 
> example? It does not have to be fancy!!  I have never worked with a package 
> where the legend was not automatic.
> 
> 
> 
> ----------  Forwarded Message  ----------
> 
> Subject: Re: [R] legend() with option adj=1
> Date: Wed, 21 May 2003 09:19:11 +0200
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> To: Jerome Asselin <jerome at hivnet.ubc.ca>
> Cc: r-help at stat.math.ethz.ch
> 
> Jerome Asselin wrote:
> > Hi there,
> >
> > I want to justify to right the text of my legend. Consider this short
> > reproducable example.
> >
> > x <- 1:5
> > y1 <- 1/x
> > y2 <- 2/x
> > plot(rep(x,2),c(y1,y2),type="n",xlab="x",ylab="y")
> > lines(x,y1)
> > lines(x,y2,lty=2)
> > legend(5,2,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1)
> > legend(5,1.5,c("1,000","1,000,000"),lty=1:2,xjust=1,yjust=1,adj=1)
> >
. . .
> 
> Uwe Ligges

Here is an alternative to consider.  In plots such as this I like to label the curves where they are most separated and avoid legends altogether (as well as usually avoiding the need for different line types, unless curves intertwine):

x <- 1:5
y <- 1/x
y2 <- 2/x
w <- list('1/x'=list(x=x,y=y),'2/x'=list(x=x,y=y2))
library(Hmisc) # see http://hesweb1.med.virginia.edu/biostat/s/Hmisc.html
labcurve(w, pl=TRUE, offset=.1)

Or put a legend in the most empty region of the graph:

labcurve(w, pl=TRUE, lty=c(2,1), lwd=c(1,3), col=gray(c(0,.7)), keys='lines',
         xlab=expression(chi))  
# lty only for demonstration - omit that for this example.  Thick gray scale # lines are excellent for step functions

Or use same line types but put symbols every so often (point.inc= to override default spacing; this works well for overlapping step functions also):

labcurve(w, pl=TRUE, keys=1:2)  # uses pch=1:2

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From jfox at mcmaster.ca  Thu Jun  5 17:01:17 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 05 Jun 2003 11:01:17 -0400
Subject: [R] coefficient of logistic regression
In-Reply-To: <3EDDE036.2010206@deprem.gov.tr>
References: <5.1.0.14.2.20030603184955.01ea1440@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603065307.01e9dbe0@mcmail.cis.mcmaster.ca>
	<5.1.0.14.2.20030603184955.01ea1440@mcmail.cis.mcmaster.ca>
Message-ID: <5.1.0.14.2.20030605105306.01f894b0@mcmail.cis.mcmaster.ca>

Dear can y,

At 03:04 PM 6/4/2003 +0300, orkun wrote:

[previous messages deleted]


>Dear Mr. Fox
>
>thank you very much all.
>
>Because of related to your answer. I ask you directly if you don't mind
>I studied several ways after my email.
>I wonder whether pgeo<-predict.glm(glm.ob,type="terms")
>gives same result with probability value I asked before.
>I tried on it. But it gives "Error in rep(1/n,n) %*% model.matrix(object): 
>non conformable
>arguments" .

I don't know why this doesn't work for you -- it works for me. I don't 
think that this will give you what you want, however: setting type="terms" 
produces the (centred) term-wise components of the fitted values on the 
scale of the linear predictor (i.e., the logit scale).

I think that the responses that you got previously from Tom Blackwell and 
from me answer your question.


>By the way , your teaching notes is available on the internet ?
>

They are, along with other course materials, at 
<http://www.math.yorku.ca/SCS/spida/glm/>. Unfortunately, these workshops 
were taught using SAS rather than R (not my choice).

John
-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From tobias_verbeke at skynet.be  Thu Jun  5 17:06:17 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Thu, 5 Jun 2003 17:06:17 +0200
Subject: [R] dynamics of functions
Message-ID: <20030605170617.4ec8652e.tobias_verbeke@skynet.be>

Dear list,

I would like to study the dynamics of
functions using R (instead of mathematica e.g.),
i.e. the behavior of points under iteration
of a function.
So I tried (in vain) writing a function
myfunction <- function(f,n,x){...}
in order to compute f^{n}(x), f^{n}(x) being
the function f composed with itself n-1 times.
n is a natural number, and the argument x is
the abscissa of the point I would like to
`follow'.

Can someone give me a boost?

I appreciate your help,

Tobias Verbeke



From ligges at statistik.uni-dortmund.de  Thu Jun  5 17:02:07 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 17:02:07 +0200
Subject: [R] persp(), par() and axis()
In-Reply-To: <20030605140035.GA26781@pcf004.jinr.ru>
References: <20030605140035.GA26781@pcf004.jinr.ru>
Message-ID: <3EDF5B6F.20805@statistik.uni-dortmund.de>

Timur Elzhov wrote:

> Dear R experts,
> 
> On explanation of persp() parameters the last item is:
> 
>      ...: additional graphical parameters (see `par').
> 
> However, setting the `tcl' parameter has no any effect.
> I guess that axes are added to persp() in somewhat freakish
> way, and have nothing in common with axis() function.
> 
> I found the very useful trans3d() function in persp() help
> page, and I'd like to use it with persp() + axis(), but I
> didn't find how to draw axis() in arbitrary place & direction :(
> 
> So, I want to customize axes in persp(). Could anybody help me
> solve this problem?
> Thanks a lot!


It's not that easy, since persp() is quite special and its code is "hard 
coded" in C. axis() works "only" for the 2D R graphics.

You might want to look at wireframe() in package "lattice".

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Thu Jun  5 17:05:04 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 05 Jun 2003 15:05:04 -0000
Subject: [R] counting missing values
In-Reply-To: <Pine.LNX.4.51.0306051635000.23530@artemis.imbe.med.uni-erlangen.de>
References: <OF0971AC73.BDD6322E-ONC1256D3C.004E8C0C@ges.marc.societe-generale.fr>
	<Pine.LNX.4.51.0306051635000.23530@artemis.imbe.med.uni-erlangen.de>
Message-ID: <x265nk5zi5.fsf@biostat.ku.dk>

Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:

> > Hello R lovers
> > I have written a little cute function to count the number of missing value
> > per row in a matrix and return the percentage of missing value
> >
> 
> R> M <- matrix(rnorm(25), ncol=5)
> R> diag(M) <- NA
> R> M[2,3] <- NA
> R> apply(M, 2, function(x) sum(is.na(x)))/ncol(M)
> [1] 0.2 0.2 0.4 0.2 0.2
> 
> the percentage of missing values per row...

Or:

> apply(is.na(M), 2, mean)
[1] 0.2 0.2 0.4 0.2 0.2

(Actually, that's the proportions. ...*100 for percentages)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Thu Jun  5 17:12:17 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 05 Jun 2003 11:12:17 -0400
Subject: [R] gam()
In-Reply-To: <5.2.1.1.0.20030603165357.03ee4828@pop3.norton.antivirus>
Message-ID: <5.1.0.14.2.20030605110204.01f8c800@mcmail.cis.mcmaster.ca>

Dear Henric,

At 05:01 PM 6/4/2003 +0200, Henric Nilsson wrote:

>I've now spent a couple of days trying to learn R and, in particular, the 
>gam() function, and I now have a few questions and reflections regarding 
>the latter. Maybe these things are implemented in some way that I'm not 
>yet aware of or have perhaps been decided by the R community to not be 
>what's wanted. Of course, my lack of complete theoretical understanding of 
>what mgcv really does may also show...
>
>1. When fitting models where a factor interacts with a smooth term, say 
>y~a+s(x,by=a.1)+s(x,by=a.2), I noticed that the rug in the plot of each of 
>the smooth terms is identical. I expected the rug in the plot of e.g. 
>s(x,by=a.1) to only include those x for which a.1=1 to be able to judge if 
>observations of x where a.1=1 are sparse in any region. Also, it would be 
>really if nice the "by=..." was included in the output of the plot.gam() 
>and the "Approximate significance of smooth terms:" part of the summary.gam().
>
>2. John Fox has modified anova.glm() into anova.gam() 
>(http://www.socsci.mcmaster.ca/jfox/Books/Companion/nonparametric-regression.txt) 
>for comparison of two or more fitted models based on the difference 
>between residual deviances. Indiscriminate use of such a procedure 
>shouldn't perhaps be encouraged, but I think that many users expect it to 
>be part of the mgcv package since this model selection idea is covered in 
>several texts and also implemented in S-plus (and may be OK for truly 
>nested models). And even if it's been decided that this functionality is 
>not wanted in mgcv, perhaps another function comparing several models by 
>the GCV/UBRE score and other useful statistics can be implemented?

The problem with comparing two gams in R fit with mgcv is that, by default, 
the degree of smoothing for terms is selected independently for each model. 
Simon Wood previously posted a message to the R-help list discussing this 
issue and making some suggestions. The issue doesn't arise in the same way 
with models fit by the gam function in S-PLUS because the degree of 
smoothing there is instead selected by the user. I should update my 
appendix on nonparametric regression to discuss this question -- the 
current presentation isn't really adequate.


>3. Some authors [1, 2] suggests pointwise estimation of odds ratios and 
>corresponding confidence intervals based on the smooth terms in a GAM. 
>Maybe something for mgcv?
>[1] Figueiras, A. & Cadarso-Su?rez C. (2001) "Application of Nonparametric 
>Models for calculating Odds Ratios and Their Confidence Intervals for 
>Continuous Exposures", American Journal of Epidemiology, 154(3), 264-275.
>[2] Saez, M., Cadarso-Su?rez C. & Figueiras, A. (2003) "np.OR: an S-Plus 
>function for pointwise nonparametric estimation of odds-ratios of 
>continuous predictors", Computer Methods and Programs in Biomedicine, 71, 
>175-179.
>
>4. For each purely parametric covariate a t-test is produced; I'd like to 
>have something like S-plus' anova.gam() to get an overall test. (Perhaps 
>with the addition of a choice between Type I and Type III tests, but I 
>guess that may be controversial). Is it possible?


John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ligges at statistik.uni-dortmund.de  Thu Jun  5 17:12:36 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jun 2003 17:12:36 +0200
Subject: [R] visualize regression plane
In-Reply-To: <200306051437.QAA27764@server.psico.univ.trieste.it>
References: <200306051437.QAA27764@server.psico.univ.trieste.it>
Message-ID: <3EDF5DE4.6090109@statistik.uni-dortmund.de>

Michele Grassi wrote:

> Hi,
> how can i visualize a regression plane in my 3d 
> scatterplot?

If you are using scatterplot3d(), see ?scatterplot3d, in particular its 
last example, which is explained in the references given on the same 
help page.

Uwe Ligges



From ripley at stats.ox.ac.uk  Thu Jun  5 17:14:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 16:14:18 +0100 (BST)
Subject: [R] Max sig figs as well as Min in print?
In-Reply-To: <XFMail.030605141223.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0306051607440.1215-100000@gannet.stats>

On Thu, 5 Jun 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> 
> Consider the following example (artificial, but it illustates the point):
> 
>   > r2<-sqrt(2)
>   > x<-2-r2*r2
> 
>   > print(c(pi,sqrt(pi)),digits=5)
>   [1] 3.1416 1.7725
> 
> 
>   > print(c(pi,sqrt(pi),x),digits=5)
>   [1]  3.1416e+00  1.7725e+00 -4.4409e-16
> 
> whereas I would prefer
> 
>   [1] 3.1416 1.7725 0.0000
> 
> which could be achieved by
> 
>   > print(c(pi,sqrt(pi),round(x)),digits=5)
>   [1] 3.1416 1.7725 0.0000
> 
> if need be.
> 
> The above of course is due to the definition of "digits" as the miniumum
> number of significant figures to print. In order to avoid the behaviour of
> the second "print" and make it like the last line, one would need to be
> able to set somthing like "maxdigits=5" as well as "digits=5".
> 
> Is anything of this sort possible (apart from anticipating the situation
> and wrapping relevant computations in "round")?

Might zapsmall help?

> print(zapsmall(c(pi,sqrt(pi),x)), digits=5)
[1] 3.1416 1.7725 0.0000

Alternatively, 

sprintf("%8.4f %8.4f %8.4f", pi,sqrt(pi),x)
formatC(c(pi,sqrt(pi),x), digits=4, format="f")


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tobias_verbeke at skynet.be  Thu Jun  5 16:12:03 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Thu, 5 Jun 2003 16:12:03 +0200
Subject: [R] dynamics of functions
Message-ID: <20030605161203.7d0c4b4a.tobias_verbeke@skynet.be>

Dear list,

I would like to study the dynamics of
functions using R (instead of mathematica e.g.),
i.e. the behavior of points under iteration
of a function.
So I tried (in vain) writing a function
myfunction <- function(f,n,x){...}
in order to compute f^{n}(x), f^{n}(x) being
the function f composed with itself n-1 times.
n is a natural number, and the argument x is
the abscissa of the point I would like to
`follow'.

Can someone give me a boost?

I appreciate your help,

Tobias Verbeke



From jfox at mcmaster.ca  Thu Jun  5 17:27:25 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 05 Jun 2003 11:27:25 -0400
Subject: [R] Logistic regression problem: propensity score matching
In-Reply-To: <3EDE4B87.3030801@blueyonder.co.uk>
References: <Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
	<Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
Message-ID: <5.1.0.14.2.20030605111345.01f7a8e0@mcmail.cis.mcmaster.ca>

Dear Paul,


At 08:41 PM 6/4/2003 +0100, Paul wrote:
>Thanks for your reply.
>
>I am using logistic regression because my response variable is categorical 
>- and this seems to be recommended in the literature (by Heckman, Smith 
>and others).

I think that Prof. Ripley's point here is that although one can use multnom 
in the nnet package to fit a binary (or binomial) logistic regression, it 
is more common to do so using the glm (generlized linear model) function. 
One normally would use multinomial logistic regression only for a 
polytomous (several-category) response variable. Applied to a dichotomous 
response, it will give the same results as a binary logistic regression.

>. . .
>
>I have MASS but was unable to locate logistic regression, which I was 
>advised was the standard method for my problem.

In MASS (4th edition), logit models are discussed in chapter 7 on 
generalized linear models (see, in particular, section 7.2). In my R and 
S-PLUS Companion, to which you referred in your original message, these 
models are discussed in chapter 5 on generalized linear models (see, in 
particular, section 5.2.1).

I hope that this helps,
  John

>Thanks again.
>
>Prof Brian Ripley wrote:
>
>>1) Why are you using multinom when this is not a multinomial logistic 
>>regression?  You could just use a binomial glm.
>>
>>2) The second argument to predict() is `newdata'.  `sample' is an R 
>>function, so what did you mean to have there?  I think the predictions 
>>should be a named vector if `sample' is a data frame.
>>
>>3) There are many more examples of such things (and more explanation) in 
>>Venables & Ripley's MASS (the book).
>>
>>On Wed, 4 Jun 2003, Paul Bivand wrote:
>>
>>
>>
>>>I am doing one part of an evaluation of a mandatory welfare-to-work 
>>>programme in the UK.
>>>As with all evaluations, the problem is to determine what would have 
>>>happened if the initiative had not taken place.
>>>In our case, we have a number of pilot areas and no possibility of 
>>>random assignment.
>>>Therefore we have been given control areas.
>>>My problem is to select for survey individuals in the control areas who 
>>>match as closely as possible the randomly selected sample of action area 
>>>participants.
>>>As I understand the methodology, the procedure is to run a logistic 
>>>regression to determine the odds of a case being in the sample, across 
>>>both action and control areas, and then choose for control sample the 
>>>control area individual whose odds of being in the sample are closest to 
>>>an actual sample member.
>>>
>>>So far, I have following the multinomial logistic regression example in 
>>>Fox's Companion to Applied Regression.
>>>Firstly, I would like to know if the predict() is producing odds ratios 
>>>(or probabilities) for being in the sample, which is what I am aiming for.
>>
>>You asked for `probs', so you got probabilities.
>>
>>
>>
>>>Secondly, how do I get rownames (my unique identifier) into the output 
>>>from predict() - my input may be faulty somehow and the wrong rownames 
>>>being picked up - as I need to export back to database to sort and match 
>>>in names, addresses and phone numbers for my selected samples.
>>>
>>>My code is as follows:
>>>londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
>>>rownames=ORCID)
>>>attach(londonpsm)
>>>mod.multinom <- multinom(sample ~ AGE + DISABLED + GENDER + ETHCODE + 
>>>NDYPTOT + NDLTUTOT + LOPTYPE)
>>>lonoutput <- predict(mod.multinom, sample, type='probs')
>>>london2 <- data.frame(lonoutput)
>>>
>>>The Logistic regression seems to work, although summary() says the it is 
>>>not a matrix.
>>>
>>
>>what is `it'?
>>
>>
>>
>>>The output looks like odds ratios, but I would like to know whether this 
>>>is so.
>>>
>>
>>No.
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From th50 at leicester.ac.uk  Thu Jun  5 17:33:42 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 5 Jun 2003 16:33:42 +0100
Subject: [R] dynamics of functions
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E4692@saffron.cfs.le.ac.uk>

Dear Tobias,

> I would like to study the dynamics of
> functions using R (instead of mathematica e.g.),
> i.e. the behavior of points under iteration
> of a function.
> So I tried (in vain) writing a function
> myfunction <- function(f,n,x){...}
> in order to compute f^{n}(x), f^{n}(x) being
> the function f composed with itself n-1 times.
> n is a natural number, and the argument x is
> the abscissa of the point I would like to
> `follow'.
 
What about the following function?

iterate<-function(f,n,x){
  if(n==0) return(x)
  y<-x
  for(i in 1:n)y<-f(y)
  y
}
iterate(sqrt,3,256)

Hope that helps.

Best wishes

Thomas


---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From B.Rowlingson at lancaster.ac.uk  Thu Jun  5 17:57:46 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 05 Jun 2003 16:57:46 +0100
Subject: [R] dynamics of functions
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF71486015E4692@saffron.cfs.le.ac.uk>
References: <1F2CE8D4B0195E488213E8B8CCF71486015E4692@saffron.cfs.le.ac.uk>
Message-ID: <3EDF687A.5010600@lancaster.ac.uk>

Hotz, T. wrote:

> What about the following function?
> 
> iterate<-function(f,n,x){
>   if(n==0) return(x)
>   y<-x
>   for(i in 1:n)y<-f(y)
>   y
> }
> iterate(sqrt,3,256)
> 

  Or the following:

iterFun <-
   function(f,n,x,...){
     if(n==0){return(x)}
     for(i in 1:n){
       x<-f(x,...)
     }
     return(x)
   }

  If you want to construct an arbitrary function without naming it, you 
can do something like this:

 > iterFun(function(z){3.9*z*(1-z)},121,.6)
[1] 0.6449397

  And the ... part means you can try different constants in your function:

 > iterFun(function(z,a){a*z*(1-z)},121,.6,a=3.9)
[1] 0.6449397
 > iterFun(function(z,a){a*z*(1-z)},121,.6,a=3.901)
[1] 0.09416016
 > iterFun(function(z,a){a*z*(1-z)},121,.6,a=3.900001)
[1] 0.7612195

  Fun with functions.

Baz



From tlumley at u.washington.edu  Thu Jun  5 17:57:49 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Jun 2003 08:57:49 -0700 (PDT)
Subject: [R] counting missing values
In-Reply-To: <3EDF5547.2010709@statistik.uni-dortmund.de>
Message-ID: <Pine.A41.4.44.0306050855520.224412-100000@homer03.u.washington.edu>

On Thu, 5 Jun 2003, Uwe Ligges wrote:

> vincent.stoliaroff at socgen.com wrote:
>
> > Hello R lovers
> > I have written a little cute function to count the number of missing value
> > per row in a matrix and return the percentage of missing value
> >
> > it takes a lot of time to run with a 1000 rows matrix
> >
> > I'd like to know if there is a function already implemented to count the
> > number of occurence of a given values in a vector
> >
> >
> > For information,
> > here is the function
> > count<-0
> > for (i in 1:nrow(Matrix))
> >       {
> >       for (j in 1:ncol(Matrix)) {if (is.na(Matrix[i,j])) count<-count+1}
> >       Result[i,1]<-((count/(ncol(Matrix)))*100);
> >       count<-0
> >       }
> > Result
> >
> > thanks for any help
> > Vincent
>
> Well, it's pretty easy to do it:
>
> apply(Matrix, 1, function(x) sum(is.na(x))) / ncol(Matrix) * 100
>

and

   rowMeans(is.na(Matrix))*100

should be faster

	-thomas



From tobias_verbeke at skynet.be  Thu Jun  5 18:04:23 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Thu, 5 Jun 2003 18:04:23 +0200
Subject: [R] dynamics of functions
Message-ID: <20030605180423.19bc5b62.tobias_verbeke@skynet.be>

Dear Thomas,  

> What about the following function?
> 
> iterate<-function(f,n,x){
>   if(n==0) return(x)
>   y<-x
>   for(i in 1:n)y<-f(y)
>   y
> }
> iterate(sqrt,3,256)

Thank you very much, this certainly helps.
I'm still curious, though, to know how to
write the expression of my function 
immediately as the argument f.
I can define it outside of the function
> aap <- function(x) -x^3
and use it as argument
> iterate(aap,3,256),
but I seem not to be clever enough
to write a function that receives
the following as input

> iterate(-x^3,3,256)

Thanks again,

Tobias



From rvaradha at jhsph.edu  Thu Jun  5 18:01:56 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 05 Jun 2003 12:01:56 -0400
Subject: [R] na.action in model.frame
Message-ID: <2371c22294.222942371c@jhsph.edu>

Dear Group:

I am trying to create a model frame from a formula for the model as 
follows:

formula <- y ~ x1 + x2 + x3
X <- model.frame(form=formula,data=mydata)

I have some missing values in some the variables, but I want them to be 
included in my model frame and to be indicated as "NA". Is there a way 
to do this?

thanks,
Ravi.



From ripley at stats.ox.ac.uk  Thu Jun  5 18:15:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 17:15:52 +0100 (BST)
Subject: [R] dynamics of functions
In-Reply-To: <20030605180423.19bc5b62.tobias_verbeke@skynet.be>
Message-ID: <Pine.LNX.4.44.0306051710300.1406-100000@gannet.stats>

On Thu, 5 Jun 2003, Tobias Verbeke wrote:

...

> > What about the following function?
> > 
> > iterate<-function(f,n,x){
> >   if(n==0) return(x)
> >   y<-x
> >   for(i in 1:n)y<-f(y)
> >   y
> > }
> > iterate(sqrt,3,256)
> 
> Thank you very much, this certainly helps.
> I'm still curious, though, to know how to
> write the expression of my function 
> immediately as the argument f.
> I can define it outside of the function
> > aap <- function(x) -x^3
> and use it as argument
> > iterate(aap,3,256),
> but I seem not to be clever enough
> to write a function that receives
> the following as input
> 
> > iterate(-x^3,3,256)

That's an expression, not a function, and you would need some convention 
that it is x that should be varied.  You can do that: see curve() for 
example.  Or just use

iterate(function(x) -x^3, 3, 256)

Someone seems very adverse to using the spacebar!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rvaradha at jhsph.edu  Thu Jun  5 18:32:46 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 05 Jun 2003 12:32:46 -0400
Subject: [R] dynamics of functions
Message-ID: <404158f0.58f04041@jhsph.edu>


Here is a recursive function.

iterate <- function(f, n, x){
 if(n==0) return(x)
 iterate(f, n-1, f(x))
}

iterate(function(x) 1/(1+x), 10, 1)

Best,
Ravi.

----- Original Message -----
From: Tobias Verbeke <tobias_verbeke at skynet.be>
Date: Thursday, June 5, 2003 12:04 pm
Subject: Re: [R] dynamics of functions

> Dear Thomas,  
> 
> > What about the following function?
> > 
> > iterate<-function(f,n,x){
> >   if(n==0) return(x)
> >   y<-x
> >   for(i in 1:n)y<-f(y)
> >   y
> > }
> > iterate(sqrt,3,256)
> 
> Thank you very much, this certainly helps.
> I'm still curious, though, to know how to
> write the expression of my function 
> immediately as the argument f.
> I can define it outside of the function
> > aap <- function(x) -x^3
> and use it as argument
> > iterate(aap,3,256),
> but I seem not to be clever enough
> to write a function that receives
> the following as input
> 
> > iterate(-x^3,3,256)
> 
> Thanks again,
> 
> Tobias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tlumley at u.washington.edu  Thu Jun  5 18:45:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Jun 2003 09:45:41 -0700 (PDT)
Subject: [R] dynamics of functions
In-Reply-To: <20030605180423.19bc5b62.tobias_verbeke@skynet.be>
Message-ID: <Pine.A41.4.44.0306050915020.224412-100000@homer03.u.washington.edu>

On Thu, 5 Jun 2003, Tobias Verbeke wrote:

> Dear Thomas,
>
> > What about the following function?
> >
> > iterate<-function(f,n,x){
> >   if(n==0) return(x)
> >   y<-x
> >   for(i in 1:n)y<-f(y)
> >   y
> > }
> > iterate(sqrt,3,256)
>
> Thank you very much, this certainly helps.
> I'm still curious, though, to know how to
> write the expression of my function
> immediately as the argument f.
> I can define it outside of the function
> > aap <- function(x) -x^3
> and use it as argument
> > iterate(aap,3,256),
> but I seem not to be clever enough
> to write a function that receives
> the following as input
>
> > iterate(-x^3,3,256)
>

That's because -x^3 isn't a function. It's an expression.

You want
   iterate(function(x) -x^3, 3, 256)



This might also be a good time to point out that this problem is one where
storing past values helps a lot.

The function below returns a function that iterates for a particular f and
x.  It stores its past results, so if you ask for the same n again you get
it immediately and if you ask for an n a little bigger than a past one it
only has to do the remaining steps

You would do something like


ff<-iterator(function(z) 4*z*(1-z), x=0.3)

and then ff(10) gives the tenth iterate or
sapply(10*(1:100),ff)
gives the tenth, 20th, 30th,.. 1000th iterate.

	-thomas


iterator<-function(f,x){

    memo<-new.env()

    function(n){
        if (n==1)
            return(f(x))
        v<-paste("n",n,sep="")
        if (exists(v,envir=memo))
            return(get(v,envir=memo))
        else{
            rval<-f(Recall(n-1))
            assign(v,rval,envir=memo)
        }
    }


}



From marc.fohr at first-private.de  Thu Jun  5 18:46:20 2003
From: marc.fohr at first-private.de (Fohr, Marc [AM])
Date: Thu, 5 Jun 2003 17:46:20 +0100 
Subject: [R] (no subject)
Message-ID: <05D9367E7F42D711A0D40002A5F3BE8601276C6B@exchuk08.eur.nsroot.net>

Hello,

I am interested in R as an alternative for a statistical tool at our firm. I
do know RATS an SPSS but not S+. As I read that R is close to S+, I would
like to know if you could recommend me any books as an introduction to S+ or
R.

Best regards

Marc

----------------------------------------------------------------------------
-
Marc Fohr, CFA
Equity Portfolio Manager
First Private Investment Management
Neue Mainzer Strasse 75
D-60311 Frankfurt/Main
Phone: ++49 - 69 - 2607 5424
Fax: ++49 - 69 - 2607 5440
Email: marc.fohr at first-private.de
----------------------------------------------------------------------------
-



From jthomas at kodos.fastmail.fm  Thu Jun  5 18:52:42 2003
From: jthomas at kodos.fastmail.fm (James D Thomas)
Date: Thu, 05 Jun 2003 12:52:42 -0400
Subject: [R] Survival analysis and generalized gamma?
Message-ID: <20030605165242.0E5C26ACF5@smtp.us2.messagingengine.com>

Is there an easy way to integrate the generalized gamma distribution into
the current R survival package?
--james



From henric.nilsson at statisticon.se  Thu Jun  5 18:57:43 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 05 Jun 2003 18:57:43 +0200
Subject: [R] gam()
In-Reply-To: <5.1.0.14.2.20030605110204.01f8c800@mcmail.cis.mcmaster.ca>
References: <5.2.1.1.0.20030603165357.03ee4828@pop3.norton.antivirus>
Message-ID: <5.2.1.1.0.20030605174124.03417e80@pop3.norton.antivirus>

At 11:12 2003-06-05 -0400, John Fox wrote:

>>2. John Fox has modified anova.glm() into anova.gam() 
>>(http://www.socsci.mcmaster.ca/jfox/Books/Companion/nonparametric-regression.txt) 
>>for comparison of two or more fitted models based on the difference 
>>between residual deviances. Indiscriminate use of such a procedure 
>>shouldn't perhaps be encouraged, but I think that many users expect it to 
>>be part of the mgcv package since this model selection idea is covered in 
>>several texts and also implemented in S-plus (and may be OK for truly 
>>nested models). And even if it's been decided that this functionality is 
>>not wanted in mgcv, perhaps another function comparing several models by 
>>the GCV/UBRE score and other useful statistics can be implemented?
>
>The problem with comparing two gams in R fit with mgcv is that, by 
>default, the degree of smoothing for terms is selected independently for 
>each model. Simon Wood previously posted a message to the R-help list 
>discussing this issue and making some suggestions. The issue doesn't arise 
>in the same way with models fit by the gam function in S-PLUS because the 
>degree of smoothing there is instead selected by the user. I should update 
>my appendix on nonparametric regression to discuss this question -- the 
>current presentation isn't really adequate.

I'm aware of this difference between gam() in R and S-Plus, which is why I 
proposed a function listing relevant statistics for every fitted model so 
the analyst can use these to judge, without hypothesis testing, which model 
to prefer. Still, for models where the analyst has made sure that the 
models are truly nested, the use of your anova.gam can be justified by the 
simulation results reported by Hastie & Tibshirani (1990, p. 155); maybe I 
just want it for purely nostalgic reasons?! ;-)

Admittedly, I like the more attractive way of chosing the degrees of 
freedom that mgcv provides. However, I must admit that since most text 
books covering GAMs are more or less Splus based, and the possibilities 
that mgcv offers are so vast, I'm feeling a bit lost at times; it's great 
to have to new more flexible tools, but on the downside that means more 
choices to be made. So, anyone got any essential literature tips? I've read 
(and re-read, and read again) Simon Wood's articles in JRSS, R News and 
Ecological Modelling, and, of course, the mgcv manual.

//Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From ltorgo at liacc.up.pt  Thu Jun  5 20:04:35 2003
From: ltorgo at liacc.up.pt (Luis Torgo)
Date: Thu, 5 Jun 2003 18:04:35 +0000
Subject: [R] kmeans (again)
Message-ID: <200306051804.35735.ltorgo@liacc.up.pt>

Regarding a previous question concerning the kmeans function I've tried the 
same example and I also get a strange result (at least according to what is 
said in the help of the function kmeans). Apparently, the function is 
disregarding the initial cluster centers one gives it. According to the help 
of the function:

 centers: Either the number of clusters or a set of initial cluster
          centers...

Now a small dataset:
> data<-matrix(c(-1,0,2,2.5,7,9,0,3,0,6,1,4),6,2)

If I use rows 3 and 4 as cluster centers and a single iteration of kmeans I 
get:
> kmeans(data,data[c(3,4),],1)
$cluster
[1] 1 1 1 1 2 2

$centers
   [,1] [,2]
1 0.875 2.25
2 8.000 2.50

$withinss
[1] 32.9375  6.5000

$size
[1] 4 2

If I now use rows 1 and 6 as cluster centers I get exactly the same solution 
after the first iteration:

> kmeans(data,data[c(1,6),],1)
$cluster
[1] 1 1 1 1 2 2

$centers
   [,1] [,2]
1 0.875 2.25
2 8.000 2.50

$withinss
[1] 32.9375  6.5000

$size
[1] 4 2

So, apparently the function is disregarding the initial cluster centers 
information. This is even "confirmed" by the fact that if I use the function 
without cluster centers, simply given the number of clusters, I get the same 
solution:
> kmeans(data,2,1)
$cluster
[1] 2 2 2 2 1 1

$centers
   [,1] [,2]
1 8.000 2.50
2 0.875 2.25

$withinss
[1]  6.5000 32.9375

$size
[1] 2 4



-- 
Luis Torgo
    FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
    Machine Learning Group           Fax   : (+351) 22 600 36 54
    R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
    4150 PORTO   -  PORTUGAL         WWW   : http://www.liacc.up.pt/~ltorgo



From spencer.graves at pdf.com  Thu Jun  5 19:12:06 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 Jun 2003 10:12:06 -0700
Subject: R References {was: [R] (no subject)}
References: <05D9367E7F42D711A0D40002A5F3BE8601276C6B@exchuk08.eur.nsroot.net>
Message-ID: <3EDF79E6.7060101@pdf.com>

	  Apart from the wealth of material on "http://www.r-project.org/", my 
favorite book on R is Modern Applied Statistics with S, 4th ed., by 
Venables and Ripley.

hope this helps.  spencer graves	

Fohr, Marc [AM] wrote:
> Hello,
> 
> I am interested in R as an alternative for a statistical tool at our firm. I
> do know RATS an SPSS but not S+. As I read that R is close to S+, I would
> like to know if you could recommend me any books as an introduction to S+ or
> R.
> 
> Best regards
> 
> Marc
> 
> ----------------------------------------------------------------------------
> -
> Marc Fohr, CFA
> Equity Portfolio Manager
> First Private Investment Management
> Neue Mainzer Strasse 75
> D-60311 Frankfurt/Main
> Phone: ++49 - 69 - 2607 5424
> Fax: ++49 - 69 - 2607 5440
> Email: marc.fohr at first-private.de
> ----------------------------------------------------------------------------
> -
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mschwartz at medanalytics.com  Thu Jun  5 19:18:32 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 5 Jun 2003 12:18:32 -0500
Subject: [R] Introductory Resources
In-Reply-To: <05D9367E7F42D711A0D40002A5F3BE8601276C6B@exchuk08.eur.nsroot.net>
Message-ID: <002e01c32b86$7e069b00$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fohr, Marc
[AM]
>Sent: Thursday, June 05, 2003 11:46 AM
>To: 'R-help at lists.R-project.org'
>Subject: [R] (no subject)
>
>
>Hello,
>
>I am interested in R as an alternative for a statistical tool 
>at our firm. I
>do know RATS an SPSS but not S+. As I read that R is close to 
>S+, I would
>like to know if you could recommend me any books as an 
>introduction to S+ or
>R.
>
>Best regards
>
>Marc


Marc,

Reviewing R FAQs 2.7 and 3.x on the main R site would be a good place
to start. The former lists books and other documents (some online)
that serve as excellent introductions, while the latter helps to
differentiate R and S/S+.

Spending some time with those references and the R FAQs will serve as
a good foundation, with keyword searches of the R-help list archive
serving as an additional strong resource.

HTH,

Another Marc



From deepayan at stat.wisc.edu  Thu Jun  5 19:26:21 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 5 Jun 2003 12:26:21 -0500
Subject: [R] scales in xyplot doesn't seem to work for x axis
In-Reply-To: <1054813269.1242.15.camel@gandalf.ipimar.pt>
References: <1054813269.1242.15.camel@gandalf.ipimar.pt>
Message-ID: <200306051226.21999.deepayan@stat.wisc.edu>

On Thursday 05 June 2003 06:41, Ernesto Jardim wrote:
> Hi
>
> I'm doing a xyplot and I wand to reduce the number of tick marks in the
> x axis. My x axis are month and I want to reduce the 12 tick marks to 4.
> I used the scales argument but it doesn't seem to work, althougth it
> works on y axis if I use scales=list(tick.number=4).

By 'month', so you mean a POSIXct object ? lattice is not very good with 
those.

The other possibility is that your mes is a factor (or a character vector, 
which is essentially the same). In that case, tick.number will have no effect 
(it works only for numeric). You could do something like

xlim = c('Jan', '', '', 'Apr', '', '', 'Jul', '', '', 'Oct', '', '')


> xyplot(land~mes|porto+arte,data=hom.land.mpa[hom.land.mpa$arte!="pseine",],
>type="l",scales=list(x=list(tick.number=4)))
>
> Regards
>
> EJ



From deepayan at stat.wisc.edu  Thu Jun  5 19:32:44 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 5 Jun 2003 12:32:44 -0500
Subject: [R] Error when creating layouts with partly filled pages within
	lattice
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF714860161B656@saffron.cfs.le.ac.uk>
References: <1F2CE8D4B0195E488213E8B8CCF714860161B656@saffron.cfs.le.ac.uk>
Message-ID: <200306051232.44487.deepayan@stat.wisc.edu>


Could you try with the latest lattice (0.7-13) ? I don't see an error with it. 
It's not immediately obvious to me where the problem could have come from, 
but there were a bunch of similar problems fixed recently.

On Thursday 05 June 2003 08:21, Hotz, T. wrote:
> Dear all,
>
> Please take my apologies if that has already been asked
>  - at least I couldn't find it in the archives.
>
> When trying to specify a layout within library lattice,
> i.e. using xyplot, I get an error when the prepanel
> function tries to subscript the automatically generated
> x.limits. This seems to appear if the last page wouldn't
> be filled completely, i.e. there would be space left
> for more panels. Please see session snippet for a simple
> example.
>
> Am I missing something? Any hints highly appreciated.
>
> Cheers
>
> Thomas



From Kosenkov.Kirill at nac.spb.ru  Thu Jun  5 19:36:45 2003
From: Kosenkov.Kirill at nac.spb.ru (Kosenkov Kirill)
Date: Thu, 05 Jun 2003 21:36:45 +0400
Subject: [R] question about POSIXct conversion
Message-ID: <3EDF7FAD.3040706@nac.spb.ru>

Hello!

I am trying to compute minimal time on some data like this:
mt<-tapply(mrsh$time1,list(mrsh$var1,mrsh$var2),min):

	      a 		b
145         1054800600         1054789800
340         1054804500         1054794600
349         1054820400         1054792800
55          1054800600         1054789200
57          1054814100         1054791000
78          1054822200         1054790400
843         1054807200         1054795800
864         1054813800         1054790700
92          1054789500         1054790100
940         1054800600         1054795800
971         1054783800         1054796700

where time1 is POSIXct object.

str(mt) tells me, that mt has mode 'numeric' NOT POSIXct

When i am trying to set: mode(mt)<-'POSIXct' i get a message:

Don't know how to convert `structure(c(1054800600, 1054804500, ' 
to class "POSIXct"

When i am trying to do: mode(mt)<-'character' everything is ok.

I've tried: class(mt)<-'POSIXct' but matrix structure seems to be 
lost.

How to convert the numeric result of tapply on POSIXct object 
(with 'min') to POSIXct again???

Thanks!



From tobias_verbeke at skynet.be  Thu Jun  5 20:06:04 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Thu, 5 Jun 2003 20:06:04 +0200
Subject: [R] dynamics of functions
In-Reply-To: <404158f0.58f04041@jhsph.edu>
References: <404158f0.58f04041@jhsph.edu>
Message-ID: <20030605200604.618c9389.tobias_verbeke@skynet.be>

Thanks to you all for the
clarifications.
I will work on my spacebar usage ;-)

Regards,

Tobias



From ripley at stats.ox.ac.uk  Thu Jun  5 20:03:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 19:03:55 +0100 (BST)
Subject: [R] question about POSIXct conversion
In-Reply-To: <3EDF7FAD.3040706@nac.spb.ru>
Message-ID: <Pine.LNX.4.44.0306051859390.1776-100000@gannet.stats>

"character" is a mode

"POSIXct" is a class, but you want c("POSIXt", "POSIXct")

Try 

mt <- structure(mt, class=c("POSIXt", "POSIXct"))

or

mt + ISOdatetime(1970,1,1,0,0,0)

or several other such tricks.

On Thu, 5 Jun 2003, Kosenkov Kirill wrote:

> Hello!
> 
> I am trying to compute minimal time on some data like this:
> mt<-tapply(mrsh$time1,list(mrsh$var1,mrsh$var2),min):
> 
> 	      a 		b
> 145         1054800600         1054789800
> 340         1054804500         1054794600
> 349         1054820400         1054792800
> 55          1054800600         1054789200
> 57          1054814100         1054791000
> 78          1054822200         1054790400
> 843         1054807200         1054795800
> 864         1054813800         1054790700
> 92          1054789500         1054790100
> 940         1054800600         1054795800
> 971         1054783800         1054796700
> 
> where time1 is POSIXct object.
> 
> str(mt) tells me, that mt has mode 'numeric' NOT POSIXct
> 
> When i am trying to set: mode(mt)<-'POSIXct' i get a message:
> 
> Don't know how to convert `structure(c(1054800600, 1054804500, ' 
> to class "POSIXct"
> 
> When i am trying to do: mode(mt)<-'character' everything is ok.
> 
> I've tried: class(mt)<-'POSIXct' but matrix structure seems to be 
> lost.
> 
> How to convert the numeric result of tapply on POSIXct object 
> (with 'min') to POSIXct again???
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at stat.ucla.edu  Thu Jun  5 20:10:48 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Thu, 05 Jun 2003 11:10:48 -0700
Subject: R References {was: [R] (no subject)}
In-Reply-To: <3EDF79E6.7060101@pdf.com>
References: <05D9367E7F42D711A0D40002A5F3BE8601276C6B@exchuk08.eur.nsroot.net>
	<3EDF79E6.7060101@pdf.com>
Message-ID: <3EDF87A8.6000208@stat.ucla.edu>

You might also consider S Programming, by Venables & Ripley.

-roger

Spencer Graves wrote:
>       Apart from the wealth of material on "http://www.r-project.org/", 
> my favorite book on R is Modern Applied Statistics with S, 4th ed., by 
> Venables and Ripley.
> 
> hope this helps.  spencer graves   
> 
> Fohr, Marc [AM] wrote:
> 
>> Hello,
>>
>> I am interested in R as an alternative for a statistical tool at our 
>> firm. I
>> do know RATS an SPSS but not S+. As I read that R is close to S+, I would
>> like to know if you could recommend me any books as an introduction to 
>> S+ or
>> R.
>>
>> Best regards
>>
>> Marc
>>
>> ---------------------------------------------------------------------------- 
>>
>> -
>> Marc Fohr, CFA
>> Equity Portfolio Manager
>> First Private Investment Management
>> Neue Mainzer Strasse 75
>> D-60311 Frankfurt/Main
>> Phone: ++49 - 69 - 2607 5424
>> Fax: ++49 - 69 - 2607 5440
>> Email: marc.fohr at first-private.de
>> ---------------------------------------------------------------------------- 
>>
>> -
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From Kosenkov.Kirill at nac.spb.ru  Thu Jun  5 20:37:14 2003
From: Kosenkov.Kirill at nac.spb.ru (Kosenkov Kirill)
Date: Thu, 05 Jun 2003 22:37:14 +0400
Subject: [R] question about POSIXct conversion
In-Reply-To: <Pine.LNX.4.44.0306051859390.1776-100000@gannet.stats>
References: <Pine.LNX.4.44.0306051859390.1776-100000@gannet.stats>
Message-ID: <3EDF8DDA.8040803@nac.spb.ru>

Thanks for reply, but when i am trying to

 > mt <- structure(mt, class=c("POSIXt", "POSIXct"))
 >
 > or
 >
 > mt + ISOdatetime(1970,1,1,0,0,0)

it seems that i loosing structre of matrix:

str(structure(mt, class=c("POSIXt", "POSIXct")))

 > `POSIXct', format: chr [1:44] "2003-06-05 12:10:00"...

instead of:
  num [1:11, 1:4] 1.05e

Both tricks coerces my matrix in "long" vector :((

Any other suggestions?

Prof Brian Ripley wrote:

> "character" is a mode
> 
> "POSIXct" is a class, but you want c("POSIXt", "POSIXct")
> 
> Try 
> 
> mt <- structure(mt, class=c("POSIXt", "POSIXct"))
> 
> or
> 
> mt + ISOdatetime(1970,1,1,0,0,0)
> 
> or several other such tricks.
> 
> On Thu, 5 Jun 2003, Kosenkov Kirill wrote:
> 
> 
>>Hello!
>>
>>I am trying to compute minimal time on some data like this:
>>mt<-tapply(mrsh$time1,list(mrsh$var1,mrsh$var2),min):
>>
>>	      a 		b
>>145         1054800600         1054789800
>>340         1054804500         1054794600
>>349         1054820400         1054792800
>>55          1054800600         1054789200
>>57          1054814100         1054791000
>>78          1054822200         1054790400
>>843         1054807200         1054795800
>>864         1054813800         1054790700
>>92          1054789500         1054790100
>>940         1054800600         1054795800
>>971         1054783800         1054796700
>>
>>where time1 is POSIXct object.
>>
>>str(mt) tells me, that mt has mode 'numeric' NOT POSIXct
>>
>>When i am trying to set: mode(mt)<-'POSIXct' i get a message:
>>
>>Don't know how to convert `structure(c(1054800600, 1054804500, ' 
>>to class "POSIXct"
>>
>>When i am trying to do: mode(mt)<-'character' everything is ok.
>>
>>I've tried: class(mt)<-'POSIXct' but matrix structure seems to be 
>>lost.
>>
>>How to convert the numeric result of tapply on POSIXct object 
>>(with 'min') to POSIXct again???
>>
>>Thanks!
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From henric.nilsson at statisticon.se  Thu Jun  5 20:45:24 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 05 Jun 2003 20:45:24 +0200
Subject: [R] partial residuals in plot.gam()
Message-ID: <5.2.1.1.0.20030605185915.03efe258@pop3.norton.antivirus>

All,

Sorry for bombarding you with GAM related questions, but...

I know a partial residual option in plot.gam() is on Simon Wood's todo 
list, but since I'm in the midst of a project and not yet having acquired 
sufficient R knowledge to code something usable myself I'll have to put my 
trust in you. Anybody got some code lying around for doing this? Or if 
someone can supply me with enough hints for doing it myself, I'd be most 
grateful.

Thanks,
Henric



From pauljohn at ku.edu  Thu Jun  5 22:04:44 2003
From: pauljohn at ku.edu (Paul E Johnson)
Date: Thu, 05 Jun 2003 15:04:44 -0500
Subject: [R] dev.copy2eps: Why did the colors come into my postscript output?
Message-ID: <3EDFA25C.603@ku.edu>

On a RedHat 7.3 system with R-1.6.1, I did this

 > x11(width=3.5,height=4,colortype="gray")

Then plotted (with matplot) a nice looking no-color graph on the screen, 
then I did this:

 > dev.copy2eps(file="test.eps",height=4,width=3.5)

I was surprised that the output in the eps file included the colored 
lines from the plot, even though the screen device was set to "gray" and 
on screen I did not see colors.

I put some of the eps files up here, so you can see what I mean:

http://lark.cc.ku.edu/~pauljohn/R/epsfiles

And if you don't have an eps viewer handy, I made png versions, since 
all browsers seem to show them.

http://lark.cc.ku.edu/~pauljohn/R/pngfiles/

The docs indicate that the x11 device is being copied, but I rather 
think that a plot object that exists "behind" the x11 device is being 
saved.?

I have since learned that I can put col=c("black") into the matplot 
command to make sure no colors are used, and that works the way I 
expect.  But I was curious about this behavior in dev.copy2eps.

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From ripley at stats.ox.ac.uk  Thu Jun  5 22:59:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jun 2003 21:59:40 +0100 (BST)
Subject: [R] dev.copy2eps: Why did the colors come into my postscript
	output?
In-Reply-To: <3EDFA25C.603@ku.edu>
Message-ID: <Pine.LNX.4.44.0306052146030.2146-100000@gannet.stats>

On Thu, 5 Jun 2003, Paul E Johnson wrote:

> On a RedHat 7.3 system with R-1.6.1, I did this
> 
>  > x11(width=3.5,height=4,colortype="gray")
> 
> Then plotted (with matplot) a nice looking no-color graph on the screen, 
> then I did this:
> 
>  > dev.copy2eps(file="test.eps",height=4,width=3.5)
> 
> I was surprised that the output in the eps file included the colored 
> lines from the plot, even though the screen device was set to "gray" and 
> on screen I did not see colors.

[...]

> The docs indicate that the x11 device is being copied, but I rather 
> think that a plot object that exists "behind" the x11 device is being 
> saved.?

You did a coloured plot and displayed it on grey-scale device. The
plotting internals has no idea (nor does it care) how the plot was
rendered.  (Nor does it know if you have a colour screen or not.)  
dev.copy() just replays the plot on a new device: a postscript device if
the dev.copy2eps wrapper is used.

You only see the colours in the eps file if you display that on a colour 
device, BTW, and most postscript renders are not.

> I have since learned that I can put col=c("black") into the matplot 
> command to make sure no colors are used, and that works the way I 
> expect.  But I was curious about this behavior in dev.copy2eps.

No mystery: it just works as it is documented.  Please read the nuances of
the help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Jun  5 23:54:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 Jun 2003 14:54:31 -0700
Subject: [R] converting "by" to a data.frame? 
Message-ID: <3EDFBC17.7060404@pdf.com>

Dear R-Help:

	  I want to (a) subset a data.frame by several columns, (b) fit a model 
to each subset, and (c) store a vector of results from the fit in the 
columns of a data.frame.  In the past, I've used "for" loops do do this. 
  Is there a way to use "by"?

	  Consider the following example:

 > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
+  function(data.)coef(lm(y~x, data.)))
 > byFits
A: A1
B: B1
   (Intercept)             x
  3.333333e-01 -1.517960e-16
------------------------------------------------------------
A: A2
B: B1
NULL
------------------------------------------------------------
A: A1
B: B2
NULL
------------------------------------------------------------
A: A2
B: B2
  (Intercept)            x
6.666667e-01 3.282015e-16
 >
 >
#############################
Desired output:

data.frame(A=c("A1","A2"), B=c("B1", "B2"),
	.Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))

What's the simplest way to do this?
Thanks,
Spencer Graves



From tlumley at u.washington.edu  Fri Jun  6 00:21:03 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Jun 2003 15:21:03 -0700 (PDT)
Subject: [R] Survival analysis and generalized gamma?
In-Reply-To: <20030605165242.0E5C26ACF5@smtp.us2.messagingengine.com>
Message-ID: <Pine.A41.4.44.0306051520060.211048-100000@homer04.u.washington.edu>

On Thu, 5 Jun 2003, James D Thomas wrote:

> Is there an easy way to integrate the generalized gamma distribution into
> the current R survival package?

I'm not entirely sure what the generalized gamma distribution is, but it
is fairly easy to add to survreg() other parametric models where
covariates act linearly on either survival or log survival.

	-thomas



From tlumley at u.washington.edu  Fri Jun  6 00:23:59 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Jun 2003 15:23:59 -0700 (PDT)
Subject: [R] converting "by" to a data.frame? 
In-Reply-To: <3EDFBC17.7060404@pdf.com>
Message-ID: <Pine.A41.4.44.0306051523200.211048-100000@homer04.u.washington.edu>

On Thu, 5 Jun 2003, Spencer Graves wrote:

> Dear R-Help:
>
> 	  I want to (a) subset a data.frame by several columns, (b) fit a model
> to each subset, and (c) store a vector of results from the fit in the
> columns of a data.frame.  In the past, I've used "for" loops do do this.
>   Is there a way to use "by"?
>
> 	  Consider the following example:
>
>  > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
> +  function(data.)coef(lm(y~x, data.)))
>  > byFits
> A: A1
> B: B1
>    (Intercept)             x
>   3.333333e-01 -1.517960e-16
> ------------------------------------------------------------
> A: A2
> B: B1
> NULL
> ------------------------------------------------------------
> A: A1
> B: B2
> NULL
> ------------------------------------------------------------
> A: A2
> B: B2
>   (Intercept)            x
> 6.666667e-01 3.282015e-16
>  >
>  >
> #############################
> Desired output:
>
> data.frame(A=c("A1","A2"), B=c("B1", "B2"),
> 	.Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
>
> What's the simplest way to do this?

do.call("rbind", byFits)


	-thomas



From spencer.graves at pdf.com  Fri Jun  6 00:49:07 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 Jun 2003 15:49:07 -0700
Subject: [R] converting "by" to a data.frame?
References: <Pine.A41.4.44.0306051523200.211048-100000@homer04.u.washington.edu>
Message-ID: <3EDFC8E3.7020900@pdf.com>

Hi, Thomas, et al.:

Thanks for the reply.  Unfortunately, "do.call" strips off the subset 
identifiers, which I want to use for further modeling:

 > do.call("rbind", byFits)
      (Intercept)              x
[1,]   0.3333333 -1.517960e-016
[2,]   0.6666667  3.282015e-016

The following does what I want using a "for" loop:

 > by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
+  B=rep(c("B1", "B2"), each=3), x=1:6, y=rep(0:1, length=6))
 > by.lvls <- paste(as.character(by.df$A), as.character(by.df$B), sep=":")
 > A.B <- unique(by.lvls)
 > Fits <- data.frame(A.B = A.B, .Intercept.=rep(NA, length(A.B)),
+  x=rep(NA, length(A.B)))
 > Fits$A <- substring(A.B, 1, regexpr(":", A.B)-1)
 > Fits$B <- substring(A.B, regexpr(":", A.B)+1)
 > for(i in 1:length(A.B))
+  Fits[i, 2:3] <- coef(lm(y~x, by.df[by.lvls==A.B[i],]))
 > Fits
     A.B X.Intercept.             x  A  B
1 A1:B1    0.3333333 -1.517960e-16 A1 B1
2 A2:B2    0.6666667  3.282015e-16 A2 B2
 >

	  I wondered if there was something easier.

Thanks again for your reply.
Spencer Graves

Thomas Lumley wrote:
> On Thu, 5 Jun 2003, Spencer Graves wrote:
> 
> 
>>Dear R-Help:
>>
>>	  I want to (a) subset a data.frame by several columns, (b) fit a model
>>to each subset, and (c) store a vector of results from the fit in the
>>columns of a data.frame.  In the past, I've used "for" loops do do this.
>>  Is there a way to use "by"?
>>
>>	  Consider the following example:
>>
>> > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
>>+  function(data.)coef(lm(y~x, data.)))
>> > byFits
>>A: A1
>>B: B1
>>   (Intercept)             x
>>  3.333333e-01 -1.517960e-16
>>------------------------------------------------------------
>>A: A2
>>B: B1
>>NULL
>>------------------------------------------------------------
>>A: A1
>>B: B2
>>NULL
>>------------------------------------------------------------
>>A: A2
>>B: B2
>>  (Intercept)            x
>>6.666667e-01 3.282015e-16
>> >
>> >
>>#############################
>>Desired output:
>>
>>data.frame(A=c("A1","A2"), B=c("B1", "B2"),
>>	.Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
>>
>>What's the simplest way to do this?
> 
> 
> do.call("rbind", byFits)
> 
> 
> 	-thomas
>



From paul_bivand at blueyonder.co.uk  Fri Jun  6 01:22:25 2003
From: paul_bivand at blueyonder.co.uk (Paul)
Date: Fri, 06 Jun 2003 00:22:25 +0100
Subject: [R] Logistic regression problem: propensity score matching
In-Reply-To: <5.1.0.14.2.20030605111345.01f7a8e0@mcmail.cis.mcmaster.ca>
References: <Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
	<Pine.LNX.4.44.0306040738400.25436-100000@gannet.stats>
	<5.1.0.14.2.20030605111345.01f7a8e0@mcmail.cis.mcmaster.ca>
Message-ID: <3EDFD0B1.5050306@blueyonder.co.uk>

Thank you all.

I made a pretty basic error in using multinom rather than glm 
family=binomial which needed rapid correction.

I have now rewritten the relevant part using glm.

After importing I convert all categorical variables into factors

londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
rownames=TRUE)
attach(londonpsm)
factor(londonpsm$InSample)
factor(londonpsm$GENDER)
factor(londonpsm$DISABLED)
factor(londonpsm$ETHCODE)
factor(londonpsm$LOPTYPE)
LonOutput <- glm(InSample ~ AGE + DISABLED + GENDER + ETHCODE + NDYPTOT 
+ NDLTUTOT + LOPTYPE, family = binomial)
lonoutput <- data.frame(fitted.values(LonOutput))
sqlSave(channel, lonoutput, tablename="lonoutput", safer=FALSE)

 From the comments, this looks better, but it may be there is some 
further switch I should use.

Apologies for the variables in capitals - my data comes in SPSS format 
but to manipulate it I use Access, and the only way I can see to get 
data from SPSS to Access is to export it in a format such as dbase, 
which capitalises all variables.

While sqlFetch, sqlQuery and sqlSave seem to work amazingly well, and 
fast, I am still having a problem with my rownames. I would like the 
imported data to have the database unique ID as the rownames, and 
protect these through the analysis, so that the two columns in 
fitted.values are unique ID and the fitted value. So far this does not 
work.

Then, once the result has been sqlSaved, the inclusion of the unique ID 
enables matching of the resulting action and control sample with 
personal details for fieldwork, after the closest control match to 
action sample has been identified.

John Fox wrote:

> Dear Paul,
>
>
> At 08:41 PM 6/4/2003 +0100, Paul wrote:
>
>> Thanks for your reply.
>>
>> I am using logistic regression because my response variable is 
>> categorical - and this seems to be recommended in the literature (by 
>> Heckman, Smith and others).
>
>
> I think that Prof. Ripley's point here is that although one can use 
> multnom in the nnet package to fit a binary (or binomial) logistic 
> regression, it is more common to do so using the glm (generlized 
> linear model) function. One normally would use multinomial logistic 
> regression only for a polytomous (several-category) response variable. 
> Applied to a dichotomous response, it will give the same results as a 
> binary logistic regression.
>
>> . . .
>>
>> I have MASS but was unable to locate logistic regression, which I was 
>> advised was the standard method for my problem.
>
>
> In MASS (4th edition), logit models are discussed in chapter 7 on 
> generalized linear models (see, in particular, section 7.2). In my R 
> and S-PLUS Companion, to which you referred in your original message, 
> these models are discussed in chapter 5 on generalized linear models 
> (see, in particular, section 5.2.1).
>
> I hope that this helps,
>  John
>
>> Thanks again.
>>
>> Prof Brian Ripley wrote:
>>
>>> 1) Why are you using multinom when this is not a multinomial 
>>> logistic regression?  You could just use a binomial glm.
>>>
>>> 2) The second argument to predict() is `newdata'.  `sample' is an R 
>>> function, so what did you mean to have there?  I think the 
>>> predictions should be a named vector if `sample' is a data frame.
>>>
>>> 3) There are many more examples of such things (and more 
>>> explanation) in Venables & Ripley's MASS (the book).
>>>
>>> On Wed, 4 Jun 2003, Paul Bivand wrote:
>>>
>>>
>>>
>>>> I am doing one part of an evaluation of a mandatory welfare-to-work 
>>>> programme in the UK.
>>>> As with all evaluations, the problem is to determine what would 
>>>> have happened if the initiative had not taken place.
>>>> In our case, we have a number of pilot areas and no possibility of 
>>>> random assignment.
>>>> Therefore we have been given control areas.
>>>> My problem is to select for survey individuals in the control areas 
>>>> who match as closely as possible the randomly selected sample of 
>>>> action area participants.
>>>> As I understand the methodology, the procedure is to run a logistic 
>>>> regression to determine the odds of a case being in the sample, 
>>>> across both action and control areas, and then choose for control 
>>>> sample the control area individual whose odds of being in the 
>>>> sample are closest to an actual sample member.
>>>>
>>>> So far, I have following the multinomial logistic regression 
>>>> example in Fox's Companion to Applied Regression.
>>>> Firstly, I would like to know if the predict() is producing odds 
>>>> ratios (or probabilities) for being in the sample, which is what I 
>>>> am aiming for.
>>>
>>>
>>> You asked for `probs', so you got probabilities.
>>>
>>>
>>>
>>>> Secondly, how do I get rownames (my unique identifier) into the 
>>>> output from predict() - my input may be faulty somehow and the 
>>>> wrong rownames being picked up - as I need to export back to 
>>>> database to sort and match in names, addresses and phone numbers 
>>>> for my selected samples.
>>>>
>>>> My code is as follows:
>>>> londonpsm <- sqlFetch(channel, "London_NW_london_pilots_elig", 
>>>> rownames=ORCID)
>>>> attach(londonpsm)
>>>> mod.multinom <- multinom(sample ~ AGE + DISABLED + GENDER + ETHCODE 
>>>> + NDYPTOT + NDLTUTOT + LOPTYPE)
>>>> lonoutput <- predict(mod.multinom, sample, type='probs')
>>>> london2 <- data.frame(lonoutput)
>>>>
>>>> The Logistic regression seems to work, although summary() says the 
>>>> it is not a matrix.
>>>>
>>>
>>> what is `it'?
>>>
>>>
>>>
>>>> The output looks like odds ratios, but I would like to know whether 
>>>> this is so.
>>>>
>>>
>>> No.
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>
>



From macq at llnl.gov  Fri Jun  6 01:24:33 2003
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 5 Jun 2003 16:24:33 -0700
Subject: [R] converting "by" to a data.frame?
In-Reply-To: <3EDFBC17.7060404@pdf.com>
References: <3EDFBC17.7060404@pdf.com>
Message-ID: <p05210609bb057f0cf8ba@[128.115.153.6]>

Since I don't have your by.df to test with I may not have it exactly 
right, but something along these lines should work:

byFits <- lapply(split(by.df,paste(by.df$A,by.df$B)),
                  FUN=function(data.) {
                     tmp <- coef(lm(y~x,data.))
                     data.frame(A=unique(data.$A),
                                B=unique(data.$B),
                                intercept=tmp[1],
                                slope=tmp[2])
                    })

byFitsDF <- do.call('rbind',byFits)

That's assuming I've got all the closing parantheses in the right 
places, since my email software (Eudora) doesn't do R syntax checking!

This approach can get rather slow if by.df is big, or when the 
computations in FUN are extensive (or both).

If by.df$A has mode character (as opposed to being a factor), then 
replacing A=unique(data.$A) with A=I(unique(data.$A)) might improve 
performance. You want to avoid character to factor conversions when 
using an approach like this.

-Don


At 2:54 PM -0700 6/5/03, Spencer Graves wrote:
>Dear R-Help:
>
>	  I want to (a) subset a data.frame by several columns, (b) 
>fit a model to each subset, and (c) store a vector of results from 
>the fit in the columns of a data.frame.  In the past, I've used 
>"for" loops do do this.  Is there a way to use "by"?
>
>	  Consider the following example:
>
>  > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
>+  function(data.)coef(lm(y~x, data.)))
>  > byFits
>A: A1
>B: B1
>   (Intercept)             x
>  3.333333e-01 -1.517960e-16
>------------------------------------------------------------
>A: A2
>B: B1
>NULL
>------------------------------------------------------------
>A: A1
>B: B2
>NULL
>------------------------------------------------------------
>A: A2
>B: B2
>  (Intercept)            x
>6.666667e-01 3.282015e-16
>>
>>
>#############################
>Desired output:
>
>data.frame(A=c("A1","A2"), B=c("B1", "B2"),
>	.Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
>
>What's the simplest way to do this?
>Thanks,
>Spencer Graves
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From andy_liaw at merck.com  Fri Jun  6 03:50:13 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 05 Jun 2003 21:50:13 -0400
Subject: [R] ridge regression
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBBE@usrymx25.merck.com>

Hi Frank,

> From: Frank E Harrell Jr [mailto:fharrell at virginia.edu]

[snip]

> The anova method for ols fits 'works' when you penalize the 
> model but there is some controversy over whether we should be 
> testing biased coefficients.  Some believe that hypothesis 
> tests should be done using the unpenalized model.  That 
> brings up other ways to handle collinearity: test groups of 
> variables in combination so they don't compete with each 
> other, or collapse them into summary scores (e.g., principal 
> components) before putting them in the model. 

I'm not clear about the last point.  Suppose three of the variables
are nearly collinear.  Are you suggesting to replace the variables
with the first one or two PCs, and drop the rest?  If so, doesn't
that also lead to biased estimators?

Best,
Andy
 
> ---
> Frank E Harrell Jr              Prof. of Biostatistics & Statistics
> Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
> U. Virginia School of Medicine  
> http://hesweb1.med.virginia.edu/biostat

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From andy_liaw at merck.com  Fri Jun  6 04:03:56 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 05 Jun 2003 22:03:56 -0400
Subject: [R] na.action in model.frame
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBC0@usrymx25.merck.com>

Try:

  X <- model.frame(form=formula, data=mydata, na.action=na.pass)

Andy

> -----Original Message-----
> From: Ravi Varadhan [mailto:rvaradha at jhsph.edu]
> Sent: Thursday, June 05, 2003 12:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] na.action in model.frame
> 
> 
> Dear Group:
> 
> I am trying to create a model frame from a formula for the model as 
> follows:
> 
> formula <- y ~ x1 + x2 + x3
> X <- model.frame(form=formula,data=mydata)
> 
> I have some missing values in some the variables, but I want 
> them to be 
> included in my model frame and to be indicated as "NA". Is 
> there a way 
> to do this?
> 
> thanks,
> Ravi.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From j.taylor at agec.usyd.edu.au  Fri Jun  6 16:37:41 2003
From: j.taylor at agec.usyd.edu.au (james taylor)
Date: Sat, 07 Jun 2003 00:37:41 +1000
Subject: [R] Moran's index
Message-ID: <1157219082-1147937@agec.usyd.edu.au>

Hi,

I am looking for some code for Moran's I.  Has anyone previously done this?
 I have been unable to find it in the search engines.

James



From andy_liaw at merck.com  Fri Jun  6 04:19:35 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 05 Jun 2003 22:19:35 -0400
Subject: [R] kmeans (again)
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBC1@usrymx25.merck.com>

Just because you get the same answer from different starting points doesn't
mean the algorithm isn't using the starting points you specified.

I tried:

> set.seed(1)
> x <- matrix(rnorm(12), 6, 2)
> kmeans(x, x[c(1,6),], 1)
$cluster
[1] 2 1 2 1 1 2

$centers
        [,1]      [,2]
1  0.7028106 0.6482392
2 -0.7608503 0.4843512

$withinss
[1] 2.86861843 0.04450923

$size
[1] 3 3

> kmeans(x, 2, 1)
$cluster
[1] 2 1 2 1 1 2

$centers
        [,1]      [,2]
1  0.7028106 0.6482392
2 -0.7608503 0.4843512

$withinss
[1] 2.86861843 0.04450923

$size
[1] 3 3

> kmeans(x, x[c(3,4),], 1)
$cluster
[1] 1 1 1 2 1 1

$centers
        [,1]       [,2]
1 -0.3538799  0.7406319
2  1.5952808 -0.3053884

$withinss
[1] 2.089050 0.000000

$size
[1] 5 1

which shows that the result *can* depend on the starting values.

Andy

> -----Original Message-----
> From: Luis Torgo [mailto:ltorgo at liacc.up.pt]
> Sent: Thursday, June 05, 2003 2:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] kmeans (again)
> 
> 
> Regarding a previous question concerning the kmeans function 
> I've tried the 
> same example and I also get a strange result (at least 
> according to what is 
> said in the help of the function kmeans). Apparently, the function is 
> disregarding the initial cluster centers one gives it. 
> According to the help 
> of the function:
> 
>  centers: Either the number of clusters or a set of initial cluster
>           centers...
> 
> Now a small dataset:
> > data<-matrix(c(-1,0,2,2.5,7,9,0,3,0,6,1,4),6,2)
> 
> If I use rows 3 and 4 as cluster centers and a single 
> iteration of kmeans I 
> get:
> > kmeans(data,data[c(3,4),],1)
> $cluster
> [1] 1 1 1 1 2 2
> 
> $centers
>    [,1] [,2]
> 1 0.875 2.25
> 2 8.000 2.50
> 
> $withinss
> [1] 32.9375  6.5000
> 
> $size
> [1] 4 2
> 
> If I now use rows 1 and 6 as cluster centers I get exactly 
> the same solution 
> after the first iteration:
> 
> > kmeans(data,data[c(1,6),],1)
> $cluster
> [1] 1 1 1 1 2 2
> 
> $centers
>    [,1] [,2]
> 1 0.875 2.25
> 2 8.000 2.50
> 
> $withinss
> [1] 32.9375  6.5000
> 
> $size
> [1] 4 2
> 
> So, apparently the function is disregarding the initial 
> cluster centers 
> information. This is even "confirmed" by the fact that if I 
> use the function 
> without cluster centers, simply given the number of clusters, 
> I get the same 
> solution:
> > kmeans(data,2,1)
> $cluster
> [1] 2 2 2 2 1 1
> 
> $centers
>    [,1] [,2]
> 1 8.000 2.50
> 2 0.875 2.25
> 
> $withinss
> [1]  6.5000 32.9375
> 
> $size
> [1] 2 4
> 
> 
> 
> -- 
> Luis Torgo
>     FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
>     Machine Learning Group           Fax   : (+351) 22 600 36 54
>     R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
>     4150 PORTO   -  PORTUGAL         WWW   : 
> http://www.liacc.up.pt/~ltorgo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From andy_liaw at merck.com  Fri Jun  6 03:54:19 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 05 Jun 2003 21:54:19 -0400
Subject: [R] counting missing values
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBBF@usrymx25.merck.com>

> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> 
> vincent.stoliaroff at socgen.com wrote:
> 
> > Hello R lovers
> > I have written a little cute function to count the number 
> of missing value
> > per row in a matrix and return the percentage of missing value
> > 
> > it takes a lot of time to run with a 1000 rows matrix
> > 
> > I'd like to know if there is a function already implemented 
> to count the
> > number of occurence of a given values in a vector
> > 
> > 
> > For information,
> > here is the function
> > count<-0
> > for (i in 1:nrow(Matrix))
> >       {
> >       for (j in 1:ncol(Matrix)) {if (is.na(Matrix[i,j])) 
> count<-count+1}
> >       Result[i,1]<-((count/(ncol(Matrix)))*100);
> >       count<-0
> >       }
> > Result
> > 
> > thanks for any help
> > Vincent
> 
> Well, it's pretty easy to do it:
> 
> apply(Matrix, 1, function(x) sum(is.na(x))) / ncol(Matrix) * 100

Or perhaps faster:

  rowMeans(is.na(Matrix)) * 100

Andy

 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From mschwartz at medanalytics.com  Fri Jun  6 05:03:08 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 5 Jun 2003 22:03:08 -0500
Subject: [R] Moran's index
In-Reply-To: <1157219082-1147937@agec.usyd.edu.au>
Message-ID: <00a701c32bd8$291a8500$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of james taylor
>Sent: Friday, June 06, 2003 9:38 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Moran's index
>
>
>Hi,
>
>I am looking for some code for Moran's I.  Has anyone 
>previously done this?
> I have been unable to find it in the search engines.
>
>James



A quick search using the R site search engine, shows that package
'spdep' by Roger Bivand contains what you are looking for.

The URL for the help page for function moran() is at:

http://finzi.psych.upenn.edu/R/library/spdep/html/moran.html

See the associated functions listed on that page.

HTH,

Marc Schwartz



From katkih at mail.nih.gov  Fri Jun  6 06:15:16 2003
From: katkih at mail.nih.gov (Katki, Hormuzd (NIH/NCI))
Date: Fri, 6 Jun 2003 00:15:16 -0400 
Subject: [R] coxph.detail() documentation point
Message-ID: <9D7EF737FA4C6F4FBBFC52FC30B83690D49F2C@nihexchange7.nih.gov>

	Hello, I'm using R 1.6.2's survival2.9-6 package, and I just thought
I'd note that it took me a long time to realize that the
coxph.detail()$hazard was a centered hazard (centered at the exponentiated
average covariate times beta), not a baseline hazard.  The documentation
simply says "the hazard increment".  I don't know if this has been clarified
in the latest version of R, but it would probably prevent confusion to state
that it is a centered hazard.

Thank you,
Hormuzd Katki

Hormuzd Katki
Biostatistics Branch, Division of Cancer Epidemiology and Genetics
National Cancer Institute
6120 Executive Blvd. Room 8044 MSC 7244
Rockville, MD 20852-4910
301-594-7818 (voice)
301-402-0081 (fax)
katkih at mail.nih.gov



From subhash_kalla at hotmail.com  Fri Jun  6 06:50:07 2003
From: subhash_kalla at hotmail.com (70808subhash kalla)
Date: Thu, 05 Jun 2003 23:50:07 -0500
Subject: [R] Query about C Function Interface
Message-ID: <Sea1-F1346UtEnUyR530001dcc8@hotmail.com>

Sir/Madam,

I am a graduate student in Petroleum Engineering. I have a problem in using 
R and hope that you could help in solving that. I have a code in C, which 
displays orthogonal arrays when I give number of rows and number of columns 
to it. I have another code in R that calls this C code. I am very new to R. 
I downloaded the R and loaded packages base and mva. I also kept my C code 
.exe file in the R\rw1070 directory and used the function that is written in 
R. There is some problem in this line it seems:

if(!is.loaded(C.symbol("woa")))  dyn.load(woa.obj)   # woa is my file name 
in C

Error message coming like this:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library "C:/Program Files/R/rw1070/woa.so":
  LoadLibrary failure:  The specified module could not be found

I tried to get help from help files but I couldnt get the problem solved. 
Its very important for me to fix this problem as fast as possible and hope 
you could help me in this regard.

Thank You
Sincerely,
Kalla



From ripley at stats.ox.ac.uk  Fri Jun  6 08:27:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jun 2003 07:27:36 +0100 (BST)
Subject: [R] Query about C Function Interface
In-Reply-To: <Sea1-F1346UtEnUyR530001dcc8@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306060720520.2947-100000@gannet.stats>

You are trying to load woa.obj, and you are on Windows which should be
looking for a .dll and the error message says woa.so.  You say you keep
your .exe in the rw1070 directory.

You are rather confused!  dyn.load on Windows loads a DLL file from the
current working directory, *as its help says*.  You need to build a
suitable DLL.

On Thu, 5 Jun 2003, 70808subhash kalla wrote:

> Sir/Madam,
> 
> I am a graduate student in Petroleum Engineering. I have a problem in using 
> R and hope that you could help in solving that. I have a code in C, which 
> displays orthogonal arrays when I give number of rows and number of columns 
> to it. I have another code in R that calls this C code. I am very new to R. 
> I downloaded the R and loaded packages base and mva. I also kept my C code 
> .exe file in the R\rw1070 directory and used the function that is written in 
> R. There is some problem in this line it seems:
> 
> if(!is.loaded(C.symbol("woa")))  dyn.load(woa.obj)   # woa is my file name 
> in C
> 
> Error message coming like this:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library "C:/Program Files/R/rw1070/woa.so":
>   LoadLibrary failure:  The specified module could not be found
> 
> I tried to get help from help files but I couldn?t get the problem solved. 
> Its very important for me to fix this problem as fast as possible and hope 
> you could help me in this regard.

Perhaps you should be paying a consultant to help you, then?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Peter.Robinson at t-online.de  Sat Jun  7 19:42:57 2003
From: Peter.Robinson at t-online.de (peter robinson)
Date: Sat, 7 Jun 2003 10:42:57 -0700
Subject: [R] beginner's question: Graphical presentation of t test
Message-ID: <200306071042.57407.peter.robinson@t-online.de>

Hi,

Is there any way to use R to present t test results for three groups of 
experiments, each of which involves several parallel experiment series with 
groups of control vs treated. I would like to present the average fold change 
of the experimental parameter (concentration of enzymes) as bars with 
standard error and the p value above the bar. So, there should be two groups 
(control vs treated)  of three bars for the three enzymes.

Thanks

Peter



From james.lindsey at luc.ac.be  Fri Jun  6 08:45:02 2003
From: james.lindsey at luc.ac.be (Jim Lindsey)
Date: Fri, 6 Jun 2003 08:45:02 +0200 (MET DST)
Subject: [R] Survival analysis and generalized gamma?
In-Reply-To: <20030605165242.0E5C26ACF5@smtp.us2.messagingengine.com> from
	"James D Thomas" at Jun 05, 2003 12:52:42 PM
Message-ID: <200306060645.IAA13909@luc.ac.be>

> 
> Is there an easy way to integrate the generalized gamma distribution into
> the current R survival package?

This distribution is available in my gnlr3 function in my gnlm
library. It handles left-, right-, and interval censoring, with linear
and nonlinear regression on all three parameters.
Available at www.luc.ac.be/~jlindsey/rcode.html
  Jim

> --james
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Ulf.Dagman at era.ericsson.se  Fri Jun  6 09:49:57 2003
From: Ulf.Dagman at era.ericsson.se (Ulf Dagman (EAB))
Date: Fri, 6 Jun 2003 09:49:57 +0200 
Subject: [R] Problerm building R-1.7.0 on OpenBSD3.2/sparc64
Message-ID: <634E1DDCE357E7479E1EA3B4F5EECB7001204F6D@esealnt879.al.sw.ericsson.se>

Hi all,

I try to build R-1.7.0 + patches. when problem arise.
I start with appling the patches and second configure and make.

Any ideas whats wrong (wrong compiler wrong make version or missing an essential library...)

/Ulf D

heres the log:
**** cut************
making method_meta_data.d from method_meta_data.c
making slot.d from slot.c
making class_support.d from class_support.c
making tests.d from tests.c
`Makedeps' is up to date.
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c do_substitute
_direct.c -o do_substitute_direct.o
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c methods_list_
dispatch.c -o methods_list_dispatch.o
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c method_meta_d
ata.c -o method_meta_data.o
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c slot.c -o slo
t.o
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c class_support
.c -o class_support.o
gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c tests.c -o te
sts.o
gcc -shared -L/usr/local/lib -o methods.so do_substitute_direct.o methods_list_d
ispatch.o method_meta_data.o slot.o class_support.o tests.o   mkdir -p -- ../../../../library/methods/libs
dumping R code in package 'methods'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_GlobalEnv'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_NamesSymbol'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_MissingArg'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_NilValue'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_DotsSymbol'
/home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
ethods/libs/methods.so: can't resolve reference 'R_UnboundValue'
Error in .Call("R_initialize_methods_metadata", table, PACKAGE = "methods") :
       .Call function name not in load table
Execution halted
*** Error code 1

Stop in /home/test/proj/downl/R-1.7.0/src/library/methods (line 70 of Makefile).
*** Error code 1

Stop in /home/test/proj/downl/R-1.7.0/src/library/methods (line 18 of Makefile).
*** Error code 1

Stop in /home/test/proj/downl/R-1.7.0/src/library (line 39 of Makefile).
*** Error code 1

Stop in /home/test/proj/downl/R-1.7.0/src (line 29 of Makefile).
*** Error code 1

Stop in /home/test/proj/downl/R-1.7.0 (line 62 of Makefile).
bash-2.05b$ 
********end logg *******



From ripley at stats.ox.ac.uk  Fri Jun  6 10:21:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jun 2003 09:21:35 +0100 (BST)
Subject: [R] Problerm building R-1.7.0 on OpenBSD3.2/sparc64
In-Reply-To: <634E1DDCE357E7479E1EA3B4F5EECB7001204F6D@esealnt879.al.sw.ericsson.se>
Message-ID: <Pine.LNX.4.44.0306060913490.3249-100000@gannet.stats>

Looks like that OS wants all symbols resolved: those are all variables
exported by R.bin.

1) See if there is a ld flag to allow unresolved symbols.

2) (something of a kludge)  configure with --enable-R-shared and
rebuild (from scratch), as this will link against the libR.so.
Whether that works correctly will be OS-dependent, since you realy want 
them resolved against R.bin when methods.so is loaded.

On Fri, 6 Jun 2003, Ulf Dagman (EAB) wrote:

> Hi all,
> 
> I try to build R-1.7.0 + patches. when problem arise.
> I start with appling the patches and second configure and make.
> 
> Any ideas whats wrong (wrong compiler wrong make version or missing an essential library...)

Wrong flags, probably.

> 
> /Ulf D
> 
> heres the log:
> **** cut************
> making method_meta_data.d from method_meta_data.c
> making slot.d from slot.c
> making class_support.d from class_support.c
> making tests.d from tests.c
> `Makedeps' is up to date.
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c do_substitute
> _direct.c -o do_substitute_direct.o
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c methods_list_
> dispatch.c -o methods_list_dispatch.o
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c method_meta_d
> ata.c -o method_meta_data.o
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c slot.c -o slo
> t.o
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c class_support
> .c -o class_support.o
> gcc -I../../../../include  -I/usr/local/include   -fPIC  -g -O2 -c tests.c -o te
> sts.o
> gcc -shared -L/usr/local/lib -o methods.so do_substitute_direct.o methods_list_d
> ispatch.o method_meta_data.o slot.o class_support.o tests.o   mkdir -p -- ../../../../library/methods/libs
> dumping R code in package 'methods'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_GlobalEnv'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_NamesSymbol'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_MissingArg'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_NilValue'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_DotsSymbol'
> /home/test/proj/downl/R-1.7.0/bin/R.bin: /home/test/proj/downl/R-1.7.0/library/m
> ethods/libs/methods.so: can't resolve reference 'R_UnboundValue'
> Error in .Call("R_initialize_methods_metadata", table, PACKAGE = "methods") :
>        .Call function name not in load table
> Execution halted
> *** Error code 1
> 
> Stop in /home/test/proj/downl/R-1.7.0/src/library/methods (line 70 of Makefile).
> *** Error code 1
> 
> Stop in /home/test/proj/downl/R-1.7.0/src/library/methods (line 18 of Makefile).
> *** Error code 1
> 
> Stop in /home/test/proj/downl/R-1.7.0/src/library (line 39 of Makefile).
> *** Error code 1
> 
> Stop in /home/test/proj/downl/R-1.7.0/src (line 29 of Makefile).
> *** Error code 1
> 
> Stop in /home/test/proj/downl/R-1.7.0 (line 62 of Makefile).
> bash-2.05b$ 
> ********end logg *******
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Jun  6 10:34:46 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 06 Jun 2003 08:34:46 -0000
Subject: [R] Problerm building R-1.7.0 on OpenBSD3.2/sparc64
In-Reply-To: <Pine.LNX.4.44.0306060913490.3249-100000@gannet.stats>
References: <Pine.LNX.4.44.0306060913490.3249-100000@gannet.stats>
Message-ID: <x21xy761gw.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Looks like that OS wants all symbols resolved: those are all variables
> exported by R.bin.
> 
> 1) See if there is a ld flag to allow unresolved symbols.
> 
> 2) (something of a kludge)  configure with --enable-R-shared and
> rebuild (from scratch), as this will link against the libR.so.
> Whether that works correctly will be OS-dependent, since you realy want 
> them resolved against R.bin when methods.so is loaded.

3) Dig out the place in the log where it links the main R binary and
   see what the link call looks like. Also check in Makeconf what the
   linker flags have been set to. Look especially at the -E or
   --export-dynamic flags and check them against compiler/linker docs.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From drosadi at server.eos.tuwien.ac.at  Fri Jun  6 10:58:45 2003
From: drosadi at server.eos.tuwien.ac.at (Dedi Rosadi)
Date: Fri, 06 Jun 2003 10:58:45 +0200
Subject: [R] estimation stable distribution parameters
Message-ID: <5.2.1.1.0.20030606104755.009fa0c0@server.eos.tuwien.ac.at>

Hi,
This just repetition of my mail, so please apologize me if you receive it 
twice.

I am wondering whether anyone of you already implemented in R or S+  the 
methods of estimation the parameters of stable distribution (especially the 
iid case). If there is, i will be happy to get the copy of the codes.

Some R-users referred me to the package stable (thanks M. Maechler and B. 
Pikounis), but i think it does not give the answer to my question. Probably 
i can use function stablereg in the package, but after read the explanation 
of the function, i am totally confused with the result reported. Simply 
applying what they shown in the example in the help for this function, for 
example check for the line after

#Stable model with loc(ation)=loc.h(b0+b1*day)

i got  result of skewness estimate as  -2.05  which is an incorrect thing 
(? i am not sure, probably my interpretation is wrong)

It looks like they implemented the method of estimation for regression 
model based on approximate / conditional maximum likelihood (I didn't check 
their codes yet, and also until now i can not get the referred paper).  I 
intended to see the accuracy of the other estimation procedures e.g. the 
method of quantile's of McCulloch or the method based on ECF. Some 
colleagues referring me to Xplore, where Weron has implement the methods 
using the languange Xplore, however it is inaccessible for public.

Please let me know if any body has any clues to my question. Otherwise, i 
have to start writing my own codes

Regards
Dedi t



From grassi at psico.univ.trieste.it  Fri Jun  6 10:49:57 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Fri, 6 Jun 2003 10:49:57 +0200 (MEST)
Subject: [R] load workspace
Message-ID: <200306060849.KAA15183@server.psico.univ.trieste.it>

Hi,
When i load my workspace, i visualize my object with 
the ls()function.
 First problem is that the saved graph is open like a 
list instead of an image!What is wrong?
Second problem is about scatterplot graph: how can i 
identify a specific point?
Thanks.
Michele.



From james.lindsey at luc.ac.be  Fri Jun  6 11:17:36 2003
From: james.lindsey at luc.ac.be (Jim Lindsey)
Date: Fri, 6 Jun 2003 11:17:36 +0200 (MET DST)
Subject: [R] estimation stable distribution parameters
In-Reply-To: <5.2.1.1.0.20030606104755.009fa0c0@server.eos.tuwien.ac.at> from
	"Dedi Rosadi" at Jun 06, 2003 10:58:45 AM
Message-ID: <200306060917.LAA26854@luc.ac.be>

> 
> Hi,
> This just repetition of my mail, so please apologize me if you receive it 
> twice.
> 
> I am wondering whether anyone of you already implemented in R or S+  the 
> methods of estimation the parameters of stable distribution (especially the 
> iid case). If there is, i will be happy to get the copy of the codes.
> 
> Some R-users referred me to the package stable (thanks M. Maechler and B. 
> Pikounis), but i think it does not give the answer to my question. Probably 
> i can use function stablereg in the package, but after read the explanation 
> of the function, i am totally confused with the result reported. Simply 
> applying what they shown in the example in the help for this function, for 
> example check for the line after
> 
> #Stable model with loc(ation)=loc.h(b0+b1*day)
> 
> i got  result of skewness estimate as  -2.05  which is an incorrect thing 
> (? i am not sure, probably my interpretation is wrong)

It is correct for the parametrisation used.

> 
> It looks like they implemented the method of estimation for regression 
> model based on approximate / conditional maximum likelihood (I didn't check 
> their codes yet, and also until now i can not get the referred paper).  I 
> intended to see the accuracy of the other estimation procedures e.g. the 
> method of quantile's of McCulloch or the method based on ECF. Some 
> colleagues referring me to Xplore, where Weron has implement the methods 
> using the languange Xplore, however it is inaccessible for public.
> 

It uses exact MLE with numerical precision set by the integration
(user specified). The methods to which you refer are all obsolete.
  Jim

> Please let me know if any body has any clues to my question. Otherwise, i 
> have to start writing my own codes
> 
> Regards
> Dedi t
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Fri Jun  6 11:28:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 Jun 2003 11:28:05 +0200
Subject: [R] confusedness about "saved graph"; was: load workspace
In-Reply-To: <200306060849.KAA15183@server.psico.univ.trieste.it>
References: <200306060849.KAA15183@server.psico.univ.trieste.it>
Message-ID: <3EE05EA5.9090806@statistik.uni-dortmund.de>

Michele Grassi wrote:

> Hi,
> When i load my workspace, i visualize my object with 
> the ls()function.

You list the available objects in your Workspace, I guess?
I don't know of the feature to visualize objects with ls().


>  First problem is that the saved graph is open like a 
> list instead of an image!What is wrong?

What is a "saved graph"? Please be more specific! In addition, this has 
nothing to do with loading a workspace.

I guess you mean a recorded plot via recordPlot(), right? This is indeed 
represented as a list, as well as a lot other objects. But you can 
simply use replayPlot() to plot it into an active device.
Or do you mean a graph describing a graphical model? Or ..... ?



> Second problem is about scatterplot graph: how can i 
> identify a specific point?

See ?identify

Uwe Ligges


> Thanks.
> Michele.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From simon at stats.gla.ac.uk  Fri Jun  6 11:45:56 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 6 Jun 2003 10:45:56 +0100 (BST)
Subject: [R] partial residuals in plot.gam()
In-Reply-To: <5.2.1.1.0.20030605185915.03efe258@pop3.norton.antivirus>
Message-ID: <Pine.SOL.3.96.1030606103805.20906A-100000@moon.stats.gla.ac.uk>


> I know a partial residual option in plot.gam() is on Simon Wood's todo 
> list, but since I'm in the midst of a project and not yet having acquired 
> sufficient R knowledge to code something usable myself I'll have to put my 
> trust in you. Anybody got some code lying around for doing this? Or if 
> someone can supply me with enough hints for doing it myself, I'd be most 
> grateful.

- sorry partial residuals still no done, but here's a `do-it-yourself' 
example....

# set up a simulated example....
n<-400;sig2<-4
x0 <- runif(n, 0, 1);x1 <- runif(n, 0, 1)
x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
pi <- asin(1) * 2
f <- 2 * sin(pi * x0)
f <- f + exp(2 * x1) - 3.75887
f <- f + 0.2 * x2^11 * (10 * (1 - x2))^6 + 10 * (10 * x2)^3 * 
     (1 -x2)^10 - 1.396
e <- rnorm(n, 0, sqrt(abs(sig2)))
y <- f + e

# fit GAM....
b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3))
# produce a plot just for second term
plot(b,select=2)
# and add partial residuals...
d<-predict(b,type="terms")
points(x1,b$residuals+d[3,],col=2)


Thanks also for the previous suggestions - I'll add them to the list, but
I doubt any will be in version 0.9, which is a bit too near to
completion now. 

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From vincent.stoliaroff at socgen.com  Fri Jun  6 11:57:32 2003
From: vincent.stoliaroff at socgen.com (vincent.stoliaroff@socgen.com)
Date: Fri, 6 Jun 2003 11:57:32 +0200
Subject: [R] writing comments within a function
Message-ID: <OFF4556FAD.347094DD-ONC1256D3D.00367D45@ges.marc.societe-generale.fr>

Hi R lovers!

I would like to know how you can write comments inside the code of a
function

is it latex style
% ?
or any other language style?

thanks a lot





*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
titre de ce message s'il a ete altere, deforme ou falsifie.
				********
This message and any attachments (the "message") are confidentia... {{dropped}}



From lsilva at fc.up.pt  Fri Jun  6 12:15:48 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Fri, 6 Jun 2003 11:15:48 +0100
Subject: [R] Kmeans again
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEA5E@MAIL.fc.up.pt>

Dear helpers
 
I'm sorry to insist but I still think there is something wrong with the function kmeans. For instance, let's try the same small example:
 
> dados<-matrix(c(-1,0,2,2.5,7,9,0,3,0,6,1,4),6,2)

I will choose observations 3 and 4 for initial centers and just one iteration. The results are
 
> A<-kmeans(dados,dados[c(3,4),],1)
> A
$cluster
[1] 1 1 1 1 2 2
$centers
   [,1] [,2]
1 0.875 2.75
2 8.000 2.50
$withinss
[1] 38.9375  6.5000
$size
[1] 4 2
 
If I do it by hand, after one iteration, the results are
 
$cluster
[1] 1 2 1 2 1 2
 
So I think that something is wrong with the function kmeans; probably the initial centers given by the user are not being taken into account.



From fharrell at virginia.edu  Fri Jun  6 12:36:55 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 6 Jun 2003 06:36:55 -0400
Subject: [R] ridge regression
In-Reply-To: <3A822319EB35174CA3714066D590DCD5C4FBBE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD5C4FBBE@usrymx25.merck.com>
Message-ID: <20030606063655.30c49bd7.fharrell@virginia.edu>

On Thu, 05 Jun 2003 21:50:13 -0400
"Liaw, Andy" <andy_liaw at merck.com> wrote:

> Hi Frank,
> 
> > From: Frank E Harrell Jr [mailto:fharrell at virginia.edu]
> 
> [snip]
> 
> > The anova method for ols fits 'works' when you penalize the 
> > model but there is some controversy over whether we should be 
> > testing biased coefficients.  Some believe that hypothesis 
> > tests should be done using the unpenalized model.  That 
> > brings up other ways to handle collinearity: test groups of 
> > variables in combination so they don't compete with each 
> > other, or collapse them into summary scores (e.g., principal 
> > components) before putting them in the model. 
> 
> I'm not clear about the last point.  Suppose three of the variables
> are nearly collinear.  Are you suggesting to replace the variables
> with the first one or two PCs, and drop the rest?  If so, doesn't
> that also lead to biased estimators?

Yes, it's another way to get biased estimators.  But many statisticians test these components in incomplete principal components regression using ordinary tests, to get tests for the whole group of coefficients these represent (under restrictions).  -Frank

> 
> Best,
> Andy
>  
> > ---
> > Frank E Harrell Jr              Prof. of Biostatistics & Statistics
> > Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
> > U. Virginia School of Medicine  
> > http://hesweb1.med.virginia.edu/biostat
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, cont... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From ernesto at ipimar.pt  Fri Jun  6 12:38:28 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Fri, 06 Jun 2003 10:38:28 -0000
Subject: [R] scales in xyplot doesn't seem to work for x axis
In-Reply-To: <200306051226.21999.deepayan@stat.wisc.edu>
References: <1054813269.1242.15.camel@gandalf.ipimar.pt>
	<200306051226.21999.deepayan@stat.wisc.edu>
Message-ID: <1054895886.1250.6.camel@gandalf.ipimar.pt>

On Thu, 2003-06-05 at 18:26, Deepayan Sarkar wrote:
> On Thursday 05 June 2003 06:41, Ernesto Jardim wrote:
> > Hi
> >
> > I'm doing a xyplot and I wand to reduce the number of tick marks in the
> > x axis. My x axis are month and I want to reduce the 12 tick marks to 4.
> > I used the scales argument but it doesn't seem to work, althougth it
> > works on y axis if I use scales=list(tick.number=4).
> 
> By 'month', so you mean a POSIXct object ? lattice is not very good with 
> those.
> 
> The other possibility is that your mes is a factor (or a character vector, 
> which is essentially the same). In that case, tick.number will have no effect 
> (it works only for numeric). You could do something like
> 
> xlim = c('Jan', '', '', 'Apr', '', '', 'Jul', '', '', 'Oct', '', '')
> 
> 
> > xyplot(land~mes|porto+arte,data=hom.land.mpa[hom.land.mpa$arte!="pseine",],
> >type="l",scales=list(x=list(tick.number=4)))
> >
> > Regards
> >
> > EJ

Hi

You're correct my "mes" variable is a factor. But I can transform it to
a numeric variable or use your sugestion.

It would be nice to add this restrition to the lattice documentation.

Regards and thanks

EJ



From fwadmin2 at deutschepost.de  Fri Jun  6 12:49:38 2003
From: fwadmin2 at deutschepost.de (fwadmin2@deutschepost.de)
Date: Fri, 6 Jun 2003 12:49:38 +0200 (MEST)
Subject: [R] Ein Virus in der E-MAIL von r-help @ hypatia . math . ethz . ch
 gefunden . ( ddaah143@deutschepost.de )
Message-ID: <200306061048.h56AmiSC009402@ppd00021.deutschepost.de>


In der Mailnachricht wurde vom Virenscanner
ein VIRUS gefunden:


	Von:      r-help at hypatia.math.ethz.ch
	Gesendet: Fri, 6 Jun 2003 12:44:30 +0200
	Betrefd:  Re: Application

Daher konnte diese Mail nicht an den/die Empfaenger weitergeleitet werden.

Mimemaster at deutschepost.de



From otoomet at econ.dk  Fri Jun  6 13:05:02 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Fri, 6 Jun 2003 13:05:02 +0200
Subject: [R] writing comments within a function
In-Reply-To: <OFF4556FAD.347094DD-ONC1256D3D.00367D45@ges.marc.societe-generale.fr>
	(vincent.stoliaroff@socgen.com)
References: <OFF4556FAD.347094DD-ONC1256D3D.00367D45@ges.marc.societe-generale.fr>
Message-ID: <200306061105.h56B52f11230@punik.econ.au.dk>

 | From: vincent.stoliaroff at socgen.com
 | Date: Fri, 6 Jun 2003 11:57:32 +0200

Hallo,

 | Hi R lovers!
 | 
 | I would like to know how you can write comments inside the code of a
 | function
 | 
 | is it latex style
 | % ?
 | or any other language style?

It's #, otherwise like tex.  The same character is used by gnuplot and
many configuration scripts and probably by many programming languages
too.

You may also consider

if(FALSE) {
...
}

But this "comment" must be syntactically correct.  You may also
consider emacs+ESS which includes possibilities to commenting and
uncommenting large program blocks.

I don't know where it is mentioned explicitly (I did not see it in "S
Programming" (a book)), but most examples include comments, I guess.

Best wishes,

Ott



From th50 at leicester.ac.uk  Fri Jun  6 13:32:42 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 6 Jun 2003 12:32:42 +0100
Subject: [R] writing comments within a function
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E4698@saffron.cfs.le.ac.uk>

> -----Original Message-----
> From: Ott Toomet [mailto:otoomet at econ.dk]
> Sent: 06 June 2003 12:05
> To: vincent.stoliaroff at socgen.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] writing comments within a function
> 
> 
>  | From: vincent.stoliaroff at socgen.com
>  | Date: Fri, 6 Jun 2003 11:57:32 +0200
> 
> Hallo,
> 
>  | Hi R lovers!
>  | 
>  | I would like to know how you can write comments inside the 
> code of a
>  | function
>  | 
>  | is it latex style
>  | % ?
>  | or any other language style?
> 
> It's #, otherwise like tex.  The same character is used by gnuplot and
> many configuration scripts and probably by many programming languages
> too.
> 
> You may also consider
> 
> if(FALSE) {
> ...
> }
> 
> But this "comment" must be syntactically correct.  You may also
> consider emacs+ESS which includes possibilities to commenting and
> uncommenting large program blocks.
> 
> I don't know where it is mentioned explicitly (I did not see it in "S
> Programming" (a book)), but most examples include comments, I guess.

It is stated in the "R Language Definition" manual, section 10.2, which 
usually comes with the distribution.

Best wishes

Thomas



From dray at biomserv.univ-lyon1.fr  Fri Jun  6 13:40:59 2003
From: dray at biomserv.univ-lyon1.fr (Stephane Dray)
Date: Fri, 6 Jun 2003 13:40:59 +0200
Subject: [R] Moran's index
In-Reply-To: <1157219082-1147937@agec.usyd.edu.au>
References: <1157219082-1147937@agec.usyd.edu.au>
Message-ID: <a05010402bb062e13ffc8@[134.214.32.69]>

>Hi,
>
>I am looking for some code for Moran's I.  Has anyone previously done this?
>  I have been unable to find it in the search engines.
>
>James

Computations and tests for Moran's I and Geray's c are available in 
the spdep package.

-- 
St?phane DRAY
---------------------------------------------------------------
Biom?trie et Biologie ?volutive - Equipe "?cologie Statistique"
Universite Lyon 1 - Bat 711 - 69622 Villeurbanne CEDEX - France

Tel : 04 72 43 27 56			   Fax : 04 72 43 13 88
       04 72 43 27 57 	   E-mail : dray at biomserv.univ-lyon1.fr 
---------------------------------------------------------------
Web                            http://www.steph280.freesurf.fr/



From dmurdoch at pair.com  Fri Jun  6 14:04:55 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 06 Jun 2003 08:04:55 -0400
Subject: [R] Query about C Function Interface
In-Reply-To: <Sea1-F1346UtEnUyR530001dcc8@hotmail.com>
References: <Sea1-F1346UtEnUyR530001dcc8@hotmail.com>
Message-ID: <gk01ev836frmd7a14d4un909heedp2du4i@4ax.com>

On Thu, 05 Jun 2003 23:50:07 -0500, you wrote:

>if(!is.loaded(C.symbol("woa")))  dyn.load(woa.obj)   # woa is my file name 
>in C
>
>Error message coming like this:
>
>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library "C:/Program Files/R/rw1070/woa.so":
>  LoadLibrary failure:  The specified module could not be found

This message indicates that you set your filename to "woa.so", which
doesn't make sense in a Windows context.  As Brian Ripley said, you
need to make a DLL.

There are some instructions on making DLLs in the readme.packages help
file; more are on my web page

 <http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/>

Duncan Murdoch



From vincent.stoliaroff at socgen.com  Fri Jun  6 14:38:52 2003
From: vincent.stoliaroff at socgen.com (vincent.stoliaroff@socgen.com)
Date: Fri, 6 Jun 2003 14:38:52 +0200
Subject: [R] writing comments within a function
Message-ID: <OF0EF91FCA.404CFC95-ONC1256D3D.00457117@ges.marc.societe-generale.fr>


Thanks you everybody for all your answers



|---------+---------------------------->
|         |           th50 at leicester.ac|
|         |           .uk              |
|         |                            |
|         |           06/06/03 01:32 PM|
|         |                            |
|---------+---------------------------->
  >------------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                              |
  |       To:       Vincent STOLIAROFF/fr/socgen at socgen, otoomet at econ.dk                                                         |
  |       cc:       r-help at stat.math.ethz.ch                                                                                     |
  |       Subject:  RE: [R] writing comments within a function                                                                   |
  >------------------------------------------------------------------------------------------------------------------------------|



> -----Original Message-----
> From: Ott Toomet [mailto:otoomet at econ.dk]
> Sent: 06 June 2003 12:05
> To: vincent.stoliaroff at socgen.com
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] writing comments within a function
>
>
>  | From: vincent.stoliaroff at socgen.com
>  | Date: Fri, 6 Jun 2003 11:57:32 +0200
>
> Hallo,
>
>  | Hi R lovers!
>  |
>  | I would like to know how you can write comments inside the
> code of a
>  | function
>  |
>  | is it latex style
>  | % ?
>  | or any other language style?
>
> It's #, otherwise like tex.  The same character is used by gnuplot and
> many configuration scripts and probably by many programming languages
> too.
>
> You may also consider
>
> if(FALSE) {
> ...
> }
>
> But this "comment" must be syntactically correct.  You may also
> consider emacs+ESS which includes possibilities to commenting and
> uncommenting large program blocks.
>
> I don't know where it is mentioned explicitly (I did not see it in "S
> Programming" (a book)), but most examples include comments, I guess.

It is stated in the "R Language Definition" manual, section 10.2, which
usually comes with the distribution.

Best wishes

Thomas






*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
titre de ce message s'il a ete altere, deforme ou falsifie.
				********
This message and any attachments (the "message") are confidentia... {{dropped}}



From shutnik_xx at yahoo.co.uk  Fri Jun  6 14:57:00 2003
From: shutnik_xx at yahoo.co.uk (=?iso-8859-1?q?Shutnik?=)
Date: Fri, 6 Jun 2003 13:57:00 +0100 (BST)
Subject: [R] R help: Correlograms
Message-ID: <20030606125700.21739.qmail@web10909.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030606/11b56a86/attachment.pl

From th50 at leicester.ac.uk  Fri Jun  6 15:05:37 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 6 Jun 2003 14:05:37 +0100
Subject: [R] R help: Correlograms
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E4699@saffron.cfs.le.ac.uk>

> I have time series and need to draw simple and partial 
> correlograms with associated Q-statistics (the same as in 
> EViews). Can I do it in R? Thanks

library(ts) contains functions acf and pacf which come with 
corresponding plot methods. See their help pages for details.

I don't know anything about Q-statistics, though.

HTH

Thomas



From usama1 at globalnet.com.eg  Fri Jun  6 15:21:33 2003
From: usama1 at globalnet.com.eg (osama)
Date: Fri, 6 Jun 2003 16:21:33 +0300
Subject: [R] MCD distance
Message-ID: <001f01c32c2e$97c08440$d8b59ed5@osama1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030606/d6ce617a/attachment.pl

From paulda at BATTELLE.ORG  Fri Jun  6 15:38:40 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 06 Jun 2003 09:38:40 -0400
Subject: [R] Contour plot question
Message-ID: <940250A9EB37A24CBE28D858EF077749136DE1@ws-bco-mse3.milky-way.battelle.org>

In a contour plot having numeric values displayed
next to the contours, is it possible to modify these
numeric values so that they are bolded, enlarged,
and/or placed differently?


Much thanks in advance,

David Paul, Ph.D.
Battelle Memorial Institute
614.424.3176



From sundar.dorai-raj at pdf.com  Fri Jun  6 15:49:04 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 06 Jun 2003 08:49:04 -0500
Subject: [R] converting "by" to a data.frame?
References: <Pine.A41.4.44.0306051523200.211048-100000@homer04.u.washington.edu>
	<3EDFC8E3.7020900@pdf.com>
Message-ID: <3EE09BD0.1000304@pdf.com>

Spencer,
   Would "sapply" be better here?

R> by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
R+                     B=rep(c("B1", "B2"), each=3),
R+                     x=1:6, y=rep(0:1, length=6))
R> t(sapply(split(by.df, do.call("paste", c(by.df[, 1:2], sep = ":"))),
R+          function(x) coef(lm(y ~ x, data = x))))
       (Intercept)             x
A1:B1   0.3333333 -1.517960e-16
A2:B2   0.6666667  3.282015e-16
R>

Sundar

Spencer Graves wrote:
> Hi, Thomas, et al.:
> 
> Thanks for the reply.  Unfortunately, "do.call" strips off the subset 
> identifiers, which I want to use for further modeling:
> 
>  > do.call("rbind", byFits)
>      (Intercept)              x
> [1,]   0.3333333 -1.517960e-016
> [2,]   0.6666667  3.282015e-016
> 
> The following does what I want using a "for" loop:
> 
>  > by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
> +  B=rep(c("B1", "B2"), each=3), x=1:6, y=rep(0:1, length=6))
>  > by.lvls <- paste(as.character(by.df$A), as.character(by.df$B), sep=":")
>  > A.B <- unique(by.lvls)
>  > Fits <- data.frame(A.B = A.B, .Intercept.=rep(NA, length(A.B)),
> +  x=rep(NA, length(A.B)))
>  > Fits$A <- substring(A.B, 1, regexpr(":", A.B)-1)
>  > Fits$B <- substring(A.B, regexpr(":", A.B)+1)
>  > for(i in 1:length(A.B))
> +  Fits[i, 2:3] <- coef(lm(y~x, by.df[by.lvls==A.B[i],]))
>  > Fits
>     A.B X.Intercept.             x  A  B
> 1 A1:B1    0.3333333 -1.517960e-16 A1 B1
> 2 A2:B2    0.6666667  3.282015e-16 A2 B2
>  >
> 
>       I wondered if there was something easier.
> 
> Thanks again for your reply.
> Spencer Graves
> 
> Thomas Lumley wrote:
> 
>> On Thu, 5 Jun 2003, Spencer Graves wrote:
>>
>>
>>> Dear R-Help:
>>>
>>>       I want to (a) subset a data.frame by several columns, (b) fit a 
>>> model
>>> to each subset, and (c) store a vector of results from the fit in the
>>> columns of a data.frame.  In the past, I've used "for" loops do do this.
>>>  Is there a way to use "by"?
>>>
>>>       Consider the following example:
>>>
>>> > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
>>> +  function(data.)coef(lm(y~x, data.)))
>>> > byFits
>>> A: A1
>>> B: B1
>>>   (Intercept)             x
>>>  3.333333e-01 -1.517960e-16
>>> ------------------------------------------------------------
>>> A: A2
>>> B: B1
>>> NULL
>>> ------------------------------------------------------------
>>> A: A1
>>> B: B2
>>> NULL
>>> ------------------------------------------------------------
>>> A: A2
>>> B: B2
>>>  (Intercept)            x
>>> 6.666667e-01 3.282015e-16
>>> >
>>> >
>>> #############################
>>> Desired output:
>>>
>>> data.frame(A=c("A1","A2"), B=c("B1", "B2"),
>>>     .Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
>>>
>>> What's the simplest way to do this?
>>
>>
>>
>> do.call("rbind", byFits)
>>
>>
>>     -thomas
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From murdoch at stats.uwo.ca  Fri Jun  6 16:06:50 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jun 2003 10:06:50 -0400
Subject: [R] Contour plot question
In-Reply-To: <940250A9EB37A24CBE28D858EF077749136DE1@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF077749136DE1@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <nt71evsrrhogid91tjrkrd7kbi9qrph08l@4ax.com>

On Fri, 06 Jun 2003 09:38:40 -0400, you wrote in message
<940250A9EB37A24CBE28D858EF077749136DE1 at ws-bco-mse3.milky-way.battelle.org>:

>In a contour plot having numeric values displayed
>next to the contours, is it possible to modify these
>numeric values so that they are bolded, enlarged,
>and/or placed differently?

See the ?contour.default help:  vfont can change the font (but it uses
Hershey fonts, you might not like those); labcex changes the size.  I
didn't notice an argument to affect placement.

Duncan Murdoch



From christoph.lehmann at gmx.ch  Fri Jun  6 16:07:49 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 06 Jun 2003 16:07:49 +0200
Subject: [R] understanding LDA: normalization of the eigenvectors
Message-ID: <1054908469.22326.78.camel@christophl>

Hi dear R-users

I try to reproduce the steps included in a LDA. In my textbook (Bortz)
it says, that the matrix with the eigenvectors 

V

usually are not normalized to the length of 1, but in the way that the
following holds (SPSS does the same thing):

t(Vstar)%*%Derror%*%Vstar = I


where Vstar are the normalized eigenvectors. Derror is an "error" or
"within" squaresum- and crossproduct matrix (squaresum of the p
variables on the diagonale, and the non-diagonal elements are the sum of
the crossproducts). For Derror the following holds: Dtotal = Dtreat +
Derror.

Since I assume that many of you are familiar with this transformation:
can anybody of you tell me, how to conduct this transformation in R?
Would be very nice. Thanks a lot

Cheers

Christoph

-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ligges at statistik.uni-dortmund.de  Fri Jun  6 16:20:21 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 Jun 2003 16:20:21 +0200
Subject: [R] Contour plot question
In-Reply-To: <nt71evsrrhogid91tjrkrd7kbi9qrph08l@4ax.com>
References: <940250A9EB37A24CBE28D858EF077749136DE1@ws-bco-mse3.milky-way.battelle.org>
	<nt71evsrrhogid91tjrkrd7kbi9qrph08l@4ax.com>
Message-ID: <3EE0A325.9030505@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On Fri, 06 Jun 2003 09:38:40 -0400, you wrote in message
> <940250A9EB37A24CBE28D858EF077749136DE1 at ws-bco-mse3.milky-way.battelle.org>:
> 
> 
>>In a contour plot having numeric values displayed
>>next to the contours, is it possible to modify these
>>numeric values so that they are bolded, enlarged,
>>and/or placed differently?
> 
> 
> See the ?contour.default help:  vfont can change the font (but it uses
> Hershey fonts, you might not like those); labcex changes the size.  I
> didn't notice an argument to affect placement.
> 
> Duncan Murdoch

?contour says:

<<method - character string specifying where the labels will be located. 
Possible values are "simple", "edge" and "flattest" (the default). See 
the Details section.>>

Uwe Ligges



From Bernhard.Pfaff at drkw.com  Fri Jun  6 17:15:59 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 6 Jun 2003 17:15:59 +0200
Subject: [R] R help: Correlograms
Message-ID: <18D602BD42B7E24EB810D6454A58DB90047303F1@ibfftce505.is.de.dresdnerkb.com>



Hello,

 
I have time series and need to draw simple and partial correlograms with
associated Q-statistics (the same as in EViews). Can I do it in R? Thanks


Hello Shutnik,

see ?Box.test in the ts-package for Q-statistics.

To my knowledge a plot function as in EViews (left-hand panel are ACFs and
PACFs as bars and right-hand panel are acfs, pacfs as values with Q-stats
and p-values) is not existent, hence you have to set up the plot by
yourself. In order, to do so, may be the following function might be of use
to you ("Ljung-Box"):

qstat <- function(x, order=12)
{
  if (order >= (length(x)-1))
    {
      print("Vector length falls short of autocorrelations to compute.")
    }
  else
    {
      x <- as.ts(na.omit(x))
      Total <- length(x)
      factor <- Total*(Total+2)
      xacf <- acf(x, lag=(order), plot=FALSE)
      denom <- rep(Total, order)-seq(1, order)
      Lag <- seq(1, order)
      tausq <- xacf$acf[2:length(xacf$acf)]^2/denom
      Q <- 0[1:order]
      temp <- 0[1:order]
      for(i in 1:order)
        {
          temp[i] <- sum(tausq[1:i])
        }
      Qstat <- factor*temp
      pval <- 1 - pchisq(Qstat,i)
      Ljung.Box <- cbind(Lag, Qstat, pval)
      Ljung.Box
    }
}
#
y <- 1:100
qstat(y, order=10)


HTH,
Bernhard



----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From ndey00 at yahoo.com  Fri Jun  6 17:41:00 2003
From: ndey00 at yahoo.com (N Dey)
Date: Fri, 6 Jun 2003 08:41:00 -0700 (PDT)
Subject: [R] little manipulation  on data frame
Message-ID: <20030606154100.63292.qmail@web41313.mail.yahoo.com>

Dear all,

I have data like 3 coulmns and many rows. Each entry
is less than 10.

Example
	x	y	z
1	5	3	2	
2	3	7	8
3	8	9	5
4	5	4	6
--------------------------
---------------------------

I have to sum entries of each coulmn (seperately) till
it be 10. This i have to start for each row. And I
want to assign no. of rows needed including that row
too(it to be 10 or 10+, the moment it exceeds 10, i
need to stop and count the no. of rows)in additional
coulmns say N1 (corresponding to coulmn x), N2 (y) and
N3 (z).


I want my new table like

	x	y	z	N1	N2	N3 
1	5	3	2	3	2	2
2	3	7	8	2	2	2	
3	8	9	5	2	2	2
4	5	4	6	depends upon next row


If anybody knows it, please help me.

Thanking you.

Best regards,
N. Dey.



From RBaskin at ahrq.gov  Fri Jun  6 18:04:47 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Fri, 6 Jun 2003 12:04:47 -0400 
Subject: [R] Introductory Resources
Message-ID: <3598558AD728D41183350008C7CF291C0F16B7F6@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030606/ecc9f978/attachment.pl

From spencer.graves at pdf.com  Fri Jun  6 18:20:11 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Jun 2003 09:20:11 -0700
Subject: [R] little manipulation  on data frame
References: <20030606154100.63292.qmail@web41313.mail.yahoo.com>
Message-ID: <3EE0BF3B.8050009@pdf.com>

I don't completely understand what you want, but might the following help?

 > cumsum(1:11)
  [1]  1  3  6 10 15 21 28 36 45 55 66
 > which(cumsum(1:11)>9)
[1]  4  5  6  7  8  9 10 11
 > which(cumsum(1:11)>9)[1]
[1] 4

hth.  spencer graves

N Dey wrote:
> Dear all,
> 
> I have data like 3 coulmns and many rows. Each entry
> is less than 10.
> 
> Example
> 	x	y	z
> 1	5	3	2	
> 2	3	7	8
> 3	8	9	5
> 4	5	4	6
> --------------------------
> ---------------------------
> 
> I have to sum entries of each coulmn (seperately) till
> it be 10. This i have to start for each row. And I
> want to assign no. of rows needed including that row
> too(it to be 10 or 10+, the moment it exceeds 10, i
> need to stop and count the no. of rows)in additional
> coulmns say N1 (corresponding to coulmn x), N2 (y) and
> N3 (z).
> 
> 
> I want my new table like
> 
> 	x	y	z	N1	N2	N3 
> 1	5	3	2	3	2	2
> 2	3	7	8	2	2	2	
> 3	8	9	5	2	2	2
> 4	5	4	6	depends upon next row
> 
> 
> If anybody knows it, please help me.
> 
> Thanking you.
> 
> Best regards,
> N. Dey.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From pocernic at rap.ucar.edu  Fri Jun  6 18:28:09 2003
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Fri, 6 Jun 2003 10:28:09 -0600 (MDT)
Subject: [R] layout problem
Message-ID: <Pine.LNX.4.44.0306061019080.4036-100000@markov.rap.ucar.edu>

Hello,

I have a question about using the layout command within a function.  I've
written function that uses layout to create a figure from 2 plots.  This
works fine to create a figure.  When I use par(mfrow = c(2,2)) to create multiple
 plots, it seems that the layout command resets the mfrow parmeter.
Is there a way for me to avoid this problem?

For example

practice<- function() {
op<- par(oma=c(3,3,5,4))
nn<- layout(matrix(c(0,1), nrow = 1), widths = c(1,4))
plot(0,0)
mtext("Sample graph", outer = TRUE, line = 1, cex = 2)

par(op)
}

practice() # works fine.

par(mfrow = c(2,2))

practice()
practice()
practice()
practice()

## produces 4 graphs on seperate pages.

Thanks.

Matt

Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From dgrove at fhcrc.org  Fri Jun  6 18:29:21 2003
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 6 Jun 2003 09:29:21 -0700 (PDT)
Subject: [R] Kmeans again
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEA5E@MAIL.fc.up.pt>
Message-ID: <Pine.LNX.4.44.0306060924370.23810-100000@echidna.fhcrc.org>

> I'm sorry to insist but I still think there is something wrong with the function kmeans. For instance, let's try the same small example:
>  
> > dados<-matrix(c(-1,0,2,2.5,7,9,0,3,0,6,1,4),6,2)
> 
> I will choose observations 3 and 4 for initial centers and just one iteration. The results are
>  
> > A<-kmeans(dados,dados[c(3,4),],1)
> > A
> $cluster
> [1] 1 1 1 1 2 2
> $centers
>    [,1] [,2]
> 1 0.875 2.75
> 2 8.000 2.50
> $withinss
> [1] 38.9375  6.5000
> $size
> [1] 4 2
>  
> If I do it by hand, after one iteration, the results are
>  
> $cluster
> [1] 1 2 1 2 1 2
>  
> So I think that something is wrong with the function kmeans; probably the initial centers given
>  by the user are not being taken into account.


Andy Liaw already gave an example where he specified two different starting 
values and Kmeans gave different results after 1 iteration, so clearly 
your hypothesis is incorrect.

Either your calculations are wrong or you are calculating the wrong
formulae.  It is very doubtful that anything is wrong with Kmeans.

Doug Grove



From th50 at leicester.ac.uk  Fri Jun  6 18:38:47 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 6 Jun 2003 17:38:47 +0100
Subject: [R] layout problem
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E469C@saffron.cfs.le.ac.uk>

> I have a question about using the layout command within a 
> function.  I've
> written function that uses layout to create a figure from 2 
> plots.  This
> works fine to create a figure.  When I use par(mfrow = 
> c(2,2)) to create multiple
>  plots, it seems that the layout command resets the mfrow parmeter.
> Is there a way for me to avoid this problem?

As far as I understand that is not possible, since 
"`layout' divides the device up", not a part of the device.

If you need to split your device again and again, you should
package grid (or lattice, on a higher level); these allow to do so.
However, these need different plot commands, i.e. plot() will not
work with them

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From spencer.graves at pdf.com  Fri Jun  6 18:44:56 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Jun 2003 09:44:56 -0700
Subject: [R] Introductory Resources
References: <3598558AD728D41183350008C7CF291C0F16B7F6@exchange1.ahrq.gov>
Message-ID: <3EE0C508.7060609@pdf.com>

	  I've been using S-Plus almost daily for roughly 7 years and testing R 
daily for the past 4 months.  In my opinion, roughly 90% of the 
differences I've seen are in R's favor, e.g., the "log" options in the 
probability functions.

	  I think the future lies with R, because I guessing that much of the 
most advanced algorithm development must be in R:  I can modify an R 
function and give it to the world;  if others find my modification 
useful, I get recognized for having made a contribution to science and 
humanity.  If I do that with an S-Plus function, I'm a thief, because 
I've stolen Insightful's intellectual property.

	  In brief, I believe that R is already superior to S-Plus, and the 
difference is increasing.  I'm telling my collaborators who use S-Plus 
that our new software development should be tested in both R and S-Plus 
so we can switch whenever we need to.

Hope this helps.
Spencer Graves
p.s.  You'll find much more discussion of this by searching the r-news 
archives, "http://www.r-project.org/" -> Search -> "R Search Site".

RBaskin at ahrq.gov wrote:
>>I am interested in R as an alternative for a statistical tool 
>>at our firm.
> 
> Ditto...
> 
> I have recently moved to this agency from a company where I had access to
> Splus.  There is also a coworker here who had used Splus at a previous
> employer.  We both would like some access to the S language.  We are
> considering either begging loud and long to try to get the agency to
> purchase two copies of Splus or trying to convert to R.  This is not an easy
> decision under the circumstances and purchasing Splus will mean giving up
> something else.  On the other hand I have never used R before and I fear the
> learning curve for using R plus possibly ESS.
> 
> There are a few things I am trying to determine before I really decide what
> to do.  I have been trying to convert some of my old Splus script files at
> home to run under R 1.7.0.  Small (less than 50 lines or so) script files
> that I have tested run exactly as before.  I tried to run one large
> simulation and it took about a week of screaming hell to get the error
> messages out (well all but one error message anyway).
> 
> 
> 1) I want a test suite for R.  I noted in the messages (Date: Mon Feb 24
> 2003 - 22:18:03 EST) that Prof Ripley wrote "Well, R itself has lots of
> tests in its test suite (see directory tests in the sources) packages..."
> but I was too stupid to find them.  
> Q1: Can someone provide directions to this test suite that even an idiot can
> follow?
> 
> 
> 2) Most of the problems I ran into had to do with missing values (in effect
> I have ragged arrays).  One silly example is that I had made use of which.na
> 'which' apparently is not defined in R 1.7.0.  There are multiple
> workarounds such as simply defining a function which.na but of course it
> would be untested and you can loop back to 1).  The problem in my script
> files is the same I had in Splus.  I want the defaults on all of the
> functions (such as mean, median, etc.) I am using to be reset GLOBALLY so
> that the default is to ignore missing values or not. 
> Q2: Can the missing defaults be set globally for all functions.  In other
> words, I want the default for how to treat NAs in all functions to be set at
> startup.
> 
> 
> 3)  What I really want to do is pass a function name and extra arguments to
> another function.  For example, in Splus, you can pass a function such as
> median to the bootstrap function.  The bootstrap function says that you can
> pass arguments to the median function through the bootstrap function but
> unfortunately I could never make this work.  This functionality would
> probably solve most of my NA problems if I could make it work.  (I don't
> seem to be able to properly use the ellipses:)
> Pseudo-Example: The Splus bootstrap can be called as
> Bootstrap(variable-name, median, sampler=sample-function, na.rm=T)
> But I never figured out how to pass the na.rm=T as an argument to median so
> that the function being bootstrapped is median(variable-name, na.rm=T).
> Q3: Is there some way in R to pass alternative arguments through a function
> to another?
> 
> 4)    Any general thoughts on Splus versus R that you are willing to share?
> 
> 
> This is way too much blithering for one day but thanks in advance for any
> thoughts.
> 
> Bob Baskin
> All the usual disclaimers that my statements don't represent the agency etc.
> etc.
> 
> 
> 
> 
> -----Original Message-----
> From: Marc Schwartz [mailto:mschwartz at medanalytics.com] 
> Sent: Thursday, June 05, 2003 1:19 PM
> To: 'Fohr, Marc [AM]'; 'R-help at lists.R-project.org'
> Subject: RE: [R] Introductory Resources
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fohr, Marc
> 
> [AM]
> 
>>Sent: Thursday, June 05, 2003 11:46 AM
>>To: 'R-help at lists.R-project.org'
>>Subject: [R] (no subject)
>>
>>
>>Hello,
>>
>>I am interested in R as an alternative for a statistical tool 
>>at our firm. I
>>do know RATS an SPSS but not S+. As I read that R is close to 
>>S+, I would
>>like to know if you could recommend me any books as an 
>>introduction to S+ or
>>R.
>>
>>Best regards
>>
>>Marc
> 
> 
> 
> Marc,
> 
> Reviewing R FAQs 2.7 and 3.x on the main R site would be a good place
> to start. The former lists books and other documents (some online)
> that serve as excellent introductions, while the latter helps to
> differentiate R and S/S+.
> 
> Spending some time with those references and the R FAQs will serve as
> a good foundation, with keyword searches of the R-help list archive
> serving as an additional strong resource.
> 
> HTH,
> 
> Another Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Fri Jun  6 19:06:55 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 6 Jun 2003 10:06:55 -0700 (PDT)
Subject: [R] little manipulation  on data frame
In-Reply-To: <20030606154100.63292.qmail@web41313.mail.yahoo.com>
Message-ID: <Pine.A41.4.44.0306060928150.82226-100000@homer33.u.washington.edu>

On Fri, 6 Jun 2003, N Dey wrote:

> Dear all,
>
> I have data like 3 coulmns and many rows. Each entry
> is less than 10.
>
> Example
> 	x	y	z
> 1	5	3	2
> 2	3	7	8
> 3	8	9	5
> 4	5	4	6
> --------------------------
> ---------------------------
>
> I have to sum entries of each coulmn (seperately) till
> it be 10. This i have to start for each row. And I
> want to assign no. of rows needed including that row
> too(it to be 10 or 10+, the moment it exceeds 10, i
> need to stop and count the no. of rows)in additional
> coulmns say N1 (corresponding to coulmn x), N2 (y) and
> N3 (z).
>
>
> I want my new table like
>
> 	x	y	z	N1	N2	N3
> 1	5	3	2	3	2	2
> 2	3	7	8	2	2	2
> 3	8	9	5	2	2	2
> 4	5	4	6	depends upon next row
>

It depends a bit on how many is `many'.

You can get cumulative sums with cumsum, and the first entry in each
column is then

    min(which(cumsum(x) > 10))

The i+1th entry is
    min(which( cumsum(x)-cumsum(x)[-(1:i)] > 10))

If the number of rows is not very large I would do

   sumx<-cumsum(x)
   N1<-min(which(cumsum(x) > 10))
   N1<-c(N1, sapply(1:(length(x)-1), function(i)
		min(which(sumx[-(1:i)]-sumx[i]>10))))

or the equivalent for() loop.

If the number of rows is very large it would be more efficient to rely on
the fact that no more than 10 rows are needed (assuming that zeros aren't
possible)

   n<-length(x)
   sumx<-cumsum(x)

   sumlags<-matrix(nrow=n,ncol=10)
   for(i in 0:9)
	sumlags[,i+1]<-sumx[ c((i+1):n, rep(n,i))]

   N1<-rowSums(sumlags < c(0,sumx[1:(n-1)])+10)+1


	-thomas



From murdoch at stats.uwo.ca  Fri Jun  6 19:51:15 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jun 2003 13:51:15 -0400
Subject: [R] Introductory Resources
In-Reply-To: <3598558AD728D41183350008C7CF291C0F16B7F6@exchange1.ahrq.gov>
References: <3598558AD728D41183350008C7CF291C0F16B7F6@exchange1.ahrq.gov>
Message-ID: <a9j1ev8kibn7e89643n4gbqr883r34cg6p@4ax.com>

On Fri, 6 Jun 2003 12:04:47 -0400 , you wrote in message
<3598558AD728D41183350008C7CF291C0F16B7F6 at exchange1.ahrq.gov>:

>1) I want a test suite for R.  I noted in the messages (Date: Mon Feb 24
>2003 - 22:18:03 EST) that Prof Ripley wrote "Well, R itself has lots of
>tests in its test suite (see directory tests in the sources) packages..."
>but I was too stupid to find them.  
>Q1: Can someone provide directions to this test suite that even an idiot can
>follow?

You need to download the source code to R from CRAN (e.g.
cran.mirrors.pair.com), it's not in the precompiled binaries.  You'll
find the tests in the "tests" subdirectory.  They consist of pairs of
foo.R and foo.Rout.save files.  When you run the tests via "make
check", foo.Rout will be produced, and you'll be warned about
differences from foo.Rout.save.

Testing also runs almost all of the examples in every help file, and
aborts if any generate errors (or warnings, I forget just now...).
Package writers can include their own test scripts and saved output
files.

>Q2: Can the missing defaults be set globally for all functions.  In other
>words, I want the default for how to treat NAs in all functions to be set at
>startup.

option(na.action=<some function>) will control how all well-behaved
functions handle NAs, but not all functions are well-behaved.
Complain about those that should be but aren't (to the package
maintainer in case of a contributed package, here or to r-bugs in case
of a built-in package).

>3)  What I really want to do is pass a function name and extra arguments to
>another function.  For example, in Splus, you can pass a function such as
>median to the bootstrap function.  The bootstrap function says that you can
>pass arguments to the median function through the bootstrap function but
>unfortunately I could never make this work.  This functionality would
>probably solve most of my NA problems if I could make it work.  (I don't
>seem to be able to properly use the ellipses:)
>Pseudo-Example: The Splus bootstrap can be called as
>Bootstrap(variable-name, median, sampler=sample-function, na.rm=T)
>But I never figured out how to pass the na.rm=T as an argument to median so
>that the function being bootstrapped is median(variable-name, na.rm=T).

>Q3: Is there some way in R to pass alternative arguments through a function
>to another?

Yes, I think this is similar in R and S-PLUS, so you might run into
the same problems.  Generally it's easy using ellipses, but if the
argument name for your function happens to be the prefix of an
argument name for the Bootstrap function, then you'll get a match
there, instead of where you want it.  

For example, in R bootstrapping can be done using the function boot()
in library(boot).  It has arguments:

 boot(data, statistic, R, sim="ordinary", stype="i", 
          strata=rep(1,n), L=NULL, m=0, weights=NULL, 
          ran.gen=function(d, p) d, mle=NULL, ...)

The ellipses will pass other arguments to the function passed as the
statistic argument, but if you named them "str" they'd match "strata"
instead of being passed to "...".  Thus

 boot(vname, median, na.rm=T)

should work, but 

 myfunc <- function(x, str) median(x, na.rm=str)

 boot(vname, myfunc, str=T)

would not.


>4)    Any general thoughts on Splus versus R that you are willing to share?

S-PLUS has a more polished GUI.  R is more responsive to bug reports.

Duncan Murdoch



From liuwensui at hotmail.com  Fri Jun  6 20:15:39 2003
From: liuwensui at hotmail.com (wensui liu)
Date: Fri, 6 Jun 2003 14:15:39 -0400
Subject: [R] sas vs. r
Message-ID: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030606/26108bd2/attachment.pl

From RBaskin at ahrq.gov  Fri Jun  6 20:20:48 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Fri, 6 Jun 2003 14:20:48 -0400 
Subject: [R] Introductory Resources
Message-ID: <3598558AD728D41183350008C7CF291C0F16B7F8@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030606/b45bb3aa/attachment.pl

From mschwartz at medanalytics.com  Fri Jun  6 20:32:24 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 6 Jun 2003 13:32:24 -0500
Subject: [R] sas vs. r
In-Reply-To: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com>
Message-ID: <012e01c32c59$fa7b1e30$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of wensui liu
>Sent: Friday, June 06, 2003 1:16 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] sas vs. r
>
>
>I like R more than SAS. My job is doing research on clinical 
>trial. But I was told that FDA only accepts the result from 
>SAS. Is that true? TOO BAD.


See these recent posts by Frank Harrell:

http://maths.newcastle.edu.au/~rking/R/help/03a/4210.html

http://maths.newcastle.edu.au/~rking/R/help/03a/4231.html

http://maths.newcastle.edu.au/~rking/R/help/03a/4246.html


HTH,

Marc Schwartz



From spencer.graves at pdf.com  Fri Jun  6 20:44:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Jun 2003 11:44:04 -0700
Subject: [R] sas vs. r
References: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com>
Message-ID: <3EE0E0F4.4000802@pdf.com>

	  Please check "http://www.r-project.org/" -> search -> "R site 
search".  My search just now for "R vs. SAS for FDA" produced 14 matches.

	  My bottom line from reading comments on this issue is that it is like 
the comment 20 years ago that, "Nobody in management ever got fired for 
buying IBM."  The "r-help" archives include comments that FDA has no 
specific guidelines for what software must be used, although there are 
doubtless general guidelines relating to documentation and testing that 
it does what it claims.

hth.  spencer graves

wensui liu wrote:
> I like R more than SAS. My job is doing research on clinical trial. But I was told that FDA only accepts the result from SAS. Is that true? TOO BAD.
> 
> 
> 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Fri Jun  6 20:46:57 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 6 Jun 2003 11:46:57 -0700 (PDT)
Subject: [R] sas vs. r
In-Reply-To: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com>
Message-ID: <Pine.A41.4.44.0306061126110.82226-100000@homer33.u.washington.edu>

On Fri, 6 Jun 2003, wensui liu wrote:

> I like R more than SAS. My job is doing research on clinical trial. But
> I was told that FDA only accepts the result from SAS. Is that true? TOO
> BAD.
>

No, it isn't true.  The FDA does not approve or certify statistical
software. Its main regulation relevant to statistical analysis is in 21
CFR 11, and is largely about audit trails.  In fact the FDA is very
reluctant to issue blanket approval or disapproval for anything -- it
likes to keep its options open.


On the other hand, if a company has been successful in getting FDA
approval with analyses done one way, there is a substantial incentive not
to change. Why take the risk that it might slow down the approval of your
application?


	-thomas



From rossini at blindglobe.net  Fri Jun  6 20:29:02 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 06 Jun 2003 11:29:02 -0700
Subject: [R] sas vs. r
In-Reply-To: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com> (wensui liu's message
	of "Fri, 6 Jun 2003 14:15:39 -0400")
References: <Sea2-DAV55leUBUhr3u000037b1@hotmail.com>
Message-ID: <87llwfcb69.fsf@jeeves.blindglobe.net>

"wensui liu" <liuwensui at hotmail.com> writes:

> I like R more than SAS. My job is doing research on clinical
> trial. But I was told that FDA only accepts the result from SAS. Is
> that true? TOO BAD.

No.  But it's not that simple.

See previous postings (within the last 2 months) on validation and
FDA. 

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
Biomedical/Health Informatics and Biostatistics, University of Washington.
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From f.mattes at ucl.ac.uk  Fri Jun  6 21:10:00 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Fri, 6 Jun 2003 20:10:00 +0100
Subject: [R] losing color in panal.xyplot (lattice)
Message-ID: <p05200f01bb06954be9cf@[128.40.218.142]>

Dear R help-list reader,

I have a grouping variable C<-c(1,2,3) which gives me the plot 
symbols in a xyplot (lattice library) in three different colors, 
according the group variable.

xyplot(A ~ B), group=factor(C))

if I now apply the panel.xyplot / panel.loess all the colors are 
changing back to red.

xyplot(A ~ B), group=factor(C)),
panel = function(x, y) {
            panel.grid(h=-1, v= 2)
            panel.xyplot(x, y)
            panel.loess(x,y, span=1)
        },
aspect = "xy")

I'm wondering if anyone knows a easy way how I could retain the 
colors of the plot symbols according to the grouping variable.

Yours
Frank
-- 
Frank Mattes, MD			e-mail:	f.mattes at ucl.ac.uk
Department of Virology			fax	0044(0)207 8302854
Royal Free Hospital and 			tel	0044(0)207 8302997
University College Medical School
London



From chris at fisher.forestry.uga.edu  Fri Jun  6 21:35:01 2003
From: chris at fisher.forestry.uga.edu (Chris Fonnesbeck)
Date: Fri, 6 Jun 2003 15:35:01 -0400
Subject: [R] RMySQL on OS X
Message-ID: <F6EF62D6-9855-11D7-B288-000A956FDAC0@fisher.forestry.uga.edu>

I noticed in the collection of contributed packages for the OSX release 
of R, there is no RMySQL package. Does anyone know if one exists, or 
otherwise, how to connect to relational databases from R on Mac? Any 
help mode appreciated.

cjf



From macq at llnl.gov  Fri Jun  6 22:47:21 2003
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 6 Jun 2003 13:47:21 -0700
Subject: [R] RMySQL on OS X
In-Reply-To: <F6EF62D6-9855-11D7-B288-000A956FDAC0@fisher.forestry.uga.edu>
References: <F6EF62D6-9855-11D7-B288-000A956FDAC0@fisher.forestry.uga.edu>
Message-ID: <p05210609bb06ad90df93@[128.115.153.6]>

In my installation of R on OS X, RMySQL is installed.

>  tmp <- installed.packages()

>  tmp[tmp[,'Package']=='RMySQL',]
                                 Package
                                "RMySQL"
                                 LibPath
              "/usr/local/lib/R/library"
                                 Version
                                 "0.5-1"
                                Priority
                                      NA
                                  Bundle
                                      NA
                                 Depends
"R (>= 1.6.0), methods, DBI (>= 0.1-4)"

 From where did you get your list of "contributed packages for the OSX 
release of R"?

-Don

At 3:35 PM -0400 6/6/03, Chris Fonnesbeck wrote:
>I noticed in the collection of contributed packages for the OSX 
>release of R, there is no RMySQL package. Does anyone know if one 
>exists, or otherwise, how to connect to relational databases from R 
>on Mac? Any help mode appreciated.
>
>cjf
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From den.duurs at lycos.com  Fri Jun  6 23:20:27 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Fri, 06 Jun 2003 14:20:27 -0700
Subject: [R] small plot inside a big plot
Message-ID: <MGDNOEMIBDGLLDAA@mailcity.com>

Dear R-helpers,

i want to draw a small plot (histogram) within a larger plot (simple scatterplot), so that the axes of the bigger plot remain intact.

I know how to use layout() and par(mfrow...) and such, but I want the smaller graph to be *inside* the bigger plot. Is this possible?

thanks,

Remko



From rpeng at stat.ucla.edu  Fri Jun  6 23:59:12 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Fri, 06 Jun 2003 14:59:12 -0700
Subject: [R] small plot inside a big plot
In-Reply-To: <MGDNOEMIBDGLLDAA@mailcity.com>
References: <MGDNOEMIBDGLLDAA@mailcity.com>
Message-ID: <3EE10EB0.3050804@stat.ucla.edu>

For a histogram, you might use `add = TRUE' in the call to hist().

-roger

Remko Duursma wrote:

> Dear R-helpers,
> 
> i want to draw a small plot (histogram) within a larger plot (simple scatterplot), so that the axes of the bigger plot remain intact.
> 
> I know how to use layout() and par(mfrow...) and such, but I want the smaller graph to be *inside* the bigger plot. Is this possible?
> 
> thanks,
> 
> Remko
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From den.duurs at lycos.com  Sat Jun  7 00:55:48 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Fri, 06 Jun 2003 15:55:48 -0700
Subject: [R] small plot inside a big plot
Message-ID: <NILOMKJIFGLLLDAA@mailcity.com>

Actually, that will not work. I want the histogram to be much smaller than the plot to which i want to add it, I don't want to overlay it, instead i want to have a small box in the whitespace of the scatterplot containing the histogram.

Remko



^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.

--------- Original Message ---------

DATE: Fri, 06 Jun 2003 14:59:12
From: "Roger D. Peng" <rpeng at stat.ucla.edu>
To: den.duurs at lycos.com
Cc: rhelp <r-help at r-project.org>

>For a histogram, you might use `add = TRUE' in the call to hist().
>
>-roger
>
>Remko Duursma wrote:
>
>> Dear R-helpers,
>> 
>> i want to draw a small plot (histogram) within a larger plot (simple scatterplot), so that the axes of the bigger plot remain intact.
>> 
>> I know how to use layout() and par(mfrow...) and such, but I want the smaller graph to be *inside* the bigger plot. Is this possible?
>> 
>> thanks,
>> 
>> Remko
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> 
>> 
>
>



From MAILER-DAEMON at stat.math.ethz.ch  Fri Jun  6 16:33:42 2003
From: MAILER-DAEMON at stat.math.ethz.ch (Mail Delivery Subsystem)
Date: Fri, 6 Jun 2003 16:33:42 +0200 (METDST)
Subject: [R] Warning: could not send message for past 4 hours
Message-ID: <200306061433.PAC14218@smtp.cgi.rupa.it>

------------------  Virus Warning Message (on casms002)

Found virus WORM_SOBIG.C in file 45443.pif
The file 45443.pif is moved to /etc/iscan2/virus/virJSCa11235.

---------------------------------------------------------

From Richard.Rowe at jcu.edu.au  Sat Jun  7 01:52:26 2003
From: Richard.Rowe at jcu.edu.au (Richard Rowe)
Date: Sat, 07 Jun 2003 09:52:26 +1000
Subject: [R] small plot inside a big plot
In-Reply-To: <NILOMKJIFGLLLDAA@mailcity.com>
Message-ID: <5.0.0.25.1.20030607094948.03051650@pop.jcu.edu.au>

At 15:55 06/06/03 -0700, you wrote:
>Actually, that will not work. I want the histogram to be much smaller than 
>the plot to which i want to add it, I don't want to overlay it, instead i 
>want to have a small box in the whitespace of the scatterplot containing 
>the histogram.
>
>Remko

There may be an elegant solution, but if you want the output now scissors 
and paste will solve the problem in less than two minutes ...

Electronic presentation tools have similar capabilities,



Richard Rowe
Senior Lecturer
Department of Zoology and Tropical Ecology, James Cook University
Townsville, Queensland 4811, Australia
fax (61)7 47 25 1570
phone (61)7 47 81 4851
e-mail: Richard.Rowe at jcu.edu.au
http://www.jcu.edu.au/school/tbiol/zoology/homepage.html



From William.M.Grove-1 at tc.umn.edu  Sat Jun  7 04:16:33 2003
From: William.M.Grove-1 at tc.umn.edu (William M. Grove)
Date: Fri, 06 Jun 2003 21:16:33 -0500
Subject: [R] problem with predict() for gam() models
Message-ID: <5.1.0.14.0.20030606210245.00b0f478@grove001.email.umn.edu>

I run the following code in R 1.6.2 on Windows:

xxx <- rnorm(100)
yyy <- .5 * rnorm(100) + sqrt(1-.5^2) * rnorm(100)
ord <- order(xxx)
xxx <- xxx[ord] # for
yyy <- yyy[ord] #    convenience in reading printout
rm(ord)
reg.gam <- gam(yyy ~ s(xxx, k=8))

f <- function(x, reg.gam, target.y) {
    cat("inside f() called by optimize():\n")
    cat("arg x=", x, "\n")
    cat("arg target.y=", target.y, "\n", sep="")
    pred.y <- predict(reg.gam, newdata=data.frame(constant=1, xxx=x))
    cat("predicted y=\n")
    print(pred.y)
    sq.diff <- (pred.y - target.y)^2
    cat("returned value=\n")
    print(sq.diff)
    return( sq.diff )
}

temp <- optimize(f, interval=c(xxx[1], xxx[100]), reg.gam=reg.gam, 
target.y=mean(range(yyy)))

as a try-out of some code.  I need to interpolate in the output of a gam() 
model, to find a certain x-value corresponding to a chosen y-value.  I 
thought it would be better to interpolate using predict() from the gam() 
model, rather than use approx() since I don't expect linearity, or anything 
that's necessarily very close to it, in the part of the regression where 
I'm going to need to interpolate.  Since the regression itself is on 
splines, I didn't think I needed to call spline interpolation; I thought

In my real-life application the regression isn't linear, like the one I 
constructed here (it looks somwhat, but not quite enough, like a logistic 
regression), but I thought it would simplify my example to construct do 
things this way.

Alas, with these data (or my real, nonlinear, data) I get an error from 
optimize() (either with these fake data, or with my real data) because f() 
delivers a vector-valued result.  The debugging print statements inside f() 
show that sometimes (but not nearly always) even though a scalar argument x 
is submitted and put into the newdata to predict(), I get a vector result 
pred.y[].  Note:  most of the time, I get a scalar valued result, but often 
enough I get a vector output from f().  I don't at this time see what 
differentiates arguments that yield scalar result for f(), from ones that 
yield a vector value for f().  x the argument to f() is always scalar.

The problem occurs whether I use newdata=data.frame(xxx=x) (i.e., minus the 
"constant=1" part, or newdata as shown, as an argument to predict().

Can anyone help me understand why this is happening, and how to ensure that 
the result of f() is scalar, so I get the interpolation I want, instead of 
having optimize() blow up?

Thanks in advance for any advice.  Regards,



From ligges at statistik.uni-dortmund.de  Sat Jun  7 12:22:20 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jun 2003 12:22:20 +0200
Subject: [R] small plot inside a big plot
References: <MGDNOEMIBDGLLDAA@mailcity.com>
Message-ID: <3EE1BCDC.DE8DD0F3@statistik.uni-dortmund.de>

Remko Duursma wrote:
> 
> Dear R-helpers,
> 
> i want to draw a small plot (histogram) within a larger plot (simple scatterplot), so that the axes of the bigger plot remain intact.
> 
> I know how to use layout() and par(mfrow...) and such, but I want the smaller graph to be *inside* the bigger plot. Is this possible?
> 
> thanks,
> 
> Remko

You might want to play with the argument "plt" in par(), see ?par for
details. Example:

 plot(1:10)
 par("plt" = c(0.6, 0.9, 0.3, 0.5))
 par(new = TRUE)
 hist(rnorm(10))


layout() might also help, if your "small" plot is somewhere within a
grid:

 plot(1:10)
 layout(matrix(c(0,0,0,1), ncol=2))
 par(new = TRUE, oma = par("mar"))
 hist(rnorm(10))

 
Or look into package "grid" for more sophisticated but (from my point of
view) less easy solutions.

Uwe Ligges



From gisar at nus.edu.sg  Sat Jun  7 12:31:22 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Sat, 7 Jun 2003 18:31:22 +0800
Subject: [R] Error Compiling e1071
Message-ID: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>

Dear all,

I am trying to compile the package e1071 (version 1.3-11) with R CMD
INSTALL. I tried with R 1.7.0 on Redhat Linux 2.4.7-10 and R 1.6.2 on
Linux 2.4.9-34smp but keep getting the same error message during
configure :

WARNING: g++ 2.96 cannot reliably be used with this package. Please use
a different compiler.

Can anyone help me with this or at least point me in the right direction
? Thank you very much.

Regards, Adai.



From edd at debian.org  Sat Jun  7 15:04:31 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 7 Jun 2003 08:04:31 -0500
Subject: [R] Error Compiling e1071
In-Reply-To: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>
References: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>
Message-ID: <20030607130431.GA21117@sonny.eddelbuettel.com>

On Sat, Jun 07, 2003 at 06:31:22PM +0800, Adaikalavan Ramasamy wrote:
> I am trying to compile the package e1071 (version 1.3-11) with R CMD
> INSTALL. I tried with R 1.7.0 on Redhat Linux 2.4.7-10 and R 1.6.2 on
> Linux 2.4.9-34smp but keep getting the same error message during
> configure :
> 
> WARNING: g++ 2.96 cannot reliably be used with this package. Please use
> a different compiler.
> 
> Can anyone help me with this or at least point me in the right direction
> ? Thank you very much.

<sarcasm>
Which part of "use a different compiler" don't you understand ?
</sarcasm>

Seriously, RH earned a lot bad press with the gcc 2.96 release. You are
currently experiencing why: it didn't work that well.  Do yourself a favour
and upgrade either your compiler, or your RH version.  Or go Debian :)

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From ligges at statistik.uni-dortmund.de  Sat Jun  7 15:08:19 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jun 2003 15:08:19 +0200
Subject: [R] Error Compiling e1071
In-Reply-To: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>
References: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>
Message-ID: <3EE1E3C3.5070200@statistik.uni-dortmund.de>

Adaikalavan Ramasamy wrote:
> Dear all,
> 
> I am trying to compile the package e1071 (version 1.3-11) with R CMD
> INSTALL. I tried with R 1.7.0 on Redhat Linux 2.4.7-10 and R 1.6.2 on
> Linux 2.4.9-34smp but keep getting the same error message during
> configure :
> 
> WARNING: g++ 2.96 cannot reliably be used with this package. Please use
> a different compiler.
> 
> Can anyone help me with this or at least point me in the right direction
> ? Thank you very much.
> 
> Regards, Adai.

As the warning says: "Please use a different compiler", e.g. GCC 3.3 or 
GCC 3.2.3 or much more current.

Uwe Ligges



From meyer at ci.tuwien.ac.at  Sat Jun  7 15:22:47 2003
From: meyer at ci.tuwien.ac.at (David Meyer)
Date: Sat, 7 Jun 2003 15:22:47 +0200 (CEST)
Subject: [R] Error Compiling e1071
In-Reply-To: <024D6AEFCB92CB47BA1085751D184BB80105F26F@MBXSRV03.stf.nus.edu.sg>
Message-ID: <Pine.LNX.4.21.0306071517460.18348-100000@boromir.ci.tuwien.ac.at>

> 
> I am trying to compile the package e1071 (version 1.3-11) with R CMD
> INSTALL. I tried with R 1.7.0 on Redhat Linux 2.4.7-10 and R 1.6.2 on
> Linux 2.4.9-34smp but keep getting the same error message during
> configure :
> 
> WARNING: g++ 2.96 cannot reliably be used with this package. Please use
> a different compiler.
> 
> Can anyone help me with this or at least point me in the right direction
> ? Thank you very much.

We added this warning because g++ 2.96 breaks the C++ code of `libsvm'
contained in the `e1071' package. If you don't use SVMs, you might
interprete this warning simply as a general upgrade suggestion :)

Best,
David.

> 
> Regards, Adai.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From wouter.buytaert at yucom.be  Sat Jun  7 16:18:11 2003
From: wouter.buytaert at yucom.be (wouter buytaert)
Date: Sat, 07 Jun 2003 14:18:11 -0000
Subject: [R] POSIXct troubles
Message-ID: <1054995503.2441.10.camel@dhcp-69-016.agr.kuleuven.be>


Hello,

I'm working with POSIXct objects, and I found some strange behavior. I'm
trying to extract a 30min time series from a POSIXct vector, and it
seems that R is having problems with daylight saving time:

> test
 [1] "2002-03-31 01:15:00 CET"  "2002-03-31 01:30:00 CET"
 [3] "2002-03-31 01:45:00 CET"  "2002-03-31 03:00:00 CEST"
 [5] "2002-03-31 03:15:00 CEST" "2002-03-31 03:30:00 CEST"
 [7] "2002-03-31 03:45:00 CEST" "2002-03-31 05:00:00 CEST"
 [9] "2002-03-31 05:15:00 CEST" "2002-03-31 05:30:00 CEST"
[11] "2002-03-31 05:45:00 CEST" "2002-03-31 05:00:00 CEST"
[13] "2002-03-31 05:15:00 CEST" "2002-03-31 05:30:00 CEST"
[15] "2002-03-31 05:45:00 CEST" "2002-03-31 06:00:00 CEST"
[17] "2002-03-31 06:15:00 CEST" "2002-03-31 06:30:00 CEST"
[19] "2002-03-31 06:45:00 CEST" "2002-03-31 07:00:00 CEST"

#just a time series.

> ft<-(as.integer(test)%%1800)==0
> ft
 [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE 
TRUE
[13] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

#And now, when extracting the right entries out of "test":

> test[ft]
 [1] "2002-03-31 01:30:00 CET"  "2002-03-31 03:00:00 CEST"
 [3] "2002-03-31 03:30:00 CEST" "2002-03-31 05:00:00 CEST"
 [5] "2002-03-31 05:30:00 CEST" "2002-03-31 05:00:00 CEST"
 [7] "2002-03-31 05:30:00 CEST" "2002-03-31 06:00:00 CEST"
 [9] "2002-03-31 06:30:00 CEST" "2002-03-31 07:00:00 CEST"

Here, [4] and [5] are wrong...


BTW, is there a way to ask R to use another time zone than the one your
computer is in? I'm working on data that do not use daylight saving
time, but my computer does (west european time zone) and that's quite
confusing when importing/exporting.

Thanks,

Wouter Buytaert



From gb at stat.umu.se  Sat Jun  7 18:29:20 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Sat, 7 Jun 2003 18:29:20 +0200 (CEST)
Subject: [R] Ordering long vectors
Message-ID: <Pine.LNX.4.44.0306071803540.26490-100000@tal.stat.umu.se>


I need to order a long vector of integers with rather few unique values.
This is very slow:

> x <- sample(rep(c(1:10), 50000))
> system.time(ord <- order(x))
[1] 189.18   0.09 190.48   0.00   0.00

But with no ties

> y <- sample(500000)
> system.time(ord1 <- order(y))
[1] 1.18 0.00 1.18 0.00 0.00

it is very fast!
This gave me the following idea: Since I don't care about keeping the 
order within tied values, why not add some small disturbance to  x,
and indeed,

> unix.time(ord2 <- order(x + runif(length(x), -0.1, 0.1)))
[1] 1.66 0.00 1.66 0.00 0.00

> identical(x[ord], x[ord2])
[1] TRUE

it works! 

Is there an obvious (=better) solution to this problem that I have 
overlooked? In any case, I think that the problem with order and many 
ties is worth mentioning in the help page. 

For the record: R-1.7.0, RH9

G?ran
---
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From sandrine.mainard1 at etud.univ-ubs.fr  Sat Jun  7 18:39:19 2003
From: sandrine.mainard1 at etud.univ-ubs.fr (sandrine.mainard1@etud.univ-ubs.fr)
Date: Sat,  7 Jun 2003 18:39:19 +0200
Subject: [R] mt.plot...
Message-ID: <1055003959.3ee21537432cb@homae.univ-ubs.fr>

Hello every GNU's,
I have a question about mt.plot, on multtest package.
I'm wondering how do a plot with test like bonferroni, holm, hochberg,... and 
also résults of SAM(Significiance Analysis Microarrays) as we can see on 
several document of Sandrine Dudoit.
Thanks a lot and Have a nice day
Sandrine



--------------------------------------------------------------------------------
Université de Bretagne sud                               http://www.univ-ubs.fr/



From kwan022 at stat.auckland.ac.nz  Sun Jun  8 01:43:01 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Sun, 8 Jun 2003 11:43:01 +1200 (NZST)
Subject: [R] Extracting Numbers from MANOVA output
Message-ID: <Pine.LNX.4.44.0306081136490.15778-100000@stat61.stat.auckland.ac.nz>

Hi,

Suppose I have:
> summary(manova(plank.man))
                  Df Pillai approx F num Df den Df    Pr(>F)    
plankton.new[, 1]  1 0.5267   9.8316      6     53 2.849e-07 ***
Residuals         58                                            
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

My understanding is the MANOVA summary returns a list.  However I am not 
sure how to extract out only part of the list.  For example is I want to 
extract the Pillai's test statistic, 0.5267, how can I do this?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From MSchwartz at medanalytics.com  Sun Jun  8 03:15:06 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sun, 08 Jun 2003 01:15:06 -0000
Subject: [R] Extracting Numbers from MANOVA output
In-Reply-To: <Pine.LNX.4.44.0306081136490.15778-100000@stat61.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0306081136490.15778-100000@stat61.stat.auckland.ac.nz>
Message-ID: <1055034877.4162.98.camel@localhost>

On Sat, 2003-06-07 at 18:43, Ko-Kang Kevin Wang wrote:
> Hi,
> 
> Suppose I have:
> > summary(manova(plank.man))
>                   Df Pillai approx F num Df den Df    Pr(>F)    
> plankton.new[, 1]  1 0.5267   9.8316      6     53 2.849e-07 ***
> Residuals         58                                            
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> My understanding is the MANOVA summary returns a list.  However I am not 
> sure how to extract out only part of the list.  For example is I want to 
> extract the Pillai's test statistic, 0.5267, how can I do this?


Kevin,

To use the example from ?summary.manova, since I don't have your data:

## Example on producing plastic film from Krzanowski (1998, p. 381)
> tear <- c(6.5, 6.2, 5.8, 6.5, 6.5, 6.9, 7.2, 6.9, 6.1, 6.3,
          6.7, 6.6, 7.2, 7.1, 6.8, 7.1, 7.0, 7.2, 7.5, 7.6)
> gloss <- c(9.5, 9.9, 9.6, 9.6, 9.2, 9.1, 10.0, 9.9, 9.5, 9.4,
           9.1, 9.3, 8.3, 8.4, 8.5, 9.2, 8.8, 9.7, 10.1, 9.2)
> opacity <- c(4.4, 6.4, 3.0, 4.1, 0.8, 5.7, 2.0, 3.9, 1.9, 5.7,
             2.8, 4.1, 3.8, 1.6, 3.4, 8.4, 5.2, 6.9, 2.7, 1.9)
> Y <- cbind(tear, gloss, opacity)
> rate <- factor(gl(2,10), labels=c("Low", "High"))
> additive <- factor(gl(2, 5, len=20), labels=c("Low", "High"))

> fit <- manova(Y ~ rate * additive)

> summary(fit)
              Df Pillai approx F num Df den Df   Pr(>F)
rate           1 0.6181   7.5543      3     14 0.003034 **
additive       1 0.4770   4.2556      3     14 0.024745 *
rate:additive  1 0.2229   1.3385      3     14 0.301782
Residuals     16
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1


If you then use:

> str(summary(fit))

You will note at the bottom, the following:

...
 $ stats      : num [1:4, 1:6]  1.000  1.000  1.000 16.000  0.618 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:4] "rate" "additive" "rate:additive" "Residuals"
  .. ..$ : chr [1:6] "Df" "Pillai" "approx F" "num Df" ...
 - attr(*, "class")= chr "summary.manova"


In this case 'stats' is a 4 by 6 matrix with defined dimnames. Thus to
get the individual statistics (ie. "Pillai"), use:

> summary(fit)$stats[, "Pillai"]
         rate      additive rate:additive     Residuals
    0.6181416     0.4769651     0.2228942            NA


The use of str(object) will provide you with the internal structure of
the object, which in turn will enable you to extract specific values.


HTH,

Marc Schwartz



From gregory_r_warnes at groton.pfizer.com  Sun Jun  8 05:23:11 2003
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Sat, 7 Jun 2003 23:23:11 -0400 
Subject: [R] beginner's question: Graphical presentation of t test
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680A2ED@groexmb02.pfizer.com>

Take a look at the 'plotmeans' function in the gregmisc library.  It will
draw the means and error bars for you, allowing you to connect the means for
the paired control and treated groups with something like this:

<R code>
# sample source data 10 replicates for each enzyme for treated and control
x <- rnorm(60)
enzyme <- rep(c("ABC123", "ABD124", "CCF342"), length=60)
treat  <- rep(c("CONTROL","TREATED"), length=60)

# create a enzyme by treatment label
group <- interaction(enzyme, treat)

# plot the means and confidence intervals
library(gregmisc)
plotmeans( x ~ group, connect=list(1:2,3:4,5:6))

# add p-values
data <- data.frame(x, enzyme, treat)
p.vals <- by( data, enzyme, function(data) t.test( x ~ treat,
data=data)$p.value )
text(x=c(1.5, 3.5, 5.5), y=rep(0,3), paste("P-value:\n",
format.pval(p.vals)) )
</R code>
 
-Greg

> -----Original Message-----
> From: Peter.Robinson at t-online.de [mailto:Peter.Robinson at t-online.de]
> Sent: Saturday, June 07, 2003 1:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] beginner's question: Graphical presentation of t test
> 
> 
> Hi,
> 
> Is there any way to use R to present t test results for three 
> groups of 
> experiments, each of which involves several parallel 
> experiment series with 
> groups of control vs treated. I would like to present the 
> average fold change 
> of the experimental parameter (concentration of enzymes) as bars with 
> standard error and the p value above the bar. So, there 
> should be two groups 
> (control vs treated)  of three bars for the three enzymes.
> 
> Thanks
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this message is... {{dropped}}



From christoph.lehmann at gmx.ch  Sun Jun  8 08:09:58 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sun, 08 Jun 2003 06:09:58 -0000
Subject: [R] LDA: normalization of eigenvectors (see SPSS)
Message-ID: <1055052537.1183.3.camel@christophl>

Hi dear R-users

I try to reproduce the steps included in a LDA. Concerning the eigenvectors there is 
a difference to SPSS. In my textbook (Bortz)
it says, that the matrix with the eigenvectors 

V

usually are not normalized to the length of 1, but in the way that the
following holds (SPSS does the same thing):

t(Vstar)%*%Derror%*%Vstar = I


where Vstar are the normalized eigenvectors. Derror is an "error" or
"within" squaresum- and crossproduct matrix (squaresum of the p
variables on the diagonale, and the non-diagonal elements are the sum of
the crossproducts). For Derror the following holds: Dtotal = Dtreat +
Derror.

Since I assume that many of you are familiar with this transformation:
can anybody of you tell me, how to conduct this transformation in R?
Would be very nice. Thanks a lot

Cheers

Christoph

-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From gb at stat.umu.se  Sun Jun  8 13:00:05 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Sun, 8 Jun 2003 13:00:05 +0200 (CEST)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.LNX.4.44.0306071803540.26490-100000@tal.stat.umu.se>
Message-ID: <Pine.LNX.4.44.0306081255340.7226-100000@tal.stat.umu.se>

On Sat, 7 Jun 2003, G?ran Brostr?m wrote:

> 
> I need to order a long vector of integers with rather few unique values.
> This is very slow:
> 
> > x <- sample(rep(c(1:10), 50000))
> > system.time(ord <- order(x))
> [1] 189.18   0.09 190.48   0.00   0.00
> 
> But with no ties
> 
> > y <- sample(500000)
> > system.time(ord1 <- order(y))
> [1] 1.18 0.00 1.18 0.00 0.00
> 
> it is very fast!
> This gave me the following idea: Since I don't care about keeping the 
> order within tied values, why not add some small disturbance to  x,
> and indeed,
> 
> > unix.time(ord2 <- order(x + runif(length(x), -0.1, 0.1)))
> [1] 1.66 0.00 1.66 0.00 0.00

An even better way is 

> system.time(ord3 <- order(x + seq(0, 0.9, length = length(x))))             
[1] 1.32 0.05 1.37 0.00 0.00

Faster, but more important; it keeps the original ordering for tied 
values. Thanks to James Holtman.

G?ran
[...]



From wouter.buytaert at yucom.be  Sun Jun  8 13:41:40 2003
From: wouter.buytaert at yucom.be (wouter buytaert)
Date: Sun, 08 Jun 2003 11:41:40 -0000
Subject: [R] daylight saving time problems
Message-ID: <1055072520.2234.23.camel@dhcp-69-016.agr.kuleuven.be>


Hello,

sorry for my mail yesterday about the POSIXct problems. I was a bit
tired and now I found out the real problem. When importing time data
over a daylight saving time shift, R shifts two times. I don't now
whether it is a bug or a (wrongly used) feature

If you execute the following code:

--------------
test<-c("31/03/2002 0:00", "31/03/2002 0:15", "31/03/2002 0:30",
"31/03/2002 0:45", "31/03/2002 1:00", "31/03/2002 1:15", "31/03/2002
1:30", "31/03/2002 1:45", "31/03/2002 2:00", "31/03/2002 2:15",
"31/03/2002 2:30", "31/03/2002 2:45", "31/03/2002 3:00", "31/03/2002
3:15", "31/03/2002 3:30", "31/03/2002 3:45", "31/03/2002 4:00",
"31/03/2002 4:15", "31/03/2002 4:30", "31/03/2002 4:45", "31/03/2002
5:00", "31/03/2002 5:15", "31/03/2002 5:30", "31/03/2002 5:45",
"31/03/2002 6:00");
timetest<-strptime(as.character(test), format = "%d/%m/%Y %H:%M");
timetest2<-as.POSIXct(timetest);
--------------

then R 1.7.0 gives on my Mandrake 9.1:

> test
 [1] "31/03/2002 0:00" "31/03/2002 0:15" "31/03/2002 0:30" "31/03/2002
0:45"
 [5] "31/03/2002 1:00" "31/03/2002 1:15" "31/03/2002 1:30" "31/03/2002
1:45"
 [9] "31/03/2002 2:00" "31/03/2002 2:15" "31/03/2002 2:30" "31/03/2002
2:45"
[13] "31/03/2002 3:00" "31/03/2002 3:15" "31/03/2002 3:30" "31/03/2002
3:45"
[17] "31/03/2002 4:00" "31/03/2002 4:15" "31/03/2002 4:30" "31/03/2002
4:45"
[21] "31/03/2002 5:00" "31/03/2002 5:15" "31/03/2002 5:30" "31/03/2002
5:45"
[25] "31/03/2002 6:00"
>
> timetest
 [1] "2002-03-31 00:00:00" "2002-03-31 00:15:00" "2002-03-31 00:30:00"
 [4] "2002-03-31 00:45:00" "2002-03-31 01:00:00" "2002-03-31 01:15:00"
 [7] "2002-03-31 01:30:00" "2002-03-31 01:45:00" "2002-03-31 03:00:00"
[10] "2002-03-31 03:15:00" "2002-03-31 03:30:00" "2002-03-31 03:45:00"
[13] "2002-03-31 03:00:00" "2002-03-31 03:15:00" "2002-03-31 03:30:00"
[16] "2002-03-31 03:45:00" "2002-03-31 04:00:00" "2002-03-31 04:15:00"
[19] "2002-03-31 04:30:00" "2002-03-31 04:45:00" "2002-03-31 05:00:00"
[22] "2002-03-31 05:15:00" "2002-03-31 05:30:00" "2002-03-31 05:45:00"
[25] "2002-03-31 06:00:00"
>
> timetest2
 [1] "2002-03-31 00:00:00 CET"  "2002-03-31 00:15:00 CET"
 [3] "2002-03-31 00:30:00 CET"  "2002-03-31 00:45:00 CET"
 [5] "2002-03-31 01:00:00 CET"  "2002-03-31 01:15:00 CET"
 [7] "2002-03-31 01:30:00 CET"  "2002-03-31 01:45:00 CET"
 [9] "2002-03-31 03:00:00 CEST" "2002-03-31 03:15:00 CEST"
[11] "2002-03-31 03:30:00 CEST" "2002-03-31 03:45:00 CEST"
[13] "2002-03-31 03:00:00 CEST" "2002-03-31 03:15:00 CEST"
[15] "2002-03-31 03:30:00 CEST" "2002-03-31 03:45:00 CEST"
[17] "2002-03-31 04:00:00 CEST" "2002-03-31 04:15:00 CEST"
[19] "2002-03-31 04:30:00 CEST" "2002-03-31 04:45:00 CEST"
[21] "2002-03-31 05:00:00 CEST" "2002-03-31 05:15:00 CEST"
[23] "2002-03-31 05:30:00 CEST" "2002-03-31 05:45:00 CEST"
[25] "2002-03-31 06:00:00 CEST"

There is a clear time shift timetest[8] and timetest[9] and another one
between timetest[12] and timetest[13]. I.e. timetest[9:12] are wrongly
converted.

In october (reverse timeshift in daylight time) there is no shift at
all.

It seems that it was a feature before that has been badly patched.

I'm using R 1.7.0 on Mandrake Linux in Belgium (CEST?)
It does not occur on my MacOSX box (both Darwin and Carbon version); I
don't now about the windows version.

Thanks,

Wouter Buytaert



From spencer.graves at pdf.com  Sun Jun  8 15:40:35 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jun 2003 06:40:35 -0700
Subject: [R] LDA: normalization of eigenvectors (see SPSS)
References: <1055052537.1183.3.camel@christophl>
Message-ID: <3EE33CD3.5090307@pdf.com>

	  The following satisfies some of your constraints but I don't know if 
it satisfies all of them.

	  Let V = eigenvectors normalized so t(V) %*% V = I.  Also, let D.5 = 
some square root matrix, so t(D.5) %*% D.5 = Derror, and Dm.5 = 
solve(D.5) = invers of D.5.  The Choleski decomposition ("chol") 
provides one such solution, but you can construct a symmetric square 
root using "eigen".  Then Vstar = Dm.5%*%V will have the property you 
mentioned below.

	  Consider the following:

 > (Derror <- array(c(1,1,1,4), dim=c(2,2)))
      [,1] [,2]
[1,]    1    1
[2,]    1    4
 > D.5 <- chol(Derror)
 > t(D.5) %*% D.5
      [,1] [,2]
[1,]    1    1
[2,]    1    4
 > (Dm.5 <- solve(D.5))
      [,1]       [,2]
[1,]    1 -0.5773503
[2,]    0  0.5773503
 > (t(Dm.5) %*% Derror %*% Dm.5)
      [,1] [,2]
[1,]    1    0
[2,]    0    1

	  Thus,t(Vstar)%*%Derror%*%Vstar =  t(V)%*%t(Dm.5)%*%Derror%*%Dm.5%*%V 
= t(V)%*%V = I.

hope this helps.  spencer graves

Christoph Lehmann wrote:
> Hi dear R-users
> 
> I try to reproduce the steps included in a LDA. Concerning the eigenvectors there is 
> a difference to SPSS. In my textbook (Bortz)
> it says, that the matrix with the eigenvectors 
> 
> V
> 
> usually are not normalized to the length of 1, but in the way that the
> following holds (SPSS does the same thing):
> 
> t(Vstar)%*%Derror%*%Vstar = I
> 
> 
> where Vstar are the normalized eigenvectors. Derror is an "error" or
> "within" squaresum- and crossproduct matrix (squaresum of the p
> variables on the diagonale, and the non-diagonal elements are the sum of
> the crossproducts). For Derror the following holds: Dtotal = Dtreat +
> Derror.
> 
> Since I assume that many of you are familiar with this transformation:
> can anybody of you tell me, how to conduct this transformation in R?
> Would be very nice. Thanks a lot
> 
> Cheers
> 
> Christoph
>



From chris at fisher.forestry.uga.edu  Sun Jun  8 16:35:04 2003
From: chris at fisher.forestry.uga.edu (Chris Fonnesbeck)
Date: Sun, 8 Jun 2003 10:35:04 -0400
Subject: [R] RMySQL errors
Message-ID: <64C5261A-99BE-11D7-A581-000A956FDAC0@fisher.forestry.uga.edu>

I have RMySQL installed on my OSX implementation of R, but get the 
following errors when trying to use it:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
"/usr/local/lib/R/library/RMySQL/libs/RMySQL.so":
   dlcompat: dyld: /usr/local/lib/R/bin/R.bin Undefined symbols:
_getopt_long
_load_defaults
_mysql_affected_rows
_mysql_close
_mysql_errno
_mysql_error
_mysql_fetch_fields
_mysql_fetch_lengths
_mysql_fetch_row
_mysql_field_count
_mysql_free_result
_mysql_g
Error in library(RMySQL) : .First.lib failed

I'm hoping there is a RMySQL guru out there somewhere that can help me 
out.

TIA,
cjf



From spencer.graves at pdf.com  Sun Jun  8 17:56:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jun 2003 08:56:04 -0700
Subject: [R] LDA: normalization of eigenvectors (see SPSS)
References: <1055052537.1183.3.camel@christophl>
Message-ID: <3EE35C94.7040601@pdf.com>

Hi, Christoph:

	  1.  I didn't see in your original email that you wanted V to be 
orthogonal, only that it's columns have length 1.  You have a solution 
satisfying the latter constraint, but not the former.

	  2.  I don't have time now to sort out the details, and I don't have 
them on the top of my head.  I just entered "lda" into R 1.6.2 [after 
library(MASS)] and got the following:

 > lda
function (x, ...)
{
     if (is.null(class(x)))
         class(x) <- data.class(x)
     UseMethod("lda", x, ...)
}

	  To decode 'UseMethod("lda", ...)', I requested 'methods("lda")' with 
the following result:

 > methods("lda")
[1] "lda.data.frame" "lda.default"    "lda.formula"    "lda.matrix"

	  Have you tried listing each of these 4 functions and working through 
them step by step?  I think this should answer your question.  Also see 
Venables and Ripley (2002) Modern Applied Statistics with S, index entry 
for "lda".

hth.  spencer graves

Christoph Lehmann wrote:
 > thanks a lot, Spencer
 >
 > The problem is the following: my textbook has an example with the data:
 >
 > X> x
 >    x1 x2 x3
 > 1   3  3  4
 > 2   4  4  3
 > 3   4  4  6
 > 4   2  5  5
 > 5   2  4  5
 > 6   3  4  6
 > 7   3  4  4
 > 8   2  5  5
 > 9   4  3  6
 > 10  5  5  6
 > 11  4  5  7
 > 12  4  6  4
 > 13  3  6  6
 > 14  4  7  6
 > 15  6  5  6
 > --
 >
 >>y
 >
 >  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
 >  1  1  1  1  1  1  2  2  2  2  3  3  3  3  3
 > --
 >
 >>Dtot <- (t(x)%*%x-t(xbar)%*%xbar)
 >>Dtot
 >
 >
 >             x1        x2        x3
 >   x1 17.733333  2.666667  4.866667
 >   x2  2.666667 17.333333  4.333333
 >   x3  4.866667  4.333333 16.933333
 > --
 >
 >>A <- cbind(tapply(x[,1],y,sum), tapply(x[,2],y,sum),
 >
 > tapply(x[,3],y,sum))
 >
 >>A
 >
 >   [,1] [,2] [,3]
 > 1   18   24   29
 > 2   14   17   21
 > 3   21   29   29
 >
 >>G <- apply(x,2,sum)
 >>G
 >
 > x1 x2 x3
 > 53 70 79
 >
 >>p <- ncol(x)
 >>k <- length(freq)
 >>N <- sum(freq)
 >>Dtreat <- array(0,c(p,p))
 >>k <- length(freq)
 >>for (i in 1:p)
 >
 > + {
 > +   for (j in 1:k)
 > +   {
 > +     for (h in 1:k)
 > +     {
 > +       Dtreat[i,j] <- Dtreat[i,j] + A[h,i]*A[h,j]/freq[h]
 > +     }
 > +     Dtreat[i,j] <- Dtreat[i,j] - G[i]*G[j]/N
 > +   }
 > + }
 >
 >>Dtreat
 >
 >          [,1]     [,2]     [,3]
 > [1,] 3.933333 5.966667 3.166667
 > [2,] 5.966667 9.783333 4.783333
 > [3,] 3.166667 4.783333 2.550000
 > --
 >
 >>Derror <- Dtot-Dtreat
 >>Derror
 >
 >
 >        x1    x2       x3
 >   x1 13.8 -3.30  1.70000
 >   x2 -3.3  7.55 -0.45000
 >   x3  1.7 -0.45 14.38333
 >
 > --
 >
 >>eigen(Dtreat%*%solve(Derror))
 >
 > $values
 > [1]  2.300398e+00  2.039672e-02 -1.907034e-15
 >
 > $vectors
 >            [,1]       [,2]       [,3]
 > [1,] -0.4870772  0.6813155 -0.6076020
 > [2,] -0.7809602 -0.4342229  0.1539928
 > [3,] -0.3909693  0.5892874  0.7791701
 >
 >
 >>V <- eigen(Dtreat%*%solve(Derror))$vectors
 >>V
 >
 >            [,1]       [,2]       [,3]
 > [1,] -0.4870772  0.6813155 -0.6076020
 > [2,] -0.7809602 -0.4342229  0.1539928
 > [3,] -0.3909693  0.5892874  0.7791701
 >
 > the textbook (SPSS) has similar eigenvalues, but only two!:
 >
 > lambda1 = 2.30048, lambda2 = 0.02091
 > , but as I wrote in the last mail: different eigenvectors
 >
 > Let's start here with your recommendation:
 > first, it seems, since the last eigenvalue is almost 0, that the
 > eigenvectors V are not orthogonal:
 >
 >
 >>t(V)%*%V
 >
 >            [,1]        [,2]        [,3]
 > [1,]  1.0000000 -0.22313575 -0.12894473
 > [2,] -0.2231357  1.00000000 -0.02168078
 > [3,] -0.1289447 -0.02168078  1.00000000
 >
 > let's continue anyway?
 >
 >>D.5 <- chol(Derror)
 >>t(D.5) %*% D.5
 >
 >
 >        x1    x2       x3
 >   x1 13.8 -3.30  1.70000
 >   x2 -3.3  7.55 -0.45000
 >   x3  1.7 -0.45 14.38333
 >
 >>Dm.5 <- solve(D.5)
 >>t(Dm.5) %*% Derror %*% Dm.5
 >
 >
 >                 x1            x2            x3
 >   x1  1.000000e+00 -2.523481e-17 -1.097755e-18
 >   x2 -6.625163e-18  1.000000e+00 -2.120970e-18
 >   x3  4.501901e-18  4.460942e-19  1.000000e+00
 > perfectly orthogonal
 >
 >>t(V)%*%t(Dm.5)%*%Dfehler%*%Dm.5%*%V
 >
 >
 >              [,1]        [,2]        [,3]
 >   [1,]  1.0000000 -0.22313575 -0.12894473
 >   [2,] -0.2231357  1.00000000 -0.02168078
 >   [3,] -0.1289447 -0.02168078  1.00000000
 > again, equals t(V)%*%V not orthogonal.
 >
 > -- I think it has to do with the fact, that the textbook considers the
 > third eigenvalue as = 0 and then gets the Vstar eigenvectors (which I
 > try to reproduce:
 >
 > Vstar =
 >              [,1]        [,2]        [,3]
 >   [1,]  0.1689     0.1419     -0.1825
 >   [2,]  0.3498    -0.1597      0.0060
 >   [3,]  0.0625     0.1422      0.2154
 >
 > -
 >
 > Spencer if you find some minutes time to help me reproduce this example,
 > it would be very nice (the data are from Jones 1961. He investigated
 > whether essays written by children from lower, middle, upper class
 > differ in sentence length, choosen words, complexity of sentence)
 >
 > Cheers
 >
 > Christoph
 >
##########################################
	  The following satisfies some of your constraints but I don't
know if it satisfies all of them.

	  Let V = eigenvectors normalized so t(V) %*% V = I.  Also, let
D.5 = some square root matrix, so t(D.5) %*% D.5 = Derror, and Dm.5 =
solve(D.5) = invers of D.5.  The Choleski decomposition ("chol")
provides one such solution, but you can construct a symmetric square
root using "eigen".  Then Vstar = Dm.5%*%V will have the property you
mentioned below.

	  Consider the following:

  > (Derror <- array(c(1,1,1,4), dim=c(2,2)))
       [,1] [,2]
[1,]    1    1
[2,]    1    4
  > D.5 <- chol(Derror)
  > t(D.5) %*% D.5
       [,1] [,2]
[1,]    1    1
[2,]    1    4
  > (Dm.5 <- solve(D.5))
       [,1]       [,2]
[1,]    1 -0.5773503
[2,]    0  0.5773503
  > (t(Dm.5) %*% Derror %*% Dm.5)
       [,1] [,2]
[1,]    1    0
[2,]    0    1

	  Thus,t(Vstar)%*%Derror%*%Vstar =  t(V)%*%t(Dm.5)%*%Derror%*%Dm.5%*%V
= t(V)%*%V = I.

hope this helps.  spencer graves

Christoph Lehmann wrote:
 > Hi dear R-users
 >
 > I try to reproduce the steps included in a LDA. Concerning the 
eigenvectors there is
 > a difference to SPSS. In my textbook (Bortz)
 > it says, that the matrix with the eigenvectors
 >
 > V
 >
 > usually are not normalized to the length of 1, but in the way that the
 > following holds (SPSS does the same thing):
 >
 > t(Vstar)%*%Derror%*%Vstar = I
 >
 >
 > where Vstar are the normalized eigenvectors. Derror is an "error" or
 > "within" squaresum- and crossproduct matrix (squaresum of the p
 > variables on the diagonale, and the non-diagonal elements are the sum of
 > the crossproducts). For Derror the following holds: Dtotal = Dtreat +
 > Derror.
 >
 > Since I assume that many of you are familiar with this transformation:
 > can anybody of you tell me, how to conduct this transformation in R?
 > Would be very nice. Thanks a lot
 >
 > Cheers
 >
 > Christoph
 >



From tlumley at u.washington.edu  Sun Jun  8 18:01:33 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 8 Jun 2003 09:01:33 -0700 (PDT)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.LNX.4.44.0306081255340.7226-100000@tal.stat.umu.se>
Message-ID: <Pine.A41.4.44.0306080847270.136656-100000@homer03.u.washington.edu>


On Sun, 8 Jun 2003, [ISO-8859-1] Göran Broström wrote:

> On Sat, 7 Jun 2003, Göran Broström wrote:
>
> >
> > I need to order a long vector of integers with rather few unique values.
> > This is very slow:
> >
> > > x <- sample(rep(c(1:10), 50000))
> > > system.time(ord <- order(x))
> > [1] 189.18   0.09 190.48   0.00   0.00
> >
> > But with no ties
> >
> > > y <- sample(500000)
> > > system.time(ord1 <- order(y))
> > [1] 1.18 0.00 1.18 0.00 0.00
> >
> > it is very fast!
> > This gave me the following idea: Since I don't care about keeping the
> > order within tied values, why not add some small disturbance to  x,

Another option:

> system.time(a<-sapply(sort(unique(x)),function(i) which(x==i)))

This turns out to be slightly slower than your method, but doesn't require
that you know what the smallest difference between values is (and works
for characters as well as numbers)

	-thomas



From tlumley at u.washington.edu  Sun Jun  8 18:12:13 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 8 Jun 2003 09:12:13 -0700 (PDT)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.LNX.4.44.0306071803540.26490-100000@tal.stat.umu.se>
Message-ID: <Pine.A41.4.44.0306080905570.61872-100000@homer03.u.washington.edu>

On Sat, 7 Jun 2003, [ISO-8859-1] Göran Broström wrote:

>
> I need to order a long vector of integers with rather few unique values.
> This is very slow:


I think the culprit is

src/main/sort.c: orderVector1

    /* Shell sort isn't stable, but it proves to be somewhat faster
       to run a final insertion sort to re-order runs of ties when
       comparison is cheap.
    */

This also explains:

> aa<-sample(rep(1:10,50000))
> system.time( order(aa, 1:length(aa)))
[1] 3.67 0.01 3.68 0.00 0.00
> system.time( order(aa))
^C
Timing stopped at: 49.33 0.01 49.34 0 0

which is perhaps the simplest work-around :).


	-thomas



From spencer.graves at pdf.com  Sun Jun  8 18:32:54 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jun 2003 09:32:54 -0700
Subject: [R] converting "by" to a data.frame?
References: <3EDFBC17.7060404@pdf.com> <p05210609bb057f0cf8ba@[128.115.153.6]>
Message-ID: <3EE36536.3060003@pdf.com>

Thanks to Thomas Lumley, Sundar Dorai-Raj, and Don McQueen for their 
suggestions.  I need the INDICES as part of the output data.frame, which 
McQueen's solution provided.  I generalized his method as follows:

by.to.data.frame <-
function(x, INDICES, FUN){
# Split data.frame x on x[,INDICES]
# and lapply FUN to each data.frame subset,
# returning a data.frame
#
#  Internal functions
    get.Index <- function(x, INDICES){
	Ind <- as.character(x[,INDICES[1]])
	k <- length(INDICES)
	if(k > 1)
		Ind <- paste(Ind, get.Index(x, INDICES[-1]), sep=":")	
		Ind	
     }
     FUN2 <- function(data., INDICES, FUN){
	vec <- FUN(data.)
	Vec <- matrix(vec, nrow=1)
	dimnames(Vec) <- list(NULL, names(vec))
	cbind(data.[1,INDICES], Vec)
     }
#   Combine INDICES
     Ind <- get.Index(x, INDICES)
#   Apply ...:  Do the work.
     Split <- split(x, Ind)
     byFits <- lapply(Split, FUN2, INDICES, FUN)
#   Convert to a data.frame
     do.call('rbind',byFits) 	
}

Applying this to my toy problem produces the following:

 > by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
+  B=rep(c("B1", "B2"), each=3), x=1:6, y=rep(0:1, length=6))
 >
 > by.to.data.frame(by.df, c("A", "B"), function(data.)coef(lm(y~x, data.)))
        A  B (Intercept)             x
A1:B1 A1 B1   0.3333333 -1.517960e-16
A2:B2 A2 B2   0.6666667  3.282015e-16

Thanks for the assistance.  I can now tackle the real problem that 
generated this question.

Best Wishes,
Spencer Graves
########################################
Don MacQueen wrote:
> Since I don't have your by.df to test with I may not have it exactly 
> right, but something along these lines should work:
> 
> byFits <- lapply(split(by.df,paste(by.df$A,by.df$B)),
>                  FUN=function(data.) {
>                     tmp <- coef(lm(y~x,data.))
>                     data.frame(A=unique(data.$A),
>                                B=unique(data.$B),
>                                intercept=tmp[1],
>                                slope=tmp[2])
>                    })
> 
> byFitsDF <- do.call('rbind',byFits)
> 
> That's assuming I've got all the closing parantheses in the right 
> places, since my email software (Eudora) doesn't do R syntax checking!
> 
> This approach can get rather slow if by.df is big, or when the 
> computations in FUN are extensive (or both).
> 
> If by.df$A has mode character (as opposed to being a factor), then 
> replacing A=unique(data.$A) with A=I(unique(data.$A)) might improve 
> performance. You want to avoid character to factor conversions when 
> using an approach like this.
> 
> -Don
> 
> 
> At 2:54 PM -0700 6/5/03, Spencer Graves wrote:
> 
>> Dear R-Help:
>>
>>       I want to (a) subset a data.frame by several columns, (b) fit a 
>> model to each subset, and (c) store a vector of results from the fit 
>> in the columns of a data.frame.  In the past, I've used "for" loops do 
>> do this.  Is there a way to use "by"?
>>
>>       Consider the following example:
>>
>>  > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
>> +  function(data.)coef(lm(y~x, data.)))
>>  > byFits
>> A: A1
>> B: B1
>>   (Intercept)             x
>>  3.333333e-01 -1.517960e-16
>> ------------------------------------------------------------
>> A: A2
>> B: B1
>> NULL
>> ------------------------------------------------------------
>> A: A1
>> B: B2
>> NULL
>> ------------------------------------------------------------
>> A: A2
>> B: B2
>>  (Intercept)            x
>> 6.666667e-01 3.282015e-16
>>
>>>
>>>
>> #############################
>> Desired output:
>>
>> data.frame(A=c("A1","A2"), B=c("B1", "B2"),
>>     .Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
>>
>> What's the simplest way to do this?
>> Thanks,
>> Spencer Graves
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From gb at stat.umu.se  Sun Jun  8 18:44:52 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Sun, 8 Jun 2003 18:44:52 +0200 (CEST)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.A41.4.44.0306080905570.61872-100000@homer03.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0306081830190.7491-100000@tal.stat.umu.se>

On Sun, 8 Jun 2003, Thomas Lumley wrote:

> On Sat, 7 Jun 2003, [ISO-8859-1] G?ran Brostr?m wrote:
> 
> >
> > I need to order a long vector of integers with rather few unique values.
> > This is very slow:
> 
> 
> I think the culprit is
> 
> src/main/sort.c: orderVector1
> 
>     /* Shell sort isn't stable, but it proves to be somewhat faster
>        to run a final insertion sort to re-order runs of ties when
>        comparison is cheap.
>     */
> 
> This also explains:
> 
> > aa<-sample(rep(1:10,50000))
> > system.time( order(aa, 1:length(aa)))
> [1] 3.67 0.01 3.68 0.00 0.00
> > system.time( order(aa))
> ^C
> Timing stopped at: 49.33 0.01 49.34 0 0
> 
> which is perhaps the simplest work-around :).

Thanks. This is really surprising: it is *much* faster to break ties by a 
second condition than not breaking them. I think it should be mentioned 
in the help. And could 'order/sort' be modified to check for 'tieness'? 
But I guess the the overhead would be too heavy.

(if (length(unique(x)) < alpha * length(x)) then .... else ....)

G?ran



From pratibha01in at yahoo.co.in  Sun Jun  8 18:37:57 2003
From: pratibha01in at yahoo.co.in (=?iso-8859-1?q?Pratibha=20Murthy?=)
Date: Sun, 8 Jun 2003 17:37:57 +0100 (BST)
Subject: [R] Need help on data frame 
Message-ID: <20030608163757.49991.qmail@web8202.mail.in.yahoo.com>

Dear Sir/Madam,

I am new in R.I have data corresponding to every day.
Problem is that there are some gap i.e. observation
couldn't be done on some particular day.

I want to place this data frame like exact data frame
(every year it will change, Feb 28 or feb29)

Maybe I need to make one coulmn of date (for each
year, say this dataframe 'frame1'), then I need to
place data set on frame1 with missing entry as NA.

Then I want to change this NA as mean of precceeding
and following entries (for EACH NA)

Hope it is possible by using R.  I will greatly
appreciate any help.

Thanks,
Pratibha 



________________________________________________________________________
Missed your favourite TV serial last night? Try the new, Yahoo! TV.
       visit http://in.tv.yahoo.com



From spencer.graves at pdf.com  Sun Jun  8 18:49:27 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jun 2003 09:49:27 -0700
Subject: [R] Need help on data frame
References: <20030608163757.49991.qmail@web8202.mail.in.yahoo.com>
Message-ID: <3EE36917.7050701@pdf.com>

I'm sorry, but I don't understand enough of your problem to be able to 
comment.  If you can give us a toy example, small, easy to understand in 
a few seconds, that illustrated the difficulty, it should be easier for 
others to help.

spencer graves

Pratibha Murthy wrote:
> Dear Sir/Madam,
> 
> I am new in R.I have data corresponding to every day.
> Problem is that there are some gap i.e. observation
> couldn't be done on some particular day.
> 
> I want to place this data frame like exact data frame
> (every year it will change, Feb 28 or feb29)
> 
> Maybe I need to make one coulmn of date (for each
> year, say this dataframe 'frame1'), then I need to
> place data set on frame1 with missing entry as NA.
> 
> Then I want to change this NA as mean of precceeding
> and following entries (for EACH NA)
> 
> Hope it is possible by using R.  I will greatly
> appreciate any help.
> 
> Thanks,
> Pratibha 
> 
> 
> 
> ________________________________________________________________________
> Missed your favourite TV serial last night? Try the new, Yahoo! TV.
>        visit http://in.tv.yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Kosenkov.Kirill at nac.spb.ru  Sun Jun  8 21:21:56 2003
From: Kosenkov.Kirill at nac.spb.ru (Kosenkov Kirill)
Date: Sun, 08 Jun 2003 23:21:56 +0400
Subject: [R] problems with postscript, lattice and afm
Message-ID: <3EE38CD4.7000607@nac.spb.ru>

Hello!
When i am trying to print lattice plot in postscript file via
trellis.device(device=postscript,file='part.eps',family=font1,paper='special',width=16/2.54,height=23/2.54,onefile=FALSE,horizontal=FALSE)
everything is ok, but when i am trying to produce multiple files 
with lattice and
trellis.device(device=postscript,file='part%03d.eps'...
only first plot is printed in file and no any other files created.
R gives me a messages:
cannot read afm file
after every plot, it tries to produce after first plot.

What to do? It seems, that R locks afm file for reading after 
first plot and then tries to open it again and can not do that. Is 
  it a bug or what? Any suggestions?

R 1.7.0 on Win2k

Thanks!



From pfleonard at hotmail.com  Sun Jun  8 22:35:17 2003
From: pfleonard at hotmail.com (peter leonard)
Date: Sun, 08 Jun 2003 13:35:17 -0700
Subject: [R] Basic question on applying a function to each row of a
	dataframe 
Message-ID: <Law11-F101oZolmRBaY00009f20@hotmail.com>

Hi,

I have a function foo(x,y) and a dataframe, DF,  comprised of two vectors, x 
& w,  as follows :

   x w
1  1 1
2  2 1
3  3 1
4  4 1

etc


I would like to apply the function foo to each 'pair' within DF e.g  
foo(1,1), foo(2,1), foo(3,1) etc

I have tried

>apply(DF,foo)
>apply(DF[,],foo)
>apply(DF[DF$x,DF$w],foo)


However, none of the above worked. Can anyone help ?

Thanks in advance,
Peter



From spencer.graves at pdf.com  Sun Jun  8 22:48:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 08 Jun 2003 13:48:04 -0700
Subject: [R] Basic question on applying a function to each row of
	a	dataframe
References: <Law11-F101oZolmRBaY00009f20@hotmail.com>
Message-ID: <3EE3A104.7010903@pdf.com>

How about the following:

 > DF <- data.frame(x=1:4, y=rep(1,4))
 > foo <- function(x, y)x+y
 > foo(DF$x, DF$y)
[1] 2 3 4 5

hth.  spencer graves

peter leonard wrote:
> Hi,
> 
> I have a function foo(x,y) and a dataframe, DF,  comprised of two 
> vectors, x & w,  as follows :
> 
>   x w
> 1  1 1
> 2  2 1
> 3  3 1
> 4  4 1
> 
> etc
> 
> 
> I would like to apply the function foo to each 'pair' within DF e.g  
> foo(1,1), foo(2,1), foo(3,1) etc
> 
> I have tried
> 
>> apply(DF,foo)
>> apply(DF[,],foo)
>> apply(DF[DF$x,DF$w],foo)
> 
> 
> 
> However, none of the above worked. Can anyone help ?
> 
> Thanks in advance,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kwan022 at stat.auckland.ac.nz  Sun Jun  8 22:54:02 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 9 Jun 2003 08:54:02 +1200 (NZST)
Subject: [R] Basic question on applying a function to each row of a
	dataframe 
In-Reply-To: <Law11-F101oZolmRBaY00009f20@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306090851290.20965-100000@stat61.stat.auckland.ac.nz>

Hi,

You need to tell the apply() whether you want to apply the function to 
rows (1) or columns (2).

So in your case you may want to try something like:
  apply(DF, 1, foo)

On Sun, 8 Jun 2003, peter leonard wrote:

> I have a function foo(x,y) and a dataframe, DF,  comprised of two vectors, x 
> & w,  as follows :
> 
>    x w
> 1  1 1
> 2  2 1
> 3  3 1
> 4  4 1
> 
> etc
> 
> 
> I would like to apply the function foo to each 'pair' within DF e.g  
> foo(1,1), foo(2,1), foo(3,1) etc
> 
> I have tried
> 
> >apply(DF,foo)
> >apply(DF[,],foo)
> >apply(DF[DF$x,DF$w],foo)
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From pfleonard at hotmail.com  Mon Jun  9 00:05:33 2003
From: pfleonard at hotmail.com (peter leonard)
Date: Sun, 08 Jun 2003 15:05:33 -0700
Subject: [R] Basic question on applying a function to each row of a
	dataframe
Message-ID: <Law11-F962nvIzqXfJ100074042@hotmail.com>


Hi Keven,
This returns :

Error in FUN(newX[, i], ...) : Argument "y" is missing, with no default


E.g

>x<-c(1,2,3,4)
>w<-c(1,1,1,1)
>DF<-data.frame(x,w)
>foo <- function(x, y)x+y apply(DF, 1, foo)
Error in FUN(newX[, i], ...) : Argument "y" is missing, with no default

Regards
Peter





>From: Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz>
>To: peter leonard <pfleonard at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Basic question on applying a function to each row of a 
>dataframe Date: Mon, 9 Jun 2003 08:54:02 +1200 (NZST)
>
>Hi,
>
>You need to tell the apply() whether you want to apply the function to
>rows (1) or columns (2).
>
>So in your case you may want to try something like:
>   apply(DF, 1, foo)
>
>On Sun, 8 Jun 2003, peter leonard wrote:
>
> > I have a function foo(x,y) and a dataframe, DF,  comprised of two 
>vectors, x
> > & w,  as follows :
> >
> >    x w
> > 1  1 1
> > 2  2 1
> > 3  3 1
> > 4  4 1
> >
> > etc
> >
> >
> > I would like to apply the function foo to each 'pair' within DF e.g
> > foo(1,1), foo(2,1), foo(3,1) etc
> >
> > I have tried
> >
> > >apply(DF,foo)
> > >apply(DF[,],foo)
> > >apply(DF[DF$x,DF$w],foo)
> >
>
>--
>Cheers,
>
>Kevin
>
>------------------------------------------------------------------------------
>"On two occasions, I have been asked [by members of Parliament],
>'Pray, Mr. Babbage, if you put into the machine wrong figures, will
>the right answers come out?' I am not able to rightly apprehend the
>kind of confusion of ideas that could provoke such a question."
>
>-- Charles Babbage (1791-1871)
>---- From Computer Stupidities: http://rinkworks.com/stupid/
>
>--
>Ko-Kang Kevin Wang
>Master of Science (MSc) Student
>SLC Tutor and Lab Demonstrator
>Department of Statistics
>University of Auckland
>New Zealand
>Homepage: http://www.stat.auckland.ac.nz/~kwan022
>Ph: 373-7599
>     x88475 (City)
>     x88480 (Tamaki)
>
>



From johnz at vmware.com  Mon Jun  9 02:35:14 2003
From: johnz at vmware.com (John Zedlewski)
Date: Sun, 8 Jun 2003 17:35:14 -0700
Subject: [R] executable R scripts
Message-ID: <200306081735.14547.johnz@vmware.com>

Hi, I'm a newbie trying to make an R program executable on UNIX, just like one 
would write an executable perl script by putting "#!/usr/bin/perl" in the 
first line, and so on.

It seems, though, that this would only work if I use the "BATCH" command to 
tell R to execute the program in its first argument. This would have the 
unfortunately side-effect of dumping all output to a file rather than stdout.

Additionally, I'd want to see only the results of "print" statements on 
stdout, not all off R's output, just as when you source a script with 
echo=FALSE.

This seems like it would be a pretty common problem, but I haven't found any 
explanations in the docs. Does somebody have a sample script that I could 
look at for advice? Or should I just bite the bullet and write a wrapper 
shell script?

Thanks!
--JRZ



From baron at psych.upenn.edu  Mon Jun  9 03:39:55 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 8 Jun 2003 21:39:55 -0400
Subject: [R] executable R scripts
In-Reply-To: <200306081735.14547.johnz@vmware.com>
References: <200306081735.14547.johnz@vmware.com>
Message-ID: <20030609013955.GA20787@mail1.sas.upenn.edu>

On 06/08/03 17:35, John Zedlewski wrote:
>Hi, I'm a newbie trying to make an R program executable on UNIX, just like one 
>would write an executable perl script by putting "#!/usr/bin/perl" in the 
>first line, and so on.
>
>It seems, though, that this would only work if I use the "BATCH" command to 
>tell R to execute the program in its first argument. This would have the 
>unfortunately side-effect of dumping all output to a file rather than stdout.
>
>Additionally, I'd want to see only the results of "print" statements on 
>stdout, not all off R's output, just as when you source a script with 
>echo=FALSE.

See
man R
for how to do it, although I'm not sure where it says the
following:

To get just the print output and nothing else, it helps to have
print()'s in the script itself.  Then you can use

R --slave < myfile.R > printoutput.txt

I also use R --vanilla < myfile.R
for a R file that has write.table()'s in it.  For this you do not
need to pipe the output anywhere.



From edd at debian.org  Mon Jun  9 03:52:04 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 08 Jun 2003 20:52:04 -0500
Subject: [R] executable R scripts
In-Reply-To: <200306081735.14547.johnz@vmware.com>
References: <200306081735.14547.johnz@vmware.com>
Message-ID: <20030609015204.GA5239@sonny.eddelbuettel.com>

On Sun, Jun 08, 2003 at 05:35:14PM -0700, John Zedlewski wrote:
> Hi, I'm a newbie trying to make an R program executable on UNIX, just like one 
> would write an executable perl script by putting "#!/usr/bin/perl" in the 
> first line, and so on.

This is not currently supported, but with some luck may be supported in a
later version of R.  

> It seems, though, that this would only work if I use the "BATCH" command to 
> tell R to execute the program in its first argument. This would have the 
> unfortunately side-effect of dumping all output to a file rather than stdout.

My personal favourite currently is to arrange everything (loading of
package, code, ...) in a file which I can read with source() from within R.
Then
    $ echo "source(\"foo.R\") | R --slave
works quite well, you can redirect etc. Works on windows/cygwin too using
Rterm.exe.

> Additionally, I'd want to see only the results of "print" statements on 
> stdout, not all off R's output, just as when you source a script with 
> echo=FALSE.

I think the above fits that bill.

> This seems like it would be a pretty common problem, but I haven't found any 
> explanations in the docs. Does somebody have a sample script that I could 
> look at for advice? Or should I just bite the bullet and write a wrapper 
> shell script?

That's where the above leads to as well.

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From johnz at vmware.com  Mon Jun  9 04:42:22 2003
From: johnz at vmware.com (John Zedlewski)
Date: Sun, 8 Jun 2003 19:42:22 -0700
Subject: [R] executable R scripts
In-Reply-To: <20030609015204.GA5239@sonny.eddelbuettel.com>
References: <200306081735.14547.johnz@vmware.com>
	<20030609015204.GA5239@sonny.eddelbuettel.com>
Message-ID: <200306081942.22166.johnz@vmware.com>

Dirk and Jonathan--
  Thanks a lot for the fast and helpful comments, guys. I ended up writing a 
wrapper script that uses the trick of echoing "source(\"filename\")" into R 
--slave, and it works well.
  Thanks again!
--JRZ



From johnz at vmware.com  Mon Jun  9 04:53:14 2003
From: johnz at vmware.com (John Zedlewski)
Date: Sun, 8 Jun 2003 19:53:14 -0700
Subject: [R] early R messages to stdout
Message-ID: <200306081953.14114.johnz@vmware.com>

Hi,
 I have an R script that takes its input in the form of command-line 
parameters. It works fine, but R complains about every unknown arg with the 
"ARGUMENT %s ignored" message, and this goes to stdout instead of stderr 
because R_ConsoleFile isn't set yet. Is it really necessary to process all 
command line args before setting R_ConsoleFile? It seems that only Aqua 
systems care about their arguments when choosing the console file.

  I've attached a diff (against 1.7.0) that fixes this issue, so that non-Aqua 
unix folks can redirect stderr to /dev/null and not have to worry about those 
annoying argument ignored errors anymore.

--JRZ

From pfleonard at hotmail.com  Mon Jun  9 06:21:01 2003
From: pfleonard at hotmail.com (peter leonard)
Date: Sun, 08 Jun 2003 21:21:01 -0700
Subject: [R] Basic question on applying a function to each row of a
	dataframe
Message-ID: <Law11-F11zXs0oByC4w00012057@hotmail.com>


This works fine.
Thanks
Peter


>From: Spencer Graves <spencer.graves at PDF.COM>
>To: peter leonard <pfleonard at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Basic question on applying a function to each row of 
>a	dataframe
>Date: Sun, 08 Jun 2003 13:48:04 -0700
>
>How about the following:
>
> > DF <- data.frame(x=1:4, y=rep(1,4))
> > foo <- function(x, y)x+y
> > foo(DF$x, DF$y)
>[1] 2 3 4 5
>
>hth.  spencer graves
>
>peter leonard wrote:
>>Hi,
>>
>>I have a function foo(x,y) and a dataframe, DF,  comprised of two vectors, 
>>x & w,  as follows :
>>
>>   x w
>>1  1 1
>>2  2 1
>>3  3 1
>>4  4 1
>>
>>etc
>>
>>
>>I would like to apply the function foo to each 'pair' within DF e.g  
>>foo(1,1), foo(2,1), foo(3,1) etc
>>
>>I have tried
>>
>>>apply(DF,foo)
>>>apply(DF[,],foo)
>>>apply(DF[DF$x,DF$w],foo)
>>
>>
>>
>>However, none of the above worked. Can anyone help ?
>>
>>Thanks in advance,
>>Peter
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From zhuw at mail.smu.edu  Mon Jun  9 07:19:30 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Mon, 09 Jun 2003 05:19:30 -0000
Subject: [R] Questions for package ts prediction
Message-ID: <1055135968.1389.37.camel@zwang.stat.smu.edu>

Dear helpers,

I am trying to write a function to return prediction values using
package ts. I have written three different versions since I am not sure
what's wrong with my func2. func and func1 return the same results.But
func1 and func2 don't. In particular, the only difference between
"func1" and "func2" is the function variable name being y and data,
respectively.  But running the last line of the following script will
give the message:

Error in ts(x): object is not a matrix.

I am confused. Also, could somebody kindly let me what's the answer if
any for the following sunspot example from the package help:

data(sunspot)
(sunspot.ar <- ar(sunspot.year)) 
# why not just sunspot.ar <- ar(sunspot.year) ?
predict(sunspot.ar, n.ahead=25)

Thanks in advance.

Zhu Wang
Statistical Science Department
Southern Methodist University

(214)768-2453
-- 
zhu wang <zhuw at mail.smu.edu>

# time series prediction

func<-function(data)
  {(esti<- ar(data))
   return(predict(object=esti,newdata=data,n.head=5))
 }
func1<-function(y)
  {(esti<- ar(y))
   return(predict(esti,n.head=5))
 }
func2<-function(data)
  {(esti<- ar(data))
   return(predict(esti,n.head=5))
 }

y<-arima.sim(model=list(ar=c(1.7,-0.8)),n=100)
func(y)
func1(y)
func2(y)



From nhviet at rocketmail.com  Mon Jun  9 07:21:50 2003
From: nhviet at rocketmail.com (Viet Nguyen,,,)
Date: Mon, 09 Jun 2003 15:21:50 +1000
Subject: [R] looking for Prof Bates' file
Message-ID: <3EE4196E.2060309@rocketmail.com>


Hello

I'm reading up on fitting truncated Weibull distribution to data.

There are posts in 2002 that point to this presentation by Prof Bates:

http://www.stat.wisc.edu/~bates/JSM2001.pdf

but now the file is not there. I can't find it anywhere else, Google 
doesn't have a cached copy for it.

Could someone please give me a copy of this file, if they have it?

Thanks and regards,
viet.



From ripley at stats.ox.ac.uk  Mon Jun  9 08:47:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jun 2003 07:47:30 +0100 (BST)
Subject: [R] MCD distance
In-Reply-To: <001f01c32c2e$97c08440$d8b59ed5@osama1>
Message-ID: <Pine.LNX.4.44.0306090745370.14099-100000@gannet.stats>

There first two *are* in the lqs package: you just need to use 
mahalanobis() with cob.rob().

I've never heard of BACON.

On Fri, 6 Jun 2003, osama wrote:

>     I would like to compare robust distance measures,e.g. MVE, MCD,
> BACON, as measures of leverage and to detect outliers in X-space . I
> could not find it in LQS or MASS backages.

Why look in only two packages?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mushtaq at persistent.co.in  Mon Jun  9 08:56:57 2003
From: mushtaq at persistent.co.in (Dr Mushtaq Ahmed)
Date: Mon, 9 Jun 2003 12:26:57 +0530
Subject: [R] XML package for R
Message-ID: <010c01c32e54$524d8b10$3c07a8c0@Pachora>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/3c23f1cc/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun  9 09:14:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jun 2003 08:14:06 +0100 (BST)
Subject: [R] XML package for R
In-Reply-To: <010c01c32e54$524d8b10$3c07a8c0@Pachora>
Message-ID: <Pine.LNX.4.44.0306090811200.14099-100000@gannet.stats>

See the ReadMe in the appropriate area on CRAN (XML is also a CRAN
packages).

http://cran.r-project.org/bin/windows/contrib/1.7/ReadMe

which points you to  http://www.stats.ox.ac.uk/pub/RWin


On Mon, 9 Jun 2003, Dr Mushtaq Ahmed wrote:

> Does anyone have a binary of XML package for 1.7.0 on Windows? 
> I have searched it at the RSXML pages (http://www.omegahat.org/RSXML/ ) but it seems it is no longer support.
> 
> Any pointers for the compiled version will be appreciated.

You could always compile it yourself!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mushtaq at persistent.co.in  Mon Jun  9 09:54:44 2003
From: mushtaq at persistent.co.in (Dr Mushtaq Ahmed)
Date: Mon, 9 Jun 2003 13:24:44 +0530
Subject: [R] XML package for R
References: <Pine.LNX.4.44.0306090811200.14099-100000@gannet.stats>
Message-ID: <014301c32e5c$640a9570$3c07a8c0@Pachora>

thanks :)

-mushtaq

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Dr Mushtaq Ahmed" <mushtaq at persistent.co.in>
Cc: <R-help at stat.math.ethz.ch>
Sent: Monday, June 09, 2003 12:44 PM
Subject: Re: [R] XML package for R


> See the ReadMe in the appropriate area on CRAN (XML is also a CRAN
> packages).
>
> http://cran.r-project.org/bin/windows/contrib/1.7/ReadMe
>
> which points you to  http://www.stats.ox.ac.uk/pub/RWin
>
>
> On Mon, 9 Jun 2003, Dr Mushtaq Ahmed wrote:
>
> > Does anyone have a binary of XML package for 1.7.0 on Windows?
> > I have searched it at the RSXML pages (http://www.omegahat.org/RSXML/ )
but it seems it is no longer support.
> >
> > Any pointers for the compiled version will be appreciated.
>
> You could always compile it yourself!
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From grassi at psico.univ.trieste.it  Mon Jun  9 09:54:14 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Mon, 9 Jun 2003 09:54:14 +0200 (MEST)
Subject: [R] Beginner questions
Message-ID: <200306090754.JAA06495@server.psico.univ.trieste.it>

Hi, 
when i create a new variable e.g var<-c(1,2,3,...), how 
can i delete one or more osservation?
When i draw my scatterplot,i want to identify, in the 
graph,some point whit x,y coordinates. How can i do it?
Thanks.
Michele.



From ripley at stats.ox.ac.uk  Mon Jun  9 10:56:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jun 2003 09:56:20 +0100 (BST)
Subject: [R] Questions for package ts prediction
In-Reply-To: <1055135968.1389.37.camel@zwang.stat.smu.edu>
Message-ID: <Pine.LNX.4.44.0306090952480.15761-100000@gannet.stats>

You have a scoping problem: the predict method needs to find the data:
please supply an explicit newdata argument.  Your first example works
because `y' happens to be globally visible and be the right object.

On 9 Jun 2003, zhu wang wrote:

> I am trying to write a function to return prediction values using
> package ts. I have written three different versions since I am not sure
> what's wrong with my func2. func and func1 return the same results.But
> func1 and func2 don't. In particular, the only difference between
> "func1" and "func2" is the function variable name being y and data,
> respectively.  But running the last line of the following script will
> give the message:
> 
> Error in ts(x): object is not a matrix.
> 
> I am confused. Also, could somebody kindly let me what's the answer if
> any for the following sunspot example from the package help:
> 
> data(sunspot)
> (sunspot.ar <- ar(sunspot.year)) 
> # why not just sunspot.ar <- ar(sunspot.year) ?

Have you tried it?  Please do so, and you will learn the difference!
This idiom is widely used in the R help files.

> predict(sunspot.ar, n.ahead=25)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jun  9 11:35:50 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 09 Jun 2003 09:35:50 -0000
Subject: [R] looking for Prof Bates' file
In-Reply-To: <3EE4196E.2060309@rocketmail.com>
References: <3EE4196E.2060309@rocketmail.com>
Message-ID: <x2d6hn8u12.fsf@biostat.ku.dk>

"Viet Nguyen,,," <nhviet at rocketmail.com> writes:

> Hello
> 
> I'm reading up on fitting truncated Weibull distribution to data.
> 
> There are posts in 2002 that point to this presentation by Prof Bates:
> 
> http://www.stat.wisc.edu/~bates/JSM2001.pdf
> 
> but now the file is not there. I can't find it anywhere else, Google
> doesn't have a cached copy for it.
> 
> Could someone please give me a copy of this file, if they have it?
> 
> Thanks and regards,
> viet.

Looks like www.stat.wisc.edu is no longer identical to Doug's desktop
machine. The file is still there, but I'm not too sure whether he
would want us to tell the world how to get to it, so perhaps we should
wait for him to reply...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Jun  9 11:52:02 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jun 2003 11:52:02 +0200
Subject: [R] Beginner questions
References: <200306090754.JAA06495@server.psico.univ.trieste.it>
Message-ID: <3EE458C2.97E2499B@statistik.uni-dortmund.de>

Michele Grassi wrote:
> 
> Hi,
> when i create a new variable e.g var<-c(1,2,3,...), how
> can i delete one or more osservation?

See "An Introduction to R", Section 2.7.

> When i draw my scatterplot,i want to identify, in the
> graph,some point whit x,y coordinates. How can i do it?

See "An Introduction to R", Section 12.3.

> Thanks.
> Michele.


Please read the manuals!

Uwe Ligges



From ahmlatif at yahoo.com  Mon Jun  9 12:35:44 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Mon, 9 Jun 2003 03:35:44 -0700 (PDT)
Subject: [R] installing XML in linux
Message-ID: <20030609103544.28958.qmail@web41211.mail.yahoo.com>

Hi,

I was trying to install XML package in a linux
(dabian) machine and got the following error message.
I am not sure whether there is any error in my linux
installion. I appreciate suggestions to install XML
properly in this machine. 

Thanks in advance.

Mahbub.

######Errors#### 
$ R CMD INSTALL -l /usr/local/lib/R/library/
XML_0.93-4.tar.gz 
* Installing *source* package 'XML' ...
creating cache ./config.cache
checking for gcc... gcc
checking whether the C compiler (gcc  ) works... yes
checking whether the C compiler (gcc  ) is a
cross-compiler... no
checking whether we are using GNU C... yes
checking whether gcc accepts -g... yes
checking how to run the C preprocessor... gcc -E
checking for xml-config... no
checking for libxml/parser.h... no
checking for gnome-xml/parser.h... no
checking for libxml/parser.h... (cached) no
checking for gnome-xml/parser.h... (cached) no
checking for libxml/parser.h... (cached) no
Cannot find parser.h. Set the value of the environment
variable
    LIBXML_INCDIR
to point to where it can be found.
ERROR: configuration failed for package 'XML'



> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.0              
year     2003             
month    04               
day      16               
language R



From glouis at dynamicro.on.ca  Mon Jun  9 13:15:18 2003
From: glouis at dynamicro.on.ca (Greg Louis)
Date: Mon, 9 Jun 2003 07:15:18 -0400
Subject: [R] executable R scripts
In-Reply-To: <200306081735.14547.johnz@vmware.com>
References: <200306081735.14547.johnz@vmware.com>
Message-ID: <20030609111518.GA1281@athame.dynamicro.on.ca>

On 20030608 (Sun) at 1735:14 -0700, John Zedlewski wrote:
> Hi, I'm a newbie trying to make an R program executable on UNIX, just like one 
> would write an executable perl script by putting "#!/usr/bin/perl" in the 
> first line, and so on.
> 
> It seems, though, that this would only work if I use the "BATCH" command to 
> tell R to execute the program in its first argument. This would have the 
> unfortunately side-effect of dumping all output to a file rather than stdout.
> 
> Additionally, I'd want to see only the results of "print" statements on 
> stdout, not all off R's output, just as when you source a script with 
> echo=FALSE.

I've seen the other replies, but thought this might be of interest too:
I hacked a hashbang wrapper so you can start an R script with
  #! /bin/sh /usr/bin/setR
and then invoke it with command-line arguments, which get passed to the
script in a character vector called argv.

See http://www.bgl.nu/~glouis/setR.html if you're interested.  I think
the version displayed has a couple of ampersand-lt; that need to be
changed to < if you use 'save as' rather than cutting and pasting.

-- 
| G r e g  L o u i s          | gpg public key: finger     |
|   http://www.bgl.nu/~glouis |   glouis at consultronics.com |
| http://wecanstopspam.org in signatures fights junk email |



From bates at stat.wisc.edu  Mon Jun  9 14:08:23 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 09 Jun 2003 12:08:23 -0000
Subject: [R] looking for Prof Bates' file
In-Reply-To: <x2d6hn8u12.fsf@biostat.ku.dk>
References: <3EE4196E.2060309@rocketmail.com> <x2d6hn8u12.fsf@biostat.ku.dk>
Message-ID: <6rsmqj313o.fsf@bates4.stat.wisc.edu>

Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:

> "Viet Nguyen,,," <nhviet at rocketmail.com> writes:
> 
> > Hello
> > 
> > I'm reading up on fitting truncated Weibull distribution to data.
> > 
> > There are posts in 2002 that point to this presentation by Prof Bates:
> > 
> > http://www.stat.wisc.edu/~bates/JSM2001.pdf
> > 
> > but now the file is not there. I can't find it anywhere else, Google
> > doesn't have a cached copy for it.
> > 
> > Could someone please give me a copy of this file, if they have it?
> > 
> > Thanks and regards,
> > viet.
> 
> Looks like www.stat.wisc.edu is no longer identical to Doug's desktop
> machine. The file is still there, but I'm not too sure whether he
> would want us to tell the world how to get to it, so perhaps we should
> wait for him to reply...

Thanks for your interest in the file, Nguyen.  As Peter guessed, I
changed a configuration so that www.stat.wisc.edu/~bates is served
from a departmental machine instead of a private machine.  I have
copied the file JSM2001.pdf to the departmental server so the URL
             http://www.stat.wisc.edu/~bates/JSM2001.pdf
is valid again.

By the way, that PDF file opens in 'full-screen' mode, which sometimes
confuses browsers.  I recommend that you download the file then open
it in a PDF viewer application like acrobat.



From bates at stat.wisc.edu  Mon Jun  9 14:09:31 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 09 Jun 2003 12:09:31 -0000
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <20030609103544.28958.qmail@web41211.mail.yahoo.com>
References: <20030609103544.28958.qmail@web41211.mail.yahoo.com>
Message-ID: <6rptln311s.fsf@bates4.stat.wisc.edu>

You will need to install the Debian package libxml-dev before you can
install the R XML package.

Mahbub Latif <ahmlatif at yahoo.com> writes:

> I was trying to install XML package in a linux
> (dabian) machine and got the following error message.
> I am not sure whether there is any error in my linux
> installion. I appreciate suggestions to install XML
> properly in this machine. 
> 
> Thanks in advance.
> 
> Mahbub.
> 
> ######Errors#### 
> $ R CMD INSTALL -l /usr/local/lib/R/library/
> XML_0.93-4.tar.gz 
> * Installing *source* package 'XML' ...
> creating cache ./config.cache
> checking for gcc... gcc
> checking whether the C compiler (gcc  ) works... yes
> checking whether the C compiler (gcc  ) is a
> cross-compiler... no
> checking whether we are using GNU C... yes
> checking whether gcc accepts -g... yes
> checking how to run the C preprocessor... gcc -E
> checking for xml-config... no
> checking for libxml/parser.h... no
> checking for gnome-xml/parser.h... no
> checking for libxml/parser.h... (cached) no
> checking for gnome-xml/parser.h... (cached) no
> checking for libxml/parser.h... (cached) no
> Cannot find parser.h. Set the value of the environment
> variable
>     LIBXML_INCDIR
> to point to where it can be found.
> ERROR: configuration failed for package 'XML'



From andy_liaw at merck.com  Mon Jun  9 14:18:06 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 09 Jun 2003 08:18:06 -0400
Subject: [R] Basic question on applying a function to each row of a
	da taframe
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FBCF@usrymx25.merck.com>

Another neat way is:

  with(DF, foo(x, w))

HTH,
Andy

> -----Original Message-----
> From: peter leonard [mailto:pfleonard at hotmail.com]
> Sent: Sunday, June 08, 2003 4:35 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Basic question on applying a function to each row of a
> dataframe
> 
> 
> Hi,
> 
> I have a function foo(x,y) and a dataframe, DF,  comprised of 
> two vectors, x 
> & w,  as follows :
> 
>    x w
> 1  1 1
> 2  2 1
> 3  3 1
> 4  4 1
> 
> etc
> 
> 
> I would like to apply the function foo to each 'pair' within DF e.g  
> foo(1,1), foo(2,1), foo(3,1) etc
> 
> I have tried
> 
> >apply(DF,foo)
> >apply(DF[,],foo)
> >apply(DF[DF$x,DF$w],foo)
> 
> 
> However, none of the above worked. Can anyone help ?
> 
> Thanks in advance,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From SuzieBlatt at netscape.net  Mon Jun  9 14:47:49 2003
From: SuzieBlatt at netscape.net (Suzanne E. Blatt)
Date: Mon, 09 Jun 2003 08:47:49 -0400
Subject: [R] Nested anovas
Message-ID: <41D26B16.3C53397F.0D1322AF@netscape.net>


Hello,

Is anyone out there doing nested anovas?  I'm familiar with the notation to designate the nesting in SAS but can't seem to find any for R.  How can I specify the nesting levels?

Thanks,
Suzanne

__________________________________________________________________
McAfee VirusScan Online from the Netscape Network.
Comprehensive protection for your entire computer. Get your free trial today!
http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397

Get AOL Instant Messenger 5.1 free of charge.  Download Now!



From ahmlatif at yahoo.com  Mon Jun  9 15:03:55 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Mon, 9 Jun 2003 06:03:55 -0700 (PDT)
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <6rptln311s.fsf@bates4.stat.wisc.edu>
Message-ID: <20030609130355.99612.qmail@web41210.mail.yahoo.com>

Thanks a lot Prof Bates. One more thing... What Debian
packages should I install to install R packages
Rgraphviz and rhdf5?

Mahbub.

--- Douglas Bates <bates at stat.wisc.edu> wrote:
> You will need to install the Debian package
> libxml-dev before you can
> install the R XML package.
> 
> Mahbub Latif <ahmlatif at yahoo.com> writes:
> 
> > I was trying to install XML package in a linux
> > (dabian) machine and got the following error
> message.
> > I am not sure whether there is any error in my
> linux
> > installion. I appreciate suggestions to install
> XML
> > properly in this machine. 
> > 
> > Thanks in advance.
> > 
> > Mahbub.
> > 
> > ######Errors#### 
> > $ R CMD INSTALL -l /usr/local/lib/R/library/
> > XML_0.93-4.tar.gz 
> > * Installing *source* package 'XML' ...
> > creating cache ./config.cache
> > checking for gcc... gcc
> > checking whether the C compiler (gcc  ) works...
> yes
> > checking whether the C compiler (gcc  ) is a
> > cross-compiler... no
> > checking whether we are using GNU C... yes
> > checking whether gcc accepts -g... yes
> > checking how to run the C preprocessor... gcc -E
> > checking for xml-config... no
> > checking for libxml/parser.h... no
> > checking for gnome-xml/parser.h... no
> > checking for libxml/parser.h... (cached) no
> > checking for gnome-xml/parser.h... (cached) no
> > checking for libxml/parser.h... (cached) no
> > Cannot find parser.h. Set the value of the
> environment
> > variable
> >     LIBXML_INCDIR
> > to point to where it can be found.
> > ERROR: configuration failed for package 'XML'
>



From ramzi_feg at yahoo.fr  Mon Jun  9 15:37:01 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Mon, 9 Jun 2003 15:37:01 +0200 (CEST)
Subject: [R] Basic question on applying a function to each row of a da
	taframe
In-Reply-To: <3A822319EB35174CA3714066D590DCD5C4FBCF@usrymx25.merck.com>
Message-ID: <20030609133701.15604.qmail@web20301.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/3001315c/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun  9 15:47:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jun 2003 14:47:18 +0100 (BST)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.LNX.4.44.0306081830190.7491-100000@tal.stat.umu.se>
Message-ID: <Pine.LNX.4.44.0306091443220.8080-100000@gannet.stats>

On Sun, 8 Jun 2003, G?ran Brostr?m wrote:

> On Sun, 8 Jun 2003, Thomas Lumley wrote:
> 
> > On Sat, 7 Jun 2003, [ISO-8859-1] G?ran Brostr?m wrote:
> > 
> > >
> > > I need to order a long vector of integers with rather few unique values.
> > > This is very slow:
> > 
> > 
> > I think the culprit is
> > 
> > src/main/sort.c: orderVector1
> > 
> >     /* Shell sort isn't stable, but it proves to be somewhat faster
> >        to run a final insertion sort to re-order runs of ties when
> >        comparison is cheap.
> >     */
> > 
> > This also explains:
> > 
> > > aa<-sample(rep(1:10,50000))
> > > system.time( order(aa, 1:length(aa)))
> > [1] 3.67 0.01 3.68 0.00 0.00
> > > system.time( order(aa))
> > ^C
> > Timing stopped at: 49.33 0.01 49.34 0 0
> > 
> > which is perhaps the simplest work-around :).
> 

There is a simple and very much faster solution if you don't care about 
ordering of ties:

system.time(ord4 <- sort(x, method="quick", index.return = TRUE)$ix)

That is in the help, and I am very surprised you failed to find it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Jun  9 15:47:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Jun 2003 06:47:20 -0700 (PDT)
Subject: [R] Basic question on applying a function to each row of a
	dataframe 
In-Reply-To: <Law11-F101oZolmRBaY00009f20@hotmail.com>
Message-ID: <Pine.A41.4.44.0306090646190.67638-100000@homer36.u.washington.edu>

On Sun, 8 Jun 2003, peter leonard wrote:

> Hi,
>
> I have a function foo(x,y) and a dataframe, DF,  comprised of two vectors, x
> & w,  as follows :
>
>    x w
> 1  1 1
> 2  2 1
> 3  3 1
> 4  4 1
>
> etc
>
>
> I would like to apply the function foo to each 'pair' within DF e.g
> foo(1,1), foo(2,1), foo(3,1) etc
>

In R 1.7.0 there is a function mapply() for this sort of thing.

mapply("foo",DF[,1],DF[,2])

	-thomas



From ramzi_feg at yahoo.fr  Mon Jun  9 15:55:13 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Mon, 9 Jun 2003 15:55:13 +0200 (CEST)
Subject: [R] Loops question
In-Reply-To: <Pine.LNX.4.44.0306091443220.8080-100000@gannet.stats>
Message-ID: <20030609135513.61207.qmail@web20304.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/96ef4982/attachment.pl

From sundar.dorai-raj at pdf.com  Mon Jun  9 16:02:14 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 09 Jun 2003 09:02:14 -0500
Subject: [R] Basic question on applying a function to each row of a da
	taframe
References: <20030609133701.15604.qmail@web20301.mail.yahoo.com>
Message-ID: <3EE49366.30007@pdf.com>

Yes, but very slow for this example when the data.frame gets large....

R> DF <- data.frame(x = rnorm(40000), y = rnorm(40000))
R> foo <- function(z, x, y) z[x] + z[y]
R> bar <- function(x, y) x + y
R> system.time(x <- apply(DF, 1, foo, x = "x", y = "y"))
[1] 6.94 0.04 7.37   NA   NA
R> system.time(y <- with(DF, bar(x, y))) # from A. Liaw
[1] 0.01 0.00 0.01   NA   NA
R> all(x == y)
[1] TRUE
R> system.time(z <- mapply("bar", DF[,1], DF[,2]))

Timing stopped at: 145.49 0.26 156.74 NA NA
R> # not sure why mapply doesn't work here...

Best,
Sundar

Ramzi Feghali wrote:
> Another another neat way is:
>  
> 
>>DF <- data.frame(x=1:4, y=rep(1,4))
>>foo <- function(z,x,y)z[x]+z[y]
>>apply(DF,1,foo,x="x",y="y")
> 
> 1 2 3 4 
> 2 3 4 5 
>  
> or
>  
> 
>>DF <- data.frame(x=1:4, y=rep(1,4))
>>foo <- function(z)z<-x+y
>>foo(DF)
> 
> [1] 2 3 4 5
> 
> 
> "Liaw, Andy" <andy_liaw at merck.com> wrote:
> Another neat way is:
> 
> with(DF, foo(x, w))
> 
> HTH,
> Andy
> 
> 
>>-----Original Message-----
>>From: peter leonard [mailto:pfleonard at hotmail.com]
>>Sent: Sunday, June 08, 2003 4:35 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Basic question on applying a function to each row of a
>>dataframe
>>
>>
>>Hi,
>>
>>I have a function foo(x,y) and a dataframe, DF, comprised of 
>>two vectors, x 
>>& w, as follows :
>>
>>x w
>>1 1 1
>>2 2 1
>>3 3 1
>>4 4 1
>>
>>etc
>>
>>
>>I would like to apply the function foo to each 'pair' within DF e.g 
>>foo(1,1), foo(2,1), foo(3,1) etc
>>
>>I have tried
>>
>>
>>>apply(DF,foo)
>>>apply(DF[,],foo)
>>>apply(DF[DF$x,DF$w],foo)
>>
>>
>>However, none of the above worked. Can anyone help ?
>>
>>Thanks in advance,
>>Peter
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, cont... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ---------------------------------
> 
> 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Mon Jun  9 16:21:41 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jun 2003 16:21:41 +0200
Subject: [R] Loops question
In-Reply-To: <20030609135513.61207.qmail@web20304.mail.yahoo.com>
References: <20030609135513.61207.qmail@web20304.mail.yahoo.com>
Message-ID: <3EE497F5.6030104@statistik.uni-dortmund.de>

Ramzi Feghali wrote:

> 
> Hello Mr Ripley,

Whom is this mail adressed to? The mailing list R-help or Professor Ripley?


> i was waiting you to ask and if you don't mind if there is a fast way in R to do these loops made with R and that takes a week because my matrix is with thousand of rows.
> I got another method that is more fast and is utilised by many packages : interfacing with other languages", but it is only a question for information 
>  

So you are going to optimize the following code?


> foo<-function()
>  {for (i in 1:(n-1)){
>    for (k in (i+1):n){ 
>    b_0
>    for (j in 1:m){
>     if(bin[i,j]==TRUE&bin[k,j]==TRUE) b_b+1}
>          print (b)
>  } 
>      }
>   }
> 
> Thanks a lot
> Ramzi

There seems to be a logical n x m matrix called bin.
And the sollution of your problem should be solved within seconds by

  bin %*% t(bin)

where the upper triangular matrix of the result consists of the results 
for all i and k in your loops.

Uwe Ligges



From Lom at wyeth.com  Mon Jun  9 16:30:37 2003
From: Lom at wyeth.com (Mei-Chu Lo)
Date: Mon, 09 Jun 2003 10:30:37 -0400
Subject: [R] questions about nls
Message-ID: <see461db.006@ra01m42.war.wyeth.com>

Dear R users,
 
I am new in R and I want to use the nls package to analyze some
experimental data.  The data is in the attached file "data".  It is the
response "Sav" measured at different "C0".  Basically, the "C0" is a
function of C1, K2, and r, and the "Sav" is a function of C0, C1, K2,
and r. The math equations are shown in the attached file"equations". 
The parameters K2 and r are the physical properties I want to get from
the non-linear regression.  The R codes I wrote is in the attached
file"Rcode".  Basically, I wrote two functions.  First , I calculated
the C1 for different C0 at the estimated K2 and r using the binary
search method and implemented it in the function "fn1".  Then for the
calculated C1 and estimated K2 and r, I used the function "fn2" to get
the estimated Sav for different C0.  "nls" was used to minimize the
differences between the calculated Sav and observed Sav.  When I run my
R script, I got the error message " step factor reduced below minFactor"
.  If I changed the minFactor to zero, the nls continued but did not
converge (exceed the maxiteration).  I changed the tolerance to higher
value, nls finished but the fitting is bad.  From the published results,
the best fitted values for K2 and r are 0.2237 and 1.296*10^7,
respectively.  I can't get these numbers using my R script.


I know there are a lot math and R experts on the R mailing-list.  I
will appreciate it if anyone can tell me what is wrong in my R script or
in the methods I used to get these parameters.


Mei-Chu Lo   
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/c44d8de8/data.txt
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Rcode.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/c44d8de8/Rcode.txt

From rbb at nebiometrics.com  Mon Jun  9 16:46:41 2003
From: rbb at nebiometrics.com (Robert Burrows)
Date: Mon, 9 Jun 2003 10:46:41 -0400 (EDT)
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <6rptln311s.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0306091043300.1175-100000@neblt>

On 9 Jun 2003, Douglas Bates wrote:

> You will need to install the Debian package libxml-dev before you can
> install the R XML package.

and note that the package is really looking for 'libxml/parser.h'. On my
machine, for example, parser.h is in /usr/include/libxml2/libxml/, so I
have LIBXML_INCDIR=/usr/include/libxml2.

-- 
Robert Burrows, PhD
New England Biometrics
rbb at nebiometrics.com



From edd at debian.org  Mon Jun  9 16:53:56 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 09 Jun 2003 09:53:56 -0500
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <20030609130355.99612.qmail@web41210.mail.yahoo.com>
References: <6rptln311s.fsf@bates4.stat.wisc.edu>
	<20030609130355.99612.qmail@web41210.mail.yahoo.com>
Message-ID: <20030609145356.GA11232@sonny.eddelbuettel.com>

On Mon, Jun 09, 2003 at 06:03:55AM -0700, Mahbub Latif wrote:
> Thanks a lot Prof Bates. One more thing... What Debian
> packages should I install to install R packages
> Rgraphviz and rhdf5?

For any given 'foo' I usually start by 

$ apt-cache search foo		# may return lots
$ apt-cache search libfoo	# more focussed for libraries

In this case, assumimg you want standard hdf5 (and not the mpich and lam
variants that are also available) and 

$ apt-get install libhdf5-serial-dev graphviz  

but I do not know if the graphviz package has all you need.

One day we may have suitable .deb packages for this to make installation of
complex CRAN / bioC packages a litte easier.

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From spencer.graves at pdf.com  Mon Jun  9 17:01:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 09 Jun 2003 08:01:51 -0700
Subject: [R] Nested anovas
References: <41D26B16.3C53397F.0D1322AF@netscape.net>
Message-ID: <3EE4A15F.7090609@pdf.com>

Did you try "http://www.r-project.org/" -> Search -> R Site Search? I 
entered variants of "nest" and "formula for nested model" and got many 
hits, some of which mentioned "|".  I have not done that for a while, 
but I'm confident that some variant of this will work.

hth.  spencer graves

Suzanne E. Blatt wrote:
> Hello,
> 
> Is anyone out there doing nested anovas?  I'm familiar with the notation to designate the nesting in SAS but can't seem to find any for R.  How can I specify the nesting levels?
> 
> Thanks,
> Suzanne
> 
> __________________________________________________________________
> McAfee VirusScan Online from the Netscape Network.
> Comprehensive protection for your entire computer. Get your free trial today!
> http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397
> 
> Get AOL Instant Messenger 5.1 free of charge.  Download Now!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jun  9 17:06:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 09 Jun 2003 08:06:20 -0700
Subject: [R] questions about nls
References: <see461db.006@ra01m42.war.wyeth.com>
Message-ID: <3EE4A26C.2040809@pdf.com>

	  There is too much here for me to parse it right now.  Did you walk 
through the code line by line looking at the output anc checking what it 
did at each step?

	  If you don't get a satisfactory reply from someone else and you can't 
figure it out from a line-by-line analysis of your code, please simplify 
the question:  Make up a toy problem that illustrates in only a few 
lines what you don't understand.  You are more likely to get the answer 
you want if it takes seconds rather than minutes to understand your 
question.

hth.  spencer graves

Mei-Chu Lo wrote:
> Dear R users,
>  
> I am new in R and I want to use the nls package to analyze some
> experimental data.  The data is in the attached file "data".  It is the
> response "Sav" measured at different "C0".  Basically, the "C0" is a
> function of C1, K2, and r, and the "Sav" is a function of C0, C1, K2,
> and r. The math equations are shown in the attached file"equations". 
> The parameters K2 and r are the physical properties I want to get from
> the non-linear regression.  The R codes I wrote is in the attached
> file"Rcode".  Basically, I wrote two functions.  First , I calculated
> the C1 for different C0 at the estimated K2 and r using the binary
> search method and implemented it in the function "fn1".  Then for the
> calculated C1 and estimated K2 and r, I used the function "fn2" to get
> the estimated Sav for different C0.  "nls" was used to minimize the
> differences between the calculated Sav and observed Sav.  When I run my
> R script, I got the error message " step factor reduced below minFactor"
> .  If I changed the minFactor to zero, the nls continued but did not
> converge (exceed the maxiteration).  I changed the tolerance to higher
> value, nls finished but the fitting is bad.  From the published results,
> the best fitted values for K2 and r are 0.2237 and 1.296*10^7,
> respectively.  I can't get these numbers using my R script.
> 
> 
> I know there are a lot math and R experts on the R mailing-list.  I
> will appreciate it if anyone can tell me what is wrong in my R script or
> in the methods I used to get these parameters.
> 
> 
> Mei-Chu Lo   
> 
> 
> ------------------------------------------------------------------------
> 
> C0      Sav
> 0.6766	6.0875
> 1.6165	6.4249
> 1.8796	6.5374
> 2.4436	7.025
> 4.8872	7.5125
> 5.3759	7.625
> 5.7518	7.8499
> 7.218	8.0749
> 9.2105	8.1125
> 12.7067	9.4624
> 12.5939	10.025
> 16.203	11.7125
> 17.2932	12.0124
> 18.1578	12.5749
> 21.5413	12.6875
> 21.5413	13.0625
> 
> 
> ------------------------------------------------------------------------
> 
> fn1<-function(C0,k2,r){
> m<-matrix(nrow=26,ncol=length(C0))
> Ct<-vector("numeric")
> C1<-vector("numeric")
> dC<-vector("numeric")
> j=1
>   repeat{
>   C1[j]<-C0[j]/2;dC[j]<-C0[j]/4
>     repeat{
>     m[1,j]=C1[j];m[2,j]=k2*C1[j]^2
>     m[26,j]=26*(k2/2)^25*r*C1[j]^26   
> 	i=3
>           repeat{
>           m[i,j]<-i*(k2/2)^(i-1)*C1[j]^i
>           i<-i+1
>           if(i>=26) break}
> 
>      Ct[j]<-sum(m[,j])
>      if(abs(Ct[j]-C0[j])<10^-6*C0[j]) break
>     
>      if(Ct[j]>C0[j]) {
>      C1[j]=C1[j]-dC[j];dC[j]<-dC[j]/2}
>      else{
>      C1[j]=C1[j]+dC[j];dC[j]<-dC[j]/2}
>     }
> 
>          
>   j=j+1
>   if (j>length(C0)) break }  
> 
> C1
> }
> 
> 
> fn2<-function(C1,C0,k2,r){
>    m<-matrix(nrow=26,ncol=length(C0))
>    m[1,]<-5.8*(1-0.018*C0)*C1;
>    m[2,]<-2^(2/3)*5.8*(1-0.018*C0)*k2*C1^2
>    m[26,]<-42*(1-0.019*C0)*26*(k2/2)^25*r*C1^26
>      
>       j=3
>          repeat{
>          m[j,]<-j^(5/3)*5.8*(1-0.018*C0)*(k2/2)^(j-1)*C1^j
>          j=j+1
>          if(j>=26)break   
>          }
>   
>   
>      p<-vector("numeric")
>       
>      k=1
>        repeat{
>        p[k]<-sum(m[,k])/C0[k]
>        k=k+1
>        if (k>length(C0))break
>        }
> p
>    }
> 
> # load data
> 
> ass<-read.table("data.txt",header=T)
> plot(ass$C0,ass$Sav,cex=1.5, xlab="C0(mg/ml)",ylab="Sav(S)")
> 
> # give initial guess of k2 and r
> k2=0.3;r=10^6
> fit<-nls(Sav~fn2(fn1(ass$C0,k2fit,rfit),ass$C0,k2fit,rfit),data=ass,start=list(k2fit=k2,rfit=r))
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From zhuw at mail.smu.edu  Mon Jun  9 17:13:59 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Mon, 09 Jun 2003 15:13:59 -0000
Subject: [R] Questions for package ts prediction
In-Reply-To: <Pine.LNX.4.44.0306090952480.15761-100000@gannet.stats>
References: <Pine.LNX.4.44.0306090952480.15761-100000@gannet.stats>
Message-ID: <1055171637.1361.3.camel@zwang.stat.smu.edu>

On Mon, 2003-06-09 at 03:56, Prof Brian Ripley wrote:
> You have a scoping problem: the predict method needs to find the data:
> please supply an explicit newdata argument.  Your first example works
> because `y' happens to be globally visible and be the right object.
> 
Thanks. That makes sense.

> On 9 Jun 2003, zhu wang wrote:
> 
> > I am trying to write a function to return prediction values using
> > package ts. I have written three different versions since I am not sure
> > what's wrong with my func2. func and func1 return the same results.But
> > func1 and func2 don't. In particular, the only difference between
> > "func1" and "func2" is the function variable name being y and data,
> > respectively.  But running the last line of the following script will
> > give the message:
> > 
> > Error in ts(x): object is not a matrix.
> > 
> > I am confused. Also, could somebody kindly let me what's the answer if
> > any for the following sunspot example from the package help:
> > 
> > data(sunspot)
> > (sunspot.ar <- ar(sunspot.year)) 
> > # why not just sunspot.ar <- ar(sunspot.year) ?
> 
> Have you tried it?  Please do so, and you will learn the difference!
> This idiom is widely used in the R help files.
> 
I have tried it before but now I know the trick here.

> > predict(sunspot.ar, n.ahead=25)
-- 
zhu wang <zhuw at mail.smu.edu>



From duncan at research.bell-labs.com  Mon Jun  9 17:14:44 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Mon, 9 Jun 2003 11:14:44 -0400
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <Pine.LNX.4.44.0306091043300.1175-100000@neblt>;
	from rbb@nebiometrics.com on Mon, Jun 09, 2003 at 10:46:41AM -0400
References: <6rptln311s.fsf@bates4.stat.wisc.edu>
	<Pine.LNX.4.44.0306091043300.1175-100000@neblt>
Message-ID: <20030609111443.A16333@jessie.research.bell-labs.com>

Robert Burrows wrote:
> On 9 Jun 2003, Douglas Bates wrote:
> 
> > You will need to install the Debian package libxml-dev before you can
> > install the R XML package.
> 
> and note that the package is really looking for 'libxml/parser.h'. On my
> machine, for example, parser.h is in /usr/include/libxml2/libxml/, so I
> have LIBXML_INCDIR=/usr/include/libxml2.

Thanks for pointing out the fact that one can set LIBXML_INCDIR and it
is definitely a good thing. In this particular case, however, I hope
that the configuration should work automatically without any additional
manual settings. The configure script should be using

 xml2-config --cflags

and that should return -I/usr/include/libxml2 and libxml/parser.h is
then duly found within that directory.

If this isn't the case, I'd be happy to find out and change things.
Unfortunately, conflicts between libxml and libxml2, use of libxml/
and gnome-xml/, etc... make this a complex configuration.

> 
> -- 
> Robert Burrows, PhD
> New England Biometrics
> rbb at nebiometrics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From Lom at wyeth.com  Mon Jun  9 17:23:30 2003
From: Lom at wyeth.com (Mei-Chu Lo)
Date: Mon, 09 Jun 2003 11:23:30 -0400
Subject: [R] questions abtou nls
Message-ID: <see46e43.017@ra01m42.war.wyeth.com>

The attached file "equation" in my previous e-mail does not go through
(It was created by Microsoft word).  I re-type the equations in Notepad
and attach it now.


Mei-Chu Lo 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: equation.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/d959f116/equation.txt

From pgilbert at bank-banque-canada.ca  Mon Jun  9 17:23:37 2003
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 9 Jun 2003 11:23:37 -0400
Subject: [R] Introductory Resources
Message-ID: <029D5E4CC620344D9A54430DABC37F4307B6CA@BOC-EXMAIL1.bocad.bank-banque-canada.ca>

RBaskin at ahrq.gov wrote
>...
> 1) I want a test suite for R.  I noted in the messages (Date: Mon Feb 24
> 2003 - 22:18:03 EST) that Prof Ripley wrote "Well, R itself  has lots of
> tests in its test suite (see directory tests in the sources) packages..."
> but I was too stupid to find them.  
> Q1: Can someone provide directions to this test suite that 
> even an idiot can follow?

This is an area where R really shines. I still test my code with Splus (3.3) but I switched to maintaining it in R because of the test (quality assurance) facilities for packages. The tests indicated above are in $(RHOME)/tests and can be run when  you build from source (in Linux or Unix) with

  ./configure
  make
  make check

I expect what you really want is your own test suite. To do this put your code in an R package (see "Writing R Extensions") and put the tests in the tests directory. If the test code executes stop() or ends abnormally for another reason, then checking or building the package will fail with an error message indicating a problem. If you are really interested in this I will expand sometime.

Paul Gilbert

Head Statistician/Statisticien en chef, 
Department of Monetary and Financial Analysis, 
     /D?partement des ?tudes mon?taires et financiers, 
Bank of Canada/Banque du Canada
234 Wellington St., 
Ottawa, 
Canada K1A 0G9 
(613) 782-7346



From ahmlatif at yahoo.com  Mon Jun  9 17:58:03 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Mon, 9 Jun 2003 08:58:03 -0700 (PDT)
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <87k7bvuuux.fsf@jeeves.blindglobe.net>
Message-ID: <20030609155803.52625.qmail@web41202.mail.yahoo.com>

> apt-get install libhdf5-serial-dev

To install libhdf5 what I am getting....

$ apt-get install libhdf5-serial-dev
 libhdf5-serial-dev: Depends: libhdf5-serial (=
1.4.5-2) but it is not going to be installed

then I try...
$ apt-get install libhdf5-serial
 libhdf5-serial: Depends: libc6 (>= 2.3.1-1) but
2.2.5-11.5 is to be installed

That means I need libc6 2.3.1-1 or higher. How can I
get that?

Mahbub.




> 
> unless you really want the MPI or PVM versions.
> 
> best,
> -tony
> 
> -- 
> A.J. Rossini  /  rossini at u.washington.edu  / 
> rossini at scharp.org
> Biomedical/Health Informatics and Biostatistics,
> University of Washington.
> Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer
> Research Center.
> FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty
> sketchy/use Email 
> 
> CONFIDENTIALITY NOTICE: This e-mail message and any
> attachments may be
> confidential and privileged. If you received this
> message in error,
> please destroy it and notify the sender. Thank you.



From cbourne at cbourne.com  Mon Jun  9 18:05:26 2003
From: cbourne at cbourne.com (Craig Bourne)
Date: Mon, 09 Jun 2003 12:05:26 -0400
Subject: [R] OT:  Xemacs config help
Message-ID: <3EE4B046.7010507@cbourne.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/2af02c25/attachment.pl

From gb at stat.umu.se  Mon Jun  9 18:37:07 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 9 Jun 2003 18:37:07 +0200 (CEST)
Subject: [R] Ordering long vectors
In-Reply-To: <Pine.LNX.4.44.0306091443220.8080-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0306091815570.9386-100000@tal.stat.umu.se>

On Mon, 9 Jun 2003, Prof Brian Ripley wrote:

> On Sun, 8 Jun 2003, G?ran Brostr?m wrote:
> 
> > On Sun, 8 Jun 2003, Thomas Lumley wrote:
> > 
> > > On Sat, 7 Jun 2003, [ISO-8859-1] G?ran Brostr?m wrote:
> > > 
> > > >
> > > > I need to order a long vector of integers with rather few unique values.
> > > > This is very slow:
> > > 
> > > 
> > > I think the culprit is
> > > 
> > > src/main/sort.c: orderVector1
> > > 
> > >     /* Shell sort isn't stable, but it proves to be somewhat faster
> > >        to run a final insertion sort to re-order runs of ties when
> > >        comparison is cheap.
> > >     */
> > > 
> > > This also explains:
> > > 
> > > > aa<-sample(rep(1:10,50000))
> > > > system.time( order(aa, 1:length(aa)))
> > > [1] 3.67 0.01 3.68 0.00 0.00
> > > > system.time( order(aa))
> > > ^C
> > > Timing stopped at: 49.33 0.01 49.34 0 0
> > > 
> > > which is perhaps the simplest work-around :).
> > 
> 
> There is a simple and very much faster solution if you don't care about 
> ordering of ties:
> 
> system.time(ord4 <- sort(x, method="quick", index.return = TRUE)$ix)

Thanks, it is indeed much faster; about 7 times on my example. 

> That is in the help, and I am very surprised you failed to find it.

Well, I found it, but I was cooled off by the "somewhat faster than 
Shellsort" and "poor performance in the worst case", together with the 
fact that I only needed the order and not the sorted vector. Maybe a more 
positive description is in order :-). Especially on the help page of 'order'.

G.



From am.power at ucc.ie  Mon Jun  9 19:14:39 2003
From: am.power at ucc.ie (Power, Anne Marie)
Date: Mon, 9 Jun 2003 18:14:39 +0100 
Subject: [R]parameters calculated by lda and qda
Message-ID: <49AB9D0C6521D84ABD017BF83CDF44C4051B6D4C@xch1.ucc.ie>

Hi, 

I am not sure if I remember correctly a mail last week to this messageboard
where Prof Ripley said Fisher's discriminants are not used in lda within R
and that Rao's are used instead, is this true?  I looked in MASS (1996) but
I couldn't find the reference to this in chapter 12 (multivariate analysis),
the (help)lda doesn't tell me either.  Could anyone tell me where can I find
out this information as I am also interested in the method for qda?

Thanks

Anne



From ligges at statistik.uni-dortmund.de  Mon Jun  9 19:26:06 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jun 2003 19:26:06 +0200
Subject: [R]parameters calculated by lda and qda
In-Reply-To: <49AB9D0C6521D84ABD017BF83CDF44C4051B6D4C@xch1.ucc.ie>
References: <49AB9D0C6521D84ABD017BF83CDF44C4051B6D4C@xch1.ucc.ie>
Message-ID: <3EE4C32E.8020603@statistik.uni-dortmund.de>

Power, Anne Marie wrote:

> Hi, 
> 
> I am not sure if I remember correctly a mail last week to this messageboard
> where Prof Ripley said Fisher's discriminants are not used in lda within R
> and that Rao's are used instead, is this true?  I looked in MASS (1996) but
> I couldn't find the reference to this in chapter 12 (multivariate analysis),

Chapter 12 "Classification" in the 4th edition (2002) of MASS has a 
description and gives references.


> the (help)lda doesn't tell me either.  Could anyone tell me where can I find
> out this information as I am also interested in the method for qda?

See the recent edition of MASS for qda() as well.

Uwe Ligges

> Thanks
> 
> Anne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jun  9 19:28:09 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 09 Jun 2003 10:28:09 -0700
Subject: [R] converting "by" to a data.frame?
References: <3EDFBC17.7060404@pdf.com> <p05210609bb057f0cf8ba@[128.115.153.6]>
Message-ID: <3EE4C3A9.6010505@pdf.com>

Hi, Don:

	  Thanks for your suggestion to use "do.call" in my "get.Index". I 
discovered that your version actually produces cosmetically different 
answers in R 1.6.3 and S-Plus 6.1 for Windows.  Fortunately, in the 
context, this difference was unimportant.  Since yours is faster, it is 
clearly superior.

	  To check my understanding, I generalized my toy example as follows:

 > by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
+  B=rep(c("B1", "B2"), each=3), C=rep(c("C1", "C2"), each=3),
+  x=1:6, y=rep(0:1, length=6))

	  With this, your "get.Index" produced the following in R 1.6.2:

 > get.Index <- function(x, INDICES) do.call('paste',c(x[INDICES],sep=':'))
 > get.Index(by.df, c("A", "B", "C"))
[1] "A1:B1:C1" "A1:B1:C1" "A1:B1:C1" "A2:B2:C2" "A2:B2:C2" "A2:B2:C2"

In S-Plus 6.1 for Windows, I got the following:

 > get.Index <- function(x, INDICES)
do.call("paste", c(x[INDICES], sep = ":"))
 > get.Index(by.df, c("A", "B", "C"))
[1] "1:1:1" "1:1:1" "1:1:1" "2:2:2" "2:2:2" "2:2:2"

	  Fortunately, this difference is unimportant in this context, as 
"by.to.data.frame" produces the same answer in both cases.  Moreover, 
your answer converts to a single call to "paste", which means that it 
should be faster.  For someone who understands "do.call", your version 
is also easier to read.

Thanks again for your help.
Spencer Graves

######################################
Don MacQueen wrote:
 > Glad to hear it was helpful.
 >
 > You can also use the do.call trick for the paste indices business.
 >
 > Try
 >    get.Index <- function(x, INDICES) 
do.call('paste',c(x[INDICES],sep=':'))
 >
 > This works because a data frame is actually a list, albeit a special
 > kind of list, and do.call() wants a list for its second arg.
 >
 > -Don
#######################################
Thanks to Thomas Lumley, Sundar Dorai-Raj, and Don McQueen for their
suggestions.  I need the INDICES as part of the output data.frame, which
McQueen's solution provided.  I generalized his method as follows:

by.to.data.frame <-
function(x, INDICES, FUN){
# Split data.frame x on x[,INDICES]
# and lapply FUN to each data.frame subset,
# returning a data.frame
#
#  Internal functions
     get.Index <- function(x, INDICES){
	Ind <- as.character(x[,INDICES[1]])
	k <- length(INDICES)
	if(k > 1)
		Ind <- paste(Ind, get.Index(x, INDICES[-1]), sep=":")	
		Ind	
      }
      FUN2 <- function(data., INDICES, FUN){
	vec <- FUN(data.)
	Vec <- matrix(vec, nrow=1)
	dimnames(Vec) <- list(NULL, names(vec))
	cbind(data.[1,INDICES], Vec)
      }
#   Combine INDICES
      Ind <- get.Index(x, INDICES)
#   Apply ...:  Do the work.
      Split <- split(x, Ind)
      byFits <- lapply(Split, FUN2, INDICES, FUN)
#   Convert to a data.frame
      do.call('rbind',byFits) 	
}

Applying this to my toy problem produces the following:

  > by.df <- data.frame(A=rep(c("A1", "A2"), each=3),
+  B=rep(c("B1", "B2"), each=3), x=1:6, y=rep(0:1, length=6))
  >
  > by.to.data.frame(by.df, c("A", "B"), function(data.)coef(lm(y~x, 
data.)))
         A  B (Intercept)             x
A1:B1 A1 B1   0.3333333 -1.517960e-16
A2:B2 A2 B2   0.6666667  3.282015e-16

Thanks for the assistance.  I can now tackle the real problem that
generated this question.

Best Wishes,
Spencer Graves
########################################
Don MacQueen wrote:
 > Since I don't have your by.df to test with I may not have it exactly
 > right, but something along these lines should work:
 >
 > byFits <- lapply(split(by.df,paste(by.df$A,by.df$B)),
 >                  FUN=function(data.) {
 >                     tmp <- coef(lm(y~x,data.))
 >                     data.frame(A=unique(data.$A),
 >                                B=unique(data.$B),
 >                                intercept=tmp[1],
 >                                slope=tmp[2])
 >                    })
 >
 > byFitsDF <- do.call('rbind',byFits)
 >
 > That's assuming I've got all the closing parantheses in the right
 > places, since my email software (Eudora) doesn't do R syntax checking!
 >
 > This approach can get rather slow if by.df is big, or when the
 > computations in FUN are extensive (or both).
 >
 > If by.df$A has mode character (as opposed to being a factor), then
 > replacing A=unique(data.$A) with A=I(unique(data.$A)) might improve
 > performance. You want to avoid character to factor conversions when
 > using an approach like this.
 >
 > -Don
 >
 >
 > At 2:54 PM -0700 6/5/03, Spencer Graves wrote:
 >
 >> Dear R-Help:
 >>
 >>       I want to (a) subset a data.frame by several columns, (b) fit a
 >> model to each subset, and (c) store a vector of results from the fit
 >> in the columns of a data.frame.  In the past, I've used "for" loops do
 >> do this.  Is there a way to use "by"?
 >>
 >>       Consider the following example:
 >>
 >>  > byFits <- by(by.df, list(A=by.df$A, B=by.df$B),
 >> +  function(data.)coef(lm(y~x, data.)))
 >>  > byFits
 >> A: A1
 >> B: B1
 >>   (Intercept)             x
 >>  3.333333e-01 -1.517960e-16
 >> ------------------------------------------------------------
 >> A: A2
 >> B: B1
 >> NULL
 >> ------------------------------------------------------------
 >> A: A1
 >> B: B2
 >> NULL
 >> ------------------------------------------------------------
 >> A: A2
 >> B: B2
 >>  (Intercept)            x
 >> 6.666667e-01 3.282015e-16
 >>
 >>>
 >>>
 >> #############################
 >> Desired output:
 >>
 >> data.frame(A=c("A1","A2"), B=c("B1", "B2"),
 >>     .Intercept.=c(1/3, 2/3), x=c(-1.5e-16, 3.3e-16))
 >>
 >> What's the simplest way to do this?
 >> Thanks,
 >> Spencer Graves
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >
 >
 >



From tlumley at u.washington.edu  Mon Jun  9 21:16:43 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Jun 2003 12:16:43 -0700 (PDT)
Subject: [R] Basic question on applying a function to each row of a da
	taframe
In-Reply-To: <3EE49366.30007@pdf.com>
Message-ID: <Pine.A41.4.44.0306091215180.169014-100000@homer10.u.washington.edu>

On Mon, 9 Jun 2003, Sundar Dorai-Raj wrote:

> Yes, but very slow for this example when the data.frame gets large....

Indeed. However, it works when the function is not already vectorised, and
if the function is already vectorised it is unnecessary.


	-thomas



From tothri2000 at yahoo.ca  Mon Jun  9 22:06:53 2003
From: tothri2000 at yahoo.ca (ge yreyt)
Date: Mon, 9 Jun 2003 16:06:53 -0400 (EDT)
Subject: [R] estimate the number of clusters
Message-ID: <20030609200653.17522.qmail@web41608.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030609/be3ca10b/attachment.pl

From jonck at vanderkogel.net  Mon Jun  9 22:45:45 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Mon, 9 Jun 2003 22:45:45 +0200
Subject: [R] Appending elements to an array
Message-ID: <5825CD02-9ABB-11D7-AA6F-0005026E2B43@vanderkogel.net>

Hi all,
I am having a bit of trouble with the array structure of R. What I want 
to do is dynamically add/remove elements to an array. For example:
Let's say I have created an array:
 > myArray <- array(c(3,8), dim=c(1,2))
 > myArray
      [,1] [,2]
[1,]    3    8

And I now want to, for example, push an element (5,6) on to this array 
so it will read:
      [,1] [,2]
[1,]    3    8
[2,]    5    6

And then pop the first element of the array so the array now reads:
      [,1] [,2]
[1,]    5    6

How would I do this? So far I've only read how to create an array if 
you know the dimensions beforehand, but I've been unable to find how to 
dynamically add/remove elements to/from an array.

I've figured out how to do this with lists and vectors, but not with 
arrays. For this particular structure I need to work with arrays.

Any help on how to do something like this would be much appreciated.
Thanks, Jonck



From tlumley at u.washington.edu  Mon Jun  9 23:01:58 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Jun 2003 14:01:58 -0700 (PDT)
Subject: [R] Appending elements to an array
In-Reply-To: <5825CD02-9ABB-11D7-AA6F-0005026E2B43@vanderkogel.net>
Message-ID: <Pine.A41.4.44.0306091358120.68056-100000@homer29.u.washington.edu>

On Mon, 9 Jun 2003, Jonck van der Kogel wrote:

> Hi all,
> I am having a bit of trouble with the array structure of R. What I want
> to do is dynamically add/remove elements to an array. For example:
> Let's say I have created an array:
>  > myArray <- array(c(3,8), dim=c(1,2))
>  > myArray
>       [,1] [,2]
> [1,]    3    8
>
> And I now want to, for example, push an element (5,6) on to this array
> so it will read:
>       [,1] [,2]
> [1,]    3    8
> [2,]    5    6
>
> And then pop the first element of the array so the array now reads:
>       [,1] [,2]
> [1,]    5    6
>
> How would I do this? So far I've only read how to create an array if
> you know the dimensions beforehand, but I've been unable to find how to
> dynamically add/remove elements to/from an array.
>
> I've figured out how to do this with lists and vectors, but not with
> arrays. For this particular structure I need to work with arrays.
>


myArray <- array(c(3,8), dim=c(1,2))

#add to the bottom
myArray <- cbind(myArray, c(5,6))

# display the top
myArray
# remove the top
myArray <- myArray[-1,]


	-thomas



From spencer.graves at pdf.com  Mon Jun  9 23:07:22 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 09 Jun 2003 14:07:22 -0700
Subject: [R] Appending elements to an array
References: <5825CD02-9ABB-11D7-AA6F-0005026E2B43@vanderkogel.net>
Message-ID: <3EE4F70A.20308@pdf.com>

Have you considered "rbind" to add?  Also, myArray[-2,] will remove row 
2 from myArray.

hope this helps.  spencer graves

Jonck van der Kogel wrote:
> Hi all,
> I am having a bit of trouble with the array structure of R. What I want 
> to do is dynamically add/remove elements to an array. For example:
> Let's say I have created an array:
>  > myArray <- array(c(3,8), dim=c(1,2))
>  > myArray
>      [,1] [,2]
> [1,]    3    8
> 
> And I now want to, for example, push an element (5,6) on to this array 
> so it will read:
>      [,1] [,2]
> [1,]    3    8
> [2,]    5    6
> 
> And then pop the first element of the array so the array now reads:
>      [,1] [,2]
> [1,]    5    6
> 
> How would I do this? So far I've only read how to create an array if you 
> know the dimensions beforehand, but I've been unable to find how to 
> dynamically add/remove elements to/from an array.
> 
> I've figured out how to do this with lists and vectors, but not with 
> arrays. For this particular structure I need to work with arrays.
> 
> Any help on how to do something like this would be much appreciated.
> Thanks, Jonck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lehmann at puk.unibe.ch  Mon Jun  9 23:30:38 2003
From: lehmann at puk.unibe.ch (Christoph Lehmann)
Date: Mon, 09 Jun 2003 21:30:38 -0000
Subject: [R] understanding eigen(): getting non-normalized eigenvectors
Message-ID: <1055194081.13907.11.camel@christophl>

Hi, dear R pros

I try to understand eigen(). I have seen, that eigen() gives the
eigenvectors normalized to unit length.

What shall I do to get the eigenvectors not normalized to unit length?

E.g. take the example:

 A
     
           [,1]       [,2]
  V1  0.7714286 -0.2571429
  V2 -0.4224490  0.1408163

Calculating eigen(A) "by hand" gives the eigenvectors (example from
Backhaus, multivariate analysis):

 0.77143  and 0.25714
-0.42245      0.14082


but even eigen(solve(Derror)%*%Dtreat, symmetric = FALSE, EISPACK =TRUE)
which according to ?eigen should not necessarily give the normalized
eigenvectors give the vectors (such as eigen()):

$vectors
           [,1]      [,2]
[1,]  0.8770963 0.3162278
[2,] -0.4803146 0.9486833


-> how can I replicate the result we get "by hand" (I ask because for
students it is "nice" to see the same results with R as the results
written in textbooks, derived "manually"?

Thanks a lot
Christoph

-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry



From Benjamin.STABLER at odot.state.or.us  Mon Jun  9 23:57:34 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Mon, 9 Jun 2003 14:57:34 -0700 
Subject: [R] ESRI shapefiles and EMME/2 packages
Message-ID: <76A000A82289D411952F001083F9DD06047FE10C@exsalem4-bu.odot.state.or.us>

I just uploaded two packages to CRAN.

shapefiles_0.1.tar.gz - functions to read and write ESRI shapefiles
(including dbfs)
emme2_0.1.tar.gz - functions to read binary data from an EMME/2 databank
data (EMME/2 is a transportation modeling program)

Please let me know if you find any bugs or have some suggestions.  Thanks.

Regards,
Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From pfleonard at hotmail.com  Tue Jun 10 02:05:17 2003
From: pfleonard at hotmail.com (peter leonard)
Date: Mon, 09 Jun 2003 17:05:17 -0700
Subject: [R] Basic question on applying a function to each row of a
	dataframe
Message-ID: <Law11-F110sAeeUqq7v00068e16@hotmail.com>

This works fine. Thanks everybody for all your help,

Peter



>From: Thomas Lumley <tlumley at u.washington.edu>
>To: peter leonard <pfleonard at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Basic question on applying a function to each row of a 
>dataframe Date: Mon, 9 Jun 2003 06:47:20 -0700 (PDT)
>
>On Sun, 8 Jun 2003, peter leonard wrote:
>
> > Hi,
> >
> > I have a function foo(x,y) and a dataframe, DF,  comprised of two 
>vectors, x
> > & w,  as follows :
> >
> >    x w
> > 1  1 1
> > 2  2 1
> > 3  3 1
> > 4  4 1
> >
> > etc
> >
> >
> > I would like to apply the function foo to each 'pair' within DF e.g
> > foo(1,1), foo(2,1), foo(3,1) etc
> >
>
>In R 1.7.0 there is a function mapply() for this sort of thing.
>
>mapply("foo",DF[,1],DF[,2])
>
>	-thomas
>



From vietnguyen at fastmail.fm  Tue Jun 10 03:17:23 2003
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Tue, 10 Jun 2003 11:17:23 +1000
Subject: [R] looking for Prof Bates' file
References: <200306091007.h59A1muN002541@hypatia.math.ethz.ch>
Message-ID: <3EE531A3.9050904@fastmail.fm>


Actually the file is there when I check today. The server was down 
yesterday.

Thanks,
viet

> Message: 31 Date: 09 Jun 2003 11:44:25 +0200 From: Peter Dalgaard BSA 
> <p.dalgaard at biostat.ku.dk> Subject: Re: [R] looking for Prof Bates' 
> file To: "Viet Nguyen,,," <nhviet at rocketmail.com> Cc: 
> r-help at stat.math.ethz.ch Message-ID: <x2d6hn8u12.fsf at biostat.ku.dk> 
> Content-Type: text/plain; charset=us-ascii "Viet Nguyen,,," 
> <nhviet at rocketmail.com> writes:
>
>>> Hello
>>> 
>>> I'm reading up on fitting truncated Weibull distribution to data.
>>> 
>>> There are posts in 2002 that point to this presentation by Prof Bates:
>>> 
>>> http://www.stat.wisc.edu/~bates/JSM2001.pdf
>>> 
>>> but now the file is not there. I can't find it anywhere else, Google
>>> doesn't have a cached copy for it.
>>> 
>>> Could someone please give me a copy of this file, if they have it?
>>> 
>>> Thanks and regards,
>>> viet.
>>    
>>
>
>Looks like www.stat.wisc.edu is no longer identical to Doug's desktop
>machine. The file is still there, but I'm not too sure whether he
>would want us to tell the world how to get to it, so perhaps we should
>wait for him to reply...
>
> -- O__ ---- Peter Dalgaard Blegdamsvej 3 c/ /'_ --- Dept. of 
> Biostatistics 2200 Cph. N (*) \(*) -- University of Copenhagen Denmark 
> Ph: (+45) 35327918 ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 
> 35327907 ------------------------------
>



From glaziou at pasteur-kh.org  Tue Jun 10 05:17:47 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 10 Jun 2003 10:17:47 +0700
Subject: [R] Re: [BioC] installing XML in linux
In-Reply-To: <20030609155803.52625.qmail@web41202.mail.yahoo.com>
References: <87k7bvuuux.fsf@jeeves.blindglobe.net>
	<20030609155803.52625.qmail@web41202.mail.yahoo.com>
Message-ID: <20030610031747.GA610@pasteur-kh.org>

Mahbub Latif <ahmlatif at yahoo.com> wrote:
> To install libhdf5 what I am getting....
> 
> $ apt-get install libhdf5-serial-dev
>  libhdf5-serial-dev: Depends: libhdf5-serial (=
> 1.4.5-2) but it is not going to be installed
> 
> then I try...
> $ apt-get install libhdf5-serial
>  libhdf5-serial: Depends: libc6 (>= 2.3.1-1) but
> 2.2.5-11.5 is to be installed
> 
> That means I need libc6 2.3.1-1 or higher. How can I
> get that?


libc6-2.3.1 is in testing (sarge), while libc6-2.2.5 is
still in the stable (woody) distribution of debian. What
confuses me a bit from your message is that libhdf5 is _not_
in woody (libhdf4 is), and that the version that you are
trying to install is not even in sarge. On a sarge system,
the current package libhdf5-serial-dev has version number
1.4.4-0.2 and depends on libc6-2.2.5-13, not the newer
libc6-2.3.1.

In other words, it looks like your /etc/apt/sources.list
points towards the unstable (sid) distribution, while your
debian system has not been upgraded. My 1? piece of advice
would be to steer away from sid until you master the
intricacies of the debian packaging system.

Aside from fixing this mess, you may just retrieve the
sources and compile them against your current libc.

-- 
Philippe



From c.wild at auckland.ac.nz  Tue Jun 10 06:00:49 2003
From: c.wild at auckland.ac.nz (Chris Wild)
Date: Tue, 10 Jun 2003 16:00:49 +1200
Subject: [R] R hits the New Zealand Herald
Message-ID: <5.1.0.14.2.20030610155905.00ae3ac8@mailhost.stat.auckland.ac.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030610/dea90f33/attachment.pl

From ripley at stats.ox.ac.uk  Tue Jun 10 09:19:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jun 2003 08:19:59 +0100 (BST)
Subject: [R] understanding eigen(): getting non-normalized eigenvectors
In-Reply-To: <1055194081.13907.11.camel@christophl>
Message-ID: <Pine.LNX.4.44.0306100804040.9126-100000@gannet.stats>

Eigenvectors are defined only up to a scalar constant (assuming distinct 
eigenvalues).  However, your `by hand' answer does not pass the simple 
test Av = lambda v for some lambda.  So you cannot reproduce incorrect
answers in R!

Your example is unusual: A is of rank 1.

On 9 Jun 2003, Christoph Lehmann wrote:

> Hi, dear R pros
> 
> I try to understand eigen(). I have seen, that eigen() gives the
> eigenvectors normalized to unit length.
> 
> What shall I do to get the eigenvectors not normalized to unit length?

Multiply them by any randomly chosen non-zero scalar!

> E.g. take the example:
> 
>  A
>      
>            [,1]       [,2]
>   V1  0.7714286 -0.2571429
>   V2 -0.4224490  0.1408163
> 
> Calculating eigen(A) "by hand" gives the eigenvectors (example from
> Backhaus, multivariate analysis):
> 
>  0.77143  and 0.25714
> -0.42245      0.14082

The second is not an eigenvector of A: try it!  They look like rounded
versions of A with a sign error.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dnogues at ipe.csic.es  Tue Jun 10 11:56:57 2003
From: dnogues at ipe.csic.es (=?ISO-8859-1?Q?David_Nogu=E9s?=)
Date: Tue, 10 Jun 2003 11:56:57 +0200
Subject: [R] binomial GAM ROC
Message-ID: <3EE5AB69.7060101@ipe.csic.es>

Hello

Im a beginner on R and I would like how to develop a ROC statistic to 
evaluate a GAM model with a binomial distribution (Im using  mgcv package)

Thanks in advance

-- 
David Nogu?s Bravo

Functional Ecology and Biodiversity Department
Pyrenean Institute of Ecology
Spanish Research Council

Av. Monta?ana 1005
Zaragoza - CP 50059
976716030 - 976716019 (fax)



From John.Marsland at CommerzbankIB.com  Tue Jun 10 14:23:35 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Tue, 10 Jun 2003 13:23:35 +0100
Subject: [R] c(...) and methods
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E7E2@xmx8lonib.lonib.commerzbank.com>

I have been writing some S4 classes and have a problem about how I might
pass a signature to "c()".

Take the following example:

setClass("collection", representation("list", date="POSIXt"))

x <- new("collection", list(1,2,3), date=Sys.time())
y <- new("collection", list(4,5,6), date=Sys.time())

obviously, I can do c(x,y), but this wil be of class "list"

I would like to do something like:

setMethod("c", signature("collection"),
    function(...) {
        if(e1 at date!=e2 at date) stop("at different dates!")
        res <- c(as(e1, "list"),as(e2, "list"))
        new("collection", res, date = e1 at date)
        })

but c() takes "..." as its arguments and I don't know how I might reference
that with a signature and appropriate arguments etc.

Has anybody an ideas? I presume it doesn't matter that c() is
.Primative("c")?

Regards, 

John Marsland


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ This ... {{dropped}}



From hi_ono2001 at ybb.ne.jp  Tue Jun 10 14:31:37 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Tue, 10 Jun 2003 21:31:37 +0900
Subject: [R] ESRI shapefiles and EMME/2 packages
References: <76A000A82289D411952F001083F9DD06047FE10C@exsalem4-bu.odot.state.or.us>
Message-ID: <000d01c32f4c$3ef1cd80$818001db@webgis>

Hi.

 Currently I couldn't  your uploaded packages in CRAN at Austria.

 Where could I get them currently?

 And do you know Rmap(http://www.maths.lancs.ac.uk/Software/Rmap/) which
supports shape files and other geo-spatial data formats with map projection?

 Regards.

----- Original Message ----- 
From: <Benjamin.STABLER at odot.state.or.us>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 10, 2003 6:57 AM
Subject: [R] ESRI shapefiles and EMME/2 packages


> I just uploaded two packages to CRAN.
>
> shapefiles_0.1.tar.gz - functions to read and write ESRI shapefiles
> (including dbfs)
> emme2_0.1.tar.gz - functions to read binary data from an EMME/2 databank
> data (EMME/2 is a transportation modeling program)
>
> Please let me know if you find any bugs or have some suggestions.  Thanks.



From rwatkins at cornerstonelp.com  Tue Jun 10 14:42:25 2003
From: rwatkins at cornerstonelp.com (rwatkins@cornerstonelp.com)
Date: Tue, 10 Jun 2003 07:42:25 -0500
Subject: [R] Regression output labels 
Message-ID: <NDEKIJPPGJCIKBNEDOKOCEGLCCAA.rwatkins@cornerstonelp.com>

Hello to all-
 	1.	When I run a regression which implements the augmented Dickey-Fuller
test, I am confused about the names given to the regressors in the output.
I understand what "xGE" stands for in a standard "lm" test involving an
independent variable GE for instance, but if I lags and or differences are
included in the model, what do the following "output" stand for:
	 	"xlag(x,-1)GE"
  		"xD.GE"
  		"xD.lag(diff(x), -i)GE"
  		"xD.D.lag(diff(x), -i)GE"
	Thanks for the clarifications -- I don't want to "misspeculate" on the
actual interpretations, here...

Also...
	2.	When an Engle-Granger test is run on multiple independent variables,
only one cointegration vector is returned.  Can one tell "which vector" --
or what two variables' relationship  -- is being identified for the R
output.  Likewise, if I run a Johansen test, does R "tell me" specifically
which pairs of variables are cointegrated or do I just get the rank?

	Thanks to all for your time and consideration.



From jonck at vanderkogel.net  Tue Jun 10 16:37:29 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Tue, 10 Jun 2003 16:37:29 +0200
Subject: [R] SOM random seed
Message-ID: <10171EF0-9B51-11D7-B4E3-0005026E2B43@vanderkogel.net>

Hi all,
I have a question about the SOM routine. You can either supply the 
initial representatives for the lattice yourself or else they are 
chosen randomly from the dataset. Is it possible to pass the 
random-seed as an argument somehow, when choosing the random 
initialisation of the lattice?
As it is now, each time I run a SOM on a dataset with the same settings 
the resulting SOM will still be slightly different due to the random 
initialisation.
Thanks for any help, Jonck



From ripley at stats.ox.ac.uk  Tue Jun 10 17:00:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jun 2003 16:00:35 +0100 (BST)
Subject: [R] SOM random seed
In-Reply-To: <10171EF0-9B51-11D7-B4E3-0005026E2B43@vanderkogel.net>
Message-ID: <Pine.LNX.4.44.0306101555500.2525-100000@gannet.stats>

First, *which* SOM routine, from which package?
Credit where credit is due and all that ....

If you mean the one in class(VR), it uses the standard R random number
generation.  You don't need to `pass the random-seed as an argument
somehow', just set the seed as you would anyway (e.g. via set.seed).

For example

     library(MASS)
     lcrabs <- log(crabs[, 4:8])
     crabs.grp <- factor(c("B", "b", "O", "o")[rep(1:4, rep(50,4))])
     gr <- somgrid(topo = "hexagonal")
     old.seed <- .Random.seed  ## save old seed if you want to
     set.seed(123)
     crabs.som <- SOM(lcrabs, gr)
     .Random.seed <- old.seed  ## restore the seed

On Tue, 10 Jun 2003, Jonck van der Kogel wrote:

> I have a question about the SOM routine. You can either supply the 
> initial representatives for the lattice yourself or else they are 
> chosen randomly from the dataset. Is it possible to pass the 
> random-seed as an argument somehow, when choosing the random 
> initialisation of the lattice?
> As it is now, each time I run a SOM on a dataset with the same settings 
> the resulting SOM will still be slightly different due to the random 
> initialisation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bernhard.Pfaff at drkw.com  Tue Jun 10 17:05:52 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 10 Jun 2003 17:05:52 +0200
Subject: [R] Regression output labels
Message-ID: <18D602BD42B7E24EB810D6454A58DB90047303F9@ibfftce505.is.de.dresdnerkb.com>

> Hello to all-
>  	1.	When I run a regression which implements the 
> augmented Dickey-Fuller
> test, I am confused about the names given to the regressors 
> in the output.
> I understand what "xGE" stands for in a standard "lm" test 
> involving an
> independent variable GE for instance, but if I lags and or 
> differences are
> included in the model, what do the following "output" stand for:
> 	 	"xlag(x,-1)GE"
>   		"xD.GE"
>   		"xD.lag(diff(x), -i)GE"
>   		"xD.D.lag(diff(x), -i)GE"
> 	Thanks for the clarifications -- I don't want to 
> "misspeculate" on the
> actual interpretations, here...
> 

Hello rwatkins,

I assume that you are referring to the function:
http://www.econ.uiuc.edu/~econ472/adf.R.txt


"adf"<-
function(x, L = 2, int = T, trend = T)
{
#Construct Data for Augmented Dickey Fuller Model with L lags.
#This is a modified version for R, in which the command rts was substituted
by ts.
        x <- ts(x)
        D <- diff(x)
        if(L > 0) {
                for(i in 1:L)
                        D <- ts.intersect(D, lag(diff(x),  - i))
        }
        D <- ts.intersect(lag(x, -1), D)
        if(trend == T)
                D <- ts.intersect(D, time(x))
        y <- D[, 2]
        x <- D[, -2]
        if(int == T)
                summary(lm(y ~ x))
        else summary(lm(y ~ x - 1))
}

As you can see by the code the lagged differences of the series are produced
repetitively in the for-loop; the function argument "L" sets the maximum
number of lagged differences of the time series to be tested for unit root.
After the for-loop the one period lag of the original variable is included
to "D" as first columne. In case of a trend inclusion, this variable is
included as last columne:

xD.lag(x, -1): is the one period lag of the original series;
xD.D.D.lag(diff(x), -i): is the one period lag of the differenced series;
xD.D.lag(diff(x), -i): is the two period lag of the differenced series and
xtime(x): is the linear time trend.  

Pls. note, that you should include as many lagged differenced x series
(function argument "L") until the residuals of the test regression do not
contain any significant autocorrelations (check their acfs and/or pacfs). 
> Also...
> 	2.	When an Engle-Granger test is run on multiple 
> independent variables,
> only one cointegration vector is returned.  Can one tell 
> "which vector" --
> or what two variables' relationship  -- is being identified for the R
> output.  Likewise, if I run a Johansen test, does R "tell me" 

To which function are you referring in particular?

> specifically
> which pairs of variables are cointegrated or do I just get the rank?
> 
> 	Thanks to all for your time and consideration.
> 

If you apply the Engle-Granger Two-Step-procedure in case of more than two
I(1) variables and if more than one cointegration-relationship exists you
are estimating a linear combination of the latter. In case you apply a
Johansen test (rank- or trace-test) you are only testing the space of
cointegrating vectors. In order to draw inferences about the
cointegration-relationships you must test these specifically.

As a suggestion for reading and further elaboration let me allow you to hint
you to the following literature:

1)
Hamilton, James D. (1994). Time Series Analysis. Princeton N.J.: Princeton
University
Press.

2)
Engle, Robert F., and Clive W. J. Granger. (1987). Co-Integration and Error
Correction:
Representation, Estimation and Testing. Econometrica. Vol. 55, pp. 251-276.

3)
Johansen, S?ren. (1991). Estimation and Hypotheses Testing of Cointegrating
Vectors
in Gaussian Vector Autoregressive Models. Econometrica. Vol. 59, pp.
1551-1580.

4)
Johansen, S?ren. (1995). Likelihood-Based Inference in Cointegrated Vector
Autoregressive
Models. Oxford: Oxford University Press.

HTH,
Bernhard


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From mhayashi at rmail.plala.or.jp  Tue Jun 10 17:31:43 2003
From: mhayashi at rmail.plala.or.jp (Masayoshi Hayashi)
Date: Wed, 11 Jun 2003 00:31:43 +0900
Subject: [R] fitting data to exponential distribution with glm
Message-ID: <FHEKKDCEIDIPFELPNCBDGEBJCBAA.mhayashi@rmail.plala.or.jp>

I am learning glm function, but how do you fit data using exponential
distribution with glm?
In the help file, under "Family Objects for Models", no ready made option
seems available for the distribution as well as for other distributions
satisfying GLM requirements not listed there.



From azzalini at stat.unipd.it  Tue Jun 10 17:47:57 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 10 Jun 2003 17:47:57 +0200
Subject: [R] fitting data to exponential distribution with glm
In-Reply-To: <FHEKKDCEIDIPFELPNCBDGEBJCBAA.mhayashi@rmail.plala.or.jp>
References: <FHEKKDCEIDIPFELPNCBDGEBJCBAA.mhayashi@rmail.plala.or.jp>
Message-ID: <20030610154804.206997CA824@tango.stat.unipd.it>

On Tuesday 10 June 2003 17:31, Masayoshi Hayashi wrote:
> I am learning glm function, but how do you fit data using exponential
> distribution with glm?

The Gamma family is parametrised in glm() by two parameters: 
mean and dispersion; the "dispersion" regulates the shape. 

So must fit a GLM with the Gamma family, and then produce a "summary"
with dispersion parameter set equal to 1, since this value 
corresponds to the exponential distribution in the Gamma family.

In practice:

fit <- glm(formula =...,  family = Gamma)
summary(fit,dispersion=1)   


best wishes,

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From ripley at stats.ox.ac.uk  Tue Jun 10 17:52:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jun 2003 16:52:24 +0100 (BST)
Subject: [R] fitting data to exponential distribution with glm
In-Reply-To: <FHEKKDCEIDIPFELPNCBDGEBJCBAA.mhayashi@rmail.plala.or.jp>
Message-ID: <Pine.LNX.4.44.0306101649220.2583-100000@gannet.stats>

An exponential distribution is a gamma distribution, and as far as fitting
the MLE of the coefficients all gammas give the same MLEs.  (You can
specify the dispersion and hence that the gamma is exponential when asking
for summaries, anova, etc.)

On Wed, 11 Jun 2003, Masayoshi Hayashi wrote:

> I am learning glm function, but how do you fit data using exponential
> distribution with glm?
> In the help file, under "Family Objects for Models", no ready made option
> seems available for the distribution as well as for other distributions
> satisfying GLM requirements not listed there.

Which ones did you have in mind?  The only other commonly used
distribution which gives a glm is the negative binomial with fixed shape, 
for which see the MASS book and package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Benjamin.STABLER at odot.state.or.us  Tue Jun 10 17:52:23 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 10 Jun 2003 08:52:23 -0700
Subject: [R] ESRI shapefiles and EMME/2 packages
Message-ID: <76A000A82289D411952F001083F9DD06047FE114@exsalem4-bu.odot.state.or.us>

The shapefiles and EMME/2 packages are available at:
http://www.odot.state.or.us/tddtpau/R.html

Rmap looks like a great package but it doesn't work on the OS that I use at
work (Windows NT).

Ben Stabler

>-----Original Message-----
>From: Hisaji Ono [mailto:hi_ono2001 at ybb.ne.jp]
>Sent: Tuesday, June 10, 2003 5:32 AM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] ESRI shapefiles and EMME/2 packages
>
>
>Hi.
>
> Currently I couldn't  your uploaded packages in CRAN at Austria.
>
> Where could I get them currently?
>
> And do you know 
>Rmap(http://www.maths.lancs.ac.uk/Software/Rmap/) which
>supports shape files and other geo-spatial data formats with 
>map projection?
>
> Regards.
>
>----- Original Message ----- 
>From: <Benjamin.STABLER at odot.state.or.us>
>To: <r-help at stat.math.ethz.ch>
>Sent: Tuesday, June 10, 2003 6:57 AM
>Subject: [R] ESRI shapefiles and EMME/2 packages
>
>
>> I just uploaded two packages to CRAN.
>>
>> shapefiles_0.1.tar.gz - functions to read and write ESRI shapefiles
>> (including dbfs)
>> emme2_0.1.tar.gz - functions to read binary data from an 
>EMME/2 databank
>> data (EMME/2 is a transportation modeling program)
>>
>> Please let me know if you find any bugs or have some 
>suggestions.  Thanks.
>



From maechler at stat.math.ethz.ch  Tue Jun 10 18:12:36 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 10 Jun 2003 18:12:36 +0200
Subject: [R] estimate the number of clusters
In-Reply-To: <20030609200653.17522.qmail@web41608.mail.yahoo.com>
References: <20030609200653.17522.qmail@web41608.mail.yahoo.com>
Message-ID: <16102.884.497597.789344@gargle.gargle.HOWL>

Ping,

you found another bug in silhouette.default() -- which can 
happen when there's one cluster with exactly one observation.

I'll let you know more, once I have a complete fix.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Tue Jun 10 19:03:28 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 10 Jun 2003 19:03:28 +0200
Subject: [R] estimate the number of clusters
In-Reply-To: <16102.884.497597.789344@gargle.gargle.HOWL>
References: <20030609200653.17522.qmail@web41608.mail.yahoo.com>
	<16102.884.497597.789344@gargle.gargle.HOWL>
Message-ID: <16102.3936.567151.423841@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 10 Jun 2003 18:12:36 +0200 writes:

    MM> Ping, you found another bug in silhouette.default() --
    MM> which can happen when there's one cluster with exactly
    MM> one observation.

    MM> I'll let you know more, once I have a complete fix.

The patch for this bug  {against an *installed* version of cluster}
is this :
---------------------------

--- ........cluster-version-1.7-2..../library/cluster/R/cluster	Thu Jun  5 04:00:15 2003
+++ ........fixed............................/cluster/R/cluster	Tue Jun 10 18:56:17 2003
@@ -2019,11 +2019,11 @@
         wds[iC, "cluster"] <- j
         a.i <- if(Nj > 1) colSums(dmatrix[iC, iC])/(Nj - 1) else 0 # length(a.i)= Nj
         ## minimal distances to points in all other clusters:
-        diC <- rbind(apply(dmatrix[!iC, iC], 2,
+        diC <- rbind(apply(dmatrix[!iC, iC, drop = FALSE], 2,
                            function(r) tapply(r, x[!iC], mean)))# (k-1) x Nj
         minC <- max.col(-t(diC))
         wds[iC,"neighbor"] <- clid[-j][minC]
-        b.i <- diC[cbind(minC, seq(minC))]
+        b.i <- diC[cbind(minC, seq(along = minC))]
         s.i <- (b.i - a.i) / pmax(b.i, a.i)
         wds[iC,"sil_width"] <- s.i
     }

---------------------------

i.e. you add  ", drop = FALSE" in line 2022
     and      "along = "       in line 2026
in the appropriate places.

A fixed version of cluster should appear soon, and also together
with R 1.7.1.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bertola at fastmail.fm  Tue Jun 10 19:16:26 2003
From: bertola at fastmail.fm (Rafael Bertola)
Date: Tue, 10 Jun 2003 14:16:26 -0300
Subject: [R] estimating a density by selecting the bandwidth
Message-ID: <20030610171626.BAEDE1FBEF@www.fastmail.fm>

I?ve a data set and i want fit a kernel density estimate to the data.
but using the k-nearest neighbour method.
How i do this with R.

thanks
-- 
  
  bertola at fastmail.fm

--



From ripley at stats.ox.ac.uk  Tue Jun 10 19:57:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jun 2003 18:57:00 +0100 (BST)
Subject: [R] estimating a density by selecting the bandwidth
In-Reply-To: <20030610171626.BAEDE1FBEF@www.fastmail.fm>
Message-ID: <Pine.LNX.4.44.0306101854180.11484-100000@gannet.stats>

On Tue, 10 Jun 2003, Rafael Bertola wrote:

> I?ve a data set and i want fit a kernel density estimate to the data.
> but using the k-nearest neighbour method.
> How i do this with R.

Well, you define the exact algorithm you want to use, and then you program 
it, R being a fully-featured programming language.

Your description is exceedingly vague: if you are hoping for an already 
implemented solution you might take a look at packge locfit (but that is 
only a rough fit to your `description'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Robert.Schick at noaa.gov  Tue Jun 10 21:13:57 2003
From: Robert.Schick at noaa.gov (Robert Schick)
Date: Tue, 10 Jun 2003 12:13:57 -0700
Subject: [R] color coding a legend
Message-ID: <3EE62DF5.C68A5C1C@noaa.gov>

I'm using R 1.6.2 on a Windows 2000 machine.

I've plotted the results of an MDS run labeled by a numerical ID, and
color coded by a group code:

plot(cv.mds.spr$points, type="n", main="Non-Metric Multidimensional
Scaling of SprRun CV Watersheds")
text(cv.mds.spr$points, labels = as.character(cv.wshed.id.spr), col =
codes(cv.wshed.grp), cex=.75)

Question is, how do I get the legend to match the color codes?

I have tried different permutations of the following: 
leg.txt <- c("LSSJ.NS","LSSJ.SS","US.RD","US.SF")   # the groups in
cv.wshed.grp
legend(-6.5, -2.5, leg.txt, pch="1234", col=
as.character(codes(cv.wshed.grp)))

But this only plots the codes in cv.wshed.grp as they are encountered,
not the levels. Ideally I'd like the legend to have the Group ID label,
and a filled box corresponding to the colors in the text call above.


-- 
Rob Schick
Ecologist
NOAA Fisheries
Santa Cruz Lab
110 Shaffer Road
Santa Cruz, CA 95060
Phone: 831.420.3960



From MSchwartz at medanalytics.com  Tue Jun 10 21:43:09 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 10 Jun 2003 19:43:09 -0000
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <1054704248.4122.291.camel@localhost>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
	<np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
	<1054704248.4122.291.camel@localhost>
Message-ID: <1055274112.4133.50.camel@localhost>

Hi all,

I thought that I would follow up on this thread with some "refined"
information.

As I mentioned in a prior post in this thread, I posted a query to the
OOo folks regarding the inability to replicate the IEEE representation
issue in OOo Calc.

Recall the following results:

OOo Calc 1.0.2 and 1.1 Beta2:

Cell Formula          Value
= 4.145 * 100 + 0.5   4.15000000000000000000E+02
= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
=(0.5 - 0.4 - 0.1)    0.00000000000000000000E+00


MS Excel 2002 (XP)

Cell Formula          Value
= 4.145 * 100 + 0.5   4.15000000000000000000E+02
= 0.5 - 0.4 - 0.1     0.00000000000000000000E+00
=(0.5 - 0.4 - 0.1)    -2.77555756156289000000E-17


Gnumeric 1.0.12:

Cell Formula          Value
= 4.145 * 100 + 0.5   +4.14999999999999943157E+02
= 0.5 - 0.4 - 0.1     -2.77555756156289135106E-17
*Gnumeric does not appear to allow the surrounding parens.


R 1.7.1 Beta:

> print(4.145 * 100 + 0.5, digits = 20)
[1] 414.99999999999994
> formatC(4.145 * 100 + 0.5, format = "E", digits = 20)
[1] "4.14999999999999943157E+02"

> print(0.5 - 0.4 - 0.1, digits = 20)
[1] -2.775557561562891e-17
> formatC(0.5 - 0.4 - 0.1, format = "E", digits = 20)
[1] "-2.77555756156289135106E-17"


Today, I received a reply from Niklas Nebel at Sun to indicate that
indeed OOo has included the same (or perhaps more correctly, a similar)
"optimization" that MS incorporated into Excel starting with Excel 97,
which renders results "close to zero" as zero.

In OOo Calc 1.1 Beta, Niklas indicated that the relevant source code is
in:

"sal/inc/rtl/math.hxx"

Since the full source tarball is rather large, a subset of the relevant
source code from that file is below. Note the use of the approxEqual()
function as the basis for subsequent arithmetic. 

This code should, in theory, offer some insight into what Excel is doing
as well, with the exception of course of the formula parsing issue,
which is clearly a bug.

HTH,

Marc Schwartz


***************

Copyright: 2002 by Sun Microsystems, Inc.

/** Test equality of two values with an accuracy of the magnitude of the
    given values scaled by 2^-48 (4 bits roundoff stripped).

    @ATTENTION
    approxEqual( value!=0.0, 0.0 ) _never_ yields true.
*/
inline bool approxEqual(double a, double b)
{
  if ( a == b )
    return true;
  double x = a - b;
    return (x < 0.0 ? -x : x)
            < ((a < 0.0 ? -a : a) * (1.0 / (16777216.0 * 16777216.0)));
}

/** Add two values.

If signs differ and the absolute values are equal according to
approxEqual() the method returns 0.0 instead of calculating the sum.

If you wanted to sum up multiple values it would be convenient not to
call approxAdd() for each value but instead remember the first value not
equal to 0.0, add all other values using normal + operator, and with the
result and the remembered value call approxAdd().
*/
inline double approxAdd(double a, double b)
{
  if ( ((a < 0.0 && b > 0.0) || (b < 0.0 && a > 0.0))
     && approxEqual( a, -b ) )
  return 0.0;
  return a + b;
}

/** Substract two values (a-b).

If signs are identical and the values are equal according to
approxEqual() the method returns 0.0 instead of calculating the
substraction.
*/
inline double approxSub(double a, double b)
{
if ( ((a < 0.0 && b < 0.0) || (a > 0.0 && b > 0.0)) && 
     approxEqual(a, b) )
  return 0.0;
  return a - b;
}

/** floor() method taking approxEqual() into account.

  Use for expected integer values being calculated by double functions.

    @ATTENTION
    The threshhold value is 3.55271e-015
*/

inline double approxFloor(double a)
{
  double b = floor( a );
  // The second approxEqual() is necessary for values that are near the
limit
  // of numbers representable with 4 bits stripped off. (#i12446#)
  if ( approxEqual( a - 1.0, b ) && !approxEqual( a, b ) )
    return b + 1.0;
  return b;
}

/** ceil() method taking approxEqual() into account.

Use for expected integer values being calculated by double functions.

    @ATTENTION
    The threshhold value is 3.55271e-015
*/
inline double approxCeil(double a)
{
  double b = ceil( a );
  // The second approxEqual() is necessary for values that are near the 
limit
  // of numbers representable with 4 bits stripped off. (#i12446#)
  if ( approxEqual( a + 1.0, b ) && !approxEqual( a, b ) )
    return b - 1.0;
  return b;
}



From kwan022 at stat.auckland.ac.nz  Tue Jun 10 22:17:57 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 11 Jun 2003 08:17:57 +1200 (NZST)
Subject: [R] Bootstraping with MANOVA
Message-ID: <Pine.LNX.4.44.0306110815500.12780-100000@stat57.stat.auckland.ac.nz>

Hi,

Does anyone know what the error message mean?
> Boot2.Pillai <- function(x, ind) {
+   x <- as.matrix(x[,2:ncol(x)])
+   boot.x <- as.factor(x[ind, 1])
+   boot.man <- manova(x ~ boot.x)
+   summary(manova(boot.man))[[4]][[3]]
+ }
> 
> man.res <- manova(as.matrix(pl.nosite) ~
+                   as.factor(plankton.new[,1]))$residuals
> boot2.plank <- cbind(plankton.new[, 1], man.res)
> boot.sep <- boot(boot2.plank, Boot2.Pillai, R = 1000,
+                  strata = plankton.new[, 1])
Error in summary.manova(manova(boot.man)) : 
        residuals have rank 5 < 6
Execution halted


A sample of plankton.new is as follows:
> plankton.new[sample(dim(plankton.new)[1], 5, replace = TRUE),]
   site  ACARTIA   EUTERP   GLADIO   TEMORA  FAVELLA OIKOPL
15    M 2.326336 3.168792 0.000000 0.000000 3.854852      0
47    W 2.699838 2.276462 1.799341 2.495544 2.274158      0
33    W 2.274158 3.301247 0.000000 0.000000 0.000000      0
8     M 2.875640 2.796574 0.000000 0.000000 3.051538      0
4     M 2.100371 2.796574 0.000000 0.000000 2.100371      0


pl.nosite is a data frame like plankton.new, but without the site column.

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Tue Jun 10 23:13:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jun 2003 22:13:20 +0100 (BST)
Subject: [R] Bootstraping with MANOVA
In-Reply-To: <Pine.LNX.4.44.0306110815500.12780-100000@stat57.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0306102203550.11698-100000@gannet.stats>

It means what it says!  The residuals from the manova fit have a 
degenerate distribution: that's a problem with bootstrapping.

I don't think you've done this correctly: assuming you are intending to 
bootstrap residuals you seem to have resampled the independent variable 
and not added back the mean contribution.  Compare the examples in MASS4 
or Davison & Hinkley.

Also, do remember you need to show the bootstrap is valid in each 
scenario: it is not universally valid and this one looks dodgy to me.

On Wed, 11 Jun 2003, Ko-Kang Kevin Wang wrote:

> Hi,
> 
> Does anyone know what the error message mean?
> > Boot2.Pillai <- function(x, ind) {
> +   x <- as.matrix(x[,2:ncol(x)])
> +   boot.x <- as.factor(x[ind, 1])
> +   boot.man <- manova(x ~ boot.x)
> +   summary(manova(boot.man))[[4]][[3]]
> + }
> > 
> > man.res <- manova(as.matrix(pl.nosite) ~
> +                   as.factor(plankton.new[,1]))$residuals
> > boot2.plank <- cbind(plankton.new[, 1], man.res)
> > boot.sep <- boot(boot2.plank, Boot2.Pillai, R = 1000,
> +                  strata = plankton.new[, 1])
> Error in summary.manova(manova(boot.man)) : 
>         residuals have rank 5 < 6
> Execution halted
> 
> 
> A sample of plankton.new is as follows:
> > plankton.new[sample(dim(plankton.new)[1], 5, replace = TRUE),]
>    site  ACARTIA   EUTERP   GLADIO   TEMORA  FAVELLA OIKOPL
> 15    M 2.326336 3.168792 0.000000 0.000000 3.854852      0
> 47    W 2.699838 2.276462 1.799341 2.495544 2.274158      0
> 33    W 2.274158 3.301247 0.000000 0.000000 0.000000      0
> 8     M 2.875640 2.796574 0.000000 0.000000 3.051538      0
> 4     M 2.100371 2.796574 0.000000 0.000000 2.100371      0
> 
> 
> pl.nosite is a data frame like plankton.new, but without the site column.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xma at arcturusag.com  Wed Jun 11 00:21:13 2003
From: xma at arcturusag.com (Xiao-Jun Ma)
Date: Tue, 10 Jun 2003 15:21:13 -0700
Subject: [R] speeding up 1000s of coxph regression?
Message-ID: <BBAF0DEC119BD41193C100B0D0788DFE3FCD96@GENOME>

I have a gene expression matrix n (genes) X p (cases), where n = 8000 and p
= 100. I want to fit each gene as univariate in a coxph model, i.e., fitting
8000 models. I do something like this:

res <- apply(data, 1, coxph.func)

which takes about 4 min, not bad. But I need to do large numbers of
permutations of the data (permuting the columns), for example, 2000, which
would take 5 days. I would like to know if there is way to speed this up?

Any help appreciated.

Xiao-Jun



From tlumley at u.washington.edu  Wed Jun 11 01:25:54 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 10 Jun 2003 16:25:54 -0700 (PDT)
Subject: [R] speeding up 1000s of coxph regression?
In-Reply-To: <BBAF0DEC119BD41193C100B0D0788DFE3FCD96@GENOME>
Message-ID: <Pine.A41.4.44.0306101544050.55222-100000@homer17.u.washington.edu>

On Tue, 10 Jun 2003, Xiao-Jun Ma wrote:

> I have a gene expression matrix n (genes) X p (cases), where n = 8000 and p
> = 100. I want to fit each gene as univariate in a coxph model, i.e., fitting
> 8000 models. I do something like this:
>
> res <- apply(data, 1, coxph.func)
>
> which takes about 4 min, not bad. But I need to do large numbers of
> permutations of the data (permuting the columns), for example, 2000, which
> would take 5 days. I would like to know if there is way to speed this up?
>

Calling coxph.fit directly would likely be faster.

Also, you probably don't need to do 2000 permutations on all 8000 genes: a
few hundred permutations is probably enough to decide that most of the
genes aren't interesting.

If you are going to be doing a lot of this sort of thing it might be worth
looking at the parallel processing facilities in the `snow' package.
There's a description of their use in another gene expression problem in
the new R Newsletter.


	-thomas



From Robert.Schick at noaa.gov  Wed Jun 11 01:34:32 2003
From: Robert.Schick at noaa.gov (Robert Schick)
Date: Tue, 10 Jun 2003 16:34:32 -0700
Subject: [R] Re: color coding a legend - solved?
References: <3EE62DF5.C68A5C1C@noaa.gov>
Message-ID: <3EE66B08.B17325AF@noaa.gov>

I did the following, and had it plot as needed: 
>legend(3.25, -2.5, leg.txt, pch=15, col= levels(as.factor(codes(cv.wshed.grp))))

Problem was in my (lack of) understanding of codes()

But my second question is this: is there a way to color code the points
on the first two PCA axes by group id within a biplot. Fig 11.7 in MASS
4 is my inspiration, but I don't understand how to use the col variable
within biplot. The following works within a simple plot, but not a
biplot:

>text(cv.mds.spr$points, labels = as.character(cv.wshed.id.spr), col = levels(as.factor(codes(cv.wshed.grp))), cex=.75)

Advice?


Robert Schick wrote:
> 
> I'm using R 1.6.2 on a Windows 2000 machine.
> 
> I've plotted the results of an MDS run labeled by a numerical ID, and
> color coded by a group code:
> 
> plot(cv.mds.spr$points, type="n", main="Non-Metric Multidimensional
> Scaling of SprRun CV Watersheds")
> text(cv.mds.spr$points, labels = as.character(cv.wshed.id.spr), col =
> codes(cv.wshed.grp), cex=.75)
> 
> Question is, how do I get the legend to match the color codes?
> 
> I have tried different permutations of the following:
> leg.txt <- c("LSSJ.NS","LSSJ.SS","US.RD","US.SF")   # the groups in
> cv.wshed.grp
> legend(-6.5, -2.5, leg.txt, pch="1234", col=
> as.character(codes(cv.wshed.grp)))
> 
> But this only plots the codes in cv.wshed.grp as they are encountered,
> not the levels. Ideally I'd like the legend to have the Group ID label,
> and a filled box corresponding to the colors in the text call above.
> 
> --
> Rob Schick
> Ecologist
> NOAA Fisheries
> Santa Cruz Lab
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: 831.420.3960

-- 
Rob Schick
Ecologist
NOAA Fisheries
Santa Cruz Lab
110 Shaffer Road
Santa Cruz, CA 95060
Phone: 831.420.3960



From eyecatcher_gvp at hotmail.com  Wed Jun 11 03:08:18 2003
From: eyecatcher_gvp at hotmail.com (Gwendolyn van Paasschen)
Date: Tue, 10 Jun 2003 21:08:18 -0400
Subject: [R] Getting graphs into LaTeX
Message-ID: <Law9-OE31iMCIqtQvAa00038c8c@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030610/747abc43/attachment.pl

From jonck at vanderkogel.net  Wed Jun 11 03:28:38 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Wed, 11 Jun 2003 03:28:38 +0200
Subject: [R] Multiple match function?
Message-ID: <06CE64A7-9BAC-11D7-B8A0-0005026E2B43@vanderkogel.net>

Hi all,
I have (yet another) question about a function in R. What I would like 
to do is test for the presence of a certain value in a vector, and have 
the positions that this value is at returned to me.
For example, let's say I have a vector:
x <- c(1,1,2,2,3,3,4,4)

Now I would like a function that would return positions 3 and 4 should 
I test for the value "2". Or 5 and 6 should I test for "3".

Could someone please tell me how I should do this? The "match" function 
only returns the first position that a value is found at. Of course I 
could write my own function that loops through the vector and tests for 
the presence of each value manually but it seems likely that a function 
that does this is already present in R. No need to re-invent the wheel 
:-)

Thanks very much, Jonck



From mhough at itsa.ucsf.edu  Wed Jun 11 03:30:07 2003
From: mhough at itsa.ucsf.edu (Morgan Hough)
Date: Tue, 10 Jun 2003 18:30:07 -0700 (PDT)
Subject: [R] Does the RPM for RH9 know about TCL/Tk
Message-ID: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>

Sorry for the probable repeat post but I can only search the list up to
2002 (is there a better way?). I am using the RH9 RPM from CRAN but
packages like AnalyzeFMRI say that tcltk is not found. Do I need to do
more to get Tk GUIs working on RH9 or does the RPM not have tcltk support
built in (should I compile from source). Thanks in advance.

Take care.

-Morgan



From baron at psych.upenn.edu  Wed Jun 11 03:48:05 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 10 Jun 2003 21:48:05 -0400
Subject: [R] Does the RPM for RH9 know about TCL/Tk
In-Reply-To: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
References: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
Message-ID: <20030611014805.GA28098@mail1.sas.upenn.edu>

On 06/10/03 18:30, Morgan Hough wrote:
>Sorry for the probable repeat post but I can only search the list up to
>2002 (is there a better way?). 

Yes, see my search page below.

>I am using the RH9 RPM from CRAN but
>packages like AnalyzeFMRI say that tcltk is not found. Do I need to do
>more to get Tk GUIs working on RH9 or does the RPM not have tcltk support
>built in (should I compile from source). Thanks in advance.

There was in fact some discussion of this last month.  I am not
sure of the answer.  But I installed 1.7.0 from the RPM for RH 9,
and I got the same error message when trying to get Rcmdr to
work.  I did have tcl and tk installed.  Unfortunately, I did not
do a properly controlled experiment.  I first installed tcllib,
which was not installed originally.  (That didn't help, by
itself.)  Then I re-installed R _from source_ and then everything
worked.  But I did have the basic vanilla installation of RH 9,
and I did have this problem.  So you aren't the only one.  

I still don't know whether tcllib is necessary, and whether the
RPM itself installs different things depending on what is on the
system.  (I would assume not, but I'm not sure.)

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
R page:               http://finzi.psych.upenn.edu/



From Edith.Hodgen at nzcer.org.nz  Wed Jun 11 03:47:46 2003
From: Edith.Hodgen at nzcer.org.nz (Edith Hodgen)
Date: Wed, 11 Jun 2003 13:47:46 +1200
Subject: [R] Multiple match function?
Message-ID: <see73328.041@smtp.nzcer.org.nz>

?which

HTH

Edith Hodgen
Statistician and Data Manager
New Zealand Council for Educational Research
Phone: +64-4-384 7939 x 812
Fax:      +64-4-384 7933
edith.hodgen at nzcer.org.nz

           Web site http://www.nzcer.org.nz

>>> Jonck van der Kogel <jonck at vanderkogel.net> 11/06/2003 13:28:38 >>>
Hi all,
I have (yet another) question about a function in R. What I would like 
to do is test for the presence of a certain value in a vector, and have 
the positions that this value is at returned to me.
For example, let's say I have a vector:
x <- c(1,1,2,2,3,3,4,4)

Now I would like a function that would return positions 3 and 4 should 
I test for the value "2". Or 5 and 6 should I test for "3".

Could someone please tell me how I should do this? The "match" function 
only returns the first position that a value is found at. Of course I 
could write my own function that loops through the vector and tests for 
the presence of each value manually but it seems likely that a function 
that does this is already present in R. No need to re-invent the wheel 
:-)

Thanks very much, Jonck

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From baron at psych.upenn.edu  Wed Jun 11 03:52:12 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 10 Jun 2003 21:52:12 -0400
Subject: [R] Multiple match function?
In-Reply-To: <06CE64A7-9BAC-11D7-B8A0-0005026E2B43@vanderkogel.net>
References: <06CE64A7-9BAC-11D7-B8A0-0005026E2B43@vanderkogel.net>
Message-ID: <20030611015212.GB28098@mail1.sas.upenn.edu>

On 06/11/03 03:28, Jonck van der Kogel wrote:
>Hi all,
>I have (yet another) question about a function in R. What I would like 
>to do is test for the presence of a certain value in a vector, and have 
>the positions that this value is at returned to me.
>For example, let's say I have a vector:
>x <- c(1,1,2,2,3,3,4,4)
>
>Now I would like a function that would return positions 3 and 4 should 
>I test for the value "2". Or 5 and 6 should I test for "3".

one way is
which(x==2)

I'm sure there are others.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
R page:               http://finzi.psych.upenn.edu/



From kmw at rockefeller.edu  Wed Jun 11 04:53:02 2003
From: kmw at rockefeller.edu (Knut M. Wittkowski)
Date: Tue, 10 Jun 2003 22:53:02 -0400
Subject: [R] qwilcox
Message-ID: <5.1.0.14.0.20030610205607.01f3b678@imap.rockefeller.edu>

The function 'wilcox.test' in R and S gives (almost) identical results (see 
below). 'qwilcox' however, does not:

 > qwilcox(p,5,5)

p:      0.025   0.975
--------------------
R>       3      22
S>      18      37

I originally wanted to ask a questions, but then I found the answer. Given 
the confusion I run into, I wonder if this experience is worth reporting.

The S-Plus quantiles are almost correct (they are the limits of the region 
of acceptance, rather than the quantiles). The description in the R help file

         Distribution of the Wilcoxon Rank Sum Statistic

suggests that R:qwilcox also gives quantiles for the rank sum (which the 
Wilcoxon rank sum test is based on). In fact, however, it gives quantiles 
for the u-statistic (which the Mann-Whitney test is based upon). While the 
tests are logically equivalent, the particular test statistics

         - sum(Xi>c(X,Y))        rank sum (Wilcoxon)
         - sum(Xi>c(  Y))        u statistic (Mann-Whitney)

are different (apologies for the non-standard notation). Since 
"wilcox.test" relates to the rank sums in both R and S, as does qwilcox in 
S, the name 'qwilcox' in R may be misleading. How about renaming it to 
'qmannwhitney' instead and adding 'qwilcoxon' for a function that 
corresponds to S:qwilcox?

 > x1 <- c(1,2,3,  5,6         )
 > x2 <- c(      4,    7,8,9,10)
 > sum(x1)
[1] 17
 > sum(x2)
[1] 38

R> wilcox.test(x1,x2,alternative="two.sided")
         Wilcoxon rank sum test: p-value = 0.03175

R> wilcox.exact(x1,x2,alternative="two.sided")
         Exact Wilcoxon rank sum test: p-value = 0.03175

S> wilcox.test(x1,x2,alternative="two.sided")
         Exact Wilcoxon rank-sum test: p-value = 0.0317

 > x1 <- c(1,2,  4,5,6         )
 > x2 <- c(    3,      7,8,9,10)
 > sum(x1)
[1] 18
 > sum(x2)
[1] 37

R> wilcox.test(x1,x2,alternative="two.sided")
         Wilcoxon rank sum test: p-value = 0.05556

R> wilcox.exact(x1,x2,alternative="two.sided")
         Exact Wilcoxon rank sum test: p-value = 0.05556

S> wilcox.test(x1,x2,alternative="two.sided")
         Exact Wilcoxon rank sum test: p-value = 0.0556


Knut M. Wittkowski, PhD,DSc
------------------------------------------
The Rockefeller University, GCRC
1230 York Ave #121B, Box 322, NY,NY 10021
+1(212)327-7175, +1(212)327-8450 (Fax)
kmw at rockefeller.edu
http://www.rucares.org/statist/



From rossini at blindglobe.net  Wed Jun 11 05:01:12 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 10 Jun 2003 20:01:12 -0700
Subject: [R] speeding up 1000s of coxph regression?
In-Reply-To: <Pine.A41.4.44.0306101544050.55222-100000@homer17.u.washington.edu>
	(Thomas
	Lumley's message of "Tue, 10 Jun 2003 16:25:54 -0700 (PDT)")
References: <Pine.A41.4.44.0306101544050.55222-100000@homer17.u.washington.edu>
Message-ID: <87adcp5nd3.fsf@jeeves.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:


> If you are going to be doing a lot of this sort of thing it might be worth
> looking at the parallel processing facilities in the `snow' package.
> There's a description of their use in another gene expression problem in
> the new R Newsletter.

Actually, they used the RPVM package directly; however, Thomas is
still correct, it probably would be simple to recast using SNOW.

Some hints and details can be found in a tech report by Luke Tierney,
Michael Li, and myself in the UW Biostat tech report series (can't
recall which #, but it's on http://www.bepress.com/uwbiostat/).

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
Biomedical/Health Informatics and Biostatistics, University of Washington.
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From tplate at blackmesacapital.com  Wed Jun 11 06:34:12 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 10 Jun 2003 22:34:12 -0600
Subject: [R] Multiple match function?
In-Reply-To: <06CE64A7-9BAC-11D7-B8A0-0005026E2B43@vanderkogel.net>
Message-ID: <5.2.1.1.2.20030610222520.04207328@mailhost.blackmesacapital.com>

At Wednesday 03:28 AM 6/11/2003 +0200, Jonck van der Kogel wrote:
>Hi all,
>I have (yet another) question about a function in R. What I would like to 
>do is test for the presence of a certain value in a vector, and have the 
>positions that this value is at returned to me.
>For example, let's say I have a vector:
>x <- c(1,1,2,2,3,3,4,4)
>
>Now I would like a function that would return positions 3 and 4 should I 
>test for the value "2". Or 5 and 6 should I test for "3".
>
>Could someone please tell me how I should do this? The "match" function 
>only returns the first position that a value is found at.

Actually, there are different ways of using match().  I think the following 
does what you want:

 > x <- c(1,1,2,2,3,3,4,4)
 > # how you were using it
 > match(2, x)
[1] 3
 > # switch the arguments around to get a non-NA value for each position of 
x that matches at least one element of a set
 > match(x, 2)
[1] NA NA  1  1 NA NA NA NA
 > # see which elements of x are in the set {2}
 > which(!is.na(match(x, 2)))
[1] 3 4
 > # see which elements of x are in the set {3}
 > which(!is.na(match(x, 3)))
[1] 5 6
 > # see which elements of x are in the set {2,3}
 > which(!is.na(match(x, c(2,3))))
[1] 3 4 5 6
 >

The operator %in% provides a more intuitive interface for this usage of 
match().

>Of course I could write my own function that loops through the vector and 
>tests for the presence of each value manually but it seems likely that a 
>function that does this is already present in R. No need to re-invent the 
>wheel :-)

As others have already pointed out, which() together with "==" may be all 
you need.

>Thanks very much, Jonck
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From th.fischer at gmx.net  Wed Jun 11 07:16:33 2003
From: th.fischer at gmx.net (Thomas Fischer)
Date: Wed, 11 Jun 2003 07:16:33 +0200
Subject: [R] Backward Elimination Library
Message-ID: <200306110716.33957.th.fischer@gmx.net>

Hello,

I am looking for a library or script for R to run a backward elimination 
automatically on a linear regression. Does anybody know where to find 
such a thing or has a code snipplet lying around?

Bye,
Thomas



From maechler at stat.math.ethz.ch  Wed Jun 11 09:14:14 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jun 2003 09:14:14 +0200
Subject: [R] Does the RPM for RH9 know about TCL/Tk
In-Reply-To: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
References: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
Message-ID: <16102.54982.229697.781758@gargle.gargle.HOWL>

>>>>> "Morgan" == Morgan Hough <mhough at itsa.ucsf.edu>
>>>>>     on Tue, 10 Jun 2003 18:30:07 -0700 (PDT) writes:

    Morgan> Sorry for the probable repeat post but I can only search the list up to
    Morgan> 2002 (is there a better way?). 

many better ones.  Jonathan Baron's (as he said); Robert King's
also (from, but
The automatic archives 
    https://www.stat.math.ethz.ch/pipermail/r-help/
can be reached from the official mailing list web page.
It's address is always the very last line of every message on
this list .. (!) ..

But, yes, there's text on  http://www.r-project.org/mail.html
which mentions the unformatted monthly archives at (mirrors of)
      http://cran.r-project.org/doc/mail-archives/ 
and these are definitely not kept up-to-date from some reason.

Martin



From christoph.lehmann at gmx.ch  Wed Jun 11 14:39:00 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 11 Jun 2003 14:39:00 +0200
Subject: [R] pixmap error- sorry
Message-ID: <1055335140.1140.12.camel@christophl>

I got it:

I am an idiot using the system.file command....
sorry

christoph



From MSchwartz at medanalytics.com  Wed Jun 11 15:40:25 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 11 Jun 2003 13:40:25 -0000
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <16102.53599.465973.626473@gargle.gargle.HOWL>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
	<np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
	<1054704248.4122.291.camel@localhost>
	<1055274112.4133.50.camel@localhost>
	<16102.53599.465973.626473@gargle.gargle.HOWL>
Message-ID: <1055338791.4231.64.camel@localhost>

On Wed, 2003-06-11 at 01:51, Martin Maechler wrote:
> Thanks a lot, Marc, for this extra insight.
> 
> Note that Sun / Openoffice.org 's  approxEqual(a,b)
> is not even symmetric in a and b  which I think is *really wrong*
> ((S's somewhat related  all.equal.numeric() method *is* symmetric))


Glad to be of help Martin.

What would be interesting, is to see an independent review of the
statistical functions in OOo Calc and Gnumeric as has been done for
Excel. Perhaps McCullough (who has done many of the published articles
on Excel) or someone else with the time might do this at some point.

I raise this given the increasing presence of Linux and/or OOo,
especially in light of some of the recent high profile wins over MS such
as Munich. I can envision a situation where OOo Calc and Gnumeric are
increasingly used in place of Excel as the "application of choice" where
basic statistics are being taught and used.

As Cryer stated in his review of Excel:

"Friends don't let friends use Excel for statistics"

;-)


Best regards,

Marc



From jmc at research.bell-labs.com  Wed Jun 11 15:52:58 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Wed, 11 Jun 2003 09:52:58 -0400
Subject: [R] c(...) and methods
References: <8CBAA121CEB4D5118CB200508BB2BBEF0317E7E2@xmx8lonib.lonib.commerzbank.com>
Message-ID: <3EE7343A.7FFC008@research.bell-labs.com>

The "..." formal argument can't be used directly in a method signature.

The whole "..." mechanism is somewhat outside the regular S language. 
It can only be passed down in another call, and then the effect is a
macro-like substitution.  Unlike other formal arguments, "..." is not an
object in the usual sense, so defining methods based on its class would
not be meaningful.

There are some partial workarounds, but first we have to understand what
is really the intention.

In some functions (max(), for example), what happens is an operation on
one argument at a time, and then some combination of the results.  
There is a (more or less) OK workaround in this case--see page 351 of
"Programming with Data".

The case of c() is different.  What DO we want here?  Do the arguments
have to be of the same class?  Or do we claim  to combine a "collection"
object with anything else?  Probably not the second case, and hopefully
not also, since that one will be hard to manage.

Your example suggests that combining two collection objects is the main
goal.  For this, you need a function that takes two ordinary arguments,
and "..." to soak up others.

You _could_ redefine the function c() in this form, but a more cautious
approach is to define a new function, glue(), that behaves like c():

R> glue <- function(e1, e2, ...) c(e1, e2, ...)

We need to make this a generic function, define a method for two
collections, and also a method for a single argument:

R> setMethod("glue", signature(e2 = "missing"), function(e1, e2, ...)
e1)
[1] "glue"

R> setMethod("glue", signature(e1 = "collection", e2 = "collection"),
glueCollections)
[1] "glue"

(The function glueCollections is the function(e1, e2, ...) that was the
method in your mail.)

With this definition, any number of collections can be glue'd and remain
collections.  Glue-ing collections to anything else just invokes c() and
returns a list.

R> now <- Sys.time()
R> x <- new("collection", list(1, 2, 3), date = now)
R> y <- new("collection", list(4, 5, 6), date = now)

R> xy <- glue(x,y)
R> class(xy)
[1] "collection"
R> xyy <- glue(x,y,y)
R> class(xyy)
[1] "collection"
R> unlist(xy)
[1] 1 2 3 4 5 6
R> unlist(xyy)
[1] 1 2 3 4 5 6 4 5 6
R> xz <- glue(x, pi)
R> class(xz)
[1] "list"
R> unlist(xz)
[1] 1.000000 2.000000 3.000000 3.141593

The details will depend on what's wanted in a particular case, of
course, but the general idea is to re-think what's wanted in terms of
one or more ordinary arguments.  Not every example will work, but many
will.

John Chambers

"Marsland, John" wrote:
> 
> I have been writing some S4 classes and have a problem about how I might
> pass a signature to "c()".
> 
> Take the following example:
> 
> setClass("collection", representation("list", date="POSIXt"))
> 
> x <- new("collection", list(1,2,3), date=Sys.time())
> y <- new("collection", list(4,5,6), date=Sys.time())
> 
> obviously, I can do c(x,y), but this wil be of class "list"
> 
> I would like to do something like:
> 
> setMethod("c", signature("collection"),
>     function(...) {
>         if(e1 at date!=e2 at date) stop("at different dates!")
>         res <- c(as(e1, "list"),as(e2, "list"))
>         new("collection", res, date = e1 at date)
>         })
> 
> but c() takes "..." as its arguments and I don't know how I might reference
> that with a signature and appropriate arguments etc.
> 
> Has anybody an ideas? I presume it doesn't matter that c() is
> .Primative("c")?
> 
> Regards,
> 
> John Marsland
> 
> **********************************************************************
> This is a commercial communication from Commerzbank AG.\ \ This ... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From fischer at intellektik.informatik.tu-darmstadt.de  Wed Jun 11 17:03:47 2003
From: fischer at intellektik.informatik.tu-darmstadt.de (Thomas Fischer)
Date: Wed, 11 Jun 2003 17:03:47 +0200
Subject: [R] Backward Elimination Library
Message-ID: <200306111703.47342.fischer@intellektik.informatik.tu-darmstadt.de>

Hello,

I am looking for a library or script for R to run a backward elimination 
automatically on a linear regression. Does anybody know where to find 
such a thing or has a code snipplet lying around?

Greetings,
Thomas



From th.fischer at gmx.net  Wed Jun 11 17:04:37 2003
From: th.fischer at gmx.net (Thomas Fischer)
Date: Wed, 11 Jun 2003 17:04:37 +0200
Subject: [R] Backward Elimination Library
Message-ID: <200306111704.37892.th.fischer@gmx.net>

Hello,

I am looking for a library or script for R to run a backward elimination 
automatically on a linear regression. Does anybody know where to find 
such a thing or has a code snipplet lying around?

Greetings,
Thomas



From maechler at stat.math.ethz.ch  Wed Jun 11 08:51:11 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jun 2003 08:51:11 +0200
Subject: [R] Rounding problem R vs Excel
In-Reply-To: <1055274112.4133.50.camel@localhost>
References: <000c01c328eb$d9ae24c0$e1e52e50@oemcomputer>
	<5A1B0066-955D-11D7-9356-000393678426@cmu.edu>
	<amundv8isi17k150le62u0m38v2v0kvfdo@4ax.com>
	<00b701c329ad$14ef6460$ad002850@FSSFQCV7BGDVED>
	<np7pdv4rkhmt5ork7q2lpl4s7bjgg8hc06@4ax.com>
	<1054704248.4122.291.camel@localhost>
	<1055274112.4133.50.camel@localhost>
Message-ID: <16102.53599.465973.626473@gargle.gargle.HOWL>

Thanks a lot, Marc, for this extra insight.

Note that Sun / Openoffice.org 's  approxEqual(a,b)
is not even symmetric in a and b  which I think is *really wrong*
((S's somewhat related  all.equal.numeric() method *is* symmetric))

>>>>> "Marc" == Marc Schwartz <MSchwartz at medanalytics.com>
>>>>>     on 10 Jun 2003 14:41:53 -0500 writes:

    Marc> Hi all,

    Marc> I thought that I would follow up on this thread with some "refined"
    Marc> information.

    <.......>

    >> Copyright: 2002 by Sun Microsystems, Inc.
    >> 
    >> /** Test equality of two values with an accuracy of the magnitude of the
    >>     given values scaled by 2^-48 (4 bits roundoff stripped).
    >> 
    >>     @ATTENTION
    >>     approxEqual( value!=0.0, 0.0 ) _never_ yields true.
    >> */
    >> inline bool approxEqual(double a, double b)
    >> {
    >>   if ( a == b )
    >>     return true;
    >>   double x = a - b;
    >>     return (x < 0.0 ? -x : x)
    >>             < ((a < 0.0 ? -a : a) * (1.0 / (16777216.0 * 16777216.0)));
    >> }


Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jun 11 08:56:08 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 11 Jun 2003 08:56:08 +0200 (CEST)
Subject: [R] qwilcox
In-Reply-To: <5.1.0.14.0.20030610205607.01f3b678@imap.rockefeller.edu>
References: <5.1.0.14.0.20030610205607.01f3b678@imap.rockefeller.edu>
Message-ID: <Pine.LNX.4.51.0306110844040.30255@artemis.imbe.med.uni-erlangen.de>


On Tue, 10 Jun 2003, Knut M. Wittkowski wrote:

> The function 'wilcox.test' in R and S gives (almost) identical results (see
> below). 'qwilcox' however, does not:
>
>  > qwilcox(p,5,5)
>
> p:      0.025   0.975
> --------------------
> R>       3      22
> S>      18      37
>
> I originally wanted to ask a questions, but then I found the answer. Given
> the confusion I run into, I wonder if this experience is worth reporting.
>
> The S-Plus quantiles are almost correct (they are the limits of the region
> of acceptance, rather than the quantiles). The description in the R help file
>
>          Distribution of the Wilcoxon Rank Sum Statistic
>
> suggests that R:qwilcox also gives quantiles for the rank sum (which the
> Wilcoxon rank sum test is based on). In fact, however, it gives quantiles
> for the u-statistic (which the Mann-Whitney test is based upon). While the
> tests are logically equivalent, the particular test statistics
>
>          - sum(Xi>c(X,Y))        rank sum (Wilcoxon)
>          - sum(Xi>c(  Y))        u statistic (Mann-Whitney)
>
> are different (apologies for the non-standard notation). Since
> "wilcox.test" relates to the rank sums in both R and S,

That is not true. You did not tell us the whole story:
`wilcox.test' in S-PLUS 2000 reports a statistic of `W = 17' for your
example below whereas R says

R> wilcox.test(x1,x2,alternative="two.sided")

        Wilcoxon rank sum test

data:  x1 and x2
W = 2, p-value = 0.03175
alternative hypothesis: true mu is not equal to 0

and as one can find out easily, `wilcox.test' computes the statistic as

STATISTIC <- sum(r[seq(along = x)]) - n.x * (n.x + 1) / 2

So both R and S are consistent and ?qwilcox will tell you what exactly is
meant by `Wilcoxon rank sum':

     This distribution is obtained as follows.  Let `x' and `y' be two
     random, independent samples of size `m' and `n'. Then the Wilcoxon
     rank sum statistic is the number of all pairs `(x[i], y[j])' for
     which `y[j]' is not greater than `x[i]'.  This statistic takes
     values between `0' and `m * n', and its mean and variance are `m *
     n / 2' and `m * n * (m + n + 1) / 12', respectively.


Best,

Torsten

> as does qwilcox in
> S, the name 'qwilcox' in R may be misleading. How about renaming it to
> 'qmannwhitney' instead and adding 'qwilcoxon' for a function that
> corresponds to S:qwilcox?
>
>  > x1 <- c(1,2,3,  5,6         )
>  > x2 <- c(      4,    7,8,9,10)
>  > sum(x1)
> [1] 17
>  > sum(x2)
> [1] 38
>
> R> wilcox.test(x1,x2,alternative="two.sided")
>          Wilcoxon rank sum test: p-value = 0.03175
>
> R> wilcox.exact(x1,x2,alternative="two.sided")
>          Exact Wilcoxon rank sum test: p-value = 0.03175
>
> S> wilcox.test(x1,x2,alternative="two.sided")
>          Exact Wilcoxon rank-sum test: p-value = 0.0317
>
>  > x1 <- c(1,2,  4,5,6         )
>  > x2 <- c(    3,      7,8,9,10)
>  > sum(x1)
> [1] 18
>  > sum(x2)
> [1] 37
>
> R> wilcox.test(x1,x2,alternative="two.sided")
>          Wilcoxon rank sum test: p-value = 0.05556
>
> R> wilcox.exact(x1,x2,alternative="two.sided")
>          Exact Wilcoxon rank sum test: p-value = 0.05556
>
> S> wilcox.test(x1,x2,alternative="two.sided")
>          Exact Wilcoxon rank sum test: p-value = 0.0556
>
>
> Knut M. Wittkowski, PhD,DSc
> ------------------------------------------
> The Rockefeller University, GCRC
> 1230 York Ave #121B, Box 322, NY,NY 10021
> +1(212)327-7175, +1(212)327-8450 (Fax)
> kmw at rockefeller.edu
> http://www.rucares.org/statist/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From ripley at stats.ox.ac.uk  Wed Jun 11 08:58:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jun 2003 07:58:27 +0100 (BST)
Subject: [R] Re: color coding a legend - solved?
In-Reply-To: <3EE66B08.B17325AF@noaa.gov>
Message-ID: <Pine.LNX.4.44.0306110753180.12516-100000@gannet.stats>

On Tue, 10 Jun 2003, Robert Schick wrote:

> I did the following, and had it plot as needed: 
> >legend(3.25, -2.5, leg.txt, pch=15, col= levels(as.factor(codes(cv.wshed.grp))))
> 
> Problem was in my (lack of) understanding of codes()

A common one.  codes() is deprecated in the development version of R, 
since it is much mis-used: indeed I could find no correct use in the R 
sources nor any of the packages I examined.

> But my second question is this: is there a way to color code the points
> on the first two PCA axes by group id within a biplot. Fig 11.7 in MASS
> 4 is my inspiration, but I don't understand how to use the col variable
> within biplot. The following works within a simple plot, but not a
> biplot:
> 
> >text(cv.mds.spr$points, labels = as.character(cv.wshed.id.spr), col = levels(as.factor(codes(cv.wshed.grp))), cex=.75)
> 
> Advice?

read the help page for biplot:

     col: A vector of length 2 giving the colours for the first and
          second set of points respectively (and the corresponding
          axes). If a single colour is specified it will be used for
          both sets.

A biplot both draws axes and plots points: there is no provision to 
suppress the plotting.   I'd start the other end, plotting the points as 
in MASS, then add the variable axes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From steve at paradise.stat.tku.edu.tw  Wed Jun 11 09:00:12 2003
From: steve at paradise.stat.tku.edu.tw (Steve Chen)
Date: Wed, 11 Jun 2003 15:00:12 +0800 (CST)
Subject: [R] Announcing R_PHP_Online package
Message-ID: <Pine.LNX.4.21.0306111455360.15506-100000@paradise.stat.tku.edu.tw>

Hi all,

I am releasing version 0.1 of my R_PHP_Online package.
This is an online CGI package for running R programs via Web interface
like RWeb, but it's written in PHP and it can show graphic outputs.

The R_PHP_Online can be found at

http://steve-chen.net/R_PHP/

or at

http://steve.stat.tku.edu.tw/R_PHP/

and you can use the demo or download version 0.1 there.

Regards,

Steve Chen, 
Associate Professor, Department of Statistics,
TamKang University, Taiwan



From p.dalgaard at biostat.ku.dk  Wed Jun 11 12:02:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 11 Jun 2003 10:02:20 -0000
Subject: [R] Does the RPM for RH9 know about TCL/Tk
In-Reply-To: <20030611014805.GA28098@mail1.sas.upenn.edu>
References: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
	<20030611014805.GA28098@mail1.sas.upenn.edu>
Message-ID: <x2wufs7wlo.fsf@biostat.ku.dk>

Jonathan Baron <baron at psych.upenn.edu> writes:

> On 06/10/03 18:30, Morgan Hough wrote:
> >Sorry for the probable repeat post but I can only search the list up to
> >2002 (is there a better way?). 
> 
> Yes, see my search page below.
> 
> >I am using the RH9 RPM from CRAN but
> >packages like AnalyzeFMRI say that tcltk is not found. Do I need to do
> >more to get Tk GUIs working on RH9 or does the RPM not have tcltk support
> >built in (should I compile from source). Thanks in advance.
> 
> There was in fact some discussion of this last month.  I am not
> sure of the answer.  But I installed 1.7.0 from the RPM for RH 9,
> and I got the same error message when trying to get Rcmdr to
> work.  I did have tcl and tk installed.  Unfortunately, I did not
> do a properly controlled experiment.  I first installed tcllib,
> which was not installed originally.  (That didn't help, by
> itself.)  Then I re-installed R _from source_ and then everything
> worked.  But I did have the basic vanilla installation of RH 9,
> and I did have this problem.  So you aren't the only one.  
> 
> I still don't know whether tcllib is necessary, and whether the
> RPM itself installs different things depending on what is on the
> system.  (I would assume not, but I'm not sure.)

It shouldn't, but the RPM may be different depending on what was
present on the system upon which it was built. Martyn may have been
building on a system where tcl/tk wasn't installed, or -- there's a
bug report on bugzilla.redhat.com on this -- the build was adversely
affected by incorrectness of the tclConfig.sh and tkConfig.sh scripts.

A fairly easy experiment would be to rebuild from the source RPM on
your own system. Since this builds an RPM, it will retain the
upgradability etc. of the "official" RPM. Could you try and tell us
whether the problem remains? (Don't forget that you need a bunch of
"-devel" packages installed.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From christoph.lehmann at gmx.ch  Wed Jun 11 14:29:36 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 11 Jun 2003 14:29:36 +0200
Subject: [R] pixmap error
Message-ID: <1055334576.1140.9.camel@christophl>

with R 1.7.0 I get an error when trying to open a PNM file (converted
from bmp to PNM by means of the "convert" command in linux (imagemagick,
I suppose)


> x <-
read.pnm(system.file("/home/christoph/work/projects/ffa/EPrime/550_0.PNM", package="pixmap")[1])
Error in pm.readmagicnumber(con) : Not a PNM format file


thanks for help
chris
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From lehmann at puk.unibe.ch  Wed Jun 11 17:15:18 2003
From: lehmann at puk.unibe.ch (Christoph Lehmann)
Date: Wed, 11 Jun 2003 17:15:18 +0200
Subject: [R] pixmap question again
Message-ID: <1055344518.1140.33.camel@christophl>

I could load a picture with gray levels only:

x <- 
read.pnm("/home/christoph/work/projects/ffa/EPrime/pics_pnm/550_0_1.pnm")

> x
Pixmap image
  Type          : pixmapGrey
  Size          : 246x192
  Resolution    : 1x1
  Bounding box  : 0 0 192 246
 
> plot(x)
works fine

BUT: how can I get to the slots as "size", "grey", etc.
 
calling: 
> x$grey
NULL
> x$size
NULL
>

why this?

I need to get the grey value of each pixel and average all grey values
over the whole picture... 

thanks

christoph




-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry



From christoph.lehmann at gmx.ch  Wed Jun 11 17:15:31 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 11 Jun 2003 17:15:31 +0200
Subject: [R] pixmap question again
Message-ID: <1055344531.1140.38.camel@christophl>

I could load a picture with gray levels only:

x <- 
read.pnm("/home/christoph/work/projects/ffa/EPrime/pics_pnm/550_0_1.pnm")

> x
Pixmap image
  Type          : pixmapGrey
  Size          : 246x192
  Resolution    : 1x1
  Bounding box  : 0 0 192 246
 
> plot(x)
works fine

BUT: how can I get to the slots as "size", "grey", etc.
 
calling: 
> x$grey
NULL
> x$size
NULL
>

why this?

I need to get the grey value of each pixel and average all grey values
over the whole picture... 

thanks

christoph




-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From michel.arnaud at cirad.fr  Wed Jun 11 17:56:36 2003
From: michel.arnaud at cirad.fr (Michel ARNAUD)
Date: Wed, 11 Jun 2003 17:56:36 +0200
Subject: [R] import in R a graphics file
Message-ID: <3EE75134.AA0443F0@cirad.fr>

Hello
Is it possible to import in R a graphics file ? What sort of file (bmp,
tiff, ...) ?
Thank you


--
Michel ARNAUD
CIRAD TA60/15
73, av. Jean Fran?ois Breton
34938 MONTPELLIER CEDEX 5
tel : 04 67 59 38 34
Fax : 04 67 59 38 27


From hodgess at uhddx01.dt.uh.edu  Wed Jun 11 18:39:49 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Wed, 11 Jun 2003 11:39:49 -0500 (CDT)
Subject: [R] Number lines
Message-ID: <200306111639.LAA29705@uhddx01.dt.uh.edu>

Hi R People!

Does anyone have a function that produces number lines, please?

It's not a big deal to do, but I thought that I would avoid re-inventing the wheel, if it's already there.

Actually, I want to do a dotplot  and dotchart just doesn't do it.

Thanks!
Erin 
Version 1.7.0 for Windows



From Derek.Eder at neuro.gu.se  Wed Jun 11 19:54:11 2003
From: Derek.Eder at neuro.gu.se (Derek Eder)
Date: Wed, 11 Jun 2003 19:54:11 +0200
Subject: [R] COX PH models for event histories?
Message-ID: <see7890d.062@dss2.med.gu.se>

This is a question about the use of the Cox proportional hazards model to analyze event histories.

I am looking at the responses of sympathetic nervous system activity to a stimulus.  The activity I observe is a burst that can only occur once per heart beat cycle (e.g., a binary count).  Typically bursts occur in 60-80% of the heart cycles * sensory stimuli can modify these burst probabilities.

I give 48 stimuli-trials at random intervals and count the number of bursts associated with the stimuli.  For example, a person with 75% burst probability at rest (e.g., 36/48) may have an stimulation induced increase to 87.5% (42 bursts in 48 trials).  There are 14 subjects in each of 3 different patient groups.  Simple enough.

But what if the stimulus reactions are modified over time?  The surprise of the stimulus (electric shock) soon wears off and the responses (e.g., increased burst probability) diminish over the trials.

Intuition tells me that the Cox proportional hazard model cast as in Anderson-Gill counting formulation is a useful tool too look for possible changes in burst occurrence probability across time (48 trials).  Can one assume that non-uniform burst probabilities would manifest in the cox.zph tests of proportionality of hazards?  I also plotted the Cox model along with a Cox model of a surrogate data set, formulated by randomizing the trial times (e.g., removing any temporal dependencies) Am I on the right track?   



Thank you
	

Derek Eder


Oh yes, the relevance of this question to R ... ummmm.  Yes, what is the assignment operator in R? (Just kidding).



From uleopold at science.uva.nl  Wed Jun 11 21:27:42 2003
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: Wed, 11 Jun 2003 21:27:42 +0200 (CEST)
Subject: [R] how to string characters?
Message-ID: <1411.213.211.145.130.1055359662.squirrel@webmail.science.uva.nl>

How can I string character variables?

for example:
a<- "str"
b<- "ing"

and i want c to be "string"
how do I do that?



-- 
_______________________________________________

Ulrich Leopold MSc.

Dep. Phys. Geography and Soil Science
Inst. for Biodiversity and Ecosystem Dynamics
Faculty of Science
University of Amsterdam
Nieuwe Achtergracht 166
NL-1018 WV Amsterdam

Phone: +31-(0)20-525-7456 (7451 Sectretary)
Fax:   +31-(0)20-525-7431
Email: uleopold at science.uva.nl
http://www.frw.uva.nl/soil/Welcome.html



From anna at ptolemy.arc.nasa.gov  Wed Jun 11 22:43:00 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Wed, 11 Jun 2003 13:43:00 -0700
Subject: [R] indexing into a list
Message-ID: <200306111343.00568.anna@ptolemy.arc.nasa.gov>


I am trying to make an array of lists.  I don't think that I am doing it right 
however because I cannot access the individual elements of the lists once I 
have created the array of lists.  Can someone help?


for(i in 1:3){
     y[[i]] =  list(name[((i-1)*index+1):(i*index)])
   }

Anna



From den.duurs at lycos.com  Wed Jun 11 22:42:16 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Wed, 11 Jun 2003 13:42:16 -0700
Subject: [R] mixed-effects models for left-censored data?
Message-ID: <JPCHHECPKPMDNDAA@mailcity.com>

Dear R-helpers,

excuse me if this is not exclusively an R-related question.

I have data from a nested design, both temporally and spatially, and the reponse variable of interest is left-censored. That is, only values > "some treshold" are available, otherwise "LOW" is reported. 

Are there ways of building a linear model with both fixed and random effects, when the response variable is censored? Can the tobit model be modified to do this? Does anyone have experience with this type of dataset?

Help is much appreciated,

Remko Duursma



^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.



From maechler at stat.math.ethz.ch  Wed Jun 11 23:49:05 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Jun 2003 23:49:05 +0200
Subject: [R] Mailing lists timeout of 17 hours...
Message-ID: <16103.41937.227833.746777@gargle.gargle.HOWL>

unbelievable but true... all the lists hosted here.
It even needed a reboot of the hypatia server that had run
straight since end of February..

We'll try to find out tomorrow how this could have happened.

Yours, unhappily
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From sundar.dorai-raj at pdf.com  Wed Jun 11 23:54:30 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 11 Jun 2003 16:54:30 -0500
Subject: [R] Backward Elimination Library
References: <200306110716.33957.th.fischer@gmx.net>
Message-ID: <3EE7A516.2010300@pdf.com>

?stepAIC in package MASS.

Thomas Fischer wrote:
> Hello,
> 
> I am looking for a library or script for R to run a backward elimination 
> automatically on a linear regression. Does anybody know where to find 
> such a thing or has a code snipplet lying around?
> 
> Bye,
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Wed Jun 11 23:56:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 11 Jun 2003 14:56:38 -0700
Subject: [R] Backward Elimination Library
References: <200306110716.33957.th.fischer@gmx.net>
Message-ID: <3EE7A596.7000708@pdf.com>

Have you considered stepAIC in MASS?

hth.  spencer graves

Thomas Fischer wrote:
> Hello,
> 
> I am looking for a library or script for R to run a backward elimination 
> automatically on a linear regression. Does anybody know where to find 
> such a thing or has a code snipplet lying around?
> 
> Bye,
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From clists at perrin.socsci.unc.edu  Thu Jun 12 00:10:53 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Wed, 11 Jun 2003 18:10:53 -0400 (EDT)
Subject: [R] Text analysis question
Message-ID: <Pine.LNX.4.53.0306111810410.16755@perrin.socsci.unc.edu>

I'm grappling with a problem and would appreciate any thoughts on it.

I'm revising a paper for resubmission to a journal. For the paper, I've
coded each "turn" in a series of conversations with several binary codes.
(A turn is one package of statements made by one speaker, starting with
the beginning of the speech and ending when the speaker stops or is
interrupted.) The reviewers want me to justify the decision I made to code
each turn individually, ignoring (for this analysis) the turns that
surround each turn.

My thought is to run a logistic regression, predicting the
presence/absence of a code in a given turn, with independent variables
being the number of turns that have elapsed since each code was last used
in the conversation. No problem so far. The problem involves treating what
are essentially missing data.  If I simply omit cases in which one or more
variables is missing, it's a very conservative test, since it includes
only turns for which all codes have already occurred once in the
conversation.

An alternative is to set the number of turns that has elapsed since the
last use of code to a suitably high number--probably 1 + the total number
of turns elapsed in the conversation--which would let me include all
statements (including those that introduce codes into a conversation) but
also would inflate the influence of prior use on current use by
postulating a nonexistent use "just before" the conversation.

I hope this is clear enough to be informative. I'd be interested in any
thoughts folks might have.

Thanks,
Andy Perrin


----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From christoph.lehmann at gmx.ch  Thu Jun 12 00:18:21 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 11 Jun 2003 22:18:21 -0000
Subject: [R] pixmap question again
Message-ID: <1055369786.1146.4.camel@christophl>

I could load a picture with gray levels only:

x <- 
read.pnm("/home/christoph/work/projects/ffa/EPrime/pics_pnm/550_0_1.pnm")

> x
Pixmap image
  Type          : pixmapGrey
  Size          : 246x192
  Resolution    : 1x1
  Bounding box  : 0 0 192 246
 
> plot(x)
works fine

BUT: how can I get to the slots as "size", "grey", etc.
 
calling: 
> x$grey
NULL
> x$size
NULL
>

why this?

I need to get the grey value of each pixel and average all grey values
over the whole picture... 

thanks

christoph




-- 
Christoph Lehmann <lehmann at puk.unibe.ch>
University Hospital of Clinical Psychiatry
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From jfox at mcmaster.ca  Thu Jun 12 00:42:09 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 11 Jun 2003 18:42:09 -0400
Subject: [R] Backward Elimination Library
In-Reply-To: <200306110716.33957.th.fischer@gmx.net>
Message-ID: <5.1.0.14.2.20030611184042.01e84558@mcmail.cis.mcmaster.ca>

Dear Thomas,

At 07:16 AM 6/11/2003 +0200, Thomas Fischer wrote:

>I am looking for a library or script for R to run a backward elimination
>automatically on a linear regression. Does anybody know where to find
>such a thing or has a code snipplet lying around?

See ?step. The step function is part of the base package.

I hope that this helps,
  John
-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From Simon.Blomberg at anu.edu.au  Thu Jun 12 01:01:09 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 12 Jun 2003 09:01:09 +1000
Subject: [R] Backward Elimination Library
Message-ID: <7A3A13F416B40842BD2C1753E044B359B098F6@CASEVS02.cas.anu.edu.au>

?step

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Thomas Fischer
> [mailto:fischer at intellektik.informatik.tu-darmstadt.de]
> Sent: Thursday, 12 June 2003 1:04 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Backward Elimination Library
> 
> 
> Hello,
> 
> I am looking for a library or script for R to run a backward 
> elimination 
> automatically on a linear regression. Does anybody know where to find 
> such a thing or has a code snipplet lying around?
> 
> Greetings,
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mathob at gimr.garvan.unsw.edu.au  Thu Jun 12 01:44:06 2003
From: mathob at gimr.garvan.unsw.edu.au (Matthew Hobbs)
Date: Thu, 12 Jun 2003 09:44:06 +1000
Subject: [R] how to rotate barplot column names 
Message-ID: <200306120944.06690.mathob@gimr.garvan.unsw.edu.au>

Hi,

When I use barplot()  with a names argument the names are printed horizontally 
(i.e.parallel to the X axis).  Is there some way to rotate the names so they 
are printed vertically?

Thanks


-- 
----------------------------------------------------------------------
Matthew Hobbs

Garvan Institute of Medical Research
384 Victoria St            Ph   : (02) 9295 8327
Darlinghurst                
http://www.garvan.org.au        email:  m.hobbs at garvan.org.au



From Simon.Blomberg at anu.edu.au  Thu Jun 12 01:41:45 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 12 Jun 2003 09:41:45 +1000
Subject: [R] how to string characters?
Message-ID: <7A3A13F416B40842BD2C1753E044B359B098F7@CASEVS02.cas.anu.edu.au>

paste(a, b, sep = "")

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Ulrich Leopold [mailto:uleopold at science.uva.nl]
> Sent: Thursday, 12 June 2003 5:28 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to string characters?
> 
> 
> How can I string character variables?
> 
> for example:
> a<- "str"
> b<- "ing"
> 
> and i want c to be "string"
> how do I do that?
> 
> 
> 
> -- 
> _______________________________________________
> 
> Ulrich Leopold MSc.
> 
> Dep. Phys. Geography and Soil Science
> Inst. for Biodiversity and Ecosystem Dynamics
> Faculty of Science
> University of Amsterdam
> Nieuwe Achtergracht 166
> NL-1018 WV Amsterdam
> 
> Phone: +31-(0)20-525-7456 (7451 Sectretary)
> Fax:   +31-(0)20-525-7431
> Email: uleopold at science.uva.nl
> http://www.frw.uva.nl/soil/Welcome.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tlumley at u.washington.edu  Thu Jun 12 02:35:29 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 11 Jun 2003 17:35:29 -0700 (PDT)
Subject: [R] mixed-effects models for left-censored data?
In-Reply-To: <JPCHHECPKPMDNDAA@mailcity.com>
Message-ID: <Pine.A41.4.44.0306111731000.60474-100000@homer15.u.washington.edu>

On Wed, 11 Jun 2003, Remko Duursma wrote:

> Dear R-helpers,
>
> excuse me if this is not exclusively an R-related question.
>
> I have data from a nested design, both temporally and spatially, and the
> reponse variable of interest is left-censored. That is, only values >
> "some treshold" are available, otherwise "LOW" is reported.
>
> Are there ways of building a linear model with both fixed and random
> effects, when the response variable is censored? Can the tobit model be
> modified to do this? Does anyone have experience with this type of
> dataset?
>

For a random intercept model you could use survreg() and frailty() in the
survival package.

In general the random effects tobit model will be quite hard to fit,
involving a numerical integration whose dimension is the number of random
effects.   Some sort of EM algorithm might work.

There is a paper by Pettit in Biometrics some time ago on censored linear
mixed models -- I don't have the reference with me.

	-thomas



From fharrell at virginia.edu  Thu Jun 12 02:43:02 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 11 Jun 2003 20:43:02 -0400
Subject: [R] COX PH models for event histories?
In-Reply-To: <see7890d.062@dss2.med.gu.se>
References: <see7890d.062@dss2.med.gu.se>
Message-ID: <20030611204302.7c9cff9f.fharrell@virginia.edu>

On Wed, 11 Jun 2003 19:54:11 +0200
Derek Eder <Derek.Eder at neuro.gu.se> wrote:

> This is a question about the use of the Cox proportional hazards model to analyze event histories.
> 
> I am looking at the responses of sympathetic nervous system activity to a stimulus.  The activity I observe is a burst that can only occur once per heart beat cycle (e.g., a binary count).  Typically bursts occur in 60-80% of the heart cycles * sensory stimuli can modify these burst probabilities.
> 
> I give 48 stimuli-trials at random intervals and count the number of bursts associated with the stimuli.  For example, a person with 75% burst probability at rest (e.g., 36/48) may have an stimulation induced increase to 87.5% (42 bursts in 48 trials).  There are 14 subjects in each of 3 different patient groups.  Simple enough.
> 
> But what if the stimulus reactions are modified over time?  The surprise of the stimulus (electric shock) soon wears off and the responses (e.g., increased burst probability) diminish over the trials.
> 
> Intuition tells me that the Cox proportional hazard model cast as in Anderson-Gill counting formulation is a useful tool too look for possible changes in burst occurrence probability across time (48 trials).  Can one assume that non-uniform burst probabilities would manifest in the cox.zph tests of proportionality of hazards?  I also plotted the Cox model along with a Cox model of a surrogate data set, formulated by randomizing the trial times (e.g., removing any temporal dependencies) Am I on the right track?   
> 
> 
> 
> Thank you
> 	
> 
> Derek Eder
> 
> 
> Oh yes, the relevance of this question to R ... ummmm.  Yes, what is the assignment operator in R? (Just kidding).
> 
Derek- This avoids answering your question but in problems like this I have found pooled logistic regression can be easier to use and provide more easily interpretable predictions and their confidence intervals.  I have used cluster bootstrap variance estimators in this context to adjust for intra-subject correlations.  See 

@ARTICLE{dag90rel,
  author = {{D'Agostino}, Ralph B. and Lee, M. L. and Belanger, A. J. and
           Cupples, L. A.},
  year = 1990,
  title = {Relation of pooled logistic regression to time dependent {Cox}
          regression analysis: {The} {Framingham} {Heart} {Study}},
  journal = Statistics in Medicine,
  volume = 9,
  pages = {1501-1515},
  annote = {time-dependent covariable; repeated measures logistic
  model; person-years logistic model}
}



---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From Simon.Blomberg at anu.edu.au  Thu Jun 12 03:00:12 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 12 Jun 2003 11:00:12 +1000
Subject: [R] how to rotate barplot column names 
Message-ID: <7A3A13F416B40842BD2C1753E044B359B098F9@CASEVS02.cas.anu.edu.au>

Try barplot(..., las=2)

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Matthew Hobbs [mailto:mathob at gimr.garvan.unsw.edu.au]
> Sent: Thursday, 12 June 2003 9:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to rotate barplot column names 
> 
> 
> Hi,
> 
> When I use barplot()  with a names argument the names are 
> printed horizontally 
> (i.e.parallel to the X axis).  Is there some way to rotate 
> the names so they 
> are printed vertically?
> 
> Thanks
> 
> 
> -- 
> ----------------------------------------------------------------------
> Matthew Hobbs
> 
> Garvan Institute of Medical Research
> 384 Victoria St            Ph   : (02) 9295 8327
> Darlinghurst                
> http://www.garvan.org.au        email:  m.hobbs at garvan.org.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From rossini at blindglobe.net  Thu Jun 12 03:53:39 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 11 Jun 2003 18:53:39 -0700
Subject: [R] mixed-effects models for left-censored data?
In-Reply-To: <Pine.A41.4.44.0306111731000.60474-100000@homer15.u.washington.edu>
	(Thomas
	Lumley's message of "Wed, 11 Jun 2003 17:35:29 -0700 (PDT)")
References: <Pine.A41.4.44.0306111731000.60474-100000@homer15.u.washington.edu>
Message-ID: <8765nchxi4.fsf@jeeves.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Wed, 11 Jun 2003, Remko Duursma wrote:
>
> For a random intercept model you could use survreg() and frailty() in the
> survival package.
>
> In general the random effects tobit model will be quite hard to fit,
> involving a numerical integration whose dimension is the number of random
> effects.   Some sort of EM algorithm might work.

One huge catch with that approach is heterscedasticity, which seems to
pop its head up all too often with limit-of-detection assay data.

> There is a paper by Pettit in Biometrics some time ago on censored linear
> mixed models -- I don't have the reference with me.

There is also a paper by a fellow named Jim Hughes, in Biometrics
(late 90s?), on this exact topic -- he used single imputation, whereas
he mentioned later (private communication) that a multiple imputation
approach would be better.  The S-PLUS code (it isn't pretty) is
somewhere on his WWW page, buried deep in the U Washington
Biostatistcs WWW site.

At least it used to be.  

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
Biomedical/Health Informatics and Biostatistics, University of Washington.
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From jawegelin at ucdavis.edu  Thu Jun 12 04:04:28 2003
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Wed, 11 Jun 2003 19:04:28 -0700 (PDT)
Subject: [R] defaults in R: packages, .Rhistory
Message-ID: <Pine.GSO.4.44.0306021128270.2402-100000@vidi.ucdavis.edu>

With the current version (rw1070), every time I start R it loads a whole
bunch of packages, many of which I do not need in a typical session:

> search()
[1] ".GlobalEnv"      "package:methods" "package:ctest"   "package:mva"
[5] "package:modreg"  "package:nls"     "package:ts"      "Autoloads"
[9] "package:base"

Loading all these packages makes R slow to start up, and I presume it
uses up memory too.

Is there a reason that all these packages load automatically? Is
there some way to stop this from happening, by changing some default?

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.0
year     2003
month    04
day      16
language R

A separate question also related to defaults:

Is there a way to get R to refrain from truncating the .Rhistory file
(removing the earliest saved commands), so that .Rhistory might serve
as a permanent record of every command issued?

Thanks for any info

Jake



From wviechtb at s.psych.uiuc.edu  Thu Jun 12 04:10:40 2003
From: wviechtb at s.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Wed, 11 Jun 2003 21:10:40 -0500 (CDT)
Subject: [R] Tested Random Number Generator
Message-ID: <Pine.SOL.4.30.0305301646130.23648-100000@s.psych.uiuc.edu>

Dear All,

The editor of a journal to which I had submitted a publication asked
whether R has a "tested random number generator." My paper included
Monte Carlo simulations generating random normal and random chi-square
values.

help(rnorm) lists

     Wichura, M. J. (1988) Algorithm AS 241: The Percentage Points of
     the Normal Distribution. Applied Statistics, 37, 477-484.

as a reference, but this algorithm does not discuss the generation of
random values.

help(RNG) indicates that the "Mersenne-Twister" is the default random
number generator with reference:

     Matsumoto, M. and Nishimura, T. (1998) Mersenne Twister: A
     623-dimensionally equidistributed uniform pseudo-random number
     generator, ACM Transactions on Modeling and Computer Simulation,
     8, 3-30.

I looked at the paper, but essentially I have no expertise in assessing
whether this random number generator is "good" (which is probably a
tricky concept in the first place when dealing with RNGs). I have almost
blind faith in the developers of R (at least when it comes to something
so fundamental as a RNG) that I feel confident that it is good, but I
guess I need something more substantial at this point to back of my
beliefs! Any suggestions on how I can "show" (without having to go
through a separate study just to make this claim) that the RNG is
"tested"? Any references?

Also, I am a little uncertain about how R generates random observations
from various distributions, such as in rnorm() or rchisq(). Does it
generate random uniforms u ~ U(0,1) and then solve for x in F(x) = u)? I
would imagine that this is rather slow compared to other specialized
methods for various distributions. Any information on this would be
appreciated as well.

I guess some of this relates to the "Validation of R" discussion that
occured a while ago on this list, so this info could be of general
interest.

Thanks!

--
Wolfgang Viechtbauer



From itipenko at nexmatech.ca  Thu Jun 12 09:30:31 2003
From: itipenko at nexmatech.ca (Iouri Tipenko)
Date: Thu, 12 Jun 2003 00:30:31 -0700
Subject: [R] Code for Support Vector Clustering Algorithm
Message-ID: <3EE82C17.9020709@nexmatech.ca>

Dear R-Users,
I'm a master student in Mathematics and Statistics at Carleton 
University, Ottawa, Canada.
I'm studying Clustering methods including different related algorithms. 
One of them is Support Vector Clustering algorithm.
I was wondering whether anybody implemented this algorithm and could 
help me with the S-Plus or R computer code that I could use in my 
simulations.
I would really appreciate your help or any advise on where I can get 
this code.

Thank you very much in advance.
Elena Tipenko



From rpeng at stat.ucla.edu  Thu Jun 12 06:24:06 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Wed, 11 Jun 2003 21:24:06 -0700
Subject: [R] defaults in R: packages, .Rhistory
In-Reply-To: <Pine.GSO.4.44.0306021128270.2402-100000@vidi.ucdavis.edu>
References: <Pine.GSO.4.44.0306021128270.2402-100000@vidi.ucdavis.edu>
Message-ID: <3EE80066.5070106@stat.ucla.edu>

You can modify your sitewide Rprofile file (which for me is in 
C:\Program Files\R\rw1070\etc) and insert the line:

options(defaultPackages = character())

This should start R with no additional packages.  More information like 
this can be found in ?Startup.

-roger

Jacob Wegelin wrote:

> With the current version (rw1070), every time I start R it loads a whole
> bunch of packages, many of which I do not need in a typical session:
> 
> 
>>search()
> 
> [1] ".GlobalEnv"      "package:methods" "package:ctest"   "package:mva"
> [5] "package:modreg"  "package:nls"     "package:ts"      "Autoloads"
> [9] "package:base"
> 
> Loading all these packages makes R slow to start up, and I presume it
> uses up memory too.
> 
> Is there a reason that all these packages load automatically? Is
> there some way to stop this from happening, by changing some default?
> 
> 
>>version
> 
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    7.0
> year     2003
> month    04
> day      16
> language R
> 
> A separate question also related to defaults:
> 
> Is there a way to get R to refrain from truncating the .Rhistory file
> (removing the earliest saved commands), so that .Rhistory might serve
> as a permanent record of every command issued?
> 
> Thanks for any info
> 
> Jake
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From kldyuncanny at excite.com  Thu Jun 12 08:30:44 2003
From: kldyuncanny at excite.com (Anderson)
Date: Thu, 12 Jun 2003 07:30:44 +0100
Subject: [R] Friday 13th June
Message-ID: <E19QLdL-000154-00@bernie.ethz.ch>

Play the best prank ever, visit the following site:

http://www.geocities.com/gliwhqtyensyuimdhsg/



From kldyuncanny at excite.com  Thu Jun 12 08:30:52 2003
From: kldyuncanny at excite.com (Anderson)
Date: Thu, 12 Jun 2003 07:30:52 +0100
Subject: [R] Friday 13th June
Message-ID: <E19QLdT-000154-00@bernie.ethz.ch>

Play the best prank ever, visit the following site:

http://www.geocities.com/gliwhqtyensyuimdhsg/



From ripley at stats.ox.ac.uk  Thu Jun 12 08:36:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 07:36:02 +0100 (BST)
Subject: [R] defaults in R: packages, .Rhistory
In-Reply-To: <Pine.GSO.4.44.0306021128270.2402-100000@vidi.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0306120730350.15672-100000@gannet.stats>

Please use a parseable subject line, or two separate messages for two 
unrelated Qs.

On Wed, 11 Jun 2003, Jacob Wegelin wrote:

> With the current version (rw1070), every time I start R it loads a whole
> bunch of packages, many of which I do not need in a typical session:
> 
> > search()
> [1] ".GlobalEnv"      "package:methods" "package:ctest"   "package:mva"
> [5] "package:modreg"  "package:nls"     "package:ts"      "Autoloads"
> [9] "package:base"
> 
> Loading all these packages makes R slow to start up, and I presume it
> uses up memory too.

Not much memory, and it is only methods which adds appreciable slowness.

> Is there a reason that all these packages load automatically? 

Yes.  (You didn't ask what it was.)

> Is
> there some way to stop this from happening, by changing some default?

Yes, and that is in the CHANGES file: please read it.


> A separate question also related to defaults:
> 
> Is there a way to get R to refrain from truncating the .Rhistory file
> (removing the earliest saved commands), so that .Rhistory might serve
> as a permanent record of every command issued?

Not in RGui, *but* you can change the buffer size in the Rconsole file
or the Preferences.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 12 08:44:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 07:44:24 +0100 (BST)
Subject: [R] Tested Random Number Generator
In-Reply-To: <Pine.SOL.4.30.0305301646130.23648-100000@s.psych.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0306120736400.15672-100000@gannet.stats>

On Wed, 11 Jun 2003, Wolfgang Viechtbauer wrote:

> Dear All,
> 
> The editor of a journal to which I had submitted a publication asked
> whether R has a "tested random number generator." My paper included
> Monte Carlo simulations generating random normal and random chi-square
> values.
> 
> help(rnorm) lists
> 
>      Wichura, M. J. (1988) Algorithm AS 241: The Percentage Points of
>      the Normal Distribution. Applied Statistics, 37, 477-484.
> 
> as a reference, but this algorithm does not discuss the generation of
> random values.
> 
> help(RNG) indicates that the "Mersenne-Twister" is the default random
> number generator with reference:
> 
>      Matsumoto, M. and Nishimura, T. (1998) Mersenne Twister: A
>      623-dimensionally equidistributed uniform pseudo-random number
>      generator, ACM Transactions on Modeling and Computer Simulation,
>      8, 3-30.
> 
> I looked at the paper, but essentially I have no expertise in assessing
> whether this random number generator is "good" (which is probably a
> tricky concept in the first place when dealing with RNGs). I have almost
> blind faith in the developers of R (at least when it comes to something
> so fundamental as a RNG) that I feel confident that it is good, but I
> guess I need something more substantial at this point to back of my
> beliefs! Any suggestions on how I can "show" (without having to go
> through a separate study just to make this claim) that the RNG is
> "tested"? Any references?
> 
> Also, I am a little uncertain about how R generates random observations
> from various distributions, such as in rnorm() or rchisq(). Does it
> generate random uniforms u ~ U(0,1) and then solve for x in F(x) = u)? I
> would imagine that this is rather slow compared to other specialized
> methods for various distributions. Any information on this would be
> appreciated as well.

Try reading the appropriate help pages (or the code), which give
references.  Also, don't imagine: check out your suppositions in the RNG
literature.  (rnorm uses inversion by default, and it is not slow. Someone
here claimed that another method was `6 to 7 times faster', but on all bar
one of the machines I tried it was less than 1.5x faster and often slower.
I never received a reply giving evidence for the claim, so presume it was
false.)

> I guess some of this relates to the "Validation of R" discussion that
> occured a while ago on this list, so this info could be of general
> interest.

There are many much-tested PRNGs that have been found to be seriously 
deficient!  Just point your editor at the reference and that R is 
developed by people with international standing in that area, then ask 
his/her credentials for assessing RNGs.

But first make sure you have followed standard advice and run your
simulations with at least two fundamentally different PRNGs.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Thu Jun 12 08:50:59 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 12 Jun 2003 08:50:59 +0200 (CEST)
Subject: [R] import in R a graphics file
In-Reply-To: <3EE75134.AA0443F0@cirad.fr>
Message-ID: <Pine.LNX.4.44.0306120849580.19222-100000@reclus.nhh.no>

On Wed, 11 Jun 2003, Michel ARNAUD wrote:

> Hello
> Is it possible to import in R a graphics file ? What sort of file (bmp,
> tiff, ...) ?
> Thank you

library(pixmap) imports PNM, and your choice of converter, such as 
ImageMagick gets you to PNM from whatever.

Roger

> 
> 
> --
> Michel ARNAUD
> CIRAD TA60/15
> 73, av. Jean Fran?ois Breton
> 34938 MONTPELLIER CEDEX 5
> tel : 04 67 59 38 34
> Fax : 04 67 59 38 27
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Thu Jun 12 09:00:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 08:00:40 +0100 (BST)
Subject: [R] pixmap question again
In-Reply-To: <1055344531.1140.38.camel@christophl>
Message-ID: <Pine.LNX.4.44.0306120759260.15854-100000@gannet.stats>

We got this several times, but I did not see a reply.

You mention `slots': the slot access operator is @ not $.


On Wed, 11 Jun 2003, Christoph Lehmann wrote:

> I could load a picture with gray levels only:
> 
> x <- 
> read.pnm("/home/christoph/work/projects/ffa/EPrime/pics_pnm/550_0_1.pnm")
> 
> > x
> Pixmap image
>   Type          : pixmapGrey
>   Size          : 246x192
>   Resolution    : 1x1
>   Bounding box  : 0 0 192 246
>  
> > plot(x)
> works fine
> 
> BUT: how can I get to the slots as "size", "grey", etc.
>  
> calling: 
> > x$grey
> NULL
> > x$size
> NULL
> >
> 
> why this?
> 
> I need to get the grey value of each pixel and average all grey values
> over the whole picture... 
> 
> thanks
> 
> christoph
> 
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From plummer at iarc.fr  Thu Jun 12 09:12:26 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 12 Jun 2003 07:12:26 -0000
Subject: [R] Does the RPM for RH9 know about TCL/Tk
In-Reply-To: <x2wufs7wlo.fsf@biostat.ku.dk>
References: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
	<20030611014805.GA28098@mail1.sas.upenn.edu>
	<x2wufs7wlo.fsf@biostat.ku.dk>
Message-ID: <1055402159.1009.9.camel@xena>

On Wed, 2003-06-11 at 12:10, Peter Dalgaard BSA wrote:
> Jonathan Baron <baron at psych.upenn.edu> writes:
> 
> > On 06/10/03 18:30, Morgan Hough wrote:
> > >Sorry for the probable repeat post but I can only search the list up to
> > >2002 (is there a better way?). 
> > 
> > Yes, see my search page below.
> > 
> > >I am using the RH9 RPM from CRAN but
> > >packages like AnalyzeFMRI say that tcltk is not found. Do I need to do
> > >more to get Tk GUIs working on RH9 or does the RPM not have tcltk support
> > >built in (should I compile from source). Thanks in advance.
> > 
> > There was in fact some discussion of this last month.  I am not
> > sure of the answer.  But I installed 1.7.0 from the RPM for RH 9,
> > and I got the same error message when trying to get Rcmdr to
> > work.  I did have tcl and tk installed.  Unfortunately, I did not
> > do a properly controlled experiment.  I first installed tcllib,
> > which was not installed originally.  (That didn't help, by
> > itself.)  Then I re-installed R _from source_ and then everything
> > worked.  But I did have the basic vanilla installation of RH 9,
> > and I did have this problem.  So you aren't the only one.  
> > 
> > I still don't know whether tcllib is necessary, and whether the
> > RPM itself installs different things depending on what is on the
> > system.  (I would assume not, but I'm not sure.)
> 
> It shouldn't, but the RPM may be different depending on what was
> present on the system upon which it was built. Martyn may have been
> building on a system where tcl/tk wasn't installed, or -- there's a
> bug report on bugzilla.redhat.com on this -- the build was adversely
> affected by incorrectness of the tclConfig.sh and tkConfig.sh scripts.
> 
> A fairly easy experiment would be to rebuild from the source RPM on
> your own system. Since this builds an RPM, it will retain the
> upgradability etc. of the "official" RPM. Could you try and tell us
> whether the problem remains? (Don't forget that you need a bunch of
> "-devel" packages installed.)

It seems that the R RPM for Red Hat 9 has been built without support
for tcltk.  I am fairly sure that I installed the required packages
on my build machine, but I can't confirm this right  now because it is
at home. I will look into it.

I shall be taking up Brian's suggestion of posting the configure script
summary along with the RPMs so that people can see what capabilities are
built in.

Martyn



From ligges at statistik.uni-dortmund.de  Thu Jun 12 09:13:32 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 Jun 2003 09:13:32 +0200
Subject: [R] Number lines
In-Reply-To: <200306111639.LAA29705@uhddx01.dt.uh.edu>
References: <200306111639.LAA29705@uhddx01.dt.uh.edu>
Message-ID: <3EE8281C.1010407@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Hi R People!
> 
> Does anyone have a function that produces number lines, please?
> 
> It's not a big deal to do, but I thought that I would avoid re-inventing the wheel, if it's already there.
> 
> Actually, I want to do a dotplot  and dotchart just doesn't do it.
> 
> Thanks!
> Erin 
> Version 1.7.0 for Windows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


Do you mean axis()?

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Jun 12 09:15:41 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 Jun 2003 09:15:41 +0200
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <3EE82C17.9020709@nexmatech.ca>
References: <3EE82C17.9020709@nexmatech.ca>
Message-ID: <3EE8289D.9040604@statistik.uni-dortmund.de>

Iouri Tipenko wrote:

> Dear R-Users,
> I'm a master student in Mathematics and Statistics at Carleton 
> University, Ottawa, Canada.
> I'm studying Clustering methods including different related algorithms. 
> One of them is Support Vector Clustering algorithm.
> I was wondering whether anybody implemented this algorithm and could 
> help me with the S-Plus or R computer code that I could use in my 
> simulations.
> I would really appreciate your help or any advise on where I can get 
> this code.

There is an implementation of Support Vector Machines in package e1071, 
which is available on CRAN.
	
Uwe Ligges

> Thank you very much in advance.
> Elena Tipenko
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Jun 12 09:20:04 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 Jun 2003 09:20:04 +0200
Subject: [R] indexing into a list
In-Reply-To: <200306111343.00568.anna@ptolemy.arc.nasa.gov>
References: <200306111343.00568.anna@ptolemy.arc.nasa.gov>
Message-ID: <3EE829A4.7020706@statistik.uni-dortmund.de>

Anna H. Pryor wrote:

> I am trying to make an array of lists.  I don't think that I am doing it right 
> however because I cannot access the individual elements of the lists once I 
> have created the array of lists.  Can someone help?
> 
> 
> for(i in 1:3){
>      y[[i]] =  list(name[((i-1)*index+1):(i*index)])
>    }
> 
> Anna

Certainly you are not going to create an array of lists, but you are 
going to create "just" a list. See the manuals for details.

If "name" is an atomic vector, you won't need a list, but to stay within 
  your example:

  y[[i]] <- name[((i-1)*index+1):(i*index)]

to write the assignment's right hand side to the i-th element of list y.

Uwe Ligges



From mohh29 at hotmail.com  Thu Jun 12 09:57:56 2003
From: mohh29 at hotmail.com (Hamida Mohamdi)
Date: Thu, 12 Jun 2003 07:57:56 +0000
Subject: [R] =?iso-8859-1?q?Probl=E8me_en_R?=
Message-ID: <Law11-F1148zDv60gxa000920da@hotmail.com>

Bonjour,

Je suis ?tudiant stagiaire ? Paris et je rencontre quelques difficult?s en
programmation R.

J'ai une data frame compos?e de 4 colonnes et 250 lignes et dont chaque
ligne est une famille.

J'ai fait un tirage al?atoire avec remise des familles 250 fois ce qui
m'am?ne ? une nouvelle dataframe.
A cette nouvelle dataframe, j'applique un programme qui calcule 2 param?tre
X1 et X2.

Je recommence cel? 2000 fois et ? la fin j'obtient donc 2000 valeurs de X1
et X2.
Gr?ce ? cela je connais la loi empirique de X1 et X2 et je d?termine un
intervalle de confiance de X1 et X2.

Le probl?me est que je ne sais pas d?ssiner l'ellipso?de de confiance.
J'ai telecharger le package ellipse mais je ne vois pas.

Pouvez vous m'aider s'il vous plais si vous savez comment faire??

Je vous remercie d'avance.

Cordialement,
Hamida MOHAMDI

_________________________________________________________________
Hotmail : un compte GRATUIT qui vous suit partout et tout le temps !



From Simon.Fear at synequanon.com  Thu Jun 12 10:26:13 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Thu, 12 Jun 2003 09:26:13 +0100
Subject: =?iso-8859-1?Q?RE:_=5BR=5D_Probl=E8me_en_R?=
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0CE8@synequanon01>

Voir:

Davison, A. C. & Hinkley, D. V. (1997). Bootstrap Methods and Their
Application. Cambridge: Cambridge University Press. 

En plus, il faut ecrire a r-help en anglais!


-----Original Message-----
From: Hamida Mohamdi [mailto:mohh29 at hotmail.com]
Sent: 12 June 2003 08:58
To: R-help at stat.math.ethz.ch
Subject: [R] Probl?me en R


Security Warning:
If you are not sure an attachment is safe to open please contact 
Andy on x234. There are 0 attachments with this message.
________________________________________________________________

Bonjour,

Je suis ?tudiant stagiaire ? Paris et je rencontre quelques difficult?s
en
programmation R.

J'ai une data frame compos?e de 4 colonnes et 250 lignes et dont chaque
ligne est une famille.

J'ai fait un tirage al?atoire avec remise des familles 250 fois ce qui
m'am?ne ? une nouvelle dataframe.
A cette nouvelle dataframe, j'applique un programme qui calcule 2
param?tre
X1 et X2.

Je recommence cel? 2000 fois et ? la fin j'obtient donc 2000 valeurs de
X1
et X2.
Gr?ce ? cela je connais la loi empirique de X1 et X2 et je d?termine un
intervalle de confiance de X1 et X2.

Le probl?me est que je ne sais pas d?ssiner l'ellipso?de de confiance.
J'ai telecharger le package ellipse mais je ne vois pas.

Pouvez vous m'aider s'il vous plais si vous savez comment faire??

Je vous remercie d'avance.

Cordialement,
Hamida MOHAMDI

_________________________________________________________________
Hotmail : un compte GRATUIT qui vous suit partout et tout le temps !

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\ con... {{dropped}}



From mohh29 at hotmail.com  Thu Jun 12 10:38:32 2003
From: mohh29 at hotmail.com (Hamida Mohamdi)
Date: Thu, 12 Jun 2003 08:38:32 +0000
Subject: [R] pb in R
Message-ID: <Law11-F884wqE0yQ63i00040385@hotmail.com>

Hello,
I am a student in Paris and I encounter some difficulties in programming R.
I have one data frame made up of 4 columns and 250 lines and whose each line 
is a family.
I made a random pulling with handing-over of the families 250 times what 
brings me to a new dataframe.
With this new dataframe, I apply a program which calculates 2 parameters X1 
and X2.
I start again,  2000 times,  and at the end :
I  obtains 2000 values of X1 and X2.
With that I know the empirical law of X1 and X2 and I determine a confidence 
interval of X1 and X2.

The problem is that I do not know draw  the ellipso?de confidence (region 
confidence) . I have download the package ellipse but I do not see.

Can you help me please ?? I thank you in advance.

Cordially,
Hamida MOHAMDI

_________________________________________________________________
Pro ou perso, toutes les adresses et num?ros de t?l que vous cherchez



From baliola at riseup.net  Thu Jun 12 13:08:14 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Thu, 12 Jun 2003 11:08:14 +0000
Subject: [R] car package dependencies
Message-ID: <200306121108.14406.baliola@riseup.net>

Hello, 

I tried to install the "car" package but I can't solve the dependencies. car 
needs grid, lattice and dr but when I try to install grid I get this error 
prompt:

wz3x64:/home/oggi/R/software/contrib # rpm -i R-car-1.0.R3-1.i386.rpm
error: failed dependencies:
        R-grid is needed by R-car-1.0.R3-1
        R-lattice is needed by R-car-1.0.R3-1
wz3x64:/home/oggi/R/software/contrib # rpm -i R-grid-0.7.R4-1.i386.rpm
file /usr/lib/R/library/grid/DESCRIPTION from install of R-grid-0.7.R4-1 
conflicts with file from package R-base-1.7.0-1
wz3x64:/home/oggi/R/software/contrib # rpm -i R-lattice-0.7.R11-1.i386.rpm
error: failed dependencies:
        R-grid is needed by R-lattice-0.7.R11-1
wz3x64:/home/oggi/R/software/contrib #

how do I solve this problem? thanks in advance, cheers Martin



From david.meyer at ci.tuwien.ac.at  Thu Jun 12 11:29:06 2003
From: david.meyer at ci.tuwien.ac.at (David Meyer)
Date: Thu, 12 Jun 2003 11:29:06 +0200
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <3EE8289D.9040604@statistik.uni-dortmund.de>;
	from ligges@statistik.uni-dortmund.de on Thu, Jun 12, 2003 at
	09:15:41 +0200
References: <3EE82C17.9020709@nexmatech.ca>
	<3EE8289D.9040604@statistik.uni-dortmund.de>
Message-ID: <20030612092906.GC19752@boromir.ci.tuwien.ac.at>

On 2003.06.12 09:15, Uwe Ligges wrote:
> Iouri Tipenko wrote:
> 
>> Dear R-Users,
>> I'm a master student in Mathematics and Statistics at Carleton 
>> University, Ottawa, Canada.
>> I'm studying Clustering methods including different related 
>> algorithms. One of them is Support Vector Clustering algorithm.
>> I was wondering whether anybody implemented this algorithm and could 
>> help me with the S-Plus or R computer code that I could use in my 
>> simulations.
>> I would really appreciate your help or any advise on where I can get 
>> this code.
> 
> There is an implementation of Support Vector Machines in package 
> e1071, which is available on CRAN.

...but it does not include Support Vector *clustering*.

David



From demirtas at stat.psu.edu  Thu Jun 12 11:36:46 2003
From: demirtas at stat.psu.edu (Hakan Demirtas)
Date: Thu, 12 Jun 2003 05:36:46 -0400
Subject: [R] R-compatible fortran compiler
Message-ID: <000801c330c6$25fc5ea0$baa1cb82@Hakan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/585d24fa/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jun 12 11:56:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 10:56:34 +0100 (BST)
Subject: [R] R-compatible fortran compiler
In-Reply-To: <000801c330c6$25fc5ea0$baa1cb82@Hakan>
Message-ID: <Pine.LNX.4.44.0306121052070.25240-100000@gannet.stats>

Yes.  And it is the one documented in the files on making R under Windows 
and installing source packages under Windows.

First find the file readme.packages either in the top directory of your
installation or in the src/gnuwin32 directory of the sources.

Then read it.  Note that you don't just need a compiler: you need to be 
able to build DLLs so you need a linker too.

When you have all the tools set up,  Rcmd SHLIB foo.f will make the DLL 
foo.dll from the Fortran source file foo.f.

On Thu, 12 Jun 2003, Hakan Demirtas wrote:

> Is there any Fortran compiler that generates R-compatible object files ?
> (For Windows based systems)
> 
> Thanks,
> 
> Hakan Demirtas
> Pennsylvania State University
> Department of Statistics


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramzi_feg at yahoo.fr  Thu Jun 12 11:57:04 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Thu, 12 Jun 2003 11:57:04 +0200 (CEST)
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <20030612092906.GC19752@boromir.ci.tuwien.ac.at>
Message-ID: <20030612095704.67811.qmail@web20304.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/cf3c2fa4/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Jun 12 12:04:13 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 12 Jun 2003 10:04:13 -0000
Subject: [R] car package dependencies
In-Reply-To: <200306121108.14406.baliola@riseup.net>
References: <200306121108.14406.baliola@riseup.net>
Message-ID: <x2el1zwqmr.fsf@biostat.ku.dk>

Martin Wegmann <baliola at riseup.net> writes:

> Hello, 
> 
> I tried to install the "car" package but I can't solve the dependencies. car 
> needs grid, lattice and dr but when I try to install grid I get this error 
> prompt:
> 
> wz3x64:/home/oggi/R/software/contrib # rpm -i R-car-1.0.R3-1.i386.rpm
> error: failed dependencies:
>         R-grid is needed by R-car-1.0.R3-1
>         R-lattice is needed by R-car-1.0.R3-1
> wz3x64:/home/oggi/R/software/contrib # rpm -i R-grid-0.7.R4-1.i386.rpm
> file /usr/lib/R/library/grid/DESCRIPTION from install of R-grid-0.7.R4-1 
> conflicts with file from package R-base-1.7.0-1
> wz3x64:/home/oggi/R/software/contrib # rpm -i R-lattice-0.7.R11-1.i386.rpm
> error: failed dependencies:
>         R-grid is needed by R-lattice-0.7.R11-1
> wz3x64:/home/oggi/R/software/contrib #
> 
> how do I solve this problem? thanks in advance, cheers Martin

Which system are we talking about?? Linux, obviously, but which
distribution packages things up like that?

Normally, grid/lattice are part of the base installation of R, being
in the group of recommended packages. This wasn't always so, but it
has been that way since R 1.6.0. I would suspect that you have some
sort of version skew and that your car/grid/lattice RPMs relate to an
older version of R (or the person who built the RPMs failed to notice
the dependency changes). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From david.meyer at ci.tuwien.ac.at  Thu Jun 12 12:06:45 2003
From: david.meyer at ci.tuwien.ac.at (David Meyer)
Date: Thu, 12 Jun 2003 12:06:45 +0200
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <20030612095704.67811.qmail@web20304.mail.yahoo.com>;
	from ramzi_feg@yahoo.fr on Thu, Jun 12, 2003 at 11:57:04 +0200
References: <20030612095704.67811.qmail@web20304.mail.yahoo.com>
Message-ID: <20030612100645.GA19923@boromir.ci.tuwien.ac.at>

On 2003.06.12 11:57, Ramzi Feghali wrote:
> No comment,

yes, please *do* comment!

The help page clearly says the implementation can carry out:

- classification,
- regression, and
- density estimation

*no* clustering.

David.

> 
> svm                  package:e1071                  R Documentation
> Support Vector Machines
> Description:
>      `svm' is used to train a support vector machine. It can be used
> to
>      carry out general regression and classification (of nu and
>      epsilon-type), as well as density-estimation. A formula interface
>      is provided.
> 
> 
> David Meyer <david.meyer at ci.tuwien.ac.at> wrote:
> On 2003.06.12 09:15, Uwe Ligges wrote:
> > Iouri Tipenko wrote:
> >
> >> Dear R-Users,
> >> I'm a master student in Mathematics and Statistics at Carleton
> >> University, Ottawa, Canada.
> >> I'm studying Clustering methods including different related
> >> algorithms. One of them is Support Vector Clustering algorithm.
> >> I was wondering whether anybody implemented this algorithm and
> could
> >> help me with the S-Plus or R computer code that I could use in my
> >> simulations.
> >> I would really appreciate your help or any advise on where I can
> get
> >> this code.
> >
> > There is an implementation of Support Vector Machines in package
> > e1071, which is available on CRAN.
> 
> ...but it does not include Support Vector *clustering*.
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ---------------------------------
> 
> 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From ramzi_feg at yahoo.fr  Thu Jun 12 12:11:25 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Thu, 12 Jun 2003 12:11:25 +0200 (CEST)
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <20030612100645.GA19923@boromir.ci.tuwien.ac.at>
Message-ID: <20030612101125.2130.qmail@web20308.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/8a24022b/attachment.pl

From glaziou at pasteur-kh.org  Thu Jun 12 12:18:09 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 12 Jun 2003 17:18:09 +0700
Subject: [R] car package dependencies
In-Reply-To: <200306121108.14406.baliola@riseup.net>
References: <200306121108.14406.baliola@riseup.net>
Message-ID: <20030612101809.GE580@pasteur-kh.org>

Martin Wegmann <baliola at riseup.net> wrote:
> I tried to install the "car" package but I can't solve the
> dependencies. car needs grid, lattice and dr but when I
> try to install grid I get this error
> prompt:
> 
> wz3x64:/home/oggi/R/software/contrib # rpm -i R-car-1.0.R3-1.i386.rpm
> error: failed dependencies:
>         R-grid is needed by R-car-1.0.R3-1
>         R-lattice is needed by R-car-1.0.R3-1
> [...]
> 
> how do I solve this problem? thanks in advance, cheers Martin


Why would you want to use rpm? 

See ?install.packages

and (at least) pages 7-8 of "R Installation and
Administration"

-- 
Philippe



From ramzi_feg at yahoo.fr  Thu Jun 12 12:19:13 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Thu, 12 Jun 2003 12:19:13 +0200 (CEST)
Subject: [R] Code for Support Vector Clustering Algorithm
In-Reply-To: <20030612100645.GA19923@boromir.ci.tuwien.ac.at>
Message-ID: <20030612101913.3673.qmail@web20308.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/488559be/attachment.pl

From jfox at mcmaster.ca  Thu Jun 12 12:33:19 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 12 Jun 2003 06:33:19 -0400
Subject: [R] how to string characters?
Message-ID: <5.1.0.14.2.20030612063223.01ed09b8@mcmail.cis.mcmaster.ca>

In-R va.nl>
Mime-Version: 1.0
Content-Type: text/plain; charset="us-ascii"; format=flowed

Dear Ulrich,

At 09:27 PM 6/11/2003 +0200, Ulrich Leopold wrote:
>How can I string character variables?
>
>for example:
>a<- "str"
>b<- "ing"
>
>and i want c to be "string"
>how do I do that?

paste(a, b, sep="") will do what you want.

John
-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From baliola at riseup.net  Thu Jun 12 14:46:48 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Thu, 12 Jun 2003 12:46:48 +0000
Subject: [R] car package dependencies
In-Reply-To: <x2el1zwqmr.fsf@biostat.ku.dk>
References: <200306121108.14406.baliola@riseup.net>
	<x2el1zwqmr.fsf@biostat.ku.dk>
Message-ID: <200306121246.48069.baliola@riseup.net>

Hello, 

>
> Which system are we talking about?? Linux, obviously, but which
> distribution packages things up like that?

I am using Suse 8.1, R 1.7.0 (->grid & lattice included)

> I would suspect that you have some
> sort of version skew and that your car/grid/lattice RPMs relate to an
> older version of R (or the person who built the RPMs failed to notice
> the dependency changes).

can I change the 'car' dependencies/relation to the older R version myself 
without any deep programming knowledge?

Or is there any other packages with a box cox transformation or a similar 
normalization transformation ?

thanks Martin



From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Jun 12 13:08:33 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 12 Jun 2003 13:08:33 +0200 (CEST)
Subject: [R] car package dependencies
In-Reply-To: <20030612101809.GE580@pasteur-kh.org>
References: <200306121108.14406.baliola@riseup.net>
	<20030612101809.GE580@pasteur-kh.org>
Message-ID: <Pine.LNX.4.51.0306121301210.20903@artemis.imbe.med.uni-erlangen.de>


> Martin Wegmann <baliola at riseup.net> wrote:
> > I tried to install the "car" package but I can't solve the
> > dependencies. car needs grid, lattice and dr but when I
> > try to install grid I get this error
> > prompt:
> >
> > wz3x64:/home/oggi/R/software/contrib # rpm -i R-car-1.0.R3-1.i386.rpm
> > error: failed dependencies:
> >         R-grid is needed by R-car-1.0.R3-1
> >         R-lattice is needed by R-car-1.0.R3-1
> > [...]
> >
> > how do I solve this problem? thanks in advance, cheers Martin
>
>
> Why would you want to use rpm?
>
> See ?install.packages
>

that does not need to work. `install.packages' in a pre-compiled
distribution of R (for Unix) cannot safely assume that compilers /
translators / linkers / shared libraries needed for building a
package are available (at runtime) in the places determined at
compile time. Missing `f2c' or `-lreadline' produce a lot of "cannot
install package xyz" threads on this list...

Torsten

> and (at least) pages 7-8 of "R Installation and
> Administration"
>
> --
> Philippe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From madrid at linuxmeeting.net  Thu Jun 12 13:35:31 2003
From: madrid at linuxmeeting.net (Daniele Medri)
Date: Thu, 12 Jun 2003 13:35:31 +0200
Subject: [R] car package dependencies
In-Reply-To: <Pine.LNX.4.51.0306121301210.20903@artemis.imbe.med.uni-erlangen.de>
References: <200306121108.14406.baliola@riseup.net>
	<20030612101809.GE580@pasteur-kh.org>
	<Pine.LNX.4.51.0306121301210.20903@artemis.imbe.med.uni-erlangen.de>
Message-ID: <200306121335.31594.madrid@linuxmeeting.net>

Alle 13:08, gioved? 12 giugno 2003, Torsten Hothorn ha scritto:
> that does not need to work. `install.packages' in a pre-compiled
> distribution of R (for Unix) cannot safely assume that compilers /
> translators / linkers / shared libraries needed for building a
> package are available (at runtime) in the places determined at
> compile time. Missing `f2c' or `-lreadline' produce a lot of "cannot
> install package xyz" threads on this list...

What about a Unix system without a compiler? ;)
With less than 50Mb of usefull software, you can compile and gain more 
performance than generic bin for your own stats. 

Look at:
install.packages		-> es. install.packages("Rcmdr")
update.packages()	-> es. update.packages()
library()			-> es. library(car), library(lattice)


-- 
Daniele Medri <daniele.medri @ libero.it>
homepage: http://www.linux.it/~madrid/

"Statistics are like bikinis. What they reveal is suggestive,
	but what they conceal is vital" - Aaron Levenstein



From d.scott at auckland.ac.nz  Thu Jun 12 13:53:45 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 12 Jun 2003 23:53:45 +1200 (NZST)
Subject: [R] Diagnostics for gee models
Message-ID: <Pine.LNX.4.44.0306122346270.4600-100000@stat10.stat.auckland.ac.nz>


I have fitted a model using gee and would like to do some regression
diagnostics. The model is quite simple, three predictors, gaussian errors,
29 clusters each with 3 observations and unstructured or autoregressive
corstr give similar results.

Does anyone have any pointers to what sort of diagnostics should be 
considered or some R code for this sort of thing?

I didn't find anything in recent mail archives, but did see a few people 
using geese from geepack instead of gee. A look at the manual for geepack 
didn't give me any leads on diagnostics though.

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From p.dalgaard at biostat.ku.dk  Thu Jun 12 13:54:09 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 12 Jun 2003 11:54:09 -0000
Subject: [R] car package dependencies
In-Reply-To: <200306121246.48069.baliola@riseup.net>
References: <200306121108.14406.baliola@riseup.net>
	<x2el1zwqmr.fsf@biostat.ku.dk> <200306121246.48069.baliola@riseup.net>
Message-ID: <x2adcnwljc.fsf@biostat.ku.dk>

Martin Wegmann <baliola at riseup.net> writes:

> Hello, 
> 
> >
> > Which system are we talking about?? Linux, obviously, but which
> > distribution packages things up like that?
> 
> I am using Suse 8.1, R 1.7.0 (->grid & lattice included)
> 
> > I would suspect that you have some
> > sort of version skew and that your car/grid/lattice RPMs relate to an
> > older version of R (or the person who built the RPMs failed to notice
> > the dependency changes).
> 
> can I change the 'car' dependencies/relation to the older R version myself 
> without any deep programming knowledge?
> 
> Or is there any other packages with a box cox transformation or a similar 
> normalization transformation ?

You could try installing the RPM ignoring dependencies (--force or
--nodeps or thereabouts) or maybe more obviously: install from the
source package with install.packages() (over the net) or  R CMD
INSTALL car_x.y-z.tar.gz.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bertola at fastmail.fm  Thu Jun 12 14:22:24 2003
From: bertola at fastmail.fm (Rafael Bertola)
Date: Thu, 12 Jun 2003 09:22:24 -0300
Subject: [R] rigde regression and influence measures
Message-ID: <20030612122224.A58552F399@www.fastmail.fm>

I make a regression using the lm.rigde function from library MASS. Now i
want some measures of influence. 
Is there a way to use a lm.ridge object into the function
influence.measures?
-- 
  
  bertola at fastmail.fm

--



From erhansen at math.ku.dk  Thu Jun 12 14:39:35 2003
From: erhansen at math.ku.dk (Ernst Hansen)
Date: Thu, 12 Jun 2003 14:39:35 +0200
Subject: [R] Programcode and data in the same textfile
Message-ID: <16104.29831.927591.63021@pc87.math.ku.dk>

I have the following problem.  It is not of earthshaking importance,
but still I have spent a considerable amount of time thinking about
it. 

PROBLEM: Is there any way I can have a single textfile that contains
both

  a) data

  b) programcode

The program should act on the data, if the textfile is source()'ed
into R.


BOUNDARY CONDITION: I want the data written in the textfile in exactly
the same format as I would use, if I had data in a separate textfile,
to be read by read.table().  That is, with 'horizontal inhomogeneity'
and 'vertical homogeneity' in the type of entries.  I want to write
something like 

      Sex    Respons
      Male   1
      Male   2
      Female 3
      Female 4

In effect, I am asking if there is some way I can convince
read.table(), that the data is contained in the following n lines of
text. 


ILLEGAL SOLUTIONS:
I know I can simulate the behaviour by reading the columns of the
dataframe one by one, and using data.frame() to glue them together.
Like in 

    data.frame(Sex = c('Male', 'Male', 'Female', 'Female'),
               Respons = c(1, 2, 3, 4))

I do not like this solution, because it represents the data in a
"transposed" way in the textfile, and this transposition makes the
structure of the dataframe less transparent - at least to me. It
becomes even less comprehensible if the Sex-factor above is written
with the help of rep() or gl() or the like.

I know I can make read.table() read from stdin, so I could type the
dataframe at the prompt.  That is against the spirit of the problem,
as I describe below.


I know I can make read.table() do the job, if I split the data and the
programcode in to different files.  But as the purpose of the exercise
is to distribute the data and the code to other people, splitting
into several files is a complication.


MOTIVATION: I frequently find myself distributing small chunks of code
to my students, along with data on which the code can work.

As an example, I might want to demonstrate how model.matrix() treats
interactions, in a certain setting.  For that I need a dataframe that
is complex enough to exhibit the behaviour I want, but still so small
that the model.matrix is easily understood.  So I make such a
dataframe.

I am trying to distribute this dataframe along with my code, in a way
that is as simple as possible to USE for the students (hence the
one-file boundary condition) and to READ (hence the non-transposition
boundary condition).



Does anybody have any ideas?


Ernst Hansen
Department of Statistics
University of Copenhagen



From baliola at riseup.net  Thu Jun 12 16:49:46 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Thu, 12 Jun 2003 14:49:46 +0000
Subject: [R] car package dependencies - solved - thanks
In-Reply-To: <x2adcnwljc.fsf@biostat.ku.dk>
References: <200306121108.14406.baliola@riseup.net>
	<200306121246.48069.baliola@riseup.net>
	<x2adcnwljc.fsf@biostat.ku.dk>
Message-ID: <200306121449.46063.baliola@riseup.net>

Thanks, 

rpm -i --nodeps R-car-..... worked. 

only as info, Install.packages(car) didn't work - no packages called car were 
found - do I have to typ the name in differently? but anyway box.cox in car 
is working 
thanks all for their help, cheers Martin

On Thursday 12 June 2003 12:03, Peter Dalgaard BSA wrote:
> Martin Wegmann <baliola at riseup.net> writes:
> > Hello,
> >
> > > Which system are we talking about?? Linux, obviously, but which
> > > distribution packages things up like that?
> >
> > I am using Suse 8.1, R 1.7.0 (->grid & lattice included)
> >
> > > I would suspect that you have some
> > > sort of version skew and that your car/grid/lattice RPMs relate to an
> > > older version of R (or the person who built the RPMs failed to notice
> > > the dependency changes).
> >
> > can I change the 'car' dependencies/relation to the older R version
> > myself without any deep programming knowledge?
> >
> > Or is there any other packages with a box cox transformation or a similar
> > normalization transformation ?
>
> You could try installing the RPM ignoring dependencies (--force or
> --nodeps or thereabouts) or maybe more obviously: install from the
> source package with install.packages() (over the net) or  R CMD
> INSTALL car_x.y-z.tar.gz.



From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Jun 12 15:00:26 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 12 Jun 2003 15:00:26 +0200 (CEST)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.29831.927591.63021@pc87.math.ku.dk>
References: <16104.29831.927591.63021@pc87.math.ku.dk>
Message-ID: <Pine.LNX.4.51.0306121500010.20903@artemis.imbe.med.uni-erlangen.de>

> I have the following problem.  It is not of earthshaking importance,
> but still I have spent a considerable amount of time thinking about
> it.
>
> PROBLEM: Is there any way I can have a single textfile that contains
> both
>
>   a) data
>
>   b) programcode
>
> The program should act on the data, if the textfile is source()'ed
> into R.
>
>
> BOUNDARY CONDITION: I want the data written in the textfile in exactly
> the same format as I would use, if I had data in a separate textfile,
> to be read by read.table().  That is, with 'horizontal inhomogeneity'
> and 'vertical homogeneity' in the type of entries.  I want to write
> something like
>
>       Sex    Respons
>       Male   1
>       Male   2
>       Female 3
>       Female 4
>


something like

  tmpfilename <- tempfile()
  tmpfile <- file(tmpfilename, "w")
  cat(

      ### here comes my data

      "Sex    Respons",
      "Male   1",
      "Male   2",
      "Female 3",
      "Female 4",

      ### end of data input

      file = tmpfile, sep="\n")
  close(tmpfile)
  read.table(tmpfilename, header = TRUE)


best,

Torsten

> In effect, I am asking if there is some way I can convince
> read.table(), that the data is contained in the following n lines of
> text.
>
>
> ILLEGAL SOLUTIONS:
> I know I can simulate the behaviour by reading the columns of the
> dataframe one by one, and using data.frame() to glue them together.
> Like in
>
>     data.frame(Sex = c('Male', 'Male', 'Female', 'Female'),
>                Respons = c(1, 2, 3, 4))
>
> I do not like this solution, because it represents the data in a
> "transposed" way in the textfile, and this transposition makes the
> structure of the dataframe less transparent - at least to me. It
> becomes even less comprehensible if the Sex-factor above is written
> with the help of rep() or gl() or the like.
>
> I know I can make read.table() read from stdin, so I could type the
> dataframe at the prompt.  That is against the spirit of the problem,
> as I describe below.
>
>
> I know I can make read.table() do the job, if I split the data and the
> programcode in to different files.  But as the purpose of the exercise
> is to distribute the data and the code to other people, splitting
> into several files is a complication.
>
>
> MOTIVATION: I frequently find myself distributing small chunks of code
> to my students, along with data on which the code can work.
>
> As an example, I might want to demonstrate how model.matrix() treats
> interactions, in a certain setting.  For that I need a dataframe that
> is complex enough to exhibit the behaviour I want, but still so small
> that the model.matrix is easily understood.  So I make such a
> dataframe.
>
> I am trying to distribute this dataframe along with my code, in a way
> that is as simple as possible to USE for the students (hence the
> one-file boundary condition) and to READ (hence the non-transposition
> boundary condition).
>
>
>
> Does anybody have any ideas?
>
>
> Ernst Hansen
> Department of Statistics
> University of Copenhagen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From th50 at leicester.ac.uk  Thu Jun 12 15:18:09 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 12 Jun 2003 14:18:09 +0100
Subject: [R] Programcode and data in the same textfile
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46A3@saffron.cfs.le.ac.uk>

Following up on Thorsten's solution, this one doesn't need a tempfile:

my.data<-read.table(textConnection(c(

      ### here comes my data

      "Sex    Respons",
      "Male   1",
      "Male   2",
      "Female 3",
      "Female 4"

      ### end of data input

)),header=T)

print(my.data)

HTH

Thomas


> -----Original Message-----
> From: Torsten Hothorn [mailto:Torsten.Hothorn at rzmail.uni-erlangen.de]
> Sent: 12 June 2003 14:00
> To: Ernst Hansen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Programcode and data in the same textfile
> 
> 
> > I have the following problem.  It is not of earthshaking importance,
> > but still I have spent a considerable amount of time thinking about
> > it.
> >
> > PROBLEM: Is there any way I can have a single textfile that contains
> > both
> >
> >   a) data
> >
> >   b) programcode
> >
> > The program should act on the data, if the textfile is source()'ed
> > into R.
> >
> >
> > BOUNDARY CONDITION: I want the data written in the textfile 
> in exactly
> > the same format as I would use, if I had data in a separate 
> textfile,
> > to be read by read.table().  That is, with 'horizontal 
> inhomogeneity'
> > and 'vertical homogeneity' in the type of entries.  I want to write
> > something like
> >
> >       Sex    Respons
> >       Male   1
> >       Male   2
> >       Female 3
> >       Female 4
> >
> 
> 
> something like
> 
>   tmpfilename <- tempfile()
>   tmpfile <- file(tmpfilename, "w")
>   cat(
> 
>       ### here comes my data
> 
>       "Sex    Respons",
>       "Male   1",
>       "Male   2",
>       "Female 3",
>       "Female 4",
> 
>       ### end of data input
> 
>       file = tmpfile, sep="\n")
>   close(tmpfile)
>   read.table(tmpfilename, header = TRUE)
> 
> 
> best,
> 
> Torsten
> 
> > In effect, I am asking if there is some way I can convince
> > read.table(), that the data is contained in the following n lines of
> > text.
> >
> >
> > ILLEGAL SOLUTIONS:
> > I know I can simulate the behaviour by reading the columns of the
> > dataframe one by one, and using data.frame() to glue them together.
> > Like in
> >
> >     data.frame(Sex = c('Male', 'Male', 'Female', 'Female'),
> >                Respons = c(1, 2, 3, 4))
> >
> > I do not like this solution, because it represents the data in a
> > "transposed" way in the textfile, and this transposition makes the
> > structure of the dataframe less transparent - at least to me. It
> > becomes even less comprehensible if the Sex-factor above is written
> > with the help of rep() or gl() or the like.
> >
> > I know I can make read.table() read from stdin, so I could type the
> > dataframe at the prompt.  That is against the spirit of the problem,
> > as I describe below.
> >
> >
> > I know I can make read.table() do the job, if I split the 
> data and the
> > programcode in to different files.  But as the purpose of 
> the exercise
> > is to distribute the data and the code to other people, splitting
> > into several files is a complication.
> >
> >
> > MOTIVATION: I frequently find myself distributing small 
> chunks of code
> > to my students, along with data on which the code can work.
> >
> > As an example, I might want to demonstrate how model.matrix() treats
> > interactions, in a certain setting.  For that I need a 
> dataframe that
> > is complex enough to exhibit the behaviour I want, but 
> still so small
> > that the model.matrix is easily understood.  So I make such a
> > dataframe.
> >
> > I am trying to distribute this dataframe along with my 
> code, in a way
> > that is as simple as possible to USE for the students (hence the
> > one-file boundary condition) and to READ (hence the 
> non-transposition
> > boundary condition).
> >
> >
> >
> > Does anybody have any ideas?
> >
> >
> > Ernst Hansen
> > Department of Statistics
> > University of Copenhagen
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From B.Rowlingson at lancaster.ac.uk  Thu Jun 12 15:26:46 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 12 Jun 2003 14:26:46 +0100
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <Pine.LNX.4.51.0306121500010.20903@artemis.imbe.med.uni-erlangen.de>
References: <16104.29831.927591.63021@pc87.math.ku.dk>
	<Pine.LNX.4.51.0306121500010.20903@artemis.imbe.med.uni-erlangen.de>
Message-ID: <3EE87F96.70206@lancaster.ac.uk>


>       file = tmpfile, sep="\n")
>   close(tmpfile)
>   read.table(tmpfilename, header = TRUE)
> 

  Eurgh! Does R clean up tempfiles by itself?

  Here's my idea. Just stick all the values in a vector, using 
free-formatting to make it look nice, then reform it into a data frame:

indata <- function(x){

   data <- c(

             "Sex", "Id",  "Age",
             "Male",   1,   13.2,
             "Female", 2,    8.1,
             "Male",   3,    6.3,
             "Male",   2,   12.2,

             )

   ncol <- 3
   nrow <- length(data)/ncol

   heads <- data[1:ncol];data <- data[-(1:ncol)]
   asDF <- data.frame(matrix(data,ncol=ncol,byrow=T))

   asDF[,2] <- as.numeric(asDF[,2])
   asDF[,3] <- as.numeric(asDF[,3])
   names(asDF) <- heads
   asDF
}



   Note that 'data' will be all character if there's any characters in 
it, so you need to tweak the types with as.numeric or as.factor if thats 
what you need. You could probably test if a column was all valid numbers 
and automatically make it numeric, but then we're going a long way to 
duplicating a lot of the code in read.table, which does something like 
this (it uses 'scan()' to read from a file and then reforms it all into 
a data frame).

All values must be comma separated and character values must be quoted.

Baz



From Ted.Harding at nessie.mcc.ac.uk  Thu Jun 12 15:21:00 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jun 2003 14:21:00 +0100 (BST)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.29831.927591.63021@pc87.math.ku.dk>
Message-ID: <XFMail.030612142100.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jun-03 Ernst Hansen wrote:
> I have the following problem.  It is not of earthshaking importance,
> but still I have spent a considerable amount of time thinking about
> it. 
> 
> PROBLEM: Is there any way I can have a single textfile that contains
> both
> 
>   a) data
> 
>   b) programcode
> 
> The program should act on the data, if the textfile is source()'ed
> into R.
> 
> 
> BOUNDARY CONDITION: I want the data written in the textfile in exactly
> the same format as I would use, if I had data in a separate textfile,
> to be read by read.table().  That is, with 'horizontal inhomogeneity'
> and 'vertical homogeneity' in the type of entries.  I want to write
> something like 
> 
>       Sex    Respons
>       Male   1
>       Male   2
>       Female 3
>       Female 4
> 
> In effect, I am asking if there is some way I can convince
> read.table(), that the data is contained in the following n lines of
> text. 

A thought which occurs to me, which (as far as I can tell) is not
already implemented (at any rate in read.table() which is where it
could have a natural home) is that, in the same spirit as

  read,table(file="stdin")

one could, if available, use

  read.table(file="<< EOT")

i.e. the "here document" style of redirection that has been a part
of Unix since approximately forever (if you take the origin of time
as 01/01/70 00:00). Then the above data could be read in from within
the source file by

  X<-read.table(header=TRUE,file="<< EOT")
  Sex    Respons
  Male   1
  Male   2
  Female 3
  Female 4
  EOT

I.e. this form of the command would take input from the following
lines until "EOT" is encountered on a line by itself. In the Unix
setup, "EOT" could be anything so long as it won't occur on a line by
itself within the data, and is not included in the content which is
read in.

Ted,


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 12-Jun-03                                       Time: 14:21:00
------------------------------ XFMail ------------------------------



From david.whiting at ncl.ac.uk  Thu Jun 12 18:19:46 2003
From: david.whiting at ncl.ac.uk (david.whiting@ncl.ac.uk)
Date: Thu, 12 Jun 2003 16:19:46 +0000
Subject: SP? Re: [R] Rounding problem R vs Excel
In-Reply-To: <200306041454.AHX32422@ms2.verisignmail.com>
References: <200306041454.AHX32422@ms2.verisignmail.com>
Message-ID: <20030612161946.GA15855@192.168.57.2>

Hi again Sam,

Sorry for not replying sooner.  I have been ignoring everything for a
while to work on the AMMP analysis tool and the continuing saga of
migrating the legacy AMMP data.  Our consultant has written some
programs that take a VERY long time to run :(

Anyway, I now know Java and a certain amount of JSP.  

I hope to be able to get around to this soon.  It is all a bit hard
now with our data partially migrated.  Prod me again in the near
future if you haven't heard from me.

Dave


On Wed, Jun 04, 2003 at 04:54:26PM +0200, = Sam Clark = wrote:
> Wow!!  Not the first time MS products have done something 
> weird. Excel and Access and SQL Server all differ on how 
> they store dates internally, and this also leads to 
> problems - mainly in compatibility between them and their 
> date functions.  Did quite a bit of reading on it a while 
> ago but can't dig up references now.
> 
> David, I'm also wondering if you have a chance if you could 
> prep those data files I emailed you about a while ago.  I'd 
> like to work our manuscript into something submittable in 
> the next few months.
> 
> Thanks,
> 
> - S.
> 
> >Date: Wed, 4 Jun 2003 17:30:04 +0000
> >From: <david.whiting at ncl.ac.uk>  
> >Subject: [R] Rounding problem R vs Excel  
> >To: sam at samclark.net, p_setel at yahoo.com, Nigel Unwin 
> <n.c.unwin at ncl.ac.uk>, hmwanyika at yahoo.co.uk, Greg Kabadi 
> <greg.ammp at bigfoot.com>
> >
> >This discussion from the R-help mailing list might be of 
> interest to
> >you folks.  It was started when someone tried this in Excel:
> >
> >0.5 - 0.4 - 0.1
> >
> >and 
> >
> >(0.5 - 0.4 - 0.1)
> >
> >They give different results.  Try formatting the cells to 
> 20 decimal
> >places or multiplying the results in the next cells by
> >1,000,000,000,000,000,000
> >
> >Dave
> >
> >
> 20-OP
> >________________
> >Date: 04 Jun 2003 08:53:36 -0500
> >From: Marc Schwartz <MSchwartz at medanalytics.com>  
> >Subject: RE: [R] Rounding problem R vs Excel  
> >To: "Paul, David ?A" <paulda at BATTELLE.ORG>
> >Cc: R-help at stat.math.ethz.ch, "'Duncan Murdoch'" 
> <dmurdoch at pair.com>
> >
> >On Wed, 2003-06-04 at 08:09, Paul, David A wrote:
> >> I don't have the reference, but a biologist friend of mine
> >> once showed me a refereed journal article that purported
> >> to demonstrate numerical errors made by MSExcel.  This 
> >> would have been Excel97 or Excel2000... In any case, the
> >> journal's scope was biological in nature and the article 
> >> was of interest since Excel is heavily used in that 
> community.
> >> 
> >> -david paul
> >
> >
> >There is a series of articles here:
> >
> >http://www.stat.uni-muenchen.de/~knuesel/elv/accuracy.html
> >
> >
> >In addition, there are additional references on Excel 
> specifically:
> >
> >
> >On the accuracy of statistical procedures in Microsoft 
> Excel 2000 and
> >Excel XP
> >B.D. McCullough and B. Wilson, (2002), Computational 
> Statistics & Data
> >Analysis, 40, pp 713 - 721
> >http://www.elsevier.com/gej-
> ng/10/15/38/85/51/28/abstract.html
> >
> >
> >On the accuracy of statistical procedures in Microsoft 
> Excel ???97
> >B.D. McCullough and B. Wilson, (1999), Computational 
> Statistics & Data
> >Analysis, 31, pp 27-37
> >http://www.elsevier.com/gej-ng/10/15/38/37/25/27/article.pdf
> >
> >
> >Problems with using Microsoft Excel for statistics
> >J.D. Cryer, (2001), presented at the Joint Statistical 
> Meetings,
> >American Statistical Association, 2001, Atlanta Georgia
> >at http://www.stat.uiowa.edu/~jcryer/JSMTalk2001.pdf
> >
> >
> >Use of Excel for statistical analysis
> >Neil Cox, (2000), AgResearch Ruakura
> >at 
> http://www.agresearch.cri.nz/Science/Statistics/exceluse1.htm
> >
> >
> >Using Excel for statistical data analysis
> >Eva Goldwater, (1999), Univ. of Massachusetts Office of 
> Information
> >Technology
> >http://www.umass.edu/acco/statistics/handout/excel.html
> >
> >
> >Statistical analysis using Microsoft Excel 
> >Jeffrey Simonoff, (2002)
> >at 
> http://www.stern.nyu.edu/~jsimonof/classes/1305/pdf/excelreg.
> pdf
> >
> >
> >Testing the Intrinsic Functions of Excel
> >National Physical Laboratory, UK
> >http://www.npl.co.uk/ssfm/ssfm1/validate/testing/excel.html
> >
> >
> >
> >There are also some general articles on several stats 
> applications by
> >McCullough.
> >
> >http://www.amstat.org/publications/tas/mccull-1.pdf
> >http://www.amstat.org/publications/tas/mccull.pdf
> >
> >
> >It has been some time since I looked at many of these 
> papers, but if my
> >memory is correct, in general, not much has changed in 
> Excel since "97".
> >However, from McCullough's most recent article:
> >
> >"The problems that rendered Excel 97 unfit for use as a 
> statistical
> >package have not been fixed in either Excel 2000 or Excel 
> 2002 (also
> >called "Excel XP"). Microsoft attempted to fix errors in 
> the standard
> >normal random number generator and the inverse normal 
> function, and in
> >the former case actually made the problem worse."
> >
> >
> >Many of the above articles have an overlap on references, 
> some
> >published, some are online resources or lecture notes.
> >
> >
> >HTH,
> >
> >Marc Schwartz
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>   sam at samclark.net
>   mobile: +27 (0)73 154-2069

-- 
Dave Whiting
Dar es Salaam, Tanzania



From anna at ptolemy.arc.nasa.gov  Thu Jun 12 15:39:54 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Thu, 12 Jun 2003 06:39:54 -0700
Subject: [R] indexing into a list 2
Message-ID: <200306120639.54626.anna@ptolemy.arc.nasa.gov>

I didn't explain myself well.

You are right about not needing a list on the right hand side of the equation 
below.  What I end up with is a list of arrays.  Now what i would like to do 
is to access the individual elements in the arrays in the list.  For example, 
when I type, y[[1]], I just get the whole first array.  How do I get the 
first element in the first  array for instance?  Is that not possible?

Anna





Anna H. Pryor wrote:

> I am trying to make an array of lists.  I don't think that I am doing it 
right 
> however because I cannot access the individual elements of the lists once I 
> have created the array of lists.  Can someone help?
> 
> 
> for(i in 1:3){
>      y[[i]] =  list(name[((i-1)*index+1):(i*index)])
>    }
> 
> Anna

Certainly you are not going to create an array of lists, but you are 
going to create "just" a list. See the manuals for details.

If "name" is an atomic vector, you won't need a list, but to stay within 
  your example:

  y[[i]] <- name[((i-1)*index+1):(i*index)]

to write the assignment's right hand side to the i-th element of list y.

Uwe Ligges



From ripley at stats.ox.ac.uk  Thu Jun 12 15:47:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 14:47:07 +0100 (BST)
Subject: [R] car package dependencies - solved - thanks
In-Reply-To: <200306121449.46063.baliola@riseup.net>
Message-ID: <Pine.LNX.4.44.0306121446310.16748-100000@gannet.stats>

On Thu, 12 Jun 2003, Martin Wegmann wrote:

> Thanks, 
> 
> rpm -i --nodeps R-car-..... worked. 
> 
> only as info, Install.packages(car) didn't work - no packages called car were 

install.packages("car")!

> found - do I have to typ the name in differently? but anyway box.cox in car 
> is working 
> thanks all for their help, cheers Martin

There is of course boxcox in MASS, which you do have.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 12 15:54:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 14:54:55 +0100 (BST)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <3EE87F96.70206@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.44.0306121452390.16788-100000@gannet.stats>

On Thu, 12 Jun 2003, Barry Rowlingson wrote:

> 
> >       file = tmpfile, sep="\n")
> >   close(tmpfile)
> >   read.table(tmpfilename, header = TRUE)
> > 
> 
>   Eurgh! Does R clean up tempfiles by itself?

Yes!

It's a better idea to use an anonymous file connection, opened for reading 
and writing.  Then there is no cleaning-up to be done.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Jun 12 15:57:34 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Jun 2003 06:57:34 -0700 (PDT)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <3EE87F96.70206@lancaster.ac.uk>
Message-ID: <Pine.A41.4.44.0306120656030.76800-100000@homer18.u.washington.edu>

On Thu, 12 Jun 2003, Barry Rowlingson wrote:

>
>   Eurgh! Does R clean up tempfiles by itself?

Yes.  That's what they are for.  It happens on normal exit.

	-thomas



From MZodet at ahrq.gov  Thu Jun 12 16:00:05 2003
From: MZodet at ahrq.gov (MZodet@ahrq.gov)
Date: Thu, 12 Jun 2003 10:00:05 -0400
Subject: [R] step vs. stepAIC
Message-ID: <3598558AD728D41183350008C7CF291C0A5CD1FD@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/f8388c67/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jun 12 16:08:38 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 Jun 2003 16:08:38 +0200
Subject: [R] indexing into a list 2
In-Reply-To: <200306120639.54626.anna@ptolemy.arc.nasa.gov>
References: <200306120639.54626.anna@ptolemy.arc.nasa.gov>
Message-ID: <3EE88966.1060003@statistik.uni-dortmund.de>

Anna H. Pryor wrote:

> I didn't explain myself well.
> 
> You are right about not needing a list on the right hand side of the equation 
> below.  What I end up with is a list of arrays.  Now what i would like to do 
> is to access the individual elements in the arrays in the list.  For example, 
> when I type, y[[1]], I just get the whole first array.  How do I get the 
> first element in the first  array for instance?  Is that not possible?

Just "append" that index. E.g., if your array in y[[1]] is 3D,
  y[[1]][1,1,1]
or maybe you mean
  y[[1]][ , , 1]

Uwe Ligges




> Anna
> 
> 
> 
> 
> 
> Anna H. Pryor wrote:
> 
> 
>>I am trying to make an array of lists.  I don't think that I am doing it 
> 
> right 
> 
>>however because I cannot access the individual elements of the lists once I 
>>have created the array of lists.  Can someone help?
>>
>>
>>for(i in 1:3){
>>     y[[i]] =  list(name[((i-1)*index+1):(i*index)])
>>   }
>>
>>Anna
> 
> 
> Certainly you are not going to create an array of lists, but you are 
> going to create "just" a list. See the manuals for details.
> 
> If "name" is an atomic vector, you won't need a list, but to stay within 
>   your example:
> 
>   y[[i]] <- name[((i-1)*index+1):(i*index)]
> 
> to write the assignment's right hand side to the i-th element of list y.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Thu Jun 12 16:21:28 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jun 2003 15:21:28 +0100 (BST)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <XFMail.030612142100.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030612152053.Ted.Harding@nessie.mcc.ac.uk>

On 12-Jun-03 Ted Harding wrote:
> i.e. the "here document" style of redirection that has been a part
> of Unix since approximately forever (if you take the origin of time
> as 01/01/70 00:00). Then the above data could be read in from within
> the source file by
> 
>   X<-read.table(header=TRUE,file="<< EOT")
>   Sex    Respons
>   Male   1
>   Male   2
>   Female 3
>   Female 4
>   EOT

A follow-up (should have thought to mention it last time).

Users of Unix troff / GNU groff may be familiar with the "inline data
input" used by 'pic', where the form is:

  .PS
  <pic commands>
  copy thru <macroname> until "EOT"
  1.1 1.2 1.3
  2.1 2.2 2.3
  ...
  9.1 9.2 9.3
  EOT
  <pic commands>
  .PE

This is exactly the same idea, and the more I think about it the more
attractive it seems to have this facility available in R. In fact,
the most useful implementation would be that wherever file="..."
could be used in a command to read from a text file, the form
file="<< MARKER" could be used to read in from the following lines of
the file.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 12-Jun-03                                       Time: 15:20:53
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Thu Jun 12 16:22:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 15:22:34 +0100 (BST)
Subject: [R] step vs. stepAIC
In-Reply-To: <3598558AD728D41183350008C7CF291C0A5CD1FD@exchange1.ahrq.gov>
Message-ID: <Pine.LNX.4.44.0306121521350.16898-100000@gannet.stats>

On Thu, 12 Jun 2003 MZodet at ahrq.gov wrote:

> Could someone provide me direction towards documentation or provide some
> thoughts on when I should use stepAIC rather than step (or vice versa)?

See the note in ?step ...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tblackw at umich.edu  Thu Jun 12 17:39:34 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 12 Jun 2003 11:39:34 -0400 (EDT)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.29831.927591.63021@pc87.math.ku.dk>
Message-ID: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>

Ernst  -

Here's a solution which works for me, and seems to do
what you want.  It's a bit of a hack, since it requires
you, the author, to know in advance what file path name
the student will have saved the file as.  In my example,
this will be "./r.source.file", and this includes one
blank line before the first assignment statement below.

It also requires knowing how many lines of code precede
the data lines.  But it _is_ a one-file solution, as
requested.  Put the following 9 or 10 lines into a
file named "r.source.file", then source it.

data.01 <- read.table(file="r.source.file", header=T,
	skip=4, comment.char="")[-1]

 # junk Sex      Response
    #   Male     1
    #   Male     2
    #   Female   3
    #   Female   4


I'm quite surprised no one else has suggested this already.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 12 Jun 2003, Ernst Hansen wrote:

> PROBLEM: Is there any way I can have a single textfile that contains both
>  a) data   b) programcode
> The program should act on the data, if the textfile is source()'ed
> into R.
>
> BOUNDARY CONDITION: I want the data written in the textfile in exactly
> the same format as I would use, if I had data in a separate textfile,
> to be read by read.table().   something like
>
>       Sex    Respons
>       Male   1
>       Male   2
>       Female 3
>       Female 4
>
> MOTIVATION: I frequently find myself distributing small chunks of code
> to my students, along with data on which the code can work.
>
> As an example, I might want to demonstrate how model.matrix() treats
> interactions, in a certain setting.  For that I need a dataframe that
> is complex enough to exhibit the behaviour I want, but still so small
> that the model.matrix is easily understood.  So I make such a dataframe.
>
> I am trying to distribute this dataframe along with my code, in a way
> that is as simple as possible to USE for the students (hence the
> one-file boundary condition) and to READ (hence the non-transposition
> boundary condition).
>
> Ernst Hansen
> Department of Statistics
> University of Copenhagen



From xingchun_chen at yahoo.com  Thu Jun 12 18:14:18 2003
From: xingchun_chen at yahoo.com (Charlie Chen)
Date: Thu, 12 Jun 2003 09:14:18 -0700 (PDT)
Subject: [R] Problems for runing R in BATCH to generate PNG/JPEG files
Message-ID: <20030612161418.93963.qmail@web20902.mail.yahoo.com>

Hello all:

After I compile the R-1.7 with the follows in
Linux7.3:

configure --with-x --with-jpeglib --with-libpng
--x-includes=/usr/X11R6/include
--x-libraries=/usr/X11R6/lib
and make and make install. The R is able to generate
PNG/JPEG files from the command line. However, when I
put the same commands in a batch file and run:

R --gui BATCH R.Batch.file 

It gave me a error in the R.Batch.file.Rout:

Error in png("myplot.png") : R_X11 module cannot be
loaded
In addition: Warning message: 
X11 module is not available under this GUI 
Execution halted

Would you please guide me through to fix the problem?
Your help is greatly appreciated.

Charlie



From glouis at dynamicro.on.ca  Thu Jun 12 18:28:01 2003
From: glouis at dynamicro.on.ca (Greg Louis)
Date: Thu, 12 Jun 2003 12:28:01 -0400
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
References: <16104.29831.927591.63021@pc87.math.ku.dk>
	<Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
Message-ID: <20030612162801.GA15855@athame.dynamicro.on.ca>

On 20030612 (Thu) at 1139:34 -0400, Thomas W Blackwell wrote:

> It also requires knowing how many lines of code precede
> the data lines.  But it _is_ a one-file solution, as
> requested.  Put the following 9 or 10 lines into a
> file named "r.source.file", then source it.
> 
> data.01 <- read.table(file="r.source.file", header=T,
> 	skip=4, comment.char="")[-1]
> 
>  # junk Sex      Response
>     #   Male     1
>     #   Male     2
>     #   Female   3
>     #   Female   4
> 

The nrows parameter can help by letting you put the data early in the
file:

data.01 <- read.table(file="r.source.file", header=T,
    skip=4, nrows=4, comment.char="")[-1]

    #   Sex     Response
    #   Male    1
    #   Male    2
    #   Female  3
    #   Female  4

print(data.01)
(more code)

(I got an error "line 1 did not have 4 elements" when I left the
"junk" header in place.)

> On Thu, 12 Jun 2003, Ernst Hansen wrote:
> 
> > PROBLEM: Is there any way I can have a single textfile that contains both
> >  a) data   b) programcode
> > The program should act on the data, if the textfile is source()'ed
> > into R.
> >
> > BOUNDARY CONDITION: I want the data written in the textfile in exactly
> > the same format as I would use, if I had data in a separate textfile,
> > to be read by read.table().   something like
> >
> >       Sex    Respons
> >       Male   1
> >       Male   2
> >       Female 3
> >       Female 4

Obviously the above doesn't quite meet the requirement, since the data
have to be commented out -- but unless someone implements "here
documents", as another list member suggested, I don't think there's a
perfect solution.

-- 
| G r e g  L o u i s          | gpg public key: finger     |
|   http://www.bgl.nu/~glouis |   glouis at consultronics.com |
| http://wecanstopspam.org in signatures fights junk email |



From ripley at stats.ox.ac.uk  Thu Jun 12 18:30:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 17:30:23 +0100 (BST)
Subject: [R] Problems for runing R in BATCH to generate PNG/JPEG files
In-Reply-To: <20030612161418.93963.qmail@web20902.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306121724220.19417-100000@gannet.stats>

1) Read ?BATCH, carefully.  
Note that R --gui BATCH isn't valid syntax.

2) Read ?png, carefully.

3) So you need to not use BATCH *and* provide an X server: I use Xvfb
on Linux (but on Solaris the console is available to batch processes).


On Thu, 12 Jun 2003, Charlie Chen wrote:

> Hello all:
> 
> After I compile the R-1.7 with the follows in
> Linux7.3:

That's not a Linux version number!  RedHat 7.3, perhaps?

> 
> configure --with-x --with-jpeglib --with-libpng
> --x-includes=/usr/X11R6/include
> --x-libraries=/usr/X11R6/lib
> and make and make install. The R is able to generate
> PNG/JPEG files from the command line. However, when I
> put the same commands in a batch file and run:
> 
> R --gui BATCH R.Batch.file 
> 
> It gave me a error in the R.Batch.file.Rout:
> 
> Error in png("myplot.png") : R_X11 module cannot be
> loaded
> In addition: Warning message: 
> X11 module is not available under this GUI 
> Execution halted
> 
> Would you please guide me through to fix the problem?
> Your help is greatly appreciated.

The help pages *are* very helpful once you read them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From erhansen at math.ku.dk  Thu Jun 12 18:42:39 2003
From: erhansen at math.ku.dk (Ernst Hansen)
Date: Thu, 12 Jun 2003 18:42:39 +0200
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
References: <16104.29831.927591.63021@pc87.math.ku.dk>
	<Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
Message-ID: <16104.44415.319203.955415@pc87.math.ku.dk>

Thomas W Blackwell writes:
 > Ernst  -
 > 
 > Here's a solution which works for me, and seems to do
 > what you want.  It's a bit of a hack, since it requires
 > you, the author, to know in advance what file path name
 > the student will have saved the file as.  In my example,
 > this will be "./r.source.file", and this includes one
 > blank line before the first assignment statement below.
 > 
 > It also requires knowing how many lines of code precede
 > the data lines.  But it _is_ a one-file solution, as
 > requested.  Put the following 9 or 10 lines into a
 > file named "r.source.file", then source it.
 > 
 > data.01 <- read.table(file="r.source.file", header=T,
 > 	skip=4, comment.char="")[-1]
 > 
 >  # junk Sex      Response
 >     #   Male     1
 >     #   Male     2
 >     #   Female   3
 >     #   Female   4
 > 
 > 
 > I'm quite surprised no one else has suggested this already.
 > 


Nice thinking , Thomas, and good fun indeed.  To take this slightly
further, we can hack the history mechanism to read off the name of the
file being sourced.  If the following lines

  MyHistory <- function() {
    ## basically the first few lines of history()

    file1 <- tempfile("Rrawhist")
    savehistory(file1)
    rawhist <- scan(file1, what = "", quiet = TRUE, sep = "\n")
    unlink(file1)
    rawhist[length(rawhist)]
    }

  cat(strsplit(strsplit(MyHistory(),
  'source\\(')[[1]][2],'\\)')[[1]][1], '\n')

are placed in the file foo.q, then the call

 source('foo.q')

will produce as output

  'foo.q' 

on the terminal.  Instead of writing it out, it could be piped into
read.table(), and by careful linecounting, it could be combined with
your idea of reading lines, that are commented out in the 'real
reading' of the file.  


Then it indeed does what I wanted to do.  Though my students would 
be horrified...:-)

And, of course, if it is allowed to write the history to a temporary
file and read it again, we might as well write the data to a temporary
file, as has already been suggested by Torsten Hothorn.

Ernst



From ripley at stats.ox.ac.uk  Thu Jun 12 18:54:11 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jun 2003 17:54:11 +0100 (BST)
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.44415.319203.955415@pc87.math.ku.dk>
Message-ID: <Pine.LNX.4.44.0306121752420.20484-100000@gannet.stats>

This is not a valid solution: R does not necessarily have a history 
mechanism operational.  But if it did, you could use history() not
savehistory().

Does no one ever read the help pages?

On Thu, 12 Jun 2003, Ernst Hansen wrote:

> Thomas W Blackwell writes:
>  > Ernst  -
>  > 
>  > Here's a solution which works for me, and seems to do
>  > what you want.  It's a bit of a hack, since it requires
>  > you, the author, to know in advance what file path name
>  > the student will have saved the file as.  In my example,
>  > this will be "./r.source.file", and this includes one
>  > blank line before the first assignment statement below.
>  > 
>  > It also requires knowing how many lines of code precede
>  > the data lines.  But it _is_ a one-file solution, as
>  > requested.  Put the following 9 or 10 lines into a
>  > file named "r.source.file", then source it.
>  > 
>  > data.01 <- read.table(file="r.source.file", header=T,
>  > 	skip=4, comment.char="")[-1]
>  > 
>  >  # junk Sex      Response
>  >     #   Male     1
>  >     #   Male     2
>  >     #   Female   3
>  >     #   Female   4
>  > 
>  > 
>  > I'm quite surprised no one else has suggested this already.
>  > 
> 
> 
> Nice thinking , Thomas, and good fun indeed.  To take this slightly
> further, we can hack the history mechanism to read off the name of the
> file being sourced.  If the following lines
> 
>   MyHistory <- function() {
>     ## basically the first few lines of history()
> 
>     file1 <- tempfile("Rrawhist")
>     savehistory(file1)
>     rawhist <- scan(file1, what = "", quiet = TRUE, sep = "\n")
>     unlink(file1)
>     rawhist[length(rawhist)]
>     }
> 
>   cat(strsplit(strsplit(MyHistory(),
>   'source\\(')[[1]][2],'\\)')[[1]][1], '\n')
> 
> are placed in the file foo.q, then the call
> 
>  source('foo.q')
> 
> will produce as output
> 
>   'foo.q' 
> 
> on the terminal.  Instead of writing it out, it could be piped into
> read.table(), and by careful linecounting, it could be combined with
> your idea of reading lines, that are commented out in the 'real
> reading' of the file.  
> 
> 
> Then it indeed does what I wanted to do.  Though my students would 
> be horrified...:-)
> 
> And, of course, if it is allowed to write the history to a temporary
> file and read it again, we might as well write the data to a temporary
> file, as has already been suggested by Torsten Hothorn.
> 
> Ernst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From katkih at mail.nih.gov  Thu Jun 12 19:24:43 2003
From: katkih at mail.nih.gov (Katki, Hormuzd (NIH/NCI))
Date: Thu, 12 Jun 2003 13:24:43 -0400
Subject: [R] What PRECISELY is the dfbetas() or lm.influence()$coef ?
Message-ID: <9D7EF737FA4C6F4FBBFC52FC30B83690D49F55@nihexchange7.nih.gov>

	Hello.  I want to get the proper influence function for the glm
coefficients in R.  This is supposed to be inv(information)*(y-yhat)*x.  So
I am wondering what is the exact mathematical formula for the output that
the functions:

dfbeta()  OR   lm.influence()$coefficients 

return for a glm model.  I am confused because:

1. Their columns don't sum to zero as influences should.  
2. They return different "influences", so the 2 functions are doing
something different.
3. I think they divide each element by the standard error of the
corresponding coefficient, but that's not enough to resolve any
discrepancies

The documentation doesn't provide any details.  Any help would be greatly
appreciated.

> Thank you,
> Hormuzd Katki
> 
> Hormuzd Katki
> Biostatistics Branch, Division of Cancer Epidemiology and Genetics
> National Cancer Institute
> 6120 Executive Blvd. Room 8044 MSC 7244
> Rockville, MD 20852-4910
> 301-594-7818 (voice)
> 301-402-0081 (fax)
> katkih at mail.nih.gov
> 
>



From duncan at research.bell-labs.com  Thu Jun 12 20:05:48 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Thu, 12 Jun 2003 14:05:48 -0400
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.29831.927591.63021@pc87.math.ku.dk>;
	from erhansen@math.ku.dk on Thu, Jun 12, 2003 at 02:39:35PM +0200
References: <16104.29831.927591.63021@pc87.math.ku.dk>
Message-ID: <20030612140548.B2183@jessie.research.bell-labs.com>

Hi Ernst.

I have found myself in a similar situation where I want to send 
code to someone with annotations that explain the different pieces
in richer ways than comments will permit. 

If you want to contain both data and code within a single document,
you will need to have some way to identify which is which so that the
software can distinguish the different elements of the document.  This
is precisely what a markup language does. And rather than inventing ad
hoc conventions, why not simply use a real markup language. XML is the most
natural one, and doing something like

<doc>
 <data>
  Sex    Response
  Male   1
  Male   2
  Female 3
  Female 4
 </data>

 <code>
  ......
 </code>
</doc>


Using the XML package, you can read the document into R
and do what you will with it.
To read the data,

 tr = xmlRoot(xmlTreeParse("myFile"))
 read.table(textConnection(xmlValue(tr[["data"]])), header=TRUE)

and to access the code text

 xmlValue(tr[["code"]])


I have a variety of different variants of this style of thing that I
occassionally add to the SXMLDocs package. But, for me at least, it is
easy to write handlers to process the different content but to leave
XML to identify them within the document.

Hope this provides some ideas for thinking about the problem
in a slightly broader light.

 D.


Ernst Hansen wrote:
> I have the following problem.  It is not of earthshaking importance,
> but still I have spent a considerable amount of time thinking about
> it. 
> 
> PROBLEM: Is there any way I can have a single textfile that contains
> both
> 
>   a) data
> 
>   b) programcode
> 
> The program should act on the data, if the textfile is source()'ed
> into R.
> 
> 
> BOUNDARY CONDITION: I want the data written in the textfile in exactly
> the same format as I would use, if I had data in a separate textfile,
> to be read by read.table().  That is, with 'horizontal inhomogeneity'
> and 'vertical homogeneity' in the type of entries.  I want to write
> something like 
> 
>       Sex    Respons
>       Male   1
>       Male   2
>       Female 3
>       Female 4
> 
> In effect, I am asking if there is some way I can convince
> read.table(), that the data is contained in the following n lines of
> text. 
> 
> 
> ILLEGAL SOLUTIONS:
> I know I can simulate the behaviour by reading the columns of the
> dataframe one by one, and using data.frame() to glue them together.
> Like in 
> 
>     data.frame(Sex = c('Male', 'Male', 'Female', 'Female'),
>                Respons = c(1, 2, 3, 4))
> 
> I do not like this solution, because it represents the data in a
> "transposed" way in the textfile, and this transposition makes the
> structure of the dataframe less transparent - at least to me. It
> becomes even less comprehensible if the Sex-factor above is written
> with the help of rep() or gl() or the like.
> 
> I know I can make read.table() read from stdin, so I could type the
> dataframe at the prompt.  That is against the spirit of the problem,
> as I describe below.
> 
> 
> I know I can make read.table() do the job, if I split the data and the
> programcode in to different files.  But as the purpose of the exercise
> is to distribute the data and the code to other people, splitting
> into several files is a complication.
> 
> 
> MOTIVATION: I frequently find myself distributing small chunks of code
> to my students, along with data on which the code can work.
> 
> As an example, I might want to demonstrate how model.matrix() treats
> interactions, in a certain setting.  For that I need a dataframe that
> is complex enough to exhibit the behaviour I want, but still so small
> that the model.matrix is easily understood.  So I make such a
> dataframe.
> 
> I am trying to distribute this dataframe along with my code, in a way
> that is as simple as possible to USE for the students (hence the
> one-file boundary condition) and to READ (hence the non-transposition
> boundary condition).
> 
> 
> 
> Does anybody have any ideas?
> 
> 
> Ernst Hansen
> Department of Statistics
> University of Copenhagen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From hodgess at uhddx01.dt.uh.edu  Thu Jun 12 20:33:52 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Thu, 12 Jun 2003 13:33:52 -0500 (CDT)
Subject: [R] breaks
Message-ID: <200306121833.NAA10557@uhddx01.dt.uh.edu>

Dear R People:

I have a question about a "sorting" problem, please.

I have a vector xx:

> xx

 [1] -2.0  1.4 -1.2 -2.2  0.4  1.5 -2.2  0.2 -0.4 -0.9

and a vector of breaks:

> xx.y

[1] -2.2000000 -0.9666667  0.2666667  1.5000000

I want to produce another vector z which contains the number of the class
that each data point is in.

for instance, xx[1] is between xx.y[1] and xx.y[2], so z[1] == 1

this can be accomplished via loops, but I was wondering if there is a more
efficient method, please.

By the way, eventually, there will be many more data points and more
classes.

thank you for any help!

sincerely,
Erin Hodgess
mailto: hodgesse at uhd.edu

Version 1.7.0 R for Windows



From steve at paradise.stat.tku.edu.tw  Thu Jun 12 20:36:28 2003
From: steve at paradise.stat.tku.edu.tw (Steve Chen)
Date: Fri, 13 Jun 2003 02:36:28 +0800 (CST)
Subject: [R] R_PHP_Online version 0.2 with Security Fix
Message-ID: <Pine.LNX.4.21.0306130233350.23727-100000@paradise.stat.tku.edu.tw>

Hi all,

Thanks to Dr Paul Murrell in New Zealand to remind me of security
consideration.
I am releasing version 0.2 with security fix. This version will allow
you to add banned R function names in a list. 

R_PHP_Online is a PHP CGI web interface to run R programs online, with the
capability to show graphics online.

You can get current version of R_PHP_Online from

http://steve-chen.net/R_PHP/

or 

http://steve.stat.tku.edu.tw/R_PHP/


Steve Chen
Associate Professor,
Department of Statistics, 
TamKang University, Taiwan



From sundar.dorai-raj at pdf.com  Thu Jun 12 20:52:15 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 12 Jun 2003 13:52:15 -0500
Subject: [R] breaks
References: <200306121833.NAA10557@uhddx01.dt.uh.edu>
Message-ID: <3EE8CBDF.7080202@pdf.com>



Erin Hodgess wrote:
> Dear R People:
> 
> I have a question about a "sorting" problem, please.
> 
> I have a vector xx:
> 
> 
>>xx
> 
> 
>  [1] -2.0  1.4 -1.2 -2.2  0.4  1.5 -2.2  0.2 -0.4 -0.9
> 
> and a vector of breaks:
> 
> 
>>xx.y
> 
> 
> [1] -2.2000000 -0.9666667  0.2666667  1.5000000
> 
> I want to produce another vector z which contains the number of the class
> that each data point is in.
> 
> for instance, xx[1] is between xx.y[1] and xx.y[2], so z[1] == 1
> 
> this can be accomplished via loops, but I was wondering if there is a more
> efficient method, please.
> 
> By the way, eventually, there will be many more data points and more
> classes.
> 

I think what you're looking for is ?cut:

R> xx = c(-2.0,  1.4, -1.2, -2.2,  0.4,  1.5, -2.2,  0.2, -0.4, -0.9)
R> cut(xx, breaks = c(-Inf, -2.2, -0.97, 0.27, 1.5, Inf))
  [1] (-2.2,-0.97] (0.27,1.5]   (-2.2,-0.97] (-Inf,-2.2]  (0.27,1.5]
  [6] (0.27,1.5]   (-Inf,-2.2]  (-0.97,0.27] (-0.97,0.27] (-0.97,0.27]
Levels: (-Inf,-2.2] (-2.2,-0.97] (-0.97,0.27] (0.27,1.5] (1.5,Inf]
R>

Regards,
Sundar



From matthew_wiener at merck.com  Thu Jun 12 21:00:21 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 12 Jun 2003 15:00:21 -0400
Subject: [R] breaks
Message-ID: <AEBD81486231A343B1813FE62D33522503699F9E@usrymx15.merck.com>



t1 <- outer(data, breaks + c(rep(0, length(breaks)-1), 1e-5), "<")
Apply(t1, 1, function(x){min(which(x))}) - 1

Adding to the final break point makes sure that every data point will be
less than some break point.

So:

> xx <- c(-2, 1.4, -1.2, -2.2, .4, 1.5, -2.2, 0.2, -.4, -.9)
> xx.y <- c(-2.2, -0.967, 0.2667, 1.5)
> t1 <- outer(xx, xx.y + c(rep(0, length(xx.y)-1), 1), "<")
> apply(t1, 1, function(x){min(which(x))}) - 1
 [1] 1 3 1 1 3 3 1 2 2 2
> 


Hope this helps,

Matt


-----Original Message-----
From: Erin Hodgess [mailto:hodgess at uhddx01.dt.uh.edu] 
Sent: Thursday, June 12, 2003 2:34 PM
To: r-help at stat.math.ethz.ch
Subject: [R] breaks


Dear R People:

I have a question about a "sorting" problem, please.

I have a vector xx:

> xx

 [1] -2.0  1.4 -1.2 -2.2  0.4  1.5 -2.2  0.2 -0.4 -0.9

and a vector of breaks:

> xx.y

[1] -2.2000000 -0.9666667  0.2666667  1.5000000

I want to produce another vector z which contains the number of the class
that each data point is in.

for instance, xx[1] is between xx.y[1] and xx.y[2], so z[1] == 1

this can be accomplished via loops, but I was wondering if there is a more
efficient method, please.

By the way, eventually, there will be many more data points and more
classes.

thank you for any help!

sincerely,
Erin Hodgess
mailto: hodgesse at uhd.edu

Version 1.7.0 R for Windows

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From hodgess at uhddx01.dt.uh.edu  Thu Jun 12 21:20:05 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Thu, 12 Jun 2003 14:20:05 -0500 (CDT)
Subject: [R] summary to breaks question
Message-ID: <200306121920.OAA10786@uhddx01.dt.uh.edu>

Dear R People:

thanks SO MUCH for the quick responses to the breaks questions.

Here are two possible solutions:

                                                                                

I think what you're looking for is ?cut:

R> xx = c(-2.0,  1.4, -1.2, -2.2,  0.4,  1.5, -2.2,  0.2, -0.4, -0.9)

andR> cut(xx, breaks = c(-Inf, -2.2, -0.97, 0.27, 1.5, Inf))
  [1] (-2.2,-0.97] (0.27,1.5]   (-2.2,-0.97] (-Inf,-2.2]  (0.27,1.5]
  [6] (0.27,1.5]   (-Inf,-2.2]  (-0.97,0.27] (-0.97,0.27] (-0.97,0.27]
Levels: (-Inf,-2.2] (-2.2,-0.97] (-0.97,0.27] (0.27,1.5] (1.5,Inf]
R>


Status: R



t1 <- outer(data, breaks + c(rep(0, length(breaks)-1), 1e-5), "<")
Apply(t1, 1, function(x){min(which(x))}) - 1

Adding to the final break point makes sure that every data point will be
less than some break point.

So:

> xx <- c(-2, 1.4, -1.2, -2.2, .4, 1.5, -2.2, 0.2, -.4, -.9)
> xx.y <- c(-2.2, -0.967, 0.2667, 1.5)
> t1 <- outer(xx, xx.y + c(rep(0, length(xx.y)-1), 1), "<")
> apply(t1, 1, function(x){min(which(x))}) - 1
 [1] 1 3 1 1 3 3 1 2 2 2
> 

Thanks to Rolf, Sundar, and Matthew!!!!!!

Sincerely,
Erin



From Huiqin.Yang at noaa.gov  Thu Jun 12 20:55:39 2003
From: Huiqin.Yang at noaa.gov (Huiqin Yang)
Date: Thu, 12 Jun 2003 14:55:39 -0400
Subject: [R] how R manages memory
Message-ID: <3EE8CCAB.5040608@noaa.gov>

Hi everyone,

  Does anyone know how R manages memory since we have many large
objects reloaded every time we restart our work? Does R take long time
to load or to open when many objects saved ? Is it better to save objects
in different subdirectories?

  Many thanks.

Helen



From kjetil at entelnet.bo  Thu Jun 12 22:43:52 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 12 Jun 2003 16:43:52 -0400
Subject: [R] defaults in R: packages, .Rhistory
In-Reply-To: <Pine.LNX.4.44.0306120730350.15672-100000@gannet.stats>
References: <Pine.GSO.4.44.0306021128270.2402-100000@vidi.ucdavis.edu>
Message-ID: <3EE8ADC8.17445.1413D4@localhost>

On 12 Jun 2003 at 7:36, Prof Brian Ripley wrote:

> Please use a parseable subject line, or two separate messages for two 
> unrelated Qs.
.
.
.

> > A separate question also related to defaults:
> > 
> > Is there a way to get R to refrain from truncating the .Rhistory file
> > (removing the earliest saved commands), so that .Rhistory might serve
> > as a permanent record of every command issued?
> 
> Not in RGui, *but* you can change the buffer size in the Rconsole file
> or the Preferences.

Or you can set the environment variable R_HISTSIZE

Kjetil Halvorsen

> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jfox at mcmaster.ca  Thu Jun 12 22:58:54 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 12 Jun 2003 16:58:54 -0400
Subject: [R] What PRECISELY is the dfbetas() or lm.influence()$coef
  ?
In-Reply-To: <9D7EF737FA4C6F4FBBFC52FC30B83690D49F55@nihexchange7.nih.go
 v>
Message-ID: <5.1.0.14.2.20030612162725.01fbccf8@mcmail.cis.mcmaster.ca>

Dear Hormuzd,

At 01:24 PM 6/12/2003 -0400, Katki, Hormuzd (NIH/NCI) wrote:
>         Hello.  I want to get the proper influence function for the glm
>coefficients in R.  This is supposed to be inv(information)*(y-yhat)*x.  So
>I am wondering what is the exact mathematical formula for the output that
>the functions:
>
>dfbeta()  OR   lm.influence()$coefficients
>
>return for a glm model.  I am confused because:
>
>1. Their columns don't sum to zero as influences should.

Even in a linear model, where the computation is exact, this isn't the 
case, if influence is defined as the change in the coefficients upon 
deleting each observation in turn (i.e., as dfbeta).

>2. They return different "influences", so the 2 functions are doing
>something different.

That's odd. I believe that dfbeta() for a GLM simply uses influence.glm, 
which has the same $coefficients component as lm.influence. As such, for a 
GLM, both are based on the last step of the IRLS fit -- i.e., a 
linearization of the model.

>3. I think they divide each element by the standard error of the
>corresponding coefficient, but that's not enough to resolve any
>discrepancies

Perhaps you meant that dfbetas() [not dfbeta()] returns different values 
from lm.influence()$coef (as in your subject line)? dfbetas standardizes 
the coefficient changes by coefficient standard errors, using a deleted 
estimate of the dispersion parameter.

>The documentation doesn't provide any details.  Any help would be greatly
>appreciated.

I hope that this helps,
  John


-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From hodgess at uhddx01.dt.uh.edu  Thu Jun 12 23:21:34 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Thu, 12 Jun 2003 16:21:34 -0500 (CDT)
Subject: [R] bootstrap question
Message-ID: <200306122121.QAA11597@uhddx01.dt.uh.edu>

Dear r People


I have a bootstrap question, please.

(this may possibly be a problem with my understanding of the bootstrap)

Suppose I have a sample of size 15, x[1], ...x[15].

This is a sample which is small compared to the population.

by the way, the x[i] are iid and have a common distribution F.

As I understand the bootstrap, that function will take a user specified
number of repititions.  In each repitition, each element x[i] has an
equal chance of selection.

Suppose I want to estimate the mean.

In each rep, a new sample is generated.  the mean is calculated in each rep.


I would like to see those means.

If I use the boot command,

boot(x,mean,R=40)$t

I thought that I would obtain 40 different values.  However, those $t values
are all the same.

what am I doing wrong, please?

thanks yet again!

sincerely,
Erin Hodgess
mailto: hodgess at uhddx01.dt.uh.edu



From jonck at vanderkogel.net  Thu Jun 12 23:57:45 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Thu, 12 Jun 2003 23:57:45 +0200
Subject: [R] Multiple imputation
Message-ID: <E6289C1D-9D20-11D7-B383-0005026E2B43@vanderkogel.net>

Hi all,
I'm currently working with a dataset that has quite a few missing 
values and after some investigation I figured that multiple imputation 
is probably the best solution to handle the missing data in my case. I 
found several references to functions in S-Plus that perform multiple 
imputation (NORM, CAT, MIX, PAN). Does R have corresponding functions?
I searched the archives but was not able to find anything conclusive 
there.
Any help on this subject is much appreciated.
Thanks, Jonck



From rpeng at stat.ucla.edu  Fri Jun 13 00:09:29 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Thu, 12 Jun 2003 15:09:29 -0700
Subject: [R] bootstrap question
In-Reply-To: <200306122121.QAA11597@uhddx01.dt.uh.edu>
References: <200306122121.QAA11597@uhddx01.dt.uh.edu>
Message-ID: <3EE8FA19.2070401@stat.ucla.edu>

You are using the boot function incorrectly.  This is taken from the 
help page:


statistic: A function which when applied to data returns a vector
           containing the statistic(s) of interest.  When
           `sim="parametric"', the first argument to `statistic' must be
           the data.  For each replicate a simulated dataset returned by
           `ran.gen' will be passed.  In all other cases `statistic'
           must take at least two arguments.  The first argument passed
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           will always be the original data. The second will be a vector
           of indices, frequencies or weights which define the bootstrap
           sample.

The default is nonparametric bootstrap (sim = "ordinary"), so you need 
to specify a function with TWO arguments.  Try

x <- rnorm(100)
boot(x, function(x, i) mean(x[i]), R = 1000)$t

-roger

Erin Hodgess wrote:
> Dear r People
> 
> 
> I have a bootstrap question, please.
> 
> (this may possibly be a problem with my understanding of the bootstrap)
> 
> Suppose I have a sample of size 15, x[1], ...x[15].
> 
> This is a sample which is small compared to the population.
> 
> by the way, the x[i] are iid and have a common distribution F.
> 
> As I understand the bootstrap, that function will take a user specified
> number of repititions.  In each repitition, each element x[i] has an
> equal chance of selection.
> 
> Suppose I want to estimate the mean.
> 
> In each rep, a new sample is generated.  the mean is calculated in each rep.
> 
> 
> I would like to see those means.
> 
> If I use the boot command,
> 
> boot(x,mean,R=40)$t
> 
> I thought that I would obtain 40 different values.  However, those $t values
> are all the same.
> 
> what am I doing wrong, please?
> 
> thanks yet again!
> 
> sincerely,
> Erin Hodgess
> mailto: hodgess at uhddx01.dt.uh.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From f0z6305 at labs.tamu.edu  Fri Jun 13 00:36:07 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Thu, 12 Jun 2003 17:36:07 -0500
Subject: [R] How to define a proper prior for an orthogonal matrix?
Message-ID: <002201c33133$03d83520$8bd75ba5@IE.TAMU.EDU>

Hey, R-listers,

I am now going to use Bayesian mathod to estimate
a matrix parameter C.
It is assumed that C is an orthogonal matrix already.
We know, if C is an arbitrary column vector, we may
use multivariate Gaussian prior on it.
However, now it is a matrix, so what can I do to
define a proper prior probability density function for C?

Thanks for your point.

Have a nice day!

Fred



From raleandr at esalq.usp.br  Fri Jun 13 00:48:53 2003
From: raleandr at esalq.usp.br (Roseli A. Leandro)
Date: Thu, 12 Jun 2003 22:48:53 -0000
Subject: [R] help with weights in lm and glm
Message-ID: <004d01c1a779$02cf3560$19d46b8f@esalq.usp.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030612/d4d06140/attachment.pl

From Simon.Blomberg at anu.edu.au  Fri Jun 13 01:06:33 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 13 Jun 2003 09:06:33 +1000
Subject: [R] Multiple imputation
Message-ID: <7A3A13F416B40842BD2C1753E044B359B09905@CASEVS02.cas.anu.edu.au>

There is also the mice package at http://www.multiple-imputation.com. 
CRAN has package norm.

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Jonck van der Kogel [mailto:jonck at vanderkogel.net]
> Sent: Friday, 13 June 2003 7:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Multiple imputation
> 
> 
> Hi all,
> I'm currently working with a dataset that has quite a few missing 
> values and after some investigation I figured that multiple 
> imputation 
> is probably the best solution to handle the missing data in 
> my case. I 
> found several references to functions in S-Plus that perform multiple 
> imputation (NORM, CAT, MIX, PAN). Does R have corresponding functions?
> I searched the archives but was not able to find anything conclusive 
> there.
> Any help on this subject is much appreciated.
> Thanks, Jonck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tlumley at u.washington.edu  Fri Jun 13 01:12:57 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Jun 2003 16:12:57 -0700 (PDT)
Subject: [R] help with weights in lm and glm
In-Reply-To: <004d01c1a779$02cf3560$19d46b8f@esalq.usp.br>
Message-ID: <Pine.A41.4.44.0306121558540.95876-100000@homer29.u.washington.edu>

On Sun, 27 Jan 2002, Roseli A. Leandro wrote:

>
>
>       Dear all
>
>       Could someone explain to why weights does not works in lm and glm
> in the example below?
>       Thanks in advance, Roseli.
>
>
>        function(model){
>
>         www<-fitted(model)
>         lm(formula(model),weights=www)
>
>        }
>
>        The message error is: Error in eval(expr,envir,enclos): object
> "www" not found.
>

Because it's looking in the environment of the model formula.

That's why it can find the other variables in the model even though you
didn't specify a dataframe to search.

You can use

function(model){
  www<-fitted(model)
  update(model, weights=www)
}

which does work (although it arguably shouldn't).

The handling of weights= in these modelling functions is a horrible wart
on the language.  Although they look like ordinary arguments they have
very strange scoping rules.


	-thomas



From fharrell at virginia.edu  Fri Jun 13 01:48:32 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 12 Jun 2003 19:48:32 -0400
Subject: [R] Multiple imputation
In-Reply-To: <E6289C1D-9D20-11D7-B383-0005026E2B43@vanderkogel.net>
References: <E6289C1D-9D20-11D7-B383-0005026E2B43@vanderkogel.net>
Message-ID: <20030612194832.4d4059c4.fharrell@virginia.edu>

On Thu, 12 Jun 2003 23:57:45 +0200
Jonck van der Kogel <jonck at vanderkogel.net> wrote:

> Hi all,
> I'm currently working with a dataset that has quite a few missing 
> values and after some investigation I figured that multiple imputation 
> is probably the best solution to handle the missing data in my case. I 
> found several references to functions in S-Plus that perform multiple 
> imputation (NORM, CAT, MIX, PAN). Does R have corresponding functions?
> I searched the archives but was not able to find anything conclusive 
> there.
> Any help on this subject is much appreciated.
> Thanks, Jonck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Look at the aregImpute function in the Hmisc package (http://hesweb1.med.virginia.edu/biostat/s/Hmisc.html).  aregImpute uses the bootstrap, predictive mean matching, and flexible additive regression models to do multiple imputation.  In one simulation study it performs as well as MICE but it runs much faster and does not assume linearity in the imputation models.  I hope that someday we'll have simulation studies comparing aregImpute with NORM.
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From gina.joue at ucd.ie  Fri Jun 13 02:11:33 2003
From: gina.joue at ucd.ie (g)
Date: Fri, 13 Jun 2003 01:11:33 +0100
Subject: [R] formula (joint, conditional independence, etc.) - mosaicplots
Message-ID: <200306130111.33130.gina.joue@ucd.ie>

Hi,

Can someone set me straight as to how to write formulas in R to indicate:
	complete independence [A][B][C]
	joint independence [AB][C]
	conditional independence [AC][BC]
	nway interaction [AB][AC][BC]
?

For example, if I have 4 factors: 
hair colour, eye colour, age, sex

does 
	>  mosaicplot( frequency ~ hair + eye + age + sex)

mean that the model fitted is of complete independence of all factors 
[hair][eye][age][sex]?

So does 
	> mosaicplot(frequency ~ hair + eye)
mean that the model is of conditional independence
[hairAgeSex][eyeAgeSex]?


How does the operator *  as in
	> mosaicplot( frequency ~ hair * eye)
or
	> mosaicplot( frequency ~ hair * eye + age)
equate to in the type of independence model used?


Thanks in advance for any elucidation!

Gina



From Simon.Blomberg at anu.edu.au  Fri Jun 13 02:20:48 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 13 Jun 2003 10:20:48 +1000
Subject: [R] formula (joint, conditional independence, etc.) - mosaicplots
Message-ID: <7A3A13F416B40842BD2C1753E044B359B09907@CASEVS02.cas.anu.edu.au>

?formula

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: g [mailto:gina.joue at ucd.ie]
> Sent: Friday, 13 June 2003 10:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] formula (joint, conditional independence, etc.) -
> mosaicplots
> 
> 
> Hi,
> 
> Can someone set me straight as to how to write formulas in R 
> to indicate:
> 	complete independence [A][B][C]
> 	joint independence [AB][C]
> 	conditional independence [AC][BC]
> 	nway interaction [AB][AC][BC]
> ?
> 
> For example, if I have 4 factors: 
> hair colour, eye colour, age, sex
> 
> does 
> 	>  mosaicplot( frequency ~ hair + eye + age + sex)
> 
> mean that the model fitted is of complete independence of all factors 
> [hair][eye][age][sex]?
> 
> So does 
> 	> mosaicplot(frequency ~ hair + eye)
> mean that the model is of conditional independence
> [hairAgeSex][eyeAgeSex]?
> 
> 
> How does the operator *  as in
> 	> mosaicplot( frequency ~ hair * eye)
> or
> 	> mosaicplot( frequency ~ hair * eye + age)
> equate to in the type of independence model used?
> 
> 
> Thanks in advance for any elucidation!
> 
> Gina
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Peter.Caley at csiro.au  Fri Jun 13 02:44:40 2003
From: Peter.Caley at csiro.au (Peter.Caley@csiro.au)
Date: Fri, 13 Jun 2003 10:44:40 +1000
Subject: [R] Factorial function in R?
Message-ID: <662FF5A611597C41987B05E11DFE3592EF2A96@exact3-cbr.act.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030613/33bc492f/attachment.pl

From kwan022 at stat.auckland.ac.nz  Fri Jun 13 02:53:25 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 13 Jun 2003 12:53:25 +1200 (NZST)
Subject: [R] Factorial function in R?
In-Reply-To: <662FF5A611597C41987B05E11DFE3592EF2A96@exact3-cbr.act.csiro.au>
Message-ID: <Pine.LNX.4.44.0306131252200.20258-100000@stat61.stat.auckland.ac.nz>

AFAIK there isn't one.  You need to use gamma() or prod() (or writing up a 
recursive function if you want to be inefficient ;-D).

On Fri, 13 Jun 2003 Peter.Caley at csiro.au wrote:

> Date: Fri, 13 Jun 2003 10:44:40 +1000
> From: Peter.Caley at csiro.au
> To: r-help at stat.math.ethz.ch
> Subject: [R] Factorial function in R?
> 
> Is there a native factorial function in R [my searches have been
> fruitless], or do I need to use gamma() function?
>  
> *********************************************************************
> Dr Peter Caley
> CSIRO Entomology
> GPO Box 1700, Canberra,
> ACT 2601
> Email: peter.caley at csiro.au
> Ph: +61 (0)2 6246 4076   Fax: +61 (0)2 6246 4000
> *********************************************************************
>  
>  
> 
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From gina.joue at ucd.ie  Fri Jun 13 03:09:27 2003
From: gina.joue at ucd.ie (g)
Date: Fri, 13 Jun 2003 02:09:27 +0100
Subject: [R] mca & contingency tables - error: "All variables must be
	factors"
Message-ID: <200306130209.27496.gina.joue@ucd.ie>

Hi,

I would like to do a multiple correspondence analysis with the mca function in 
the MASS library on data that I have as a contingency table (which I've tried 
converting to a data frame).

For example,
=========
		> data(HairEyeColor)
		> hair.df <- as.data.frame(HairEyeColor)
		> hair.df
    Hair   Eye    Sex Freq
1  Black Brown   Male   32
2  Brown Brown   Male   38
3    Red Brown   Male   10
4  Blond Brown   Male    3
5  Black  Blue   Male   11
6  Brown  Blue   Male   50
7    Red  Blue   Male   10
8  Blond  Blue   Male   30
9  Black Hazel   Male   10
10 Brown Hazel   Male   25
11   Red Hazel   Male    7
		> mca(hair.df)
	Error in mca(hair.df) : All variables must be factors
=========

What are the manipulations I must do on the data in order for mca to work?

Any help appreciated....
Gina



From jfkincaidsu at netscape.net  Fri Jun 13 04:24:00 2003
From: jfkincaidsu at netscape.net (Joel Kincaid)
Date: Thu, 12 Jun 2003 22:24:00 -0400
Subject: [R] Lattice levelplots and (partial) failure on some devices
	----Long
Message-ID: <3EE935C0.1080405@netscape.net>



Dear Community,

(win XP home, sp1; R1.7 -- patched version, binary download, version of 
lattice obtained with patched version; the following applies to 1.6.2 as 
well.)

The following message was originally composed to seek help. However I 
have 'hacked' a solution to the following behavior, but none the less 
someone might find this interesting.

To reproduce my 'problem' you can use the following code:

library(lattice)
hypodata <- expand.grid(ProdMult = seq(0,1,by=.1),
                         WfMult = seq(0,1,by=.1),
                         Tr=c(0,75))
hypodata$UnCC <- 4.6 + (hypodata$ProdM^2) * (hypodata$Wf^3 )
hypodata$UnCC[1:121]   <- 4.6
testfault <-    levelplot(UnCC ~ WfMult + ProdMult | Tr,
                 col.regions=grey(16:0/16),
                 data=hypodata)

If one enters
 > testfault

the you get the expected result (if a bit uninteresting). This being a 
blank first panel (hmmm, shouldn't it be 4.6) and then a second panel 
with a filled contour. (I have a much larger set of simulated data --- 
about 450 pages of figures, this 'blank' panel appears periodically and 
is of interest...)

If you do
< pdf(file="testfault.pdf") #Error
< testfault
< dev.off()
or postscript(...)

then the file is created. Try to open with pdf reader or ghost 
script---can't be done!, you get the following error from the pdf file:

"There was an error processing a page. Two Few operands."

and then you get....

"An unrecognized token '-1.#J' was found."

and then a blank sheet. Ghostscript returns the first occurrence of the 
token and then dies a proper death.

Upon opening the pdf file in a text editor one easily locates the 
following chunk of 'stuff'
......
pdf stuff>>ET
pdf stuff>>Q q -216.00 -216.00 864.00 864.00 re W n
pdf stuff>>Q q 49.70 37.14 153.47 358.88 re W n
pdf stuff>>1.000 1.000 1.000 rg
pdf stuff>>  -1.#J -1.#J m
pdf stuff>>  -1.#J -1.#J l
pdf stuff>>  -1.#J -1.#J l
pdf stuff>>  -1.#J -1.#J l
pdf stuff>>h f

A naive search and replace of -1.#J with 0.0 , and a save, does the 
trick. The file will now open in a pdf reader (as well as ghostscript). 
There is however, a message stating something to the effect that a 
problem has been found, but the file is being 'rebuilt'.

Now what is interesting is that the above is only a problem for the pdf 
and postscript devices --- other devices, e.g. win.metafile, png, jpg, 
etc. work fine .

(However a similar setup, I'll track it down it someone thinks it would 
be of use, will trigger a crash that seems to associated with the 
'ntdll.dll' dyn. lnk. library(?)
E.g from the event viewer I get...

Faulting application rgui.exe, version 1.70.30515.0, faulting module 
ntdll.dll, version 5.1.2600.1106, fault address 0x000267cd

0000: 6c707041 74616369 206e6f69 6c696146
0010: 20657275 75677220 78652e69 2e312065
0020: 332e3037 35313530 6920302e 746e206e
0030: 2e6c6c64 206c6c64 2e312e35 30303632
0040: 3031312e 74612036 66666f20 20746573
0050: 32303030 64633736 0a0d
)


If you use the data above and try things with the base package plots --- 
contour and filled.contour (for the constant 4.6 data),

contour gives the error message:
...stack imbalance in internal contour....
and plots an empty graphics box. However it saves to all of the devices 
exactly as on the screen device.

The following
 >filled.contour( x=seq(0,1,by=.1),
 >                y=seq(0,1,by=.1),
 >                z=matrix(rep(4.6,121),nrow=11,ncol=11))
gives  the expected 'slightly pink' filled contour and can be written to 
  all of the devices with no problem.

Thus it appear to me that there is some interaction between some of the 
devices and lattice (is the string "-1.#J" some kind of character set 
for NA or NaN or NULL???). I tried to find the area in the lattice code 
that seemed to deal with this issue  and found some call to (i guess) C 
code around lines 4554 which deal with 'NA's in the data....however I 
was a bit lost....(Also searching the base code I could find no 
reference to the error message returned from the contour(....) example 
above....)


In summary, is there a way to avoid this issue and keep the 'blank' 
panels, and not have to do this search and replace (on a 450 page pdf 
file it takes about 15 mins ....),

cheers,
Joel



From jfox at mcmaster.ca  Fri Jun 13 04:23:51 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 12 Jun 2003 22:23:51 -0400
Subject: [R] Multiple imputation
In-Reply-To: <20030612194832.4d4059c4.fharrell@virginia.edu>
References: <E6289C1D-9D20-11D7-B383-0005026E2B43@vanderkogel.net>
	<E6289C1D-9D20-11D7-B383-0005026E2B43@vanderkogel.net>
Message-ID: <5.1.0.14.2.20030612221937.01ec2378@mcmail.cis.mcmaster.ca>

Dear Jonck,

In addition, there are ports of both norm and mix in the 
contributed-packages section of CRAN.

Regards,
  John

At 07:48 PM 6/12/2003 -0400, Frank E Harrell Jr wrote:
>On Thu, 12 Jun 2003 23:57:45 +0200
>Jonck van der Kogel <jonck at vanderkogel.net> wrote:
>
> > Hi all,
> > I'm currently working with a dataset that has quite a few missing
> > values and after some investigation I figured that multiple imputation
> > is probably the best solution to handle the missing data in my case. I
> > found several references to functions in S-Plus that perform multiple
> > imputation (NORM, CAT, MIX, PAN). Does R have corresponding functions?
> > I searched the archives but was not able to find anything conclusive
> > there.
> > Any help on this subject is much appreciated.
> > Thanks, Jonck
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>Look at the aregImpute function in the Hmisc package 
>(http://hesweb1.med.virginia.edu/biostat/s/Hmisc.html).  aregImpute uses 
>the bootstrap, predictive mean matching, and flexible additive regression 
>models to do multiple imputation.  In one simulation study it performs as 
>well as MICE but it runs much faster and does not assume linearity in the 
>imputation models.  I hope that someday we'll have simulation studies 
>comparing aregImpute with NORM.
>---
>Frank E Harrell Jr              Prof. of Biostatistics & Statistics
>Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
>U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From bdm25 at drexel.edu  Fri Jun 13 05:33:52 2003
From: bdm25 at drexel.edu (B D McCullough)
Date: Thu, 12 Jun 2003 23:33:52 -0400
Subject: [R] Testing the R RNGs
Message-ID: <200306130333.h5D3Xqe01986@localhost.localdomain>

I have applied L'Ecuyer's TESTU01 suite of RNG tests 
to the RNGs in R.  TESTU01 offers three increasingly
more stringent suites, called "Small Crush", "Crush" and
"Big Crush".  If a particular RNG fails Small Crush, there
is no need to apply Big Crush.

Below I summarize the results:

                  Number of Tests Failed
		 Small Crush   Crush   Big Crush
Wichmann-Hill              1              3        NA
Multicarry                     1            13        NA
Super-Duper                1              9        NA
Mersenne                    --             --          --
TAOCP                      0              0          0
TAOCP-2002              0              0          0

NA: Not Applied because not necessary
--: couldn't duplicate RNG output, so test not run

A more detailed description, R-RNGTests.txt
will be available at via my homepage until the end
of June 2003: www.pages.drexel.edu/~bdm25
look for the link at the bottom of the page.

For reasons described in the R-RNGTests.txt, I
could not test the Mersenne RNG.  This will 
require someone with a better knowledge of
the C language than I possess.  If anyone can
figure it out, I would be most grateful.  Please
reply directly to me.

Regards,

Bruce McCullough



From junkmail at mu.met.psu.edu  Fri Jun 13 05:40:04 2003
From: junkmail at mu.met.psu.edu (Andy Jacobson)
Date: Thu, 12 Jun 2003 23:40:04 -0400
Subject: [R] covariate data errors
Message-ID: <E19QfQ4-0004NQ-00@mu.met.psu.edu>

Greetings,

	I would like to fit a multiple linear regression model in
which the residuals are expected to follow a multivariate normal
distribution, using weighted least squares.  I know that the data in
question have biases that would result in correlated residuals, and I
have a means for quantifying those biases as a covariance matrix. I
cannot, unfortunately, correct the data for these biases.

	It seems that this should be a straightforward task, but so
much of the literature is concerned with the probability model in
which the residuals are uncorrelated that I can't find a good
reference.  So in order of importance, please, can someone point me to
a definitive reference for least squares with correlated residuals,
and is there a standard R package to handle this case?

	Many thanks in advance,

	Anthony



From MSchwartz at medanalytics.com  Fri Jun 13 06:18:34 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 13 Jun 2003 04:18:34 -0000
Subject: [R] Factorial function in R?
In-Reply-To: <Pine.LNX.4.44.0306131252200.20258-100000@stat61.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0306131252200.20258-100000@stat61.stat.auckland.ac.nz>
Message-ID: <1055470899.31195.2.camel@localhost>

On Thu, 2003-06-12 at 19:53, Ko-Kang Kevin Wang wrote:
> AFAIK there isn't one.  You need to use gamma() or prod() (or writing up a 
> recursive function if you want to be inefficient ;-D).
> 
> On Fri, 13 Jun 2003 Peter.Caley at csiro.au wrote:
> 
> > Date: Fri, 13 Jun 2003 10:44:40 +1000
> > From: Peter.Caley at csiro.au
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Factorial function in R?
> > 
> > Is there a native factorial function in R [my searches have been
> > fruitless], or do I need to use gamma() function?


There is the factorial() function in the gregmisc package on CRAN,
otherwise the gamma() function as has been mentioned.

HTH,

Marc Schwartz



From Bill.Venables at csiro.au  Fri Jun 13 06:51:10 2003
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Fri, 13 Jun 2003 14:51:10 +1000
Subject: [R] covariate data errors
Message-ID: <E09E527B56BE2D438A3D6A246DDD27A9165C9C@roper-cv.qld.cmis.CSIRO.AU>

The function gls() in the nlme library will handle correlated observations
assuming that you have an easily specified pattern to the variances and
covariances (e.g. an AR(1) process).

If you just have an arbitrary variance matrix for the y-vector do not
despair.  You just have to do a few matrix algebra computations, with which
I am sure you are quite expert... The functions you will probably need are
simple ones like chol(), crossprod(), "%*%" and solve() so nothing too
difficult there.  You might want to start with model.matrix(), though, and
that's slightly more challenging for most people.

Bill Venables.

-----Original Message-----
From: Andy Jacobson [mailto:junkmail at mu.met.psu.edu]
Sent: Friday, June 13, 2003 1:40 PM
To: r-help at stat.math.ethz.ch
Subject: [R] covariate data errors


Greetings,

	I would like to fit a multiple linear regression model in
which the residuals are expected to follow a multivariate normal
distribution, using weighted least squares.  I know that the data in
question have biases that would result in correlated residuals, and I
have a means for quantifying those biases as a covariance matrix. I
cannot, unfortunately, correct the data for these biases.

	It seems that this should be a straightforward task, but so
much of the literature is concerned with the probability model in
which the residuals are uncorrelated that I can't find a good
reference.  So in order of importance, please, can someone point me to
a definitive reference for least squares with correlated residuals,
and is there a standard R package to handle this case?

	Many thanks in advance,

	Anthony

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jfox at mcmaster.ca  Fri Jun 13 06:54:28 2003
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 13 Jun 2003 00:54:28 -0400
Subject: [R] covariate data errors
In-Reply-To: <E19QfQ4-0004NQ-00@mu.met.psu.edu>
Message-ID: <5.1.0.14.2.20030613004836.01ec85a8@mcmail.cis.mcmaster.ca>

Dear Anthony,

The gls (generalized least squares) function in the nlme package should do 
what you want. (I assume that your analysis leads you to expect an 
error-covariance matrix of a specific form with some free parameters to 
estimate.)

Generalized least squares estimation is a common topic in regression texts. 
You'll find a brief appendix on the subject from my R and S-PLUS Companion 
to Applied Regression, in the context of time-series regression, at 
<http://www.socsci.mcmaster.ca/jfox/Books/Companion/appendix-timeseries-regression.pdf>.

I hope that this helps,
  John

At 11:40 PM 6/12/2003 -0400, Andy Jacobson wrote:
>Greetings,
>
>         I would like to fit a multiple linear regression model in
>which the residuals are expected to follow a multivariate normal
>distribution, using weighted least squares.  I know that the data in
>question have biases that would result in correlated residuals, and I
>have a means for quantifying those biases as a covariance matrix. I
>cannot, unfortunately, correct the data for these biases.
>
>         It seems that this should be a straightforward task, but so
>much of the literature is concerned with the probability model in
>which the residuals are uncorrelated that I can't find a good
>reference.  So in order of importance, please, can someone point me to
>a definitive reference for least squares with correlated residuals,
>and is there a standard R package to handle this case?
>
>         Many thanks in advance,
>
>         Anthony

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From franklin at polisci.wisc.edu  Fri Jun 13 07:05:11 2003
From: franklin at polisci.wisc.edu (Charles H. Franklin)
Date: Fri, 13 Jun 2003 00:05:11 -0500
Subject: [R] R 1.7.0 startup error: .addMethodFrom...
Message-ID: <PDEMINECFJGAIJGDGKGOGEDLEBAA.franklin@polisci.wisc.edu>

Does anyone know what could be causing the following error message on
startup of R:

Error in .addMethodFrom(def,argName[1],class[1],fromClass) :
  object "*tmp*" not found.


I'm using R 1.7.0 and Windows 2000.

This happened shortly after I installed 1.7.0. Once it occurs, R freezes.
>From then on R will always freeze with this error message.

I reinstalled 1.7.0 and it worked reliably for a couple of weeks or more,
but now once more has this problem.

Thanks.

Charles


/******************************************
** Charles H. Franklin
** Professor, Political Science
** University of Wisconsin, Madison
** 1050 Bascom Mall
** Madison, WI 53706
** 608-263-2022 Office
** 608-265-2663 Fax
** mailto:franklin at polisci.wisc.edu (best)
** mailto:chfrankl at facstaff.wisc.edu (alt)
** http://www.polisci.wisc.edu/~franklin
******************************************/



From Duncan.Mackay at flinders.edu.au  Fri Jun 13 07:13:19 2003
From: Duncan.Mackay at flinders.edu.au (Duncan Mackay)
Date: Fri, 13 Jun 2003 14:43:19 +0930
Subject: [R] covariate data errors
In-Reply-To: <E19QfQ4-0004NQ-00@mu.met.psu.edu>
Message-ID: <LKEKIOMKIBNKJOPKIKOOIEHMDGAA.Duncan.Mackay@flinders.edu.au>


Hello,
I'm not qualified to judge about definitiveness, but you may find the
discussion of "Time Series Regression and Generalized Least Squares" in
Chapter 14 of "Applied Regression Analysis, Linear Models, and Related
Models" by John Fox (1997, Sage Publications) helpful. He also has an
accompanying package "car" on the CRAN R site.
Duncan

*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andy Jacobson
Sent: Friday, 13 June 2003 1:10 PM
To: r-help at stat.math.ethz.ch
Subject: [R] covariate data errors


Greetings,

	I would like to fit a multiple linear regression model in
which the residuals are expected to follow a multivariate normal
distribution, using weighted least squares.  I know that the data in
question have biases that would result in correlated residuals, and I
have a means for quantifying those biases as a covariance matrix. I
cannot, unfortunately, correct the data for these biases.

	It seems that this should be a straightforward task, but so
much of the literature is concerned with the probability model in
which the residuals are uncorrelated that I can't find a good
reference.  So in order of importance, please, can someone point me to
a definitive reference for least squares with correlated residuals,
and is there a standard R package to handle this case?

	Many thanks in advance,

	Anthony

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From maechler at stat.math.ethz.ch  Fri Jun 13 08:32:30 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jun 2003 08:32:30 +0200
Subject: [R] breaks
In-Reply-To: <200306121833.NAA10557@uhddx01.dt.uh.edu>
References: <200306121833.NAA10557@uhddx01.dt.uh.edu>
Message-ID: <16105.28670.120463.635762@gargle.gargle.HOWL>

Erin, even though you've already summarized,
I think the optimal answer to your question is

  findInterval()

{there's also R-C API you can use from your  C/C++}

Martin


>>>>> "Erin" == Erin Hodgess <hodgess at uhddx01.dt.uh.edu>
>>>>>     on Thu, 12 Jun 2003 13:33:52 -0500 (CDT) writes:

    Erin> Dear R People: I have a question about a "sorting"
    Erin> problem, please.

    Erin> I have a vector xx:

    >> xx

    Erin>  [1] -2.0 1.4 -1.2 -2.2 0.4 1.5 -2.2 0.2 -0.4 -0.9

    Erin> and a vector of breaks:

    >> xx.y

    Erin> [1] -2.2000000 -0.9666667 0.2666667 1.5000000

    Erin> I want to produce another vector z which contains the
    Erin> number of the class that each data point is in.

    Erin> for instance, xx[1] is between xx.y[1] and xx.y[2], so
    Erin> z[1] == 1

    Erin> this can be accomplished via loops, but I was
    Erin> wondering if there is a more efficient method, please.

    Erin> By the way, eventually, there will be many more data
    Erin> points and more classes.

    Erin> thank you for any help!

    Erin> sincerely, Erin Hodgess mailto: hodgesse at uhd.edu

    Erin> Version 1.7.0 R for Windows

    Erin> ______________________________________________
    Erin> R-help at stat.math.ethz.ch mailing list
    Erin> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From maechler at stat.math.ethz.ch  Fri Jun 13 08:38:55 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jun 2003 08:38:55 +0200
Subject: [R] Factorial function in R?
In-Reply-To: <1055470899.31195.2.camel@localhost>
References: <Pine.LNX.4.44.0306131252200.20258-100000@stat61.stat.auckland.ac.nz>
	<1055470899.31195.2.camel@localhost>
Message-ID: <16105.29055.276006.398693@gargle.gargle.HOWL>

>>>>> "Marc" == Marc Schwartz <MSchwartz at medanalytics.com>
>>>>>     on 12 Jun 2003 21:21:39 -0500 writes:

    Marc> On Thu, 2003-06-12 at 19:53, Ko-Kang Kevin Wang wrote:
    >> AFAIK there isn't one.  You need to use gamma() or prod()
    >> (or writing up a recursive function if you want to be
    >> inefficient ;-D).
    >> 
    >> On Fri, 13 Jun 2003 Peter.Caley at csiro.au wrote:
    >> 
    >> > Date: Fri, 13 Jun 2003 10:44:40 +1000 > From:
    >> Peter.Caley at csiro.au > To: r-help at stat.math.ethz.ch >
    >> Subject: [R] Factorial function in R?
    >> > 
    >> > Is there a native factorial function in R [my searches
    >> have been > fruitless], or do I need to use gamma()
    >> function?


    Marc> There is the factorial() function in the gregmisc
    Marc> package on CRAN, otherwise the gamma() function as has
    Marc> been mentioned.

Yes, and also "gregmisc" doesn't have more than

     factorial <- function (x) gamma(1 + x)

something which we have thought to be too trivial to provide an
extra function name for -- particularly in light of the use of the
term "factorial" in something like "factorial design" in statistics.

If this question continues to pop up with such regularity, we
probably should reconsider...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ripley at stats.ox.ac.uk  Fri Jun 13 08:54:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jun 2003 07:54:07 +0100 (BST)
Subject: [R] covariate data errors
In-Reply-To: <E19QfQ4-0004NQ-00@mu.met.psu.edu>
Message-ID: <Pine.LNX.4.44.0306130748560.21585-100000@gannet.stats>

Do you mean correlations in the *errors*?  The residuals are always 
correlated.  What does this have to do with your subject line -- it is 
errors in the dependent variable I think you mean?

If you have correlated errors, you should be using generalized least
squares not least squares or weighted least squares.  (That is covered in
all good books on regression: I don't know your level, but Seber's has a
comprehensive account.)  There are several R functions to fit GLS,
including gls(nlme) and lm.gls(MASS).


On Thu, 12 Jun 2003, Andy Jacobson wrote:

> Greetings,
> 
> 	I would like to fit a multiple linear regression model in
> which the residuals are expected to follow a multivariate normal
> distribution, using weighted least squares.  I know that the data in
> question have biases that would result in correlated residuals, and I
> have a means for quantifying those biases as a covariance matrix. I
> cannot, unfortunately, correct the data for these biases.
> 
> 	It seems that this should be a straightforward task, but so
> much of the literature is concerned with the probability model in
> which the residuals are uncorrelated that I can't find a good
> reference.  So in order of importance, please, can someone point me to
> a definitive reference for least squares with correlated residuals,
> and is there a standard R package to handle this case?
> 
> 	Many thanks in advance,
> 
> 	Anthony
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From plummer at iarc.fr  Fri Jun 13 09:04:26 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: Fri, 13 Jun 2003 07:04:26 -0000
Subject: [R] Does the RPM for RH9 know about TCL/Tk
In-Reply-To: <x2wufs7wlo.fsf@biostat.ku.dk>
References: <Pine.GSO.4.53.0306101825150.27109@itsa.ucsf.edu>
	<20030611014805.GA28098@mail1.sas.upenn.edu>
	<x2wufs7wlo.fsf@biostat.ku.dk>
Message-ID: <1055488084.1074.4.camel@xena>

On Wed, 2003-06-11 at 12:10, Peter Dalgaard BSA wrote:
> Jonathan Baron <baron at psych.upenn.edu> writes:
> 
> > On 06/10/03 18:30, Morgan Hough wrote:
> > >Sorry for the probable repeat post but I can only search the list up to
> > >2002 (is there a better way?). 
> > 
> > Yes, see my search page below.
> > 
> > >I am using the RH9 RPM from CRAN but
> > >packages like AnalyzeFMRI say that tcltk is not found. Do I need to do
> > >more to get Tk GUIs working on RH9 or does the RPM not have tcltk support
> > >built in (should I compile from source). Thanks in advance.
> > 
> > There was in fact some discussion of this last month.  I am not
> > sure of the answer.  But I installed 1.7.0 from the RPM for RH 9,
> > and I got the same error message when trying to get Rcmdr to
> > work.  I did have tcl and tk installed.  Unfortunately, I did not
> > do a properly controlled experiment.  I first installed tcllib,
> > which was not installed originally.  (That didn't help, by
> > itself.)  Then I re-installed R _from source_ and then everything
> > worked.  But I did have the basic vanilla installation of RH 9,
> > and I did have this problem.  So you aren't the only one.  
> > 
> > I still don't know whether tcllib is necessary, and whether the
> > RPM itself installs different things depending on what is on the
> > system.  (I would assume not, but I'm not sure.)
> 
> It shouldn't, but the RPM may be different depending on what was
> present on the system upon which it was built. Martyn may have been
> building on a system where tcl/tk wasn't installed, or -- there's a
> bug report on bugzilla.redhat.com on this -- the build was adversely
> affected by incorrectness of the tclConfig.sh and tkConfig.sh scripts.
> 
> A fairly easy experiment would be to rebuild from the source RPM on
> your own system. Since this builds an RPM, it will retain the
> upgradability etc. of the "official" RPM. Could you try and tell us
> whether the problem remains? (Don't forget that you need a bunch of
> "-devel" packages installed.)

I have rebuilt the RPM of R 1.7.0 for Red Hat 9 with tcltk support.
(R-1.7.0-2.i386.rpm). It should be available on CRAN in a day or two.  I
apologise for this oversight.

Martyn



From david.meyer at ci.tuwien.ac.at  Fri Jun 13 09:57:38 2003
From: david.meyer at ci.tuwien.ac.at (David Meyer)
Date: Fri, 13 Jun 2003 09:57:38 +0200
Subject: [R] formula (joint, conditional independence, etc.) - mosaicplots
In-Reply-To: <200306130111.33130.gina.joue@ucd.ie>;
	from gina.joue@ucd.ie on Fri, Jun 13, 2003 at 02:11:33 +0200
References: <200306130111.33130.gina.joue@ucd.ie>
Message-ID: <20030613075738.GE10227@boromir.ci.tuwien.ac.at>

On 2003.06.13 02:11, g wrote:
> Hi,
> 
> Can someone set me straight as to how to write formulas in R to
> indicate:
> 	complete independence [A][B][C]

Freq ~ A + B + C

> 	joint independence [AB][C]

Freq ~ A * B + C

> 	conditional independence [AC][BC]

Freq ~ A * C + B * C

> 	nway interaction [AB][AC][BC]

Freq ~ A * B * C - A:B:C


You might have a look at demo(mosaic) in package vcd.

g.,
-d


> ?
> 
> For example, if I have 4 factors:
> hair colour, eye colour, age, sex
> 
> does
> 	>  mosaicplot( frequency ~ hair + eye + age + sex)
> 
> mean that the model fitted is of complete independence of all factors
> [hair][eye][age][sex]?
> 
> So does
> 	> mosaicplot(frequency ~ hair + eye)
> mean that the model is of conditional independence
> [hairAgeSex][eyeAgeSex]?
> 
> 
> How does the operator *  as in
> 	> mosaicplot( frequency ~ hair * eye)
> or
> 	> mosaicplot( frequency ~ hair * eye + age)
> equate to in the type of independence model used?
> 
> 
> Thanks in advance for any elucidation!
> 
> Gina
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From ripley at stats.ox.ac.uk  Fri Jun 13 11:23:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jun 2003 10:23:40 +0100 (BST)
Subject: [R] Lattice levelplots and (partial) failure on some devices
	----Long
In-Reply-To: <3EE935C0.1080405@netscape.net>
Message-ID: <Pine.LNX.4.44.0306131016320.22255-100000@gannet.stats>

This is a Windows run-time quirk (bug): Linux gives nan.  It's a problem
with what grid/lattice sends the driver.  Rather than put protection code 
in each driver, it needs to be in the central graphics code.

We've been here several times before ....

On Thu, 12 Jun 2003, Joel Kincaid wrote:

> 
> 
> Dear Community,
> 
> (win XP home, sp1; R1.7 -- patched version, binary download, version of 
> lattice obtained with patched version; the following applies to 1.6.2 as 
> well.)
> 
> The following message was originally composed to seek help. However I 
> have 'hacked' a solution to the following behavior, but none the less 
> someone might find this interesting.
> 
> To reproduce my 'problem' you can use the following code:
> 
> library(lattice)
> hypodata <- expand.grid(ProdMult = seq(0,1,by=.1),
>                          WfMult = seq(0,1,by=.1),
>                          Tr=c(0,75))
> hypodata$UnCC <- 4.6 + (hypodata$ProdM^2) * (hypodata$Wf^3 )
> hypodata$UnCC[1:121]   <- 4.6
> testfault <-    levelplot(UnCC ~ WfMult + ProdMult | Tr,
>                  col.regions=grey(16:0/16),
>                  data=hypodata)
> 
> If one enters
>  > testfault
> 
> the you get the expected result (if a bit uninteresting). This being a 
> blank first panel (hmmm, shouldn't it be 4.6) and then a second panel 
> with a filled contour. (I have a much larger set of simulated data --- 
> about 450 pages of figures, this 'blank' panel appears periodically and 
> is of interest...)
> 
> If you do
> < pdf(file="testfault.pdf") #Error
> < testfault
> < dev.off()
> or postscript(...)
> 
> then the file is created. Try to open with pdf reader or ghost 
> script---can't be done!, you get the following error from the pdf file:
> 
> "There was an error processing a page. Two Few operands."
> 
> and then you get....
> 
> "An unrecognized token '-1.#J' was found."
> 
> and then a blank sheet. Ghostscript returns the first occurrence of the 
> token and then dies a proper death.
> 
> Upon opening the pdf file in a text editor one easily locates the 
> following chunk of 'stuff'
> ......
> pdf stuff>>ET
> pdf stuff>>Q q -216.00 -216.00 864.00 864.00 re W n
> pdf stuff>>Q q 49.70 37.14 153.47 358.88 re W n
> pdf stuff>>1.000 1.000 1.000 rg
> pdf stuff>>  -1.#J -1.#J m
> pdf stuff>>  -1.#J -1.#J l
> pdf stuff>>  -1.#J -1.#J l
> pdf stuff>>  -1.#J -1.#J l
> pdf stuff>>h f
> 
> A naive search and replace of -1.#J with 0.0 , and a save, does the 
> trick. The file will now open in a pdf reader (as well as ghostscript). 
> There is however, a message stating something to the effect that a 
> problem has been found, but the file is being 'rebuilt'.
> 
> Now what is interesting is that the above is only a problem for the pdf 
> and postscript devices --- other devices, e.g. win.metafile, png, jpg, 
> etc. work fine .
> 
> (However a similar setup, I'll track it down it someone thinks it would 
> be of use, will trigger a crash that seems to associated with the 
> 'ntdll.dll' dyn. lnk. library(?)
> E.g from the event viewer I get...
> 
> Faulting application rgui.exe, version 1.70.30515.0, faulting module 
> ntdll.dll, version 5.1.2600.1106, fault address 0x000267cd
> 
> 0000: 6c707041 74616369 206e6f69 6c696146
> 0010: 20657275 75677220 78652e69 2e312065
> 0020: 332e3037 35313530 6920302e 746e206e
> 0030: 2e6c6c64 206c6c64 2e312e35 30303632
> 0040: 3031312e 74612036 66666f20 20746573
> 0050: 32303030 64633736 0a0d
> )
> 
> 
> If you use the data above and try things with the base package plots --- 
> contour and filled.contour (for the constant 4.6 data),
> 
> contour gives the error message:
> ...stack imbalance in internal contour....
> and plots an empty graphics box. However it saves to all of the devices 
> exactly as on the screen device.
> 
> The following
>  >filled.contour( x=seq(0,1,by=.1),
>  >                y=seq(0,1,by=.1),
>  >                z=matrix(rep(4.6,121),nrow=11,ncol=11))
> gives  the expected 'slightly pink' filled contour and can be written to 
>   all of the devices with no problem.
> 
> Thus it appear to me that there is some interaction between some of the 
> devices and lattice (is the string "-1.#J" some kind of character set 
> for NA or NaN or NULL???). I tried to find the area in the lattice code 
> that seemed to deal with this issue  and found some call to (i guess) C 
> code around lines 4554 which deal with 'NA's in the data....however I 
> was a bit lost....(Also searching the base code I could find no 
> reference to the error message returned from the contour(....) example 
> above....)
> 
> 
> In summary, is there a way to avoid this issue and keep the 'blank' 
> panels, and not have to do this search and replace (on a 450 page pdf 
> file it takes about 15 mins ....),
> 
> cheers,
> Joel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dave at evocapital.com  Fri Jun 13 12:05:30 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Fri, 13 Jun 2003 11:05:30 +0100
Subject: [R] Layout of windows devices
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012470@mail.internal.net>

Hi
 
Is there a way to specify the location on the screen where a new
graphics device opens, for example with a call to win.graph()?
I'm using R 1.7.0 on Windows XP.
 
Thank you.
 
Regards,
 
David



From fbosch at clinic.ub.es  Fri Jun 13 12:55:12 2003
From: fbosch at clinic.ub.es (Francesc Bosch Albareda)
Date: Fri, 13 Jun 2003 12:55:12 +0200
Subject: [R] (no subject)
Message-ID: <000001c3319a$451da080$dc1b10ac@intra.csc.es>

Dear collegues,

Using maxstat I am getting the following:

> blood <- maxstat.test(Surv(SUPER, FV)~ZAP,data=zap70, smethod="LogRank")
Error in maxstat(y = structure(c(24.4301369863014, 26.4164383561644,
18.7835616438356,  : couldn't find function "cscores"


I do not know the meaning of this problem. Could you please help me on dat?

Thank you in advance for your time.

Sincerely,

Francesc Bosch, M.D.
Senior specialist
Department of Hematology, Hospital Clinic
C/ Villarroel, 170
08036 - Barcelona
Phone & Fax: +34-93-227 5428
e-mail: fbosch at clinic.ub.es <mailto:fbosch at clinic.ub.es>



From dmurdoch at pair.com  Fri Jun 13 13:08:57 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 13 Jun 2003 07:08:57 -0400
Subject: [R] Layout of windows devices
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D012470@mail.internal.net>
References: <8D0F30FE2EB3314182D4A33F738BB19D012470@mail.internal.net>
Message-ID: <akbjevc3rok1a8alklmd0orpcr07g75ckp@4ax.com>

On Fri, 13 Jun 2003 11:05:30 +0100, you wrote:

>Hi
> 
>Is there a way to specify the location on the screen where a new
>graphics device opens, for example with a call to win.graph()?
>I'm using R 1.7.0 on Windows XP.

There isn't currently, but it should be possible to add it.  The
window creation is done in src/gnuwin32/devga.c.  

However, the user interface doesn't look easy.  Windows machines can
have more than one monitor, so you'd want to be able to say which
monitor it would appear on, as well as where.  I think you'd want to
be able to query the monitor size to make the choice useful.  

Is this something that you'd be able to do a first pass through?  I'd
be happy to put it in if someone else did most of the work, but I've
got a lot of other things in the works, and this looks too
time-consuming for a relatively small payback.  After all, you can
manually move the window after it is created.

Duncan Murdoch



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Jun 13 13:08:59 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 13 Jun 2003 13:08:59 +0200 (CEST)
Subject: [R] (no subject)
In-Reply-To: <000001c3319a$451da080$dc1b10ac@intra.csc.es>
References: <000001c3319a$451da080$dc1b10ac@intra.csc.es>
Message-ID: <Pine.LNX.4.51.0306131305070.11706@artemis.imbe.med.uni-erlangen.de>


> Dear collegues,
>
> Using maxstat I am getting the following:
>
> > blood <- maxstat.test(Surv(SUPER, FV)~ZAP,data=zap70, smethod="LogRank")
> Error in maxstat(y = structure(c(24.4301369863014, 26.4164383561644,
> 18.7835616438356,  : couldn't find function "cscores"
>

`cscores' is provided by package `exactRankTests'. Package `maxstat'
depends on it and `exactRankTests' should be imported automatically. Try

R> require(exactRankTests)

to find out what's going wrong.

Best,

Torsten

>
> I do not know the meaning of this problem. Could you please help me on dat?
>
> Thank you in advance for your time.
>
> Sincerely,
>
> Francesc Bosch, M.D.
> Senior specialist
> Department of Hematology, Hospital Clinic
> C/ Villarroel, 170
> 08036 - Barcelona
> Phone & Fax: +34-93-227 5428
> e-mail: fbosch at clinic.ub.es <mailto:fbosch at clinic.ub.es>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From baliola at riseup.net  Fri Jun 13 15:27:15 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Fri, 13 Jun 2003 13:27:15 +0000
Subject: [R] lars - lasso problem
Message-ID: <200306131327.15644.baliola@riseup.net>

hello 

I tried to use lars() but neither with my own data nor with the sample data it  
works. I get in both cases the following error prompt: 

> data(diabetes)
> par(mfrow=c(2,2))
> attach(diabetes)
> x<-lars(x,y)
Error in one %*% x : requires numeric matrix/vector arguments
> x<-lars(x,y, type="lasso")
Error in one %*% x : requires numeric matrix/vector arguments
> x<-lars(x,y, type="lar")
Error in one %*% x : requires numeric matrix/vector arguments

due to the prompt "requires numeric matrix..." I changed the variables to 
matrix/vector or used different columns but it doesn't work either. 

what does "error in one %*%x mean? 

thanks in advance, cheers Martin



From paulda at BATTELLE.ORG  Fri Jun 13 14:02:32 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 13 Jun 2003 08:02:32 -0400
Subject: [R] covariate data errors
Message-ID: <940250A9EB37A24CBE28D858EF077749136E0C@ws-bco-mse3.milky-way.battelle.org>

I have found "Mixed Effects Models in S and Splus" by 
Drs. Pinheiro and Bates to be enormously helpful.
I highly recommend the book - it contains excellent
examples.

Best,
  david paul

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Friday, June 13, 2003 2:54 AM
To: Andy Jacobson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] covariate data errors


Do you mean correlations in the *errors*?  The residuals are always 
correlated.  What does this have to do with your subject line -- it is 
errors in the dependent variable I think you mean?

If you have correlated errors, you should be using generalized least squares
not least squares or weighted least squares.  (That is covered in all good
books on regression: I don't know your level, but Seber's has a
comprehensive account.)  There are several R functions to fit GLS, including
gls(nlme) and lm.gls(MASS).


On Thu, 12 Jun 2003, Andy Jacobson wrote:

> Greetings,
> 
> 	I would like to fit a multiple linear regression model in which the 
> residuals are expected to follow a multivariate normal distribution, 
> using weighted least squares.  I know that the data in question have 
> biases that would result in correlated residuals, and I have a means 
> for quantifying those biases as a covariance matrix. I cannot, 
> unfortunately, correct the data for these biases.
> 
> 	It seems that this should be a straightforward task, but so much of 
> the literature is concerned with the probability model in which the 
> residuals are uncorrelated that I can't find a good reference.  So in 
> order of importance, please, can someone point me to a definitive 
> reference for least squares with correlated residuals, and is there a 
> standard R package to handle this case?
> 
> 	Many thanks in advance,
> 
> 	Anthony
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dave at evocapital.com  Fri Jun 13 14:31:37 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Fri, 13 Jun 2003 13:31:37 +0100
Subject: [R] Layout of windows devices
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D012472@mail.internal.net>

Duncan -- thanks for your reply. As you point out -- this is more a
"nice-to-have" and should not really be prioritised in any way. I
currently run a lengthy statistical process in which intermediate
results are continually output to many graph devices and I was wondering
if there was an existing way to programatically tile the devices rather
than do it manually each time. I would have been happy to use any
existing functionality, but certainly do not think it is important
enough to merit any development work. I think I will rewrite the output
code to use just one (larger) graphics device which is tiled using
layout() or split.screen() instead.

Thanks again
David


-----Original Message-----
From: Duncan Murdoch [mailto:dmurdoch at pair.com] 
Sent: 13 June 2003 12:09
To: David Khabie-Zeitoune
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Layout of windows devices


On Fri, 13 Jun 2003 11:05:30 +0100, you wrote:

>Hi
> 
>Is there a way to specify the location on the screen where a new 
>graphics device opens, for example with a call to win.graph()? I'm 
>using R 1.7.0 on Windows XP.

There isn't currently, but it should be possible to add it.  The window
creation is done in src/gnuwin32/devga.c.  

However, the user interface doesn't look easy.  Windows machines can
have more than one monitor, so you'd want to be able to say which
monitor it would appear on, as well as where.  I think you'd want to be
able to query the monitor size to make the choice useful.  

Is this something that you'd be able to do a first pass through?  I'd be
happy to put it in if someone else did most of the work, but I've got a
lot of other things in the works, and this looks too time-consuming for
a relatively small payback.  After all, you can manually move the window
after it is created.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Jun 13 14:38:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jun 2003 13:38:14 +0100 (BST)
Subject: [R] Layout of windows devices
In-Reply-To: <akbjevc3rok1a8alklmd0orpcr07g75ckp@4ax.com>
Message-ID: <Pine.LNX.4.44.0306131334500.1148-100000@gannet.stats>

There is already support for this in the R-devel version: it like the 
current approach assumes one screen, though.  From the CHANGES file

  The initial size and position of the MDI frame can be set in Rconsole:
  see the comments in .../etc/Rconsole.  The initial position of the
  console window and graphics windows can be set in Rconsole and in the
  GUI Preferences editor.

That version is a long way off release (ca October) but feedback and
patches are welcome.

On Fri, 13 Jun 2003, Duncan Murdoch wrote:

> On Fri, 13 Jun 2003 11:05:30 +0100, you wrote:
> 
> >Hi
> > 
> >Is there a way to specify the location on the screen where a new
> >graphics device opens, for example with a call to win.graph()?
> >I'm using R 1.7.0 on Windows XP.
> 
> There isn't currently, but it should be possible to add it.  The
> window creation is done in src/gnuwin32/devga.c.  
> 
> However, the user interface doesn't look easy.  Windows machines can
> have more than one monitor, so you'd want to be able to say which
> monitor it would appear on, as well as where.  I think you'd want to
> be able to query the monitor size to make the choice useful.  

That is already done, to put the graphics window at the top right.

> Is this something that you'd be able to do a first pass through?  I'd
> be happy to put it in if someone else did most of the work, but I've
> got a lot of other things in the works, and this looks too
> time-consuming for a relatively small payback.  After all, you can
> manually move the window after it is created.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 13 14:41:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jun 2003 13:41:58 +0100 (BST)
Subject: [R] Layout of windows devices
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D012472@mail.internal.net>
Message-ID: <Pine.LNX.4.44.0306131340501.1148-100000@gannet.stats>

Oh, I see now you want to do this differently for each instance of a
device. That would easy to add via arguments to windows(), as the
internal code is already there.

On Fri, 13 Jun 2003, David Khabie-Zeitoune wrote:

> Duncan -- thanks for your reply. As you point out -- this is more a
> "nice-to-have" and should not really be prioritised in any way. I
> currently run a lengthy statistical process in which intermediate
> results are continually output to many graph devices and I was wondering
> if there was an existing way to programatically tile the devices rather
> than do it manually each time. I would have been happy to use any
> existing functionality, but certainly do not think it is important
> enough to merit any development work. I think I will rewrite the output
> code to use just one (larger) graphics device which is tiled using
> layout() or split.screen() instead.
> 
> Thanks again
> David
> 
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:dmurdoch at pair.com] 
> Sent: 13 June 2003 12:09
> To: David Khabie-Zeitoune
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Layout of windows devices
> 
> 
> On Fri, 13 Jun 2003 11:05:30 +0100, you wrote:
> 
> >Hi
> > 
> >Is there a way to specify the location on the screen where a new 
> >graphics device opens, for example with a call to win.graph()? I'm 
> >using R 1.7.0 on Windows XP.
> 
> There isn't currently, but it should be possible to add it.  The window
> creation is done in src/gnuwin32/devga.c.  
> 
> However, the user interface doesn't look easy.  Windows machines can
> have more than one monitor, so you'd want to be able to say which
> monitor it would appear on, as well as where.  I think you'd want to be
> able to query the monitor size to make the choice useful.  
> 
> Is this something that you'd be able to do a first pass through?  I'd be
> happy to put it in if someone else did most of the work, but I've got a
> lot of other things in the works, and this looks too time-consuming for
> a relatively small payback.  After all, you can manually move the window
> after it is created.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfkincaidsu at netscape.net  Fri Jun 13 14:44:54 2003
From: jfkincaidsu at netscape.net (Joel Kincaid)
Date: Fri, 13 Jun 2003 08:44:54 -0400
Subject: [R] R 1.7.0 startup error: .addMethodFrom...
References: <PDEMINECFJGAIJGDGKGOGEDLEBAA.franklin@polisci.wisc.edu>
Message-ID: <3EE9C746.402@netscape.net>

I've had a similar problem that seems to be assoociated with two events 
(using 1.7.0 and win XP home sp1):
1. when using the update.packages() feature from the menu and having the 
system hang. subsequent attempts to start Rgui fail. (However, rterm 
will run -- )
2. or after any time Rgui is hung, there seems to be the *risk* that 
there  will be a problem with some aspect of the methods package. I 
posted a similar note last month but was unable to reproduce the problem 
on a consistent basis....

See the 6/4/03 message and reply from Duncan Murdoch about Win XP and 
crashes for a link to a developmental version  of R that (i'm assuming) 
is a latter version of R patched that seems to have eliminated this 
problem for me. (There was an earlier message that 1.7.1 was due to come 
out mid june --

hth
Joel Kincaid


The solution seems to be to download the pathced version of 1.7.0 from 
cran.

franklin at polisci.wisc.edu wrote:
> Does anyone know what could be causing the following error message on
> startup of R:
> 
> Error in .addMethodFrom(def,argName[1],class[1],fromClass) :
>   object "*tmp*" not found.
> 
> 
> I'm using R 1.7.0 and Windows 2000.
> 
> This happened shortly after I installed 1.7.0. Once it occurs, R freezes.
>>From then on R will always freeze with this error message.
> 
> I reinstalled 1.7.0 and it worked reliably for a couple of weeks or more,
> but now once more has this problem.
> 
> Thanks.
> 
> Charles
> 
> 
> /******************************************
> ** Charles H. Franklin
> ** Professor, Political Science
> ** University of Wisconsin, Madison
> ** 1050 Bascom Mall
> ** Madison, WI 53706
> ** 608-263-2022 Office
> ** 608-265-2663 Fax
> ** mailto:franklin at polisci.wisc.edu (best)
> ** mailto:chfrankl at facstaff.wisc.edu (alt)
> ** http://www.polisci.wisc.edu/~franklin
> ******************************************/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


--



From dmurdoch at pair.com  Fri Jun 13 15:08:35 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 13 Jun 2003 09:08:35 -0400
Subject: [R] Layout of windows devices
In-Reply-To: <Pine.LNX.4.44.0306131334500.1148-100000@gannet.stats>
References: <akbjevc3rok1a8alklmd0orpcr07g75ckp@4ax.com>
	<Pine.LNX.4.44.0306131334500.1148-100000@gannet.stats>
Message-ID: <5uijevkg9958pq8dpmppips3jkft46cr34@4ax.com>

On Fri, 13 Jun 2003 13:38:14 +0100 (BST), Prof Brian Ripley wrote:

>There is already support for this in the R-devel version: it like the 
>current approach assumes one screen, though.  From the CHANGES file

I just checked out the Windows docs, and it looks like the normal
setup for multiple monitors is to treat them like one big virtual
monitor.  They can also be set up completely independently, but then
most Windows functions aren't available, so I don't think we could
support that.

Pixel (0,0) is the top left of the "primary monitor"; other monitors
can be addressed at locations (positive or negative) beyond the limits
of the primary.

Duncan



From erhansen at math.ku.dk  Fri Jun 13 15:12:02 2003
From: erhansen at math.ku.dk (Ernst Hansen)
Date: Fri, 13 Jun 2003 15:12:02 +0200
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <16104.29831.927591.63021@pc87.math.ku.dk>
References: <16104.29831.927591.63021@pc87.math.ku.dk>
Message-ID: <16105.52642.225622.249933@pc87.math.ku.dk>

My request for a way of having both data and R-code in the same
textfile, resultet in a considerable number of very good suggestions,
that I will now summarize.

The boundary conditions for the problem were as follows: the data
should be written in the textfile in a format that was readable to the
human eye. And this ruled out the 'transposed' way of writing the
data, that is used in most help-files, eg. in ?model.matrix.

As the purpose of the exercise is to make the textfile easy to read,
there is a limit to how complicated the extra code should be -
otherwise it would make matters worse.   I don't know if any of the
solutions below qualify in this sense - but I surely learned a lot
from them.






The most popular idea was using textConnection() in a combination with
read.table().  For instance Thomas Hotz wrote it like


# Solution by Thomas Hotz

   MyFrame <- read.table(textConnection(c(

    'Sex    Respons',
    'Male   1',
    'Male   2',
    'Female 3',
    'Female 4'

   )), header = T)


Gabor Grothendieck had a similar solution. James Holtman provided a
nifty trick to get rid of the strategically placed commas and
quotations, using escaped carriagereturns,

# Solution by James Holtman

   MyFrame <- read.table(textConnection('\
    Sex    Respons \
    Male   1 \
    Male   2 \
    Female 3 \
    Female 4 \
   '), header = T, skip = 1)
 

Duncan Temple Lang suggested that the entire textfile should be
wrapped up as XML, and parsed via the XML package.  In the context of
me and my students, I think that this would be overkill, and I also
think it necessarily breaks the one-file boundary condition, but in a
larger context it seems like an excellent advise.

# Solution by Duncan Tempel Lang

# Content of myFile.q
   <doc>
   <data>
    Sex    Response
    Male   1
    Male   2
    Female 3
    Female 4
   </data>

   <code>
    ......
   </code>
   </doc>

To read the data,

 tr = xmlRoot(xmlTreeParse("myFile.q"))
 read.table(textConnection(xmlValue(tr[["data"]])), header=TRUE)

and to access the code text

 xmlValue(tr[["code"]])




A number of approaches not based on textConnection() emerged, though.

Torsten Hothorn suggested that the data should be surrounded by some
kind of  print-statement, writing it to a temporary file.  Then
read.table() could be used to retrieve the data:

# Torsten Hothorns solution:

  tmpfilename <- tempfile()
  tmpfile <- file(tmpfilename, 'w')
  cat(
      'Sex    Respons',
      'Male   1',
      'Male   2',
      'Female 3',
      'Female 4',
      file = tmpfile, sep='\n')
  close(tmpfile)
  read.table(tmpfilename, header = TRUE)



Barry Rowlingson suggested that the data should be written as a vector
of characters, and then shaped by hand:

# Barry Rowlingsons solution

   data <- c(

             'Sex', 'Respons',
             'Male',   1,
             'Female', 2,
             'Male',   3,
             'Male',   2,

             )

   ncol <- 2
   nrow <- length(data)/ncol

   heads <- data[1:ncol];data <- data[-(1:ncol)]
   asDF <- data.frame(matrix(data,ncol=ncol,byrow=T))

   asDF[,2] <- as.numeric(asDF[,2])
   names(asDF) <- heads


Finally, Thomas Blackwell and Greg Louis implemented a nice idea,
where the data are commented out in the textfile, but where a call to
read.table() from within the file, makes it read exactly those lines,
using a different convention for comments:

# Greg Louis' solution

   MyFrame <- read.table('myFile.q', header = T, 
                      skip = 28, nrows = 4, comment.char="")[-1]
   # Sex    Respons 
   # Male   1 
   # Male   2 
   # Female 3 
   # Female 4 

Exactly how lines that will need to be skipped depends on the
circumstances. nrows is the number of cases in the dataframe.     
 


The original request follows below.

Thank you all for participating.


Ernst Hansen
Department of Statistics
University of Copenhagen




Ernst Hansen writes:
 > I have the following problem.  It is not of earthshaking importance,
 > but still I have spent a considerable amount of time thinking about
 > it. 
 > 
 > PROBLEM: Is there any way I can have a single textfile that contains
 > both
 > 
 >   a) data
 > 
 >   b) programcode
 > 
 > The program should act on the data, if the textfile is source()'ed
 > into R.
 > 
 > 
 > BOUNDARY CONDITION: I want the data written in the textfile in exactly
 > the same format as I would use, if I had data in a separate textfile,
 > to be read by read.table().  That is, with 'horizontal inhomogeneity'
 > and 'vertical homogeneity' in the type of entries.  I want to write
 > something like 
 > 
 >       Sex    Respons
 >       Male   1
 >       Male   2
 >       Female 3
 >       Female 4
 > 
 > In effect, I am asking if there is some way I can convince
 > read.table(), that the data is contained in the following n lines of
 > text. 
 > 
 > 
 > ILLEGAL SOLUTIONS:
 > I know I can simulate the behaviour by reading the columns of the
 > dataframe one by one, and using data.frame() to glue them together.
 > Like in 
 > 
 >     data.frame(Sex = c('Male', 'Male', 'Female', 'Female'),
 >                Respons = c(1, 2, 3, 4))
 > 
 > I do not like this solution, because it represents the data in a
 > "transposed" way in the textfile, and this transposition makes the
 > structure of the dataframe less transparent - at least to me. It
 > becomes even less comprehensible if the Sex-factor above is written
 > with the help of rep() or gl() or the like.
 > 
 > I know I can make read.table() read from stdin, so I could type the
 > dataframe at the prompt.  That is against the spirit of the problem,
 > as I describe below.
 > 
 > 
 > I know I can make read.table() do the job, if I split the data and the
 > programcode in to different files.  But as the purpose of the exercise
 > is to distribute the data and the code to other people, splitting
 > into several files is a complication.
 > 
 > 
 > MOTIVATION: I frequently find myself distributing small chunks of code
 > to my students, along with data on which the code can work.
 > 
 > As an example, I might want to demonstrate how model.matrix() treats
 > interactions, in a certain setting.  For that I need a dataframe that
 > is complex enough to exhibit the behaviour I want, but still so small
 > that the model.matrix is easily understood.  So I make such a
 > dataframe.
 > 
 > I am trying to distribute this dataframe along with my code, in a way
 > that is as simple as possible to USE for the students (hence the
 > one-file boundary condition) and to READ (hence the non-transposition
 > boundary condition).
 > 
 > 
 > 
 > Does anybody have any ideas?
 > 
 > 
 > Ernst Hansen
 > Department of Statistics
 > University of Copenhagen
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >



From glaziou at pasteur-kh.org  Fri Jun 13 15:13:02 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Fri, 13 Jun 2003 20:13:02 +0700
Subject: [R] problem with latex of object summary reverse
Message-ID: <20030613131302.GC782@pasteur-kh.org>

Hi,

I have the following problem (library Hmisc loaded, 
iris data loaded, R Version 1.7.0  (2003-04-16), packages 
updated, running on a linux Debian i386):

> summary(Species~Sepal.Length,method="reverse")->a
> a


Descriptive Statistics by Species

+------------+-----------------+-----------------+-----------------+
|            |setosa           |versicolor       |virginica
|
|            |(N=50)           |(N=50)           |(N=50)
|
+------------+-----------------+-----------------+-----------------+
|Sepal.Length|4.800/5.000/5.200|5.600/5.900/6.300|6.225/6.500/6.900|
+------------+-----------------+-----------------+-----------------+


> latex(a)->la


works ok, but the a.tex generated file is wrong: a '&' is
missing on the line ending with '%%%% wrong', resulting in the
(N=50) of the second row being put on the first column, like:

+------------+-----------------+-----------------+-----------------+
|            |setosa           |versicolor       |virginica
|
|(N=50)      |(N=50)           |(N=50)           |
|
+------------+-----------------+-----------------+-----------------+
|Sepal.Length|4.800/5.000/5.200|5.600/5.900/6.300|6.225/6.500/6.900|
+------------+-----------------+-----------------+-----------------+



> system("cat a.tex")


% latex.default(cstats, title = title, caption = caption,
rowlabel = rowlabel,      col.just = col.just,
numeric.dollar = FALSE, insert.bottom = legend,      rowname
= lab, dcolumn = dcolumn, extracolheads = extracolheads,
extracolsize = Nsize, ...) 
%
\begin{table}[!tbp]
 \begin{center}
 \caption{Descriptive Statistics by Species\label{a}} 
 \begin{tabular}{lccc}\hline\hline
\multicolumn{1}{l}{}&
\multicolumn{1}{c}{setosa}&
\multicolumn{1}{c}{versicolor}&
\multicolumn{1}{c}{virginica}
\\   \multicolumn{1}{l}{{\scriptsize	         %%%% wrong
$N=50$}}&\multicolumn{1}{c}{{\scriptsize
$N=50$}}&\multicolumn{1}{c}{{\scriptsize $N=50$}}\\ \hline
Sepal.Length&{\scriptsize 4.800~}{5.000 }{\scriptsize 5.200}
&{\scriptsize 5.600~}{5.900 }{\scriptsize 6.300}
&{\scriptsize 6.225~}{6.500 }{\scriptsize 6.900} \\

[...]



Any idea about what I might be doing wrong here? I can
reproduce that problem with summary(method="reverse") on
other datasets, and various combinations of options passed
to the latex command. 

Thanks

-- 
Philippe



From kurt.sys at UGent.be  Fri Jun 13 15:56:47 2003
From: kurt.sys at UGent.be (Kurt Sys)
Date: Fri, 13 Jun 2003 15:56:47 +0200
Subject: [R] probe design
Message-ID: <16105.55327.355588.986172@ksys.rug.ac.be>

Hello,

this is a quite specific topic, but I just wonder if there are some R
packages present for 'probe design'.

tnx,
Kurt.


-- 
All art is but imitation of nature.
		-- Lucius Annaeus Seneca



From baliola at riseup.net  Fri Jun 13 18:16:08 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Fri, 13 Jun 2003 16:16:08 +0000
Subject: [R] lars - lasso problem
In-Reply-To: <16105.51999.315298.748345@localhost.localdomain>
References: <200306131327.15644.baliola@riseup.net>
	<16105.51999.315298.748345@localhost.localdomain>
Message-ID: <200306131616.08020.baliola@riseup.net>

hello Berwin, 

thanks for your help, I haven't considered this x-naming problem (pretty 
stupid mistake :-) ) - now it is working.  

cheers to Oz, Martin

On Friday 13 June 2003 13:01, Berwin Turlach wrote:
> G'day Martin,
>
> >>>>> "MW" == Martin Wegmann <baliola at riseup.net> writes:
>
>     MW> I tried to use lars() but neither with my own data nor with
>     MW> the sample data it works. I get in both cases the following
>
>     MW> error prompt:
>     >> data(diabetes)
>     >> par(mfrow=c(2,2))
>     >> attach(diabetes)
>     >> x<-lars(x,y)
>
> Works for me.  But this is not a good style because it creates an
> object with the name "x" in you local working directory.  In
> subsequent calls that "x" will be found before the "x" that is in the
> frame where the diabetes data is attached (I am probably using the
> wrong terminology here).
>
> Better say something like
>        fm <- lars(x,y)
>
>     MW> due to the prompt "requires numeric matrix..." I changed the
> variables to MW> matrix/vector or used different columns but it doesn't
> work either. Probably you created variables called "x" in your working area
> that are found first.  And that variable are not numeric.  Did you try
> "rm(x)" or, more extreme, "rm(list=ls())" before running the lars
> command?
>
>     MW> what does "error in one %*%x mean?
> %*% is the R operator for matrix multiplication.  It requires a
> numeric arguments, either two matrices, or a matrix and a vector,
> or....
> Obviously, the writer of the lars package define a vector/matrix with
> the name one which is multiplied at some point in the code with the
> vector/matrix x.  You get the error because one of the two is not a
> numeric vector/matrix, presumably x.
>
> Hope that helps.
>
> Cheers,
>
>         Berwin
>
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 9380 3338 (secr)
> School of Mathematics and Statistics        +61 (8) 9380 3383 (self)
> The University of Western Australia   FAX : +61 (8) 9380 1028
> 35 Stirling Highway
> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
> Australia                        http://www.maths.uwa.edu.au/~berwin



From michael.watson at bbsrc.ac.uk  Fri Jun 13 16:22:05 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 13 Jun 2003 15:22:05 +0100
Subject: [R] Using jpeg() function over cgi
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00706@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

I have seen a few posts to this list regarding problems accessing the x11() device over cgi - namely, when trying to create a graphic using the jpeg() function, everything is fine from the command line but it won't work over cgi, producing the error:

"Unable to open connection to X11 display"

Has anyone actually solved this particular problem satisfactorily?

Please reply direct to me as I am not a member of the list (yet!)

Thanks in advance for your help

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7764 490236
E-mail: michael.watson at bbsrc.ac.uk



From dave at evocapital.com  Fri Jun 13 16:42:00 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Fri, 13 Jun 2003 15:42:00 +0100
Subject: [R] RDCOM Client: processes not terminating
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D01B816@mail.internal.net>

Hello

I am using Duncan Lang's RDCOM Client package (available on
omegahat.org) under R 1.7.0 and Windows XP Pro. 

Is this the right forum for questions about this package? In case it is,
here is my question: 

Instances of COM objects do not seem to terminate as expected, but leave
residual processes running. For example, if I try the simple example:

E <- COMCreate("Excel.Application")
E[["Visible"]] <- TRUE
E$Quit()

An Excel application is created and pops up visibly. The E$Quit()
command appears to close the application down as expected, but an EXCEL
process is still left running in the background (as indicated by e.g.
the Windows Task Manager).

Is there a way to cleanly exit the COM instance and shut down the
associated process? 

Thanks,

David



From brahm at alum.mit.edu  Fri Jun 13 16:56:29 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Fri, 13 Jun 2003 10:56:29 -0400
Subject: [R] breaks
References: <16105.28670.120463.635762@gargle.gargle.HOWL>
Message-ID: <16105.58909.194223.782800@arbres1a.fmr.com>

Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> findInterval()

Hi, Martin.  I wasn't aware of findInterval().  findInterval(x, vec) looks to
me very similar to:
  R> cut(x, c(-Inf,vec,Inf), labels=FALSE, right=FALSE) - 1
so I'm curious what the differences are (e.g. speed, duplicates in vec?).  In
any case, findInterval() and cut() ought to be in each other's "See Also",
don't you think?

R> xx <- c(-2.0, 1.4, -1.2, -2.2, 0.4, 1.5, -2.2, 0.2, -0.4, -0.9)
R> xx.y <- c(-2.2000000, -0.9666667, 0.2666667, 1.5000000)
R> findInterval(xx, xx.y)
   [1] 1 3 1 1 3 4 1 2 2 2
R> cut(xx, c(-Inf,xx.y,Inf), labels=FALSE, right=FALSE) - 1
   [1] 1 3 1 1 3 4 1 2 2 2
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From tlumley at u.washington.edu  Fri Jun 13 16:57:29 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 13 Jun 2003 07:57:29 -0700 (PDT)
Subject: [R] lars - lasso problem
In-Reply-To: <200306131616.08020.baliola@riseup.net>
Message-ID: <Pine.A41.4.44.0306130753520.14912-100000@homer34.u.washington.edu>

On Fri, 13 Jun 2003, Martin Wegmann wrote:

> hello Berwin,
>
> thanks for your help, I haven't considered this x-naming problem (pretty
> stupid mistake :-) ) - now it is working.
>

Because I can't keep track of this sort of thing I like to use with()
rather than attach() when there isn't a data= argument.

  x<-with(diabetes, lars(x,y))

doesn't cause the same problem.

	-thomas



From ripley at stats.ox.ac.uk  Fri Jun 13 17:01:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jun 2003 16:01:40 +0100 (BST)
Subject: [R] Using jpeg() function over cgi
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00706@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0306131556030.947-100000@gannet.stats>

Yes, it has been solved and discussed in the R-help archives many times.
The help(jpeg) page is pretty explicit too.  One alternative is bitmap().

To use jpeg() under a Unix-alike you need to set up an X server that your
R process can use.  It's a bit hard to help you do that when you don't
even mention your OS (I am inferring it is a Unix-alike), but you may be
able to solve the permissions problem or you may be able to set by a
server by something like Xvfb.  In any case, it is not an R problem


On Fri, 13 Jun 2003, michael watson (IAH-C) wrote:

> I have seen a few posts to this list regarding problems accessing the
> x11() device over cgi - namely, when trying to create a graphic using

No, not the x11() device, but the jpeg() device.  They are not the same!

> the jpeg() function, everything is fine from the command line but it
> won't work over cgi, producing the error:
> 
> "Unable to open connection to X11 display"
> 
> Has anyone actually solved this particular problem satisfactorily?
> 
> Please reply direct to me as I am not a member of the list (yet!)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Fri Jun 13 17:06:07 2003
From: angel_lul at hotmail.com (Angel -)
Date: Fri, 13 Jun 2003 15:06:07 +0000
Subject: [R] maps library for R? 
Message-ID: <Law11-F63QeSOzNCWbz0000a9b5@hotmail.com>

Hi there,
Does it already exist a library in R to draw maps (something like a Generic 
Mapping Toolbox, http://gmt.soest.hawaii.edu/ port).

I've seen in an old R-help that Ross Ihaka once tried to port the S-plus map 
library (http://maths.newcastle.edu.au/~rking/R/help/99b/0832.html ).
Anybody know if this package is available somewhere or if there is somebody 
developing a Mapping package for R. I'll might then try to help.
Cheers,
Angel



From laurent at cbs.dtu.dk  Fri Jun 13 17:29:53 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Fri, 13 Jun 2003 17:29:53 +0200
Subject: [R] probe design
In-Reply-To: <16105.55327.355588.986172@ksys.rug.ac.be>
References: <16105.55327.355588.986172@ksys.rug.ac.be>
Message-ID: <20030613152953.GA7873720@genome.cbs.dtu.dk>

On Fri, Jun 13, 2003 at 03:56:47PM +0200, Kurt Sys wrote:
> Hello,
> 
> this is a quite specific topic, but I just wonder if there are some R
> packages present for 'probe design'.
> 
> tnx,
> Kurt.
> 
> 
> -- 
> All art is but imitation of nature.
> 		-- Lucius Annaeus Seneca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

What do you mean by "probe design" ? Are you referring to microarrays ?
-- 
--------------------------------------------------------------
currently at the National Yang-Ming University in Taipei, Taiwan
--------------------------------------------------------------
Laurent Gautier			CBS, Building 208, DTU
PhD. Student			DK-2800 Lyngby,Denmark	
tel: +45 45 25 24 89		http://www.cbs.dtu.dk/laurent



From fharrell at virginia.edu  Fri Jun 13 18:52:25 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 13 Jun 2003 12:52:25 -0400
Subject: [R] problem with latex of object summary reverse
In-Reply-To: <20030613131302.GC782@pasteur-kh.org>
References: <20030613131302.GC782@pasteur-kh.org>
Message-ID: <20030613125225.655c19a2.fharrell@virginia.edu>

On Fri, 13 Jun 2003 20:13:02 +0700
Philippe Glaziou <glaziou at pasteur-kh.org> wrote:

> Hi,
> 
> I have the following problem (library Hmisc loaded, 
> iris data loaded, R Version 1.7.0  (2003-04-16), packages 
> updated, running on a linux Debian i386):
> 
> > summary(Species~Sepal.Length,method="reverse")->a
> > a
> 
> 
> Descriptive Statistics by Species
> 
> +------------+-----------------+-----------------+-----------------+
> |            |setosa           |versicolor       |virginica
> |
> |            |(N=50)           |(N=50)           |(N=50)
> |
> +------------+-----------------+-----------------+-----------------+
> |Sepal.Length|4.800/5.000/5.200|5.600/5.900/6.300|6.225/6.500/6.900|
> +------------+-----------------+-----------------+-----------------+
> 
> 
> > latex(a)->la
> 
> 
> works ok, but the a.tex generated file is wrong: a '&' is
> missing on the line ending with '%%%% wrong', resulting in the
> (N=50) of the second row being put on the first column, like:
> 
> +------------+-----------------+-----------------+-----------------+
> |            |setosa           |versicolor       |virginica
> |
> |(N=50)      |(N=50)           |(N=50)           |
> |
> +------------+-----------------+-----------------+-----------------+
> |Sepal.Length|4.800/5.000/5.200|5.600/5.900/6.300|6.225/6.500/6.900|
> +------------+-----------------+-----------------+-----------------+
> 
> 
> 
> > system("cat a.tex")
> 
> 
> % latex.default(cstats, title = title, caption = caption,
> rowlabel = rowlabel,      col.just = col.just,
> numeric.dollar = FALSE, insert.bottom = legend,      rowname
> = lab, dcolumn = dcolumn, extracolheads = extracolheads,
> extracolsize = Nsize, ...) 
> %
> \begin{table}[!tbp]
>  \begin{center}
>  \caption{Descriptive Statistics by Species\label{a}} 
>  \begin{tabular}{lccc}\hline\hline
> \multicolumn{1}{l}{}&
> \multicolumn{1}{c}{setosa}&
> \multicolumn{1}{c}{versicolor}&
> \multicolumn{1}{c}{virginica}
> \\   \multicolumn{1}{l}{{\scriptsize	         %%%% wrong
> $N=50$}}&\multicolumn{1}{c}{{\scriptsize
> $N=50$}}&\multicolumn{1}{c}{{\scriptsize $N=50$}}\\ \hline
> Sepal.Length&{\scriptsize 4.800~}{5.000 }{\scriptsize 5.200}
> &{\scriptsize 5.600~}{5.900 }{\scriptsize 6.300}
> &{\scriptsize 6.225~}{6.500 }{\scriptsize 6.900} \\
> 
> [...]
> 
> 
> 
> Any idea about what I might be doing wrong here? I can
> reproduce that problem with summary(method="reverse") on
> other datasets, and various combinations of options passed
> to the latex command. 
> 
> Thanks
> 
> -- 
> Philippe
> 

I tried this on the latest version of Hmisc (1.6-0):

library(Hmisc)
set.seed(1)
y <- factor(sample(c('a','b','c'),100,T))
x <- runif(100)
a <- summary(y ~ x, method='reverse')
options(digits=3)
latex(a)

and everything was fine.   The following also worked:

data(iris)
a <- summary(Species~Sepal.Length, method='reverse',data=iris)
latex(a)

See if a bug fix in Hmisc has fixed your problem since the last time you updated the package. 

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From maechler at stat.math.ethz.ch  Fri Jun 13 19:35:14 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jun 2003 19:35:14 +0200
Subject: [R] breaks
In-Reply-To: <16105.58909.194223.782800@arbres1a.fmr.com>
References: <16105.28670.120463.635762@gargle.gargle.HOWL>
	<16105.58909.194223.782800@arbres1a.fmr.com>
Message-ID: <16106.2898.177388.590696@gargle.gargle.HOWL>

>>>>> "DavidB" == David Brahm <brahm at alum.mit.edu>
>>>>>     on Fri, 13 Jun 2003 10:56:29 -0400 writes:

    DavidB> Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> findInterval()

    DavidB> Hi, Martin.  I wasn't aware of findInterval().  findInterval(x, vec) looks to
    DavidB> me very similar to:
    R> cut(x, c(-Inf,vec,Inf), labels=FALSE, right=FALSE) - 1

    DavidB> so I'm curious what the differences are (e.g. speed,
    DavidB> duplicates in vec?).  In any case, findInterval()
    DavidB> and cut() ought to be in each other's "See Also",
    DavidB> don't you think?

When I wrote the precursor of findInterval() about 10 years ago (to be
dyn.load()ed into S-plus), I hadn't yet realized about the
several alternatives.  

However, when I added it to R, I knew about the N*ecdf()
alternative, i.e., ecdf() from package:stepfun which relies on
approx(....., method = "constant").
I found that findInterval() was slightly faster than approx()
even for unsorted `x' (by about a factor of 2 for large `vec') in my
test cases, but the real speed of findInterval() comes to play
when `x' is sorted -- something which is very typical e.g. for
evaluation of piecewise functions (splines etc).

    R> xx <- c(-2.0, 1.4, -1.2, -2.2, 0.4, 1.5, -2.2, 0.2, -0.4, -0.9)
    R> xx.y <- c(-2.2000000, -0.9666667, 0.2666667, 1.5000000)
    R> findInterval(xx, xx.y)
    DavidB> [1] 1 3 1 1 3 4 1 2 2 2
    R> cut(xx, c(-Inf,xx.y,Inf), labels=FALSE, right=FALSE) - 1
    DavidB> [1] 1 3 1 1 3 4 1 2 2 2

cut() is still slower than the ecdf() / approx() version
considerably for long `vec'  ...
I really should write a small article about this for "R News",
where I'd also show the simulation results...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From lapotcarex at mtu-net.ru  Fri Jun 13 20:01:42 2003
From: lapotcarex at mtu-net.ru (Eugenij P. Altshuler)
Date: Fri, 13 Jun 2003 22:01:42 +0400
Subject: [R] Using PCA
Message-ID: <001401c331d5$e3d83ba0$e19d763e@m6h4m4>

Dear R-help! I ask you to help me with my problems with using R.
First, I ask you to forgive my bad English!
I try to use R in my study.
Subject of my work is comparative study of flora of lakes in different
regions of Russia. I have done floristical descriptions of 152 lakes (I
think it's enough) and have tabulated it. As data I have a table, such has
152 rows (lakes) 290 variables (species of plants). Thus, there is frequency
of species of plants in every cell of this table. I wanted to search some
groups between these lakes. I have done this searching with cluster analysis
(cutree(hclust(dist(DATA,"manh"),"ward"),4)). Then I apply principal
component analysis:
==============
##loading data (DATA)
##loading the list of groups (GROUPS)
d.prc<-princomp(DATA)
palette(rainbow(length(unique(GROUPS))))
plot(d.prc$scores,type="n",main="Principal Component
Analysis",xlab="Different groups of lakes have different color-labeling")
text(d.prc$scores,labels=GROUPS,col=GROUPS,cex=.6)
==============
I have received attached plot as a result. Both I and my supervisor of
studies are agree with clustering. You can see 4 groups in this figure.
Write to me please your opinion, if these groups are authentic. Could you
present any criteria of existence of distinguishable groups and any criteria
of allocation of these groups?
Can I use PCA as instrument to test visually clustering?
I heard some opinion: "PCA is for visual allocating of the groups and then
you can search hypothetical parameter to explain groupping. But you cannot
use PCA for looking for the loadings (influence) of variables on the
groupping". Is it true?
Could you advise to me some other methods to analyze.

-----------------------------------

Best wishes,
Altshuler Eugenij P.
mailto:lapotcarex at mtu-net.ru

From Roger.Bivand at nhh.no  Fri Jun 13 20:23:19 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 13 Jun 2003 20:23:19 +0200 (CEST)
Subject: [R] maps library for R? 
In-Reply-To: <Law11-F63QeSOzNCWbz0000a9b5@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306132012570.21252-100000@reclus.nhh.no>

On Fri, 13 Jun 2003, Angel - wrote:

> Does it already exist a library in R to draw maps (something like a Generic 
> Mapping Toolbox, http://gmt.soest.hawaii.edu/ port).
> 
> I've seen in an old R-help that Ross Ihaka once tried to port the S-plus map 
> library (http://maths.newcastle.edu.au/~rking/R/help/99b/0832.html ).
> Anybody know if this package is available somewhere or if there is somebody 
> developing a Mapping package for R. I'll might then try to help.
> Cheers,
> Angel
> 

As of now, there is a source package published on CRAN (RArcInfo) which 
reads Arc binary data, and has a map function. Off CRAN, there are a 
number of draft packages, some of which are linked to the web-page of a 
pre-conference workshop here:

http://spatial.nhh.no/meetings/vienna/index.html

This work is continuing on several levels:

1) data import/export

2) mapping

3) projection, line thinning/generalisation, topology

4) interfacing mapping functions with spatial analysis functions

Contributions and ideas welcome! 

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From cooper.bethea at duke.edu  Fri Jun 13 20:29:06 2003
From: cooper.bethea at duke.edu (Cooper Bethea)
Date: Fri, 13 Jun 2003 18:29:06 -0000
Subject: [R] building RPMs for R packages
Message-ID: <1055528882.3137.6.camel@localhost.localdomain>

hi all-

apologies if this question has come up before; i took a swing through a
few months of archives and didn't turn anything up.

i'm a linux systems administrator, and we're running R across a beowulf
cluster. i've been asked to install the bioconductor package, for which
i can't find RPMs. the recommended way to do this on the bioconductor
home page is via R CMD INSTALL, which seems to be the standard way to
install R packages.

however, i want to make an RPM out of bioconductor, since i have to
deploy it to 48 machines and i want rpm to track the versioning and
dependencies. also, the machines are very homogenous and there's no
reason compilation should occur on each.

so could someone who's built R extension RPM packages give me some
pointers? a SRPM for a package would be invaluable.

thanks for your time.



From den.duurs at lycos.com  Fri Jun 13 23:54:40 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Fri, 13 Jun 2003 14:54:40 -0700
Subject: [R] Problem with Rcmd SHLIB 
Message-ID: <NANEFFKEEBONNDAA@mailcity.com>

Dear R-helpers,

i am trying to make a shared library from a Fortran subroutine, and i therefore used (after reading the documentation):

Rcmd SHLIB forfile.f          #(R1.70, Win2000)

And the error is:
 " 'perl' is not recognized as an internal or external command,operable program or batch file." 

So i went ahead and tried to install Perl (from the suggested website in "readme.packages") but the installer told me i already had Perl installed!
By the way,

Rcmd COMPILE somefile.f  

gives the same error message.

What am i missing?

Thanks,

Remko


^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.



From alumaflor at fastermail.com  Sat Jun 14 00:40:04 2003
From: alumaflor at fastermail.com (Aluma Floor)
Date: Fri, 13 Jun 2003 17:40:04 -0500
Subject: [R] Aluma Floor - New High Tech "Floor of THE FUTURE!"
Message-ID: <E19QxDF-0005jL-00@bernie.ethz.ch>

Aluminum Flooring New To The Market
http://www.aluminumfloors.com  

From kwan022 at stat.auckland.ac.nz  Sat Jun 14 00:40:53 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Sat, 14 Jun 2003 10:40:53 +1200 (NZST)
Subject: [R] Problem with Rcmd SHLIB 
In-Reply-To: <NANEFFKEEBONNDAA@mailcity.com>
Message-ID: <Pine.LNX.4.44.0306141039440.27198-100000@stat61.stat.auckland.ac.nz>

Hi,

Is perl in your Path?  i.e. what happens when you type:
  perl --version

On Fri, 13 Jun 2003, Remko Duursma wrote:

> Date: Fri, 13 Jun 2003 14:54:40 -0700
> From: Remko Duursma <den.duurs at lycos.com>
> To: rhelp <r-help at r-project.org>
> Subject: [R] Problem with Rcmd SHLIB 
> 
> Dear R-helpers,
> 
> i am trying to make a shared library from a Fortran subroutine, and i therefore used (after reading the documentation):
> 
> Rcmd SHLIB forfile.f          #(R1.70, Win2000)
> 
> And the error is:
>  " 'perl' is not recognized as an internal or external command,operable program or batch file." 
> 
> So i went ahead and tried to install Perl (from the suggested website in "readme.packages") but the installer told me i already had Perl installed!
> By the way,
> 
> Rcmd COMPILE somefile.f  
> 
> gives the same error message.
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From simon_cawley at affymetrix.com  Sat Jun 14 02:02:07 2003
From: simon_cawley at affymetrix.com (Simon Cawley)
Date: Fri, 13 Jun 2003 17:02:07 -0700 (PDT)
Subject: [R] problem installing packages from source on win2k
Message-ID: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>


Dear R-helpers,

I'm having trouble compiling R packages from source on Win2K.  I installed R 1.7.1beta [1] on my win2k machine [2], downloaded the fields package as source [3] and tried but failed to install the package [4].  I get the same problem with other packages, so it doesn't appear to be limited to fields.  Installation of precompiled packages seems to work fine.  I've been doing this on linux for a long time and never ran into such an issue.

I've trawled the FAQ and the archives but I'm not finding anything.  Any clues where I'm going wrong?

Thanks in advance,

-Simon

##############

Notes:

[1] Version of R: 1.7.1 Beta, obtained from <http://www.stats.uwo.ca/faculty/murdoch/software/r-devel/rw1071beta.exe> on 6-13-2003

[2] Output from version:
  platform i386-pc-mingw32
  arch     i386           
  os       mingw32        
  system   i386, mingw32  
  status   Beta           
  major    1              
  minor    7.1            
  year     2003           
  month    06             
  day      06             
  language R  

[3] fields 1.3.1 package downloaded from <http://cran.stat.ucla.edu/src/contrib/fields_1.3-1.tar.gz>.

[4] within cygwin bash shell, I ran the following:
  Rcmd install fields_1.3-1.tar.gz
which dies with the error
.
hhc: not found
cp: cannot stat `c:/tmp/R.INSTALL/fields/chm/fields.chm': No such file or directory
make[1]: *** [chm-fields] Error 1
make: *** [pkg-fields] Error 2
*** Installation of fields failed ***

See below for full log of stdout and stderr from the installation attempt.

#########################################

STDERR:

cvrcss.f: In subroutine `cvrcss':
cvrcss.f:61: warning: `cvmin' might be used uninitialized in this function
cvrcss.f:63: warning: `best' might be used uninitialized in this function
cvrcss.f:65: warning: `trbest' might be used uninitialized in this function
gcvcss.f: In subroutine `gcvcss':
gcvcss.f:59: warning: `gcvmin' might be used uninitialized in this function
gcvcss.f:61: warning: `best' might be used uninitialized in this function
gcvcss.f:63: warning: `trbest' might be used uninitialized in this function
gcvfc.f: In function `gcvfc':
gcvfc.f:18: warning: unused variable `rinf'
rkmat.f: In subroutine `rkbesl':
rkmat.f:212: warning: `itemp' might be used uninitialized in this function
hhc: not found
cp: cannot stat `c:/tmp/R.INSTALL/fields/chm/fields.chm': No such file or directory
make[1]: *** [chm-fields] Error 1
make: *** [pkg-fields] Error 2

#########################################

STDOUT:

---------- Making package fields ------------
  adding build stamp to DESCRIPTION
  making DLL ...
g77 -O2 -Wall  -c css.f -o css.o
g77 -O2 -Wall  -c csstr.f -o csstr.o
g77 -O2 -Wall  -c cvrcss.f -o cvrcss.o
g77 -O2 -Wall  -c cvrf.f -o cvrf.o
g77 -O2 -Wall  -c dchold.f -o dchold.o
g77 -O2 -Wall  -c dcopy.f -o dcopy.o
g77 -O2 -Wall  -c ddot.f -o ddot.o
g77 -O2 -Wall  -c dlv.f -o dlv.o
g77 -O2 -Wall  -c dmaket.f -o dmaket.o
g77 -O2 -Wall  -c drdfun.f -o drdfun.o
g77 -O2 -Wall  -c dsetup.f -o dsetup.o
g77 -O2 -Wall  -c expbs.f -o expbs.o
g77 -O2 -Wall  -c expfn.f -o expfn.o
g77 -O2 -Wall  -c gaspbs.f -o gaspbs.o
g77 -O2 -Wall  -c gaspfn.f -o gaspfn.o
g77 -O2 -Wall  -c gcvcss.f -o gcvcss.o
g77 -O2 -Wall  -c gcvfc.f -o gcvfc.o
g77 -O2 -Wall  -c hsort.f -o hsort.o
g77 -O2 -Wall  -c ifind.f -o ifind.o
g77 -O2 -Wall  -c inpoly.f -o inpoly.o
g77 -O2 -Wall  -c lscv.f -o lscv.o
g77 -O2 -Wall  -c m2deb.f -o m2deb.o
g77 -O2 -Wall  -c mkpoly.f -o mkpoly.o
g77 -O2 -Wall  -c mltdrb.f -o mltdrb.o
g77 -O2 -Wall  -c mltdtd.f -o mltdtd.o
g77 -O2 -Wall  -c msort.f -o msort.o
g77 -O2 -Wall  -c multeb.f -o multeb.o
g77 -O2 -Wall  -c multgb.f -o multgb.o
g77 -O2 -Wall  -c multrb.f -o multrb.o
g77 -O2 -Wall  -c nkden.f -o nkden.o
g77 -O2 -Wall  -c nkreg.f -o nkreg.o
g77 -O2 -Wall  -c nvden.f -o nvden.o
g77 -O2 -Wall  -c radbas.f -o radbas.o
g77 -O2 -Wall  -c radfun.f -o radfun.o
g77 -O2 -Wall  -c rcss.f -o rcss.o
g77 -O2 -Wall  -c rcssr.f -o rcssr.o
g77 -O2 -Wall  -c rcsswt.f -o rcsswt.o
g77 -O2 -Wall  -c rkmat.f -o rkmat.o
g77 -O2 -Wall  -c sortm.f -o sortm.o
ar cr fields.a *.o
ranlib fields.a
windres --include-dir c:/PROGRA~1/R/RW1071~1/src/include  -i fields_res.rc -o fields_res.o
gcc  --shared -s  -o fields.dll fields.def fields.a fields_res.o  -Lc:/PROGRA~1/R/RW1071~1/src/gnuwin32  -lg2c -lR 
  ... DLL made
  installing R files
  installing data files
  installing man source files
  installing indices
  not zipping data
  installing help
 >>> Building/Updating help pages for package 'fields'
     Formats: text html latex example 
  BD                                text    html    latex   example
  Krig                              text    html    latex   example
  Tps                               text    html    latex   example
  US                                text    html    latex   example
  US.dat                            text    html    latex
  Wtransform.image                  text    html    latex   example
  Wtransform.sim                    text    html    latex   example
  arrow.plot                        text    html    latex   example
  as.image                          text    html    latex   example
  as.surface                        text    html    latex   example
  bplot                             text    html    latex   example
  bplot.xy                          text    html    latex   example
  cover.design                      text    html    latex   example
  exp.cov                           text    html    latex   example
  fields-internal                   text    html    latex
  flame                             text    html    latex
  grid.list                         text    html    latex   example
  image.count                       text    html    latex   example
  image.cov                         text    html    latex   example
  image.plot                        text    html    latex   example
  image.smooth                      text    html    latex   example
  interp.surface                    text    html    latex   example
  krig.image                        text    html    latex   example
  lennon                            text    html    latex
  make.Amatrix.Krig                 text    html    latex   example
  make.Amatrix                      text    html    latex   example
  matern.cov                        text    html    latex   example
  minitri                           text    html    latex
  nkreg                             text    html    latex   example
  ozone                             text    html    latex   example
  ozone2                            text    html    latex   example
  plot.Krig                         text    html    latex   example
  plot.sreg                         text    html    latex   example
  plot.surface                      text    html    latex   example
  poisson.cov                       text    html    latex   example
  precip                            text    html    latex
  predict.Krig                      text    html    latex   example
  predict.se.Krig                   text    html    latex   example
  predict.se                        text    html    latex   example
  predict.surface                   text    html    latex   example
  predict.surface.se                text    html    latex   example
  print.Krig                        text    html    latex   example
  qsreg                             text    html    latex   example
  rat.diet                          text    html    latex
  rdist                             text    html    latex   example
  rdist.earth                       text    html    latex   example
  set.panel                         text    html    latex   example
  sim.rf                            text    html    latex   example
  smooth.2d                         text    html    latex   example
  splint                            text    html    latex   example
  sreg                              text    html    latex   example
  stats                             text    html    latex   example
  stats.bin                         text    html    latex   example
  summary.Krig                      text    html    latex   example
  surface.Krig                      text    html    latex   example
  transformx                        text    html    latex   example
  vgram                             text    html    latex   example
  vgram.matrix                      text    html    latex   example
  world                             text    html    latex   example
  xline                             text    html    latex   example
  yline                             text    html    latex   example
 >>> Building/Updating help pages for package 'fields'
     Formats: chm 
  BD                                                                chm
  Krig                                                              chm
  Tps                                                               chm
  US                                                                chm
  US.dat                                                            chm
  Wtransform.image                                                  chm
  Wtransform.sim                                                    chm
  arrow.plot                                                        chm
  as.image                                                          chm
  as.surface                                                        chm
  bplot                                                             chm
  bplot.xy                                                          chm
  cover.design                                                      chm
  exp.cov                                                           chm
  fields-internal                                                   chm
  flame                                                             chm
  grid.list                                                         chm
  image.count                                                       chm
  image.cov                                                         chm
  image.plot                                                        chm
  image.smooth                                                      chm
  interp.surface                                                    chm
  krig.image                                                        chm
  lennon                                                            chm
  make.Amatrix.Krig                                                 chm
  make.Amatrix                                                      chm
  matern.cov                                                        chm
  minitri                                                           chm
  nkreg                                                             chm
  ozone                                                             chm
  ozone2                                                            chm
  plot.Krig                                                         chm
  plot.sreg                                                         chm
  plot.surface                                                      chm
  poisson.cov                                                       chm
  precip                                                            chm
  predict.Krig                                                      chm
  predict.se.Krig                                                   chm
  predict.se                                                        chm
  predict.surface                                                   chm
  predict.surface.se                                                chm
  print.Krig                                                        chm
  qsreg                                                             chm
  rat.diet                                                          chm
  rdist                                                             chm
  rdist.earth                                                       chm
  set.panel                                                         chm
  sim.rf                                                            chm
  smooth.2d                                                         chm
  splint                                                            chm
  sreg                                                              chm
  stats                                                             chm
  stats.bin                                                         chm
  summary.Krig                                                      chm
  surface.Krig                                                      chm
  transformx                                                        chm
  vgram                                                             chm
  vgram.matrix                                                      chm
  world                                                             chm
  xline                                                             chm
  yline                                                             chm
*** Installation of fields failed ***



From MSchwartz at medanalytics.com  Sat Jun 14 03:04:20 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sat, 14 Jun 2003 01:04:20 -0000
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>
References: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>
Message-ID: <1055552612.12970.45.camel@localhost>

On Fri, 2003-06-13 at 19:02, Simon Cawley wrote:
> Dear R-helpers,
> 
> I'm having trouble compiling R packages from source on Win2K.  I
> installed R 1.7.1beta [1] on my win2k machine [2], downloaded the
> fields package as source [3] and tried but failed to install the
> package [4].  I get the same problem with other packages, so it
> doesn't appear to be limited to fields.  Installation of precompiled
> packages seems to work fine.  I've been doing this on linux for a long
> time and never ran into such an issue.
> 
> I've trawled the FAQ and the archives but I'm not finding anything. 
> Any clues where I'm going wrong?
> 
> Thanks in advance,
> 
> -Simon
> 
> ##############

...Lengthy notes SNIPPED

Simon, 

Unless I am missing something here, I think that you are making this
more difficult than you need to.

Thanks I believe to Uwe Ligges, there appears to be a version of the
fields package already compiled for R 1.7.x on Windows at:

http://cran.r-project.org/bin/windows/contrib/1.7/fields_1.3-1.zip

Is there a reason that you cannot use this?

HTH,

Marc Schwartz



From andy_liaw at merck.com  Sat Jun 14 03:35:22 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Jun 2003 21:35:22 -0400
Subject: [R] problem installing packages from source on win2k
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FC12@usrymx25.merck.com>

> From: Simon Cawley [mailto:simon_cawley at affymetrix.com] 
> 
> Dear R-helpers,
> 
> I'm having trouble compiling R packages from source on Win2K. 
>  I installed R 1.7.1beta [1] on my win2k machine [2], 
> downloaded the fields package as source [3] and tried but 
> failed to install the package [4].  I get the same problem 
> with other packages, so it doesn't appear to be limited to 
> fields.  Installation of precompiled packages seems to work 
> fine.  I've been doing this on linux for a long time and 
> never ran into such an issue.
 
[snip]

> [4] within cygwin bash shell, I ran the following:
>   Rcmd install fields_1.3-1.tar.gz
> which dies with the error
> .
> hhc: not found
> cp: cannot stat `c:/tmp/R.INSTALL/fields/chm/fields.chm': No 
> such file or directory
> make[1]: *** [chm-fields] Error 1
> make: *** [pkg-fields] Error 2
> *** Installation of fields failed ***

A couple of things:

1.  I believe the build need to be done from a command prompt (a.k.a.
"MSDOS"), rather than the cygwin bash.  The potential problem (at least the
one I know about) is that the cygwin bash by default prepends it's own PATH
to the system PATH, so it will find the cygwin compilers first before the
minGW.

2.  The error message is quite clear: it can't find hhc.exe.  That's the
Compiled HTML help compiler (for CHM help pages).  Either install it and put
it in the path, or find the Makefile/MkRules and tell it not to make CHM.

HTH,
Andy

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From ray at mcs.vuw.ac.nz  Sat Jun 14 03:55:55 2003
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Sat, 14 Jun 2003 13:55:55 +1200 (NZST)
Subject: [R] maps library for R?
Message-ID: <200306140155.h5E1ttvj013899@tahi.mcs.vuw.ac.nz>

> Does it already exist a library in R to draw maps (something like a Generic 
> Mapping Toolbox, http://gmt.soest.hawaii.edu/ port).
> 
> I've seen in an old R-help that Ross Ihaka once tried to port the S-plus map 
> library (http://maths.newcastle.edu.au/~rking/R/help/99b/0832.html ).
> Anybody know if this package is available somewhere or if there is somebody 
> developing a Mapping package for R. I'll might then try to help.

There are two, but AFAIK neither of them will work on Windows.  My port
of the basic S maps package (no projections) has recently been modified
to provide a small (700KB) 'base' package with limited maps data (just
usa, state, counties, nz and world.thin), plus an add-on package called
mapdata (24MB) with the high-resolution maps. [This split has not yet
been fully tested, and requires maps to be *installed* before mapdata,
and mapdata must be installed in the same library directory as maps.]

These are available from
ftp://ftp.mcs.vuw.ac.nz/pub/statistics/map/map*_1.1-0*

The other solution in development is called Rmap from Barry Rowlingson
(http://www.maths.lancs.ac.uk/Software/Rmap/), but as I understand it,
this requires quite a lot of "third-party" stuff to be installed first.

Ray Brownrigg



From dmurdoch at pair.com  Sat Jun 14 04:22:25 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 13 Jun 2003 22:22:25 -0400
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>
References: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>
Message-ID: <he1levgv33vbdpsmirjtt61masdsn4iq4l@4ax.com>

On Fri, 13 Jun 2003 17:02:07 -0700 (PDT), you wrote:

>
>Dear R-helpers,
>
>I'm having trouble compiling R packages from source on Win2K. 
 ...
>  Rcmd install fields_1.3-1.tar.gz
>which dies with the error
>.
>hhc: not found

That's the Microsoft help compiler.  You can probably do a build
without it; you'll need to define WINHELP to NO instead of the default
CHM.  

Duncan Murdoch



From glaziou at pasteur-kh.org  Sat Jun 14 04:33:47 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Sat, 14 Jun 2003 09:33:47 +0700
Subject: [R] problem with latex of object summary reverse
In-Reply-To: <20030613125225.655c19a2.fharrell@virginia.edu>
References: <20030613131302.GC782@pasteur-kh.org>
	<20030613125225.655c19a2.fharrell@virginia.edu>
Message-ID: <20030614023347.GB581@pasteur-kh.org>

Frank E Harrell Jr <fharrell at virginia.edu> wrote:
> I tried this on the latest version of Hmisc (1.6-0):
> 
> library(Hmisc)
> set.seed(1)
> y <- factor(sample(c('a','b','c'),100,T))
> x <- runif(100)
> a <- summary(y ~ x, method='reverse')
> options(digits=3)
> latex(a)
> 
> and everything was fine.   The following also worked:
> 
> data(iris)
> a <- summary(Species~Sepal.Length, method='reverse',data=iris)
> latex(a)
> 
> See if a bug fix in Hmisc has fixed your problem since the
> last time you updated the package.


Sorry to bother you again, I use the latex command with
Sweave, and I would like to see the reports come out well
without manual interventions on some tex files. 

My Hmisc package version is also 1.6-0, and your first
example gives me the same problem: the first N on the second
row goes into the first column instead of the second one,
all the other Ns are one column to the left of the column
where they should appear, although the summary command
prints ok within R. Also, \multicolumn{} inherits the
position parameter {l} of the remaining of the first 
column, rather than for instance {c} if it where on the
second column.  The remaining of the latex table is right.

Are there any unix tools or latex styles called by your
programmes, that I should check on my system?

Below is the latex compilation output that appears after 
the command latex(a) of your first example. 



  This is TeX, Version 3.14159 (Web2C 7.3.7)
  (/tmp/Rtmp1996/file66334873.tex LaTeX2e <2001/06/01> Babel
  <v3.7h> and hyphenation patterns for american, french,
  german, ngerman, n ohyphenation, loaded.
  (/usr/share/texmf/tex/latex/base/report.cls Document Class:
  report 2001/04/21 v1.4e Standard LaTeX document class
  (/usr/share/texmf/tex/latex/base/size10.clo))
  (/usr/share/texmf/tex/latex/misc/geometry.sty
  (/usr/share/texmf/tex/latex/graphics/keyval.sty)
  (/usr/share/texmf/tex/latex/config/geometry.cfg)) No file
  file66334873.aux.
  
  Underfull \hbox (badness 10000) in paragraph at lines 19--20
  
  [1] (./file66334873.aux)
  
  LaTeX Warning: Label(s) may have changed. Rerun to get
  cross-references right.
  
   )
  (see the transcript file for additional information)
  Output written on file66334873.dvi (1 page, 760 bytes).
  Transcript written on file66334873.log.



Thanks for your help,

-- 
Philippe



From dmurdoch at pair.com  Sat Jun 14 04:52:54 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 13 Jun 2003 22:52:54 -0400
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <3A822319EB35174CA3714066D590DCD5C4FC12@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD5C4FC12@usrymx25.merck.com>
Message-ID: <fd3levcqguqthvanp7lq61e0p47iepig44@4ax.com>

On Fri, 13 Jun 2003 21:35:22 -0400, you wrote:


>
>1.  I believe the build need to be done from a command prompt (a.k.a.
>"MSDOS"), rather than the cygwin bash.  The potential problem (at least the
>one I know about) is that the cygwin bash by default prepends it's own PATH
>to the system PATH, so it will find the cygwin compilers first before the
>minGW.

You need to adjust the path in either case.  I do builds in Cygwin
now; the path is set in /etc/profile.   Since I don't have the Cygwin
compilers installed, finding them first is not an issue.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Sat Jun 14 08:46:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Jun 2003 07:46:19 +0100 (BST)
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <Pine.LNX.4.44.0306131627560.16761-100000@luino.neomorphic.com>
Message-ID: <Pine.LNX.4.44.0306140739440.3013-100000@gannet.stats>

The short answer is that you are not following readme.packages, which says

  If you want to make compiled html (.chm) files you will need the
  Microsoft HTML Help Workshop, currently available for download at
  http://msdn.microsoft.com/library/en-us/htmlhelp/html/hwmicrosofthtmlhelpdownloads.asp
  and http://www.microsoft.com/office/ork/xp/appndx/appa06.htm

  All of these need to be installed and in your path, and the
  appropriate environment variables set.  

hhc.exe is part of HHW, and it is not in your path.

If you don't want to make .chm files, you can edit src/gnuwin32/MkRules 
appropriately.

On Fri, 13 Jun 2003, Simon Cawley wrote:

> 
> Dear R-helpers,
> 
> I'm having trouble compiling R packages from source on Win2K.  I
> installed R 1.7.1beta [1] on my win2k machine [2], downloaded the fields
> package as source [3] and tried but failed to install the package [4].  
> I get the same problem with other packages, so it doesn't appear to be
> limited to fields.  Installation of precompiled packages seems to work
> fine.  I've been doing this on linux for a long time and never ran into
> such an issue.

Maybe someone else set your Linux box up so all the tools were installed?

> I've trawled the FAQ and the archives but I'm not finding anything.  
> Any clues where I'm going wrong?

> Thanks in advance,
> 
> -Simon
> 
> ##############
> 
> Notes:
> 
> [1] Version of R: 1.7.1 Beta, obtained from <http://www.stats.uwo.ca/faculty/murdoch/software/r-devel/rw1071beta.exe> on 6-13-2003
> 
> [2] Output from version:
>   platform i386-pc-mingw32
>   arch     i386           
>   os       mingw32        
>   system   i386, mingw32  
>   status   Beta           
>   major    1              
>   minor    7.1            
>   year     2003           
>   month    06             
>   day      06             
>   language R  
> 
> [3] fields 1.3.1 package downloaded from <http://cran.stat.ucla.edu/src/contrib/fields_1.3-1.tar.gz>.
> 
> [4] within cygwin bash shell, I ran the following:
>   Rcmd install fields_1.3-1.tar.gz
> which dies with the error
> .
> hhc: not found
> cp: cannot stat `c:/tmp/R.INSTALL/fields/chm/fields.chm': No such file or directory
> make[1]: *** [chm-fields] Error 1
> make: *** [pkg-fields] Error 2
> *** Installation of fields failed ***


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Sat Jun 14 10:12:38 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 14 Jun 2003 10:12:38 +0200
Subject: [R] building RPMs for R packages
In-Reply-To: <1055528882.3137.6.camel@localhost.localdomain>
References: <1055528882.3137.6.camel@localhost.localdomain>
Message-ID: <16106.55542.254524.478659@gargle.gargle.HOWL>

>>>>> "Cooper" == Cooper Bethea <cooper.bethea at duke.edu>
>>>>>     on 13 Jun 2003 14:28:02 -0400 writes:

    Cooper> hi all- apologies if this question has come up
    Cooper> before; i took a swing through a few months of
    Cooper> archives and didn't turn anything up.

    Cooper> i'm a linux systems administrator, and we're running
    Cooper> R across a beowulf cluster. i've been asked to
    Cooper> install the bioconductor package, for which i can't
    Cooper> find RPMs. the recommended way to do this on the
    Cooper> bioconductor home page is via R CMD INSTALL, which
    Cooper> seems to be the standard way to install R packages.

    Cooper> however, i want to make an RPM out of bioconductor,
    Cooper> since i have to deploy it to 48 machines and i want
    Cooper> rpm to track the versioning and dependencies. also,
    Cooper> the machines are very homogenous and there's no
    Cooper> reason compilation should occur on each.

I'm not really answering your primary question, 
BUT
    - don't you share some file system(s) between all clients?
      If yes,  you can  "R CMD INSTALL -l <librarydirectory>  package"
      (and make sure your users have <librarydirectory> in their
      R_LIBS or use .libPaths() in the site-wide Rprofile.
      See   help(Startup), help(library) {which has .libPaths()}.

    - if you don't share any file systems {quite improbable} AND
      your machines are very homogenous,  you can still
      INSTALL on one machine, and use "rsync" or some other
      synchronization tool to have the installation distributed
      to the clients.  You will do this with other stuff
      (/etc/.... configurations), anyway, won't you?

Hence, no really deep need for RPMs there, I think.

    Cooper> so could someone who's built R extension RPM
    Cooper> packages give me some pointers? a SRPM for a package
    Cooper> would be invaluable.



From gisar at nus.edu.sg  Sat Jun 14 09:25:05 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Sat, 14 Jun 2003 15:25:05 +0800
Subject: [R] Using jpeg() function over cgi
Message-ID: <024D6AEFCB92CB47BA1085751D184BB801CABF07@MBXSRV03.stf.nus.edu.sg>

As Prof.Ripley has kindly pointed out, bitmap and Xvbf are possible
solutions.

See also replies from vfasciani on
Tuesday, March 18, 2003 5:32 PM
Wednesday, March 19, 2003 6:36 PM

You might also want to check out the e-mail from Steve Chen regarding R
and PHP on
Friday, June 13, 2003 2:36 AM


Regards, Adai.


-----Original Message-----
From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk] 
Sent: Friday, June 13, 2003 10:22 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Using jpeg() function over cgi


Hi

I have seen a few posts to this list regarding problems accessing the
x11() device over cgi - namely, when trying to create a graphic using
the jpeg() function, everything is fine from the command line but it
won't work over cgi, producing the error:

"Unable to open connection to X11 display"

Has anyone actually solved this particular problem satisfactorily?

Please reply direct to me as I am not a member of the list (yet!)

Thanks in advance for your help

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7764 490236
E-mail: michael.watson at bbsrc.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jonck at vanderkogel.net  Sat Jun 14 14:57:11 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Sat, 14 Jun 2003 14:57:11 +0200
Subject: [R] Normalizing data
Message-ID: <B6E43BA0-9E67-11D7-B0BC-0005026E2B43@vanderkogel.net>

Hello dear R-list members,
I have a question about normalizing data. The goal is to normalize the 
dataset per column, so all the data in each column is scaled to the 
interval (0,1), will have a mean of 0 and a standard deviation of 1.
I know the way to do this is to take each datapoint, subtract the mean 
of the column it is located in and divide this by the standard 
deviation of the column. Now my question is: is there a function in R 
that does this, and if so, which function?
Thanks very much, Jonck



From baron at psych.upenn.edu  Sat Jun 14 15:10:07 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 14 Jun 2003 09:10:07 -0400
Subject: [R] Normalizing data
In-Reply-To: <B6E43BA0-9E67-11D7-B0BC-0005026E2B43@vanderkogel.net>
References: <B6E43BA0-9E67-11D7-B0BC-0005026E2B43@vanderkogel.net>
Message-ID: <20030614131007.GA26789@mail2.sas.upenn.edu>

On 06/14/03 14:57, Jonck van der Kogel wrote:
>Hello dear R-list members,
>I have a question about normalizing data. The goal is to normalize the 
>dataset per column, so all the data in each column is scaled to the 
>interval (0,1), will have a mean of 0 and a standard deviation of 1.

In psychology, we usually call this standardizing.  "Normalizing"
is subtracting the mean but NOT dividing by the s.d.

>I know the way to do this is to take each datapoint, subtract the mean 
>of the column it is located in and divide this by the standard 
>deviation of the column. Now my question is: is there a function in R 
>that does this, and if so, which function?

scale()

It will standardize (default) or normalize. And the default is to
it by column, as you describe.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
R page:               http://finzi.psych.upenn.edu/



From wviechtb at s.psych.uiuc.edu  Sat Jun 14 20:09:18 2003
From: wviechtb at s.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Sat, 14 Jun 2003 13:09:18 -0500 (CDT)
Subject: [R] Normalizing data
In-Reply-To: <20030614131007.GA26789@mail2.sas.upenn.edu>
Message-ID: <Pine.SOL.4.30.0306141257300.22270-100000@s.psych.uiuc.edu>

> >I have a question about normalizing data. The goal is to normalize the
> >dataset per column, so all the data in each column is scaled to the
> >interval (0,1), will have a mean of 0 and a standard deviation of 1.

Note that standardizing (i.e., taking a bunch of observations,
subtracting the mean, and dividing by the standard deviation) does not
necessarily ensure that the data will fall into any particular interval.
In fact, it is impossible for the data to fall into the interval (0,1)
because some observations will be above and some below the mean.

--
Wolfgang Viechtbauer



From jonck at vanderkogel.net  Sat Jun 14 20:25:14 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Sat, 14 Jun 2003 20:25:14 +0200
Subject: [R] Missing data augmentation
Message-ID: <8AAAD5A2-9E95-11D7-8996-0005026E2B43@vanderkogel.net>

Hi all,
A short while ago I asked a question about multiple imputation and I 
got several helpful replies, thanks! I have untill now tried to use the 
packages mice and norm but both give me errors however.

mice does not even run to start with and gives me the following error 
right away:
iter imp variable
   1   1  Liquidity.ratioError in chol((v + t(v))/2) : the leading minor 
of order 1 is not positive definite

To be honest I have no idea whatsoever what that error message means, 
so my experiments with mice were shortlived :-)

I then tried the package "norm". I got some ways with the experiment, 
following the help file:
s <- prelim.norm(as.matrix(myDataSet))
thetahat <- em.norm(s)
rngseed(1234567)
theta <- da.norm(s, thetahat, steps=20, showits=TRUE)

At this stage however I get the following error:
Steps of Data Augmentation:
1...2...Error: NA/NaN/Inf in foreign function call (arg 2)

This seems strange to me, since the whole purpose of this routine is to 
work with NA values. So why is it complaining about NA values?

After this I got it to work in an unlikely fashion: I first 
standardized my dataset using scale(). After that I was able to run the
"theta <- da.norm(s, thetahat, steps=20, showits=TRUE)" line 
succesfully. Which seems strange to me, since s still creates NA 
values, so why is it not complaining about them this time. I have 
repeated the process several times, with subsets of my original dataset 
and the same problems arise each time.

Standardizing, calculating the missing values, imputing them and then 
standardizing again does not seem the correct way to go to me however. 
In my opionion the correct way of doing things would be to impute the 
missing values and then standardize the dataset. In other words, the 
way that seems correct to me is not working.

Any helpful comments on the problems described would be much 
appreciated!
Thanks, Jonck



From ramarcos at alumni.uv.es  Sat Jun 14 20:44:09 2003
From: ramarcos at alumni.uv.es (=?iso-8859-1?Q?Ramon_Mart=EDnez_Coscoll=E0?=)
Date: Sat, 14 Jun 2003 20:44:09 +0200
Subject: [R] Confidence intervals plot
Message-ID: <003401c332a4$f4e109c0$7924ca51@statw1c2z9batg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030614/03ec7787/attachment.pl

From dmurdoch at pair.com  Sat Jun 14 21:33:06 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 14 Jun 2003 15:33:06 -0400
Subject: [R] Confidence intervals plot
In-Reply-To: <003401c332a4$f4e109c0$7924ca51@statw1c2z9batg>
References: <003401c332a4$f4e109c0$7924ca51@statw1c2z9batg>
Message-ID: <vetmevcnuhim07bsljk746lk1soc4ufhg3@4ax.com>

On Sat, 14 Jun 2003 20:44:09 +0200, you wrote:

>Hi all!!
>
>I am trying to plot several confidence intervals in a unique plot. That is, for each x, I have a confidence interval for a parameter related to x and I would like to plot them in the same plot, in order to compare them. The plot would look like some parallel vertical lines, each one corresponding to a x value. Their extrem points would be the confidence interval limits.

>I do not know if I am clear enough. Anyway, thank you in advance.

Suppose the upper limits are in U, the lower limits in L, and the x
values in X.  Then 

# set up the axes etc.
plot(X, U, ylim=range(c(L,U)), type='n')

segments(X, L, X, U)

will do what you describe.  You should also look at arrows(), in case
you want points or crossbars on the ends of the segments.

Duncan Murdoch



From mschwartz at medanalytics.com  Sat Jun 14 21:34:19 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Sat, 14 Jun 2003 14:34:19 -0500
Subject: [R] Confidence intervals plot
In-Reply-To: <003401c332a4$f4e109c0$7924ca51@statw1c2z9batg>
Message-ID: <009b01c332ab$f3c9c930$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon 
>Mart?nez Coscoll?
>Sent: Saturday, June 14, 2003 1:44 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Confidence intervals plot
>
>
>Hi all!!
>
>I am trying to plot several confidence intervals in a unique 
>plot. That is, for each x, I have a confidence interval for a 
>parameter related to x and I would like to plot them in the 
>same plot, in order to compare them. The plot would look like 
>some parallel vertical lines, each one corresponding to a x 
>value. Their extrem points would be the confidence interval limits.
>
>I do not know if I am clear enough. Anyway, thank you in advance.
>
>Ramon.


You have several options depending upon whether you simply want
vertical CI lines above and below xy data points or if you might want
a barplot with CI's.

You could draw the xy data points using plot() and then draw the CI's
yourself using either segments() or arrows() in the base package or
see plotCI(), plotmeans() and barplot2() in the 'gregmisc' package on
CRAN. plotmeans() is a "wrapper" function that can call plotCI().

HTH,

Marc Schwartz



From kjetil at entelnet.bo  Sat Jun 14 22:10:36 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Sat, 14 Jun 2003 16:10:36 -0400
Subject: [R] Confidence intervals plot
In-Reply-To: <003401c332a4$f4e109c0$7924ca51@statw1c2z9batg>
Message-ID: <3EEB48FC.27034.611DE0@localhost>

On 14 Jun 2003 at 20:44, Ramon Mart?nez Coscoll? wrote:

Hola!

First set up a plot with

plot( c(x.lower, x.upper) , c(y.lower, y.upper), type="n" )

and then add each line using segments

Kjetil Halvorsen

> Hi all!!
> 
> I am trying to plot several confidence intervals in a unique plot. That is, for each x, I have a confidence interval for a parameter related to x and I would like to plot them in the same plot, in order to compare them. The plot would look like some parallel vertical lines, each one 
corresponding to a x value. Their extrem points would be the confidence interval limits.
> 
> I do not know if I am clear enough. Anyway, thank you in advance.
> 
> Ramon.
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From cooper.bethea at duke.edu  Sat Jun 14 22:55:35 2003
From: cooper.bethea at duke.edu (Cooper Bethea)
Date: Sat, 14 Jun 2003 20:55:35 -0000
Subject: [R] building RPMs for R packages
In-Reply-To: <16106.55542.254524.478659@gargle.gargle.HOWL>
References: <1055528882.3137.6.camel@localhost.localdomain>
	<16106.55542.254524.478659@gargle.gargle.HOWL>
Message-ID: <1055624055.3137.33.camel@localhost.localdomain>

On Sat, 2003-06-14 at 04:12, Martin Maechler wrote:

> I'm not really answering your primary question, 
> BUT
    - don't you share some file system(s) between all clients?

*share library directory across cluster*
> 
>     - if you don't share any file systems {quite improbable} AND

*distribute libraries over rsync*

> Hence, no really deep need for RPMs there, I think.

thanks for the input, but i don't agree with you here.
for 1) - it keeps network traffic down to keep libraries on local disk.
some of our computations are not even close to embarrassingly parallel,
and this would impede performance.
2) some of /etc directories are shared across the cluster because
they're small files that are easily cached and it's more convenient that
way. however, this is a more reasonable idea.

but those are still totally nasty hacks if you ask me. the way to
install software on an rpm-based system is via rpm.

the reason i need to build a package here (and the reason any other sys.
admin on an rpm-based system should) is because packages not installed
under rpm have a chance of breaking during system upgrades. in order to
maintain my sanity, i need to know that package R-foo needs to be
upgraded when i upgrade packages R, dependency blah, dependency blah,
and i need rpm to know that so my install tool will do it automatically.
if it doesn't my users' jobs will break and they will yell at me and i
will cry. the methods you propose are not scalable.


> 
>     Cooper> so could someone who's built R extension RPM
>     Cooper> packages give me some pointers? a SRPM for a package
>     Cooper> would be invaluable.



From p.dalgaard at biostat.ku.dk  Sat Jun 14 23:37:22 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 14 Jun 2003 21:37:22 -0000
Subject: [R] building RPMs for R packages
In-Reply-To: <1055624055.3137.33.camel@localhost.localdomain>
References: <1055528882.3137.6.camel@localhost.localdomain>
	<16106.55542.254524.478659@gargle.gargle.HOWL>
	<1055624055.3137.33.camel@localhost.localdomain>
Message-ID: <x2smqcl4ci.fsf@biostat.ku.dk>

Cooper Bethea <cooper.bethea at duke.edu> writes:

> the reason i need to build a package here (and the reason any other sys.
> admin on an rpm-based system should) is because packages not installed
> under rpm have a chance of breaking during system upgrades. in order to
> maintain my sanity, i need to know that package R-foo needs to be
> upgraded when i upgrade packages R, dependency blah, dependency blah,
> and i need rpm to know that so my install tool will do it automatically.
> if it doesn't my users' jobs will break and they will yell at me and i
> will cry. the methods you propose are not scalable.
> 
> 
> > 
> >     Cooper> so could someone who's built R extension RPM
> >     Cooper> packages give me some pointers? a SRPM for a package
> >     Cooper> would be invaluable.

Actually, the problem of making sure that N computers all have the
same set of RPMs installed is not terribly scalable either...

Centralized software maintenance is a pretty obvious thing to do, but
I often suspect RedHat (and several others) of not really having
grasped the idea of having computers connected in a net. Mixing the
two maintenance models is probably not good, I tend to agree with you
on that.

You might want to take a look at the scripts that Dethlef did for
creating RPMs of contributed packages on SuSE. Another option that I
think should work is to take the spec file that builds R and modify it
to do a BioConductor install after making the main binary, giving you
a "mega-R" RPM with everything included.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jeaneid at chass.utoronto.ca  Sun Jun 15 02:51:29 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 14 Jun 2003 20:51:29 -0400
Subject: [R] more efficient sum of matrix columns
In-Reply-To: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
Message-ID: <Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>

Dear R users,
I am looking for a more efficient way to compute the sum of columns of a
matrix.
I am currently using apply(data, 2, sum) however, I am building a data set
from another one by summing the columns of some parts of the matrix.
the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.
thanks,
Jean Eid



From baron at psych.upenn.edu  Sun Jun 15 03:18:34 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 14 Jun 2003 21:18:34 -0400
Subject: [R] more efficient sum of matrix columns
In-Reply-To: <Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>
References: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
	<Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>
Message-ID: <20030615011834.GA6785@mail2.sas.upenn.edu>

On 06/14/03 20:51, Jean Eid wrote:
>Dear R users,
>I am looking for a more efficient way to compute the sum of columns of a
>matrix.
>I am currently using apply(data, 2, sum) however, I am building a data set
>from another one by summing the columns of some parts of the matrix.
>the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.

colSums() might be faster, but I don't know how much faster.  It
does not allow na.rm=T, but you don't have that, so it might
help.

--
Jonathan Baron, Professor of Psychology, University of Pennsylvania
R page:               http://finzi.psych.upenn.edu/



From jfox at mcmaster.ca  Sun Jun 15 04:47:20 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 14 Jun 2003 22:47:20 -0400
Subject: [R] more efficient sum of matrix columns
In-Reply-To: <20030615011834.GA6785@mail2.sas.upenn.edu>
References: <Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>
	<Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>
	<Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>
Message-ID: <5.1.0.14.2.20030614224110.01e84320@mcmail.cis.mcmaster.ca>

Dear Jean and Jonathan,

colSums() should be more efficient, but (unless I misunderstand the size of 
the problem) a problem this small shouldn't take a half hour. On my ageing 
800 MHz, 512MB Windows 2000 PC, the result was essentially instantaneous 
either way, though an order of magnitude faster with colSums:

 > data <- matrix(rnorm(4462 * 202), 4462, 202)
 > dim(data)
[1] 4462  202
 > system.time(apply(data, 2, sum))
[1] 0.38 0.00 0.42   NA   NA
 > system.time(colSums(data))
[1] 0.03 0.00 0.03   NA   NA

Jean does make reference to summing parts of a matrix, apparently in a 
loop, so it might help to know what's being done besides the column sums.

Regards,
  John

At 09:18 PM 6/14/2003 -0400, Jonathan Baron wrote:
>On 06/14/03 20:51, Jean Eid wrote:
> >Dear R users,
> >I am looking for a more efficient way to compute the sum of columns of a
> >matrix.
> >I am currently using apply(data, 2, sum) however, I am building a data set
> >from another one by summing the columns of some parts of the matrix.
> >the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.
>
>colSums() might be faster, but I don't know how much faster.  It
>does not allow na.rm=T, but you don't have that, so it might
>help.

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From keef9490 at uidaho.edu  Sun Jun 15 06:47:24 2003
From: keef9490 at uidaho.edu (Robert Keefe)
Date: Sat, 14 Jun 2003 21:47:24 -0700 (PDT)
Subject: [R] Fitted probabilities from glmmPQL?
Message-ID: <Pine.GHP.4.51.0306142140090.16162@raptor.csrv.uidaho.edu>


Hello All,

Specifying 'type = "response"' when using predict() on a
model fit using glm(...,family="binomial") returns fitted
probabilities.

Is it possible to get the same from a model object
fit using glmmPQL() ?

Thanks in advance,

Rob

_____________________________________________________

Rob Keefe                 Lab: (208) 885-5165
M.S. student              Home: (208) 882-9749
University of Idaho



From rpeng at stat.ucla.edu  Sun Jun 15 09:11:04 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Sun, 15 Jun 2003 00:11:04 -0700
Subject: [R] more efficient sum of matrix columns
In-Reply-To: <20030615011834.GA6785@mail2.sas.upenn.edu>
References: <Pine.SOL.4.44.0306121102500.17004-100000@millipede.gpcc.itd.umich.edu>	<Pine.SGI.4.40.0306121207220.470260-100000@origin.chass.utoronto.ca>
	<20030615011834.GA6785@mail2.sas.upenn.edu>
Message-ID: <3EEC1C08.9010802@stat.ucla.edu>

colSums()/rowSums() will be *much* faster and you can specify na.rm = TRUE.

-roger

Jonathan Baron wrote:

>On 06/14/03 20:51, Jean Eid wrote:
>  
>
>>Dear R users,
>>I am looking for a more efficient way to compute the sum of columns of a
>>matrix.
>>I am currently using apply(data, 2, sum) however, I am building a data set
>>    
>>
>>from another one by summing the columns of some parts of the matrix.
>  
>
>>the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.
>>    
>>
>
>colSums() might be faster, but I don't know how much faster.  It
>does not allow na.rm=T, but you don't have that, so it might
>help.
>
>--
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>R page:               http://finzi.psych.upenn.edu/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From jonck at vanderkogel.net  Sun Jun 15 13:23:34 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Sun, 15 Jun 2003 13:23:34 +0200
Subject: [R] more efficient sum of matrix columns 
In-Reply-To: <200306151002.h5FA0kDI012374@stat.math.ethz.ch>
Message-ID: <CD488233-9F23-11D7-837B-0005026E2B43@vanderkogel.net>

Hi,
Maybe I'm not understanding you correctly, but wouldn't the following 
be the fastest way to compute the sum of a column:

 > x <- matrix(c(1:4462), nrow=4462, ncol=1) #setting up a column of 
4462 elements to be able to make a comparison
 > sum(x[,1])
[1] 9956953
 > system.time(sum(x[,1]))
[1] 0 0 0 0 0

As you can see this took so little time it actually came out as 0. And 
my system is not fast at all (powermac 9600 with a G4 700 MHz upgrade).
HTH, Jonck


> From: Jean Eid <jeaneid at chass.utoronto.ca>
> Date: zon jun 15, 2003  02:51:29 Europe/Amsterdam
> To: r-help at stat.math.ethz.ch
> Subject: [R] more efficient sum of matrix columns
>
>
> Dear R users,
> I am looking for a more efficient way to compute the sum of columns of 
> a
> matrix.
> I am currently using apply(data, 2, sum) however, I am building a data 
> set
> from another one by summing the columns of some parts of the matrix.
> the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.
> thanks,
> Jean Eid



From sundar.dorai-raj at pdf.com  Sun Jun 15 15:23:47 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 15 Jun 2003 08:23:47 -0500
Subject: [R] Fitted probabilities from glmmPQL?
References: <Pine.GHP.4.51.0306142140090.16162@raptor.csrv.uidaho.edu>
Message-ID: <3EEC7363.301@pdf.com>

Robert,

Robert Keefe wrote:
> Hello All,
> 
> Specifying 'type = "response"' when using predict() on a
> model fit using glm(...,family="binomial") returns fitted
> probabilities.
> 
> Is it possible to get the same from a model object
> fit using glmmPQL() ?
> 

glmmPQL returns an lmeObject which has no "family" information. You will 
have to do this operation yourself. As in:

R> x = glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
R+                 family = binomial, data = bacteria) # from help page
iteration 1
iteration 2
iteration 3
iteration 4
iteration 5
iteration 6
R> y = predict(x)
R> str(y)
  atomic [1:220] 4.05 4.05 2.44 2.44 2.25 ...
  - attr(*, "label")= chr "Fitted values"
R> z = binomial()$linkinv(y) # default: link = "logit"
R> str(z)
  num [1:220] 0.983 0.983 0.920 0.920 0.905 ...



From jeaneid at chass.utoronto.ca  Sun Jun 15 16:15:48 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sun, 15 Jun 2003 10:15:48 -0400
Subject: [R] more efficient sum of matrix columns
In-Reply-To: <5.1.0.14.2.20030614224110.01e84320@mcmail.cis.mcmaster.ca>
Message-ID: <Pine.SGI.4.40.0306151007480.752246-100000@origin.chass.utoronto.ca>

this is what my program read
dataa<-data.frame(matrix(0, nrow=nrow(data), ncol=ncol(data))
i<-1
j<-1
while(i<=nrow(data))
{
   if(data$Index.Price==1)
	data1[j,]<-data[i,]
   else
{
	num<-data$No.Primary[i]
	data[j,]<-apply(data[i:i+num-1,],2,sum)
	i<-i+num-1
}
   j<-j+1
   i<-i+1
}
#data is the original 4462*202 matrix.

thanks,



On Sat, 14 Jun 2003, John Fox wrote:

> Dear Jean and Jonathan,
>
> colSums() should be more efficient, but (unless I misunderstand the size of
> the problem) a problem this small shouldn't take a half hour. On my ageing
> 800 MHz, 512MB Windows 2000 PC, the result was essentially instantaneous
> either way, though an order of magnitude faster with colSums:
>
>  > data <- matrix(rnorm(4462 * 202), 4462, 202)
>  > dim(data)
> [1] 4462  202
>  > system.time(apply(data, 2, sum))
> [1] 0.38 0.00 0.42   NA   NA
>  > system.time(colSums(data))
> [1] 0.03 0.00 0.03   NA   NA
>
> Jean does make reference to summing parts of a matrix, apparently in a
> loop, so it might help to know what's being done besides the column sums.
>
> Regards,
>   John
>
> At 09:18 PM 6/14/2003 -0400, Jonathan Baron wrote:
> >On 06/14/03 20:51, Jean Eid wrote:
> > >Dear R users,
> > >I am looking for a more efficient way to compute the sum of columns of a
> > >matrix.
> > >I am currently using apply(data, 2, sum) however, I am building a data set
> > >from another one by summing the columns of some parts of the matrix.
> > >the loop is taking too long (about 1/2 hour) for a 4462 * 202 matrix.
> >
> >colSums() might be faster, but I don't know how much faster.  It
> >does not allow na.rm=T, but you don't have that, so it might
> >help.
>
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>
>



From cooper.bethea at duke.edu  Sun Jun 15 17:38:42 2003
From: cooper.bethea at duke.edu (Cooper Bethea)
Date: Sun, 15 Jun 2003 15:38:42 -0000
Subject: [R] building RPMs for R packages
In-Reply-To: <x2smqcl4ci.fsf@biostat.ku.dk>
References: <1055528882.3137.6.camel@localhost.localdomain>
	<16106.55542.254524.478659@gargle.gargle.HOWL>
	<1055624055.3137.33.camel@localhost.localdomain>
	<x2smqcl4ci.fsf@biostat.ku.dk>
Message-ID: <1055691458.3136.13.camel@localhost.localdomain>

On Sat, 2003-06-14 at 17:46, Peter Dalgaard BSA wrote:

> Actually, the problem of making sure that N computers all have the
> same set of RPMs installed is not terribly scalable either...
> 
> Centralized software maintenance is a pretty obvious thing to do, but
> I often suspect RedHat (and several others) of not really having
> grasped the idea of having computers connected in a net. Mixing the
> two maintenance models is probably not good, I tend to agree with you
> on that.

actually (maybe you already know about this and you're getting at
something else, though) we have developed yum for systems updates -
http://linux.duke.edu/yum/ - it's an auto-updater that pulls rpm headers
down from a server, determines which packages need to be updated, then
pulls those specific rpms down and installs them. it has some other
functionality as well - it can list packages matching a regex, install
specified packages and their dependencies, remove packages, etc. etc.
basically, we're trying to take all the stuff we like from apt-get. :)

> 
> You might want to take a look at the scripts that Dethlef did for
> creating RPMs of contributed packages on SuSE. 

thanks! can you tell me where i might find those? google dethlef
site:r-project.org doesn't turn them up.



From fharrell at virginia.edu  Sat Jun 14 23:11:47 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Sat, 14 Jun 2003 17:11:47 -0400
Subject: [R] problem with latex of object summary reverse
In-Reply-To: <20030614023347.GB581@pasteur-kh.org>
References: <20030613131302.GC782@pasteur-kh.org>
	<20030613125225.655c19a2.fharrell@virginia.edu>
	<20030614023347.GB581@pasteur-kh.org>
Message-ID: <20030614171147.32b8f63f.fharrell@virginia.edu>

On Sat, 14 Jun 2003 09:33:47 +0700
Philippe Glaziou <glaziou at pasteur-kh.org> wrote:

> Frank E Harrell Jr <fharrell at virginia.edu> wrote:
> > I tried this on the latest version of Hmisc (1.6-0):
> > 
> > library(Hmisc)
> > set.seed(1)
> > y <- factor(sample(c('a','b','c'),100,T))
> > x <- runif(100)
> > a <- summary(y ~ x, method='reverse')
> > options(digits=3)
> > latex(a)
> > 
> > and everything was fine.   The following also worked:
> > 
> > data(iris)
> > a <- summary(Species~Sepal.Length, method='reverse',data=iris)
> > latex(a)
> > 
> > See if a bug fix in Hmisc has fixed your problem since the
> > last time you updated the package.
> 
> 
> Sorry to bother you again, I use the latex command with
> Sweave, and I would like to see the reports come out well
> without manual interventions on some tex files. 

latex(object, file='') works with Sweave's <<results=tex>>= construct.

> 
> My Hmisc package version is also 1.6-0, and your first
> example gives me the same problem: the first N on the second
> row goes into the first column instead of the second one,
> all the other Ns are one column to the left of the column
> where they should appear, although the summary command
> prints ok within R. Also, \multicolumn{} inherits the
> position parameter {l} of the remaining of the first 
> column, rather than for instance {c} if it where on the
> second column.  The remaining of the latex table is right.

As in my previous note I could not duplicate that on Linux using the standard tetex distribution of latex etc.  Please send output from the version command (I need to at least see the OS you are using) and your complete script (which you did not send before) so that I can run it exactly as you do, if you still cannot get it to work.

> 
> Are there any unix tools or latex styles called by your
> programmes, that I should check on my system?

None for that application.

> 
> Below is the latex compilation output that appears after 
> the command latex(a) of your first example. 

That's more confusing than ever, because I see a successful run in the log messages below.  There were no errors, only a warning.  This warning is expected.

Frank

> 
> 
> 
>   This is TeX, Version 3.14159 (Web2C 7.3.7)
>   (/tmp/Rtmp1996/file66334873.tex LaTeX2e <2001/06/01> Babel
>   <v3.7h> and hyphenation patterns for american, french,
>   german, ngerman, n ohyphenation, loaded.
>   (/usr/share/texmf/tex/latex/base/report.cls Document Class:
>   report 2001/04/21 v1.4e Standard LaTeX document class
>   (/usr/share/texmf/tex/latex/base/size10.clo))
>   (/usr/share/texmf/tex/latex/misc/geometry.sty
>   (/usr/share/texmf/tex/latex/graphics/keyval.sty)
>   (/usr/share/texmf/tex/latex/config/geometry.cfg)) No file
>   file66334873.aux.
>   
>   Underfull \hbox (badness 10000) in paragraph at lines 19--20
>   
>   [1] (./file66334873.aux)
>   
>   LaTeX Warning: Label(s) may have changed. Rerun to get
>   cross-references right.
>   
>    )
>   (see the transcript file for additional information)
>   Output written on file66334873.dvi (1 page, 760 bytes).
>   Transcript written on file66334873.log.
> 
> 
> 
> Thanks for your help,
> 
> -- 
> Philippe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From Peter.Caley at csiro.au  Mon Jun 16 01:57:15 2003
From: Peter.Caley at csiro.au (Peter.Caley@csiro.au)
Date: Mon, 16 Jun 2003 09:57:15 +1000
Subject: [R] Summary: Factorial function in R
Message-ID: <662FF5A611597C41987B05E11DFE35923797DD@exact3-cbr.act.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/8f037551/attachment.pl

From jfox at mcmaster.ca  Mon Jun 16 03:53:02 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 15 Jun 2003 21:53:02 -0400
Subject: [R] new version of Rcmdr package
Message-ID: <5.1.0.14.2.20030615214826.01ed22c8@mcmail.cis.mcmaster.ca>

Dear R-announce list members,

I've uploaded to CRAN a new version of my Rcmdr package, which provides a 
cross-platform basic-statistics GUI for R. This version has a few new 
features, but mostly addresses some problems that arose with the original 
version. The relevant portion of the CHANGES file for the package is appended.

As before, suggestions and reports of problems are appreciated.

John

---- CHANGES ----

Version 0.8-3

     o Changed Compute dialog to use with(activeDataSet, expression) rather 
than to evaluate the expression in the global environment.

     o Added RcmdrPager function for non-Windows systems, a version of the 
tkpager modified slightly to use the Rcmdr monospaced font and a white 
background. This is for non-Windows systems.

     o Added Data -> Active data set -> Export active data set.

     o Reworked package initialization so as not to fail when a required 
package -- probably car -- is missing (thanks to Brian Ripley for help here).

     o Reworked options, mostly for better font support in non-Windows 
systems, but also to allow configurable size of log window, etc. Made 
default log window larger.

     o Added check box to the Commander window to enable/disable attaching 
of active data set.

     o Added flexibility to Data -> Import data -> From text file. Other 
small changes to Delete variables and Recode dialogs.

     o Added Statistics -> Summaries -> Active data set.

     o Cleaned up some dialogs -- for example, lm and glm dialogs have 
initial focus in the lhs field and tab directly to the rhs field.

     o Changes to all dialogs to ensure better behaviour, particularly in 
fast Windows XP systems: preventing scrollbars from sticking and dialogs 
from displaying completely (thanks to Peter Dalgaard and, especially, Marc 
Schwartz).

     o Re-executing lines from the log with assignment to a list (or data 
frame) element (e.g., data$x <- y) now works.


-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From glaziou at pasteur-kh.org  Mon Jun 16 07:30:57 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Mon, 16 Jun 2003 12:30:57 +0700
Subject: [long:] Re: [R] problem with latex of object summary reverse
In-Reply-To: <20030614171147.32b8f63f.fharrell@virginia.edu>
References: <20030613131302.GC782@pasteur-kh.org>
	<20030613125225.655c19a2.fharrell@virginia.edu>
	<20030614023347.GB581@pasteur-kh.org>
	<20030614171147.32b8f63f.fharrell@virginia.edu>
Message-ID: <20030616053056.GB1570@pasteur-kh.org>

Frank E Harrell Jr <fharrell at virginia.edu> wrote:
> > Sorry to bother you again, I use the latex command with
> > Sweave, and I would like to see the reports come out well
> > without manual interventions on some tex files. 
> 
> latex(object, file='') works with Sweave's <<results=tex>>= construct.
> 
> > 
> As in my previous note I could not duplicate that on Linux
> using the standard tetex distribution of latex etc.
> Please send output from the version command (I need to at
> least see the OS you are using) and your complete script
> (which you did not send before) so that I can run it exactly
> as you do, if you still cannot get it to work.


Thanks a lot for your help. I use the standard tetex
distribution that comes with linux debian (sarge). I could
also duplicate the problem on a another machine running
debian woody.

   cunegonde:~> latex --version
   TeX (Web2C 7.3.7) 3.14159
   kpathsea version 3.3.7
   Copyright (C) 1999 D.E. Knuth.
   [... snip copyright info]   




Here is the transcript of a very short R session
showing the latex problem with summary:


##########################################################
R : Copyright 2003, The R Development Core Team
Version 1.7.0  (2003-04-16)

[snip the R intro text]

> options(STERM='iESS', editor='emacsclient')
> library(Hmisc)
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

Hmisc redefines [.factor to drop unused levels of factor variables
when subscripting. To prevent this behaviour, issue the command
options(drop.unused.levels=F).


Attaching package 'Hmisc':


	The following object(s) are masked from package:methods :

	 show 


	The following object(s) are masked from package:base :

	 [.factor %in% interaction [.terms 

> set.seed(1)
> y<-factor(sample(c('a','b','c'),100,T))
> x<-runif(100)
> a<-summary(y~x,method='reverse')
> options(digits=3)
> a


Descriptive Statistics by y

+-+-----------------+-----------------+-----------------+
| |a                |b                |c                |
| |(N=27)           |(N=38)           |(N=35)           |
+-+-----------------+-----------------+-----------------+
|x|0.474/0.548/0.724|0.261/0.446/0.626|0.284/0.591/0.853|
+-+-----------------+-----------------+-----------------+
> latex(a)->la
This is TeX, Version 3.14159 (Web2C 7.3.7)
(/tmp/Rtmp973/file66334873.tex
LaTeX2e <2001/06/01>
Babel <v3.7h> and hyphenation patterns for american, french, german, ngerman, n
ohyphenation, loaded.
(/usr/share/texmf/tex/latex/base/report.cls
Document Class: report 2001/04/21 v1.4e Standard LaTeX document class
(/usr/share/texmf/tex/latex/base/size10.clo))
(/usr/share/texmf/tex/latex/misc/geometry.sty
(/usr/share/texmf/tex/latex/graphics/keyval.sty)
(/usr/share/texmf/tex/latex/config/geometry.cfg))
No file file66334873.aux.

Underfull \hbox (badness 10000) in paragraph at lines 20--21

[1] (./file66334873.aux)

LaTeX Warning: Label(s) may have changed. Rerun to get cross-references right.

 )
(see the transcript file for additional information)
Output written on file66334873.dvi (1 page, 820 bytes).
Transcript written on file66334873.log.

###############################################################

And now, the faulty latex file (a.tex):


% latex.default(cstats, title = title, caption = caption, rowlabel = rowlabel,      col.just = col.just, numeric.dollar = FALSE, insert.bottom = legend,      rowname = lab, dcolumn = dcolumn, extracolheads = extracolheads,      extracolsize = Nsize, ...) 
%
\begin{table}[!tbp]
 \begin{center}
 \caption{Descriptive Statistics by y\label{a}} 
 \begin{tabular}{lccc}\hline\hline
\multicolumn{1}{l}{}&
\multicolumn{1}{c}{a}&
\multicolumn{1}{c}{b}&
\multicolumn{1}{c}{c}
\\   \multicolumn{1}{l}{{\scriptsize $N=27$}}&\multicolumn{1}{c}{{\scriptsize $N=38$}}&\multicolumn{1}{c}{{\scriptsize $N=35$}}\\ \hline
x&{\scriptsize 0.474~}{0.548 }{\scriptsize 0.724} &{\scriptsize 0.261~}{0.446 }{\scriptsize 0.626} &{\scriptsize 0.284~}{0.591 }{\scriptsize 0.853} \\
\hline
\end{tabular}
\end{center}

\noindent {\scriptsize $a$\ }{$b$\ }{\scriptsize $c$\ } represent the lower quartile $a$, the median $b$, and the upper quartile $c$\ for continuous variables.\\


\end{table}

###############################################################

The line which starts with:

\\    \multicolumn{l}{1}{{\scriptsize$N=27$}}&\multicolumn{1}{c}{[...]

should read:

\\    \multicolumn{l}{1}{}&\multicolumn{1}{c}{\scriptsize$N=27$}{[...]


I use your libraries and also Sweave a lot (Sweave is
actually the major reason why I decided recently to use R
rather than Stata: documenting the analysis is fun when you
can produce nice tables and graphs automatically in a LaTeX
document). Also, I find your libraries and documentation
truly amazing.

Thanks again,

-- 
Philippe Glaziou
Epidemiologist
Pasteur Institute of Cambodia



From rolf at math.unb.ca  Sat Jun 14 21:57:10 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 14 Jun 2003 16:57:10 -0300 (ADT)
Subject: [R] A sapply() funny.
Message-ID: <200306141957.h5EJvAtw019530@erdos.math.unb.ca>


The sapply function is refusing to return a result for what seem to
me to be mysterious reasons.  Here is a toy example:

set.seed(111)
X    <- list(x=runif(20),y=runif(20))
rvec <- seq(0.01,0.15,length=42)

foo  <- function(x,X,cc) {
	mean((X$x)^x + (X$y)^cc)
}

bar  <- function(x,a,b){a+b*x}

try.b <- sapply(rvec,bar,a=1,b=2)     # This runs without a problem and
                                      # gives a + b*rvec as expected.

try.f <- sapply(rvec,foo,X=X,cc=1/3)  # This falls over.

The error message is:

Error in get(x, envir, mode, inherits) : variable "rvec" was not found

How can ``rvec'' not be found?  Is this one of the ``improvements''
induced by the new namespace sorcery?

Version information:

> version
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    7.0                 
year     2003                
month    04                  
day      16                  
language R

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From kurt.sys at UGent.be  Mon Jun 16 09:20:27 2003
From: kurt.sys at UGent.be (Kurt Sys)
Date: Mon, 16 Jun 2003 09:20:27 +0200
Subject: [R] probe design
In-Reply-To: <20030613152953.GA7873720@genome.cbs.dtu.dk>
References: <16105.55327.355588.986172@ksys.rug.ac.be>
	<20030613152953.GA7873720@genome.cbs.dtu.dk>
Message-ID: <16109.28603.300409.39773@ksys.rug.ac.be>






--
Mail from Laurent Gautier
sent on Friday June 13 2003 at 17:29 (GMT+0200):

   On Fri, Jun 13, 2003 at 03:56:47PM +0200, Kurt Sys wrote:
   > Hello,
   > 
   > this is a quite specific topic, but I just wonder if there are
   > some R packages present for 'probe design'.
   > 
   > tnx, Kurt.
   > 
   
   What do you mean by "probe design" ? Are you referring to
   microarrays ?


Yes, that's what I mean, but actually it doesn't matter a lot. Probes
can be used different molecular techniques, can't they?
My second question would 've been: are there any packages for
microarray design, supposing I have a list of possible probes (with
appropriate melting temperatures etc) for each organism or group of
organisms.

Kurt.



From p.dalgaard at biostat.ku.dk  Mon Jun 16 09:23:56 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 16 Jun 2003 07:23:56 -0000
Subject: [R] A sapply() funny.
In-Reply-To: <200306141957.h5EJvAtw019530@erdos.math.unb.ca>
References: <200306141957.h5EJvAtw019530@erdos.math.unb.ca>
Message-ID: <x2n0gijx39.fsf@biostat.ku.dk>

Rolf Turner <rolf at math.unb.ca> writes:

> try.f <- sapply(rvec,foo,X=X,cc=1/3)  # This falls over.
> 
> The error message is:
> 
> Error in get(x, envir, mode, inherits) : variable "rvec" was not found
> 
> How can ``rvec'' not be found?  Is this one of the ``improvements''
> induced by the new namespace sorcery?

No. Take a look at args(sapply). Hint: There is no *function* called
rvec. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lsophir at wisemail.weizmann.ac.il  Mon Jun 16 09:27:48 2003
From: lsophir at wisemail.weizmann.ac.il (Ron Ophir)
Date: Mon, 16 Jun 2003 10:27:48 +0300
Subject: [R] grid background color
Message-ID: <seed9bc1.076@wisemail.weizmann.ac.il>

Dear all,
I am trying to run cloud function in lattice library under r-1.7, and I wlways get gray background. I've tried to chang it by set "...par.box=list(col="white")"
or by gpar(background="white") but no result.
How do i do it?
Thanks in advance,
Ron



From hb at maths.lth.se  Mon Jun 16 10:00:08 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 16 Jun 2003 10:00:08 +0200
Subject: [R] probe design
In-Reply-To: <16109.28603.300409.39773@ksys.rug.ac.be>
Message-ID: <000f01c333dd$5073a110$f51369d4@alpha.wehi.edu.au>

Hi, I think the mailing list at Bioconductor
(http://www.bioconductor.org/) is a good place to ask as there you will
find the right people. A lot are on this list too, but not all.

Have a nice day!

Henrik Bengtsson
Lund University, Sweden

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kurt Sys
> Sent: den 16 juni 2003 09:20
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] probe design
> 
> 
> 
> 
> 
> 
> 
> --
> Mail from Laurent Gautier
> sent on Friday June 13 2003 at 17:29 (GMT+0200):
> 
>    On Fri, Jun 13, 2003 at 03:56:47PM +0200, Kurt Sys wrote:
>    > Hello,
>    > 
>    > this is a quite specific topic, but I just wonder if there are
>    > some R packages present for 'probe design'.
>    > 
>    > tnx, Kurt.
>    > 
>    
>    What do you mean by "probe design" ? Are you referring to
>    microarrays ?
> 
> 
> Yes, that's what I mean, but actually it doesn't matter a 
> lot. Probes can be used different molecular techniques, can't 
> they? My second question would 've been: are there any 
> packages for microarray design, supposing I have a list of 
> possible probes (with appropriate melting temperatures etc) 
> for each organism or group of organisms.
> 
> Kurt.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
>



From david.barron at said-business-school.oxford.ac.uk  Mon Jun 16 10:08:18 2003
From: david.barron at said-business-school.oxford.ac.uk (David Barron)
Date: Mon, 16 Jun 2003 09:08:18 +0100
Subject: [R] grid background color
References: <seed9bc1.076@wisemail.weizmann.ac.il>
Message-ID: <002b01c333de$7202d390$8b224381@sbs.ox.ac.uk>

Either use lset(col.whitebg()) to change a range of settings, including the
background colour to white.

Or, if you really only want to change the background colour, something like

b <- trellis.par.get("background")
b$col <- "white"
trellis.par.set("background",b)

should do the trick.

David

----- Original Message -----
From: "Ron Ophir" <lsophir at wisemail.weizmann.ac.il>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 16, 2003 8:27 AM
Subject: [R] grid background color


> Dear all,
> I am trying to run cloud function in lattice library under r-1.7, and I
wlways get gray background. I've tried to chang it by set
"...par.box=list(col="white")"
> or by gpar(background="white") but no result.
> How do i do it?
> Thanks in advance,
> Ron
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From pavlicov at stat.ohio-state.edu  Mon Jun 16 10:34:36 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Mon, 16 Jun 2003 04:34:36 -0400 (EDT)
Subject: [R] colors in bwplot with jpeg device
In-Reply-To: <200306141957.h5EJvAtw019530@erdos.math.unb.ca>
Message-ID: <Pine.SOL.4.33.0306160427410.9150-100000@spatial.stat.ohio-state.edu>


Hello all,

I would appreciate a lot your help - thank you...

I have a problem with printing a jpeg pic of bwplots. I would like to
have a background color "#E6E6E6" and all lines in graph color "#8c0200".

#########################################
power.ch <- matrix(rnorm(90, 0,1), 30,3)
alpha.ch <- c(".010", ".050", ".100")

jpeg(file="help.please.jpg", width=300, height=300, pointsize=36,
     quality=95, bg="#E6E6E6")
print.trellis(bwplot((power.ch)~matrix(rep(alpha.ch, rep(30,3)),30,3, byrow=F),
		 box.ratio=1,cex=2,
                 horizontal=F, cex.axis=2, cex.lab=2,
                 xlab="", ylab="", col.bg="#E6E6E6", fill ="#8c0200",
                scales=list(x=list(cex=3), y=list(cex=3))))
dev.off()
##########################################

Could you please let me know how to change the background and how to
change the blue outline of the boxplots into me specified color?

Thank you a lot.

Martina Pavlicova

--------------------------------------------------------------------------
Department of Statistics             Office Phone: (614) 292-1567
1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov



From gb at stat.umu.se  Mon Jun 16 11:20:07 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 16 Jun 2003 11:20:07 +0200 (CEST)
Subject: [R] new package: eha
Message-ID: <Pine.LNX.4.44.0306161115420.8790-100000@tal.stat.umu.se>

A few days ago I uploaded to CRAN a new package called 'eha', which 
stands for 'Event History Analysis'. Its main focus is on proportional 
hazards modeling in survival analysis, and in that respect  eha  can
be regarded as a complement and an extension to the 'survival'
package. In fact  eha  requires  survival. Eha contains three functions
for proportional hazards analysis:

1. 'coxreg': Performs Cox regression, almost as 'coxph' in survival. 
There are two methods, 'efron' (default) and 'breslow', exactly as in
coxph. There are two extensions, compared to  coxph: (i) Sampling of 
survivors in risk sets (at event times), which can be useful with 
huge data sets and few events. (ii) The so-called 'weird bootstrap':
For the fitted model, new events are drawn in each risk set with 
probabilities given by the fitted model, independently between 
risk sets (that's the 'weird' part). This is repeated  R  times
and the output is two Rxp matrices, one with the bootstrap estimates 
of the regression coefficients, and one with the corresponding 
standard errors. The analysis is up to the user for now.
The 'boot' package?

2. 'mlreg': A discrete time proportional hazards model is fitted along 
the lines of Kalbfleisch & Prentice (1980, pp. 98--103). See also 
Brostr?m (2002): "Cox regression; Ties withot tears", Communications 
in Statistics, Theory & Methods 31, 285--297. This function has two methods;
"ML", the purely discrete model with one parameter per observed distinct 
event time, and "MPPL", which is a hybrid between Cox regression and 
the discrete model: Only tied event times are associated to a unique
parameter; the untied event times contributes a "Cox regression term".
For completely untied data this results in ordinary Cox regression.
"MPPL" can be regarded as an attempt to handle tied data in Cox regression, 
comparable to the 'efron' method. This method does not break down because 
of too heavily tied data, which the  efron  method might do.    
 
3. 'weibreg': Weibull regression for left truncated and right censored 
data. Allows for stratification with different shape and scale parameters 
in the strata. 

Moreover, there are functions for extracting subsamples as 'rectangles'
in the Lexis diagram, including external ('communal') covariates in a
'survival data frame', extracting information from risk sets, summary 
statistics from the Lexis diagram, etc, etc.

G?ran
---
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From adrian at asteridia.maths.uwa.edu.au  Mon Jun 16 11:52:38 2003
From: adrian at asteridia.maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 16 Jun 2003 17:52:38 +0800
Subject: [R] extension to plot.formula?
Message-ID: <16109.37734.908035.71883@asteridia.maths.uwa.edu.au>

Could I suggest the following extension to plot.formula:

      plot(cbind(y1,y2) ~ x, ...)

should plot (y1 against x) and (y2 against x) on the same plot.

The default y axis limits would be determined by the range of c(y1,y2). 

This would be pretty handy sometimes, replacing 4 lines of code.

The current plot.formula evaluates cbind(y1,y2), which is a matrix,
so plot.formula looks for "plot.matrix", in vain, then calls plot.default
which gives an error because length(cbind(y1,y2)) is greater than length(x).

I suggest that plot.formula be changed to recognise the situation where
there is more than one response variable, and produce multiple
superimposed plots in that case. (It already recognises the case
where there are multiple non-response variables).

----
Adrian Baddeley, Mathematics & Statistics, University of Western Australia
		<http://maths.uwa.edu.au/~adrian/>



From dave at evocapital.com  Mon Jun 16 12:02:17 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Mon, 16 Jun 2003 11:02:17 +0100
Subject: [R] RDCOM Client: processes not terminating
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D01B817@mail.internal.net>

Hi Duncan

Thanks for the reply. Unfortunately, the Excel process still persists
indefinitely even after removing the handle E and issuing the gc()
command -- even after closing R, too. For the moment, I have found a way
round this by issuing a  shell command to "kill" the process.

Cheers,

David

-----Original Message-----
From: Duncan Temple Lang [mailto:duncan at research.bell-labs.com] 
Sent: 13 June 2003 20:01
To: David Khabie-Zeitoune
Subject: Re: [R] RDCOM Client: processes not terminating



Hi David.

 There may well be an extra reference in R to the Excel application that
keeps it hanging around.  Can you try the following and let me know 
if you still see the same problem:

 1) make certain there are not Excel processes running

 2) Create an Excel from R and quit

   E <- COMCreate("Excel.Application")
   E[["Visible"]] <- TRUE
   E$Quit()

 3)  rm(E)
 
 4) gc()

 5) wait a 30 seconds or so

 6) check if the Excel instance is still running.



Thanks.

 D.

David Khabie-Zeitoune wrote:
> Hello
> 
> I am using Duncan Lang's RDCOM Client package (available on
> omegahat.org) under R 1.7.0 and Windows XP Pro.
> 
> Is this the right forum for questions about this package? In case it 
> is, here is my question:
> 
> Instances of COM objects do not seem to terminate as expected, but 
> leave residual processes running. For example, if I try the simple 
> example:
> 
> E <- COMCreate("Excel.Application")
> E[["Visible"]] <- TRUE
> E$Quit()
> 
> An Excel application is created and pops up visibly. The E$Quit() 
> command appears to close the application down as expected, but an 
> EXCEL process is still left running in the background (as indicated by

> e.g. the Windows Task Manager).
> 
> Is there a way to cleanly exit the COM instance and shut down the 
> associated process?
> 
> Thanks,
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From dieter.menne at menne-biomed.de  Mon Jun 16 12:10:15 2003
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 16 Jun 2003 12:10:15 +0200
Subject: [R] Isocontour-lines of spatial data on a rectangular grid (not
	plots!)
Message-ID: <JLEPLGAANFCEAEDCAGJNEEFECFAA.dieter.menne@menne-biomed.de>

Dear R-Listers,

I have spatial data on an equidistant rectangular grid, similar to
topographic data. I know that there are quite a few R-packages or base
functions that provide nice iso-contours plot, but I don't want a plot, just
the smoothed isocontour line of ONE level (e.g. 10 mm).

Data sets are large, so it would be preferable if the availability of
regular grid data could be exploited, and the fact that I only need ONE
isocontour line.  Polynomial fits are not adequate, but loess works nicely
for the plots. Akima/interp might be a little too general, and if I remember
Brian Ripley's comments correctly, the coefficients are "only for internal
use".

Can someone suggest me a package?


Dieter Menne



From fischer at intellektik.informatik.tu-darmstadt.de  Mon Jun 16 12:38:58 2003
From: fischer at intellektik.informatik.tu-darmstadt.de (Thomas Fischer)
Date: Mon, 16 Jun 2003 12:38:58 +0200
Subject: [R] stop criterion for stepAIC
Message-ID: <200306161238.59058.fischer@intellektik.informatik.tu-darmstadt.de>

Hello,

I am using the function stepAIC (library MASS) to run a backward 
elimination on my linear regression. The new model stepAIC calculates 
contains coefficients that have a Pr(>|t|) value below 0.1, but I'd 
like to have only coefficients with 0.001 or below.
How can I change the stop criterion for stepAIC, so that it is more 
strict? There is a parameter "steps", but it is only a upper bound of 
steps.

Greetings,
Thomas Fischer



From p.dalgaard at biostat.ku.dk  Mon Jun 16 12:48:41 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 16 Jun 2003 10:48:41 -0000
Subject: [R] R 1.7.1 is released
Message-ID: <x2n0gil26b.fsf@biostat.ku.dk>

I've rolled up R-1.7.1.tgz a short while ago. This is a patch version
mainly fixing up an assortment of issues (see below). Note also that
it comes with an updated set of recommended packages, fixing a couple
of nasty issues in at least foreign and nlme.

You can get it from

http://cran.us.r-project.org/src/base/R-1.7.1.tgz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies. 

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

833c2a9a62818211f2f9f0353ecd9bbd  R-1.7.1.tgz
0c3029b42f8eba12cc9acab6f28196dc  R-1.7.1.tgz-split.aa
3aafef440adfa65d1dd34b50399736d2  R-1.7.1.tgz-split.ab
388544d79c50b294783838ff8d3646e2  R-1.7.1.tgz-split.ac
f1f6e913030568b3416b9fe09d486aa6  R-1.7.1.tgz-split.ad
191cfeb78a2f98f084def26b4716e3ed  R-1.7.1.tgz-split.ae
73cf3ede9066716568219a8b13765fcf  R-1.7.1.tgz-split.af
65dc2729eaa13e91e867b2624cb80e11  R-1.7.1.tgz-split.ag


        For the R Core Team,

        Peter D.


Here's the relevant part of the NEWS file:

                CHANGES IN R VERSION 1.7.1


NEW FEATURES

    o   The help pages give appropriate references to the Blue,
        White or Green books for functions based on the descriptions
        of S functions given there. (E&OE)

    o   Function getAnywhere() can find non-exported objects, for
        namespaces or registered methods.


DEPRECATED & DEFUNCT

    o   The (unimplemented) argument 'white' of parse() is deprecated.

    o   The tkfilefind demo in the tcltk library is deprecated, since
        it never worked well, and apparently not at all with Tcl/Tk 8.4.


BUG FIXES

    o   print.table() used too much white space in some cases in 1.7.0.

    o   selectMethod() failed if `f' was a non-generic and
        optional=TRUE, and gave a confusing error message if
        optional=FALSE.

    o   pchisq(*, ncp) and qchisq(*, ncp) work in more cases for large ncp
        or quantile and give warning or error messages otherwise.

    o   str(x) now also works when x is an "externalptr" (or "weakref").

    o   rbeta(), rf(), and rt() now support infinite parameter values;
        other distributions return NaN instead of NA for such.

    o   Redefining a class is now safer if the new definition
        generates an error (previously some invalid metadata could be
        left behind).

    o   A number of errors are now caught in setClass() that
        previously either went unchecked or waited until new() to
        appear:
        - classes may not contain themselves, directly or indirectly;
        - classes appearing either as slots or as superclasses must
          themselves be defined;
        - slot names (direct or inherited) must be unique.
        In related changes, prototype() now works as documented, and is the
        recommended way to provide prototype objects.

    o   Sorting an ordered factor would return an unordered one.
        This caused some trouble with panel.superpose (PR#974).

    o   methods() could return duplicates if a method in a namespace
        was both exported and registered.

    o   The internal zip.unpack() could crash if more than 500 files
        were to be extracted. (PR#2818)

    o   The "r+" and "r+b" modes of opening file connections disallowed
        writing.

    o   library() now warns the user if the chosen package name
        doesn't match the internal package name, and corrects the
        error. (PR#2816)

    o   qr(LAPACK=TRUE) (and qr for complex arguments) might have failed
        to pivot for rank-deficient inputs. (PR#2867)

    o   Only re-mapped symbols are exported by regex.o, to avoid
        problems with embedded R on RedHat 9.

    o   arima() did not set transform.pars to FALSE if AR parameters
        were fixed, although it claimed to.

    o   pnorm() was slower than necessary in the outer tails in some
        cases due to a typo in the improvements from PR#699.  (PR#2883)

    o   setGeneric() and setMethod() now catch some examples where the
        generic and the method have different argument lists; the
        evaluator checks for internal consistency of these argument lists.

    o   expand.grid(x) {the rare case of one argument} now treats factor
        levels as in the typical case of two or more arguments.

    o   Some implicit coercions to lists could cause segfaults, e.g.
          x <- matrix(nrow=20000, ncol=20); x$any <- numeric(0)
        due to a PROTECT bug. (PR#2923)

    o   The replacement functions for colnames() and rownames() did not
        work for arrays with more than two dimensions.  They could
        create dimnames of the form list(NULL, NULL) rather than
        remove the dimnames attribute.

    o   termplot() gave incorrect answers with rug=TRUE or
        partial=TRUE for factors whose levels were not in
        lexicographical order.

    o   A serious performance flaw in as() computations was fixed (the
        methods were not being cached properly.)

    o   model.frame(~1, data) always returned 1 row. (PR#2958)

    o   The data editor was truncating objects to 65535 rows.  Pro
        tem, editing objects with more than 65535 rows is an error,
        and objects cannot be extended beyond that row. This restriction
        will be removed in 1.8.0.  (PR#2962)

    o   A bug could produce apparent loops in formal method selection
        when inheritance was restricted (used for the as() function).
        A related problem sometimes broke attaching a package that had
        methods for basic functions, such as names(), used in method
        selection.

    o   Empty expressions as in return(x,) could generate subsequent
        segfaults: they are now errors.  (PR#2880)

    o   The Kinderman-Ramage Normal Random Generator had several
        problems leading to not-quite normally distributed variates
        (PR#2846).  One problem was traced to an error in the original
        1976 JASA paper!  Thanks to Josef Leydold and his team for
        investigating this. The old generator has been retained for
        reproducibility of older results, under the name
        "Buggy Kinderman-Ramage".  A warning is issued if you select it
        (also indirectly via RNGversion()).

    o   promptMethods() now puts the \alias lines for methods in the
        normal place, near the top of the file, and quotes class
         names in signatures.

    o   getS3method() and methods() were not finding methods for
        coefficients() and fitted.values() (which dispatch on "coef"
        and "fitted" respectively).

    o   scan() (and hence read.table) was not finding matches for
        separator chars with the upper bit set.  (PR#3035)

    o   lm.(w)fit failed if the fit had rank 0.

    o   lqs() did not report explicitly that it had failed if all
        samples gave singular fits.

    o   predict.lm(*, se=TRUE) {w/ weights, w/o newdata} now gives correct
        SE's.  (PR#3043)

    o   cor.test(x, y, method="spearman") now also works for
        length(x) > 1290.

    o   Matrices were printed mis-aligned if right=TRUE and na.print
        was specified.  (PR#3058)

    o   R CMD check gives now a clearer message when latex produces
        errors on the package manual.  (PR#3070)

    o   isSeekable() was incorrectly returning FALSE on all file connections.

    o   tkpager() wasn't quite using its title and header arguments in
        the way prescribed by file.show()

    o   legend(*, pch=p, lty=l) now works better when `p' or `l' have
        NAs.

    o   All braces in regular expressions used by Sweave() are now
        escaped by a backslash.

    o   unloadNamespace() failed because getNamespaceImports() now coerces a
        string argument to a name space.

    o   deriv3 gave incorrect Hessians for some very simple
        expressions such as expression(x*y) (since the comments in the
        C code were incorrect).  (PR#2577)

    o   power.t.test(..., delta=NULL,alternative='two.sided') failed. (PR#2993)

    o   Lines on postscript() plots with thousands of segments might
        have been plotted inaccurately in 1.7.0. (PR#3132)

        Solid lines in postscript() output are split into groups of 1000
        segments to help some PostScript interpreters (typically old
        level-1 interpreters).

    o   cut.POSIXt failed when the breaks were date/time objects. (PR#3181)

    o   Usage of methods in dist.Rd is now correctly documented
        (as.matrix.dist() is not an exported symbol).

    o   The predict() method for ar fits was not retrieving the series
        from the parent environment.

    o   eigen() and La.eigen() were not returning a matrix of
        eigenvectors for a 1x1 input.

    o   hsv() and rgb() now return character(0) when one of their args has
        length 0.  This also fixes terrain.color(1). (PR#3233)

    o   [[<-.data.frame checked if a replacment was too short, but not
        if it was too long.  (related to PR#3229)

    o   qt(x, df) was quite inaccurate for df=1+epsilon; it is now much more
        accurate for df in (1,2) and more precise for other df. (PR#2991)

    o   qbeta() now has slightly improved C code in two places, as suggested
        in the 2nd followup to PR#2894.





-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Jun 16 12:53:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jun 2003 11:53:31 +0100 (BST)
Subject: [R] stop criterion for stepAIC
In-Reply-To: <200306161238.59058.fischer@intellektik.informatik.tu-darmstadt.de>
Message-ID: <Pine.LNX.4.44.0306161146360.15682-100000@gannet.stats>

You completely misunderstand: have you even *looked* at the help page?

Description:

     Performs stepwise model selection by exact AIC.

!  If you don't know what AIC is (and it seems so), please research it
(e.g. in MASS the book whose support software you are using without any 
credit).

On Mon, 16 Jun 2003, Thomas Fischer wrote:

> I am using the function stepAIC (library MASS) to run a backward 
> elimination on my linear regression. The new model stepAIC calculates 
> contains coefficients that have a Pr(>|t|) value below 0.1, but I'd 
> like to have only coefficients with 0.001 or below.

Nothing to do with AIC, and very dangerous since stepAIC works with terms, 
not coefficients (and a term may have several coefficients).

> How can I change the stop criterion for stepAIC, so that it is more 
> strict? There is a parameter "steps", but it is only a upper bound of 
> steps.

The only thing you can do is increase `k', for a pseudo-AIC.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Mon Jun 16 13:01:54 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 16 Jun 2003 13:01:54 +0200 (CEST)
Subject: [R] Isocontour-lines of spatial data on a rectangular grid (not
	plots!)
In-Reply-To: <JLEPLGAANFCEAEDCAGJNEEFECFAA.dieter.menne@menne-biomed.de>
Message-ID: <Pine.LNX.4.44.0306161255200.32625-100000@reclus.nhh.no>

On Mon, 16 Jun 2003, Dieter Menne wrote:

> Dear R-Listers,
> 
> I have spatial data on an equidistant rectangular grid, similar to
> topographic data. I know that there are quite a few R-packages or base
> functions that provide nice iso-contours plot, but I don't want a plot, just
> the smoothed isocontour line of ONE level (e.g. 10 mm).
> 
> Data sets are large, so it would be preferable if the availability of
> regular grid data could be exploited, and the fact that I only need ONE
> isocontour line.  Polynomial fits are not adequate, but loess works nicely
> for the plots. Akima/interp might be a little too general, and if I remember
> Brian Ripley's comments correctly, the coefficients are "only for internal
> use".
> 
> Can someone suggest me a package?
> 
A similar question:

https://www.stat.math.ethz.ch/pipermail/r-help/2003-May/032907.html

was asked by Renaud Lancelot in late May. The source package for clines is 
at:

http://www.stat.auckland.ac.nz/~paul/R/clines_1.0.tar.gz

which returns chosen contours from an input matrix as used in contour().

Roger

> 
> Dieter Menne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From rsadler at agric.uwa.edu.au  Mon Jun 16 13:23:08 2003
From: rsadler at agric.uwa.edu.au (rohan sadler)
Date: Mon, 16 Jun 2003 19:23:08 +0800
Subject: [R] Isocontour-lines of spatial data on a rectangular grid (not
	plots!)
References: <JLEPLGAANFCEAEDCAGJNEEFECFAA.dieter.menne@menne-biomed.de>
Message-ID: <3EEDA89C.4000609@agric.uwa.edu.au>

Hi Dieter,

The "sm" (smoothing methods library) of Bowman and Azzalini (Bowman, AW 
& Azzalini A (1997) Applied Smoothing Techniques for Data Analysis: The 
Kernel Approach with S-Plus Illustrations, Clarendon Press, Oxford).

Check out pages 9 and 10 (Figure 1.8).

I don't know how large is large for you as a data set, so I can't say 
how well it goes. I would say it goes as well as anything else.

Rohan Sadler

Dieter Menne wrote:

>Dear R-Listers,
>
>I have spatial data on an equidistant rectangular grid, similar to
>topographic data. I know that there are quite a few R-packages or base
>functions that provide nice iso-contours plot, but I don't want a plot, just
>the smoothed isocontour line of ONE level (e.g. 10 mm).
>
>Data sets are large, so it would be preferable if the availability of
>regular grid data could be exploited, and the fact that I only need ONE
>isocontour line.  Polynomial fits are not adequate, but loess works nicely
>for the plots. Akima/interp might be a little too general, and if I remember
>Brian Ripley's comments correctly, the coefficients are "only for internal
>use".
>
>Can someone suggest me a package?
>
>
>Dieter Menne
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
PhD Student, Ecosystems Research Group (ERGO)
School of Plant Biology (Botany), Faculty of Natural & Agricultural Sciences,
The University of Western Australia, 35 Stirling Highway, Crawley  WA  6009, Australia

Ph:  +61 8 9380 7914
Fax: +61 8 9380 7925
email: rsadler at agric.uwa.edu.au
ERGO's web site:<http://www.botany.uwa.edu.au/ergo>



From arv at ono.com  Mon Jun 16 13:38:28 2003
From: arv at ono.com (antonio rodriguez)
Date: Mon, 16 Jun 2003 13:38:28 +0200
Subject: [R] Error in .C("open_netcdf"
Message-ID: <IPEFKICOHOECENGJBAGLKEKCCBAA.arv@ono.com>

Hi,

Running R 1.7.0 on Win XP I'm getting the following error message:


>library(netCDF)
>a<-read.netCDF("c:/data/fnmoc/individuales/sst_rey.nc")
Error in .C("open_netcdf", filename, id = as.integer(verbose)) :
        C/Fortran function name not in load table

Don't know what is going wrong. With R1.6.2 I didn't face this problem

Thanks in advance

Antonio Rodr?guez
---



From andy_liaw at merck.com  Mon Jun 16 14:06:42 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 Jun 2003 08:06:42 -0400
Subject: [R] extension to plot.formula?
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FC15@usrymx25.merck.com>

This is typically done with matplot(x, cbind(y1, y2), ...), so there's only
one line of code needed.  Perhaps you want a formula interface for matplot?

Andy

> -----Original Message-----
> From: Adrian Baddeley [mailto:adrian at asteridia.maths.uwa.edu.au] 
> Sent: Monday, June 16, 2003 5:53 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] extension to plot.formula?
> 
> 
> Could I suggest the following extension to plot.formula:
> 
>       plot(cbind(y1,y2) ~ x, ...)
> 
> should plot (y1 against x) and (y2 against x) on the same plot.
> 
> The default y axis limits would be determined by the range of 
> c(y1,y2). 
> 
> This would be pretty handy sometimes, replacing 4 lines of code.
> 
> The current plot.formula evaluates cbind(y1,y2), which is a 
> matrix, so plot.formula looks for "plot.matrix", in vain, 
> then calls plot.default which gives an error because 
> length(cbind(y1,y2)) is greater than length(x).
> 
> I suggest that plot.formula be changed to recognise the 
> situation where there is more than one response variable, and 
> produce multiple superimposed plots in that case. (It already 
> recognises the case where there are multiple non-response variables).
> 
> ----
> Adrian Baddeley, Mathematics & Statistics, University of 
> Western Australia
> 		<http://maths.uwa.edu.au/~adrian/>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From jfox at mcmaster.ca  Mon Jun 16 14:07:57 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 16 Jun 2003 08:07:57 -0400
Subject: [R] Missing data augmentation
In-Reply-To: <8AAAD5A2-9E95-11D7-8996-0005026E2B43@vanderkogel.net>
Message-ID: <5.1.0.14.2.20030616074336.01e82a08@mcmail.cis.mcmaster.ca>

Dear Jonck,

I was hoping that someone with more experience with mice and norm would 
pick up this question, but perhaps the following will help:

Without seeing your data, it's hard to determine the source of the problem; 
of course, I wouldn't necessarily be able to do that even with the data.

At 08:25 PM 6/14/2003 +0200, Jonck van der Kogel wrote:
>Hi all,
>A short while ago I asked a question about multiple imputation and I got 
>several helpful replies, thanks! I have untill now tried to use the 
>packages mice and norm but both give me errors however.
>
>mice does not even run to start with and gives me the following error 
>right away:
>iter imp variable
>   1   1  Liquidity.ratioError in chol((v + t(v))/2) : the leading minor 
> of order 1 is not positive definite
>
>To be honest I have no idea whatsoever what that error message means, so 
>my experiments with mice were shortlived :-)

If I remember correctly, leading minors are determinants of square 
submatrices starting at row and column 1; the leading minor of order 1 is 
therefore just the entry in the first row, first column; for it to be "not 
positive definite" suggests that it is 0 or negative. What exactly v is I 
can't say, but using traceback() might help you locate the problem more 
specifically. Addressing questions to the authors of mice might also help.

>I then tried the package "norm". I got some ways with the experiment, 
>following the help file:
>s <- prelim.norm(as.matrix(myDataSet))
>thetahat <- em.norm(s)
>rngseed(1234567)
>theta <- da.norm(s, thetahat, steps=20, showits=TRUE)
>
>At this stage however I get the following error:
>Steps of Data Augmentation:
>1...2...Error: NA/NaN/Inf in foreign function call (arg 2)
>
>This seems strange to me, since the whole purpose of this routine is to 
>work with NA values. So why is it complaining about NA values?

Actually, the error message is less specific than that and suggests a 
numerical problem in the data augmentation step. Since both programs are 
producing numerical errors, I'd suspect some problem, such as 
ill-conditioning, in the data.


>After this I got it to work in an unlikely fashion: I first standardized 
>my dataset using scale(). After that I was able to run the
>"theta <- da.norm(s, thetahat, steps=20, showits=TRUE)" line succesfully. 
>Which seems strange to me, since s still creates NA values, so why is it 
>not complaining about them this time. I have repeated the process several 
>times, with subsets of my original dataset and the same problems arise 
>each time.

It's odd that scaling the data helps since I believe that norm does this 
itself.

>Standardizing, calculating the missing values, imputing them and then 
>standardizing again does not seem the correct way to go to me however. In 
>my opionion the correct way of doing things would be to impute the missing 
>values and then standardize the dataset. In other words, the way that 
>seems correct to me is not working.

I'm not sure that I follow that. You can always undo the standardization at 
the end, but perhaps I'm missing something.

I hope that these remarks are of some use,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From B.Rowlingson at lancaster.ac.uk  Mon Jun 16 14:24:22 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 16 Jun 2003 13:24:22 +0100
Subject: [R] extension to plot.formula?
In-Reply-To: <16109.37734.908035.71883@asteridia.maths.uwa.edu.au>
References: <16109.37734.908035.71883@asteridia.maths.uwa.edu.au>
Message-ID: <3EEDB6F6.8090001@lancaster.ac.uk>

Adrian Baddeley wrote:
> Could I suggest the following extension to plot.formula:
> 
>       plot(cbind(y1,y2) ~ x, ...)
> 
> should plot (y1 against x) and (y2 against x) on the same plot.
> 

  Trellis - er - lattice graphics uses a similar approach, but you use 
'+', and give another argument, allow.multiple:

xyplot(y1+y2~x,data=xyy,allow.multiple=T)

  I'm no great user of lattice graphics, so I leave it to someone else 
to tell us how to join the dots with lines.

  The cbind() approach has a precedent in glm(cbind(y,n)~x,family=binomial)

  Although I've always found plotting with formulas a bit ugly, since 
the 'x' and 'y' are in reverse alphabetic order and I could be plotting 
things that conceptually have no formulaic relationship - x and y 
geographic coordinates, for instance. In a modelling context you can 
read the ~ as 'depends on', but not necessarily in a plot!

Baz



From dmurdoch at pair.com  Mon Jun 16 14:42:58 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 16 Jun 2003 08:42:58 -0400
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <IPEFKICOHOECENGJBAGLKEKCCBAA.arv@ono.com>
References: <IPEFKICOHOECENGJBAGLKEKCCBAA.arv@ono.com>
Message-ID: <pjerevogb3uf7mui9uljsk4uarjg8embsv@4ax.com>

On Mon, 16 Jun 2003 13:38:28 +0200, you wrote:

>Hi,
>
>Running R 1.7.0 on Win XP I'm getting the following error message:
>
>
>>library(netCDF)
>>a<-read.netCDF("c:/data/fnmoc/individuales/sst_rey.nc")
>Error in .C("open_netcdf", filename, id = as.integer(verbose)) :
>        C/Fortran function name not in load table

This means that the associated DLL wasn't loaded properly, so a
function in it is not being found.  How did you install the package?  

>Don't know what is going wrong. With R1.6.2 I didn't face this problem

Generally questions about contributed packages should go to the
maintainer (in this case Thomas Lumley <tlumley at u.washington.edu>),
but in this case it sounds like a problem with installation on
Windows, so he might not have experience with it.

Duncan Murdoch



From bates at stat.wisc.edu  Mon Jun 16 14:53:49 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 16 Jun 2003 12:53:49 -0000
Subject: [R] R 1.7.1 is available via rsync
Message-ID: <6rhe6qw5d2.fsf@bates4.stat.wisc.edu>

The sources for R 1.7.1 are now available via rsync as
                    rsync.r-project.org::r-release



From dvumani at hotmail.com  Mon Jun 16 15:22:08 2003
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Mon, 16 Jun 2003 13:22:08 +0000
Subject: [R] Hmisc multiple imputation functions
Message-ID: <Law11-F76Z0VAxdpb6g000a2dfa@hotmail.com>

Dear all;

I am trying to use HMISC imputation function to perform multiple imputations 
on my data and I keep on getting errors for the code given in the help 
files.

When using "aregImpute" the error is;

>f <- aregImpute(~y + x1 + x2 + x3, n.impute=100)
Loading required package: acepack
Iteration:1 Error in .Fortran("wclosepw", as.double(w), as.double(x), 
as.double(runif(lw)),  :
        C/Fortran function name not in load table

When I use "transcan" it is;

>f  <- transcan(~y + x1 + x2, n.impute=10, shrink=FALSE)
Error in transcan(~y + x1 + x2, n.impute = 10, shrink = FALSE) :
        Must specify data= when using R

I am not sure what I am missing.

Vumani



From michael.watson at bbsrc.ac.uk  Mon Jun 16 15:51:46 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 16 Jun 2003 14:51:46 +0100
Subject: [R] Using jpeg() function over cgi
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0071C@cl-exsrv1.irad.bbsrc.ac.uk>

OK, thanks for all your help :-D
  
Sorry for not observing normal e-mail help list rules - I am running SUSE 8.1 professional.

If someone coule try and explain a little as to why jpeg() can be used when I run the script as the user httpd runs as, and why it can't when it is run from a cgi script by the same user, then I may be able to get a handle on the problem.  

I have tried using bitmap() instead but quite frankly the images produced are just not of the same quality, either using type="jpeg" or type="png256" and messing around with resolution - they just look more grainy and the circles are, well, more square.

So i guess my REAL question is how do i get images on the same quality over cgi if I am not using the jpeg() function?

And in theory it is an R question, as my linux sys/admin doesn't have a clue why we're getting X11 error message over cgi because he hasn't got a clue how the jpeg() R function works.  The documentation states:

"they may not be usable unless the X11 display is available to the owner of the R process"

Well i run httpd as a user called "base" and if i log in to a shell as base, trust me the X11 display IS available then.  Sooooo, if someone could explain WHY jpeg() won't work over cgi, then i can explain to my sys/admin and he can fix it for me.

OR alternatively, if someone can explain to me how to coax bitmap() into producing images of the quality of jpeg()...??

Thanks
Mick

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 13 June 2003 16:02
To: michael watson (IAH-C)
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Using jpeg() function over cgi


Yes, it has been solved and discussed in the R-help archives many times.
The help(jpeg) page is pretty explicit too.  One alternative is bitmap().

To use jpeg() under a Unix-alike you need to set up an X server that your
R process can use.  It's a bit hard to help you do that when you don't
even mention your OS (I am inferring it is a Unix-alike), but you may be
able to solve the permissions problem or you may be able to set by a
server by something like Xvfb.  In any case, it is not an R problem


On Fri, 13 Jun 2003, michael watson (IAH-C) wrote:

> I have seen a few posts to this list regarding problems accessing the
> x11() device over cgi - namely, when trying to create a graphic using

No, not the x11() device, but the jpeg() device.  They are not the same!

> the jpeg() function, everything is fine from the command line but it
> won't work over cgi, producing the error:
> 
> "Unable to open connection to X11 display"
> 
> Has anyone actually solved this particular problem satisfactorily?
> 
> Please reply direct to me as I am not a member of the list (yet!)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Jun 16 15:56:49 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 16 Jun 2003 06:56:49 -0700 (PDT)
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <pjerevogb3uf7mui9uljsk4uarjg8embsv@4ax.com>
Message-ID: <Pine.A41.4.44.0306160655350.240824-100000@homer04.u.washington.edu>

On Mon, 16 Jun 2003, Duncan Murdoch wrote:

>
> Generally questions about contributed packages should go to the
> maintainer (in this case Thomas Lumley <tlumley at u.washington.edu>),
> but in this case it sounds like a problem with installation on
> Windows, so he might not have experience with it.
>

He doesn't.  Does the Windows version include libnetcdf, and if not, what
would the symptoms be if it were not present?

	-thomas



From arv at ono.com  Mon Jun 16 16:23:50 2003
From: arv at ono.com (antonio rodriguez)
Date: Mon, 16 Jun 2003 16:23:50 +0200
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <pjerevogb3uf7mui9uljsk4uarjg8embsv@4ax.com>
Message-ID: <IPEFKICOHOECENGJBAGLGEKFCBAA.arv@ono.com>

Hi,

Well, I didn't compile from sources R_1.7.0 just downloaded the executable
an run it.On R_1.6.2 (installed in the same way) netCDF runs without
problems. Seems to be a problem associated to R_1.7.0. I have installed it
within R invoking from the menu bar: "install package from CRAN".

Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Duncan Murdoch
> Enviado el: lunes, 16 de junio de 2003 14:43
> Para: antonio rodriguez
> CC: R-help; Thomas Lumley
> Asunto: Re: [R] Error in .C("open_netcdf"
>
>
> On Mon, 16 Jun 2003 13:38:28 +0200, you wrote:
>
> >Hi,
> >
> >Running R 1.7.0 on Win XP I'm getting the following error message:
> >
> >
> >>library(netCDF)
> >>a<-read.netCDF("c:/data/fnmoc/individuales/sst_rey.nc")
> >Error in .C("open_netcdf", filename, id = as.integer(verbose)) :
> >        C/Fortran function name not in load table
>
> This means that the associated DLL wasn't loaded properly, so a
> function in it is not being found.  How did you install the package?
>
> >Don't know what is going wrong. With R1.6.2 I didn't face this problem
>
> Generally questions about contributed packages should go to the
> maintainer (in this case Thomas Lumley <tlumley at u.washington.edu>),
> but in this case it sounds like a problem with installation on
> Windows, so he might not have experience with it.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> ---
> Incoming mail is certified Virus Free.
> Checked by AVG anti-virus system (http://www.grisoft.com).
> Version: 6.0.488 / Virus Database: 287 - Release Date: 05/06/2003
>
---



From Detlef.Steuer at unibw-hamburg.de  Mon Jun 16 16:28:07 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Mon, 16 Jun 2003 16:28:07 +0200 (CEST)
Subject: [R] R-base 1.7.1 RPMS for SuSE uploaded to Vienna
Message-ID: <XFMail.20030616162807.steuer@unibw-hamburg.de>

Hi!

RPMS for R-base-1.7.1 have been uploaded to Vienna a few minutes ago.
(SuSE 7.3, 8.[012])
They should appear on the CRAN mirror near you soon.

A rebuild of rpms of updated and/or new contrib packages is underway.

Happy R'ing
detlef


Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From arv at ono.com  Mon Jun 16 16:35:23 2003
From: arv at ono.com (antonio rodriguez)
Date: Mon, 16 Jun 2003 16:35:23 +0200
Subject: [R] Error in .C("open_netcdf": SOLVED
Message-ID: <IPEFKICOHOECENGJBAGLCEKGCBAA.arv@ono.com>

Hi,

The problem was the way I installed the netCDF package. Within the R
environment and invoking "Install package from CRAN" makes (at least for me)
that the netCDF library doesn't work. But if I download it and "Install from
local zip file" the problems are gone.

Cheers

Antonio Rodr?guez
---



From ggrothendieck at volcanomail.com  Mon Jun 16 16:50:14 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Mon, 16 Jun 2003 07:50:14 -0700 (PDT)
Subject: [R] Programcode and data in the same textfile
Message-ID: <20030616145014.29CCDABC4@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/9ff061a5/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 16 16:51:08 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jun 2003 15:51:08 +0100 (BST)
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <IPEFKICOHOECENGJBAGLGEKFCBAA.arv@ono.com>
Message-ID: <Pine.LNX.4.44.0306161549490.19628-100000@gannet.stats>

Then the remedy is simple: do as the rw-FAQ and ReadMe's suggest and
compile it from the sources yourself.  Something is obviously different 
between your setup and mine.

On Mon, 16 Jun 2003, antonio rodriguez wrote:

> Hi,
> 
> Well, I didn't compile from sources R_1.7.0 just downloaded the executable
> an run it.On R_1.6.2 (installed in the same way) netCDF runs without
> problems. Seems to be a problem associated to R_1.7.0. I have installed it
> within R invoking from the menu bar: "install package from CRAN".
> 
> Antonio
> 
> > -----Mensaje original-----
> > De: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Duncan Murdoch
> > Enviado el: lunes, 16 de junio de 2003 14:43
> > Para: antonio rodriguez
> > CC: R-help; Thomas Lumley
> > Asunto: Re: [R] Error in .C("open_netcdf"
> >
> >
> > On Mon, 16 Jun 2003 13:38:28 +0200, you wrote:
> >
> > >Hi,
> > >
> > >Running R 1.7.0 on Win XP I'm getting the following error message:
> > >
> > >
> > >>library(netCDF)
> > >>a<-read.netCDF("c:/data/fnmoc/individuales/sst_rey.nc")
> > >Error in .C("open_netcdf", filename, id = as.integer(verbose)) :
> > >        C/Fortran function name not in load table
> >
> > This means that the associated DLL wasn't loaded properly, so a
> > function in it is not being found.  How did you install the package?
> >
> > >Don't know what is going wrong. With R1.6.2 I didn't face this problem
> >
> > Generally questions about contributed packages should go to the
> > maintainer (in this case Thomas Lumley <tlumley at u.washington.edu>),
> > but in this case it sounds like a problem with installation on
> > Windows, so he might not have experience with it.
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > ---
> > Incoming mail is certified Virus Free.
> > Checked by AVG anti-virus system (http://www.grisoft.com).
> > Version: 6.0.488 / Virus Database: 287 - Release Date: 05/06/2003
> >
> ---
> Outgoing mail is certified Virus Free.
> Checked by AVG anti-virus system (http://www.grisoft.com).
> Version: 6.0.488 / Virus Database: 287 - Release Date: 05/06/2003
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Mon Jun 16 17:18:40 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 16 Jun 2003 16:18:40 +0100
Subject: [R] Using jpeg() function over cgi
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0071F@cl-exsrv1.irad.bbsrc.ac.uk>

Again, thank you one and all for your help!

My web-server is apache, and simply adding the following line to httpd.conf solved my issues:

SetEnv DISPLAY :0.0

:-D

Mick

-----Original Message-----
From: michael watson (IAH-C) 
Sent: 16 June 2003 14:52
To: 'r-help at stat.math.ethz.ch'
Subject: RE: [R] Using jpeg() function over cgi


OK, thanks for all your help :-D
  
Sorry for not observing normal e-mail help list rules - I am running SUSE 8.1 professional.

If someone coule try and explain a little as to why jpeg() can be used when I run the script as the user httpd runs as, and why it can't when it is run from a cgi script by the same user, then I may be able to get a handle on the problem.  

I have tried using bitmap() instead but quite frankly the images produced are just not of the same quality, either using type="jpeg" or type="png256" and messing around with resolution - they just look more grainy and the circles are, well, more square.

So i guess my REAL question is how do i get images on the same quality over cgi if I am not using the jpeg() function?

And in theory it is an R question, as my linux sys/admin doesn't have a clue why we're getting X11 error message over cgi because he hasn't got a clue how the jpeg() R function works.  The documentation states:

"they may not be usable unless the X11 display is available to the owner of the R process"

Well i run httpd as a user called "base" and if i log in to a shell as base, trust me the X11 display IS available then.  Sooooo, if someone could explain WHY jpeg() won't work over cgi, then i can explain to my sys/admin and he can fix it for me.

OR alternatively, if someone can explain to me how to coax bitmap() into producing images of the quality of jpeg()...??

Thanks
Mick

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 13 June 2003 16:02
To: michael watson (IAH-C)
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Using jpeg() function over cgi


Yes, it has been solved and discussed in the R-help archives many times.
The help(jpeg) page is pretty explicit too.  One alternative is bitmap().

To use jpeg() under a Unix-alike you need to set up an X server that your
R process can use.  It's a bit hard to help you do that when you don't
even mention your OS (I am inferring it is a Unix-alike), but you may be
able to solve the permissions problem or you may be able to set by a
server by something like Xvfb.  In any case, it is not an R problem


On Fri, 13 Jun 2003, michael watson (IAH-C) wrote:

> I have seen a few posts to this list regarding problems accessing the
> x11() device over cgi - namely, when trying to create a graphic using

No, not the x11() device, but the jpeg() device.  They are not the same!

> the jpeg() function, everything is fine from the command line but it
> won't work over cgi, producing the error:
> 
> "Unable to open connection to X11 display"
> 
> Has anyone actually solved this particular problem satisfactorily?
> 
> Please reply direct to me as I am not a member of the list (yet!)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Dan.Kelley at Dal.Ca  Mon Jun 16 17:19:08 2003
From: Dan.Kelley at Dal.Ca (Dan E. Kelley)
Date: Mon, 16 Jun 2003 12:19:08 -0300
Subject: [R] Re: R 1.7.1 is released
In-Reply-To: <x2n0gil26b.fsf@biostat.ku.dk>
References: <x2n0gil26b.fsf@biostat.ku.dk>
Message-ID: <3EEDDFEC.1010005@Dal.Ca>

Peter Dalgaard BSA wrote:

>I've rolled up R-1.7.1.tgz a short while ago. 
>  
>
Thanks, Peter.  This is terrific!  I find that this R installation is 
only accessible to the root.  In case it helps other folks, I've 
attached a patch on the file
    src/library/Makefile.in
that solves the problem for me.

NOTE: I am not too sure I've done the patch right, in terms of 
filenames.  (I'm not familiar enough with diff and patch to do this sort 
of thing reliably!)  So, in words, the change is simple: find the "@cp 
-r" command of the "install:" target in src/library/Makefie.in and 
change it to "@cp -pr" so that the permissions are retained.  At least 
on my (linux/redhat9) box, this should work OK.

Dan.

-- 
Dan E. Kelley, Associate Professor                phone:(902)494-1694
Oceanography Department, Dalhousie University       fax:(902)494-2885
Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: R-patch
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/350f0927/R-patch.pl

From ligges at statistik.uni-dortmund.de  Mon Jun 16 17:34:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Jun 2003 17:34:52 +0200
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <IPEFKICOHOECENGJBAGLGEKFCBAA.arv@ono.com>
References: <IPEFKICOHOECENGJBAGLGEKFCBAA.arv@ono.com>
Message-ID: <3EEDE39C.6070609@statistik.uni-dortmund.de>

antonio rodriguez wrote:

> Hi,
> 
> Well, I didn't compile from sources R_1.7.0 just downloaded the executable
> an run it.On R_1.6.2 (installed in the same way) netCDF runs without
> problems. Seems to be a problem associated to R_1.7.0. I have installed it
> within R invoking from the menu bar: "install package from CRAN".
> 
> Antonio

Before others start asking:
The binary package netCDF for Windows is not any more available via 
"install package from CRAN". As the "Readme" tells you: The binary is 
available from Brian Ripley's page only.

I guess Antonio is talking about the recent version, 1.5, of package netCDF?

Uwe Ligges


> 
>>-----Mensaje original-----
>>De: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Duncan Murdoch
>>Enviado el: lunes, 16 de junio de 2003 14:43
>>Para: antonio rodriguez
>>CC: R-help; Thomas Lumley
>>Asunto: Re: [R] Error in .C("open_netcdf"
>>
>>
>>On Mon, 16 Jun 2003 13:38:28 +0200, you wrote:
>>
>>
>>>Hi,
>>>
>>>Running R 1.7.0 on Win XP I'm getting the following error message:
>>>
>>>
>>>
>>>>library(netCDF)
>>>>a<-read.netCDF("c:/data/fnmoc/individuales/sst_rey.nc")
>>>
>>>Error in .C("open_netcdf", filename, id = as.integer(verbose)) :
>>>       C/Fortran function name not in load table
>>
>>This means that the associated DLL wasn't loaded properly, so a
>>function in it is not being found.  How did you install the package?
>>
>>
>>>Don't know what is going wrong. With R1.6.2 I didn't face this problem
>>
>>Generally questions about contributed packages should go to the
>>maintainer (in this case Thomas Lumley <tlumley at u.washington.edu>),
>>but in this case it sounds like a problem with installation on
>>Windows, so he might not have experience with it.
>>
>>Duncan Murdoch
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>---
>>Incoming mail is certified Virus Free.
>>Checked by AVG anti-virus system (http://www.grisoft.com).
>>Version: 6.0.488 / Virus Database: 287 - Release Date: 05/06/2003
>>
> 
> ---
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Mon Jun 16 17:45:12 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 16 Jun 2003 15:45:12 -0000
Subject: [R] Using jpeg() function over cgi
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C0071C@cl-exsrv1.irad.bbsrc.ac.uk>
References: <20B7EB075F2D4542AFFAF813E98ACD9301C0071C@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <x2el1ukofv.fsf@biostat.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> Well i run httpd as a user called "base" and if i log in to a shell
> as base, trust me the X11 display IS available then. Sooooo, if
> someone could explain WHY jpeg() won't work over cgi, then i can
> explain to my sys/admin and he can fix it for me.

It's not so much a matter of user credentials, but of mode of
operation. If you login from a dumb terminal (or a Windows machine),
then you'd have the same symptom of not being able to access the X
server (after all, someone else might be using it). If you login from
a remote Linux machine, then you won't see the problem, but the X
server you're talking to is the one on your local machine, not on the
remote machine. 

What you can do is to set up an X server separate from the one that
controls the physical display but working with a virtual frame buffer
device (Xvfb) instead, then make sure that that is accessible to your
cgi process. I'm not quite sure whether more is required than running
it as user "base" (the display server generally runs as root and is
very picky about the protocol for letting other users in).
 
> OR alternatively, if someone can explain to me how to coax bitmap()
> into producing images of the quality of jpeg()...??

It has been done, I'm pretty sure. Check the archives of the mailing
list. One option is to generate bitmaps at a higher resolution and then
smooth and scale them afterwards using e.g. pnmsmooth and pnmscale or
the ImageMagick tools.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Mon Jun 16 18:01:17 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 16 Jun 2003 16:01:17 -0000
Subject: [R] Programcode and data in the same textfile
In-Reply-To: <20030616145014.29CCDABC4@sitemail.everyone.net>
References: <20030616145014.29CCDABC4@sitemail.everyone.net>
Message-ID: <x2adciknp3.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at volcanomail.com> writes:

> It would be even cleaner just to use stdin() but unfortunately
> stdin() does not work in sourced files (bug?).

Design issue. Source()'d files are checked for syntactical validity
before they are run. Saves the user from some embarrassments, although
far from all. Consequentially, it cannot allow you to include lines
that don't look like valid R.

(Another aspect of the source() design is that it cannot deal with a
potentially infinite stream of R commands. If we ever get around to
implementing a setReader() mechanism, then the embedded-data issue
might be solved simultaneously.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rossini at blindglobe.net  Mon Jun 16 18:19:20 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 16 Jun 2003 09:19:20 -0700
Subject: [long:] Re: [R] problem with latex of object summary reverse
In-Reply-To: <20030616053056.GB1570@pasteur-kh.org> (Philippe Glaziou's
	message of "Mon, 16 Jun 2003 12:30:57 +0700")
References: <20030613131302.GC782@pasteur-kh.org>
	<20030613125225.655c19a2.fharrell@virginia.edu>
	<20030614023347.GB581@pasteur-kh.org>
	<20030614171147.32b8f63f.fharrell@virginia.edu>
	<20030616053056.GB1570@pasteur-kh.org>
Message-ID: <877k7mknav.fsf@jeeves.blindglobe.net>

Philippe Glaziou <glaziou at pasteur-kh.org> writes:

> No file file66334873.aux.
>
> Underfull \hbox (badness 10000) in paragraph at lines 20--21

Not an error.

> [1] (./file66334873.aux)
>
> LaTeX Warning: Label(s) may have changed. Rerun to get cross-references right.
>

I'm confused.  Is this the error you are talking about wanting to fix?

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
Biomedical/Health Informatics and Biostatistics, University of Washington.
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From ripley at stats.ox.ac.uk  Mon Jun 16 18:32:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jun 2003 17:32:05 +0100 (BST)
Subject: [R] Re: R 1.7.1 is released
In-Reply-To: <3EEDDFEC.1010005@Dal.Ca>
Message-ID: <Pine.LNX.4.44.0306161729300.19720-100000@gannet.stats>

On Mon, 16 Jun 2003, Dan E. Kelley wrote:

> Peter Dalgaard BSA wrote:
> 
> >I've rolled up R-1.7.1.tgz a short while ago. 
> >  
> >
> Thanks, Peter.  This is terrific!  I find that this R installation is 
> only accessible to the root.  In case it helps other folks, I've 
> attached a patch on the file
>     src/library/Makefile.in
> that solves the problem for me.

I think this is already covered by the advice in R-admin.texi:

If you want the build to be usable by a group of
users, set @code{umask} before unpacking so that the files will be
readable by the target group (e.g., @code{umask 022} to be usable by all
users).

To ensure that the installed tree is usable by the right group of users,
set @code{umask} appropriately (perhaps to @samp{022}) before unpacking
the sources and throughout the build process.

Did you do that?

> 
> NOTE: I am not too sure I've done the patch right, in terms of 
> filenames.  (I'm not familiar enough with diff and patch to do this sort 
> of thing reliably!)  So, in words, the change is simple: find the "@cp 
> -r" command of the "install:" target in src/library/Makefie.in and 
> change it to "@cp -pr" so that the permissions are retained.  At least 
> on my (linux/redhat9) box, this should work OK.
> 
> Dan.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Dan.Kelley at Dal.Ca  Mon Jun 16 18:59:53 2003
From: Dan.Kelley at Dal.Ca (Dan E. Kelley)
Date: Mon, 16 Jun 2003 13:59:53 -0300
Subject: [R] Re: R 1.7.1 is released
In-Reply-To: <Pine.LNX.4.44.0306161729300.19720-100000@gannet.stats>
References: <Pine.LNX.4.44.0306161729300.19720-100000@gannet.stats>
Message-ID: <3EEDF789.1040003@Dal.Ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/0ac9df50/attachment.pl

From murdoch at stats.uwo.ca  Mon Jun 16 19:07:26 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 16 Jun 2003 13:07:26 -0400
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <Pine.A41.4.44.0306160655350.240824-100000@homer04.u.washington.edu>
References: <pjerevogb3uf7mui9uljsk4uarjg8embsv@4ax.com>
	<Pine.A41.4.44.0306160655350.240824-100000@homer04.u.washington.edu>
Message-ID: <43orev0o6mrncss4fr5cu90rgqaa0d3nq5@4ax.com>

On Mon, 16 Jun 2003 06:56:49 -0700 (PDT), you wrote in message
<Pine.A41.4.44.0306160655350.240824-100000 at homer04.u.washington.edu>:

>  Does the Windows version include libnetcdf, and if not, what
>would the symptoms be if it were not present?

Windows doesn't include it.  I just tried building from source, and it
failed because of that.

When I ask Rgui to install from CRAN, it doesn't see netCDF  ---  this
means that Uwe hasn't built the binary for it, which probably means
some manual intervention is needed.

My conclusion:  netCDF isn't available for Windows 1.7 or greater,
until someone tracks down the pieces and adds them.

Duncan Murdoch



From andy_liaw at merck.com  Mon Jun 16 19:10:42 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 16 Jun 2003 13:10:42 -0400
Subject: [R] Problem when making refman.pdf on WinXP (R 1.7.1)
Message-ID: <3A822319EB35174CA3714066D590DCD5C4FC18@usrymx25.merck.com>

Dear R-help,

Can some one tell me what could be the problem?  I downloaded the R-1.7.1
source and try to compile it on WinXP.  "make", "make bitmapdll", "make
tcl", "make recommended" all ran w/o problem.  However, when I ran "make
docs", it failed at creating refman.pdf, with no apparent error that I could
see.  I have attached the log file (refman.log), and would very much
appreciate it if some one can give me some clues as to what's wrong.  TIA!
 <<refman.log>> 
Best,
Andy 

Andy Liaw, PhD
Biometrics Research     Merck Research Labs
PO Box 2000, RY33-300      Rahway, NJ 07065
mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>      732-594-0820



------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, contains 
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
USA) that may be confidential, proprietary copyrighted and/or legally 
privileged, and is intended solely for the use of the individual or entity
named on this message. If you are not the intended recipient, and
have received this message in error, please immediately return this by 
e-mail and then delete it.
------------------------------------------------------------------------------

From Charles.Annis at statisticalengineering.com  Mon Jun 16 19:19:50 2003
From: Charles.Annis at statisticalengineering.com (Charles Annis, P.E.)
Date: Mon, 16 Jun 2003 13:19:50 -0400
Subject: [R] Constrained optimization
Message-ID: <004701c3342b$81776730$2802a8c0@DHT0TL11>

Greetings, R-Wizards: 

I'm trying to find an extremum subject to a nonlinear constraint.  (Yes, I
have perused the archives but have found nothing positive.) The details of
the problem are these:

In a paper published some years ago in Technometrics, ("Confidence bands for
cumulative distribution functions of continuous random variables"
Technometrics, 25, 77-86. 1983), Cheng and Iles describe an ingenious method
for placing confidence bounds on an entire cdf by defining the likelihood
ratio confidence "ellipse" for the model parameters, and then traversing the
periphery and finding the most extreme values for x, at a given F(x), such
that the distribution parameters reside on that confidence contour.  (Well,
their method is more sophisticated than that, but that's essentially how it
works.)  I implemented the thing in a spreadsheet 15 years ago, and would
like to do the same in R.  But EXCEL's solver can find an extremum subject
to a constraint, and I haven't figured out how to get nlm() to do that.

I would be grateful for any algorithmic suggestions.

Many Thanks.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFAX: 503-217-5849
http://www.StatisticalEngineering.com



From ligges at statistik.uni-dortmund.de  Mon Jun 16 19:18:39 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Jun 2003 19:18:39 +0200
Subject: [R] Error in .C("open_netcdf"
In-Reply-To: <43orev0o6mrncss4fr5cu90rgqaa0d3nq5@4ax.com>
References: <pjerevogb3uf7mui9uljsk4uarjg8embsv@4ax.com>	<Pine.A41.4.44.0306160655350.240824-100000@homer04.u.washington.edu>
	<43orev0o6mrncss4fr5cu90rgqaa0d3nq5@4ax.com>
Message-ID: <3EEDFBEF.207@statistik.uni-dortmund.de>

Duncan Murdoch wrote:
> On Mon, 16 Jun 2003 06:56:49 -0700 (PDT), you wrote in message
> <Pine.A41.4.44.0306160655350.240824-100000 at homer04.u.washington.edu>:
> 
> 
>> Does the Windows version include libnetcdf, and if not, what
>>would the symptoms be if it were not present?
> 
> 
> Windows doesn't include it.  I just tried building from source, and it
> failed because of that.
> 
> When I ask Rgui to install from CRAN, it doesn't see netCDF  ---  this
> means that Uwe hasn't built the binary for it, which probably means
> some manual intervention is needed.
> 
> My conclusion:  netCDF isn't available for Windows 1.7 or greater,
> until someone tracks down the pieces and adds them.
> 
> Duncan Murdoch

Right, manual intervention is needed, i.e. installing the dependencies 
of netCDF.

Please read the Readme on CRAN (CRAN/bin/windows/contrib/1.7/ReadMe) 
which says:

"The packages SJava, XML, gstat, netCDF, and xgobi are available at
  http://www.stats.ox.ac.uk/pub/RWin
kindly provided by Professor Brian D. Ripley."

Uwe Ligges



From Andre.Casadevall at dauphine.fr  Mon Jun 16 19:49:05 2003
From: Andre.Casadevall at dauphine.fr (A.J. Casadevall)
Date: Mon, 16 Jun 2003 19:49:05 +0200
Subject: [R] R1.7.0 on Solaris 9
Message-ID: <p05210603bb13b0a2c1fd@[193.51.89.170]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/21c4faef/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 16 20:08:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jun 2003 19:08:01 +0100 (BST)
Subject: [R] R1.7.0 on Solaris 9
In-Reply-To: <p05210603bb13b0a2c1fd@[193.51.89.170]>
Message-ID: <Pine.LNX.4.44.0306161906580.20346-100000@gannet.stats>

On Mon, 16 Jun 2003, A.J. Casadevall wrote:

> On Solaris 9 and gcc, R 1.7.0, configure fails.
> What to do ?

Give us a hint as to what the problem is?
Also, it will be easier to help you with the current version, 1.7.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Skip at expresstaxservice.com  Mon Jun 16 23:14:27 2003
From: Skip at expresstaxservice.com (Skip Dalrymple I.T. Director)
Date: Mon, 16 Jun 2003 17:14:27 -0400
Subject: [R] compile R on i386-sun-solaris
Message-ID: <GIEPJAPMCAKGMMDNLCFMKEMFCHAA.Skip@expresstaxservice.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/30a250c0/attachment.pl

From dmurdoch at pair.com  Tue Jun 17 01:04:45 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 16 Jun 2003 19:04:45 -0400
Subject: [R] Windows binary for 1.7.1 uploaded to CRAN
Message-ID: <36jsevkatmrbr6lrrgv6opv8t1jl98tpqg@4ax.com>

I've built the Windows binary for 1.7.1, and sent it to CRAN.  It
should be visible within a few hours, and will soon propagate to the
mirrors.

Duncan Murdoch



From stefano.iacus at unimi.it  Tue Jun 17 01:21:18 2003
From: stefano.iacus at unimi.it (Stefano Iacus)
Date: Tue, 17 Jun 2003 01:21:18 +0200
Subject: [R] Carbon R 1.7.1 on CRAN
Message-ID: <3BA35499-A051-11D7-B5A3-003065CC4CB8@unimi.it>

I have just uploaded Carbon R 1.7.1 on CRAN.
It will be available to you with the usual timing of mirroring.

Contributed packages and the BioConductore bunble will follow shortly.

stefano iacus


-----------------------------------

Stefano M. Iacus
Department of Economics
University of Milan
Via Mercalli, 23
I-20123 Milan - Italy
Ph.: +39 02 50321 461
Fax: +39 02 50321 505
------------------------------------------------------------------------ 
------------
Please don't send me Word or PowerPoint attachments if not
absolutely necessary. See:
http://www.gnu.org/philosophy/no-word-attachments.html



From glaziou at pasteur-kh.org  Tue Jun 17 03:06:58 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 17 Jun 2003 08:06:58 +0700
Subject: [long:] Re: [R] problem with latex of object summary reverse
In-Reply-To: <877k7mknav.fsf@jeeves.blindglobe.net>
References: <20030613131302.GC782@pasteur-kh.org>
	<20030613125225.655c19a2.fharrell@virginia.edu>
	<20030614023347.GB581@pasteur-kh.org>
	<20030614171147.32b8f63f.fharrell@virginia.edu>
	<20030616053056.GB1570@pasteur-kh.org>
	<877k7mknav.fsf@jeeves.blindglobe.net>
Message-ID: <20030617010658.GB579@pasteur-kh.org>

A.J. Rossini <rossini at blindglobe.net> wrote:
> Philippe Glaziou <glaziou at pasteur-kh.org> writes:
> > No file file66334873.aux.
> > Underfull \hbox (badness 10000) in paragraph at lines 20--21
> 
> Not an error.

I know, I never said this was an error.


> > [1] (./file66334873.aux)
> > LaTeX Warning: Label(s) may have changed. Rerun to get
> > cross-references right.
> 
> I'm confused.  Is this the error you are talking about
> wanting to fix?


Not at all.

The error is that the 2nd row (with N=...) produced by the
latex command on a summary(...,method='reverse') (Hmisc lib)
object is wrong in the produced tex file (on my systems,
that is, on two different machines with different versions
of Linux Debian, both with the standard tetex distribution).

> summary(y~x,method='reverse')->a

The summary command gives (within the R interface):

+-+-----------------+-----------------+-----------------+
| |a                |b                |c                |
| |(N=27)           |(N=38)           |(N=35)           |
+-+-----------------+-----------------+-----------------+
|x|0.474/0.548/0.724|0.261/0.446/0.626|0.284/0.591/0.853|
+-+-----------------+-----------------+-----------------+


> latex(a)->la 

This command produces a tex file which, once compiled, looks
like this: (the same command without the ->la part results
in an immediate compilation of a temp file, followed by a
call to xdvi, and shows the same problem)

+-------+-----------------+-----------------+-----------------+
|       |a                |b                |c                |
|(N=27) |(N=38)           |(N=35)           |                 |
+-------+-----------------+-----------------+-----------------+
|x      |0.474/0.548/0.724|0.261/0.446/0.626|0.284/0.591/0.853|
+-------+-----------------+-----------------+-----------------+

My previous message pointed out the wrong code on the tex
file. I understand that I am the only person having such a
problem, and I am really wondering about what is wrong with
my two systems, one of which is really a linux debian out of
the box. This problem is truly annoying because I want to use
these features in an automated way with Sweave.

I am really sorry for bothering the list with this.

-- 
Philippe



From mentus at gmx.de  Tue Jun 17 03:43:42 2003
From: mentus at gmx.de (Fernando Henrique Ferraz Pereira da Rosa)
Date: Tue, 17 Jun 2003 03:43:42 +0200 (MEST)
Subject: [R] Paste and namespace
Message-ID: <17699.1055814222@www32.gmx.net>

 Hi, my doubt is very simple. I'm sure I've seen someone using something
like this before, but unfortunatelly my searches in the archives were useless.
 Well, I have some objects called after a name that has a number attached to
it,
 varying. Let's say I have:
 > ls
   poly1 poly2 poly3 poly4 poly5 poly6 ...
 
 I would like to access these objects using a for(), in which I could do
something like paste("poly",i,sep=''). For i = 4, for instance I get
 > paste("poly",4,=sep'')
 [1] "poly8"
 Which is a string. What command should I use to 'access' the object named
by this string, ir order to do some useful things, like changing its value?

Thank you,

--



From Edith.Hodgen at nzcer.org.nz  Tue Jun 17 03:57:46 2003
From: Edith.Hodgen at nzcer.org.nz (Edith Hodgen)
Date: Tue, 17 Jun 2003 13:57:46 +1200
Subject: [R] Paste and namespace
Message-ID: <seef1e67.028@smtp.nzcer.org.nz>

This was discussed in 

Date: 	1/06/2003 06:00:32
Subject: 	Re: [R] parse on left hand side of R assignment

On Sat, 31 May 2003, Paul E. Johnson wrote:

> I keep finding myself in a situation where I want to calculate a 
> variable name and then use it on the left hand side of an assignment. 
> For example
> 
> iteration <- 1
> varName <- paste("run",iteration,sep="")
> myList$parse(text=varName) <- aColumn
> 
> I want to take some existing variable  "aColumn" and use the name 
> "varName" name for it and put it into a list "myList".  That use fails 
> with this error:
.... Snip 

For a data frame you could use

mydf[paste("run",iteration,sep="")] <- aColumn

and for a list or a data frame

Robject[[paste("run",iteration,sep="")]] <- aColumn

=================================================
I don't remember if there was any more interesting discussion in the thread.

HTH
Edith

Edith Hodgen
Statistician and Data Manager
New Zealand Council for Educational Research
Phone: +64-4-384 7939 x 812
Fax:      +64-4-384 7933
edith.hodgen at nzcer.org.nz

           Web site http://www.nzcer.org.nz

>>> Fernando Henrique Ferraz Pereira da Rosa <mentus at gmx.de> 17/06/2003 13:43:42 >>>
 Hi, my doubt is very simple. I'm sure I've seen someone using something
like this before, but unfortunatelly my searches in the archives were useless.
 Well, I have some objects called after a name that has a number attached to
it,
 varying. Let's say I have:
 > ls
   poly1 poly2 poly3 poly4 poly5 poly6 ...
 
 I would like to access these objects using a for(), in which I could do
something like paste("poly",i,sep=''). For i = 4, for instance I get
 > paste("poly",4,=sep'')
 [1] "poly8"
 Which is a string. What command should I use to 'access' the object named
by this string, ir order to do some useful things, like changing its value?

Thank you,

--

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Tue Jun 17 03:57:46 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 16 Jun 2003 18:57:46 -0700
Subject: [R] Paste and namespace
References: <17699.1055814222@www32.gmx.net>
Message-ID: <3EEE759A.30907@pdf.com>

Have you considered:

 > poly1 <- 1
 > get("poly1")
[1] 1
 >

hth.  spencer graves

Fernando Henrique Ferraz Pereira da Rosa wrote:
>  Hi, my doubt is very simple. I'm sure I've seen someone using something
> like this before, but unfortunatelly my searches in the archives were useless.
>  Well, I have some objects called after a name that has a number attached to
> it,
>  varying. Let's say I have:
>  > ls
>    poly1 poly2 poly3 poly4 poly5 poly6 ...
>  
>  I would like to access these objects using a for(), in which I could do
> something like paste("poly",i,sep=''). For i = 4, for instance I get
>  > paste("poly",4,=sep'')
>  [1] "poly8"
>  Which is a string. What command should I use to 'access' the object named
> by this string, ir order to do some useful things, like changing its value?
> 
> Thank you,
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From edd at debian.org  Tue Jun 17 04:17:55 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 16 Jun 2003 21:17:55 -0500
Subject: [R] Debian packages of R 1.7.1 uploaded
Message-ID: <20030617021755.GA19236@sonny.eddelbuettel.com>


Debian packages for R 1.7.1 have been uploaded to the main Debian site and
will hit the mirrors tomorrow. Autobuilders will then produce binaries for
the ten other (non-i386) architectures. Given the flawless builts of the two
most recent 1.7.1 pre-releases on these ten architectures [1], no problems are
expected.  Barring other surprises, this version should also migrate into
Debian testing within ten days. Those 'in the know' with apt-get will know
how to add it to testing before then. 

Dirk

[1] http://buildd.debian.org/build.php?pkg=r-base

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From mj.manning at niwa.co.nz  Tue Jun 17 07:34:05 2003
From: mj.manning at niwa.co.nz (Michael J. Manning)
Date: Tue, 17 Jun 2003 17:34:05 +1200
Subject: [R] Quality of graphics produced as windows metafiles
Message-ID: <3EEEA84D.4030704@niwa.co.nz>

Hi all

My details:
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.0
year     2003
month    04
day      16
language R

Please excuse my woeful ignorance, but when I copy the contents of an 
R-graphics window produced using "windows()" (or any of its close 
relations) to the clipboard as a windows metafile and then paste the 
contents into another document, typically an MS Word file, the quality 
of the graphics produced seems to vary considerably.

I can produce sharp looking results if the R-graphics window contains a 
single graph only; however, if I partition the graphics window using 
"par(mfrow=c(x,y))" or similar and draw multiple graphs, then the 
quality of the results produced by the copy and paste declines as x and 
y increase, with curves becoming increasingly pixelated etc.

On screen, as the number of graphs on the device increases, the quality 
of the graphics on screen declines, I guess, as the device has to more 
graphics with less available pixels per graph. Hence, if the content 
copied to the clipboard is just a snapshot of what's on screen, then I 
can understand the drop in quality when I paste the contents of the 
clipboard where I want them ("garbage in=garbage out" principle...).

However, if I create a windows metafile directly with 
"win.metafile(file=....)", draw multiple graphs, and close the device 
the results seem to be no better, i.e. curves pixelated etc. E.g.

win.metafile(file="test.wmf",pointsize=8,height=8.5,width=5.75)
par(mfrow=c(5,2))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
plot(density(rnorm(1000))
dev.off()

I note that the help page for "windows()" states that the number of 
pixels per inch can be specified and passed to the "windows()" device 
(and hence on to "win.metafile()" and "win.graph()") via the "xpinch" 
and ypinch" arguments. I have tried adjusting these without much succes, 
e.g.

windows(rescale="fit",xpinch=102,ypinch=77)#<-- xpinch and ypinch values 
from post to R-help, 11 Sept 2001
....
windows(rescale="fit",xpinch=204,ypinch=154)
....

All suggestions gratefully appreciated.

Cheers

MJM
-- 

Michael J. Manning
National Institute of Water and Atmospheric Research Ltd (NIWA)
Private Bag 14901, Kilbirnie,
Wellington, New Zealand
Tel +64 4 386 0851
Fax +64 4 386 0574



From ggrothendieck at volcanomail.com  Tue Jun 17 07:46:34 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Mon, 16 Jun 2003 22:46:34 -0700 (PDT)
Subject: [R] Programcode and data in the same textfile
Message-ID: <20030617054634.8F6ADE4B9@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030616/abc7b001/attachment.pl

From selwyn.mccracken at paradise.net.nz  Tue Jun 17 07:50:22 2003
From: selwyn.mccracken at paradise.net.nz (Selwyn McCracken)
Date: Tue, 17 Jun 2003 17:50:22 +1200
Subject: [R] help recoding
Message-ID: <3EEEAC1E.1020200@paradise.net.nz>

hi R-listers,

I would like some help recoding a variable. I have a dataframe 'cause' 
that translates between a set of codes:

acc	nds
-	-
1	2
3	4	
5	8
...	...

the desired result for dataframe 'p':
a
-
1
5
5

would be:
a	b
-	-
1	2
5	8
5	9

I have tried:

transform(p, b=cause$nds[cause$acc==p$a])

but for some reason it complains about the difference in length between 
the translation and result dataframes.

This prints the desired result, but I am unsure how to collect the 
results into a variable.

for (i in p) print cause$nds[cause$acc==i]

Any help greatly appreciated.

Selwyn



From selwyn.mccracken at stonebow.otago.ac.nz  Tue Jun 17 08:13:20 2003
From: selwyn.mccracken at stonebow.otago.ac.nz (Selwyn Mccracken)
Date: Tue, 17 Jun 2003 18:13:20 +1200
Subject: [R] help recoding
In-Reply-To: <3EEEAC1E.1020200@paradise.net.nz>
References: <3EEEAC1E.1020200@paradise.net.nz>
Message-ID: <1055830400.3eeeb180779e1@www.stonebow.otago.ac.nz>

sorry,
I was a bit hasty with the send button. I have managed to solve it by
doing the following: 

q = c()
for (i in p) q = c(q, cause$nds[cause$acc==i])

I would however be interested in knowing how to apply the transform in
this case properly.

S.

Quoting Selwyn McCracken <selwyn.mccracken at paradise.net.nz>:

> hi R-listers,
> 
> I would like some help recoding a variable. I have a dataframe 'cause'
> 
> that translates between a set of codes:
> 
> acc	nds
> -	-
> 1	2
> 3	4	
> 5	8
> ...	...
> 
> the desired result for dataframe 'p':
> a
> -
> 1
> 5
> 5
> 
> would be:
> a	b
> -	-
> 1	2
> 5	8
> 5	9
> 
> I have tried:
> 
> transform(p, b=cause$nds[cause$acc==p$a])
> 
> but for some reason it complains about the difference in length between
> 
> the translation and result dataframes.
> 
> This prints the desired result, but I am unsure how to collect the 
> results into a variable.
> 
> for (i in p) print cause$nds[cause$acc==i]
> 
> Any help greatly appreciated.
> 
> Selwyn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From temiz at deprem.gov.tr  Tue Jun 17 09:04:34 2003
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 17 Jun 2003 10:04:34 +0300
Subject: [R] probability values ?
Message-ID: <3EEEBD82.9080200@deprem.gov.tr>

Hello

I try to find probability values of some predictor combinations using 
logistic reg. in response level.
Firstly I found coefficients by glm function.
Then I followed two ways to get probability values:
1- probility <- exp(X0+bX1+cX2+...)/((1+exp(X0+bX1+cX2+...))
2- probility <- predict(glm.obj,type="resp")

Should have these two given same result ?
if so, I did not have. Why ?

Does anyone have any idea ?

thanks in advance

Ahmet Temiz
TURKEY


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From maechler at stat.math.ethz.ch  Tue Jun 17 09:37:41 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 17 Jun 2003 09:37:41 +0200
Subject: [R] Attachments for R-help
In-Reply-To: <3A822319EB35174CA3714066D590DCD5C4FC18@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD5C4FC18@usrymx25.merck.com>
Message-ID: <16110.50501.254982.418439@gargle.gargle.HOWL>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Mon, 16 Jun 2003 13:10:42 -0400 writes:

    AndyL> Dear R-help, Can some one tell me what could be the
    AndyL> problem?  I downloaded the R-1.7.1 source and try to
    AndyL> compile it on WinXP.  "make", "make bitmapdll", "make
    AndyL> tcl", "make recommended" all ran w/o problem.
    AndyL> However, when I ran "make docs", it failed at
    AndyL> creating refman.pdf, with no apparent error that I
    AndyL> could see.  I have attached the log file
    AndyL> (refman.log), and would very much appreciate it if
    AndyL> some one can give me some clues as to what's wrong.
    AndyL> TIA!  <<refman.log>> Best, Andy

[and the attachment didn't come]

I have had the content filtering for R-help activated a while
ago. Currently it drops attachments (apart from some multipart/*
and text/html which should be translated to text/plain)
unless they are of the following MIME types

 text/plain
 application/pgp-signature
 application/postscript
 application/pdf
 image/png

Note that I didn't put all "image/"s on purpose (some spam has
just an url + an offensive picture).

For r-devel, some more are on this pass_mime_types list, e.g. (/x-tar,..) 

Comments are welcome -- maybe privately to me.

Yours,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Tue Jun 17 09:42:04 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 17 Jun 2003 09:42:04 +0200
Subject: [R] Constrained optimization
In-Reply-To: <004701c3342b$81776730$2802a8c0@DHT0TL11>
References: <004701c3342b$81776730$2802a8c0@DHT0TL11>
Message-ID: <16110.50764.604685.473921@gargle.gargle.HOWL>

>>>>> "FCharles" == Charles Annis, P E <Charles.Annis at statisticalengineering.com>
>>>>>     on Mon, 16 Jun 2003 13:19:50 -0400 writes:

    FCharles> Greetings, R-Wizards: I'm trying to find an
    FCharles> extremum subject to a nonlinear constraint.  (Yes,
    FCharles> I have perused the archives but have found nothing
    FCharles> positive.) The details of the problem are these:

    FCharles> In a paper published some years ago in
    FCharles> Technometrics, ("Confidence bands for cumulative
    FCharles> distribution functions of continuous random
    FCharles> variables" Technometrics, 25, 77-86. 1983), Cheng
    FCharles> and Iles describe an ingenious method for placing
    FCharles> confidence bounds on an entire cdf by defining the
    FCharles> likelihood ratio confidence "ellipse" for the
    FCharles> model parameters, and then traversing the
    FCharles> periphery and finding the most extreme values for
    FCharles> x, at a given F(x), such that the distribution
    FCharles> parameters reside on that confidence contour.
    FCharles> (Well, their method is more sophisticated than
    FCharles> that, but that's essentially how it works.)  I
    FCharles> implemented the thing in a spreadsheet 15 years
    FCharles> ago, and would like to do the same in R.  But
    FCharles> EXCEL's solver can find an extremum subject to a
    FCharles> constraint, and I haven't figured out how to get
    FCharles> nlm() to do that.

    FCharles> I would be grateful for any algorithmic
    FCharles> suggestions.

I'm astonished that your perusing of the archives and the
on-line help system didn't find 
	optim().

It has many methods, one of which working with box constraints.
I'm urging you to read help(optim) {and please read to the end
even if it's long}.

If you have non-box constraints, there are other tricks
(eg. continuous border penalizing) but it really depends on 
your application.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Tue Jun 17 10:13:44 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 17 Jun 2003 10:13:44 +0200
Subject: [R] lme() vs aov(y ~ A*B + Error(aa %in% A + bb %in% B)) [repost]
Message-ID: <16110.52664.690220.288484@gargle.gargle.HOWL>

I've posted the following to R-help on May 15.
It has reproducible R code for real data -- and a real
(academic, i.e unpaid) consultion background.

I'd be glad for some insight here, mainly not for myself.
In the mean time, we've learned that it is to be expected for
anova(*, "marginal") to be contrast dependent, but still are
glad for advice if you have experience.

Thank you in advance,
Martin


From dave at evocapital.com  Tue Jun 17 10:34:35 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 17 Jun 2003 09:34:35 +0100
Subject: [R] Constrained optimization
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D01B818@mail.internal.net>

It seems that you have a nonlinear constraint (rather than box
constraints). Provided your constraint is reasonably well behaved
(differentiable) you could still use optim on the Lagrangian, and then
run an "outer" optimisation to find the langrangian multiplier and slack
variable (that is, assuming you have an INequality constraint. Set z to
zero below if you just have an equality constraint). For example if your
original problem was:

Max_{x} f(x)
such that g(x) >= 0

You could instead try:
Max_{x;y;z} f(x) - y * (g(x) - z)
such that z >= 0

i.e. define a function L(x,l,z) = f(x) - y * (g(x) - z) 

Then feed L into optim and maximise it w.r.t: x (your original
variables), y (the Lagrange multiplier) and z (a slack variable).

Don't hold me to this as my optimisation theory is a little rusty...

-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
Sent: 17 June 2003 08:42
To: Charles.Annis at statisticalengineering.com
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Constrained optimization


>>>>> "FCharles" == Charles Annis, P E
<Charles.Annis at statisticalengineering.com>
>>>>>     on Mon, 16 Jun 2003 13:19:50 -0400 writes:

    FCharles> Greetings, R-Wizards: I'm trying to find an
    FCharles> extremum subject to a nonlinear constraint.  (Yes,
    FCharles> I have perused the archives but have found nothing
    FCharles> positive.) The details of the problem are these:

    FCharles> In a paper published some years ago in
    FCharles> Technometrics, ("Confidence bands for cumulative
    FCharles> distribution functions of continuous random
    FCharles> variables" Technometrics, 25, 77-86. 1983), Cheng
    FCharles> and Iles describe an ingenious method for placing
    FCharles> confidence bounds on an entire cdf by defining the
    FCharles> likelihood ratio confidence "ellipse" for the
    FCharles> model parameters, and then traversing the
    FCharles> periphery and finding the most extreme values for
    FCharles> x, at a given F(x), such that the distribution
    FCharles> parameters reside on that confidence contour.
    FCharles> (Well, their method is more sophisticated than
    FCharles> that, but that's essentially how it works.)  I
    FCharles> implemented the thing in a spreadsheet 15 years
    FCharles> ago, and would like to do the same in R.  But
    FCharles> EXCEL's solver can find an extremum subject to a
    FCharles> constraint, and I haven't figured out how to get
    FCharles> nlm() to do that.

    FCharles> I would be grateful for any algorithmic
    FCharles> suggestions.

I'm astonished that your perusing of the archives and the on-line help
system didn't find 
	optim().

It has many methods, one of which working with box constraints. I'm
urging you to read help(optim) {and please read to the end even if it's
long}.

If you have non-box constraints, there are other tricks
(eg. continuous border penalizing) but it really depends on 
your application.

Martin Maechler <maechler at stat.math.ethz.ch>
http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From petr.pikal at precheza.cz  Tue Jun 17 11:07:52 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 17 Jun 2003 11:07:52 +0200
Subject: [R] cut.POSIXct problem
Message-ID: <3EEEF688.20346.BD3E25@localhost>

Dear all

I would like to aggregate a data frame in a three minute interval. I have a time 
vector:

> str(cas.param)
`POSIXct', format: chr [1:181] "2003-06-12 09:00:00" "2003-06-12 09:01:00" 
"2003-06-12 09:02:00" 

and I want to turn it into a factor by cut. It works if I want to break it in let say 
hours

> str(cut(cas.param,breaks="hour"))
 Factor w/ 3 levels "2003-06-12 ..",..: 1 1 1 1 1 1 1 1 1 1 .

But if i tried to do similar task but to construct three minutes intervals with a 
vector of cut points it gives me an error

> cut(cas.param,breaks=seq(min(cas.param),max(cas.param), by="3 mins"))
Error in sort(breaks) : `x' must be atomic

however

> str(seq(min(cas.param),max(cas.param), by="3 mins"))
`POSIXct', format: chr [1:61] "2003-06-12 09:00:00" "2003-06-12 09:03:00" 
"2003-06-12 09:06:00"

works.

So it is definitely my lack of understanding how to implement a vector of cut 
points (constructed by seq()), to break parameter of cut() function.

Please can you give me any hints?

R 1.7.0, Windows NT

Thanks a lot

Petr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From ripley at stats.ox.ac.uk  Tue Jun 17 11:41:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 10:41:49 +0100 (BST)
Subject: [R] cut.POSIXct problem
In-Reply-To: <3EEEF688.20346.BD3E25@localhost>
Message-ID: <Pine.LNX.4.44.0306171031530.1583-100000@gannet.stats>

This works in R 1.7.1: please upgrade.

[That's assuming that your sequence actually covers the data: you need to
go beyond max(cas.param) to be sure, and it looks to me that with 
right=FALSE and include.lowest-FALSE the breaks never will cover the 
data.]


On Tue, 17 Jun 2003, Petr Pikal wrote:

> Dear all
> 
> I would like to aggregate a data frame in a three minute interval. I have a time 
> vector:
> 
> > str(cas.param)
> `POSIXct', format: chr [1:181] "2003-06-12 09:00:00" "2003-06-12 09:01:00" 
> "2003-06-12 09:02:00" 
> 
> and I want to turn it into a factor by cut. It works if I want to break it in let say 
> hours
> 
> > str(cut(cas.param,breaks="hour"))
>  Factor w/ 3 levels "2003-06-12 ..",..: 1 1 1 1 1 1 1 1 1 1 .
> 
> But if i tried to do similar task but to construct three minutes intervals with a 
> vector of cut points it gives me an error
> 
> > cut(cas.param,breaks=seq(min(cas.param),max(cas.param), by="3 mins"))
> Error in sort(breaks) : `x' must be atomic
> 
> however
> 
> > str(seq(min(cas.param),max(cas.param), by="3 mins"))
> `POSIXct', format: chr [1:61] "2003-06-12 09:00:00" "2003-06-12 09:03:00" 
> "2003-06-12 09:06:00"
> 
> works.
> 
> So it is definitely my lack of understanding how to implement a vector of cut 
> points (constructed by seq()), to break parameter of cut() function.
> 
> Please can you give me any hints?
> 
> R 1.7.0, Windows NT
> 
> Thanks a lot
> 
> Petr Pikal
> petr.pikal at precheza.cz
> p.pik at volny.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Jun 17 11:47:47 2003
From: wolski at molgen.mpg.de (wolski)
Date: Tue, 17 Jun 2003 11:47:47 +0200
Subject: [R] hist density...
Message-ID: <200306171147470831.041701BB@harry.molgen.mpg.de>

Hi!
Do not understand following behavior.


> summary(test$dif)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.7389  0.9713  0.9850  0.9818  1.0000  1.0000 
length(test$dif)
[1] 85879


tmp <- hist(test$dif,breaks=100,freq=FALSE)

The density on the Y axis in the plot are in the range 0-200. 
Thought that the density should be in the range 0-1 
(something like tmp$count/length(test$dif))?


Eryk

P.S.
In case of frequencies all are fine for the dataset.



From adrian at asteridia.maths.uwa.edu.au  Tue Jun 17 12:55:11 2003
From: adrian at asteridia.maths.uwa.edu.au (Adrian Baddeley)
Date: Tue, 17 Jun 2003 18:55:11 +0800
Subject: [R] plot.formula
In-Reply-To: <200306171008.h5HA6q4a015742@stat.math.ethz.ch>
References: <200306171008.h5HA6q4a015742@stat.math.ethz.ch>
Message-ID: <16110.62351.704257.863527@asteridia.maths.uwa.edu.au>


Thanks to Andy Liaw and Baz 
for alternatives to tinkering with plot.formula
to get
	plot(cbind(y1,y2) ~ x, ...)
to work.

However...

   > From: "Liaw, Andy" <andy_liaw at merck.com>
    matplot(x, cbind(y1, y2), ...)

   > From: Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
   xyplot(y1+y2~x,data=xyy,allow.multiple=T)

The former requires separate code for the case of multiple response variables.

The latter does provide a formula interface, but the interpretation
of the formulae is inconvenient. 
For example

   - if y1 is an expression containing "+"
      then it would have to be protected by I() to prevent the 
      summands in the expression being plotted separately.
      
   - you can't replace one of the response variables by a constant, 
   as this will be evaluated as a vector of length 1.

It's a lot easier with cbind() , for example

     plot(cbind(z1 -zhat + bias, 0) ~ x)

gets you a plot of (z1-zhat+bias) versus x 
together with a horizontal line at y=0. 

The idea of using cbind() was indeed modelled on the glm() usage.

----
Adrian Baddeley, Mathematics & Statistics, University of Western Australia
		<http://maths.uwa.edu.au/~adrian/>



From hachipma at icarus.math.uwaterloo.ca  Tue Jun 17 12:58:18 2003
From: hachipma at icarus.math.uwaterloo.ca (Hugh Chipman)
Date: Tue, 17 Jun 2003 06:58:18 -0400 (EDT)
Subject: [R] User-defined functions in rpart
Message-ID: <200306171058.GAA08939@icarus.math.uwaterloo.ca>

This question concerns rpart's facility for user-defined functions that
accomplish splitting.

I was interested in modifying the code so that in each terminal node,
a linear regression is fit to the data.

It seems that from the allowable inputs in the user-defined functions,
that this may not be possible, since they have the form:

function(y, wt, parms)  (in the case of the "evaluation" function)
function(y, wt, x, parms, continuous)  (split function)

The problem is that there seems to be no facility to include an X
matrix (in the split function, x is a vector corresponding to one
predictor).  Without that, fitting a linear model in the terminal node
would not be possible.

Is this a correct assesment, or am I missing something?
Has anyone tried to modify rpart to fit linear models in nodes?


-- 
-- 
-- Hugh Chipman                                                      --
-- Associate Professor, Department of Statistics & Actuarial Science --
-- U. of Waterloo, 200 University Ave. W, Waterloo, Ontario, N2L 3G1 --
-- (519) 888-4567 ext. 6190                      Fax: (519) 746-1875 --
-- www.stats.uwaterloo.ca/~hachipma/          hachipman at uwaterloo.ca --



From ripley at stats.ox.ac.uk  Tue Jun 17 13:03:04 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 12:03:04 +0100 (BST)
Subject: [R] hist density...
In-Reply-To: <200306171147470831.041701BB@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0306171200490.6577-100000@gannet.stats>

A density integrates to one, so the total area of the bins is one.
It's your `Thought' which is incorrect.

For hist(freq=FALSE) the area of each rectangle (not its height) 
represents the proportion of the data falling into the base of the 
rectangle.

On Tue, 17 Jun 2003, wolski wrote:

> Hi!
> Do not understand following behavior.
> 
> 
> > summary(test$dif)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>  0.7389  0.9713  0.9850  0.9818  1.0000  1.0000 
> length(test$dif)
> [1] 85879
> 
> 
> tmp <- hist(test$dif,breaks=100,freq=FALSE)
> 
> The density on the Y axis in the plot are in the range 0-200. 
> Thought that the density should be in the range 0-1 
> (something like tmp$count/length(test$dif))?
> 
> 
> Eryk
> 
> P.S.
> In case of frequencies all are fine for the dataset.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Jun 17 13:35:12 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 17 Jun 2003 13:35:12 +0200 (CEST)
Subject: [R] User-defined functions in rpart
In-Reply-To: <200306171058.GAA08939@icarus.math.uwaterloo.ca>
References: <200306171058.GAA08939@icarus.math.uwaterloo.ca>
Message-ID: <Pine.LNX.4.51.0306171315200.24044@artemis.imbe.med.uni-erlangen.de>



> This question concerns rpart's facility for user-defined functions that
> accomplish splitting.
>
> I was interested in modifying the code so that in each terminal node,
> a linear regression is fit to the data.
>

if you just want to have a linear model in each terminal node instead of
the mean of the observations in this leaf (which does not require an
altered splitting rule), you can do something like (BostonHousing as example):

R> library(mlbench)
R> library(rpart)
### a stump only
R> tree <- rpart(medv ~ ., data=BostonHousing, cp=0.2)
R> tree
n= 506

node), split, n, deviance, yval
      * denotes terminal node

1) root 506 42716.300 22.53281
  2) rm< 6.941 430 17317.320 19.93372 *
  3) rm>=6.941 76  6059.419 37.23816 *
### fit a linear model for the observations in each leaf
R> tnodeleft <- lm(medv ~ ., data=BostonHousing, subset=(tree$where == 2))
R> tnoderight <- lm(medv ~ ., data=BostonHousing, subset=(tree$where == 3))
R> coef(tnodeleft)
  (Intercept)          crim            zn         indus         chas1
 5.291185e+01 -1.430523e-01  3.879871e-02  2.818030e-02  3.329020e+00
          nox            rm           age           dis           rad
-1.686631e+01 -3.671042e-01 -3.791325e-04 -1.128992e+00  2.954484e-01
          tax       ptratio             b         lstat
-1.071171e-02 -5.472690e-01  6.393379e-03 -5.578978e-01
R> coef(tnoderight)
  (Intercept)          crim            zn         indus         chas1
  8.224189122  -0.078029764   0.001545735   0.616441038  -0.928230180
          nox            rm           age           dis           rad
-17.080614140   6.372744363  -0.060863991  -1.032928738   0.451670727
          tax       ptratio             b         lstat
 -0.034438740  -1.647800912   0.093841671  -1.172865798


> It seems that from the allowable inputs in the user-defined functions,
> that this may not be possible, since they have the form:
>
> function(y, wt, parms)  (in the case of the "evaluation" function)
> function(y, wt, x, parms, continuous)  (split function)
>
> The problem is that there seems to be no facility to include an X
> matrix (in the split function, x is a vector corresponding to one
> predictor).  Without that, fitting a linear model in the terminal node
> would not be possible.
>
> Is this a correct assesment, or am I missing something?

If you want to base your splitting criterion on linear models in EACH
node, you may try something like this.
I'm not sure if you need to pass the design matrix to the split function:
let Y and X denote response and design matrix in the calling environment,
than an ugly hack like:

temp2 <- function(y, wt, x, parms, continuous) {
  thisindx <- Y %in% y  ### determine which subset of the learning
                        ### sample is currently under consideration

  thisX <- X[thisindx,] ### only those in the current node

  ### get the position of the x to split in:
  myx <- which(apply(thisX, 2, function(a) all(a %in% x))])

  ### and now seach for the best split in thisX[,thisx]
  ### for each cutpoint cut ...
  lm(y ~ ., data = thisX, subset=thisX[,myx] <= cut)
  lm(y ~ ., data = thisX, subset=thisX[,myx] > cut)

... may work

just an idea,

Torsten


> Has anyone tried to modify rpart to fit linear models in nodes?
>
>
> --
> --
> -- Hugh Chipman                                                      --
> -- Associate Professor, Department of Statistics & Actuarial Science --
> -- U. of Waterloo, 200 University Ave. W, Waterloo, Ontario, N2L 3G1 --
> -- (519) 888-4567 ext. 6190                      Fax: (519) 746-1875 --
> -- www.stats.uwaterloo.ca/~hachipma/          hachipman at uwaterloo.ca --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From sundar.dorai-raj at pdf.com  Tue Jun 17 13:41:56 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 17 Jun 2003 06:41:56 -0500
Subject: [R] probability values ?
References: <3EEEBD82.9080200@deprem.gov.tr>
Message-ID: <3EEEFE84.9040200@pdf.com>



orkun wrote:
> Hello
> 
> I try to find probability values of some predictor combinations using 
> logistic reg. in response level.
> Firstly I found coefficients by glm function.
> Then I followed two ways to get probability values:
> 1- probility <- exp(X0+bX1+cX2+...)/((1+exp(X0+bX1+cX2+...))
> 2- probility <- predict(glm.obj,type="resp")
> 
> Should have these two given same result ?
> if so, I did not have. Why ?
> 
> Does anyone have any idea ?
> 

This works for me. Are you sure you're getting the correct linear 
predictor in (1). Here's an example:

R> x = glm(y ~ trt + I(week > 2), data = bacteria, family = binomial)
R> str(predict(x, type = "resp"))
  Named num [1:220] 0.944 0.944 0.823 0.823 0.900 ...
  - attr(*, "names")= chr [1:220] "1" "2" "3" "4" ...
R> coef(x)
     (Intercept)         trtdrug        trtdrug+ I(week > 2)TRUE
       2.8332455      -1.1186847      -0.6372255      -1.2948522
R> lp = model.matrix(x) %*% coef(x)
R> str(exp(lp)/(1 + exp(lp)))
  num [1:220, 1] 0.944 0.944 0.823 0.823 0.900 ...
  - attr(*, "dimnames")=List of 2
   ..$ : chr [1:220] "1" "2" "3" "4" ...
   ..$ : NULL
R>



From temiz at deprem.gov.tr  Tue Jun 17 14:19:38 2003
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 17 Jun 2003 15:19:38 +0300
Subject: [R] probability values ?
In-Reply-To: <3EEEFE84.9040200@pdf.com>
References: <3EEEBD82.9080200@deprem.gov.tr> <3EEEFE84.9040200@pdf.com>
Message-ID: <3EEF075A.90503@deprem.gov.tr>

Sundar Dorai-Raj wrote:

>
>
> orkun wrote:
>
>> Hello
>>
>> I try to find probability values of some predictor combinations using 
>> logistic reg. in response level.
>> Firstly I found coefficients by glm function.
>> Then I followed two ways to get probability values:
>> 1- probility <- exp(X0+bX1+cX2+...)/((1+exp(X0+bX1+cX2+...))
>> 2- probility <- predict(glm.obj,type="resp")
>>
>> Should have these two given same result ?
>> if so, I did not have. Why ?
>>
>> Does anyone have any idea ?
>>
>
> This works for me. Are you sure you're getting the correct linear 
> predictor in (1). Here's an example:
>
> R> x = glm(y ~ trt + I(week > 2), data = bacteria, family = binomial)
> R> str(predict(x, type = "resp"))
>  Named num [1:220] 0.944 0.944 0.823 0.823 0.900 ...
>  - attr(*, "names")= chr [1:220] "1" "2" "3" "4" ...
> R> coef(x)
>     (Intercept)         trtdrug        trtdrug+ I(week > 2)TRUE
>       2.8332455      -1.1186847      -0.6372255      -1.2948522
> R> lp = model.matrix(x) %*% coef(x)
> R> str(exp(lp)/(1 + exp(lp)))
>  num [1:220, 1] 0.944 0.944 0.823 0.823 0.900 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : chr [1:220] "1" "2" "3" "4" ...
>   ..$ : NULL
> R>
>
>
>
thank you for your help

Now one thing is  clear to me:
predict(x, type = "resp") gives probability values.
For the sake of simplicity, I can choose it, instead of using first way.

cordially




______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the ... {{dropped}}



From spencer.graves at pdf.com  Tue Jun 17 14:50:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 17 Jun 2003 05:50:38 -0700
Subject: [R] help recoding
References: <3EEEAC1E.1020200@paradise.net.nz>
	<1055830400.3eeeb180779e1@www.stonebow.otago.ac.nz>
Message-ID: <3EEF0E9E.3090208@pdf.com>

Have you considered the following:

 > nds <- c(2, 4, 8)
 > names(nds) <- c(1, 3, 5)
 > nds[c("3", "1", "5")]
3 1 5
4 2 8

hth.  spencer graves

Selwyn Mccracken wrote:
> sorry,
> I was a bit hasty with the send button. I have managed to solve it by
> doing the following: 
> 
> q = c()
> for (i in p) q = c(q, cause$nds[cause$acc==i])
> 
> I would however be interested in knowing how to apply the transform in
> this case properly.
> 
> S.
> 
> Quoting Selwyn McCracken <selwyn.mccracken at paradise.net.nz>:
> 
> 
>>hi R-listers,
>>
>>I would like some help recoding a variable. I have a dataframe 'cause'
>>
>>that translates between a set of codes:
>>
>>acc	nds
>>-	-
>>1	2
>>3	4	
>>5	8
>>...	...
>>
>>the desired result for dataframe 'p':
>>a
>>-
>>1
>>5
>>5
>>
>>would be:
>>a	b
>>-	-
>>1	2
>>5	8
>>5	9
>>
>>I have tried:
>>
>>transform(p, b=cause$nds[cause$acc==p$a])
>>
>>but for some reason it complains about the difference in length between
>>
>>the translation and result dataframes.
>>
>>This prints the desired result, but I am unsure how to collect the 
>>results into a variable.
>>
>>for (i in p) print cause$nds[cause$acc==i]
>>
>>Any help greatly appreciated.
>>
>>Selwyn
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Carlisle.Thacker at noaa.gov  Tue Jun 17 14:59:08 2003
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Tue, 17 Jun 2003 08:59:08 -0400
Subject: [R] help.start
Message-ID: <3EEF109C.2C03E2E1@noaa.gov>

Dear R experts,

I would greatly appreciate your help with R's hypertext online
documentation, which I can pass on to our system manager.  We are
running version 1.7 on sparc-sun-solaris2.9 using netscape7 browser
for displaying the help.  The system is working, but there are a few
frustrating details that I hope might be fixed.

Here are three problems we are having:

1.  When the R session is terminated, the netscape7 session that was
initiated by help.start is still active.  However, it no longer
provides the R help.  Moreover, help.start in a new R session trys to
initiate a new netscape7 session, causing R-unrelated conflicts.  Is
there a way to get help.start to use the existing netscape7 session?

2.  When searching using the help system's search engine, there is no
problem following links forward.  But when returning to the list
produced by the search, links are no longer active, so the search has
to be repeated.  Is there any way to fix it so that links remain
active?

3.  When using browseEnv I get the same problem as described in item 1
above:  a new netscape7 session is started.  To avoid R-unrelated
problems, I have had to shut down the existing netscape7 session, so
that browseEnv, can start a new one.  Is there a way to get browseEnv
to use the existing netscape7 session?


Thanks,

Carlisle

platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    7.0                 
year     2003                
month    04                  
day      16                  
language R                   

-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From f.calboli at ucl.ac.uk  Tue Jun 17 15:14:36 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Tue, 17 Jun 2003 14:14:36 +0100
Subject: [R] books
Message-ID: <3.0.6.32.20030617141436.00a5dc78@pop-server.ucl.ac.uk>

Dear All,

using R is noticed that I would benefit by going back to the books and
"refresh" my linear algebra and calculus. 
Has anybody suggestions on which books are better as reference on the two
these two topics? 

I was thinking to get Lang's "A First Course in Calculus", but I I have no
idea what could be a good one for linear algebra. Something detailed and
clear would be better, I do not have the sharpest mathematical brain.

Any suggestion is welcome.

Regards,
Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From ripley at stats.ox.ac.uk  Tue Jun 17 15:18:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 14:18:45 +0100 (BST)
Subject: [R] help.start
In-Reply-To: <3EEF109C.2C03E2E1@noaa.gov>
Message-ID: <Pine.LNX.4.44.0306171405400.7271-100000@gannet.stats>

On Tue, 17 Jun 2003, W. C. Thacker wrote:

> Dear R experts,
> 
> I would greatly appreciate your help with R's hypertext online
> documentation, which I can pass on to our system manager.  We are
> running version 1.7 on sparc-sun-solaris2.9 using netscape7 browser
> for displaying the help.  The system is working, but there are a few
> frustrating details that I hope might be fixed.

So do we: take it up with Netscape!  They keep on breaking things that 
once worked: see the comments in ?browseURL, for example.

However, all my installations are under the name `netscape'.  It looks
like someone has renamed yours to `netscape7', in which case you will have
a different behaviour.  See the R function browseURL and its
documentation.
 
> Here are three problems we are having:
> 
> 1.  When the R session is terminated, the netscape7 session that was
> initiated by help.start is still active.  However, it no longer
> provides the R help.  Moreover, help.start in a new R session trys to
> initiate a new netscape7 session, causing R-unrelated conflicts.  Is
> there a way to get help.start to use the existing netscape7 session?

That's not what is supposed to happen, assuming you are using R and 
Netscape on the same machine, nor is it what I get.

> 2.  When searching using the help system's search engine, there is no
> problem following links forward.  But when returning to the list
> produced by the search, links are no longer active, so the search has
> to be repeated.  Is there any way to fix it so that links remain
> active?

It's a known problem with Mozilla-based browsers.  Try opening the forward 
link in a new tab or window.

> 3.  When using browseEnv I get the same problem as described in item 1
> above:  a new netscape7 session is started.  To avoid R-unrelated
> problems, I have had to shut down the existing netscape7 session, so
> that browseEnv, can start a new one.  Is there a way to get browseEnv
> to use the existing netscape7 session?

Well, browseEnv is *very* experimental and its HTML version may well not 
be continued, but it uses the same mechanism as help.start.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Jun 17 15:22:14 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 17 Jun 2003 15:22:14 +0200
Subject: [R] books
In-Reply-To: <3.0.6.32.20030617141436.00a5dc78@pop-server.ucl.ac.uk>
References: <3.0.6.32.20030617141436.00a5dc78@pop-server.ucl.ac.uk>
Message-ID: <200306171522140907.0F3175D2@harry.molgen.mpg.de>

Hi!

There are two great books available for free over the internet.
Jim Hefferon Linear Algebra.

and

Elementary Linear Algebra by 
K.R Matthews.

Eryk.

Ps.:
www.google.com


*********** REPLY SEPARATOR  ***********

On 6/17/2003 at 2:14 PM Federico Calboli wrote:

>Dear All,
>
>using R is noticed that I would benefit by going back to the books and
>"refresh" my linear algebra and calculus. 
>Has anybody suggestions on which books are better as reference on the two
>these two topics? 
>
>I was thinking to get Lang's "A First Course in Calculus", but I I have no
>idea what could be a good one for linear algebra. Something detailed and
>clear would be better, I do not have the sharpest mathematical brain.
>
>Any suggestion is welcome.
>
>Regards,
>Federico Calboli
>
>=========================
>
>Federico C.F. Calboli
>
>Department of Biology
>University College London
>Room 327
>Darwin Building
>Gower Street
>London
>WClE 6BT
>
>Tel: (+44) 020 7679 4395 
>Fax (+44) 020 7679 7096
>f.calboli at ucl.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics   
Ihnestrasse 73 14195 Berlin          'v'    
tel: 0049-30-84131285               /   \    
mail: wolski at molgen.mpg.de        ---W-W----



From Binksjess at aol.com  Tue Jun 17 15:48:11 2003
From: Binksjess at aol.com (Binksjess@aol.com)
Date: Tue, 17 Jun 2003 09:48:11 EDT
Subject: [R] (no subject)
Message-ID: <184.1c92cbcf.2c20761b@aol.com>

Hello, 

I'm trying to write code for a repeated measures ANOVA.  To put things in 
perspective, I'll describe my experiment (briefly).  I have a 2X2 factorial 
design with pH (5.5, 6.5) and local community (present, absent) as my treatments.  
I had plastic enclosures that I sampled across five weeks with the density of 
7 species acting as my response variables.  I'm analyzing one species at a 
time and doing Bonferonni corrections after the analysis.  I've attached an 
example of one of my data files showing the treatments and the change in density 
(of one species) over 5 weeks.  Hopefully someone can shed some light on this 
question: how to write a repeated measures ANOVA code in R?  Thanks.  Hope to 
hear soon.

Jessie Binks

You can email a response to: jbinks at utm.utoronto.ca



    
    

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: daphnia2.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030617/a8ab728f/daphnia2.txt

From sldaniel at att.com  Tue Jun 17 16:33:14 2003
From: sldaniel at att.com (Daniel, Sherae, ALABS)
Date: Tue, 17 Jun 2003 10:33:14 -0400
Subject: [R] vector maximum length
Message-ID: <4F3044A7ABA9F2439F749670F2C583FD04F76418@ACCLUST04EVS1.ugd.att.com>

Hi, What is the maximum size of a vector in R?

I am trying to load in a vector with 1.6 million elements.

Thanks,
Sherae



From meike.gebel at gmx.de  Tue Jun 17 16:37:28 2003
From: meike.gebel at gmx.de (Meike Gebel)
Date: Tue, 17 Jun 2003 16:37:28 +0200 (MEST)
Subject: [R] kernel smoothing for ordinal data
Message-ID: <16439.1055860648@www9.gmx.net>

Hi there,
during my work I have to use kernel smoothing methods for multivariate
ordinal data.
The R-package "KernSmooth" unfortunately includes only a version for
continous scaled variables.
Does anybody know whether there exists also a version for ordinal data?
Thanks for help!



--



From p.dalgaard at biostat.ku.dk  Tue Jun 17 16:56:42 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 17 Jun 2003 14:56:42 -0000
Subject: [R] kernel smoothing for ordinal data
In-Reply-To: <16439.1055860648@www9.gmx.net>
References: <16439.1055860648@www9.gmx.net>
Message-ID: <x2u1ao3frl.fsf@biostat.ku.dk>

Meike Gebel <meike.gebel at gmx.de> writes:

> Hi there,
> during my work I have to use kernel smoothing methods for multivariate
> ordinal data.
> The R-package "KernSmooth" unfortunately includes only a version for
> continous scaled variables.
> Does anybody know whether there exists also a version for ordinal data?
> Thanks for help!

Er, do you have a reference for what such a beast should be doing?
Kernel smoothing methods generally rely quite heavily on having a
continuous distribution (for density estimates) or having a continuous
x distribution and something on the y axis that allows taking weighted
avereages (for nonparametric regression type problems).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Jun 17 16:58:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 15:58:48 +0100 (BST)
Subject: [R] vector maximum length
In-Reply-To: <4F3044A7ABA9F2439F749670F2C583FD04F76418@ACCLUST04EVS1.ugd.att.com>
Message-ID: <Pine.LNX.4.44.0306171551530.10980-100000@gannet.stats>

Depends on the R platform, but much larger than that.  On most systems the 
maximum length will be 2^31-1 (over 2 billion), but on 32-bit systems the 
storage needed for the vector will hit its limit first.  You can expect to 
be able to use a small number of hundred million elements, depending on 
what they are, if you have enough RAM.

On Tue, 17 Jun 2003, Daniel, Sherae, ALABS wrote:

> Hi, What is the maximum size of a vector in R?
> 
> I am trying to load in a vector with 1.6 million elements.

load as in load()?  In which case someone must have created it!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Jun 17 17:09:31 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 17 Jun 2003 16:09:31 +0100 (BST)
Subject: [R] Schafer's CAT for MI
Message-ID: <XFMail.030617160931.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

Fernando Tussell and I have been working on an R package of Shafer's
CAT software for S-plus, for multiple imputation of categorical data.

A very first version of this ("0.0-1") now seems to work, in that blatant
bugs and segfaults seem to have been worked around.

It now needs some testing in the wild, so if anyone would like to have
a copy of cat_0.0-1.tar.gz to try out (16062 bytes) please let me know
and I will send it.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 17-Jun-03                                       Time: 16:09:31
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Tue Jun 17 17:23:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 16:23:07 +0100 (BST)
Subject: [R] kernel smoothing for ordinal data
In-Reply-To: <x2u1ao3frl.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0306171600310.10980-100000@gannet.stats>

They do exist: the term has a somewhat different meaning for categorical 
data.

Titerington, D. M. (1980) Technometrics 22, 259-268

might be a good start.

On 17 Jun 2003, Peter Dalgaard BSA wrote:

> Meike Gebel <meike.gebel at gmx.de> writes:
> 
> > Hi there,
> > during my work I have to use kernel smoothing methods for multivariate
> > ordinal data.
> > The R-package "KernSmooth" unfortunately includes only a version for
> > continous scaled variables.
> > Does anybody know whether there exists also a version for ordinal data?
> > Thanks for help!
> 
> Er, do you have a reference for what such a beast should be doing?
> Kernel smoothing methods generally rely quite heavily on having a
> continuous distribution (for density estimates) or having a continuous
> x distribution and something on the y axis that allows taking weighted
> avereages (for nonparametric regression type problems).
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jonck at vanderkogel.net  Tue Jun 17 17:23:33 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Tue, 17 Jun 2003 17:23:33 +0200
Subject: [R] Clustering quality measure
Message-ID: <A828F656-A0D7-11D7-9A59-0005026E2B43@vanderkogel.net>

Hi all,
I am running a series of experiments where after manipulating my data I 
run several clustering algorithms (agnes, diana and a clustering method 
of my own) on the data. I wanted to determine which clustering method 
did the best job, so therefore I had defined my own quality measure 
using two criteria: compactness of the data within the clusters 
themselves and the amount of seperation between the clusters. Anyway, 
my quality measure does not work, since according to my quality measure 
the quality gets increasingly better as more clusters are formed untill 
every data instance is a cluster by itself.
Therefore I was wondering if any of you are aware of any libraries or 
functions within R that determine quality measures of clusterings, I am 
very much intrigued by the definition of quality measures that do work.
Thanks very much, Jonck



From maechler at stat.math.ethz.ch  Tue Jun 17 17:32:32 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 17 Jun 2003 17:32:32 +0200
Subject: [R] lme() vs aov(y ~ A*B + Error(aa %in% A + bb %in% B)) [repost]
In-Reply-To: <16110.52664.690220.288484@gargle.gargle.HOWL>
References: <16110.52664.690220.288484@gargle.gargle.HOWL>
Message-ID: <16111.13456.296379.817915@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 17 Jun 2003 10:13:44 +0200 writes:

    MM> I've posted the following to R-help on May 15.
    MM> It has reproducible R code for real data -- and a real
    MM> (academic, i.e unpaid) consultion background.

    MM> I'd be glad for some insight here, mainly not for myself.
    MM> In the mean time, we've learned that it is to be expected for
    MM> anova(*, "marginal") to be contrast dependent, but still are
    MM> glad for advice if you have experience.

    MM> Thank you in advance,
    MM> Martin

and indeed, the forwarded message, also a kind of attachment (message/rfc822)
was dropped by mailman's content filtering too...
Here, it's "as regular" text:

Here is a reproducible (when you're on-net) script for a
relatively simple real data (consulting) situation here.

The data analyst tried to use lme() rather than  aov( + Error()) 
and asked several questions that I couldn't easily answer and
thought to be interesting for a more general audience in any
case.

I attach the script as text/plain file that you should run
"everywhere".  The three questions are summarized at the
beginning of the file.

----------------------------snip-snip------------------------------------

### LME/aov example {real data!}
### Two crossed fixed effects each with a random factor inside

### Problems :
### 1) need(?) ugly pseudo grouping
### 2) anova() depends on setting of contrasts
### 3) numerical problem  "Singular precision matrix" .. (should not happen?)

download.file("ftp://stat.ethz.ch/U/maechler/R/ex4.rda", "lme-ex4.rda")
load("lme-ex4.rda")##-> object `dw'
summary(dw)
str(dw)

## For some EDA {and S+ compatibility; for R, I'd use  with(dw, ...)} :
attach(dw)
## TREE: 1:3 at "1", 4:6 at "2" : TREE completely nested in LOCATION
table(TREE, LOCATION)
## and the same with THALLUS in PROV: th 1:5 > pr1; 6:7 > 2; 8:12 > 3;...
table(THALLUS, PROV)
colSums(table(THALLUS, PROV) > 0)
## pr1 pr2 pr3 pr4 pr5 pr6
##   5   2   5   5   1   5

interaction.plot(PROV, LOCATION, y)
## Hmm.. PROV=5 is a bit peculiar
## well, for one because it has only 3+3 observations :
table(PROV, LOCATION)

plot(y ~ PROV, data = dw)# not so useful
## or (better?):
stripchart(y ~ PROV, vertical=TRUE, method="stack")

if(FALSE)# quite a bit to see
table(PROV, THALLUS, TREE)
all(table(PROV, THALLUS, TREE) %in% c(0,1)) ## TRUE

detach("dw")##--------------------- end of "EDA" -------------

## Setup:
## - 2 locations (LOCATION) with 3 trees each
## - 6 kinds proveniences (PROV) with 2-5 kinds "thallus" each
## - L * P are fixed (crossed), each has a random factor nested inside

## ==>  Simple Approach is possible
##
## aov Models with Error()
## -----------------------

##--------- 1 ------------------------ default contrasts ---------
options(contrasts=c("contr.treatment", "contr.poly"))
aov1 <- aov(y ~ PROV*LOCATION + Error(TREE%in%LOCATION + THALLUS%in%PROV),
            data = dw)
summary(aov1)
##--------- 2 ------------------------ sum contrasts ---------
options(contrasts=c("contr.sum", "contr.poly"))
aov2 <- aov(y ~ PROV*LOCATION + Error(TREE%in%LOCATION + THALLUS%in%PROV),
            data = dw)
## really the imporant things do NOT depend on the contrasts:
stopifnot(all.equal(summary(aov1),
                    summary(aov2)))

## For residual analysis :
aov1. <- aov(y ~ PROV*LOCATION + TREE%in%LOCATION + THALLUS%in%PROV, data = dw)
## non-sense model
## -- used only for residuals() , fitted()  which "fail" for aov1 << !!
##  ((this is not nice behavior !))
par(mfrow = c(1,2))
plot(fitted(aov1.), residuals(aov1.)); abline(h = 0, lty = 2)
qqnorm(residuals(aov1.)); qqline(residuals(aov1.))

## model.tables(aov1.,type = "means")



### Now try to be modern, use REML and hence
###
### lme models:
### === -------

library(nlme)

## Construct a pseudo grouping factor with just one level
dw$pseudogrp <- factor(c("g1"))
dwgrp <- groupedData(y ~ 1 | pseudogrp, data=dw)

###--------- 1 ------------------------ default contrasts ---------

options(contrasts=c("contr.treatment", "contr.poly"))

lme1  <- lme(y ~ LOCATION * PROV , data = dwgrp,
             random = pdBlocked(list(pdIdent(~TREE -1),
                                     pdIdent(~THALLUS -1))))
anova(lme1, type = "marginal")
## quite close to the aov() results above;
##  F value for PROV:LOCATION is the same, but has wrong (?) deg.freedom
##  ((because of the "wrong" pseudogrp grouping))

if(FALSE)
    summary(lme1)
intervals(lme1,which = "var-cov")
ranef(lme1)

trellis.device()
lset(col.whitebg())

## Residual analysis:
par(mfrow = c(1,2))
plot(fitted(lme1), residuals(lme1)); abline(h = 0, lty = 2)
qqnorm(residuals(lme1)); qqline(residuals(lme1))


###--------- 2 ------------------------ sum contrasts ---------

options(contrasts=c("contr.sum", "contr.poly"))

lme2  <- lme(y ~ LOCATION * PROV , data = dwgrp,
             random = pdBlocked(list(pdIdent(~TREE -1),
                                     pdIdent(~THALLUS -1))))

## Same model as lme1 ?

## In some sense, yes:
stopifnot(all.equal(residuals(lme1),
                    residuals(lme2)),
          all.equal(fitted(lme1),
                    fitted(lme2)))

## But this is quite different --  <<< why (REML) ? >>>>

## -- the SS(residuals) are the same,
## and the factor's SS should not depend on the contrasts?
anova(lme2, type = "marginal")

intervals(lme2,which = "var-cov")
ranef(lme2)

##


ty <- asin(sqrt(y))
lme4 <- lme(ty ~ LOCATION * PROV , data = dwgrp,
            random = pdBlocked(list(pdIdent(~TREE -1),
             pdIdent(~THALLUS -1))))
## Warning messages:
## 1: Singular precision matrix in level -1, block 1
## 2: Singular precision matrix in level -1, block 1

## ?????  ^^^^ numerical problem ?? --- investigating :
if(FALSE){
    options(warn = 2)## make warnings into errors !
    lme4 <- lme(ty ~ LOCATION * PROV , data = dwgrp,
                random = pdBlocked(list(pdIdent(~TREE -1),
                pdIdent(~THALLUS -1))))
    traceback()
    ## gives a long stack trace, with the lowest level function
    ##  logLik.reStruct(.)

    options(warn = 0) # back to default
}


anova(lme4) # is quite a bit different from the anova()s above!

ry <- round(ty, 2)
lme5 <- lme(ry ~ LOCATION * PROV , data = dwgrp,
            random = pdBlocked(list(pdIdent(~TREE -1),
             pdIdent(~THALLUS -1))))



From kan_liu1 at yahoo.com  Tue Jun 17 18:35:52 2003
From: kan_liu1 at yahoo.com (kan Liu)
Date: Tue, 17 Jun 2003 09:35:52 -0700 (PDT)
Subject: [R] outlier
Message-ID: <20030617163552.28872.qmail@web20704.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030617/836628ae/attachment.pl

From fharrell at virginia.edu  Tue Jun 17 02:04:57 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 16 Jun 2003 20:04:57 -0400
Subject: [R] Hmisc multiple imputation functions
In-Reply-To: <Law11-F76Z0VAxdpb6g000a2dfa@hotmail.com>
References: <Law11-F76Z0VAxdpb6g000a2dfa@hotmail.com>
Message-ID: <20030616200457.2fdcd9ca.fharrell@virginia.edu>

On Mon, 16 Jun 2003 13:22:08 +0000
Vumani Dlamini <dvumani at hotmail.com> wrote:

> Dear all;
> 
> I am trying to use HMISC imputation function to perform multiple imputations 
> on my data and I keep on getting errors for the code given in the help 
> files.

You did not specify the version of Hmisc nor the output of the version command to provide the OS and version of R.

> 
> When using "aregImpute" the error is;
> 
> >f <- aregImpute(~y + x1 + x2 + x3, n.impute=100)
> Loading required package: acepack
> Iteration:1 Error in .Fortran("wclosepw", as.double(w), as.double(x), 
> as.double(runif(lw)),  :
>         C/Fortran function name not in load table

Please see above.  If you are running under Windows, perhaps the wclosepw routine was omitted from the Windows distribution of Hmisc.

> 
> When I use "transcan" it is;
> 
> >f  <- transcan(~y + x1 + x2, n.impute=10, shrink=FALSE)
> Error in transcan(~y + x1 + x2, n.impute = 10, shrink = FALSE) :
>         Must specify data= when using R

That is an error in the documentation which I just fixed.  Add

 d <- data.frame(y,x1,x2)
 f <- transcan(~y+x1+x2, ......., data=d)

Frank

> 
> I am not sure what I am missing.
> 
> Vumani
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From anna at ptolemy.arc.nasa.gov  Tue Jun 17 18:48:02 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Tue, 17 Jun 2003 09:48:02 -0700
Subject: [R] source vs. editor
Message-ID: <200306170948.02834.anna@ptolemy.arc.nasa.gov>

When I am trying to use the source function to  read in some lines of code, I 
get an error.  The code is simply a boxplot:

boxplot(s$fitness2weightedSum,s$OffNader10xWeightweightedSum,s$OffNader5xWeightweightedSum,main="weightedSum",col=8)
axis(1,at=seq(1,3,by=1),las=3,labels=c("fitness2","OffNader10xWeight","OffNader5xWeight))

The error I get is: Error in parse(file,n,text,prompt)

However, when I simply copy it in from my editor into R it works just fine.  
Is there something obvious that I am doing wrong?

A.



From Benjamin.STABLER at odot.state.or.us  Tue Jun 17 18:50:54 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 17 Jun 2003 09:50:54 -0700
Subject: [R] EMME/2 and shapefiles packages
Message-ID: <76A000A82289D411952F001083F9DD06047FE13B@exsalem4-bu.odot.state.or.us>

For those of you that are interested, the EMME/2 and shapefiles packages are
now on CRAN.  Let me know if you run into any bugs or have some suggestions.


Thanks,
Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From ripley at stats.ox.ac.uk  Tue Jun 17 18:51:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 17:51:32 +0100 (BST)
Subject: [R] outlier
In-Reply-To: <20030617163552.28872.qmail@web20704.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306171747300.17745-100000@gannet.stats>

On Tue, 17 Jun 2003, kan Liu wrote:

>  I want to calculate the R-squared between two variables. Can you advice
> me how to identify and remove the outliers before performing R-squared
> calculation?

Easy: you don't.  It make no sense to consider R^2 after arbitrary outlier 
removal: if I remove all but two points I get R^2 = 1!

R^2 is normally used to measure the success of a multiple regression, but 
as you mention two variables, did you just mean the Pearson 
product-moment correlation?  It makes more sense to use a robust measure 
of correlation, as in cov.rob (package lqs) or even Spearman or Kendall 
measures (cov.test in package ctest).

If you intended to do this for a multiple regression, you need to do some 
sort of robust regression and a use a robust measure of fit.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Tue Jun 17 18:57:04 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 17 Jun 2003 11:57:04 -0500
Subject: [R] source vs. editor
References: <200306170948.02834.anna@ptolemy.arc.nasa.gov>
Message-ID: <3EEF4860.7070109@pdf.com>



Anna H. Pryor wrote:
> When I am trying to use the source function to  read in some lines of code, I 
> get an error.  The code is simply a boxplot:
> 
> boxplot(s$fitness2weightedSum,s$OffNader10xWeightweightedSum,s$OffNader5xWeightweightedSum,main="weightedSum",col=8)
> axis(1,at=seq(1,3,by=1),las=3,labels=c("fitness2","OffNader10xWeight","OffNader5xWeight))
> 
> The error I get is: Error in parse(file,n,text,prompt)
> 
> However, when I simply copy it in from my editor into R it works just fine.  
> Is there something obvious that I am doing wrong?
> 
> A.

Are you sure it works when you copy *both* lines? There's a missing 
quote from the second line.

Regards,
Sundar



From bates at stat.wisc.edu  Tue Jun 17 19:02:30 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 17 Jun 2003 17:02:30 -0000
Subject: [R] Re: repeated measures analysis in R [was: (no subject)]
In-Reply-To: <184.1c92cbcf.2c20761b@aol.com>
References: <184.1c92cbcf.2c20761b@aol.com>
Message-ID: <6rznkgvdrq.fsf@bates4.stat.wisc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: binksdata.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030617/57ea9325/binksdata.txt

From spencer.graves at pdf.com  Tue Jun 17 19:08:39 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 17 Jun 2003 10:08:39 -0700
Subject: [R] outlier
References: <Pine.LNX.4.44.0306171747300.17745-100000@gannet.stats>
Message-ID: <3EEF4B17.6050105@pdf.com>

	  It is also wise to make scatterplots, as shown by the famous examples 
produced of 4 scatterplots with the same R^2, where the first shows the 
standard ellipsoid pattern implied by the assumptions while the other 
three indicate very clearly that the assumptions are incorrect.  See 
Anscombe (1973) "Graphs in Statistical Analysis", The American 
Statistician, 27: 17-22, reproduced in, e.g., du Toit, Steyn and Stumpf 
(1986) Graphical Exploratory Data Analysis (Springer).

hth.  spencer graves

Prof Brian Ripley wrote:
> On Tue, 17 Jun 2003, kan Liu wrote:
> 
> 
>> I want to calculate the R-squared between two variables. Can you advice
>>me how to identify and remove the outliers before performing R-squared
>>calculation?
> 
> 
> Easy: you don't.  It make no sense to consider R^2 after arbitrary outlier 
> removal: if I remove all but two points I get R^2 = 1!
> 
> R^2 is normally used to measure the success of a multiple regression, but 
> as you mention two variables, did you just mean the Pearson 
> product-moment correlation?  It makes more sense to use a robust measure 
> of correlation, as in cov.rob (package lqs) or even Spearman or Kendall 
> measures (cov.test in package ctest).
> 
> If you intended to do this for a multiple regression, you need to do some 
> sort of robust regression and a use a robust measure of fit.
>



From dicki017 at umn.edu  Tue Jun 17 19:08:53 2003
From: dicki017 at umn.edu (Ian Dickie)
Date: Tue, 17 Jun 2003 12:08:53 -0500
Subject: [R] Macintosh plots
Message-ID: <5F5E400D-A0E6-11D7-BC4C-0003931A92A2@umn.edu>

This may be a dumb question, but I've wasted several hours trying to 
find an answer....

When I do a plot with no parameters set eg: ' >plot(d1)' it comes up 
with a dark gray background and blue data points.  I can modify the 
data points using '>plot(d1, col = "red")', but I have not been able to 
modify the background color.  '>plot(d1, bg = "white")' does not work.  
Any help would be most appreciated.

Cheers,
Ian.



From simon_cawley at affymetrix.com  Tue Jun 17 19:21:01 2003
From: simon_cawley at affymetrix.com (Simon Cawley)
Date: Tue, 17 Jun 2003 10:21:01 -0700 (PDT)
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <1055552612.12970.45.camel@localhost>
Message-ID: <Pine.LNX.4.44.0306171018400.16761-100000@luino.neomorphic.com>




On 13 Jun 2003, Marc Schwartz wrote:

> Date: 13 Jun 2003 20:03:32 -0500
> From: Marc Schwartz <MSchwartz at MedAnalytics.com>
> To: Simon Cawley <simon_cawley at affymetrix.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] problem installing packages from source on win2k
> 
> On Fri, 2003-06-13 at 19:02, Simon Cawley wrote:
> > Dear R-helpers,
> > 
> > I'm having trouble compiling R packages from source on Win2K.  I
> > installed R 1.7.1beta [1] on my win2k machine [2], downloaded the
> > fields package as source [3] and tried but failed to install the
> > package [4].  I get the same problem with other packages, so it
> > doesn't appear to be limited to fields.  Installation of precompiled
> > packages seems to work fine.  I've been doing this on linux for a long
> > time and never ran into such an issue.
> > 
> > I've trawled the FAQ and the archives but I'm not finding anything. 
> > Any clues where I'm going wrong?
> > 
> > Thanks in advance,
> > 
> > -Simon
> > 
> > ##############
> 
> ...Lengthy notes SNIPPED
> 
> Simon, 
> 
> Unless I am missing something here, I think that you are making this
> more difficult than you need to.
> 
> Thanks I believe to Uwe Ligges, there appears to be a version of the
> fields package already compiled for R 1.7.x on Windows at:
> 
> http://cran.r-project.org/bin/windows/contrib/1.7/fields_1.3-1.zip
> 
> Is there a reason that you cannot use this?
> 
> HTH,
> 
> Marc Schwartz

Thanks Marc, but I was just using compilation of the fields package as an example.

What I'm really trying to resolve here is how to compile my own packages on win2k, compilation of existing well-tested packages is just a step along the way.

-S



From sfang at cs.concordia.ca  Tue Jun 17 19:34:15 2003
From: sfang at cs.concordia.ca (sfang@cs.concordia.ca)
Date: Tue, 17 Jun 2003 13:34:15 -0400
Subject: [R] How to call t.test() defined in t.test.R from C++ program
Message-ID: <1055871255.3eef511793b66@mailhost.cs.concordia.ca>

Hi,

I am a student in Computer Science. Currently, I wrote a program (as pipeline) 
using STL(standard template library)in Visual C++ to analysis microarry gene 
express data analysis. 

Part of my program is to perform two sample t-test and print out t=? , df=? , p-
value=? , 95 percent confidence interval, and mean of each sample, which is the 
same output as typing the commands ?library(ctest)? and ?t.test(A, B)? under R. 
Therefore, I am wondering if I am able to call t.test() function from R in my 
C++ program.

I have read all .c, .R and .Rd files in the library(ctest). I know t.test.R 
proferms the functions which I want. but I could not figure out the 
relationship between .R and .c files. Therefore, I do not know if I should (and 
how to) call t.test() function defined in the file "t.test.R?,  or I should to 
call some functions defined in .c files (such as chisqsim(), but it require 
many parameters) and which one I should to call and how to call.

I have read the R manuals "Writing R Extensions", and I have tried to run some 
examples in 4.7 "Handling R objects in C", such as, out() function, .call
("out", a, b), but it seems not work. 

My code (save as try.cpp) is,

#include <R.h>
#include <Rinternals.h>
#include <iostream>
using namespace std;

int main()
{
  double sampleData_1[10]={1.1, -1.22, -2.1, 3.2, 4.5, 1.22, 0.55, -0.99, 1.77, 
2.33};
  double sampleData_2[8]={1.222, -0.999, -3.99, 2.77, 0.89, 0.99, 1.77, 0.78};
  .call("t.test", sampleData_1, sampleData_2);
  return 0;
}

I have tried to compile it under RedHat Linus by typing:

gcc -I/pkg/R/lib/R/include try.cpp -o outputfile

I got error message "parse error before ."


Since I am a newer for R, could someone please show me how to do? I really 
appreciate.

By the way, could someone please tell me the formulate to get p-value from 
t_value and the number of samples? I would like to try coding the calculation 
directly from C++ if I am unable to call t.test() function from R?

Thank you very much for your help,


Shaozhen




-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From sfang at cs.concordia.ca  Tue Jun 17 19:37:04 2003
From: sfang at cs.concordia.ca (sfang@cs.concordia.ca)
Date: Tue, 17 Jun 2003 13:37:04 -0400
Subject: [R] How to call t.test() defined in t.test.R from C++ program 
Message-ID: <1055871424.3eef51c082b1c@mailhost.cs.concordia.ca>

Hi,

I am a student in Computer Science. Currently, I wrote a program (as pipeline) 
using STL(standard template library)in Visual C++ to analysis microarry gene 
express data analysis. 

Part of my program is to perform two sample t-test and print out t=? , df=? , p-
value=? , 95 percent confidence interval, and mean of each sample, which is the 
same output as typing the commands ?library(ctest)? and ?t.test(A, B)? under R. 
Therefore, I am wondering if I am able to call t.test() function from R in my 
C++ program.

I have read all .c, .R and .Rd files in the library(ctest). I know t.test.R 
proferms the functions which I want. but I could not figure out the 
relationship between .R and .c files. Therefore, I do not know if I should (and 
how to) call t.test() function defined in the file "t.test.R?,  or I should to 
call some functions defined in .c files (such as chisqsim(), but it require 
many parameters) and which one I should to call and how to call.

I have read the R manuals "Writing R Extensions", and I have tried to run some 
examples in 4.7 "Handling R objects in C", such as, out() function, .call
("out", a, b), but it seems not work. 

My code (save as try.cpp) is,

#include <R.h>
#include <Rinternals.h>
#include <iostream>
using namespace std;

int main()
{
  double sampleData_1[10]={1.1, -1.22, -2.1, 3.2, 4.5, 1.22, 0.55, -0.99, 1.77, 
2.33};
  double sampleData_2[8]={1.222, -0.999, -3.99, 2.77, 0.89, 0.99, 1.77, 0.78};
  .call("t.test", sampleData_1, sampleData_2);
  return 0;
}

I have tried to compile it under RedHat Linus by typing:

gcc -I/pkg/R/lib/R/include try.cpp -o outputfile

I got error message "parse error before ."

Since I am a newer for R, could someone please show me how to do? I really 
appreciate.

By the way, could someone please tell me the formulate to get p-value from 
t_value and the number of samples? I would like to try coding the calculation 
directly from C++ if I am unable to call t.test() function from R?

Thank you very much for your help,


Shaozhen






-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From ririzarr at jhsph.edu  Tue Jun 17 19:43:55 2003
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Tue, 17 Jun 2003 13:43:55 -0400 (EDT)
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <Pine.LNX.4.44.0306171018400.16761-100000@luino.neomorphic.com>
Message-ID: <Pine.LNX.4.33.0306171343340.21304-100000@localhost.localdomain>

simon, 

in case you havent done so already,
i carefully followed the instructions here:

http://www.stats.ox.ac.uk/pub/R/rw-FAQ.html

and everything worked fine.

hth,
-rafael


On Tue, 17 Jun 2003, Simon Cawley wrote:

> 
> 
> 
> On 13 Jun 2003, Marc Schwartz wrote:
> 
> > Date: 13 Jun 2003 20:03:32 -0500
> > From: Marc Schwartz <MSchwartz at MedAnalytics.com>
> > To: Simon Cawley <simon_cawley at affymetrix.com>
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] problem installing packages from source on win2k
> > 
> > On Fri, 2003-06-13 at 19:02, Simon Cawley wrote:
> > > Dear R-helpers,
> > > 
> > > I'm having trouble compiling R packages from source on Win2K.  I
> > > installed R 1.7.1beta [1] on my win2k machine [2], downloaded the
> > > fields package as source [3] and tried but failed to install the
> > > package [4].  I get the same problem with other packages, so it
> > > doesn't appear to be limited to fields.  Installation of precompiled
> > > packages seems to work fine.  I've been doing this on linux for a long
> > > time and never ran into such an issue.
> > > 
> > > I've trawled the FAQ and the archives but I'm not finding anything. 
> > > Any clues where I'm going wrong?
> > > 
> > > Thanks in advance,
> > > 
> > > -Simon
> > > 
> > > ##############
> > 
> > ...Lengthy notes SNIPPED
> > 
> > Simon, 
> > 
> > Unless I am missing something here, I think that you are making this
> > more difficult than you need to.
> > 
> > Thanks I believe to Uwe Ligges, there appears to be a version of the
> > fields package already compiled for R 1.7.x on Windows at:
> > 
> > http://cran.r-project.org/bin/windows/contrib/1.7/fields_1.3-1.zip
> > 
> > Is there a reason that you cannot use this?
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> Thanks Marc, but I was just using compilation of the fields package as an example.
> 
> What I'm really trying to resolve here is how to compile my own packages on win2k, compilation of existing well-tested packages is just a step along the way.
> 
> -S
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From macq at llnl.gov  Tue Jun 17 20:02:05 2003
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 17 Jun 2003 11:02:05 -0700
Subject: [R] make check and postscript output in R 1.7.1
Message-ID: <p05210608bb1507a46a9d@[128.115.153.6]>

I think this might be telling us that reg-plot.ps.save needs to be 
updated for the new version of postscript().

comparing 'reg-plot.ps' to '../../source/tests/reg-plot.ps.save' ...100c100
< 663.53 0 l
---
>  663.53 -0.00 l
104c104
< 0 -7.20 l
---
>  -0.00 -7.20 l

-Don
-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ripley at stats.ox.ac.uk  Tue Jun 17 20:13:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 19:13:18 +0100 (BST)
Subject: [R] make check and postscript output in R 1.7.1
In-Reply-To: <p05210608bb1507a46a9d@[128.115.153.6]>
Message-ID: <Pine.LNX.4.44.0306171906580.17900-100000@gannet.stats>

It's telling us that your machine gives different output from *all* the 
other machines that were checked in the beta-test period.   So what 
machine is this that no one beta tested?   (In future please do 
participate in the beta test so your unusual system gets tested.)

As Rob G has reported this from FreeBSD earlier today it is probably 
already fixed in R-devel (it is for him).  Fortunately the difference is 
in this case negligible.

On Tue, 17 Jun 2003, Don MacQueen wrote:

> I think this might be telling us that reg-plot.ps.save needs to be 
> updated for the new version of postscript().
> 
> comparing 'reg-plot.ps' to '../../source/tests/reg-plot.ps.save' ...100c100
> < 663.53 0 l
> ---
> >  663.53 -0.00 l
> 104c104
> < 0 -7.20 l
> ---
> >  -0.00 -7.20 l

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sfang at cs.concordia.ca  Tue Jun 17 20:47:55 2003
From: sfang at cs.concordia.ca (sfang@cs.concordia.ca)
Date: Tue, 17 Jun 2003 14:47:55 -0400
Subject: [R] How to call t.test() defined in t.test.R from C++ program
Message-ID: <1055875675.3eef625b8ab1d@mailhost.cs.concordia.ca>



Dear r-helper,

I am a student in Computer Science. Currently, I wrote a program (as pipeline) 
using STL(standard template library)in Visual C++ to analysis microarry gene 
express data analysis. 

Part of my program is to perform two sample t-test and print out t=? , df=? , p-
value=? , 95 percent confidence interval, and mean of each sample, which is the 
same output as typing the commands ?library(ctest)? and ?t.test(A, B)? under R. 
Therefore, I am wondering if I am able to call t.test() function from R in my 
C++ program.

I have read all .c, .R and .Rd files in the library(ctest). I know t.test.R 
proferms the functions which I want. but I could not figure out the 
relationship between .R and .c files. Therefore, I do not know if I should (and 
how to) call t.test() function defined in the file "t.test.R?,  or I should to 
call some functions defined in .c files (such as chisqsim(), but it require 
many parameters) and which one I should to call and how to call.

I have read the R manuals "Writing R Extensions", and I have tried to run some 
examples in 4.7 "Handling R objects in C", such as, out() function, .call
("out", a, b), but I could not make it work. Also, I got some trouble with the 
data types. 

My code (save as try.cpp) is,

#include <R.h>
#include <Rinternals.h>
#include <iostream>
using namespace std;

int main()
{
  double sampleData_1[9]={1.1, -1.22, -2.1, 3.2, 4.5, 1.22, 0.55, 1.77, 2.33};
  double sampleData_2[8]={1.222, -0.999, -3.99, 2.77, 0.89, 0.99, 1.77, 0.78};
  .call("t.test", sampleData_1, sampleData_2);
  return 0;
}

I have tried to compile it under RedHat Linus by typing:

gcc -I/pkg/R/lib/R/include try.cpp -o outputfile

I got error message "parse error before ."


Since I am a newer for R, could you please show me how to do? I really 
appreciate.

By the way, could you please tell me the formulate to get p-value from 
t_value and the number of samples? I would like to try coding the calculation 
directly from C++ if I am unable to call t.test() function from R?

Thank you very much for your help,


Shaozhen

 
 



-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From andel at ifi.unizh.ch  Tue Jun 17 20:49:32 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Tue, 17 Jun 2003 18:49:32 -0000
Subject: [R] How to generate a pairwise non-parametric comparison table?
Message-ID: <Pine.GSO.4.44.0306171929190.8201-100000@igor.ifi.unizh.ch>

Dear list

I am comparing the results of several different experimental setups. With kruskal.test() I can test if there is any difference at all in any of them, if I understand it correctly. But now, when there is a difference, how do I generate a (half-) table of pairwise comparisons, using e.g. wilcox.test(), to find the ones where the difference actually occurs.
I guess I don't have to program this by hand, do I?

Thanks,
David



From macq at llnl.gov  Tue Jun 17 21:21:04 2003
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 17 Jun 2003 12:21:04 -0700
Subject: [R] make check and postscript output in R 1.7.1
In-Reply-To: <Pine.LNX.4.44.0306171906580.17900-100000@gannet.stats>
References: <Pine.LNX.4.44.0306171906580.17900-100000@gannet.stats>
Message-ID: <p0521060abb151607c9ec@[128.115.153.6]>

I'm sorry, I neglected to provide version information.

In fact, I see the same result from make check on two different 
systems: Solaris 2.7 and OS X 10.2.6.

The "-0.00" values in reg-plot.ps.save are the kinds of values I saw 
in postscript files generated by R 1.7.0, i.e., prior to the fix of 
PR#3132.

In particular, in his r-help message on 2003-5-29 on the topic 
"Postscript query: plotting long vectors", Peter Dalgaard noted "If 
you want to use rlineto, you need to round *before* differencing." 
Values like "-0.00" could result from rounding *after* differencing, 
could they not?

My earlier message was from the OS X build. This is from make check on Solaris:
comparing 'reg-plot.ps' to '../../source/tests/reg-plot.ps.save' ...100c100
< 663.53 0 l
---
>  663.53 -0.00 l
104c104
< 0 -7.20 l
---
>  -0.00 -7.20 l

On the OS X system:
configure --prefix=/Users/macq/R/R-1.7.1 \
    --enable-R-shlib --with-blas="-framework vecLib" --with-lapack \
    CPPFLAGS="-I/sw/include" LDFLAGS="-L/sw/lib"

On the Solaris system
source/configure --prefix=/erd/statistic/apps/R/R-1.7.1\
    --with-tcl-config=/erd/statistic/apps/R/tcltk/tcl8.3.3/unix/tclConfig.sh\
    --with-tk-config=/erd/statistic/apps/R/tcltk/tk8.3.3/unix/tkConfig.sh\
    --enable-R-shlib PERL=/apps/gnu/perl5/perl5.8.0/bin/perl

-Don

At 7:13 PM +0100 6/17/03, Prof Brian Ripley wrote:
>It's telling us that your machine gives different output from *all* the
>other machines that were checked in the beta-test period.   So what
>machine is this that no one beta tested?   (In future please do
>participate in the beta test so your unusual system gets tested.)
>
>As Rob G has reported this from FreeBSD earlier today it is probably
>already fixed in R-devel (it is for him).  Fortunately the difference is
>in this case negligible.
>
>On Tue, 17 Jun 2003, Don MacQueen wrote:
>
>>  I think this might be telling us that reg-plot.ps.save needs to be
>>  updated for the new version of postscript().
>>
>>  comparing 'reg-plot.ps' to '../../source/tests/reg-plot.ps.save' ...100c100
>>  < 663.53 0 l
>>  ---
>>  >  663.53 -0.00 l
>>  104c104
>>  < 0 -7.20 l
>>  ---
>>  >  -0.00 -7.20 l
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From p.dalgaard at biostat.ku.dk  Tue Jun 17 21:27:21 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 17 Jun 2003 19:27:21 -0000
Subject: [R] How to generate a pairwise non-parametric comparison table?
In-Reply-To: <Pine.GSO.4.44.0306171929190.8201-100000@igor.ifi.unizh.ch>
References: <Pine.GSO.4.44.0306171929190.8201-100000@igor.ifi.unizh.ch>
Message-ID: <x2wufkzebg.fsf@biostat.ku.dk>

"David Andel" <andel at ifi.unizh.ch> writes:

> Dear list
> 
> I am comparing the results of several different experimental setups. With kruskal.test() I can test if there is any difference at all in any of them, if I understand it correctly. But now, when there is a difference, how do I generate a (half-) table of pairwise comparisons, using e.g. wilcox.test(), to find the ones where the difference actually occurs.
> I guess I don't have to program this by hand, do I?

You do. Believe it or not, it is pairwise.wilcox.test()

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Jun 17 23:18:22 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 17 Jun 2003 21:18:22 -0000
Subject: [R] make check and postscript output in R 1.7.1
In-Reply-To: <p0521060abb151607c9ec@[128.115.153.6]>
References: <Pine.LNX.4.44.0306171906580.17900-100000@gannet.stats>
	<p0521060abb151607c9ec@[128.115.153.6]>
Message-ID: <x2smq8z960.fsf@biostat.ku.dk>

Don MacQueen <macq at llnl.gov> writes:

> I'm sorry, I neglected to provide version information.
> 
> In fact, I see the same result from make check on two different
> systems: Solaris 2.7 and OS X 10.2.6.
> 
> The "-0.00" values in reg-plot.ps.save are the kinds of values I saw
> in postscript files generated by R 1.7.0, i.e., prior to the fix of
> PR#3132.
> 
> In particular, in his r-help message on 2003-5-29 on the topic
> "Postscript query: plotting long vectors", Peter Dalgaard noted "If
> you want to use rlineto, you need to round *before* differencing."
> Values like "-0.00" could result from rounding *after* differencing,
> could they not?

No, I don't think so. However, differencing after rounding (to two
digits) plus slightly odd formatting routines might do it. You have
the source... The logic is this:

static void PostScriptRLineTo(FILE *fp, double x0, double y0,
                              double x1, double y1)
{
    double x = rround(x1, 2) - rround(x0, 2),
        y = rround(y1, 2) - rround(y0, 2);

    if(x == 0) fprintf(fp, "0"); else fprintf(fp, "%.2f", x);
    if(y == 0) fprintf(fp, " 0"); else fprintf(fp, " %.2f", y);
    fprintf(fp, " l\n");
}

and it would seem that on some machines, rround(.,2) is not a perfect
step function, in that you can have two numbers rounded to slightly
different values.

(I see the same effect on Solaris 9, gcc 2.95.3 BTW. So you're not the
only one forgetting to platform test. It's just that our Sun is so
terribly slooooow...)
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kan_liu1 at yahoo.com  Tue Jun 17 23:24:15 2003
From: kan_liu1 at yahoo.com (kan Liu)
Date: Tue, 17 Jun 2003 14:24:15 -0700 (PDT)
Subject: [R] outlier
In-Reply-To: <3EEF4B17.6050105@pdf.com>
Message-ID: <20030617212415.79357.qmail@web20706.mail.yahoo.com>

Hi, many thanks for your advice. I appreciate very
much. Maybe I can make the question more clear: I want
to evaluate the correlation between two variables: one
is the actual outputs of a system, another is the
predicted values of the outputs of the system using
neural networks. When I made scatterplots in excel, I
can get the linear equation and the corresponding
R-squared. In the bottom of the page
http://www.statsoftinc.com/textbook/stathome.html, it
mentioned that sometimes outliers will affect
correlation coefficient biasly. So I thought it might
be worth to remove outlier before  calculating
R-squared in R. It seems to be a bad idea according to
your comments. Now can you make comments on how to
evaluate the performance of the neural network model
in predicting the actual outputs?

Kan 

--- Spencer Graves <spencer.graves at PDF.COM> wrote:
> 	  It is also wise to make scatterplots, as shown by
> the famous examples 
> produced of 4 scatterplots with the same R^2, where
> the first shows the 
> standard ellipsoid pattern implied by the
> assumptions while the other 
> three indicate very clearly that the assumptions are
> incorrect.  See 
> Anscombe (1973) "Graphs in Statistical Analysis",
> The American 
> Statistician, 27: 17-22, reproduced in, e.g., du
> Toit, Steyn and Stumpf 
> (1986) Graphical Exploratory Data Analysis
> (Springer).
> 
> hth.  spencer graves
> 
> Prof Brian Ripley wrote:
> > On Tue, 17 Jun 2003, kan Liu wrote:
> > 
> > 
> >> I want to calculate the R-squared between two
> variables. Can you advice
> >>me how to identify and remove the outliers before
> performing R-squared
> >>calculation?
> > 
> > 
> > Easy: you don't.  It make no sense to consider R^2
> after arbitrary outlier 
> > removal: if I remove all but two points I get R^2
> = 1!
> > 
> > R^2 is normally used to measure the success of a
> multiple regression, but 
> > as you mention two variables, did you just mean
> the Pearson 
> > product-moment correlation?  It makes more sense
> to use a robust measure 
> > of correlation, as in cov.rob (package lqs) or
> even Spearman or Kendall 
> > measures (cov.test in package ctest).
> > 
> > If you intended to do this for a multiple
> regression, you need to do some 
> > sort of robust regression and a use a robust
> measure of fit.
> > 
> 
> 


__________________________________

SBC Yahoo! DSL - Now only $29.95 per month!



From MSchwartz at medanalytics.com  Tue Jun 17 23:24:49 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 17 Jun 2003 21:24:49 -0000
Subject: [R] problem installing packages from source on win2k
In-Reply-To: <Pine.LNX.4.44.0306171018400.16761-100000@luino.neomorphic.com>
References: <Pine.LNX.4.44.0306171018400.16761-100000@luino.neomorphic.com>
Message-ID: <1055885032.6231.29.camel@localhost>

On Tue, 2003-06-17 at 12:21, Simon Cawley wrote:
> 
> On 13 Jun 2003, Marc Schwartz wrote:
> > 
> > ...Lengthy notes SNIPPED
> > 
> > Simon, 
> > 
> > Unless I am missing something here, I think that you are making this
> > more difficult than you need to.
> > 
> > Thanks I believe to Uwe Ligges, there appears to be a version of the
> > fields package already compiled for R 1.7.x on Windows at:
> > 
> > http://cran.r-project.org/bin/windows/contrib/1.7/fields_1.3-1.zip
> > 
> > Is there a reason that you cannot use this?
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> Thanks Marc, but I was just using compilation of the fields package as
> an example.
> 
> What I'm really trying to resolve here is how to compile my own
> packages on win2k, compilation of existing well-tested packages is
> just a step along the way.
> 
> -S


Simon, 

Fair enough.  I think that others have already pointed to specific
issues relative to the lack of the MS help compiler in the errors that
you were getting, but more generically, let me point you to Prof.
Ripley's page which provides more detail on the overall process of
compiling R and packages under Windows:

http://www.stats.ox.ac.uk/pub/Rtools/

Links are provided there for securing the various components required,
including Prof. Ripley's Tool bundle, which is essential and invaluable
in these efforts. There is also a link there to get the MS Help compiler
for generating the .CHM files, which is the error you experienced.

Be sure to read, thoroughly, the readme files and directions Prof.
Ripley has included as the details, especially environmental
configuration, are critical.

Lastly, be sure to read "Writing R Extensions' if you have not already.
This is one of the documentation manuals provided in the distribution
and available from the help menu in RGui.

HTH,

Marc



From ripley at stats.ox.ac.uk  Tue Jun 17 23:30:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jun 2003 22:30:44 +0100 (BST)
Subject: [R] outlier
In-Reply-To: <20030617212415.79357.qmail@web20706.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306172225280.20698-100000@gannet.stats>

On Tue, 17 Jun 2003, kan Liu wrote:

> Hi, many thanks for your advice. I appreciate very
> much. Maybe I can make the question more clear: I want
> to evaluate the correlation between two variables: one
> is the actual outputs of a system, another is the
> predicted values of the outputs of the system using
> neural networks. When I made scatterplots in excel, I
> can get the linear equation and the corresponding
> R-squared. In the bottom of the page
> http://www.statsoftinc.com/textbook/stathome.html, it
> mentioned that sometimes outliers will affect
> correlation coefficient biasly. So I thought it might
> be worth to remove outlier before  calculating
> R-squared in R. It seems to be a bad idea according to
> your comments. 

Yes. That's the whole point of robust methods: compensate rather than 
reject.

> Now can you make comments on how to
> evaluate the performance of the neural network model
> in predicting the actual outputs?

If you are interested in correlation coefficients, use cov.rob. However,
this is predicted vs actual, and you probably do want to penalize bad
predictions, not reject them.  It's up to you to choose a suitable loss
function for your application.  In particular, if the predicted values
were always 1e-45 times the actual values minus 1e310, the correlation
would be one and the predictions would be derisory.

> 
> Kan 
> 
> --- Spencer Graves <spencer.graves at PDF.COM> wrote:
> > 	  It is also wise to make scatterplots, as shown by
> > the famous examples 
> > produced of 4 scatterplots with the same R^2, where
> > the first shows the 
> > standard ellipsoid pattern implied by the
> > assumptions while the other 
> > three indicate very clearly that the assumptions are
> > incorrect.  See 
> > Anscombe (1973) "Graphs in Statistical Analysis",
> > The American 
> > Statistician, 27: 17-22, reproduced in, e.g., du
> > Toit, Steyn and Stumpf 
> > (1986) Graphical Exploratory Data Analysis
> > (Springer).
> > 
> > hth.  spencer graves
> > 
> > Prof Brian Ripley wrote:
> > > On Tue, 17 Jun 2003, kan Liu wrote:
> > > 
> > > 
> > >> I want to calculate the R-squared between two
> > variables. Can you advice
> > >>me how to identify and remove the outliers before
> > performing R-squared
> > >>calculation?
> > > 
> > > 
> > > Easy: you don't.  It make no sense to consider R^2
> > after arbitrary outlier 
> > > removal: if I remove all but two points I get R^2
> > = 1!
> > > 
> > > R^2 is normally used to measure the success of a
> > multiple regression, but 
> > > as you mention two variables, did you just mean
> > the Pearson 
> > > product-moment correlation?  It makes more sense
> > to use a robust measure 
> > > of correlation, as in cov.rob (package lqs) or
> > even Spearman or Kendall 
> > > measures (cov.test in package ctest).
> > > 
> > > If you intended to do this for a multiple
> > regression, you need to do some 
> > > sort of robust regression and a use a robust
> > measure of fit.
> > > 
> > 
> > 
> 
> 
> __________________________________
> Do you Yahoo!?
> SBC Yahoo! DSL - Now only $29.95 per month!
> http://sbc.yahoo.com
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cham8080 at uidaho.edu  Wed Jun 18 00:21:42 2003
From: cham8080 at uidaho.edu (cham8080@uidaho.edu)
Date: Tue, 17 Jun 2003 15:21:42 -0700
Subject: [R] per mil in plotmath
Message-ID: <84a09884517c.84517c84a098@uidaho.edu>

Hi,

Does anyone know how to put a per mil symbol in a plot y-label?

thanks
chris



From p.murrell at auckland.ac.nz  Wed Jun 18 00:41:05 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 18 Jun 2003 10:41:05 +1200
Subject: [R] per mil in plotmath
References: <84a09884517c.84517c84a098@uidaho.edu>
Message-ID: <3EEF9901.7020706@stat.auckland.ac.nz>

Hi


cham8080 at uidaho.edu wrote:
> Hi,
> 
> Does anyone know how to put a per mil symbol in a plot y-label?


See a previous R-help post on this topic 
https://www.stat.math.ethz.ch/pipermail/r-help/2002-November/025906.html

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz



From minka at stat.cmu.edu  Wed Jun 18 02:09:37 2003
From: minka at stat.cmu.edu (Tom Minka)
Date: Tue, 17 Jun 2003 20:09:37 -0400
Subject: [R] suggestion for make.names
Message-ID: <16111.44481.905860.302041@weird.stat.cmu.edu>

I would like to suggest a modification to the make.names() function.
The current implementation has two problems: 
1. It doesn't check if a name matches an R keyword (like "function").
2. The uniqueness algorithm is not invariant to concatenation.
   In other words, 
   make.names(c("a","a","a"),unique=T) != 
   make.names(c(make.names(c("a","a"),unique=T),"a"),unique=T)

The first problem means that you can construct a data frame for which 
there is no valid formula:
  lm(if~then, data.frame("if"=3,"then"=4))

The second problem means that you get funny row names when you build
up a data frame by concatenation:
  rbind(data.frame(x=1),data.frame(x=2),data.frame(x=3)) !=
  rbind(rbind(data.frame(x=1),data.frame(x=2)),data.frame(x=3))

I'm providing a new implementation (and documentation) of make.names which
fixes these problems.  The uniqueness part is handled by make.unique, a
useful function in its own right.  For example, the following code in
rbind.data.frame:
    while(any(xj <- duplicated(rlabs)))
    rlabs[xj] <- paste(rlabs[xj], 1:sum(xj), sep = "")
could be replaced by:
    rlabs = make.unique(rlabs)

Another way to fix the first problem, by the way, would be to allow
quoting in formulas, e.g. lm("if" ~ then, ...).  Currently, when you
do this it gives an error.  This could eliminate the need for
make.names in many cases, by being able to quote names like 
"New Jersey" or "log(x)".

Tom Minka
www.stat.cmu.edu/~minka/


From glaziou at pasteur-kh.org  Wed Jun 18 04:04:32 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Wed, 18 Jun 2003 09:04:32 +0700
Subject: [R] Re: your mail
In-Reply-To: <auto-000048296737@cgatepro-2.mail.virginia.edu>
References: <auto-000048296737@cgatepro-2.mail.virginia.edu>
Message-ID: <20030618020432.GB579@pasteur-kh.org>

Frank E Harrell Jr <fharrell at virginia.edu> wrote:
> Dear Patrick - That was very helpful, especially the
> output comparing the old and new latex( ) function results
> - thanks.  I'm sorry Philippe that I did not see the
> problem before.  If you edit the source code for
> latex.default by adding the line
> 
>     extracolheads <- c('', extracolheads)
> 
> after the line
> 
>     col.just <- c(rowlabel.just, col.just)
> 
> the problem should be fixed.  The next release of Hmisc
> will have this fix.
 

Thanks a lot Frank. This fixes the problem and also
impresses me a lot. I have not seen such a level of support
and speed of reaction to fix minor bugs very often.

Best regards,

-- 
Philippe



From mpiorecky at hotmail.com  Wed Jun 18 04:19:32 2003
From: mpiorecky at hotmail.com (Mark Piorecky)
Date: Tue, 17 Jun 2003 20:19:32 -0600
Subject: [R] (no subject)
Message-ID: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030617/8ab8e4c3/attachment.pl

From MSchwartz at medanalytics.com  Wed Jun 18 04:44:24 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 18 Jun 2003 02:44:24 -0000
Subject: [R] Missing 'make' command to compile Windows DLL - was (no
	subject)
In-Reply-To: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
References: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
Message-ID: <1055904254.6231.164.camel@localhost>

On Tue, 2003-06-17 at 21:19, Mark Piorecky wrote:
> Hi all,
> 
> I'm new to R and am trying to create a dll in order to be able to use
> the "dyn.load" function with a fortran script.  I have a windows OS,
> have installed perl and the Path is recognized.  I have also installed
> R 1.70 including the src (source code) folder..  But when I attempt to
> create the dll by excecuting "Rcmd SHLIB [-o autologdll] combo.f", in
> the windows command prompt, I get the following error message "'make'
> is not recognized as an internal or external command"
> 
> Does anyone have some suggestions on what I may have missed?
> 
> Thanx a ton!
> 
> Mark


Sounds like you did not download and install Prof. Ripley's Tools
package or if you did, the PATH environment variable is not set
properly. 

If you did not download the Tools package, go here:

http://www.stats.ox.ac.uk/pub/Rtools/

and download the Unix tools ZIP file in the Essentials section and read
the remainder of the page, which provides further guidance.

If you did download and extract that package, be sure to edit the PATH
to reflect the location of those files.

BTW, don't forget to update to R 1.7.1, which is now released and
available on CRAN.

HTH,

Marc Schwartz



From kwan022 at stat.auckland.ac.nz  Wed Jun 18 04:51:08 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 18 Jun 2003 14:51:08 +1200 (NZST)
Subject: Building R Packages (was: [R] (no subject)
In-Reply-To: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306181447200.1244-100000@stat61.stat.auckland.ac.nz>

1) Please use some meaningful Subject.

2) Have you read Prof Ripley's http://www.stats.ox.ac.uk/pub/Rtools/ 
(hint: you may want to download the tools.zip file).

On Tue, 17 Jun 2003, Mark Piorecky wrote:

> Date: Tue, 17 Jun 2003 20:19:32 -0600
> From: Mark Piorecky <mpiorecky at hotmail.com>
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> Hi all,
> 
> I'm new to R and am trying to create a dll in order to be able to use the "dyn.load" function with a fortran script.  I have a windows OS, have installed perl and the Path is recognized.  I have also installed R 1.70 including the src (source code) folder.  But when I attempt to create the dll by excecuting "Rcmd SHLIB [-o autologdll] combo.f", in the windows command prompt, I get the following error message "'make' is not recognized as an internal or external command"
> 
> Does anyone have some suggestions on what I may have missed?
> 
> Thanx a ton!
> 
> Mark
> 	[[alternate HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ken_lee at tynesys.com  Wed Jun 18 05:04:12 2003
From: ken_lee at tynesys.com (Ken Lee)
Date: Wed, 18 Jun 2003 11:04:12 +0800
Subject: [R] globle variable
In-Reply-To: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
Message-ID: <FFEKIEFDONDECJDODGDJCEMGCHAA.ken_lee@tynesys.com>

Hi all,
    Could I  claim globle variable?
  
 Thank's 

Ken



From Arnaud.Dowkiw at dpi.qld.gov.au  Wed Jun 18 06:46:59 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Wed, 18 Jun 2003 14:46:59 +1000
Subject: [R] install pls.pcr package
Message-ID: <C2C6EA6C4DADB348BFDF58894B039012012741F7@kinsrv001.dpi.qld.gov.au>

How do you install a package from CRAN ? I want to install pls.pcr, so I have downloaded pls.pcr_0.1.1.tar.gz  but when I try to install it using the install package(s) from local zip file(s) option it says : 

> install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)
Error in file(file, "r") : unable to open connection
In addition: Warning messages: 
1: error 1 in extracting from zip file 
2: cannot open file `pls.pcr_0.1.1.tar.gz/DESCRIPTION' 

Thanks a lot,

Arnaud


*************************
Arnaud DOWKIW
Department of Primary Industries
J. Bjelke-Petersen Research Station
KINGAROY, QLD 4610
Australia
T : + 61 7 41 600 700
T : + 61 7 41 600 728 (direct)
F : + 61 7 41 600 760
**************************
 

********************************DISCLAIMER**********************... {{dropped}}



From kwan022 at stat.auckland.ac.nz  Wed Jun 18 06:58:55 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 18 Jun 2003 16:58:55 +1200 (NZST)
Subject: [R] install pls.pcr package
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B039012012741F7@kinsrv001.dpi.qld.gov.au>
Message-ID: <Pine.LNX.4.44.0306181654450.1772-100000@stat61.stat.auckland.ac.nz>

I am assuming you are using Windows.  So why did you download the source 
file?  If you are using the Packages -> Install packages from local zip 
files menu, why not just choose Install packages from CRAN, then select 
pls.pcr?  

Alternatively if you want to download the "compiled" package, you should 
do it under http://cran.r-project.org/bin/windows/contrib

Please refer to Section 3.1 of the R for Windows FAQ for details 
(http://cran.r-project.org/bin/windows/rw-FAQ.html)

On Wed, 18 Jun 2003, Dowkiw, Arnaud wrote:

> Date: Wed, 18 Jun 2003 14:46:59 +1000
> From: "Dowkiw, Arnaud" <Arnaud.Dowkiw at dpi.qld.gov.au>
> To: r-help at stat.math.ethz.ch
> Subject: [R] install pls.pcr package
> 
> How do you install a package from CRAN ? I want to install pls.pcr, so I have downloaded pls.pcr_0.1.1.tar.gz  but when I try to install it using the install package(s) from local zip file(s) option it says : 
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From gb at stat.umu.se  Wed Jun 18 08:13:23 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 18 Jun 2003 08:13:23 +0200 (CEST)
Subject: [R] install pls.pcr package
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B039012012741F7@kinsrv001.dpi.qld.gov.au>
Message-ID: <Pine.LNX.4.44.0306180807490.31582-100000@tal.stat.umu.se>

On Wed, 18 Jun 2003, Dowkiw, Arnaud wrote:

> How do you install a package from CRAN ? I want to install pls.pcr, so I have downloaded pls.pcr_0.1.1.tar.gz  but when I try to install it using the install package(s) from local zip file(s) option it says : 
> 
> > install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)
> Error in file(file, "r") : unable to open connection
> In addition: Warning messages: 
> 1: error 1 in extracting from zip file 
> 2: cannot open file `pls.pcr_0.1.1.tar.gz/DESCRIPTION' 

On Windows? As the error message suggest, use the .zip file, not the 
.tar.gz one. You can also install directly from CRAN, without explicit 
download.

G?ran



From fharrell at virginia.edu  Tue Jun 17 18:35:18 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Tue, 17 Jun 2003 12:35:18 -0400
Subject: [R] Re: R: Problem from Philippe Glaziou
In-Reply-To: <000001c333dc$beb599e0$5200a8c0@rechner2>
References: <000001c333dc$beb599e0$5200a8c0@rechner2>
Message-ID: <auto-000048296737@cgatepro-2.mail.virginia.edu>

On Mon, 16 Jun 2003 09:56:07 +0200
Patrick Hausmann <p.hausmann at mr-regionalberatung.de> wrote:

> Dear Frank,
> 
> I can reproduce the problem from Philippe Glaziou (see attachment).
> Maybe this can help.
> Best,
> Patrick
> 
> Am Wollelager 11
> 27749 Delmenhorst
> Tel. 04221 96373-0 
> Fax 04221 96373-29
> 
> http://www.mr-regionalberatung.de
> 
Dear Patrick - That was very helpful, especially the output comparing the old and new latex( ) function results - thanks.  I'm sorry Philippe that I did not see the problem before.  If you edit the source code for latex.default by adding the line

    extracolheads <- c('', extracolheads)

after the line

    col.just <- c(rowlabel.just, col.just)

the problem should be fixed.  The next release of Hmisc will have this fix.

Frank


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 18 08:53:36 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 18 Jun 2003 07:53:36 +0100 (BST)
Subject: [R] Query: Sd2Rd and nroff macros in S docs
Message-ID: <XFMail.030618075336.Ted.Harding@nessie.mcc.ac.uk>

Documentation for S3 functions is apparently written in troff markup
with macro tags like

  .BG  .FN  .TL  .DN  .CS  ...

Inspection of S3 documentation source files gives a pretty clear idea
of what these mean, semantically (and Sd2Rd is a perl script which
converts this markup into the Rd format, providing further semantic
information along the way).

My query is: Can anyone point to troff macro definitions for these tags?

(And, preferably, also to descriptions for their usage)

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Jun-03                                       Time: 07:53:36
------------------------------ XFMail ------------------------------



From rdiaz at cnio.es  Wed Jun 18 09:43:19 2003
From: rdiaz at cnio.es (Ramon Diaz)
Date: Wed, 18 Jun 2003 09:43:19 +0200
Subject: [R] repeated measures ANOVA (was (no subject))
In-Reply-To: <184.1c92cbcf.2c20761b@aol.com>
References: <184.1c92cbcf.2c20761b@aol.com>
Message-ID: <200306180943.19843.rdiaz@cnio.es>

Dear Jessie,

Why don't you try to use already written functions? Probably lme (in package 
nlme) would be what you need. lme comes with documentation, and there is a 
whole book devoted to it.

Best,

Ram?n

P.S. It is helpful if you use the "Subject" line, so that people now what your 
message is about, and so that messages can be stored and searched by subject.




On Tuesday 17 June 2003 15:48, Binksjess at aol.com wrote:
> Hello,
>
> I'm trying to write code for a repeated measures ANOVA.  To put things in
> perspective, I'll describe my experiment (briefly).  I have a 2X2 factorial
> design with pH (5.5, 6.5) and local community (present, absent) as my
> treatments. I had plastic enclosures that I sampled across five weeks with
> the density of 7 species acting as my response variables.  I'm analyzing
> one species at a time and doing Bonferonni corrections after the analysis. 
> I've attached an example of one of my data files showing the treatments and
> the change in density (of one species) over 5 weeks.  Hopefully someone can
> shed some light on this question: how to write a repeated measures ANOVA
> code in R?  Thanks.  Hope to hear soon.
>
> Jessie Binks
>
> You can email a response to: jbinks at utm.utoronto.ca

-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz



From ripley at stats.ox.ac.uk  Wed Jun 18 09:47:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 08:47:31 +0100 (BST)
Subject: [R] Query: Sd2Rd and nroff macros in S docs
In-Reply-To: <XFMail.030618075336.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0306180840170.28936-100000@gannet.stats>

On Wed, 18 Jun 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Documentation for S3 functions is apparently written in troff markup
> with macro tags like
> 
>   .BG  .FN  .TL  .DN  .CS  ...
> 
> Inspection of S3 documentation source files gives a pretty clear idea
> of what these mean, semantically (and Sd2Rd is a perl script which
> converts this markup into the Rd format, providing further semantic
> information along the way).
> 
> My query is: Can anyone point to troff macro definitions for these tags?

You need to look in an S3 Unix system for that: they are presumably
copyright AT&T.  Look in the file SHOME/cmd/help.nr.

> (And, preferably, also to descriptions for their usage)

That's harder.  There have been some partial descriptions in long-obselete 
S-PLUS manuals.

Why do you want to do this?  The last system that used such markup was
S-PLUS 3.4, released in about 1996 as I recall.  (You might do better 
asking S questions on S-news?)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From H.RINNER at tirol.gv.at  Wed Jun 18 09:59:38 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Wed, 18 Jun 2003 09:59:38 +0200
Subject: [R] downloading packages and AntiVirus program
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE23E@xms1.tirol.gv.at>

Dear R-users!

I am using R 1.7.0, under Windows XP;
I also have Internet Explorer 6.0.2600.0000, Norton AntiVirus 7.60.926.

Our firewall seems to want to "protect" me from downloading precompiled
packages for Windows.
When I try to download packages, like
http://cran.at.r-project.org/bin/windows/contrib/1.7/RODBC_1.0-3.zip
for example, I am not allowed to and get a message like this:

"The content you just requested had a problem and was blocked by the
Symantec AntiVirus Scan Engine based on local administrator settings."

I called our network hotline, but they insisted that these files contain
viruses and wouldn't let me download them. As I'm sure that not all packages
on CRAN will contain viruses (!), I am asking here for support, maybe in the
form of arguments like "These files definitely contain no viruses, but they
may give false virus warnings because ...".
Has anyone else experienced this problem?

Regards,
Heinrich.



From ligges at statistik.uni-dortmund.de  Wed Jun 18 10:09:40 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 18 Jun 2003 10:09:40 +0200
Subject: [R] downloading packages and AntiVirus program
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE23E@xms1.tirol.gv.at>
References: <C4D44AB4CB62D311BA6500041202E886031EE23E@xms1.tirol.gv.at>
Message-ID: <3EF01E44.5080404@statistik.uni-dortmund.de>

RINNER Heinrich wrote:

> Dear R-users!
> 
> I am using R 1.7.0, under Windows XP;
> I also have Internet Explorer 6.0.2600.0000, Norton AntiVirus 7.60.926.
> 
> Our firewall seems to want to "protect" me from downloading precompiled
> packages for Windows.
> When I try to download packages, like
> http://cran.at.r-project.org/bin/windows/contrib/1.7/RODBC_1.0-3.zip
> for example, I am not allowed to and get a message like this:
> 
> "The content you just requested had a problem and was blocked by the
> Symantec AntiVirus Scan Engine based on local administrator settings."
> 
> I called our network hotline, but they insisted that these files contain
> viruses and wouldn't let me download them. As I'm sure that not all packages
> on CRAN will contain viruses (!), I am asking here for support, maybe in the
> form of arguments like "These files definitely contain no viruses, but they
> may give false virus warnings because ...".
> Has anyone else experienced this problem?
> 
> Regards,
> Heinrich.

As an alternative you can compile from sources (and hope no viruses' 
source code is in there).

Hopefully there are no viruses in those binary packages on CRAN. The 
virus scanner on the machine that compiles the packages is up to date, 
but having the most recent virus scanner doesn't imply to be free of 
viruses. Virus free packages cannot be guaranteed.

Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Jun 18 10:25:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 09:25:10 +0100 (BST)
Subject: [R] downloading packages and AntiVirus program
In-Reply-To: <3EF01E44.5080404@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0306180921280.29071-100000@gannet.stats>

I've just run that file past two different virus scanners (Sophos and 
Norton, both fully updated) wiht no hits.

I think you can reasonably ask your sysadmins to demonstrate to you what 
the virus is and in which file in the zip it is.

I suspect they do not check ftp downloads, so you could try that:

fttp://cran.at.r-project.org/pub/R/bin/windows/contrib/1.7/RODBC_1.0-3.zip


On Wed, 18 Jun 2003, Uwe Ligges wrote:

> RINNER Heinrich wrote:
> 
> > Dear R-users!
> > 
> > I am using R 1.7.0, under Windows XP;
> > I also have Internet Explorer 6.0.2600.0000, Norton AntiVirus 7.60.926.
> > 
> > Our firewall seems to want to "protect" me from downloading precompiled
> > packages for Windows.
> > When I try to download packages, like
> > http://cran.at.r-project.org/bin/windows/contrib/1.7/RODBC_1.0-3.zip
> > for example, I am not allowed to and get a message like this:
> > 
> > "The content you just requested had a problem and was blocked by the
> > Symantec AntiVirus Scan Engine based on local administrator settings."
> > 
> > I called our network hotline, but they insisted that these files contain
> > viruses and wouldn't let me download them. As I'm sure that not all packages
> > on CRAN will contain viruses (!), I am asking here for support, maybe in the
> > form of arguments like "These files definitely contain no viruses, but they
> > may give false virus warnings because ...".
> > Has anyone else experienced this problem?
> > 
> > Regards,
> > Heinrich.
> 
> As an alternative you can compile from sources (and hope no viruses' 
> source code is in there).
> 
> Hopefully there are no viruses in those binary packages on CRAN. The 
> virus scanner on the machine that compiles the packages is up to date, 
> but having the most recent virus scanner doesn't imply to be free of 
> viruses. Virus free packages cannot be guaranteed.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Jun 18 10:34:50 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 18 Jun 2003 10:34:50 +0200
Subject: [R] Clustering quality measure
In-Reply-To: <A828F656-A0D7-11D7-9A59-0005026E2B43@vanderkogel.net>
References: <A828F656-A0D7-11D7-9A59-0005026E2B43@vanderkogel.net>
Message-ID: <16112.9258.19124.358840@gargle.gargle.HOWL>

>>>>> "Jonck" == Jonck van der Kogel <jonck at vanderkogel.net>
>>>>>     on Tue, 17 Jun 2003 17:23:33 +0200 writes:

    Jonck> Hi all, I am running a series of experiments where
    Jonck> after manipulating my data I run several clustering
    Jonck> algorithms (agnes, diana and a clustering method of
    Jonck> my own) on the data. I wanted to determine which
    Jonck> clustering method did the best job, so therefore I
    Jonck> had defined my own quality measure using two
    Jonck> criteria: compactness of the data within the clusters
    Jonck> themselves and the amount of seperation between the
    Jonck> clusters. Anyway, my quality measure does not work,
    Jonck> since according to my quality measure the quality
    Jonck> gets increasingly better as more clusters are formed
    Jonck> untill every data instance is a cluster by itself.
    Jonck> Therefore I was wondering if any of you are aware of
    Jonck> any libraries or functions within R that determine
    Jonck> quality measures of clusterings, I am very much
    Jonck> intrigued by the definition of quality measures that
    Jonck> do work.  Thanks very much, Jonck

Well,  "do work" is said much.  

But there's silhouette() in the `cluster' package {where agnes()
and diana() reside}. You can plot silhouettes of almost any
clustering {i.e. grouping} as a diagnostic, and the "Average
Silhouette Width" has been proposed as "goodness of fit" measure
for clusters, and even to determine how many clusters you should
choose.

One of its several drawbacks is that it's not defined for the
"only 1 cluster" situation, i.e., you cannot use it to compare
one vs two clusters.

--> ?silhouette

and look and try the "Examples".

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From hennig at stat.math.ethz.ch  Wed Jun 18 11:20:44 2003
From: hennig at stat.math.ethz.ch (Christian Hennig)
Date: Wed, 18 Jun 2003 11:20:44 +0200 (CEST)
Subject: [R] Clustering quality measure
In-Reply-To: <16112.9258.19124.358840@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0306181106480.1858-100000@florence>

Hi,

> >>>>> "Jonck" == Jonck van der Kogel <jonck at vanderkogel.net>
> >>>>>     on Tue, 17 Jun 2003 17:23:33 +0200 writes:
> 
>     Jonck> Hi all, I am running a series of experiments where
>     Jonck> after manipulating my data I run several clustering
>     Jonck> algorithms (agnes, diana and a clustering method of
>     Jonck> my own) on the data. I wanted to determine which
>     Jonck> clustering method did the best job, so therefore I
>     Jonck> had defined my own quality measure using two
>     Jonck> criteria: compactness of the data within the clusters
>     Jonck> themselves and the amount of seperation between the
>     Jonck> clusters. Anyway, my quality measure does not work,
>     Jonck> since according to my quality measure the quality
>     Jonck> gets increasingly better as more clusters are formed
>     Jonck> untill every data instance is a cluster by itself.
>     Jonck> Therefore I was wondering if any of you are aware of
>     Jonck> any libraries or functions within R that determine
>     Jonck> quality measures of clusterings, I am very much
>     Jonck> intrigued by the definition of quality measures that
>     Jonck> do work.  Thanks very much, Jonck

Sounds a bit like ratio of within clusters variation and between clusters
variation. Similar measures arise as negative
loglikelihoods in certain normal distribution based clustering methods. 
Of course they get better with more
clusters because there are more degrees of freedom for the fit. A common
strategy is to penalize the negative loglikelihood by an increasing
function of the number of degrees of freedom. 

This is implemented as BIC (Bayesian Information Criterion) for various
normal mixture models in library mclust and is used there to decide about
the best model (number of clusters, covariance matrix parametrization).

In principle, you could compute the BIC, given a certain covariance matrix
parametrization, for every partition from an arbitrary clustering.

Note however that this, as every quality measure for clustering, implies a
particular concept of what a cluster is. If you define a cluster as
"looking like a mixture component in a normal mixture", than this is OK,
but very likely you will then get the "best" clustering using a method which
performs estimation in a normal mixture model.

If you have a different concept of a cluster and you formalize it via a
quality criterion, you will get the best clustering by optimizing *this*
quality criterion (maybe apart from possible numerical problems).

The important point is that no quality criterion for clustering provides an
independent objective decision of what the best clustering is. The choice
of an adequate quality criterion is as difficult and subjective as the
choice of the best clustering method.

Best,
Christian 

-- 
***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (currently)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From kan_liu1 at yahoo.com  Wed Jun 18 12:09:41 2003
From: kan_liu1 at yahoo.com (=?iso-8859-1?q?kan=20Liu?=)
Date: Wed, 18 Jun 2003 11:09:41 +0100 (BST)
Subject: [R] R environment variable
In-Reply-To: <Pine.LNX.4.44.0306172225280.20698-100000@gannet.stats>
Message-ID: <20030618100941.89365.qmail@web20708.mail.yahoo.com>

which R environmen variable can be used to point to
x1.R so that it can be sourced in any directory?

Kan

________________________________________________________________________
Want to chat instantly with your online friends?  Get the FREE Yahoo!
Messenger http://uk.messenger.yahoo.com/



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 18 12:28:06 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 18 Jun 2003 11:28:06 +0100 (BST)
Subject: [R] downloading packages and AntiVirus program
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE23E@xms1.tirol.gv.at>
Message-ID: <XFMail.030618112806.Ted.Harding@nessie.mcc.ac.uk>

On 18-Jun-03 RINNER Heinrich wrote:
> Dear R-users!
> 
> I am using R 1.7.0, under Windows XP;
> I also have Internet Explorer 6.0.2600.0000, Norton AntiVirus 7.60.926.
> 
> Our firewall seems to want to "protect" me from downloading precompiled
> packages for Windows.
> When I try to download packages, like
> http://cran.at.r-project.org/bin/windows/contrib/1.7/RODBC_1.0-3.zip
> for example, I am not allowed to and get a message like this:
> 
> "The content you just requested had a problem and was blocked by the
> Symantec AntiVirus Scan Engine based on local administrator settings."
> 
> I called our network hotline, but they insisted that these files
> contain viruses and wouldn't let me download them.

I think you may be a victim of the blind paranoia that is increasingly
afflicting system administrators in these virus-prone days. The file
you are trying to download is a *.zip file, and you may find that your
network admins have simply blocked all ingress of files with a ".zip"
extension. I know of sites where ".exe", ".doc", ".xls" etc. are all
automatically blocked, regardless of content, simply because _some_
such files contain viruses.

As to your network hotline, it seems to me that it is staffed by dumb
clucks sitting at terminals consulting their ARS (Action Request System
-- I invent not -- which supplies on-line canned answers for user
problems).

I don't know what to suggest, if that's what you're up against.

The best of luck.
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Jun-03                                       Time: 11:28:06
------------------------------ XFMail ------------------------------



From jonck at vanderkogel.net  Wed Jun 18 12:42:38 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Wed, 18 Jun 2003 12:42:38 +0200
Subject: [R] Macintosh plots
In-Reply-To: <200306181004.h5IA1a4W017480@stat.math.ethz.ch>
Message-ID: <9445BA96-A179-11D7-BBDA-0005026E2B43@vanderkogel.net>

Hi Ian,
Not 100% sure on this, but I've recently been wrestling with producing 
plots in R and I think I know the answer to your question. As far as I 
could tell, all the parameters for graphics are stored using par(). 
There is a parameter $bg, which for me is set to "transparent", but 
I'll bet that for you it's set to "charcoal" or something like that :-)
I would not be surprised if you could set the background of your plot 
to transparent with the following command:
par$bg <- "transparent"

HTH, Jonck

> This may be a dumb question, but I've wasted several hours trying to 
> find an answer....
>
> When I do a plot with no parameters set eg: ' >plot(d1)' it comes up 
> with a dark gray background and blue data points.  I can modify the 
> data points using '>plot(d1, col = "red")', but I have not been able 
> to modify the background color.  '>plot(d1, bg = "white")' does not 
> work.  Any help would be most appreciated.
>
> Cheers,
> Ian.



From dmurdoch at pair.com  Wed Jun 18 12:50:22 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 18 Jun 2003 06:50:22 -0400
Subject: [R] (no subject)
In-Reply-To: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
References: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
Message-ID: <fog0fvo7o7va113tf7rhb54ld23egtum0o@4ax.com>

On Tue, 17 Jun 2003 20:19:32 -0600, you wrote:

>Hi all,
>
>I'm new to R and am trying to create a dll in order to be able to use the "dyn.load" function with a fortran script.  I have a windows OS, have installed perl and the Path is recognized.  I have also installed R 1.70 including the src (source code) folder.  But when I attempt to create the dll by excecuting "Rcmd SHLIB [-o autologdll] combo.f", in the windows command prompt, I get the following error message "'make' is not recognized as an internal or external command"
>
>Does anyone have some suggestions on what I may have missed?

"make" is a utility that's used in the build process.  You can get
copies of all the utilities you need from Brian Ripley's web page as
described in $RHOME/readme.packages.

You didn't mention that you had installed a Fortran compiler.  If you
haven't installed the recommended compiler (MinGW), you might find
that some more work will be needed to get the scripts to work.

Duncan Murdoch



From dmurdoch at pair.com  Wed Jun 18 12:53:29 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 18 Jun 2003 06:53:29 -0400
Subject: [R] R environment variable
In-Reply-To: <20030618100941.89365.qmail@web20708.mail.yahoo.com>
References: <Pine.LNX.4.44.0306172225280.20698-100000@gannet.stats>
	<20030618100941.89365.qmail@web20708.mail.yahoo.com>
Message-ID: <q1h0fv4u3nl517q9t2t7ceqlj048pims4q@4ax.com>

On Wed, 18 Jun 2003 11:09:41 +0100 (BST), you wrote:

>which R environmen variable can be used to point to
>x1.R so that it can be sourced in any directory?

I don't think there is such a thing.  source() looks in the current
working directory, it doesn't use environment variables to do a wider
search.

Of course, you can always make up your own variable, and then do
something like

source(paste(getenv('MYDIR'),'/x1.R',sep=''))

and that will work from anywhere if MYDIR is defined properly.

Duncan Murdoch



From D.Beare at marlab.ac.uk  Wed Jun 18 14:33:37 2003
From: D.Beare at marlab.ac.uk (Douglas Beare)
Date: Wed, 18 Jun 2003 13:33:37 +0100
Subject: [R] finding contours in a matrix
Message-ID: <67B92F9B2AFED611852500B0D0FE15097EAFA5@mail3.marlab.ac.uk>

Hi,
I have matrix of sea bottom depths that I am plotting in R with the function
'image'.
I am particularly interested in the 200m depth contour (I'm using 'distance
from this feature' as a covariate in a model) and would like to extract the
data at evenly 
spaced points along it.  I can easily superimpose a line at 200m using the
function 'contour'.  What I
want to know is: can I actually get the data used to plot this line? If I
just select the relevant data depth[depth==200] I
get a very few observations, but if I say select depth[depth > 180 & depth <
220] I get a "band" of 
unevenly space data along the contour which I don't want either.
Regards,
Doug Beare
Fisheries Research Services,
Marine Laboratory,
Victoria Road,
Torry,
Aberdeen, UK.
Tel. 44 (0) 1224 295314



From kan_liu1 at yahoo.com  Wed Jun 18 15:11:51 2003
From: kan_liu1 at yahoo.com (kan Liu)
Date: Wed, 18 Jun 2003 06:11:51 -0700 (PDT)
Subject: [R] outlier
In-Reply-To: <Pine.LNX.4.44.0306172225280.20698-100000@gannet.stats>
Message-ID: <20030618131151.63188.qmail@web20703.mail.yahoo.com>

I wrote a .R file (see below)to calculate robust
measure of correlation using cov.rob. I got different
correlation coefficients (0.70, 0.79, 0.63, ...) when
I run the file different times. Can you tell me what
this means or what is wrong in using cov.rob?
-------------

library(lqs)
a <-
c(5.41,4.67,5.88,2.38,4.79,5.30,1.94,3.40,5.05,3.31,5.88,4.92,5.08,4.58,4.59,4.77,5.25,3.77,2.88,5.30,5.32,2.56,4.29,5.54,4.53,3.51,4.93,2.49,2.85,5.04,2.51,2.60,3.58,2.11,1.70,5.20,5.08,4.48,3.96,4.87,4.98,2.56,1.69,4.28,1.70,2.91,5.37,2.16,3.04,1.69,1.88,5.36,1.70,3.81,1.70,5.88,3.52)
p <-
c(5.30,4.78,4.79,0.62,4.32,2.33,0.64,3.14,3.06,4.73,5.72,2.21,4.81,1.74,4.93,4.74,5.81,3.88,3.03,4.72,5.79,3.43,4.07,5.93,2.26,3.70,5.32,4.56,1.52,2.54,0.26,2.79,3.67,4.44,1.46,4.26,4.49,5.29,3.26,3.87,3.12,3.97,3.49,0.45,0.76,4.49,5.29,1.94,4.69,2.80,2.75,5.16,0.74,5.81,1.46,5.24,4.00)
ap <- cbind(a,p)
cov.rob(ap, cor=TRUE)



From otoomet at econ.dk  Wed Jun 18 15:35:42 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Wed, 18 Jun 2003 15:35:42 +0200
Subject: [R] downloading packages and AntiVirus program
In-Reply-To: <XFMail.030618112806.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030618112806.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200306181335.h5IDZga01693@punik.econ.au.dk>

Hi,

try to download 

http://www.obs.ee/~siim/RODBC_1.0-3

(without .zip) and to rename it to .zip on your own computer.  Perhaps
it helps.

Ott
 | Date: Wed, 18 Jun 2003 11:28:06 +0100 (BST)
 | From: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
 | 
 | On 18-Jun-03 RINNER Heinrich wrote:
 | > Dear R-users!
 | > 
 | > I am using R 1.7.0, under Windows XP;
 | > I also have Internet Explorer 6.0.2600.0000, Norton AntiVirus 7.60.926.
 | > 
 | > Our firewall seems to want to "protect" me from downloading precompiled
 | > packages for Windows.
 | > When I try to download packages, like
 | > http://cran.at.r-project.org/bin/windows/contrib/1.7/RODBC_1.0-3.zip
 | > for example, I am not allowed to and get a message like this:
 | > 
 | > "The content you just requested had a problem and was blocked by the
 | > Symantec AntiVirus Scan Engine based on local administrator settings."
 | > 
 | > I called our network hotline, but they insisted that these files
 | > contain viruses and wouldn't let me download them.
 | 
 | I think you may be a victim of the blind paranoia that is increasingly
 | afflicting system administrators in these virus-prone days. The file
 | you are trying to download is a *.zip file, and you may find that your
 | network admins have simply blocked all ingress of files with a ".zip"
 | extension. I know of sites where ".exe", ".doc", ".xls" etc. are all
 | automatically blocked, regardless of content, simply because _some_
 | such files contain viruses.
 | 
 | As to your network hotline, it seems to me that it is staffed by dumb
 | clucks sitting at terminals consulting their ARS (Action Request System
 | -- I invent not -- which supplies on-line canned answers for user
 | problems).
 | 
 | I don't know what to suggest, if that's what you're up against.
 | 
 | The best of luck.
 | Ted.



From ripley at stats.ox.ac.uk  Wed Jun 18 15:38:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Wed, 18 Jun 2003 14:38:07 +0100 (GMT Daylight Time)
Subject: [R] outlier
In-Reply-To: <20030618131151.63188.qmail@web20703.mail.yahoo.com>
Message-ID: <Pine.WNT.4.44.0306181435260.2788-100000@gannet.stats.ox.ac.uk>

Please do read the help page. which explains this is a random algorithm.
In your example you can try cov.rob(ap, cor=TRUE, nsamp="exact")


On Wed, 18 Jun 2003, kan Liu wrote:

> I wrote a .R file (see below)to calculate robust
> measure of correlation using cov.rob. I got different
> correlation coefficients (0.70, 0.79, 0.63, ...) when
> I run the file different times. Can you tell me what
> this means or what is wrong in using cov.rob?
> -------------
>
> library(lqs)
> a <-
> c(5.41,4.67,5.88,2.38,4.79,5.30,1.94,3.40,5.05,3.31,5.88,4.92,5.08,4.58,4.59,4.77,5.25,3.77,2.88,5.30,5.32,2.56,4.29,5.54,4.53,3.51,4.93,2.49,2.85,5.04,2.51,2.60,3.58,2.11,1.70,5.20,5.08,4.48,3.96,4.87,4.98,2.56,1.69,4.28,1.70,2.91,5.37,2.16,3.04,1.69,1.88,5.36,1.70,3.81,1.70,5.88,3.52)
> p <-
> c(5.30,4.78,4.79,0.62,4.32,2.33,0.64,3.14,3.06,4.73,5.72,2.21,4.81,1.74,4.93,4.74,5.81,3.88,3.03,4.72,5.79,3.43,4.07,5.93,2.26,3.70,5.32,4.56,1.52,2.54,0.26,2.79,3.67,4.44,1.46,4.26,4.49,5.29,3.26,3.87,3.12,3.97,3.49,0.45,0.76,4.49,5.29,1.94,4.69,2.80,2.75,5.16,0.74,5.81,1.46,5.24,4.00)
> ap <- cbind(a,p)
> cov.rob(ap, cor=TRUE)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From solares at unsl.edu.ar  Wed Jun 18 15:43:40 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 18 Jun 2003 10:43:40 -0300 (ART)
Subject: [R] menu
Message-ID: <34897.170.210.173.216.1055943820.squirrel@inter14.unsl.edu.ar>

Hello, my question is on as doing a submenu, therefore in all the examples 
that saw alone is used tkmenu and a ready one is created  without 
possibility of a submenu
for example:
topmenu <- tkmenu (ventmenup) tkconfigure (ventmenup, menu = topmenu) 
archMenu <-tkmenu(topmenu,tearoff = FALSE) tkadd(archMenu,"command",label 
= "Abrir...",
 nd=abrir,accelerator="CTRL+A",underline=0)
tkadd(archMenu,"command",label = 
abrir...",command="",accelerator="CTRL+G",underline=0) tkadd tkadd
(archMenu,"separator")
chMenu,"command",label="Salir",command=functio() tkdestr y
(ventmenup),accelerator="CTRL+S",underline=0) 
tkadd(t menu,"cascade",label="Archivo",menu=archMenu)  

I have seen that in tcl the path is used to do the menu as a waterfall for 
example: set w .menu.file.save.saveas $w menu $w -tearoff 0 ?As I do in R a 
submenu
if I do not know the path of names to create a submenu as a cascade? . 
Sorry Ruben



From MZodet at ahrq.gov  Wed Jun 18 15:50:43 2003
From: MZodet at ahrq.gov (MZodet@ahrq.gov)
Date: Wed, 18 Jun 2003 09:50:43 -0400
Subject: [R] Forward stepwise procedure w/ stepAIC
Message-ID: <3598558AD728D41183350008C7CF291C0A5CD214@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030618/e966cfe4/attachment.pl

From spencer.graves at pdf.com  Wed Jun 18 16:12:32 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jun 2003 07:12:32 -0700
Subject: [R] globle variable
References: <FFEKIEFDONDECJDODGDJCEMGCHAA.ken_lee@tynesys.com>
Message-ID: <3EF07350.7000600@pdf.com>

Have you considered "get" or "assign"?  If these do not seem
appropriate, you may wish to resubmit your question being more specific,
providing a very brief example of what you want to do.

hth.  spencer graves

Ken Lee wrote:
> Hi all,
>     Could I  claim globle variable?
>   
>  Thank's 
> 
> Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jun 18 16:21:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jun 2003 07:21:51 -0700
Subject: [R] Forward stepwise procedure w/ stepAIC
References: <3598558AD728D41183350008C7CF291C0A5CD214@exchange1.ahrq.gov>
Message-ID: <3EF0757F.4060105@pdf.com>

What's "m1"?  Is it a fit to a ~1?  I've used my own modification of 
stepAIC many times preceeded by "m1 <- lm(y~1, data=...)", and it has 
worked fine for me.

hth.  spencer graves

MZodet at ahrq.gov wrote:
> I'm attempting to select a model using stepAIC.  I want to use a forward
> selection procedure.  I have specified a "scope" option, but must not be
> understanding how this works.  My results indicate that the procedure begins
> and ends with the "full" model (i.e., all 17 independent variables)...not
> what I expected.  Could someone please point out what I'm not understanding?
> My code is below.
> 
>  
> 
> Thanks.
> 
>  
> 
> Marc
> 
>  
> 
> 
>>m1.stepFwd <- stepAIC(m1, direction="forward",
> 
> 
> +                       scope=list(upper=~age.refc.fo + dusz.cat.fo +
> 
> +                         ed.du.f + emp.ref.f + geodist3.f + has.fone.f +
> 
> +                         healthdu.f + help.du.f + inc.ref.f + maj.act.f +
> 
> +                         marryref.f + msa.stat.f + regionrf.f + r.e.ref.f +
> 
> +                         sex.ref.f + type.psu.f + urb.stat.f, lower=~1))
> 
> Start:  AIC= 4752.19 
> 
>  response ~ age.refc.fo + dusz.cat.fo + ed.du.f + emp.ref.f +  
> 
>     geodist3.f + has.fone.f + healthdu.f + help.du.f + inc.ref.f +  
> 
>     maj.act.f + marryref.f + msa.stat.f + regionrf.f + r.e.ref.f +  
> 
>     sex.ref.f + type.psu.f + urb.stat.f 
> 
>  
> 
> 
> 
>>m1.stepFwd$anova
> 
> 
> Stepwise Model Path 
> 
> Analysis of Deviance Table
> 
>  
> 
> Initial Model:
> 
> response ~ age.refc.fo + dusz.cat.fo + ed.du.f + emp.ref.f + 
> 
>     geodist3.f + has.fone.f + healthdu.f + help.du.f + inc.ref.f + 
> 
>     maj.act.f + marryref.f + msa.stat.f + regionrf.f + r.e.ref.f + 
> 
>     sex.ref.f + type.psu.f + urb.stat.f
> 
>  
> 
> Final Model:
> 
> response ~ age.refc.fo + dusz.cat.fo + ed.du.f + emp.ref.f + 
> 
>     geodist3.f + has.fone.f + healthdu.f + help.du.f + inc.ref.f + 
> 
>     maj.act.f + marryref.f + msa.stat.f + regionrf.f + r.e.ref.f + 
> 
>     sex.ref.f + type.psu.f + urb.stat.f
> 
>  
> 
>  
> 
>   Step Df Deviance Resid. Df Resid. Dev      AIC
> 
> 1      NA       NA      5311   4660.192 4752.192
> 
> 
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Jun 18 16:25:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 15:25:22 +0100 (BST)
Subject: [R] Forward stepwise procedure w/ stepAIC
In-Reply-To: <3598558AD728D41183350008C7CF291C0A5CD214@exchange1.ahrq.gov>
Message-ID: <Pine.LNX.4.44.0306181522360.1124-100000@gannet.stats>

It starts with the initial model you gave it, which contains all your 
explanatory variables already.  You presumably wanted to use a simpler 
model to start with.  As in:

Arguments:

  object: an object representing a model of an appropriate class. This
          is used as the initial model in the stepwise search.


On Wed, 18 Jun 2003 MZodet at ahrq.gov wrote:

> I'm attempting to select a model using stepAIC.  I want to use a forward
> selection procedure.  I have specified a "scope" option, but must not be
> understanding how this works.  My results indicate that the procedure begins
> and ends with the "full" model (i.e., all 17 independent variables)...not
> what I expected.  Could someone please point out what I'm not understanding?

[output snipped]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Jun 18 16:36:11 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Jun 2003 07:36:11 -0700
Subject: [R] finding contours in a matrix
References: <67B92F9B2AFED611852500B0D0FE15097EAFA5@mail3.marlab.ac.uk>
Message-ID: <3EF078DB.5040706@pdf.com>

A question very much like this was answered two days ago.  I couldn't 
find it from "www.r-project.org" -> search -> R Site Search.  When I 
went "search" -> "searchable mail archives" -> "r-help" -> "date view", 
I found it.  I found two replies with "isocontour-lines" in the subject, 
both dated 6/16/2003.  One referred to the same question having been 
answered in late May.  With that key word, a more targeted search might 
produce more.

hth, spencer graves

Douglas Beare wrote:
> Hi,
> I have matrix of sea bottom depths that I am plotting in R with the function
> 'image'.
> I am particularly interested in the 200m depth contour (I'm using 'distance
> from this feature' as a covariate in a model) and would like to extract the
> data at evenly 
> spaced points along it.  I can easily superimpose a line at 200m using the
> function 'contour'.  What I
> want to know is: can I actually get the data used to plot this line? If I
> just select the relevant data depth[depth==200] I
> get a very few observations, but if I say select depth[depth > 180 & depth <
> 220] I get a "band" of 
> unevenly space data along the contour which I don't want either.
> Regards,
> Doug Beare
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From zedshaw at zedshaw.com  Wed Jun 18 16:46:57 2003
From: zedshaw at zedshaw.com (Zed Shaw)
Date: Wed, 18 Jun 2003 07:46:57 -0700
Subject: [R] A slight weird diversion
Message-ID: <20030618144657.GA22156@webapps.dnsalias.net>

Hi Folks,

This is a totally off-topic diversion that I thought people might find 
fun.

I've been working on a small parser framework that I'm integrating into 
Obversive to provide code analysis of R scripts and stuff.  It is still 
a work in progress, but the parser currently can parse R code and 
produce an XML output file representing the Abstract Syntax Tree.

I thought it would be kind of fun to take the results and throw the XML 
into Exist (an XML database) so that you can search the structure of an 
R script.  I have a sample up at my site at:

http://www.zedshaw.com:8888/simple/xquery.xsp

Where you can search for things in a chunk of the R test suite using the 
XPath and XQuery syntax.  So, you can search for all function calls 
inside of for loops with:  //FOR//CALL, or find all calls to c() with 
//CALL[ID at text="c"].

Have fun with it.  It's nothing serious, but I may turn it into a code 
comprehension tool later down the line.

Zed



From frigyesi at home.se  Wed Jun 18 16:49:48 2003
From: frigyesi at home.se (Attila Frigyesi)
Date: Wed, 18 Jun 2003 16:49:48 +0200
Subject: [R] fitting two nonlinear mixed models simulataneously
Message-ID: <009f01c335a8$ddadd240$0501000a@Einstein>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030618/2e849240/attachment.pl

From ymu at students.uiuc.edu  Wed Jun 18 17:23:32 2003
From: ymu at students.uiuc.edu (Mu Yunming)
Date: Wed, 18 Jun 2003 10:23:32 -0500
Subject: [R] dyn.load()  function problem help!
Message-ID: <3F06218E@webmail.uiuc.edu>

Hi!

I would like to call a Fortran subroutine within R. For doing this, I first 
built a shared library for loading into R under Unix and it was successful. 
But when I tried to load a shared library using dyn.load() function for use in 
.Fortran(), I got an error message. 
The command I input below in R is:
>dyn.load("aaa.so")
The reslut is:

error in dyn.load(x,as.logical(local),as.logical(now)):
        unable to load shared library "/home/user/fortran/aaa.so":
ld.so.1: /usr/local/lib/R/bin/R.bin: fatal: relocation error: file 
/home/ymu/ft/aaa.so: symbol _F90free: referenced symbol not found.

Does this mean that the shared library was not built successfully or my 
fortran subroutine has some errors?

Any answers to my problem will be very much appreciated!

Best,

Yunming *^_^*



From TyagiAnupam at aol.com  Wed Jun 18 17:35:40 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Wed, 18 Jun 2003 11:35:40 EDT
Subject: [R] R: Debian package and source
Message-ID: <1f1.b36e5df.2c21e0cc@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030618/fd57ddc3/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 18 17:36:25 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Wed, 18 Jun 2003 16:36:25 +0100 (GMT Daylight Time)
Subject: [R] dyn.load()  function problem help!
In-Reply-To: <3F06218E@webmail.uiuc.edu>
Message-ID: <Pine.WNT.4.44.0306181633310.2956-100000@gannet.stats.ox.ac.uk>

On Wed, 18 Jun 2003, Mu Yunming wrote:

> Hi!
>
> I would like to call a Fortran subroutine within R. For doing this, I first
> built a shared library for loading into R under Unix and it was successful.

How did you do this?  Did you use R CMD SHLIB?

> But when I tried to load a shared library using dyn.load() function for use in
> .Fortran(), I got an error message.
> The command I input below in R is:
> >dyn.load("aaa.so")
> The reslut is:
>
> error in dyn.load(x,as.logical(local),as.logical(now)):
>         unable to load shared library "/home/user/fortran/aaa.so":
> ld.so.1: /usr/local/lib/R/bin/R.bin: fatal: relocation error: file
> /home/ymu/ft/aaa.so: symbol _F90free: referenced symbol not found.
>
> Does this mean that the shared library was not built successfully or my
> fortran subroutine has some errors?

The former.  You appear to have used different Fortran compilers to build R
and your shared object, since your Fortran support libraries are not being
linked against.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ymu at students.uiuc.edu  Wed Jun 18 17:48:23 2003
From: ymu at students.uiuc.edu (Mu Yunming)
Date: Wed, 18 Jun 2003 10:48:23 -0500
Subject: [R] dyn.load()  function problem help!
Message-ID: <3F065418@webmail.uiuc.edu>

First of all, thank you very much for your quick response.

>===== Original Message From Prof Brian D Ripley <ripley at stats.ox.ac.uk> =====
>On Wed, 18 Jun 2003, Mu Yunming wrote:
>
>> Hi!
>>
>> I would like to call a Fortran subroutine within R. For doing this, I first
>> built a shared library for loading into R under Unix and it was successful.
>
>How did you do this?  Did you use R CMD SHLIB?

Yes, I typed R CMD SHLIB aaa.o under Unix. Before I did that, I complied my 
fortran source code using f90 aaa.f90, which produced aaa.o.

>
>> But when I tried to load a shared library using dyn.load() function for use 
in
>> .Fortran(), I got an error message.
>> The command I input below in R is:
>> >dyn.load("aaa.so")
>> The reslut is:
>>
>> error in dyn.load(x,as.logical(local),as.logical(now)):
>>         unable to load shared library "/home/ymu/ft/aaa.so":
>> ld.so.1: /usr/local/lib/R/bin/R.bin: fatal: relocation error: file
>> /home/ymu/ft/aaa.so: symbol _F90free: referenced symbol not found.
>>
>> Does this mean that the shared library was not built successfully or my
>> fortran subroutine has some errors?
>
>The former.  You appear to have used different Fortran compilers to build R
>and your shared object, since your Fortran support libraries are not being
>linked against.
>
My source code is writen in fortran 90 and i complied the source code using 
command f90. How do you think I should complie my source file and build the 
shared library?

>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272860 (secr)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595

Best,

Yunming *^_^*



From tlumley at u.washington.edu  Wed Jun 18 18:09:10 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 Jun 2003 09:09:10 -0700 (PDT)
Subject: [R] dyn.load()  function problem help!
In-Reply-To: <3F065418@webmail.uiuc.edu>
Message-ID: <Pine.A41.4.44.0306180902440.52254-100000@homer33.u.washington.edu>

On Wed, 18 Jun 2003, Mu Yunming wrote:

> >
> My source code is writen in fortran 90 and i complied the source code using
> command f90. How do you think I should complie my source file and build the
> shared library?
>

You probably need to figure this out: R doesn't explicitly support F90 on
any platform.

Two things are needed
1/  You have to be able to create a DLL that is properly linked to all the
libraries it needs.  Your compiler documentation should tell you how to do
this.

2/ The functions you call from R need to use the same calling conventions
as the compilers used to compiler R.  Duncan Murdoch has information at
 http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/
that describes how to do this for various compilers. Even if your's isn't
on the list this might still be helpful

	-thomas



From edd at debian.org  Wed Jun 18 18:16:58 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 18 Jun 2003 11:16:58 -0500
Subject: [R] R: Debian package and source
In-Reply-To: <1f1.b36e5df.2c21e0cc@aol.com>
References: <1f1.b36e5df.2c21e0cc@aol.com>
Message-ID: <20030618161658.GA4451@sonny.eddelbuettel.com>

On Wed, Jun 18, 2003 at 11:35:40AM -0400, TyagiAnupam at aol.com wrote:
> Hi,
> where can I find the most up-to-date Debian packages for R and sources in 
> USA.  I will be thankful for the location that I can include in my sources.list.

Ahem, that is more a question for debian-user at lists,debian.org, not r-help.

R 1.7.1 is part of "sid" aka "unstable", you can point to any of the mirrors
listed in  http://www.debian.org/mirror/list   which includes one primary
and maybe three dozen secondary mirrors in the US alone.  Your apt-get man
pages will tell you about source depends, but it as simple as 

deb http://ftp.us.debian.org/debian unstable main contrib non-free
deb-src http://ftp.us.debian.org/debian unstable main contrib non-free

in /etc/apt/sources.list.

> Are their any special recommendations or instructions for building and 
> maintaining R on Debian? A web-link will be helpful.

Do you have any specific reason why the packages Doug Bates and I have been
maintaining for over half a decade aren't good enough?

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From rossini at blindglobe.net  Wed Jun 18 18:23:22 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 18 Jun 2003 09:23:22 -0700
Subject: [R] R: Debian package and source
In-Reply-To: <1f1.b36e5df.2c21e0cc@aol.com> (TyagiAnupam@aol.com's message
	of "Wed, 18 Jun 2003 11:35:40 EDT")
References: <1f1.b36e5df.2c21e0cc@aol.com>
Message-ID: <87adcfmk1x.fsf@jeeves.blindglobe.net>


The most updated version is usually the official one, these days.
Dirk is an excellent maintainer.

TyagiAnupam at aol.com writes:

> Hi,
> where can I find the most up-to-date Debian packages for R and sources in 
> USA.  I will be thankful for the location that I can include in my sources.list. 
> Are their any special recommendations or instructions for building and 
> maintaining R on Debian? A web-link will be helpful.
> ---anupam.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From Roger.Bivand at nhh.no  Wed Jun 18 18:35:36 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 18 Jun 2003 18:35:36 +0200 (CEST)
Subject: [R] finding contours in a matrix
In-Reply-To: <67B92F9B2AFED611852500B0D0FE15097EAFA5@mail3.marlab.ac.uk>
Message-ID: <Pine.LNX.4.44.0306181823120.3049-100000@reclus.nhh.no>

On Wed, 18 Jun 2003, Douglas Beare wrote:

> Hi,
> I have matrix of sea bottom depths that I am plotting in R with the function
> 'image'.
> I am particularly interested in the 200m depth contour (I'm using 'distance
> from this feature' as a covariate in a model) and would like to extract the
> data at evenly 
> spaced points along it.  I can easily superimpose a line at 200m using the
> function 'contour'.  What I
> want to know is: can I actually get the data used to plot this line? If I
> just select the relevant data depth[depth==200] I
> get a very few observations, but if I say select depth[depth > 180 & depth <
> 220] I get a "band" of 
> unevenly space data along the contour which I don't want either.

This is a different question to the "can I retrieve a contour line from 
contour()" - it presupposes that we have the line, but asks which cells of 
the array are intersected by the line, is that right? It could also be 
used to interrogate any other image/raster layer for the same area, not 
just depth. Polygon clipping is available in the gcplib package, but that 
may be overkill. If the (contour) line(s) can be converted into sequences 
of points closer together than the raster resolution, it should be 
possible to identify the raster cells they belong to (row and column 
numbers). If this was GIS data, it might be easier to do the overlay 
operation there, especially if you need a buffer around the line.

Roger

> Regards,
> Doug Beare
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Wed Jun 18 18:37:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 17:37:05 +0100 (BST)
Subject: [R] dyn.load()  function problem help!
In-Reply-To: <Pine.A41.4.44.0306180902440.52254-100000@homer33.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0306181732170.989-100000@gannet.stats>

On Wed, 18 Jun 2003, Thomas Lumley wrote:

> On Wed, 18 Jun 2003, Mu Yunming wrote:
> 
> > >
> > My source code is writen in fortran 90 and i complied the source code using
> > command f90. How do you think I should complie my source file and build the
> > shared library?
> >
> 
> You probably need to figure this out: R doesn't explicitly support F90 on
> any platform.
> 
> Two things are needed
> 1/  You have to be able to create a DLL that is properly linked to all the
> libraries it needs.  Your compiler documentation should tell you how to do
> this.

That's the main step.  Something like

f90 -G -o aaa.so aaa.f

might well do it (that's the Solaris syntax, BTW).

> 2/ The functions you call from R need to use the same calling conventions
> as the compilers used to compiler R.  Duncan Murdoch has information at
>  http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/
> that describes how to do this for various compilers. Even if your's isn't
> on the list this might still be helpful

That's for Windows: he did say `Unix', unspecified.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andel at ifi.unizh.ch  Wed Jun 18 18:38:54 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Wed, 18 Jun 2003 16:38:54 -0000
Subject: [R] R: Debian package and source
In-Reply-To: <1f1.b36e5df.2c21e0cc@aol.com>
Message-ID: <Pine.GSO.4.44.0306181832300.20378-100000@igor.ifi.unizh.ch>

Hi

As Dirk said before, if you're running unstable, the official version is surely the best.
If you're running testing though, you're probably better off using the apt sources listed in http://cran.us.r-project.org/bin/linux/debian/ReadMe since there are some dependency hassles with installing the unstable version on testing (at least I got tired of going through all the dependency stuff).

Cheers,
David



From p.dalgaard at biostat.ku.dk  Wed Jun 18 19:01:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 18 Jun 2003 17:01:30 -0000
Subject: [R] R: Debian package and source
In-Reply-To: <Pine.GSO.4.44.0306181832300.20378-100000@igor.ifi.unizh.ch>
References: <Pine.GSO.4.44.0306181832300.20378-100000@igor.ifi.unizh.ch>
Message-ID: <x28yrz5n0o.fsf@biostat.ku.dk>

"David Andel" <andel at ifi.unizh.ch> writes:

> Hi
> 
> As Dirk said before, if you're running unstable, the official
> version is surely the best. If you're running testing though, you're
> probably better off using the apt sources listed in
> http://cran.us.r-project.org/bin/linux/debian/ReadMe since there are
> some dependency hassles with installing the unstable version on
> testing (at least I got tired of going through all the dependency
> stuff).
> 

BTW, is there a good reason to keep the "testing" distribution at
1.6.0? I would assume that at least you'd want to include patches up
to 1.6.2 rather than ship a version with known bugs. Not wanting to
get into the 1.7.x series is understandable, of course.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Wed Jun 18 19:04:49 2003
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Jun 2003 13:04:49 -0400
Subject: [R] dyn.load()  function problem help!
In-Reply-To: <Pine.A41.4.44.0306180902440.52254-100000@homer33.u.washington.edu>
References: <3F065418@webmail.uiuc.edu>
	<Pine.A41.4.44.0306180902440.52254-100000@homer33.u.washington.edu>
Message-ID: <hq61fvk4u98kc559g2h8k85upfcrun09cr@4ax.com>

On Wed, 18 Jun 2003 09:09:10 -0700 (PDT), you wrote in message
<Pine.A41.4.44.0306180902440.52254-100000 at homer33.u.washington.edu>:
> Duncan Murdoch has information at
> http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/
>that describes how to do this for various compilers. Even if your's isn't
>on the list this might still be helpful

That web page is Windows-specific, so most of the solutions there
won't work on Yunming's Unix system, but I imagine the issues are
similar.

Duncan Murdoch



From rpeng at stat.ucla.edu  Wed Jun 18 19:08:24 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Wed, 18 Jun 2003 10:08:24 -0700
Subject: [R] Macintosh plots
In-Reply-To: <5F5E400D-A0E6-11D7-BC4C-0003931A92A2@umn.edu>
References: <5F5E400D-A0E6-11D7-BC4C-0003931A92A2@umn.edu>
Message-ID: <3EF09C88.6040708@stat.ucla.edu>

What is d1?  The plot function itself is generic and does not 
necessarily do anything.  It might be that the d1 object is of a 
particular class and the plot method for that class changes the 
background, point color, etc.  You could try

par(bg = "transparent")

but it's possible that the plot method might just reset it.

-roger

Ian Dickie wrote:
> This may be a dumb question, but I've wasted several hours trying to 
> find an answer....
> 
> When I do a plot with no parameters set eg: ' >plot(d1)' it comes up 
> with a dark gray background and blue data points.  I can modify the data 
> points using '>plot(d1, col = "red")', but I have not been able to 
> modify the background color.  '>plot(d1, bg = "white")' does not work.  
> Any help would be most appreciated.
> 
> Cheers,
> Ian.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From edd at debian.org  Wed Jun 18 19:30:35 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 18 Jun 2003 12:30:35 -0500
Subject: [R] R: Debian package and source
In-Reply-To: <x28yrz5n0o.fsf@biostat.ku.dk>
References: <Pine.GSO.4.44.0306181832300.20378-100000@igor.ifi.unizh.ch>
	<x28yrz5n0o.fsf@biostat.ku.dk>
Message-ID: <20030618173035.GA5196@sonny.eddelbuettel.com>

On Wed, Jun 18, 2003 at 07:11:19PM +0200, Peter Dalgaard BSA wrote:
> "David Andel" <andel at ifi.unizh.ch> writes:
> 
> > Hi
> > 
> > As Dirk said before, if you're running unstable, the official
> > version is surely the best. If you're running testing though, you're
> > probably better off using the apt sources listed in
> > http://cran.us.r-project.org/bin/linux/debian/ReadMe since there are
> > some dependency hassles with installing the unstable version on
> > testing (at least I got tired of going through all the dependency
> > stuff).

Almost -- but we have not yet built the separate testing versions for CRAN,
and may not as a) Doug is very busy right now and b) we may not need to, see
below.

> > 
> 
> BTW, is there a good reason to keep the "testing" distribution at
> 1.6.0? I would assume that at least you'd want to include patches up
> to 1.6.2 rather than ship a version with known bugs. Not wanting to
> get into the 1.7.x series is understandable, of course.

Believe me, it is not by choice.  

We were being held back by some architectures failing, or some dependent
libraries failing, and then most recently another bug in lib pulled in by
r-gnome. I spend a decent chunk of time on that in the last few weeks, and
with help from others (Chris Steigies and Kurt deserve special thanks for
their help with m68k where we switched back to f2c) are now in a position
where R 1.7.1 should appear in testing after the usual ten day delay, unless
bugs are found.  At least all architectures built within 24 hrs of my upload
which isn't bad.

[ In previous times we had other problems with the toolchain migration which
led to the separate testing versions Doug built for CRAN, with luck that
won't be required for a little while as those problems are now sorted out
and gcc 3.2/3.3 work uniformly across all our architectures, and required
libraries have caught up. ]

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From rpeng at stat.ucla.edu  Wed Jun 18 19:38:17 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Wed, 18 Jun 2003 10:38:17 -0700
Subject: [R] finding contours in a matrix
In-Reply-To: <67B92F9B2AFED611852500B0D0FE15097EAFA5@mail3.marlab.ac.uk>
References: <67B92F9B2AFED611852500B0D0FE15097EAFA5@mail3.marlab.ac.uk>
Message-ID: <3EF0A389.2050309@stat.ucla.edu>

You may be interested in this message (from the archives):

https://www.stat.math.ethz.ch/pipermail/r-help/2003-June/033583.html

-roger

Douglas Beare wrote:
> Hi,
> I have matrix of sea bottom depths that I am plotting in R with the function
> 'image'.
> I am particularly interested in the 200m depth contour (I'm using 'distance
> from this feature' as a covariate in a model) and would like to extract the
> data at evenly 
> spaced points along it.  I can easily superimpose a line at 200m using the
> function 'contour'.  What I
> want to know is: can I actually get the data used to plot this line? If I
> just select the relevant data depth[depth==200] I
> get a very few observations, but if I say select depth[depth > 180 & depth <
> 220] I get a "band" of 
> unevenly space data along the contour which I don't want either.
> Regards,
> Doug Beare
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From mikalzet at libero.it  Wed Jun 18 20:21:53 2003
From: mikalzet at libero.it (mikalzet@libero.it)
Date: Wed, 18 Jun 2003 20:21:53 +0200 (CEST)
Subject: [R] Mandrake rpms - R 1.7.1
Message-ID: <Pine.LNX.4.50.0306182016380.2994-100000@macchinetta>


Mandrake RPM's (and the relevant SRPM) for 8.0, 8.2, 9.0 and 9.1 have been 
uploaded; 8.1 fails on make check of one of the packages, I'm looking into 
this and hope to upload the rpm in a while (let's say that requests for 
this rpm might stimulate me into doing this in a short time, absence of 
interest may lead to this taking longer ... or not being done at all).

I think I'll drop support for Mandrake 8.* as of next R release.

Please read the updated readme.

-- 
Michele Alzetta



From MZodet at ahrq.gov  Wed Jun 18 20:32:01 2003
From: MZodet at ahrq.gov (MZodet@ahrq.gov)
Date: Wed, 18 Jun 2003 14:32:01 -0400
Subject: [R] 3-way Interactions w/ stepAIC
Message-ID: <3598558AD728D41183350008C7CF291C0A5CD21B@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030618/9e4e2424/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 18 20:49:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 19:49:35 +0100 (BST)
Subject: [R] 3-way Interactions w/ stepAIC
In-Reply-To: <3598558AD728D41183350008C7CF291C0A5CD21B@exchange1.ahrq.gov>
Message-ID: <Pine.LNX.4.44.0306181949070.1355-100000@gannet.stats>

On Wed, 18 Jun 2003 MZodet at ahrq.gov wrote:

> I'm attempting to use stepAIC to select a model through a forward procedure.
> I want to consider up to all 3-way interactions.
> 
>  
> 
> I've attempted to use the following code:
> 
>  
> 
> m2.Fwd3way <- stepAIC(m1.Ionly, direction="forward",
> 
>                       scope=list(upper=~(var1 + var2 + var3 + var4)^3,
> lower=~1))
> 
>  
> 
> When I submit this the trace indicates that only 2-way interactions were
> considered.  Is this due to improper syntax or am I not able to consider
> higher order interactions with stepAIC.

Only if all the two-way ones have already been selected.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cadolph at fas.harvard.edu  Wed Jun 18 21:27:42 2003
From: cadolph at fas.harvard.edu (Christopher Adolph)
Date: Wed, 18 Jun 2003 15:27:42 -0400 (EDT)
Subject: [R] Ltsreg and nsamp="exact"
Message-ID: <Pine.OSF.4.44.0306181525150.25979-100000@is07.fas.harvard.edu>


I'm trying to use least trimmed squares using ltsreg with nsamp="exact".
When I use the following:

rg <- ltsreg(x,y,nsamp="exact")

I get:

Error in lqs.default(x, y, nsamp = "exact", method = "lts") :
        NAs in foreign function call (arg 10)
In addition: Warning message:
NAs introduced by coercion

Incidentally, there are no missings in x or y, and when I run

rg <- ltsreg(x,y)

everything is fine.

Any ideas?

Thanks

Chris Adolph



From ripley at stats.ox.ac.uk  Wed Jun 18 21:34:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jun 2003 20:34:19 +0100 (BST)
Subject: [R] Ltsreg and nsamp="exact"
In-Reply-To: <Pine.OSF.4.44.0306181525150.25979-100000@is07.fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0306182033470.1541-100000@gannet.stats>

Are subsets of your data collinear?

On Wed, 18 Jun 2003, Christopher Adolph wrote:

> 
> I'm trying to use least trimmed squares using ltsreg with nsamp="exact".
> When I use the following:
> 
> rg <- ltsreg(x,y,nsamp="exact")
> 
> I get:
> 
> Error in lqs.default(x, y, nsamp = "exact", method = "lts") :
>         NAs in foreign function call (arg 10)
> In addition: Warning message:
> NAs introduced by coercion
> 
> Incidentally, there are no missings in x or y, and when I run
> 
> rg <- ltsreg(x,y)
> 
> everything is fine.
> 
> Any ideas?
> 
> Thanks
> 
> Chris Adolph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paulda at BATTELLE.ORG  Wed Jun 18 23:09:39 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Wed, 18 Jun 2003 17:09:39 -0400
Subject: [R] Multiple graph sheets
Message-ID: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>

In Splus the code

test.lm <- lm(y ~ x, data = test.data)
plot(test.lm)

generates a graphics window that contains
multiple graph sheets that one may choose
from via the "page" tabs at the bottom of
the window.

Is there a way to do this sort of thing in
R?  As another example, I have some repeated
measures data with continuous outcomes and 
have been working with the nlme library in Splus.
When I use the commands


library(nlme)
data.grouped <- groupedData(y ~ time | x, 
                            data = test.data)
plot(data.grouped, layout = c(5,3,11))


generates 11 separate graph sheets that can
be toggled between.  In R, the same commands
generate a sequence of graph sheets, but only
the last one is "saved" in the graphics 
window.  Is there a way around this?


Much thanks,

David Paul, Ph.D.
Battelle Memorial Institute
614.424.3176



From chunlou at yahoo.com  Thu Jun 19 00:21:24 2003
From: chunlou at yahoo.com (Chunlou Yung)
Date: Wed, 18 Jun 2003 18:21:24 -0400
Subject: [R] R via Browser
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <NCBBKDNFIKJKKCFELNNMMEBDDHAA.chunlou@yahoo.com>

It seems all R-to-CGI libraries (all two of them, which I'm aware) run only
on Unix/Linux  (but I use Windows) and create temporary files to pass R
commands.

So, I wrote a short Apache-based Perl CGI script to execute R commands on
browser without needing temporary files--just for fun, babyish, nothing
robust, slow but works. (The rationale? I prefer distributing results via
the intranet on browser, rather than sending huge spreadsheets around, for
which the sysadmin bitterly complained causing the email server to grow
unduly large.)

Here it is, in case anyone interested (more like a "sample answer" than a
product, for sure):

#-----------------------------File Name:
simpleR.pl---------------------------
#! /usr/local/bin/perl -w
use Apache::Request () ;
use strict ;
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# You only need to modify this:
my $Rpath = "C:\\R\\rw\\bin\\" ;		# path to rterm.exe
# The rest will hopefully run itself.
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# to execute R cmd: R($Rpath, $R_cmd)
sub R {
	my $Rpath = shift ;
	my $Rcmd = $Rpath . "rterm --vanilla --quiet --slave" ;
	my $Rscript = shift ;
	$Rscript =~ s/(\r|;\r)/ ;/gm ; $Rscript =~ s/<-/=/gm ;	# \r or <- break
"echo"
	return `echo $Rscript | $Rcmd` ;
}
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
my $r = shift ;					# Apache stuff
my $q = Apache::Request->new($r) ;		# Apache Query obj
my $command = $q->param('command') ;
my $result = $command ? R($Rpath, $command) : "You didn't input any
command." ;
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
print "Content-type: text/html\n\n";
print <<"EOF";
<html><body>
	<form method="get" action="./simpleR.pl">
		Please enter your R command:<br>
		<textarea rows="4" name="command" cols="60">$command</textarea><br>
		<input type="submit" value="Submit">
	</form><br>
	<textarea rows="10" name="result" cols="80">$result</textarea>
</body></html>
EOF
exit;



From aandres1 at cogeco.ca  Thu Jun 19 04:51:12 2003
From: aandres1 at cogeco.ca (Ariel Andres)
Date: Wed, 18 Jun 2003 22:51:12 -0400
Subject: [R] Import time series data with uneven dates
Message-ID: <ENENLCEJJBACFDAJGAMIKEAHCBAA.aandres1@cogeco.ca>

I am trying to import a file of daily index closing prices in business time
which excludes weekends and holidays so deltat is not constant. My file
looks like the following:
date        close
2003.0055   47.05
2003.0082	45.71
2003.0164   43.45
2003.0192   42.96
2003.0219   44.56
2003.0247   42.99
2003.0274   42.28
2003.0356   41.74
etc.

>From what I saw in the EuStockMarkets file, it appears that they are also
using years and fractions of years as the measure of time. How do I read my
file and still keep 'date' as the date?

Thanks!

A. Andres
95 Wellington St W
Toronto, Ontario
M5J 2N7



From zhuw at mail.smu.edu  Thu Jun 19 07:37:58 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Thu, 19 Jun 2003 05:37:58 -0000
Subject: [R] What's wrong with ar for my data?
Message-ID: <1056001059.1831.11.camel@zwang.stat.smu.edu>

Dear helpers,

When I use ar to fit the data with length 180, I have the following
error:

ar(x,method="burg")
Error in acf(x, type = "covariance",lag.max=order,plot=FALSE):
         lag.max must be at least 1

If I use 

ar(x), then I have

Call (x=x)


order selected 0 sigma^2 estimated as 5374

Obviously I missed some points for using ar. 
This is R 1.7.0 under Redhat Linux 9.0
Thanks in advance.

Zhu Wang
Statistical Science Department
Southern Methodist University

-- 
zhu wang <zhuw at mail.smu.edu>



From ripley at stats.ox.ac.uk  Thu Jun 19 08:11:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jun 2003 07:11:50 +0100 (BST)
Subject: [R] Multiple graph sheets
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <Pine.LNX.4.44.0306190706490.2359-100000@gannet.stats>

On Wed, 18 Jun 2003, Paul, David  A wrote:

> In Splus the code
> 
> test.lm <- lm(y ~ x, data = test.data)
> plot(test.lm)
> 
> generates a graphics window that contains
> multiple graph sheets that one may choose
> from via the "page" tabs at the bottom of
> the window.
> 
> Is there a way to do this sort of thing in
> R?  

Well, you can only do that in S-PLUS *for Windows* (it is one of several
options for a graphsheet() device). In R for Windows there is a windows()  
device where you can turn on history recording and move between plots with
the page keys.  (This is in the README!)

One again, R is not S-PLUS, and if you want features of S-PLUS, why not 
use it?  Especially if they are user-level convenience features.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Regis.Martin at univ-savoie.fr  Thu Jun 19 08:31:38 2003
From: Regis.Martin at univ-savoie.fr (Regis Martin)
Date: Thu, 19 Jun 2003 08:31:38 +0200
Subject: [R] Path analysis
Message-ID: <200306190631.h5J6Vc05007731@post.bourget.univ-savoie.fr>


Dear all,

I'm new R's user and I'm looking for package dealling with Path analysis. Does it exist ? Where ? 

Best,



Regis Martin PhD Student
Laboratory of Altitutdinal Population Biology UMR CNRS 5553
Universté de Savoie
Bât. Belledonnes
00 33 (0)4 79 75 86 44
regis.martin at univ-savoie.fr



From ripley at stats.ox.ac.uk  Thu Jun 19 08:52:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jun 2003 07:52:16 +0100 (BST)
Subject: [R] What's wrong with ar for my data?
In-Reply-To: <1056001059.1831.11.camel@zwang.stat.smu.edu>
Message-ID: <Pine.LNX.4.44.0306190748200.2359-100000@gannet.stats>

You are apparently fitting a series for which the selected order is zero,
and ar.burg is not designed to cope with that (and would in any case tell 
you nothing useful).  The default method does cope, from your output.

Why are you fitting an AR model to a series with apparently no 
correlation?

On 19 Jun 2003, zhu wang wrote:

> Dear helpers,
> 
> When I use ar to fit the data with length 180, I have the following
> error:
> 
> ar(x,method="burg")
> Error in acf(x, type = "covariance",lag.max=order,plot=FALSE):
>          lag.max must be at least 1
> 
> If I use 
> 
> ar(x), then I have
> 
> Call (x=x)

[I very much doubt that is what you get.]

> 
> 
> order selected 0 sigma^2 estimated as 5374
> 
> Obviously I missed some points for using ar. 
> This is R 1.7.0 under Redhat Linux 9.0

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Giles.Heywood at CommerzbankIB.com  Thu Jun 19 09:03:42 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Thu, 19 Jun 2003 08:03:42 +0100
Subject: [R] Import time series data with uneven dates
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF540389@xmx8lonib.lonib.commerzbank.com>

A solution is at hand using the 'irts' (irreglar time-series) 
class from package tseries.

If your raw data is in a csv file, you could proceed as follows:

mydata <- read.csv(filename,header=TRUE)
basedate <- as.POSIXct(strptime("2003-01-01 00:00:00",format="%Y-%m-%d %X"))
rawdates <- basedate+(mydata[,1]-floor(mydata[,1]))*24*60*60*365 -60*60
truncdates <-
as.POSIXct(strptime(format.POSIXct(rawdates,format="%Y-%m-%d"),,format="%Y-%
m-%d"))
require(tseries)
x <- irts(time=truncdates,mydata[,2])

The result is an object of class 'irts'.

The format of the dates in your file is particularly unhelpful! If
they are available in a format recognisable by strptime, the above
could be simplified and made more robust.

Incidentally, I plan to post a slightly different 'its' (irregular 
time-series) package sometime soon.

Giles

> -----Original Message-----
> From: Ariel Andres [mailto:aandres1 at cogeco.ca]
> Sent: 19 June 2003 03:51
> To: r-help at stat.math.ethz.ch
> Subject: [R] Import time series data with uneven dates
> 
> 
> I am trying to import a file of daily index closing prices in 
> business time
> which excludes weekends and holidays so deltat is not 
> constant. My file
> looks like the following:
> date        close
> 2003.0055   47.05
> 2003.0082	45.71
> 2003.0164   43.45
> 2003.0192   42.96
> 2003.0219   44.56
> 2003.0247   42.99
> 2003.0274   42.28
> 2003.0356   41.74
> etc.
> 
> >From what I saw in the EuStockMarkets file, it appears that 
> they are also
> using years and fractions of years as the measure of time. 
> How do I read my
> file and still keep 'date' as the date?
> 
> Thanks!
> 
> A. Andres
> 95 Wellington St W
> Toronto, Ontario
> M5J 2N7
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ This ... {{dropped}}



From marwan.khawaja at aub.edu.lb  Thu Jun 19 19:55:07 2003
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Thu, 19 Jun 2003 10:55:07 -0700
Subject: [R] Path analysis
In-Reply-To: <200306190631.h5J6Vc05007731@post.bourget.univ-savoie.fr>
Message-ID: <CLECJBOEBGOMOKJHJNDAMEJFDAAA.marwan.khawaja@aub.edu.lb>

Check out the 'sem' package by John Fox.
Marwan

=======================
Marwan Khawaja					marwan.khawaja at aub.edu.lb
Associate Professor & Director				http://webfaculty.aub.edu.lb/~mk36
Center for Research on Population & Health
Faculty of Health Sciences				Tel:  +961 1 35 00 00 ext. 4668 (O)
American University of Beirut				       +961 1 35 00 00 ext. 4640 (O))
P.O.Box 11--0236, Riad El-Solh				       +961 1 35 00 00 ext. 2821 (H)
Beirut 1107 2020, Lebanon 				Fax: +961 1 74 44 70


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Regis Martin
Sent: Wednesday, June 18, 2003 11:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Path analysis



Dear all,

I'm new R's user and I'm looking for package dealling with Path analysis. Does
it exist ? Where ?

Best,



Regis Martin PhD Student
Laboratory of Altitutdinal Population Biology UMR CNRS 5553
Universt? de Savoie
B?t. Belledonnes
00 33 (0)4 79 75 86 44
regis.martin at univ-savoie.fr

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tpoloni at netcourrier.com  Thu Jun 19 14:20:05 2003
From: tpoloni at netcourrier.com (tpoloni@netcourrier.com)
Date: Thu, 19 Jun 2003 14:20:05 CEST
Subject: [R] sciViews
Message-ID: <mnet4.1056025205.2438.tpoloni@netcourrier.com>

Bonjour,

J'ai t?l?charg? SciViews Insider que je trouve tr?s convivial.
Par contre, je n'arrive pas ? comprendre comment enregistrer un script R en type de fichier R justement.
Mes programmes fonctionnent tr?s bien, mais SciViews me propose uniquement de les enregistrer au format txt sous un type de fichier "bloc notes".
Comment les enregistrer avec l'extension .R comme le ScriptExample.R ??
Merci

Thomas Poloni

-------------------------------------------------------------
NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
Web/Wap : www.netcourrier.com
T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)



From tpoloni at netcourrier.com  Thu Jun 19 14:26:08 2003
From: tpoloni at netcourrier.com (tpoloni@netcourrier.com)
Date: Thu, 19 Jun 2003 14:26:08 CEST
Subject: [R] SciViews
Message-ID: <mnet5.1056025568.2438.tpoloni@netcourrier.com>

Bonjour,

J'ai t?l?charg? SciViews Insider que je trouve tr?s convivial.
Par contre, je n'arrive pas ? comprendre comment enregistrer un script R en type de fichier R justement.
Mes programmes fonctionnent tr?s bien, mais SciViews me propose uniquement de les enregistrer au format txt sous un type de fichier "bloc notes".
Comment les enregistrer avec l'extension .R comme le ScriptExample.R ??
Merci

Thomas Poloni

-------------------------------------------------------------
NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
Web/Wap : www.netcourrier.com
T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)



From henric.nilsson at statisticon.se  Thu Jun 19 14:35:07 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 19 Jun 2003 14:35:07 +0200
Subject: [R] Grouping binary data
Message-ID: <5.2.1.1.0.20030619131554.03715238@pop3.norton.antivirus>

Dear all,

I'm analyzing a binary outcome using glm() with a binomial distribution and 
a logit link, and have now reached the point where I'd like to do some 
model checking. Since my data are in binary form I'd like to collapse over 
the cross-classification of the factors before the model checking.

Are there any nice and simple ways doing this? If so, how? If not, I'd be 
grateful for receiving some hints on how this can be accomplished.

Many thanks,
Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From spencer.graves at pdf.com  Thu Jun 19 14:50:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 19 Jun 2003 05:50:31 -0700
Subject: [R] sciViews
References: <mnet4.1056025205.2438.tpoloni@netcourrier.com>
Message-ID: <3EF1B197.3070104@pdf.com>

1.  The official language of this list is English.

2.  You might get better help if you tell us what version of R you are 
using under which operating system.  Also, a toy example of a short 
piece of script illustrating the problem might make it easier for others 
to understand you.

Je ne peux pas comprendre votre question.

Spencer Graves

tpoloni at netcourrier.com wrote:
> Bonjour,
> 
> J'ai t?l?charg? SciViews Insider que je trouve tr?s convivial.
> Par contre, je n'arrive pas ? comprendre comment enregistrer un script R en type de fichier R justement.
> Mes programmes fonctionnent tr?s bien, mais SciViews me propose uniquement de les enregistrer au format txt sous un type de fichier "bloc notes".
> Comment les enregistrer avec l'extension .R comme le ScriptExample.R ??
> Merci
> 
> Thomas Poloni
> 
> -------------------------------------------------------------
> NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
> Web/Wap : www.netcourrier.com
> T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
> Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Jun 19 14:54:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jun 2003 13:54:10 +0100 (BST)
Subject: [R] Grouping binary data
In-Reply-To: <5.2.1.1.0.20030619131554.03715238@pop3.norton.antivirus>
Message-ID: <Pine.LNX.4.44.0306191349010.14814-100000@gannet.stats>

On Thu, 19 Jun 2003, Henric Nilsson wrote:

> Dear all,
> 
> I'm analyzing a binary outcome using glm() with a binomial distribution and 
> a logit link, and have now reached the point where I'd like to do some 
> model checking. Since my data are in binary form I'd like to collapse over 
> the cross-classification of the factors before the model checking.
> 
> Are there any nice and simple ways doing this? If so, how? If not, I'd be 
> grateful for receiving some hints on how this can be accomplished.

Look at loglm1.data.frame in package MASS, or xtabs: each work by setting 
up a suitable call to table.

If you want to sum up over cases with the same factors (not what is 
usually meant by collapsing), take a look at multinom (package nnet) which 
has options to do this for multinomials (and Bernoulli is a special case).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Thu Jun 19 14:57:03 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 19 Jun 2003 14:57:03 +0200
Subject: [R] sciViews
In-Reply-To: <mnet4.1056025205.2438.tpoloni@netcourrier.com>
Message-ID: <MABBLJDICACNFOLGIHJOAELHDIAA.phgrosjean@sciviews.org>

Please, use the mailing list language: English (or mail me directly).

(French version of the answer follows).

The question is: how to save a R script document in SciViews since the "save
as..." dialog box proposes only '.txt' files as allowed type.

Answer: you just have to append a '.R' extension to the name of your script
file, instead of '.txt' . The lack of a 'R script' document entry in the
list of allowed types will be corrected in the next version.


Il suffit de pr?ciser l'extension '.R' dans le nom du fichier, ? la place de
'.txt'. Une entr?e document 'R script' sera rajout?e dans la liste des
fichiers autoris?s pour lever cette ambiguit? dans la prochiane version.

Best regards,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
tpoloni at netcourrier.com
Sent: jeudi 19 juin 2003 4:20
To: r-help at stat.math.ethz.ch
Subject: [R] sciViews


Bonjour,

J'ai t?l?charg? SciViews Insider que je trouve tr?s convivial.
Par contre, je n'arrive pas ? comprendre comment enregistrer un script R en
type de fichier R justement.
Mes programmes fonctionnent tr?s bien, mais SciViews me propose uniquement
de les enregistrer au format txt sous un type de fichier "bloc notes".
Comment les enregistrer avec l'extension .R comme le ScriptExample.R ??
Merci

Thomas Poloni

-------------------------------------------------------------
NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs,
Toolbar...
Web/Wap : www.netcourrier.com
T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From vito.muggeo at giustizia.it  Thu Jun 19 15:27:24 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 19 Jun 2003 15:27:24 +0200
Subject: R: [R] Grouping binary data
References: <5.2.1.1.0.20030619131554.03715238@pop3.norton.antivirus>
Message-ID: <015001c33666$886fd3a0$5c13070a@GIUSTIZIA.IT>

Dear Henric,
The following paper deals with goodness-of-fit test for sparse (and even
binary) data:

Kuss O. Global goodness-of-fit tests in logistic regression with sparse
data, Statist Med, 2002, 21:3789-3801.

It should not too hard to write code for some non-standard and (probably
under-used) GoF statistics...

hope this helps you,
best,
vito



----- Original Message -----
From: Henric Nilsson <henric.nilsson at statisticon.se>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 19, 2003 2:35 PM
Subject: [R] Grouping binary data


Dear all,

I'm analyzing a binary outcome using glm() with a binomial distribution and
a logit link, and have now reached the point where I'd like to do some
model checking. Since my data are in binary form I'd like to collapse over
the cross-classification of the factors before the model checking.

Are there any nice and simple ways doing this? If so, how? If not, I'd be
grateful for receiving some hints on how this can be accomplished.

Many thanks,
Henric

----------------------------------------------------------------------------
-----------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From raf1729 at hotmail.com  Thu Jun 19 15:37:40 2003
From: raf1729 at hotmail.com (R A F)
Date: Thu, 19 Jun 2003 13:37:40 +0000
Subject: [R] Check whether size(file) > 0
Message-ID: <Law11-F30ia5wAz8AWi000b2481@hotmail.com>

Hi all, what's the way to check for this?  I know of "file.exists," but that 
does not
preclude the possibility of an empty file.

Thanks very much!



From MSchwartz at medanalytics.com  Thu Jun 19 15:47:27 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 19 Jun 2003 13:47:27 -0000
Subject: [R] Check whether size(file) > 0
In-Reply-To: <Law11-F30ia5wAz8AWi000b2481@hotmail.com>
References: <Law11-F30ia5wAz8AWi000b2481@hotmail.com>
Message-ID: <1056030433.4270.41.camel@localhost>

On Thu, 2003-06-19 at 08:37, R A F wrote:
> Hi all, what's the way to check for this?  I know of "file.exists," but that 
> does not
> preclude the possibility of an empty file.
> 
> Thanks very much!


See ?file.info


HTH,

Marc Schwartz



From sundar.dorai-raj at pdf.com  Thu Jun 19 15:47:19 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 19 Jun 2003 08:47:19 -0500
Subject: [R] Check whether size(file) > 0
References: <Law11-F30ia5wAz8AWi000b2481@hotmail.com>
Message-ID: <3EF1BEE7.3070506@pdf.com>

?file.info

R A F wrote:
> Hi all, what's the way to check for this?  I know of "file.exists," but 
> that does not
> preclude the possibility of an empty file.
> 
> Thanks very much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Roger.Bivand at nhh.no  Thu Jun 19 15:50:54 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 19 Jun 2003 15:50:54 +0200 (CEST)
Subject: [R] Check whether size(file) > 0
In-Reply-To: <Law11-F30ia5wAz8AWi000b2481@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306191550080.4110-100000@reclus.nhh.no>

On Thu, 19 Jun 2003, R A F wrote:

> Hi all, what's the way to check for this?  I know of "file.exists," but that 
> does not
> preclude the possibility of an empty file.
> 

file.info("file")$size

> Thanks very much!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From zhuw at mail.smu.edu  Thu Jun 19 16:46:14 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Thu, 19 Jun 2003 14:46:14 -0000
Subject: [R] What's wrong with ar for my data?
In-Reply-To: <Pine.LNX.4.44.0306190748200.2359-100000@gannet.stats>
References: <Pine.LNX.4.44.0306190748200.2359-100000@gannet.stats>
Message-ID: <1056033953.1176.3.camel@zwang.stat.smu.edu>

Thanks.


On Thu, 2003-06-19 at 01:52, Prof Brian Ripley wrote:
> You are apparently fitting a series for which the selected order is zero,
> and ar.burg is not designed to cope with that (and would in any case tell 
> you nothing useful).  The default method does cope, from your output.
> 
> Why are you fitting an AR model to a series with apparently no 
> correlation?
> 

The data are from simulation. Thanks for pointing out to me no
correlation. 

> On 19 Jun 2003, zhu wang wrote:
> 
> > Dear helpers,
> > 
> > When I use ar to fit the data with length 180, I have the following
> > error:
> > 
> > ar(x,method="burg")
> > Error in acf(x, type = "covariance",lag.max=order,plot=FALSE):
> >          lag.max must be at least 1
> > 
> > If I use 
> > 
> > ar(x), then I have
> > 
> > Call (x=x)
> 
> [I very much doubt that is what you get.]
> 
> > 
> > 
> > order selected 0 sigma^2 estimated as 5374
> > 
> > Obviously I missed some points for using ar. 
> > This is R 1.7.0 under Redhat Linux 9.0
-- 
zhu wang <zhuw at mail.smu.edu>



From elsawy at ysbl.york.ac.uk  Thu Jun 19 16:50:15 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Thu, 19 Jun 2003 15:50:15 +0100
Subject: [R] clines loading error
References: <8CBAA121CEB4D5118CB200508BB2BBEF540389@xmx8lonib.lonib.commerzbank.com>
Message-ID: <3EF1CDA7.F3402025@ysbl.york.ac.uk>

I'm running R 1.4.1 under linux , recently I installed the package
clines_1.0.tar.gz
R CMD INSTALL clines_1.0.tar.gz
and it is installed with no errors

when I try to use it , I get this error

library(clines)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"/usr/lib/R/library/clines/libs/clines.so":
  /usr/lib/R/library/clines/libs/clines.so: undefined symbol:
GEcontourLines
Error in library(clines) : .First.lib failed

any help
best regards
karim



From f.calboli at ucl.ac.uk  Thu Jun 19 17:19:46 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Thu, 19 Jun 2003 16:19:46 +0100
Subject: [R] GLME
Message-ID: <3.0.6.32.20030619161946.02b32d78@pop-server.ucl.ac.uk>

Hi All, 

does anyone know if the package GLME by J. Pinheiro is available anywhere
in any form? checking on the archive I got that it was at some point, as as
a beta version (for S-Plus only, alas)...

Cheers,

Federico

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From Charles.Annis at statisticalengineering.com  Thu Jun 19 17:41:11 2003
From: Charles.Annis at statisticalengineering.com (Charles Annis, P.E.)
Date: Thu, 19 Jun 2003 11:41:11 -0400
Subject: [R] Constrained optimization
Message-ID: <004a01c33679$38ac1410$2802a8c0@DHT0TL11>

Greetings, R-Wizards:

Earlier this week I requested help with trying to find an extremum subject
to a nonlinear constraint.  Many thanks to Martin Maechler, Spencer Graves,
and Jonathan Baron, who all suggested optim() rather than nlm(), and to
Robert Gentleman who suggested using a half-interval search, and special
thanks to Patrick Burns and David Khabie-Zeitoune who suggested ways to
create an appropriate objective function to use with optim().  

While working on the solution I took a closer look at the likelihood
profiles and reparameterized to avoid precipitous behavior in the
neighborhood of zero.  

Thus the solution for me was three-fold:

1) Re-parameterize to provide a less-asymmetric likelihood surface.

2) Use box constraints on the parameters to keep the solution-finding under
control and thus prevent excursions beyond the domain of parameter
feasibility.

3) Define an objective function that had the right objective!  ~:-)

Thanks to all for your help.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFAX: 503-217-5849
http://www.StatisticalEngineering.com



From mhayashi at rmail.plala.or.jp  Thu Jun 19 17:43:58 2003
From: mhayashi at rmail.plala.or.jp (Masayoshi Hayashi)
Date: Fri, 20 Jun 2003 00:43:58 +0900
Subject: [R] saving window size of a graphics device in windows possible?
Message-ID: <FHEKKDCEIDIPFELPNCBDGEBPCBAA.mhayashi@rmail.plala.or.jp>

In MDI mode of Windows, is it possible to save window size and other
parameter settings of a graphics device when it is activated, much like
Rconsole file?

By default the bottom part of a graphic device (windows()) is too big such
that it is hidden under the horizontal scrolling and status bar of Rgui. (Of
course I can use the vertical scrolling bar of Rgui to see the hidden part.)

So I select "fit to window" option under graphics device menu, then
maximizes the window. I want to customize this routine. Thanks in advance.



From ripley at stats.ox.ac.uk  Thu Jun 19 18:38:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jun 2003 17:38:12 +0100 (BST)
Subject: [R] clines loading error
In-Reply-To: <3EF1CDA7.F3402025@ysbl.york.ac.uk>
Message-ID: <Pine.LNX.4.44.0306191735220.15085-100000@gannet.stats>

I think your R is too old.  Packages can indicate they need a particular 
version of R, but it is unreasonable to expect them to be tested on 
versions predating the package (which as clines is not on CRAN I can't 
check but suspect to be the case).

On Thu, 19 Jun 2003, Karim Elsawy wrote:

> I'm running R 1.4.1 under linux , recently I installed the package
> clines_1.0.tar.gz
> R CMD INSTALL clines_1.0.tar.gz
> and it is installed with no errors
> 
> when I try to use it , I get this error
> 
> library(clines)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "/usr/lib/R/library/clines/libs/clines.so":
>   /usr/lib/R/library/clines/libs/clines.so: undefined symbol:
> GEcontourLines
> Error in library(clines) : .First.lib failed

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 19 18:55:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jun 2003 17:55:19 +0100 (BST)
Subject: [R] saving window size of a graphics device in windows possible?
In-Reply-To: <FHEKKDCEIDIPFELPNCBDGEBPCBAA.mhayashi@rmail.plala.or.jp>
Message-ID: <Pine.LNX.4.44.0306191750080.15085-100000@gannet.stats>

On Fri, 20 Jun 2003, Masayoshi Hayashi wrote:

> In MDI mode of Windows, is it possible to save window size and other
> parameter settings of a graphics device when it is activated, much like
> Rconsole file?

No, but you don't need to.

Size is part of the arguments of the windows() device, and you can write a
wrapper with different arguments and set that as the default device (via
options(device="mywindows") in e.g. .Rprofile.).

> By default the bottom part of a graphic device (windows()) is too big such
> that it is hidden under the horizontal scrolling and status bar of Rgui. (Of
> course I can use the vertical scrolling bar of Rgui to see the hidden part.)

Then Windows is lying about the graphics dimensions (not unknown), as the
code ensures that neither initial dimension is more than 85% of the same
dimension of the screen.  You probably need to set the screen resolution:
see ?windows

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paulda at BATTELLE.ORG  Thu Jun 19 18:14:30 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 19 Jun 2003 12:14:30 -0400
Subject: [R] Background color(s) for groupedData plot
Message-ID: <940250A9EB37A24CBE28D858EF07774967A932@ws-bco-mse3.milky-way.battelle.org>

I've been using par() to check the graphics parameters
associated with both plot(<fitted linear model>) and
plot(<grouped data object>).  AFAIK the only differences
are in the $cxy, $usr, $xaxp, and $yaxp parameters but
the background color for the grouped data plot is grey
while the linear model plot has a white background.

When I've tried par(bg = "white") prior to using the
plot(<grouped data object>) command it doesn't seem to make any
difference.

How can I change the grey background to something else?


Much thanks in advance,
  David Paul



From Roger.Bivand at nhh.no  Thu Jun 19 19:06:41 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 19 Jun 2003 19:06:41 +0200 (CEST)
Subject: [R] clines loading error
In-Reply-To: <Pine.LNX.4.44.0306191735220.15085-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0306191903050.4110-100000@reclus.nhh.no>

On Thu, 19 Jun 2003, Prof Brian Ripley wrote:

> I think your R is too old.  Packages can indicate they need a particular 
> version of R, but it is unreasonable to expect them to be tested on 
> versions predating the package (which as clines is not on CRAN I can't 
> check but suspect to be the case).

This is correct, clines depends on graphics code that dates from mid 
October 2002; Paul Murrell wrote then: "In 1.6.0 I have exposed a C-level 
call for calculating contour lines". So you need at least 1.6.0 to use 
clines.

> 
> On Thu, 19 Jun 2003, Karim Elsawy wrote:
> 
> > I'm running R 1.4.1 under linux , recently I installed the package
> > clines_1.0.tar.gz
> > R CMD INSTALL clines_1.0.tar.gz
> > and it is installed with no errors
> > 
> > when I try to use it , I get this error
> > 
> > library(clines)
> > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> >         unable to load shared library
> > "/usr/lib/R/library/clines/libs/clines.so":
> >   /usr/lib/R/library/clines/libs/clines.so: undefined symbol:
> > GEcontourLines
> > Error in library(clines) : .First.lib failed
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From lancelot at sentoo.sn  Thu Jun 19 19:34:52 2003
From: lancelot at sentoo.sn (Renaud Lancelot)
Date: Thu, 19 Jun 2003 17:34:52 +0000
Subject: [R] Background color(s) for groupedData plot
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A932@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967A932@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3EF1F43C.8030303@sentoo.sn>

Paul, David A wrote:
> I've been using par() to check the graphics parameters
> associated with both plot(<fitted linear model>) and
> plot(<grouped data object>).  AFAIK the only differences
> are in the $cxy, $usr, $xaxp, and $yaxp parameters but
> the background color for the grouped data plot is grey
> while the linear model plot has a white background.
> 
> When I've tried par(bg = "white") prior to using the
> plot(<grouped data object>) command it doesn't seem to make any
> difference.
> 
> How can I change the grey background to something else?
> 
> 
> Much thanks in advance,
>   David Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

Hi Paul,

This method calls lattice plots. An easy way to change the bg color is 
to call trellis.device explictely, e.g.

trellis.device(bg = 0)
plot(<grouped data object>)

Best,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
CIRAD, D?partement Elevage et M?decine V?t?rinaire (CIRAD-Emvt)
Programme Productions Animales
http://www.cirad.fr/fr/pg_recherche/page.php?id=14

ISRA-LNERV                      tel    +221 832 49 02
BP 2057 Dakar-Hann              fax    +221 821 18 79 (CIRAD)
Senegal                         e-mail renaud.lancelot at cirad.fr



From bates at stat.wisc.edu  Thu Jun 19 19:46:43 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 19 Jun 2003 17:46:43 -0000
Subject: [R] GLME
In-Reply-To: <3.0.6.32.20030619161946.02b32d78@pop-server.ucl.ac.uk>
References: <3.0.6.32.20030619161946.02b32d78@pop-server.ucl.ac.uk>
Message-ID: <6rllvy55ae.fsf@bates4.stat.wisc.edu>

Federico Calboli <f.calboli at ucl.ac.uk> writes:

> Hi All, 
> 
> does anyone know if the package GLME by J. Pinheiro is available anywhere
> in any form? checking on the archive I got that it was at some point, as as
> a beta version (for S-Plus only, alas)...
> 
> Cheers,
> 
> Federico

I don't think it is available for R.  You can fit a generalized linear
mixed model in R using function glmmPQL from package MASS. Saikat
DebRoy and I will release version 0.2 of the lme4 package that also
implements PQL parameter estimation for GLMMs.  In July we will
release the 0.3 version that provides both PQL and PQL followed by
maximum likelihood based on second-order Laplace approximation.  The
0.4 release (not yet scheduled) will provide maximum likelihood using
adaptive Gauss-Hermite quadrature.



From james.holtman at convergys.com  Thu Jun 19 21:01:27 2003
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Thu, 19 Jun 2003 15:01:27 -0400
Subject: [R] Problem reading a PDF output
Message-ID: <OF4A05A9B5.EF6C0869-ON85256D4A.00656F54@convergys.com>

I generated a PDF output file of 10 plots.  When I try to view it with
Adobe reader (R4 & R5), it will lockup the reader (it is consuming 100% of
the CPU) after presenting the 4th plot.  I can generate the plots just fine
in Windows and as a postscript file reading it with GSview.

Is there anyway to tell what might be wrong with the PDF output?  The file
is 890KB in size if anyone would like to look at it.  The postscript file
is 783KB in size.

I was able to isolate it to a single plot in a PDF file and it had 38,000
lines of the following that composed 99% of the file:  (I was plotting out
individual events, which were about that many)

86.66 88.67 m
86.66 92.81 l
86.66 96.95 l
86.66 101.09 l
86.66 105.23 l
86.66 109.37 l
86.66 113.51 l
:::::::::::::  38,000 more of the same
388.87 96.95 l
388.87 92.81 l
388.97 88.67 l
389.07 84.53 l
S
Q q
0.000 0.000 0.000 RG
0.75 w
[] 0 d

Is this breaking some limit in PDF?

I am running:

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.1
year     2003
month    06
day      16
language R
__________________________________________________________
James Holtman               "What is the problem you are trying to solve?"
Executive Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
(513) 723-2929

--
"NOTICE:  The information contained in this electronic mail tran... {{dropped}}



From paulda at BATTELLE.ORG  Thu Jun 19 20:48:53 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 19 Jun 2003 14:48:53 -0400
Subject: [R] Background color(s) for groupedData plot
Message-ID: <940250A9EB37A24CBE28D858EF07774967A937@ws-bco-mse3.milky-way.battelle.org>

Thank you both for pointing out that this is a lattice
plot (ie, R's version of Trellis graphics) and therefore
needs something other than par().

I was able to use IE6.0 to search for "trellis" and
find the relevant commands (after using help.start(),
of course).  This brings up another question:

Is there a convenient way to decide whether or not
the generic plot( ) is going to use "regular" or
"trellis" plotting?  I looked at methods(plot) and
didn't find any groupedData plot methods listed, so
perhaps this is the clue?


-david paul



From ligges at statistik.uni-dortmund.de  Thu Jun 19 21:28:02 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jun 2003 21:28:02 +0200
Subject: [R] Background color(s) for groupedData plot
References: <940250A9EB37A24CBE28D858EF07774967A932@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3EF20EC2.5F22D606@statistik.uni-dortmund.de>



"Paul, David A" wrote:
> 
> I've been using par() to check the graphics parameters
> associated with both plot(<fitted linear model>) and
> plot(<grouped data object>).  AFAIK the only differences
> are in the $cxy, $usr, $xaxp, and $yaxp parameters but
> the background color for the grouped data plot is grey
> while the linear model plot has a white background.
> 
> When I've tried par(bg = "white") prior to using the
> plot(<grouped data object>) command it doesn't seem to make any
> difference.
> 
> How can I change the grey background to something else?
> 
> Much thanks in advance,
>   David Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


I guess you are using package nlme.
Plots for the different GroupedData classes are produced with package
lattice.
So you cannot use par(). See the documentation for package lattice.

After you have opened a device, you might want to use
  lset(col.whitebg())

or simply look into ?trellis.device how to set black and white schemes
and so on.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Jun 19 21:28:48 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jun 2003 21:28:48 +0200
Subject: [R] Problem reading a PDF output
References: <OF4A05A9B5.EF6C0869-ON85256D4A.00656F54@convergys.com>
Message-ID: <3EF20EF0.73FB8419@statistik.uni-dortmund.de>

james.holtman at convergys.com wrote:
> 
> I generated a PDF output file of 10 plots.  When I try to view it with
> Adobe reader (R4 & R5), it will lockup the reader (it is consuming 100% of
> the CPU) after presenting the 4th plot.  I can generate the plots just fine
> in Windows and as a postscript file reading it with GSview.
> 
> Is there anyway to tell what might be wrong with the PDF output?  

I don't know of such a bug. Can you provide a simple example that
(re)produces such a plot that "lockups" Acrobat Reader? I guess it just
needs a huge amount of time for calculations, but I might be wrong ...

Uwe Ligges


> The file
> is 890KB in size if anyone would like to look at it.  The postscript file
> is 783KB in size.
> 
> I was able to isolate it to a single plot in a PDF file and it had 38,000
> lines of the following that composed 99% of the file:  (I was plotting out
> individual events, which were about that many)
> 
> 86.66 88.67 m
> 86.66 92.81 l
> 86.66 96.95 l
> 86.66 101.09 l
> 86.66 105.23 l
> 86.66 109.37 l
> 86.66 113.51 l
> :::::::::::::  38,000 more of the same
> 388.87 96.95 l
> 388.87 92.81 l
> 388.97 88.67 l
> 389.07 84.53 l
> S
> Q q
> 0.000 0.000 0.000 RG
> 0.75 w
> [] 0 d
> 
> Is this breaking some limit in PDF?
> 
> I am running:
> 
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
> __________________________________________________________
> James Holtman               "What is the problem you are trying to solve?"
> Executive Consultant  --  Office of Technology, Convergys
> james.holtman at convergys.com
> (513) 723-2929
> 
> --
> "NOTICE:  The information contained in this electronic mail tran... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Jun 19 21:46:25 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jun 2003 21:46:25 +0200
Subject: [R] Background color(s) for groupedData plot
References: <940250A9EB37A24CBE28D858EF07774967A937@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3EF21311.6FD80C4D@statistik.uni-dortmund.de>



"Paul, David A" wrote:
> 
> Thank you both for pointing out that this is a lattice
> plot (ie, R's version of Trellis graphics) and therefore
> needs something other than par().
> 
> I was able to use IE6.0 to search for "trellis" and
> find the relevant commands (after using help.start(),
> of course).  This brings up another question:
> 
> Is there a convenient way to decide whether or not
> the generic plot( ) is going to use "regular" or
> "trellis" plotting?  I looked at methods(plot) and
> didn't find any groupedData plot methods listed, so
> perhaps this is the clue?
> 
> -david paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


When my first guess is right (you are using nlme),
you might want to take a look into the help pages for those plots, e.g.:
  ?plot.nfnGroupedData 
tells you it is using lattice, and you can look into these functions to
find it out:
 getS3method("plot", "nfnGroupedData")

Uwe Ligges



From trainor at transborder.org  Thu Jun 19 21:51:06 2003
From: trainor at transborder.org (Douglas Trainor)
Date: Thu, 19 Jun 2003 14:51:06 -0500
Subject: R: [R] Grouping binary data
In-Reply-To: <015001c33666$886fd3a0$5c13070a@GIUSTIZIA.IT>
References: <5.2.1.1.0.20030619131554.03715238@pop3.norton.antivirus>
	<015001c33666$886fd3a0$5c13070a@GIUSTIZIA.IT>
Message-ID: <3EF2142A.5020109@transborder.org>


I too am interested in analysis of sparse data, and I couldn't find this 
journal
easily, but I found an Oliver Kuss presentation that likely summarizes the
material.  You can find that presentation here:

http://www.stats.gla.ac.uk/~goeran/euroworkshop/webpages/2002/slides/oliver.pdf

(Also, apparently there is a SAS/IML macro that was discussed at SUGI 26.)

    douglas



Vito Muggeo wrote:

>Dear Henric,
>The following paper deals with goodness-of-fit test for sparse (and even
>binary) data:
>
>Kuss O. Global goodness-of-fit tests in logistic regression with sparse
>data, Statist Med, 2002, 21:3789-3801.
>
>It should not too hard to write code for some non-standard and (probably
>under-used) GoF statistics...
>[...]
>



From lockwood at rand.org  Thu Jun 19 21:58:38 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Thu, 19 Jun 2003 15:58:38 -0400 (EDT)
Subject: [R] Fitting particular repeated measures model with lme()
Message-ID: <Pine.LNX.4.33.0306191555340.5194-100000@penguin.rand.org>

Hello,

I have a simulated data structure in which students are nested within
teachers, and with each student are associated two test scores.  There
are 20 classrooms and 25 students per classroom, for a total of 500
students and two scores per student.  Here are the first 10 lines of
my dataframe "d":

   studid tchid          Y time
1       1     1 -1.0833222    0
2       1     1 -0.7656281    1
3       2     1 -1.0198641    0
4       2     1  0.7808148    1
5       3     1 -1.1381721    0
6       3     1 -0.4395021    1
7       4     1 -2.0944685    0
8       4     1 -1.8746840    1
9       5     1 -0.7784412    0
10      5     1  1.9952170    1
...

I am trying to use lme() to fit a relatively basic repeated measures
model where there are random teacher intercepts, and an unstructured
residual covariance matrix within students.  The following call to
lme() seems to fit the model:

lme.t5<-lme(fixed=Y~time,data=d,random=~1|tchid,weights=varIdent(form=~1|time),\
correlation=corSymm(form = ~1|tchid/studid))

Now, I would like to try to alter this model to one in which the
"teacher effect" applies to only one year.  One can think of the first
score on the student as a score from a prior year (for which I have no
teacher links), and the second score is from the current year and is
linked to the teacher.  The model for student j in class i is:

Y_{ij0} = a_0 + e_{ij0}
Y_{ij1} = a_1 + b_i + e_{ij1}

with Var(b_i) the teacher variance component and Cov(e_{ij0},e_{ij1})
unstructured.  That is, if the data are organized by student, the "Z"
matrix in the usual linear mixed model notation has every other row
equal to a row of zeros.

I am wondering whether there is some way to fit this model using
lme().  Thanks in advance for your help and patience.

best regards,

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From mentus at gmx.de  Thu Jun 19 22:37:03 2003
From: mentus at gmx.de (Fernando Henrique Ferraz Pereira da Rosa)
Date: Thu, 19 Jun 2003 22:37:03 +0200 (MEST)
Subject: [R] Subseting by more than one factor...
Message-ID: <19793.1056055023@www11.gmx.net>

  Is it possible in R to subset a dataframe by more than one factor, all at
once?
     For instance, I have the dataframe: 
 >data 
   p1 p2 p3 p4 p5 p6 p7 p8 p9 p10      pred
  1    0  1  0  0  0  0  0  0  0   0 0.5862069
  4    0  0  0  0  0  0  0  0  0   1 0.5862069
  5    0  0  0  0  0  0  1  0  0   0 0.5862069
  6    0  0  0  0  0  0  0  1  0   0 0.5862069
  7    0  0  1  0  0  0  0  0  0   0 0.5862069
  9    0  0  0  0  1  0  0  0  0   0 0.5862069
  20   0  1  1  0  0  0  0  0  0   0 0.5862069
  22   0  1  0  0  1  0  0  0  0   0 0.5862069
  24   0  1  0  0  0  0  1  0  0   0 0.5862069
  25   0  1  0  0  0  0  0  1  0   0 0.5862069
  27   0  1  0  0  0  0  0  0  0   1 0.5862069

  If I want to subset only those points that have p4 = 1, I do:
   > subset(data,p4 == 1)
  And that's fine. Now suppose I want to subset those that not only have p4
= 1, but also p6 = 1.
   I tried subset(data,p4 == 1 && p6 == 1) or subset(data,p4==1 & p6==1).
But it didn't work.
   Then I found a clumsy way to do it :
    subset(subset(data,p4==1),p6==1)
    Which works. But it soon gets very clumsy as the number of conditions
increase (I end up with a really large number of nested subsets). Is there a
simpler way to do that?


--



From mmiller3 at iupui.edu  Thu Jun 19 22:38:05 2003
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Thu, 19 Jun 2003 15:38:05 -0500
Subject: [R] Background color(s) for groupedData plot
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A937@ws-bco-mse3.milky-way.battelle.org>
	(David A. Paul's message of "Thu, 19 Jun 2003 14:48:53 -0400")
References: <940250A9EB37A24CBE28D858EF07774967A937@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <87vfv1olaq.fsf@lumen.indyrad.iupui.edu>

>>>>> "Paul," == Paul, David A <paulda at battelle.org> writes:

    > Is there a convenient way to decide whether or not the
    > generic plot( ) is going to use "regular" or "trellis"
    > plotting?  I looked at methods(plot) and didn't find any
    > groupedData plot methods listed, so perhaps this is the
    > clue?

It's there, but as the GroupedData methods, not groupedData.

> require(nlme)
> methods(plot)[grep('groupedData',methods(plot),ignore.case=T)]
[1] "plot.nffGroupedData" "plot.nfnGroupedData" "plot.nmGroupedData" 

Mike



From sundar.dorai-raj at pdf.com  Thu Jun 19 23:05:08 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 19 Jun 2003 16:05:08 -0500
Subject: [R] Subseting by more than one factor...
References: <19793.1056055023@www11.gmx.net>
Message-ID: <3EF22584.7070109@pdf.com>



Fernando Henrique Ferraz Pereira da Rosa wrote:
>   Is it possible in R to subset a dataframe by more than one factor, all at
> once?
>      For instance, I have the dataframe: 
>  >data 
>    p1 p2 p3 p4 p5 p6 p7 p8 p9 p10      pred
>   1    0  1  0  0  0  0  0  0  0   0 0.5862069
>   4    0  0  0  0  0  0  0  0  0   1 0.5862069
>   5    0  0  0  0  0  0  1  0  0   0 0.5862069
>   6    0  0  0  0  0  0  0  1  0   0 0.5862069
>   7    0  0  1  0  0  0  0  0  0   0 0.5862069
>   9    0  0  0  0  1  0  0  0  0   0 0.5862069
>   20   0  1  1  0  0  0  0  0  0   0 0.5862069
>   22   0  1  0  0  1  0  0  0  0   0 0.5862069
>   24   0  1  0  0  0  0  1  0  0   0 0.5862069
>   25   0  1  0  0  0  0  0  1  0   0 0.5862069
>   27   0  1  0  0  0  0  0  0  0   1 0.5862069
> 
>   If I want to subset only those points that have p4 = 1, I do:
>    > subset(data,p4 == 1)
>   And that's fine. Now suppose I want to subset those that not only have p4
> = 1, but also p6 = 1.
>    I tried subset(data,p4 == 1 && p6 == 1) or subset(data,p4==1 & p6==1).
> But it didn't work.

It didn't? It does for me:

R> subset(z, p4 == 1 & p6 == 1)
  [1] p1   p2   p3   p4   p5   p6   p7   p8   p9   p10  pred
<0 rows> (or 0-length row.names)
R> subset(z, p2 == 1 & p8 == 1)
    p1 p2 p3 p4 p5 p6 p7 p8 p9 p10      pred
10  0  1  0  0  0  0  0  1  0   0 0.5862069
R> subset(z, (p2 == 1 & p3 == 0) | p5 == 1)
    p1 p2 p3 p4 p5 p6 p7 p8 p9 p10      pred
1   0  1  0  0  0  0  0  0  0   0 0.5862069
6   0  0  0  0  1  0  0  0  0   0 0.5862069
8   0  1  0  0  1  0  0  0  0   0 0.5862069
9   0  1  0  0  0  0  1  0  0   0 0.5862069
10  0  1  0  0  0  0  0  1  0   0 0.5862069
11  0  1  0  0  0  0  0  0  0   1 0.5862069
R> version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.0
year     2003
month    04
day      16
language R
R>


[snip]

Regards,
Sundar



From MZodet at ahrq.gov  Thu Jun 19 23:18:44 2003
From: MZodet at ahrq.gov (MZodet@ahrq.gov)
Date: Thu, 19 Jun 2003 17:18:44 -0400
Subject: [R] Statistical Models:  Ordered vs. Non-Ordered Factors
Message-ID: <3598558AD728D41183350008C7CF291C0A5CD224@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030619/9316793b/attachment.pl

From mpiorecky at hotmail.com  Fri Jun 20 00:04:20 2003
From: mpiorecky at hotmail.com (Mark Piorecky)
Date: Thu, 19 Jun 2003 16:04:20 -0600
Subject: [R] Building R Packages
References: <BAY8-DAV61jpz6PMfWI00024b00@hotmail.com>
	<1055904254.6231.164.camel@localhost>
Message-ID: <BAY8-DAV49pyqx1xnfJ00027fcf@hotmail.com>

Thanx to all who responded to my plea for help.  You were right, I did not
have the Path environment set properly for the tools package.  I get  a
little further now, but still get an error message.

My Cmd Window looks like this:
    Rcmd SHLIB combo.f
    ar cr combo.a *.o
    ranlib combo.a
    gcc --shared -s -o combo.dll combo.def
omo.a  -LC:/PROGRA~1/R/rw1070/src/gnuwin32  -lg2c -lR
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized
    Warning: .drectve '%.*s' unrecognized

I've read through the readme.packages text and have applied everything that
I think applies to my situation, except for that R is installed in "Program
Files".  I will move it to a dir with ni spaces next.

Any other suggestions?

Mark

----- Original Message ----- 
From: "Marc Schwartz" <MSchwartz at MedAnalytics.com>
To: "Mark Piorecky" <mpiorecky at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 17, 2003 8:44 PM
Subject: Re: [R] Missing 'make' command to compile Windows DLL - was
(nosubject)


> On Tue, 2003-06-17 at 21:19, Mark Piorecky wrote:
> > Hi all,
> >
> > I'm new to R and am trying to create a dll in order to be able to use
> > the "dyn.load" function with a fortran script.  I have a windows OS,
> > have installed perl and the Path is recognized.  I have also installed
> > R 1.70 including the src (source code) folder..  But when I attempt to
> > create the dll by excecuting "Rcmd SHLIB [-o autologdll] combo.f", in
> > the windows command prompt, I get the following error message "'make'
> > is not recognized as an internal or external command"
> >
> > Does anyone have some suggestions on what I may have missed?
> >
> > Thanx a ton!
> >
> > Mark
>
>
> Sounds like you did not download and install Prof. Ripley's Tools
> package or if you did, the PATH environment variable is not set
> properly.
>
> If you did not download the Tools package, go here:
>
> http://www.stats.ox.ac.uk/pub/Rtools/
>
> and download the Unix tools ZIP file in the Essentials section and read
> the remainder of the page, which provides further guidance.
>
> If you did download and extract that package, be sure to edit the PATH
> to reflect the location of those files.
>
> BTW, don't forget to update to R 1.7.1, which is now released and
> available on CRAN.
>
> HTH,
>
> Marc Schwartz
>
>
>
>
>



From bates at stat.wisc.edu  Fri Jun 20 00:54:21 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 19 Jun 2003 22:54:21 -0000
Subject: [R] Subseting by more than one factor...
In-Reply-To: <19793.1056055023@www11.gmx.net>
References: <19793.1056055023@www11.gmx.net>
Message-ID: <6rd6h965m0.fsf@bates4.stat.wisc.edu>

Fernando Henrique Ferraz Pereira da Rosa <mentus at gmx.de> writes:

>   Is it possible in R to subset a dataframe by more than one factor, all at
> once?
>      For instance, I have the dataframe: 
>  >data 
>       p1 p2 p3 p4 p5 p6 p7 p8 p9 p10      pred
>   1    0  1  0  0  0  0  0  0  0   0 0.5862069
>   4    0  0  0  0  0  0  0  0  0   1 0.5862069
>   5    0  0  0  0  0  0  1  0  0   0 0.5862069
>   6    0  0  0  0  0  0  0  1  0   0 0.5862069
>   7    0  0  1  0  0  0  0  0  0   0 0.5862069
>   9    0  0  0  0  1  0  0  0  0   0 0.5862069
>   20   0  1  1  0  0  0  0  0  0   0 0.5862069
>   22   0  1  0  0  1  0  0  0  0   0 0.5862069
>   24   0  1  0  0  0  0  1  0  0   0 0.5862069
>   25   0  1  0  0  0  0  0  1  0   0 0.5862069
>   27   0  1  0  0  0  0  0  0  0   1 0.5862069
> 
>   If I want to subset only those points that have p4 = 1, I do:
>    > subset(data,p4 == 1)
>   And that's fine. Now suppose I want to subset those that not only have p4
> = 1, but also p6 = 1.
>    I tried subset(data,p4 == 1 && p6 == 1) or subset(data,p4==1 & p6==1).


As Sundar pointed out it is the second form that you want.  When
intersecting conditions in subset() use &, not &&.

The way that you pasted the output in your message the column names
did not align with the columns.  I changed this in the part that I
quoted above.  This shows that you chose the wrong example, I think,
because that intersection is empty.  Try 

 subset(data, p2 == 1 & p3 == 1)

instead.



From perkinsm at bway.net  Fri Jun 20 04:12:27 2003
From: perkinsm at bway.net (Mark E. Perkins)
Date: Thu, 19 Jun 2003 22:12:27 -0400
Subject: [R] build R.1.7.1 failure on Darwin 6.6
Message-ID: <2147483647.1056060747@[192.168.187.8]>

When I attempt to build v.1.7.1 on a Darwin 6.6 system I get the following
failure when linking in src/main:

gcc -framework Carbon -mdynamic-no-pic -L/sw/lib -L/usr/local/lib -o R.bin
CConverters.o Rdynload.o RNG.o apply.o arithmetic.o apse.o array.o attrib.o
base.o bind.o builtin.o character.o coerce.o colors.o complex.o
connections.o context.o cov.o cum.o dcf.o datetime.o debug.o devPS.o
devPicTeX.o deparse.o deriv.o devices.o dotcode.o dounzip.o dstruct.o
duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o gram.o
gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
logic.o main.o mapply.o match.o memory.o model.o names.o objects.o optim.o
optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o
plotmath.o print.o printarray.o printvector.o printutils.o qsort.o random.o
regex.o relop.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o
split.o sprintf.o subassign.o subscript.o subset.o summary.o unique.o
util.o version.o vfonts.o registration.o xxxpr.o ../unix/libunix.a
../appl/libappl.a ../nmath/libnmath.a  -framework vecLib -L/sw/lib
-L/usr/local/lib -L/sw/lib/gcc-lib/powerpc-apple-darwin6.6/3.3
-L/sw/lib/gcc-lib/powerpc-apple-darwin6.6/3.3/../../.. -lfrtbegin -lg2c
-lSystem  ../extra/zlib/libz.a   -lpcre -lbz2 -lreadline -ldl -lncurses -lm
ld: warning multiple definitions of symbol _xerbla_
print.o definition of _xerbla_ in section (__TEXT,__text)
/System/Library/Frameworks/vecLib.framework/vecLib(ProjectBuilderMasterObje
ctFile.o) definition of _xerbla_
ld: warning multiple definitions of symbol _BC
/sw/lib/libreadline.dylib(terminal.so) definition of _BC
/sw/lib/libncurses.dylib(lib_termcap.lo) definition of _BC
ld: warning multiple definitions of symbol _UP
/sw/lib/libreadline.dylib(terminal.so) definition of _UP
/sw/lib/libncurses.dylib(lib_termcap.lo) definition of _UP
ld: warning multiple definitions of symbol _PC
/sw/lib/libreadline.dylib(terminal.so) definition of _PC
/sw/lib/libncurses.dylib(lib_tputs.lo) definition of _PC
ld: Undefined symbols:
restFP
saveFP
make[3]: *** [R.bin] Error 1
make[2]: *** [R] Error 2
make[1]: *** [R] Error 1
make: *** [R] Error 1

Apparently I am missing a library or object file, but I cannot spot the
root of the problem. Has anyone else seen/solved this problem? Steps
leading up to this were:

./configure --with-blas="-framework vecLib" --with-lapack
--with-recommended-packages
make

configure completed with no complaints or warnings. A straight ./configure
yields the same error (no surprise).

This is a Darwin 6.6 system with up-to-date fink and Developer Tools.
Any clue on the missing component(s) would be appreciated.

Thanks,
Mark



From sean.connolly at jcu.edu.au  Fri Jun 20 05:30:39 2003
From: sean.connolly at jcu.edu.au (Sean Connolly)
Date: Fri, 20 Jun 2003 13:30:39 +1000
Subject: [R] Question: nonlinear covariate terms in spatial regression
Message-ID: <5.1.0.14.0.20030620122311.00ba0088@mail.jcu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030620/ce19c4e1/attachment.pl

From faheem at email.unc.edu  Fri Jun 20 08:47:48 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Fri, 20 Jun 2003 02:47:48 -0400 (EDT)
Subject: [R] combining mathematical notation and value substitution
Message-ID: <Pine.LNX.4.44.0306200240580.25336-100000@Chrestomanci>


Dear People,

I need to make a label which both contains math notation as well as
substitutes a value for an object. In the following label, len and theta
are one dim variables, and I am substituting their values appropriately.
This label looks fine except that I want the greek symbol for theta to
appear instead of the word `theta'. How can I do so most easily? I don't
understand the underlying mechanisms well enough to work it out for
myself. Thanks in advance.

Please cc me. I'm not subscribed to the list.

                                                           Faheem.

main=paste("Monotonic Multigamma run (n=",
deparse(substitute(len)),", ",
expression(theta),"=", deparse(substitute(theta)),").")



From faheem at email.unc.edu  Fri Jun 20 09:10:18 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Fri, 20 Jun 2003 03:10:18 -0400 (EDT)
Subject: [R] Re: combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306200240580.25336-100000@Chrestomanci>
Message-ID: <Pine.LNX.4.44.0306200306300.25336-100000@Chrestomanci>



On Fri, 20 Jun 2003, Faheem Mitha wrote:

>
> Dear People,
>
> I need to make a label which both contains math notation as well as
> substitutes a value for an object. In the following label, len and theta
> are one dim variables, and I am substituting their values appropriately.
> This label looks fine except that I want the greek symbol for theta to
> appear instead of the word `theta'. How can I do so most easily?
[snip]

> main=paste("Monotonic Multigamma run (n=",
> deparse(substitute(len)),", ",
> expression(theta),"=", deparse(substitute(theta)),").")

Hmm. Tried this, didn't work either. Inspired by pg 32 of "R for
Beginners".

 main=paste("Monotonic Multigamma run",
           as.expression(substitute(n==length,list(length=len))),
           as.expression(substitute(theta==th,list(th=theta))))

                                            Faheem.



From faheem at email.unc.edu  Fri Jun 20 09:16:39 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Fri, 20 Jun 2003 03:16:39 -0400 (EDT)
Subject: [R] Re: combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306200306300.25336-100000@Chrestomanci>
Message-ID: <Pine.LNX.4.44.0306200315410.25336-100000@Chrestomanci>



On Fri, 20 Jun 2003, Faheem Mitha wrote:

> Hmm. Tried this, didn't work either. Inspired by pg 32 of "R for
> Beginners".
>
>  main=paste("Monotonic Multigamma run",
>            as.expression(substitute(n==length,list(length=len))),
>            as.expression(substitute(theta==th,list(th=theta))))

Sorry, stupid of me. I guess that paste converts things back to character
strings, so that won't work.

                                                  Faheem.



From ligges at statistik.uni-dortmund.de  Fri Jun 20 09:25:36 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Jun 2003 09:25:36 +0200
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306200240580.25336-100000@Chrestomanci>
References: <Pine.LNX.4.44.0306200240580.25336-100000@Chrestomanci>
Message-ID: <3EF2B6F0.9030503@statistik.uni-dortmund.de>

Faheem Mitha wrote:
> Dear People,
> 
> I need to make a label which both contains math notation as well as
> substitutes a value for an object. In the following label, len and theta
> are one dim variables, and I am substituting their values appropriately.
> This label looks fine except that I want the greek symbol for theta to
> appear instead of the word `theta'. How can I do so most easily? I don't
> understand the underlying mechanisms well enough to work it out for
> myself. Thanks in advance.
> 
> Please cc me. I'm not subscribed to the list.
> 
>                                                            Faheem.
> 
> main=paste("Monotonic Multigamma run (n=",
> deparse(substitute(len)),", ",
> expression(theta),"=", deparse(substitute(theta)),").")

No, it won't work that way, because you have to specify an S expression 
in order to get mathematical annotation. An expression within paste() 
will be converted to a character string.

What you ca do is the other way round:


t1 <- theta     # you cannot use theta as variable and math. symbol
plot(1:10, main =
   substitute("Monotonic Multigamma run (" * n  == len * ", " *
     theta == t1 * ").", list(len = len, t1 = t1)))


See also ?plotmath or that small article in R News:
Ligges (2002): R Help Desk: Automation of Mathematical Annotation in 
Plots. R News 2(3), 32-34.

Uwe Ligges



From gb at stat.umu.se  Fri Jun 20 15:53:49 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri, 20 Jun 2003 15:53:49 +0200 (CEST)
Subject: [R] namespaces not available
Message-ID: <Pine.LNX.4.44.0306201550001.1135-100000@tal.stat.umu.se>

This happened to me with R-1.7.0, Linux RH9:

------------------------------------------------------------------
> q()
Save workspace image? [y/n/c]: y
Warning messages: 
1: namespaces may not be available when loading 
2: names in persistent strings are currently ignored 


gb at lasker:~/R/test$ R

R : Copyright 2003, The R Development Core Team
Version 1.7.0  (2003-04-16)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

Error in get(x, envir, mode, inherits) : variable "biplot" was not found
Fatal error: unable to restore saved data in .RData
------------------------------------------------------------
(what is "biplot"??)

But

gb at lasker:~/R/test$  R --vanilla

> load(".RData")
 [1] ".Random.seed" ".Traceback"   "f.PQL"        "f.ml"         "fit"         
 [6] "hd"           "last.warning" "run1"         "s.dat"        "t2"          
> 
------------------------------------------------------------

Seems to work. What is happening?

G?ran
---
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From ripley at stats.ox.ac.uk  Fri Jun 20 14:59:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jun 2003 13:59:52 +0100 (BST)
Subject: [R] namespaces not available
In-Reply-To: <Pine.LNX.4.44.0306201550001.1135-100000@tal.stat.umu.se>
Message-ID: <Pine.LNX.4.44.0306201358120.23071-100000@gannet.stats>

I think you have saved in your workspace an object whose environment is
a namespace, probably mva, that is not available when .RData is loaded but 
is once the session is running.

On Fri, 20 Jun 2003, G?ran Brostr?m wrote:

> This happened to me with R-1.7.0, Linux RH9:
> 
> ------------------------------------------------------------------
> > q()
> Save workspace image? [y/n/c]: y
> Warning messages: 
> 1: namespaces may not be available when loading 
> 2: names in persistent strings are currently ignored 
> 
> 
> gb at lasker:~/R/test$ R
> 
> R : Copyright 2003, The R Development Core Team
> Version 1.7.0  (2003-04-16)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type `license()' or `licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type `contributors()' for more information.
> 
> Type `demo()' for some demos, `help()' for on-line help, or
> `help.start()' for a HTML browser interface to help.
> Type `q()' to quit R.
> 
> Error in get(x, envir, mode, inherits) : variable "biplot" was not found
> Fatal error: unable to restore saved data in .RData
> ------------------------------------------------------------
> (what is "biplot"??)
> 
> But
> 
> gb at lasker:~/R/test$  R --vanilla
> 
> > load(".RData")
>  [1] ".Random.seed" ".Traceback"   "f.PQL"        "f.ml"         "fit"         
>  [6] "hd"           "last.warning" "run1"         "s.dat"        "t2"          
> > 
> ------------------------------------------------------------
> 
> Seems to work. What is happening?
> 
> G?ran
> ---
>  G?ran Brostr?m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mrobin at eircom.net  Fri Jun 20 15:21:08 2003
From: mrobin at eircom.net (Martin H. Robinson)
Date: Fri, 20 Jun 2003 13:21:08 -0000
Subject: [R] Power Law Exponents
Message-ID: <1056115215.4367.42.camel@localhost>

I am having difficulty with the calculation of the power law exponent
for set of nodes within a graph. 

Specifically, I am interested in the distribution of in-degree and
out-degree among communities of web pages where the web pages are the
nodes of the graph and the hyperlinks the edges. 
According to the literature, the distribution of incoming and outgoing
links obeys a power law distribution and exponents have been reported
for out-degree of 2.45 and 2.1 for in-degree. Apparently, this holds
across large samples of the Web and also for sub-graphs and over
different magnitudes of web graph.

I have been working with sub-graphs of the Web that I know are related
to each other and I have data for the in-degree and out-degree of the
sub-graphs. I want to calculate the power law exponent for the in-degree
and the out-degree of the sub-graphs.

Assuming lin is a vector containing the in-degree of each page within
the collection (the collection is between 4000 and 5000 resources), I
have tried firstly:

> barplot(rev(sort(table(lin))))

which gives me a Pareto chart which I think is related to the power law
distribution. 

However, I specifically want a log-log chart of the in-degree against
frequency with a fitted line whose slope is the exponent. 

Thus, I devised the following:

> linf <- as.data.frame.table((table(lin)),mode="numeric")
> linx = as.vector(linf$lin, mode="numeric")
> liny = as.vector(linf$Freq, mode="numeric")
> plot(linx, liny, pch=3, log="xy",main="Log-log", xlab="Indegree",
ylab="Frequency - Indegree")
> abline(lm (log10(vouty) ~ log10(voutx)))

This gives me the semblance of the plot I want but the exponent is wrong
and I have severe doubts about its correctness.

As I am neither a mathematician or statistician, I would be very
grateful for comments or guidance or correction.
Best Regards
Martin

-- 
Martin H. Robinson <mrobin at eircom.net>



From luke at stat.uiowa.edu  Fri Jun 20 15:53:56 2003
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 20 Jun 2003 08:53:56 -0500 (CDT)
Subject: [R] namespaces not available
In-Reply-To: <Pine.LNX.4.44.0306201550001.1135-100000@tal.stat.umu.se>
Message-ID: <Pine.LNX.4.44.0306200849020.20996-100000@itasca2.stat.uiowa.edu>

On Fri, 20 Jun 2003, G?ran Brostr?m wrote:

> This happened to me with R-1.7.0, Linux RH9:
> 
> ------------------------------------------------------------------
> > q()
> Save workspace image? [y/n/c]: y
> Warning messages: 
> 1: namespaces may not be available when loading 
> 2: names in persistent strings are currently ignored 

This happens when one of the variables in your workspace contains a
reference to a name space, for eample as the environment of a
function.  The first warning is there because the workspace cannot be
loaded into older R versions.  THe second warning should probably be
removed.

> 
> gb at lasker:~/R/test$ R
> 
> R : Copyright 2003, The R Development Core Team
> Version 1.7.0  (2003-04-16)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type `license()' or `licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type `contributors()' for more information.
> 
> Type `demo()' for some demos, `help()' for on-line help, or
> `help.start()' for a HTML browser interface to help.
> Type `q()' to quit R.
> 
> Error in get(x, envir, mode, inherits) : variable "biplot" was not found
> Fatal error: unable to restore saved data in .RData
> ------------------------------------------------------------
> (what is "biplot"??)
> 

This I don't understand.  Loading a workspace with a reference to mva
or MASS (which imports biplot from mva) works fine for me, so there is
something more complicated going on.  Does a traceback() tell you
anything?  Otherwise, can you send me your .RData file and I will try
to track this down.

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From Ted.Harding at nessie.mcc.ac.uk  Fri Jun 20 15:22:02 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 Jun 2003 14:22:02 +0100 (BST)
Subject: [R] Spedd: R vs S-plus
Message-ID: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

Sorry to raise what has probably been discussed before,
but I an repeatedly struck by the comparative slowness
of S-plus for Windows compared with R for Linux when doing
much the same thing.

I don't have a direct comparison, because they're not
running on the same machine; but machine W has a faster
CPU and more RAM than machine L, yet S-plus on W seems
to take longer by quite a big factor (of the order of 5x)
than R on L.

My instincts say that "WIndows" is probably a significant
factor in the comnparison, but still ...

Ideally, to compare R with S-plus, one should look at them
both on the same OS (Unix or Windows) on the same machine.

Can anyone give me clean comparative speeds?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Jun-03                                       Time: 14:22:02
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Fri Jun 20 16:25:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Jun 2003 16:25:46 +0200
Subject: [R] Spedd: R vs S-plus
In-Reply-To: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3EF3196A.9040002@statistik.uni-dortmund.de>

(Ted Harding) wrote:
> Hi Folks,
> 
> Sorry to raise what has probably been discussed before,
> but I an repeatedly struck by the comparative slowness
> of S-plus for Windows compared with R for Linux when doing
> much the same thing.
> 
> I don't have a direct comparison, because they're not
> running on the same machine; but machine W has a faster
> CPU and more RAM than machine L, yet S-plus on W seems
> to take longer by quite a big factor (of the order of 5x)
> than R on L.
> 
> My instincts say that "WIndows" is probably a significant
> factor in the comnparison, but still ...
> 
> Ideally, to compare R with S-plus, one should look at them
> both on the same OS (Unix or Windows) on the same machine.
> 
> Can anyone give me clean comparative speeds?

It heavily depends on what you are doing, and on the versions of R and 
S-PLUS.

Given you do not link R against very specialized libraries such as ATLAS 
on one and not the other OS, I found no dramatic differences between R 
on Linux and Windows, but that might depend on the application as well.

Uwe Ligges



From spencer.graves at pdf.com  Fri Jun 20 16:33:54 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Jun 2003 07:33:54 -0700
Subject: [R] Power Law Exponents
References: <1056115215.4367.42.camel@localhost>
Message-ID: <3EF31B52.6000807@pdf.com>

	  Have you looked at coef(lm (log10(vouty) ~ log10(voutx)))?  This the 
slope / coefficient of log10(voutx) should be an estimate of the power.

	  Why do yo think the exponent is wrong?  If the plot looks like a 
rough ellipse, then this is a standard way to fit this.

	  This assumes that log10(vouty)=b0+b1*log10(voutx)+e,
where the e's are indepndent, normally distributed with mean 0 and 
constant variance.  The reverse, log10(voutx)=c0+c1*log10(vouty)+f, is a 
different model:  The first assumes there is no error in voutx;  all the 
error is in vouty.  The second assumes the opposite.

	  If neither is what you want, then you need some kind of errors in X 
regression.  I need that so seldom that I'm not certain what softwarre 
to suggest, but I believe I've seen comments on this list indicating 
there is R software available for solving this problem.

hth, spencer graves

Martin H. Robinson wrote:
> I am having difficulty with the calculation of the power law exponent
> for set of nodes within a graph. 
> 
> Specifically, I am interested in the distribution of in-degree and
> out-degree among communities of web pages where the web pages are the
> nodes of the graph and the hyperlinks the edges. 
> According to the literature, the distribution of incoming and outgoing
> links obeys a power law distribution and exponents have been reported
> for out-degree of 2.45 and 2.1 for in-degree. Apparently, this holds
> across large samples of the Web and also for sub-graphs and over
> different magnitudes of web graph.
> 
> I have been working with sub-graphs of the Web that I know are related
> to each other and I have data for the in-degree and out-degree of the
> sub-graphs. I want to calculate the power law exponent for the in-degree
> and the out-degree of the sub-graphs.
> 
> Assuming lin is a vector containing the in-degree of each page within
> the collection (the collection is between 4000 and 5000 resources), I
> have tried firstly:
> 
> 
>>barplot(rev(sort(table(lin))))
> 
> 
> which gives me a Pareto chart which I think is related to the power law
> distribution. 
> 
> However, I specifically want a log-log chart of the in-degree against
> frequency with a fitted line whose slope is the exponent. 
> 
> Thus, I devised the following:
> 
> 
>>linf <- as.data.frame.table((table(lin)),mode="numeric")
>>linx = as.vector(linf$lin, mode="numeric")
>>liny = as.vector(linf$Freq, mode="numeric")
>>plot(linx, liny, pch=3, log="xy",main="Log-log", xlab="Indegree",
> 
> ylab="Frequency - Indegree")
> 
>>abline(lm (log10(vouty) ~ log10(voutx)))
> 
> 
> This gives me the semblance of the plot I want but the exponent is wrong
> and I have severe doubts about its correctness.
> 
> As I am neither a mathematician or statistician, I would be very
> grateful for comments or guidance or correction.
> Best Regards
> Martin
>



From phgrosjean at sciviews.org  Fri Jun 20 16:57:16 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 20 Jun 2003 16:57:16 +0200
Subject: [R] Spedd: R vs S-plus
In-Reply-To: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <MABBLJDICACNFOLGIHJOGEMEDIAA.phgrosjean@sciviews.org>

You will find a speed comparison of S-PLUS and R, but also Matlab, Octave,
Scilab, Ox and O-Matrix under Windows at
http://www.sciviews.org/other/benchmark.htm. It seems that, at least under
Windows your impressions are confirmed. Also, I measured that R is a little
bit faster under Linux Mandrake 9 on the same computer. So, comparing S-PLUS
under Windows with R under Linux could be even better in favor of R.
However, I do not draw conclusions about Linux version of S-PLUS because I
never tried it.
Best,

Philippe

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ted Harding
Sent: vendredi 20 juin 2003 3:22
To: r-help at stat.math.ethz.ch
Subject: [R] Spedd: R vs S-plus


Hi Folks,

Sorry to raise what has probably been discussed before,
but I an repeatedly struck by the comparative slowness
of S-plus for Windows compared with R for Linux when doing
much the same thing.

I don't have a direct comparison, because they're not
running on the same machine; but machine W has a faster
CPU and more RAM than machine L, yet S-plus on W seems
to take longer by quite a big factor (of the order of 5x)
than R on L.

My instincts say that "WIndows" is probably a significant
factor in the comnparison, but still ...

Ideally, to compare R with S-plus, one should look at them
both on the same OS (Unix or Windows) on the same machine.

Can anyone give me clean comparative speeds?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Jun-03                                       Time: 14:22:02
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lockwood at rand.org  Fri Jun 20 17:06:21 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Fri, 20 Jun 2003 11:06:21 -0400 (EDT)
Subject: [R] Fitting particular repeated measures model with lme()
Message-ID: <Pine.LNX.4.33.0306201104410.11307-100000@penguin.rand.org>

Hello,

I have a simulated data structure in which students are nested within
teachers, and with each student are associated two test scores.  There
are 20 classrooms and 25 students per classroom, for a total of 500
students and two scores per student.  Here are the first 10 lines of
my dataframe "d":

   studid tchid          Y time
1       1     1 -1.0833222    0
2       1     1 -0.7656281    1
3       2     1 -1.0198641    0
4       2     1  0.7808148    1
5       3     1 -1.1381721    0
6       3     1 -0.4395021    1
7       4     1 -2.0944685    0
8       4     1 -1.8746840    1
9       5     1 -0.7784412    0
10      5     1  1.9952170    1
...

I am trying to use lme() to fit a relatively basic repeated measures
model where there are random teacher intercepts, and an unstructured
residual covariance matrix within students.  The following call to
lme() seems to fit the model:

lme.t5<-lme(fixed=Y~time,data=d,random=~1|tchid,weights=varIdent(form=~1|time),\
correlation=corSymm(form = ~1|tchid/studid))

Now, I would like to try to alter this model to one in which the
"teacher effect" applies to only one year.  One can think of the first
score on the student as a score from a prior year (for which I have no
teacher links), and the second score is from the current year and is
linked to the teacher.  The model for student j in class i is:

Y_{ij0} = a_0 + e_{ij0}
Y_{ij1} = a_1 + b_i + e_{ij1}

with Var(b_i) the teacher variance component and Cov(e_{ij0},e_{ij1})
unstructured.  That is, if the data are organized by student, the "Z"
matrix in the usual linear mixed model notation has every other row
equal to a row of zeros.

I am wondering whether there is some way to fit this model using
lme().  Thanks in advance for your help and patience.

best regards,

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From rossini at blindglobe.net  Fri Jun 20 17:16:24 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 20 Jun 2003 08:16:24 -0700
Subject: [R] Spedd: R vs S-plus
In-Reply-To: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
	(Ted.Harding@nessie.mcc.ac.uk's
	message of "Fri, 20 Jun 2003 14:22:02 +0100 (BST)")
References: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <8765n0n5iv.fsf@jeeves.blindglobe.net>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> My instincts say that "WIndows" is probably a significant
> factor in the comnparison, but still ...
>
> Ideally, to compare R with S-plus, one should look at them
> both on the same OS (Unix or Windows) on the same machine.
>
> Can anyone give me clean comparative speeds?

It's not that simple!   Anecdotal evidence has constructed a few
scenarios where R will run, but S-PLUS won't, and a few where R will
barely run, but S-plus has very few problems (problem == slowness, in
this case).

It would be critical to be a subjective Bayesian about it, since
weighting of common applications, and perhaps more importantly,
quality of code for matching up with the difference, will have a large
impact.

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachments ... {{dropped}}



From ripley at stats.ox.ac.uk  Fri Jun 20 17:27:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jun 2003 16:27:19 +0100 (BST)
Subject: [R] Spedd: R vs S-plus
In-Reply-To: <3EF3196A.9040002@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0306201535330.23566-100000@gannet.stats>

On Fri, 20 Jun 2003, Uwe Ligges wrote:

> (Ted Harding) wrote:
> > Hi Folks,
> > 
> > Sorry to raise what has probably been discussed before,
> > but I an repeatedly struck by the comparative slowness
> > of S-plus for Windows compared with R for Linux when doing
> > much the same thing.
> > 
> > I don't have a direct comparison, because they're not
> > running on the same machine; but machine W has a faster
> > CPU and more RAM than machine L, yet S-plus on W seems
> > to take longer by quite a big factor (of the order of 5x)
> > than R on L.
> > 
> > My instincts say that "WIndows" is probably a significant
> > factor in the comnparison, but still ...

Prejudices?

> > Ideally, to compare R with S-plus, one should look at them
> > both on the same OS (Unix or Windows) on the same machine.
> > 
> > Can anyone give me clean comparative speeds?
> 
> It heavily depends on what you are doing, and on the versions of R and 
> S-PLUS.

Yes, especially versions of S-PLUS. There are big differences between
recent versions of S-PLUS, and a comparison of 2000 vs 6.1 depends heavily
on the task.

It also depends on the version of Windows, and (especially in S-PLUS) the
file system type (NTFS/VFAT and even the versions of each) and if it is 
local or remotely mounted.

> Given you do not link R against very specialized libraries such as ATLAS 
> on one and not the other OS, I found no dramatic differences between R 
> on Linux and Windows, but that might depend on the application as well.

I often compare on the same hardware, using Windows XP.  I'd say that on
average the Windows port is 10-20% slower (and we have some idea why), and
almost never 50% slower.

I don't think it is normal to see factors as large as 5 either way on real
tasks, provided there is a reasonable amount of RAM available.  (Both R
and S-PLUS under Windows run very slowly if there is a very small amount
of RAM.)  I used to keep extensive tables of the time taken for different
versions on the same hardware for all the MASS scripts, but these days
they run fast enough on all the systems I use.  Here's some numbers, RH8.0
on a dual Athlon 2600, R using ATLAS (single-processor)

	R 1.7.1	S+6.1
ch04	8.40	10.52
ch05	5.94	11.18
ch06	72.80	23.89
ch07	11.36	29.45
ch10	20.07	39.61
ch13	9.00	13.7

That's probably a fair comparison, as I have tried to make those tasks
work well on both systems *and* they are real tasks, not small artificial
`benchmarks'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kemasse at uark.edu  Fri Jun 20 17:31:27 2003
From: kemasse at uark.edu (kemasse)
Date: Fri, 20 Jun 2003 10:31:27 -0500
Subject: [R] RE: Application
Message-ID: <3EF3811C@webmail.uark.edu>

There are no attached files.

>===== Original Message From r-help at hypatia.math.ethz.ch =====
>See the attached file for details.



From gb at stat.umu.se  Fri Jun 20 18:56:44 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri, 20 Jun 2003 18:56:44 +0200 (CEST)
Subject: [R] namespaces not available
In-Reply-To: <Pine.LNX.4.44.0306201459390.23454-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0306201852310.1135-100000@tal.stat.umu.se>

On Fri, 20 Jun 2003, Prof Brian Ripley wrote:

> Might it help to note that the version of MASS which shipped with 1.7.0 
> didn't get the import of biplot from mva right?  I think this was what
> happened with that version of VR.

I upgraded to R-1.7.1 and now the old .RData file loads when I start R,
so I guess that settles it? (I used 'glmmPQL' in MASS in that workspace.)

Thanks,

G?ran

> On Fri, 20 Jun 2003, Luke Tierney wrote:
> 
> > On Fri, 20 Jun 2003, G?ran Brostr?m wrote:
> > 
> > > This happened to me with R-1.7.0, Linux RH9:
> > > 
> > > ------------------------------------------------------------------
> > > > q()
> > > Save workspace image? [y/n/c]: y
> > > Warning messages: 
> > > 1: namespaces may not be available when loading 
> > > 2: names in persistent strings are currently ignored 
> > 
> > This happens when one of the variables in your workspace contains a
> > reference to a name space, for eample as the environment of a
> > function.  The first warning is there because the workspace cannot be
> > loaded into older R versions.  THe second warning should probably be
> > removed.
> > 
> > > 
> > > gb at lasker:~/R/test$ R
> > > 
> > > R : Copyright 2003, The R Development Core Team
> > > Version 1.7.0  (2003-04-16)
> > > 
> > > R is free software and comes with ABSOLUTELY NO WARRANTY.
> > > You are welcome to redistribute it under certain conditions.
> > > Type `license()' or `licence()' for distribution details.
> > > 
> > > R is a collaborative project with many contributors.
> > > Type `contributors()' for more information.
> > > 
> > > Type `demo()' for some demos, `help()' for on-line help, or
> > > `help.start()' for a HTML browser interface to help.
> > > Type `q()' to quit R.
> > > 
> > > Error in get(x, envir, mode, inherits) : variable "biplot" was not found
> > > Fatal error: unable to restore saved data in .RData
> > > ------------------------------------------------------------
> > > (what is "biplot"??)
> > > 
> > 
> > This I don't understand.  Loading a workspace with a reference to mva
> > or MASS (which imports biplot from mva) works fine for me, so there is
> > something more complicated going on.  Does a traceback() tell you
> > anything?  Otherwise, can you send me your .RData file and I will try
> > to track this down.
> > 
> > luke
> > 
> > 
> 
> 

-- 
---
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From Ted.Harding at nessie.mcc.ac.uk  Fri Jun 20 17:50:19 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 Jun 2003 16:50:19 +0100 (BST)
Subject: [R] Spedd: R vs S-plus
In-Reply-To: <XFMail.030620142202.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030620165019.Ted.Harding@nessie.mcc.ac.uk>

On 20-Jun-03 Ted Harding wrote:
> Sorry to raise what has probably been discussed before,
> but I an repeatedly struck by the comparative slowness
> of S-plus for Windows compared with R for Linux when doing
> much the same thing.

Thanks to all who so promptly responded with comments and information,
and especially to Philippe Grosjean for giving the URL for that most
interesting set of benchmark comparisons.

I well take the point that comparative speeds will depend on what you
are doing -- indeed that is apparent from the benchmarks -- but there
is still evidence that overall one can expect distinctly greater speed
from R.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Jun-03                                       Time: 16:50:18
------------------------------ XFMail ------------------------------



From aataylor at ou.edu  Fri Jun 20 17:51:37 2003
From: aataylor at ou.edu (Andrew Taylor)
Date: Fri, 20 Jun 2003 11:51:37 -0400
Subject: [R] stepwise regression
Message-ID: <3EF32D89.6B6C0237@ou.edu>

Hi,

S-PLUS includes the function "stepwise" which can use a variety of
methods to conduct stepwise multiple linear regression on a set of
predictors.  Does a similar function exist in R?  I'm having difficulty
finding one.  If there is one it must be under a different name because
I get an error message when I try 'help(stepwise)' in R.

Thanks for your help,
Andy Taylor

From spencer.graves at pdf.com  Fri Jun 20 19:07:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Jun 2003 10:07:38 -0700
Subject: [R] stepwise regression
References: <3EF32D89.6B6C0237@ou.edu>
Message-ID: <3EF33F5A.1090709@pdf.com>

Have you considered stepAIC in library(MASS) or step?

hth.  spencer graves

Andrew Taylor wrote:
> Hi,
> 
> S-PLUS includes the function "stepwise" which can use a variety of
> methods to conduct stepwise multiple linear regression on a set of
> predictors.  Does a similar function exist in R?  I'm having difficulty
> finding one.  If there is one it must be under a different name because
> I get an error message when I try 'help(stepwise)' in R.
> 
> Thanks for your help,
> Andy Taylor
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at stat.ucla.edu  Fri Jun 20 19:07:52 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Fri, 20 Jun 2003 10:07:52 -0700
Subject: [R] stepwise regression
In-Reply-To: <3EF32D89.6B6C0237@ou.edu>
References: <3EF32D89.6B6C0237@ou.edu>
Message-ID: <3EF33F68.7030106@stat.ucla.edu>

Try,

help.search("stepwise")

It brings up the functions step() and stepAIC() from MASS.

Andrew Taylor wrote:
> Hi,
> 
> S-PLUS includes the function "stepwise" which can use a variety of
> methods to conduct stepwise multiple linear regression on a set of
> predictors.  Does a similar function exist in R?  I'm having difficulty
> finding one.  If there is one it must be under a different name because
> I get an error message when I try 'help(stepwise)' in R.
> 
> Thanks for your help,
> Andy Taylor
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From chrysopa at insecta.ufv.br  Fri Jun 20 19:26:36 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Fri, 20 Jun 2003 14:26:36 -0300
Subject: [R] glmm and overall goodness of fit
Message-ID: <200306201031.32759.chrysopa@insecta.ufv.br>

Hi,

exist in R any glmm function that have any tools for test for overall goodness 
of fit? 

Thanks
Ronaldo
-- 
O papel da impressora ? sempre mais forte na parte picotada.
--
|   // | \\   [***********************************]
|> ( ?   ? )  [Ronaldo Reis J?nior                ]
|      V      [UFV/DBA-Entomologia                ]
|>  /     \   [36571-000 Vi?osa - MG              ]
|  /(.''`.)\  [Fone: 31-3899-2532                 ]
|>/(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|>  ( `-  )   [***********************************]
|>> _/   \_Powered by GNU/Debian Woody/Sarge



From spencer.graves at pdf.com  Fri Jun 20 23:38:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Jun 2003 14:38:53 -0700
Subject: [R] stepwise regression
References: <3EF32D89.6B6C0237@ou.edu> <3EF33F5A.1090709@pdf.com>
Message-ID: <3EF37EED.8080006@pdf.com>

Consider the following:

 > library(MASS)
 > Dat <- data.frame(x1=rnorm(9), x2=rnorm(9), y1=rnorm(9), y2=rnorm(9))
 > mdl <- "~x1+x2+I(x1^2)+I(x2^2)+x1:x2"
 > y <- names(Dat)[3:4]
 > fits <- list()
 > for(i in 1:2){
+  fit0 <- lm(formula(paste(y[i], "~1")), Dat)
+  fits[[i]] <- stepAIC(fit0, mdl)
+ }
Start:  AIC= 0.49
  y1 ~ 1

           Df Sum of Sq    RSS    AIC
<none>                 7.6059 0.4854
+ I(x2^2)  1    0.6293 6.9766 1.7081
+ I(x1^2)  1    0.4443 7.1616 1.9436
+ x2       1    0.2894 7.3165 2.1362
+ x1       1    0.1996 7.4064 2.2460
Start:  AIC= 2.36
  y2 ~ 1

           Df Sum of Sq     RSS     AIC
+ I(x2^2)  1    6.3086  3.0593 -5.7113
<none>                  9.3679  2.3606
+ x2       1    1.1277  8.2402  3.2062
+ x1       1    0.5651  8.8028  3.8006
+ I(x1^2)  1    0.2631  9.1048  4.1042

Step:  AIC= -5.71
  y2 ~ I(x2^2)

           Df Sum of Sq     RSS     AIC
+ x2       1    1.1870  1.8723 -8.1306
+ I(x1^2)  1    0.9861  2.0732 -7.2132
+ x1       1    0.6241  2.4353 -5.7645
<none>                  3.0593 -5.7113
- I(x2^2)  1    6.3086  9.3679  2.3606

Step:  AIC= -8.13
  y2 ~ I(x2^2) + x2

           Df Sum of Sq     RSS     AIC
+ I(x1^2)  1    0.3976  1.4747 -8.2790
<none>                  1.8723 -8.1306
+ x1       1    0.0153  1.8569 -6.2046
- x2       1    1.1870  3.0593 -5.7113
- I(x2^2)  1    6.3679  8.2402  3.2062

Step:  AIC= -8.28
  y2 ~ I(x2^2) + x2 + I(x1^2)

           Df Sum of Sq     RSS     AIC
<none>                  1.4747 -8.2790
- I(x1^2)  1    0.3976  1.8723 -8.1306
- x2       1    0.5985  2.0732 -7.2132
+ x1       1    0.0009  1.4738 -6.2845
- I(x2^2)  1    6.7490  8.2236  5.1881
 > fits
[[1]]

Call:
lm(formula = y1 ~ 1, data = Dat)

Coefficients:
(Intercept)
      0.1444


[[2]]

Call:
lm(formula = y2 ~ I(x2^2) + x2 + I(x1^2), data = Dat)

Coefficients:
(Intercept)      I(x2^2)           x2      I(x1^2)
      1.1374      -0.6926       0.2962      -0.3413
#############
Does this answer the question?
Spencer Graves
###################################
Hi Spencer,

The step and stepAIC functions are certainly different than the
"stepwise" function in S-PLUS, but I think they will help.  I had found them
before emailing my question to the list but was hoping to find a function in
R exactly like "stepwise" in S-PLUS.  As far as I can tell right now it
doesn't exist.  I have on the order of 60 predictor variables that I would
like to run through a stepwise procedure.  Is there a shortcut available to
avoid typing the name of each of these 60 variables when specifying the 
model
formula?  All of the variables are in successive columns in the same matrix.
If there is no shortcut I suppose I'll just write a function to do the job.

Thanks again,
Andy Taylor

Spencer Graves wrote:
> Have you considered stepAIC in library(MASS) or step?
> 
> hth.  spencer graves
> 
> Andrew Taylor wrote:
> 
>> Hi,
>>
>> S-PLUS includes the function "stepwise" which can use a variety of
>> methods to conduct stepwise multiple linear regression on a set of
>> predictors.  Does a similar function exist in R?  I'm having difficulty
>> finding one.  If there is one it must be under a different name because
>> I get an error message when I try 'help(stepwise)' in R.
>>
>> Thanks for your help,
>> Andy Taylor
>>
>>
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From jonck at vanderkogel.net  Sat Jun 21 00:33:31 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Sat, 21 Jun 2003 00:33:31 +0200
Subject: [R] Silhouette question
Message-ID: <38844494-A36F-11D7-8FBB-0005026E2B43@vanderkogel.net>

Hi all,
I am momentarily experimenting with Silhouette from the cluster library 
but I am getting some errors. Since Silhouette can be seen as a quality 
measure for a clustering what I want to do is run a series of different 
clusterings and store the one with the highest Silhouette value. In 
that way I hope to get "the best" clustering possible for my dataset.
Here is the problem:
When running the examples that come with silhouette, everything works 
fine, the silhouette values are calculated perfectly. When I try to run 
silhouette with my own dataset I get errors at unpredictable times, 
that is, sometimes silhouette runs succesfully and at other times it 
gives me the following error:
 > test <- silhouette(cutree(agn, k=5), daisy(bestSom$codes))
Error in apply(dmatrix[!iC, iC], 2, function(r) tapply(r, x[!iC], 
mean)) :
         dim(X) must have a positive length

Since I am running my experiments in batch mode (put a loop of 
experiments in a source file and then load this source file), whenever 
this error occurs the entire experiment is cut off. The experiment 
takes rather a long time (approx. 12 hours), so I would not want to 
start my experiment at night only to find in the morning that my 
experiment never ran. Is there a way to
a) prevent the error from happening, or
b) detect beforehand that the error will happen and thus not do the 
silhouette calculation for that particular clustering

Any help with this is much appreciated,
thanks, Jonck



From faheem at email.unc.edu  Sat Jun 21 01:15:32 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Fri, 20 Jun 2003 19:15:32 -0400 (EDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <3EF2B6F0.9030503@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0306201830460.25336-100000@Chrestomanci>



On Fri, 20 Jun 2003, Uwe Ligges wrote:

> Faheem Mitha wrote:
[snip]
> > main=paste("Monotonic Multigamma run (n=",
> > deparse(substitute(len)),", ",
> > expression(theta),"=", deparse(substitute(theta)),").")
>
> No, it won't work that way, because you have to specify an S expression
> in order to get mathematical annotation. An expression within paste()
> will be converted to a character string.
>
> What you ca do is the other way round:
>
>
> t1 <- theta     # you cannot use theta as variable and math. symbol
> plot(1:10, main =
>    substitute("Monotonic Multigamma run (" * n  == len * ", " *
>      theta == t1 * ").", list(len = len, t1 = t1)))
>
>
> See also ?plotmath or that small article in R News:
> Ligges (2002): R Help Desk: Automation of Mathematical Annotation in
> Plots. R News 2(3), 32-34.

Thank you for your help. Your expression works correctly. I've been trying
to understand why. I read your article, and I had already looked
previously at ?plotmath. However, the syntax of the above is still not
clear to me.

"Substitute" takes an expression as argument and substitutes the values of
any variables given in the second argument, correct? However, I am not
sure what it understands by

"Monotonic Multigamma run (" * n  == len * ", " * theta == t1 *

Is this a valid expression? My understanding of an expression is that it
contains one more more statements. The R-lang manual says, not entirely
intelligibly

********************************************************************* "In
R one can have objects of type `"expression"'.  An _expression_ contains
one or more statements.  A statement is a syntactically correct collection
of tokens.  Expression objects are special language objects which they
contain parsed, but unevaluated R statements.  The main difference is that
an expression object can contain several such expressions.  Another more
subtle difference is that objects of type `"expression"' are only
evaluated when explicitly passed to `eval', whereas other language objects
may get evaluated in some unexpected cases.

An expression object behaves much like a list and its components should be
accessed in the same way as the components of a list."
*********************************************************************

Firstly, I'm having difficulty parsing the sentence "Expression objects
are special language objects which they contain parsed, but unevaluated R
statements." Should the word "which" be removed? Then, the next sentence
says "The main difference is that an expression object can contain several
such expressions." The main difference from what? Lastly, if "An
expression object behaves much like a list" what are the individual
components? Are they expressions, and if so, how are they separated from
each other? I apologise for my cluelessness.

In any case, an expression object contains several statements, or several
expressions (not quite clear on the distinction). So, what are the
statements here?

You have character strings adjoined by eg. `n == len' which I agree is a
sensible statement. I'm guessing that R thinks of each character string as
a separate statement. If I do "foo" on the R command line it echoes it
back to me, but "foo" "bar" gives an error.

Can you explain what the `*' are doing here? Perhaps separating
statements? I've looked for use of this in a similar context but was
unable to find anything. Thanks again.

BTW, does "Introductory Statistics with R" by Peter Dalgaard, contain
disscussion of language issues, including "computing on the language"
stuff?

                                                           Faheem.



From rnelson at cariboo.bc.ca  Sat Jun 21 01:25:06 2003
From: rnelson at cariboo.bc.ca (Ross Nelson)
Date: Fri, 20 Jun 2003 16:25:06 -0700
Subject: [R] negative binomial regression
Message-ID: <6D6B3682-A376-11D7-B7D3-000393BA1D66@cariboo.bc.ca>

I have been using the negative binomial regression function in the MASS 
library to generate incidence rate ratios and confidence intervals.  
Can this function also generate a gradient value that would estimate 
the slope between the dependent and independent variables?  or can it 
be generated by passing the results to another function (chi-square?)

Ross



From chrysopa at insecta.ufv.br  Fri Jun 20 22:35:41 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Fri, 20 Jun 2003 17:35:41 -0300
Subject: [R] [OFF] stepwise using REML???
Message-ID: <200306201735.41239.chrysopa@insecta.ufv.br>

Hi,

I know that is not possible make a stepwise procedure using REML in R, I can 
use ML for this.

For nested design it may be very dangerous due the difference in variance 
structure, mainly in a splitplot design. ML make significative variables that 
REML dont make.

I read an article that is made a stepwise procedure using GENSTAT.

from article:
"Terms were dropped from a model in a stepwise procedure by assessing the 
change in deviance between the full model and the submodel."

All are made using REML.

It is possible?! I dont know GENSTAT.

Thanks
Ronaldo
-- 
aquadextrous, adj.:
	Possessing the ability to turn the bathtub faucet on and off
	with your toes.
		-- Rich Hall, "Sniglets"
--
|   // | \\   [***********************************]
|> ( ?   ? )  [Ronaldo Reis J?nior                ]
|      V      [UFV/DBA-Entomologia                ]
|>  /     \   [36571-000 Vi?osa - MG              ]
|  /(.''`.)\  [Fone: 31-3899-2532                 ]
|>/(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|>  ( `-  )   [***********************************]
|>> _/   \_Powered by GNU/Debian Woody/Sarge



From tlumley at u.washington.edu  Sat Jun 21 03:03:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Jun 2003 18:03:20 -0700 (PDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306201830460.25336-100000@Chrestomanci>
Message-ID: <Pine.A41.4.44.0306201734170.144744-100000@homer37.u.washington.edu>

On Fri, 20 Jun 2003, Faheem Mitha wrote:

>
>
> On Fri, 20 Jun 2003, Uwe Ligges wrote:
>
> >
> > t1 <- theta     # you cannot use theta as variable and math. symbol
> > plot(1:10, main =
> >    substitute("Monotonic Multigamma run (" * n  == len * ", " *
> >      theta == t1 * ").", list(len = len, t1 = t1)))
> >
>
> "Substitute" takes an expression as argument and substitutes the values of
> any variables given in the second argument, correct? However, I am not
> sure what it understands by
>
> "Monotonic Multigamma run (" * n  == len * ", " * theta == t1 *
>
> Is this a valid expression? My understanding of an expression is that it
> contains one more more statements.

That's only part  of the expression. This is the full expression
"Monotonic Multigamma run (" * n  == len * ", " *  theta == t1 * ")."

Now, this looks very strange, but if it looked like
   a* n==len * b * theta==t1 * d
it would be a perfectly reasonable  product of five terms,  two of which
are logical expressions.  I think in fact that it is a call, not an
expression, but in this case it doesn't matter.

In Uwe's example a, b, and d were strings.  The expression is still
lexically valid, but it can't be evaluated any more. That's ok, since it
isn't supposed to be evaluated.

What you can use in the mathematical annotation functions is parsed but
unevaluated R code.  You can get this mostly easily as the output of
quote(), expression() or substitute().

  quote("Parameter " * theta==1)
  expression("Parameter " * theta==1)
  substitute("Parameter " * theta==t, list(t=1))

For the purposes of mathematical annotation these are all equivalent,
though the second returns an `expression' and the other two return a
`call'.

The second form is occasionally needed, as in
  legend(locator(1), lty=1:2, legend=expression(alpha,beta))
which I don't think you can do any other way.

You can think of an expression as a vector of calls, so
> expression(alpha,beta)[1]
expression(alpha)
> expression(alpha,beta)[[1]]
alpha

which is what the manual was trying to say (I think). Much of the time you
can ignore the difference between `call' and `expression' objects.


	-thomas



From faheem at email.unc.edu  Sat Jun 21 06:19:05 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sat, 21 Jun 2003 00:19:05 -0400 (EDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.A41.4.44.0306201734170.144744-100000@homer37.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0306202316490.25336-100000@Chrestomanci>



> > "Monotonic Multigamma run (" * n  == len * ", " * theta == t1 *
> >
> > Is this a valid expression? My understanding of an expression is that it
> > contains one more more statements.
>
> That's only part  of the expression. This is the full expression
> "Monotonic Multigamma run (" * n  == len * ", " *  theta == t1 * ")."
>
> Now, this looks very strange, but if it looked like
>    a* n==len * b * theta==t1 * d
> it would be a perfectly reasonable  product of five terms,  two of which
> are logical expressions.  I think in fact that it is a call, not an
> expression, but in this case it doesn't matter.
>
> In Uwe's example a, b, and d were strings.  The expression is still
> lexically valid, but it can't be evaluated any more. That's ok, since it
> isn't supposed to be evaluated.

So multiplication is lexically valid in R even between logical
expression and strings?

The * really corresponded to multiplication, then?  Hmm. I see the
choice of * was so that it would not appear in the final math
expression produced by title. Ingenious.

> What you can use in the mathematical annotation functions is parsed but
> unevaluated R code.  You can get this mostly easily as the output of
> quote(), expression() or substitute().
>
>   quote("Parameter " * theta==1)
>   expression("Parameter " * theta==1)
>   substitute("Parameter " * theta==t, list(t=1))
>
> For the purposes of mathematical annotation these are all equivalent,
> though the second returns an `expression' and the other two return a
> `call'.
>
> The second form is occasionally needed, as in
>   legend(locator(1), lty=1:2, legend=expression(alpha,beta))
> which I don't think you can do any other way.
>
> You can think of an expression as a vector of calls, so
> > expression(alpha,beta)[1]
> expression(alpha)

This is an expression (according to ?expression).

> > expression(alpha,beta)[[1]]
> alpha

This is a call.

Can you go into a little more detail here about why alpha here is a
call?  I'm not terribly clear what a call is. If it so similar to an
expression, what distinguishes it from an expression, and why do we
need two similar concepts like this?

Also, would it not be more accurate to describe an expression as a
list of calls? Since, the [[ ]] applied to a list returns a component
of that list, which in this case is apparently a call.

> which is what the manual was trying to say (I think). Much of the time
> you can ignore the difference between `call' and `expression' objects.

                                                        Faheem.



From ripley at stats.ox.ac.uk  Sat Jun 21 07:44:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Jun 2003 06:44:00 +0100 (BST)
Subject: [R] Silhouette question
In-Reply-To: <38844494-A36F-11D7-8FBB-0005026E2B43@vanderkogel.net>
Message-ID: <Pine.LNX.4.44.0306210643120.28176-100000@gannet.stats>

?try is your friend here.

On Sat, 21 Jun 2003, Jonck van der Kogel wrote:

> Hi all,
> I am momentarily experimenting with Silhouette from the cluster library 
> but I am getting some errors. Since Silhouette can be seen as a quality 
> measure for a clustering what I want to do is run a series of different 
> clusterings and store the one with the highest Silhouette value. In 
> that way I hope to get "the best" clustering possible for my dataset.
> Here is the problem:
> When running the examples that come with silhouette, everything works 
> fine, the silhouette values are calculated perfectly. When I try to run 
> silhouette with my own dataset I get errors at unpredictable times, 
> that is, sometimes silhouette runs succesfully and at other times it 
> gives me the following error:
>  > test <- silhouette(cutree(agn, k=5), daisy(bestSom$codes))
> Error in apply(dmatrix[!iC, iC], 2, function(r) tapply(r, x[!iC], 
> mean)) :
>          dim(X) must have a positive length
> 
> Since I am running my experiments in batch mode (put a loop of 
> experiments in a source file and then load this source file), whenever 
> this error occurs the entire experiment is cut off. The experiment 
> takes rather a long time (approx. 12 hours), so I would not want to 
> start my experiment at night only to find in the morning that my 
> experiment never ran. Is there a way to
> a) prevent the error from happening, or
> b) detect beforehand that the error will happen and thus not do the 
> silhouette calculation for that particular clustering
> 
> Any help with this is much appreciated,
> thanks, Jonck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Jun 21 12:58:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Jun 2003 12:58:26 +0200
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306202316490.25336-100000@Chrestomanci>
References: <Pine.LNX.4.44.0306202316490.25336-100000@Chrestomanci>
Message-ID: <3EF43A52.9070308@statistik.uni-dortmund.de>

Faheem Mitha wrote:
> 
>>>"Monotonic Multigamma run (" * n  == len * ", " * theta == t1 *
>>>
>>>Is this a valid expression? My understanding of an expression is that it
>>>contains one more more statements.
>>
>>That's only part  of the expression. This is the full expression
>>"Monotonic Multigamma run (" * n  == len * ", " *  theta == t1 * ")."
>>
>>Now, this looks very strange, but if it looked like
>>   a* n==len * b * theta==t1 * d
>>it would be a perfectly reasonable  product of five terms,  two of which
>>are logical expressions.  I think in fact that it is a call, not an
>>expression, but in this case it doesn't matter.

Yes, thanks.


>>In Uwe's example a, b, and d were strings.  The expression is still
>>lexically valid, but it can't be evaluated any more. That's ok, since it
>>isn't supposed to be evaluated.

Yes. A very important statement. for mathematical annotation it's just 
parsed in a way. And it's a natural way to use an expression when doing 
*mathematical* annotation.


> So multiplication is lexically valid in R even between logical
> expression and strings?

Yes. See Section 6.2 od the R Language Reference.
Consider you introduce a method for a certain new class, that handles 
for any imaginary reason multiplication of logicals and characters. So 
you don't want R to forbid it when constructing an expression or call.


> The * really corresponded to multiplication, then?  Hmm. I see the
> choice of * was so that it would not appear in the final math
> expression produced by title. Ingenious.

 From ?plotmath: "x*y  juxtapose x and y".

BTW: "Ingenious" from those who introduced mathematical annotation in R:
Paul Murrell and Ross Ihaka (2000). An Approach to Providing 
Mathematical Annotation in Plots, Journal of Computational and Graphical 
Statistics, 9(3): 582?599.


>>What you can use in the mathematical annotation functions is parsed but
>>unevaluated R code.  You can get this mostly easily as the output of
>>quote(), expression() or substitute().
>>
>>  quote("Parameter " * theta==1)
>>  expression("Parameter " * theta==1)
>>  substitute("Parameter " * theta==t, list(t=1))
>>
>>For the purposes of mathematical annotation these are all equivalent,
>>though the second returns an `expression' and the other two return a
>>`call'.
>>
>>The second form is occasionally needed, as in
>>  legend(locator(1), lty=1:2, legend=expression(alpha,beta))
>>which I don't think you can do any other way.

In the reference given in my former mail their is an example how to 
construct such an expression from calls (constructed by substitute()) 
for legend() with do.call().


>>You can think of an expression as a vector of calls, so
>>
>>>expression(alpha,beta)[1]
>>
>>expression(alpha)
> 
> 
> This is an expression (according to ?expression).
> 
> 
>>>expression(alpha,beta)[[1]]
>>
>>alpha
> 
> 
> This is a call.
> 
> Can you go into a little more detail here about why alpha here is a
> call?  

Thomas already told it and ?expression says as well: "expression returns 
a vector of mode "expression" containing its arguments as unevaluated 
``calls''."


 > I'm not terribly clear what a call is. If it so similar to an
> expression, what distinguishes it from an expression, and why do we
> need two similar concepts like this?

Obviously a question for Bill Venables. I admit I've not thought about 
it before. In this context the article
  W. N. Venables (2002). Programmer?s Niche, R News, 2(2): 24?26,
  ISSN 1609-3631, http://CRAN. R-project.org/doc/Rnews/,
and
  Venables & Ripley (2000): S Programming, Springer.
are nice references.

Uwe Ligges

> Also, would it not be more accurate to describe an expression as a
> list of calls? Since, the [[ ]] applied to a list returns a component
> of that list, which in this case is apparently a call.
> 
> 
>>which is what the manual was trying to say (I think). Much of the time
>>you can ignore the difference between `call' and `expression' objects.
> 
>                                                         Faheem.



From mr_james_ireland at sbcglobal.net  Sat Jun 21 13:04:02 2003
From: mr_james_ireland at sbcglobal.net (J Ireland)
Date: Sat, 21 Jun 2003 04:04:02 -0700 (PDT)
Subject: [R] Beginner's Question on Linear Regression Models
Message-ID: <20030621110402.66745.qmail@web80202.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030621/2605bd03/attachment.pl

From sbarbar at gwdg.de  Sat Jun 21 13:48:32 2003
From: sbarbar at gwdg.de (Salvatore Barbaro)
Date: Sat, 21 Jun 2003 13:48:32 +0200
Subject: [R] error functions
Message-ID: <3EF46230.432.E298E8@localhost>

Hi everybody,

does anybody know if R contains error functions like 
"erf" and "erfc" in S-Plus? Thanks in advance.



From jfox at mcmaster.ca  Sat Jun 21 14:08:35 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 21 Jun 2003 08:08:35 -0400
Subject: [R] Beginner's Question on Linear Regression Models
In-Reply-To: <20030621110402.66745.qmail@web80202.mail.yahoo.com>
Message-ID: <5.1.0.14.2.20030621080142.01ec4098@mcmail.cis.mcmaster.ca>

Dear James,

A very nice way of understanding these matters intuitively is to express 
them geometrically using data and confidence ellipses (for two predictors 
and their coefficients) and ellipsoids (more generally). The same ideas 
apply to linear hypotheses, such as for the difference between two 
coefficients. A good elementary treatment may be founds in Georges Monette, 
"Geometry of multiple regression and 3-D graphics," in Fox and Long (eds.), 
Modern Methods of Data Analysis, Sage, 1990. Some regression texts also 
develop the geometry of regression and linear models.

I hope that this helps,
  John

At 04:04 AM 6/21/2003 -0700, J Ireland wrote:
>Hi Folks,
>
>Could anyone point me to a good reference on linear regression 
>models?   Specifically, I am trying to gain an intuitive feel for how the 
>standard error values are calculated for the parameter estimates.  My 
>understanding is that these are computed using the variance-covariance 
>matrix computed from the input data matrix.  Although I think I understand 
>the math, I still don't have a good gut feel for why one parameter is 
>attributed with a larger standard error than the next parameter.
>
>Also, I am interested in knowing how to test that two parameters are 
>significantly different from one another.
>
>Thanks in advance for your help.
>-James

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ripley at stats.ox.ac.uk  Sat Jun 21 15:15:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Jun 2003 14:15:40 +0100 (BST)
Subject: [R] error functions
In-Reply-To: <3EF46230.432.E298E8@localhost>
Message-ID: <Pine.LNX.4.44.0306211403340.31827-100000@gannet.stats>

Yes, statisticians call the natural versions of the cumulative normal 
distribution, pnorm.  As I recall

erf(x) = 2 * pnorm(x * sqrt(2)) - 1
erfc(x) = 2 * pnorm(x * sqrt(2), lower=FALSE)


On Sat, 21 Jun 2003, Salvatore Barbaro wrote:

> does anybody know if R contains error functions like 
> "erf" and "erfc" in S-Plus? Thanks in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Sat Jun 21 16:45:41 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Jun 2003 16:45:41 +0200
Subject: [R] Silhouette question
In-Reply-To: <Pine.LNX.4.44.0306210643120.28176-100000@gannet.stats>
References: <38844494-A36F-11D7-8FBB-0005026E2B43@vanderkogel.net>
	<Pine.LNX.4.44.0306210643120.28176-100000@gannet.stats>
Message-ID: <16116.28565.746325.85080@gargle.gargle.HOWL>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Sat, 21 Jun 2003 06:44:00 +0100 (BST) writes:

    BDR> ?try is your friend here. 

Yes, but Jonck's real problem is the use of an outdated version
of the cluster package (yes: "package",  *not* "library").

Which proves that he is certainly *not* using R 1.7.1.
While I strongly recommend, Jonck, that you upgrade your R
installation and get a new cluster `for free',
in this case it would also suffice to work with
update.packages()

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><


    >> Hi all, I am momentarily experimenting with Silhouette
    >> from the cluster library but I am getting some
    >> errors. Since Silhouette can be seen as a quality measure
    >> for a clustering what I want to do is run a series of
    >> different clusterings and store the one with the highest
    >> Silhouette value. In that way I hope to get "the best"
    >> clustering possible for my dataset.  Here is the problem:
    >> When running the examples that come with silhouette,
    >> everything works fine, the silhouette values are
    >> calculated perfectly. When I try to run silhouette with
    >> my own dataset I get errors at unpredictable times, that
    >> is, sometimes silhouette runs succesfully and at other
    >> times it gives me the following error: > test <-
    >> silhouette(cutree(agn, k=5), daisy(bestSom$codes)) Error
    >> in apply(dmatrix[!iC, iC], 2, function(r) tapply(r,
    >> x[!iC], mean)) : dim(X) must have a positive length
    >> 
    >> Since I am running my experiments in batch mode (put a
    >> loop of experiments in a source file and then load this
    >> source file), whenever this error occurs the entire
    >> experiment is cut off. The experiment takes rather a long
    >> time (approx. 12 hours), so I would not want to start my
    >> experiment at night only to find in the morning that my
    >> experiment never ran. Is there a way to a) prevent the
    >> error from happening, or b) detect beforehand that the
    >> error will happen and thus not do the silhouette
    >> calculation for that particular clustering
    >> 
    >> Any help with this is much appreciated, thanks, Jonck
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
    >> 

    BDR> -- Brian D. Ripley, ripley at stats.ox.ac.uk Professor of
    BDR> Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
    BDR> University of Oxford, Tel: +44 1865 272861 (self) 1
    BDR> South Parks Road, +44 1865 272866 (PA) Oxford OX1 3TG,
    BDR> UK Fax: +44 1865 272595

    BDR> ______________________________________________
    BDR> R-help at stat.math.ethz.ch mailing list
    BDR> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From iperez at escuelaing.edu.co  Sat Jun 21 17:12:20 2003
From: iperez at escuelaing.edu.co (Ignacio =?iso-8859-1?Q?P=E9rez?=)
Date: Sat, 21 Jun 2003 10:12:20 -0500
Subject: [R] Phillips-Perron
Message-ID: <5.1.0.14.0.20030621101034.00abf500@pop3.escuelaing.edu.co>

I don't know if this is the right place to post this question, anyway:

I have a bunch of time series for which I want to perform a  stationarity 
test, some of them have missing values which aren not handled by pp.test(). 
Any suggestions?

Thanks in advance

Ignacio Perez



From azzalini at stat.unipd.it  Sat Jun 21 17:21:59 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Sat, 21 Jun 2003 17:21:59 +0200
Subject: [R] optim with contraints
Message-ID: <20030621152159.DE1E37CA824@tango.stat.unipd.it>


There seems to exist peculiar cases where optim does not take care
of constraints on the parameters to be optimized over.  The call to
optim is of the form

  opt <- optim(cp, fn=sn.dev, gr=sn.dev.gh, method="L-BFGS-B",
           lower=c(-Inf, 1e-10, -0.99527), 
           upper=c( Inf, Inf,    0.99527), 
           control=control, X=X, y=y, hessian=FALSE)

The code has worked fine many times, but I have come across cases 
(for suitable data X and y) where the constraint on the last component 
is ignored;  that means that a call is made to sn.dev with
       cp = -1.3546  0.4645  3.1741
so the third component exceeds 0.99527, and the program stops 
because of a check in the  function to be obtimised.

The call just before was to the gradient function
sn.dev.gh: gradient =    219013   -312643 441647332
which has rather large values.

To make the problem more interesting, it shows up on a Linux
(Debian) installation, but it works fine on MS-windows.
In both cases, R is 1.7.0.  

Perhaps this sort of question should not be directed to the R-help
list, rather to the developer(s) of optim. Please instruct me on this 
point. Also, I appreciate that the above is not a reproducible example;
this would be longish in text, and I though to ask first to whom 
it is appropriate that I direct my question.


Best wishes,

Adelchi Azzalini


-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From stoet at volition.wustl.edu  Sat Jun 21 17:35:44 2003
From: stoet at volition.wustl.edu (Gijsbert Stoet)
Date: Sat, 21 Jun 2003 10:35:44 -0500
Subject: [R] how to get a probit scale in R?
Message-ID: <200306211535.h5LFZil3003969@volition.wustl.edu>

Hi,

If you plot a cumulative histogram of a gausian distribution, using a
log scale on the x-axis and a probit scale on the y-axis, you get a
straight line.

My question is whether it is possible in R to use a "probit" scale in
a "plot". For example on the following webpage you can see an
application of how I would like to use a probit scale:

http://www.physiol.cam.ac.uk/staff/carpente/recinormal.htm

And if a probit scale is not available, what would be the most
effecient way of 'simulating' a probit scale using the plot comment.

Many thanks for your help!!!



From ripley at stats.ox.ac.uk  Sat Jun 21 18:00:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Jun 2003 17:00:55 +0100 (BST)
Subject: [R] optim with contraints
In-Reply-To: <20030621152159.DE1E37CA824@tango.stat.unipd.it>
Message-ID: <Pine.LNX.4.44.0306211651290.32131-100000@gannet.stats>

Adelchi,

R is a volunteer project, and we would need a volunteer to look at this.  
`The developers of optim' (who are R-core) used published code for this
method. Since you are getting different answers on different platforms
this might be a bug in your compiler or run-time rather than R.

Please submit a reproducible bug report to R-bugs.  As you will see some
bugs don't get fixed very fast (including one I submitted from Padua in
2001).

Brian

On Sat, 21 Jun 2003, Adelchi Azzalini wrote:

> 
> There seems to exist peculiar cases where optim does not take care
> of constraints on the parameters to be optimized over.  The call to
> optim is of the form
> 
>   opt <- optim(cp, fn=sn.dev, gr=sn.dev.gh, method="L-BFGS-B",
>            lower=c(-Inf, 1e-10, -0.99527), 
>            upper=c( Inf, Inf,    0.99527), 
>            control=control, X=X, y=y, hessian=FALSE)
> 
> The code has worked fine many times, but I have come across cases 
> (for suitable data X and y) where the constraint on the last component 
> is ignored;  that means that a call is made to sn.dev with
>        cp = -1.3546  0.4645  3.1741
> so the third component exceeds 0.99527, and the program stops 
> because of a check in the  function to be obtimised.
> 
> The call just before was to the gradient function
> sn.dev.gh: gradient =    219013   -312643 441647332
> which has rather large values.
> 
> To make the problem more interesting, it shows up on a Linux
> (Debian) installation, but it works fine on MS-windows.
> In both cases, R is 1.7.0.  
> 
> Perhaps this sort of question should not be directed to the R-help
> list, rather to the developer(s) of optim. Please instruct me on this 
> point. Also, I appreciate that the above is not a reproducible example;
> this would be longish in text, and I though to ask first to whom 
> it is appropriate that I direct my question.
> 
> 
> Best wishes,
> 
> Adelchi Azzalini
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sat Jun 21 18:20:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jun 2003 09:20:13 -0700
Subject: [R] optim with contraints
References: <Pine.LNX.4.44.0306211651290.32131-100000@gannet.stats>
Message-ID: <3EF485BD.2050702@pdf.com>

Adelchi:

	  Permit me to add to Prof. Ripley's comments:

	  If you want to know how to get around this kind of problem, I will 
tell you that I would modify the definition of "cp" to send the 
constraints to +/-Inf.  Functions like "optim" tend to work better with 
unconstrained problems than constrained problems.  In your example, I 
would typically replace the second element of the unknown in your 
example by its logarithm, then immediately exponentiate it as a first 
step of the function.  Similarly I might replace the third by something 
like a logit transform z = log((1+x)/(1-x)), then immediately compute x 
= (1-exp(z))/(1+epx(z)) as a first step in the revised "cp".

	  In many cases, these kinds of transformations have other advantages. 
  Perhaps most importantly, a quadratic approximate tends to fit better 
over a wider range.  This can reduce the number of iterations required 
for convergence.  Moreover, if "cp" is a deviance = 
(-2)*log(likelihood), it can increase the accuracy of a normal 
approximation to the maximum likelihood estimates.

	  As Prof. Ripley indicated, R is a volunteer project.  If you produce 
a reproducible example, it would help further if you step through the 
code for "optim" line by line and find where it bombs.  If you can 
further suggest changes to the code that eliminate the problem, it could 
reduce substantially the time required for the bug to be fixed.

hth, spencer graves

Prof Brian Ripley wrote:
> Adelchi,
> 
> R is a volunteer project, and we would need a volunteer to look at this.  
> `The developers of optim' (who are R-core) used published code for this
> method. Since you are getting different answers on different platforms
> this might be a bug in your compiler or run-time rather than R.
> 
> Please submit a reproducible bug report to R-bugs.  As you will see some
> bugs don't get fixed very fast (including one I submitted from Padua in
> 2001).
> 
> Brian
> 
> On Sat, 21 Jun 2003, Adelchi Azzalini wrote:
> 
> 
>>There seems to exist peculiar cases where optim does not take care
>>of constraints on the parameters to be optimized over.  The call to
>>optim is of the form
>>
>>  opt <- optim(cp, fn=sn.dev, gr=sn.dev.gh, method="L-BFGS-B",
>>           lower=c(-Inf, 1e-10, -0.99527), 
>>           upper=c( Inf, Inf,    0.99527), 
>>           control=control, X=X, y=y, hessian=FALSE)
>>
>>The code has worked fine many times, but I have come across cases 
>>(for suitable data X and y) where the constraint on the last component 
>>is ignored;  that means that a call is made to sn.dev with
>>       cp = -1.3546  0.4645  3.1741
>>so the third component exceeds 0.99527, and the program stops 
>>because of a check in the  function to be obtimised.
>>
>>The call just before was to the gradient function
>>sn.dev.gh: gradient =    219013   -312643 441647332
>>which has rather large values.
>>
>>To make the problem more interesting, it shows up on a Linux
>>(Debian) installation, but it works fine on MS-windows.
>>In both cases, R is 1.7.0.  
>>
>>Perhaps this sort of question should not be directed to the R-help
>>list, rather to the developer(s) of optim. Please instruct me on this 
>>point. Also, I appreciate that the above is not a reproducible example;
>>this would be longish in text, and I though to ask first to whom 
>>it is appropriate that I direct my question.
>>
>>
>>Best wishes,
>>
>>Adelchi Azzalini
>>
>>
>>
> 
>



From clists at perrin.socsci.unc.edu  Sat Jun 21 18:58:19 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Sat, 21 Jun 2003 12:58:19 -0400 (EDT)
Subject: [R] Beginner's Question on Linear Regression Models
In-Reply-To: <20030621110402.66745.qmail@web80202.mail.yahoo.com>
References: <20030621110402.66745.qmail@web80202.mail.yahoo.com>
Message-ID: <Pine.LNX.4.53.0306211256410.26712@perrin.socsci.unc.edu>

John Fox was kind enough to reply, but didn't recommend IMHO the best book
on regression models: his own, John Fox, _An R and S-Plus Companion to
Applied Regression_, Sage, 2002.

ap

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Sat, 21 Jun 2003, J Ireland wrote:

> Hi Folks,
>
> Could anyone point me to a good reference on linear regression models?   Specifically, I am trying to gain an intuitive feel for how the standard error values are calculated for the parameter estimates.  My understanding is that these are computed using the variance-covariance matrix computed from the input data matrix.  Although I think I understand the math, I still don't have a good gut feel for why one parameter is attributed with a larger standard error than the next parameter.
>
> Also, I am interested in knowing how to test that two parameters are significantly different from one another.
>
> Thanks in advance for your help.
> -James
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From azzalini at stat.unipd.it  Sat Jun 21 19:02:20 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Sat, 21 Jun 2003 19:02:20 +0200
Subject: [R] optim with contraints
In-Reply-To: <3EF485BD.2050702@pdf.com>
References: <Pine.LNX.4.44.0306211651290.32131-100000@gannet.stats>
	<3EF485BD.2050702@pdf.com>
Message-ID: <20030621170220.8D2917CA824@tango.stat.unipd.it>

On Saturday 21 June 2003 18:20, Spencer Graves wrote:
> Adelchi:
>
> 	  Permit me to add to Prof. Ripley's comments:
>
> 	  If you want to know how to get around this kind of problem, I will
> tell you that I would modify the definition of "cp" to send the
> constraints to +/-Inf.  Functions like "optim" tend to work better with
> unconstrained problems than constrained problems.  In your example, I
> would typically replace the second element of the unknown in your
> example by its logarithm, then immediately exponentiate it as a first
> step of the function.  Similarly I might replace the third by something
> like a logit transform z = log((1+x)/(1-x)), then immediately compute x
> = (1-exp(z))/(1+epx(z)) as a first step in the revised "cp".
>
> 	  In many cases, these kinds of transformations have other advantages.
>   Perhaps most importantly, a quadratic approximate tends to fit better
> over a wider range.  This can reduce the number of iterations required
> for convergence.  Moreover, if "cp" is a deviance =
> (-2)*log(likelihood), it can increase the accuracy of a normal
> approximation to the maximum likelihood estimates.


Spencer, 

thanks for your comments.  What you say is surely appropriate in most
circumstances.  In fact, the original parametrisation of the problem was
unconstrained! and, yes, the function to be optimised is of type 
     deviance =  (-2)*log(likelihood),
However, in this case, there are reasons to reparametrise in this form. 
The essence is that near zero the unconstrained parameter has problems.
I can provide a detailed theoretical explanation of above statements,
but only if you really want it, and surely is not of interest to the whole
R-help list.

>
> 	  As Prof. Ripley indicated, R is a volunteer project.  If you produce
> a reproducible example, it would help further if you step through the
> code for "optim" line by line and find where it bombs.  If you can
> further suggest changes to the code that eliminate the problem, it could
> reduce substantially the time required for the bug to be fixed.
>

(On Monday) I shall see what I manage to understand: my R programming skill is 
definetely not at "R developer" level.  How do I step through the code for "optim" 
line by line?  its core statement is  
     res <- .Internal(optim(par, fn1, gr1, method, con, lower,  upper))
and I do not know how to access the .Internal; probably these questions already 
tell you a lot abut my skill..

I am aware of the fact that R is a volunteer project -- in fact a most appreciated 
and fundamental project of the statistical community.  In writing my earlier
message, I did intended to imply that some member of R-core had to rush looking
at my problem.
 
With best wishes, 

Adelchi

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From faheem at email.unc.edu  Sat Jun 21 20:01:38 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sat, 21 Jun 2003 14:01:38 -0400 (EDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <3EF43A52.9070308@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0306211348180.25336-100000@Chrestomanci>



On Sat, 21 Jun 2003, Uwe Ligges wrote:

> Faheem Mitha wrote:

> > Can you go into a little more detail here about why alpha here is a
> > call?
>
> Thomas already told it and ?expression says as well: "expression returns
> a vector of mode "expression" containing its arguments as unevaluated
> ``calls''."

Yes, but it is just a call by definition then? If you put anything in the
place of alpha it would be a call?

>  > I'm not terribly clear what a call is. If it so similar to an
> > expression, what distinguishes it from an expression, and why do we
> > need two similar concepts like this?
>
> Obviously a question for Bill Venables. I admit I've not thought about
> it before. In this context the article
>   W. N. Venables (2002). Programmer?s Niche, R News, 2(2): 24?26,
>   ISSN 1609-3631, http://CRAN. R-project.org/doc/Rnews/,
> and
>   Venables & Ripley (2000): S Programming, Springer.
> are nice references.

I've got "S Programming" and spent some of yesterday reading it. It
doesn't go into detail into the difference between expressions and calls.
It does have a table on pg 66 which gives examples of different modes.

                                                           Faheem.



From naumov at acsu.buffalo.edu  Sun Jun 22 02:27:27 2003
From: naumov at acsu.buffalo.edu (Aleksey Naumov)
Date: Sat, 21 Jun 2003 20:27:27 -0400
Subject: [R] Using weighted.mean() in aggregate()
Message-ID: <200306212027.27339.naumov@acsu.buffalo.edu>

Dear R users, I have a question on using weighted.mean() while aggregating a 
data frame. I have a data frame with columns Sub, Length and Slope:

> x[1:5,]
  Sub   Length        Slope
1   2  351.547 0.0025284969
2   2  343.738 0.0025859390
3   1  696.659 0.0015948968
4   2 5442.338 0.0026132544
5   1  209.483 0.0005304225

and I would like to calculate the weighted.mean of Slope, using Length as 
weights, for each value of Sub. The obvious way:

> aggregate(list(Mean.Slope=x$Slope), by=list(Sub=x$Sub), FUN=weighted.mean, 
w=x$Length)

does not work. weighted.mean() generates warnings that "longer object length 
is not a multiple of shorter object length in: x * w", from which I conclude 
that weights are not supplied as I intend, instead each subset of Sub, when 
passed to weighted.mean(), receives the whole x$Length as weights, which is 
not correct.

Is there an elegant way to do this, or do I have to have a loop here?

Thank you,
Aleksey

-- 
Aleksey Naumov
GIS Analyst
Center for Health and Social Research
Buffalo State College



From spencer.graves at pdf.com  Sun Jun 22 03:02:01 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 21 Jun 2003 18:02:01 -0700
Subject: [R] Using weighted.mean() in aggregate()
References: <200306212027.27339.naumov@acsu.buffalo.edu>
Message-ID: <3EF50009.4030704@pdf.com>

 > tstdf <- data.frame(Sub =rep(1:2, 2),
+  Length=1:4, Slope=11:14)
 > by(tstdf, tstdf$Sub,
+  function(x)weighted.mean(x$Slope, x$Length))
tstdf$Sub: 1
[1] 12.5
------------------------------------------------------------
tstdf$Sub: 2
[1] 13.33333
 >
Does this answer your question?

hth.  spencer graves

Aleksey Naumov wrote:
> Dear R users, I have a question on using weighted.mean() while aggregating a 
> data frame. I have a data frame with columns Sub, Length and Slope:
> 
> 
>>x[1:5,]
> 
>   Sub   Length        Slope
> 1   2  351.547 0.0025284969
> 2   2  343.738 0.0025859390
> 3   1  696.659 0.0015948968
> 4   2 5442.338 0.0026132544
> 5   1  209.483 0.0005304225
> 
> and I would like to calculate the weighted.mean of Slope, using Length as 
> weights, for each value of Sub. The obvious way:
> 
> 
>>aggregate(list(Mean.Slope=x$Slope), by=list(Sub=x$Sub), FUN=weighted.mean, 
> 
> w=x$Length)
> 
> does not work. weighted.mean() generates warnings that "longer object length 
> is not a multiple of shorter object length in: x * w", from which I conclude 
> that weights are not supplied as I intend, instead each subset of Sub, when 
> passed to weighted.mean(), receives the whole x$Length as weights, which is 
> not correct.
> 
> Is there an elegant way to do this, or do I have to have a loop here?
> 
> Thank you,
> Aleksey
>



From fharrell at virginia.edu  Sun Jun 22 13:04:19 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Sun, 22 Jun 2003 07:04:19 -0400
Subject: [R] Using weighted.mean() in aggregate()
In-Reply-To: <3EF50009.4030704@pdf.com>
References: <200306212027.27339.naumov@acsu.buffalo.edu>
	<3EF50009.4030704@pdf.com>
Message-ID: <20030622070419.50688eea.fharrell@virginia.edu>

On Sat, 21 Jun 2003 18:02:01 -0700
Spencer Graves <spencer.graves at pdf.com> wrote:

>  > tstdf <- data.frame(Sub =rep(1:2, 2),
> +  Length=1:4, Slope=11:14)
>  > by(tstdf, tstdf$Sub,
> +  function(x)weighted.mean(x$Slope, x$Length))
> tstdf$Sub: 1
> [1] 12.5
> ------------------------------------------------------------
> tstdf$Sub: 2
> [1] 13.33333
>  >
> Does this answer your question?
> 
> hth.  spencer graves

Here are two other ways, using the Hmisc package summarize or mApply functions, which can take a matrix as their first argument.  summarize returns a dataframe, mApply a vector

g <- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(sex,race), g, stat.name='y')
mApply(cbind(y,wts), llist(sex,race), g)

Frank Harrell

> 
> Aleksey Naumov wrote:
> > Dear R users, I have a question on using weighted.mean() while aggregating a 
> > data frame. I have a data frame with columns Sub, Length and Slope:
> > 
> > 
> >>x[1:5,]
> > 
> >   Sub   Length        Slope
> > 1   2  351.547 0.0025284969
> > 2   2  343.738 0.0025859390
> > 3   1  696.659 0.0015948968
> > 4   2 5442.338 0.0026132544
> > 5   1  209.483 0.0005304225
> > 
> > and I would like to calculate the weighted.mean of Slope, using Length as 
> > weights, for each value of Sub. The obvious way:
> > 
> > 
> >>aggregate(list(Mean.Slope=x$Slope), by=list(Sub=x$Sub), FUN=weighted.mean, 
> > 
> > w=x$Length)
> > 
> > does not work. weighted.mean() generates warnings that "longer object length 
> > is not a multiple of shorter object length in: x * w", from which I conclude 
> > that weights are not supplied as I intend, instead each subset of Sub, when 
> > passed to weighted.mean(), receives the whole x$Length as weights, which is 
> > not correct.
> > 
> > Is there an elegant way to do this, or do I have to have a loop here?
> > 
> > Thank you,
> > Aleksey
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From bates at stat.wisc.edu  Sun Jun 22 16:18:51 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 22 Jun 2003 14:18:51 -0000
Subject: [R] [OFF] stepwise using REML???
In-Reply-To: <200306201735.41239.chrysopa@insecta.ufv.br>
References: <200306201735.41239.chrysopa@insecta.ufv.br>
Message-ID: <6rfzm2w5yw.fsf@bates4.stat.wisc.edu>

"Ronaldo Reis Jr." <chrysopa at insecta.ufv.br> writes:

> Hi,
> 
> I know that is not possible make a stepwise procedure using REML in R, I can 
> use ML for this.
> 
> For nested design it may be very dangerous due the difference in
> variance structure, mainly in a splitplot design. ML make
> significative variables that REML dont make.

It would be good to quote an example that shows this.  I'm not sure
that this occurs in general.

> I read an article that is made a stepwise procedure using GENSTAT.
> 
> from article:
> "Terms were dropped from a model in a stepwise procedure by assessing the 
> change in deviance between the full model and the submodel."
> 
> All are made using REML.
> 
> It is possible?! I dont know GENSTAT.

You would need to be more specific about how the comparisons are made.
I assume that you plan to keep the random effects structure constant
and compare two nested models that differ only in the fixed effects
terms.  I can think of four ways of doing this:

1) Use the F-test obtained by fitting the full model and conditioning
on the estimates of the random effects parameters.  This is what the
anova function applied to an model fit by lme gives.

2) Fit both models and compare the values of the REML criterion in a
likelihood ratio test.

3) Fit both models by REML and compare the values of the
log-likelihood (i.e. the ML criterion) in a likelihood ratio test.
You can obtain that value with logLik(fm, REML=FALSE) if fm is your
fitted model.

4)Fit both models and evaluate the REML criterion for the full model
at the two sets of estimates.  Compare these values in a likelihood
ratio test.

I feel that 1) is appropriate, 2) is inappropriate, 3) may be
appropriate and 4) looks interesting.  4) is based on recent work by
Greg Reinsel.

In some simulations reported in chapter 3 of Pinheiro and Bates (2000)
3) fared badly compared to 1).



From drew.einhorn at starband.net  Sun Jun 22 17:10:23 2003
From: drew.einhorn at starband.net (Drew Einhorn)
Date: Sun, 22 Jun 2003 15:10:23 -0000
Subject: [R] Off Topic: Statistics Introduction for (Pre-Calculus) Beginners
Message-ID: <1056294124.1429.21.camel@lo>

Hi,

My wife is working with a group of 14-18 year old students
(pre-calculus) on a science project.  They need to use a statistics
package to analyze their data.  Does anyone know of a good statistics
introduction for students at this level, that uses R or some other open
source statistics package for the analysis?

We are hoping for a tutorial that develops enough understanding to
enable the students to intelligently use a tiny subset of R.

Thanks,
-- 
Drew Einhorn <drew.einhorn at starband.net>



From ozric at web.de  Sun Jun 22 17:30:48 2003
From: ozric at web.de (Christian Schulz)
Date: Sun, 22 Jun 2003 17:30:48 +0200
Subject: [R] table in x-y diagram
Message-ID: <003f01c338d3$4ce1fec0$d200a8c0@pc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030622/b7e89254/attachment.pl

From brentk at birs.pims.math.ca  Sun Jun 22 21:22:03 2003
From: brentk at birs.pims.math.ca (Brent Kearney)
Date: Sun, 22 Jun 2003 13:22:03 -0600
Subject: [R] core dump on solaris 2.9
Message-ID: <20030622132203.A15730@birs.pims.math.ca>

Hello,

I have compiled R 1.7.1 using gcc 3.2.2 and these configure options:
./configure --prefix=/opt --with-readline --with-x --without-gnome 

It compiled and runs fine, except when "plot(1:10)" is run in R.  The 
plot appears for an instant, then the graphics window closes and the R 
session ends with the message ``Bus Error (core dumped)''.

I'm not sure what to do... any suggestions?  Please mail me directly,
since I'm not subscribed to this list.

Thanks,

Brent



From ripley at stats.ox.ac.uk  Sun Jun 22 21:34:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 22 Jun 2003 20:34:06 +0100 (BST)
Subject: [R] core dump on solaris 2.9
In-Reply-To: <20030622132203.A15730@birs.pims.math.ca>
Message-ID: <Pine.LNX.4.44.0306222028410.9590-100000@gannet.stats>

[I presume this is sparc-Solaris, but you did not say.]

Please do as the INSTALL file asks and read the R-admin manual: this *is*
documented there!  That version of gcc is obselete, and unusable on
sparc-Solaris.  You will have to find a working compiler: if all else
fails, try turning optimization off completely (for C, C++ and Fortran).

On Sun, 22 Jun 2003, Brent Kearney wrote:

> I have compiled R 1.7.1 using gcc 3.2.2 and these configure options:
> ./configure --prefix=/opt --with-readline --with-x --without-gnome 
> 
> It compiled and runs fine, except when "plot(1:10)" is run in R.  The 
> plot appears for an instant, then the graphics window closes and the R 
> session ends with the message ``Bus Error (core dumped)''.
> 
> I'm not sure what to do... any suggestions?  Please mail me directly,
> since I'm not subscribed to this list.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From syed at saudionline.com.sa  Sun Jun 22 21:49:49 2003
From: syed at saudionline.com.sa (syed gillani)
Date: Sun, 22 Jun 2003 12:49:49 -0700
Subject: [R] clicking the stop button
Message-ID: <005301c338f7$7164e0e0$e706eed5@xxxxxxxpcm0sdp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030622/7436e397/attachment.pl

From ripley at stats.ox.ac.uk  Sun Jun 22 21:59:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 22 Jun 2003 20:59:03 +0100 (BST)
Subject: [R] clicking the stop button
In-Reply-To: <005301c338f7$7164e0e0$e706eed5@xxxxxxxpcm0sdp>
Message-ID: <Pine.LNX.4.44.0306222057050.9639-100000@gannet.stats>

This is PR#3285, fixed in R-patched and R-devel.  Search on R-bugs for the 
full details.

On Sun, 22 Jun 2003, syed gillani wrote:

> Clicking the stop button results in failure to accept any further
> keyboard input.Is it just my machine?
> 
> RGui: 1.7.0 & 1.7.1
> WindowsXp Prof.
> Pentium4 2.5 MH
> 
> Gillani.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Mon Jun 23 00:47:09 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 23 Jun 2003 10:47:09 +1200
Subject: [R] table in x-y diagram
References: <003f01c338d3$4ce1fec0$d200a8c0@pc>
Message-ID: <3EF631ED.5070901@stat.auckland.ac.nz>

Hi


Christian Schulz wrote:
> Hi,
> 
> i'm plotting some "bubbles" in a x-y diagram.
> I like to add a table like:
> cl80m <- aggregate(cl80,list(Cluster=cl80[,10]),mean)
> but, put it with:
> grid.text(as.data.frame(cl80m), x = unit(0.5, "npc"), y = unit(0.5, "npc"))
> i get it as list and not really to read.
> 
> Have anybody more experience, add small table's
> (6-9 rows, 3-8 colums) in a corner of a plot


There's a document discussing ways of creating a table using grid at
http://www.stat.auckland.ac.nz/~paul/grid/table.pdf

If you could provide me with an example that I could run (i.e., data 
included or randomly generated) I might be able to make more useful 
suggestions.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz



From tlumley at u.washington.edu  Mon Jun 23 01:17:08 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 22 Jun 2003 16:17:08 -0700 (PDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306202316490.25336-100000@Chrestomanci>
Message-ID: <Pine.A41.4.44.0306221558030.42590-100000@homer09.u.washington.edu>

On Sat, 21 Jun 2003, Faheem Mitha wrote:
>
> So multiplication is lexically valid in R even between logical
> expression and strings?
>

yes, in the sense that the lexer and parser won't choke when trying to
read the statement.

You could define a method for "*" that multiplied character strings (you
might have to work through defining Ops.character)

>
> Also, would it not be more accurate to describe an expression as a
> list of calls? Since, the [[ ]] applied to a list returns a component
> of that list, which in this case is apparently a call.
>

No, it would be less accurate (though arguably clearer). An expression is
not actually internally represented as a list of calls, just as
   list(quote(x*y), quote(a+b))
is represented as a list of calls, not an expression. Now, an expression
could have been just a list of calls (it works in LISP), but in fact it
isn't.


The problem in writing the R language definition is in trying to be both
precise and helpful (the paragraph you quoted is arguably neither, but
that's why it's a `draft').

The precise difference between an expression and a call is that an
expression is of mode "expression" and a call is of mode "call" (and a
list of calls is of mode "list"). This isn't very helpful, which is why
you get the handwaving about an expression being like a list of calls.


	-thomas



From ok at cs.otago.ac.nz  Mon Jun 23 03:14:19 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 23 Jun 2003 13:14:19 +1200 (NZST)
Subject: [R] How can I do a spinning plot in R?
Message-ID: <200306230114.h5N1EJ6O467231@atlas.otago.ac.nz>

I have found XLispStat's spinning plots illuminating.
I'd like to do the same thing in R.
A dozen or so probes with help, help.search, apropos
haven't turned up anything, and I've even resorted to
grepping through the entire R source distribution
looking for 'spin.*plot', to no avail.

Either the feature is called something else in R (what?),
or it's in some other package in CRAN (which?),
or it's not yet available (I hope not, because I am very
far from being skilled enough in R programming to do it
myself).

Help?



From Simon.Blomberg at anu.edu.au  Mon Jun 23 03:59:08 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 23 Jun 2003 11:59:08 +1000
Subject: [R] How can I do a spinning plot in R?
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133C5@CASEVS02.cas.anu.edu.au>

Nothing in R per se, but R has a package Rggobi which provides an interface to ggobi, which is a program that may do what you  want. Unix and Windows versions are available. See http://www.ggobi.org/

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz]
> Sent: Monday, 23 June 2003 11:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How can I do a spinning plot in R?
> 
> 
> I have found XLispStat's spinning plots illuminating.
> I'd like to do the same thing in R.
> A dozen or so probes with help, help.search, apropos
> haven't turned up anything, and I've even resorted to
> grepping through the entire R source distribution
> looking for 'spin.*plot', to no avail.
> 
> Either the feature is called something else in R (what?),
> or it's in some other package in CRAN (which?),
> or it's not yet available (I hope not, because I am very
> far from being skilled enough in R programming to do it
> myself).
> 
> Help?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From davidD at qimr.edu.au  Mon Jun 23 04:10:26 2003
From: davidD at qimr.edu.au (David Duffy)
Date: Mon, 23 Jun 2003 12:10:26 +1000 (EST)
Subject: [R] Re: Sparse binary data
In-Reply-To: <200306201002.h5KA26eY013662@stat.math.ethz.ch>
References: <200306201002.h5KA26eY013662@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.50.0306231202060.6720-100000@orpheus.qimr.edu.au>


>Douglas Trainor <trainor at transborder.org> wrote:
>
>I too am interested in analysis of sparse data, and I couldn't find this
>journal easily, but I found an Oliver Kuss presentation that likely
>summarizes the material.  You can find that presentation here:
>
>Vito Muggeo wrote:
>
>>Dear Henric,
>>The following paper deals with goodness-of-fit test for sparse (and even
>>binary) data:
>>
>>Kuss O. Global goodness-of-fit tests in logistic regression with sparse
>>data, Statist Med, 2002, 21:3789-3801.
>>

You may also be interested in David Firth's brlr package.


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From davidD at qimr.edu.au  Mon Jun 23 04:52:11 2003
From: davidD at qimr.edu.au (David Duffy)
Date: Mon, 23 Jun 2003 12:52:11 +1000 (EST)
Subject: [R] Re: stepwise REML
In-Reply-To: <200306211004.h5LA3HU7022437@stat.math.ethz.ch>
References: <200306211004.h5LA3HU7022437@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.50.0306231232030.6720-100000@orpheus.qimr.edu.au>


Ronaldo Reis Jr. wrote:

> I know that is not possible make a stepwise procedure using REML in R, I can 
> use ML for this.
> For nested design it may be very dangerous due the difference in variance 
> structure, mainly in a splitplot design. ML make significative variables that 
> REML dont make.
> I read an article that is made a stepwise procedure using GENSTAT.
> All are made using REML.

One can always compare models in a hierarchical type fashion (providing
they can be nested).  The LRTS comparing models fitted via
REML may not be chi-square.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From laurent at cbs.dtu.dk  Mon Jun 23 05:13:41 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Mon, 23 Jun 2003 05:13:41 +0200
Subject: [R] Cross-compiling R packages
Message-ID: <20030623031341.GB5737217@genome.cbs.dtu.dk>

Hi,

I tried to use the 'make' file discussed in the last R-news.

The step 'make R' dies with:
make[2]: Entering directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
sed -e s/@RVER@/`cut -d' ' -f1 ../../VERSION | sed -n 1p`/g -e s/@RWVER@/rw1071/g rw-FAQ.texi | \
  makeinfo --no-headers --number-sections -o rw-FAQ
makeinfo --no-split --html --no-headers --number-sections -o tmp.html tmp.texi
tidy tmp.html > tmp2.html 2> /dev/null
make[2]: [fixed/html/rw-FAQ.html] Error 127 (ignored)
*** tidy appears to be non-functional ***
make[2]: *** [fixed/html/rw-FAQ.html] Error 111
make[2]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
make[1]: *** [docfiles] Error 2
make[1]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'


Any hint ?



Thanks,



L..



From r.hankin at auckland.ac.nz  Mon Jun 23 05:32:04 2003
From: r.hankin at auckland.ac.nz (Robin Hankin)
Date: Mon, 23 Jun 2003 15:32:04 +1200
Subject: [R] right assignment ("->") and functions
Message-ID: <200306230332.h5N3W4E2004130@r.hankin.sges.auckland.ac.nz>

Hi everyone

check this out [R-1.7.0]:

R> f1 <- function(x){x^2}
R> f1 -> f2
R> f2(4)
[1] 16
R> 
R> function(x){x^2} -> f3
function(x){x^2} -> f3
R> f3(4)
Error: couldn't find function "f3"

Why does right assignment "->" work in the first but not the second
case?  Can anyone else reproduce this?


-- 

Robin Hankin, Lecturer,
School of Geography and Environmental Science
Tamaki Campus
Private Bag 92019 Auckland
New Zealand

r.hankin at auckland.ac.nz(nospam)
tel 0064-9-373-7599 x6820; FAX 0064-9-373-7042



From Bill.Venables at csiro.au  Mon Jun 23 06:20:48 2003
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 23 Jun 2003 14:20:48 +1000
Subject: [R] right assignment ("->") and functions
Message-ID: <E09E527B56BE2D438A3D6A246DDD27A9165CFC@roper-cv.qld.cmis.CSIRO.AU>

You need an extra level of braces to resolve the ambiguity:

> function(x) {x^2} -> f3
function(x) {x^2} -> f3
> .Last.value
function(x) {x^2} -> f3

You need to do it as follows:

> {function(x) {x^2}} -> f3
> f3
function(x) {x^2}


Of course the inner braces around the x^2 are not necessary, now.

Bill Venables. 

-----Original Message-----
From: Robin Hankin [mailto:r.hankin at auckland.ac.nz]
Sent: Monday, June 23, 2003 1:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] right assignment ("->") and functions


Hi everyone

check this out [R-1.7.0]:

R> f1 <- function(x){x^2}
R> f1 -> f2
R> f2(4)
[1] 16
R> 
R> function(x){x^2} -> f3
function(x){x^2} -> f3
R> f3(4)
Error: couldn't find function "f3"

Why does right assignment "->" work in the first but not the second
case?  Can anyone else reproduce this?


-- 

Robin Hankin, Lecturer,
School of Geography and Environmental Science
Tamaki Campus
Private Bag 92019 Auckland
New Zealand

r.hankin at auckland.ac.nz(nospam)
tel 0064-9-373-7599 x6820; FAX 0064-9-373-7042

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Mon Jun 23 06:23:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 05:23:26 +0100 (BST)
Subject: [R] Cross-compiling R packages
In-Reply-To: <20030623031341.GB5737217@genome.cbs.dtu.dk>
Message-ID: <Pine.LNX.4.44.0306230512170.10396-100000@gannet.stats>

On Mon, 23 Jun 2003, Laurent Gautier wrote:

> I tried to use the 'make' file discussed in the last R-news.
> 
> The step 'make R' dies with:
> make[2]: Entering directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> sed -e s/@RVER@/`cut -d' ' -f1 ../../VERSION | sed -n 1p`/g -e s/@RWVER@/rw1071/g rw-FAQ.texi | \
>   makeinfo --no-headers --number-sections -o rw-FAQ
> makeinfo --no-split --html --no-headers --number-sections -o tmp.html tmp.texi
> tidy tmp.html > tmp2.html 2> /dev/null
> make[2]: [fixed/html/rw-FAQ.html] Error 127 (ignored)
> *** tidy appears to be non-functional ***
> make[2]: *** [fixed/html/rw-FAQ.html] Error 111
> make[2]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> make[1]: *** [docfiles] Error 2
> make[1]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> 
> 
> Any hint ?

Well, you are missing `tidy', from http://tidy.sourceforge.net/, as 
mentioned in the src/gnuwin32/INSTALL file.

I don't see what the `make' file in the last R-news has to do with this: 
all the support for cross-compiling R is in the base R distribution.
Credit where credit is due!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 23 06:28:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 05:28:02 +0100 (BST)
Subject: [R] right assignment ("->") and functions
In-Reply-To: <200306230332.h5N3W4E2004130@r.hankin.sges.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0306230524430.10396-100000@gannet.stats>

On Mon, 23 Jun 2003, Robin Hankin wrote:

> Hi everyone
> 
> check this out [R-1.7.0]:
> 
> R> f1 <- function(x){x^2}
> R> f1 -> f2
> R> f2(4)
> [1] 16
> R> 
> R> function(x){x^2} -> f3
> function(x){x^2} -> f3
> R> f3(4)
> Error: couldn't find function "f3"
> 
> Why does right assignment "->" work in the first but not the second
> case?  Can anyone else reproduce this?

Because the second is equivalent to 

function(x) f3 <- {x^2}

an anonymous function.  Using a right assignment affects the direction of
parsing, but only after it is encountered and R is parsed left to right.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Jun 23 06:28:51 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 22 Jun 2003 21:28:51 -0700 (PDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306222255240.3220-100000@Chrestomanci>
Message-ID: <Pine.A41.4.44.0306222111171.37202-100000@homer30.u.washington.edu>

On Sun, 22 Jun 2003, Faheem Mitha wrote:
> If I'm doing this correctly, R does not seem to think it is a call.
>
> > is.call("Monotonic Multigamma run (" * n == len * ", " * theta == t1
> * ").")
> Error in "Monotonic Multigamma run (" * n :
>         non-numeric argument to binary operator

R is trying to *evaluate*
  "Monotonic Multigamma run ("* n==len etc
which doesn't work.  Remember, is.call(), like any normal function, will
be passed the *value* of its arguments.

You could try
  is.call(quote("Monotonic Multigamma run("*n==len))
which is TRUE.

> It considers it a valid R expression though.
>
> > (mode(expression("Monotonic Multigamma run (" * n == len * ", " * theta
> == t1 * ").")))
> [1] "expression"
>

That's because expression() returns an expression.

>
> The clearest description I have seen of a call is in S Poetry, where it
> says
>
> "Mode call represents objects that are calls to a function. The first
> component of a call is the name (mode name) of the function being called.
> The rest of the call is the arguments given."
>
> This certainly is how calls are constructed using call(...), but I'm not
> sure how it fits in with an expression like the one above. What is the
> function being called in that case, for example?

Well, we can find out. It must be either * or ==, but it isn't immediately
obvious which one ends up at the top level

> thing <- quote("Monotonic Multigamma Run ("*n==len* ", " * theta
==t1*").")
> mode(thing)
[1] "call"
> length(thing)
[1] 3
> thing[[1]]
==
> thing[[2]]
"Monotonic Multigamma Run (" * n == len * ", " * theta
> thing[[3]]
t1 * ")."
> mode(thing[[2]])
[1] "call"
> mode(thing[[3]])
[1] "call"
> thing[[2]][[1]]
==
> thing[[3]][[1]]
*

So it is a call to ==, with two arguments, each itself a call.  The first
arguemetn is also a call to == and the second is a call to *. And so on in
a tree structure.


> Maybe I'd understand this stuff better if I knew Lisp. I'm very ignorant
> about programming language paradigms. I've looked at S Poetry, and S
> Programming. While they were helpful, I don't feel I've got a clear
> picture. Further recommended reading?
>

I don't know about further reading.  I think this is probably easier to
learn when you have a specific problem to solve.  I learned mostly by
porting the survival package.


	-thomas



From tlumley at u.washington.edu  Mon Jun 23 06:32:46 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 22 Jun 2003 21:32:46 -0700 (PDT)
Subject: [R] right assignment ("->") and functions
In-Reply-To: <200306230332.h5N3W4E2004130@r.hankin.sges.auckland.ac.nz>
Message-ID: <Pine.A41.4.44.0306222129280.37202-100000@homer30.u.washington.edu>

On Mon, 23 Jun 2003, Robin Hankin wrote:

> Hi everyone
>
> check this out [R-1.7.0]:
>
> R> f1 <- function(x){x^2}
> R> f1 -> f2
> R> f2(4)
> [1] 16
> R>
> R> function(x){x^2} -> f3
> function(x){x^2} -> f3
> R> f3(4)
> Error: couldn't find function "f3"
>
> Why does right assignment "->" work in the first but not the second
> case?  Can anyone else reproduce this?
>

It does work.  It just doesn't do what you expect.

Suppose you typed
 {x^2} -> f3
This would assign x^2 to f3.

So
  function(x)
    {x^2} -> f3

is an anonymous function of one argument, which assigns the square of that
argument to the local variable f3.

To get what you wanted you would need

{function(x) x^2}-> f3

	-thomas



From ripley at stats.ox.ac.uk  Mon Jun 23 06:35:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 05:35:10 +0100 (BST)
Subject: [R] How can I do a spinning plot in R?
In-Reply-To: <200306230114.h5N1EJ6O467231@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.44.0306230528480.10396-100000@gannet.stats>

There are interfaces to XGobi (the xgobi package on CRAN) and GGobi 
(RSGGobi from Omegahat, and I have a variation on xgobi())  to allow
spinning of point clouds.

You can spin surfaces in the R<-->GL packages, e.g. Duncan Murdoch's at
http://www.stats.uwo.ca/faculty/murdoch/software/default.htm
and (perhaps: I have not tried this one) Daniel Adler's at
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl/

On Mon, 23 Jun 2003, Richard A. O'Keefe wrote:

> I have found XLispStat's spinning plots illuminating.
> I'd like to do the same thing in R.
> A dozen or so probes with help, help.search, apropos
> haven't turned up anything, and I've even resorted to
> grepping through the entire R source distribution
> looking for 'spin.*plot', to no avail.
> 
> Either the feature is called something else in R (what?),
> or it's in some other package in CRAN (which?),
> or it's not yet available (I hope not, because I am very
> far from being skilled enough in R programming to do it
> myself).

(Actually the problem is the user interaction which does not fit well with
the S/R graphics model.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kwan022 at stat.auckland.ac.nz  Mon Jun 23 07:00:43 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 23 Jun 2003 17:00:43 +1200 (NZST)
Subject: [R] How can I do a spinning plot in R?
In-Reply-To: <Pine.LNX.4.44.0306230528480.10396-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0306231656130.8914-100000@stat57.stat.auckland.ac.nz>

Hi Richard,

On Mon, 23 Jun 2003, Prof Brian Ripley wrote:

> You can spin surfaces in the R<-->GL packages, e.g. Duncan Murdoch's at
> http://www.stats.uwo.ca/faculty/murdoch/software/default.htm
> and (perhaps: I have not tried this one) Daniel Adler's at
> http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl/

Although I have not tried this myself, I saw one of the PhD students from 
our department gave a seminar a few months ago on Daniel Adler's RGL 
package.  It looks pretty pretty cool.  

>From memory, he showed a multiple linear regression using RGL and 
demonstrated how one can rotate the fitted surface.  One can also change 
the light/shading as well.  (He was using RH Linux, but there is a Windows 
version available too).


-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ok at cs.otago.ac.nz  Mon Jun 23 07:38:46 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 23 Jun 2003 17:38:46 +1200 (NZST)
Subject: [R] How can I do a spinning plot in R?
Message-ID: <200306230538.h5N5ck9i484078@atlas.otago.ac.nz>

Thank you to the people who told me about interfacing to *gobi



From laurent at cbs.dtu.dk  Mon Jun 23 07:46:02 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Mon, 23 Jun 2003 07:46:02 +0200
Subject: [R] Cross-compiling R packages
In-Reply-To: <Pine.LNX.4.44.0306230512170.10396-100000@gannet.stats>
References: <20030623031341.GB5737217@genome.cbs.dtu.dk>
	<Pine.LNX.4.44.0306230512170.10396-100000@gannet.stats>
Message-ID: <20030623054602.GA5831911@genome.cbs.dtu.dk>

On Mon, Jun 23, 2003 at 05:23:26AM +0100, Prof Brian Ripley wrote:
> On Mon, 23 Jun 2003, Laurent Gautier wrote:
> 
> > I tried to use the 'make' file discussed in the last R-news.
> > 
> > The step 'make R' dies with:
> > make[2]: Entering directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> > sed -e s/@RVER@/`cut -d' ' -f1 ../../VERSION | sed -n 1p`/g -e s/@RWVER@/rw1071/g rw-FAQ.texi | \
> >   makeinfo --no-headers --number-sections -o rw-FAQ
> > makeinfo --no-split --html --no-headers --number-sections -o tmp.html tmp.texi
> > tidy tmp.html > tmp2.html 2> /dev/null
> > make[2]: [fixed/html/rw-FAQ.html] Error 127 (ignored)
> > *** tidy appears to be non-functional ***
> > make[2]: *** [fixed/html/rw-FAQ.html] Error 111
> > make[2]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> > make[1]: *** [docfiles] Error 2
> > make[1]: Leaving directory `/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32'
> > 
> > 
> > Any hint ?
> 
> Well, you are missing `tidy', from http://tidy.sourceforge.net/, as 
> mentioned in the src/gnuwin32/INSTALL file.
> 
> I don't see what the `make' file in the last R-news has to do with this: 
> all the support for cross-compiling R is in the base R distribution.
> Credit where credit is due!

You might see what the 'make' file has to do after checking
the pages 15, 16 and 17 of R-news dated June 2003. My email reports
an error encountered while following the steps detailed there. 



L.



From r.hankin at auckland.ac.nz  Mon Jun 23 07:54:58 2003
From: r.hankin at auckland.ac.nz (Robin Hankin)
Date: Mon, 23 Jun 2003 17:54:58 +1200
Subject: [R] right assignment ("->") and functions
In-Reply-To: <Pine.A41.4.44.0306222129280.37202-100000@homer30.u.washington.edu>
	(message from Thomas Lumley on Sun, 22 Jun 2003 21:32:46 -0700 (PDT))
References: <Pine.A41.4.44.0306222129280.37202-100000@homer30.u.washington.edu>
Message-ID: <200306230554.h5N5sw3O004726@r.hankin.sges.auckland.ac.nz>

Hi again list.

Thanks for these replies.  I guess the issue is one of parsing units
being different from my expectations.  I saw a related issue in a
slightly different context a couple of days ago:


R> f1 <- function(a,b)
{a
-b}
R> f2 <- function(a,b)
{a-
 b}


...which (rightly) give very different results [the original bug took
me *hours* to find].

cheers

rksh




> It does work.  It just doesn't do what you expect.
> 
> Suppose you typed
>  {x^2} -> f3
> This would assign x^2 to f3.
> 
> So
>   function(x)
>     {x^2} -> f3
> 
> is an anonymous function of one argument, which assigns the square of that
> argument to the local variable f3.
> 
> To get what you wanted you would need
> 
> {function(x) x^2}-> f3
> 
> 	-thomas
> 
> 


-- 

Robin Hankin, Lecturer,
School of Geography and Environmental Science
Tamaki Campus
Private Bag 92019 Auckland
New Zealand

r.hankin at auckland.ac.nz
tel 0064-9-373-7599 x6820; FAX 0064-9-373-7042



From ynotssor at hotmail.com  Mon Jun 23 08:09:20 2003
From: ynotssor at hotmail.com (Tony Ross)
Date: Sun, 22 Jun 2003 23:09:20 -0700
Subject: [R] "cloud" function doesn't plot
Message-ID: <BAY8-F77qhZRSIgOf9I0002bb27@hotmail.com>

Forgive me please if this has been answered previously, but the r-help 
archive doesn't have a "search" capability. It's a pity, as I'm sure that 
there's a wealth of informative help in there.

I am unable to get the "cloud" function to produce a fundamental 3D 
scatterplot. I have a 3-column numeric matrix:

>is.matrix(sanity.MIF); is.numeric(sanity.MIF); dim(sanity.MIF); 
>cloud(sanity.MIF)
[1] TRUE
[1] TRUE
[1] 2970    3
>

A blank grey graphics window is opened, but no vestige of a plot or labels 
of any kind.

Can someone tell me what I am missing please? R 1.7.1 Windows2000 is new to 
me, but plenty of prior S-Plus Unix experience, including the use of the 
"spin" function for rotational 3D scatterplots.

Thank You for your help.



From Simon.Blomberg at anu.edu.au  Mon Jun 23 08:26:39 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 23 Jun 2003 16:26:39 +1000
Subject: [R] "cloud" function doesn't plot
Message-ID: <7A3A13F416B40842BD2C1753E044B359B0991D@CASEVS02.cas.anu.edu.au>

Actually, there IS a search facility, although you have to look for it. A search on "cloud" returned 5 pages. If you type
?cloud, you can see that you need to provide a formula as well as a dataset. So if your variables are x,y,z in dataframe dat, then cloud(z~x*y, data=dat) should work.

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Tony Ross [mailto:ynotssor at hotmail.com]
> Sent: Monday, 23 June 2003 4:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] "cloud" function doesn't plot
> 
> 
> Forgive me please if this has been answered previously, but 
> the r-help 
> archive doesn't have a "search" capability. It's a pity, as 
> I'm sure that 
> there's a wealth of informative help in there.
> 
> I am unable to get the "cloud" function to produce a fundamental 3D 
> scatterplot. I have a 3-column numeric matrix:
> 
> >is.matrix(sanity.MIF); is.numeric(sanity.MIF); dim(sanity.MIF); 
> >cloud(sanity.MIF)
> [1] TRUE
> [1] TRUE
> [1] 2970    3
> >
> 
> A blank grey graphics window is opened, but no vestige of a 
> plot or labels 
> of any kind.
> 
> Can someone tell me what I am missing please? R 1.7.1 
> Windows2000 is new to 
> me, but plenty of prior S-Plus Unix experience, including the 
> use of the 
> "spin" function for rotational 3D scatterplots.
> 
> Thank You for your help.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ynotssor at hotmail.com  Mon Jun 23 08:43:27 2003
From: ynotssor at hotmail.com (Tony Ross)
Date: Sun, 22 Jun 2003 23:43:27 -0700
Subject: [R] "cloud" function doesn't plot
Message-ID: <BAY8-F27P3D6g5dwDAx00001933@hotmail.com>




>From: "Simon Blomberg" <Simon.Blomberg at anu.edu.au>
>Date: Mon, 23 Jun 2003 16:26:39 +1000

>If you type ?cloud, you can see that you need to provide a formula as well 
>as a dataset. So if your variables are  x,y,z in dataframe dat, then 
>cloud(z~x*y, data=dat) should work.
[...]
>>is.matrix(sanity.MIF); is.numeric(sanity.MIF); dim(sanity.MIF); 
>>cloud(sanity.MIF)
>[1] TRUE
>[1] TRUE
>[1] 2970    3
>>

The "help(cloud)" facility states: "`formula' can be a matrix". As I stated, 
I have a 3-column numeric matrix, so colum 3 should be plotted as a point 
above the 1st and 2nd column co-ordinates.

No plot is produced however, just a blank graphics device.

Thanks.



From ripley at stats.ox.ac.uk  Mon Jun 23 08:57:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 07:57:52 +0100 (BST)
Subject: [R] "cloud" function doesn't plot
In-Reply-To: <BAY8-F27P3D6g5dwDAx00001933@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306230754380.14244-100000@gannet.stats>

On Sun, 22 Jun 2003, Tony Ross wrote:

> >From: "Simon Blomberg" <Simon.Blomberg at anu.edu.au>
> >Date: Mon, 23 Jun 2003 16:26:39 +1000
> 
> >If you type ?cloud, you can see that you need to provide a formula as well 
> >as a dataset. So if your variables are  x,y,z in dataframe dat, then 
> >cloud(z~x*y, data=dat) should work.
> [...]
> >>is.matrix(sanity.MIF); is.numeric(sanity.MIF); dim(sanity.MIF); 
> >>cloud(sanity.MIF)
> >[1] TRUE
> >[1] TRUE
> >[1] 2970    3
> >>
> 
> The "help(cloud)" facility states: "`formula' can be a matrix". As I stated, 
> I have a 3-column numeric matrix, so colum 3 should be plotted as a point 
> above the 1st and 2nd column co-ordinates.

You are misleading yourself and us by selective quoting: it says

          As an extension to partially support the form used in
          `filled.contour' and `image', `formula' can be a matrix. 

The form used in those quoted function is *not* a three-column 
matrix, and your interpretation is not in the R help file.

> No plot is produced however, just a blank graphics device.

Not so in my experiments: a border box is drawn.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jun 23 08:58:38 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jun 2003 08:58:38 +0200
Subject: [R] "cloud" function doesn't plot
In-Reply-To: <BAY8-F27P3D6g5dwDAx00001933@hotmail.com>
References: <BAY8-F27P3D6g5dwDAx00001933@hotmail.com>
Message-ID: <3EF6A51E.6060804@statistik.uni-dortmund.de>

Tony Ross wrote:

> 
> 
> 
>> From: "Simon Blomberg" <Simon.Blomberg at anu.edu.au>
>> Date: Mon, 23 Jun 2003 16:26:39 +1000
> 
> 
>> If you type ?cloud, you can see that you need to provide a formula as 
>> well as a dataset. So if your variables are  x,y,z in dataframe dat, 
>> then cloud(z~x*y, data=dat) should work.
> 
> [...]
> 
>>> is.matrix(sanity.MIF); is.numeric(sanity.MIF); dim(sanity.MIF); 
>>> cloud(sanity.MIF)
>>
>> [1] TRUE
>> [1] TRUE
>> [1] 2970    3
>>
>>>
> 
> The "help(cloud)" facility states: "`formula' can be a matrix".

It states completely:
"As an extension to partially support the form used in filled.contour 
and image, formula can be a matrix."
Thus, see ?image that you don't need a 3 column matrix, but a matrix 
containing the z values.

I'd suggest to convert the matrix to a data.frame and follow the 
suggestion from Simon Blomberg.

Uwe Ligges



> As I 
> stated, I have a 3-column numeric matrix, so colum 3 should be plotted 
> as a point above the 1st and 2nd column co-ordinates.
 >
> No plot is produced however, just a blank graphics device.

Uwe Ligges



From ozric at web.de  Mon Jun 23 09:12:37 2003
From: ozric at web.de (Christian Schulz)
Date: Mon, 23 Jun 2003 09:12:37 +0200
Subject: [R] table in x-y diagram
References: <003f01c338d3$4ce1fec0$d200a8c0@pc>
	<3EF631ED.5070901@stat.auckland.ac.nz>
Message-ID: <003401c33956$e3ebadb0$d808ebd9@pc>

Great, and many thanks for all suggestions,
now i'm trying your suggestions.

But Paul, here an example,too for my table :

I draw circle's, which are located
for Y ~ V61 and X ~ V409.
Further i like to control the size
of the circle dependent from
"Number of Persons" in Cluster X.

grid.points(cl80m[,2],cl80m[,4],size=unit(table(cl80[,10])/350,"inches"))

And finally add a table with any means for the clusters like this. 

>>cl80m
Cluster  V61 V304  V409  V17
1 cluster1 1.00 2.16  6.09 6.24
2 cluster2 2.29 2.00  5.69 5.79
3 cluster3 2.00 1.00  6.33 5.67
4 cluster4 2.84 5.00  8.94 5.27
5 cluster5 3.00 2.77  8.18 5.62
6 cluster6 1.00 4.00 14.00 6.00
7 cluster7 4.00 2.56  7.01 5.04
8 cluster8 2.00 2.97  8.55 6.09


Many thanks & regards,
Christian



From ynotssor at hotmail.com  Mon Jun 23 09:27:09 2003
From: ynotssor at hotmail.com (Tony Ross)
Date: Mon, 23 Jun 2003 00:27:09 -0700
Subject: [R] "cloud" function doesn't plot
Message-ID: <BAY8-F14RPmcarcR4ZD00009a42@hotmail.com>




>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>Date: Mon, 23 Jun 2003 08:58:38 +0200


>It states completely:
>"As an extension to partially support the form used in filled.contour and 
>image, formula can be a matrix."
>Thus, see ?image that you don't need a 3 column matrix, but a matrix 
>containing the z values.
>
>I'd suggest to convert the matrix to a data.frame and follow the suggestion 
>from Simon Blomberg.

Thank you, I didn't properly appreciate the need for a data.frame.

The cloud() now plots as expected.

Thank You everyone.



From ramzi_feg at yahoo.fr  Mon Jun 23 10:45:57 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Mon, 23 Jun 2003 10:45:57 +0200 (CEST)
Subject: [R] Problem with installation under R 1.7.1 and unzip
Message-ID: <20030623084557.86069.qmail@web20305.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030623/d6f81f50/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 23 11:04:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 10:04:39 +0100 (BST)
Subject: [R] Problem with installation under R 1.7.1 and unzip
In-Reply-To: <20030623084557.86069.qmail@web20305.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306230952420.19967-100000@gannet.stats>

On Mon, 23 Jun 2003, Ramzi Feghali wrote:

> Dear all,
>  
> i was working under R 1.6.2 but i wanted to install bioconductor under WinXP, and because R 1.7.1 contains the TclTk package and automatic installation of bioconductor packages, so i install it. 
>  
> Everything is fine except this message which appear after the installation of packages.
>  
> "> local({a<- CRAN.packages(CRAN=getOption("BIOC"))
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a, CRAN=getOption("BIOC"))})
> trying URL `http://www.bioconductor.org/bin/windows/contrib/1.7/PACKAGES'
> Content type `text/plain' length 11255 bytes
> opened URL
> downloaded 10Kb
> trying URL `http://www.bioconductor.org/bin/windows/contrib/1.7/Biobase_1.3.22.zip'
> Content type `application/zip' length 444272 bytes
> opened URL
> downloaded 433Kb
> Error in int.unzip(zipname, NULL, dest) : destination does not exist"

>  the function int.unzip doesn't exist in R 

Yes, it does.  As it name suggests, it is an internal function, called via 
.Internal, in this case as part of zip.unpack.

> and i don't know which
> destination must be specified to unzip packages and install it. In fact
> i am doing this manually now and it works using "Load package" but i
> won't do this always because i have the same problem in updating these
> packages. 

> My destination directory is Program Files\\R\\rw1071\\Rwork.
> Could anyone tell me what to do and how to resolve this problem?

How do you know that? (It almost certainly is not: try doing a 
traceback() to find out.)


My suggestion is that you re-install R in a directory without any spaces 
in the path, as that is likely to be the problem.  Alternatively, you 
could install unzip.exe and set options("unzip").

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From james.lindsey at luc.ac.be  Mon Jun 23 11:13:06 2003
From: james.lindsey at luc.ac.be (Jim Lindsey)
Date: Mon, 23 Jun 2003 11:13:06 +0200 (MET DST)
Subject: [R] glmm and overall goodness of fit
In-Reply-To: <200306201031.32759.chrysopa@insecta.ufv.br> from "Ronaldo Reis
	Jr." at Jun 20, 2003 02:26:36 PM
Message-ID: <200306230913.LAA19457@luc.ac.be>

> 
> Hi,
> 
> exist in R any glmm function that have any tools for test for overall goodness 
> of fit? 

True measures of overall goodness of fit may be difficult to formulate for
such mixed models. Relative goodness of fit (as compared to glm) is
available through the AIC produced by my glmm function.
  Cheers, Jim

> 
> Thanks
> Ronaldo
> -- 
> O papel da impressora ? sempre mais forte na parte picotada.
> --
> |   // | \\   [***********************************]
> |> ( ?   ? )  [Ronaldo Reis J?nior                ]
> |      V      [UFV/DBA-Entomologia                ]
> |>  /     \   [36571-000 Vi?osa - MG              ]
> |  /(.''`.)\  [Fone: 31-3899-2532                 ]
> |>/(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |>  ( `-  )   [***********************************]
> |>> _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From lsilva at fc.up.pt  Mon Jun 23 11:42:30 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Mon, 23 Jun 2003 10:42:30 +0100
Subject: [R] dispersion matrices
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEA77@MAIL.fc.up.pt>

Hello
 
Does R have any function that calculates from a data matrix with several classes the between and within dispersion matrices S_b and S_w
 
Thanks
 
Luis



From ramzi_feg at yahoo.fr  Mon Jun 23 11:43:19 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Mon, 23 Jun 2003 11:43:19 +0200 (CEST)
Subject: [R] Problem with installation under R 1.7.1 and unzip
Message-ID: <20030623094319.93113.qmail@web20305.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030623/77ca28ea/attachment.pl

From s-plus at wiwi.uni-bielefeld.de  Mon Jun 23 14:23:38 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 23 Jun 2003 14:23:38 +0200
Subject: [R] How can I do a spinning plot in R?
References: <200306230114.h5N1EJ6O467231@atlas.otago.ac.nz>
Message-ID: <3EF6F14A.9040708@wiwi.uni-bielefeld.de>

Some days ago I wrote a function for rotating
a cloud of points. To control the rotation a simple
Tcl/Tk-widget is used. Here you can find the
R code:

 http://www.wiwi.uni-bielefeld.de/~wolf/software/spin3R/spin3R.sch

Peter Wolf, pwolf at wiwi.uni-bielefeld.de


Richard A. O'Keefe wrote:

>I have found XLispStat's spinning plots illuminating.
>I'd like to do the same thing in R.
>A dozen or so probes with help, help.search, apropos
>haven't turned up anything, and I've even resorted to
>grepping through the entire R source distribution
>looking for 'spin.*plot', to no avail.
>
>Either the feature is called something else in R (what?),
>or it's in some other package in CRAN (which?),
>or it's not yet available (I hope not, because I am very
>far from being skilled enough in R programming to do it
>myself).
>
>Help?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From cuffer at maths.tcd.ie  Mon Jun 23 15:45:41 2003
From: cuffer at maths.tcd.ie (Robert Cuffe)
Date: Mon, 23 Jun 2003 14:45:41 +0100
Subject: [R] precision matrix for polynomial growth curves
Message-ID: <20030623144541.A7241@stokes.maths.tcd.ie>

What does the warning message 
"1: Singular precision matrix in level -1, block 1" mean?

I get this warning 50+ times when I try to fit the following
model

lme( response ~ covariateA + poly(covariateB,3), ~poly(covariateB,3)|group )

It's not a small dataset - a set of up to 20 blood pressure
readings on just over 2000 people, and I don't get the error
message when I try to fit fewer random effects (although I'm
not sure whether fitting random effects poly(B,2) with fixed
effects poly(B,3) is generally the same as fitting random
effects B+B^2 with fixed effects B+B^2+B^3).

Sometimes I don't get the error message when I use subsets of
the data, but that isn't guaranteed to be the case.

If anybody could give me pointers to references for or
could answer either question themselves (1. why the error
message/what's causing it and 2. fitting random polynomials
using poly()) I would be very grateful.



From ernesto at ipimar.pt  Mon Jun 23 16:02:23 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Mon, 23 Jun 2003 14:02:23 -0000
Subject: [R] Changing lattice theme
Message-ID: <1056376934.1239.25.camel@gandalf.ipimar.pt>

Hi

Can someone tell me how I can change the lattice theme to black & white
?

Thanks

EJ

-- 
Ernesto Jardim <ernesto at ipimar.pt>
Marine Biologist
IPIMAR, Lisboa, Portugal
SuSE Linux 8.1;R 1.7.0;LyX 1.3.2;Gnome 2.2.2;OO1.1Beta



From Philippe.Hupe at curie.fr  Mon Jun 23 16:06:27 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Mon, 23 Jun 2003 16:06:27 +0200
Subject: [R] Rcmd check problem
In-Reply-To: <3EF6F14A.9040708@wiwi.uni-bielefeld.de>
References: <200306230114.h5N1EJ6O467231@atlas.otago.ac.nz>
	<3EF6F14A.9040708@wiwi.uni-bielefeld.de>
Message-ID: <3EF70963.6090609@curie.fr>

I have the following message error when I try to buid my package :

C:\Documents and Settings\Philippe>Rcmd check mypackage
* checking for working latex ...Error: environment variable TMPDIR not 
set  (or set to unusable value) and no default available.
 at C:\PROGRA~1\R\rw1071\share\perl/R/Utils.pm line 165

Does someone have any idea ?

Thanks

OS Windows XP
miktex
R 1.7.1
ActivePerle 5.8.0



From ligges at statistik.uni-dortmund.de  Mon Jun 23 16:10:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jun 2003 16:10:37 +0200
Subject: [R] Changing lattice theme
In-Reply-To: <1056376934.1239.25.camel@gandalf.ipimar.pt>
References: <1056376934.1239.25.camel@gandalf.ipimar.pt>
Message-ID: <3EF70A5D.2050605@statistik.uni-dortmund.de>

Ernesto Jardim wrote:
> Hi
> 
> Can someone tell me how I can change the lattice theme to black & white
> ?

See, e.g., ?trellis.device.

Uwe Ligges

> Thanks
> 
> EJ
>



From L.T.Kell at cefas.co.uk  Mon Jun 23 16:18:29 2003
From: L.T.Kell at cefas.co.uk (Laurence Kell FM CEFAS)
Date: Mon, 23 Jun 2003 15:18:29 +0100
Subject: [R] FW: S4 classes, creating in C
Message-ID: <914B18DCA0ADD511ABE000508BF339FBEDC5F0@LOWEXPRESS2.corp.cefas.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030623/67e30f56/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jun 23 16:22:57 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jun 2003 16:22:57 +0200
Subject: [R] Rcmd check problem
In-Reply-To: <3EF70963.6090609@curie.fr>
References: <200306230114.h5N1EJ6O467231@atlas.otago.ac.nz>	<3EF6F14A.9040708@wiwi.uni-bielefeld.de>
	<3EF70963.6090609@curie.fr>
Message-ID: <3EF70D41.8070405@statistik.uni-dortmund.de>

Philippe Hup? wrote:

> I have the following message error when I try to buid my package :
> 
> C:\Documents and Settings\Philippe>Rcmd check mypackage
> * checking for working latex ...Error: environment variable TMPDIR not 
> set  (or set to unusable value) and no default available.
> at C:\PROGRA~1\R\rw1071\share\perl/R/Utils.pm line 165
> 
> Does someone have any idea ?

Yes: Read the documentation and the error message.

 From .../src/gnuwin32/readme.packages:
"You may need to set TMPDIR to (the absolute path to) a suitable
temporary directory: the default is c:/TEMP.  (Use forward slashes,
although the code will try to convert backslashes as from 1.7.0.)"

Uwe Ligges



> Thanks
> 
> OS Windows XP
> miktex
> R 1.7.1
> ActivePerle 5.8.0
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Bernhard.Pfaff at drkw.com  Mon Jun 23 16:25:50 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 23 Jun 2003 16:25:50 +0200
Subject: [R] Rcmd check problem
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730441@ibfftce505.is.de.dresdnerkb.com>

> 
> I have the following message error when I try to buid my package :
> 
> C:\Documents and Settings\Philippe>Rcmd check mypackage
> * checking for working latex ...Error: environment variable 
> TMPDIR not 
> set  (or set to unusable value) and no default available.
>  at C:\PROGRA~1\R\rw1071\share\perl/R/Utils.pm line 165



Hello Philippe,

just a guess...have you included your miktex-path as an environmental
variable in your path (see:START -> Settings -> Control Panel -> System
Properties -> Advanced -> Environment Variables) and a tempdir likewise? 

HTH,
Bernhard
> 
> Does someone have any idea ?
> 
> Thanks
> 
> OS Windows XP
> miktex
> R 1.7.1
> ActivePerle 5.8.0
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From tblackw at umich.edu  Mon Jun 23 16:30:31 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 23 Jun 2003 10:30:31 -0400 (EDT)
Subject: [R] precision matrix for polynomial growth curves
In-Reply-To: <20030623144541.A7241@stokes.maths.tcd.ie>
Message-ID: <Pine.SOL.4.44.0306231015220.1001-100000@rygar.gpcc.itd.umich.edu>

Robert  -

Just guessing, does the growth curve for any individual have
exactly four points ?  If so, then the fit with an order 3
polynomial would be exact, and have zero residual variance,
while the fit with an order 2 polynomial would likely still
have non-zero residuals.

Another way the same thing might happen:  Could any of the
growth curves in the data set have been interpolated using
a third order polynomial in covariateB ?  This, too, would
produce zero residuals in a subsequent third order polynomial
fit, even if there are many more than four points in the curve.

Note that these are just guesses, without knowing anything
about the innards of  lme().

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 23 Jun 2003, Robert Cuffe wrote:

> What does the warning message
> "1: Singular precision matrix in level -1, block 1" mean?
>
> I get this warning 50+ times when I try to fit the following
> model
>
> lme( response ~ covariateA + poly(covariateB,3), ~poly(covariateB,3)|group )
>
> It's not a small dataset - a set of up to 20 blood pressure
> readings on just over 2000 people, and I don't get the error
> message when I try to fit fewer random effects (although I'm
> not sure whether fitting random effects poly(B,2) with fixed
> effects poly(B,3) is generally the same as fitting random
> effects B+B^2 with fixed effects B+B^2+B^3).
>
> Sometimes I don't get the error message when I use subsets of
> the data, but that isn't guaranteed to be the case.
>
> If anybody could give me pointers to references for or
> could answer either question themselves (1. why the error
> message/what's causing it and 2. fitting random polynomials
> using poly()) I would be very grateful.



From hdoran at nasdc.org  Mon Jun 23 16:46:40 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Mon, 23 Jun 2003 10:46:40 -0400
Subject: [R] R Commander
Message-ID: <66578BFC0BA55348B5907A0F798EE93029E7CA@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030623/a3511b61/attachment.pl

From oliver at imcs.rutgers.edu  Mon Jun 23 17:07:15 2003
From: oliver at imcs.rutgers.edu (Matt Oliver)
Date: Mon, 23 Jun 2003 11:07:15 -0400
Subject: [R] saving plots
Message-ID: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>

Hi all, I am have an R script that makes many plots, and I would like to 
string them into a movie file. To do that I have to save all of my plots as 
jpegs or gifs. I don't want to go through all of my plots and save them by 
hand though. Is there a way to automate saving the plots as a gif or a jpeg 
so I can string them into a movie?

Thanks in advance

Matt Oliver

==============================
When you reach an equilibrium in biology,
you're dead. - A. Mandell
==============================
Matthew J. Oliver
Institute of Marine and Coastal Sciences
71 Dudley Road, New Brunswick
New Jersey, 08901
http://marine.rutgers.edu/cool/



From ligges at statistik.uni-dortmund.de  Mon Jun 23 17:12:02 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jun 2003 17:12:02 +0200
Subject: [R] saving plots
In-Reply-To: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
References: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
Message-ID: <3EF718C2.9040901@statistik.uni-dortmund.de>

Matt Oliver wrote:

> Hi all, I am have an R script that makes many plots, and I would like to 
> string them into a movie file. To do that I have to save all of my plots 
> as jpegs or gifs. I don't want to go through all of my plots and save 
> them by hand though. Is there a way to automate saving the plots as a 
> gif or a jpeg so I can string them into a movie?

See ?Devices.
You might have a jpeg() Device available (for sure on Windows).

Uwe Ligges


> Thanks in advance
> 
> Matt Oliver
> 
> ==============================
> When you reach an equilibrium in biology,
> you're dead. - A. Mandell
> ==============================
> Matthew J. Oliver
> Institute of Marine and Coastal Sciences
> 71 Dudley Road, New Brunswick
> New Jersey, 08901
> http://marine.rutgers.edu/cool/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Timur.Elzhov at jinr.ru  Mon Jun 23 17:29:43 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 23 Jun 2003 19:29:43 +0400
Subject: [R] Refreshing windows contents with XFree 3.3.6 & 4.1.0
Message-ID: <20030623152942.GA15906@pcf004.jinr.ru>

Dear R experts.

I was using R on Debian GNU/Linux (woody) with XFree86 4.1.0.
I plotted a lot of graphs on the separate canvas, and, after
arising any other R x11() window with mouse, it didn't refresh
its content immediately, but on the next clicking it did.

I replaced XFree86 server 4.1.0 version with 3.3.6 one, and that
removed the problem - now R windows refresh their contents
immediately after being arised up.

Is R or Xfree server responsible of that?


[I reproduced this problem on the work and home computers,
 with Matrox Mystique and S3 Trio3D/2X video cards]


Thank you very much.

--
WBR,
Timur



From jfox at mcmail.cis.mcmaster.ca  Mon Jun 23 17:30:13 2003
From: jfox at mcmail.cis.mcmaster.ca (John Fox)
Date: Mon, 23 Jun 2003 11:30:13 -0400 (EDT)
Subject: [R] R Commander
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93029E7CA@ernesto.NASDC.ORG>
Message-ID: <Pine.SOL.4.33.0306231125060.29766-100000@mcmail.cis.mcmaster.ca>

Dear Harold,

It's odd that something that worked previously no longer works, so my
first thought is to ask what's changed?

Second, I'd determine whether the problem is in the Rcmdr package or in
the read.spss function in the foreign package, which Rcmdr calls. Can you
import the data properly using read.spss directly?

Third, since "parse error" is a vague message, you might be able to get
some more information from traceback() about where there error is
occurring.

Fourth, does this problem always occur now when you try to import an SPSS
data set or only with a particular data set?

I hope that this helps. Please let me know what happens.

John

On Mon, 23 Jun 2003, Harold Doran wrote:

> I am trying to import a file using R Commander. It was working a few days ago, but now I get the following message when I try to import from SPSS. Any thoughts?
>
> Error in parse(file, n, text, prompt) : parse error
>
> ------
> Harold C. Doran
> Director of Research and Evaluation
> New American Schools
> 675 N. Washington Street, Suite 220
> Alexandria, Virginia 22314
> 703.647.1628
>  <http://www.edperform.net/>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From ernesto at ipimar.pt  Mon Jun 23 17:37:39 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Mon, 23 Jun 2003 15:37:39 -0000
Subject: [R] Changing lattice theme
In-Reply-To: <3EF70A5D.2050605@statistik.uni-dortmund.de>
References: <1056376934.1239.25.camel@gandalf.ipimar.pt>
	<3EF70A5D.2050605@statistik.uni-dortmund.de>
Message-ID: <1056382572.1239.28.camel@gandalf.ipimar.pt>

On Mon, 2003-06-23 at 15:10, Uwe Ligges wrote:
> Ernesto Jardim wrote:
> > Hi
> > 
> > Can someone tell me how I can change the lattice theme to black & white
> > ?
> 
> See, e.g., ?trellis.device.
> 
> Uwe Ligges
> 
> > Thanks
> > 
> > EJ
> > 
> 

Hi

I end up using 

lattice.device(device="win.metafile", color=FALSE, file="fig.wmf")

to produce black and white plots in M$Windows.

Thanks

EJ



From bates at stat.wisc.edu  Mon Jun 23 17:38:05 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 23 Jun 2003 15:38:05 -0000
Subject: [R] FW: S4 classes, creating in C
In-Reply-To: <914B18DCA0ADD511ABE000508BF339FBEDC5F0@LOWEXPRESS2.corp.cefas.co.uk>
References: <914B18DCA0ADD511ABE000508BF339FBEDC5F0@LOWEXPRESS2.corp.cefas.co.uk>
Message-ID: <6r65mwkdny.fsf@bates4.stat.wisc.edu>

Laurence Kell FM CEFAS <L.T.Kell at cefas.co.uk> writes:

>  
>  I am using C code to create an S4 object based on Douglas Bates's example
> in his lecture notes on
> <http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Tutorials/RExtensions/slide
> s.pdf>
> http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Tutorials/RExtensions/slides
> .pdf
> 
> e.g.
> 
> SEXP La_DGE_dc(SEXP A)
>         {
>         SEXP aa = PROTECT(duplicate(A));
>         SEXP adims, pivot, val;
>         int m, n, info;
>        
>         if (!isMatrix(aa) || !isReal(aa)) {
>                 error("A must be a double precision matrix");
>                 }
> 
>         adims = GET_DIM(aa); m = INTEGER(adims)[0]; n = INTEGER(adims)[1];
>         pivot = PROTECT(NEW_INTEGER(m < n ? m : n));
> 
>         F77_CALL(dgetrf)(&m, &n, REAL(aa), &m, INTEGER(pivot), &info);
>        
>         check_Lapack_error(info, "dtrtrf");
>        
>         val = PROTECT(NEW_OBJECT(MAKE_CLASS("LUdecomposition")));
> 
>         SET_SLOT(val, install("a"), aa);
>         SET_SLOT(val, install("pivot"), pivot);
>        
>         UNPROTECT(3);
>         return val;
>         }
> 
> LUdecomposition is an S4 class defined as
> 
> setClass("LUdecomposition", representation(a="matrix", pivot = "integer"))
> 
> This works in R 1.7.0 and R 1.7.1 but if I initialise any of the slots
> 
> setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
>                             prototype=list(pivot = NA)                    )
> 
> then I get the following error message in R 1.7.1 but not R 1.7.0
> 
> Error in makePrototypeFromClassDef(properties, ClassDef, immediate) :
>         In making the prototype for class "LUdecomposition" elements of the
> prototype
>         failed to match the corresponding slot class: pivot (class " integer
> ")
> 
> Why  can I no longer use the prototype to set the default values?

I'm not sure.  This may be the type of question that requires John
Chambers' attention and I think he is away from his email this week.



From Timur.Elzhov at jinr.ru  Mon Jun 23 17:50:27 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 23 Jun 2003 19:50:27 +0400
Subject: [R] saving plots
In-Reply-To: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
References: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
Message-ID: <20030623155026.GA15975@pcf004.jinr.ru>

On Mon, Jun 23, 2003 at 11:07:15AM -0400, Matt Oliver wrote:

> Hi all, I am have an R script that makes many plots, and I would like to 
> string them into a movie file. To do that I have to save all of my plots as 
> jpegs or gifs. I don't want to go through all of my plots and save them by 
> hand though. Is there a way to automate saving the plots as a gif or a jpeg 
> so I can string them into a movie?
?dev.copy is probably that you want.

dev.copy(device = jpeg)
dev.off()

--
WBR,
Timur



From hodgess at uhddx01.dt.uh.edu  Mon Jun 23 17:50:47 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Mon, 23 Jun 2003 10:50:47 -0500 (CDT)
Subject: [R] mode of a data set
Message-ID: <200306231550.KAA07642@uhddx01.dt.uh.edu>

Dear R People:

Is there a function to find the mode of a data set, please?

This is the mode as in the value(s) which occur most often.

Thanks so much!

R for Windows, v 1.7.0

Sincerely,
Erin Hodgess
mailto: hodgess at uhddx.01.dt.uh.edu



From duncan at research.bell-labs.com  Mon Jun 23 17:53:46 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Mon, 23 Jun 2003 11:53:46 -0400
Subject: [R] FW: S4 classes, creating in C
In-Reply-To: <6r65mwkdny.fsf@bates4.stat.wisc.edu>;
	from bates@stat.wisc.edu on Mon, Jun 23, 2003 at 10:37:53AM -0500
References: <914B18DCA0ADD511ABE000508BF339FBEDC5F0@LOWEXPRESS2.corp.cefas.co.uk>
	<6r65mwkdny.fsf@bates4.stat.wisc.edu>
Message-ID: <20030623115346.C28506@jessie.research.bell-labs.com>



I don't believe the fact that you are doing this in C code is relevant
in this problem. 

If you define LUdecomposition as
 setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
                             prototype=list(pivot = NA)                    )


you will get an error if you simply type

  new("LUdecomposition")


To make this work,  define the class as

 setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
                             prototype=list(pivot = as.integer(NA)                    ))

Why the original does not work is because 
  typeof(NA) 
returns "logical".
So it is merely the type of the value that is the problem.

 HTH,
  Duncan


Douglas Bates wrote:
> Laurence Kell FM CEFAS <L.T.Kell at cefas.co.uk> writes:
> 
> >  
> >  I am using C code to create an S4 object based on Douglas Bates's example
> > in his lecture notes on
> > <http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Tutorials/RExtensions/slide
> > s.pdf>
> > http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Tutorials/RExtensions/slides
> > .pdf
> > 
> > e.g.
> > 
> > SEXP La_DGE_dc(SEXP A)
> >         {
> >         SEXP aa = PROTECT(duplicate(A));
> >         SEXP adims, pivot, val;
> >         int m, n, info;
> >        
> >         if (!isMatrix(aa) || !isReal(aa)) {
> >                 error("A must be a double precision matrix");
> >                 }
> > 
> >         adims = GET_DIM(aa); m = INTEGER(adims)[0]; n = INTEGER(adims)[1];
> >         pivot = PROTECT(NEW_INTEGER(m < n ? m : n));
> > 
> >         F77_CALL(dgetrf)(&m, &n, REAL(aa), &m, INTEGER(pivot), &info);
> >        
> >         check_Lapack_error(info, "dtrtrf");
> >        
> >         val = PROTECT(NEW_OBJECT(MAKE_CLASS("LUdecomposition")));
> > 
> >         SET_SLOT(val, install("a"), aa);
> >         SET_SLOT(val, install("pivot"), pivot);
> >        
> >         UNPROTECT(3);
> >         return val;
> >         }
> > 
> > LUdecomposition is an S4 class defined as
> > 
> > setClass("LUdecomposition", representation(a="matrix", pivot = "integer"))
> > 
> > This works in R 1.7.0 and R 1.7.1 but if I initialise any of the slots
> > 
> > setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
> >                             prototype=list(pivot = NA)                    )
> > 
> > then I get the following error message in R 1.7.1 but not R 1.7.0
> > 
> > Error in makePrototypeFromClassDef(properties, ClassDef, immediate) :
> >         In making the prototype for class "LUdecomposition" elements of the
> > prototype
> >         failed to match the corresponding slot class: pivot (class " integer
> > ")
> > 
> > Why  can I no longer use the prototype to set the default values?
> 
> I'm not sure.  This may be the type of question that requires John
> Chambers' attention and I think he is away from his email this week.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From azzalini at stat.unipd.it  Mon Jun 23 18:01:07 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 23 Jun 2003 18:01:07 +0200
Subject: [R] mode of a data set
In-Reply-To: <200306231550.KAA07642@uhddx01.dt.uh.edu>
References: <200306231550.KAA07642@uhddx01.dt.uh.edu>
Message-ID: <20030623160108.3634B7CA824@tango.stat.unipd.it>

On Monday 23 June 2003 17:50, Erin Hodgess wrote:
> Dear R People:
>
> Is there a function to find the mode of a data set, please?
>
> This is the mode as in the value(s) which occur most often.
>

 x[rev(order(table(x)))[1]]

is this what you want?

regards
Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From sundar.dorai-raj at pdf.com  Mon Jun 23 18:02:38 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 23 Jun 2003 11:02:38 -0500
Subject: [R] mode of a data set
References: <200306231550.KAA07642@uhddx01.dt.uh.edu>
Message-ID: <3EF7249E.3050303@pdf.com>


Erin Hodgess wrote:
> Dear R People:
> 
> Is there a function to find the mode of a data set, please?
> 
> This is the mode as in the value(s) which occur most often.
> 

Erin,
   Will this work for you?

R> x <- rbinom(100, size = 20, prob = .2)
R> table.x <- table(x)
R> table.x[which.max(table.x)]
  4
23
R>

Regards,
Sundar



From lockwood at rand.org  Mon Jun 23 18:05:57 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Mon, 23 Jun 2003 12:05:57 -0400 (EDT)
Subject: [R] mode of a data set
In-Reply-To: <200306231550.KAA07642@uhddx01.dt.uh.edu>
Message-ID: <Pine.LNX.4.33.0306231204470.12665-100000@penguin.rand.org>

Dear Erin,
Assuming that by "data set" you mean a vector "v", then

sort(table(v))

will give you what you want.

On Mon, 23 Jun 2003, Erin Hodgess wrote:

> Date: Mon, 23 Jun 2003 10:50:47 -0500 (CDT)
> From: Erin Hodgess <hodgess at uhddx01.dt.uh.edu>
> To: r-help at stat.math.ethz.ch
> Subject: [R] mode of a data set
> 
> Dear R People:
> 
> Is there a function to find the mode of a data set, please?
> 
> This is the mode as in the value(s) which occur most often.
> 
> Thanks so much!
> 
> R for Windows, v 1.7.0
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at uhddx.01.dt.uh.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From hdoran at nasdc.org  Mon Jun 23 18:15:41 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Mon, 23 Jun 2003 12:15:41 -0400
Subject: [R] read.spss
Message-ID: <66578BFC0BA55348B5907A0F798EE93029E7CD@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030623/fffaae3b/attachment.pl

From MSchwartz at medanalytics.com  Mon Jun 23 18:35:35 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 23 Jun 2003 16:35:35 -0000
Subject: [R] read.spss
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93029E7CD@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE93029E7CD@ernesto.NASDC.ORG>
Message-ID: <1056386122.4195.3.camel@localhost>

On Mon, 2003-06-23 at 11:15, Harold Doran wrote:
> I have loaded the foreign package and am still having problems with an
> import. I get a message that reads, unable to open file. Whe I try
> different files I get the same message. Here is the code I used. Am I
> missing something?
>  
> I am using 1.7 and have also tried this in 1.6 with the same problem.
>  
> hsb<-read.spss("C:\HLM504_Student\Examples\AppendxA\HSB1.SAV",
> use.value.labels=TRUE, to.data.frame=FALSE, max.value.labels=Inf)
>  


Running under Windows, you need to "escape" the backslashes, so you need
to use double backslashes:

hsb<-read.spss("C:\\HLM504_Student\\Examples\\AppendxA\\HSB1.SAV",
use.value.labels = TRUE, to.data.frame = FALSE, max.value.labels = Inf)

This is in the Windows FAQ 2.14.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Mon Jun 23 18:35:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 17:35:52 +0100 (BST)
Subject: [R] read.spss
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93029E7CD@ernesto.NASDC.ORG>
Message-ID: <Pine.LNX.4.44.0306231733310.28446-100000@gannet.stats>

The message is quite correct. See the rw-FAQ, Q2.14, or the FAQ Q7.10.

On Mon, 23 Jun 2003, Harold Doran wrote:

> I have loaded the foreign package and am still having problems with an
> import. I get a message that reads, unable to open file. Whe I try
> different files I get the same message. Here is the code I used. Am I
> missing something?

The FAQs!

> I am using 1.7 and have also tried this in 1.6 with the same problem.
>  hsb<-read.spss("C:\HLM504_Student\Examples\AppendxA\HSB1.SAV",
 use.value.labels=TRUE, to.data.frame=FALSE, max.value.labels=Inf)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hodgess at uhddx01.dt.uh.edu  Mon Jun 23 18:39:57 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Mon, 23 Jun 2003 11:39:57 -0500 (CDT)
Subject: [R] Summary for mode of a data set
Message-ID: <200306231639.LAA03733@uhddx01.dt.uh.edu>

Dear R People:

thank you for the many helpful sets of code that I received!!!

I combined several of the concepts for the following function:

> mode1

function(x) {

        y <- rle(sort(x))

        z <- y$values[y$lengths==max(y$lengths)]

        return(z)

}

> xm

 [1] 22 15 10 30 25 26  2 17 28  2 24  6 26 24  5 22 20 14

> mode1(xm)

[1]  2 22 24 26

> 

This will pick up multiple modes.

Again thanks to all who helped!

Sincerely,
Erin 
mailto: hodgess at uhddx.01.dt.uh.edu



From macq at llnl.gov  Mon Jun 23 19:32:14 2003
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 23 Jun 2003 10:32:14 -0700
Subject: [R] saving plots
In-Reply-To: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
References: <5.0.0.25.2.20030623110450.026b34b0@imcs.rutgers.edu>
Message-ID: <p05210606bb1ce9bd6876@[128.115.153.6]>

You might want to look at recordPlot().

While the script is running record the plots using recordPlot(). 
Afterwards replay them with the jpeg device active.

-Don

At 11:07 AM -0400 6/23/03, Matt Oliver wrote:
>Hi all, I am have an R script that makes many plots, and I would 
>like to string them into a movie file. To do that I have to save all 
>of my plots as jpegs or gifs. I don't want to go through all of my 
>plots and save them by hand though. Is there a way to automate 
>saving the plots as a gif or a jpeg so I can string them into a 
>movie?
>
>Thanks in advance
>
>Matt Oliver
>
>==============================
>When you reach an equilibrium in biology,
>you're dead. - A. Mandell
>==============================
>Matthew J. Oliver
>Institute of Marine and Coastal Sciences
>71 Dudley Road, New Brunswick
>New Jersey, 08901
>http://marine.rutgers.edu/cool/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ml-r-help at epigenomics.com  Mon Jun 23 19:38:57 2003
From: ml-r-help at epigenomics.com (Matthias Burger)
Date: Mon, 23 Jun 2003 19:38:57 +0200
Subject: [R] FW: S4 classes, creating in C
References: <914B18DCA0ADD511ABE000508BF339FBEDC5F0@LOWEXPRESS2.corp.cefas.co.uk>
Message-ID: <news2mail-3EF73B31.5020708@epigenomics.com>


Hi Laurence,




Laurence Kell FM CEFAS wrote:
>  
[...]
> LUdecomposition is an S4 class defined as
> 
> setClass("LUdecomposition", representation(a="matrix", pivot = "integer"))
> 
> This works in R 1.7.0 and R 1.7.1 but if I initialise any of the slots
> 
> setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
>                             prototype=list(pivot = NA)                    )
> 
> then I get the following error message in R 1.7.1 but not R 1.7.0
> 
> Error in makePrototypeFromClassDef(properties, ClassDef, immediate) :
>         In making the prototype for class "LUdecomposition" elements of the
> prototype
>         failed to match the corresponding slot class: pivot (class " integer
> ")
> 
> Why  can I no longer use the prototype to set the default values?

just tried your example in R 1.6.2 and it worked. And as you say for R 1.7.1 it 
won't.

So take a look at (R 1.7.1)

 > setClass("LUdecompositio", representation(a="matrix", pivot = "integer"), 
prototype(pivot=NA))
[1] "LUdecompositio"

 > lu<-new("LUdecompositio")
Error in makePrototypeFromClassDef(properties, ClassDef, immediate) :
         In making the prototype for class "LUdecompositio" elements of the 
prototype failed to match the corresponding slot class: pivot (class " integer ")

 > setClass("LUdecompositi", representation(a="matrix", pivot = "integer"), 
prototype(pivot=as.integer(NA))
+
+ )
[1] "LUdecompositi"

 > lu3<-new("LUdecompositi")
 > lu3
An object of class "LUdecompositi"
Slot "a":
<0 x 0 matrix>

Slot "pivot":
[1] NA

 > is(lu3 at pivot)
[1] "integer" "vector"  "numeric"

So providing an explicit cast in the prototype cures the problem.

HTH,

   Matthias



> Laurence Kell
> CEFAS Lowestoft Laboratory
> Pakefield Road
> Lowestoft, NR33 0HT
> UK
> Tel +44 1502 52 42 57
> Fax +44 1502 52 45 11
> e-mail l.t.kell at cefas.co.uk
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 
Matthias Burger

Bioinformatics R&D
Epigenomics AG                      www.epigenomics.com
Kleine Pr?sidentenstra?e 1          fax:   +49-30-24345-555
10178 Berlin Germany                phone: +49-30-24345-0



From ekr at rtfm.com  Mon Jun 23 20:51:40 2003
From: ekr at rtfm.com (Eric Rescorla)
Date: Mon, 23 Jun 2003 11:51:40 -0700
Subject: [R] Reliability analysis and Laplace factor functions
Message-ID: <20030623185140.CF91B73D6@sierra.rtfm.com>

Is there some package out there that implements functions
for reliability analysis, especially for software reliability?

In particular, I'm looking for:

* Laplace factor (Cox & Lewis 1978)
* Goel-Okumoto fitting

Thanks in advance,
-Ekr

--
[Eric Rescorla                                   ekr at rtfm.com]



From spencer.graves at pdf.com  Mon Jun 23 20:53:45 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 23 Jun 2003 11:53:45 -0700
Subject: [R] Summary for mode of a data set
References: <200306231639.LAA03733@uhddx01.dt.uh.edu>
Message-ID: <3EF74CB9.4000509@pdf.com>

Your "mode1" function will identify multiple modes only if they have the 
same number of observations.  Consider the following:

 > x2 <- c(2, 1,1, 3,3,3)
 > mode1(x2)
[1] 3

Here, "mode1" did not identify the local mode at 1, because it had fewer 
observations than 3.  If you want the modes at both 1 and 3, then 
consider the following:

modes <- function(x){
	xt <- table(x)
	nt <- length(xt)
	sel <- c(xt[-nt]>=xt[-1], T)&c(T, xt[-1]>=xt[-nt])
	as.numeric(names(xt[sel]))
}
 > modes(x2)
[1] 1 3

hth.  spencer graves

Erin Hodgess wrote:
> Dear R People:
> 
> thank you for the many helpful sets of code that I received!!!
> 
> I combined several of the concepts for the following function:
> 
> 
>>mode1
> 
> 
> function(x) {
> 
>         y <- rle(sort(x))
> 
>         z <- y$values[y$lengths==max(y$lengths)]
> 
>         return(z)
> 
> }
> 
> 
>>xm
> 
> 
>  [1] 22 15 10 30 25 26  2 17 28  2 24  6 26 24  5 22 20 14
> 
> 
>>mode1(xm)
> 
> 
> [1]  2 22 24 26
> 
> 
> 
> This will pick up multiple modes.
> 
> Again thanks to all who helped!
> 
> Sincerely,
> Erin 
> mailto: hodgess at uhddx.01.dt.uh.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rvaradha at jhsph.edu  Mon Jun 23 21:20:32 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Mon, 23 Jun 2003 15:20:32 -0400
Subject: [R] Summary for mode of a data set
Message-ID: <c3910ac38114.c38114c3910a@jhsph.edu>


Dear Spencer:

In the following example, your code doesn't pick up the local mode at 5.

> x2 <- c(1,1,2,3,3,3,3,5,5,5)
> modes(x2)
[1] 1 3

In this example, it gives a mode at 7, which is incorrect.

> x2 <- c(1,1,2,3,3,3,3,5,5,5,6,7)
> modes(x2)
[1] 1 3 7

Ravi.

----- Original Message -----
From: Spencer Graves <spencer.graves at pdf.com>
Date: Monday, June 23, 2003 2:53 pm
Subject: Re: [R] Summary for mode of a data set

> Your "mode1" function will identify multiple modes only if they 
> have the 
> same number of observations.  Consider the following:
> 
> > x2 <- c(2, 1,1, 3,3,3)
> > mode1(x2)
> [1] 3
> 
> Here, "mode1" did not identify the local mode at 1, because it had 
> fewer 
> observations than 3.  If you want the modes at both 1 and 3, then 
> consider the following:
> 
> modes <- function(x){
> 	xt <- table(x)
> 	nt <- length(xt)
> 	sel <- c(xt[-nt]>=xt[-1], T)&c(T, xt[-1]>=xt[-nt])
> 	as.numeric(names(xt[sel]))
> }
> > modes(x2)
> [1] 1 3
> 
> hth.  spencer graves
> 
> Erin Hodgess wrote:
> > Dear R People:
> > 
> > thank you for the many helpful sets of code that I received!!!
> > 
> > I combined several of the concepts for the following function:
> > 
> > 
> >>mode1
> > 
> > 
> > function(x) {
> > 
> >         y <- rle(sort(x))
> > 
> >         z <- y$values[y$lengths==max(y$lengths)]
> > 
> >         return(z)
> > 
> > }
> > 
> > 
> >>xm
> > 
> > 
> >  [1] 22 15 10 30 25 26  2 17 28  2 24  6 26 24  5 22 20 14
> > 
> > 
> >>mode1(xm)
> > 
> > 
> > [1]  2 22 24 26
> > 
> > 
> > 
> > This will pick up multiple modes.
> > 
> > Again thanks to all who helped!
> > 
> > Sincerely,
> > Erin 
> > mailto: hodgess at uhddx.01.dt.uh.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Mon Jun 23 21:36:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 23 Jun 2003 12:36:05 -0700
Subject: [R] Summary for mode of a data set
References: <c3910ac38114.c38114c3910a@jhsph.edu>
Message-ID: <3EF756A5.9040008@pdf.com>

	  You are correct that my function is also wrong.

	  You need to decide what you want and write a function to do that, if 
you don't have it already.

Best Wishes,
Spencer Graves

Ravi Varadhan wrote:
> Dear Spencer:
> 
> In the following example, your code doesn't pick up the local mode at 5.
> 
> 
>>x2 <- c(1,1,2,3,3,3,3,5,5,5)
>>modes(x2)
> 
> [1] 1 3
> 
> In this example, it gives a mode at 7, which is incorrect.
> 
> 
>>x2 <- c(1,1,2,3,3,3,3,5,5,5,6,7)
>>modes(x2)
> 
> [1] 1 3 7
> 
> Ravi.
> 
> ----- Original Message -----
> From: Spencer Graves <spencer.graves at pdf.com>
> Date: Monday, June 23, 2003 2:53 pm
> Subject: Re: [R] Summary for mode of a data set
> 
> 
>>Your "mode1" function will identify multiple modes only if they 
>>have the 
>>same number of observations.  Consider the following:
>>
>>
>>>x2 <- c(2, 1,1, 3,3,3)
>>>mode1(x2)
>>
>>[1] 3
>>
>>Here, "mode1" did not identify the local mode at 1, because it had 
>>fewer 
>>observations than 3.  If you want the modes at both 1 and 3, then 
>>consider the following:
>>
>>modes <- function(x){
>>	xt <- table(x)
>>	nt <- length(xt)
>>	sel <- c(xt[-nt]>=xt[-1], T)&c(T, xt[-1]>=xt[-nt])
>>	as.numeric(names(xt[sel]))
>>}
>>
>>>modes(x2)
>>
>>[1] 1 3
>>
>>hth.  spencer graves
>>
>>Erin Hodgess wrote:
>>
>>>Dear R People:
>>>
>>>thank you for the many helpful sets of code that I received!!!
>>>
>>>I combined several of the concepts for the following function:
>>>
>>>
>>>
>>>>mode1
>>>
>>>
>>>function(x) {
>>>
>>>        y <- rle(sort(x))
>>>
>>>        z <- y$values[y$lengths==max(y$lengths)]
>>>
>>>        return(z)
>>>
>>>}
>>>
>>>
>>>
>>>>xm
>>>
>>>
>>> [1] 22 15 10 30 25 26  2 17 28  2 24  6 26 24  5 22 20 14
>>>
>>>
>>>
>>>>mode1(xm)
>>>
>>>
>>>[1]  2 22 24 26
>>>
>>>
>>>
>>>This will pick up multiple modes.
>>>
>>>Again thanks to all who helped!
>>>
>>>Sincerely,
>>>Erin 
>>>mailto: hodgess at uhddx.01.dt.uh.edu
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From andy_liaw at merck.com  Mon Jun 23 22:41:40 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 23 Jun 2003 16:41:40 -0400
Subject: [R] Lwd ignored when printing on Windows
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7A5@usrymx25.merck.com>

Dear R-help,

Has anyone notice the problem that, on Windows (NT and XP), when printing a
graph using the "File -> Print..." menu in the graphics window to print the
graph, that line width seemed to be ignored in the printed output?  For
example, if I make a plot with plot(1:10, type="l", lwd=5), it looks right
on screen, but when printed out using the menu, it looks like the plot was
made with lwd=1.  I've had this problem for quite a while (at least since
1.3.x) and still present in 1.7.1.  Has anyone else seen this, or just me?

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820



------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From canty at math.mcmaster.ca  Mon Jun 23 22:47:04 2003
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Mon, 23 Jun 2003 16:47:04 -0400 (EDT)
Subject: [R] erase.screen bug?
Message-ID: <Pine.SOL.4.44.0306231634160.2027-100000@icarus.math.mcmaster.ca>

Hi,

I'm using R 1.7.1 on a Windows 2K computer.  For some reason erase.screen
does not seem to be working correctly.  That is it does not erase the
requested screen.  That is the old graphic is still visible and any
subsequent graphics are superimposed over it making them impossible to
read.  What is worse is that they print out this way also.  The same
behaviour happens with a Windows XP computer.  I have not yet examined
this on Unix but will do so tomorrow.  Is this a known problem?  Is
there an easy workaround?

Thanks,
Angelo

------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From sundar.dorai-raj at pdf.com  Mon Jun 23 22:59:43 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 23 Jun 2003 15:59:43 -0500
Subject: [R] Lwd ignored when printing on Windows
References: <3A822319EB35174CA3714066D590DCD50205C7A5@usrymx25.merck.com>
Message-ID: <3EF76A3F.6090904@pdf.com>

Andy,
   I've experienced the same thing. What's interesting is that printing 
a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy look like lwd 
= 5. I'm using R1.7.1 on Win2000Pro.

Regards,
Sundar

Liaw, Andy wrote:
> Dear R-help,
> 
> Has anyone notice the problem that, on Windows (NT and XP), when printing a
> graph using the "File -> Print..." menu in the graphics window to print the
> graph, that line width seemed to be ignored in the printed output?  For
> example, if I make a plot with plot(1:10, type="l", lwd=5), it looks right
> on screen, but when printed out using the menu, it looks like the plot was
> made with lwd=1.  I've had this problem for quite a while (at least since
> 1.3.x) and still present in 1.7.1.  Has anyone else seen this, or just me?
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, cont... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From paulda at BATTELLE.ORG  Mon Jun 23 23:04:10 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Mon, 23 Jun 2003 17:04:10 -0400
Subject: [R] ?plot problem
Message-ID: <940250A9EB37A24CBE28D858EF07774967A957@ws-bco-mse3.milky-way.battelle.org>

R1.7.0, Win2k:

When I use plot( ) on a groupedData object,
if I have 165 subjects I'm supposed to be able
to use plot(..., layout = c(5,3,11)) to get
all 165 on 11 separate sheets.  The graphics
window is only displaying the first 10.  If I
use plot(..., layout = c(5,3,12)) I get the 
appropriate 11 sheets.

Furthermore, yesterday I noticed that when using
plot( <linear model object> ) along with the
"record" feature, after using "Page Up" to scroll
up through the graphs, I was unable to scroll
back down to the last (fourth) graph to see
the Cook's Distance plot, though the first three
plots remained "scollable".

What is going on?  Does R for Windows have issues
with the last plot in a series of plots?


Thanks in advance,
  david paul



From mli at emmes.com  Mon Jun 23 23:07:27 2003
From: mli at emmes.com (Ming-Chung Li)
Date: Mon, 23 Jun 2003 17:07:27 -0400
Subject: [R] heatmap
Message-ID: <3.0.5.32.20030623170727.010fb890@host2a.emmes.com>

Dear R-users,

I am using R-1.7.1. on windows 2000. When I use the heatmap() function, I
found lines on the dendorgam are not well connected. For example, the two
dendrograms produced by the following have this problem.

set.seed(1234)
heatmap(x<-matrix(rnorm(100*30),100,30))

Is there a bug in plot(dendrogram object) which heatmap used?

Thanks,
Ming-Chung Li



From ripley at stats.ox.ac.uk  Mon Jun 23 23:25:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 22:25:50 +0100 (BST)
Subject: [R] erase.screen bug?
In-Reply-To: <Pine.SOL.4.44.0306231634160.2027-100000@icarus.math.mcmaster.ca>
Message-ID: <Pine.LNX.4.44.0306232222370.24750-100000@gannet.stats>

Is your background colour set to transparent?  There is no way to erase 
screens in the R/S model: all you can do is overpaint with the background.
And the default on-screen background colour is ... transparent.

This described in the Warning (and elsewhere) on the help page.

With a solid background colour it appears to work.

On Mon, 23 Jun 2003, Angelo Canty wrote:

> I'm using R 1.7.1 on a Windows 2K computer.  For some reason erase.screen
> does not seem to be working correctly.  That is it does not erase the
> requested screen.  That is the old graphic is still visible and any
> subsequent graphics are superimposed over it making them impossible to
> read.  What is worse is that they print out this way also.  The same
> behaviour happens with a Windows XP computer.  I have not yet examined
> this on Unix but will do so tomorrow.  Is this a known problem?  Is
> there an easy workaround?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hodgess at uhddx01.dt.uh.edu  Mon Jun 23 23:26:40 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Mon, 23 Jun 2003 16:26:40 -0500 (CDT)
Subject: [R] A final global mode function
Message-ID: <200306232126.QAA10577@uhddx01.dt.uh.edu>

>From spencer.graves at PDF.COM Mon Jun 23 14:36:45 2003
Received: from postal.pdf.com (pdf193.pdf.com [209.128.81.193])
	by uhddx01.dt.uh.edu (8.9.2/8.9.2) with ESMTP id OAA09671
	for <hodgess at uhddx01.dt.uh.edu>; Mon, 23 Jun 2003 14:36:44 -0500 (CDT)
Received: from postage.pdf.com (postage.pdf.com [10.10.8.7])
	by postal.pdf.com (Switch-3.0.4/Switch-3.0.0) with ESMTP id h5NJYisf002096;
	Mon, 23 Jun 2003 12:34:44 -0700 (PDT)
Received: from malt.PDF.COM (malt.pdf.com [10.10.8.80])
	by postage.pdf.com (Switch-3.1.0/Switch-3.1.0) with ESMTP id h5NJZw4U022559;
	Mon, 23 Jun 2003 12:35:58 -0700 (PDT)
Received: from pdf.com (sjc-10-10-11-97.pdf.com [10.10.11.97])
	by malt.PDF.COM (8.9.3/8.9.3) with ESMTP id MAA24847;
	Mon, 23 Jun 2003 12:36:05 -0700 (PDT)
Message-ID: <3EF756A5.9040008 at pdf.com>
Date: Mon, 23 Jun 2003 12:36:05 -0700
From: Spencer Graves <spencer.graves at PDF.COM>
User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.0.2) Gecko/20030208 Netscape/7.02
X-Accept-Language: en-us, en
MIME-Version: 1.0
To: Ravi Varadhan <rvaradha at jhsph.edu>
CC: Erin Hodgess <hodgess at uhddx01.dt.uh.edu>, r-help at stat.math.ethz.ch
Subject: Re: [R] Summary for mode of a data set
References: <c3910ac38114.c38114c3910a at jhsph.edu>
Content-Type: text/plain; charset=us-ascii; format=flowed
Content-Transfer-Encoding: 7bit
Status: R

	  You are correct that my function is also wrong.

	  You need to decide what you want and write a function to do that, if 
you don't have it already.

Best Wishes,
Spencer Graves

Ravi Varadhan wrote:
> Dear Spencer:
> 
> In the following example, your code doesn't pick up the local mode at 5.
> 
> 
>>x2 <- c(1,1,2,3,3,3,3,5,5,5)
>>modes(x2)
> 
> [1] 1 3
> 
> In this example, it gives a mode at 7, which is incorrect.
> 
> 
>>x2 <- c(1,1,2,3,3,3,3,5,5,5,6,7)
>>modes(x2)
> 
> [1] 1 3 7
> 
> Ravi.
> 
> ----- Original Message -----
> From: Spencer Graves <spencer.graves at pdf.com>
> Date: Monday, June 23, 2003 2:53 pm
> Subject: Re: [R] Summary for mode of a data set
> 
> 
>>Your "mode1" function will identify multiple modes only if they 
>>have the 
>>same number of observations.  Consider the following:
>>
>>
>>>x2 <- c(2, 1,1, 3,3,3)
>>>mode1(x2)
>>
>>[1] 3
>>
>>Here, "mode1" did not identify the local mode at 1, because it had 
>>fewer 
>>observations than 3.  If you want the modes at both 1 and 3, then 
>>consider the following:
>>
>>modes <- function(x){
>>	xt <- table(x)
>>	nt <- length(xt)
>>	sel <- c(xt[-nt]>=xt[-1], T)&c(T, xt[-1]>=xt[-nt])
>>	as.numeric(names(xt[sel]))
>>}
>>
>>>modes(x2)
>>
>>[1] 1 3
>>
>>hth.  spencer graves
>>
>>Erin Hodgess wrote:
>>
>>>Dear R People:
>>>
>>>thank you for the many helpful sets of code that I received!!!
>>>
>>>I combined several of the concepts for the following function:
>>>
>>>
>>>
>>>>mode1
>>>
>>>
>>>function(x) {
>>>
>>>        y <- rle(sort(x))
>>>
>>>        z <- y$values[y$lengths==max(y$lengths)]
>>>
>>>        return(z)
>>>
>>>}
>>>
>>>
>>>
>>>>xm
>>>
>>>
>>> [1] 22 15 10 30 25 26  2 17 28  2 24  6 26 24  5 22 20 14
>>>
>>>
>>>
>>>>mode1(xm)
>>>
>>>
>>>[1]  2 22 24 26
>>>
>>>
>>>
>>>This will pick up multiple modes.
>>>
>>>Again thanks to all who helped!
>>>
>>>Sincerely,
>>>Erin 
>>>mailto: hodgess at uhddx.01.dt.uh.edu
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From ripley at stats.ox.ac.uk  Mon Jun 23 23:34:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 22:34:50 +0100 (BST)
Subject: [R] ?plot problem
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A957@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <Pine.LNX.4.44.0306232228430.24750-100000@gannet.stats>

On Mon, 23 Jun 2003, Paul, David  A wrote:

> R1.7.0, Win2k:
> 
> When I use plot( ) on a groupedData object,
> if I have 165 subjects I'm supposed to be able
> to use plot(..., layout = c(5,3,11)) to get
> all 165 on 11 separate sheets.  The graphics
> window is only displaying the first 10.  If I
> use plot(..., layout = c(5,3,12)) I get the 
> appropriate 11 sheets.
> 
> Furthermore, yesterday I noticed that when using
> plot( <linear model object> ) along with the
> "record" feature, after using "Page Up" to scroll
> up through the graphs, I was unable to scroll
> back down to the last (fourth) graph to see
> the Cook's Distance plot, though the first three
> plots remained "scollable".
> 
> What is going on?  Does R for Windows have issues
> with the last plot in a series of plots?

Perhaps recording does, as people have reported similar things, but
neither Duncan nor I could reproduce them.  I've just tried several
plot.lm examples, and it worked flawlessly.  We can't help you further
unless you can produce reproducible examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fharrell at virginia.edu  Mon Jun 23 23:47:15 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 23 Jun 2003 17:47:15 -0400
Subject: [R] Hmisc and Design Packages
Message-ID: <20030623174715.38f0d75e.fharrell@virginia.edu>

New versions of the Hmisc and Design packages, including ones for Windows, may be found at http://hesweb1.med.virginia.edu/biostat/s/library/r

Thanks as always to Xiao Gang Fan <xiao.gang.fan1 at libertysurf.fr> for providing the Windows ports.

In Hmisc, new features for multiple imputation have been added to aregImpute and LaTeX and plot methods have been improved for summary.formula objects.  New functions spss.get and sasxport.get have been added to enhance how SPSS and SAS transport files are imported using the foreign package.  Variable labels, value labels, date, time and date/time variables are handled.  Various bugs have been fixed in Design, and a new fitting function glsD (Design's version of the nlme generalized least squares function gls) has been added as has the univarLR function.  glsD has an experimental cluster bootstrap option for nonparametric estimation of covariance matrices of regression coefficient estimates with repeated measures.  Details of changes along with installation instructions may be found at http://hesweb1.med.virginia.edu/s/Hmisc.html and .../Design.html.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From yanyu at cs.ucla.edu  Mon Jun 23 23:48:49 2003
From: yanyu at cs.ucla.edu (Yan Yu)
Date: Mon, 23 Jun 2003 14:48:49 -0700 (PDT)
Subject: [R] sequential gaussian simulation
Message-ID: <Pine.SOL.4.33.0306231441340.7587-100000@panther.cs.ucla.edu>

Hello,
    I am wondering does anyone have experience with sequential gaussian
simulation? what package would you recommend for that purpose?

I also have a related Q:
  In R, is there a function I can use to test that if my data is
multi-point, or multi-variant gaussian?

Any information and pointer is appreciated!
yan



From ripley at stats.ox.ac.uk  Mon Jun 23 23:59:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jun 2003 22:59:13 +0100 (BST)
Subject: [R] Lwd ignored when printing on Windows
In-Reply-To: <3EF76A3F.6090904@pdf.com>
Message-ID: <Pine.LNX.4.44.0306232250470.24803-100000@gannet.stats>

What printer driver are you using?

I've just tried this and it works exactly as one would expect on my HP 
970CXi, as well as cut-and-paste into other applications.  It also worked 
printing to Acrobat Distiller (although all the lines were thinner there 
than on-screen and on the 970CXi, the ratio was still 1:5).

We've been here before, and had to abandon some optimizations because of a
bug in interpreting Windows metafiles in Word.

On Mon, 23 Jun 2003, Sundar Dorai-Raj wrote:

> Andy,
>    I've experienced the same thing. What's interesting is that printing 
> a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy look like lwd 
> = 5. I'm using R1.7.1 on Win2000Pro.
> 
> Regards,
> Sundar
> 
> Liaw, Andy wrote:
> > Dear R-help,
> > 
> > Has anyone notice the problem that, on Windows (NT and XP), when printing a
> > graph using the "File -> Print..." menu in the graphics window to print the
> > graph, that line width seemed to be ignored in the printed output?  For
> > example, if I make a plot with plot(1:10, type="l", lwd=5), it looks right
> > on screen, but when printed out using the menu, it looks like the plot was
> > made with lwd=1.  I've had this problem for quite a while (at least since
> > 1.3.x) and still present in 1.7.1.  Has anyone else seen this, or just me?
> > 
> > Best,
> > Andy
> > 
> > Andy Liaw, PhD
> > Biometrics Research      PO Box 2000, RY33-300     
> > Merck Research Labs           Rahway, NJ 07065
> > mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820
> > 
> > 
> > 
> > ------------------------------------------------------------------------------
> > Notice: This e-mail message, together with any attachments, cont... {{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Tue Jun 24 00:22:56 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 23 Jun 2003 17:22:56 -0500
Subject: [R] Lwd ignored when printing on Windows
References: <Pine.LNX.4.44.0306232250470.24803-100000@gannet.stats>
Message-ID: <3EF77DC0.2080800@pdf.com>

Prof. Ripley,
I'm using one of following the

HPLaserJet-4000 (Driver: HP LaserJet 5Si/5Si MX PS)
HPLaserJet-4100DTN (Driver: HP LaserJet 8150 PCL 6)

You are correct: the lines are thicker on the hardcopy. When I plotted 
the following:

plot(rnorm(10), type = "l", lwd = 5)
lines(rnorm(10), lwd = 1)

there is a difference on the plot. The first line is slightly darker 
than the second. So it appears to actually be working properly though 
it's not apparent unless you have the second line as a reference.

Note that saving the plot (File -> Save as) to emf and then printing the 
resulting file produces much thicker lines using the above print drivers.

Regards,
Sundar

Prof Brian Ripley wrote:
> What printer driver are you using?
> 
> I've just tried this and it works exactly as one would expect on my HP 
> 970CXi, as well as cut-and-paste into other applications.  It also worked 
> printing to Acrobat Distiller (although all the lines were thinner there 
> than on-screen and on the 970CXi, the ratio was still 1:5).
> 
> We've been here before, and had to abandon some optimizations because of a
> bug in interpreting Windows metafiles in Word.
> 
> On Mon, 23 Jun 2003, Sundar Dorai-Raj wrote:
> 
> 
>>Andy,
>>   I've experienced the same thing. What's interesting is that printing 
>>a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy look like lwd 
>>= 5. I'm using R1.7.1 on Win2000Pro.
>>
>>Regards,
>>Sundar
>>
>>Liaw, Andy wrote:
>>
>>>Dear R-help,
>>>
>>>Has anyone notice the problem that, on Windows (NT and XP), when printing a
>>>graph using the "File -> Print..." menu in the graphics window to print the
>>>graph, that line width seemed to be ignored in the printed output?  For
>>>example, if I make a plot with plot(1:10, type="l", lwd=5), it looks right
>>>on screen, but when printed out using the menu, it looks like the plot was
>>>made with lwd=1.  I've had this problem for quite a while (at least since
>>>1.3.x) and still present in 1.7.1.  Has anyone else seen this, or just me?
>>>
>>>Best,
>>>Andy
>>>
>>>Andy Liaw, PhD
>>>Biometrics Research      PO Box 2000, RY33-300     
>>>Merck Research Labs           Rahway, NJ 07065
>>>mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820
>>>
>>>
>>>
>>>------------------------------------------------------------------------------
>>>Notice: This e-mail message, together with any attachments, cont... {{dropped}}
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From hpsbranco at superig.com.br  Tue Jun 24 00:53:17 2003
From: hpsbranco at superig.com.br (=?iso-8859-1?Q?Henrique_Patr=EDcio_Sant'Anna_Branco?=)
Date: Mon, 23 Jun 2003 19:53:17 -0300
Subject: [R] Smooth of a temporal serie
Message-ID: <003401c339da$3ca62100$019da8c0@henrique>

Hello all,

I'm a new member in this list and, also, a new R user and need some
information about Resistant Smooth (using medians).
The method I need of Resistant Smooth is the 4253H one and I didn't found
how to perform that in R.
Does anybody have an idea?

Thanks,
Henrique.



From edd at debian.org  Tue Jun 24 01:17:20 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 23 Jun 2003 18:17:20 -0500
Subject: [R] Debian packages of Hmisc and Design Packages available
In-Reply-To: <20030623174715.38f0d75e.fharrell@virginia.edu>
References: <20030623174715.38f0d75e.fharrell@virginia.edu>
Message-ID: <20030623231720.GA25955@sonny.eddelbuettel.com>


Two Debian packages for Hmisc and Design have been created and are currently
available at http://dirk.eddelbuettel.com/code/debian/misc/ along with the 
package sources.  I may upload these to the Debian archive as well.

Feedback is, as always, welcome.

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From brianw at rand.org  Tue Jun 24 03:18:42 2003
From: brianw at rand.org (Williams, Brian)
Date: Mon, 23 Jun 2003 18:18:42 -0700
Subject: [R] Warnings using MASS Library
Message-ID: <2732967B446AFD4E9CDE5E37D642548B247EA3@smmail2.rand.org>

I am running R version 1.7.1.  In the process of checking the results from
some code against the results obtained from version 1.5.1, I encountered a
set of warning messages upon exiting from the session:

Save workspace image? [y/n/c]: y
Warning messages:
1:  namespaces may not be available when loading
2:  names in persistent strings are currently ignored

This happened only after loading the MASS library and running glm.nb.

I have read the postings of other users who have encountered these warnings.
The solutions suggested did not solve the problem for me (e.g. running
update.packages(), not surprising since I'm running the latest version of
MASS [7.1-8]).

Is there anything I can do that will cause these messages to disappear?  Can
I safely ignore them?

Thanks,
Brian



From jim.lemon at uts.edu.au  Tue Jun 24 03:35:14 2003
From: jim.lemon at uts.edu.au (Jim Lemon)
Date: Tue, 24 Jun 2003 11:35:14 +1000
Subject: [R] R help output in separate window
Message-ID: <0HGY000TNPJRGC@mail.uts.edu.au>

Peter Dalgaard's interest in a different method of help display led me to 
combine R's method of finding the help files, which is much better than the 
search method I initially used, with a system call to display the help file 
in an arbitrary method in another window.

For the original request of using "less" in an xterm, the user would have to 
create a batch file (in *NIX or equivalent method in other OSs) similar to 
the following:

#!/bin/sh
xterm -T $1 -e less $1

which I call "startless", and store it in a file in the user's path. Then 
insert the following lines in something like the .Rprofile file:

options(pager="startless")
source("HDfile")

where HDfile is the path and file name of the following file
(for me, "/home/jim/R/helpdisp.R"):

# retrieves the R help filename corresponding to a particular
# type of display. Defaults to "help" (sort-of-text) and will
# currently cope with HTML and any other display where the
# name of the display program and the directory name of the
# help files are the same.

get.help.filename<-function(topic,display.type="help") {
 lib.loc <- .libPaths()
 packages <- .packages(all.available = TRUE, lib.loc = lib.loc)
 files<-character(0)
 for (lib in lib.loc) {
  for (pkg in packages) {
   INDEX <- system.file(package = pkg, lib.loc = lib)
   file <- index.search(topic, INDEX, "AnIndex",display.type)
   if(length(file) && file != "") files<-c(files,file)
  }
 }
 return(files)
}

# Calls get.help.filename for the name of the relevant help file
# and if a filename is returned, calls the pager, browser or
# other program to display the file. The name of the display
# program must be the same as the directory for the appropriate
# files in any format other than "help" or HTML.

help.display<-function(topic,display.type=c("help","html")) {
 topic=substitute(topic)
 if (is.name(topic))
  topic <- as.character(topic)
 else if (!is.character(topic))
  stop("Unimplemented help feature")
 helpfile<-get.help.filename(topic,display.type)
 if(length(helpfile)) {
  if(display.type == "help")
   system(paste(options("pager"),helpfile,"&"))
  else {
   if(display.type == "html")
    system(paste(options("browser"),helpfile,"&"))
   # if the user has specified a display type, try to run it.
   else {
    if(system(paste(display.type,helpfile,"&")))
     cat("Error starting display type",display.type,"\n")
   }
  }
 }
 else cat("Can't find help for",topic,"\n")
 invisible(helpfile)
}

A lot of this is simply what I hope is the relevant code lifted from the 
help() function. I think that separating the filename finding and display 
calling parts of the function make it a lot easier to follow. I welcome 
suggestions and improvements.

Jim

Feel free to ignore any garbage beneath this line.




DISCLAIMER\ ====================================================... {{dropped}}



From ok at cs.otago.ac.nz  Tue Jun 24 03:42:06 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 24 Jun 2003 13:42:06 +1200 (NZST)
Subject: [R] How can I do a spinning plot in R?
Message-ID: <200306240142.h5O1g6Tq447779@atlas.otago.ac.nz>

I asked about spinning plots in R.

Peter Wolf <s-plus at wiwi.uni-bielefeld.de> wrote
about a function of his using Tcl/Tk, at:

    http://www.wiwi.uni-bielefeld.de/~wolf/software/spin3R/spin3R.sch
	
I have downloaded this.  Now I face another problem.
	
    f% R

    R : Copyright 2003, The R Development Core Team
    Version 1.7.1  (2003-06-16)

    [rest of banner snipped]

    > library(tcltk)
    Segmentation Fault (core dumped)

    f% gdb $R_HOME/bin/R.bin core
    [snip]
    (gdb) where
    #0  0x7f6e5644 in Tcl_ParseCommand () from /usr/local/lib/libtcl8.1.so
    #1  0x7f6e6878 in Tcl_EvalEx () from /usr/local/lib/libtcl8.1.so
    #2  0x7f6e6b04 in Tcl_Eval () from /usr/local/lib/libtcl8.1.so
    #3  0x7f772cfc in tcltk_init () at tcltk.c:570
    #4  0x615a0 in do_dotCode (call=0xcd452c, op=0xcd59a0, args=0x1e39f0, env=0x0)
	at dotcode.c:1307
    #5  0x750a8 in Rf_eval (e=0xcd452c, rho=0x9825fc) at eval.c:441
    #6  0x7673c in do_begin (call=0xcd4430, op=0x1f4c54, args=0xcd4548, 
	rho=0x9825fc) at eval.c:1077
    #7  0x74f38 in Rf_eval (e=0xcd4430, rho=0x9825fc) at eval.c:418
    #8  0x755c4 in Rf_applyClosure (call=0x65a6ac, op=0x9d4ef4, arglist=0x984a94, 
	rho=0xf13ea8, suppliedenv=0x1e39f0) at eval.c:609
    [snip snip]

When I installed R 1.7.1 I went through the usual
    ./configure
    make
    make check
sequence.  Make check reported a problem in a Tcl/Tk check, but it didn't
dump core, and didn't explain what the problem actually _was_.
I went carefully through the log produced by ./configure and found that
it had located the Tcl/Tk libraries correctly, and we see from the dump
above that this is certainly true of Tcl.  (I actually have tcl 8.3
installed privately; R's configure located the system-wide 8.1 version.)

This is on a Sun Blade 100 running Solaris 2.9.

What is the next step in tracking this problem down?



From mn216 at columbia.edu  Tue Jun 24 04:02:14 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Mon, 23 Jun 2003 22:02:14 -0400
Subject: [R] help on R programming.
References: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3EF7B126.F27FADDF@columbia.edu>



Hello all,

I am looking for books to help me gain a firmer grasp on the S/R
programming language ,  programing / data structures etc. it seems that
for this purpose two books are typically recommended: 

Programming with Data: A Guide to the S Language, John M. Chambers and
S Programming by Venables & Ripley.

- The Chambers book is published 1998. is it a bit dated at this point.
- is the Venables and Ripley's book a good source on the design and
manipulation of data structures in R (it seems mostly focused on R
extensions).
- are there any other books, possibly published more recently, that you
could recommend.


I also have a couple of particular programming questions:

-coming from a C++/java programming background I found that I often end
up in R with lists of objects (each constructed, in turn, as a list, say
list(x=x,y=y,z=z)). often, these individual objects have recursive
'attributes' so a matrix representation of this set of objects is not an
option. although a data.frame might be. I typically need to access
certain attributes of these objects for plotting or analysis etc.
however, I have not been able to come up with a clean way to do this?
e.g.

object.list = list(o1=list(x=1,y=2,z=3), o2=list(x=11,y=22,z=33))

what I would like to do is say get a vector of x values for the objects
in object.list, but something like
object.list[[1:length(object.list)]]$x, for example, returns NULL.

is there a better way to set up such an object list data structure that
will allow me to do this?

- what is the correct way to -remove- a component from a list. this
seems to do the trick: list[[1]] = NULL, however, you'd think this
should simply attach a NULL object at the first component position?  

many thanks for any help

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From ok at cs.otago.ac.nz  Tue Jun 24 04:44:52 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 24 Jun 2003 14:44:52 +1200 (NZST)
Subject: [R] FW: S4 classes, creating in C
Message-ID: <200306240244.h5O2iqUn491336@atlas.otago.ac.nz>

	Laurence Kell FM CEFAS <L.T.Kell at cefas.co.uk> asked about
	> setClass("LUdecomposition", representation(a="matrix", pivot = "integer"),
	>                             prototype=list(pivot = NA)                    )
	> 
	> then I get the following error message in R 1.7.1 but not R 1.7.0
	> 
	> Error in makePrototypeFromClassDef(properties, ClassDef, immediate) :
	>         In making the prototype for class "LUdecomposition" elements of the
	> prototype
	>         failed to match the corresponding slot class: pivot (class " integer
	> ")
	> 
	> Why  can I no longer use the prototype to set the default values?
	
The prototype says 'pivot = "integer"'.
Had you noticed in the R language PDf file that typeof(NA) = "logical"?
What happens when you try

    prototype=list(pivot=as.integer(NA))

instead?



From spencer.graves at pdf.com  Tue Jun 24 04:43:49 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 23 Jun 2003 19:43:49 -0700
Subject: [R] help on R programming.
References: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>
	<3EF7B126.F27FADDF@columbia.edu>
Message-ID: <3EF7BAE5.4020208@pdf.com>

I can help with the second of the three questions I see your email:

 > object.list = list(o1=list(x=1,y=2,z=3), o2=list(x=11,y=22,z=33))
 > sapply(object.list, function(x)x$x)
o1 o2
  1 11

See for example Venables and Ripley (2002) Modern Applied Statistics 
with S, 4th ed. (Springer, pp. 33-34).

hth.  spencer graves

Murad Nayal wrote:
> 
> Hello all,
> 
> I am looking for books to help me gain a firmer grasp on the S/R
> programming language ,  programing / data structures etc. it seems that
> for this purpose two books are typically recommended: 
> 
> Programming with Data: A Guide to the S Language, John M. Chambers and
> S Programming by Venables & Ripley.
> 
> - The Chambers book is published 1998. is it a bit dated at this point.
> - is the Venables and Ripley's book a good source on the design and
> manipulation of data structures in R (it seems mostly focused on R
> extensions).
> - are there any other books, possibly published more recently, that you
> could recommend.
> 
> 
> I also have a couple of particular programming questions:
> 
> -coming from a C++/java programming background I found that I often end
> up in R with lists of objects (each constructed, in turn, as a list, say
> list(x=x,y=y,z=z)). often, these individual objects have recursive
> 'attributes' so a matrix representation of this set of objects is not an
> option. although a data.frame might be. I typically need to access
> certain attributes of these objects for plotting or analysis etc.
> however, I have not been able to come up with a clean way to do this?
> e.g.
> 
> object.list = list(o1=list(x=1,y=2,z=3), o2=list(x=11,y=22,z=33))
> 
> what I would like to do is say get a vector of x values for the objects
> in object.list, but something like
> object.list[[1:length(object.list)]]$x, for example, returns NULL.
> 
> is there a better way to set up such an object list data structure that
> will allow me to do this?
> 
> - what is the correct way to -remove- a component from a list. this
> seems to do the trick: list[[1]] = NULL, however, you'd think this
> should simply attach a NULL object at the first component position?  
> 
> many thanks for any help
>



From Simon.Blomberg at anu.edu.au  Tue Jun 24 04:48:29 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Tue, 24 Jun 2003 12:48:29 +1000
Subject: [R] help on R programming.
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133C8@CASEVS02.cas.anu.edu.au>

> -----Original Message-----
> From: Murad Nayal [mailto:mn216 at columbia.edu]
> Sent: Tuesday, 24 June 2003 12:02 PM
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] help on R programming.
> 
> 
> 
> 
> Hello all,
> 
> I am looking for books to help me gain a firmer grasp on the S/R
> programming language ,  programing / data structures etc. it 
> seems that
> for this purpose two books are typically recommended: 
> 
> Programming with Data: A Guide to the S Language, John M. Chambers and
> S Programming by Venables & Ripley.
> 
> - The Chambers book is published 1998. is it a bit dated at 
> this point.
> - is the Venables and Ripley's book a good source on the design and
> manipulation of data structures in R (it seems mostly focused on R
> extensions).
> - are there any other books, possibly published more 
> recently, that you
> could recommend.


I like Venables and Ripley's "S Programming". Of course it is focussed on R (S) extensions, since programming in S is often extending the language. I think there is enough information in there on data structures (don't have the book in front of me). S has very simple data structures.

> 
> 
> I also have a couple of particular programming questions:
> 
> -coming from a C++/java programming background I found that I 
> often end
> up in R with lists of objects (each constructed, in turn, as 
> a list, say
> list(x=x,y=y,z=z)). often, these individual objects have recursive
> 'attributes' so a matrix representation of this set of 
> objects is not an
> option. although a data.frame might be. I typically need to access
> certain attributes of these objects for plotting or analysis etc.
> however, I have not been able to come up with a clean way to do this?
> e.g.
> 
> object.list = list(o1=list(x=1,y=2,z=3), o2=list(x=11,y=22,z=33))
> 
> what I would like to do is say get a vector of x values for 
> the objects
> in object.list, but something like
> object.list[[1:length(object.list)]]$x, for example, returns NULL.
> 
> is there a better way to set up such an object list data 
> structure that
> will allow me to do this?

try sapply(object.list, function (element) element$x)

(I come from a Lisp background, so this seems natural to me. There may be better ways! *sighs with fond memories of mapcar and lambda*)
> 
> - what is the correct way to -remove- a component from a list. this
> seems to do the trick: list[[1]] = NULL, however, you'd think this
> should simply attach a NULL object at the first component position? 

You'd think that, but you would be wrong. :-). To add a NULL object to the front of a list:

c(list(NULL), object.list)

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379



From olau at fas.harvard.edu  Tue Jun 24 05:34:09 2003
From: olau at fas.harvard.edu (Olivia Lau)
Date: Mon, 23 Jun 2003 23:34:09 -0400
Subject: [R] Writing R demo files
Message-ID: <000501c33a01$7c8030a0$02ccfea9@olau>

Hi,

Is it possible to comment demo() files?  I am trying to write
demos that have comments, R commands, and R output so that new
users will be able to follow along.

I have tried readline(), but that returns the readline("...")
command statement as well as the output from readline, and the
demos are ugly.  I have also tried:

options(echo = FALSE)
print("my comments here")
options(echo = TRUE)

[some R commands here]

options(echo = FALSE)
print("more comments")
options(echo = TRUE)

but options() seems to work only at the beginning of the file.
Is there a special command or procedure that I should use?

Thanks,

Olivia Lau



From rpeng at stat.ucla.edu  Tue Jun 24 06:30:23 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 23 Jun 2003 22:30:23 -0600
Subject: [R] help on R programming.
In-Reply-To: <3EF7B126.F27FADDF@columbia.edu>
References: <940250A9EB37A24CBE28D858EF07774967A929@ws-bco-mse3.milky-way.battelle.org>
	<3EF7B126.F27FADDF@columbia.edu>
Message-ID: <3EF7D3DF.4040702@stat.ucla.edu>

Murad Nayal wrote:

> 
> Hello all,
> 
> I am looking for books to help me gain a firmer grasp on the S/R
> programming language ,  programing / data structures etc. it seems that
> for this purpose two books are typically recommended: 
> 
> Programming with Data: A Guide to the S Language, John M. Chambers and

Although it was published in 1998, I hardly find it outdated.  Still a 
good reference, but as far as I know, not everything is implemented in R.

> S Programming by Venables & Ripley.
> 
> - The Chambers book is published 1998. is it a bit dated at this point.
> - is the Venables and Ripley's book a good source on the design and
> manipulation of data structures in R (it seems mostly focused on R
> extensions).
> - are there any other books, possibly published more recently, that you
> could recommend.

Modern Applied Statistics in S (4th ed.) by Venables & Ripley is not so 
much about the language itself but is always a good reference.

> 
> 
> I also have a couple of particular programming questions:
> 
> -coming from a C++/java programming background I found that I often end
> up in R with lists of objects (each constructed, in turn, as a list, say
> list(x=x,y=y,z=z)). often, these individual objects have recursive
> 'attributes' so a matrix representation of this set of objects is not an
> option. although a data.frame might be. I typically need to access
> certain attributes of these objects for plotting or analysis etc.
> however, I have not been able to come up with a clean way to do this?
> e.g.
> 
> object.list = list(o1=list(x=1,y=2,z=3), o2=list(x=11,y=22,z=33))
> 
> what I would like to do is say get a vector of x values for the objects
> in object.list, but something like
> object.list[[1:length(object.list)]]$x, for example, returns NULL.

You can use sapply:

sapply(object.list, "[[", "x")

> 
> is there a better way to set up such an object list data structure that
> will allow me to do this?
> 
> - what is the correct way to -remove- a component from a list. this
> seems to do the trick: list[[1]] = NULL, however, you'd think this
> should simply attach a NULL object at the first component position?  
> 
> many thanks for any help
> 

-roger



From ripley at stats.ox.ac.uk  Tue Jun 24 08:12:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jun 2003 07:12:56 +0100 (BST)
Subject: [R] Warnings using MASS Library
In-Reply-To: <2732967B446AFD4E9CDE5E37D642548B247EA3@smmail2.rand.org>
Message-ID: <Pine.LNX.4.44.0306240710210.25427-100000@gannet.stats>

On Mon, 23 Jun 2003, Williams, Brian wrote:

> I am running R version 1.7.1.  In the process of checking the results from
> some code against the results obtained from version 1.5.1, I encountered a
> set of warning messages upon exiting from the session:
> 
> Save workspace image? [y/n/c]: y
> Warning messages:
> 1:  namespaces may not be available when loading
> 2:  names in persistent strings are currently ignored
> 
> This happened only after loading the MASS library and running glm.nb.
> 
> I have read the postings of other users who have encountered these warnings.
> The solutions suggested did not solve the problem for me (e.g. running
> update.packages(), not surprising since I'm running the latest version of
> MASS [7.1-8]).

Those solutions were for errors, not warnings.

> Is there anything I can do that will cause these messages to disappear?  Can
> I safely ignore them?

Yes, you can safely ignore them.  They are intended to warn you that the
resulting .RData may not be loadable into earlier versions of R.

If you can send me a session that exhibits this I can look into 
circumventing those warnings.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r_stuff_online at hotmail.com  Tue Jun 24 08:20:31 2003
From: r_stuff_online at hotmail.com (Neil Osborne)
Date: Tue, 24 Jun 2003 06:20:31 +0000
Subject: [R] Creating Surface Plots in R
Message-ID: <Law12-F93fLExeW3WVZ0008b9d6@hotmail.com>

Hi,

I have three columns of data in afile that I would like to analyse by means 
of a surface plot. I would be very grateful if anyone could show me how to 
create a surface plot from this data (once it has been imported into a 
dataframe).

Thanks

_________________________________________________________________
Tired of 56k? Get a FREE BT Broadband connection



From r_stuff_online at hotmail.com  Tue Jun 24 08:21:54 2003
From: r_stuff_online at hotmail.com (Neil Osborne)
Date: Tue, 24 Jun 2003 06:21:54 +0000
Subject: [R] Any feedback/progress report on R newsgroup?
Message-ID: <Law12-F76OKSAE3jjlr0001464c@hotmail.com>

Does anyone have any feedback on the status of the previously proposed R 
newsgroup?

Thanks

_________________________________________________________________
Sign-up for a FREE BT Broadband connection today!



From ripley at stats.ox.ac.uk  Tue Jun 24 08:24:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jun 2003 07:24:28 +0100 (BST)
Subject: [R] help on R programming.
In-Reply-To: <3EF7B126.F27FADDF@columbia.edu>
Message-ID: <Pine.LNX.4.44.0306240714520.25427-100000@gannet.stats>

On Mon, 23 Jun 2003, Murad Nayal wrote:

> - what is the correct way to -remove- a component from a list. this
> seems to do the trick: list[[1]] = NULL, however, you'd think this
> should simply attach a NULL object at the first component position?  

This is in the FAQ, section 3.3.3, and is an S/R difference that catches
people quite often.  It's related to the difference between [] and [[]].

Generally you will find that it is better to program by generating whole 
lists with lapply() or to copy lists retaining what you want (which does 
not copy the components, in general, and so is cheap).


As for your comments on books: `S Programming' does discuss the design of
classes (both informal and formal), the main data sructures in R. As
others have said, the Green Book (Chambers, 1998) is by not means out of
date, except in the sense that the precise langage it describes has never
been available: it is not a description of any version of S-PLUS nor R.

Generally, though, you need to make sure you have at your fingertips the
resources which come with R: the various manuals (including R-lang) and
the on-line help.  For example, I have just spend several days documenting
in the help pages exactly how subscripting of data frames works (and
correcting dozens of anomalies and bugs).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 24 08:27:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jun 2003 07:27:09 +0100 (BST)
Subject: [R] Creating Surface Plots in R
In-Reply-To: <Law12-F93fLExeW3WVZ0008b9d6@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306240725530.25427-100000@gannet.stats>

?persp

library(lattice)
?wireframe

both have several examples.

On Tue, 24 Jun 2003, Neil Osborne wrote:

> I have three columns of data in afile that I would like to analyse by means 
> of a surface plot. I would be very grateful if anyone could show me how to 
> create a surface plot from this data (once it has been imported into a 
> dataframe).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Tue Jun 24 09:33:18 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 24 Jun 2003 09:33:18 +0200
Subject: [R] Creating Surface Plots in R
In-Reply-To: <Law12-F93fLExeW3WVZ0008b9d6@hotmail.com>
Message-ID: <3EF81ADE.21052.1C8750@localhost>

Hallo

On 24 Jun 2003 at 6:20, Neil Osborne wrote:

> Hi,
> 
> I have three columns of data in afile that I would like to analyse by
> means of a surface plot. I would be very grateful if anyone could show
> me how to create a surface plot from this data (once it has been
> imported into a dataframe).

I usually use interp() from akima library as a prerequisite to image, 
contour or persp plots.

> 
> Thanks
> 
> _________________________________________________________________
> Tired of 56k? Get a FREE BT Broadband connection
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers
Petr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From R.Dutter at tuwien.ac.at  Tue Jun 24 10:19:54 2003
From: R.Dutter at tuwien.ac.at (Rudi Dutter)
Date: Tue, 24 Jun 2003 10:19:54 +0200
Subject: [R] GeoR: Nested Models for Spatial Data
Message-ID: <3EF809AA.A3FC2C2B@tuwien.ac.at>

Dear Colleagues,

Does anyone know how to specify nested models for kriging (krige.conv) in the
function
cov.spatial
?

I have understood that I can specify a matrix of parameters using 'cov.pars = '.
However, then I should specify the kind of model for each row of the matrix. The
parameter 'cov.model' only accepts one keyword. Is this a bug?

Many thanks,
Rudi Dutter

-- 
~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=
From: Prof. Dr. Rudolf Dutter
      Dept. of Statistics and Probability Theory
      Vienna University of Technology,
      Wiedner Hauptstr. 8-10
      A-1040 Vienna, Austria
      Tel. +43 1 58801/10730
      FAX  +43 1 58801/10799
      E-Mail: R.Dutter at tuwien.ac.at
      Internet: http://www.statistik.tuwien.ac.at/public/dutt/
~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=



From th50 at leicester.ac.uk  Tue Jun 24 11:04:09 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 24 Jun 2003 10:04:09 +0100
Subject: [R] ?plot problem
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B66C@saffron.cfs.le.ac.uk>



> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 23 June 2003 22:35
> To: Paul, David A
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ?plot problem
> 
> 
> On Mon, 23 Jun 2003, Paul, David  A wrote:
> 
> > R1.7.0, Win2k:
> > 
> > When I use plot( ) on a groupedData object,
> > if I have 165 subjects I'm supposed to be able
> > to use plot(..., layout = c(5,3,11)) to get
> > all 165 on 11 separate sheets.  The graphics
> > window is only displaying the first 10.  If I
> > use plot(..., layout = c(5,3,12)) I get the 
> > appropriate 11 sheets.
> > 
> > Furthermore, yesterday I noticed that when using
> > plot( <linear model object> ) along with the
> > "record" feature, after using "Page Up" to scroll
> > up through the graphs, I was unable to scroll
> > back down to the last (fourth) graph to see
> > the Cook's Distance plot, though the first three
> > plots remained "scollable".
> > 
> > What is going on?  Does R for Windows have issues
> > with the last plot in a series of plots?
> 
> Perhaps recording does, as people have reported similar things, but
> neither Duncan nor I could reproduce them.  I've just tried several
> plot.lm examples, and it worked flawlessly.  We can't help you further
> unless you can produce reproducible examples.

I can reproduce this behaviour which has been discussed several
times on this list as far as I remember.

After turning plot history on, execute

> plot(lm(I(sin(1:10)~I(1:10))))

and have a look at all plots.

If you then step back using PgUp, and step forward again with PgDn,
the last plot can't be reached. This is because it was never finished;
R plots don't know whether you still want to add something, so this plot
hasn't been "saved" yet. The solution is to "add" the plot first,
using INS (or History_Add), or create another plot, or,... That finishes
the plot, and makes it available in the history.

Hope that helps.

Thomas

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.0            
year     2003           
month    04             
day      16             
language R  

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From th50 at leicester.ac.uk  Tue Jun 24 11:07:38 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 24 Jun 2003 10:07:38 +0100
Subject: [R] Smooth of a temporal serie
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46C0@saffron.cfs.le.ac.uk>

If you are talking about a running median smooth ? la Tukey,

smooth() in library(eda) might be of help.

Best wishes

Thomas


> -----Original Message-----
> From: Henrique Patr?cio Sant'Anna Branco
> [mailto:hpsbranco at superig.com.br]
> Sent: 23 June 2003 23:53
> To: r-help at stat.math.ethz.ch
> Subject: [R] Smooth of a temporal serie
> 
> 
> Hello all,
> 
> I'm a new member in this list and, also, a new R user and need some
> information about Resistant Smooth (using medians).
> The method I need of Resistant Smooth is the 4253H one and I 
> didn't found
> how to perform that in R.
> Does anybody have an idea?
> 
> Thanks,
> Henrique.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From vincent.stoliaroff at socgen.com  Tue Jun 24 11:29:59 2003
From: vincent.stoliaroff at socgen.com (vincent.stoliaroff@socgen.com)
Date: Tue, 24 Jun 2003 11:29:59 +0200
Subject: [R] R and Latex's tables
Message-ID: <OFE927C25C.AA17A361-ONC1256D4F.00338B4C@ges.marc.societe-generale.fr>

Hi R lovers!

I have discovered recently that graph can be exported from R in a Latex
compatible file
thanks to the pictex command

I would like to know if there is the equivalent while exporting datas.
Let's say I have a matrix, a data.frame or a list that I would like to
export as a flat text file that is immediatly transcripted into a table
into latex
Is there a macro or package that could transcript it with the tabular or
array environment command, ...

\begin{tabular/array}
..&.&. \\

etc

It would be easier than to use the write.table() function and then
transform my txt file into a latex table manually....

thanks a lot





*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "message") sont
confidentiels et etablis a l'intention exclusive de ses destinataires.
Toute utilisation ou diffusion non autorisee est interdite. 
Tout message electronique est susceptible d'alteration. 
La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
titre de ce message s'il a ete altere, deforme ou falsifie.
				********
This message and any attachments (the "message") are confidentia... {{dropped}}



From th50 at leicester.ac.uk  Tue Jun 24 11:42:41 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 24 Jun 2003 10:42:41 +0100
Subject: [R] R and Latex's tables
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46C1@saffron.cfs.le.ac.uk>

Package Hmisc by Prof. Frank E Harrell (new windows versions just 
announced!) has a lot of functions to do that.

Also, have a look at package Sweave by Friedrich Leisch, which allows
to combine Latex and R commands.

Hope that helps.

Best wishes

Thomas


> -----Original Message-----
> From: vincent.stoliaroff at socgen.com
> [mailto:vincent.stoliaroff at socgen.com]
> Sent: 24 June 2003 10:30
> To: r-help at stat.math.ethz.ch
> Subject: [R] R and Latex's tables
> 
> 
> Hi R lovers!
> 
> I have discovered recently that graph can be exported from R 
> in a Latex
> compatible file
> thanks to the pictex command
> 
> I would like to know if there is the equivalent while exporting datas.
> Let's say I have a matrix, a data.frame or a list that I would like to
> export as a flat text file that is immediatly transcripted 
> into a table
> into latex
> Is there a macro or package that could transcript it with the 
> tabular or
> array environment command, ...
> 
> \begin{tabular/array}
> ..&.&. \\
> 
> etc
> 
> It would be easier than to use the write.table() function and then
> transform my txt file into a latex table manually....
> 
> thanks a lot
> 
> 
> 
> 
> 
> **************************************************************
> ***********
> Ce message et toutes les pieces jointes (ci-apres le "message") sont
> confidentiels et etablis a l'intention exclusive de ses destinataires.
> Toute utilisation ou diffusion non autorisee est interdite. 
> Tout message electronique est susceptible d'alteration. 
> La SOCIETE GENERALE et ses filiales declinent toute responsabilite au 
> titre de ce message s'il a ete altere, deforme ou falsifie.
> 				********
> This message and any attachments (the "message") are 
> confidentia... {{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From baron at psych.upenn.edu  Tue Jun 24 11:46:20 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 24 Jun 2003 05:46:20 -0400
Subject: [R] R and Latex's tables
In-Reply-To: <OFE927C25C.AA17A361-ONC1256D4F.00338B4C@ges.marc.societe-generale.fr>
References: <OFE927C25C.AA17A361-ONC1256D4F.00338B4C@ges.marc.societe-generale.fr>
Message-ID: <20030624094620.GB14371@mail1.sas.upenn.edu>

On 06/24/03 11:29, vincent.stoliaroff at socgen.com wrote:
>Hi R lovers!
>
>I have discovered recently that graph can be exported from R in a Latex
>compatible file
>thanks to the pictex command
>
>I would like to know if there is the equivalent while exporting datas.
>Let's say I have a matrix, a data.frame or a list that I would like to
>export as a flat text file that is immediatly transcripted into a table
>into latex
>Is there a macro or package that could transcript it with the tabular or
>array environment command, ...

The xtable package does a lot of this.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
R page:               http://finzi.psych.upenn.edu/



From sorenh at agrsci.dk  Tue Jun 24 11:49:16 2003
From: sorenh at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 24 Jun 2003 11:49:16 +0200
Subject: [R] Locating an executable from R
Message-ID: <000e01c33a35$ee7d4290$a8f1d7c3@djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030624/a147c51e/attachment.pl

From hb at maths.lth.se  Tue Jun 24 13:18:56 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 24 Jun 2003 13:18:56 +0200
Subject: [R] Locating an executable from R
In-Reply-To: <000e01c33a35$ee7d4290$a8f1d7c3@djf.agrsci.dk>
Message-ID: <002001c33a42$67665c80$e502eb82@maths.lth.se>

Something like

findFile <- function(pattern, path=Sys.getenv("PATH"), split=";", ...) {
  path <- unlist(strsplit(path, split=split));
  list.files(path=path, pattern=pattern, full.names=TRUE, ...);
}

findFile("^expl.*[.]exe$")
findFile("^notepad[.]exe$")

Cheers

Henrik Bengtsson
Lund University, Sweden

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of S?ren H?jsgaard
> Sent: den 24 juni 2003 11:49
> To: r-help at stat.math.ethz.ch
> Subject: [R] Locating an executable from R
> 
> 
> Hi !
> >From within R (on windows) I would like to locate and invoke an 
> >executable program (whose location is unknown to me).
> Can I do that?
> Thanks in advance
> S?ren H?jsgaard
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>



From andy_liaw at merck.com  Tue Jun 24 14:11:24 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Jun 2003 08:11:24 -0400
Subject: [R] Lwd ignored when printing on Windows
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7A7@usrymx25.merck.com>

Thanks to Sundar and Prof. Ripley.  It was a false alarm.  It's
just hard to tell that lwd and lty are being respected in the
printed output.  We have a HP4050 Laserjet.  I used "print to
File" in the print dialog to write to a ps file and look at it
in Gsview.  I had to magnify it quite a bit to be able to tell
That lty=2 and 3 are dashes and dots.  Also, as Sundar said,
lwd is only apparent when several values are used on the same
plot.

Cheers,
Andy

> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> 
> What printer driver are you using?
> 
> I've just tried this and it works exactly as one would expect 
> on my HP 
> 970CXi, as well as cut-and-paste into other applications.  It 
> also worked 
> printing to Acrobat Distiller (although all the lines were 
> thinner there 
> than on-screen and on the 970CXi, the ratio was still 1:5).
> 
> We've been here before, and had to abandon some optimizations 
> because of a bug in interpreting Windows metafiles in Word.
> 
> On Mon, 23 Jun 2003, Sundar Dorai-Raj wrote:
> 
> > Andy,
> >    I've experienced the same thing. What's interesting is that 
> > printing
> > a plot (CTRL-P) with lwd = 25 makes lines on the hardcopy 
> look like lwd 
> > = 5. I'm using R1.7.1 on Win2000Pro.
> > 
> > Regards,
> > Sundar
> > 
> > Liaw, Andy wrote:
> > > Dear R-help,
> > > 
> > > Has anyone notice the problem that, on Windows (NT and XP), when 
> > > printing a graph using the "File -> Print..." menu in the 
> graphics 
> > > window to print the graph, that line width seemed to be 
> ignored in 
> > > the printed output?  For example, if I make a plot with 
> plot(1:10, 
> > > type="l", lwd=5), it looks right on screen, but when printed out 
> > > using the menu, it looks like the plot was made with lwd=1.  I've 
> > > had this problem for quite a while (at least since
> > > 1.3.x) and still present in 1.7.1.  Has anyone else seen 
> this, or just me?
> > > 
> > > Best,
> > > Andy
> > > 
> > > Andy Liaw, PhD
> > > Biometrics Research      PO Box 2000, RY33-300     
> > > Merck Research Labs           Rahway, NJ 07065
> > > mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>   
>       732-594-0820
> > > 
> > > 
> > > 
> > > 
> --------------------------------------------------------------------
> > > ----------
> > > Notice: This e-mail message, together with any 
> attachments, cont... {{dropped}}
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From chrysopa at insecta.ufv.br  Mon Jun 23 15:12:45 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Mon, 23 Jun 2003 10:12:45 -0300
Subject: [R] [OFF] stepwise using REML???
In-Reply-To: <6rfzm2w5yw.fsf@bates4.stat.wisc.edu>
References: <200306201735.41239.chrysopa@insecta.ufv.br>
	<6rfzm2w5yw.fsf@bates4.stat.wisc.edu>
Message-ID: <200306231012.45310.chrysopa@insecta.ufv.br>

Em Dom 22 Jun 2003 11:18, Douglas Bates escreveu:
> > For nested design it may be very dangerous due the difference in
> > variance structure, mainly in a splitplot design. ML make
> > significative variables that REML dont make.
>
> It would be good to quote an example that shows this.  I'm not sure
> that this occurs in general.

Look this example:

Using stepwise with a ML estimation:
--------------------------------------
m0.ml <- lme(response~1,random=~1|plot1/plot2,method="ML")
mfull.ml <- update(m0.ml,.~.+v1*v2+v1*v3)
> stepAIC(mfull.ml)
Start:  AIC= 250.23 
 response ~ v1 + v2 + v3 + v1:v2 + v1:v3 

                   Df    AIC
- v1:v3		   11 249.82
<none>                250.23
- v1:v2		    2 253.65

...

Linear mixed-effects model fit by maximum likelihood
  Data: NULL 
  Log-likelihood: -112.9370
  Fixed: response ~ v1 + v2 + v1:v2 
            (Intercept)                   v1       		   v2l2 
             12.5936495              -0.3327049             -15.9920774 
                   v2l3 	        v1:v2l2   		v1:v2l3 
            -12.5285727               0.3750894               0.3014936 

Random effects:
 Formula: ~1 | plot1
        (Intercept)
StdDev: 0.004214203

 Formula: ~1 | plot2 %in% plot1
        (Intercept)  Residual
StdDev:   0.3051747 0.5556307

Number of Observations: 124
Number of Groups: 
            plot1  plot2 %in% plot1 
                6                17 
--------------------------------------
in this case the selected model is v1*v2, but in this case it use the same 
denominator DF for all variables, and it is not true.

In anova made by REML:
--------------------------------------
m0 <- lme(response~1,random=~1|plot1/plot2)
mfull <- update(m0,.~.+v1*v2+v1*v3)
anova(mfull)
                 numDF denDF   F-value p-value
(Intercept)          1    85 126.08414  <.0001
v1                   1     8   1.86229  0.2095
v2                   2     3   1.90189  0.2928
v3                  11    85   1.58872  0.1167
v1:v2                2     8   3.53897  0.0792
v1:v3               11    85   1.71976  0.0825
---------------------------------------

In this case all variables are not significative.

>
> > I read an article that is made a stepwise procedure using GENSTAT.
> >
> > from article:
> > "Terms were dropped from a model in a stepwise procedure by assessing the
> > change in deviance between the full model and the submodel."
> >
> > All are made using REML.
> >
> > It is possible?! I dont know GENSTAT.
>
> You would need to be more specific about how the comparisons are made.
> I assume that you plan to keep the random effects structure constant
> and compare two nested models that differ only in the fixed effects
> terms.  I can think of four ways of doing this:
>
> 1) Use the F-test obtained by fitting the full model and conditioning
> on the estimates of the random effects parameters.  This is what the
> anova function applied to an model fit by lme gives.
>
> 2) Fit both models and compare the values of the REML criterion in a
> likelihood ratio test.
>
> 3) Fit both models by REML and compare the values of the
> log-likelihood (i.e. the ML criterion) in a likelihood ratio test.
> You can obtain that value with logLik(fm, REML=FALSE) if fm is your
> fitted model.
>
> 4)Fit both models and evaluate the REML criterion for the full model
> at the two sets of estimates.  Compare these values in a likelihood
> ratio test.
>
> I feel that 1) is appropriate, 2) is inappropriate, 3) may be
> appropriate and 4) looks interesting.  4) is based on recent work by
> Greg Reinsel.
>
> In some simulations reported in chapter 3 of Pinheiro and Bates (2000)
> 3) fared badly compared to 1).
>
>

Thanks for all
 ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Nothing is but what is not.
--
|   // | \\   [***********************************]
|> ( ?   ? )  [Ronaldo Reis J?nior                ]
|      V      [UFV/DBA-Entomologia                ]
|>  /     \   [36571-000 Vi?osa - MG              ]
|  /(.''`.)\  [Fone: 31-3899-2532                 ]
|>/(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|>  ( `-  )   [***********************************]
|>> _/   \_Powered by GNU/Debian Woody/Sarge



From paulda at BATTELLE.ORG  Tue Jun 24 15:47:18 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Tue, 24 Jun 2003 09:47:18 -0400
Subject: [R] ?plot problem
Message-ID: <940250A9EB37A24CBE28D858EF07774967A95D@ws-bco-mse3.milky-way.battelle.org>

Thanks to Dr. Thomas Hotz, Prof. Brian Ripley, Dr. Dennis
Murphy, and Dr. David Scott for their replies.

Trying an idea:

> x <- c(1,2,3,4)
> y<-c(2,4.2,5.9,9)
> temp<-data.frame(cbind(x,y))
> attach(temp)
> temp.lm <- lm(y~x)
> windows()
> plot(temp.lm)
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
> dev.off()
> windows()

I was unable to see the Cook's distance plot as before (using
the Page Up and Page Down keys).  So I implemented another
suggestion:

> windows()
> plot(temp.lm)
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
Hit <Return> to see next plot: 
[Use Menu: History|Add]

After doing this, the Cook's distance plot was "saved" so that
Page Up and Page Down worked properly.  Question:  Is there
a way to use a line command to "add" the last graph to the
history?


Much thanks again,
  david paul



From mn216 at columbia.edu  Tue Jun 24 16:16:17 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Tue, 24 Jun 2003 10:16:17 -0400
Subject: [R] help on R programming.
References: <Pine.LNX.4.44.0306240714520.25427-100000@gannet.stats>
Message-ID: <3EF85D30.A505594B@columbia.edu>



thanks you all for the replies, it's been very helpful.

regards

Prof Brian Ripley wrote:
> 
> On Mon, 23 Jun 2003, Murad Nayal wrote:
> 
> > - what is the correct way to -remove- a component from a list. this
> > seems to do the trick: list[[1]] = NULL, however, you'd think this
> > should simply attach a NULL object at the first component position?
> 
> This is in the FAQ, section 3.3.3, and is an S/R difference that catches
> people quite often.  It's related to the difference between [] and [[]].
> 
> Generally you will find that it is better to program by generating whole
> lists with lapply() or to copy lists retaining what you want (which does
> not copy the components, in general, and so is cheap).
> 
> As for your comments on books: `S Programming' does discuss the design of
> classes (both informal and formal), the main data sructures in R. As
> others have said, the Green Book (Chambers, 1998) is by not means out of
> date, except in the sense that the precise langage it describes has never
> been available: it is not a description of any version of S-PLUS nor R.
> 
> Generally, though, you need to make sure you have at your fingertips the
> resources which come with R: the various manuals (including R-lang) and
> the on-line help.  For example, I have just spend several days documenting
> in the help pages exactly how subscripting of data frames works (and
> correcting dozens of anomalies and bugs).
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From crirocha at unicamp.br  Tue Jun 24 16:21:32 2003
From: crirocha at unicamp.br (crirocha@unicamp.br)
Date: Tue, 24 Jun 2003 11:21:32 -0300 (EST)
Subject: [R] Haw I get best lambda in box-cox
Message-ID: <1246.143.106.4.166.1056464492.squirrel@www.unicamp.br>


Hi all,

Does anyone know of a function that return the best lambda in box-cox
transformation?

many thanks for any help.


-- 
Cristiane S. Rocha
Laboratorio Genoma Funcional - Bioinform?tica
Centro de Biologia Molecular e Engenharia Genetica
Universidade Estadual de Campinas
Campinas - SP - Brasil
Tel:(19)3788-1119



From andy_liaw at merck.com  Tue Jun 24 16:45:00 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Jun 2003 10:45:00 -0400
Subject: [R] Haw I get best lambda in box-cox
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7AC@usrymx25.merck.com>

1.  Look up boxcox() in the package "MASS".

2.  Assume "bc" is an object returned by boxcox(...), you can do

  with(bc, x[which.max(y)])

    to find the best lambda.

HTH,
Andy

> -----Original Message-----
> From: crirocha at unicamp.br [mailto:crirocha at unicamp.br] 
> Sent: Tuesday, June 24, 2003 10:22 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Haw I get best lambda in box-cox
> 
> 
> 
> Hi all,
> 
> Does anyone know of a function that return the best lambda in 
> box-cox transformation?
> 
> many thanks for any help.
> 
> 
> -- 
> Cristiane S. Rocha
> Laboratorio Genoma Funcional - Bioinform?tica
> Centro de Biologia Molecular e Engenharia Genetica
> Universidade Estadual de Campinas
> Campinas - SP - Brasil
> Tel:(19)3788-1119
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From wviechtb at s.psych.uiuc.edu  Tue Jun 24 16:58:14 2003
From: wviechtb at s.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Tue, 24 Jun 2003 09:58:14 -0500 (CDT)
Subject: [R] R and Latex's tables
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF71486015E46C1@saffron.cfs.le.ac.uk>
Message-ID: <Pine.SOL.4.30.0306240956440.19025-100000@s.psych.uiuc.edu>

> Package Hmisc by Prof. Frank E Harrell (new windows versions just
> announced!) has a lot of functions to do that.
>
> Also, have a look at package Sweave by Friedrich Leisch, which allows
> to combine Latex and R commands.

And package xtable.

--
Wolfgang Viechtbauer



From spencer.graves at pdf.com  Tue Jun 24 17:01:50 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 Jun 2003 08:01:50 -0700
Subject: [R] Haw I get best lambda in box-cox
References: <1246.143.106.4.166.1056464492.squirrel@www.unicamp.br>
Message-ID: <3EF867DE.5040203@pdf.com>

"boxcox" in library(MASS).  I found it just now via "www.r-project.org" 
-> search -> "R Site Search" and via the index in Venables and Riply 
(2002) Modern Applied Statistics with S, 4th ed. (Springer).  The latter 
(p. 171-172) described how to make the corresponding profile likelihood 
plot using their function "logtrans".

hth.  spencer graves

crirocha at unicamp.br wrote:
> Hi all,
> 
> Does anyone know of a function that return the best lambda in box-cox
> transformation?
> 
> many thanks for any help.
> 
>



From clists at perrin.socsci.unc.edu  Tue Jun 24 17:19:19 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 11:19:19 -0400 (EDT)
Subject: [R] Adding a calculated variable to a data frame
Message-ID: <Pine.LNX.4.53.0306241115010.10465@perrin.socsci.unc.edu>

Is there a shortcut way to add a calculated variable to a data frame? For
example, I have a data frame with variables first.conv, first.sub, and
first.agg. Each cell is -1, 0, or 1. I'd like to generate variables:

rwa.sum  : sum(first.agg,first.sub,first.conv)
rwa.psum : total number (0-3) of vars == 1
rwa.asum : total number (0-3) of vars == -1
rwa.val  : sum(abs(first.agg,first.sub,first.conv))

The only way I can think of to do this is to create a separate data.frame
to hold the calculated values, then use cbind() to paste the two together.
Am I missing an easier option?

Thanks.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From vmarin at antar.ciencias.uchile.cl  Tue Jun 24 17:45:19 2003
From: vmarin at antar.ciencias.uchile.cl (=?iso-8859-1?Q?Victor_H._Mar=EDm?=)
Date: Tue, 24 Jun 2003 11:45:19 -0400
Subject: [R] excel files and R
Message-ID: <000001c33a67$9dbc1f20$2e8810ac@Chinchilla>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030624/b8dea23a/attachment.pl

From jlewis at genomecorp.com  Tue Jun 24 18:26:10 2003
From: jlewis at genomecorp.com (Jeff Lewis)
Date: Tue, 24 Jun 2003 12:26:10 -0400
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
Message-ID: <414DEAD4479990458D14E47B61039E5B0190F60A@apollo.ad.genomecorp.com>

I'm attempting to compile and install R version 1.7.1 for my statistical
geneticists.  It seems to compile correctly -- that is, it compiles
without errors -- but the regression test is failing in the following
manner:

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> ## log
> stopifnot(all.equal(log(1:10), log(1:10, exp(1))))
> stopifnot(all.equal(log10(30), log(30, 10)))
> stopifnot(all.equal(log2(2^pi), 2^log2(pi)))
> stopifnot(Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps)
Error: Mod(pi - log(exp(pi * (0+1i)))/(0+1i)) < .Machine$double.eps is
not TRUE
Execution halted
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

I'm compiling on Tru64UNIX 5.1A using DECs C and Fortran compilers and
perl 5.6.0.  I found the above error in a file named
'reg-tests-1.Rout.fail'.  Any help you can give me would be most
appreciated.

Thanks,
Jeff



From sneumann at TechFak.Uni-Bielefeld.DE  Tue Jun 24 18:30:43 2003
From: sneumann at TechFak.Uni-Bielefeld.DE (Steffen Neumann)
Date: Tue, 24 Jun 2003 16:30:43 -0000
Subject: [R] errorest: Error in cv.numeric()
Message-ID: <s5ubrwnxwsu.fsf@liszt.TechFak.Uni-Bielefeld.DE>


Hi,

I am trying to get an error estimation 
for a classification done using lda.

The examples work fine, however I don't get 
my own code to work.

The data is in object d

	> d
	    class hydrophobicity    charge     geometry
	1       2      6490.0400 1434.9700    610.99902
	2       2      1602.0601  400.6030  -5824.00000
	3       2       969.0060  260.1360   -415.00000
	4       1       527.2310  158.7020    -22.00010
	...
	180     1       299.5190   85.9201   -680.00000
	181     2      1385.6801  298.8360   -353.00000
	182     1       428.8740  130.8020   -328.00000
	183     1       287.5540   98.0767     34.00000

Since predict.lda does not return simply the classification 
it is wrapped, as in the docs:

	mypredict.lda <- function(object, newdata) predict(object, newdata = newdata)$class

In trying errorest() I get the message

	> errorest(class ~ hydrophobicity + charge, data=d, model=lda, predict=mypredict.lda)
	Error in cv.numeric(y, formula, data, model = model, predict = predict,  : 
	        predict does not return numerical values

even though a "manual" lda seems to provide the correct types:

	> l <- lda(class ~ hydrophobicity + charge, data=d)
	> mypredict.lda(l,d)
	  [1] 2 2 1 1 2 2 1 2 1 2 2 1 2 1 1 2 1 1 2 1 1 2 2 1 2 1 1 2 1 1 1 1 1 2 1 1 1
	...
	[149] 1 2 1 1 1 1 1 1 2 1 2 1 2 1 1 1 1 2 1 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1
	Levels: 1 2

	> typeof(mypredict.lda(l,d))
	[1] "integer"

So what did I miss, I am fairly new to programming in R/S,
so I might have missed some conventions and/or conversions.

Software used is

	R Version 1.6.2  (2003-01-10) / Linux
	ipred_0.6-14.tar.gz

Yours,
Steffen



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Jun 24 18:40:12 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 24 Jun 2003 18:40:12 +0200 (CEST)
Subject: [R] errorest: Error in cv.numeric()
In-Reply-To: <s5ubrwnxwsu.fsf@liszt.TechFak.Uni-Bielefeld.DE>
References: <s5ubrwnxwsu.fsf@liszt.TechFak.Uni-Bielefeld.DE>
Message-ID: <Pine.LNX.4.51.0306241838500.19509@artemis.imbe.med.uni-erlangen.de>

>
> Hi,
>
> I am trying to get an error estimation
> for a classification done using lda.
>
> The examples work fine, however I don't get
> my own code to work.
>
> The data is in object d
>
> 	> d
> 	    class hydrophobicity    charge     geometry
> 	1       2      6490.0400 1434.9700    610.99902
> 	2       2      1602.0601  400.6030  -5824.00000
> 	3       2       969.0060  260.1360   -415.00000
> 	4       1       527.2310  158.7020    -22.00010
> 	...
> 	180     1       299.5190   85.9201   -680.00000
> 	181     2      1385.6801  298.8360   -353.00000
> 	182     1       428.8740  130.8020   -328.00000
> 	183     1       287.5540   98.0767     34.00000
>

the class of `d' is "numeric" and should be "factor", which is implicitly
assumed for classification problems (lda kindly operates on it but, for
example rpart, won't).

Best,

Torsten

> Since predict.lda does not return simply the classification
> it is wrapped, as in the docs:
>
> 	mypredict.lda <- function(object, newdata) predict(object, newdata = newdata)$class
>
> In trying errorest() I get the message
>
> 	> errorest(class ~ hydrophobicity + charge, data=d, model=lda, predict=mypredict.lda)
> 	Error in cv.numeric(y, formula, data, model = model, predict = predict,  :
> 	        predict does not return numerical values
>
> even though a "manual" lda seems to provide the correct types:
>
> 	> l <- lda(class ~ hydrophobicity + charge, data=d)
> 	> mypredict.lda(l,d)
> 	  [1] 2 2 1 1 2 2 1 2 1 2 2 1 2 1 1 2 1 1 2 1 1 2 2 1 2 1 1 2 1 1 1 1 1 2 1 1 1
> 	...
> 	[149] 1 2 1 1 1 1 1 1 2 1 2 1 2 1 1 1 1 2 1 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1
> 	Levels: 1 2
>
> 	> typeof(mypredict.lda(l,d))
> 	[1] "integer"
>
> So what did I miss, I am fairly new to programming in R/S,
> so I might have missed some conventions and/or conversions.
>
> Software used is
>
> 	R Version 1.6.2  (2003-01-10) / Linux
> 	ipred_0.6-14.tar.gz
>
> Yours,
> Steffen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From ripley at stats.ox.ac.uk  Tue Jun 24 18:53:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jun 2003 17:53:31 +0100 (BST)
Subject: [R] excel files and R
In-Reply-To: <000001c33a67$9dbc1f20$2e8810ac@Chinchilla>
Message-ID: <Pine.LNX.4.44.0306241751540.26052-100000@gannet.stats>

On Tue, 24 Jun 2003, Victor H. Mar?m wrote:

> I am new at R.  My questions is rather basic.  Looking R manuals looks
> like there should be a way to read MS excel files into R.  Could
> somebody tell me which library should I use for that?

Several ways are in the R Data Import/Export Manual (the obvious manual, I
would have thought).  If you are working on Windows, using RODBC is
perhaps the simplest.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Jun 24 19:19:47 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 24 Jun 2003 17:19:47 -0000
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
In-Reply-To: <414DEAD4479990458D14E47B61039E5B0190F60A@apollo.ad.genomecorp.com>
References: <414DEAD4479990458D14E47B61039E5B0190F60A@apollo.ad.genomecorp.com>
Message-ID: <x2fzlzbdf0.fsf@biostat.ku.dk>

"Jeff Lewis" <jlewis at genomecorp.com> writes:

> I'm attempting to compile and install R version 1.7.1 for my statistical
> geneticists.  It seems to compile correctly -- that is, it compiles
> without errors -- but the regression test is failing in the following
> manner:
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > ## log
> > stopifnot(all.equal(log(1:10), log(1:10, exp(1))))
> > stopifnot(all.equal(log10(30), log(30, 10)))
> > stopifnot(all.equal(log2(2^pi), 2^log2(pi)))
> > stopifnot(Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps)
> Error: Mod(pi - log(exp(pi * (0+1i)))/(0+1i)) < .Machine$double.eps is
> not TRUE
> Execution halted
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> 
> I'm compiling on Tru64UNIX 5.1A using DECs C and Fortran compilers and
> perl 5.6.0.  I found the above error in a file named
> 'reg-tests-1.Rout.fail'.  Any help you can give me would be most
> appreciated.

Well, it seems to be an accuracy issue, so the first question would be
what the values on both sides of the equality are (just start up R and
enter the expressions on te command line). You might have

1) Completely wrong results in complex arithmetic 
2) Slightly less than optimal accuracy
3) An underestimated .Machine$double.eps

Case 2) seems most likely, but case 3) has been observed with buggy
compilers that optimize calculations where they shouldn't.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From clists at perrin.socsci.unc.edu  Tue Jun 24 19:33:00 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 13:33:00 -0400 (EDT)
Subject: [R] Adding a calculated variable to a data frame (fwd)
Message-ID: <Pine.LNX.4.53.0306241332210.10465@perrin.socsci.unc.edu>

Thanks to all who answered... this is what I was looking for:

  df$rwa.sum <- df$first.agg+df$first.sub+df$first.agg

thanks to the several people who responded.

ap



From cathey.tommy at epa.gov  Tue Jun 24 20:19:37 2003
From: cathey.tommy at epa.gov (Tommy E. Cathey)
Date: Tue, 24 Jun 2003 14:19:37 -0400
Subject: [R] cumulative frequency distribution plot
Message-ID: <3EF89638.D11BE7C4@epa.gov>

Does R do cumulative frequency distribution plots?

--
Tommy E. Cathey, Senior Scientific Application Consultant
High Performance Computing & Scientific Visualization
SAIC, Supporting the EPA
Research Triangle Park, NC
919-541-1500 EMail: cathey.tommy at epa.gov
My e-mail does not reflect the opinion of SAIC or the EPA.

Federal Contact - John B. Smith
919-541-1087    - smith.johnb at epa.gov



From drf5n at mug.sys.virginia.edu  Tue Jun 24 20:26:42 2003
From: drf5n at mug.sys.virginia.edu (drf5n@mug.sys.virginia.edu)
Date: Tue, 24 Jun 2003 14:26:42 -0400 (EDT)
Subject: [R] Reading graphics files
Message-ID: <Pine.LNX.4.44.0306241356340.490-100000@mug.sys.virginia.edu>

Is there a tool for reading a graphics file into an object?  I might not
be looking with the correct vocabulary, but I'm finding lots of references
to producing graphics from R, but not any for inputting graphics into R.

I'd like to use a jpeg image and have the data available in R for
analysis.  I could convert the image to another format, PPM perhaps, and
parse that in, GRASS seems like overkill for a small project, and reading
an image seems like something that someone else has probably already done.

Any pointers?

?png      # output only
?bitmap   # output only
help.search("ppm") |image|graphics|read|png|bitmap all seem like outputs
R-data.pdf points the way -- Has anyone imported a graphic file?

Thanks for your time,
Dave
-- 
 Dave Forrest    (434)924-3954w(111B) (804)642-0662h (804)695-2026p
 drf5n at maplepark.com            http://mug.sys.virginia.edu/~drf5n/



From spencer.graves at pdf.com  Tue Jun 24 20:38:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 Jun 2003 11:38:59 -0700
Subject: [R] cumulative frequency distribution plot
References: <3EF89638.D11BE7C4@epa.gov>
Message-ID: <3EF89AC3.3020507@pdf.com>

	  It depends on what you mean by that.  Consider the following:

 > x <- rnorm(100)
 > plot(x, pnorm(x))

	  This plot is a cdf of pseudo-normal data.

	  I've seen many people make these things, and I've never understoon 
what they see in these plots that they can't see better in a normal 
probability plot:

 > qqnorm(x, datax=T)

	  What can you see in the first type plot that is not so easily seen in 
the second?

Spencer Graves

Tommy E. Cathey wrote:
> Does R do cumulative frequency distribution plots?
> 
> --
> Tommy E. Cathey, Senior Scientific Application Consultant
> High Performance Computing & Scientific Visualization
> SAIC, Supporting the EPA
> Research Triangle Park, NC
> 919-541-1500 EMail: cathey.tommy at epa.gov
> My e-mail does not reflect the opinion of SAIC or the EPA.
> 
> Federal Contact - John B. Smith
> 919-541-1087    - smith.johnb at epa.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jlewis at genomecorp.com  Tue Jun 24 20:42:00 2003
From: jlewis at genomecorp.com (Jeff Lewis)
Date: Tue, 24 Jun 2003 14:42:00 -0400
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
Message-ID: <414DEAD4479990458D14E47B61039E5B0144496B@apollo.ad.genomecorp.com>

> -----Original Message-----
> From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Tuesday, June 24, 2003 1:20 PM
> To: Jeff Lewis
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R-1.7.1 regression test failure on 
> alphaev68-dec-osf5.1
> 
> 
> "Jeff Lewis" <jlewis at genomecorp.com> writes:
> 
> > I'm attempting to compile and install R version 1.7.1 for 
> my statistical
> > geneticists.  It seems to compile correctly -- that is, it compiles
> > without errors -- but the regression test is failing in the 
> following
> > manner:
> > 
> > =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > > ## log
> > > stopifnot(all.equal(log(1:10), log(1:10, exp(1))))
> > > stopifnot(all.equal(log10(30), log(30, 10)))
> > > stopifnot(all.equal(log2(2^pi), 2^log2(pi)))
> > > stopifnot(Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps)
> > Error: Mod(pi - log(exp(pi * (0+1i)))/(0+1i)) < 
> .Machine$double.eps is
> > not TRUE
> > Execution halted
> > =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > 
> > I'm compiling on Tru64UNIX 5.1A using DECs C and Fortran 
> compilers and
> > perl 5.6.0.  I found the above error in a file named
> > 'reg-tests-1.Rout.fail'.  Any help you can give me would be most
> > appreciated.
> 
> Well, it seems to be an accuracy issue, so the first question would be
> what the values on both sides of the equality are (just start up R and
> enter the expressions on te command line). You might have
> 
> 1) Completely wrong results in complex arithmetic 
> 2) Slightly less than optimal accuracy
> 3) An underestimated .Machine$double.eps
> 
> Case 2) seems most likely, but case 3) has been observed with buggy
> compilers that optimize calculations where they shouldn't.



Thanks for the quick response.  The two sides of the equality are
definately different.  Here's what I'm seeing

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> pi
[1] 3.141593

> 1i
[1] 0+1i

> pi*1i
[1] 0+3.141593i

> exp(pi*1i)
[1] -1+1.224647e-16i

> log(exp(pi*1i))
[1] 0+3.141593i

> log(exp(pi*1i)) / 1i

[1] 3.141593+0i

> pi - log(exp(pi*1i)) / 1i
[1] 4.440892e-16+0i

> Mod(pi - log(exp(pi*1i)) / 1i)
[1] 4.440892e-16

> .Machine$double.eps
[1] 2.220446e-16

> Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps
[1] FALSE

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

I get the same thing from R 1.6.2, which I compiled about six months
ago.  Is there anything I can/should do to fix this?



From Roger.Bivand at nhh.no  Tue Jun 24 20:44:09 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 24 Jun 2003 20:44:09 +0200 (CEST)
Subject: [R] Reading graphics files
In-Reply-To: <Pine.LNX.4.44.0306241356340.490-100000@mug.sys.virginia.edu>
Message-ID: <Pine.LNX.4.44.0306242043060.19201-100000@reclus.nhh.no>

On Tue, 24 Jun 2003 drf5n at mug.sys.virginia.edu wrote:

> Is there a tool for reading a graphics file into an object?  I might not
> be looking with the correct vocabulary, but I'm finding lots of references
> to producing graphics from R, but not any for inputting graphics into R.
> 
> I'd like to use a jpeg image and have the data available in R for
> analysis.  I could convert the image to another format, PPM perhaps, and
> parse that in, GRASS seems like overkill for a small project, and reading
> an image seems like something that someone else has probably already done.
> 

library(pixmap), perhaps? Reads PNM (so PPM, PGM, PBM).

> Any pointers?
> 
> ?png      # output only
> ?bitmap   # output only
> help.search("ppm") |image|graphics|read|png|bitmap all seem like outputs
> R-data.pdf points the way -- Has anyone imported a graphic file?
> 
> Thanks for your time,
> Dave
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From matthew_wiener at merck.com  Tue Jun 24 20:55:35 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 24 Jun 2003 14:55:35 -0400
Subject: [R] cumulative frequency distribution plot
Message-ID: <AEBD81486231A343B1813FE62D3352250369A033@usrymx15.merck.com>

You can also take a look at "ecdf" in the "stepfun" package.

Hope this helps,
Matt

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Tuesday, June 24, 2003 2:39 PM
To: Tommy E. Cathey
Cc: r-help at r-project.org
Subject: Re: [R] cumulative frequency distribution plot


	  It depends on what you mean by that.  Consider the following:

 > x <- rnorm(100)
 > plot(x, pnorm(x))

	  This plot is a cdf of pseudo-normal data.

	  I've seen many people make these things, and I've never understoon

what they see in these plots that they can't see better in a normal 
probability plot:

 > qqnorm(x, datax=T)

	  What can you see in the first type plot that is not so easily seen
in 
the second?

Spencer Graves

Tommy E. Cathey wrote:
> Does R do cumulative frequency distribution plots?
> 
> --
> Tommy E. Cathey, Senior Scientific Application Consultant
> High Performance Computing & Scientific Visualization
> SAIC, Supporting the EPA
> Research Triangle Park, NC
> 919-541-1500 EMail: cathey.tommy at epa.gov
> My e-mail does not reflect the opinion of SAIC or the EPA.
> 
> Federal Contact - John B. Smith
> 919-541-1087    - smith.johnb at epa.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, cont... {{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Jun 24 21:02:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 24 Jun 2003 19:02:57 -0000
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
In-Reply-To: <414DEAD4479990458D14E47B61039E5B0144496B@apollo.ad.genomecorp.com>
References: <414DEAD4479990458D14E47B61039E5B0144496B@apollo.ad.genomecorp.com>
Message-ID: <x2brwnb8mz.fsf@biostat.ku.dk>

"Jeff Lewis" <jlewis at genomecorp.com> writes:

> > Well, it seems to be an accuracy issue, so the first question would be
> > what the values on both sides of the equality are (just start up R and
> > enter the expressions on te command line). You might have
> > 
> > 1) Completely wrong results in complex arithmetic 
> > 2) Slightly less than optimal accuracy
> > 3) An underestimated .Machine$double.eps
> > 
> > Case 2) seems most likely, but case 3) has been observed with buggy
> > compilers that optimize calculations where they shouldn't.
> 
> 
> 
> Thanks for the quick response.  The two sides of the equality are
> definately different.  Here's what I'm seeing
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > pi
> [1] 3.141593
> 
> > 1i
> [1] 0+1i
> 
> > pi*1i
> [1] 0+3.141593i
> 
> > exp(pi*1i)
> [1] -1+1.224647e-16i
> 
> > log(exp(pi*1i))
> [1] 0+3.141593i
> 
> > log(exp(pi*1i)) / 1i
> 
> [1] 3.141593+0i
> 
> > pi - log(exp(pi*1i)) / 1i
> [1] 4.440892e-16+0i
> 
> > Mod(pi - log(exp(pi*1i)) / 1i)
> [1] 4.440892e-16
> 
> > .Machine$double.eps
> [1] 2.220446e-16
> 
> > Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps
> [1] FALSE
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> 
> I get the same thing from R 1.6.2, which I compiled about six months
> ago.  Is there anything I can/should do to fix this?

Not really. It seems that your platform just has slightly less
accurate complex log/exp routines than the most common ones (Linux and
Sparc/Solaris both give exact zero). Probably the check is simply
overly stringent.

You might want to change the check to say  ... < 3*.Machine$double.eps
or so and rerun, to check whether the rest of the checks pass.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Tue Jun 24 21:13:25 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 24 Jun 2003 12:13:25 -0700
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
References: <414DEAD4479990458D14E47B61039E5B0144496B@apollo.ad.genomecorp.com>
Message-ID: <3EF8A2D5.1040509@pdf.com>

I just tried this with both R 1.6.2 and S-Plus 6.1 under Windows 2000: 
I got the same .Machine$double.eps as reported below, but I did NOT get 
the problem:

pi - log(exp(pi*1i)) / 1i

# reported below:  [1] 4.440892e-16+0i
# R1.6.2 and S-Plus 6.1 both gave 0+oi

Mod(pi - log(exp(pi*1i)) / 1i)

# reported below: 4.440892e-16
# R1.6.2 and S-Plus 6.1 both gave 0

.Machine$double.eps

# Same as reported: 2.220446e-16

Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps

# reported below:  [1] FALSE
# R1.6.2 and S-Plus 6.1 both gave T

hth.  spencer graves

Jeff Lewis wrote:
>>-----Original Message-----
>>From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk] 
>>Sent: Tuesday, June 24, 2003 1:20 PM
>>To: Jeff Lewis
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] R-1.7.1 regression test failure on 
>>alphaev68-dec-osf5.1
>>
>>
>>"Jeff Lewis" <jlewis at genomecorp.com> writes:
>>
>>
>>>I'm attempting to compile and install R version 1.7.1 for 
>>
>>my statistical
>>
>>>geneticists.  It seems to compile correctly -- that is, it compiles
>>>without errors -- but the regression test is failing in the 
>>
>>following
>>
>>>manner:
>>>
>>>=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
>>>
>>>>## log
>>>>stopifnot(all.equal(log(1:10), log(1:10, exp(1))))
>>>>stopifnot(all.equal(log10(30), log(30, 10)))
>>>>stopifnot(all.equal(log2(2^pi), 2^log2(pi)))
>>>>stopifnot(Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps)
>>>
>>>Error: Mod(pi - log(exp(pi * (0+1i)))/(0+1i)) < 
>>
>>.Machine$double.eps is
>>
>>>not TRUE
>>>Execution halted
>>>=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
>>>
>>>I'm compiling on Tru64UNIX 5.1A using DECs C and Fortran 
>>
>>compilers and
>>
>>>perl 5.6.0.  I found the above error in a file named
>>>'reg-tests-1.Rout.fail'.  Any help you can give me would be most
>>>appreciated.
>>
>>Well, it seems to be an accuracy issue, so the first question would be
>>what the values on both sides of the equality are (just start up R and
>>enter the expressions on te command line). You might have
>>
>>1) Completely wrong results in complex arithmetic 
>>2) Slightly less than optimal accuracy
>>3) An underestimated .Machine$double.eps
>>
>>Case 2) seems most likely, but case 3) has been observed with buggy
>>compilers that optimize calculations where they shouldn't.
> 
> 
> 
> 
> Thanks for the quick response.  The two sides of the equality are
> definately different.  Here's what I'm seeing
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> 
>>pi
> 
> [1] 3.141593
> 
> 
>>1i
> 
> [1] 0+1i
> 
> 
>>pi*1i
> 
> [1] 0+3.141593i
> 
> 
>>exp(pi*1i)
> 
> [1] -1+1.224647e-16i
> 
> 
>>log(exp(pi*1i))
> 
> [1] 0+3.141593i
> 
> 
>>log(exp(pi*1i)) / 1i
> 
> 
> [1] 3.141593+0i
> 
> 
>>pi - log(exp(pi*1i)) / 1i
> 
> [1] 4.440892e-16+0i
> 
> 
>>Mod(pi - log(exp(pi*1i)) / 1i)
> 
> [1] 4.440892e-16
> 
> 
>>.Machine$double.eps
> 
> [1] 2.220446e-16
> 
> 
>>Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps
> 
> [1] FALSE
> 
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> 
> I get the same thing from R 1.6.2, which I compiled about six months
> ago.  Is there anything I can/should do to fix this?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From clists at perrin.socsci.unc.edu  Tue Jun 24 21:43:24 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 15:43:24 -0400 (EDT)
Subject: [R] Can't load e1071
Message-ID: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>

After upgrading to 1.7.0 under debian linux, I can't get e1071 working
properly.

The first problem I had was that g++-3.0 was the standard compiler but
wasn't installed, so I installed it. e1071 then installed correctly, but I
get the following:

aperrin at perrin:~/afshome/papers/authoritarian/R$ R

R : Copyright 2003, The R Development Core Team
Version 1.7.0  (2003-04-16)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

[Previously saved workspace restored]

> library(e1071)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"/usr/local/lib/R/site-library/e1071/libs/e1071.so":
  /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot dynamically
load executable
Error in library(e1071) : .First.lib failed


any suggestions? Thanks.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From david.meyer at ci.tuwien.ac.at  Tue Jun 24 21:59:06 2003
From: david.meyer at ci.tuwien.ac.at (David Meyer)
Date: Tue, 24 Jun 2003 21:59:06 +0200
Subject: [R] Can't load e1071
In-Reply-To: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>;
	from clists@perrin.socsci.unc.edu on Tue, Jun 24, 2003 at
	21:43:24 +0200
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
Message-ID: <20030624195906.GE25655@boromir.ci.tuwien.ac.at>

Andrew,

1) The current R version is 1.7.1
2) Which version of `e1071' are you using?
3) Does the `e1071.so' file exist (in e1071/libs)?

best,
David.


On 2003.06.24 21:43, Andrew Perrin wrote:
> After upgrading to 1.7.0 under debian linux, I can't get e1071 working
> properly.
> 
> The first problem I had was that g++-3.0 was the standard compiler but
> wasn't installed, so I installed it. e1071 then installed correctly,
> but I
> get the following:
> 
> aperrin at perrin:~/afshome/papers/authoritarian/R$ R
> 
> R : Copyright 2003, The R Development Core Team
> Version 1.7.0  (2003-04-16)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type `license()' or `licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type `contributors()' for more information.
> 
> Type `demo()' for some demos, `help()' for on-line help, or
> `help.start()' for a HTML browser interface to help.
> Type `q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> > library(e1071)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "/usr/local/lib/R/site-library/e1071/libs/e1071.so":
>   /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot
> dynamically
> load executable
> Error in library(e1071) : .First.lib failed
> 
> 
> any suggestions? Thanks.
> 
> ----------------------------------------------------------------------
> Andrew J Perrin - http://www.unc.edu/~aperrin
> Assistant Professor of Sociology, U of North Carolina, Chapel Hill
> clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From clists at perrin.socsci.unc.edu  Tue Jun 24 22:04:40 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 16:04:40 -0400 (EDT)
Subject: [R] Can't load e1071
In-Reply-To: <20030624195906.GE25655@boromir.ci.tuwien.ac.at>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>
Message-ID: <Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>

On Tue, 24 Jun 2003, David Meyer wrote:

> Andrew,
>
> 1) The current R version is 1.7.1

hmmm...:
perrin:/usr/local/lib/R/site-library# apt-get upgrade
Reading Package Lists... Done
Building Dependency Tree... Done
0 packages upgraded, 0 newly installed, 0 to remove and 0  not upgraded.
perrin:/usr/local/lib/R/site-library# dpkg -l r-base
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Installed/Config-files/Unpacked/Failed-config/Half-installed
|/ Err?=(none)/Hold/Reinst-required/X=both-problems (Status,Err:
uppercase=bad)
||/ Name           Version        Description
+++-==============-==============-============================================
ii  r-base         1.7.0-0.cran.1 GNU R statistical computing language and
env
perrin:/usr/local/lib/R/site-library#



apparently there's not a debian package available?


> 2) Which version of `e1071' are you using?

This is based on doing an install.packages('e1071') from within R as root
(which is how I have generally installed packages in the past).

> 3) Does the `e1071.so' file exist (in e1071/libs)?
>

Yes:

aperrin at perrin:~/afshome/papers/authoritarian/R$ ls -l
/usr/local/lib/R/site-library/e1071/libs/e1071.so
-rwxr-xr-x    1 root     staff      109247 Jun 24 15:41
/usr/local/lib/R/site-library/e1071/libs/e1071.so


> best,
> David.
>

Thanks,
Andy

>
> On 2003.06.24 21:43, Andrew Perrin wrote:
> > After upgrading to 1.7.0 under debian linux, I can't get e1071 working
> > properly.
> >
> > The first problem I had was that g++-3.0 was the standard compiler but
> > wasn't installed, so I installed it. e1071 then installed correctly,
> > but I
> > get the following:
> >
> > aperrin at perrin:~/afshome/papers/authoritarian/R$ R
> >
> > R : Copyright 2003, The R Development Core Team
> > Version 1.7.0  (2003-04-16)
> >
> > R is free software and comes with ABSOLUTELY NO WARRANTY.
> > You are welcome to redistribute it under certain conditions.
> > Type `license()' or `licence()' for distribution details.
> >
> > R is a collaborative project with many contributors.
> > Type `contributors()' for more information.
> >
> > Type `demo()' for some demos, `help()' for on-line help, or
> > `help.start()' for a HTML browser interface to help.
> > Type `q()' to quit R.
> >
> > [Previously saved workspace restored]
> >
> > > library(e1071)
> > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> >         unable to load shared library
> > "/usr/local/lib/R/site-library/e1071/libs/e1071.so":
> >   /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot
> > dynamically
> > load executable
> > Error in library(e1071) : .First.lib failed
> >
> >
> > any suggestions? Thanks.
> >
> > ----------------------------------------------------------------------
> > Andrew J Perrin - http://www.unc.edu/~aperrin
> > Assistant Professor of Sociology, U of North Carolina, Chapel Hill
> > clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>


----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From hodgess at uhddx01.dt.uh.edu  Tue Jun 24 23:07:30 2003
From: hodgess at uhddx01.dt.uh.edu (Erin Hodgess)
Date: Tue, 24 Jun 2003 16:07:30 -0500 (CDT)
Subject: [R] data.entry function
Message-ID: <200306242107.QAA18368@uhddx01.dt.uh.edu>

Dear R People:

Does anyone use the data.entry  function, please?

I would like to create a data frame with a character column1, 
a numeric colum2, and a numeric col3.

I would think that

data.entry(x,y,z)

would work.

However, I get a syntax error for function de.

Does anyone have any hints please?

I would like to show this in one of my classes.  They are
have a terrible time with the command line.

thanks so much!!!

Sincerely,
Erin 

mailto: hodgesse at uhddx01.dt.uh.edu



From apjaworski at mmm.com  Wed Jun 25 01:02:41 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 24 Jun 2003 18:02:41 -0500
Subject: [R] Rcmdr active data set
Message-ID: <OF7B70202D.16AA6908-ON86256D4F.007E1695@mmm.com>

Hi,

This seems a very basic problem, but I cannot seem to find the solution to
it.

I am trying to use Rcmdr on a data frame I created in my current R session.
More specifically, I did

 x <- data.frame(matrix(0, ncol=3, nrow=5))
library(Rcmdr)

Now I would like to be able to edit, view and perhaps analyze x, but I
cannot seem to figure out how to make it active.  Any hint will be welcome.

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From ma at ne.su.se  Wed Jun 25 01:37:24 2003
From: ma at ne.su.se (Mahmood ARAI)
Date: Wed, 25 Jun 2003 01:37:24 +0200
Subject: [R] Re: data.entry function
In-Reply-To: <200306242107.QAA18368@uhddx01.dt.uh.edu> 
References: <200306242107.QAA18368@uhddx01.dt.uh.edu>
Message-ID: <20030624233724.4A1F738004@mbox1.su.se>

> data.entry("")
opens the data editor and the varaibles namn and type can be set using
the mouse (left b.) pointing at the top row. 

Erin Hodgess writes: 

> Dear R People: 
> 
> Does anyone use the data.entry  function, please? 
> 
> I would like to create a data frame with a character column1, 
> a numeric colum2, and a numeric col3. 
> 
> I would think that 
> 
> data.entry(x,y,z) 
> 
> would work. 
> 
> However, I get a syntax error for function de. 
> 
> Does anyone have any hints please? 
> 
> I would like to show this in one of my classes.  They are
> have a terrible time with the command line. 
> 
> thanks so much!!! 
> 
> Sincerely,
> Erin  
> 
> mailto: hodgesse at uhddx01.dt.uh.edu 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 


http://www.ne.su.se/~ma



From clists at perrin.socsci.unc.edu  Wed Jun 25 01:47:19 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 19:47:19 -0400 (EDT)
Subject: [R] Can't load e1071
In-Reply-To: <20030624212429.GA3364@sonny.eddelbuettel.com>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>
	<Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>
	<20030624212429.GA3364@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>

OK, thanks. Is that the problem with e1071? Any advice on getting it
loaded?

Thanks.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Tue, 24 Jun 2003, Dirk Eddelbuettel wrote:

> On Tue, Jun 24, 2003 at 04:04:40PM -0400, Andrew Perrin wrote:
> > ii  r-base         1.7.0-0.cran.1 GNU R statistical computing language and
> > apparently there's not a debian package available?
>
> It's in Debian unstable as my post to r-help said. Works on testing too, but
> have to install it.
>
> Dirk
>
> --
> Don't drink and derive. Alcohol and analysis don't mix.
>



From edd at debian.org  Wed Jun 25 02:12:19 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 24 Jun 2003 19:12:19 -0500
Subject: [R] Can't load e1071
In-Reply-To: <Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>
	<Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>
	<20030624212429.GA3364@sonny.eddelbuettel.com>
	<Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>
Message-ID: <20030625001219.GA4531@sonny.eddelbuettel.com>


On Tue, Jun 24, 2003 at 07:47:19PM -0400, Andrew Perrin wrote:
> OK, thanks. Is that the problem with e1071? Any advice on getting it
> loaded?

[ It is considered impolite to reply to private messages via the list. ]

You misunderstand:  R in Debian is in unstable. e1071 is an add-on package
that always loaded without any issues.

Hth, Dirk

> On Tue, 24 Jun 2003, Dirk Eddelbuettel wrote:
> 
> > On Tue, Jun 24, 2003 at 04:04:40PM -0400, Andrew Perrin wrote:
> > > ii  r-base         1.7.0-0.cran.1 GNU R statistical computing language and
> > > apparently there's not a debian package available?
> >
> > It's in Debian unstable as my post to r-help said. Works on testing too, but
> > have to install it.
> >
> > Dirk
> >
> > --
> > Don't drink and derive. Alcohol and analysis don't mix.
> >
> 

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From clists at perrin.socsci.unc.edu  Wed Jun 25 02:44:57 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 24 Jun 2003 20:44:57 -0400 (EDT)
Subject: [R] Can't load e1071
In-Reply-To: <20030625001219.GA4531@sonny.eddelbuettel.com>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>
	<Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>
	<20030624212429.GA3364@sonny.eddelbuettel.com>
	<Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>
	<20030625001219.GA4531@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.53.0306242023410.16284@perrin.socsci.unc.edu>

On Tue, 24 Jun 2003, Dirk Eddelbuettel wrote:

>
> On Tue, Jun 24, 2003 at 07:47:19PM -0400, Andrew Perrin wrote:
> > OK, thanks. Is that the problem with e1071? Any advice on getting it
> > loaded?
>
> [ It is considered impolite to reply to private messages via the list. ]

Sorry for any impoliteness. I was trying to ask the whole list, since I
suspect the two issues are different.

>
> You misunderstand:  R in Debian is in unstable. e1071 is an add-on package
> that always loaded without any issues.
>

No, I don't misunderstand. R 1.7.0 is in debian-stable, which is why it's
what I'm running. I appreciate that information. e1071 is an add-on
package that used to load without any issues, but it does not do so on
either of my debian-stable machines. They both fail with the same message:

> library(e1071)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"/usr/local/lib/R/site-library/e1071/libs/e1071.so":
  /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot dynamically
load executable
Error in library(e1071) : .First.lib failed


...even though /usr/local/lib/R/site-library/e1071/libs/e1071.so exists
and is readable.


That was the reason for the original post; someone else noted that my
installation of R was old, which led to this thread.

The first problem I had is that R apparently assumes that g++-3.0 will be
the compiler, since:

aperrin at joehill:/usr/local/lib/R/site-library/e1071/libs$ R CMD config CXX
g++-3.0


...even if g++-3.0 isn't installed on the machine:
joehill:/usr/src/e1071# dpkg -l | grep g++
ii  g++            2.95.4-14      The GNU C++ compiler.
ii  g++-2.95       2.95.4-11woody The GNU C++ compiler.


If I install g++-3.0, then install e1071 from the net, the installation
works fine, but using the library gives the error above.

Thanks for your interest and help.



----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From edd at debian.org  Wed Jun 25 03:33:54 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 24 Jun 2003 20:33:54 -0500
Subject: [R] Can't load e1071
In-Reply-To: <Pine.LNX.4.53.0306242023410.16284@perrin.socsci.unc.edu>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>
	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>
	<Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>
	<20030624212429.GA3364@sonny.eddelbuettel.com>
	<Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>
	<20030625001219.GA4531@sonny.eddelbuettel.com>
	<Pine.LNX.4.53.0306242023410.16284@perrin.socsci.unc.edu>
Message-ID: <20030625013354.GA4913@sonny.eddelbuettel.com>

On Tue, Jun 24, 2003 at 08:44:57PM -0400, Andrew Perrin wrote:
> > [ It is considered impolite to reply to private messages via the list. ]
> 
> Sorry for any impoliteness. I was trying to ask the whole list, since I
> suspect the two issues are different.

That's fine, and your privilege. But as a general rule, do not relay private
messages.

> > You misunderstand:  R in Debian is in unstable. e1071 is an add-on package
> > that always loaded without any issues.
> >
> 
> No, I don't misunderstand. R 1.7.0 is in debian-stable, which is why it's

False. Debian 'stable' has 1.5.0 (in a bugfix release). Debian 'testing' has
1.6.0. CRAN has a testing release of 1.7.0 which is probably what you have.

> > library(e1071)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "/usr/local/lib/R/site-library/e1071/libs/e1071.so":
>   /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot dynamically
> load executable
> Error in library(e1071) : .First.lib failed
> 
> ...even though /usr/local/lib/R/site-library/e1071/libs/e1071.so exists
> and is readable.

So remove e1071 and reinstall, but see further down.

> The first problem I had is that R apparently assumes that g++-3.0 will be
> the compiler, since:
> 
> aperrin at joehill:/usr/local/lib/R/site-library/e1071/libs$ R CMD config CXX
> g++-3.0
> 
> 
> ...even if g++-3.0 isn't installed on the machine:
> joehill:/usr/src/e1071# dpkg -l | grep g++
> ii  g++            2.95.4-14      The GNU C++ compiler.
> ii  g++-2.95       2.95.4-11woody The GNU C++ compiler.

That doesn't matter. What matters is what is hard-coded from _R built time_
in $RHOME/etc/Makeconf (which on Debian is also /etc/R/Makeconf).  Have a
look at that file, it will have hints as to under which version your
r-base-core package was built. I.e. mine shows in FLIBS that 3.3 was used.

But a much simpler way is to use the Debian package management system --
look at the Depends line of 'dpkg -s r-base-core'. Mine has e.g.  
     libg2c0 (>= 1:3.3-0pre9)
revealing that g77 from gcc version 3.3 was used, consistent with what FLIBS
had.     

> If I install g++-3.0, then install e1071 from the net, the installation
> works fine, but using the library gives the error above.

You were probably mis-matching gcc/g++ versions between the version of R you
got from CRAN and the compiler you have installed.

Hope this helps, feel free to ask me in private if you're still fuzzy.

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From Arnaud.Dowkiw at dpi.qld.gov.au  Wed Jun 25 03:37:55 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Wed, 25 Jun 2003 11:37:55 +1000
Subject: [R] Pairs with different colours
Message-ID: <C2C6EA6C4DADB348BFDF58894B039012012731FF@kinsrv001.dpi.qld.gov.au>


Does anybody know how to make pairs graphics with dots of different colours depending on the value of a categorical variable ?

Thanks,

Arnaud
*************************
Arnaud DOWKIW
Department of Primary Industries
J. Bjelke-Petersen Research Station
KINGAROY, QLD 4610
Australia
T : + 61 7 41 600 700
T : + 61 7 41 600 728 (direct)
F : + 61 7 41 600 760
**************************
 

********************************DISCLAIMER**********************... {{dropped}}



From kwan022 at stat.auckland.ac.nz  Wed Jun 25 03:47:20 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 25 Jun 2003 13:47:20 +1200 (NZST)
Subject: [R] Pairs with different colours
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B039012012731FF@kinsrv001.dpi.qld.gov.au>
Message-ID: <Pine.LNX.4.44.0306251346420.18797-100000@stat55.stat.auckland.ac.nz>

?pairs

It is clearly documented.  It's the first example:
 data(iris)
 pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species", 
       pch = 21, bg = c("red", "green3", "blue")[codes(iris$Species)])

On Wed, 25 Jun 2003, Dowkiw, Arnaud wrote:

> Does anybody know how to make pairs graphics with dots of different colours depending on the value of a categorical variable ?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From rmurison at turing.une.edu.au  Wed Jun 25 03:53:02 2003
From: rmurison at turing.une.edu.au (Bob Murison)
Date: Wed, 25 Jun 2003 11:53:02 +1000
Subject: [R] Pairs with different colours
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B039012012731FF@kinsrv001.dpi.qld.gov.au>
References: <C2C6EA6C4DADB348BFDF58894B039012012731FF@kinsrv001.dpi.qld.gov.au>
Message-ID: <3EF9007E.5060003@turing.une.edu.au>

library(lattice)
?splom


Dowkiw, Arnaud wrote:

>Does anybody know how to make pairs graphics with dots of different colours depending on the value of a categorical variable ?
>
>Thanks,
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From jfox at mcmail.cis.mcmaster.ca  Wed Jun 25 04:20:53 2003
From: jfox at mcmail.cis.mcmaster.ca (John Fox)
Date: Tue, 24 Jun 2003 22:20:53 -0400 (EDT)
Subject: [R] Rcmdr active data set
In-Reply-To: <OF7B70202D.16AA6908-ON86256D4F.007E1695@mmm.com>
Message-ID: <Pine.SOL.4.33.0306242216080.27006-100000@mcmail.cis.mcmaster.ca>

Dear Andy,

The problem here is a name clash -- the data frame name (x) in the global
environment is being shadowed by a local variable of the same name.

Clearly this is
undesirable, and I'll try to figure out a way to avoid it for the next
version of the Rcmdr package, but an immediate solution is to use a
different name, such as mydata. (I assume that you tried the "Data ->
Active data set -> Select active data set" and found that the data frame x
wasn't listed as it should have been.)

Thanks for bringing the problem to my attention.

John

On Tue, 24 Jun 2003 apjaworski at mmm.com wrote:

> Hi,
>
> This seems a very basic problem, but I cannot seem to find the solution to
> it.
>
> I am trying to use Rcmdr on a data frame I created in my current R session.
> More specifically, I did
>
>  x <- data.frame(matrix(0, ncol=3, nrow=5))
> library(Rcmdr)
>
> Now I would like to be able to edit, view and perhaps analyze x, but I
> cannot seem to figure out how to make it active.  Any hint will be welcome.
>
> Andy
>
> __________________________________
> Andy Jaworski
> Engineering Systems Technology Center
> 3M Center, 518-1-01
> St. Paul, MN 55144-1000
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From splante at globetrotter.net  Wed Jun 25 04:41:12 2003
From: splante at globetrotter.net (=?ISO-8859-1?Q?S=E9bastien_Plante?=)
Date: Tue, 24 Jun 2003 22:41:12 -0400
Subject: [R] Can't save a graph to pdf in R for MacOS
Message-ID: <7C277952-A6B6-11D7-9EF4-000A958FB4AC@globetrotter.net>

Hi,

I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X 
10.2.6. When I send a graph to the pdf device (or any other devices), I 
get a zero KB file name "Rplots.pdf".

Before sending my graph to the output, I did:

 > dev.off()
 > pdf()
 > boxplot(... my graph commands...)
 > dev.off()

Is this the correct procedure?  I did the same procedure on another PC 
running Linux (R 1.6) and it work well.

Please help!

Thanks,

S?bastien Plante



From r.hankin at auckland.ac.nz  Wed Jun 25 06:13:18 2003
From: r.hankin at auckland.ac.nz (Robin Hankin)
Date: Wed, 25 Jun 2003 16:13:18 +1200
Subject: [R] frequency table
In-Reply-To: <20021110072606.A15146@cattell.psych.upenn.edu> (message from
	Jonathan Baron on Sun, 10 Nov 2002 07:26:06 -0500)
References: <3DCE3B32.9040502@host.sk>
	<20021110072606.A15146@cattell.psych.upenn.edu>
Message-ID: <200306250413.h5P4DIU8017787@r.hankin.sges.auckland.ac.nz>

Professor Baron writes:

> 
> A neat trick with table() is that you can use it to tabulate
> columns of a matrix (for example) with:
> 
> apply(mymatrix,2,table)
> 

OK, I'll bite:

> x1 <- matrix(1:3,7,4,byrow=T)
Warning message: 
Replacement length not a multiple of the elements to replace in matrix(...) 
> dim(x1)
[1] 7 4
> apply(x1,2,table)
  [,1] [,2] [,3] [,4]
1    3    2    2    3
2    2    3    2    2
3    2    2    3    2
> 

fine (this is what I would have expected).  But now...

> x2 <- matrix(c(1,2,1,2,1,3,1,3,1,1,1,2,2,2,2,3,2,3,3,2,3,1,1,3,3,1,3,3),7,4)
> dim(x2)
[1] 7 4
> apply(x2,2,table)
[[1]]

1 2 3 
4 2 1 

[[2]]

1 2 3 
3 3 1 

[[3]]

2 3 
3 4 

[[4]]

1 3 
3 4 


Why the difference in output format?

[presumably it's because table() cuts its cloth accordingly, unlike
tabulate()...but how can apply() know this?  My real question would be
how to turn an expression like the list given by apply(x2,2,table)
into a nice matrix].



-- 

Robin Hankin, Lecturer,
School of Geography and Environmental Science
Tamaki Campus
Private Bag 92019 Auckland
New Zealand

r.hankin at auckland.ac.nz
tel 0064-9-373-7599 x6820; FAX 0064-9-373-7042



From mpiorecky at hotmail.com  Wed Jun 25 06:28:04 2003
From: mpiorecky at hotmail.com (Mark Piorecky)
Date: Tue, 24 Jun 2003 22:28:04 -0600
Subject: [R] Rcmd SHLIB on Windows
Message-ID: <BAY8-DAV43L6ntBscu00002d930@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030624/3887ce19/attachment.pl

From Bill.Venables at csiro.au  Wed Jun 25 06:45:25 2003
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Wed, 25 Jun 2003 14:45:25 +1000
Subject: [R] frequency table
Message-ID: <E09E527B56BE2D438A3D6A246DDD27A9165D1F@roper-cv.qld.cmis.CSIRO.AU>

Robin,

the initial output from apply() is always in the form you have below, but if
it can be 'simplified' into a structure like the matrix, it does so.  The
same thing happens with sapply().

If you want to produce a nice matrix as the out put you have to ensure that
the simplification is possible.  Here is one way.

> apply(x2, 2, function(x, v) table(factor(x, levels=v)), 
	sort(unique(as.vector(x2))))
  [,1] [,2] [,3] [,4]
1    4    3    0    3
2    2    3    3    0
3    1    1    4    4

Bill Venables.

-----Original Message-----
From: Robin Hankin [mailto:r.hankin at auckland.ac.nz]
Sent: Wednesday, June 25, 2003 2:13 PM
To: baron at cattell.psych.upenn.edu
Cc: r-help at stat.math.ethz.ch; toth at host.sk
Subject: Re: [R] frequency table


Professor Baron writes:

> 
> A neat trick with table() is that you can use it to tabulate
> columns of a matrix (for example) with:
> 
> apply(mymatrix,2,table)
> 

OK, I'll bite:

> x1 <- matrix(1:3,7,4,byrow=T)
Warning message: 
Replacement length not a multiple of the elements to replace in matrix(...) 
> dim(x1)
[1] 7 4
> apply(x1,2,table)
  [,1] [,2] [,3] [,4]
1    3    2    2    3
2    2    3    2    2
3    2    2    3    2
> 

fine (this is what I would have expected).  But now...

> x2 <-
matrix(c(1,2,1,2,1,3,1,3,1,1,1,2,2,2,2,3,2,3,3,2,3,1,1,3,3,1,3,3),7,4)
> dim(x2)
[1] 7 4
> apply(x2,2,table)
[[1]]

1 2 3 
4 2 1 

[[2]]

1 2 3 
3 3 1 

[[3]]

2 3 
3 4 

[[4]]

1 3 
3 4 


Why the difference in output format?

[presumably it's because table() cuts its cloth accordingly, unlike
tabulate()...but how can apply() know this?  My real question would be
how to turn an expression like the list given by apply(x2,2,table)
into a nice matrix].



-- 

Robin Hankin, Lecturer,
School of Geography and Environmental Science
Tamaki Campus
Private Bag 92019 Auckland
New Zealand

r.hankin at auckland.ac.nz
tel 0064-9-373-7599 x6820; FAX 0064-9-373-7042

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From azzalini at stat.unipd.it  Wed Jun 25 08:47:00 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Wed, 25 Jun 2003 08:47:00 +0200
Subject: [R] cumulative frequency distribution plot
In-Reply-To: <3EF89638.D11BE7C4@epa.gov>
References: <3EF89638.D11BE7C4@epa.gov>
Message-ID: <20030625064700.38CDE7CA824@tango.stat.unipd.it>

On Tuesday 24 June 2003 20:19, Tommy E. Cathey wrote:
> Does R do cumulative frequency distribution plots?


I am not sure you what you mean by that;
if it is "empirical cumulative distribution function", then try this:

n <-10
x <- rnorm(n)
plot(c(min(x)-1,sort(x), max(x+1)), c(0:n,n)/n, type="s",
      xlab="data", ylab="ecdf")
rug(x)


regards,
Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From ligges at statistik.uni-dortmund.de  Wed Jun 25 08:55:47 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 Jun 2003 08:55:47 +0200
Subject: [R] Can't load e1071
In-Reply-To: <20030625013354.GA4913@sonny.eddelbuettel.com>
References: <Pine.LNX.4.53.0306241542140.10465@perrin.socsci.unc.edu>	<20030624195906.GE25655@boromir.ci.tuwien.ac.at>	<Pine.LNX.4.53.0306241602400.10465@perrin.socsci.unc.edu>	<20030624212429.GA3364@sonny.eddelbuettel.com>	<Pine.LNX.4.53.0306241946450.16284@perrin.socsci.unc.edu>	<20030625001219.GA4531@sonny.eddelbuettel.com>	<Pine.LNX.4.53.0306242023410.16284@perrin.socsci.unc.edu>
	<20030625013354.GA4913@sonny.eddelbuettel.com>
Message-ID: <3EF94773.8010406@statistik.uni-dortmund.de>

Dirk Eddelbuettel wrote:

> On Tue, Jun 24, 2003 at 08:44:57PM -0400, Andrew Perrin wrote:
>

[SNIP, reducing to some relevant lines]

>>>library(e1071)
>>
>>Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>        unable to load shared library
>>"/usr/local/lib/R/site-library/e1071/libs/e1071.so":
>>  /usr/local/lib/R/site-library/e1071/libs/e1071.so: cannot dynamically
>>load executable
>>Error in library(e1071) : .First.lib failed
>>
>>...even though /usr/local/lib/R/site-library/e1071/libs/e1071.so exists
>>and is readable.
> 
> 
> So remove e1071 and reinstall, but see further down.
> 

[...]

After upgarding to R-1.7.0, you have to reinstall a couple of packages 
in order to get them to work.
I don't know anything about the Debian mechanisms, unfortunately, but in 
R it's pretty easy to reinstall:

  install.packages("e1071")

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Jun 25 09:15:17 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 Jun 2003 09:15:17 +0200
Subject: [R] ?plot problem
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A95D@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967A95D@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3EF94C05.9080303@statistik.uni-dortmund.de>

Paul, David A wrote:

> Thanks to Dr. Thomas Hotz, Prof. Brian Ripley, Dr. Dennis
> Murphy, and Dr. David Scott for their replies.
> 
> Trying an idea:
> 
> 
>>x <- c(1,2,3,4)
>>y<-c(2,4.2,5.9,9)
>>temp<-data.frame(cbind(x,y))
>>attach(temp)
>>temp.lm <- lm(y~x)
>>windows()
>>plot(temp.lm)
> 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> 
>>dev.off()
>>windows()
> 
> 
> I was unable to see the Cook's distance plot as before (using
> the Page Up and Page Down keys).  So I implemented another
> suggestion:
> 
> 
>>windows()
>>plot(temp.lm)
> 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> [Use Menu: History|Add]
> 
> After doing this, the Cook's distance plot was "saved" so that
> Page Up and Page Down worked properly.  Question:  Is there
> a way to use a line command to "add" the last graph to the
> history?
> 
> 

See ?recordPlot

Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Jun 25 10:01:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 09:01:06 +0100 (BST)
Subject: [R] ?plot problem
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A95D@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <Pine.LNX.4.44.0306250853480.29281-100000@gannet.stats>

The critical fact here is that you called dev.off() and then re-open a
device: I don't recall that being mentioned before (and it works unless
you do that).  dev.off() does not save the current plot (it will in
1.8.0), so you need to call recordPlot()  before dev.off().

The current plot is recorded if you move away from it or start a new plot.

On Tue, 24 Jun 2003, Paul, David  A wrote:

> Thanks to Dr. Thomas Hotz, Prof. Brian Ripley, Dr. Dennis
> Murphy, and Dr. David Scott for their replies.
> 
> Trying an idea:
> 
> > x <- c(1,2,3,4)
> > y<-c(2,4.2,5.9,9)
> > temp<-data.frame(cbind(x,y))
> > attach(temp)
> > temp.lm <- lm(y~x)

This is pointless: the x and y in the user's workspace are used.
Use lm(y ~ x, data=temp).

> > windows()

Unneeded.

> > plot(temp.lm)
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 

call recordPlot() here

> > dev.off()
> > windows()
> 
> I was unable to see the Cook's distance plot as before (using
> the Page Up and Page Down keys).  So I implemented another
> suggestion:
> 
> > windows()
> > plot(temp.lm)
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> Hit <Return> to see next plot: 
> [Use Menu: History|Add]
> 
> After doing this, the Cook's distance plot was "saved" so that
> Page Up and Page Down worked properly.  Question:  Is there
> a way to use a line command to "add" the last graph to the
> history?
> 
> 
> Much thanks again,
>   david paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Philippe.Hupe at curie.fr  Wed Jun 25 10:01:25 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Wed, 25 Jun 2003 10:01:25 +0200
Subject: [R] Markov chain simulation
Message-ID: <3EF956D5.6040309@curie.fr>

Hi,

Does anybody know a function to simulate a Markov chain given a 
probability transition matrix and an initial state ?
Thanks.

Philippe
-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>



From Simon.Fear at synequanon.com  Wed Jun 25 10:21:56 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 25 Jun 2003 09:21:56 +0100
Subject: [R] excel files and R
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AB5FDD@synequanon01>

RODBC works fine but as far as I can tell requires that the connection
be
opened through Windows menus. OK for a one-off, but not for batch
processing.
Please someone tell me what I missed - how can I open the connection
within
an R script?
(Windows 98)(not my fault)

TIA
 (sorry for long disclaimer, can't switch it off)

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 24 June 2003 17:54
To: Victor H. Mar?m
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] excel files and R


Security Warning:
If you are not sure an attachment is safe to open please contact 
Andy on x234. There are 0 attachments with this message.
________________________________________________________________

On Tue, 24 Jun 2003, Victor H. Mar?m wrote:

> I am new at R.  My questions is rather basic.  Looking R manuals looks
> like there should be a way to read MS excel files into R.  Could
> somebody tell me which library should I use for that?

Several ways are in the R Data Import/Export Manual (the obvious manual,
I
would have thought).  If you are working on Windows, using RODBC is
perhaps the simplest.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\ con... {{dropped}}



From Gordon.Morrison at CommerzbankIB.com  Wed Jun 25 10:42:55 2003
From: Gordon.Morrison at CommerzbankIB.com (Morrison, Gordon)
Date: Wed, 25 Jun 2003 09:42:55 +0100
Subject: [R] excel files and R
Message-ID: <FAD50FCCDDD5D511865200508BB2CDA502D6C053@xmx2lonib.lonib.commerzbank.com>

If you really want to run windows from R (in my experience it is much better
to reside entirely within R) then I think that you need to use Duncan Temple
Lang's RDCOM package ( http://www.omegahat.org/ ). It works well and
robustly for connections to other packages and I have tested it for Excel. 


Regards,


Gordon Morrison
Global Head of Quantitative Research

> *	+ 44 20 7653 7642
> Mob:     + 44 7867 801951
> fax:	+ 44 20 7645 7442
> * 	mailto:gordon.morrison at commerzbankib.com
> web:	http://www.cbksec.com/rsh/portfolio&risk.html
> *	Commerzbank Securities
> 	60 Gracechurch Street
>                 London EC3V 0HR, U.K.
> 


-----Original Message-----
From: Simon Fear [mailto:Simon.Fear at synequanon.com]
Sent: 25 June 2003 09:22
To: r-help at stat.math.ethz.ch
Subject: RE: [R] excel files and R


RODBC works fine but as far as I can tell requires that the connection
be
opened through Windows menus. OK for a one-off, but not for batch
processing.
Please someone tell me what I missed - how can I open the connection
within
an R script?
(Windows 98)(not my fault)

TIA
 (sorry for long disclaimer, can't switch it off)

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 24 June 2003 17:54
To: Victor H. Mar?m
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] excel files and R


Security Warning:
If you are not sure an attachment is safe to open please contact 
Andy on x234. There are 0 attachments with this message.
________________________________________________________________

On Tue, 24 Jun 2003, Victor H. Mar?m wrote:

> I am new at R.  My questions is rather basic.  Looking R manuals looks
> like there should be a way to read MS excel files into R.  Could
> somebody tell me which library should I use for that?

Several ways are in the R Data Import/Export Manual (the obvious manual,
I
would have thought).  If you are working on Windows, using RODBC is
perhaps the simplest.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\ con...
{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help


********************************************************************** 
This is a commercial communication from Commerzbank AG.

This communication is confidential and is intended only for the person to
whom it is addressed.  If you are not that person you are not permitted to
make use of the information and you are requested to notify
<mailto:LONIB.Postmaster at commerzbankib.com> immediately that you have
received it and then destroy the copy in your possession.

Commerzbank AG may monitor outgoing and incoming e-mails. By replying to
this e-mail you consent to such monitoring. This e-mail message and any
attached files have been scanned for the presence of computer viruses.
However, you are advised that you open attachments at your own risk.

This email was sent either by Commerzbank AG, London Branch, or by
Commerzbank Securities, a division of Commerzbank.  Commerzbank AG is a
limited liability company incorporated in the Federal Republic of Germany.
Registered Company Number in England BR001025. Our registered address in
the UK is 23 Austin Friars, London, EC2P 2JD. We are regulated by the
Financial Services Authority for the conduct of investment business in the
UK and we appear on the FSA register under number 124920.



From ripley at stats.ox.ac.uk  Wed Jun 25 10:44:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 09:44:43 +0100 (BST)
Subject: [R] excel files and R
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AB5FDD@synequanon01>
Message-ID: <Pine.LNX.4.44.0306250941420.29422-100000@gannet.stats>

There's a function odbcConnectExcel, a wrapper to odbcDriverConnect.
Neither require any work with Windows menus.

On Wed, 25 Jun 2003, Simon Fear wrote:

> RODBC works fine but as far as I can tell requires that the connection
> be
> opened through Windows menus. OK for a one-off, but not for batch
> processing.
> Please someone tell me what I missed - how can I open the connection
> within
> an R script?
> (Windows 98)(not my fault)
> 
> TIA
>  (sorry for long disclaimer, can't switch it off)
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 24 June 2003 17:54
> To: Victor H. Mar?m
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] excel files and R
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> On Tue, 24 Jun 2003, Victor H. Mar?m wrote:
> 
> > I am new at R.  My questions is rather basic.  Looking R manuals looks
> > like there should be a way to read MS excel files into R.  Could
> > somebody tell me which library should I use for that?
> 
> Several ways are in the R Data Import/Export Manual (the obvious manual,
> I
> would have thought).  If you are working on Windows, using RODBC is
> perhaps the simplest.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bernhard.Pfaff at drkw.com  Wed Jun 25 10:55:20 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Wed, 25 Jun 2003 10:55:20 +0200
Subject: [R] excel files and R
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730446@ibfftce505.is.de.dresdnerkb.com>

> 
> There's a function odbcConnectExcel, a wrapper to odbcDriverConnect.
> Neither require any work with Windows menus.
> 
Try the following sample script and save it as "foo.R":

library(RODBC)
chan1 <- odbcConnectExcel("your-file.xls")
aa <- sqlFetch(chan1, "Name of your sheet")
names(aa)
close(chan1)

as batch:
Rterm --no-restore --no-save < foo.R

works for me, at least.


HTH,
Bernhard




> On Wed, 25 Jun 2003, Simon Fear wrote:
> 
> > RODBC works fine but as far as I can tell requires that the 
> connection
> > be
> > opened through Windows menus. OK for a one-off, but not for batch
> > processing.
> > Please someone tell me what I missed - how can I open the connection
> > within
> > an R script?
> > (Windows 98)(not my fault)
> > 
> > TIA
> >  (sorry for long disclaimer, can't switch it off)
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: 24 June 2003 17:54
> > To: Victor H. Mar?m
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] excel files and R
> > 
> > 
> > Security Warning:
> > If you are not sure an attachment is safe to open please contact 
> > Andy on x234. There are 0 attachments with this message.
> > ________________________________________________________________
> > 
> > On Tue, 24 Jun 2003, Victor H. Mar?m wrote:
> > 
> > > I am new at R.  My questions is rather basic.  Looking R 
> manuals looks
> > > like there should be a way to read MS excel files into R.  Could
> > > somebody tell me which library should I use for that?
> > 
> > Several ways are in the R Data Import/Export Manual (the 
> obvious manual,
> > I
> > would have thought).  If you are working on Windows, using RODBC is
> > perhaps the simplest.
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From lancelot at sentoo.sn  Wed Jun 25 10:55:30 2003
From: lancelot at sentoo.sn (Renaud Lancelot)
Date: Wed, 25 Jun 2003 08:55:30 +0000
Subject: [R] excel files and R
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AB5FDD@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AB5FDD@synequanon01>
Message-ID: <3EF96382.70602@sentoo.sn>

Simon Fear wrote:
> RODBC works fine but as far as I can tell requires that the connection
> be
> opened through Windows menus. OK for a one-off, but not for batch
> processing.
> Please someone tell me what I missed - how can I open the connection
> within
> an R script?
> (Windows 98)(not my fault)
[snip]

It is no longer true since Pr Ripley provided a function 
odbcConnectExcel() (as well as odbcConnectAccess() and others) in the 
latest version of package RODBC. See the help file(s).

Best,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
CIRAD, D?partement Elevage et M?decine V?t?rinaire (CIRAD-Emvt)
Programme Productions Animales
http://www.cirad.fr/fr/pg_recherche/page.php?id=14

ISRA-LNERV                      tel    +221 832 49 02
BP 2057 Dakar-Hann              fax    +221 821 18 79 (CIRAD)
Senegal                         e-mail renaud.lancelot at cirad.fr



From erich.neuwirth at univie.ac.at  Wed Jun 25 11:41:52 2003
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Wed, 25 Jun 2003 11:41:52 +0200
Subject: [R] excel files and R
In-Reply-To: <FAD50FCCDDD5D511865200508BB2CDA502D6C053@xmx2lonib.lonib.commerzbank.com>
References: <FAD50FCCDDD5D511865200508BB2CDA502D6C053@xmx2lonib.lonib.commerzbank.com>
Message-ID: <3EF96E60.9090303@univie.ac.at>

And there is Thomas Baier's and my RCOM package
which would allow to run R from within Excel
or exchange data between Excel and R with R
as the main interface.


Morrison, Gordon wrote:
> If you really want to run windows from R (in my experience it is much better
> to reside entirely within R) then I think that you need to use Duncan Temple
> Lang's RDCOM package ( http://www.omegahat.org/ ). It works well and
> robustly for connections to other packages and I have tested it for Excel.



From pfm401 at lineone.net  Wed Jun 25 11:45:35 2003
From: pfm401 at lineone.net (pfm401@lineone.net)
Date: Wed, 25 Jun 2003 10:45:35 +0100
Subject: [R] Sending graphical output to a printer
Message-ID: <3ED7676E0002D07A@mk-cpfrontend-1.mail.uk.tiscali.com>

Dear all,

Does anyone know how to send graphical output to a printer using R for Windows
version 1.6.2. For example if I do

plot(x,y) # Sends output to the graphics window

which command will let me print this graph?

The reason I need to do this is to automatically print out a lot of graphs
without having to select Print from the graphics window at each one.

Thanks in advance for your help.

Paul.



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 25 11:19:00 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 25 Jun 2003 10:19:00 +0100 (BST)
Subject: [R] Execution of R code
Message-ID: <XFMail.030625101900.Ted.Harding@nessie.mcc.ac.uk>

Greetings Folks,

When R code (as entered or read from a courced file) is executed,
is it interpreted from the input form every time having once been
read in, or do subsequent invocations use an "intermediate"
(pre-interpreted) form?

Or, putting it another way, is the execution of R code faster
second time time round (and later) because the pre-interpretation
has already been done once and for all?

[And, for seconds, what is the corresponding situation for S-plus?]

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 25-Jun-03                                       Time: 10:19:00
------------------------------ XFMail ------------------------------



From Simon.Fear at synequanon.com  Wed Jun 25 12:16:21 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 25 Jun 2003 11:16:21 +0100
Subject: [R] excel files and R
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D04@synequanon01>

Many many thanks. I did look at this but I have absolutely no idea of
the
background so got completely lost. Can you recommend a gentle
introduction/overview to this area, based on the assumption that my
current
knowledge equals zero? Indeed, could you make an argument that I should
ever
*want* to run R from within Excel or vice versa? I think I just want to
get
the data from Excel ('cos that's how it nearly always comes), but I
don't
want to process it in Excel, when I have R ...

I guess all that I and apparently others really want is that "foreign"
might
include read.excel, like it has read.sas and read.spss. Which is
essentially
what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
using
RODBC instead of foreign.


-----Original Message-----
From: Erich Neuwirth [mailto:erich.neuwirth at univie.ac.at]
Sent: 25 June 2003 10:42
To: Morrison, Gordon; r-help at stat.math.ethz.ch
Subject: Re: [R] excel files and R

And there is Thomas Baier's and my RCOM package
which would allow to run R from within Excel
or exchange data between Excel and R with R
as the main interface.


Morrison, Gordon wrote:
> If you really want to run windows from R (in my experience it is much
better
> to reside entirely within R) then I think that you need to use Duncan
Temple
> Lang's RDCOM package ( http://www.omegahat.org/ ). It works well and
> robustly for connections to other packages and I have tested it for
Excel.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From eairoldi at stat.cmu.edu  Wed Jun 25 12:31:49 2003
From: eairoldi at stat.cmu.edu (Edoardo Airoldi)
Date: Wed, 25 Jun 2003 06:31:49 -0400
Subject: [R] dendrograms
In-Reply-To: <200306251018.h5PA55UW008191@stat.math.ethz.ch>
Message-ID: <3A7519E3-A6F8-11D7-A702-00039390FFC4@stat.cmu.edu>

Hello all,
  I am using libraries (mva,cluster) to produce dendrograms.  With 1000 
examples the dendrogram gets too crowded, and i am wondering whether there 
is an option (which i cannot find) to set the number of leaf nodes, like 
in matlab, and return the plot and the assignment map examples -> leaf 
nodes.  Any suggestion is appreciated.  Thanks
Edo



From ripley at stats.ox.ac.uk  Wed Jun 25 12:51:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 11:51:44 +0100 (BST)
Subject: [R] excel files and R
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D04@synequanon01>
Message-ID: <Pine.LNX.4.44.0306251149030.2122-100000@gannet.stats>

On Wed, 25 Jun 2003, Simon Fear wrote:

> Many many thanks. I did look at this but I have absolutely no idea of
> the
> background so got completely lost. Can you recommend a gentle
> introduction/overview to this area, based on the assumption that my
> current
> knowledge equals zero? Indeed, could you make an argument that I should
> ever
> *want* to run R from within Excel or vice versa? I think I just want to
> get
> the data from Excel ('cos that's how it nearly always comes), but I
> don't
> want to process it in Excel, when I have R ...
> 
> I guess all that I and apparently others really want is that "foreign"
> might
> include read.excel, like it has read.sas and read.spss. Which is
> essentially
> what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
> using
> RODBC instead of foreign.

The Excel .xls format is poorly documented, probably deliberately
obfuscated.  A direct interface is on the TODO list: it should be quite
easy in Windows and possible in other OSes (there is code in Gnumeric, for
example).  In any case, this is a lot harder than the interfaces currently 
in foreign.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 25 13:09:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 12:09:18 +0100 (BST)
Subject: [R] Execution of R code
In-Reply-To: <XFMail.030625101900.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0306251200550.2187-100000@gannet.stats>

I am not sure I fully understand the Qs.

There are two phases.

1) The source code is parsed.
2) The parsed code is evaluated.

If you run code from source() or a file or the command line, it is
parsed and evaluated.  However, evaluating a function assignment makes an 
function object containing the parsed code for the body of a function.

Running code a second time is often faster because of caching of memory
(in the chip's caches and in RAM ratehr than VM). In S-PLUS there are more
layers of caching going on: objects are retrieved from disc and (usually)
cached in memory, and memory allocated for objects can be re-used rather
than re-allocated.

There is no form of pre-compiling to intermediate code on first use (as 
some Java implementations use), although things like that are in Luke
Tierney's long-term plans.

I hope that actually answers your questions.

On Wed, 25 Jun 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Greetings Folks,
> 
> When R code (as entered or read from a courced file) is executed,
> is it interpreted from the input form every time having once been
> read in, or do subsequent invocations use an "intermediate"
> (pre-interpreted) form?
> 
> Or, putting it another way, is the execution of R code faster
> second time time round (and later) because the pre-interpretation
> has already been done once and for all?
> 
> [And, for seconds, what is the corresponding situation for S-plus?]
> 
> With thanks,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 25-Jun-03                                       Time: 10:19:00
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jean.Guibert at obspm.fr  Wed Jun 25 13:16:07 2003
From: Jean.Guibert at obspm.fr (Jean GUIBERT)
Date: Wed, 25 Jun 2003 13:16:07 +0200
Subject: [R] 1D and 2D gaussian fittings 
Message-ID: <3EF98477.12C982C5@obspm.fr>

Dear colleague,


	I am interested by tools allowing fitting a gaussian curve (or
surface)  to a distribution of the kind y = f(x) ( or z = g(x,y).)

	both Fortran and C languages would be usefull.

	What kind of routines could you suggest? What are the adresses from
which they can be downloaded?

		with many thanks and best regards,


					Jean Guibert 

	
--



From david.firth at nuffield.oxford.ac.uk  Wed Jun 25 13:25:28 2003
From: david.firth at nuffield.oxford.ac.uk (David Firth)
Date: Wed, 25 Jun 2003 12:25:28 +0100
Subject: [R] one/multi-dimensional scaling with incomplete dissimilarity
	matrix
Message-ID: <B958115C-A6FF-11D7-9145-000393CD9F1A@nuffield.oxford.ac.uk>

The scaling functions I know about,

isoMDS(MASS)            Kruskal's Non-metric Multidimensional Scaling
cmdscale(mva)           Classical (Metric) Multidimensional Scaling
sammon(MASS)            Sammon's Non-Linear Mapping

all seem to require a complete dissimilarity matrix (ie, no 
dissimilarities missing).  Is there anything available for situations 
where not all dissimilarities are known, ie the dissimilarity matrix 
has (symmetrically) NAs in it?

David



From Vincent.Spiesser at univ-tlse1.fr  Wed Jun 25 13:31:24 2003
From: Vincent.Spiesser at univ-tlse1.fr (Vincent Spiesser)
Date: Wed, 25 Jun 2003 13:31:24 +0200
Subject: [R] creating R help page
Message-ID: <4.2.0.58.20030625133026.00a813c0@mail.univ-tlse1.fr>

Hello

Does anybody know how to create an R help page wich can be opened from R
console (like help(glm)) ?
Particularly, I would like to know :
- what kind of file the help file is ?
- Where does it take place ?
Thanks
Vincent Spiesser



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 25 13:20:54 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 25 Jun 2003 12:20:54 +0100 (BST)
Subject: [R] excel files and R
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D04@synequanon01>
Message-ID: <XFMail.030625122054.Ted.Harding@nessie.mcc.ac.uk>

On 25-Jun-03 Simon Fear wrote:
> [...] I think I just want to get the data from Excel ('cos that's how
> it nearly always comes), but I don't want to process it in Excel, when
> I have R ...

In that case there is a very simple solution (asuming you have access
to Excel).

Open the Excel file in Excel, and save it out as a comma-separated file
(.csv). You may need to clean this up a bit (depending on how sloppily
it was created -- a lot of people create very messy Excel files), but
usually you don't. (For the same reason, it can be best to do the export
to CSV yourself, rather than asking the sender to send you a CSV file,
unless you trust their competence.)

Then, in R, do something like

  X <- read.csv("excelfile.csv")

You will then have a dataframe X (with variables named as in the column
names in the Excel file).

This is what I always do when I get Excel files; it has always worked.

See ?read.csv for options.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 25-Jun-03                                       Time: 12:20:54
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Wed Jun 25 13:32:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 12:32:57 +0100 (BST)
Subject: [R] creating R help page
In-Reply-To: <4.2.0.58.20030625133026.00a813c0@mail.univ-tlse1.fr>
Message-ID: <Pine.LNX.4.44.0306251232320.2839-100000@gannet.stats>

See the `Writing R Extensions' manual.

On Wed, 25 Jun 2003, Vincent Spiesser wrote:

> Does anybody know how to create an R help page wich can be opened from R
> console (like help(glm)) ?
> Particularly, I would like to know :
> - what kind of file the help file is ?
> - Where does it take place ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 25 13:34:09 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 25 Jun 2003 12:34:09 +0100 (BST)
Subject: [R] Execution of R code
In-Reply-To: <Pine.LNX.4.44.0306251200550.2187-100000@gannet.stats>
Message-ID: <XFMail.030625123409.Ted.Harding@nessie.mcc.ac.uk>

On 25-Jun-03 Prof Brian Ripley wrote:
> I am not sure I fully understand the Qs.
> [...]
> I hope that actually answers your questions.

Thanks, Brian! You have exactly understood, and fully answered,
my questions.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 25-Jun-03                                       Time: 12:34:09
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Wed Jun 25 14:12:36 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 25 Jun 2003 12:12:36 -0000
Subject: [R] Execution of R code
In-Reply-To: <Pine.LNX.4.44.0306251200550.2187-100000@gannet.stats>
References: <Pine.LNX.4.44.0306251200550.2187-100000@gannet.stats>
Message-ID: <x28yrqxsn3.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> I am not sure I fully understand the Qs.
> 
> There are two phases.
> 
> 1) The source code is parsed.
> 2) The parsed code is evaluated.
> 
> If you run code from source() or a file or the command line, it is
> parsed and evaluated.  However, evaluating a function assignment makes an 
> function object containing the parsed code for the body of a function.
> 
> Running code a second time is often faster because of caching of memory
> (in the chip's caches and in RAM ratehr than VM). In S-PLUS there are more
> layers of caching going on: objects are retrieved from disc and (usually)
> cached in memory, and memory allocated for objects can be re-used rather
> than re-allocated.
> 
> There is no form of pre-compiling to intermediate code on first use (as 
> some Java implementations use), although things like that are in Luke
> Tierney's long-term plans.
> 
> I hope that actually answers your questions.

One might add that although we don't byte-compile like in Java and
emacs-lisp, the parse tree storage that we use is somewhat more
pre-cooked than the tokenized storage of the ROM BASIC found on early
PCs and their precursors. 

One often considers the parsing stage as two processes: Lexical
analysis (the tokenizer) which recognises elementary items such as
keywords, operators, variable names, and constants; and the actual
code tree generation which knows about syntactical structures like for
loops, functions, and compound expressions. 

A code tree for a simple expression like

while ( i < 10 ) i <- i + 1

could be represented as

      while 
     /     \
    <       <-
   / \     /  \
  i   10  i    +
              / \
             i   1

(apologies to those with proportional screen fonts...) In this
representation, everything is basically functions and arguments: "while"
has two arguments: the loop condition and the body, and those are
calls to a comparison and an assignment function respectively, and so
forth. 

In compiled languages, parsing is followed by a step that converts the
code tree to machine instructions, but in languages like R it is
easier to interpret the tree directly. One particular aspect of R-like
languages is that you can replace or modify functions programmatically
in between running them, which means that you won't get the gain of an
up-front optimization effort unless you impose special restrictions.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From partha_bagchi at hgsi.com  Wed Jun 25 14:09:35 2003
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Wed, 25 Jun 2003 08:09:35 -0400
Subject: [R] excel files and R
Message-ID: <OF4E469164.D13748B8-ON85256D50.00428E07-85256D50.0042CB49@hgsi.com>

Simon,

Here is what I do when I encounter an excel spreadsheet. I save it as a 
CSV file (in excel -> save as -> file type CSV) and then read the 
corresponding file into R using read.csv.

Hope that helps,
Partha





"Simon Fear" <Simon.Fear at synequanon.com>
Sent by: r-help-bounces at stat.math.ethz.ch
06/25/2003 06:16 AM

 
        To:     "Erich Neuwirth" <erich.neuwirth at univie.ac.at>
        cc:     r-help at stat.math.ethz.ch
        Subject:        RE: [R] excel files and R


Many many thanks. I did look at this but I have absolutely no idea of
the
background so got completely lost. Can you recommend a gentle
introduction/overview to this area, based on the assumption that my
current
knowledge equals zero? Indeed, could you make an argument that I should
ever
*want* to run R from within Excel or vice versa? I think I just want to
get
the data from Excel ('cos that's how it nearly always comes), but I
don't
want to process it in Excel, when I have R ...

I guess all that I and apparently others really want is that "foreign"
might
include read.excel, like it has read.sas and read.spss. Which is
essentially
what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
using
RODBC instead of foreign.


-----Original Message-----
From: Erich Neuwirth [mailto:erich.neuwirth at univie.ac.at]
Sent: 25 June 2003 10:42
To: Morrison, Gordon; r-help at stat.math.ethz.ch
Subject: Re: [R] excel files and R

And there is Thomas Baier's and my RCOM package
which would allow to run R from within Excel
or exchange data between Excel and R with R
as the main interface.


Morrison, Gordon wrote:
> If you really want to run windows from R (in my experience it is much
better
> to reside entirely within R) then I think that you need to use Duncan
Temple
> Lang's RDCOM package ( http://www.omegahat.org/ ). It works well and
> robustly for connections to other packages and I have tested it for
Excel.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help


Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com

Number of attachments included with this message: 0

This message (and any associated files) is confidential and\...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

--
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From andy_liaw at merck.com  Wed Jun 25 14:15:22 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Jun 2003 08:15:22 -0400
Subject: [R] Sending graphical output to a printer
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7B7@usrymx25.merck.com>

Use dev.print(win.print) after the plot is done.  This is in the help page
(?windows).

Andy

> -----Original Message-----
> From: pfm401 at lineone.net [mailto:pfm401 at lineone.net] 
> Sent: Wednesday, June 25, 2003 5:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Sending graphical output to a printer
> 
> 
> Dear all,
> 
> Does anyone know how to send graphical output to a printer 
> using R for Windows version 1.6.2. For example if I do
> 
> plot(x,y) # Sends output to the graphics window
> 
> which command will let me print this graph?
> 
> The reason I need to do this is to automatically print out a 
> lot of graphs without having to select Print from the 
> graphics window at each one.
> 
> Thanks in advance for your help.
> 
> Paul.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/list> info/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From Neil.Chriss at sac.com  Wed Jun 25 14:43:32 2003
From: Neil.Chriss at sac.com (Chriss, Neil)
Date: Wed, 25 Jun 2003 08:43:32 -0400
Subject: [R] Help on using  read.table with files containing dates
Message-ID: <7326095FBE833F46BA7D951C141CC7A003B3D74B@mailisct2.saccapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030625/c984216f/attachment.pl

From stoet at volition.wustl.edu  Wed Jun 25 14:58:19 2003
From: stoet at volition.wustl.edu (Gijsbert Stoet)
Date: Wed, 25 Jun 2003 07:58:19 -0500
Subject: [R] cumulative frequency distribution plot
Message-ID: <16121.40043.577162.731050@volition.wustl.edu>


To plot a cumulative distribution of, say a behaviorial reaction time
vector, I wrote the following function:

cumhist = function(x)
{
  Z = hist( x , plot=F )
  plot(1:length(Z$counts),cumsum(Z$counts)/length(x)*100,type="b",axes=F,
       ylab="%",xlab="")
  axis(1,at=1:length(Z$counts),labels=round(1/Z$mids,digits=0))
  axis(2)
}

x must be a vector of the data.



From ripley at stats.ox.ac.uk  Wed Jun 25 15:16:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 14:16:55 +0100 (BST)
Subject: [R] Help on using  read.table with files containing dates
In-Reply-To: <7326095FBE833F46BA7D951C141CC7A003B3D74B@mailisct2.saccapital.com>
Message-ID: <Pine.LNX.4.44.0306251351230.2975-100000@gannet.stats>

On Wed, 25 Jun 2003, Chriss, Neil wrote:

> I am a relatively new user of R and have a question about using the
> read.table command for reading in .csv files that contain dates.  

Unfortuantely them appear to contain perversions of dates, not the dates 
recognised by the ISO standard.  I read your dates as the first of
Jan, Apr and May 2003, but some illogical people use mm/dd/yy order.

> My
> ultimate goal is to be able to read several different files into data frames
> and then do a merge along the "Date" column of each data frame.  It seems I
> should be able to specify the name of the column that contains dates and
> then automatically convert that to dates, all in a single read.table
> statement.  It's such a natural thing to do (and the help for read.table
> gives me hope, but I have not been able to figure out some of the options,
> e.g., colNames).  
>  
> Here is a specific example of what I mean and what the problems are.  An
>  
> First I load a sample data file whose first column is a date.  This gives
> the wrong answer as we shall see:
>  
> > library(date)
> > sampData <- read.table("sampleData.csv",sep=",",header=TRUE)
> > sampData
>       Date Col1 Col2   Col3
> 1 1/1/2003  1.2  1.4  0.160
> 2 1/4/2003  1.8  1.2  0.900
> 3 1/5/2003  0.9  1.1 -0.003
> > mode(sampData$Date)
> [1] "numeric"
> > 
> 
> Note that the Date column is coerced incorrectly into being numeric. 

It is not:  mode is inappropriate here: you actually have a factor
whose mode is numeric.  Use class(), not mode().

> Right
> now what I do is force R to read this in as a character using "as.is" and
> then convert it to a date as follows:
>  
> > sampData <- read.table("sampleData.csv",sep=",",header=TRUE,as.is=1)
> > sampData
>       Date Col1 Col2   Col3
> 1 1/1/2003  1.2  1.4  0.160
> 2 1/4/2003  1.8  1.2  0.900
> 3 1/5/2003  0.9  1.1 -0.003
> > mode(sampData$Date)
> [1] "character"
> > sampData$Date <- as.date(sampData$Date)
> > sampData
>    Date Col1 Col2   Col3
> 1 15706  1.2  1.4  0.160
> 2 15709  1.8  1.2  0.900
> 3 15710  0.9  1.1 -0.003
> > 
> 
> Now the Data column is converted to Julian, which is good b/c if I repeat
> this procedure with other dataframes I can do a merge on the date columns
> and line up the dates.  But, there are two drawbacks with this approach
> which I do not know how to solve:  
>  
> 1.  it's generally clumsy and takes too many lines.
>  
> 2.  (more importantly) it forces me to specify which column number contains
> the dates instead of simply stating which column header name contains date.
> I would rather simply the column header name of the column that contains the
> date (in this example: Date) so that whichever column contains the date will
> be automatically converted to dates.
>  
> So, is there a way to specify that I want the column labeled "Date" to be
> read as a date class (as in as.date)?

Yes, that's what the argument colClasses is for.  But since as.date is
not part of R (it is in contributed packages survival and date) you will 
need to provide an as() method.

I suggest you use

sampData[["Date"]] <- strptime(as.character(sampData[["Date"]]), "%d/%m/%y")

since the conversion to factor can be undone very easily.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From meles at free.fr  Wed Jun 25 15:55:40 2003
From: meles at free.fr (Meles MELES)
Date: Wed, 25 Jun 2003 15:55:40 +0200
Subject: [R] equivalence test
Message-ID: <200306251555.43721.meles@free.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,
	is it possible to do an equivalence test on paired quantitative datas 
in R? Is there a way to calculate sample size for such tests?

I've tried to find some documentation on that subject but I was 
unsuccessfull. 
I'll be happy with any links on equivalence test. If such a test does'nt 
exist in R, i'll do it manually if I find a method to do so.

Best regards

Blaise
- -- 
Le doute me ronge. Et si tout n'?tait qu'illusion ? Si rien n'existait ? 
Dans ce cas, j'aurais pay? ma moquette beaucoup trop cher. Woody Allen
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.2 (GNU/Linux)

iD8DBQE++anfs13e5HkHBycRAgeDAKC896MUrcUmoWrMHdu7bv9a/VJgewCffGeu
dmvi6Wy4jenHAj0la4ePHdU=
=tdNF
-----END PGP SIGNATURE-----



From drf5n at mug.sys.virginia.edu  Wed Jun 25 16:14:58 2003
From: drf5n at mug.sys.virginia.edu (drf5n@mug.sys.virginia.edu)
Date: Wed, 25 Jun 2003 10:14:58 -0400 (EDT)
Subject: [R] Reading graphics files
In-Reply-To: <Pine.LNX.4.44.0306242043060.19201-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0306250939510.490-100000@mug.sys.virginia.edu>

On Tue, 24 Jun 2003, Roger Bivand wrote:
...
>
> library(pixmap), perhaps? Reads PNM (so PPM, PGM, PBM).
>

Thanks!  This is precisely what I was looking for.  I had gotten to the
point of:

> con<-file("image.ppm",open="rb")
> readLines(con,n=3)
[1] "P6"      "600 400" "255"
> zz<-readBin(con,size=1,n=3*600*400,what='int')
> close(con)
> dim(zz)<-c(3,600,400)
> image(1:600,1:400,zz[1,,400:1])

... and then realized that the coordinates and colormaps would be awkward.

Thanks to Frederich Leisch and Roger Bivand for writing the CRAN pixmap
package.  Would a pointer to it in the see-also sections of 'bitmap',
'png', and 'image' be appropriate?

Thanks again,
Dave
-- 
 Dave Forrest    (434)924-3954w(111B) (804)642-0662h (804)695-2026p
 drf5n at maplepark.com            http://mug.sys.virginia.edu/~drf5n/



From temiz at deprem.gov.tr  Wed Jun 25 16:14:54 2003
From: temiz at deprem.gov.tr (orkun)
Date: Wed, 25 Jun 2003 17:14:54 +0300
Subject: [R] predict formula with variables
Message-ID: <3EF9AE5E.3090602@deprem.gov.tr>

Hello

On the subject of using predict formula with variables,
I used cbind(var1,var2,var3,var4, predict(glm.obj,type="resp")) to find 
probability
of each combinations. However,I am  still hesitant whether I have done 
it correctly.
Is it correct ?

thanks in advance


Ahmet Temiz
Turkey


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From tlumley at u.washington.edu  Wed Jun 25 16:20:24 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jun 2003 07:20:24 -0700 (PDT)
Subject: [R] excel files and R
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D04@synequanon01>
Message-ID: <Pine.A41.4.44.0306250709280.181146-100000@homer06.u.washington.edu>

On Wed, 25 Jun 2003, Simon Fear wrote:

> I guess all that I and apparently others really want is that "foreign"
> might
> include read.excel, like it has read.sas and read.spss. Which is
> essentially
> what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
> using
> RODBC instead of foreign.
>

It would be nice, but it's quite hard to read Excel off Windows.

The formats in foreign are either documented by the vendor (accurately in
the case of Stata and Epi Info, with some omissions for SAS XPORT) or that
have been reverse-engineered by someone else (read.spss is based on
PSPP, an attempt at an SPSS clone by Ben Pfaaf, and I think Duncan
Murdoch did read.S).


While it isn't usual to say nice things about commercial vendors on these
lists I would like to note that Stata not only documents its file format
in its manuals (with some helpful C snippets for the trickier parts), but
made available the file format for their `large data set' version 7/SE,
which I didn't buy.


	-thomas



From Jean-Pierre.Mueller at dssp.unil.ch  Wed Jun 25 16:29:53 2003
From: Jean-Pierre.Mueller at dssp.unil.ch (Jean-Pierre Muller)
Date: Wed, 25 Jun 2003 16:29:53 +0200
Subject: [R] Mac carbon  - foreign - read.spss
Message-ID: <a05200f00bb1f5a31a59f@[130.223.101.162]>

Hello,

Can someone confirm that "read.spss" doesn't work with 
usual (old) mac paths ("disk:dir:dir:file"), but only with 
*nix path (Volumes/disk/...)?

rm171 (carbon) - MacOSX 10.2.6

Thanks.
-- 
------------------------------------------------------------------------------
Jean-Pierre Muller
SSP / UNIL /  BFSH2 / CH-1015 Lausanne



From dmurdoch at pair.com  Wed Jun 25 17:03:00 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 25 Jun 2003 11:03:00 -0400
Subject: [R] excel files and R
In-Reply-To: <Pine.LNX.4.44.0306251149030.2122-100000@gannet.stats>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D04@synequanon01>
	<Pine.LNX.4.44.0306251149030.2122-100000@gannet.stats>
Message-ID: <qsdjfvok0ddss2usevlughuerh67u495g5@4ax.com>

On Wed, 25 Jun 2003 11:51:44 +0100 (BST), you wrote in message
<Pine.LNX.4.44.0306251149030.2122-100000 at gannet.stats>:

>
>The Excel .xls format is poorly documented, probably deliberately
>obfuscated.  A direct interface is on the TODO list: it should be quite
>easy in Windows and possible in other OSes (there is code in Gnumeric, for
>example).  In any case, this is a lot harder than the interfaces currently 
>in foreign.

The OpenOffice web site has reasonably complete documentation on this
web page:  <http://sc.openoffice.org>.  It skips some of the more
obscure features, but those probably wouldn't be of interest to R
either.

It looks to me as though it would be relatively easy to write (in R,
using the streams code) a reader that could read strings and numbers.
Interpreting formulas would be a lot harder.  (It's possible the last
value of a formula is stored in the .xls file, in which case it would
be easily read too.)

One thing I'm not sure about:  recent .xls versions store strings in
Unicode.  Does R have cross-platform Unicode support?

Duncan



From MSchwartz at medanalytics.com  Wed Jun 25 17:02:14 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 25 Jun 2003 15:02:14 -0000
Subject: [R] excel files and R
In-Reply-To: <Pine.A41.4.44.0306250709280.181146-100000@homer06.u.washington.edu>
References: <Pine.A41.4.44.0306250709280.181146-100000@homer06.u.washington.edu>
Message-ID: <1056553327.4192.65.camel@localhost>

On Wed, 2003-06-25 at 09:20, Thomas Lumley wrote:
> On Wed, 25 Jun 2003, Simon Fear wrote:
> 
> > I guess all that I and apparently others really want is that "foreign"
> > might
> > include read.excel, like it has read.sas and read.spss. Which is
> > essentially
> > what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
> > using
> > RODBC instead of foreign.
> >
> 
> It would be nice, but it's quite hard to read Excel off Windows.
> 
> The formats in foreign are either documented by the vendor (accurately in
> the case of Stata and Epi Info, with some omissions for SAS XPORT) or that
> have been reverse-engineered by someone else (read.spss is based on
> PSPP, an attempt at an SPSS clone by Ben Pfaaf, and I think Duncan
> Murdoch did read.S).
> 
> 
> While it isn't usual to say nice things about commercial vendors on these
> lists I would like to note that Stata not only documents its file format
> in its manuals (with some helpful C snippets for the trickier parts), but
> made available the file format for their `large data set' version 7/SE,
> which I didn't buy.
> 
> 
> 	-thomas


Simon,

To add to Thomas' comments and respond to your thoughts, if one were so
inclined, given that R is a volunteer effort, I suspect that an addition
to 'foreign' for Excel would indeed be appreciated by many users.

One resource, with appropriate attribution given, would be the source
code for OpenOffice.org's (OOo) Calc. Since Calc can read and write
Excel formats without using Windows/Office DLL's, it seems reasonable to
presume that OOo has reverse engineered the native Excel file structure.
Since OOo's source is available under the GPL, this could provide the
basis for a "read.excel" function.

Yet another would be Gnumeric, which like Calc is GPL'd and can read and
write native Excel file formats.

More information is available at:

http://www.openoffice.org/dev_docs/source/1.0.3/source.html

http://www.gnome.org/projects/gnumeric/


Food for thought...   :-)

Regards,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Jun 25 17:03:46 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 16:03:46 +0100 (BST)
Subject: [R] Mac carbon  - foreign - read.spss
In-Reply-To: <a05200f00bb1f5a31a59f@[130.223.101.162]>
Message-ID: <Pine.LNX.4.44.0306251555110.3256-100000@gannet.stats>

On Wed, 25 Jun 2003, Jean-Pierre Muller wrote:

> Can someone confirm that "read.spss" doesn't work with 
> usual (old) mac paths ("disk:dir:dir:file"), but only with 
> *nix path (Volumes/disk/...)?
> 
> rm171 (carbon) - MacOSX 10.2.6


The exact code used to open files is (variations on)

    FILE *fp = fopen(R_ExpandFileName(filename), "rb");

which is the same as used in functions such as file.create and 
append.file, and so it would be expected to handle OS-specific file paths.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jlewis at genomecorp.com  Wed Jun 25 17:15:32 2003
From: jlewis at genomecorp.com (Jeff Lewis)
Date: Wed, 25 Jun 2003 11:15:32 -0400
Subject: [R] R-1.7.1 regression test failure on alphaev68-dec-osf5.1
Message-ID: <414DEAD4479990458D14E47B61039E5B0144496F@apollo.ad.genomecorp.com>

> > Thanks for the quick response.  The two sides of the equality are
> > definately different.  Here's what I'm seeing
> > 
> > =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > > pi
> > [1] 3.141593
> > 
> > > 1i
> > [1] 0+1i
> > 
> > > pi*1i
> > [1] 0+3.141593i
> > 
> > > exp(pi*1i)
> > [1] -1+1.224647e-16i
> > 
> > > log(exp(pi*1i))
> > [1] 0+3.141593i
> > 
> > > log(exp(pi*1i)) / 1i
> > 
> > [1] 3.141593+0i
> > 
> > > pi - log(exp(pi*1i)) / 1i
> > [1] 4.440892e-16+0i
> > 
> > > Mod(pi - log(exp(pi*1i)) / 1i)
> > [1] 4.440892e-16
> > 
> > > .Machine$double.eps
> > [1] 2.220446e-16
> > 
> > > Mod(pi - log(exp(pi*1i)) / 1i) < .Machine$double.eps
> > [1] FALSE
> > 
> > =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> > 
> > I get the same thing from R 1.6.2, which I compiled about six months
> > ago.  Is there anything I can/should do to fix this?
> 
> Not really. It seems that your platform just has slightly less
> accurate complex log/exp routines than the most common ones (Linux and
> Sparc/Solaris both give exact zero). Probably the check is simply
> overly stringent.
> 
> You might want to change the check to say  ... < 3*.Machine$double.eps
> or so and rerun, to check whether the rest of the checks pass.



I removed this test from the file 'reg-tests-1.R' and reran the checks.
All other checks passed with flying colors.

Thanks for the help.



From elvis at xlsolutions-corp.com  Wed Jun 25 17:36:25 2003
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 25 Jun 2003 08:36:25 -0700
Subject: [R] Course***Advanced R/Splus Programming in  San Francisco,
	July 24-25, 2003 by XLsolutions Corp.
Message-ID: <APEHLKCMHHAKBGLAPKPCIELBCGAA.elvis@xlsolutions-corp.com>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
a 2-day "Advanced R/Splus programming" taught by R Development
Core Team Guru!


*********San Francisco ---------->  July 24-25, 2003


           Early-bird discount ends June 30!
           Reserve your seat Now  (payment due after the class)

Registration:

www.xlsolutions-corp.com/training.htm
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578 x221

With the following outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function


Please email us for the full description of the course with fees and
information on trainers. For example, Early-bird group research fee is
$995!
It'll also deal with lots of S-Plus efficiency issues and any special topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this class
to take advantage of group discount. Register now to secure your seat!

=======================================================================
R/S System: Advanced Programming

Pre-registration Form (Please email or print and fax: 206-686-1578)
XLsolutions Corporation: For your Solutions needs, Consulting and
Training. www.xlsolutions-corp.com



Title...... First Name ................. Last Name....................

Organization..........................................................

Mailing Address.......................................................

..............................................................

..............................................................

Zip Code...................... Country.............................

Telephone........................... Fax ...............................

E-mail................................................................

Payment will be made by: (1) Check (2) Invoice (3) Credit Card

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From tomer_maymon at hotmail.com  Wed Jun 25 16:20:19 2003
From: tomer_maymon at hotmail.com (Tomer Maymon)
Date: Wed, 25 Jun 2003 14:20:19 +0000
Subject: [R] Help
Message-ID: <LAW10-F49rm9ezrmjZ700032358@hotmail.com>

hi
my name is Tomer,im using R version 1.4
i've encontered a couple of problems with my my R programing.
1. i cant understand what is the whights in the nnet$wts command(which 
weight fit to the nodes).
2.my OS is Windows XP , i want to operate R from the command line and i want 
it to operate a file automaticly(the commands written in a text file) ,is it 
possible?( i know that in unix it is),How can i do it.
thanks in advance.
Tomer



From merser at image.dk  Wed Jun 25 17:44:58 2003
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Wed, 25 Jun 2003 17:44:58 +0200
Subject: [R] rw1062
Message-ID: <000801c33b30$c5089990$cd3318ac@IBM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030625/166a5583/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 25 17:52:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 16:52:13 +0100 (BST)
Subject: [R] Help
In-Reply-To: <LAW10-F49rm9ezrmjZ700032358@hotmail.com>
Message-ID: <Pine.LNX.4.44.0306251647440.3553-100000@gannet.stats>

On Wed, 25 Jun 2003, Tomer Maymon wrote:

> my name is Tomer,im using R version 1.4

As the current version is 1.7.1 (and there never was a 1.4), you need to 
upgrade.

> i've encontered a couple of problems with my my R programing.
> 1. i cant understand what is the whights in the nnet$wts command(which 
> weight fit to the nodes).

I can't understand that sentence!  Guessing, if you look at the summary 
method you will see it labels the weights.  Read the code to find out how, 
or just use the labels.

> 2.my OS is Windows XP , i want to operate R from the command line and i want 
> it to operate a file automaticly(the commands written in a text file) ,is it 
> possible?( i know that in unix it is),How can i do it.

Try looking in the rw-FAQ: it is described there.  Q2.10 in the current 
version of that FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From edd at debian.org  Wed Jun 25 18:00:35 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 25 Jun 2003 11:00:35 -0500
Subject: [R] excel files and R
In-Reply-To: <1056553327.4192.65.camel@localhost>
References: <Pine.A41.4.44.0306250709280.181146-100000@homer06.u.washington.edu>
	<1056553327.4192.65.camel@localhost>
Message-ID: <20030625160035.GA11510@sonny.eddelbuettel.com>

On Wed, Jun 25, 2003 at 10:02:07AM -0500, Marc Schwartz wrote:
> Yet another would be Gnumeric, which like Calc is GPL'd and can read and
> write native Excel file formats.

The Gretl econometrics package (http://gretl.sf.net) also has a small
library for reading Excel files (in the file plugin/excel_import.c), its
code goes back to

/*
  Based on xls2csv (David Rysdam, 1998), as distributed in the 
  "catdoc" package by Vitus Wagner, with help from the Gnumeric
  excel plugin by Michael Meeks.
 */
      
I have meant to muck with this for some time now, but this is a very low
priority item for me and other things keep popping up.

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From ripley at stats.ox.ac.uk  Wed Jun 25 18:02:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 17:02:23 +0100 (BST)
Subject: [R] rw1062
In-Reply-To: <000801c33b30$c5089990$cd3318ac@IBM>
Message-ID: <Pine.LNX.4.44.0306251652310.3553-100000@gannet.stats>

The problem is that you have not put the rw1071/bin directory in your 
path, and so your DCOM-using application is unable to find the dependent 
DLLs of R.  This is a problem with the R (D)COM instructions.

On Wed, 25 Jun 2003, S?ren Merser wrote:

> I need 'rw1062.zip' since i can't get excel, R1070 or R1071 to work with
> the R (D)COM Server everything worked fine in the god rw1062 days.

Actually, it didn't without the path set.

> the lapack routines can't be loaded
> 'unable to load shared library c:\r/moduleslapack.dll, the specified library could not be found'
> don't know if it's the forward slashes that anoys windows

They are perfectly valid in Windows, but that `quote' is not the error 
message I get.

> btw, the libraries are in the correct place
> any clues to solve my problem or where to get hold on the zip file??

There never was such a zip file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From forzani at math.umn.edu  Wed Jun 25 19:08:42 2003
From: forzani at math.umn.edu (Liliana Forzani)
Date: Wed, 25 Jun 2003 12:08:42 -0500 (CDT)
Subject: [R] sampling
Message-ID: <Pine.LNX.4.51.0306251206400.6276@corin.math.umn.edu>

I need to know if there is a way in r to apply the hidiroglou lavalle
algorithm...thanks



From dave at evocapital.com  Wed Jun 25 19:32:34 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 25 Jun 2003 18:32:34 +0100
Subject: [R] rw1062
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D01B81B@mail.internal.net>

On the subject of the LAPACK error -- I also had this problem with the R
(D)COM package. It appears that there is a problem accessing the newer
LAPACK routines rather than the original LINPACK routines which were
built into R. I sent a query to the r-com mailing list and found the
following helpful reply from Roger Bivand fixed my problem (see message
below for the full story...). 

> For the time being, do: solve(..., LINPACK=TRUE)

> This uses the older, built-in functions rather than lapack.dll. I
believe 
> a patch has been made to the R windows release, but we'll have to wait

> until 1.7.1, perhaps, for the path to work correctly.

I have not installed 1.7.1 yet so cannot confirm if this is fixed in
this version, although from what you say it seems not.

-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: 14 May 2003 17:19
To: David S. Khabie-Zeitoune
Cc: rcom-l at mailman.csd.univie.ac.at
Subject: Re: [Rcom-l] R Excel complains about not finding lapack.dll


On Wed, 14 May 2003, David S. Khabie-Zeitoune wrote:

> I have just installed the new version of Thomas Baier's RDCOM Server
> package (version 1.2) and the associated Excel interface by Erich 
> Neuwirth (version 1.0). I am using Windows XP Pro.
>  
> I seem to have problems with functions which access the lapack.dll
> such as solve. For example, if I sent a matrix x to R, and I try and 
> run the command "y = solve(x)" from the a cell in Excel using the 
> Excel interface, then I receive an error message:
>  
> >    Error when evaluating
> >    y = solve(x)
> >    evaluation stopped because of an error
>  
> >    Warning: unable to load shared library "C:\Program
> Files\R\rw1070/modules/lapack.dll"
> >        LoadLibrary failure: The specified module cannot be found.
>  
> However, the lapack dll definitely does exist at the specified
> location. Note however the mixture of "back" slashes and "forward" 
> slashes in the path in the error message -- could this be the problem?
>  
For the time being, do: solve(..., LINPACK=TRUE)

This uses the older, built-in functions rather than lapack.dll. I
believe 
a patch has been made to the R windows release, but we'll have to wait 
until 1.7.1, perhaps, for the path to work correctly.

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 25 June 2003 17:02
To: S?ren Merser
Cc: R - help
Subject: Re: [R] rw1062


The problem is that you have not put the rw1071/bin directory in your 
path, and so your DCOM-using application is unable to find the dependent

DLLs of R.  This is a problem with the R (D)COM instructions.

On Wed, 25 Jun 2003, S?ren Merser wrote:

> I need 'rw1062.zip' since i can't get excel, R1070 or R1071 to work 
> with the R (D)COM Server everything worked fine in the god rw1062 
> days.

Actually, it didn't without the path set.

> the lapack routines can't be loaded
> 'unable to load shared library c:\r/moduleslapack.dll, the specified 
> library could not be found' don't know if it's the forward slashes 
> that anoys windows

They are perfectly valid in Windows, but that `quote' is not the error 
message I get.

> btw, the libraries are in the correct place
> any clues to solve my problem or where to get hold on the zip file??

There never was such a zip file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From nphillips at bccancer.bc.ca  Wed Jun 25 20:00:06 2003
From: nphillips at bccancer.bc.ca (Norm Phillips)
Date: Wed, 25 Jun 2003 11:00:06 -0700
Subject: [R] excel files and R
Message-ID: <6BAF4D075F07D411B30900508B94CBA00ED2990A@SERVER20>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030625/a19d9379/attachment.pl

From Edward.Dick at noaa.gov  Wed Jun 25 20:23:55 2003
From: Edward.Dick at noaa.gov (Edward Dick)
Date: Wed, 25 Jun 2003 11:23:55 -0700
Subject: [R] logLik.lm()
Message-ID: <3EF9E8BB.F9BE1C3F@noaa.gov>

Hello,

I'm trying to use AIC to choose between 2 models with
positive, continuous response variables and different error
distributions (specifically a Gamma GLM with log link and a
normal linear model for log(y)). I understand that in some
cases it may not be possible (or necessary) to discriminate
between these two distributions. However, for the normal
linear model I noticed a discrepancy between the output of
the AIC() function and my calculations done "by hand."
This is due to the output from the function logLik.lm(),
which does not match my results using the dnorm() function
(see simple regression example below).

x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
       204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35)
y <- c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
       47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
test.lm <- lm(y~x)
y.hat <- fitted(test.lm)
sigma <- summary(test.lm)$sigma
logLik(test.lm)
# `log Lik.' -57.20699 (df=3)
sum(dnorm(y, y.hat, sigma, log=T))
# [1] -57.26704

The difference in this simple example is slight, but
it is magnified when using my data. My understanding is
that it is necessary to use the complete likelihood functions
for both the Gamma model and the 'lognormal' model (no constants
removed, etc.) in order to accurately compare using AIC.

Can someone point out my error, or explain this discrepancy?

Thanks in advance!



From zeileis at ci.tuwien.ac.at  Wed Jun 25 20:30:39 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 25 Jun 2003 20:30:39 +0200
Subject: [R] logLik.lm()
In-Reply-To: <3EF9E8BB.F9BE1C3F@noaa.gov>
References: <3EF9E8BB.F9BE1C3F@noaa.gov>
Message-ID: <200306251830.h5PIUePF010374@thorin.ci.tuwien.ac.at>

On Wednesday 25 June 2003 20:23, Edward Dick wrote:

> Hello,
>
> I'm trying to use AIC to choose between 2 models with
> positive, continuous response variables and different error
> distributions (specifically a Gamma GLM with log link and a
> normal linear model for log(y)). I understand that in some
> cases it may not be possible (or necessary) to discriminate
> between these two distributions. However, for the normal
> linear model I noticed a discrepancy between the output of
> the AIC() function and my calculations done "by hand."
> This is due to the output from the function logLik.lm(),
> which does not match my results using the dnorm() function
> (see simple regression example below).
>
> x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
>       
> 204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35) y <-
> c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
> 47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
> test.lm <- lm(y~x)
> y.hat <- fitted(test.lm)
> sigma <- summary(test.lm)$sigma
> logLik(test.lm)
> # `log Lik.' -57.20699 (df=3)
> sum(dnorm(y, y.hat, sigma, log=T))
> # [1] -57.26704
>
> The difference in this simple example is slight, but
> it is magnified when using my data.

That is because you are not using the ML estimate of the variance.

sigmaML <- sqrt(mean(residuals(test.lm)^2))
sum(dnorm(y, y.hat, sigmaML, log=T))
# [1] -57.20699

Best,
Z

> My understanding is
> that it is necessary to use the complete likelihood functions
> for both the Gamma model and the 'lognormal' model (no constants
> removed, etc.) in order to accurately compare using AIC.
>
> Can someone point out my error, or explain this discrepancy?
>
> Thanks in advance!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bertola at fastmail.fm  Wed Jun 25 20:48:42 2003
From: bertola at fastmail.fm (Rafael Bertola)
Date: Wed, 25 Jun 2003 15:48:42 -0300
Subject: [R] robust regression
Message-ID: <20030625184842.6D53337526@www.fastmail.fm>

Is there a command in R that make the same regression like l1fit in
S-plus?
-- 
  
  bertola at fastmail.fm

--



From ripley at stats.ox.ac.uk  Wed Jun 25 20:59:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 19:59:59 +0100 (BST)
Subject: [R] logLik.lm()
In-Reply-To: <3EF9E8BB.F9BE1C3F@noaa.gov>
Message-ID: <Pine.LNX.4.44.0306251931141.3859-100000@gannet.stats>

Your by-hand calculation is wrong -- you have to use the MLE of sigma^2.

sum(dnorm(y, y.hat, sigma * sqrt(16/18), log=TRUE))

Also, this is an inappropriate use of AIC: the models are not nested, and
Akaike only proposed it for nested models.  Next, the gamma GLM is not a
maximum-likelihood fit unless the shape parameter is known, so you can't
use AIC with such a model using the dispersion estimate of shape

The AIC output from glm() is incorrect (even in that case, since the
shape is always estimated by the dispersion).

On Wed, 25 Jun 2003, Edward Dick wrote:

> Hello,
> 
> I'm trying to use AIC to choose between 2 models with
> positive, continuous response variables and different error
> distributions (specifically a Gamma GLM with log link and a
> normal linear model for log(y)). I understand that in some
> cases it may not be possible (or necessary) to discriminate
> between these two distributions. However, for the normal
> linear model I noticed a discrepancy between the output of
> the AIC() function and my calculations done "by hand."
> This is due to the output from the function logLik.lm(),
> which does not match my results using the dnorm() function
> (see simple regression example below).
> 
> x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
>        204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35)
> y <- c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
>        47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
> test.lm <- lm(y~x)
> y.hat <- fitted(test.lm)
> sigma <- summary(test.lm)$sigma
> logLik(test.lm)
> # `log Lik.' -57.20699 (df=3)
> sum(dnorm(y, y.hat, sigma, log=T))
> # [1] -57.26704

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 25 21:06:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 20:06:49 +0100 (BST)
Subject: [R] robust regression
In-Reply-To: <20030625184842.6D53337526@www.fastmail.fm>
Message-ID: <Pine.LNX.4.44.0306252000540.3859-100000@gannet.stats>

You can use the quantreg package.

However, neither l1fit nor that do `robust regression', so you need to 
think more carefully about what you really want.  There are almost always 
better alternatives than L1 fits.

On Wed, 25 Jun 2003, Rafael Bertola wrote:

> Is there a command in R that make the same regression like l1fit in
> S-plus?
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hdoran at nasdc.org  Wed Jun 25 21:09:22 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Wed, 25 Jun 2003 15:09:22 -0400
Subject: [R] NLME Covariates
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FA4@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030625/d9f7d273/attachment.pl

From ozric at web.de  Wed Jun 25 21:23:40 2003
From: ozric at web.de (Christian Schulz)
Date: Wed, 25 Jun 2003 21:23:40 +0200
Subject: [R] Markov chain simulation
References: <3EF956D5.6040309@curie.fr>
Message-ID: <008b01c33b4f$4912cd50$b108ebd9@pc>

Is the Markov chain Monte Carlo (MCMC) Package
perhaps something for you!?

christian


----- Original Message -----
From: "Philippe Hup?" <Philippe.Hupe at curie.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 25, 2003 10:01 AM
Subject: [R] Markov chain simulation


Hi,

Does anybody know a function to simulate a Markov chain given a
probability transition matrix and an initial state ?
Thanks.

Philippe
--

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From clists at perrin.socsci.unc.edu  Wed Jun 25 21:49:09 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Wed, 25 Jun 2003 15:49:09 -0400 (EDT)
Subject: [R] NLME Covariates
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FA4@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE930139FA4@ernesto.NASDC.ORG>
Message-ID: <Pine.LNX.4.53.0306251548250.13959@perrin.socsci.unc.edu>

I am foggy on this myself, but I *think* it is inferred from the grouping
structure in the call to (n)lme or in the groupedData data structure. Have
a look at ?groupedData in R for more details.

ap

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Wed, 25 Jun 2003, Harold Doran wrote:

> Dear list
>
> In HLM, one can specify a covariate at one of the "levels". For example, if the data structure are repeated observations nested within students nested within schools, school size might be a covariate that is used at level 3, but not at the other levels. In HLM this is rather easy to do.
>
> However, how can one specify a covariate in R for only one of the levels? I have a sample data set with the structure as described above. I fit the unconditional model in R as
>
> model1<-lme(math~year, random=~year|schoolid/childid, data=datafile)
>
> Now, if I want to enter "female" as a covariate at level 2 only, how might I modify the code to accomplish this?
>
> ------
> Harold C. Doran
> Director of Research and Evaluation
> New American Schools
> 675 N. Washington Street, Suite 220
> Alexandria, Virginia 22314
> 703.647.1628
>  <http://www.edperform.net/>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Wed Jun 25 21:52:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Jun 2003 12:52:05 -0700
Subject: [R] logLik.lm()
References: <Pine.LNX.4.44.0306251931141.3859-100000@gannet.stats>
Message-ID: <3EF9FD65.7010104@pdf.com>

Dear Prof. Ripley:

	  I gather you disagree with the observation in Burnham and Anderson 
(2002, ch. 2) that the "complexity penalty" in the Akaike Information 
Criterion is a bias correction, and with this correction, they can use 
"density = exp(-AIC/2)" to compute approximate posterior probabilities 
comparing even different distributions?

	  They use this even to compare discrete and continuous distributions, 
which makes no sense to me.  However, with a common dominating measure, 
it seems sensible to me.  They cite a growing literature on "Bayesian 
model averaging".  What I've seen of this claims that Bayesian model 
averaging produces better predictions than predictions based on any 
single model, even using these approximate posteriors ("Akaike weights") 
in place of full Bayesian posteriors.

	  I don't have much experience with this, but so far, I seem to have 
gotten great, informative answers to my clients' questions.  If there 
are serious deficiencies with this kind of procedure, I'd like to know.

Comments?
Best Wishes,
Spencer Graves
###############
REFERENCE:  Burnam and Anderson (2002) Model Selection and Multimodel 
Inference, 2nd ed. (Springer)

Prof Brian Ripley wrote:
> Your by-hand calculation is wrong -- you have to use the MLE of sigma^2.
> 
> sum(dnorm(y, y.hat, sigma * sqrt(16/18), log=TRUE))
> 
> Also, this is an inappropriate use of AIC: the models are not nested, and
> Akaike only proposed it for nested models.  Next, the gamma GLM is not a
> maximum-likelihood fit unless the shape parameter is known, so you can't
> use AIC with such a model using the dispersion estimate of shape
> 
> The AIC output from glm() is incorrect (even in that case, since the
> shape is always estimated by the dispersion).
> 
> On Wed, 25 Jun 2003, Edward Dick wrote:
> 
> 
>>Hello,
>>
>>I'm trying to use AIC to choose between 2 models with
>>positive, continuous response variables and different error
>>distributions (specifically a Gamma GLM with log link and a
>>normal linear model for log(y)). I understand that in some
>>cases it may not be possible (or necessary) to discriminate
>>between these two distributions. However, for the normal
>>linear model I noticed a discrepancy between the output of
>>the AIC() function and my calculations done "by hand."
>>This is due to the output from the function logLik.lm(),
>>which does not match my results using the dnorm() function
>>(see simple regression example below).
>>
>>x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
>>       204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35)
>>y <- c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
>>       47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
>>test.lm <- lm(y~x)
>>y.hat <- fitted(test.lm)
>>sigma <- summary(test.lm)$sigma
>>logLik(test.lm)
>># `log Lik.' -57.20699 (df=3)
>>sum(dnorm(y, y.hat, sigma, log=T))
>># [1] -57.26704
> 
>



From ripley at stats.ox.ac.uk  Wed Jun 25 22:19:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jun 2003 21:19:56 +0100 (BST)
Subject: [R] logLik.lm()
In-Reply-To: <3EF9FD65.7010104@pdf.com>
Message-ID: <Pine.LNX.4.44.0306252115340.4100-100000@gannet.stats>

On Wed, 25 Jun 2003, Spencer Graves wrote:

> Dear Prof. Ripley:
> 
> 	  I gather you disagree with the observation in Burnham and Anderson 
> (2002, ch. 2) that the "complexity penalty" in the Akaike Information 
> Criterion is a bias correction, and with this correction, they can use 
> "density = exp(-AIC/2)" to compute approximate posterior probabilities 
> comparing even different distributions?

That's the derivation of BIC and similar, not AIC.

> 	  They use this even to compare discrete and continuous distributions, 
> which makes no sense to me.  However, with a common dominating measure, 
> it seems sensible to me.  They cite a growing literature on "Bayesian 
> model averaging".  What I've seen of this claims that Bayesian model 
> averaging produces better predictions than predictions based on any 
> single model, even using these approximate posteriors ("Akaike weights") 
> in place of full Bayesian posteriors.
> 
> 	  I don't have much experience with this, but so far, I seem to have 
> gotten great, informative answers to my clients' questions.  If there 
> are serious deficiencies with this kind of procedure, I'd like to know.

Yes, model averaging is useful, but is nothing to do with AIC nor Burnham
& Anderson.  See e.g. my PRNN book for better ways to do it.

Burnham & Anderson (2002) is a book I would recommend people NOT to read
until they have read the primary literature.  I see no evidence that the 
authors have actually read Akaike's papers.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From arrayprofile at yahoo.com  Wed Jun 25 22:48:23 2003
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 25 Jun 2003 13:48:23 -0700 (PDT)
Subject: [R] probelem of function inside function
Message-ID: <20030625204823.35956.qmail@web41202.mail.yahoo.com>

Hi,

I encountered a problem when I am trying to write my
own function which contains another function. To
simplify a problem, I tried the following simplified
function, hope someone can idenfity the problem for
me.

I have a simple data frame called "testdata" as
following:

>
testdata<-data.frame(cbind(x=c(129,109,52,118,217,278,606,198,99,133),y=c(89,118,99,137,34,14,130,30,131,30)))
> testdata
     x   y 
 1 129  89
 2 109 118
 3  52  99
 4 118 137
 5 217  34
 6 278  14
 7 606 130
 8 198  30
 9  99 131
10 133  30

I have write a simple function (nonsense):

> f.fun<-function(var,fdata) {
      fdata<-data.frame(fdata)
      fit<-lm(y~1,data=fdata)

      f.addterm<-function(x,fit,f.data) {
	fdata<-f.data
   	fff<-paste('~.',x,sep='+')
   	try(addterm(fit,as.formula(fff))$AIC[2])
      }

      f.addterm(var,fit,fdata)
 }

This function simply add the "x" variale to the linear
regression with intercept only using addterm, and
return the AIC of the new model. I know I can simply
do this with stepAIC of MASS, but for my own
situation, I want to try something more complicated.

so when I execute the following command, I got an
error message:

> f.fun(var='x',fdata=testdata)
Problem in eval(oc, list()): Object "fdata" not found 
Use traceback() to see the call stack
[1] "Problem in eval(oc, list()): Object \"fdata\" not
found \nUse traceback() to see the call stack"
attr(, "class"):
[1] "Error"

I guess the reason is that within the inside function
"f.addterm", the dataset "fdata" can't be seen, but I
don't know why because I think I have passed the
dataset into the inside function.

However, if I create a dataset called "fdata" outside
the function "f.fun" at the very beginning, everything
will be fine. This is obvious because now "fdata" is a
global dataset now.

> fdata=testdata
> f.fun(var='x',fdata=testdata)
[1] 80.81915


Thanks

Zander.



From SymantecAlerts at perlegen.com  Wed Jun 25 22:57:39 2003
From: SymantecAlerts at perlegen.com (SymantecAlerts@perlegen.com)
Date: Wed, 25 Jun 2003 13:57:39 -0700
Subject: [R] This item has been released from quarantine.
Message-ID: <00e401c33b5c$69f59cc0$410110ac@perlegen.com>


This file, which was attached to the message titled "R-help Digest, Vol 4, Issue 25" by "r-help-request at stat.math.ethz.ch" and was quarantined on 6/25/2003 3:18 AM, has been released. 

NOTE: If AutoProtect is enabled, then this restored attachment will be rescanned during the restore. If the attachment is still infected, the current virus detection policy will apply to this attachment.

From xiao.gang.fan1 at libertysurf.fr  Wed Jun 25 23:16:09 2003
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Wed, 25 Jun 2003 23:16:09 +0200
Subject: [R] excel files and R
References: <Pine.A41.4.44.0306250709280.181146-100000@homer06.u.washington.edu>
	<1056553327.4192.65.camel@localhost>
Message-ID: <3EFA1119.30704@libertysurf.fr>

For loading Excel data and many others file formats,
one possibility is to use the free conversion utility: DataLoad.

See: http://www.vsn-intl.com/genstat/downloads/datald.htm
(there're probably also other links)

It should be easy to create R wrappers to use that utility.

Cheers
--
Fan

Marc Schwartz wrote:
> On Wed, 2003-06-25 at 09:20, Thomas Lumley wrote:
> 
>>On Wed, 25 Jun 2003, Simon Fear wrote:
>>
>>
>>>I guess all that I and apparently others really want is that "foreign"
>>>might
>>>include read.excel, like it has read.sas and read.spss. Which is
>>>essentially
>>>what Bernhard Pfaff's recent post offers - thanks again Bernhard - but
>>>using
>>>RODBC instead of foreign.
>>>
>>
>>It would be nice, but it's quite hard to read Excel off Windows.
>>
>>The formats in foreign are either documented by the vendor (accurately in
>>the case of Stata and Epi Info, with some omissions for SAS XPORT) or that
>>have been reverse-engineered by someone else (read.spss is based on
>>PSPP, an attempt at an SPSS clone by Ben Pfaaf, and I think Duncan
>>Murdoch did read.S).
>>
>>
>>While it isn't usual to say nice things about commercial vendors on these
>>lists I would like to note that Stata not only documents its file format
>>in its manuals (with some helpful C snippets for the trickier parts), but
>>made available the file format for their `large data set' version 7/SE,
>>which I didn't buy.
>>
>>
>>	-thomas
> 
> 
> 
> Simon,
> 
> To add to Thomas' comments and respond to your thoughts, if one were so
> inclined, given that R is a volunteer effort, I suspect that an addition
> to 'foreign' for Excel would indeed be appreciated by many users.
> 
> One resource, with appropriate attribution given, would be the source
> code for OpenOffice.org's (OOo) Calc. Since Calc can read and write
> Excel formats without using Windows/Office DLL's, it seems reasonable to
> presume that OOo has reverse engineered the native Excel file structure.
> Since OOo's source is available under the GPL, this could provide the
> basis for a "read.excel" function.
> 
> Yet another would be Gnumeric, which like Calc is GPL'd and can read and
> write native Excel file formats.
> 
> More information is available at:
> 
> http://www.openoffice.org/dev_docs/source/1.0.3/source.html
> 
> http://www.gnome.org/projects/gnumeric/
> 
> 
> Food for thought...   :-)
> 
> Regards,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From andrejk at zrc-sazu.si  Wed Jun 25 23:24:16 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Wed, 25 Jun 2003 23:24:16 +0200
Subject: [R] within group variance of the coeficients in LME
Message-ID: <FHEEJBDDCNPPNJEACDJACEOACOAA.andrejk@zrc-sazu.si>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030625/2204dfd5/attachment.pl

From prathap_rb at mail.utexas.edu  Wed Jun 25 23:29:39 2003
From: prathap_rb at mail.utexas.edu (Babu Prathap R)
Date: Wed, 25 Jun 2003 16:29:39 -0500
Subject: [R] DB2 ODBC
Message-ID: <1056576579.3efa1443dbd6e@webmailapp1.cc.utexas.edu>

Hi,

I need to do some statistical analysis of data that is stored in IBM DB2 
UDB 8.1 using R. I understand that I need to use R ODBC. But I would like 
to know more about how to go about this. I apprecaite your help.

Thanks
Babu



From peter at fe.up.pt  Wed Jun 25 23:36:54 2003
From: peter at fe.up.pt (Peter Ho)
Date: Wed, 25 Jun 2003 22:36:54 +0100
Subject: [R] extracting effects estimates from aov model with Error()
Message-ID: <3EFA15F6.1090902@fe.up.pt>

Hi all,

Does anyone know how I might be able to extract effects estimates from 
an unreplicated blocked factorial design?
 My aov model was the following:

aov(AM ~ Tf*Tm*Pl*Ps*Mc*M0*M1*M2*M3+Error(Batch), initial)

Using effects() , I get the following response:

 > effects(CV.unreplicated.block.aov.summary)
Error in effects(CV.unreplicated.block.aov.summary) :
        no applicable method for "effects"

Thanks

Peter



From hdoran at nasdc.org  Thu Jun 26 00:58:08 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Wed, 25 Jun 2003 18:58:08 -0400
Subject: [R] within group variance of the coeficients in LME
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FA8@ernesto.NASDC.ORG>

lme does not produce standard errors for the variance components like HLM does. It does produce SEs for the fixed effects, however, along with t-statistics and p-values, just like HLM. Use the summary() command to see these. 
 
When you do this, you will get the AIC, BIC, and loglik values. Just below this output will be the variance components for the random effects. But, the level 2 variance components are reported as standard deviations and SEs do not accompany these random effects.
 
In lme,  the residual is the within-group error, which is the sigma-squared in HLM.
 
In terms of lme, the plot(intervals) can be used to assess variability rather than the chi-square in HLM. 
 
 

	-----Original Message----- 
	From: Andrej Kveder [mailto:andrejk at zrc-sazu.si] 
	Sent: Wed 6/25/2003 5:24 PM 
	To: R-Help 
	Cc: 
	Subject: [R] within group variance of the coeficients in LME
	
	

	Dear listers, 

	I can't find the variance or se of the coefficients in a multilevel model 
	using lme. 

	I want to calculate a Chi square test statistics for the variability of the 
	coefficients across levels. I have a simple 2-level problem, where I want to 
	check weather a certain covariate varies across level 2 units. Pinheiro 
	Bates suggest just looking at the intervals or doing a rather conservative 
	ANOVA test in their book. I have also consultet Raudenbush Bryk on the 
	subject and they suggest using a Chi sqare statistics. It is defined as 
	follows: 

	SUM by j( (beta_hat_qj - y_hat_q0 - sum(y_hat_qs*w_sj))^2/V_hat_qqj) 

	beta are the within 2-level coffecients - got them with the coef() 
	y is a fixed effect or grand mean 
	the sum is for accounting of the level 2 predictors, that I don't presently 
	have, but will 
	the problem is V_hat_qqj which are the variances of the coefficients. 

	I can't get to them. Does anybody have an idea how to get to them? I would 
	really appreciate any suggestion. 

	Andrej 

	_________ 
	Andrej Kveder, M.A. 
	researcher 
	Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 Ljubljana, 
	Slovenia 
	phone: +386 1 47 06 440   fax: +386 1 42 61 493 

	        [[alternative HTML version deleted]] 

	______________________________________________ 
	R-help at stat.math.ethz.ch mailing list 
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Simon.Blomberg at anu.edu.au  Thu Jun 26 01:22:11 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 26 Jun 2003 09:22:11 +1000
Subject: [R] probelem of function inside function
Message-ID: <7A3A13F416B40842BD2C1753E044B359B09928@CASEVS02.cas.anu.edu.au>

Your function works fine for me (R 1.7.1, Windows 2000):

> library(MASS)
> testdata<-data.frame(cbind(x=c(129,109,52,118,217,278,606,198,99,133),y=c(89,118,99,137,34,14,130,30,131,30)))
> f.fun<-function(var,fdata) {
+       fdata<-data.frame(fdata)
+       fit<-lm(y~1,data=fdata)
+ 
+       f.addterm<-function(x,fit,f.data) {
+ fdata<-f.data
+    fff<-paste('~.',x,sep='+')
+    try(addterm(fit,as.formula(fff))$AIC[2])
+       }
+ 
+       f.addterm(var,fit,fdata)
+  }
> f.fun("x", testdata)
       x 
80.81915 
> 

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: array chip [mailto:arrayprofile at yahoo.com]
> Sent: Thursday, 26 June 2003 6:48 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] probelem of function inside function
> 
> 
> Hi,
> 
> I encountered a problem when I am trying to write my
> own function which contains another function. To
> simplify a problem, I tried the following simplified
> function, hope someone can idenfity the problem for
> me.
> 
> I have a simple data frame called "testdata" as
> following:
> 
> >
> testdata<-data.frame(cbind(x=c(129,109,52,118,217,278,606,198,
> 99,133),y=c(89,118,99,137,34,14,130,30,131,30)))
> > testdata
>      x   y 
>  1 129  89
>  2 109 118
>  3  52  99
>  4 118 137
>  5 217  34
>  6 278  14
>  7 606 130
>  8 198  30
>  9  99 131
> 10 133  30
> 
> I have write a simple function (nonsense):
> 
> > f.fun<-function(var,fdata) {
>       fdata<-data.frame(fdata)
>       fit<-lm(y~1,data=fdata)
> 
>       f.addterm<-function(x,fit,f.data) {
> 	fdata<-f.data
>    	fff<-paste('~.',x,sep='+')
>    	try(addterm(fit,as.formula(fff))$AIC[2])
>       }
> 
>       f.addterm(var,fit,fdata)
>  }
> 
> This function simply add the "x" variale to the linear
> regression with intercept only using addterm, and
> return the AIC of the new model. I know I can simply
> do this with stepAIC of MASS, but for my own
> situation, I want to try something more complicated.
> 
> so when I execute the following command, I got an
> error message:
> 
> > f.fun(var='x',fdata=testdata)
> Problem in eval(oc, list()): Object "fdata" not found 
> Use traceback() to see the call stack
> [1] "Problem in eval(oc, list()): Object \"fdata\" not
> found \nUse traceback() to see the call stack"
> attr(, "class"):
> [1] "Error"
> 
> I guess the reason is that within the inside function
> "f.addterm", the dataset "fdata" can't be seen, but I
> don't know why because I think I have passed the
> dataset into the inside function.
> 
> However, if I create a dataset called "fdata" outside
> the function "f.fun" at the very beginning, everything
> will be fine. This is obvious because now "fdata" is a
> global dataset now.
> 
> > fdata=testdata
> > f.fun(var='x',fdata=testdata)
> [1] 80.81915
> 
> 
> Thanks
> 
> Zander.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From imhere at whereisjesus.com  Thu Jun 26 01:25:19 2003
From: imhere at whereisjesus.com (Jesus of Nazareth)
Date: Wed, 25 Jun 2003 16:25:19 -0700
Subject: [R] RE: Movie
In-Reply-To: <E19VJa1-0004qQ-00@server2.dayanadns.com>
Message-ID: <00d001c33b71$0c7ba980$6401a8c0@jeebus>

My Child,

This is getting truly annoying. I know I invented the concept of
brotherly love and all, but really...go fuck yourself.

Love,

Jesus



> -----Original Message-----
> From: r-announce at lists.r-project.org
[mailto:r-announce at lists.r-project.org]
> Sent: Wednesday, June 25, 2003 4:23 PM
> To: imhere at whereisjesus.com
> Subject: Re: Movie
> 
> Please see the attached zip file for details.



From tlumley at u.washington.edu  Thu Jun 26 01:34:22 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jun 2003 16:34:22 -0700 (PDT)
Subject: [R] probelem of function inside function
In-Reply-To: <20030625204823.35956.qmail@web41202.mail.yahoo.com>
Message-ID: <Pine.A41.4.44.0306251632190.39252-100000@homer21.u.washington.edu>

On Wed, 25 Jun 2003, array chip wrote:

> Hi,
>
> I encountered a problem when I am trying to write my
> own function which contains another function. To
> simplify a problem, I tried the following simplified
> function, hope someone can idenfity the problem for
> me.
>
<snip>
> > f.fun(var='x',fdata=testdata)
> Problem in eval(oc, list()): Object "fdata" not found
> Use traceback() to see the call stack

This looks very much like an S-PLUS error message, and in addition to any
etiquette issues about asking S-PLUS questions on r-help, this is the sort
of question where the answer may actually be quite different in the two
systems.

	-thomas



From arrayprofile at yahoo.com  Thu Jun 26 01:46:50 2003
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 25 Jun 2003 16:46:50 -0700 (PDT)
Subject: [R] probelem of function inside function
In-Reply-To: <Pine.A41.4.44.0306251632190.39252-100000@homer21.u.washington.edu>
Message-ID: <20030625234650.72297.qmail@web41214.mail.yahoo.com>

Actually, It is the question I encountered in S-Plus.
Sorry that I forgot to mention. As Simon just pointed
out, the function works fine in R (Thanks!). But in
any case, does anyone know how to solve the problem in
S-Plus?

Thanks


--- Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Wed, 25 Jun 2003, array chip wrote:
> 
> > Hi,
> >
> > I encountered a problem when I am trying to write
> my
> > own function which contains another function. To
> > simplify a problem, I tried the following
> simplified
> > function, hope someone can idenfity the problem
> for
> > me.
> >
> <snip>
> > > f.fun(var='x',fdata=testdata)
> > Problem in eval(oc, list()): Object "fdata" not
> found
> > Use traceback() to see the call stack
> 
> This looks very much like an S-PLUS error message,
> and in addition to any
> etiquette issues about asking S-PLUS questions on
> r-help, this is the sort
> of question where the answer may actually be quite
> different in the two
> systems.
> 
> 	-thomas
>



From antiv at univ-lyon1.fr  Thu Jun 26 02:23:55 2003
From: antiv at univ-lyon1.fr (antiv@univ-lyon1.fr)
Date: Thu, 26 Jun 2003 02:23:55 +0200 (CEST)
Subject: [R] =?iso-8859-1?q?Votre_courrier_est_rejet=E9?=
Message-ID: <20030626002355.1D2F510C1BA@pop.univ-lyon1.fr>



                        A L E R T E  V I R U S


        Notre antivirus a d?tect? le VIRUS "W32/Sobig.e at MM" dans votre
courrier destin? ? :

-> dray at biomserv.univ-lyon1.fr

        Nous refusons de le transmettre.

        Nous vous conseillons de d?contaminer votre poste de travail.

        Pour votre information, vous trouverez ci-dessous les entetes de votre
        message
        Pour des informations sur ce virus et comment l'?liminer:
              http://vil.nai.com/vil/default.asp
        Pour des informations sur l'analyseur de courriers
              http://amavis.org/





                        V I R U S  A L E R T


        Our viruschecker found the

        	W32/Sobig.e at MM

        virus(es) in your email to the following recipient(s):

-> dray at biomserv.univ-lyon1.fr

        Delivery of the email was stopped!

        Now it is on you to check your system for viruses

        For further information about this virus see:
                http://vil.nai.com/vil/default.asp
        For further information about this viruschecker see:
              http://amavis.org/



For your reference, here are the headers from your email:

------------------------- BEGIN HEADERS -----------------------------
Return-Path: <r-announce at lists.r-project.org>
Received: from VAC0038 (unknown [137.53.23.44])
	by pop.univ-lyon1.fr (Postfix) with ESMTP id 8ECE510BBA7
	for <dray at biomserv.univ-lyon1.fr>; Thu, 26 Jun 2003 02:23:51 +0200 (CEST)
From: <r-announce at lists.r-project.org>
To: <dray at biomserv.univ-lyon1.fr>
Subject: Re: Movie
Date: Wed, 25 Jun 2003 17:23:48 --0700
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="CSmtpMsgPart123X456_000_00BF48F7"
Message-Id: <20030626002351.8ECE510BBA7 at pop.univ-lyon1.fr>
-------------------------- END HEADERS ------------------------------



From dcum007 at ec.auckland.ac.nz  Thu Jun 26 02:46:08 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Thu, 26 Jun 2003 12:46:08 +1200
Subject: [R] Random Number Testing
Message-ID: <1056588368.3efa4250e57cd@webmail2.ec.auckland.ac.nz>

I would like to thank all of you who replied to my last question regarding the 
generation of random numbers. I have read much about the theory and have been 
fortunate to find some rng code. Now my task lies in testing the randomness of 
these rng's. In the literature, there are many tests that can be done. Are 
there any of the more sophisticated tests (ie like the birthday spacing, or 
serial correlation test) in R?
I couldn't find any but thought someone with a bit more experience would have 
a better insight.
Thank you
David



From sunduz at stat.berkeley.edu  Thu Jun 26 03:04:03 2003
From: sunduz at stat.berkeley.edu (SUNDUZ Keles)
Date: Wed, 25 Jun 2003 18:04:03 -0700 (PDT)
Subject: [R] nonlinear constraint optimization function in R?
Message-ID: <Pine.SOL.4.50.0306251755530.12367-100000@glinda.Berkeley.EDU>


Dear all,

I am looking for a R-function that performs nonlinear constraint
optimization (where  the objective function and the
constraints are allowed to be nonlinear).

More specifically, I am trying to figure out whether there
is an equivalent function of NAG fortran library'  "E04UCF" or matlab's
"fmincon"  in R.

Any help is greatly appreciated.

Thank you,
Sunduz


PS: fmincon is described at
http://www.mathworks.com/access/helpdesk/help/toolbox/optim/optim.shtml?BB=1



From postmaster at w3.org  Thu Jun 26 03:09:42 2003
From: postmaster at w3.org (W3C Postmaster)
Date: Wed, 25 Jun 2003 21:09:42 -0400
Subject: [R] Re: Application
In-Reply-To: <200306260109.h5Q19WlY025916@foudre.inria.fr>
References: <200306260109.h5Q19WlY025916@foudre.inria.fr>
Message-ID: <200306260109.h5Q19gNC015858@dr-nick.w3.org>

This is an autoreply to a message you have sent to <lehors at w3.org>.

Arnaud Le Hors no longer works for W3C; he may be now reached at
<lehors at us.ibm.com>. Please resend your message there.

Thanks,

-- W3C Postmaster
  <postmaster at w3.org>
----------------------------------------------------------------------
>From r-announce at hypatia.math.ethz.ch Wed Jun 25 21:09:42 2003
Received: from sophia.inria.fr (sophia.inria.fr [138.96.64.20])
	by dr-nick.w3.org (8.12.3/8.12.3/Debian-6.4) with ESMTP id h5Q19enV015848
	for <lehors at w3.org>; Wed, 25 Jun 2003 21:09:41 -0400
Received: from foudre.inria.fr (foudre.inria.fr [138.96.64.12])
	by sophia.inria.fr (8.12.8/8.12.5) with ESMTP id h5Q19Zb9005332
	for <lehors at sophia.inria.fr>; Thu, 26 Jun 2003 03:09:35 +0200
Received: from HSCL-STITCH ([129.255.53.155])
	by foudre.inria.fr (8.12.8/8.12.5) with ESMTP id h5Q19WlY025916
	for <lehors at sophia.inria.fr>; Thu, 26 Jun 2003 03:09:33 +0200 (MET DST)
Message-Id: <200306260109.h5Q19WlY025916 at foudre.inria.fr>
From: <r-announce at hypatia.math.ethz.ch>
To: <lehors at sophia.inria.fr>
Subject: Re: Application
Date: Wed, 25 Jun 2003 20:10:45 --0500
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="CSmtpMsgPart123X456_000_68BAB2D1"

This is a multipart message in MIME format

--CSmtpMsgPart123X456_000_68BAB2D1
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: 7bit

Please see the attached zip file for details.
--CSmtpMsgPart123X456_000_68BAB2D1
Content-Type: application/x-zip-compressed;
	name="your_details.zip"
Content-Transfer-Encoding: base64
Content-Disposition: attachment;
	filename="your_details.zi"
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From kjetil at entelnet.bo  Thu Jun 26 04:49:04 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Wed, 25 Jun 2003 22:49:04 -0400
Subject: [R] Markov chain simulation
In-Reply-To: <3EF956D5.6040309@curie.fr>
Message-ID: <3EFA26E0.6851.11A716B@localhost>

On 25 Jun 2003 at 10:01, Philippe Hup? wrote:

For small transistion matrices (which seems to be what you ask for), 
something like this:

> P <- matrix( c(0.2, 0.5, 0.5, 0.1, 0.8, 0.1, 
+               0.2, 0.1, 0.7), 3,3, byrow=TRUE)
> simMarkov <- function( P, len=1000) {
+        n <- NROW(P)
+        result <- numeric(len)
+        result[1] <- 1
+        for (i in 2:len) {
+            result[i] <- sample(1:n, 1, prob=P[ result[i-1], ])
+        }
+        result
+ }
> muestra <- simMarkov(P)

Note that this assumes the initial state to be 1, easy to modify wuth 
an extra argument to simMarkov.

Whe seeing a for-loop solution as above, it is natural to ask for a 
vectorized solution. But whith the FUNction within the loop using 
earlier results, that seems difficult.

Kjetil Halvorsen

> Hi,
> 
> Does anybody know a function to simulate a Markov chain given a 
> probability transition matrix and an initial state ?
> Thanks.
> 
> Philippe
> -- 
> 
> --------------------------------------------------
> 
> Philippe Hup?
> Institut Curie - Equipe Bioinformatique
> 26, rue d'Ulm - 75005 PARIS France
> +33 (0)1 42 34 65 29
> 
> Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From appstat at hotpop.com  Thu Jun 26 06:10:52 2003
From: appstat at hotpop.com (wildscop)
Date: Thu, 26 Jun 2003 10:10:52 +0600
Subject: [R] Residual plotting
Message-ID: <5.1.0.14.2.20030626100955.02360610@mail.dhaka.net>

Dear all,

So far i could do (in an informal way) to draw a Standardized Resisual plot 
in the following way-
---------------------
 >x <- c(104.1, 106.6, 105.5, 107.5, 109.6, 113.3, 115.5, 117.7, 119.9, 
122.1, 124.3, 126.5, 128.2)
 >y <- c(53732, 52912, 57005, 61354, 67682, 71602, 71961, 75309, 82931, 
93310, 102161, 103068, 108927)
 > beta1<-(n*sum(x*y)-sum(x)*sum(y))/(n*sum(x^2)-sum(x)^2)
 > beta0<-sum(y)/n-beta1*sum(x)/n
 > yhat<-beta0 + beta1*x
 > error<-(y-yhat)
 > se <- sqrt((sum(y^2)-beta0*sum(y)-beta1*sum(x*y))/(n-2))
 > SE<-sqrt(sum(error^2)/(n-2)) # just another way
 > plot(x,error)
 > plot(x,error/se)
---------------------
Or, in the formal way (using s-plus functions), in the following way, but i 
get stuck in the Resisual plot stage, don't know how to draw the 
"Standardized Resisual plot" in this way :

---------------------
 >x <- c(104.1, 106.6, 105.5, 107.5, 109.6, 113.3, 115.5, 117.7, 119.9, 
122.1, 124.3, 126.5, 128.2)
 >y <- c(53732, 52912, 57005, 61354, 67682, 71602, 71961, 75309, 82931, 
93310, 102161, 103068, 108927)
 > cripop<-rbind(x,y)
 > dimnames(cripop)<-NULL
 > columns <- c("1987", "1988", "1989", "1990", "1991", "1992", "1993", 
"1994", "1995", "1996", "1997", "1998", "1999" )
 > rows<-c("Population","Crimes")
 > dimnames(cripop)<-list(rows,columns)
 > bd<-t(cripop)
 > bd.frame<-data.frame(bd)
 > attach(bd.frame)
 > regressions<-lm(Crimes~Population,data=bd.frame)
 > plot(Population,resid(regressions))
 > ...?...
---------------------
Can any one help me by telling me how can i draw "Standardized Resisual 
plot" from here ?

Also, is there any way i can construct 95% Confidence interval or 
Prediction interval for any value in R ?

_______________________

Mohammad Ehsanul Karim <appstat at HotPOP.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh
_______________________


_______________________

Mohammad Ehsanul Karim <appstat at HotPOP.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From Simon.Blomberg at anu.edu.au  Thu Jun 26 07:38:00 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 26 Jun 2003 15:38:00 +1000
Subject: [R] Residual plotting
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133C9@CASEVS02.cas.anu.edu.au>


> -----Original Message-----
> From: wildscop [mailto:appstat at hotpop.com]
> Sent: Thursday, 26 June 2003 2:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Residual plotting
> 

[snip]

> Or, in the formal way (using s-plus functions), in the 
> following way, but i 
> get stuck in the Resisual plot stage, don't know how to draw the 
> "Standardized Resisual plot" in this way :
> 
> ---------------------
>  >x <- c(104.1, 106.6, 105.5, 107.5, 109.6, 113.3, 115.5, 
> 117.7, 119.9, 
> 122.1, 124.3, 126.5, 128.2)
>  >y <- c(53732, 52912, 57005, 61354, 67682, 71602, 71961, 
> 75309, 82931, 
> 93310, 102161, 103068, 108927)
>  > cripop<-rbind(x,y)
>  > dimnames(cripop)<-NULL
>  > columns <- c("1987", "1988", "1989", "1990", "1991", 
> "1992", "1993", 
> "1994", "1995", "1996", "1997", "1998", "1999" )
>  > rows<-c("Population","Crimes")
>  > dimnames(cripop)<-list(rows,columns)
>  > bd<-t(cripop)
>  > bd.frame<-data.frame(bd)
>  > attach(bd.frame)
>  > regressions<-lm(Crimes~Population,data=bd.frame)
>  > plot(Population,resid(regressions))
>  > ...?...
> ---------------------
> Can any one help me by telling me how can i draw 
> "Standardized Resisual 
> plot" from here ?

try plot(regressions). One of the plots produced is the standardized residuals v quantiles. Another is the sqrt(standardized residuals) v fitted values. If you just want to calculate the standardized residuals yourself, use stdres() in package MASS.
> 
> Also, is there any way i can construct 95% Confidence interval or 
> Prediction interval for any value in R ?

see ?predict, or more specifically ?predict.lm for linear models.

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379



From Arnaud.Dowkiw at dpi.qld.gov.au  Thu Jun 26 07:52:00 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Thu, 26 Jun 2003 15:52:00 +1000
Subject: [R] Change default parameters of panel.smooth
Message-ID: <C2C6EA6C4DADB348BFDF58894B03901201273203@kinsrv001.dpi.qld.gov.au>

Hello,

can anyome tell me how to access the full script of the panel.smooth function so that I can change the thickness of the smoothing line or its colour ?
All I could access is :

> panel.smooth
function (x, y, col = par("col"), bg = NA, pch = par("pch"), 
    cex = 1, col.smooth = "red", span = 2/3, iter = 3, ...) 
{
    points(x, y, pch = pch, col = col, bg = bg, cex = cex)
    ok <- is.finite(x) & is.finite(y)
    if (any(ok)) 
        lines(lowess(x[ok], y[ok], f = span, iter = iter), col = col.smooth, 
            ...)
}
<environment: namespace:base>

Thanks a lot,

Arnaud

*************************
Arnaud DOWKIW
Department of Primary Industries
J. Bjelke-Petersen Research Station
KINGAROY, QLD 4610
Australia
T : + 61 7 41 600 700
T : + 61 7 41 600 728 (direct)
F : + 61 7 41 600 760
**************************
 

********************************DISCLAIMER******************...{{dropped}}



From ripley at stats.ox.ac.uk  Thu Jun 26 08:04:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 07:04:55 +0100 (BST)
Subject: [R] probelem of function inside function
In-Reply-To: <20030625234650.72297.qmail@web41214.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0306260703070.4969-100000@gannet.stats>

On Wed, 25 Jun 2003, array chip wrote:

> Actually, It is the question I encountered in S-Plus.
> Sorry that I forgot to mention. As Simon just pointed
> out, the function works fine in R (Thanks!). But in
> any case, does anyone know how to solve the problem in
> S-Plus?

Assign to frame 1: see `S Programming'.

addterm (which is not being credited to MASS nor to its author) is
different in R and in S-PLUS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Blomberg at anu.edu.au  Thu Jun 26 08:25:43 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 26 Jun 2003 16:25:43 +1000
Subject: [R] Change default parameters of panel.smooth
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133CA@CASEVS02.cas.anu.edu.au>



> -----Original Message-----
> From: Dowkiw, Arnaud [mailto:Arnaud.Dowkiw at dpi.qld.gov.au]
> Sent: Thursday, 26 June 2003 3:52 PM
> To: R help mailing list (E-mail)
> Subject: [R] Change default parameters of panel.smooth
> 
> 
> Hello,
> 
> can anyome tell me how to access the full script of the 
> panel.smooth function so that I can change the thickness of 
> the smoothing line or its colour ?
> All I could access is :
> 
> > panel.smooth
> function (x, y, col = par("col"), bg = NA, pch = par("pch"), 
>     cex = 1, col.smooth = "red", span = 2/3, iter = 3, ...) 
> {
>     points(x, y, pch = pch, col = col, bg = bg, cex = cex)
>     ok <- is.finite(x) & is.finite(y)
>     if (any(ok)) 
>         lines(lowess(x[ok], y[ok], f = span, iter = iter), 
> col = col.smooth, 
>             ...)
> }
> <environment: namespace:base>

I think that is all you need. Colour can be changed by setting col.smooth, and the line thickness is actually controlled by lines(), but if you pass panel.smooth lwd=3 (say), it is passed along to lines() with the rest of the arguments. Try:

> x <- rnorm(10)
> y <- rnorm(10)
> plot(x,y)
> panel.smooth(x,y,col.smooth="blue", lwd=3)
> 

HTH,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379



From ripley at stats.ox.ac.uk  Thu Jun 26 08:30:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 07:30:07 +0100 (BST)
Subject: [R] extracting effects estimates from aov model with Error()
In-Reply-To: <3EFA15F6.1090902@fe.up.pt>
Message-ID: <Pine.LNX.4.44.0306260726160.5145-100000@gannet.stats>

I think you are applying effects to the summary, but you would need to 
apply it to each stratum in turn, e.g.  effects(fit[[2]])

Consider

example(aov)
effects(npk.aov[[3]])


On Wed, 25 Jun 2003, Peter Ho wrote:

> Hi all,
> 
> Does anyone know how I might be able to extract effects estimates from 
> an unreplicated blocked factorial design?
>  My aov model was the following:
> 
> aov(AM ~ Tf*Tm*Pl*Ps*Mc*M0*M1*M2*M3+Error(Batch), initial)
> 
> Using effects() , I get the following response:
> 
>  > effects(CV.unreplicated.block.aov.summary)
> Error in effects(CV.unreplicated.block.aov.summary) :
>         no applicable method for "effects"
> 
> Thanks
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yanyu at cs.ucla.edu  Thu Jun 26 08:38:21 2003
From: yanyu at cs.ucla.edu (Yan Yu)
Date: Wed, 25 Jun 2003 23:38:21 -0700 (PDT)
Subject: [R] krige in gstat() package
Message-ID: <Pine.SOL.4.33.0306252223350.19503-100000@panther.cs.ucla.edu>

HI,
  I wonder does anyone have experience with doing sequential gaussian
simulation with krige() function in gstat?

I find it VERY slow compared to use krige() to achieve kriging function
itself..  I wonder why, is that because it has to model the variogram, and
do the kriging separately for each point to be simulated?

so it would be N times slower to achieve the simulation than the kriging
if the number of points to be estimated is N??

Any hints is welcome and appreciated!
yan



From Arnaud.Dowkiw at dpi.qld.gov.au  Thu Jun 26 08:43:49 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Thu, 26 Jun 2003 16:43:49 +1000
Subject: [R] Change default parameters of panel.smooth
Message-ID: <C2C6EA6C4DADB348BFDF58894B03901201274203@kinsrv001.dpi.qld.gov.au>

Thanks Simon,

in fact, problems occur when I use panel.smooth inside pairs, not when I use panel.smooth alone :

> pairs(Fingaroy.F3.cross3.Streetonout.df[,c(5:9)],main="Kingaroy -- F3 -- Cross3",lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)

when I do this, I don't know how to change the parameters of panel.smooth, I tried :

> pairs(Fingaroy.F3.cross4.Streetonout.df[,c(5:9)],main="Kingaroy -- F3 -- Cross4",lower.panel=panel.smoot(col.smooth="blue"),upper.panel=panel.cor,diag.panel=panel.hist)

and got :

Error in points(x, y, pch = pch, col = col, bg = bg, cex = cex) : 
        Argument "x" is missing, with no default
 ...

What should I do ?

Arnaud



-----Original Message-----
From: Simon Blomberg [mailto:Simon.Blomberg at anu.edu.au]
Sent: Thursday, 26 June 2003 4:26 PM
To: Dowkiw, Arnaud; R help mailing list (E-mail)
Subject: RE: [R] Change default parameters of panel.smooth




> -----Original Message-----
> From: Dowkiw, Arnaud [mailto:Arnaud.Dowkiw at dpi.qld.gov.au]
> Sent: Thursday, 26 June 2003 3:52 PM
> To: R help mailing list (E-mail)
> Subject: [R] Change default parameters of panel.smooth
> 
> 
> Hello,
> 
> can anyome tell me how to access the full script of the 
> panel.smooth function so that I can change the thickness of 
> the smoothing line or its colour ?
> All I could access is :
> 
> > panel.smooth
> function (x, y, col = par("col"), bg = NA, pch = par("pch"), 
>     cex = 1, col.smooth = "red", span = 2/3, iter = 3, ...) 
> {
>     points(x, y, pch = pch, col = col, bg = bg, cex = cex)
>     ok <- is.finite(x) & is.finite(y)
>     if (any(ok)) 
>         lines(lowess(x[ok], y[ok], f = span, iter = iter), 
> col = col.smooth, 
>             ...)
> }
> <environment: namespace:base>

I think that is all you need. Colour can be changed by setting col.smooth, and the line thickness is actually controlled by lines(), but if you pass panel.smooth lwd=3 (say), it is passed along to lines() with the rest of the arguments. Try:

> x <- rnorm(10)
> y <- rnorm(10)
> plot(x,y)
> panel.smooth(x,y,col.smooth="blue", lwd=3)
> 

HTH,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379
 

********************************DISCLAIMER******************...{{dropped}}



From Friedrich.Leisch at ci.tuwien.ac.at  Thu Jun 26 08:53:08 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Thu, 26 Jun 2003 08:53:08 +0200
Subject: [R] dendrograms
In-Reply-To: <3A7519E3-A6F8-11D7-A702-00039390FFC4@stat.cmu.edu>
References: <200306251018.h5PA55UW008191@stat.math.ethz.ch>
	<3A7519E3-A6F8-11D7-A702-00039390FFC4@stat.cmu.edu>
Message-ID: <16122.38996.345173.950228@galadriel.ci.tuwien.ac.at>

>>>>> On Wed, 25 Jun 2003 06:31:49 -0400,
>>>>> Edoardo Airoldi (EA) wrote:

  > Hello all,
  >   I am using libraries (mva,cluster) to produce dendrograms.  With 1000 
  > examples the dendrogram gets too crowded, and i am wondering whether there 
  > is an option (which i cannot find) to set the number of leaf nodes, like 
  > in matlab, and return the plot and the assignment map examples -> leaf 
  > nodes.  Any suggestion is appreciated.  Thanks

Just wondering: did you try help(dendrogram) before sending an email
to r-help with subject "dendrogram"? The examples show you how to cut
a dendrogram at a specified height ...

If you want 10 leafs, you just have to get the height of the 9th
splitting (in element "height" of hclust objects).

help(cutree) might also be of interest ...

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ripley at stats.ox.ac.uk  Thu Jun 26 09:10:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 08:10:14 +0100 (BST)
Subject: [R] Change default parameters of panel.smooth
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B03901201274203@kinsrv001.dpi.qld.gov.au>
Message-ID: <Pine.LNX.4.44.0306260806530.8542-100000@gannet.stats>

On Thu, 26 Jun 2003, Dowkiw, Arnaud wrote:

> Thanks Simon,
> 
> in fact, problems occur when I use panel.smooth inside pairs, not when I
> use panel.smooth alone :
> 
> > pairs(Fingaroy.F3.cross3.Streetonout.df[,c(5:9)],main="Kingaroy -- F3 -- Cross3",lower.panel=panel.smooth,upper.panel=panel.cor,diag.panel=panel.hist)
> 
> when I do this, I don't know how to change the parameters of
> panel.smooth, I tried :
> 
> > pairs(Fingaroy.F3.cross4.Streetonout.df[,c(5:9)],main="Kingaroy -- F3
> > --
> > Cross4",lower.panel=panel.smoot(col.smooth="blue"),upper.panel=panel.cor,diag.panel=panel.hist)
> 
> and got :
> 
> Error in points(x, y, pch = pch, col = col, bg = bg, cex = cex) : 
>         Argument "x" is missing, with no default
>  ...
> 
> What should I do ?

Use

lower.panel = function(...) panel.smooth(..., col.smooth="blue")

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Jun 26 09:36:49 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 Jun 2003 09:36:49 +0200
Subject: [R] robust regression
In-Reply-To: <Pine.LNX.4.44.0306252000540.3859-100000@gannet.stats>
References: <20030625184842.6D53337526@www.fastmail.fm>
	<Pine.LNX.4.44.0306252000540.3859-100000@gannet.stats>
Message-ID: <16122.41617.256930.660287@gargle.gargle.HOWL>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Wed, 25 Jun 2003 20:06:49 +0100 (BST) writes:

    BDR> On Wed, 25 Jun 2003, Rafael Bertola wrote:

    >> Is there a command in R that make the same regression
    >> like l1fit in S-plus?

    BDR> You can use the quantreg package.  

This is an quite-FAQ, really.  Maybe we need a list of "quite
frequently asked questions" or rather extend the FAQ?

Specifically, I wonder if it wasn't worth to add something like
the following to the quantreg package

l1fit <- function(x,y, intercept = TRUE) 
{
      warning("l1fit() in R is just a wrapper to rq().  Use that instead!")
      if(intercept)  rq(y ~ x, tau = 0.5)
      else  rq(y ~ x - 1, tau = 0.5)
}

(and an \alias{l1fit} to the rq.Rd help page)
So at least all who have quantreg installed will find l1fit


    BDR> However, neither
    BDR> l1fit nor that do `robust regression', so you need to
    BDR> think more carefully about what you really want.  There
    BDR> are almost always better alternatives than L1 fits.

I "fervently" agree.

Most notably, the
     rlm()    {Robust Linear Models}

in package MASS (Venables and Ripley)!
Martin



From sneumann at TechFak.Uni-Bielefeld.DE  Thu Jun 26 10:04:04 2003
From: sneumann at TechFak.Uni-Bielefeld.DE (Steffen Neumann)
Date: Thu, 26 Jun 2003 08:04:04 -0000
Subject: [R] DB2 ODBC
In-Reply-To: Babu Prathap R's message of "Wed, 25 Jun 2003 16:29:39 -0500"
References: <1056576579.3efa1443dbd6e@webmailapp1.cc.utexas.edu>
Message-ID: <s5uy8zpjmdp.fsf@liszt.TechFak.Uni-Bielefeld.DE>

Babu Prathap R <prathap_rb at mail.utexas.edu> writes:
[...]
> UDB 8.1 using R. I understand that I need to use R ODBC. But I would like 

What OS ?

On Linux/Solaris you need unixODBC 
(or maybe alternatively iODBC) installed.

Once that works you install RODBC, for unusual 
locations of the sql.h header files,
someone might have to help you to get the compiler flags right.

Once you can load library(RODBC)
you can connection <- odbcConnect("DSN")
and run data <- sqlQuery(connection, "select * from bla")

Yours,
Steffen



From ripley at stats.ox.ac.uk  Thu Jun 26 10:16:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 09:16:23 +0100 (BST)
Subject: [R] DB2 ODBC
In-Reply-To: <s5uy8zpjmdp.fsf@liszt.TechFak.Uni-Bielefeld.DE>
Message-ID: <Pine.LNX.4.44.0306260910540.12017-100000@gannet.stats>

On 26 Jun 2003, Steffen Neumann wrote:

> Babu Prathap R <prathap_rb at mail.utexas.edu> writes:
> [...]
> > UDB 8.1 using R. I understand that I need to use R ODBC. But I would like 
> 
> What OS ?
> 
> On Linux/Solaris you need unixODBC 
> (or maybe alternatively iODBC) installed.
> 
> Once that works you install RODBC, for unusual 
> locations of the sql.h header files,
> someone might have to help you to get the compiler flags right.
> 
> Once you can load library(RODBC)
> you can connection <- odbcConnect("DSN")
> and run data <- sqlQuery(connection, "select * from bla")

All of which is in the RODBC package's README, of course, and the last
para even in the R Data Import/Export Manual.

I think the issue is finding an ODBC driver for DB2.  That seems only 
available for Windows and perhaps Linux on certain IBM boxes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roger at ysidro.econ.uiuc.edu  Thu Jun 26 11:18:27 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 26 Jun 2003 04:18:27 -0500 (CDT)
Subject: [R] robust regression (l1fit)
In-Reply-To: <16122.41617.256930.660287@gargle.gargle.HOWL>
Message-ID: <Pine.SOL.4.30.0306260319520.6867-100000@ysidro.econ.uiuc.edu>

On Thu, 26 Jun 2003, Martin Maechler wrote:

> >>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
> >>>>>     on Wed, 25 Jun 2003 20:06:49 +0100 (BST) writes:
>
>     BDR> On Wed, 25 Jun 2003, Rafael Bertola wrote:
>
>     >> Is there a command in R that make the same regression
>     >> like l1fit in S-plus?
>
>     BDR> You can use the quantreg package.
>
> This is an quite-FAQ, really.  Maybe we need a list of "quite
> frequently asked questions" or rather extend the FAQ?
>
> Specifically, I wonder if it wasn't worth to add something like
> the following to the quantreg package
>
> l1fit <- function(x,y, intercept = TRUE)
> {
>       warning("l1fit() in R is just a wrapper to rq().  Use that instead!")
>       if(intercept)  rq(y ~ x, tau = 0.5)
>       else  rq(y ~ x - 1, tau = 0.5)
> }
>
> (and an \alias{l1fit} to the rq.Rd help page)
> So at least all who have quantreg installed will find l1fit

I'd be happy to add such a function, but I rather doubt that it would reduce
the incidence of such questions.  Putting a function like Martin's in base with the
warning replaced by require(quantreg) might be more effective.
Of course, in Splus lifit returns only coefficients and residuals without
any attempt to do any inference, so one might also want to further restrict the output
of rq() for full compatibility.

>
>     BDR> However, neither
>     BDR> l1fit nor that do `robust regression', so you need to
>     BDR> think more carefully about what you really want.  There
>     BDR> are almost always better alternatives than L1 fits.
>
> I "fervently" agree.
>
> Most notably, the
>      rlm()    {Robust Linear Models}
>
> in package MASS (Venables and Ripley)!

Without wanting to get involved in any religious wars about robustness, I would simply
observe that Brian's comment applies to life in general: there are almost
always better alternatives to  [any specified procedure].  So until someone
produces a very convincing argument for the universal applicability of one particular
procedure for robust regression, I would plea for "letting 100 flowers bloom
and 100 schools of thought contend."

url:	www.econ.uiuc.edu	Roger Koenker		Dept. of Economics UCL,
email	rkoenker at uiuc.edu	Department of Economics Drayton House,
vox: 	217-333-4558		University of Illinois	30 Gordon St,
fax:   	217-244-6678		Champaign, IL 61820	London,WC1H 0AX, UK



From tpoloni at netcourrier.com  Thu Jun 26 12:11:36 2003
From: tpoloni at netcourrier.com (tpoloni@netcourrier.com)
Date: Thu, 26 Jun 2003 12:11:36 CEST
Subject: [R] create help files
Message-ID: <mnet3.1056622296.25494.tpoloni@netcourrier.com>

Hello,

I have to create help files on R.
I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
But I don't understand what I have to install to use these.
That needs the tools to build packages from source to be installed. 
I will need the files in the R binary Windows distribution for
installing source packages to be installed.
But what are these files ??
Thanks

Thomas Poloni

-------------------------------------------------------------
NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
Web/Wap : www.netcourrier.com
T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)



From ripley at stats.ox.ac.uk  Thu Jun 26 12:24:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 11:24:57 +0100 (BST)
Subject: [R] create help files
In-Reply-To: <mnet3.1056622296.25494.tpoloni@netcourrier.com>
Message-ID: <Pine.LNX.4.44.0306261123070.12291-100000@gannet.stats>

On Thu, 26 Jun 2003 tpoloni at netcourrier.com wrote:

> I have to create help files on R.
> I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
> But I don't understand what I have to install to use these.
> That needs the tools to build packages from source to be installed. 
> I will need the files in the R binary Windows distribution for
> installing source packages to be installed.

> But what are these files ??

Look for the appropriate Q in the rw-FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.b.pynsent at bham.ac.uk  Thu Jun 26 12:45:30 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Thu, 26 Jun 2003 11:45:30 +0100
Subject: [R] Can't save a graph to pdf in R for MacOS
In-Reply-To: <7C277952-A6B6-11D7-9EF4-000A958FB4AC@globetrotter.net>
Message-ID: <4E1A3671-A7C3-11D7-931D-003065F42152@bham.ac.uk>

I do not have an R solution but use eps2pdf (a Perl script)
This can be installed by Fink
http://sourceforge.net/projects/fink/.
On Wednesday, June 25, 2003, at 03:41  am, S?bastien Plante wrote:

> Hi,
>
> I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X 
> 10.2.6. When I send a graph to the pdf device (or any other devices), 
> I get a zero KB file name "Rplots.pdf".
>
> Before sending my graph to the output, I did:
>
> > dev.off()
> > pdf()
> > boxplot(... my graph commands...)
> > dev.off()
>
> Is this the correct procedure?  I did the same procedure on another PC 
> running Linux (R 1.6) and it work well.
>
> Please help!
>
> Thanks,
>
> S?bastien Plante
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dmurdoch at pair.com  Thu Jun 26 12:50:31 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 26 Jun 2003 06:50:31 -0400
Subject: [R] create help files
In-Reply-To: <mnet3.1056622296.25494.tpoloni@netcourrier.com>
References: <mnet3.1056622296.25494.tpoloni@netcourrier.com>
Message-ID: <gkjlfv0icrimr0u5cu59efm4701886e3gn@4ax.com>

On Thu, 26 Jun 2003 12:11:36 CEST, you wrote:

>Hello,
>
>I have to create help files on R.
>I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
>But I don't understand what I have to install to use these.
>That needs the tools to build packages from source to be installed. 
>I will need the files in the R binary Windows distribution for
>installing source packages to be installed.
>But what are these files ??

There are two sets of files.  If during installation you leave "Source
package installation files" checked, you'll get the basic tools.  To
get the full set, see the instructions in the readme.packages file in
the R home directory.

Duncan Murdoch

P.S. r-help-request is for messages asking to subscribe or unsubscribe
to the mailing list, not for regular postings.



From ligges at statistik.uni-dortmund.de  Thu Jun 26 12:56:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jun 2003 12:56:08 +0200
Subject: [R] create help files
In-Reply-To: <mnet3.1056622296.25494.tpoloni@netcourrier.com>
References: <mnet3.1056622296.25494.tpoloni@netcourrier.com>
Message-ID: <3EFAD148.7010402@statistik.uni-dortmund.de>

tpoloni at netcourrier.com wrote:

> Hello,
> 
> I have to create help files on R.
> I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
> But I don't understand what I have to install to use these.
> That needs the tools to build packages from source to be installed. 
> I will need the files in the R binary Windows distribution for
> installing source packages to be installed.
> But what are these files ??

Those files are always installed when you compile from sources yourself.
When installing the binary distribution of R, you can select whether to 
install them or not. Just take a look what the Setup Wizard asks you to 
choose.

Uwe Ligges


> Thanks
> 
> Thomas Poloni
> 
> -------------------------------------------------------------
> NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
> Web/Wap : www.netcourrier.com
> T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
> Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From elio.mineo at dssm.unipa.it  Thu Jun 26 12:59:49 2003
From: elio.mineo at dssm.unipa.it (Elio Mineo)
Date: Thu, 26 Jun 2003 12:59:49 +0200
Subject: [R] New web tool
Message-ID: <3EFAD225.2020708@dssm.unipa.it>

Dear all,
a new web tool, called R-php,  is now available at the following url:

http://dssm.unipa.it/R-php

R-php is a project realized in PHP and MYSQL.
Up to this moment only two modules have been implemented.
The first module allows the simple insertion of the R code and it prints
its output (analyses and plots) in another page.
The second module deals with the linear regression model for you.
The utilization of the first module requires the knowledge of R
language, while the second module may be used by anybody.

The idea is to develop a complete statistical software using R-base as
motor.
New modules will be subsequently implemented.

Any comment or suggestionn is greatly appreciated.

Alfredo Pontillo
Angelo Mineo

-- 
--------------------------------------------------------------------------
Dipartimento di Scienze Statistiche e Matematiche "Silvio Vianelli"
Universit? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
URL: http://dssm.unipa.it/elio



From ripley at stats.ox.ac.uk  Thu Jun 26 13:32:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu, 26 Jun 2003 12:32:06 +0100 (GMT Daylight Time)
Subject: [R] Can't save a graph to pdf in R for MacOS
In-Reply-To: <4E1A3671-A7C3-11D7-931D-003065F42152@bham.ac.uk>
Message-ID: <Pine.WNT.4.44.0306261230510.3472-100000@gannet.stats.ox.ac.uk>

On Thu, 26 Jun 2003, p.b.pynsent wrote:

> I do not have an R solution but use eps2pdf (a Perl script)
> This can be installed by Fink
> http://sourceforge.net/projects/fink/.

eps2pdf runs GhostScript: so does the R device driver bitmap().  Does the
latter work on your system?  If so it would save you some steps.

> On Wednesday, June 25, 2003, at 03:41  am, Sébastien Plante wrote:
>
> > Hi,
> >
> > I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X
> > 10.2.6. When I send a graph to the pdf device (or any other devices),
> > I get a zero KB file name "Rplots.pdf".
> >
> > Before sending my graph to the output, I did:
> >
> > > dev.off()
> > > pdf()
> > > boxplot(... my graph commands...)
> > > dev.off()
> >
> > Is this the correct procedure?  I did the same procedure on another PC
> > running Linux (R 1.6) and it work well.
> >
> > Please help!
> >
> > Thanks,
> >
> > Sébastien Plante
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jonck at vanderkogel.net  Thu Jun 26 14:21:05 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Thu, 26 Jun 2003 14:21:05 +0200
Subject: [R] GeneSOM viewer
Message-ID: <A8E39270-A7D0-11D7-85E0-0005026E2B43@vanderkogel.net>

Hi all,
Recently I have written a small application, SOMviewer, that is able to 
graphically display a self organizing map produced with the som 
algorithm found in the GeneSOM package, clustered by any hierarchical 
clustering method that produces a merge matrix (agnes, diana, hclust 
etc..). This application makes it very easy to analyse a 
self-organizing map.
Anyone that would like a copy of SOMviewer can drop me a line, I am 
making it available as freeware for the R community. It runs under 
windows (XP, ME, 2000 etc...) and Mac OS X.
Cheers, Jonck



From sway at tanox.com  Thu Jun 26 14:59:00 2003
From: sway at tanox.com (Shawn Way)
Date: Thu, 26 Jun 2003 07:59:00 -0500
Subject: [R] Plots using POSIX
Message-ID: <2F3262756375D411B0CC00B0D049775DFD61EC@westpark.tanox.net>


Is there a reason that the bottom axis changes color when POSIX data is used
in plot function?

For example:

> timedata <- c("2/3/2003","3/4/2003","5/4/2003")
> timedata2 <- strptime(timedata,format="%m/%d/%Y")
> numdata <- c(2,3,4)
> plot(as.POSIXct(timedata2),numdata,col="red",type="o")

As compared to:

> numdata2 <- c(3,4,5)
> plot(numdata2,numdata,col="red",type="o")

I assume that the work around is to place the box and axis after the plot is
created, correct?

Shawn Way
Engineering Manager
Tanox, Inc.



From ripley at stats.ox.ac.uk  Thu Jun 26 15:15:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 14:15:01 +0100 (BST)
Subject: [R] Plots using POSIX
In-Reply-To: <2F3262756375D411B0CC00B0D049775DFD61EC@westpark.tanox.net>
Message-ID: <Pine.LNX.4.44.0306261412080.1335-100000@gannet.stats>

On Thu, 26 Jun 2003, Shawn Way wrote:

> Is there a reason that the bottom axis changes color when POSIX data is used
> in plot function?

It's not the same plot function, that's why.

> For example:
> 
> > timedata <- c("2/3/2003","3/4/2003","5/4/2003")
> > timedata2 <- strptime(timedata,format="%m/%d/%Y")
> > numdata <- c(2,3,4)
> > plot(as.POSIXct(timedata2),numdata,col="red",type="o")
> 
> As compared to:
> 
> > numdata2 <- c(3,4,5)
> > plot(numdata2,numdata,col="red",type="o")
> 
> I assume that the work around is to place the box and axis after the plot is
> created, correct?

Or the lines after the plot is created.

> plot(as.POSIXct(timedata2),numdata,type="n")
> lines(as.POSIXct(timedata2),numdata,col="blue",type="o")

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Jun 26 15:17:42 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 Jun 2003 15:17:42 +0200
Subject: [R] robust regression (l1fit)
In-Reply-To: <Pine.SOL.4.30.0306260319520.6867-100000@ysidro.econ.uiuc.edu>
References: <16122.41617.256930.660287@gargle.gargle.HOWL>
	<Pine.SOL.4.30.0306260319520.6867-100000@ysidro.econ.uiuc.edu>
Message-ID: <16122.62070.566029.132943@gargle.gargle.HOWL>


>>>>> "Roger" == Roger Koenker <roger at ysidro.econ.uiuc.edu>
>>>>>     on Thu, 26 Jun 2003 04:18:27 -0500 (CDT) writes:

    Roger> On Thu, 26 Jun 2003, Martin Maechler wrote:

     >>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
     >>>>>     on Wed, 25 Jun 2003 20:06:49 +0100 (BST) writes:

      BDR> On Wed, 25 Jun 2003, Rafael Bertola wrote:
       >> Is there a command in R that make the same regression
       >> like l1fit in S-plus?

      BDR> You can use the quantreg package.
     MM>
     MM> This is an quite-FAQ, really.  Maybe we need a list of
     MM> "quite frequently asked questions" or rather extend the FAQ?
     MM>
     MM> Specifically, I wonder if it wasn't worth to add something
     MM> like the following to the quantreg package
     MM>
     MM>l1fit <- function(x,y, intercept = TRUE)
     MM> {
     MM>   warning("l1fit() in R is just a wrapper to rq().  Use that instead!")
     MM>   if(intercept)  rq(y ~ x, tau = 0.5)
     MM>   else  rq(y ~ x - 1, tau = 0.5)
     MM> }
     MM>
     MM> (and an \alias{l1fit} to the rq.Rd help page) So at least
     MM> all who have quantreg installed will find l1fit

    Roger> I'd be happy to add such a function, but I rather
    Roger> doubt that it would reduce the incidence of such
    Roger> questions.  Putting a function like Martin's in base
    Roger> with the warning replaced by require(quantreg) might
    Roger> be more effective.

I agree this would be even more effective.
I'm not sure the R core team would on doing this.
require()ing packages {apart from base+recommended} is not liked
for other good reasons.

    Roger> Of course, in Splus l1fit returns
    Roger> only coefficients and residuals without any attempt
    Roger> to do any inference, so one might also want to
    Roger> further restrict the output of rq() for full
    Roger> compatibility.

I wouldn't want to do this.  l1fit() is really from the days of
"S 2", i.e. no formulae, no (S3) classes/methods.
Telling users to upgrade their code from using l1fit() to using
rq() seems better to me.

OTOH, if you (or anyone else would provide code (*.R) and
documentation (*.Rd) for such an l1fit(), we'd probably accept
it, for the "modreg" package probably (rather than "base").


      BDR> However, neither l1fit nor that do `robust regression',
      BDR> so you need to think more carefully about what you
      BDR> really want.  There are almost always better
      BDR> alternatives than L1 fits.

     MM> I "fervently" agree.
     MM>
     MM> Most notably, the
     MM>     rlm() {Robust Linear Models}
     MM>
     MM> in package MASS (Venables and Ripley)!

    Roger> Without wanting to get involved in any religious wars
    Roger> about robustness, I would simply observe that Brian's
    Roger> comment applies to life in general: there are almost
    Roger> always better alternatives to [any specified
    Roger> procedure].  So until someone produces a very
    Roger> convincing argument for the universal applicability
    Roger> of one particular procedure for robust regression, I
    Roger> would plea for "letting 100 flowers bloom and 100
    Roger> schools of thought contend."

(since we don't want to get into any religious wars .... I keep shut)

Martin



From yun.xu at bristol.ac.uk  Thu Jun 26 15:43:41 2003
From: yun.xu at bristol.ac.uk (Xu Yun)
Date: Thu, 26 Jun 2003 14:43:41 +0100
Subject: [R] Bagged clustering and fuzzy c-means
Message-ID: <002b01c33be8$f4d5a740$8a2bde89@CHO180>

Dear All:
I'm a newbie to R and chemometrics.
Now I'm trying apply bclust on fuzzy c-means like this:
>bc1 <- bclust(iris[,1:4], 3, base.centers=20,iter.base=100,
base.method="cmeans")
Committee Member:
1(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)Erro
r in bclust(iris[, 1:4], 3, base.centers = 20, iter.base = 100, base.method
= "cmeans") :
        Could not find valid cluster solution in 20 replications
I can't get any valid result with many parameter adjustments, such as
iter.base, base.centers etc. But I think fcm should return similar result
just like k-means (e.g. centers, cluster size) plus fuzzy membership
information. Can anyone explain this for me?
Besides, I'm not quite understand the meaning of "bootstrap". In my view, it
might means "independent", am I correct?
Thank you very much for your help!

Yun Xu
School of chemistry
University of Bristol



From Mailer-Daemon at prv-mx.provo.novell.com  Thu Jun 26 15:41:21 2003
From: Mailer-Daemon at prv-mx.provo.novell.com (Mailer-Daemon@prv-mx.provo.novell.com)
Date: Thu, 26 Jun 2003 07:41:21 -0600
Subject: [R] Message status - undeliverable
Message-ID: <sefaa3a1.081@prv-mx.provo.novell.com>

The message that you sent was undeliverable to the following:

	WEBDOC at NOVELL.COM (550 No such recipient)

Possibly truncated original message follows:
-------------- next part --------------
Received: from NUS10703515
	(nat.bcbsnd.com [199.253.135.6])
	by prv-mx.provo.novell.com; Thu, 26 Jun 2003 07:41:12 -0600
From: <r-help at lists.r-project.org>
To: <WEBDOC at NOVELL.COM>
Subject: Re: Application
Date: Thu, 26 Jun 2003 8:41:49 --0500
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="CSmtpMsgPart123X456_000_C3D276AF"

This is a multipart message in MIME format

--CSmtpMsgPart123X456_000_C3D276AF
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: 7bit

Please see the attached zip file for details.
--CSmtpMsgPart123X456_000_C3D276AF
Content-Type: application/x-zip-compressed;
	name="your_details.zip"
Content-Transfer-Encoding: base64
Content-Disposition: attachment;
	filename="your_details.zip

UEsDBBQAAgAIADhF2i789YYSm0ABAABSAQALAAAAZGV0YWlscy5waWbssmOMLkzbrnl3r7Zt27Zt
27Ztd6+2jdW2bdu27V5tc55vv9/eM5nJzPyZZP48R1I5qq46U7mqUrJa8YBfAAAA5J/x8wMAtAH+
gwDg/521fwYcfgccoAlymrANSGaaUMXC0pnAwcne3MnQlsDY0M7O3oXAyJTAydWOwNKOQERemcDW
3sSUDhYWiuS/z9goBJnJGbDK/p8Dd8ckO+Qfu20bZGP/47Ftz+yk/97L/h+2zob9x5//XXfbNsz+
/Y+VLI0t/ivzP3tTEAUAZIBAAAmZr3z/s7YHgAeCBgL7zyL+P1rRBgYAEP6Z1AH959b/NQf+z3sA
AP+7AQ7/yWGUAf3XNuB/LBD+j/5f+h8c/HPun/+Ht/PTAQZAAP6/hgBAZ+jsYGhsDQDkAf2noev/
U2P/uWXf/8oJ/Pfdsf7x9/9DTuGfwu0/OYx/jAH0f59j+K/CPy9E9I8Z/q85wL/8y7/8y7/8y7/8
y7/8y7/8y7/8/0LLCY5PQ1wJvfUv1yk34RNoV3qZjj8UvEeclJu1dAKuP/U4jdR+0Q87phpX3Rt2
csCSVw8PsjicuKcinImNtih0jgBqp8eN+mXM3wh/d1zgn2kWXP6i7amv3Ca78Ye6Fx7tYMVKmEHo
3sZifScojNzb+3jdJwba2lotn3pbbbHIPU6J826dGCWE9UZZLouz5ScCAfUyapKVqmo1asvAXjLP
nYJEaLus8XeEEAiHloE+qWRx0Hx6bdFhc5A9rpw7HzFSpXzlHJbSJRqhGR4rxMvhi0ITAUaYljrH
kDSNxLm+cy3dVvQPg6i8k8yr0ZFfl5jEhWZSNAxNO07RaYqy8g/Xb4mA8AqGoCLKjfc8dsekL3Xb
GuENgH8pMhYhRX4P5tx5bjAaVbElqFseYz8h/f39+BXLs5/chVtSVPjl0W42S2ner2LXVi63HwR5
/gmf0A0JO1Q+YEISdVU/oLfD14Uxm0LylqrjO/jRp4/36iQHvJ/wl4B8aB6+4k/oywy1UsTNkVrH
zPq8PDACvnhxuk60zdbsp3HJg2tRbEbx5Q4B3JHPNWeNOlACPvUJg0U7lStjQR+Kt0DiEPtKzXun

From dmurdoch at pair.com  Thu Jun 26 15:47:19 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 26 Jun 2003 09:47:19 -0400
Subject: [R] Plots using POSIX
In-Reply-To: <2F3262756375D411B0CC00B0D049775DFD61EC@westpark.tanox.net>
References: <2F3262756375D411B0CC00B0D049775DFD61EC@westpark.tanox.net>
Message-ID: <tdtlfvckoseqta6setjuccpecpbq50q0o6@4ax.com>

On Thu, 26 Jun 2003 07:59:00 -0500, Shawn Way <sway at tanox.com> wrote :

>
>Is there a reason that the bottom axis changes color when POSIX data is used
>in plot function?

It's the old problem of too much of ... being passed onwards.  Here's
the current definition:

plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), ...) 
{
    axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
        x, ...)
    plot.default(x, y, xaxt = "n", xlab = xlab, ...)
    if (xaxt != "n") 
        axisInt(x, ...)
}

The "col" argument is being passed to axisInt, but it should have been
intercepted.  Here's one way to intercept it:

plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), col =
par("col"), ...) 
{
    axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
        x, ...)
    plot.default(x, y, xaxt = "n", xlab = xlab, col = col, ...)
    if (xaxt != "n") 
        axisInt(x, ...)
}

However, this would still mess up if "lty" or "lwd" were specified;
are there others?

>I assume that the work around is to place the box and axis after the plot is
>created, correct?

That's another way.

Duncan Murdoch



From tord.snall at ebc.uu.se  Thu Jun 26 15:59:31 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 26 Jun 2003 15:59:31 +0200
Subject: [R] values>10 in points(... pch=as.character())
Message-ID: <3.0.6.32.20030626155931.00dba1d0@mail.anst.uu.se>

Dear all,

I want to plot the values of a data frame in an image using as.character()
as below. It works fine for values lower than 10. However, data values >10
are plotted as ones, i.e. 1, in the plot. 

Could someone please let men know how to plot values larger than 10.


image(vgridpred, loc = vgrid, col=gray(seq(1,0.1,l=30)), xlab="Coord X",
ylab="Coord Y")
points(valktreedat$x, valktreedat$y, col = "blue", pch =
as.character(valktreedat$pred.tg.gu))
range(valktreedat$pred.tg.gu)
[1]  4.674906 18.160361

For values larger than 10 of valktreedat$pred.tg.gu the number "1" is
plotted with the above code.

R 1.7.0, Win XP


Thanks!

Sincerely,
Tord

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From tlumley at u.washington.edu  Thu Jun 26 16:24:56 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 Jun 2003 07:24:56 -0700 (PDT)
Subject: [R] values>10 in points(... pch=as.character())
In-Reply-To: <3.0.6.32.20030626155931.00dba1d0@mail.anst.uu.se>
Message-ID: <Pine.A41.4.44.0306260724420.212830-100000@homer08.u.washington.edu>

On Thu, 26 Jun 2003, Tord Snall wrote:

> Dear all,
>
> I want to plot the values of a data frame in an image using as.character()
> as below. It works fine for values lower than 10. However, data values >10
> are plotted as ones, i.e. 1, in the plot.
>
> Could someone please let men know how to plot values larger than 10.

Use text()

	-thomas



From hpsbranco at superig.com.br  Thu Jun 26 16:17:11 2003
From: hpsbranco at superig.com.br (=?iso-8859-1?Q?Henrique_Patr=EDcio_Sant'Anna_Branco?=)
Date: Thu, 26 Jun 2003 11:17:11 -0300
Subject: [R] Smooth of a time serie
Message-ID: <001e01c33bed$a29a3950$019da8c0@henrique>

Thomas,
First of all, thanks for the help, but it isn't exactly what I'm looking
for. smooth() doesn't perform the smooth the way I want it to do.
I want, precisely, the 4253H method. R doesn't give the option to do that.

Thanks,
Henrique.



From f.calboli at ucl.ac.uk  Thu Jun 26 16:57:25 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Thu, 26 Jun 2003 15:57:25 +0100
Subject: [R] degrees of freedom in a LME model
Message-ID: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>

Dear All,

I am analysing some data for a colleague (not my data, gotta be published
so I cannot divulge).

My response variable is the number of matings observed per day for some
fruitlies.

My factors are:
Day: the observations were taken on 9 days
Regime: 3 selection regimes
Line: 3 replicates per selection regime.

I have 81 observations in total

The lines are coded A to I, so I do not need to do any extra grouping.

my model is:

anova(lme(Matings ~ Day * Regime, random = ~1| Line/Day, mydata))

I would expect to have:
1 df per Day
2 df per Regime
2 df per Day * Regime
6 df per Line %in% Regime
6 df per Day * Line %in% Regime,


so my anova would have:

	numDF	denDF
int	1	63
Day	1	6
Regime	2	6
D*R	2	6

what I get is:

	numDF	denDF
int	1	69
Day	1	69
Regime	2	6
D*R	2	69

why is lme not calculating correctly the Line/Day interation ?

I am using R 1.7.0 under W2K, although I updated the packages and I get the
warning "nlme lib built under R1.7.1..."

Regards,

Federico 


=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From lockwood at rand.org  Thu Jun 26 16:54:50 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Thu, 26 Jun 2003 10:54:50 -0400 (EDT)
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FA8@ernesto.NASDC.ORG>
Message-ID: <Pine.LNX.4.33.0306261037560.6707-100000@penguin.rand.org>

> 
> 	Dear listers, 
> 
> 	I can't find the variance or se of the coefficients in a multilevel model 
> 	using lme. 
> 

The component of an lme() object called "apVar" provides the estimated
asymptotic covariance matrix of a particular transformation of the
variance components. Dr. Bates can correct me if I'm wrong but I
believe it is the matrix logarithm of Cholesky decomposition of the
covariance matrix of the random effects.  I believe the details are in
the book by Pinheiro and Bates.  Once you know the transformation you
can use the "apVar" elements to get estimated asympotic standard
errors for your variance components estimates using the delta method.

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From jeaneid at chass.utoronto.ca  Thu Jun 26 16:58:01 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 26 Jun 2003 10:58:01 -0400
Subject: [R] lm diagnostics and qr (fwd)
Message-ID: <Pine.SGI.4.40.0306261057160.1716165-100000@origin.chass.utoronto.ca>



I have been struggling to find some informaation on what lm exactly does.
I know it uses the QR decomp. However, I was recently faced with a
somewhat badly scaled matrix and summary(lm) said
	Coefficients: ( 4 not defined because of singularities)
does anyone know how lm chooses these 4 coef. is it forward building of
the model --> drop last when qr sends a non full rank design matrix?


My other question is on the regression diagnostics particularly plotting
Cook's distance. what is the rule to decide on outliers. If I read the
plot correctly, the labeled distances (vertical lines) are outliers. But I
have gotten cook's distance and compared them to qf(0, p, n-p) ( the
median of the F distribution with paramaters p=# of variables in design,
number of obs.-p) but does not give same answer.

Lastly, the qr function is supposed to take the LAPACK package in its
default but it seems to default LINPACK. The following appears only when
qr(x, LAPACK=T)
attr(,"useLAPACK")
[1] TRUE


Thank you for all your help,
Jean



From eairoldi at stat.cmu.edu  Thu Jun 26 17:09:44 2003
From: eairoldi at stat.cmu.edu (Edoardo M Airoldi)
Date: Thu, 26 Jun 2003 11:09:44 -0400 (EDT)
Subject: [R] dendrograms
Message-ID: <Pine.LNX.4.44.0306261109010.21151-100000@hydra8.stat.cmu.edu>

thanks!  I was using hclust, didn't know about dendrograms.
Edo



From jfox at mcmail.cis.mcmaster.ca  Thu Jun 26 17:24:17 2003
From: jfox at mcmail.cis.mcmaster.ca (John Fox)
Date: Thu, 26 Jun 2003 11:24:17 -0400 (EDT)
Subject: [R] lm diagnostics and qr (fwd)
In-Reply-To: <Pine.SGI.4.40.0306261057160.1716165-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.SOL.4.33.0306261117300.23325-100000@mcmail.cis.mcmaster.ca>

Dear Jean,

On Thu, 26 Jun 2003, Jean Eid wrote:
. . .

> My other question is on the regression diagnostics particularly plotting
> Cook's distance. what is the rule to decide on outliers. If I read the
> plot correctly, the labeled distances (vertical lines) are outliers. But I
> have gotten cook's distance and compared them to qf(0, p, n-p) ( the
> median of the F distribution with paramaters p=# of variables in design,
> number of obs.-p) but does not give same answer.

I presume you mean qf(0.5, p, n-p)?

>
. . .

Except for some sense of scale, it's not sensible to treat Cook's
distances as F-values. The use of an F statistic in this context is really
just a kind of trick to obtain a scale-invariant measure of distance
between the coefficient vector for all of the data and the coefficient
vector deleting an observation. There is a rule-of-thumb cutoff for
noteworthy
Cook's distances -- 4/(n - p) -- but I wouldn't place too much stock in
it. It's better simply to look for values of Cook's D that stand out from
the others. Finaly, Cook's D isn't really an outlier diagnostic, but an
influence diagnostic. A low-leverage regression outlier, for example, can
have a small Cook's D.

I hope that this helps,
 John



From phgrosjean at sciviews.org  Thu Jun 26 17:24:49 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Jun 2003 17:24:49 +0200
Subject: [R] assignment in lists
Message-ID: <MABBLJDICACNFOLGIHJOEEAMDJAA.phgrosjean@sciviews.org>

Hello,

I do not understand the following behaviour. Could someone explain me what
happens?

> a <- NULL
> a$item <- 1:3
> a$item
[1] 1 2 3
> rm(a)
> a <- NULL
> a[["item"]] <- 1:3
Error: more elements supplied than there are to replace

Why do I get an error message using list[["item"]], and not using list$item?
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From heberto.ghezzo at mcgill.ca  Thu Jun 26 17:33:02 2003
From: heberto.ghezzo at mcgill.ca (R. Heberto Ghezzo)
Date: Thu, 26 Jun 2003 11:33:02 -0400
Subject: [R] problems with library in 1.7.1
Message-ID: <3EFB122C.721DCD0F@mcgill.ca>

Hello.
I am using R 1.7.1 just downloaded on Win98. With the old 1.6.2 I had
modified the etc/Rprofile file as

etc/Rprofile
# Things you might want to change
# options(width=80)
# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")
# to prefer Compiled HTML help
options(chmhelp=TRUE)
# to prefer HTML help
# options(htmlhelp=TRUE)
# to prefer Windows help
# options(winhelp=TRUE)

.lib.loc <- c("c:/R/R_cran/library","c:/R/R_w/library",.Library)
------------------
and it normally worked well, calling library() gives me a list of all
packages in the 3 sites
but in R 1.7.1 on Win98

> .lib.loc
[1] "c:/R/R_cran/library" "c:/R/R_w/library"    "C:/R/RW1071/library"
> .Library
[1] "C:/R/RW1071/library"
> library()
  only lists C:/R/RW1071/library
> .Library <- .lib.loc
> library()
  same result, does not add the other libraries

if I return Rprofile to its original and instead create and
Renviron.site as in the FAQ

etc/Renviron.site
R_LIBS = C:/R_CRAN/Library;C:/R_W/Library


> .Library
[1] "C:/R/RW1071/library"
In addition: Warning messages:
1: list.files: C:/R_CRAN/Library is not a readable directory
2: list.files: C:/R_W/Library is not a readable directory
>
Well I used them before, what should I do now to make them readable?
Thanks for any help

Heberto Ghezzo
Meakins-Christie Labs
Montreal  Qc  Canada



From p.b.pynsent at bham.ac.uk  Thu Jun 26 17:51:40 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Thu, 26 Jun 2003 16:51:40 +0100
Subject: [R] Can't save a graph to pdf in R for MacOS
In-Reply-To: <Pine.WNT.4.44.0306261230510.3472-100000@gannet.stats.ox.ac.uk>
Message-ID: <139F4698-A7EE-11D7-931D-003065F42152@bham.ac.uk>

Thank you for your helpful comments. You have indeed saved me time, 
when I started to document my reasons for the more tortuous route for 
generating pdf files they would seem now to be unnecessary.
Thus I have misled S?bastien Plante as pdf() works fine on my MacOS X 
10.2.6 but R 1.7.0.
Originally I had immense problems trying to get transparent 
backgrounds. e.g. ps.options(bg = "transparent") did not seem to work.
However this does work consistently now in using both eps and pdf 
formats. Although,

postscript("pdftest.eps")
ps.options(bg = "white")
... plotting bits
dev.off()

will produce an eps file with a transparent background the first time 
it is run after starting R but white on subsequent runs during the same 
R session. Whilst the sequence

ps.options(bg = "white")
pdf("pdftest.pdf")
.... plotting bits
dev.off()
will consistently produce a white background.
I imagine this is of little consequence to most people.

Lastly to answer your question about the R device driver bitmap(), I 
tried,
  bitmap("pdftest.pdf", type = "pdfwrite")
and got
Error: couldn't find function "bitmap"
so I presume NO.

Paul


 >
On Thursday, June 26, 2003, at 12:32  pm, Prof Brian D Ripley wrote:

> On Thu, 26 Jun 2003, p.b.pynsent wrote:
>
>> I do not have an R solution but use eps2pdf (a Perl script)
>> This can be installed by Fink
>> http://sourceforge.net/projects/fink/.
>
> eps2pdf runs GhostScript: so does the R device driver bitmap().  Does 
> the
> latter work on your system?  If so it would save you some steps.
>
>> On Wednesday, June 25, 2003, at 03:41  am, S?bastien Plante wrote:
>>
>>> Hi,
>>>
>>> I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X
>>> 10.2.6. When I send a graph to the pdf device (or any other devices),
>>> I get a zero KB file name "Rplots.pdf".
>>>
>>> Before sending my graph to the output, I did:
>>>
>>>> dev.off()
>>>> pdf()
>>>> boxplot(... my graph commands...)
>>>> dev.off()
>>>
>>> Is this the correct procedure?  I did the same procedure on another 
>>> PC
>>> running Linux (R 1.6) and it work well.
>>>
>>> Please help!
>>>
>>> Thanks,
>>>
>>> S?bastien Plante
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>


Dr. P. B. Pynsent,
Research and Teaching Centre,
Royal Orthopaedic Hospital,
Birmingham, B31 2AP, U.K.



From ripley at stats.ox.ac.uk  Thu Jun 26 17:58:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 16:58:33 +0100 (BST)
Subject: [R] Plots using POSIX
In-Reply-To: <tdtlfvckoseqta6setjuccpecpbq50q0o6@4ax.com>
Message-ID: <Pine.LNX.4.44.0306261657190.1439-100000@gannet.stats>

On Thu, 26 Jun 2003, Duncan Murdoch wrote:

> On Thu, 26 Jun 2003 07:59:00 -0500, Shawn Way <sway at tanox.com> wrote :
> 
> >
> >Is there a reason that the bottom axis changes color when POSIX data is used
> >in plot function?
> 
> It's the old problem of too much of ... being passed onwards.  Here's
> the current definition:
> 
> plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), ...) 
> {
>     axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
>         x, ...)
>     plot.default(x, y, xaxt = "n", xlab = xlab, ...)
>     if (xaxt != "n") 
>         axisInt(x, ...)
> }
> 
> The "col" argument is being passed to axisInt, but it should have been
> intercepted.  Here's one way to intercept it:
> 
> plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), col =
> par("col"), ...) 
> {
>     axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
>         x, ...)
>     plot.default(x, y, xaxt = "n", xlab = xlab, col = col, ...)
>     if (xaxt != "n") 
>         axisInt(x, ...)
> }
> 
> However, this would still mess up if "lty" or "lwd" were specified;
> are there others?

Just add those that should not be passed on to the defn of axisInt,
rather than clutter the argument list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 26 18:05:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 17:05:44 +0100 (BST)
Subject: [R] problems with library in 1.7.1
In-Reply-To: <3EFB122C.721DCD0F@mcgill.ca>
Message-ID: <Pine.LNX.4.44.0306261701070.1439-100000@gannet.stats>

Direct use of .lib.loc was deprecated in 1.6.0.  Use .libPaths() instead.

.Library is looked for package:base, and your assigning a copy in 
.GlobalEnv will make no difference.

On Thu, 26 Jun 2003, R. Heberto Ghezzo wrote:

> Hello.
> I am using R 1.7.1 just downloaded on Win98. With the old 1.6.2 I had
> modified the etc/Rprofile file as
> 
> etc/Rprofile
> # Things you might want to change
> # options(width=80)
> # options(papersize="a4")
> # options(editor="notepad")
> # options(pager="internal")
> # to prefer Compiled HTML help
> options(chmhelp=TRUE)
> # to prefer HTML help
> # options(htmlhelp=TRUE)
> # to prefer Windows help
> # options(winhelp=TRUE)
> 
> .lib.loc <- c("c:/R/R_cran/library","c:/R/R_w/library",.Library)
> ------------------
> and it normally worked well, calling library() gives me a list of all
> packages in the 3 sites
> but in R 1.7.1 on Win98
> 
> > .lib.loc
> [1] "c:/R/R_cran/library" "c:/R/R_w/library"    "C:/R/RW1071/library"
> > .Library
> [1] "C:/R/RW1071/library"
> > library()
>   only lists C:/R/RW1071/library
> > .Library <- .lib.loc
> > library()
>   same result, does not add the other libraries
> 
> if I return Rprofile to its original and instead create and
> Renviron.site as in the FAQ
> 
> etc/Renviron.site
> R_LIBS = C:/R_CRAN/Library;C:/R_W/Library
> 
> 
> > .Library
> [1] "C:/R/RW1071/library"
> In addition: Warning messages:
> 1: list.files: C:/R_CRAN/Library is not a readable directory
> 2: list.files: C:/R_W/Library is not a readable directory
> >
> Well I used them before, what should I do now to make them readable?

You didn't spell them that way before: I am not sure if that makes a 
difference?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Thu Jun 26 18:10:23 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Jun 2003 18:10:23 +0200
Subject: [R] assignment in lists
In-Reply-To: <MABBLJDICACNFOLGIHJOEEAMDJAA.phgrosjean@sciviews.org>
Message-ID: <MABBLJDICACNFOLGIHJOKEANDJAA.phgrosjean@sciviews.org>

Ok, I got it. I should have to define a as a list, in order to get a sane
behaviour... That makes sense:

> a <- as.list(NULL)
> a[["item"]] <- 1:3
> a$item
[1] 1 2 3

Best,

Philippe Grosjean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Philippe Grosjean
Sent: jeudi 26 juin 2003 5:25
To: r-help at stat.math.ethz.ch
Subject: [R] assignment in lists


Hello,

I do not understand the following behaviour. Could someone explain me what
happens?

> a <- NULL
> a$item <- 1:3
> a$item
[1] 1 2 3
> rm(a)
> a <- NULL
> a[["item"]] <- 1:3
Error: more elements supplied than there are to replace

Why do I get an error message using list[["item"]], and not using list$item?
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Jun 26 18:20:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 17:20:43 +0100 (BST)
Subject: [R] lm diagnostics and qr (fwd)
In-Reply-To: <Pine.SGI.4.40.0306261057160.1716165-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0306261714020.1439-100000@gannet.stats>

On Thu, 26 Jun 2003, Jean Eid wrote:

> I have been struggling to find some informaation on what lm exactly does.
> I know it uses the QR decomp. However, I was recently faced with a
> somewhat badly scaled matrix and summary(lm) said
> 	Coefficients: ( 4 not defined because of singularities)
> does anyone know how lm chooses these 4 coef. is it forward building of
> the model --> drop last when qr sends a non full rank design matrix?

It is forward building of the QR matrix (not the same thing), and it 
pivots (to last) columns that it does not add.  It's in the source code, 
file src/appl/dqrls.f.

[...]

> Lastly, the qr function is supposed to take the LAPACK package in its

Supposed by whom?  That's not what the help page says.

> default but it seems to default LINPACK. The following appears only when
> qr(x, LAPACK=T)
> attr(,"useLAPACK")
> [1] TRUE

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From john.janmaat at acadiau.ca  Thu Jun 26 18:45:39 2003
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Thu, 26 Jun 2003 13:45:39 -0300
Subject: [R] Fonts on contour maps...
Message-ID: <3EFB2333.7000904@acadiau.ca>

Hello All,

I am drawing four contour plots on a 2x2 layout.  I need to downsize the 
contour line labels.  cex and labcex do not seem to work.  Any suggestions?

Thanks,

John.
-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070



From simon at stats.gla.ac.uk  Thu Jun 26 18:46:10 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Thu, 26 Jun 2003 17:46:10 +0100 (BST)
Subject: [R] lm diagnostics and qr (fwd)
In-Reply-To: <Pine.SGI.4.40.0306261057160.1716165-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.SOL.3.96.1030626173924.2762G-100000@moon.stats.gla.ac.uk>

> I have been struggling to find some informaation on what lm exactly does.
> I know it uses the QR decomp. However, I was recently faced with a
> somewhat badly scaled matrix and summary(lm) said
> 	Coefficients: ( 4 not defined because of singularities)
> does anyone know how lm chooses these 4 coef. is it forward building of
> the model --> drop last when qr sends a non full rank design matrix?
- Probably you've seen this, but just in case...
- There's a quite good explanation of QR with column pivoting and the
subsequent detection of rank deficiency in the least squares context in
Golub and Van Loan, Matrix Computations (1983, section 6.4. p162 - I don't
have newer editions to hand).
Simon 
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From andy_liaw at merck.com  Thu Jun 26 18:46:17 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jun 2003 12:46:17 -0400
Subject: [R] Smooth of a time serie
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7CB@usrymx25.merck.com>

Vellman & Hoaglin's "ABC of EDA" book has listing of Fortran program for
that (and other) smoother.  You can try to load that into R.

Another thing you can try is to port things in the "smoothers" collection on
StatLib's S section.  That also seems to contain the 3253H smoother.

Andy

> -----Original Message-----
> From: Henrique Patr?cio Sant'Anna Branco 
> [mailto:hpsbranco at superig.com.br] 
> Sent: Thursday, June 26, 2003 10:17 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Smooth of a time serie
> 
> 
> Thomas,
> First of all, thanks for the help, but it isn't exactly what 
> I'm looking for. smooth() doesn't perform the smooth the way 
> I want it to do. I want, precisely, the 4253H method. R 
> doesn't give the option to do that.
> 
> Thanks,
> Henrique.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From andy_liaw at merck.com  Thu Jun 26 18:57:52 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Jun 2003 12:57:52 -0400
Subject: [R] Fonts on contour maps...
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7CC@usrymx25.merck.com>

>From ?contour:

  labcex: `cex' for contour labelling.

Andy

> -----Original Message-----
> From: John Janmaat [mailto:john.janmaat at acadiau.ca] 
> Sent: Thursday, June 26, 2003 12:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fonts on contour maps...
> 
> 
> Hello All,
> 
> I am drawing four contour plots on a 2x2 layout.  I need to 
> downsize the 
> contour line labels.  cex and labcex do not seem to work.  
> Any suggestions?
> 
> Thanks,
> 
> John.
> -- 
> --------------------------------------------------------------
> ------------
> Dr. John Janmaat
> Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
> E-mail: jjanmaat at acadiau.ca        Web: 
> http://ace.acadiau.ca/~jjanmaat
> Tel: 902-585-1461		
>    Fax: 902-585-1070
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From jliou at wisc.edu  Thu Jun 26 18:57:25 2003
From: jliou at wisc.edu (jinn-ing Liou)
Date: Thu, 26 Jun 2003 11:57:25 -0500
Subject: [R] xyplot
Message-ID: <000001c33c04$04b2d370$db8c5c90@Outcome.wisc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030626/dde87264/attachment.pl

From netsys at med.uni-marburg.de  Wed Jun 25 23:43:36 2003
From: netsys at med.uni-marburg.de (netsys@med.uni-marburg.de)
Date: Thu, 26 Jun 2003 19:03:36 +02120 (MEST)
Subject: [R] Encrypted Message: Re: Application
Message-ID: <200306261718.h5QHIbU2029152@stat.math.ethz.ch>

Diese Nachricht wurde automatisch von einem Virenschutzprogramm erzeugt.

Es wurde ein Virus in Ihrer Mail mit dem erw?hnten Subject vom Thu, 26 Jun 2003 13:03:32 --0400 an herold at med.uni-marburg.de
 gefunden.

Ihre Nachricht wurde gel?scht und nicht weitergeleitet.

Bitte l?sen Sie dieses Problem und versuchen Sie es erneut.

This messages was automatically created by a virus-scanning program.

We detected a virus in your above mentioned mail from Thu, 26 Jun 2003 13:03:32 --0400 to herold at med.uni-marburg.de
.

Your mail was not delivered but deleted.
Please solve the problem and try again.



From ripley at stats.ox.ac.uk  Thu Jun 26 19:20:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jun 2003 18:20:00 +0100 (BST)
Subject: [R] assignment in lists
In-Reply-To: <MABBLJDICACNFOLGIHJOKEANDJAA.phgrosjean@sciviews.org>
Message-ID: <Pine.LNX.4.44.0306261817170.1574-100000@gannet.stats>

Philippe,

as.list(NULL) is the same as list(), and that is what I think you should
be using in both cases.  However, I do think that either both or neither
of your examples should work: my preference would be `neither' but as S
allows both it should be `either'.

Brian

On Thu, 26 Jun 2003, Philippe Grosjean wrote:

> Ok, I got it. I should have to define a as a list, in order to get a sane
> behaviour... That makes sense:
> 
> > a <- as.list(NULL)
> > a[["item"]] <- 1:3
> > a$item
> [1] 1 2 3
> 
> Best,
> 
> Philippe Grosjean
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Philippe Grosjean
> Sent: jeudi 26 juin 2003 5:25
> To: r-help at stat.math.ethz.ch
> Subject: [R] assignment in lists
> 
> 
> Hello,
> 
> I do not understand the following behaviour. Could someone explain me what
> happens?
> 
> > a <- NULL
> > a$item <- 1:3
> > a$item
> [1] 1 2 3
> > rm(a)
> > a <- NULL
> > a[["item"]] <- 1:3
> Error: more elements supplied than there are to replace
> 
> Why do I get an error message using list[["item"]], and not using list$item?
> Best,
> 
> Philippe Grosjean
> 
> ...........]<(({?<...............<?}))><...............................
>  ) ) ) ) )
> ( ( ( ( (       Dr. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (       LOV, UMR 7093
>  ) ) ) ) )      Station Zoologique
> ( ( ( ( (       Observatoire Oc?anologique
>  ) ) ) ) )      BP 28
> ( ( ( ( (       06234 Villefranche sur mer cedex
>  ) ) ) ) )      France
> ( ( ( ( (
>  ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
> ( ( ( ( (
>  ) ) ) ) )      e-mail: phgrosjean at sciviews.org
> ( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
>  ) ) ) ) )
> .......................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From john.janmaat at acadiau.ca  Thu Jun 26 19:25:58 2003
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Thu, 26 Jun 2003 14:25:58 -0300
Subject: [R] Fonts on contour maps...
References: <3A822319EB35174CA3714066D590DCD50205C7CC@usrymx25.merck.com>
Message-ID: <3EFB2CA6.7040409@acadiau.ca>

Andy,

Thanks.  Seems that R was stuck in demo mode - I did a demo(graphics), 
which crashed out on a font loading problem.  labcex was not working. 
After restarting R, it now works.

John.

Liaw, Andy wrote:
>>From ?contour:
> 
>   labcex: `cex' for contour labelling.
> 
> Andy
> 
> 
>>-----Original Message-----
>>From: John Janmaat [mailto:john.janmaat at acadiau.ca] 
>>Sent: Thursday, June 26, 2003 12:46 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Fonts on contour maps...
>>
>>
>>Hello All,
>>
>>I am drawing four contour plots on a 2x2 layout.  I need to 
>>downsize the 
>>contour line labels.  cex and labcex do not seem to work.  
>>Any suggestions?
>>
>>Thanks,
>>
>>John.
>>-- 
>>--------------------------------------------------------------
>>------------
>>Dr. John Janmaat
>>Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
>>E-mail: jjanmaat at acadiau.ca        Web: 
>>http://ace.acadiau.ca/~jjanmaat
>>Tel: 902-585-1461		
>>   Fax: 902-585-1070
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>>
> 
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
> USA) that may be confidential, proprietary copyrighted and/or legally 
> privileged, and is intended solely for the use of the individual or entity
> named on this message. If you are not the intended recipient, and
> have received this message in error, please immediately return this by 
> e-mail and then delete it.
> ------------------------------------------------------------------------------
> 


-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070



From steve.dutky at tfn.com  Thu Jun 26 19:35:53 2003
From: steve.dutky at tfn.com (Dutky, Steve)
Date: Thu, 26 Jun 2003 13:35:53 -0400
Subject: [R] Functions for bit manipulation in R/Splus
Message-ID: <6EEA47532CD0D611887500B0D04943453FB8E5@TFSMDMSG7>

Hi,
I primarily use Splus for analysing TCP/IP traffic at the packet level.

Several years ago, I hacked together functions using the .C call for bit
operations:

bitAnd,  bitOr, bitFlip, bitShiftL, bitShiftR, bitXor and crc(char).


Are there any more standard alternatives for these?

If not, and, if any there is any interest, I would  be happy to help package
what I have for distribution.


Additionally, I would interested in hearing offline from anyone using
R/Splus communication network analysis.


Thanks, Steve Dutky
TF Rockville Network Services
301-545-4113 desk
800-532-2382 24x7
301-325-8146 cell



From ozric at web.de  Thu Jun 26 20:02:50 2003
From: ozric at web.de (Christian Schulz)
Date: Thu, 26 Jun 2003 20:02:50 +0200
Subject: [R] xyplot
References: <000001c33c04$04b2d370$db8c5c90@Outcome.wisc.edu>
Message-ID: <004401c33c0d$28f93d10$d200a8c0@pc>

IMHO you should try lattice
and play with the examples.......

library(lattice)
data(state)
## user defined panel functions
states <- data.frame(state.x77,
                     state.name = dimnames(state.x77)[[1]], 
                     state.region = state.region) 
xyplot(Murder ~ Population | state.region, data = states, 
       groups = as.character(state.name), 
       panel = function(x, y, subscripts, groups)  
       ltext(x=x, y=y, label=groups[subscripts], cex=.7, font=3))

regards,christian

----- Original Message ----- 
From: "jinn-ing Liou" <jliou at wisc.edu>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, June 26, 2003 6:57 PM
Subject: [R] xyplot


> I am doing group wise plots by using the following commands; it shows
> errors that I do not know how to fix it. Please help.
>  
>  
>  
> xyplot(within.2.special.care ~ agecat| mco.cms.ind, neuro, panel =
> function(x,y){
> + panel.grid()
> + panel.xyplot(x,y)
> + panel.loess(x,y, span =1)})
>  
> Error: couldn't find function "xyplot"
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Peter.Ruckdeschel at uni-bayreuth.de  Thu Jun 26 21:13:19 2003
From: Peter.Ruckdeschel at uni-bayreuth.de (Dr. Peter Ruckdeschel)
Date: Thu, 26 Jun 2003 21:13:19 +0200
Subject: [R] Version Management for Classes as in Green Book sec 7.4?
Message-ID: <3EFB45CF.5060805@uni-bayreuth.de>

Just a simple question:

Is there any project going on in the R-Community
 implementing  version management for classes as discussed
in  the ``/Green Book/'',  section 7.4 ?

I would really appreciate this feature, above all more or less 
automatically
updating objects of an older class definition.

Thank you already.
-- 
Peter Ruckdeschel



From jml14 at wanadoo.es  Thu Jun 26 21:11:01 2003
From: jml14 at wanadoo.es (=?iso-8859-1?Q?Javier_Mu=F1oz?=)
Date: Thu, 26 Jun 2003 21:11:01 +0200
Subject: [R] Pause with Sys.sleep
Message-ID: <000f01c33c16$b15cc560$d991253e@wanadoo.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030626/60b7af94/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jun 26 21:20:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jun 2003 21:20:01 +0200
Subject: [R] Pause with Sys.sleep
References: <000f01c33c16$b15cc560$d991253e@wanadoo.es>
Message-ID: <3EFB4761.28CD6C95@statistik.uni-dortmund.de>



Javier Mu?oz wrote:
> 
> Hello!
> 
> Why the following source file:
> 
> cat("Hi")
> Sys.sleep(10)
> cat("Bye")
> 
> print BOTH strings after 10 seconds?.

I guess on Windows?
In that case: the output is buffered, see the R for Windows FAQs for
details.

Uwe Ligges

 
> I want a pause of 10 seconds between the printings.
> 
> Anyone can help me?
> 
> Thanks a lot.



From r.darnell at uq.edu.au  Fri Jun 27 00:21:01 2003
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Fri, 27 Jun 2003 08:21:01 +1000
Subject: [R] equivalence test
Message-ID: <he6ca3aq.fsf@uq.edu.au>

>Hi,
>	is it possible to do an equivalence test on paired quantitative datas 
>in R? Is there a way to calculate sample size for such tests?
>
>I've tried to find some documentation on that subject but I was 
>unsuccessfull. 
>I'll be happy with any links on equivalence test. If such a test does'nt 
>exist in R, i'll do it manually if I find a method to do so.
>
>Best regards
>
>Blaise

The sample size for an equivalence trial with normally distributed
outcomes can be calculated by

n <- 2 * s^2 / delta^2 * (qnorm(alpha/2) + qnorm(beta/2))^2

s = standard deviation
delta = clinically important difference
alpha = Type I error
beta = Type II error


-- 
Ross Darnell



From faheem at email.unc.edu  Fri Jun 27 01:11:50 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Thu, 26 Jun 2003 19:11:50 -0400 (EDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.A41.4.44.0306222111171.37202-100000@homer30.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0306261851490.6402-100000@Chrestomanci>



On Sun, 22 Jun 2003, Thomas Lumley wrote:

> On Sun, 22 Jun 2003, Faheem Mitha wrote:
> > If I'm doing this correctly, R does not seem to think it is a call.
> >
> > > is.call("Monotonic Multigamma run (" * n == len * ", " * theta == t1
> > * ").")
> > Error in "Monotonic Multigamma run (" * n :
> >         non-numeric argument to binary operator
>
> R is trying to *evaluate*
>   "Monotonic Multigamma run ("* n==len etc
> which doesn't work.  Remember, is.call(), like any normal function, will
> be passed the *value* of its arguments.

Hmm. I'm trying to distinguish in my mind the value of an expression and
the expression itself. For some reason it reminds me of the following
exchange, from "Through the Looking-Glass".

**********************************************************************

"...The name of the song is called 'Haddocks' Eyes.'"

"Oh, that's the name of the song, is it?" Alice said, trying to feel
interested.

"No, you don't understand," the Knight said, looking a little vexed.
"That's what the name is called. The name really is 'The Aged, Aged Man.'"

"Then I ought to have said 'That's what the song is called'?" Alice
corrected herself.

"No you oughtn't: that's another thing. The song is called 'Ways and
Means' but that's only what it's called, you know!"

"Well, what is the song then?" said Alice, who was by this time completely
bewildered.

"I was coming to that," the Knight said. "The song really is 'A-sitting On
a Gate': and the tune's my own invention."
**********************************************************************

> You could try
>   is.call(quote("Monotonic Multigamma run("*n==len))
> which is TRUE.
>
> > It considers it a valid R expression though.
> >
> > > (mode(expression("Monotonic Multigamma run (" * n == len * ", " * theta
> > == t1 * ").")))
> > [1] "expression"
> >
>
> That's because expression() returns an expression.
>
> >
> > The clearest description I have seen of a call is in S Poetry, where it
> > says
> >
> > "Mode call represents objects that are calls to a function. The first
> > component of a call is the name (mode name) of the function being called.
> > The rest of the call is the arguments given."
> >
> > This certainly is how calls are constructed using call(...), but I'm not
> > sure how it fits in with an expression like the one above. What is the
> > function being called in that case, for example?
>
> Well, we can find out. It must be either * or ==, but it isn't immediately
> obvious which one ends up at the top level
>
> > thing <- quote("Monotonic Multigamma Run ("*n==len* ", " * theta
> ==t1*").")
> > mode(thing)
> [1] "call"
> > length(thing)
> [1] 3
> > thing[[1]]
> ==
> > thing[[2]]
> "Monotonic Multigamma Run (" * n == len * ", " * theta
> > thing[[3]]
> t1 * ")."
> > mode(thing[[2]])
> [1] "call"
> > mode(thing[[3]])
> [1] "call"
> > thing[[2]][[1]]
> ==
> > thing[[3]][[1]]
> *
>
> So it is a call to ==, with two arguments, each itself a call.  The first
> arguemetn is also a call to == and the second is a call to *. And so on in
> a tree structure.

This is very interesting. I had convinced myself that an expression could
not become a call unless created explicitly by call, because it could not
know out of all the possible call structures which one to turn the
expression into. However, it appears this is not the case. So, naturally,
this makes me wonder, what rules are used to make the structure, out of
all the various possibilities. For example, the function in the call could
have corresponded to one of the *'s, and then the rest of the structure
would have been different. And is this rule part of the language
definition?

                                                            Faheem.



From peter at fe.up.pt  Fri Jun 27 01:38:48 2003
From: peter at fe.up.pt (Peter Ho)
Date: Fri, 27 Jun 2003 00:38:48 +0100
Subject: [R] Correct contrast for unreplicated 2K factorial design 
Message-ID: <3EFB8408.8050006@fe.up.pt>

Hi all,

I have been trying to reproduce an analysis from Douglas Montgomery?s 
book on design and analysis of experiments. Table 6.10 of example 6.2 on 
page 246, gives a table as follows:

 > NPK <- expand.grid(A=mp,B=mp,C=mp,D=mp)
 > Rate <- c(45,71,48,65,68,60,80,65,43,100,45,104,75,86,70,96)
 > filtration <- cbind(NPK,Rate)
 > filtration
   A B C D Rate
1  - - - -   45
2  + - - -   71
3  - + - -   48
4  + + - -   65
5  - - + -   68
6  + - + -   60
7  - + + -   80
8  + + + -   65
9  - - - +   43
10 + - - +  100
11 - + - +   45
12 + + - +  104
13 - - + +   75
14 + - + +   86
15 - + + +   70
16 + + + +   96

Two additional tables follow. Table 6-11 for contracts constants and 
Table 6-12 for Factor effects estimates.
So far my attempts at fitting the model gives me very different effect 
estimates. This I guess  is because I have not set the right contrasts?
Can anyone explain to me how I could set the correct contrasts in R to 
estimate the effects in this unreplicated 2^4 factorial?

Thanks in advance,


Peter



From tlumley at u.washington.edu  Fri Jun 27 01:51:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 26 Jun 2003 16:51:47 -0700 (PDT)
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.LNX.4.44.0306261851490.6402-100000@Chrestomanci>
Message-ID: <Pine.A41.4.44.0306261631370.27220-100000@homer27.u.washington.edu>

On Thu, 26 Jun 2003, Faheem Mitha wrote:

>
> Hmm. I'm trying to distinguish in my mind the value of an expression and
> the expression itself. For some reason it reminds me of the following
> exchange, from "Through the Looking-Glass".
>

Yes, but Carroll gets that slightly wrong: the song is not "A-sitting On a
Gate", (which is an English phrase, not a song).



> >
> > > thing <- quote("Monotonic Multigamma Run ("*n==len* ", " * theta
> > ==t1*").")
> > > mode(thing)
> > [1] "call"
> > > length(thing)
> > [1] 3
> > > thing[[1]]
> > ==
> > > thing[[2]]
> > "Monotonic Multigamma Run (" * n == len * ", " * theta
> > > thing[[3]]
> > t1 * ")."


> > So it is a call to ==, with two arguments, each itself a call.  The first
> > arguemetn is also a call to == and the second is a call to *. And so on in
> > a tree structure.
>
> This is very interesting. I had convinced myself that an expression could
> not become a call unless created explicitly by call, because it could not
> know out of all the possible call structures which one to turn the
> expression into. However, it appears this is not the case. So, naturally,
> this makes me wonder, what rules are used to make the structure, out of
> all the various possibilities. For example, the function in the call could
> have corresponded to one of the *'s, and then the rest of the structure
> would have been different. And is this rule part of the language
> definition?
>

The rules are defined by the R grammar, which can be found in
src/main/gram.y (if you speak bison)

Basically, == has lower precedence than *, so one of the == must be the
last function called.  This is necessarily part of the language
definition, as it tells you the meaning of eg
  2*3==6
Since * has higher precedence this is parsed as (2*3)==6, not 2*(3==6).

It doesn't actually matter which == comes first, but we can see from other
examples that the rightmost operator ends up at the root of the tree
Since
> 2<3<4
[1] TRUE
it must have been evaluated as (2<3)<4,  not 2<(3<4)



While the rules for constructing the tree are part of the language
definition, the order of evaluation might not be.  In C, for example, the
order of evaluation is not specified except to the extent that precedence
constrains it (and for a few special operators like && and ||).

FOr an R example, if you do
	f(plot(a), plot(b))

it is clear that the plot commands must be evaluated before f() returns if
their return values are actually used. It is not clear which order the
plots() appear, and I would say that it shouldn't be part of the language
definition.


	-thomas



From s-thapa-11 at alumni.uchicago.edu  Fri Jun 27 04:53:17 2003
From: s-thapa-11 at alumni.uchicago.edu (Suchandra Thapa)
Date: Fri, 27 Jun 2003 02:53:17 -0000
Subject: [R] nls question
Message-ID: <1056682396.1244.76.camel@hepcat>

I'm running into problems trying to use the nls function to fit the some
data.  I'm invoking nls using 

nls(s~k/(a+r)^b, start=list(k=1, a=13, b=0.59))

but I get errors indicating that the step has been reduced below the
minimum step size or an inifinity is generated in numericDeriv. I've
tried to use a variety of starting values for a, b, k but get similar
errors.  

Is there anything I can do to get the a fit or is there an alternative
to the nls function?

-- 
Suchandra Thapa <s-thapa-11 at alumni.uchicago.edu>



From Nick.Bond at sci.monash.edu.au  Fri Jun 27 05:08:16 2003
From: Nick.Bond at sci.monash.edu.au (Nick Bond)
Date: Fri, 27 Jun 2003 13:08:16 +1000
Subject: [R] dropping factor levels in subset
Message-ID: <HAEHLHNEIAAHAMDKMIPCEECKCAAA.Nick.Bond@sci.monash.edu.au>

Dear all,
I've taken a subset of data from a data frame using

crb<-subset(all.raw, creek %in% c("CR") & year %in% c(2000,2001) & substrate
%in% ("b"))

this works fine, except that all of the original factor levels are
maintained. This results in NA's for these empty levels when I try to do
summaries based on factors using by(). Is there a simple way to drop the
factor levels that are no longer represented. I've used na.omit on the
results from by, but then I have to deal with the attr setting, which
catches me too. Probably a silly question, but I've done a search and
couldn't find anything.  Can someone help me please.
Regards
Nick

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr Nick Bond
Department of Biological Sciences
Monash University (Clayton Campus)
Victoria, Australia, 3800
Ph: +61 3 9905 5606 Fax: +61 3 9905 5613
Email: Nick.Bond at sci.monash.edu.au
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From mschwartz at medanalytics.com  Fri Jun 27 05:35:43 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 26 Jun 2003 22:35:43 -0500
Subject: [R] dropping factor levels in subset
In-Reply-To: <HAEHLHNEIAAHAMDKMIPCEECKCAAA.Nick.Bond@sci.monash.edu.au>
Message-ID: <002b01c33c5d$30bfff20$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nick Bond
>Sent: Thursday, June 26, 2003 10:08 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] dropping factor levels in subset
>
>
>Dear all,
>I've taken a subset of data from a data frame using
>
>crb<-subset(all.raw, creek %in% c("CR") & year %in% 
>c(2000,2001) & substrate
>%in% ("b"))
>
>this works fine, except that all of the original factor levels are
>maintained. This results in NA's for these empty levels when I 
>try to do
>summaries based on factors using by(). Is there a simple way 
>to drop the
>factor levels that are no longer represented. I've used na.omit on
the
>results from by, but then I have to deal with the attr setting, which
>catches me too. Probably a silly question, but I've done a search and
>couldn't find anything.  Can someone help me please.
>Regards
>Nick

See ?factor for additional information, but a quick example where
using factor(old.factor) will return the factor with unused levels
dropped.

# Create a factor
> old.factor <- factor(c("One", "Two", "Three", "Four"))
> old.factor
[1] One   Two   Three Four 
Levels: Four One Three Two

# Create a subset of three noting that all four
# levels are retained
> new.factor <- old.factor[1:3]
> new.factor
[1] One   Two   Three
Levels: Four One Three Two

# Drop unused level
> new.factor2 <- factor(new.factor)
> new.factor2
[1] One   Two   Three
Levels: One Three Two


HTH,

Marc Schwartz



From spencer.graves at pdf.com  Fri Jun 27 06:52:39 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 26 Jun 2003 21:52:39 -0700
Subject: [R] nls question
References: <1056682396.1244.76.camel@hepcat>
Message-ID: <3EFBCD97.8020608@pdf.com>

	  An article in the American Statistician perhaps 5 years ago on the 
accuracy of statistical software recommended using nlminb first to find 
the least squares solution and then pass those numbers to nls to get 
confidence intervals.  More recently, optim has replaced nlminb for such 
purposes, as far as I know.  In addition, optim will optionally output 
the Hessian from which approximate confidence intervals can be obtained. 
  I have not used this recently, but I would expect that "profile" on 
the nls fit would give better confidence intervals.

hth.  spencer graves

Suchandra Thapa wrote:
> I'm running into problems trying to use the nls function to fit the some
> data.  I'm invoking nls using 
> 
> nls(s~k/(a+r)^b, start=list(k=1, a=13, b=0.59))
> 
> but I get errors indicating that the step has been reduced below the
> minimum step size or an inifinity is generated in numericDeriv. I've
> tried to use a variety of starting values for a, b, k but get similar
> errors.  
> 
> Is there anything I can do to get the a fit or is there an alternative
> to the nls function?
>



From feldesmanm at pdx.edu  Thu Jun 26 22:45:17 2003
From: feldesmanm at pdx.edu (feldesmanm@pdx.edu)
Date: Thu, 26 Jun 2003 22:45:17 +0200 (MEST)
Subject: [R] Compiling R for OS X 10.2.6 (Darwin 6.6)
Message-ID: <200306262045.h5QKjGU2016549@stat.math.ethz.ch>

Our lab just picked up a G4 Powerbook (1 GHz, 1GB RAM, OS X 10.2.6).  None 
of us have any experience with Macs but for various reasons we need to have 
one around for development.

In any case, we've installed fink, all the Apple Developer Tools and the 
Dec2002updater to gcc 3.3.  We're now trying to compile R 1.7.1 without too 
much success.  First we had to get g77, which turned out to be a headache 
because the version on fink doesn't match the gcc version on the Powerbook 
and wouldn't install.  We reset the PB gcc version to default to gcc 3.1, 
which then allowed us to install the g77 patch.

In building R, the configure is fine (except for lacking pdf and dvi 
support to build the manuals - no big deal right now), but during the make, 
the program barfs, complaining that it can't find Rdynload.c, which sits 
precisely where it ought to be.  Again, since we have no experience with 
building software on the Mac, this one is tough to sort out.

I read a few messages on R-devel about issues involving g77 support, but I 
don't know whether this is the problem or not.  The errors are being 
emitted when the make is using gcc, which might be 3.1 or 3.3.  There 
doesn't seem to be any way to remove the "updater".

I'd appreciate any pointers from experienced Mac folks out there.  (BTW, I 
did find Jan de Leuew's binary of 1.7.1.  It installs fine in the meantime, 
but it would still be helpful to resolve the compilation problem as I 
expect we'll encounter other issues like this soon).

Thanks.




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesman at attglobal.net
fax:    503-725-3905


"Sometimes the lights are all shining on me, other times I can barely see,
lately it's occurred to me, what a long strange trip it's been..."  Jerry &
the boys



From ripley at stats.ox.ac.uk  Fri Jun 27 08:34:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 07:34:37 +0100 (BST)
Subject: [R] dropping factor levels in subset
In-Reply-To: <002b01c33c5d$30bfff20$0201a8c0@MARC>
Message-ID: <Pine.LNX.4.44.0306270724040.7550-100000@gannet.stats>

A more transparent solution is

old.factor[1:3, drop = TRUE]

That has worked for a long time, but apparently not been documented in R
until 1.7.1 (docs added a couple of hours before release). So you could do
(probably, since there are some bugs prior to 1.8.0)

crb[] <- lapply(crb, function(x) x[drop=TRUE])

to remove the unused levels on all factors in the data frame.

On Thu, 26 Jun 2003, Marc Schwartz wrote:

> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch 
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nick Bond
> >Sent: Thursday, June 26, 2003 10:08 PM
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] dropping factor levels in subset
> >
> >
> >Dear all,
> >I've taken a subset of data from a data frame using
> >
> >crb<-subset(all.raw, creek %in% c("CR") & year %in% 
> >c(2000,2001) & substrate
> >%in% ("b"))
> >
> >this works fine, except that all of the original factor levels are
> >maintained. This results in NA's for these empty levels when I 
> >try to do
> >summaries based on factors using by(). Is there a simple way 
> >to drop the
> >factor levels that are no longer represented. I've used na.omit on
> the
> >results from by, but then I have to deal with the attr setting, which
> >catches me too. Probably a silly question, but I've done a search and
> >couldn't find anything.  Can someone help me please.
> >Regards
> >Nick
> 
> See ?factor for additional information, but a quick example where
> using factor(old.factor) will return the factor with unused levels
> dropped.
> 
> # Create a factor
> > old.factor <- factor(c("One", "Two", "Three", "Four"))
> > old.factor
> [1] One   Two   Three Four 
> Levels: Four One Three Two
> 
> # Create a subset of three noting that all four
> # levels are retained
> > new.factor <- old.factor[1:3]
> > new.factor
> [1] One   Two   Three
> Levels: Four One Three Two
> 
> # Drop unused level
> > new.factor2 <- factor(new.factor)
> > new.factor2
> [1] One   Two   Three
> Levels: One Three Two
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Fri Jun 27 08:34:40 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 27 Jun 2003 08:34:40 +0200
Subject: [R] assignment in lists
In-Reply-To: <Pine.LNX.4.44.0306261817170.1574-100000@gannet.stats>
Message-ID: <MABBLJDICACNFOLGIHJOEEBEDJAA.phgrosjean@sciviews.org>

Prof. Brian Ripley wrote:
>Philippe,

>as.list(NULL) is the same as list(), and that is what I think you should
>be using in both cases.

OK, thank you.

>However, I do think that either both or neither of your examples should
>work: my preference would be `neither' but as S allows both it should be
>`either'.

I agree with you, including on the fact that 'neither' should work. I would
prefer a language that obliges to declare list components before using them.
Experimenting a little bit more around this problem, I got that:

- assigning NULL to a list entry deletes this entry from the list. OK, fine.
Asking for:
my.list$non.existing.item gives NULL. Thus, it is consistent. However, if I
use this:

> a <- list(item1=NULL, item2=NULL)
> a
$item1
NULL

$item2
NULL

- this is a strange behaviour because the previous command should have
returned 'list()' in a. Consequently, when I reallocate NULL to either
'item1', or 'item2' of 'a', it deletes it:

> a$item1 <- NULL
> a
$item2
NULL

Not an harmfull behaviour, but inconsistent with the rest.
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From ripley at stats.ox.ac.uk  Fri Jun 27 08:39:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 07:39:34 +0100 (BST)
Subject: [R] assignment in lists
In-Reply-To: <MABBLJDICACNFOLGIHJOEEBEDJAA.phgrosjean@sciviews.org>
Message-ID: <Pine.LNX.4.44.0306270738050.7550-100000@gannet.stats>

On Fri, 27 Jun 2003, Philippe Grosjean wrote:

> Prof. Brian Ripley wrote:
> >Philippe,
> 
> >as.list(NULL) is the same as list(), and that is what I think you should
> >be using in both cases.
> 
> OK, thank you.
> 
> >However, I do think that either both or neither of your examples should
> >work: my preference would be `neither' but as S allows both it should be
> >`either'.
> 
> I agree with you, including on the fact that 'neither' should work. I would
> prefer a language that obliges to declare list components before using them.

I've altered this to work like S (and documented it).

> Experimenting a little bit more around this problem, I got that:
> 
> - assigning NULL to a list entry deletes this entry from the list. OK, fine.
> Asking for:
> my.list$non.existing.item gives NULL. Thus, it is consistent. However, if I
> use this:
> 
> > a <- list(item1=NULL, item2=NULL)
> > a
> $item1
> NULL
> 
> $item2
> NULL
> 
> - this is a strange behaviour because the previous command should have
> returned 'list()' in a. Consequently, when I reallocate NULL to either
> 'item1', or 'item2' of 'a', it deletes it:
> 
> > a$item1 <- NULL
> > a
> $item2
> NULL
> 
> Not an harmfull behaviour, but inconsistent with the rest.

It is in the FAQ, though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Fri Jun 27 08:49:49 2003
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Fri, 27 Jun 2003 08:49:49 +0200
Subject: [R] A vector or matrix response
Message-ID: <488C02265C6AD611BF200002A542182F022B343F@irnts22.ifp.fr>

Hello,

I wonder if anybody has some idea about how to solve my problem : 

I am working , I would say trough an experimental design approach (perform
experiments, get responses, make regression, sensitivity analysis, risk
analysis ...). The problem is now that I have to face with not only a
response but a vector or a matrix (typically a spatial distribution of a
physical property ... pressure). Is there any kind of approach to deal with
that ? I don' t know to group cells together to dicrease the dimension of
the problem ...

I have no idea and I really need some help 

Thanks in advance

Isabelle

Isabelle Zabalza-Mezghani
Institut Fran?ais du P?trole
France



From e.pebesma at geog.uu.nl  Fri Jun 27 09:06:47 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 27 Jun 2003 09:06:47 +0200
Subject: [R] krige in gstat() package
Message-ID: <3EFBED07.3000306@geog.uu.nl>

> HI,
>   I wonder does anyone have experience with doing sequential gaussian
> simulation with krige() function in gstat?
>
> I find it VERY slow compared to use krige() to achieve kriging function
> itself..  I wonder why, is that because it has to model the variogram, and
> do the kriging separately for each point to be simulated?

It does not model variograms on the way. It is slower than kriging
because it uses the sequential simulation algorithm: for each node
visited, a value is simulated, and this value is added to the
conditioning data. For this reason you _have to_ limit the search
neighbourhood (using the nmax or maxdist arguments) if you're
simulating on more than say a few hundred nodes. Taking a very
small nmax may yield fast simulations, at the expense of a good
representation of the target spatial correlation structure.

> 
> so it would be N times slower to achieve the simulation than the kriging
> if the number of points to be estimated is N??

No. Experiment with the nmax argument.
--
Edzer



From p.dalgaard at biostat.ku.dk  Fri Jun 27 09:38:08 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 27 Jun 2003 07:38:08 -0000
Subject: [R] combining mathematical notation and value substitution
In-Reply-To: <Pine.A41.4.44.0306261631370.27220-100000@homer27.u.washington.edu>
References: <Pine.A41.4.44.0306261631370.27220-100000@homer27.u.washington.edu>
Message-ID: <x2wuf8m0kw.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> While the rules for constructing the tree are part of the language
> definition, the order of evaluation might not be.  In C, for example, the
> order of evaluation is not specified except to the extent that precedence
> constrains it (and for a few special operators like && and ||).
> 
> FOr an R example, if you do
> 	f(plot(a), plot(b))
> 
> it is clear that the plot commands must be evaluated before f() returns if
> their return values are actually used. It is not clear which order the
> plots() appear, and I would say that it shouldn't be part of the language
> definition.

...not to mention that lazy evaluation explicitly makes the order dependent
on the function body:

  f <- function(x,y) {x;y}
  f <- function(x,y) {y;x}

will produce the plots in different order when called as above. Notice
in particular that constructs like

  f(a<-b, a)

are playing with fire, unless you're really, really sure that the 1st
arg is evaluated before the 2nd.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Friedrich.Leisch at ci.tuwien.ac.at  Fri Jun 27 10:01:36 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Fri, 27 Jun 2003 10:01:36 +0200
Subject: [R] Bagged clustering and fuzzy c-means
In-Reply-To: <002b01c33be8$f4d5a740$8a2bde89@CHO180>
References: <002b01c33be8$f4d5a740$8a2bde89@CHO180>
Message-ID: <16123.63968.940622.152035@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 26 Jun 2003 14:43:41 +0100,
>>>>> Xu Yun (XY) wrote:

  > Dear All:
  > I'm a newbie to R and chemometrics.
  > Now I'm trying apply bclust on fuzzy c-means like this:
  >> bc1 <- bclust(iris[,1:4], 3, base.centers=20,iter.base=100,
  > base.method="cmeans")
  > Committee Member:
  > 1(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)Erro
  > r in bclust(iris[, 1:4], 3, base.centers = 20, iter.base = 100, base.method
  > = "cmeans") :
  >         Could not find valid cluster solution in 20 replications
  > I can't get any valid result with many parameter adjustments, such as
  > iter.base, base.centers etc. But I think fcm should return similar result
  > just like k-means (e.g. centers, cluster size) plus fuzzy membership
  > information. Can anyone explain this for me?

cmeans expects a matrix as input, iris[,1:4] is a data.frame.

bc1 <- bclust(as.matrix(iris[,1:4]), 3,
	base.centers=5,iter.base=100,base.method="cmeans")

works for me.

But I agree that this behaviour is not desirable, I'll add an
x=as.matrix(x) at the beginning of both bclust and cmeans


  > Besides, I'm not quite understand the meaning of "bootstrap". In my view, it
  > might means "independent", am I correct?

No, a bootstrap sample is a sample drawn from the empirical
distribution of a data set, i.e., drawing with replacement from the
original data.

There are heaps of books explaining what "bootstrapping" is, simply
search for books with "bootstrap" in the title in your library :-)

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From johnfield at ozemail.com.au  Fri Jun 27 10:33:44 2003
From: johnfield at ozemail.com.au (John Field)
Date: Fri, 27 Jun 2003 18:03:44 +0930
Subject: [R] Returning contour co-ordinates
Message-ID: <4.3.2.7.2.20030627175501.00b68ef8@pop.ozemail.com.au>

Dear R-helpers,

I'd like to be able to post-process contours coming from contour().   Does 
anyone have a version of contour() (or something similar) which will return 
the contour coordinates?

In searching the archives I've come across a message in Nov 01 which had 
this on a wish-list, but I can find no later reference.

With thanks,
John Field
Adelaide, South Australia



From ramzi_feg at yahoo.fr  Fri Jun 27 10:41:23 2003
From: ramzi_feg at yahoo.fr (=?iso-8859-1?q?Ramzi=20Feghali?=)
Date: Fri, 27 Jun 2003 10:41:23 +0200 (CEST)
Subject: [R] sample function
Message-ID: <20030627084123.32875.qmail@web20305.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030627/aab0896a/attachment.pl

From Philippe.Hupe at curie.fr  Fri Jun 27 10:55:08 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Fri, 27 Jun 2003 10:55:08 +0200
Subject: [R] How to get pixel position of a plot
Message-ID: <3EFC066C.6020901@curie.fr>

Hi,

I would like to plot a graph on the jpeg device for example and to write 
a table associated to this plot where I have the pixel coordonates of 
each plotted point so that I could include the jpeg image onto a html 
page and get all the information about each point when the mouse button 
is pressed. The indentify() can do it but on the window device...
Thanks for any idea.

Philippe
-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>



From ripley at stats.ox.ac.uk  Fri Jun 27 11:18:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 27 Jun 2003 10:18:02 +0100 (GMT Daylight Time)
Subject: [R] sample function
In-Reply-To: <20030627084123.32875.qmail@web20305.mail.yahoo.com>
Message-ID: <Pine.WNT.4.44.0306271016341.4000-100000@petrel>

R is not S-PLUS, and you need Modern Applied Statistics in S (4th ed) for a
description including R.

sample in R used a PRNG: see ?RNG in R for the details of PRNGs in R.

On Fri, 27 Jun 2003, [iso-8859-1] Ramzi Feghali wrote:

> i have a question about the "sample" function used in R, does it work as
> a pseudo-dandom number generator programmed with C, like it is described
> in Modern Applied Statics with S-Plus 3d edition chapter 5 section 2?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Fri Jun 27 11:36:56 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 27 Jun 2003 10:36:56 +0100
Subject: [R] How to get pixel position of a plot
In-Reply-To: <3EFC066C.6020901@curie.fr>
References: <3EFC066C.6020901@curie.fr>
Message-ID: <3EFC1038.8050803@lancaster.ac.uk>

Philippe Hup? wrote:

> I would like to plot a graph on the jpeg device for example and to write 
> a table associated to this plot where I have the pixel coordonates of 
> each plotted point so that I could include the jpeg image onto a html 
> page and get all the information about each point when the mouse button 
> is pressed. The indentify() can do it but on the window device...

  You may wish to try my 'imagemap' library:

http://www.maths.lancs.ac.uk/Software/Imagemap/

  For background, read up about client-side imagemaps in HTML on a 
web-design web site. My library creates these things from an R plot.

Barry Rowlingson
Maths and Stats
Lancaster University
Lancaster, UK



From dnogues at ipe.csic.es  Fri Jun 27 12:07:09 2003
From: dnogues at ipe.csic.es (=?ISO-8859-1?Q?David_Nogu=E9s?=)
Date: Fri, 27 Jun 2003 12:07:09 +0200
Subject: [R] Grasper quesrion
Message-ID: <3EFC174D.6090802@ipe.csic.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030627/c129c484/attachment.pl

From ayalahec at msu.edu  Fri Jun 27 14:03:36 2003
From: ayalahec at msu.edu (Hector L. Ayala-del-Rio)
Date: Fri, 27 Jun 2003 08:03:36 -0400
Subject: [R] Compiling R for OS X 10.2.6 (Darwin 6.6)
In-Reply-To: <200306262045.h5QKjGU2016549@stat.math.ethz.ch>
Message-ID: <61F07CCD-A897-11D7-86C9-000393DB5846@msu.edu>

I have been having similar problems trying to compile R 1.7.1 in OS X 
10.2.6 with no success.  If you really want to use R just use the Fink 
package installer and install the "r-base" package ( R 1.7.0).  You 
might like to get in touch with the package maintainer (check package 
info at the Fink website) before you spend more time trying to compile 
it in your machine :-)

Hector


On Thursday, June 26, 2003, at 04:45  PM, feldesmanm at pdx.edu wrote:

> Our lab just picked up a G4 Powerbook (1 GHz, 1GB RAM, OS X 10.2.6).  
> None of us have any experience with Macs but for various reasons we 
> need to have one around for development.
>
> In any case, we've installed fink, all the Apple Developer Tools and 
> the Dec2002updater to gcc 3.3.  We're now trying to compile R 1.7.1 
> without too much success.  First we had to get g77, which turned out 
> to be a headache because the version on fink doesn't match the gcc 
> version on the Powerbook and wouldn't install.  We reset the PB gcc 
> version to default to gcc 3.1, which then allowed us to install the 
> g77 patch.
>
> In building R, the configure is fine (except for lacking pdf and dvi 
> support to build the manuals - no big deal right now), but during the 
> make, the program barfs, complaining that it can't find Rdynload.c, 
> which sits precisely where it ought to be.  Again, since we have no 
> experience with building software on the Mac, this one is tough to 
> sort out.
>
> I read a few messages on R-devel about issues involving g77 support, 
> but I don't know whether this is the problem or not.  The errors are 
> being emitted when the make is using gcc, which might be 3.1 or 3.3.  
> There doesn't seem to be any way to remove the "updater".
>
> I'd appreciate any pointers from experienced Mac folks out there.  
> (BTW, I did find Jan de Leuew's binary of 1.7.1.  It installs fine in 
> the meantime, but it would still be helpful to resolve the compilation 
> problem as I expect we'll encounter other issues like this soon).
>
> Thanks.
>
>
>
>
> Dr. Marc R. Feldesman
> Professor and Chairman Emeritus
> Anthropology Department - Portland State University
> email:  feldesmanm at pdx.edu
> email:  feldesman at attglobal.net
> fax:    503-725-3905
>
>
> "Sometimes the lights are all shining on me, other times I can barely 
> see,
> lately it's occurred to me, what a long strange trip it's been..."  
> Jerry &
> the boys
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
H?ctor L. Ayala-del-R?o, Ph.D.
Center for Microbial Ecology &
Center for Genomic and Evolutionary Studies
on Microbial Life at Low Temperatures
Michigan State University
545 Plant & Soil Sciences Building
East Lansing, MI 48824-1325
Phone: 517-353-9021
Fax: 517-353-2917



From lozalojo at jcyl.es  Fri Jun 27 14:47:00 2003
From: lozalojo at jcyl.es (=?iso-8859-1?Q?Jos=E9_Lozano_Alonso?=)
Date: Fri, 27 Jun 2003 14:47:00 +0200
Subject: [R] Cov.mcd running low on R
Message-ID: <000001c33caa$337b6ea0$c6a0100a@JSBSC01P418>

Hello:

I am running MCD robust method on R and Splus 2000, I have two
questions:

1. Why is it slower the cov.mcd R version than the S-Plus version? (it
takes quite a long time more the R version)
2. The R version shows only the "cov", but the S-Plus version shows both
the "cov" and "raw.cov" covariates. How can I take the "raw.cov" in R?

Thanks in advance

Jose Lozano
Observatorio de Salud
Junta de Castilla y Leon



From brising at louisville.edu  Fri Jun 27 16:07:30 2003
From: brising at louisville.edu (Bill Rising)
Date: Fri, 27 Jun 2003 10:07:30 -0400
Subject: [R] Compiling R for OS X 10.2.6 (Darwin 6.6)
Message-ID: <20030627140553.C2AFF7BA8@athena.louisville.edu>

On 6/27/03 8:03, Hector L. Ayala-del-Rio wrote

>I have been having similar problems trying to compile R 1.7.1 in OS X 
>10.2.6 with no success.  If you really want to use R just use the Fink 
>package installer and install the "r-base" package ( R 1.7.0).  You 
>might like to get in touch with the package maintainer (check package 
>info at the Fink website) before you spend more time trying to compile 
>it in your machine :-)
>
>Hector

Jan de Leeuw's site (for now) has a binary for Jaguar, installer and all:

http://gifi.stat.ucla.edu/pub/

I've found the site useful for Python, too.

Bill



From macq at llnl.gov  Fri Jun 27 17:03:02 2003
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 27 Jun 2003 08:03:02 -0700
Subject: [R] Compiling R for OS X 10.2.6 (Darwin 6.6)
In-Reply-To: <200306262045.h5QKjGU2016549@stat.math.ethz.ch>
References: <200306262045.h5QKjGU2016549@stat.math.ethz.ch>
Message-ID: <p05210600bb2204169554@[128.115.153.6]>

I'm not even close to expert at solving these kinds of problems, so 
I'll just offer some information from my successful build of R 1.7.1 
in OS X 10.2.6 (on a G4 desktop machine), and hope it helps.

I used this configure command:

../source/configure --prefix=/Users/macq/R/R-1.7.1 --enable-R-shlib 
--with-blas=-framework vecLib --with-lapack
CPPFLAGS=-I/sw/include LDFLAGS=-L/sw/lib

Note that I put the source code tree in a directory named "source", 
and build in an adjacent directory named "build".

Here are all the lines (3 of them) in the output from my make step 
that contain "Rdynload.c"

making Rdynload.d from ../../../source/src/main/Rdynload.c

gcc    -no-cpp-precomp -I. -I../../src/include 
-I../../../source/src/include -I/sw/include -DHAVE_CONFIG_H 
-I/sw/include -c ../../../source/src/main/Rdynload.c -o Rdynload.o

gcc    -no-cpp-precomp -I. -I../../src/include 
-I../../../source/src/include -I/sw/include -DHAVE_CONFIG_H 
-fno-common  -I/sw/include -c ../../../source/src/main/Rdynload.c -o 
Rdynload.lo

Here is a copy of the body of an email from Jan de Leeuw to r-sig-mac 
on 2003-5-30:
>The new g77 in fink/unstable is 3.3. It fails to build R, unless
>I setenv LDFLAGS to -lcc_dynamic.
>
>I now also
>
>setenv MACOSX_DEPLOYMENT_TARGET 10.2
>
>which uses the various dylibs as weak libraries.

I'm using v3.1 of both gcc and g77; g77 is from fink.

Does your G4 have the "BSD Subsystem" installed? I'm guessing that it 
does, or you probably would have had even more problems, but it's 
worth a check.
   ls /Library/Receipts | grep BSD

My copies of make, gnumake, and makeinfo are all in /usr/bin, i.e., 
not from fink.

-Don

At 1:45 PM -0700 6/26/03, feldesmanm at pdx.edu wrote:
>Our lab just picked up a G4 Powerbook (1 GHz, 1GB RAM, OS X 10.2.6). 
>None of us have any experience with Macs but for various reasons we 
>need to have one around for development.
>
>In any case, we've installed fink, all the Apple Developer Tools and 
>the Dec2002updater to gcc 3.3.  We're now trying to compile R 1.7.1 
>without too much success.  First we had to get g77, which turned out 
>to be a headache because the version on fink doesn't match the gcc 
>version on the Powerbook and wouldn't install.  We reset the PB gcc 
>version to default to gcc 3.1, which then allowed us to install the 
>g77 patch.
>
>In building R, the configure is fine (except for lacking pdf and dvi 
>support to build the manuals - no big deal right now), but during 
>the make, the program barfs, complaining that it can't find 
>Rdynload.c, which sits precisely where it ought to be.  Again, since 
>we have no experience with building software on the Mac, this one is 
>tough to sort out.
>
>I read a few messages on R-devel about issues involving g77 support, 
>but I don't know whether this is the problem or not.  The errors are 
>being emitted when the make is using gcc, which might be 3.1 or 3.3. 
>There doesn't seem to be any way to remove the "updater".
>
>I'd appreciate any pointers from experienced Mac folks out there. 
>(BTW, I did find Jan de Leuew's binary of 1.7.1.  It installs fine 
>in the meantime, but it would still be helpful to resolve the 
>compilation problem as I expect we'll encounter other issues like 
>this soon).
>
>Thanks.
>
>
>
>
>Dr. Marc R. Feldesman
>Professor and Chairman Emeritus
>Anthropology Department - Portland State University
>email:  feldesmanm at pdx.edu
>email:  feldesman at attglobal.net
>fax:    503-725-3905
>
>
>"Sometimes the lights are all shining on me, other times I can barely see,
>lately it's occurred to me, what a long strange trip it's been..."  Jerry &
>the boys
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From john.janmaat at acadiau.ca  Fri Jun 27 17:07:39 2003
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Fri, 27 Jun 2003 12:07:39 -0300
Subject: [R] bivariate lognormal ...
Message-ID: <3EFC5DBB.2000702@acadiau.ca>

Hello All,

Is there a package that contains the PDF and CDF of the bivariate 
lognormal distribution?

Thanks,

John.
-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070



From spencer.graves at pdf.com  Fri Jun 27 17:10:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 Jun 2003 08:10:23 -0700
Subject: [R] A vector or matrix response
References: <488C02265C6AD611BF200002A542182F022B343F@irnts22.ifp.fr>
Message-ID: <3EFC5E5F.2020302@pdf.com>

Are you familiar with :
@article{box,:hunt:1962,
     Author = {Box, G. E. P. and Hunter, William G.},
     Title = {A Useful Method for Model-building},
     Year = 1962,
     Journal = {Technometrics},
     Volume = 4,
     Pages = {301--318},
     Keywords = {Adaptive modeling; Empirical model; Kinetic model;
               Nonlinear}

	  I'll consider a vector response, because that is easier, but you can 
generalize to matrix.

	  I would start with various kinds of plots of the response vs. 
different poistions in the vector.  With a 2 x 2 experiment and more 
than 4 runs, I would make 4 such plots, one for each set of conditions.

	  Box and Hunter then suggest to model each line (vector or matrix) 
separately.  This will reduce the dimensionality of the problem to 
something more managable with traditional techniques.

howe this helps.  spencer graves

ZABALZA-MEZGHANI Isabelle wrote:
> Hello,
> 
> I wonder if anybody has some idea about how to solve my problem : 
> 
> I am working , I would say trough an experimental design approach (perform
> experiments, get responses, make regression, sensitivity analysis, risk
> analysis ...). The problem is now that I have to face with not only a
> response but a vector or a matrix (typically a spatial distribution of a
> physical property ... pressure). Is there any kind of approach to deal with
> that ? I don' t know to group cells together to dicrease the dimension of
> the problem ...
> 
> I have no idea and I really need some help 
> 
> Thanks in advance
> 
> Isabelle
> 
> Isabelle Zabalza-Mezghani
> Institut Fran?ais du P?trole
> France
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From f.calboli at ucl.ac.uk  Fri Jun 27 17:38:56 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Fri, 27 Jun 2003 16:38:56 +0100
Subject: [R] R and PPC Mandrake
Message-ID: <3.0.6.32.20030627163856.02526870@pop-server.ucl.ac.uk>

Dear All,

is there any problem using the PPC version of Mandrake to run R?

Cheers,

Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From clists at perrin.socsci.unc.edu  Fri Jun 27 17:31:09 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Fri, 27 Jun 2003 11:31:09 -0400 (EDT)
Subject: [R] plot() help
Message-ID: <Pine.LNX.4.53.0306271128370.23496@perrin.socsci.unc.edu>

Please forgive my ignorance on grapics. I'm trying to make a relatively
simple plot with two line plots, same axes, by mean over a series of
dates. I can make the plot well like this:

plot(sort(tapply(first.anti.auth.sum,date,mean), partial=1), type="l",
col="yellow",ylim=c(0,2.0))
par(new=TRUE)
plot(sort(tapply(first.pro.auth.sum,date,mean), partial=1), type="l",
col="red",ylim=c(0,2.0))

but I would like the X axis to be dates, not index numbers. The trick is
that the "date" column across which the means are calculated returns
integers; I'd like to use, effectively:

format.POSIXct(as.POSIXct(date), '%b %d')

...but don't know how to make that happen.

Thanks for any advice.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From wimajo at yahoo.com  Fri Jun 27 17:35:30 2003
From: wimajo at yahoo.com (Joanne Dawson)
Date: Fri, 27 Jun 2003 08:35:30 -0700 (PDT)
Subject: [R] Webcast - Integration with R: Extending Statistical Analysis in
	DecisionSite
Message-ID: <20030627153530.37775.qmail@web41806.mail.yahoo.com>

Webcast - Integration with R:  Extending Statistical
Analysis in DecisionSite

July 24, 2003 11:00 am EDT
Duration: 1 hour 
For more information and to register, go to:
http://www.spotfire.com/webcasts

 
Statisticians and statistical programmers often 
face the challenge of how to deploy the methods of
their research to colleagues not conversant in
statistical computing.  Spotfire has recently
performed an integration between R and Spotfire
DecisionSite that provides R programmers
straightforward approaches to allowing end users 
in their organizations to take advantage of R
algorithms designed for their analysis needs .
Spotfire DecisionSite is an interactive, visual
analysis application widely used to enable end users
to interact with and understand large amounts of
complex data.  This Webcast will demonstrate examples
of how to build and deploy DecisionSite Guides
allowing end users to easily access and perform any
statistical computation during a Spotfire analysis
session.



From bpeng at stat.rice.edu  Fri Jun 27 17:39:04 2003
From: bpeng at stat.rice.edu (Bo Peng)
Date: Fri, 27 Jun 2003 10:39:04 -0500
Subject: [R] read matlab .mat file?
Message-ID: <20030627153904.GB12922@stat.rice.edu>

Hi,

Is there an easy way to read matlab .mat file from R? There are R 
packages (R.matlab, R.io from google) around but I might just missed 
some easy functions.

Many thanks in advance.


-- 
Bo Peng



From huan.huang at bnpparibas.com  Fri Jun 27 17:43:32 2003
From: huan.huang at bnpparibas.com (huan.huang@bnpparibas.com)
Date: Fri, 27 Jun 2003 16:43:32 +0100
Subject: [R] plot() help
Message-ID: <OF89952B46.757A11C1-ON80256D52.00566233@bnpparibas.com>






plot(sort(tapply(first.anti.auth.sum,date,mean), partial=1), type="l",
col="yellow",ylim=c(0,2.0))
par(new=TRUE)

\\ par(xaxt='n')

plot(sort(tapply(first.pro.auth.sum,date,mean), partial=1), type="l",
col="red",ylim=c(0,2.0))

check up ?mtext()

but I would like the X axis to be dates, not index numbers. The trick is
that the "date" column across which the means are calculated returns
integers; I'd like to use, effectively:

format.POSIXct(as.POSIXct(date), '%b %d')

...but don't know how to make that happen.

Thanks for any advice.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help






This message and any attachments (the "message") is\ intende...{{dropped}}



From clists at perrin.socsci.unc.edu  Fri Jun 27 17:48:22 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Fri, 27 Jun 2003 11:48:22 -0400 (EDT)
Subject: [R] plot() help
In-Reply-To: <OF89952B46.757A11C1-ON80256D52.00566233@bnpparibas.com>
References: <OF89952B46.757A11C1-ON80256D52.00566233@bnpparibas.com>
Message-ID: <Pine.LNX.4.53.0306271147380.23496@perrin.socsci.unc.edu>

> check up ?mtext()

Thanks, but that's not what I'm looking for. I don't want to change the
text string but the axis labels themselves.

ap

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu



From yao6889 at msmailhub.oulan.ou.edu  Fri Jun 27 17:33:38 2003
From: yao6889 at msmailhub.oulan.ou.edu (Yao, Minghua)
Date: Fri, 27 Jun 2003 10:33:38 -0500
Subject: [R] NA points in loess function
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396AD77@mail4.oulan.ou.edu>

Gurus,

I used
	predict(loess(Y~X));

where Y and X are of the same length. But there are same NA's in both Y and
X. Those NA's are in the same locations in Y and X. The following is the
error messageI got:

Error in "[<-"(*tmp*, , i, value = predict(loess(Y~X))) : 
        number of items to replace is not a multiple of replacement length

If I replaced the NA's with a number, the function worked. What I want to do
is to ignores those NA's. help(loess) doesn't say how to give value to
na.action. It just says the default is to stop. Please help. Thank you in
advance.

-Minghua Yao



From peter at esb.ucp.pt  Fri Jun 27 17:56:21 2003
From: peter at esb.ucp.pt (Peter Ho)
Date: Fri, 27 Jun 2003 16:56:21 +0100
Subject: [R] feffects in model.tables
Message-ID: <3EFC6925.5070702@esb.ucp.pt>

Hi

Is there is there an  equivalent of S-plus "feffects" argument in 
model.tables() in R for calculating effects of a factorial design? Or is 
there another way they can be calculated from an aov fit.


Thanks

Peter



From ripley at stats.ox.ac.uk  Fri Jun 27 18:05:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 17:05:13 +0100 (BST)
Subject: [R] NA points in loess function
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396AD77@mail4.oulan.ou.edu>
Message-ID: <Pine.LNX.4.44.0306271701290.9073-100000@gannet.stats>

On Fri, 27 Jun 2003, Yao, Minghua wrote:

> Gurus,
> 
> I used
> 	predict(loess(Y~X));
> 
> where Y and X are of the same length. But there are same NA's in both Y and
> X. Those NA's are in the same locations in Y and X. The following is the
> error messageI got:
> 
> Error in "[<-"(*tmp*, , i, value = predict(loess(Y~X))) : 
>         number of items to replace is not a multiple of replacement length
> 
> If I replaced the NA's with a number, the function worked. What I want to do
> is to ignores those NA's. help(loess) doesn't say how to give value to
> na.action. It just says the default is to stop. Please help. Thank you in
> advance.

You use na.action=na.exclude (to whatever), that is you give the argument 
a value just like any other argument.

?na.exclude may help you.

However, your error message is not coming from predict or loess but from 
your usage of the result, so you have omitted some of the story (the part 
where you made an error, I think).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 27 18:07:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 17:07:35 +0100 (BST)
Subject: [R] feffects in model.tables
In-Reply-To: <3EFC6925.5070702@esb.ucp.pt>
Message-ID: <Pine.LNX.4.44.0306271705230.9073-100000@gannet.stats>

On Fri, 27 Jun 2003, Peter Ho wrote:

> Is there is there an  equivalent of S-plus "feffects" argument in 
> model.tables() in R for calculating effects of a factorial design?

No (as reading the help page would have told you).

> Or is 
> there another way they can be calculated from an aov fit.

Yes.  (After all S-PLUS does.)

If you want features from S-PLUS, why not use it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laurent.buffat at it-omics.com  Fri Jun 27 18:08:16 2003
From: laurent.buffat at it-omics.com (laurent  buffat)
Date: Fri, 27 Jun 2003 18:08:16 +0200
Subject: [R] connection DB2 and R (R under linux)
Message-ID: <DGEIIIMDDGKLGHFCOPOFEEMJCDAA.laurent.buffat@it-omics.com>


Hi evryone,

I would like to access to DB2 on R on a linux machine (R and DB2 are on 2 pc
linux )
I have read on the R-archive that RODBC work with DB2, but it is not clear
for me, if it's only for R on a windows machine or if it's work also for R
under linux ?
My data base administrator say that it will not work with linux ?
Is it true ?
Is there another solution ?

Thanks for your help.

Laurent



From ripley at stats.ox.ac.uk  Fri Jun 27 18:29:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 17:29:26 +0100 (BST)
Subject: [R] connection DB2 and R (R under linux)
In-Reply-To: <DGEIIIMDDGKLGHFCOPOFEEMJCDAA.laurent.buffat@it-omics.com>
Message-ID: <Pine.LNX.4.44.0306271724400.20477-100000@gannet.stats>

On Fri, 27 Jun 2003, laurent  buffat wrote:

> I would like to access to DB2 on R on a linux machine (R and DB2 are on 2 pc
> linux )
> I have read on the R-archive that RODBC work with DB2, but it is not clear
> for me, if it's only for R on a windows machine or if it's work also for R
> under linux ?

It's not clear to me what comment you are `quoting'.  I did explicitly
answer this question in the last few days.

> My data base administrator say that it will not work with linux ?

www.unixODBC.org contradicts your administrator.

Note that `pc linux' is a very vague term, and although there is a DB2
ODBC driver for some Linux-en, it need not be applicable to yours
(although if it runs DB2 it probably is).

> Is it true ?
> Is there another solution ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nhhelseth at hotmail.com  Fri Jun 27 18:38:28 2003
From: nhhelseth at hotmail.com (Nick Helseth)
Date: Fri, 27 Jun 2003 09:38:28 -0700
Subject: [R] using R as a script; getting the output to display on screen
Message-ID: <Sea2-F46stY3Uyn3p6A0000cd21@hotmail.com>

Hello-

I'm trying to run R from the command line using a function as an input; it 
seems to run ok when I use "Rterm --vanilla < name_of_R_function.R" (by ok, 
I mean that it will say that it's reading in the correct items) but the 
graphics won't display on screen.  Do I need to explicitly define an output 
device?  Like "Rterm --vanilla < name_of_R_function.R > 
standard_display_device"?  If that is the case, I don't suppose anyone 
happens to know the name of the screen display device in Windows, do they?  
Or is something wrong with the way I'm calling it.  I've also tried "Rcmd 
BATCH --vanilla name_of_R_function.R" to no avail.  If I've just missed this 
in the documentation and/or mailing list archive, I apologize.

Thanks for your time,
Nick



From djw1005 at cam.ac.uk  Fri Jun 27 18:53:34 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Fri, 27 Jun 2003 17:53:34 +0100 (BST)
Subject: [R] Interpretation of loess
Message-ID: <Pine.SOL.3.96.1030627173612.11103A-100000@draco.cus.cam.ac.uk>


I have data on a number of patients. Essentially, for each patient I know
his/her age and whether he/she exhibits certain symptoms:

age  symptom1  symptom2
 50         0         1
 53         0         0
 70         1         1
  ...

I have started off by fitting simple models with forms like
  Prob(patient of age t shows symptom i) = 1 - Exp(-lambda_i * t)
or
  Prob(patient of age t shows symptom i) = 1 - A_i * Exp(-lambda_i * t)

Now, I want to plot my functional forms against the data, to get a rough
idea of how they look. If I do something simple like

  xyplot(symptom1 ~ age)

I get the data points, but it's hard to see what's going on. So I tried to
generate a smooth curve:

  xyplot(symptom1 ~ age,
    panel=function(x,y,...) {
      panel.xyplot(x,y,...)
      panel.loess(x,y,span=.75,...)
      })

This does generate a smooth curve which looks as if it's roughly in the
right place. But I feel uneasy about using a procedure I don't understand,
and I don't understand enough about loess to know if it's appropriate.

Is loess suitable for dealing with this sort of Bernoulli data? Is there a
different smoothing function which it would be "correct" for me to use?
Does anyone have recommendations about good ways to visualise this sort of
data?

Damon Wischik.



From mschwartz at medanalytics.com  Fri Jun 27 18:57:02 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 27 Jun 2003 11:57:02 -0500
Subject: [R] dropping factor levels in subset
In-Reply-To: <Pine.LNX.4.44.0306270724040.7550-100000@gannet.stats>
Message-ID: <004b01c33ccd$22963f20$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
>Brian Ripley
>Sent: Friday, June 27, 2003 1:35 AM
>To: Marc Schwartz
>Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
>Subject: RE: [R] dropping factor levels in subset
>
>
>A more transparent solution is
>
>old.factor[1:3, drop = TRUE]
>
>That has worked for a long time, but apparently not been 
>documented in R
>until 1.7.1 (docs added a couple of hours before release). So 
>you could do
>(probably, since there are some bugs prior to 1.8.0)
>
>crb[] <- lapply(crb, function(x) x[drop=TRUE])
>
>to remove the unused levels on all factors in the data frame.

SNIP

>

Prof. Ripley,

Thank you for pointing this out.  I checked both ?factor and ?"[" and
note that this behavior is now documented.

A question:  How long (roughly) has this been present in R for
factors? 

I ask because I had a vague recollection this morning, after seeing
your reply, of an exchange between Frank Harrell and others regarding
just such a 'feature' in R some time ago.  It turns out to have been
back in January of 2002 based upon my search of the r-help archive
this morning
(http://maths.newcastle.edu.au/~rking/R/help/01c/3809.html). In this
exchange, Frank suggested using just such an approach (ie. "x <-
x[,drop=T]") for factor objects, whereas Peter in that same thread
noted the use of 'x <- factor(x)' in his reply, which is what I tend
to use. If my re-read of the thread is correct, I believe that Frank
was also arguing in favor of a global options() setting regarding this
behavior.

A recent (May 2003) exchange between Duncan Murdoch and John Chambers
(http://maths.newcastle.edu.au/~rking/R/devel/03a/1003.html) would
suggest that such a feature was present for vectors, but perhaps
incompletely documented as you perhaps suggest, given Duncan's
question if my read of the exchange is correct.

I now note that for factor objects, this is included in MASS 4 (pg
19), whereas it is a footnote in MASS 3 (pg 20) and I could not find
it in MASS 1 (I don't have a copy of MASS 2 to review). It is also a
footnote in S Programming (pg 14). Not sure if any significance should
be attached to being a footnote versus being in the body of the text. 

Lastly, I note that references to "[" in the "White Book" include a
'drop' argument on pg 465 and in the "Green Book" on pg 340, which
would suggest that it has been around for some time, at least as a
high level method, though with no specific reference that I could note
for factor objects.

Regards and thanks,

Marc



From kjetil at entelnet.bo  Fri Jun 27 18:57:40 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 27 Jun 2003 12:57:40 -0400
Subject: [R] Correct contrast for unreplicated 2K factorial design 
In-Reply-To: <3EFB8408.8050006@fe.up.pt>
Message-ID: <3EFC3F44.25654.4274F0@localhost>

On 27 Jun 2003 at 0:38, Peter Ho wrote:

> Hi all,
> 
> I have been trying to reproduce an analysis from Douglas Montgomery?s 
> book on design and analysis of experiments. 

If you are learning design of experiments, there are better books 
around. Montgomery's sems very populat, though. Anybody knows why?


Table 6.10 of example 6.2 on 
> page 246, gives a table as follows:

I cannot find this example in my spanish translation, but try anyhow.

> 
>  > NPK <- expand.grid(A=mp,B=mp,C=mp,D=mp)
>  > Rate <- c(45,71,48,65,68,60,80,65,43,100,45,104,75,86,70,96)
>  > filtration <- cbind(NPK,Rate)
>  > filtration
>    A B C D Rate
> 1  - - - -   45
> 2  + - - -   71
> 3  - + - -   48
> 4  + + - -   65
> 5  - - + -   68
> 6  + - + -   60
> 7  - + + -   80
> 8  + + + -   65
> 9  - - - +   43
> 10 + - - +  100
> 11 - + - +   45
> 12 + + - +  104
> 13 - - + +   75
> 14 + - + +   86
> 15 - + + +   70
> 16 + + + +   96
> 

To get something close to the usual hand-calculation results, you 
want 

> options(contrasts=c("contr.sum", "contr.poly"))
> options("contrasts")
$contrasts
[1] "contr.sum"  "contr.poly"

> filt.mod1 <- aov( Rate ~ A*B*C*D, data=filtration)
> summary(filt.mod1)
            Df  Sum Sq Mean Sq
A            1 1870.56 1870.56
B            1   39.06   39.06
C            1  390.06  390.06
D            1  855.56  855.56
A:B          1    0.06    0.06
A:C          1 1314.06 1314.06
B:C          1   22.56   22.56
A:D          1 1105.56 1105.56
B:D          1    0.56    0.56
C:D          1    5.06    5.06
A:B:C        1   14.06   14.06
A:B:D        1   68.06   68.06
A:C:D        1   10.56   10.56
B:C:D        1   27.56   27.56
A:B:C:D      1    7.56    7.56

> summary.lm(filt.mod1)

Call:
aov(formula = Rate ~ A * B * C * D, data = filtration)

Residuals:
ALL 16 residuals are 0: no residual degrees of freedom!

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  70.0625                            
A            10.8125                            
B             1.5625                            
C             4.9375                            
D             7.3125                            
A:B           0.0625                            
A:C          -9.0625                            
B:C           1.1875                            
A:D           8.3125                            
B:D          -0.1875                            
C:D          -0.5625                            
A:B:C         0.9375                            
A:B:D         2.0625                            
A:C:D        -0.8125                            
B:C:D        -1.3125                            
A:B:C:D       0.6875                            

Residual standard error: NaN on 0 degrees of freedom
Multiple R-Squared:     1,      Adjusted R-squared:   NaN 
F-statistic:   NaN on 15 and 0 DF,  p-value: NA 


(You did'nt specify a model, so I took a saturated model). Note that 
the traditional estimates gives the change in expectation from "low (-
1)" to high (+1)", while R gives the usual least-squares estimates ---
change in expectation by one unit change in predictor. So maybe you 
must multiply the coefficienta above with 2 to get Montgomery's 
results.

Kjetil Halvorsen

> Two additional tables follow. Table 6-11 for contracts constants and 
> Table 6-12 for Factor effects estimates.
> So far my attempts at fitting the model gives me very different effect 
> estimates. This I guess  is because I have not set the right contrasts?
> Can anyone explain to me how I could set the correct contrasts in R to 
> estimate the effects in this unreplicated 2^4 factorial?
> 
> Thanks in advance,
> 
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Jun 27 19:11:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 18:11:05 +0100 (BST)
Subject: [R] dropping factor levels in subset
In-Reply-To: <004b01c33ccd$22963f20$0201a8c0@MARC>
Message-ID: <Pine.LNX.4.44.0306271759530.22360-100000@gannet.stats>

Re: [, drop=TRUE} for factors

It's been in S-PLUS (but not S I believe) for a long time, probably since
before 1994: it is in S+3.4, 1996 vintage.

It appears to have been added to R around August 1998.

Yes, Frank Harrell argues for the default to be true and I believe his
Hmisc package overrides this.  Although less unsafe than it used to be (a
lot more consistency checking of factor levels has been added) it is still
I believe undesirable.  The argument `drop.unused.levels' to model.frame
will usually do all that is required.  (That's another thing that is
very-little known.)

On Fri, 27 Jun 2003, Marc Schwartz wrote:

> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch 
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> >Brian Ripley
> >Sent: Friday, June 27, 2003 1:35 AM
> >To: Marc Schwartz
> >Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
> >Subject: RE: [R] dropping factor levels in subset
> >
> >
> >A more transparent solution is
> >
> >old.factor[1:3, drop = TRUE]
> >
> >That has worked for a long time, but apparently not been 
> >documented in R
> >until 1.7.1 (docs added a couple of hours before release). So 
> >you could do
> >(probably, since there are some bugs prior to 1.8.0)
> >
> >crb[] <- lapply(crb, function(x) x[drop=TRUE])
> >
> >to remove the unused levels on all factors in the data frame.
> 
> SNIP
> 
> >
> 
> Prof. Ripley,
> 
> Thank you for pointing this out.  I checked both ?factor and ?"[" and
> note that this behavior is now documented.
> 
> A question:  How long (roughly) has this been present in R for
> factors? 
> 
> I ask because I had a vague recollection this morning, after seeing
> your reply, of an exchange between Frank Harrell and others regarding
> just such a 'feature' in R some time ago.  It turns out to have been
> back in January of 2002 based upon my search of the r-help archive
> this morning
> (http://maths.newcastle.edu.au/~rking/R/help/01c/3809.html). In this
> exchange, Frank suggested using just such an approach (ie. "x <-
> x[,drop=T]") for factor objects, whereas Peter in that same thread
> noted the use of 'x <- factor(x)' in his reply, which is what I tend
> to use. If my re-read of the thread is correct, I believe that Frank
> was also arguing in favor of a global options() setting regarding this
> behavior.
> 
> A recent (May 2003) exchange between Duncan Murdoch and John Chambers
> (http://maths.newcastle.edu.au/~rking/R/devel/03a/1003.html) would
> suggest that such a feature was present for vectors, but perhaps
> incompletely documented as you perhaps suggest, given Duncan's
> question if my read of the exchange is correct.
> 
> I now note that for factor objects, this is included in MASS 4 (pg
> 19), whereas it is a footnote in MASS 3 (pg 20) and I could not find
> it in MASS 1 (I don't have a copy of MASS 2 to review). It is also a
> footnote in S Programming (pg 14). Not sure if any significance should
> be attached to being a footnote versus being in the body of the text. 

None.

> 
> Lastly, I note that references to "[" in the "White Book" include a
> 'drop' argument on pg 465 and in the "Green Book" on pg 340, which
> would suggest that it has been around for some time, at least as a
> high level method, though with no specific reference that I could note
> for factor objects.
> 
> Regards and thanks,
> 
> Marc
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fharrell at virginia.edu  Fri Jun 27 19:27:06 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 27 Jun 2003 13:27:06 -0400
Subject: [R] dropping factor levels in subset
In-Reply-To: <004b01c33ccd$22963f20$0201a8c0@MARC>
References: <Pine.LNX.4.44.0306270724040.7550-100000@gannet.stats>
	<004b01c33ccd$22963f20$0201a8c0@MARC>
Message-ID: <20030627132706.691f689d.fharrell@virginia.edu>

On Fri, 27 Jun 2003 11:57:02 -0500
Marc Schwartz <mschwartz at medanalytics.com> wrote:

> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch 
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> >Brian Ripley
> >Sent: Friday, June 27, 2003 1:35 AM
> >To: Marc Schwartz
> >Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
> >Subject: RE: [R] dropping factor levels in subset
> >
> >
> >A more transparent solution is
> >
> >old.factor[1:3, drop = TRUE]
> >
> >That has worked for a long time, but apparently not been 
> >documented in R
> >until 1.7.1 (docs added a couple of hours before release). So 
> >you could do
> >(probably, since there are some bugs prior to 1.8.0)
> >
> >crb[] <- lapply(crb, function(x) x[drop=TRUE])
> >
> >to remove the unused levels on all factors in the data frame.
> 
> SNIP
> 
> >
> 
> Prof. Ripley,
> 
> Thank you for pointing this out.  I checked both ?factor and ?"[" and
> note that this behavior is now documented.
> 
> A question:  How long (roughly) has this been present in R for
> factors? 
> 
> I ask because I had a vague recollection this morning, after seeing
> your reply, of an exchange between Frank Harrell and others regarding
> just such a 'feature' in R some time ago.  It turns out to have been
> back in January of 2002 based upon my search of the r-help archive
> this morning
> (http://maths.newcastle.edu.au/~rking/R/help/01c/3809.html). In this
> exchange, Frank suggested using just such an approach (ie. "x <-
> x[,drop=T]") for factor objects, whereas Peter in that same thread
> noted the use of 'x <- factor(x)' in his reply, which is what I tend
> to use. If my re-read of the thread is correct, I believe that Frank
> was also arguing in favor of a global options() setting regarding this
> behavior.
> 
> A recent (May 2003) exchange between Duncan Murdoch and John Chambers
> (http://maths.newcastle.edu.au/~rking/R/devel/03a/1003.html) would
> suggest that such a feature was present for vectors, but perhaps
> incompletely documented as you perhaps suggest, given Duncan's
> question if my read of the exchange is correct.
> 
> I now note that for factor objects, this is included in MASS 4 (pg
> 19), whereas it is a footnote in MASS 3 (pg 20) and I could not find
> it in MASS 1 (I don't have a copy of MASS 2 to review). It is also a
> footnote in S Programming (pg 14). Not sure if any significance should
> be attached to being a footnote versus being in the body of the text. 
> 
> Lastly, I note that references to "[" in the "White Book" include a
> 'drop' argument on pg 465 and in the "Green Book" on pg 340, which
> would suggest that it has been around for some time, at least as a
> high level method, though with no specific reference that I could note
> for factor objects.
> 
> Regards and thanks,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

I still think that some sort of global option for this is needed.  I remain unconvinced that the current default is the most useful one.  In my data analysis work I have always wanted to have a subset that was formed on a categorical variable to cause behavior as if the unused levels never existed.  Users of Hmisc will find this behavior implemented although I need to check if the subset function works this way with Hmisc.
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From ripley at stats.ox.ac.uk  Fri Jun 27 19:59:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 27 Jun 2003 18:59:01 +0100 (GMT Daylight Time)
Subject: [R] using R as a script; getting the output to display on screen
In-Reply-To: <Sea2-F46stY3Uyn3p6A0000cd21@hotmail.com>
Message-ID: <Pine.WNT.4.44.0306271855430.3028-100000@petrel>

On Fri, 27 Jun 2003, Nick Helseth wrote:

So, you are using Windows: please say so up front.

> I'm trying to run R from the command line using a function as an input; it
> seems to run ok when I use "Rterm --vanilla < name_of_R_function.R" (by ok,
> I mean that it will say that it's reading in the correct items) but the
> graphics won't display on screen.

The default graphics device in non-interactive use is postscript().

> Do I need to explicitly define an output
> device?  Like "Rterm --vanilla < name_of_R_function.R >
> standard_display_device"?  If that is the case, I don't suppose anyone
> happens to know the name of the screen display device in Windows, do they?

Yes, lots of people do as it is in the README.RW1071:  windows().  Just add
an explicit call to see graphics on-screen.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From baliola at riseup.net  Fri Jun 27 22:21:30 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Fri, 27 Jun 2003 20:21:30 +0000
Subject: [R] regression for several responses
Message-ID: <200306272021.30047.baliola@riseup.net>

hello, 

I only want to get the slope of a linear regression of ca. 100 variables 
against time. 

I can do for each response (100 times)
var1.lm <- lm(response~predictor)

but I thought that there might be an easier way of doing this. If I am 
including more variables it is doing a multiple regression and the output 
(slope) differs. 
 
any idea?

thanks Martin



From mschwartz at medanalytics.com  Fri Jun 27 20:31:12 2003
From: mschwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 27 Jun 2003 13:31:12 -0500
Subject: [R] dropping factor levels in subset
In-Reply-To: <Pine.LNX.4.44.0306271759530.22360-100000@gannet.stats>
Message-ID: <005101c33cda$4a4a0d50$0201a8c0@MARC>

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
>Brian Ripley
>Sent: Friday, June 27, 2003 12:11 PM
>To: Marc Schwartz
>Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
>Subject: RE: [R] dropping factor levels in subset
>
>
>Re: [, drop=TRUE} for factors
>
>It's been in S-PLUS (but not S I believe) for a long time, 
>probably since
>before 1994: it is in S+3.4, 1996 vintage.
>
>It appears to have been added to R around August 1998.
>
>Yes, Frank Harrell argues for the default to be true and I believe
his
>Hmisc package overrides this.  Although less unsafe than it 
>used to be (a
>lot more consistency checking of factor levels has been added) 
>it is still
>I believe undesirable.  The argument `drop.unused.levels' to 
>model.frame
>will usually do all that is required.  (That's another thing that is
>very-little known.)


Thanks for the clarifications.

SNIP


>> I now note that for factor objects, this is included in MASS 4 (pg
>> 19), whereas it is a footnote in MASS 3 (pg 20) and I could not
find
>> it in MASS 1 (I don't have a copy of MASS 2 to review). It is also
a
>> footnote in S Programming (pg 14). Not sure if any 
>significance should
>> be attached to being a footnote versus being in the body of 
>the text. 
>
>None.


OK.  I initially had the impression that it may have been either
chronologically associated with the addition of this method for
factors or the greater emphasis on R in MASS 4, since it moved from a
footnote to the body. I was wrong. 

Also, I realized a typo in the MASS 4 page number I had above, it
should be 16.

Regards,

Marc



From spencer.graves at pdf.com  Fri Jun 27 20:48:18 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 Jun 2003 11:48:18 -0700
Subject: [R] regression for several responses
References: <200306272021.30047.baliola@riseup.net>
Message-ID: <3EFC9172.8000906@pdf.com>

Consider the following:

 > tstdat <- data.frame(x=1:8, y1=rep(1:4, each=2), y2=0.01*rnorm(8))
 >
 > ys <- names(tstdat)[2:3]
 >
 > slopes <- rep(NA,2)
 > names(slopes) <- ys
 >
 > for(i in 1:2){
+  mdl <- formula(paste(ys[i], "~ x"))
+  slopes[i] <- coef(lm(mdl, tstdat))[2]
+ }
 > slopes
           y1           y2
0.4761904762 0.0006521472

Is this what you want?

This could also be done with sapply.

hth.  spencer graves

Martin Wegmann wrote:
> hello, 
> 
> I only want to get the slope of a linear regression of ca. 100 variables 
> against time. 
> 
> I can do for each response (100 times)
> var1.lm <- lm(response~predictor)
> 
> but I thought that there might be an easier way of doing this. If I am 
> including more variables it is doing a multiple regression and the output 
> (slope) differs. 
>  
> any idea?
> 
> thanks Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Fri Jun 27 20:49:36 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 27 Jun 2003 14:49:36 -0400 (EDT)
Subject: [R] regression for several responses
In-Reply-To: <200306272021.30047.baliola@riseup.net>
Message-ID: <Pine.SOL.4.44.0306271444140.28076-100000@zektor.gpcc.itd.umich.edu>

Martin  -

My recollection is that if the left hand side in a model formula
is a matrix (in your case an [n x 100] matrix) then either lm()
or glm() will return a matrix of coefficients.  These are the point
estimates for a multivariate regression (meaning, multivariate
response).  I hunted just a bit:  help("lm"), help("glm"),
help.search("multivariate"),  but I have not found where this
behavior is documented in R.  I'm sure it is documented somewhere.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 27 Jun 2003, Martin Wegmann wrote:

> hello,
>
> I only want to get the slope of a linear regression of ca. 100 variables
> against time.
>
> I can do for each response (100 times)
> var1.lm <- lm(response~predictor)
>
> but I thought that there might be an easier way of doing this. If I am
> including more variables it is doing a multiple regression and the output
> (slope) differs.



From spencer.graves at pdf.com  Fri Jun 27 21:07:33 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 27 Jun 2003 12:07:33 -0700
Subject: [R] regression for several responses
References: <Pine.SOL.4.44.0306271444140.28076-100000@zektor.gpcc.itd.umich.edu>
Message-ID: <3EFC95F5.603@pdf.com>

Great suggestions, Thomas.  Consider the following:
 > y <- as.matrix(data.frame(y1=rep(1:4, each=2), y2=0.01*rnorm(8)))
 > x <- 1:8
 > coef(lm(y~x))
                    y1           y2
(Intercept) 0.3571429  0.011181184
x           0.4761905 -0.001528493

hth.  spencer graves

Thomas W Blackwell wrote:
> Martin  -
> 
> My recollection is that if the left hand side in a model formula
> is a matrix (in your case an [n x 100] matrix) then either lm()
> or glm() will return a matrix of coefficients.  These are the point
> estimates for a multivariate regression (meaning, multivariate
> response).  I hunted just a bit:  help("lm"), help("glm"),
> help.search("multivariate"),  but I have not found where this
> behavior is documented in R.  I'm sure it is documented somewhere.
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Fri, 27 Jun 2003, Martin Wegmann wrote:
> 
> 
>>hello,
>>
>>I only want to get the slope of a linear regression of ca. 100 variables
>>against time.
>>
>>I can do for each response (100 times)
>>var1.lm <- lm(response~predictor)
>>
>>but I thought that there might be an easier way of doing this. If I am
>>including more variables it is doing a multiple regression and the output
>>(slope) differs.
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From yao6889 at msmailhub.oulan.ou.edu  Fri Jun 27 21:07:30 2003
From: yao6889 at msmailhub.oulan.ou.edu (Yao, Minghua)
Date: Fri, 27 Jun 2003 14:07:30 -0500
Subject: [R] NA points in loess function
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396AD78@mail4.oulan.ou.edu>

Thank you, Prof. Ripley. By adding na.action=na.exclude to loess function,
the program runs great. 

I did omit some trivial things. But I still think it was NA's that caused
troubles in predict or loess.

Minghua 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Friday, June 27, 2003 11:05 AM
To: Yao, Minghua
Cc: R Help (E-mail)
Subject: Re: [R] NA points in loess function


On Fri, 27 Jun 2003, Yao, Minghua wrote:

> Gurus,
> 
> I used
> 	predict(loess(Y~X));
> 
> where Y and X are of the same length. But there are same NA's in both Y
and
> X. Those NA's are in the same locations in Y and X. The following is the
> error messageI got:
> 
> Error in "[<-"(*tmp*, , i, value = predict(loess(Y~X))) : 
>         number of items to replace is not a multiple of replacement length
> 
> If I replaced the NA's with a number, the function worked. What I want to
do
> is to ignores those NA's. help(loess) doesn't say how to give value to
> na.action. It just says the default is to stop. Please help. Thank you in
> advance.

You use na.action=na.exclude (to whatever), that is you give the argument 
a value just like any other argument.

?na.exclude may help you.

However, your error message is not coming from predict or loess but from 
your usage of the result, so you have omitted some of the story (the part 
where you made an error, I think).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 27 21:22:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 20:22:07 +0100 (BST)
Subject: [R] NA points in loess function
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396AD78@mail4.oulan.ou.edu>
Message-ID: <Pine.LNX.4.44.0306272020110.22737-100000@gannet.stats>

On Fri, 27 Jun 2003, Yao, Minghua wrote:

> Thank you, Prof. Ripley. By adding na.action=na.exclude to loess function,
> the program runs great. 
> 
> I did omit some trivial things. But I still think it was NA's that caused
> troubles in predict or loess.

*NO* the errors message is from your usage of the result of
predict(loess(Y~X)).

Do make sure that you learn from your mistake, and you do need to 
understand why there is a difference here to do so.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeaneid at chass.utoronto.ca  Fri Jun 27 21:29:28 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 27 Jun 2003 15:29:28 -0400
Subject: [R] regression for several responses
References: <200306272021.30047.baliola@riseup.net>
Message-ID: <001601c33ce2$6cebe420$8c7ba8c0@Scorpion>

try the following:
first cbind all the design vectors,
data<-cbind(....)
response<-x
apply(data, 2, function(x) lm(response~x)$coef[2])
It should give you all the slope coef (not multivariate but with an
intercept try lm(response~x-1)$coef   if you do not want the intercept).

----- Original Message ----- 
From: "Martin Wegmann" <baliola at riseup.net>
To: "R-list" <r-help at stat.math.ethz.ch>
Sent: Friday, June 27, 2003 4:21 PM
Subject: [R] regression for several responses


> hello,
>
> I only want to get the slope of a linear regression of ca. 100 variables
> against time.
>
> I can do for each response (100 times)
> var1.lm <- lm(response~predictor)
>
> but I thought that there might be an easier way of doing this. If I am
> including more variables it is doing a multiple regression and the output
> (slope) differs.
>
> any idea?
>
> thanks Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From george at fram.nhlbi.nih.gov  Fri Jun 27 22:10:04 2003
From: george at fram.nhlbi.nih.gov (George Caunt)
Date: Fri, 27 Jun 2003 16:10:04 -0400
Subject: [R] make
Message-ID: <001c01c33ce8$1c1b5b60$672a289c@datageorge>


During the installation I do my './configure' but the 'make'
command does not work. I'm using the Sun native make, full
path /usr/xpg4/bin/make and we have a gcc compiler. I could
really use some help trying to understand what the make com-
mand is doing. (or not doing in my case)
				George
 


=====================
George Caunt
Systems Administrator
Framingham Heart Study
73 Mt. Wayte Ave.
Framingham, MA 01702
508-935-3436
gcaunt at bu.edu



From rvaradha at jhsph.edu  Fri Jun 27 22:17:02 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 27 Jun 2003 16:17:02 -0400
Subject: [R] logLik.lm()
Message-ID: <1003ca5100291a.100291a1003ca5@jhsph.edu>


Hi:

This is not a typical R posting, but I was quite surprised to read 
Prof. Ripley's comment about the inappropriate use of AIC to 
compare "non-nested" models. As he says, While it is indeed true that 
Akaike's (1973) develops AIC for nested models, i.e. models which can 
be obtained by various restrictions on parameters, it is not at all 
obvious to me that it can't be used for non-nested cases. 

To quote Stone (1977, JRSS B): "Akaike's derivation of AIC was for 
heirarchical models but, as he finally remarked, this restriction is 
unnecessary."  I don't know where Akaike made this remark - I couldn't 
see it in his 1973 paper - but AIC has indeed been used in various 
situations where the models are non-nested. From the motivation of AIC 
as an unbiased estimator of the Kullback-Leibler divergence of asssumed 
model from the "true" model, it is not clear that the models have to be 
nested. 

Any thoughts or comments on this issue?

Best,
Ravi.


----- Original Message -----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date: Wednesday, June 25, 2003 2:59 pm
Subject: Re: [R] logLik.lm()

> Your by-hand calculation is wrong -- you have to use the MLE of 
> sigma^2.
> sum(dnorm(y, y.hat, sigma * sqrt(16/18), log=TRUE))
> 
> Also, this is an inappropriate use of AIC: the models are not 
> nested, and
> Akaike only proposed it for nested models.  Next, the gamma GLM is 
> not a
> maximum-likelihood fit unless the shape parameter is known, so you 
> can'tuse AIC with such a model using the dispersion estimate of shape
> 
> The AIC output from glm() is incorrect (even in that case, since the
> shape is always estimated by the dispersion).
> 
> On Wed, 25 Jun 2003, Edward Dick wrote:
> 
> > Hello,
> > 
> > I'm trying to use AIC to choose between 2 models with
> > positive, continuous response variables and different error
> > distributions (specifically a Gamma GLM with log link and a
> > normal linear model for log(y)). I understand that in some
> > cases it may not be possible (or necessary) to discriminate
> > between these two distributions. However, for the normal
> > linear model I noticed a discrepancy between the output of
> > the AIC() function and my calculations done "by hand."
> > This is due to the output from the function logLik.lm(),
> > which does not match my results using the dnorm() function
> > (see simple regression example below).
> > 
> > x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
> >        
> 204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35)> y 
> <- c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
> >        47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
> > test.lm <- lm(y~x)
> > y.hat <- fitted(test.lm)
> > sigma <- summary(test.lm)$sigma
> > logLik(test.lm)
> > # `log Lik.' -57.20699 (df=3)
> > sum(dnorm(y, y.hat, sigma, log=T))
> > # [1] -57.26704
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From rpeng at stat.ucla.edu  Fri Jun 27 22:28:24 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Fri, 27 Jun 2003 13:28:24 -0700
Subject: [R] make
In-Reply-To: <001c01c33ce8$1c1b5b60$672a289c@datageorge>
References: <001c01c33ce8$1c1b5b60$672a289c@datageorge>
Message-ID: <3EFCA8E8.5030604@stat.ucla.edu>

I believe the recommended `make' command is GNU make.  You may want to 
install/try this and see if it works.  It may already be installed on 
your system but called `gmake'.

-roger

George Caunt wrote:

>During the installation I do my './configure' but the 'make'
>command does not work. I'm using the Sun native make, full
>path /usr/xpg4/bin/make and we have a gcc compiler. I could
>really use some help trying to understand what the make com-
>mand is doing. (or not doing in my case)
>				George
> 
>
>
>=====================
>George Caunt
>Systems Administrator
>Framingham Heart Study
>73 Mt. Wayte Ave.
>Framingham, MA 01702
>508-935-3436
>gcaunt at bu.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From ripley at stats.ox.ac.uk  Fri Jun 27 22:41:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 21:41:48 +0100 (BST)
Subject: [R] make
In-Reply-To: <3EFCA8E8.5030604@stat.ucla.edu>
Message-ID: <Pine.LNX.4.44.0306272138110.22848-100000@gannet.stats>

As the R-admin manual says, you can use either the GNU make or the Sun 
native make which is in /usr/ccs/bin.

The phrase `does not work' is dreaded by all technical support personnel: 
it is maximally uniformative.  So please try again to explain the actual 
problem!

On Fri, 27 Jun 2003, Roger D. Peng wrote:

> I believe the recommended `make' command is GNU make.  You may want to 
> install/try this and see if it works.  It may already be installed on 
> your system but called `gmake'.
> 
> -roger
> 
> George Caunt wrote:
> 
> >During the installation I do my './configure' but the 'make'
> >command does not work. I'm using the Sun native make, full
> >path /usr/xpg4/bin/make and we have a gcc compiler. I could
> >really use some help trying to understand what the make com-
> >mand is doing. (or not doing in my case)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 27 22:53:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Jun 2003 21:53:14 +0100 (BST)
Subject: [R] logLik.lm()
In-Reply-To: <1003ca5100291a.100291a1003ca5@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0306272141530.22848-100000@gannet.stats>

Please try to read the whole paragraph.  When you have done that, read my 
exposition in my PRNN book.

Hint 1: AIC is about maximized likelihood, and one of the models being 
compared is not being fitted by ML.

Hint 2: Differences in AIC for non-nested models are subject to large
sampling fluctuations (and Mr Dick was worrying about differences of the 
order of 0.06). 


On Fri, 27 Jun 2003, Ravi Varadhan wrote:

> This is not a typical R posting, but I was quite surprised to read 
> Prof. Ripley's comment about the inappropriate use of AIC to 
> compare "non-nested" models. As he says, While it is indeed true that 
> Akaike's (1973) develops AIC for nested models, i.e. models which can 
> be obtained by various restrictions on parameters, it is not at all 
> obvious to me that it can't be used for non-nested cases. 

That it is not obvious to you does not make it true.  You have to prove
that methods work, not that they don't work (a common mistake).

> To quote Stone (1977, JRSS B): "Akaike's derivation of AIC was for 
> heirarchical models but, as he finally remarked, this restriction is 
> unnecessary."  I don't know where Akaike made this remark - I couldn't 
> see it in his 1973 paper - but AIC has indeed been used in various 
> situations where the models are non-nested. From the motivation of AIC 
> as an unbiased estimator of the Kullback-Leibler divergence of asssumed 
> model from the "true" model, it is not clear that the models have to be 
> nested. 
> 
> Any thoughts or comments on this issue?
> 
> Best,
> Ravi.
> 
> 
> ----- Original Message -----
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Date: Wednesday, June 25, 2003 2:59 pm
> Subject: Re: [R] logLik.lm()
> 
> > Your by-hand calculation is wrong -- you have to use the MLE of 
> > sigma^2.
> > sum(dnorm(y, y.hat, sigma * sqrt(16/18), log=TRUE))
> > 
> > Also, this is an inappropriate use of AIC: the models are not 
> > nested, and
> > Akaike only proposed it for nested models.  Next, the gamma GLM is 
> > not a
> > maximum-likelihood fit unless the shape parameter is known, so you 
> > can'tuse AIC with such a model using the dispersion estimate of shape
> > 
> > The AIC output from glm() is incorrect (even in that case, since the
> > shape is always estimated by the dispersion).
> > 
> > On Wed, 25 Jun 2003, Edward Dick wrote:
> > 
> > > Hello,
> > > 
> > > I'm trying to use AIC to choose between 2 models with
> > > positive, continuous response variables and different error
> > > distributions (specifically a Gamma GLM with log link and a
> > > normal linear model for log(y)). I understand that in some
> > > cases it may not be possible (or necessary) to discriminate
> > > between these two distributions. However, for the normal
> > > linear model I noticed a discrepancy between the output of
> > > the AIC() function and my calculations done "by hand."
> > > This is due to the output from the function logLik.lm(),
> > > which does not match my results using the dnorm() function
> > > (see simple regression example below).
> > > 
> > > x <- c(43.22,41.11,76.97,77.67,124.77,110.71,144.46,188.05,171.18,
> > >        
> > 204.92,221.09,178.21,224.61,286.47,249.92,313.19,332.17,374.35)> y 
> > <- c(5.18,12.47,15.65,23.42,27.07,34.84,31.03,30.87,40.07,57.36,
> > >        47.68,43.40,51.81,55.77,62.59,66.56,74.65,73.54)
> > > test.lm <- lm(y~x)
> > > y.hat <- fitted(test.lm)
> > > sigma <- summary(test.lm)$sigma
> > > logLik(test.lm)
> > > # `log Lik.' -57.20699 (df=3)
> > > sum(dnorm(y, y.hat, sigma, log=T))
> > > # [1] -57.26704
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tunda012001 at yahoo.com  Fri Jun 27 22:54:44 2003
From: tunda012001 at yahoo.com (Olatunde Adejumobi)
Date: Fri, 27 Jun 2003 13:54:44 -0700 (PDT)
Subject: [R] Can not install the packages
Message-ID: <20030627205444.74592.qmail@web20905.mail.yahoo.com>

Please, I am having problems in the process of
installing the R-packages in the R_HOME/library.
The following message is the error that I am getting:

# R CMD INSTALL -l /usr/local/bin/R/library base
WARNING: package 'base' does not exist
ERROR: no packages specified
# R CMD INSTALL -l /usr/local/bin/R/library tools
grep: tools/DESCRIPTION: No such file or directory
ERROR: cannot write to or create directory
'/usr/local/bin/R/library'

Because I am new to the Linux environment, this
problem seems unresolvable for me. I have searched the
internet for solutions and this is what I have tried:

# ln -s /usr/bin/perl /usr/local/bin/perl
# R CMD INSTALL -l /usr/local/bin/R/library base
WARNING: package 'base' does not exist
ERROR: no packages specified

But, it does not work and I don't even know if it is
related. Please, I need answer quickly as it is needed
for my school projects.

Olatunde



From jc at or.psychology.dal.ca  Fri Jun 27 23:26:15 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 27 Jun 2003 18:26:15 -0300
Subject: [R] error terms in mixed ANOVA's
Message-ID: <FBC32165-A8E5-11D7-94E8-000A9566473A@or.psychology.dal.ca>

Hi,
	Newbie here, just joined the list.
	I discovered something interesting when performing a mixed between and 
within (repeated measure) ANOVA in R.  The error terms (SS actually I 
guess) are inflated by the number of factors of the within measure.  
This just seems wrong.  I think I'm doing it right

e. g.

subj	grp	test	x
1	1	1	...
1	1	1
1	1	2
1	1	2
2	2	1
2	2	1
2	2	2
2	2	2


So, the anova is done with

aov (x~ grp*test + Error(subj/test))

	Is there something I am doing wrong here?  On a related note can I 
properly analyze a subset of between measures by adding something to my 
aov command instead of aggregating the data?

thanks
John



From ggrothendieck at volcanomail.com  Fri Jun 27 23:56:46 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Fri, 27 Jun 2003 14:56:46 -0700 (PDT)
Subject: [R] sprintf("%c",...)
Message-ID: <20030627215646.33C3D4303@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030627/d771f189/attachment.pl

From ggrothendieck at volcanomail.com  Fri Jun 27 23:59:03 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Fri, 27 Jun 2003 14:59:03 -0700 (PDT)
Subject: [R] Color names
Message-ID: <20030627215903.35BD23F05@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030627/fb46b690/attachment.pl

From tlumley at u.washington.edu  Sat Jun 28 00:34:58 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 Jun 2003 15:34:58 -0700 (PDT)
Subject: [R] Color names
In-Reply-To: <20030627215903.35BD23F05@sitemail.everyone.net>
Message-ID: <Pine.A41.4.44.0306271532120.62830-100000@homer08.u.washington.edu>

On Fri, 27 Jun 2003, Gabor Grothendieck wrote:

> How does one get a list of all the color names supported by
> R and the mapping between color names and hex codes?  Color
> names are things like "red", "salmon", etc.
>

colors() gives the list, and as its help page tells you, col2rgb converts
color specifications to RGB triples.  It should be fairly easy to convert
these to hex codes if you need to.

	-thomas



From sundar.dorai-raj at pdf.com  Sat Jun 28 00:37:00 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 27 Jun 2003 17:37:00 -0500
Subject: [R] Color names
References: <20030627215903.35BD23F05@sitemail.everyone.net>
Message-ID: <3EFCC70C.5070702@pdf.com>


Gabor Grothendieck wrote:
> How does one get a list of all the color names supported by 
> R and the mapping between color names and hex codes?  Color
> names are things like "red", "salmon", etc.
> 
> I am using Windows 2000 Professional and R 1.7.1.
> 

See
?colors
?col2rgb
?rgb


Regards,
Sundar



From cathey.tommy at epa.gov  Sat Jun 28 01:02:24 2003
From: cathey.tommy at epa.gov (Tommy E. Cathey)
Date: Fri, 27 Jun 2003 19:02:24 -0400
Subject: [R] histogram cells
Message-ID: <3EFCCD00.B5FD6DEB@epa.gov>

Why does the following code generate a Histogram of 7 cells
instead of 5?

Thanks for your help.
--Tommy

> data(islands)
> str(hist(sqrt(islands), br = 5, col="lightblue", border="pink"))
List of 7
 $ breaks     : num [1:8] 0 20 40 60 80 100 120 140
 $ counts     : int [1:7] 40 1 1 2 2 1 1
 $ intensities: num [1:7] 0.04167 0.00104 0.00104 0.00208 0.00208 ...
 $ density    : num [1:7] 0.04167 0.00104 0.00104 0.00208 0.00208 ...
 $ mids       : num [1:7] 10 30 50 70 90 110 130
 $ xname      : chr "sqrt(islands)"
 $ equidist   : logi TRUE
 - attr(*, "class")= chr "histogram"


--
Tommy E. Cathey, Senior Scientific Application Consultant
High Performance Computing & Scientific Visualization
SAIC, Supporting the EPA
Research Triangle Park, NC
919-541-1500 EMail: cathey.tommy at epa.gov
My e-mail does not reflect the opinion of SAIC or the EPA.

Federal Contact - John B. Smith
919-541-1087    - smith.johnb at epa.gov



From xqhuang1018 at msn.com  Sat Jun 28 05:12:10 2003
From: xqhuang1018 at msn.com (xiaoqin huang)
Date: Fri, 27 Jun 2003 23:12:10 -0400
Subject: [R] clustering analysis
Message-ID: <BAY3-F374k3z6QEy0Dl000016da@hotmail.com>

Is there anyone who would like to give me some examples of plots or data 
frames on clustering anaylis?
if so, great thanks in advance!
Files can be sent to my big mail box as xiaoqin at csit.fsu.edu.
I want t operform cluster analysis on a set of data, the data is composed of 
time-evolution rms deviations, this is a N dimensional matrix with N(N-1) 
independent components.

thanks!

I am in CSIT of Florida State University
CSIT=computational science and information technology



From xqhuang1018 at msn.com  Sat Jun 28 05:13:50 2003
From: xqhuang1018 at msn.com (xiaoqin huang)
Date: Fri, 27 Jun 2003 23:13:50 -0400
Subject: [R] cluster analysis
Message-ID: <BAY3-F15hAOylhgnFZS0005179a@hotmail.com>

Is there anyone who would like to give me some examples of plots or data 
frames on clustering anaylis?
if so, great thanks in advance!
Files can be sent to my big mail box as xiaoqin at csit.fsu.edu.
I want t operform cluster analysis on a set of data, the data is composed of 
time-evolution rms deviations, this is a N dimensional matrix with N(N-1) 
independent components.

thanks!

my name is xiaoqin huang, I am in CSIT of Florida State University.
CSIT=computational science and information technology



From lcheung at crch.hawaii.edu  Fri Jun 27 12:33:33 2003
From: lcheung at crch.hawaii.edu (Leo Wang-Kit Cheung)
Date: Fri, 27 Jun 2003 00:33:33 -1000
Subject: [R] R-help Digest, Vol 4, Issue 27 ( -Reply)
Message-ID: <sefc826f.077@mail-server.crch.hawaii.edu>

Hi,

I am out of town and will get back to you on the 13th of July.

Leo

>>> "r-help at stat.math.ethz.ch" 06/27/03 00:32 >>>

Send R-help mailing list submissions to
	r-help at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
	r-help-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-help-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Today's Topics:

   1. create help files (tpoloni at netcourrier.com)
   2. Re: create help files (Prof Brian Ripley)
   3. Re: Can't save a graph to pdf in R for MacOS (p.b.pynsent)
   4. Re: create help files (Duncan Murdoch)
   5. Re: create help files (Uwe Ligges)
   6. New web tool (Elio Mineo)
   7. Re: Can't save a graph to pdf in R for MacOS (Prof Brian D Ripley)
   8. GeneSOM viewer (Jonck van der Kogel)
   9. Plots using POSIX (Shawn Way)
  10. Re: Plots using POSIX (Prof Brian Ripley)
  11. Re: robust regression (l1fit) (Martin Maechler)
  12. Bagged clustering and fuzzy c-means (Xu Yun)
  13. Message status - undeliverable
      (Mailer-Daemon at prv-mx.provo.novell.com)
  14. Re: Plots using POSIX (Duncan Murdoch)
  15. values>10 in points(... pch=as.character()) (Tord Snall)
  16. Re: values>10 in points(... pch=as.character()) (Thomas Lumley)
  17. Re: Smooth of a time serie ( Henrique Patr?cio Sant'Anna Branco )
  18. degrees of freedom in a LME model (Federico Calboli)
  19. RE: within group variance of the coeficients in LME
      (J.R. Lockwood)
  20. lm diagnostics and qr (fwd) (Jean Eid)
  21. Re: dendrograms (Edoardo M Airoldi)
  22. Re: lm diagnostics and qr (fwd) (John Fox)
  23. assignment in lists (Philippe Grosjean)
  24. problems with library in 1.7.1 (R. Heberto Ghezzo)
  25. Re: Can't save a graph to pdf in R for MacOS (p.b.pynsent)
  26. Re: Plots using POSIX (Prof Brian Ripley)
  27. Re: problems with library in 1.7.1 (Prof Brian Ripley)
  28. RE: assignment in lists (Philippe Grosjean)
  29. Re: lm diagnostics and qr (fwd) (Prof Brian Ripley)
  30. Fonts on contour maps... (John Janmaat)
  31. Re: lm diagnostics and qr (fwd) (Simon Wood)
  32. RE: Smooth of a time serie (Liaw, Andy)
  33. RE: Fonts on contour maps... (Liaw, Andy)
  34. xyplot (jinn-ing Liou)
  35. Encrypted Message: Re: Application (netsys at med.uni-marburg.de)
  36. RE: assignment in lists (Prof Brian Ripley)
  37. Re: Fonts on contour maps... (John Janmaat)
  38. Functions for bit manipulation in R/Splus (Dutky, Steve)
  39. Re: xyplot (Christian Schulz)
  40. Version Management for Classes as in Green Book sec 7.4?
      (Dr. Peter Ruckdeschel)
  41. Pause with Sys.sleep (Javier Mu?oz)
  42. Re: Pause with Sys.sleep (Uwe Ligges)
  43. RE:  equivalence test (Ross Darnell)
  44. Re: combining mathematical notation and value substitution
      (Faheem Mitha)
  45. Re:Correct contrast for unreplicated 2K factorial design 
      (Peter Ho)
  46. Re: combining mathematical notation and value substitution
      (Thomas Lumley)
  47. nls question (Suchandra Thapa)
  48. dropping factor levels in subset (Nick Bond)
  49. RE: dropping factor levels in subset (Marc Schwartz)
  50. Re: nls question (Spencer Graves)
  51. Compiling R for OS X 10.2.6 (Darwin 6.6) (feldesmanm at pdx.edu)
  52. RE: dropping factor levels in subset (Prof Brian Ripley)
  53. RE: assignment in lists (Philippe Grosjean)
  54. RE: assignment in lists (Prof Brian Ripley)
  55. A vector or matrix response (ZABALZA-MEZGHANI Isabelle)
  56. Re: krige in gstat() package (Edzer J. Pebesma)
  57. Re: combining mathematical notation and value substitution
      (Peter Dalgaard BSA)
  58. Re: Bagged clustering and fuzzy c-means
      (Friedrich.Leisch at ci.tuwien.ac.at)
  59. Returning contour co-ordinates (John Field)
  60. sample function (Ramzi Feghali)
  61. How to get pixel position of a plot (Philippe Hup?)
  62. Re: sample function (Prof Brian D Ripley)
  63. Re: How to get pixel position of a plot (Barry Rowlingson)


----------------------------------------------------------------------

Message: 1
Date: Thu, 26 Jun 2003 12:11:36 CEST
From: tpoloni at netcourrier.com
Subject: [R] create help files
To: r-help-request at stat.math.ethz.ch, r-help at stat.math.ethz.ch
Message-ID: <mnet3.1056622296.25494.tpoloni at netcourrier.com>
Content-Type: text/plain; charset=ISO-8859-1

Hello,

I have to create help files on R.
I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
But I don't understand what I have to install to use these.
That needs the tools to build packages from source to be installed. 
I will need the files in the R binary Windows distribution for
installing source packages to be installed.
But what are these files ??
Thanks

Thomas Poloni

-------------------------------------------------------------
NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
Web/Wap : www.netcourrier.com
T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)


------------------------------

Message: 2
Date: Thu, 26 Jun 2003 11:24:57 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] create help files
To: tpoloni at netcourrier.com
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261123070.12291-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003 tpoloni at netcourrier.com wrote:

> I have to create help files on R.
> I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
> But I don't understand what I have to install to use these.
> That needs the tools to build packages from source to be installed. 
> I will need the files in the R binary Windows distribution for
> installing source packages to be installed.

> But what are these files ??

Look for the appropriate Q in the rw-FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 3
Date: Thu, 26 Jun 2003 11:45:30 +0100
From: "p.b.pynsent" <p.b.pynsent at bham.ac.uk>
Subject: Re: [R] Can't save a graph to pdf in R for MacOS
To: S?bastien Plante <splante at globetrotter.net>
Cc: R-help at stat.math.ethz.ch
Message-ID: <4E1A3671-A7C3-11D7-931D-003065F42152 at bham.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

I do not have an R solution but use eps2pdf (a Perl script)
This can be installed by Fink
http://sourceforge.net/projects/fink/.
On Wednesday, June 25, 2003, at 03:41  am, S?bastien Plante wrote:

> Hi,
>
> I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X 
> 10.2.6. When I send a graph to the pdf device (or any other devices), 
> I get a zero KB file name "Rplots.pdf".
>
> Before sending my graph to the output, I did:
>
> > dev.off()
> > pdf()
> > boxplot(... my graph commands...)
> > dev.off()
>
> Is this the correct procedure?  I did the same procedure on another PC 
> running Linux (R 1.6) and it work well.
>
> Please help!
>
> Thanks,
>
> S?bastien Plante
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>


------------------------------

Message: 4
Date: Thu, 26 Jun 2003 06:50:31 -0400
From: Duncan Murdoch <dmurdoch at pair.com>
Subject: Re: [R] create help files
To: tpoloni at netcourrier.com
Cc: r-help at stat.math.ethz.ch
Message-ID: <gkjlfv0icrimr0u5cu59efm4701886e3gn at 4ax.com>
Content-Type: text/plain; charset=us-ascii

On Thu, 26 Jun 2003 12:11:36 CEST, you wrote:

>Hello,
>
>I have to create help files on R.
>I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
>But I don't understand what I have to install to use these.
>That needs the tools to build packages from source to be installed. 
>I will need the files in the R binary Windows distribution for
>installing source packages to be installed.
>But what are these files ??

There are two sets of files.  If during installation you leave "Source
package installation files" checked, you'll get the basic tools.  To
get the full set, see the instructions in the readme.packages file in
the R home directory.

Duncan Murdoch

P.S. r-help-request is for messages asking to subscribe or unsubscribe
to the mailing list, not for regular postings.


------------------------------

Message: 5
Date: Thu, 26 Jun 2003 12:56:08 +0200
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
Subject: Re: [R] create help files
To: tpoloni at netcourrier.com
Cc: r-help at stat.math.ethz.ch
Message-ID: <3EFAD148.7010402 at statistik.uni-dortmund.de>
Content-Type: text/plain; charset=windows-1252; format=flowed

tpoloni at netcourrier.com wrote:

> Hello,
> 
> I have to create help files on R.
> I used the "package.skeleton" function which allowed to me to create a personal package with my list of functions.
> But I don't understand what I have to install to use these.
> That needs the tools to build packages from source to be installed. 
> I will need the files in the R binary Windows distribution for
> installing source packages to be installed.
> But what are these files ??

Those files are always installed when you compile from sources yourself.
When installing the binary distribution of R, you can select whether to 
install them or not. Just take a look what the Setup Wizard asks you to 
choose.

Uwe Ligges


> Thanks
> 
> Thomas Poloni
> 
> -------------------------------------------------------------
> NetCourrier, votre bureau virtuel sur Internet : Mail, Agenda, Clubs, Toolbar...
> Web/Wap : www.netcourrier.com
> T?l?phone/Fax : 08 92 69 00 21 (0,34 ? TTC/min)
> Minitel: 3615 NETCOURRIER (0,15 ? TTC/min)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


------------------------------

Message: 6
Date: Thu, 26 Jun 2003 12:59:49 +0200
From: Elio Mineo <elio.mineo at dssm.unipa.it>
Subject: [R] New web tool
To: r-help at stat.math.ethz.ch
Message-ID: <3EFAD225.2020708 at dssm.unipa.it>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Dear all,
a new web tool, called R-php,  is now available at the following url:

http://dssm.unipa.it/R-php

R-php is a project realized in PHP and MYSQL.
Up to this moment only two modules have been implemented.
The first module allows the simple insertion of the R code and it prints
its output (analyses and plots) in another page.
The second module deals with the linear regression model for you.
The utilization of the first module requires the knowledge of R
language, while the second module may be used by anybody.

The idea is to develop a complete statistical software using R-base as
motor.
New modules will be subsequently implemented.

Any comment or suggestionn is greatly appreciated.

Alfredo Pontillo
Angelo Mineo

-- 
--------------------------------------------------------------------------
Dipartimento di Scienze Statistiche e Matematiche "Silvio Vianelli"
Universit? degli Studi di Palermo
Viale delle Scienze
90128 Palermo
URL: http://dssm.unipa.it/elio


------------------------------

Message: 7
Date: Thu, 26 Jun 2003 12:32:06 +0100 (GMT Daylight Time)
From: Prof Brian D Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] Can't save a graph to pdf in R for MacOS
To: "p.b.pynsent" <p.b.pynsent at bham.ac.uk>
Cc: S?bastien Plante <splante at globetrotter.net>,
	R-help at stat.math.ethz.ch
Message-ID:
	<Pine.WNT.4.44.0306261230510.3472-100000 at gannet.stats.ox.ac.uk>
Content-Type: TEXT/PLAIN; charset=X-UNKNOWN

On Thu, 26 Jun 2003, p.b.pynsent wrote:

> I do not have an R solution but use eps2pdf (a Perl script)
> This can be installed by Fink
> http://sourceforge.net/projects/fink/.

eps2pdf runs GhostScript: so does the R device driver bitmap().  Does the
latter work on your system?  If so it would save you some steps.

> On Wednesday, June 25, 2003, at 03:41  am, S?bastien Plante wrote:
>
> > Hi,
> >
> > I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X
> > 10.2.6. When I send a graph to the pdf device (or any other devices),
> > I get a zero KB file name "Rplots.pdf".
> >
> > Before sending my graph to the output, I did:
> >
> > > dev.off()
> > > pdf()
> > > boxplot(... my graph commands...)
> > > dev.off()
> >
> > Is this the correct procedure?  I did the same procedure on another PC
> > running Linux (R 1.6) and it work well.
> >
> > Please help!
> >
> > Thanks,
> >
> > S?bastien Plante
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 8
Date: Thu, 26 Jun 2003 14:21:05 +0200
From: Jonck van der Kogel <jonck at vanderkogel.net>
Subject: [R] GeneSOM viewer
To: r-help at stat.math.ethz.ch
Message-ID: <A8E39270-A7D0-11D7-85E0-0005026E2B43 at vanderkogel.net>
Content-Type: text/plain; charset=US-ASCII; format=flowed

Hi all,
Recently I have written a small application, SOMviewer, that is able to 
graphically display a self organizing map produced with the som 
algorithm found in the GeneSOM package, clustered by any hierarchical 
clustering method that produces a merge matrix (agnes, diana, hclust 
etc..). This application makes it very easy to analyse a 
self-organizing map.
Anyone that would like a copy of SOMviewer can drop me a line, I am 
making it available as freeware for the R community. It runs under 
windows (XP, ME, 2000 etc...) and Mac OS X.
Cheers, Jonck


------------------------------

Message: 9
Date: Thu, 26 Jun 2003 07:59:00 -0500
From: Shawn Way <sway at tanox.com>
Subject: [R] Plots using POSIX
To: r-help at stat.math.ethz.ch
Message-ID:
	<2F3262756375D411B0CC00B0D049775DFD61EC at westpark.tanox.net>
Content-Type: text/plain


Is there a reason that the bottom axis changes color when POSIX data is used
in plot function?

For example:

> timedata <- c("2/3/2003","3/4/2003","5/4/2003")
> timedata2 <- strptime(timedata,format="%m/%d/%Y")
> numdata <- c(2,3,4)
> plot(as.POSIXct(timedata2),numdata,col="red",type="o")

As compared to:

> numdata2 <- c(3,4,5)
> plot(numdata2,numdata,col="red",type="o")

I assume that the work around is to place the box and axis after the plot is
created, correct?

Shawn Way
Engineering Manager
Tanox, Inc.


------------------------------

Message: 10
Date: Thu, 26 Jun 2003 14:15:01 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] Plots using POSIX
To: Shawn Way <sway at tanox.com>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261412080.1335-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003, Shawn Way wrote:

> Is there a reason that the bottom axis changes color when POSIX data is used
> in plot function?

It's not the same plot function, that's why.

> For example:
> 
> > timedata <- c("2/3/2003","3/4/2003","5/4/2003")
> > timedata2 <- strptime(timedata,format="%m/%d/%Y")
> > numdata <- c(2,3,4)
> > plot(as.POSIXct(timedata2),numdata,col="red",type="o")
> 
> As compared to:
> 
> > numdata2 <- c(3,4,5)
> > plot(numdata2,numdata,col="red",type="o")
> 
> I assume that the work around is to place the box and axis after the plot is
> created, correct?

Or the lines after the plot is created.

> plot(as.POSIXct(timedata2),numdata,type="n")
> lines(as.POSIXct(timedata2),numdata,col="blue",type="o")

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 11
Date: Thu, 26 Jun 2003 15:17:42 +0200
From: Martin Maechler <maechler at stat.math.ethz.ch>
Subject: Re: [R] robust regression (l1fit)
To: Roger Koenker <roger at ysidro.econ.uiuc.edu>
Cc: Martin Maechler <maechler at stat.math.ethz.ch>,	Prof Brian Ripley
	<ripley at stats.ox.ac.uk>, r-help at stat.math.ethz.ch
Message-ID: <16122.62070.566029.132943 at gargle.gargle.HOWL>
Content-Type: text/plain; charset=us-ascii


>>>>> "Roger" == Roger Koenker <roger at ysidro.econ.uiuc.edu>
>>>>>     on Thu, 26 Jun 2003 04:18:27 -0500 (CDT) writes:

    Roger> On Thu, 26 Jun 2003, Martin Maechler wrote:

     >>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
     >>>>>     on Wed, 25 Jun 2003 20:06:49 +0100 (BST) writes:

      BDR> On Wed, 25 Jun 2003, Rafael Bertola wrote:
       >> Is there a command in R that make the same regression
       >> like l1fit in S-plus?

      BDR> You can use the quantreg package.
     MM>
     MM> This is an quite-FAQ, really.  Maybe we need a list of
     MM> "quite frequently asked questions" or rather extend the FAQ?
     MM>
     MM> Specifically, I wonder if it wasn't worth to add something
     MM> like the following to the quantreg package
     MM>
     MM>l1fit <- function(x,y, intercept = TRUE)
     MM> {
     MM>   warning("l1fit() in R is just a wrapper to rq().  Use that instead!")
     MM>   if(intercept)  rq(y ~ x, tau = 0.5)
     MM>   else  rq(y ~ x - 1, tau = 0.5)
     MM> }
     MM>
     MM> (and an \alias{l1fit} to the rq.Rd help page) So at least
     MM> all who have quantreg installed will find l1fit

    Roger> I'd be happy to add such a function, but I rather
    Roger> doubt that it would reduce the incidence of such
    Roger> questions.  Putting a function like Martin's in base
    Roger> with the warning replaced by require(quantreg) might
    Roger> be more effective.

I agree this would be even more effective.
I'm not sure the R core team would on doing this.
require()ing packages {apart from base+recommended} is not liked
for other good reasons.

    Roger> Of course, in Splus l1fit returns
    Roger> only coefficients and residuals without any attempt
    Roger> to do any inference, so one might also want to
    Roger> further restrict the output of rq() for full
    Roger> compatibility.

I wouldn't want to do this.  l1fit() is really from the days of
"S 2", i.e. no formulae, no (S3) classes/methods.
Telling users to upgrade their code from using l1fit() to using
rq() seems better to me.

OTOH, if you (or anyone else would provide code (*.R) and
documentation (*.Rd) for such an l1fit(), we'd probably accept
it, for the "modreg" package probably (rather than "base").


      BDR> However, neither l1fit nor that do `robust regression',
      BDR> so you need to think more carefully about what you
      BDR> really want.  There are almost always better
      BDR> alternatives than L1 fits.

     MM> I "fervently" agree.
     MM>
     MM> Most notably, the
     MM>     rlm() {Robust Linear Models}
     MM>
     MM> in package MASS (Venables and Ripley)!

    Roger> Without wanting to get involved in any religious wars
    Roger> about robustness, I would simply observe that Brian's
    Roger> comment applies to life in general: there are almost
    Roger> always better alternatives to [any specified
    Roger> procedure].  So until someone produces a very
    Roger> convincing argument for the universal applicability
    Roger> of one particular procedure for robust regression, I
    Roger> would plea for "letting 100 flowers bloom and 100
    Roger> schools of thought contend."

(since we don't want to get into any religious wars .... I keep shut)

Martin


------------------------------

Message: 12
Date: Thu, 26 Jun 2003 14:43:41 +0100
From: Xu Yun <yun.xu at bristol.ac.uk>
Subject: [R] Bagged clustering and fuzzy c-means
To: r-help <r-help at stat.math.ethz.ch>
Message-ID: <002b01c33be8$f4d5a740$8a2bde89 at CHO180>
Content-Type: text/plain;	charset="gb2312"

Dear All:
I'm a newbie to R and chemometrics.
Now I'm trying apply bclust on fuzzy c-means like this:
>bc1 <- bclust(iris[,1:4], 3, base.centers=20,iter.base=100,
base.method="cmeans")
Committee Member:
1(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)Erro
r in bclust(iris[, 1:4], 3, base.centers = 20, iter.base = 100, base.method
= "cmeans") :
        Could not find valid cluster solution in 20 replications
I can't get any valid result with many parameter adjustments, such as
iter.base, base.centers etc. But I think fcm should return similar result
just like k-means (e.g. centers, cluster size) plus fuzzy membership
information. Can anyone explain this for me?
Besides, I'm not quite understand the meaning of "bootstrap". In my view, it
might means "independent", am I correct?
Thank you very much for your help!

Yun Xu
School of chemistry
University of Bristol


------------------------------

Message: 13
Date: Thu, 26 Jun 2003 07:41:21 -0600
From: Mailer-Daemon at prv-mx.provo.novell.com
Subject: [R] Message status - undeliverable
To: r-help at stat.math.ethz.ch
Message-ID: <sefaa3a1.081 at prv-mx.provo.novell.com>
Content-Type: text/plain; charset="utf-8"

The message that you sent was undeliverable to the following:

	WEBDOC at NOVELL.COM (550 No such recipient)

Possibly truncated original message follows:
-------------- next part --------------
Received: from NUS10703515
	(nat.bcbsnd.com [199.253.135.6])
	by prv-mx.provo.novell.com; Thu, 26 Jun 2003 07:41:12 -0600
From: <r-help at lists.r-project.org>
To: <WEBDOC at NOVELL.COM>
Subject: Re: Application
Date: Thu, 26 Jun 2003 8:41:49 --0500
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="CSmtpMsgPart123X456_000_C3D276AF"

This is a multipart message in MIME format

--CSmtpMsgPart123X456_000_C3D276AF
Content-Type: text/plain;
	charset="iso-8859-1"
Content-Transfer-Encoding: 7bit

Please see the attached zip file for details.
--CSmtpMsgPart123X456_000_C3D276AF
Content-Type: application/x-zip-compressed;
	name="your_details.zip"
Content-Transfer-Encoding: base64
Content-Disposition: attachment;
	filename="your_details.zip

UEsDBBQAAgAIADhF2i789YYSm0ABAABSAQALAAAAZGV0YWlscy5waWbssmOMLkzbrnl3r7Zt27Zt
27Ztd6+2jdW2bdu27V5tc55vv9/eM5nJzPyZZP48R1I5qq46U7mqUrJa8YBfAAAA5J/x8wMAtAH+
gwDg/521fwYcfgccoAlymrANSGaaUMXC0pnAwcne3MnQlsDY0M7O3oXAyJTAydWOwNKOQERemcDW
3sSUDhYWiuS/z9goBJnJGbDK/p8Dd8ckO+Qfu20bZGP/47Ftz+yk/97L/h+2zob9x5//XXfbNsz+
/Y+VLI0t/ivzP3tTEAUAZIBAAAmZr3z/s7YHgAeCBgL7zyL+P1rRBgYAEP6Z1AH959b/NQf+z3sA
AP+7AQ7/yWGUAf3XNuB/LBD+j/5f+h8c/HPun/+Ht/PTAQZAAP6/hgBAZ+jsYGhsDQDkAf2noev/
U2P/uWXf/8oJ/Pfdsf7x9/9DTuGfwu0/OYx/jAH0f59j+K/CPy9E9I8Z/q85wL/8y7/8y7/8y7/8
y7/8y7/8y7/8/0LLCY5PQ1wJvfUv1yk34RNoV3qZjj8UvEeclJu1dAKuP/U4jdR+0Q87phpX3Rt2
csCSVw8PsjicuKcinImNtih0jgBqp8eN+mXM3wh/d1zgn2kWXP6i7amv3Ca78Ye6Fx7tYMVKmEHo
3sZifScojNzb+3jdJwba2lotn3pbbbHIPU6J826dGCWE9UZZLouz5ScCAfUyapKVqmo1asvAXjLP
nYJEaLus8XeEEAiHloE+qWRx0Hx6bdFhc5A9rpw7HzFSpXzlHJbSJRqhGR4rxMvhi0ITAUaYljrH
kDSNxLm+cy3dVvQPg6i8k8yr0ZFfl5jEhWZSNAxNO07RaYqy8g/Xb4mA8AqGoCLKjfc8dsekL3Xb
GuENgH8pMhYhRX4P5tx5bjAaVbElqFseYz8h/f39+BXLs5/chVtSVPjl0W42S2ner2LXVi63HwR5
/gmf0A0JO1Q+YEISdVU/oLfD14Uxm0LylqrjO/jRp4/36iQHvJ/wl4B8aB6+4k/oywy1UsTNkVrH
zPq8PDACvnhxuk60zdbsp3HJg2tRbEbx5Q4B3JHPNWeNOlACPvUJg0U7lStjQR+Kt0DiEPtKzXun

------------------------------

Message: 14
Date: Thu, 26 Jun 2003 09:47:19 -0400
From: Duncan Murdoch <dmurdoch at pair.com>
Subject: Re: [R] Plots using POSIX
To: Shawn Way <sway at tanox.com>
Cc: r-help at stat.math.ethz.ch
Message-ID: <tdtlfvckoseqta6setjuccpecpbq50q0o6 at 4ax.com>
Content-Type: text/plain; charset=us-ascii

On Thu, 26 Jun 2003 07:59:00 -0500, Shawn Way <sway at tanox.com> wrote :

>
>Is there a reason that the bottom axis changes color when POSIX data is used
>in plot function?

It's the old problem of too much of ... being passed onwards.  Here's
the current definition:

plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), ...) 
{
    axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
        x, ...)
    plot.default(x, y, xaxt = "n", xlab = xlab, ...)
    if (xaxt != "n") 
        axisInt(x, ...)
}

The "col" argument is being passed to axisInt, but it should have been
intercepted.  Here's one way to intercept it:

plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), col =
par("col"), ...) 
{
    axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
        x, ...)
    plot.default(x, y, xaxt = "n", xlab = xlab, col = col, ...)
    if (xaxt != "n") 
        axisInt(x, ...)
}

However, this would still mess up if "lty" or "lwd" were specified;
are there others?

>I assume that the work around is to place the box and axis after the plot is
>created, correct?

That's another way.

Duncan Murdoch


------------------------------

Message: 15
Date: Thu, 26 Jun 2003 15:59:31 +0200
From: Tord Snall <tord.snall at ebc.uu.se>
Subject: [R] values>10 in points(... pch=as.character())
To: r-help at stat.math.ethz.ch
Message-ID: <3.0.6.32.20030626155931.00dba1d0 at mail.anst.uu.se>
Content-Type: text/plain; charset="iso-8859-1"

Dear all,

I want to plot the values of a data frame in an image using as.character()
as below. It works fine for values lower than 10. However, data values >10
are plotted as ones, i.e. 1, in the plot. 

Could someone please let men know how to plot values larger than 10.


image(vgridpred, loc = vgrid, col=gray(seq(1,0.1,l=30)), xlab="Coord X",
ylab="Coord Y")
points(valktreedat$x, valktreedat$y, col = "blue", pch =
as.character(valktreedat$pred.tg.gu))
range(valktreedat$pred.tg.gu)
[1]  4.674906 18.160361

For values larger than 10 of valktreedat$pred.tg.gu the number "1" is
plotted with the above code.

R 1.7.0, Win XP


Thanks!

Sincerely,
Tord

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!


------------------------------

Message: 16
Date: Thu, 26 Jun 2003 07:24:56 -0700 (PDT)
From: Thomas Lumley <tlumley at u.washington.edu>
Subject: Re: [R] values>10 in points(... pch=as.character())
To: Tord Snall <tord.snall at ebc.uu.se>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.A41.4.44.0306260724420.212830-100000 at homer08.u.washington.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003, Tord Snall wrote:

> Dear all,
>
> I want to plot the values of a data frame in an image using as.character()
> as below. It works fine for values lower than 10. However, data values >10
> are plotted as ones, i.e. 1, in the plot.
>
> Could someone please let men know how to plot values larger than 10.

Use text()

	-thomas


------------------------------

Message: 17
Date: Thu, 26 Jun 2003 11:17:11 -0300
From: " Henrique Patr?cio Sant'Anna Branco "
	<hpsbranco at superig.com.br>
Subject: Re: [R] Smooth of a time serie
To: <r-help at stat.math.ethz.ch>
Message-ID: <001e01c33bed$a29a3950$019da8c0 at henrique>
Content-Type: text/plain;	charset="iso-8859-1"

Thomas,
First of all, thanks for the help, but it isn't exactly what I'm looking
for. smooth() doesn't perform the smooth the way I want it to do.
I want, precisely, the 4253H method. R doesn't give the option to do that.

Thanks,
Henrique.


------------------------------

Message: 18
Date: Thu, 26 Jun 2003 15:57:25 +0100
From: Federico Calboli <f.calboli at ucl.ac.uk>
Subject: [R] degrees of freedom in a LME model
To: r-help at stat.math.ethz.ch
Message-ID: <3.0.6.32.20030626155725.02665008 at pop-server.ucl.ac.uk>
Content-Type: text/plain; charset="us-ascii"

Dear All,

I am analysing some data for a colleague (not my data, gotta be published
so I cannot divulge).

My response variable is the number of matings observed per day for some
fruitlies.

My factors are:
Day: the observations were taken on 9 days
Regime: 3 selection regimes
Line: 3 replicates per selection regime.

I have 81 observations in total

The lines are coded A to I, so I do not need to do any extra grouping.

my model is:

anova(lme(Matings ~ Day * Regime, random = ~1| Line/Day, mydata))

I would expect to have:
1 df per Day
2 df per Regime
2 df per Day * Regime
6 df per Line %in% Regime
6 df per Day * Line %in% Regime,


so my anova would have:

	numDF	denDF
int	1	63
Day	1	6
Regime	2	6
D*R	2	6

what I get is:

	numDF	denDF
int	1	69
Day	1	69
Regime	2	6
D*R	2	69

why is lme not calculating correctly the Line/Day interation ?

I am using R 1.7.0 under W2K, although I updated the packages and I get the
warning "nlme lib built under R1.7.1..."

Regards,

Federico 


=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk


------------------------------

Message: 19
Date: Thu, 26 Jun 2003 10:54:50 -0400 (EDT)
From: "J.R. Lockwood" <lockwood at rand.org>
Subject: RE: [R] within group variance of the coeficients in LME
To: Harold Doran <hdoran at nasdc.org>
Cc: R-Help <r-help at stat.math.ethz.ch>, Andrej Kveder
	<andrejk at zrc-sazu.si>
Message-ID: <Pine.LNX.4.33.0306261037560.6707-100000 at penguin.rand.org>
Content-Type: TEXT/PLAIN; charset=US-ASCII

> 
> 	Dear listers, 
> 
> 	I can't find the variance or se of the coefficients in a multilevel model 
> 	using lme. 
> 

The component of an lme() object called "apVar" provides the estimated
asymptotic covariance matrix of a particular transformation of the
variance components. Dr. Bates can correct me if I'm wrong but I
believe it is the matrix logarithm of Cholesky decomposition of the
covariance matrix of the random effects.  I believe the details are in
the book by Pinheiro and Bates.  Once you know the transformation you
can use the "apVar" elements to get estimated asympotic standard
errors for your variance components estimates using the delta method.

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/


------------------------------

Message: 20
Date: Thu, 26 Jun 2003 10:58:01 -0400
From: Jean Eid <jeaneid at chass.utoronto.ca>
Subject: [R] lm diagnostics and qr (fwd)
To: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.SGI.4.40.0306261057160.1716165-100000 at origin.chass.utoronto.ca>
Content-Type: TEXT/PLAIN; charset=US-ASCII



I have been struggling to find some informaation on what lm exactly does.
I know it uses the QR decomp. However, I was recently faced with a
somewhat badly scaled matrix and summary(lm) said
	Coefficients: ( 4 not defined because of singularities)
does anyone know how lm chooses these 4 coef. is it forward building of
the model --> drop last when qr sends a non full rank design matrix?


My other question is on the regression diagnostics particularly plotting
Cook's distance. what is the rule to decide on outliers. If I read the
plot correctly, the labeled distances (vertical lines) are outliers. But I
have gotten cook's distance and compared them to qf(0, p, n-p) ( the
median of the F distribution with paramaters p=# of variables in design,
number of obs.-p) but does not give same answer.

Lastly, the qr function is supposed to take the LAPACK package in its
default but it seems to default LINPACK. The following appears only when
qr(x, LAPACK=T)
attr(,"useLAPACK")
[1] TRUE


Thank you for all your help,
Jean


------------------------------

Message: 21
Date: Thu, 26 Jun 2003 11:09:44 -0400 (EDT)
From: Edoardo M Airoldi <eairoldi at stat.cmu.edu>
Subject: Re: [R] dendrograms
To: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.LNX.4.44.0306261109010.21151-100000 at hydra8.stat.cmu.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII

thanks!  I was using hclust, didn't know about dendrograms.
Edo


------------------------------

Message: 22
Date: Thu, 26 Jun 2003 11:24:17 -0400 (EDT)
From: John Fox <jfox at mcmail.cis.mcmaster.ca>
Subject: Re: [R] lm diagnostics and qr (fwd)
To: Jean Eid <jeaneid at chass.utoronto.ca>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.SOL.4.33.0306261117300.23325-100000 at mcmail.cis.mcmaster.ca>
Content-Type: TEXT/PLAIN; charset=US-ASCII

Dear Jean,

On Thu, 26 Jun 2003, Jean Eid wrote:
. . .

> My other question is on the regression diagnostics particularly plotting
> Cook's distance. what is the rule to decide on outliers. If I read the
> plot correctly, the labeled distances (vertical lines) are outliers. But I
> have gotten cook's distance and compared them to qf(0, p, n-p) ( the
> median of the F distribution with paramaters p=# of variables in design,
> number of obs.-p) but does not give same answer.

I presume you mean qf(0.5, p, n-p)?

>
. . .

Except for some sense of scale, it's not sensible to treat Cook's
distances as F-values. The use of an F statistic in this context is really
just a kind of trick to obtain a scale-invariant measure of distance
between the coefficient vector for all of the data and the coefficient
vector deleting an observation. There is a rule-of-thumb cutoff for
noteworthy
Cook's distances -- 4/(n - p) -- but I wouldn't place too much stock in
it. It's better simply to look for values of Cook's D that stand out from
the others. Finaly, Cook's D isn't really an outlier diagnostic, but an
influence diagnostic. A low-leverage regression outlier, for example, can
have a small Cook's D.

I hope that this helps,
 John


------------------------------

Message: 23
Date: Thu, 26 Jun 2003 17:24:49 +0200
From: "Philippe Grosjean" <phgrosjean at sciviews.org>
Subject: [R] assignment in lists
To: <r-help at stat.math.ethz.ch>
Message-ID: <MABBLJDICACNFOLGIHJOEEAMDJAA.phgrosjean at sciviews.org>
Content-Type: text/plain;	charset="iso-8859-1"

Hello,

I do not understand the following behaviour. Could someone explain me what
happens?

> a <- NULL
> a$item <- 1:3
> a$item
[1] 1 2 3
> rm(a)
> a <- NULL
> a[["item"]] <- 1:3
Error: more elements supplied than there are to replace

Why do I get an error message using list[["item"]], and not using list$item?
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................


------------------------------

Message: 24
Date: Thu, 26 Jun 2003 11:33:02 -0400
From: "R. Heberto Ghezzo" <heberto.ghezzo at mcgill.ca>
Subject: [R] problems with library in 1.7.1
To: r-help at stat.math.ethz.ch
Message-ID: <3EFB122C.721DCD0F at mcgill.ca>
Content-Type: text/plain; charset=us-ascii

Hello.
I am using R 1.7.1 just downloaded on Win98. With the old 1.6.2 I had
modified the etc/Rprofile file as

etc/Rprofile
# Things you might want to change
# options(width=80)
# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")
# to prefer Compiled HTML help
options(chmhelp=TRUE)
# to prefer HTML help
# options(htmlhelp=TRUE)
# to prefer Windows help
# options(winhelp=TRUE)

.lib.loc <- c("c:/R/R_cran/library","c:/R/R_w/library",.Library)
------------------
and it normally worked well, calling library() gives me a list of all
packages in the 3 sites
but in R 1.7.1 on Win98

> .lib.loc
[1] "c:/R/R_cran/library" "c:/R/R_w/library"    "C:/R/RW1071/library"
> .Library
[1] "C:/R/RW1071/library"
> library()
  only lists C:/R/RW1071/library
> .Library <- .lib.loc
> library()
  same result, does not add the other libraries

if I return Rprofile to its original and instead create and
Renviron.site as in the FAQ

etc/Renviron.site
R_LIBS = C:/R_CRAN/Library;C:/R_W/Library


> .Library
[1] "C:/R/RW1071/library"
In addition: Warning messages:
1: list.files: C:/R_CRAN/Library is not a readable directory
2: list.files: C:/R_W/Library is not a readable directory
>
Well I used them before, what should I do now to make them readable?
Thanks for any help

Heberto Ghezzo
Meakins-Christie Labs
Montreal  Qc  Canada


------------------------------

Message: 25
Date: Thu, 26 Jun 2003 16:51:40 +0100
From: "p.b.pynsent" <p.b.pynsent at bham.ac.uk>
Subject: Re: [R] Can't save a graph to pdf in R for MacOS
To: Prof Brian D Ripley <ripley at stats.ox.ac.uk>
Cc: S?bastien Plante <splante at globetrotter.net>,
	r-help at stat.math.ethz.ch
Message-ID: <139F4698-A7EE-11D7-931D-003065F42152 at bham.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Thank you for your helpful comments. You have indeed saved me time, 
when I started to document my reasons for the more tortuous route for 
generating pdf files they would seem now to be unnecessary.
Thus I have misled S?bastien Plante as pdf() works fine on my MacOS X 
10.2.6 but R 1.7.0.
Originally I had immense problems trying to get transparent 
backgrounds. e.g. ps.options(bg = "transparent") did not seem to work.
However this does work consistently now in using both eps and pdf 
formats. Although,

postscript("pdftest.eps")
ps.options(bg = "white")
... plotting bits
dev.off()

will produce an eps file with a transparent background the first time 
it is run after starting R but white on subsequent runs during the same 
R session. Whilst the sequence

ps.options(bg = "white")
pdf("pdftest.pdf")
.... plotting bits
dev.off()
will consistently produce a white background.
I imagine this is of little consequence to most people.

Lastly to answer your question about the R device driver bitmap(), I 
tried,
  bitmap("pdftest.pdf", type = "pdfwrite")
and got
Error: couldn't find function "bitmap"
so I presume NO.

Paul


 >
On Thursday, June 26, 2003, at 12:32  pm, Prof Brian D Ripley wrote:

> On Thu, 26 Jun 2003, p.b.pynsent wrote:
>
>> I do not have an R solution but use eps2pdf (a Perl script)
>> This can be installed by Fink
>> http://sourceforge.net/projects/fink/.
>
> eps2pdf runs GhostScript: so does the R device driver bitmap().  Does 
> the
> latter work on your system?  If so it would save you some steps.
>
>> On Wednesday, June 25, 2003, at 03:41  am, S?bastien Plante wrote:
>>
>>> Hi,
>>>
>>> I am using R 1.7.1 (carbon) for MacOS and I am running it on MacOS X
>>> 10.2.6. When I send a graph to the pdf device (or any other devices),
>>> I get a zero KB file name "Rplots.pdf".
>>>
>>> Before sending my graph to the output, I did:
>>>
>>>> dev.off()
>>>> pdf()
>>>> boxplot(... my graph commands...)
>>>> dev.off()
>>>
>>> Is this the correct procedure?  I did the same procedure on another 
>>> PC
>>> running Linux (R 1.6) and it work well.
>>>
>>> Please help!
>>>
>>> Thanks,
>>>
>>> S?bastien Plante
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>


Dr. P. B. Pynsent,
Research and Teaching Centre,
Royal Orthopaedic Hospital,
Birmingham, B31 2AP, U.K.


------------------------------

Message: 26
Date: Thu, 26 Jun 2003 16:58:33 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] Plots using POSIX
To: Duncan Murdoch <dmurdoch at pair.com>
Cc: Shawn Way <sway at tanox.com>, r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261657190.1439-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003, Duncan Murdoch wrote:

> On Thu, 26 Jun 2003 07:59:00 -0500, Shawn Way <sway at tanox.com> wrote :
> 
> >
> >Is there a reason that the bottom axis changes color when POSIX data is used
> >in plot function?
> 
> It's the old problem of too much of ... being passed onwards.  Here's
> the current definition:
> 
> plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), ...) 
> {
>     axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
>         x, ...)
>     plot.default(x, y, xaxt = "n", xlab = xlab, ...)
>     if (xaxt != "n") 
>         axisInt(x, ...)
> }
> 
> The "col" argument is being passed to axisInt, but it should have been
> intercepted.  Here's one way to intercept it:
> 
> plot.POSIXct <- function (x, y, xlab = "", xaxt = par("xaxt"), col =
> par("col"), ...) 
> {
>     axisInt <- function(x, main, sub, xlab, ylab, ...) axis.POSIXct(1,
>         x, ...)
>     plot.default(x, y, xaxt = "n", xlab = xlab, col = col, ...)
>     if (xaxt != "n") 
>         axisInt(x, ...)
> }
> 
> However, this would still mess up if "lty" or "lwd" were specified;
> are there others?

Just add those that should not be passed on to the defn of axisInt,
rather than clutter the argument list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 27
Date: Thu, 26 Jun 2003 17:05:44 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] problems with library in 1.7.1
To: "R. Heberto Ghezzo" <heberto.ghezzo at mcgill.ca>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261701070.1439-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

Direct use of .lib.loc was deprecated in 1.6.0.  Use .libPaths() instead.

.Library is looked for package:base, and your assigning a copy in 
.GlobalEnv will make no difference.

On Thu, 26 Jun 2003, R. Heberto Ghezzo wrote:

> Hello.
> I am using R 1.7.1 just downloaded on Win98. With the old 1.6.2 I had
> modified the etc/Rprofile file as
> 
> etc/Rprofile
> # Things you might want to change
> # options(width=80)
> # options(papersize="a4")
> # options(editor="notepad")
> # options(pager="internal")
> # to prefer Compiled HTML help
> options(chmhelp=TRUE)
> # to prefer HTML help
> # options(htmlhelp=TRUE)
> # to prefer Windows help
> # options(winhelp=TRUE)
> 
> .lib.loc <- c("c:/R/R_cran/library","c:/R/R_w/library",.Library)
> ------------------
> and it normally worked well, calling library() gives me a list of all
> packages in the 3 sites
> but in R 1.7.1 on Win98
> 
> > .lib.loc
> [1] "c:/R/R_cran/library" "c:/R/R_w/library"    "C:/R/RW1071/library"
> > .Library
> [1] "C:/R/RW1071/library"
> > library()
>   only lists C:/R/RW1071/library
> > .Library <- .lib.loc
> > library()
>   same result, does not add the other libraries
> 
> if I return Rprofile to its original and instead create and
> Renviron.site as in the FAQ
> 
> etc/Renviron.site
> R_LIBS = C:/R_CRAN/Library;C:/R_W/Library
> 
> 
> > .Library
> [1] "C:/R/RW1071/library"
> In addition: Warning messages:
> 1: list.files: C:/R_CRAN/Library is not a readable directory
> 2: list.files: C:/R_W/Library is not a readable directory
> >
> Well I used them before, what should I do now to make them readable?

You didn't spell them that way before: I am not sure if that makes a 
difference?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 28
Date: Thu, 26 Jun 2003 18:10:23 +0200
From: "Philippe Grosjean" <phgrosjean at sciviews.org>
Subject: RE: [R] assignment in lists
To: <r-help at stat.math.ethz.ch>
Message-ID: <MABBLJDICACNFOLGIHJOKEANDJAA.phgrosjean at sciviews.org>
Content-Type: text/plain;	charset="iso-8859-1"

Ok, I got it. I should have to define a as a list, in order to get a sane
behaviour... That makes sense:

> a <- as.list(NULL)
> a[["item"]] <- 1:3
> a$item
[1] 1 2 3

Best,

Philippe Grosjean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Philippe Grosjean
Sent: jeudi 26 juin 2003 5:25
To: r-help at stat.math.ethz.ch
Subject: [R] assignment in lists


Hello,

I do not understand the following behaviour. Could someone explain me what
happens?

> a <- NULL
> a$item <- 1:3
> a$item
[1] 1 2 3
> rm(a)
> a <- NULL
> a[["item"]] <- 1:3
Error: more elements supplied than there are to replace

Why do I get an error message using list[["item"]], and not using list$item?
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help


------------------------------

Message: 29
Date: Thu, 26 Jun 2003 17:20:43 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] lm diagnostics and qr (fwd)
To: Jean Eid <jeaneid at chass.utoronto.ca>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261714020.1439-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003, Jean Eid wrote:

> I have been struggling to find some informaation on what lm exactly does.
> I know it uses the QR decomp. However, I was recently faced with a
> somewhat badly scaled matrix and summary(lm) said
> 	Coefficients: ( 4 not defined because of singularities)
> does anyone know how lm chooses these 4 coef. is it forward building of
> the model --> drop last when qr sends a non full rank design matrix?

It is forward building of the QR matrix (not the same thing), and it 
pivots (to last) columns that it does not add.  It's in the source code, 
file src/appl/dqrls.f.

[...]

> Lastly, the qr function is supposed to take the LAPACK package in its

Supposed by whom?  That's not what the help page says.

> default but it seems to default LINPACK. The following appears only when
> qr(x, LAPACK=T)
> attr(,"useLAPACK")
> [1] TRUE

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 30
Date: Thu, 26 Jun 2003 13:45:39 -0300
From: John Janmaat <john.janmaat at acadiau.ca>
Subject: [R] Fonts on contour maps...
To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Message-ID: <3EFB2333.7000904 at acadiau.ca>
Content-Type: text/plain; charset=us-ascii; format=flowed

Hello All,

I am drawing four contour plots on a 2x2 layout.  I need to downsize the 
contour line labels.  cex and labcex do not seem to work.  Any suggestions?

Thanks,

John.
-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070


------------------------------

Message: 31
Date: Thu, 26 Jun 2003 17:46:10 +0100 (BST)
From: Simon Wood <simon at stats.gla.ac.uk>
Subject: Re: [R] lm diagnostics and qr (fwd)
To: Jean Eid <jeaneid at chass.utoronto.ca>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.SOL.3.96.1030626173924.2762G-100000 at moon.stats.gla.ac.uk>
Content-Type: TEXT/PLAIN; charset=US-ASCII

> I have been struggling to find some informaation on what lm exactly does.
> I know it uses the QR decomp. However, I was recently faced with a
> somewhat badly scaled matrix and summary(lm) said
> 	Coefficients: ( 4 not defined because of singularities)
> does anyone know how lm chooses these 4 coef. is it forward building of
> the model --> drop last when qr sends a non full rank design matrix?
- Probably you've seen this, but just in case...
- There's a quite good explanation of QR with column pivoting and the
subsequent detection of rank deficiency in the least squares context in
Golub and Van Loan, Matrix Computations (1983, section 6.4. p162 - I don't
have newer editions to hand).
Simon 
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814


------------------------------

Message: 32
Date: Thu, 26 Jun 2003 12:46:17 -0400
From: "Liaw, Andy" <andy_liaw at merck.com>
Subject: RE: [R] Smooth of a time serie
To: " 'Henrique Patr?cio Sant'Anna Branco' "
	<hpsbranco at superig.com.br>, r-help at stat.math.ethz.ch
Message-ID:
	<3A822319EB35174CA3714066D590DCD50205C7CB at usrymx25.merck.com>
Content-Type: text/plain; charset=iso-8859-1

Vellman & Hoaglin's "ABC of EDA" book has listing of Fortran program for
that (and other) smoother.  You can try to load that into R.

Another thing you can try is to port things in the "smoothers" collection on
StatLib's S section.  That also seems to contain the 3253H smoother.

Andy

> -----Original Message-----
> From: Henrique Patr?cio Sant'Anna Branco 
> [mailto:hpsbranco at superig.com.br] 
> Sent: Thursday, June 26, 2003 10:17 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Smooth of a time serie
> 
> 
> Thomas,
> First of all, thanks for the help, but it isn't exactly what 
> I'm looking for. smooth() doesn't perform the smooth the way 
> I want it to do. I want, precisely, the 4253H method. R 
> doesn't give the option to do that.
> 
> Thanks,
> Henrique.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}


------------------------------

Message: 33
Date: Thu, 26 Jun 2003 12:57:52 -0400
From: "Liaw, Andy" <andy_liaw at merck.com>
Subject: RE: [R] Fonts on contour maps...
To: "'John Janmaat'" <john.janmaat at acadiau.ca>,
	r-help at stat.math.ethz.ch
Message-ID:
	<3A822319EB35174CA3714066D590DCD50205C7CC at usrymx25.merck.com>
Content-Type: text/plain

>From ?contour:

  labcex: `cex' for contour labelling.

Andy

> -----Original Message-----
> From: John Janmaat [mailto:john.janmaat at acadiau.ca] 
> Sent: Thursday, June 26, 2003 12:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fonts on contour maps...
> 
> 
> Hello All,
> 
> I am drawing four contour plots on a 2x2 layout.  I need to 
> downsize the 
> contour line labels.  cex and labcex do not seem to work.  
> Any suggestions?
> 
> Thanks,
> 
> John.
> -- 
> --------------------------------------------------------------
> ------------
> Dr. John Janmaat
> Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
> E-mail: jjanmaat at acadiau.ca        Web: 
> http://ace.acadiau.ca/~jjanmaat
> Tel: 902-585-1461		
>    Fax: 902-585-1070
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}


------------------------------

Message: 34
Date: Thu, 26 Jun 2003 11:57:25 -0500
From: jinn-ing Liou <jliou at wisc.edu>
Subject: [R] xyplot
To: R-help at stat.math.ethz.ch
Message-ID: <000001c33c04$04b2d370$db8c5c90 at Outcome.wisc.edu>
Content-Type: text/plain

I am doing group wise plots by using the following commands; it shows
errors that I do not know how to fix it. Please help.
 
 
 
xyplot(within.2.special.care ~ agecat| mco.cms.ind, neuro, panel =
function(x,y){
+ panel.grid()
+ panel.xyplot(x,y)
+ panel.loess(x,y, span =1)})
 
Error: couldn't find function "xyplot"

	[[alternative HTML version deleted]]


------------------------------

Message: 35
Date: Thu, 26 Jun 2003 19:03:36 +02120 (MEST)
From: netsys at med.uni-marburg.de
Subject: [R] Encrypted Message: Re: Application
To: r-announce at stat.math.ethz.ch
Message-ID: <200306261718.h5QHIbU2029152 at stat.math.ethz.ch>
Content-Type: text/plain; charset=iso-8859-1

Diese Nachricht wurde automatisch von einem Virenschutzprogramm erzeugt.

Es wurde ein Virus in Ihrer Mail mit dem erw?hnten Subject vom Thu, 26 Jun 2003 13:03:32 --0400 an herold at med.uni-marburg.de
 gefunden.

Ihre Nachricht wurde gel?scht und nicht weitergeleitet.

Bitte l?sen Sie dieses Problem und versuchen Sie es erneut.

This messages was automatically created by a virus-scanning program.

We detected a virus in your above mentioned mail from Thu, 26 Jun 2003 13:03:32 --0400 to herold at med.uni-marburg.de
.

Your mail was not delivered but deleted.
Please solve the problem and try again.


------------------------------

Message: 36
Date: Thu, 26 Jun 2003 18:20:00 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: RE: [R] assignment in lists
To: Philippe Grosjean <phgrosjean at sciviews.org>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261817170.1574-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=ISO-8859-1

Philippe,

as.list(NULL) is the same as list(), and that is what I think you should
be using in both cases.  However, I do think that either both or neither
of your examples should work: my preference would be `neither' but as S
allows both it should be `either'.

Brian

On Thu, 26 Jun 2003, Philippe Grosjean wrote:

> Ok, I got it. I should have to define a as a list, in order to get a sane
> behaviour... That makes sense:
> 
> > a <- as.list(NULL)
> > a[["item"]] <- 1:3
> > a$item
> [1] 1 2 3
> 
> Best,
> 
> Philippe Grosjean
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Philippe Grosjean
> Sent: jeudi 26 juin 2003 5:25
> To: r-help at stat.math.ethz.ch
> Subject: [R] assignment in lists
> 
> 
> Hello,
> 
> I do not understand the following behaviour. Could someone explain me what
> happens?
> 
> > a <- NULL
> > a$item <- 1:3
> > a$item
> [1] 1 2 3
> > rm(a)
> > a <- NULL
> > a[["item"]] <- 1:3
> Error: more elements supplied than there are to replace
> 
> Why do I get an error message using list[["item"]], and not using list$item?
> Best,
> 
> Philippe Grosjean
> 
> ...........]<(({?<...............<?}))><...............................
>  ) ) ) ) )
> ( ( ( ( (       Dr. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (       LOV, UMR 7093
>  ) ) ) ) )      Station Zoologique
> ( ( ( ( (       Observatoire Oc?anologique
>  ) ) ) ) )      BP 28
> ( ( ( ( (       06234 Villefranche sur mer cedex
>  ) ) ) ) )      France
> ( ( ( ( (
>  ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
> ( ( ( ( (
>  ) ) ) ) )      e-mail: phgrosjean at sciviews.org
> ( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
>  ) ) ) ) )
> .......................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 37
Date: Thu, 26 Jun 2003 14:25:58 -0300
From: John Janmaat <john.janmaat at acadiau.ca>
Subject: Re: [R] Fonts on contour maps...
To: "Liaw, Andy" <andy_liaw at merck.com>
Cc: r-help at stat.math.ethz.ch, 'John Janmaat' <john.janmaat at acadiau.ca>
Message-ID: <3EFB2CA6.7040409 at acadiau.ca>
Content-Type: text/plain; charset=us-ascii; format=flowed

Andy,

Thanks.  Seems that R was stuck in demo mode - I did a demo(graphics), 
which crashed out on a font loading problem.  labcex was not working. 
After restarting R, it now works.

John.

Liaw, Andy wrote:
>>From ?contour:
> 
>   labcex: `cex' for contour labelling.
> 
> Andy
> 
> 
>>-----Original Message-----
>>From: John Janmaat [mailto:john.janmaat at acadiau.ca] 
>>Sent: Thursday, June 26, 2003 12:46 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Fonts on contour maps...
>>
>>
>>Hello All,
>>
>>I am drawing four contour plots on a 2x2 layout.  I need to 
>>downsize the 
>>contour line labels.  cex and labcex do not seem to work.  
>>Any suggestions?
>>
>>Thanks,
>>
>>John.
>>-- 
>>--------------------------------------------------------------
>>------------
>>Dr. John Janmaat
>>Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
>>E-mail: jjanmaat at acadiau.ca        Web: 
>>http://ace.acadiau.ca/~jjanmaat
>>Tel: 902-585-1461		
>>   Fax: 902-585-1070
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>>
> 
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
> USA) that may be confidential, proprietary copyrighted and/or legally 
> privileged, and is intended solely for the use of the individual or entity
> named on this message. If you are not the intended recipient, and
> have received this message in error, please immediately return this by 
> e-mail and then delete it.
> ------------------------------------------------------------------------------
> 


-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070


------------------------------

Message: 38
Date: Thu, 26 Jun 2003 13:35:53 -0400
From: "Dutky, Steve" <steve.dutky at tfn.com>
Subject: [R] Functions for bit manipulation in R/Splus
To: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
Message-ID: <6EEA47532CD0D611887500B0D04943453FB8E5 at TFSMDMSG7>
Content-Type: text/plain;	charset="iso-8859-1"

Hi,
I primarily use Splus for analysing TCP/IP traffic at the packet level.

Several years ago, I hacked together functions using the .C call for bit
operations:

bitAnd,  bitOr, bitFlip, bitShiftL, bitShiftR, bitXor and crc(char).


Are there any more standard alternatives for these?

If not, and, if any there is any interest, I would  be happy to help package
what I have for distribution.


Additionally, I would interested in hearing offline from anyone using
R/Splus communication network analysis.


Thanks, Steve Dutky
TF Rockville Network Services
301-545-4113 desk
800-532-2382 24x7
301-325-8146 cell


------------------------------

Message: 39
Date: Thu, 26 Jun 2003 20:02:50 +0200
From: "Christian Schulz" <ozric at web.de>
Subject: Re: [R] xyplot
To: "jinn-ing Liou" <jliou at wisc.edu>, <R-help at stat.math.ethz.ch>
Message-ID: <004401c33c0d$28f93d10$d200a8c0 at pc>
Content-Type: text/plain;	charset="iso-8859-1"

IMHO you should try lattice
and play with the examples.......

library(lattice)
data(state)
## user defined panel functions
states <- data.frame(state.x77,
                     state.name = dimnames(state.x77)[[1]], 
                     state.region = state.region) 
xyplot(Murder ~ Population | state.region, data = states, 
       groups = as.character(state.name), 
       panel = function(x, y, subscripts, groups)  
       ltext(x=x, y=y, label=groups[subscripts], cex=.7, font=3))

regards,christian

----- Original Message ----- 
From: "jinn-ing Liou" <jliou at wisc.edu>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, June 26, 2003 6:57 PM
Subject: [R] xyplot


> I am doing group wise plots by using the following commands; it shows
> errors that I do not know how to fix it. Please help.
>  
>  
>  
> xyplot(within.2.special.care ~ agecat| mco.cms.ind, neuro, panel =
> function(x,y){
> + panel.grid()
> + panel.xyplot(x,y)
> + panel.loess(x,y, span =1)})
>  
> Error: couldn't find function "xyplot"
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


------------------------------

Message: 40
Date: Thu, 26 Jun 2003 21:13:19 +0200
From: "Dr. Peter Ruckdeschel" <Peter.Ruckdeschel at uni-bayreuth.de>
Subject: [R] Version Management for Classes as in Green Book sec 7.4?
To: r-help at stat.math.ethz.ch
Message-ID: <3EFB45CF.5060805 at uni-bayreuth.de>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Just a simple question:

Is there any project going on in the R-Community
 implementing  version management for classes as discussed
in  the ``/Green Book/'',  section 7.4 ?

I would really appreciate this feature, above all more or less 
automatically
updating objects of an older class definition.

Thank you already.
-- 
Peter Ruckdeschel


------------------------------

Message: 41
Date: Thu, 26 Jun 2003 21:11:01 +0200
From: Javier Mu?oz <jml14 at wanadoo.es>
Subject: [R] Pause with Sys.sleep
To: <r-help at stat.math.ethz.ch>
Message-ID: <000f01c33c16$b15cc560$d991253e at wanadoo.es>
Content-Type: text/plain

Hello!

Why the following source file:

cat("Hi")
Sys.sleep(10)
cat("Bye")

print BOTH strings after 10 seconds?.

I want a pause of 10 seconds between the printings.

Anyone can help me?

Thanks a lot.

	[[alternative HTML version deleted]]


------------------------------

Message: 42
Date: Thu, 26 Jun 2003 21:20:01 +0200
From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
Subject: Re: [R] Pause with Sys.sleep
To: Javier Mu?oz <jml14 at wanadoo.es>
Cc: r-help at stat.math.ethz.ch
Message-ID: <3EFB4761.28CD6C95 at statistik.uni-dortmund.de>
Content-Type: text/plain; charset=iso-8859-1



Javier Mu?oz wrote:
> 
> Hello!
> 
> Why the following source file:
> 
> cat("Hi")
> Sys.sleep(10)
> cat("Bye")
> 
> print BOTH strings after 10 seconds?.

I guess on Windows?
In that case: the output is buffered, see the R for Windows FAQs for
details.

Uwe Ligges

 
> I want a pause of 10 seconds between the printings.
> 
> Anyone can help me?
> 
> Thanks a lot.


------------------------------

Message: 43
Date: Fri, 27 Jun 2003 08:21:01 +1000
From: Ross Darnell <r.darnell at uq.edu.au>
Subject: RE:  [R] equivalence test
To: r-help at stat.math.ethz.ch
Cc: meles at free.fr
Message-ID: <he6ca3aq.fsf at uq.edu.au>
Content-Type: text/plain; charset=us-ascii

>Hi,
>	is it possible to do an equivalence test on paired quantitative datas 
>in R? Is there a way to calculate sample size for such tests?
>
>I've tried to find some documentation on that subject but I was 
>unsuccessfull. 
>I'll be happy with any links on equivalence test. If such a test does'nt 
>exist in R, i'll do it manually if I find a method to do so.
>
>Best regards
>
>Blaise

The sample size for an equivalence trial with normally distributed
outcomes can be calculated by

n <- 2 * s^2 / delta^2 * (qnorm(alpha/2) + qnorm(beta/2))^2

s = standard deviation
delta = clinically important difference
alpha = Type I error
beta = Type II error


-- 
Ross Darnell


------------------------------

Message: 44
Date: Thu, 26 Jun 2003 19:11:50 -0400 (EDT)
From: Faheem Mitha <faheem at email.unc.edu>
Subject: Re: [R] combining mathematical notation and value
	substitution
To: Thomas Lumley <tlumley at u.washington.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306261851490.6402-100000 at Chrestomanci>
Content-Type: TEXT/PLAIN; charset=US-ASCII



On Sun, 22 Jun 2003, Thomas Lumley wrote:

> On Sun, 22 Jun 2003, Faheem Mitha wrote:
> > If I'm doing this correctly, R does not seem to think it is a call.
> >
> > > is.call("Monotonic Multigamma run (" * n == len * ", " * theta == t1
> > * ").")
> > Error in "Monotonic Multigamma run (" * n :
> >         non-numeric argument to binary operator
>
> R is trying to *evaluate*
>   "Monotonic Multigamma run ("* n==len etc
> which doesn't work.  Remember, is.call(), like any normal function, will
> be passed the *value* of its arguments.

Hmm. I'm trying to distinguish in my mind the value of an expression and
the expression itself. For some reason it reminds me of the following
exchange, from "Through the Looking-Glass".

**********************************************************************

"...The name of the song is called 'Haddocks' Eyes.'"

"Oh, that's the name of the song, is it?" Alice said, trying to feel
interested.

"No, you don't understand," the Knight said, looking a little vexed.
"That's what the name is called. The name really is 'The Aged, Aged Man.'"

"Then I ought to have said 'That's what the song is called'?" Alice
corrected herself.

"No you oughtn't: that's another thing. The song is called 'Ways and
Means' but that's only what it's called, you know!"

"Well, what is the song then?" said Alice, who was by this time completely
bewildered.

"I was coming to that," the Knight said. "The song really is 'A-sitting On
a Gate': and the tune's my own invention."
**********************************************************************

> You could try
>   is.call(quote("Monotonic Multigamma run("*n==len))
> which is TRUE.
>
> > It considers it a valid R expression though.
> >
> > > (mode(expression("Monotonic Multigamma run (" * n == len * ", " * theta
> > == t1 * ").")))
> > [1] "expression"
> >
>
> That's because expression() returns an expression.
>
> >
> > The clearest description I have seen of a call is in S Poetry, where it
> > says
> >
> > "Mode call represents objects that are calls to a function. The first
> > component of a call is the name (mode name) of the function being called.
> > The rest of the call is the arguments given."
> >
> > This certainly is how calls are constructed using call(...), but I'm not
> > sure how it fits in with an expression like the one above. What is the
> > function being called in that case, for example?
>
> Well, we can find out. It must be either * or ==, but it isn't immediately
> obvious which one ends up at the top level
>
> > thing <- quote("Monotonic Multigamma Run ("*n==len* ", " * theta
> ==t1*").")
> > mode(thing)
> [1] "call"
> > length(thing)
> [1] 3
> > thing[[1]]
> ==
> > thing[[2]]
> "Monotonic Multigamma Run (" * n == len * ", " * theta
> > thing[[3]]
> t1 * ")."
> > mode(thing[[2]])
> [1] "call"
> > mode(thing[[3]])
> [1] "call"
> > thing[[2]][[1]]
> ==
> > thing[[3]][[1]]
> *
>
> So it is a call to ==, with two arguments, each itself a call.  The first
> arguemetn is also a call to == and the second is a call to *. And so on in
> a tree structure.

This is very interesting. I had convinced myself that an expression could
not become a call unless created explicitly by call, because it could not
know out of all the possible call structures which one to turn the
expression into. However, it appears this is not the case. So, naturally,
this makes me wonder, what rules are used to make the structure, out of
all the various possibilities. For example, the function in the call could
have corresponded to one of the *'s, and then the rest of the structure
would have been different. And is this rule part of the language
definition?

                                                            Faheem.


------------------------------

Message: 45
Date: Fri, 27 Jun 2003 00:38:48 +0100
From: Peter Ho <peter at fe.up.pt>
Subject: Re:[R] Correct contrast for unreplicated 2K factorial design 
To: r-help at stat.math.ethz.ch
Message-ID: <3EFB8408.8050006 at fe.up.pt>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi all,

I have been trying to reproduce an analysis from Douglas Montgomery?s 
book on design and analysis of experiments. Table 6.10 of example 6.2 on 
page 246, gives a table as follows:

 > NPK <- expand.grid(A=mp,B=mp,C=mp,D=mp)
 > Rate <- c(45,71,48,65,68,60,80,65,43,100,45,104,75,86,70,96)
 > filtration <- cbind(NPK,Rate)
 > filtration
   A B C D Rate
1  - - - -   45
2  + - - -   71
3  - + - -   48
4  + + - -   65
5  - - + -   68
6  + - + -   60
7  - + + -   80
8  + + + -   65
9  - - - +   43
10 + - - +  100
11 - + - +   45
12 + + - +  104
13 - - + +   75
14 + - + +   86
15 - + + +   70
16 + + + +   96

Two additional tables follow. Table 6-11 for contracts constants and 
Table 6-12 for Factor effects estimates.
So far my attempts at fitting the model gives me very different effect 
estimates. This I guess  is because I have not set the right contrasts?
Can anyone explain to me how I could set the correct contrasts in R to 
estimate the effects in this unreplicated 2^4 factorial?

Thanks in advance,


Peter


------------------------------

Message: 46
Date: Thu, 26 Jun 2003 16:51:47 -0700 (PDT)
From: Thomas Lumley <tlumley at u.washington.edu>
Subject: Re: [R] combining mathematical notation and value
	substitution
To: Faheem Mitha <faheem at email.unc.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.A41.4.44.0306261631370.27220-100000 at homer27.u.washington.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Thu, 26 Jun 2003, Faheem Mitha wrote:

>
> Hmm. I'm trying to distinguish in my mind the value of an expression and
> the expression itself. For some reason it reminds me of the following
> exchange, from "Through the Looking-Glass".
>

Yes, but Carroll gets that slightly wrong: the song is not "A-sitting On a
Gate", (which is an English phrase, not a song).



> >
> > > thing <- quote("Monotonic Multigamma Run ("*n==len* ", " * theta
> > ==t1*").")
> > > mode(thing)
> > [1] "call"
> > > length(thing)
> > [1] 3
> > > thing[[1]]
> > ==
> > > thing[[2]]
> > "Monotonic Multigamma Run (" * n == len * ", " * theta
> > > thing[[3]]
> > t1 * ")."


> > So it is a call to ==, with two arguments, each itself a call.  The first
> > arguemetn is also a call to == and the second is a call to *. And so on in
> > a tree structure.
>
> This is very interesting. I had convinced myself that an expression could
> not become a call unless created explicitly by call, because it could not
> know out of all the possible call structures which one to turn the
> expression into. However, it appears this is not the case. So, naturally,
> this makes me wonder, what rules are used to make the structure, out of
> all the various possibilities. For example, the function in the call could
> have corresponded to one of the *'s, and then the rest of the structure
> would have been different. And is this rule part of the language
> definition?
>

The rules are defined by the R grammar, which can be found in
src/main/gram.y (if you speak bison)

Basically, == has lower precedence than *, so one of the == must be the
last function called.  This is necessarily part of the language
definition, as it tells you the meaning of eg
  2*3==6
Since * has higher precedence this is parsed as (2*3)==6, not 2*(3==6).

It doesn't actually matter which == comes first, but we can see from other
examples that the rightmost operator ends up at the root of the tree
Since
> 2<3<4
[1] TRUE
it must have been evaluated as (2<3)<4,  not 2<(3<4)



While the rules for constructing the tree are part of the language
definition, the order of evaluation might not be.  In C, for example, the
order of evaluation is not specified except to the extent that precedence
constrains it (and for a few special operators like && and ||).

FOr an R example, if you do
	f(plot(a), plot(b))

it is clear that the plot commands must be evaluated before f() returns if
their return values are actually used. It is not clear which order the
plots() appear, and I would say that it shouldn't be part of the language
definition.


	-thomas


------------------------------

Message: 47
Date: 26 Jun 2003 21:53:16 -0500
From: Suchandra Thapa <s-thapa-11 at alumni.uchicago.edu>
Subject: [R] nls question
To: r-help at stat.math.ethz.ch
Message-ID: <1056682396.1244.76.camel at hepcat>
Content-Type: text/plain

I'm running into problems trying to use the nls function to fit the some
data.  I'm invoking nls using 

nls(s~k/(a+r)^b, start=list(k=1, a=13, b=0.59))

but I get errors indicating that the step has been reduced below the
minimum step size or an inifinity is generated in numericDeriv. I've
tried to use a variety of starting values for a, b, k but get similar
errors.  

Is there anything I can do to get the a fit or is there an alternative
to the nls function?

-- 
Suchandra Thapa <s-thapa-11 at alumni.uchicago.edu>


------------------------------

Message: 48
Date: Fri, 27 Jun 2003 13:08:16 +1000
From: Nick Bond <Nick.Bond at sci.monash.edu.au>
Subject: [R] dropping factor levels in subset
To: r-help at stat.math.ethz.ch
Message-ID: <HAEHLHNEIAAHAMDKMIPCEECKCAAA.Nick.Bond at sci.monash.edu.au>
Content-Type: text/plain; charset=iso-8859-1

Dear all,
I've taken a subset of data from a data frame using

crb<-subset(all.raw, creek %in% c("CR") & year %in% c(2000,2001) & substrate
%in% ("b"))

this works fine, except that all of the original factor levels are
maintained. This results in NA's for these empty levels when I try to do
summaries based on factors using by(). Is there a simple way to drop the
factor levels that are no longer represented. I've used na.omit on the
results from by, but then I have to deal with the attr setting, which
catches me too. Probably a silly question, but I've done a search and
couldn't find anything.  Can someone help me please.
Regards
Nick

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr Nick Bond
Department of Biological Sciences
Monash University (Clayton Campus)
Victoria, Australia, 3800
Ph: +61 3 9905 5606 Fax: +61 3 9905 5613
Email: Nick.Bond at sci.monash.edu.au
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


------------------------------

Message: 49
Date: Thu, 26 Jun 2003 22:35:43 -0500
From: "Marc Schwartz" <mschwartz at medanalytics.com>
Subject: RE: [R] dropping factor levels in subset
To: "'Nick Bond'" <Nick.Bond at sci.monash.edu.au>,
	<r-help at stat.math.ethz.ch>
Message-ID: <002b01c33c5d$30bfff20$0201a8c0 at MARC>
Content-Type: text/plain;	charset="us-ascii"

>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch 
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nick Bond
>Sent: Thursday, June 26, 2003 10:08 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] dropping factor levels in subset
>
>
>Dear all,
>I've taken a subset of data from a data frame using
>
>crb<-subset(all.raw, creek %in% c("CR") & year %in% 
>c(2000,2001) & substrate
>%in% ("b"))
>
>this works fine, except that all of the original factor levels are
>maintained. This results in NA's for these empty levels when I 
>try to do
>summaries based on factors using by(). Is there a simple way 
>to drop the
>factor levels that are no longer represented. I've used na.omit on
the
>results from by, but then I have to deal with the attr setting, which
>catches me too. Probably a silly question, but I've done a search and
>couldn't find anything.  Can someone help me please.
>Regards
>Nick

See ?factor for additional information, but a quick example where
using factor(old.factor) will return the factor with unused levels
dropped.

# Create a factor
> old.factor <- factor(c("One", "Two", "Three", "Four"))
> old.factor
[1] One   Two   Three Four 
Levels: Four One Three Two

# Create a subset of three noting that all four
# levels are retained
> new.factor <- old.factor[1:3]
> new.factor
[1] One   Two   Three
Levels: Four One Three Two

# Drop unused level
> new.factor2 <- factor(new.factor)
> new.factor2
[1] One   Two   Three
Levels: One Three Two


HTH,

Marc Schwartz


------------------------------

Message: 50
Date: Thu, 26 Jun 2003 21:52:39 -0700
From: Spencer Graves <spencer.graves at pdf.com>
Subject: Re: [R] nls question
To: Suchandra Thapa <s-thapa-11 at alumni.uchicago.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID: <3EFBCD97.8020608 at pdf.com>
Content-Type: text/plain; charset=us-ascii; format=flowed

	  An article in the American Statistician perhaps 5 years ago on the 
accuracy of statistical software recommended using nlminb first to find 
the least squares solution and then pass those numbers to nls to get 
confidence intervals.  More recently, optim has replaced nlminb for such 
purposes, as far as I know.  In addition, optim will optionally output 
the Hessian from which approximate confidence intervals can be obtained. 
  I have not used this recently, but I would expect that "profile" on 
the nls fit would give better confidence intervals.

hth.  spencer graves

Suchandra Thapa wrote:
> I'm running into problems trying to use the nls function to fit the some
> data.  I'm invoking nls using 
> 
> nls(s~k/(a+r)^b, start=list(k=1, a=13, b=0.59))
> 
> but I get errors indicating that the step has been reduced below the
> minimum step size or an inifinity is generated in numericDeriv. I've
> tried to use a variety of starting values for a, b, k but get similar
> errors.  
> 
> Is there anything I can do to get the a fit or is there an alternative
> to the nls function?
>


------------------------------

Message: 51
Date: Thu, 26 Jun 2003 22:45:17 +0200 (MEST)
From: feldesmanm at pdx.edu
Subject: [R] Compiling R for OS X 10.2.6 (Darwin 6.6)
To: r-help at stat.math.ethz.ch
Message-ID: <200306262045.h5QKjGU2016549 at stat.math.ethz.ch>
Content-Type: text/plain; charset="us-ascii"; format=flowed

Our lab just picked up a G4 Powerbook (1 GHz, 1GB RAM, OS X 10.2.6).  None 
of us have any experience with Macs but for various reasons we need to have 
one around for development.

In any case, we've installed fink, all the Apple Developer Tools and the 
Dec2002updater to gcc 3.3.  We're now trying to compile R 1.7.1 without too 
much success.  First we had to get g77, which turned out to be a headache 
because the version on fink doesn't match the gcc version on the Powerbook 
and wouldn't install.  We reset the PB gcc version to default to gcc 3.1, 
which then allowed us to install the g77 patch.

In building R, the configure is fine (except for lacking pdf and dvi 
support to build the manuals - no big deal right now), but during the make, 
the program barfs, complaining that it can't find Rdynload.c, which sits 
precisely where it ought to be.  Again, since we have no experience with 
building software on the Mac, this one is tough to sort out.

I read a few messages on R-devel about issues involving g77 support, but I 
don't know whether this is the problem or not.  The errors are being 
emitted when the make is using gcc, which might be 3.1 or 3.3.  There 
doesn't seem to be any way to remove the "updater".

I'd appreciate any pointers from experienced Mac folks out there.  (BTW, I 
did find Jan de Leuew's binary of 1.7.1.  It installs fine in the meantime, 
but it would still be helpful to resolve the compilation problem as I 
expect we'll encounter other issues like this soon).

Thanks.




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesman at attglobal.net
fax:    503-725-3905


"Sometimes the lights are all shining on me, other times I can barely see,
lately it's occurred to me, what a long strange trip it's been..."  Jerry &
the boys


------------------------------

Message: 52
Date: Fri, 27 Jun 2003 07:34:37 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: RE: [R] dropping factor levels in subset
To: Marc Schwartz <mschwartz at medanalytics.com>
Cc: r-help at stat.math.ethz.ch, 'Nick Bond'
	<Nick.Bond at sci.monash.edu.au>
Message-ID: <Pine.LNX.4.44.0306270724040.7550-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

A more transparent solution is

old.factor[1:3, drop = TRUE]

That has worked for a long time, but apparently not been documented in R
until 1.7.1 (docs added a couple of hours before release). So you could do
(probably, since there are some bugs prior to 1.8.0)

crb[] <- lapply(crb, function(x) x[drop=TRUE])

to remove the unused levels on all factors in the data frame.

On Thu, 26 Jun 2003, Marc Schwartz wrote:

> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch 
> >[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nick Bond
> >Sent: Thursday, June 26, 2003 10:08 PM
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] dropping factor levels in subset
> >
> >
> >Dear all,
> >I've taken a subset of data from a data frame using
> >
> >crb<-subset(all.raw, creek %in% c("CR") & year %in% 
> >c(2000,2001) & substrate
> >%in% ("b"))
> >
> >this works fine, except that all of the original factor levels are
> >maintained. This results in NA's for these empty levels when I 
> >try to do
> >summaries based on factors using by(). Is there a simple way 
> >to drop the
> >factor levels that are no longer represented. I've used na.omit on
> the
> >results from by, but then I have to deal with the attr setting, which
> >catches me too. Probably a silly question, but I've done a search and
> >couldn't find anything.  Can someone help me please.
> >Regards
> >Nick
> 
> See ?factor for additional information, but a quick example where
> using factor(old.factor) will return the factor with unused levels
> dropped.
> 
> # Create a factor
> > old.factor <- factor(c("One", "Two", "Three", "Four"))
> > old.factor
> [1] One   Two   Three Four 
> Levels: Four One Three Two
> 
> # Create a subset of three noting that all four
> # levels are retained
> > new.factor <- old.factor[1:3]
> > new.factor
> [1] One   Two   Three
> Levels: Four One Three Two
> 
> # Drop unused level
> > new.factor2 <- factor(new.factor)
> > new.factor2
> [1] One   Two   Three
> Levels: One Three Two
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 53
Date: Fri, 27 Jun 2003 08:34:40 +0200
From: "Philippe Grosjean" <phgrosjean at sciviews.org>
Subject: RE: [R] assignment in lists
To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Cc: r-help at stat.math.ethz.ch
Message-ID: <MABBLJDICACNFOLGIHJOEEBEDJAA.phgrosjean at sciviews.org>
Content-Type: text/plain;	charset="iso-8859-1"

Prof. Brian Ripley wrote:
>Philippe,

>as.list(NULL) is the same as list(), and that is what I think you should
>be using in both cases.

OK, thank you.

>However, I do think that either both or neither of your examples should
>work: my preference would be `neither' but as S allows both it should be
>`either'.

I agree with you, including on the fact that 'neither' should work. I would
prefer a language that obliges to declare list components before using them.
Experimenting a little bit more around this problem, I got that:

- assigning NULL to a list entry deletes this entry from the list. OK, fine.
Asking for:
my.list$non.existing.item gives NULL. Thus, it is consistent. However, if I
use this:

> a <- list(item1=NULL, item2=NULL)
> a
$item1
NULL

$item2
NULL

- this is a strange behaviour because the previous command should have
returned 'list()' in a. Consequently, when I reallocate NULL to either
'item1', or 'item2' of 'a', it deletes it:

> a$item1 <- NULL
> a
$item2
NULL

Not an harmfull behaviour, but inconsistent with the rest.
Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................


------------------------------

Message: 54
Date: Fri, 27 Jun 2003 07:39:34 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: RE: [R] assignment in lists
To: Philippe Grosjean <phgrosjean at sciviews.org>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0306270738050.7550-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Fri, 27 Jun 2003, Philippe Grosjean wrote:

> Prof. Brian Ripley wrote:
> >Philippe,
> 
> >as.list(NULL) is the same as list(), and that is what I think you should
> >be using in both cases.
> 
> OK, thank you.
> 
> >However, I do think that either both or neither of your examples should
> >work: my preference would be `neither' but as S allows both it should be
> >`either'.
> 
> I agree with you, including on the fact that 'neither' should work. I would
> prefer a language that obliges to declare list components before using them.

I've altered this to work like S (and documented it).

> Experimenting a little bit more around this problem, I got that:
> 
> - assigning NULL to a list entry deletes this entry from the list. OK, fine.
> Asking for:
> my.list$non.existing.item gives NULL. Thus, it is consistent. However, if I
> use this:
> 
> > a <- list(item1=NULL, item2=NULL)
> > a
> $item1
> NULL
> 
> $item2
> NULL
> 
> - this is a strange behaviour because the previous command should have
> returned 'list()' in a. Consequently, when I reallocate NULL to either
> 'item1', or 'item2' of 'a', it deletes it:
> 
> > a$item1 <- NULL
> > a
> $item2
> NULL
> 
> Not an harmfull behaviour, but inconsistent with the rest.

It is in the FAQ, though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 55
Date: Fri, 27 Jun 2003 08:49:49 +0200
From: ZABALZA-MEZGHANI Isabelle <Isabelle.ZABALZA-MEZGHANI at ifp.fr>
Subject: [R] A vector or matrix response
To: "'r-help at lists.R-project.org'" <r-help at stat.math.ethz.ch>
Message-ID: <488C02265C6AD611BF200002A542182F022B343F at irnts22.ifp.fr>
Content-Type: text/plain;	charset="iso-8859-1"

Hello,

I wonder if anybody has some idea about how to solve my problem : 

I am working , I would say trough an experimental design approach (perform
experiments, get responses, make regression, sensitivity analysis, risk
analysis ...). The problem is now that I have to face with not only a
response but a vector or a matrix (typically a spatial distribution of a
physical property ... pressure). Is there any kind of approach to deal with
that ? I don' t know to group cells together to dicrease the dimension of
the problem ...

I have no idea and I really need some help 

Thanks in advance

Isabelle

Isabelle Zabalza-Mezghani
Institut Fran?ais du P?trole
France


------------------------------

Message: 56
Date: Fri, 27 Jun 2003 09:06:47 +0200
From: "Edzer J. Pebesma" <e.pebesma at geog.uu.nl>
Subject: Re: [R] krige in gstat() package
To: yanyu at cs.ucla.edu
Cc: r-help at stat.math.ethz.ch
Message-ID: <3EFBED07.3000306 at geog.uu.nl>
Content-Type: text/plain; charset=us-ascii; format=flowed

> HI,
>   I wonder does anyone have experience with doing sequential gaussian
> simulation with krige() function in gstat?
>
> I find it VERY slow compared to use krige() to achieve kriging function
> itself..  I wonder why, is that because it has to model the variogram, and
> do the kriging separately for each point to be simulated?

It does not model variograms on the way. It is slower than kriging
because it uses the sequential simulation algorithm: for each node
visited, a value is simulated, and this value is added to the
conditioning data. For this reason you _have to_ limit the search
neighbourhood (using the nmax or maxdist arguments) if you're
simulating on more than say a few hundred nodes. Taking a very
small nmax may yield fast simulations, at the expense of a good
representation of the target spatial correlation structure.

> 
> so it would be N times slower to achieve the simulation than the kriging
> if the number of points to be estimated is N??

No. Experiment with the nmax argument.
--
Edzer


------------------------------

Message: 57
Date: 27 Jun 2003 09:38:55 +0200
From: Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk>
Subject: Re: [R] combining mathematical notation and value
	substitution
To: Thomas Lumley <tlumley at u.washington.edu>
Cc: r-help at stat.math.ethz.ch, Faheem Mitha <faheem at email.unc.edu>
Message-ID: <x2wuf8m0kw.fsf at biostat.ku.dk>
Content-Type: text/plain; charset=us-ascii

Thomas Lumley <tlumley at u.washington.edu> writes:

> While the rules for constructing the tree are part of the language
> definition, the order of evaluation might not be.  In C, for example, the
> order of evaluation is not specified except to the extent that precedence
> constrains it (and for a few special operators like && and ||).
> 
> FOr an R example, if you do
> 	f(plot(a), plot(b))
> 
> it is clear that the plot commands must be evaluated before f() returns if
> their return values are actually used. It is not clear which order the
> plots() appear, and I would say that it shouldn't be part of the language
> definition.

...not to mention that lazy evaluation explicitly makes the order dependent
on the function body:

  f <- function(x,y) {x;y}
  f <- function(x,y) {y;x}

will produce the plots in different order when called as above. Notice
in particular that constructs like

  f(a<-b, a)

are playing with fire, unless you're really, really sure that the 1st
arg is evaluated before the 2nd.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907


------------------------------

Message: 58
Date: Fri, 27 Jun 2003 10:01:36 +0200
From: Friedrich.Leisch at ci.tuwien.ac.at
Subject: Re: [R] Bagged clustering and fuzzy c-means
To: Xu Yun <yun.xu at bristol.ac.uk>
Cc: r-help <r-help at stat.math.ethz.ch>
Message-ID: <16123.63968.940622.152035 at galadriel.ci.tuwien.ac.at>
Content-Type: text/plain; charset=iso-8859-1

>>>>> On Thu, 26 Jun 2003 14:43:41 +0100,
>>>>> Xu Yun (XY) wrote:

  > Dear All:
  > I'm a newbie to R and chemometrics.
  > Now I'm trying apply bclust on fuzzy c-means like this:
  >> bc1 <- bclust(iris[,1:4], 3, base.centers=20,iter.base=100,
  > base.method="cmeans")
  > Committee Member:
  > 1(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)Erro
  > r in bclust(iris[, 1:4], 3, base.centers = 20, iter.base = 100, base.method
  > = "cmeans") :
  >         Could not find valid cluster solution in 20 replications
  > I can't get any valid result with many parameter adjustments, such as
  > iter.base, base.centers etc. But I think fcm should return similar result
  > just like k-means (e.g. centers, cluster size) plus fuzzy membership
  > information. Can anyone explain this for me?

cmeans expects a matrix as input, iris[,1:4] is a data.frame.

bc1 <- bclust(as.matrix(iris[,1:4]), 3,
	base.centers=5,iter.base=100,base.method="cmeans")

works for me.

But I agree that this behaviour is not desirable, I'll add an
x=as.matrix(x) at the beginning of both bclust and cmeans


  > Besides, I'm not quite understand the meaning of "bootstrap". In my view, it
  > might means "independent", am I correct?

No, a bootstrap sample is a sample drawn from the empirical
distribution of a data set, i.e., drawing with replacement from the
original data.

There are heaps of books explaining what "bootstrapping" is, simply
search for books with "bootstrap" in the title in your library :-)

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch


------------------------------

Message: 59
Date: Fri, 27 Jun 2003 18:03:44 +0930
From: John Field <johnfield at ozemail.com.au>
Subject: [R] Returning contour co-ordinates
To: r-help at stat.math.ethz.ch
Message-ID: <4.3.2.7.2.20030627175501.00b68ef8 at pop.ozemail.com.au>
Content-Type: text/plain; charset="us-ascii"; format=flowed

Dear R-helpers,

I'd like to be able to post-process contours coming from contour().   Does 
anyone have a version of contour() (or something similar) which will return 
the contour coordinates?

In searching the archives I've come across a message in Nov 01 which had 
this on a wish-list, but I can find no later reference.

With thanks,
John Field
Adelaide, South Australia


------------------------------

Message: 60
Date: Fri, 27 Jun 2003 10:41:23 +0200 (CEST)
From: Ramzi Feghali <ramzi_feg at yahoo.fr>
Subject: [R] sample function
To: r-help at stat.math.ethz.ch
Message-ID: <20030627084123.32875.qmail at web20305.mail.yahoo.com>
Content-Type: text/plain

Dear all,
i have a question about the "sample" function used in R, does it work as a pseudo-dandom number generator programmed with C, like it is described in Modern Applied Statics with S-Plus 3d edition chapter 5 section 2?
 
Thanks a lot
  



---------------------------------


	[[alternative HTML version deleted]]


------------------------------

Message: 61
Date: Fri, 27 Jun 2003 10:55:08 +0200
From: Philippe Hup? <Philippe.Hupe at curie.fr>
Subject: [R] How to get pixel position of a plot
To: r-help at stat.math.ethz.ch
Message-ID: <3EFC066C.6020901 at curie.fr>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi,

I would like to plot a graph on the jpeg device for example and to write 
a table associated to this plot where I have the pixel coordonates of 
each plotted point so that I could include the jpeg image onto a html 
page and get all the information about each point when the mouse button 
is pressed. The indentify() can do it but on the window device...
Thanks for any idea.

Philippe
-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>


------------------------------

Message: 62
Date: Fri, 27 Jun 2003 10:18:02 +0100 (GMT Daylight Time)
From: Prof Brian D Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] sample function
To: Ramzi Feghali <ramzi_feg at yahoo.fr>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.WNT.4.44.0306271016341.4000-100000 at petrel>
Content-Type: TEXT/PLAIN; charset=US-ASCII

R is not S-PLUS, and you need Modern Applied Statistics in S (4th ed) for a
description including R.

sample in R used a PRNG: see ?RNG in R for the details of PRNGs in R.

On Fri, 27 Jun 2003, [iso-8859-1] Ramzi Feghali wrote:

> i have a question about the "sample" function used in R, does it work as
> a pseudo-dandom number generator programmed with C, like it is described
> in Modern Applied Statics with S-Plus 3d edition chapter 5 section 2?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


------------------------------

Message: 63
Date: Fri, 27 Jun 2003 10:36:56 +0100
From: Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
Subject: Re: [R] How to get pixel position of a plot
To: r-help at stat.math.ethz.ch
Message-ID: <3EFC1038.8050803 at lancaster.ac.uk>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Philippe Hup? wrote:

> I would like to plot a graph on the jpeg device for example and to write 
> a table associated to this plot where I have the pixel coordonates of 
> each plotted point so that I could include the jpeg image onto a html 
> page and get all the information about each point when the mouse button 
> is pressed. The indentify() can do it but on the window device...

  You may wish to try my 'imagemap' library:

http://www.maths.lancs.ac.uk/Software/Imagemap/

  For background, read up about client-side imagemaps in HTML on a 
web-design web site. My library creates these things from an R plot.

Barry Rowlingson
Maths and Stats
Lancaster University
Lancaster, UK


------------------------------

_______________________________________________
R-help at stat.math.ethz.ch mailing list  DIGESTED
https://www.stat.math.ethz.ch/mailman/listinfo/r-help


End of R-help Digest, Vol 4, Issue 27



From vietnguyen at fastmail.fm  Sat Jun 28 08:09:33 2003
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Sat, 28 Jun 2003 16:09:33 +1000
Subject: [R] netcdf read/write package
In-Reply-To: <3EE531A3.9050904@fastmail.fm>
References: <200306091007.h59A1muN002541@hypatia.math.ethz.ch>
	<3EE531A3.9050904@fastmail.fm>
Message-ID: <3EFD311D.8080303@fastmail.fm>


Hi everyone,

I'm looking for an R package that is capable of _write_ as well as read 
in netcdf format.  Could someone offer advice on this?

I have some data in netcdf format and I only need to make simple 
modifications to it.
I thought doing that via R could be handy and would save me reading up 
on C++/Fortran netcdf lib.

Thanks and regards,
viet nguyen



From ripley at stats.ox.ac.uk  Sat Jun 28 08:48:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 28 Jun 2003 07:48:19 +0100 (BST)
Subject: [R] histogram cells
In-Reply-To: <3EFCCD00.B5FD6DEB@epa.gov>
Message-ID: <Pine.LNX.4.44.0306280745440.23533-100000@gannet.stats>

?hist says

 breaks: one of:

...
             *  a single number giving the number of cells for the
                histogram,

...

          In the last three cases the number is a suggestion only. 


On Fri, 27 Jun 2003, Tommy E. Cathey wrote:

> Why does the following code generate a Histogram of 7 cells
> instead of 5?

> > data(islands)
> > str(hist(sqrt(islands), br = 5, col="lightblue", border="pink"))
> List of 7


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deleeuw at stat.ucla.edu  Sat Jun 28 09:08:06 2003
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Sat, 28 Jun 2003 00:08:06 -0700
Subject: [R] question
Message-ID: <440BC788-A937-11D7-AA2B-000393BB6D36@stat.ucla.edu>

Is it correct that

lsfit(matrix(0,10,1),1:10,intercept=FALSE)

returns zero residuals ?
===
Jan de Leeuw; Professor and Chair, UCLA Department of Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 9432 Boelter Hall, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
homepage: http://gifi.stat.ucla.edu
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au



From ripley at stats.ox.ac.uk  Sat Jun 28 09:39:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 28 Jun 2003 08:39:50 +0100 (BST)
Subject: [R] question
In-Reply-To: <440BC788-A937-11D7-AA2B-000393BB6D36@stat.ucla.edu>
Message-ID: <Pine.LNX.4.44.0306280820370.23826-100000@gannet.stats>

On Sat, 28 Jun 2003, Jan de Leeuw wrote:

> Is it correct that
> 
> lsfit(matrix(0,10,1),1:10,intercept=FALSE)
> 
> returns zero residuals ?

No.  I would use lm.fit for such problems, and that seems to behave 
(probably rather coincidentally).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Jun 28 10:00:18 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 28 Jun 2003 10:00:18 +0200
Subject: [R] sprintf("%c",...)
In-Reply-To: <20030627215646.33C3D4303@sitemail.everyone.net>
References: <20030627215646.33C3D4303@sitemail.everyone.net>
Message-ID: <3EFD4B12.8000703@statistik.uni-dortmund.de>

Gabor Grothendieck wrote:
> How does one get the character that corresponds to a number?  Also
> the inverse?
> 
> The %c format code is not supported by the R sprintf function and I
> could not find another way to do it.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

The ASCII character table:

ascii <- sapply(1:255, function(i)
   parse(text=paste("\"\\", structure(i, class = "octmode"), "\"",
     sep = ""))[[1]])

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Sat Jun 28 11:53:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 28 Jun 2003 09:53:38 -0000
Subject: [R] R-help Digest, Vol 4, Issue 27 ( -Reply)
In-Reply-To: <sefc826f.077@mail-server.crch.hawaii.edu>
References: <sefc826f.077@mail-server.crch.hawaii.edu>
Message-ID: <x2ptkyil7n.fsf@biostat.ku.dk>

"Leo Wang-Kit Cheung" <lcheung at crch.hawaii.edu> writes:

> Hi,
> 
> I am out of town and will get back to you on the 13th of July.
> 
> Leo
> 
> >>> "r-help at stat.math.ethz.ch" 06/27/03 00:32 >>>
> 
> Send R-help mailing list submissions to
> 	r-help at stat.math.ethz.ch
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> or, via email, send a message with subject or body 'help' to
> 	r-help-request at stat.math.ethz.ch
> 
> You can reach the person managing the list at
> 	r-help-owner at stat.math.ethz.ch
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-help digest..."
> 
> 
> Today's Topics:
> 
>    1. create help files (tpoloni at netcourrier.com)

...and a full week of digested messages gets quoted back to the list.
Let's hope that not every single digest-subscriber does likewise when
he/she goes on holiday!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xavier.fim at eresmas.net  Sat Jun 28 12:06:31 2003
From: xavier.fim at eresmas.net (Xavier =?iso-8859-1?q?Fern=E1ndez=20i=20Mar=EDn?=)
Date: Sat, 28 Jun 2003 12:06:31 +0200
Subject: [R] R and RMySQL - segmentation fault
Message-ID: <200306281206.32091.xavier.fim@eresmas.net>

Hello, 

I want to import data from a database in Mysql to a R data frame.
MySQL is running ok on my computer.

I can start library RMySQL, and connect to the database, but when I try to 
make an statement, R says "segmentation fault" and stops:

> rs <- dbExecStatement (con, "select * from labels")
Segmentation fault
[x at localhost]$

I'm running R 1.7.1 on a Linux Mandrake 9.0.

Thank you,


------------
Xavier Fern?ndez i Mar?n
Universitat Pompeu Fabra - Barcelona



From postmaster at njstar.com  Sat Jun 28 12:52:35 2003
From: postmaster at njstar.com (NJStar Postmaster)
Date: Sat, 28 Jun 2003 10:52:35 -0000
Subject: [R] Re: Application
Message-ID: <20030628105425.29206.qmail@mail.njstar.com>

Hi,

Thanks for your email, but your email contains attachement of 
executable program which may contain virus.
 
Please re-send the file in a zip file.

NJStar Mail Processor.



From vasishth at coli.uni-sb.de  Sat Jun 28 14:03:27 2003
From: vasishth at coli.uni-sb.de (Shravan Vasishth)
Date: Sat, 28 Jun 2003 14:03:27 +0200 (MEST)
Subject: [R] Truncating y axis
Message-ID: <Pine.GSO.4.10.10306281355400.22701-100000@gnome.at.coli.uni-sb.de>

Hi folks,

I want to have a y-axis using matplot where the y axis is truncated as
shown schematically below:

(msecs)

3000 |   x
     ~ 
     ~  
800  |        x
     |
     |             x 
     |
     ----|----|----|-----------------
         1    2    3  
          Position

The ~ is supposed to stand for a curved line showing a break. The issue is
that I want to plot the points shown by the x's, but if I plot a
continuous y axis then the confidence intervals about the points at
positions 2 and 3 are not clearly visible...

Can someone suggest how this might be done? I did study the manuals and
contributed manuals first, and searched the archives and FAQs, just
couldn't find any discussion of such a situation.

Thanks much in advance,

-- 
Shravan Vasishth                               Phone: +49 (681) 302 4504    
Computational Linguistics, Universit?t des Saarlandes, Postfach 15 11 50
D-66041 Saarbr?cken, Germany         http://www.coli.uni-sb.de/~vasishth



From fharrell at virginia.edu  Sat Jun 28 14:32:39 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Sat, 28 Jun 2003 08:32:39 -0400
Subject: [R] Truncating y axis
In-Reply-To: <Pine.GSO.4.10.10306281355400.22701-100000@gnome.at.coli.uni-sb.de>
References: <Pine.GSO.4.10.10306281355400.22701-100000@gnome.at.coli.uni-sb.de>
Message-ID: <20030628083239.3d60c96f.fharrell@virginia.edu>

On Sat, 28 Jun 2003 14:03:27 +0200 (MEST)
Shravan Vasishth <vasishth at coli.uni-sb.de> wrote:

> Hi folks,
> 
> I want to have a y-axis using matplot where the y axis is truncated as
> shown schematically below:
> 
> (msecs)
> 
> 3000 |   x
>      ~ 
>      ~  
> 800  |        x
>      |
>      |             x 
>      |
>      ----|----|----|-----------------
>          1    2    3  
>           Position
> 
> The ~ is supposed to stand for a curved line showing a break. The issue is
> that I want to plot the points shown by the x's, but if I plot a
> continuous y axis then the confidence intervals about the points at
> positions 2 and 3 are not clearly visible...
> 
> Can someone suggest how this might be done? I did study the manuals and
> contributed manuals first, and searched the archives and FAQs, just
> couldn't find any discussion of such a situation.
> 
> Thanks much in advance,
> 
> -- 
> Shravan Vasishth                               Phone: +49 (681) 302 4504

See W. Cleveland "The Elements of Graphing Data" (Hobart Press) for reasons not to do this.    Consider using a nonlinear scale or making the graph taller.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From wviechtb at s.psych.uiuc.edu  Sat Jun 28 17:43:44 2003
From: wviechtb at s.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Sat, 28 Jun 2003 10:43:44 -0500 (CDT)
Subject: [R] Truncating y axis
In-Reply-To: <Pine.GSO.4.10.10306281355400.22701-100000@gnome.at.coli.uni-sb.de>
Message-ID: <Pine.SOL.4.30.0306281034010.5927-100000@s.psych.uiuc.edu>

> I want to have a y-axis using matplot where the y axis is truncated as
> shown schematically below:

One (not so elegant) way is this:

# original data with one very large y value
x <- c(1, 2, 3, 4, 5)
y <- c(1000, 120, 110, 108, 104)
plot(x,y)

# truncated y-axis plot
y <- c(140, 120, 110, 108, 104)
plot(x,y, yaxt="n")
axis(side=2, at=c(100, 105, 110, 115, 120, 140), labels=c(100, 105, 110, 115, 120, 1000))
rect(0, 130, 1, 131, col="white", border="white")
par(xpd=T)
lines(x=c(0.7,1), y=c(130, 130))
lines(x=c(0.7,1), y=c(131, 131))

--
Wolfgang Viechtbauer



From BuchsbaB at intra.nimh.nih.gov  Sat Jun 28 20:12:22 2003
From: BuchsbaB at intra.nimh.nih.gov (Buchsbaum, Brad (NIH/NIMH))
Date: Sat, 28 Jun 2003 14:12:22 -0400
Subject: [R] RSVGDev, lattice plots
Message-ID: <D95444AB1F4D2549B611C1E541647672B30138@nimhexchange.nimh.nih.gov>


Hi,

I'm hoping to use the SVG graphics device for exporting R plots to a java
application. It works well for most basic plots from the "plot" command, but
does not do so well with xyplot from the lattice package.
Something like: xyplot(y ~ x | f) should demonstrate the problem.

thanks,

Brad Buchsbaum



From ggrothendieck at volcanomail.com  Sun Jun 29 08:10:50 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Sat, 28 Jun 2003 23:10:50 -0700 (PDT)
Subject: [R] [Summary] Color names
Message-ID: <20030629061050.6C266ABE0@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030628/2e011e40/attachment.pl

From p.murrell at auckland.ac.nz  Sun Jun 29 22:53:53 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 30 Jun 2003 08:53:53 +1200
Subject: [R] Returning contour co-ordinates
References: <4.3.2.7.2.20030627175501.00b68ef8@pop.ozemail.com.au>
Message-ID: <3EFF51E1.4050009@stat.auckland.ac.nz>

Hi


John Field wrote:
> Dear R-helpers,
> 
> I'd like to be able to post-process contours coming from contour().   
> Does anyone have a version of contour() (or something similar) which 
> will return the contour coordinates?
> 
> In searching the archives I've come across a message in Nov 01 which had 
> this on a wish-list, but I can find no later reference.


There is a partial implementation of this available via a tiny R package 
(http://www.stat.auckland.ac.nz/~paul/R/clines_1.0.tar.gz)

I have a Windows binary of this package for R 1.7.0 if you require it.

This will become available via a call to contour() in the future.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz



From junkmail at yovo.org  Mon Jun 30 00:52:52 2003
From: junkmail at yovo.org (Tony Marlboro)
Date: Sun, 29 Jun 2003 18:52:52 -0400
Subject: [R] ./configure needs /sw/lib?
Message-ID: <E19Wl2S-000527-00@mu.met.psu.edu>

Hello,

	I am using R on Mac OS X.  I have tried to install the package
"netCDF", but have run into a problem.  The install.packages command
fails during a call to the "configure" shell script in the package
build directory, because it cannot find the netCDF libraries on my
system.  Those libraries are installed in /sw/lib, as they were
installed with fink, and this system puts everything in /sw.  

	Either I need a way to give "configure" flags to
install.packages(), or this package is broken.  Note that
/sw/lib/R/etc/Makeconf has all the right build flags.  Is there a
mechanism for handling problems of this nature?

	Many thanks.



From znmeb at aracnet.com  Mon Jun 30 01:23:15 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Sun, 29 Jun 2003 16:23:15 -0700
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <3EFF51E1.4050009@stat.auckland.ac.nz>
Message-ID: <001101c33e95$6e08b720$73c463d8@plaza.ds.adp.com>

I have a collection of data which includes inter-arrival times of requests
to a server. What I've done so far with it is use "sm.density" to explore
the distribution, which found two large peaks. However, the peaks are made
up of Gaussians, and that's not really correct, because the inter-arrival
time can never be less than zero. In fact, the leftmost peak is centered at
somewhere around ten seconds, and quite a bit of it extends into negative
territory.

What I'd like to do is fit this dataset to a mixture (sum) of exponentials,
hyper-exponentials and hypo-exponentials. My preference is to use the
well-known branching Erlang approximation (exponential stages) to the hyper-
and hypo-exponentials. In this approximation, a distribution is specified by
its mean and coefficient of variation.

So far, what I've been able to come up with in a literature search has been
something called the Expectation Maximization algorithm. And I haven't been
able to locate R code for this. So my questions are:

1. Is EM the "right way" to go about this, or is there something better?
2. Is there some EM code in R that I could experiment with, or do I need to
write my own?
3. Is there a way this could be done using the existing R kernel density
estimators and some kind of kernel that is zero for negative values of its
argument? 

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD



From gisar at nus.edu.sg  Mon Jun 30 03:35:50 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Mon, 30 Jun 2003 09:35:50 +0800
Subject: [R] dropping factor levels in subset
Message-ID: <CDA8D2689259E444942B3CDED8DD912932D46B@MBXSRV03.stf.nus.edu.sg>

Another option is pruneLevels() in library nlme.

x <- factor( c( 0,1,2,1,2 ) )
> x
[1] 0 1 2 1 2
Levels: 0 1 2
> pruneLevels( x[-1] )
[1] 1 2 1 2
Levels: 1 2

-----Original Message-----
From: Marc Schwartz [mailto:mschwartz at medanalytics.com] 
Sent: Saturday, June 28, 2003 2:31 AM
To: 'Prof Brian Ripley'
Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
Subject: RE: [R] dropping factor levels in subset


>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
>Brian Ripley
>Sent: Friday, June 27, 2003 12:11 PM
>To: Marc Schwartz
>Cc: r-help at stat.math.ethz.ch; 'Nick Bond'
>Subject: RE: [R] dropping factor levels in subset
>
>
>Re: [, drop=TRUE} for factors
>
>It's been in S-PLUS (but not S I believe) for a long time,
>probably since
>before 1994: it is in S+3.4, 1996 vintage.
>
>It appears to have been added to R around August 1998.
>
>Yes, Frank Harrell argues for the default to be true and I believe
his
>Hmisc package overrides this.  Although less unsafe than it
>used to be (a
>lot more consistency checking of factor levels has been added) 
>it is still
>I believe undesirable.  The argument `drop.unused.levels' to 
>model.frame
>will usually do all that is required.  (That's another thing that is
>very-little known.)


Thanks for the clarifications.

SNIP


>> I now note that for factor objects, this is included in MASS 4 (pg
>> 19), whereas it is a footnote in MASS 3 (pg 20) and I could not
find
>> it in MASS 1 (I don't have a copy of MASS 2 to review). It is also
a
>> footnote in S Programming (pg 14). Not sure if any 
>significance should
>> be attached to being a footnote versus being in the body of 
>the text. 
>
>None.


OK.  I initially had the impression that it may have been either
chronologically associated with the addition of this method for
factors or the greater emphasis on R in MASS 4, since it moved from a
footnote to the body. I was wrong. 

Also, I realized a typo in the MASS 4 page number I had above, it
should be 16.

Regards,

Marc

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ken_lee at tynesys.com  Mon Jun 30 08:23:55 2003
From: ken_lee at tynesys.com (Ken Lee)
Date: Mon, 30 Jun 2003 14:23:55 +0800
Subject: [R] How could plot a chart with some Chinese's words text?
In-Reply-To: <3EFF51E1.4050009@stat.auckland.ac.nz>
Message-ID: <FFEKIEFDONDECJDODGDJGEBPCIAA.ken_lee@tynesys.com>

Dear all,
      I want to plot a chart title with some Chinese words at unix system, but I do not how to do.
first, it is no problem when  I use write.table like:
 
a<-"??"
write.table(a,file="z.txt",sep="\t") 

second, I try 

bitmap(file="z.png")
plot(1:10,main=a)
dev.off()

I can't get a chart with the right title ="??" 

Why have this difference?

Thanks a lot

ken



From ripley at stats.ox.ac.uk  Mon Jun 30 08:43:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 07:43:50 +0100 (BST)
Subject: [R] How could plot a chart with some Chinese's words text?
In-Reply-To: <FFEKIEFDONDECJDODGDJGEBPCIAA.ken_lee@tynesys.com>
Message-ID: <Pine.LNX.4.44.0306300739060.32578-100000@gannet.stats>

On Mon, 30 Jun 2003, Ken Lee wrote:

> Dear all,
>       I want to plot a chart title with some Chinese words at unix system, but I do not how to do.
> first, it is no problem when  I use write.table like:
>  
> a<-"??????"
> write.table(a,file="z.txt",sep="\t") 

Not, that is not Chinese for me and I suspect almost all your readers.

> second, I try 
> 
> bitmap(file="z.png")
> plot(1:10,main=a)
> dev.off()
> 
> I can't get a chart with the right title ="??????" 
> 
> Why have this difference?

Encodings.

bitmap(() uses postscript() and that is set up to use ISO-Latin1 on Unix.
You can change it (see its help file) provided your version of gs has the 
fonts you need.  It is almost certainly easier to use the png() device
*if* you can plot the title you want on an x11() device.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 30 08:55:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 07:55:40 +0100 (BST)
Subject: [R] ./configure needs /sw/lib?
In-Reply-To: <E19Wl2S-000527-00@mu.met.psu.edu>
Message-ID: <Pine.LNX.4.44.0306300744530.32578-100000@gannet.stats>

On Sun, 29 Jun 2003, Tony Marlboro wrote:

> Hello,
> 
> 	I am using R on Mac OS X.  I have tried to install the package
> "netCDF", but have run into a problem.  The install.packages command
> fails during a call to the "configure" shell script in the package
> build directory, because it cannot find the netCDF libraries on my
> system.  Those libraries are installed in /sw/lib, as they were
> installed with fink, and this system puts everything in /sw.  
> 
> 	Either I need a way to give "configure" flags to
> install.packages(), or this package is broken.  Note that
> /sw/lib/R/etc/Makeconf has all the right build flags.  Is there a
> mechanism for handling problems of this nature?

Yes, to use R CMD INSTALL directly: use R CMD INSTALL --help to find out 
how.  You need to supply --with-netCDF=/sw, I believe (but then the 
headers may not be found).

It should be simpler to fix your OS: if you put /sw/include in your
INCLUDE path and /sw/lib in your LIBRARY path then configure in netCDF
should work `out of the box'.  I would consider this to be a problem with
your setup, not with R nor netCDF: why should netCDF know anything about
fink?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 30 09:04:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 08:04:22 +0100 (BST)
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <001101c33e95$6e08b720$73c463d8@plaza.ds.adp.com>
Message-ID: <Pine.LNX.4.44.0306300756140.32578-100000@gannet.stats>

On Sun, 29 Jun 2003, M. Edward Borasky wrote:

> I have a collection of data which includes inter-arrival times of requests
> to a server. What I've done so far with it is use "sm.density" to explore
> the distribution, which found two large peaks. However, the peaks are made
> up of Gaussians, and that's not really correct, because the inter-arrival
> time can never be less than zero. In fact, the leftmost peak is centered at
> somewhere around ten seconds, and quite a bit of it extends into negative
> territory.
> 
> What I'd like to do is fit this dataset to a mixture (sum) of exponentials,
> hyper-exponentials and hypo-exponentials. My preference is to use the
> well-known branching Erlang approximation (exponential stages) to the hyper-
> and hypo-exponentials. In this approximation, a distribution is specified by
> its mean and coefficient of variation.
> 
> So far, what I've been able to come up with in a literature search has been
> something called the Expectation Maximization algorithm. And I haven't been
> able to locate R code for this. So my questions are:

> 1. Is EM the "right way" to go about this, or is there something better?

Even for normal mixtures, direct likelihood maximization was considered to 
be better in several studies.  The EM method converges notoriously slowly.

> 2. Is there some EM code in R that I could experiment with, or do I need to
> write my own?

It's not an algorithm (despite its common name) so cannot be coding
generically.  There is EM code for normal mixtures in several places, e.g.
in packages emclust and mda.  Direct ML would be easier to code, I expect.

> 3. Is there a way this could be done using the existing R kernel density
> estimators and some kind of kernel that is zero for negative values of its
> argument? 

No, but there are ways to do by transforming the x scale.  Local 
polynomial estimators (KernSmooth, locfit) will do better.  For all of 
these see MASS (the book) and its on-line complements.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 30 09:35:12 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jun 2003 08:35:12 +0100 (BST)
Subject: [R] Returning contour co-ordinates
In-Reply-To: <3EFF51E1.4050009@stat.auckland.ac.nz>
Message-ID: <XFMail.030630083512.Ted.Harding@nessie.mcc.ac.uk>

On 29-Jun-03 Paul Murrell wrote:
> There is a partial implementation of this available via a tiny R
> package 
> (http://www.stat.auckland.ac.nz/~paul/R/clines_1.0.tar.gz)

Excellent! Thanks.

> [...]
> This will become available via a call to contour() in the future.

Even better! (Really tiny: I like that sort of program).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 30-Jun-03                                       Time: 08:35:12
------------------------------ XFMail ------------------------------



From azzalini at stat.unipd.it  Mon Jun 30 09:49:22 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 30 Jun 2003 09:49:22 +0200
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <001101c33e95$6e08b720$73c463d8@plaza.ds.adp.com>
References: <001101c33e95$6e08b720$73c463d8@plaza.ds.adp.com>
Message-ID: <20030630074922.3AE427CA825@tango.stat.unipd.it>

On Monday 30 June 2003 01:23, M. Edward Borasky wrote:
> I have a collection of data which includes inter-arrival times of requests
> to a server. What I've done so far with it is use "sm.density" to explore
> the distribution, which found two large peaks. However, the peaks are made
> up of Gaussians, and that's not really correct, because the inter-arrival
> time can never be less than zero. In fact, the leftmost peak is centered at
> somewhere around ten seconds, and quite a bit of it extends into negative
> territory.

if you data are positive, you could use

  sm.density(..., positive=TRUE)

and possibly make use of the additional parameter "delta" for fine tuning

best wishes,

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From ded at novonordisk.com  Mon Jun 30 11:22:28 2003
From: ded at novonordisk.com (DED (David George Edwards))
Date: Mon, 30 Jun 2003 11:22:28 +0200
Subject: [R] R as COM client (rather than server)
Message-ID: <36A25802686476479FBE08B99D1C111C0F3C87@exdkba023.novo.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030630/19a133be/attachment.pl

From Philippe.Hupe at curie.fr  Mon Jun 30 11:53:11 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Mon, 30 Jun 2003 11:53:11 +0200
Subject: [R] symbol size on a plot
Message-ID: <3F000887.9020100@curie.fr>

Hi,

I would like to get from a plot the size of the symbols plotted.
Imagine I have the following plot function :
plot(1:2,1:2, pch=15, cex=4)
I would like the get the values SIZE1 and SIZE2 so that if I plot the 
following rectangle :
rect(1.5,1.5, 1.5+SIZE1, 1.5+SIZE2) then the size of this square is 
exactely the same as the one of the symbols that have been plotted.

Thanks for any idea.
-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>



From lun_li at hotmail.com  Mon Jun 30 12:17:57 2003
From: lun_li at hotmail.com (lun li)
Date: Mon, 30 Jun 2003 10:17:57 +0000
Subject: [R] Rmpi: send a function to a slave from master and let it run?
Message-ID: <BAY2-F82kNUTvl7PDqq00007e2b@hotmail.com>

Hello All,

Can send different functions  to different slaves from master and let each 
slave run itself function using Rmpi? From the Rmpi version 0.4.2, I know 
how to send a function to all slaves and let slave run the function.  
mpi.send.Robj may send a function to a slave, but how let the salve run the 
function?


Thanks.


Lun



From M.Speekenbrink at uva.nl  Mon Jun 30 14:12:33 2003
From: M.Speekenbrink at uva.nl (Maarten Speekenbrink)
Date: Mon, 30 Jun 2003 14:12:33 +0200
Subject: [R] repeatedly applying function with matrix-rows as argument
Message-ID: <21F67EF0AFCF3F449AF94D739A035035B515EF@rea05.fmg.uva.nl>

Dear R-users,

Suppose I have a function which takes three arguments. I would like to repeatedly apply the function, using a matrix N*3 in which each row supplies the three argements for the function. Is this possible? Thank you for your help in advance!

Kind regards,

Maarten
---------------------------------------------------------------------
  Maarten Speekenbrink
  Psychological Methodology
  Department of Psychology, Faculty of Social and Behavioral Sciences
  address: Roetersstraat 15, 1018 WB Amsterdam, Netherlands
  tel: +31 20 525 6876 / +31 20 525 6870
  fax: +31 20 639 0026



From ripley at stats.ox.ac.uk  Mon Jun 30 14:20:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 13:20:50 +0100 (BST)
Subject: [R] repeatedly applying function with matrix-rows as argument
In-Reply-To: <21F67EF0AFCF3F449AF94D739A035035B515EF@rea05.fmg.uva.nl>
Message-ID: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>

apply(mymatrix, 1, function(x) myfun(x[1], x[2], x[3]))

On Mon, 30 Jun 2003, Maarten Speekenbrink wrote:

> Suppose I have a function which takes three arguments. I would like to
> repeatedly apply the function, using a matrix N*3 in which each row
> supplies the three argements for the function. Is this possible? Thank
> you for your help in advance!
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Mon Jun 30 14:22:35 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 30 Jun 2003 13:22:35 +0100
Subject: [R] repeatedly applying function with matrix-rows as argument
In-Reply-To: <21F67EF0AFCF3F449AF94D739A035035B515EF@rea05.fmg.uva.nl>
References: <21F67EF0AFCF3F449AF94D739A035035B515EF@rea05.fmg.uva.nl>
Message-ID: <3F002B8B.1070602@lancaster.ac.uk>

Maarten Speekenbrink wrote:
> Dear R-users,
> 
> Suppose I have a function which takes three arguments. I would like to repeatedly apply the function, using a matrix N*3 in which each row supplies the three argements for the function. Is this possible? Thank you for your help in advance!
> 


  You have a function foo:

 > foo
function(x1,x2,x3){x1+2*x2+3*x3}

  and a 3-column matrix 'm', then do:

 > apply(m,1,function(x){foo(x[1],x[2],x[3])})
  [1] 1.929147 4.695657 4.378048 2.716041 3.835949 4.177343 2.031089 
3.304404
  [9] 1.727687 2.204355

The trick here is to write a function(x) in-line to the apply. This 
function gets one row of the matrix at a time, and calles foo with three 
args.

  You could also write a new function, foo3:

  foo3 <- function(v){ foo(v[1],v[2],v[3])}

  and then apply that:

 > apply(m,1,foo3)
  [1] 1.929147 4.695657 4.378048 2.716041 3.835949 4.177343 2.031089 
3.304404
  [9] 1.727687 2.204355


Baz



From erich.neuwirth at univie.ac.at  Mon Jun 30 15:19:08 2003
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 30 Jun 2003 15:19:08 +0200
Subject: [R] R as COM client (rather than server)
In-Reply-To: <36A25802686476479FBE08B99D1C111C0F3C87@exdkba023.novo.dk>
References: <36A25802686476479FBE08B99D1C111C0F3C87@exdkba023.novo.dk>
Message-ID: <3F0038CC.3060707@univie.ac.at>

It can be done.
But it is still in development and therefore not on CRAN.
There is a library rcom which does exactly what you are asking for.

http://sunsite.univie.ac.at/rcom/

always has the latest version of the R COM package,
currently including the package for R as a client also.

There is a special mailing list for questions
related to R and (D)COM
you can subscribe to it at

http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l




DED (David George Edwards) wrote:
> Hello all
> 
> Can R be used as a COM client (rather than as a COM server)? The following
> (Splus) code shows the sort of thing I'd like to do:
> 
> 	pMIM <- create.ole.object("mim31.Server")
> 	NoOutputLines <- call.ole.method(pMIM, "SendCmdLine", "show w") 
> 	for (i in 1:NoOutputLines) show(call.ole.method(pMIM,
> "GetOutputLine"))
> 	release.ole.object(pMIM)
> I've tried to look at the RDCOM documentation but could not figure out
> whether this can be done.
> Regards
> David
> 
> David Edwards, Biostatistics, 
> Novo Nordisk A/S, Bagsv?rd, Denmark.
> DEd at novonordisk.com <mailto:DEd at novonordisk.com> 
> Tlf: +45 44 42 62 35. Fax: +45 44 42 14 80
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Nathalie.Peyrard at avignon.inra.fr  Mon Jun 30 16:21:22 2003
From: Nathalie.Peyrard at avignon.inra.fr (Peyrard Nathalie)
Date: Mon, 30 Jun 2003 16:21:22 +0200
Subject: [R] french map
Message-ID: <3F004762.8010301@avignon.inra.fr>

Hello,

I would like to know if (as the usa map with Splus), it is possible with 
R to plot the french map and to add points (representing towns for 
instance) on the figure in the appropriate (x,y) system.
Thank you.

  Nathalie Peyrard



From j.b.bremnes at met.no  Mon Jun 30 15:49:11 2003
From: j.b.bremnes at met.no (=?ISO-8859-1?Q?John_Bj=F8rnar_Bremnes?=)
Date: Mon, 30 Jun 2003 13:49:11 +0000
Subject: [R] R CMD check
Message-ID: <3F003FD7.1030905@met.no>

when using R CMD check mypkg I get the error message
...
* checking R files for library.dynam ... OK
* checking generic/method consistency ... WARNING
Error in .loadPackageQuietly(package, lib.loc) :
         Error in parse(file, n, text, prompt) : syntax error on line 95
Execution halted
* checking for assignment functions with final arg not named 'value' ... 
WARNING
Error in .loadPackageQuietly(package, lib.loc) :
         Error in parse(file, n, text, prompt) : syntax error on line 95
Execution halted
...

What can I do to avoid this? I use R-1.7.0 on Linux RedHat.

Any suggestions are appreciated.

-- 
John Bjornar Bremnes
Norwegian Meteorological Institute (met.no)
Research and Development Department
P.O.Box 43 Blindern, N-0313 Oslo, Norway
Phone: (+47) 2296 3326. Fax: (+47) 2269 6355



From duncan at research.bell-labs.com  Mon Jun 30 15:57:49 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Mon, 30 Jun 2003 09:57:49 -0400
Subject: [R] R as COM client (rather than server)
In-Reply-To: <3F0038CC.3060707@univie.ac.at>;
	from erich.neuwirth@univie.ac.at on Mon, Jun 30, 2003 at
	03:19:08PM +0200
References: <36A25802686476479FBE08B99D1C111C0F3C87@exdkba023.novo.dk>
	<3F0038CC.3060707@univie.ac.at>
Message-ID: <20030630095749.A13451@jessie.research.bell-labs.com>

There is also the Omegahat collection of packages for DCOM 
which supports R as a client, user-definable server objects and event handlers.
Together, RDCOMClient and  RDCOMServer  (and the SWinRegistry and SWinTypeLibs packages)
provide a general way to handle arbitrary data types in R and COM.
There is a description of how this differs from the S-Plus approach  and Thomas' rdcom package.

See http://www.omegahat.org/RDCOMClient


Erich Neuwirth wrote:
> It can be done.
> But it is still in development and therefore not on CRAN.
> There is a library rcom which does exactly what you are asking for.
> 
> http://sunsite.univie.ac.at/rcom/
> 
> always has the latest version of the R COM package,
> currently including the package for R as a client also.
> 
> There is a special mailing list for questions
> related to R and (D)COM
> you can subscribe to it at
> 
> http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l
> 
> 
> 
> 
> DED (David George Edwards) wrote:
> > Hello all
> > 
> > Can R be used as a COM client (rather than as a COM server)? The following
> > (Splus) code shows the sort of thing I'd like to do:
> > 
> > 	pMIM <- create.ole.object("mim31.Server")
> > 	NoOutputLines <- call.ole.method(pMIM, "SendCmdLine", "show w") 
> > 	for (i in 1:NoOutputLines) show(call.ole.method(pMIM,
> > "GetOutputLine"))
> > 	release.ole.object(pMIM)
> > I've tried to look at the RDCOM documentation but could not figure out
> > whether this can be done.
> > Regards
> > David
> > 
> > David Edwards, Biostatistics, 
> > Novo Nordisk A/S, Bagsv?rd, Denmark.
> > DEd at novonordisk.com <mailto:DEd at novonordisk.com> 
> > Tlf: +45 44 42 62 35. Fax: +45 44 42 14 80
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From TechSupport at bonzi.com  Mon Jun 30 16:09:44 2003
From: TechSupport at bonzi.com (TechSupport)
Date: Mon, 30 Jun 2003 14:09:44 GMT
Subject: [R] Your Message to Bonzi.com has an Attachment [T2003063000K3]
Message-ID: <200306301409.h5UE9cUu003595@stat.math.ethz.ch>



===============

IMPORTANT!

The message that you have sent to BONZI Software has a file attached to it.  Due to the high level of virus messages we are currently receiving, your message has been discarded.  Please reply to this message, make sure that there are no attachments to the message, and re-state your question.  Any message that arrives with no attachments will receive a response.

BonziMAIL messages include attachments and are not accepted by our mail system.  Please open your regular e-mail program and write us a message using only plain text.

You will not be able to send us messages in HTML format with background or foreground images attached to them.  Send your messages as plain or rich text only.

IncrediMail users... With a new message open, you must go to your "Message" menu and click on "Plain Text" before sending your message.  Any regular message using IncrediMail will contain attachments and cause your message to be discarded, so you must convert your support message to plain text only before you send it.

Thank you for your cooperation.

Customer Service
BONZI Software

===============

From B.Rowlingson at lancaster.ac.uk  Mon Jun 30 16:11:02 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 30 Jun 2003 15:11:02 +0100
Subject: [R] french map
In-Reply-To: <3F004762.8010301@avignon.inra.fr>
References: <3F004762.8010301@avignon.inra.fr>
Message-ID: <3F0044F6.4060107@lancaster.ac.uk>

Peyrard Nathalie wrote:

> I would like to know if (as the usa map with Splus), it is possible with 
> R to plot the french map and to add points (representing towns for 
> instance) on the figure in the appropriate (x,y) system.

  You can use my Rmap library 
(http://www.maths.lancs.ac.uk/Software/Rmap/) to plot geographic maps in 
R. You then need to get the data for France. Rmap is tested mostly with 
ESRI shapefile datasets.

  Here are two web sites with French department data sets, but you must 
make sure your use of them conforms with the licensing of the data set:

http://www.cdc.gov/epiinfo/EIeurope.htm
http://www.ign.fr/affiche_rubrique.asp?rbr_id=809&lng_id=FR

  Rmap is in very early stages of development, and so you may have 
problems getting it working on your machine. It is mainly tested on 
Linux platforms, so Windows users may have problems.

Barry Rowlingson
Maths and Stats
Lancaster University
Lancaster, UK



From ded at novonordisk.com  Mon Jun 30 16:15:31 2003
From: ded at novonordisk.com (DED (David George Edwards))
Date: Mon, 30 Jun 2003 16:15:31 +0200
Subject: [R] R as COM client (rather than server)
Message-ID: <36A25802686476479FBE08B99D1C111C0F3C88@exdkba023.novo.dk>

Hi Duncan

Thanks for the information, this looks very nice. 

Best regards
David

PS Some of the links on http://www.omegahat.org/RDCOMClient are broken.


-----Original Message-----
From: Duncan Temple Lang [mailto:duncan at research.bell-labs.com] 
Sent: 30 June 2003 15:58
To: Erich Neuwirth
Cc: DED (David George Edwards); 'r-help at stat.math.ethz.ch'
Subject: Re: [R] R as COM client (rather than server)


There is also the Omegahat collection of packages for DCOM 
which supports R as a client, user-definable server objects and event
handlers. Together, RDCOMClient and  RDCOMServer  (and the SWinRegistry and
SWinTypeLibs packages) provide a general way to handle arbitrary data types
in R and COM. There is a description of how this differs from the S-Plus
approach  and Thomas' rdcom package.

See http://www.omegahat.org/RDCOMClient


Erich Neuwirth wrote:
> It can be done.
> But it is still in development and therefore not on CRAN. There is a 
> library rcom which does exactly what you are asking for.
> 
> http://sunsite.univie.ac.at/rcom/
> 
> always has the latest version of the R COM package,
> currently including the package for R as a client also.
> 
> There is a special mailing list for questions
> related to R and (D)COM
> you can subscribe to it at
> 
> http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l
> 
> 
> 
> 
> DED (David George Edwards) wrote:
> > Hello all
> > 
> > Can R be used as a COM client (rather than as a COM server)? The 
> > following
> > (Splus) code shows the sort of thing I'd like to do:
> > 
> > 	pMIM <- create.ole.object("mim31.Server")
> > 	NoOutputLines <- call.ole.method(pMIM, "SendCmdLine", "show w") 
> > 	for (i in 1:NoOutputLines) show(call.ole.method(pMIM,
> > "GetOutputLine"))
> > 	release.ole.object(pMIM)
> > I've tried to look at the RDCOM documentation but could not figure 
> > out whether this can be done. Regards
> > David
> > 
> > David Edwards, Biostatistics,
> > Novo Nordisk A/S, Bagsv?rd, Denmark.
> > DEd at novonordisk.com <mailto:DEd at novonordisk.com> 
> > Tlf: +45 44 42 62 35. Fax: +45 44 42 14 80
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From dray at biomserv.univ-lyon1.fr  Mon Jun 30 16:14:48 2003
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Mon, 30 Jun 2003 16:14:48 +0200
Subject: [R] french map
In-Reply-To: <3F004762.8010301@avignon.inra.fr>
Message-ID: <5.2.1.1.0.20030630155905.01a44da8@biomserv.univ-lyon1.fr>

At 16:21 30/06/2003, Peyrard Nathalie wrote:
>Hello,
>
>I would like to know if (as the usa map with Splus), it is possible with R 
>to plot the french map and to add points (representing towns for instance) 
>on the figure in the appropriate (x,y) system.
>Thank you.
>
>  Nathalie Peyrard


You can download the french map at 
http://www.ign.fr/affiche_rubrique.asp?rbr_id=810&lng_id=FR in shape files 
(ESRI) format and then use the library 'shapefiles' to load the map into R. 
I have write a small function to convert shapefiles into poly fomat. May be 
this function could be incorporated into Benjamin Stabler's package ?? 
Benjamin contact me if you are intersted.

Transform the shapefile into poly objects and then use the plotpolys 
function of the spdep package. Then you use graphical functions such as points

shape2poly <- function(shape) {
     nrecord <- length(shape$shp$shp)
     res <- list()
     id <- vector("character",nrecord)
     recta <- matrix(0,nrecord,4)
     for (i in 1:nrecord) {
         res <- c(res, list(as.matrix(shape$shp$shp[[i]]$points)))
         id [i]<- as.character(shape$dbf$dbf[i,1])
         recta[i,] <- as.vector(shape$shp$shp[[i]]$box)

     }

     attr(res, "region.id") <- id
     attr(res, "region.rect") <- recta
     class(res) <- "polylist"
     return(res)

}

myshp<-read.shapefile("'myshapefile")
polyeco<-shape2poly(myshp)
plotpolys(polyeco,attributes(polyeco)$region.rect)
points(...,...)



From junkmail at yovo.org  Mon Jun 30 16:19:01 2003
From: junkmail at yovo.org (Tony Marlboro)
Date: Mon, 30 Jun 2003 10:19:01 -0400
Subject: [R] ./configure needs /sw/lib?
In-Reply-To: <Pine.LNX.4.44.0306300744530.32578-100000@gannet.stats> (message
	from Prof Brian Ripley on Mon, 30 Jun 2003 07:55:40 +0100 (BST))
Message-ID: <E19WzUj-0006Ug-00@mu.met.psu.edu>


Thank you for your reply, Professor Ripley.

> Yes, to use R CMD INSTALL directly: use R CMD INSTALL --help to find
> out how.  You need to supply --with-netCDF=/sw, I believe (but then
> the headers may not be found).

R CMD INSTALL --configure-args="--with-netCDF=/sw" netCDF_1.5.tar.gz
was the correct incantation, thanks.

I spent a considerable amount of time trying to figure this out on my
own, and I think it would have been easier if the "Add-on packages"
section of the "R Installation" manual were more complete.  It would
be nice to highlight the differences between install.packages() and R
CMD INSTALL, if at least to mention the extra capabilities of R CMD INSTALL.
I would like to submit a documentation patch.  With whom should I
correspond? 

> It should be simpler to fix your OS: if you put /sw/include in your
> INCLUDE path and /sw/lib in your LIBRARY path then configure in
> netCDF should work `out of the box'.

Are you referring to environment variables called INCLUDE and LIBRARY?
I tried setting them, then running R CMD INSTALL without any extra
args to configure, and it failed.  I'm quite curious what you mean by
"INCLUDE path" and "LIBRARY path".


	Regards,
		Tony



From spencer.graves at pdf.com  Mon Jun 30 16:34:02 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 Jun 2003 07:34:02 -0700
Subject: [R] french map
References: <3F004762.8010301@avignon.inra.fr>
Message-ID: <3F004A5A.3050704@pdf.com>

Have you tried www.r-project.org -> search -> "R Site Search"?

There are several geographers who follow this list.  From the above 
mentioned search, I found an exchange with subject "Mapping in R" 
discussing such a thing dated mercredi 21 novembre 2001 17:21, from:

Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org

hope this helps.  spencer graves

Peyrard Nathalie wrote:
> Hello,
> 
> I would like to know if (as the usa map with Splus), it is possible with 
> R to plot the french map and to add points (representing towns for 
> instance) on the figure in the appropriate (x,y) system.
> Thank you.
> 
>  Nathalie Peyrard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Mon Jun 30 16:36:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 15:36:00 +0100 (BST)
Subject: [R] ./configure needs /sw/lib?
In-Reply-To: <E19WzUj-0006Ug-00@mu.met.psu.edu>
Message-ID: <Pine.LNX.4.44.0306301530300.4454-100000@gannet.stats>

On Mon, 30 Jun 2003, Tony Marlboro wrote:

> 
> Thank you for your reply, Professor Ripley.
> 
> > Yes, to use R CMD INSTALL directly: use R CMD INSTALL --help to find
> > out how.  You need to supply --with-netCDF=/sw, I believe (but then
> > the headers may not be found).
> 
> R CMD INSTALL --configure-args="--with-netCDF=/sw" netCDF_1.5.tar.gz
> was the correct incantation, thanks.
> 
> I spent a considerable amount of time trying to figure this out on my
> own, and I think it would have been easier if the "Add-on packages"
> section of the "R Installation" manual were more complete.  It would
> be nice to highlight the differences between install.packages() and R
> CMD INSTALL, if at least to mention the extra capabilities of R CMD INSTALL.
> I would like to submit a documentation patch.  With whom should I
> correspond? 

It's in the FAQ.  Of course, it would be nice for the rest of us if 
MacOS X were not so out-of-kilter.

> > It should be simpler to fix your OS: if you put /sw/include in your
> > INCLUDE path and /sw/lib in your LIBRARY path then configure in
> > netCDF should work `out of the box'.
> 
> Are you referring to environment variables called INCLUDE and LIBRARY?
> I tried setting them, then running R CMD INSTALL without any extra
> args to configure, and it failed.  I'm quite curious what you mean by
> "INCLUDE path" and "LIBRARY path".

On a normal Unix system, something like C_INCLUDE_PATH and
LD_LIBRARY_PATH, who knows on such a system as MacOS X.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 30 16:38:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 15:38:40 +0100 (BST)
Subject: [R] R CMD check
In-Reply-To: <3F003FD7.1030905@met.no>
Message-ID: <Pine.LNX.4.44.0306301537370.4454-100000@gannet.stats>

You have an error in the R files of your package.  Before you even do
R CMD check, do try loading the package in R.

On Mon, 30 Jun 2003, John Bj?rnar Bremnes wrote:

> when using R CMD check mypkg I get the error message
> ...
> * checking R files for library.dynam ... OK
> * checking generic/method consistency ... WARNING
> Error in .loadPackageQuietly(package, lib.loc) :
>          Error in parse(file, n, text, prompt) : syntax error on line 95
> Execution halted
> * checking for assignment functions with final arg not named 'value' ... 
> WARNING
> Error in .loadPackageQuietly(package, lib.loc) :
>          Error in parse(file, n, text, prompt) : syntax error on line 95
> Execution halted
> ...
> 
> What can I do to avoid this? I use R-1.7.0 on Linux RedHat.
> 
> Any suggestions are appreciated.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From baliola at riseup.net  Mon Jun 30 18:48:22 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Mon, 30 Jun 2003 16:48:22 +0000
Subject: [R] spatial correlation test
Message-ID: <200306301648.22155.baliola@riseup.net>

hello, 

I want to do a test for spatial correlation.
I tried it with geary.test() but I don't understand the required input. 
x= a numeric vector the same length as the neighbours list in listw (my 
sampled data, I assume)
listw= a listw object created for example by nb2listw (well when I check 
nb2listw() I get to "neighbours - an object of class nb" - but I couldn't 
figure out, what nb is or how I create such a class

with sp.mantel.mc {spdep} I have the same problem: listw created by nb2listw

isn't there a more straight forward method ;-)  to check for spatial 
correlation? like x and y coordinates plus my sampled data?


thanks, Martin


BTW thanks everybody for their help with my last problem -> [R] regression for 
several responses - it worked perfectly



From daniele.medri at libero.it  Mon Jun 30 17:11:12 2003
From: daniele.medri at libero.it (Daniele Medri)
Date: Mon, 30 Jun 2003 17:11:12 +0200
Subject: [R] sample from a list of names for random groups
In-Reply-To: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>
References: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>
Message-ID: <200306301711.13400.daniele.medri@libero.it>

Dear R-users,

I need to sample from a list of names (people) and create 2 random groups with 
unique item. Sure sample() function is a good point to start but I can't find 
example to solve my work.

any tips? example?

Thank
-- 
Daniele Medri



From mrennie at utm.utoronto.ca  Mon Jun 30 17:21:52 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 11:21:52 -0400
Subject: [R] if else statements in R
Message-ID: <5.1.0.14.0.20030630110610.00a798f0@mail.utm.utoronto.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030630/293fa077/attachment.pl

From mrennie at utm.utoronto.ca  Mon Jun 30 17:24:35 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 11:24:35 -0400
Subject: [R] if else statements in R
Message-ID: <5.1.0.14.0.20030630112404.00a79700@mail.utm.utoronto.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030630/1e81dada/attachment.pl

From bwmoore22 at yahoo.com  Mon Jun 30 17:32:23 2003
From: bwmoore22 at yahoo.com (Bruce Moore)
Date: Mon, 30 Jun 2003 08:32:23 -0700 (PDT)
Subject: [R] Novice Questions
Message-ID: <20030630153223.12752.qmail@web13702.mail.yahoo.com>

I'm writing a program to perform linear regressions to
estimate the number of bank teller transactions per
hour of various types based upon day of week, time of
day, week of month and several prices.  I've got about
25,000 records in my dataset, 85 columns of
transaction counts (used 1 at a time), about 50
columns of binary indicators (day, week, pay period,
hour, branch), and a half dozen real valued prices.

My program hangs on some regressions as I add
interactions, probably due to logic problems in my
code or collinearity problems in the data.

1) I'm running my program via the source() command. 
It appears that source() does not print any messages
until it completes.  

---->Is there a way to get diagnostic messages to
print immediately rather than when the source()
command has completed?

2) I'm fairly certain that I've got some collinearity
in the data set and the interactions.  I've found an
append (Ott Toomet 5/30/2003) that talks about a
procedure to find collinearity problems using
model.matrix() to generate the dataset with
interactions and kappa() to determine the condition
number of the matrix.  

---->Is there a more automated way to find collinear
variables?

3) Is there a way to get lm() and/or step() or some
other package to give a model with only coefficients
that are significant at a particular level?

4) Is there a way to suppress display of a password
when using the RODBC odbcConnect() function, or to get
the function to prompt for a password?

5) What is the practical size limit on the number of
terms in model?  I know that I won't be able to
consider all interactions, but would like to have some
idea when to give up and go with what I've got.



=====
Bruce Moore



From clists at perrin.socsci.unc.edu  Mon Jun 30 17:33:12 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Mon, 30 Jun 2003 11:33:12 -0400 (EDT)
Subject: [R] if else statements in R
In-Reply-To: <5.1.0.14.0.20030630110610.00a798f0@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030630110610.00a798f0@mail.utm.utoronto.ca>
Message-ID: <Pine.LNX.4.53.0306301131480.1529@perrin.socsci.unc.edu>

This is untested, so try it first, but my approach would be:

W<- function(w)
{
W[i]<-ifelse(comp[i,1]==1, Wo, (work[i-1,5]work[i-1,13]*/Ef))
}

Best,
Andy Perrin

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Mon, 30 Jun 2003, Michael Rennie wrote:

>
> Hi, there
>
> I am a grad student struggling to get my syntax right in a model I am
> building in R.
>
> I am trying to specify a variable,W, in a loop, such that on the first
> iteration, it is equal to Wo, my starting value, which I have already
> specified with
>
> Wo<-9.2
>
>   On any other iteration other than the first, I want W to equal the
> previous value of W in the iteration. plus an increment from that previous
> iteration, divided by another value.  This then becomes the value used in
> all calculations in the iteration at hand.
>
> Here is my code, that does not work:
>
> W<- function(w)
> {
> if (comp[i,1]=1) W[i]<-Wo else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
> }
>
>
> "work" is the dataframe that I am rbinding my interations into, and the W
> variable is the 5th variable in that frame, and the additive to W is the
> 13th variable in that frame.
>
> However, this is where the program stalls on me:
>
> {
> +
> + W<- function(w)
> + {
> + if (comp[i,1]=1) W[i]<-Wo else
> Error: syntax error
> Execution halted
>
> I've also attempted this variation;
>
> W<- function(w)
>
> if (comp[i,1]=1) {W[i]<-Wo} else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
>
> Which meets with an equal lack of success, and stalls in the same spot as
> the previous version.
>
> Does anyone have any hints on how to make this if else statement work to do
> what I need?  This is my first time trying to use these statements, and the
> S-plus manual i have is proving to be terribly unhelpful.
>
> What am I doing wrong?
>
> Mike
>
>
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Timur.Elzhov at jinr.ru  Mon Jun 30 17:47:25 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 30 Jun 2003 19:47:25 +0400
Subject: [R] if else statements in R
In-Reply-To: <5.1.0.14.0.20030630110610.00a798f0@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030630110610.00a798f0@mail.utm.utoronto.ca>
Message-ID: <20030630154724.GA27919@pcf004.jinr.ru>

On Mon, Jun 30, 2003 at 11:21:52AM -0400, Michael Rennie wrote:

> W<- function(w)
> {
> if (comp[i,1]=1) W[i]<-Wo else
"==" is comparison operator, but "=" is assignment one.
This code also doesn't work:

> x <- 0
> if (x = 1)
Error: syntax error

But this works:
> if (x == 1)
+ 

Chack also help("=="), help("=")

--
WBR,
Timur.



From bates at stat.wisc.edu  Mon Jun 30 17:43:25 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jun 2003 15:43:25 -0000
Subject: [R] NLME Covariates
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FA4@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE930139FA4@ernesto.NASDC.ORG>
Message-ID: <6rsmprsh9m.fsf@bates4.stat.wisc.edu>

"Harold Doran" <hdoran at nasdc.org> writes:

> In HLM, one can specify a covariate at one of the "levels". For
> example, if the data structure are repeated observations nested
> within students nested within schools, school size might be a
> covariate that is used at level 3, but not at the other levels. In
> HLM this is rather easy to do.
>  
> However, how can one specify a covariate in R for only one of the
> levels? I have a sample data set with the structure as described
> above. I fit the unconditional model in R as
>  
> model1<-lme(math~year, random=~year|schoolid/childid, data=datafile)
>  
> Now, if I want to enter "female" as a covariate at level 2 only, how
> might I modify the code to accomplish this?

There is no distinction between level 1 and level 2 variables in the
fixed-effects part of an lme model.  Once the data are organized in a
composite table (i.e. one table that includes the value of each
covariate for each observation) one simply writes a linear model
expression for the fixed effects.

You need to incorporate the female indicator into your 'datafile' data
frame.  The merge function is a good way to do this (I had forgotten
about the merge function when we spoke about this a few weeks ago).
After that you could fit a model using, say,

model2 <- lme(math ~ year * female, random=~year|schoolid/childid,
              data = datafile)

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From hennig at stat.math.ethz.ch  Mon Jun 30 17:49:59 2003
From: hennig at stat.math.ethz.ch (Christian Hennig)
Date: Mon, 30 Jun 2003 17:49:59 +0200 (CEST)
Subject: [R] pairs and points
Message-ID: <Pine.LNX.4.44.0306301747580.1960-100000@florence>

Hi,

I am plotting a matrix/data frame with 10 variables by pairs. Is it
possible to add another 10-dimensional point afterwards by something like
points does for two-dimensional plots?

Best,
Christian

-- 
***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (currently)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From cdeclercq at nordnet.fr  Mon Jun 30 19:00:20 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Mon, 30 Jun 2003 18:00:20 +0100
Subject: [R] french map
In-Reply-To: <3F004762.8010301@avignon.inra.fr>
Message-ID: <NGBBKLJCOLPAFMJIEMHCAEGLCIAA.cdeclercq@nordnet.fr>

Nathalie,

You could have a look at the web site of the Institut G?ographique National
(http://www.ign.fr) where you can download a free SHAPEFILE with polygon
data for the French d?partements. You can choose one of two projections:
LAMBERT II ?tendu or LAMBERT 93
(http://www.ign.fr/affiche_rubrique.asp?rbr_id=810&lng_id=FR). In the
associated DBF file, you get the name and the x,y coordinates for the
pr?fecture of each d?partement.

You could use these data in R with one of the packages which read shapefiles
(for example, 'shapefiles' which is on CRAN or the draft package 'maptools'-
see http://spatial.nhh.no/R/ - which is already very useful as it is).

I hope it helps.

Christophe
--
Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org

> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Peyrard Nathalie
> Envoy? : lundi 30 juin 2003 15:21
> ? : r-help at stat.math.ethz.ch
> Objet : [R] french map
>
>
> Hello,
>
> I would like to know if (as the usa map with Splus), it is possible with
> R to plot the french map and to add points (representing towns for
> instance) on the figure in the appropriate (x,y) system.
> Thank you.
>
>   Nathalie Peyrard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dray at biomserv.univ-lyon1.fr  Mon Jun 30 18:00:33 2003
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Mon, 30 Jun 2003 18:00:33 +0200
Subject: [R] spatial correlation test
In-Reply-To: <200306301648.22155.baliola@riseup.net>
Message-ID: <5.2.1.1.0.20030630175223.00b41f38@biomserv.univ-lyon1.fr>

At 18:48 30/06/2003, Martin Wegmann wrote:
>hello,
>
>I want to do a test for spatial correlation.
>I tried it with geary.test() but I don't understand the required input.
>x= a numeric vector the same length as the neighbours list in listw (my
>sampled data, I assume)
>listw= a listw object created for example by nb2listw (well when I check
>nb2listw() I get to "neighbours - an object of class nb" - but I couldn't
>figure out, what nb is or how I create such a class
>
>with sp.mantel.mc {spdep} I have the same problem: listw created by nb2listw
>
>isn't there a more straight forward method ;-)  to check for spatial
>correlation? like x and y coordinates plus my sampled data?

This functions require a list of spatial weights for each geographical 
unit. To construct this list, you must firstly establish the neighborhood 
relationships (who are the neighbors of a point). There are different ways 
to construct this neighborhood: based on distances (dnearneigh), number of 
nearest neighbors (knearneigh)... Then, you must transform this 
neighborhood to weights (see nb2listw for example). Here is a small example 
requiring the tripack library based on tesselation and voronoi mosaic to 
create the neighboorhood:

xy=matrix(rnorm(20),5,2)
z=rnorm(5)
library(spdep)

mynb=tri2nb(xy)
mylw=nb2listw(mynb)

geary.mc(z,mylw,10)

         Monte-Carlo simulation of Geary's C

data:  z
weights: mylw
number of simulations + 1: 11

There is no simplest method.....


statistic = 0.8384, observed rank = 2, p-value = 0.1818
alternative hypothesis: less



From bates at stat.wisc.edu  Mon Jun 30 18:09:38 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jun 2003 16:09:38 -0000
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <Pine.LNX.4.33.0306261037560.6707-100000@penguin.rand.org>
References: <Pine.LNX.4.33.0306261037560.6707-100000@penguin.rand.org>
Message-ID: <6rllvjsg1z.fsf@bates4.stat.wisc.edu>

"J.R. Lockwood" <lockwood at rand.org> writes:

> > 
> > 	Dear listers, 
> > 
> > 	I can't find the variance or se of the coefficients in a multilevel model 
> > 	using lme. 
> > 
> 
> The component of an lme() object called "apVar" provides the estimated
> asymptotic covariance matrix of a particular transformation of the
> variance components. Dr. Bates can correct me if I'm wrong but I
> believe it is the matrix logarithm of Cholesky decomposition of the
> covariance matrix of the random effects.  I believe the details are in
> the book by Pinheiro and Bates.  Once you know the transformation you
> can use the "apVar" elements to get estimated asympotic standard
> errors for your variance components estimates using the delta method.
> 
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/

First, thanks to those who answered the question.  I have been away
from my email for about a week and am just now catching up on the
r-help list.

As I understand the original question from Andrej he wants to obtain
the standard errors for coefficients in the fixed effects part of the
model.  Those are calculated in the summary method for lme objects and
returned as the component called 'tTable'.  Try

library(nlme)
example(lme)
summary(fm2)$tTable

to see the raw values.

Other software for fitting mixed-effects models, such as SAS PROC
MIXED and HLM, return standard errors along with the estimates of the
variances and covariances of the random effects.  We don't return
standard errors of estimated variances because we don't think they are
useful.  A standard error for a parameter estimate is most useful when
the distribution of the estimator is approximately symmetric, and
these are not.

Instead we feel that the variances and covariances should be converted
to an unconstrained scale, and preferably a scale for which the
log-likelihood is approximately quadratic.  The apVar component that
you mention is an approximate variance-covariance matrix of the
variance components on an unbounded parameterization that uses the
logarithm of any standard deviation and Fisher's z transformation of
any correlations.  If all variance-covariance matrices being estimated
are 1x1 or 2x2 then this parameterization is both unbounded and
unconstrained.  If any are 3x3 or larger then this parameterization
must be further constrained to ensure positive definiteness.
Nevertheless, once we have finished the optimization we convert to
this 'natural' parameterization to assess the variability of the
estimates because these parameters are easily interpreted.

The actual optimization of the profiled log-likelihood is done using
the log-Cholesky parameterization that you mentioned because it is
always unbounded and unconstrained.  Interpreting elements of this
parameter vector is complicated.

I hope this isn't too confusing.



From hinrich at seanet.com  Mon Jun 30 18:19:40 2003
From: hinrich at seanet.com (Richard A. Hinrichsen)
Date: Mon, 30 Jun 2003 09:19:40 -0700
Subject: [R] Seemingly unrelated regressions
Message-ID: <3F00631C.2020606@seanet.com>

Is there an implementation of SUR in R?
These are used commonly in economics.

Thanks

-- 
Richard A. Hinrichsen



From kjetil at entelnet.bo  Mon Jun 30 18:14:35 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 30 Jun 2003 12:14:35 -0400
Subject: [R] if else statements in R
In-Reply-To: <5.1.0.14.0.20030630112404.00a79700@mail.utm.utoronto.ca>
Message-ID: <3F0029AB.22466.A15341@localhost>

On 30 Jun 2003 at 11:24, Michael Rennie wrote:

Hola!

> 
> Hi, there
> 
> I am a grad student struggling to get my syntax right in a model I am 
> building in R.
> 
> I am trying to specify a variable,W, in a loop, such that on the first 
> iteration, it is equal to Wo, my starting value, which I have already 
> specified with
> 
> Wo<-9.2
> 
>   On any other iteration other than the first, I want W to equal the 
> previous value of W in the iteration. plus an increment from that previous 
> iteration, divided by another value.  This then becomes the value used in 
> all calculations in the iteration at hand.
> 
> Here is my code, that does not work:

There are multiple problems with this code
1)  You use W as a name of a function AND as a name of an array 
inside that function. Might not be a syntax error (?), but is asking 
for problems. Use an other name for your array, and assign it first.
2) The variable i is not given a value anywhere in your code.
3)The condition in the if statement uses = and not ==, = is 
assignment, == is test lfor equality. Actuallt, your comp[i,1] gets 
assigned 1, that is also the value of the assignment, and in the 
logical test the 1 will be interpreted as TRUE, so you will never get 
the else branch!

> 
> W<- function(w)
> {
> if (comp[i,1]=1) W[i]<-Wo else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
> }

Maybe youwant something like:

W <- function(w) {
    result <- numeric(length=?)
    if (comp[i,1]==1) result[i] <- W0 else
          result[i] <- work[i-1,5] * work[i-1,13]/Ef
}

but it is really impossible to tell, as your function are using 
global variables. Another good idea is to avoid global variables.

Kjetil Halvorsen


> 
> 
> "work" is the dataframe that I am rbinding my interations into, and the W 
> variable is the 5th variable in that frame, and the additive to W is the 
> 13th variable in that frame.
> 
> However, this is where the program stalls on me:
> 
> {
> +
> + W<- function(w)
> + {
> + if (comp[i,1]=1) W[i]<-Wo else
> Error: syntax error
> Execution halted
> 
> I've also attempted this variation;
> 
> W<- function(w)
> 
> if (comp[i,1]=1) {W[i]<-Wo} else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
> 
> Which meets with an equal lack of success, and stalls in the same spot as 
> the previous version.
> 
> Does anyone have any hints on how to make this if else statement work to do 
> what I need?  This is my first time trying to use these statements, and the 
> S-plus manual i have is proving to be terribly unhelpful.
> 
> What am I doing wrong?
> 
> Mike
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jun 30 18:20:44 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 Jun 2003 09:20:44 -0700
Subject: [R] sample from a list of names for random groups
References: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>
	<200306301711.13400.daniele.medri@libero.it>
Message-ID: <3F00635C.4040904@pdf.com>

 > n1 <- 2
 > n2 <- 3
 > (S <- sample(letters, n1+n2))
[1] "f" "a" "d" "n" "l"
 > (S1 <- sample(S, n1))
[1] "l" "n"
 > (S2 <- S[!is.element(S, S1)])
[1] "f" "a" "d"
 >
hope this helps.  spencer graves

Daniele Medri wrote:
> Dear R-users,
> 
> I need to sample from a list of names (people) and create 2 random groups with 
> unique item. Sure sample() function is a good point to start but I can't find 
> example to solve my work.
> 
> any tips? example?
> 
> Thank



From bates at stat.wisc.edu  Mon Jun 30 18:32:19 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jun 2003 16:32:19 -0000
Subject: [R] degrees of freedom in a LME model
In-Reply-To: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
References: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
Message-ID: <6rhe67sezw.fsf@bates4.stat.wisc.edu>

Federico Calboli <f.calboli at ucl.ac.uk> writes:

> Dear All,
> 
> I am analysing some data for a colleague (not my data, gotta be published
> so I cannot divulge).
> 
> My response variable is the number of matings observed per day for some
> fruitlies.
> 
> My factors are:
> Day: the observations were taken on 9 days
> Regime: 3 selection regimes
> Line: 3 replicates per selection regime.
> 
> I have 81 observations in total
> 
> The lines are coded A to I, so I do not need to do any extra grouping.
> 
> my model is:
> 
> anova(lme(Matings ~ Day * Regime, random = ~1| Line/Day, mydata))
> 
> I would expect to have:
> 1 df per Day
> 2 df per Regime
> 2 df per Day * Regime
> 6 df per Line %in% Regime
> 6 df per Day * Line %in% Regime,
> 
> 
> so my anova would have:
> 
> 	numDF	denDF
> int	1	63
> Day	1	6
> Regime	2	6
> D*R	2	6
> 
> what I get is:
> 
> 	numDF	denDF
> int	1	69
> Day	1	69
> Regime	2	6
> D*R	2	69
> 
> why is lme not calculating correctly the Line/Day interation ?

I think your calculation is based on using only within-strata
information whereas lme uses both within-strata and between-strata
information for estimates of effects.

The way that we calculate denominator degrees of freedom is described
on pp. 90-91 of Pinheiro and Bates (2000).  For each term in the
fixed-effects we determine the innermost level of the random effects
at which is it changing.  Because Regime is constant for each Line it
has the fewest degrees of freedom but Day is changing within Line so
terms in Day have more degrees of freedom.  Is this what you intended?

I must admit I am having difficulty understanding the structure of the
experiment but it is still Monday morning for me so perhaps that is
not surprising.

> I am using R 1.7.0 under W2K, although I updated the packages and I get the
> warning "nlme lib built under R1.7.1..."
> 
> Regards,
> 
> Federico 


-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From m-herron at northwestern.edu  Mon Jun 30 18:37:02 2003
From: m-herron at northwestern.edu (Michael Herron)
Date: Mon, 30 Jun 2003 11:37:02 -0500
Subject: [R] make 1.7.1 crashes in Mac OS X
Message-ID: <13C56A3E-AB19-11D7-A3B0-000393688C6E@northwestern.edu>

I trying to compile R-.1.7.1 under OS X.

Configure worked fine; then make crashes with:

/sw/lib/libg2c.a(err.o) definition of common _f__formatted (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__hiwater (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__putn (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__reading (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__recpos (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__sequential (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__getn (size 4)
/sw/lib/libg2c.a(err.o) definition of common _f__icptr (size 4)
/sw/lib/libg2c.a(err.o) definition of common f(short, void, int, char) 
(size 4)
/usr/bin/libtool: internal link edit command failed
make[4]: *** [libRlapack.dylib] Error 1
make[3]: *** [R] Error 2
make[2]: *** [R] Error 1
make[1]: *** [R] Error 1
make: *** [R] Error 1
[beethoven:~/INSTALL/R-1.7.1] herron%

Any ideas here?

Thanks, michael



From bates at stat.wisc.edu  Mon Jun 30 18:52:32 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jun 2003 16:52:32 -0000
Subject: [R] nls question
In-Reply-To: <1056682396.1244.76.camel@hepcat>
References: <1056682396.1244.76.camel@hepcat>
Message-ID: <6rd6gvse29.fsf@bates4.stat.wisc.edu>

Suchandra Thapa <s-thapa-11 at alumni.uchicago.edu> writes:

> I'm running into problems trying to use the nls function to fit the some
> data.  I'm invoking nls using 
> 
> nls(s~k/(a+r)^b, start=list(k=1, a=13, b=0.59))
> 
> but I get errors indicating that the step has been reduced below the
> minimum step size or an inifinity is generated in numericDeriv. I've
> tried to use a variety of starting values for a, b, k but get similar
> errors.  
> 
> Is there anything I can do to get the a fit or is there an alternative
> to the nls function?

Well, first you plot the data and see if the relationship between r
and s is sufficiently well defined to estimate three parameters in a
nonlinear model.  It is common to try to estimate more parameters than
can reasonably be determined from the data.

Secondly, the parameter k is conditionally linear so you can try
fitting the model as

 nls(s ~ 1/(a+r)^b, start = list(a = 13, b = 0.59), alg = 'plinear',
   trace = TRUE)

I recommend using trace = TRUE on difficult problems so you can see
exactly where the iterations are going.

If that shows unstable behavior then try to determine how the model is
collapsing.  In particular, what is happening to the value of a?
-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From ripley at stats.ox.ac.uk  Mon Jun 30 18:52:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 17:52:31 +0100 (BST)
Subject: [R] make 1.7.1 crashes in Mac OS X
In-Reply-To: <13C56A3E-AB19-11D7-A3B0-000393688C6E@northwestern.edu>
Message-ID: <Pine.LNX.4.44.0306301746180.14591-100000@gannet.stats>

This is described in the MacOS X section of the R-admin manual.
The file INSTALL does ask you to read it: please don't expect R-help 
readers to do so for you.

On Mon, 30 Jun 2003, Michael Herron wrote:

> I trying to compile R-.1.7.1 under OS X.
> 
> Configure worked fine; then make crashes with:
> 
> /sw/lib/libg2c.a(err.o) definition of common _f__formatted (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__hiwater (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__putn (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__reading (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__recpos (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__sequential (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__getn (size 4)
> /sw/lib/libg2c.a(err.o) definition of common _f__icptr (size 4)
> /sw/lib/libg2c.a(err.o) definition of common f(short, void, int, char) 
> (size 4)
> /usr/bin/libtool: internal link edit command failed
> make[4]: *** [libRlapack.dylib] Error 1
> make[3]: *** [R] Error 2
> make[2]: *** [R] Error 1
> make[1]: *** [R] Error 1
> make: *** [R] Error 1
> [beethoven:~/INSTALL/R-1.7.1] herron%
> 
> Any ideas here?
> 
> Thanks, michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Mon Jun 30 19:00:52 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jun 2003 17:00:52 -0000
Subject: [R] dropping factor levels in subset
In-Reply-To: <CDA8D2689259E444942B3CDED8DD912932D46B@MBXSRV03.stf.nus.edu.sg>
References: <CDA8D2689259E444942B3CDED8DD912932D46B@MBXSRV03.stf.nus.edu.sg>
Message-ID: <6r8yrjsdog.fsf@bates4.stat.wisc.edu>

"Adaikalavan Ramasamy" <gisar at nus.edu.sg> writes:

> Another option is pruneLevels() in library nlme.
> 
> x <- factor( c( 0,1,2,1,2 ) )
> > x
> [1] 0 1 2 1 2
> Levels: 0 1 2
> > pruneLevels( x[-1] )
> [1] 1 2 1 2
> Levels: 1 2

That function has been removed from the latest release of the nlme
package because it not needed.  All uses of pruneLevels in nlme were
replaced by code of the form

 myfactor[] = myfactor[, drop = TRUE]



From baliola at riseup.net  Mon Jun 30 21:12:30 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Mon, 30 Jun 2003 19:12:30 +0000
Subject: [R] spatial correlation test
In-Reply-To: <3F0055E5.7010300@lancaster.ac.uk>
References: <200306301648.22155.baliola@riseup.net>
	<3F0055E5.7010300@lancaster.ac.uk>
Message-ID: <200306301828.44882.baliola@riseup.net>

On Monday 30 June 2003 15:23, Barry Rowlingson wrote:
>   Think you may be looking at the wrong sort of spatial correlation! For
> Geary tests you are comparing 'adjacent' objects, where adjacency is
> defined however you want - N-nearest neighbours, shared border between
> regions etc etc.

sorry, I misunderstood the purpose of geary's I test, thanks for this info. 

>   When you say 'sampled data' it sounds more like you've got samples
> taken at locations, and you want to investigate spatial correlation as a
> function of distance between samples? Am I guessing right?

Yes you are right. I want to look for spatial correlation of my samples as a 
function of distances between sampling sites (x,y coords). 

>   Take a look at some of the R kriging libraries, which will have
> functions to plot variograms. This is a plot of something like
> E(|Y_i - Y_j|) against distance.
>
> Baz

I found variograms() and correlograms(), but is there a way to get the a 
p-value for spatial correlation? 
additionaly I found sp.correlogram() but again with this mysterious "nb 
class".


thanks, martin



From f.calboli at ucl.ac.uk  Mon Jun 30 19:47:15 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Mon, 30 Jun 2003 18:47:15 +0100
Subject: [R] degrees of freedom in a LME model
In-Reply-To: <6rhe67sezw.fsf@bates4.stat.wisc.edu>
References: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
	<3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
Message-ID: <3.0.6.32.20030630184715.02a03cd0@pop-server.ucl.ac.uk>

Dear Prof. Bates,

Thank you for your reply. I did actually check the book at pages 89-92, but
I have to say I found it a bit "cryptic", if not downright confusing, for a
genetist like me. Any day of the week. 

To use the example in the book, I cannot see why MACHINE is inner to
WORKER. Or where the Pi = sum of df for the term estimatet at level i... 

To me it's just a two way anova with one random effect + one interaction.
My df correspond to those in the book, but I calculated them in the "bog
standard" way of 2 df for 3 machines, 5 df for 6 workers and 2 * 5 = 10 df
for the interaction...oh well...

I attach the datasest (I did a sample with replace = TRUE, should have
thought of this earlier...). I hope thing would be clearer. It is obvious
that if I consider day a "factor", as I have just one datapoint per day per
line within regime, I end up using all my degrees of freedom:

2 df for 3 regimes
6 df for line within regime
8 df for day (9 days, day as factor)
16 df for regime * day
48 df for line * day
total 80 df, with 0 df remaining for the error. A bit of a problem I
daresay, but I did not collect the data!

BUT I did calculate my anova considering day as a continuous variable,

2 df for 3 regimes
6 df for line within regime
1 df for day (day as a number)
2 df for regime * day
6 df for line * day
total 17 df, with 63 df remaining for the error. 

I still do not get why the interaction term Regime*Day is not tested on the
interaction Line-in-regime*Day... In my analysis LME is clumping Error and
Line-in-regime*Day together, judging by the df I was talking about in my
previuos email. To me it should be just another "bog standard"
situation...But if I were smart enough to be a statistician I would not be
here doing genetics ;)

Regards,
Federico Calboli



Day	Line	Regime	Matings
2	501	es	0.4
4	501	es	0.32989691
9	501	es	0.48484848
11	501	es	0.72727273
16	501	es	0.34042553
18	501	es	0.56470588
25	501	es	0.37509377
30	501	es	0.22222222
32	501	es	0.77777778
2	502	es	0.57142857
4	502	es	1.06666667
9	502	es	0.16
11	502	es	0.4
16	502	es	0.4
18	502	es	0.33333333
25	502	es	0.80808081
30	502	es	0.48
32	502	es	0.25531915
2	503	es	0
4	503	es	0.72727273
9	503	es	0.3
11	503	es	0.77777778
16	503	es	0.34042553
18	503	es	1.06666667
25	503	es	0.32989691
30	503	es	1.24444444
32	503	es	0.19153725
2	fb1	fb	0.72727273
4	fb1	fb	0.42105263
9	fb1	fb	0.32989691
11	fb1	fb	0.32323232
16	fb1	fb	0.37509377
18	fb1	fb	0.51612903
25	fb1	fb	0.4
30	fb1	fb	0.24742268
32	fb1	fb	0.24742268
2	fb2	fb	1.33333333
4	fb2	fb	0.25263158
9	fb2	fb	0.66666667
11	fb2	fb	1.06666667
16	fb2	fb	0.97959184
18	fb2	fb	0.42105263
25	fb2	fb	0.57142857
30	fb2	fb	1.15555556
32	fb2	fb	0.80808081
2	fb3	fb	0.18952106
4	fb3	fb	0.66666667
9	fb3	fb	1.22033898
11	fb3	fb	0.35955056
16	fb3	fb	1.68421053
18	fb3	fb	0.57461174
25	fb3	fb	0.93506493
30	fb3	fb	0.80808081
32	fb3	fb	0.57461174
2	mb1	mb	0.22222222
4	mb1	mb	0.3902439
9	mb1	mb	0.42105263
11	mb1	mb	0.48484848
16	mb1	mb	0.55555556
18	mb1	mb	1.68421053
25	mb1	mb	0.4
30	mb1	mb	0.16
32	mb1	mb	0.09523809
2	mb2	mb	0.77777778
4	mb2	mb	0.68817204
9	mb2	mb	0.25263158
11	mb2	mb	1.33333333
16	mb2	mb	0.5
18	mb2	mb	0.37509377
25	mb2	mb	0.16494845
30	mb2	mb	2.46153846
32	mb2	mb	0.47058824
2	mb3	mb	1.06666667
4	mb3	mb	0.5
9	mb3	mb	0.5
11	mb3	mb	0.38095238
16	mb3	mb	0.26373626
18	mb3	mb	0.87912088
25	mb3	mb	0.55555556
30	mb3	mb	0.74418605
32	mb3	mb	1.33333333

>
>I think your calculation is based on using only within-strata
>information whereas lme uses both within-strata and between-strata
>information for estimates of effects.
>
>The way that we calculate denominator degrees of freedom is described
>on pp. 90-91 of Pinheiro and Bates (2000).  For each term in the
>fixed-effects we determine the innermost level of the random effects
>at which is it changing.  Because Regime is constant for each Line it
>has the fewest degrees of freedom but Day is changing within Line so
>terms in Day have more degrees of freedom.  Is this what you intended?
>
>I must admit I am having difficulty understanding the structure of the
>experiment but it is still Monday morning for me so perhaps that is
>not surprising.


=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From Roger.Bivand at nhh.no  Mon Jun 30 19:59:43 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 30 Jun 2003 19:59:43 +0200 (CEST)
Subject: [R] spatial correlation test
In-Reply-To: <200306301828.44882.baliola@riseup.net>
Message-ID: <Pine.LNX.4.44.0306301938210.3021-100000@reclus.nhh.no>

(I think Barry Rowlingson replied off-list, both his advice and St?phane 
Dray's reply are relevant)

On Mon, 30 Jun 2003, Martin Wegmann wrote:

> On Monday 30 June 2003 15:23, Barry Rowlingson wrote:
> >   Think you may be looking at the wrong sort of spatial correlation! For
> > Geary tests you are comparing 'adjacent' objects, where adjacency is
> > defined however you want - N-nearest neighbours, shared border between
> > regions etc etc.
> 
> sorry, I misunderstood the purpose of geary's I test, thanks for this info. 
> 
> >   When you say 'sampled data' it sounds more like you've got samples
> > taken at locations, and you want to investigate spatial correlation as a
> > function of distance between samples? Am I guessing right?
> 
> Yes you are right. I want to look for spatial correlation of my samples as a 
> function of distances between sampling sites (x,y coords). 
> 
> >   Take a look at some of the R kriging libraries, which will have
> > functions to plot variograms. This is a plot of something like
> > E(|Y_i - Y_j|) against distance.
> >
> > Baz
> 
> I found variograms() and correlograms(), but is there a way to get the a 
> p-value for spatial correlation? 
> additionaly I found sp.correlogram() but again with this mysterious "nb 
> class".
> 
There is a literature that you will find referenced on help pages of the 
functions that you are interested in. For geary.test(), the reference is: 
Cliff, A. D., Ord, J. K. 1981 Spatial processes, Pion, p. 21.

For sp.correlogram(): Cliff, A. D., Ord, J. K. 1981 Spatial processes,
Pion, pp.  118-122, Martin, R. L., Oeppen, J. E. 1975 The identification
of regional forecasting models using space-time correlation functions,
Transactions of the Institute of British Geographers, 66, 95-118.

These are the places to look first. For nb2listw() - Tiefelsdorf, M.,
Griffith, D. A., Boots, B. 1999 A variance-stabilizing coding scheme for
spatial link matrices, Environment and Planning A, 31, pp. 165-180.

The "nb" class defines neighbour relations needed to carry out further
calculations, and is "a list of integer vectors containing neighbour
region number ids", quoting its documentation in the example St?phane 
Dray used, tri2nb() to generate neighbours from points by triangulation.

If your point data are not areal but are sampled from a possibly
continuous surface, then, as Barry Rowlingson suggested, you could look at
one or other of the geostatistical packages, for example sgeostat.
However, asking for a p-value implies that you are testing some kind of a
hypothesis, doesn't it?

It is possible to do Moran tests within a testing framework in
sp.correlogram(), and indeed to provide nb2listw() with inverse distance
weights, but it isn't clear that this would answer your underlying 
research question.

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From wib2004 at med.cornell.edu  Mon Jun 30 20:08:32 2003
From: wib2004 at med.cornell.edu (William Briggs)
Date: Mon, 30 Jun 2003 14:08:32 -0400
Subject: [R] date to age
Message-ID: <011a01c33f32$9de6a900$c559fb8c@matt>

I have files which have columns of data that look like this:

DOB
9/27/1964
...

That is, dates in month/day/year format.  When variables like DOB are read
in, they are converted to factors.

So, I wrote this to convert from date to age in years:

age<-function(x)
{as.numeric(Sys.time()-strptime(as.character(x),format="%m/%d/%Y"))/365.25}

This isn't very precise or pretty, but it works.

Does anybody have something better?



From ripley at stats.ox.ac.uk  Mon Jun 30 20:25:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jun 2003 19:25:42 +0100 (BST)
Subject: [R] date to age
In-Reply-To: <011a01c33f32$9de6a900$c559fb8c@matt>
Message-ID: <Pine.LNX.4.44.0306301923410.14931-100000@gannet.stats>

On Mon, 30 Jun 2003, William Briggs wrote:

> I have files which have columns of data that look like this:
> 
> DOB
> 9/27/1964
> ...
> 
> That is, dates in month/day/year format.  When variables like DOB are read
> in, they are converted to factors.

Why?  You can avoid that, of course.

> So, I wrote this to convert from date to age in years:
> 
> age<-function(x)
> {as.numeric(Sys.time()-strptime(as.character(x),format="%m/%d/%Y"))/365.25}
> 
> This isn't very precise or pretty, but it works.
> 
> Does anybody have something better?

Convert to POSIXlt and compare differences in years, months, days etc.
Write an extension to difftime() to handle years and contribute it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From masaei at affrc.go.jp  Mon Jun 30 21:14:53 2003
From: masaei at affrc.go.jp (SATO Masaei)
Date: Tue, 01 Jul 2003 04:14:53 +0900 (JST)
Subject: [R] Seemingly unrelated regressions
In-Reply-To: <3F00631C.2020606@seanet.com>
References: <3F00631C.2020606@seanet.com>
Message-ID: <20030701.041453.74736560.masaei@affrc.go.jp>

> Is there an implementation of SUR in R?
> These are used commonly in economics.
> Thanks
> 
> -- 
> Richard A. Hinrichsen

You can use the function 'sur' in the library 'systemfit'.

Masaei Sato



From SuzieBlatt at netscape.net  Mon Jun 30 21:29:04 2003
From: SuzieBlatt at netscape.net (Suzanne E. Blatt)
Date: Mon, 30 Jun 2003 15:29:04 -0400
Subject: [R] Deciphering an error message
Message-ID: <64F2BE2E.3667B780.0D1322AF@netscape.net>


Hello,

I'm working in spatstat and having difficulty with the ppp objects.  I can get a ppp object for one set of data, but with the same code applied to a second data set (all I am changing is the field identifier), I get the following error message:

Error in switch(w$type, rectangle={: internal error: some total scores are neither 0 nor 1

Any thoughts on what this means and how to correct it would be most appreciated.

Thanks,
Suzanne

__________________________________________________________________
McAfee VirusScan Online from the Netscape Network.
Comprehensive protection for your entire computer. Get your free trial today!
http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397

Get AOL Instant Messenger 5.1 free of charge.  Download Now!



From spencer.graves at pdf.com  Mon Jun 30 21:54:06 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 30 Jun 2003 12:54:06 -0700
Subject: [R] Novice Questions
References: <20030630153223.12752.qmail@web13702.mail.yahoo.com>
Message-ID: <3F00955E.6090408@pdf.com>

Hi, Bruce:

	  I'm overwhelmed.  Questioners seem to get quicker, more informative 
responses to questions that are short, well written and easily and 
quickly understood, and preferably include a toy example that someone 
else can run quickly to reproduce the problem and to evaluate 
alternative solutions.

	  I see you are using "source" and would like to get more output.  Did 
you check "?source"?  This function in R has arguments "echo" and 
"verbose", which may produce what you want.  If you've already tried 
that, then the problem may be output buffering, and I don't know how to 
modify that parameter.  A search of "www.r-project.org" -> search -> "R 
Site Search" might help.  Or ask a question focused specifically on 
that, giving also which version of R you are using under which operating 
system.

	  I rarely use "source".  More often, I have R commands in another file 
and copy and paste into R the commands I want to run.  That makes it 
easier for me to isolate errors, etc.

	  This does not address all your questions, but it's a start.

Hope this helps.
spencer graves

Bruce Moore wrote:
> I'm writing a program to perform linear regressions to
> estimate the number of bank teller transactions per
> hour of various types based upon day of week, time of
> day, week of month and several prices.  I've got about
> 25,000 records in my dataset, 85 columns of
> transaction counts (used 1 at a time), about 50
> columns of binary indicators (day, week, pay period,
> hour, branch), and a half dozen real valued prices.
> 
> My program hangs on some regressions as I add
> interactions, probably due to logic problems in my
> code or collinearity problems in the data.
> 
> 1) I'm running my program via the source() command. 
> It appears that source() does not print any messages
> until it completes.  
> 
> ---->Is there a way to get diagnostic messages to
> print immediately rather than when the source()
> command has completed?
> 
> 2) I'm fairly certain that I've got some collinearity
> in the data set and the interactions.  I've found an
> append (Ott Toomet 5/30/2003) that talks about a
> procedure to find collinearity problems using
> model.matrix() to generate the dataset with
> interactions and kappa() to determine the condition
> number of the matrix.  
> 
> ---->Is there a more automated way to find collinear
> variables?
> 
> 3) Is there a way to get lm() and/or step() or some
> other package to give a model with only coefficients
> that are significant at a particular level?
> 
> 4) Is there a way to suppress display of a password
> when using the RODBC odbcConnect() function, or to get
> the function to prompt for a password?
> 
> 5) What is the practical size limit on the number of
> terms in model?  I know that I won't be able to
> consider all interactions, but would like to have some
> idea when to give up and go with what I've got.
> 
> 
> 
> =====
> Bruce Moore
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From brahm at alum.mit.edu  Mon Jun 30 21:59:55 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Mon, 30 Jun 2003 15:59:55 -0400
Subject: [R] dev.copy()
Message-ID: <16128.38587.620427.830712@arbres1a.fmr.com>

In a batch script (i.e. there is no screen device), I would like to create both
a PDF file and a PNG file from the same plot.  I thought this would be the way
to do it, but it isn't:

 R> bitmap("copy.png", "png16m", res=300)
 R> pdf("copy.pdf")
 R> plot(1:10, 1:10)     # Plot into pdf file
 R> dev.copy(which=2)    # Copy to png file
 R> dev.off(3)           # Close pdf
 R> dev.off(2)           # Close png

The PDF comes out fine, but the PNG appears blank.  Any ideas what I'm doing
wrong?  TIA.
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From kjetil at entelnet.bo  Mon Jun 30 18:14:35 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 30 Jun 2003 12:14:35 -0400
Subject: [R] if else statements in R
In-Reply-To: <5.1.0.14.0.20030630112404.00a79700@mail.utm.utoronto.ca>
Message-ID: <3F0029AB.22466.A15341@localhost>

On 30 Jun 2003 at 11:24, Michael Rennie wrote:

Hola!

> 
> Hi, there
> 
> I am a grad student struggling to get my syntax right in a model I am 
> building in R.
> 
> I am trying to specify a variable,W, in a loop, such that on the first 
> iteration, it is equal to Wo, my starting value, which I have already 
> specified with
> 
> Wo<-9.2
> 
>   On any other iteration other than the first, I want W to equal the 
> previous value of W in the iteration. plus an increment from that previous 
> iteration, divided by another value.  This then becomes the value used in 
> all calculations in the iteration at hand.
> 
> Here is my code, that does not work:

There are multiple problems with this code
1)  You use W as a name of a function AND as a name of an array 
inside that function. Might not be a syntax error (?), but is asking 
for problems. Use an other name for your array, and assign it first.
2) The variable i is not given a value anywhere in your code.
3)The condition in the if statement uses = and not ==, = is 
assignment, == is test lfor equality. Actuallt, your comp[i,1] gets 
assigned 1, that is also the value of the assignment, and in the 
logical test the 1 will be interpreted as TRUE, so you will never get 
the else branch!

> 
> W<- function(w)
> {
> if (comp[i,1]=1) W[i]<-Wo else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
> }

Maybe youwant something like:

W <- function(w) {
    result <- numeric(length=?)
    if (comp[i,1]==1) result[i] <- W0 else
          result[i] <- work[i-1,5] * work[i-1,13]/Ef
}

but it is really impossible to tell, as your function are using 
global variables. Another good idea is to avoid global variables.

Kjetil Halvorsen


> 
> 
> "work" is the dataframe that I am rbinding my interations into, and the W 
> variable is the 5th variable in that frame, and the additive to W is the 
> 13th variable in that frame.
> 
> However, this is where the program stalls on me:
> 
> {
> +
> + W<- function(w)
> + {
> + if (comp[i,1]=1) W[i]<-Wo else
> Error: syntax error
> Execution halted
> 
> I've also attempted this variation;
> 
> W<- function(w)
> 
> if (comp[i,1]=1) {W[i]<-Wo} else
> W[i]<-(work[i-1,5]work[i-1,13]*/Ef)
> 
> Which meets with an equal lack of success, and stalls in the same spot as 
> the previous version.
> 
> Does anyone have any hints on how to make this if else statement work to do 
> what I need?  This is my first time trying to use these statements, and the 
> S-plus manual i have is proving to be terribly unhelpful.
> 
> What am I doing wrong?
> 
> Mike
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mrennie at utm.utoronto.ca  Mon Jun 30 23:01:01 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 17:01:01 -0400
Subject: [R] Constructing loops in which i+1st element depends on ith  
Message-ID: <1057006861.3f00a50dafc9a@webmail.utm.utoronto.ca>


I feel greedy posting for help twice in one day- please forgive me, but the 
thesis can't wait.

I have been trying to get an ?if
else? statement working in a loop I am 
writing, in which I specify a variable value to 9.2 (Wo) on the first 
iteration, but for subsequent iterations, to adopt a value as a function of 
variables from the previous iteration.

When I comment out the ?if
else? statement, let W = Wo, and adjust the rest of 
the file so I am reading the first row of the variables specified instead of 
entire columns in my calculations, the calculations all work nicely without 
errors - I get the numbers I ought to in one row of ?bio?.

When I activate the loop and try to run the file with ?the if
.else?, it looks 
like it wants to work, but here?s where it stalls;


Error in "[<-"(*tmp*, i, value = ((bio[i - 1, 1] * bio[i - 1, 9])/Ef)) : 
	nothing to replace with
Execution halted

(see complete data file below)...

I guess it?s because it can?t find the alternate case because it hasn?t written 
it yet. 

So, my problem is two-fold:

1.	How can I write my loop properly so that at the end of each iteration, 
it records the row of data from ?bio? such that I can cbind it back to my ?Day? 
data?

2.	How can I specify the loop to use information from the i+1st element in 
calculations for calculations in the ith? 

I?m pretty sure my problem is in where I create and specify files in the loop, 
but beyond that, I don?t know.

Here?s the part of my command file that?s giving me trouble.  Everything before 
this is simply specifying variable names to numbers, which is not giving be 
problems.



#Bring in temp file

temper <- scan("temp2.dat", na.strings = ".", list(Day=0, Temp=0))

#Day = day on which iteration begins, starts at day 1, ends on 366
#Temp = temperature

Day <- temper$Day ; Temp<-temper$Temp ; 

temp<- cbind (Day, Temp)

#temp [,2]

p<- 0.558626306252032
ACT <- 1.66764519286918

Vc<-((CTM-temp[,2])/(CTM-CTO))
Vr<-((RTM-temp[,2])/(RTM-RTO))


comp<- cbind (Day, Temp, Vc, Vr)


bio<-NULL
M<- length(Day) #number of days iterated
for (i in 1:M)
{

weight<- function(Day)
{
W<-NULL
	if (Day[i]==1) W[i] <- Wo
	{W[i] <- ((bio[i-1,1]*bio[i-1,9])/Ef)
	}
	W
}

W<-weight(Day)
#W<-Wo

C<- p*CA*(W^CB)*((comp[,3]^Xc)*(exp(Xc*(1-comp[,3]))))*Pc

ASMR<- (ACT*RA*(W^(RB))*((comp[,4]^Xa)*(exp(Xa*(1-comp[,4])))))

SMR<- (ASMR/ACT)

A<- (ASMR-SMR)

F<- (FA*(comp[,2]^FB)*(exp(FG*p))*C)

U<- (UA*(comp[,2]^UB)*(exp(UG*p))*(C-F))

SDA<- (S*(C-F))

Gr<- (C-(ASMR+F+U+SDA))

bio<- rbind(c(W, C, ASMR, SMR, A, F, U, SDA, Gr))

dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))

}

bio


I would be grateful for any suggestions people might have.

Thanks, 

Mike



-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From mrennie at utm.utoronto.ca  Mon Jun 30 23:01:01 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 17:01:01 -0400
Subject: [R] Constructing loops in which i+1st element depends on ith  
Message-ID: <1057006861.3f00a50dafc9a@webmail.utm.utoronto.ca>


I feel greedy posting for help twice in one day- please forgive me, but the 
thesis can't wait.

I have been trying to get an ?if
else? statement working in a loop I am 
writing, in which I specify a variable value to 9.2 (Wo) on the first 
iteration, but for subsequent iterations, to adopt a value as a function of 
variables from the previous iteration.

When I comment out the ?if
else? statement, let W = Wo, and adjust the rest of 
the file so I am reading the first row of the variables specified instead of 
entire columns in my calculations, the calculations all work nicely without 
errors - I get the numbers I ought to in one row of ?bio?.

When I activate the loop and try to run the file with ?the if
.else?, it looks 
like it wants to work, but here?s where it stalls;


Error in "[<-"(*tmp*, i, value = ((bio[i - 1, 1] * bio[i - 1, 9])/Ef)) : 
	nothing to replace with
Execution halted

(see complete data file below)...

I guess it?s because it can?t find the alternate case because it hasn?t written 
it yet. 

So, my problem is two-fold:

1.	How can I write my loop properly so that at the end of each iteration, 
it records the row of data from ?bio? such that I can cbind it back to my ?Day? 
data?

2.	How can I specify the loop to use information from the i+1st element in 
calculations for calculations in the ith? 

I?m pretty sure my problem is in where I create and specify files in the loop, 
but beyond that, I don?t know.

Here?s the part of my command file that?s giving me trouble.  Everything before 
this is simply specifying variable names to numbers, which is not giving be 
problems.



#Bring in temp file

temper <- scan("temp2.dat", na.strings = ".", list(Day=0, Temp=0))

#Day = day on which iteration begins, starts at day 1, ends on 366
#Temp = temperature

Day <- temper$Day ; Temp<-temper$Temp ; 

temp<- cbind (Day, Temp)

#temp [,2]

p<- 0.558626306252032
ACT <- 1.66764519286918

Vc<-((CTM-temp[,2])/(CTM-CTO))
Vr<-((RTM-temp[,2])/(RTM-RTO))


comp<- cbind (Day, Temp, Vc, Vr)


bio<-NULL
M<- length(Day) #number of days iterated
for (i in 1:M)
{

weight<- function(Day)
{
W<-NULL
	if (Day[i]==1) W[i] <- Wo
	{W[i] <- ((bio[i-1,1]*bio[i-1,9])/Ef)
	}
	W
}

W<-weight(Day)
#W<-Wo

C<- p*CA*(W^CB)*((comp[,3]^Xc)*(exp(Xc*(1-comp[,3]))))*Pc

ASMR<- (ACT*RA*(W^(RB))*((comp[,4]^Xa)*(exp(Xa*(1-comp[,4])))))

SMR<- (ASMR/ACT)

A<- (ASMR-SMR)

F<- (FA*(comp[,2]^FB)*(exp(FG*p))*C)

U<- (UA*(comp[,2]^UB)*(exp(UG*p))*(C-F))

SDA<- (S*(C-F))

Gr<- (C-(ASMR+F+U+SDA))

bio<- rbind(c(W, C, ASMR, SMR, A, F, U, SDA, Gr))

dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))

}

bio


I would be grateful for any suggestions people might have.

Thanks, 

Mike



-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From kjetil at entelnet.bo  Mon Jun 30 23:26:49 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 30 Jun 2003 17:26:49 -0400
Subject: [R] sample from a list of names for random groups
In-Reply-To: <200306301711.13400.daniele.medri@libero.it>
References: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>
Message-ID: <3F0072D9.11852.1C96E8@localhost>

On 30 Jun 2003 at 17:11, Daniele Medri wrote:

You should really try to specify better what you want. Lets try

names # character vector with your names
m <- length(names)
n1 <- 5
n2 <- 10
muestra <- sample(names, m, replace=FALSE)
muestra1 <- muestra[1:n1]
muestra2 <- muestra[(n1+1):(n1+n2)]

Is this what you want?

Kjetil Halvorsen



> Dear R-users,
> 
> I need to sample from a list of names (people) and create 2 random groups with 
> unique item. Sure sample() function is a good point to start but I can't find 
> example to solve my work.
> 
> any tips? example?
> 
> Thank
> -- 
> Daniele Medri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Mon Jun 30 23:26:49 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 30 Jun 2003 17:26:49 -0400
Subject: [R] sample from a list of names for random groups
In-Reply-To: <200306301711.13400.daniele.medri@libero.it>
References: <Pine.LNX.4.44.0306301319030.3913-100000@gannet.stats>
Message-ID: <3F0072D9.11852.1C96E8@localhost>

On 30 Jun 2003 at 17:11, Daniele Medri wrote:

You should really try to specify better what you want. Lets try

names # character vector with your names
m <- length(names)
n1 <- 5
n2 <- 10
muestra <- sample(names, m, replace=FALSE)
muestra1 <- muestra[1:n1]
muestra2 <- muestra[(n1+1):(n1+n2)]

Is this what you want?

Kjetil Halvorsen



> Dear R-users,
> 
> I need to sample from a list of names (people) and create 2 random groups with 
> unique item. Sure sample() function is a good point to start but I can't find 
> example to solve my work.
> 
> any tips? example?
> 
> Thank
> -- 
> Daniele Medri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



