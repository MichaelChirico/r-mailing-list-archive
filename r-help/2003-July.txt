From sarahmclean9 at yahoo.co.nz  Tue Jul  1 03:52:43 2003
From: sarahmclean9 at yahoo.co.nz (=?iso-8859-1?q?Sarah=20Mclean?=)
Date: Tue, 1 Jul 2003 13:52:43 +1200 (NZST)
Subject: [R] crossed random effects
Message-ID: <20030701015243.48313.qmail@web40014.mail.yahoo.com>

Hi,

I have a data set on germination and plant growth with
the following variables:

dataset=fm
mass (response)
sub (fixed effect)
moist (fixed effect)
pop (fixed effect)
mum (random effect nested within population)
iheight (covariate)
plot (random effect- whole plot factor for split-plot
design).

I want to see if moist or sub interacts with mum for
any of the pops, but I am getting an error message. 

This is the formula I used:
fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
fm$grp = as.factor(rep(1,nrow(fm)))
fm$pl <- getGroups(fm, ~1|plot)
fm$mo <- getGroups(fm, ~1|moist)
fm$su <- getGroups(fm, ~1|sub)
> fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
data=fm, random=list(grp=pdBlocked(list(pdIdent(~pl -
1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
pdIdent(~pmu:mo - 1)))))
Error in chol((value + t(value))/2) : non-positive
definite matrix in chol

I know the problem is with the random interaction
terms, but I don't know how to overcome this.

Any advice would be greatly appreciated. I'm new to R
and analysis such as this.

Thank you,

Sarah Mclean
sarahmclean9 at yahoo.co.nz


http://mobile.yahoo.com.au - Yahoo! Mobile
- Check & compose your email via SMS on your Telstra or Vodafone mobile.



From mrennie at utm.utoronto.ca  Tue Jul  1 04:11:00 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 22:11:00 -0400
Subject: [R] Creating a loop that works....
Message-ID: <1057025460.3f00edb46b6d9@webmail.utm.utoronto.ca>


Hi there,

First off, thanks to everyone who has helped me so far.  Sorry to keep 
pestering you all.

I'm including my code here, and I will comment down it where it is that I am 
having problems figuring out how to write this damn thing.


> temper <- scan("temp2.dat", na.strings = ".", list(Day=0, Temp=0))
Read 366 records
> 
> Day <- temper$Day ; Temp<-temper$Temp ; 
> 
> temp<- cbind (Day, Temp)
> #Day = number of days modelled, Temp = daily avg. temp.
> #temp [,2]
> 
> p<- 0.558626306252032
> ACT <- 1.66764519286918
> 
> Vc<-((CTM-temp[,2])/(CTM-CTO))
> Vr<-((RTM-temp[,2])/(RTM-RTO))
> 
> 
> comp<- cbind (Day, Temp, Vc, Vr)
> 
> 
> bio<-NULL
> M<- length(Day) #number of days iterated
> for (i in 1:M)
+ {
+ 
+ weight<- function(Day)
+ {
+ W<-NULL
+ 	if (Day[i]==1) {W[i] <- Wo}
+ else    if (Day[i]>1) {W[i] <- ((bio[i-1,1]*bio[i-1,9])/Ef)
+ 	}
+ 	W
+ }
+ 
+ W<-weight(Day)

The problem, as many of you have already identified, is right here. I hope I 
finally have the syntax right, but even if the "if else" is coded properly, I 
don't think R can find the values in the second condition I list. I need W in 
each step of the iteration to change slightly, based on the mess of 
calculations below (which are using parameters that I have already specified).  
After all the calculations are made, I hope to get values in bio[i,1] and bio
[i,9] corresponding to the iteration that just occured, then return to the top 
of the loop to combine them into the value of W for the next iteration. What I 
think is happening here is that R is looking for values in the condition before 
they are actually there- the way I've written it, they can't be there until I 
get past the conditional step.  Which means I am coding this all wrong.  That 
said, I'm not sure how else to do it;  the value of W in the next iteration is 
dependent on the values of Gr and W in the previous iteration, with the 
exception of the first one (Day=1).  I've tried defining "bio" as 

bio<-matrix(NA, ncol=9, nrow=366)

but that doesn't help either.

Perhaps my rbind at the end of the file is incorrect? I think maybe I'm getting 
mixed up between calculating vectors and values-- should I be specifying [i] 
for everything below where I am now specifying vecotrs?  

+ #W<-Wo
+ 
+ C<- p*CA*(W^CB)*((comp[,3]^Xc)*(exp(Xc*(1-comp[,3]))))*Pc
+ 
+ ASMR<- (ACT*RA*(W^(RB))*((comp[,4]^Xa)*(exp(Xa*(1-comp[,4])))))
+ 
+ SMR<- (ASMR/ACT)
+ 
+ A<- (ASMR-SMR)
+ 
+ F<- (FA*(comp[,2]^FB)*(exp(FG*p))*C)
+ 
+ U<- (UA*(comp[,2]^UB)*(exp(UG*p))*(C-F))
+ 
+ SDA<- (S*(C-F))
+ 
+ Gr<- (C-(ASMR+F+U+SDA))
+ #Day, Temp, Vc, Vr, W, C, ASMR, SMR, A, F, U, SDA, Gr)
+ 
+ bio<- rbind(c(W, C, ASMR, SMR, A, F, U, SDA, Gr))
+ 
+ dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))
+ 
+ }
Error: length of dimnames[2] not equal to array extent
Execution halted


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From mrennie at utm.utoronto.ca  Tue Jul  1 04:11:00 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 30 Jun 2003 22:11:00 -0400
Subject: [R] Creating a loop that works....
Message-ID: <1057025460.3f00edb46b6d9@webmail.utm.utoronto.ca>


Hi there,

First off, thanks to everyone who has helped me so far.  Sorry to keep 
pestering you all.

I'm including my code here, and I will comment down it where it is that I am 
having problems figuring out how to write this damn thing.


> temper <- scan("temp2.dat", na.strings = ".", list(Day=0, Temp=0))
Read 366 records
> 
> Day <- temper$Day ; Temp<-temper$Temp ; 
> 
> temp<- cbind (Day, Temp)
> #Day = number of days modelled, Temp = daily avg. temp.
> #temp [,2]
> 
> p<- 0.558626306252032
> ACT <- 1.66764519286918
> 
> Vc<-((CTM-temp[,2])/(CTM-CTO))
> Vr<-((RTM-temp[,2])/(RTM-RTO))
> 
> 
> comp<- cbind (Day, Temp, Vc, Vr)
> 
> 
> bio<-NULL
> M<- length(Day) #number of days iterated
> for (i in 1:M)
+ {
+ 
+ weight<- function(Day)
+ {
+ W<-NULL
+ 	if (Day[i]==1) {W[i] <- Wo}
+ else    if (Day[i]>1) {W[i] <- ((bio[i-1,1]*bio[i-1,9])/Ef)
+ 	}
+ 	W
+ }
+ 
+ W<-weight(Day)

The problem, as many of you have already identified, is right here. I hope I 
finally have the syntax right, but even if the "if else" is coded properly, I 
don't think R can find the values in the second condition I list. I need W in 
each step of the iteration to change slightly, based on the mess of 
calculations below (which are using parameters that I have already specified).  
After all the calculations are made, I hope to get values in bio[i,1] and bio
[i,9] corresponding to the iteration that just occured, then return to the top 
of the loop to combine them into the value of W for the next iteration. What I 
think is happening here is that R is looking for values in the condition before 
they are actually there- the way I've written it, they can't be there until I 
get past the conditional step.  Which means I am coding this all wrong.  That 
said, I'm not sure how else to do it;  the value of W in the next iteration is 
dependent on the values of Gr and W in the previous iteration, with the 
exception of the first one (Day=1).  I've tried defining "bio" as 

bio<-matrix(NA, ncol=9, nrow=366)

but that doesn't help either.

Perhaps my rbind at the end of the file is incorrect? I think maybe I'm getting 
mixed up between calculating vectors and values-- should I be specifying [i] 
for everything below where I am now specifying vecotrs?  

+ #W<-Wo
+ 
+ C<- p*CA*(W^CB)*((comp[,3]^Xc)*(exp(Xc*(1-comp[,3]))))*Pc
+ 
+ ASMR<- (ACT*RA*(W^(RB))*((comp[,4]^Xa)*(exp(Xa*(1-comp[,4])))))
+ 
+ SMR<- (ASMR/ACT)
+ 
+ A<- (ASMR-SMR)
+ 
+ F<- (FA*(comp[,2]^FB)*(exp(FG*p))*C)
+ 
+ U<- (UA*(comp[,2]^UB)*(exp(UG*p))*(C-F))
+ 
+ SDA<- (S*(C-F))
+ 
+ Gr<- (C-(ASMR+F+U+SDA))
+ #Day, Temp, Vc, Vr, W, C, ASMR, SMR, A, F, U, SDA, Gr)
+ 
+ bio<- rbind(c(W, C, ASMR, SMR, A, F, U, SDA, Gr))
+ 
+ dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))
+ 
+ }
Error: length of dimnames[2] not equal to array extent
Execution halted


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From znmeb at aracnet.com  Tue Jul  1 05:16:06 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Mon, 30 Jun 2003 20:16:06 -0700
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <20030630074922.3AE427CA825@tango.stat.unipd.it>
Message-ID: <001101c33f7f$1d456730$73c463d8@plaza.ds.adp.com>

Unfortunately, the data are *non-negative*, not strictly positive. Zero is a
valid and frequent inter-arrival time. It is, IIRC, the most likely value of
a (negative) exponential distribution.

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD


> -----Original Message-----
[snip]

> if you data are positive, you could use
> 
>   sm.density(..., positive=TRUE)



From znmeb at aracnet.com  Tue Jul  1 05:33:19 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Mon, 30 Jun 2003 20:33:19 -0700
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <Pine.LNX.4.44.0306300756140.32578-100000@gannet.stats>
Message-ID: <001201c33f81$87bc6df0$73c463d8@plaza.ds.adp.com>

Thanks!! It does look like the easiest thing is direct ML; the code for a
normal mixture is in the book, so all I have to do is modify that for a sum
of a hyper-exponential, for which I have an approximate mean and CV, and a
normal, for which I have an approximate mean and SD.

I have two big peaks, one near zero which is probably hyperexponential with
a CV about 3, and the other near 600 seconds (a refresh that happens every
ten minutes) which looks Gaussian with a very small standard deviation. I
think what I'm going to do is fit the two peaks using ML, since I know where
they are, then subtract them out and look at the structure of the residuals.
The stuff over 600 seconds is sparse and totally uninteresting. After I'm
done with this, I get to look at the distribution of the network traffic.
The good news is that I get those inter-arrival times to the nearest
microsecond. :)

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD


> -----Original Message-----
[snip]

> For all of these see MASS (the book) and its on-line complements.



From sarahmclean9 at yahoo.co.nz  Tue Jul  1 07:32:22 2003
From: sarahmclean9 at yahoo.co.nz (=?iso-8859-1?q?Sarah=20Mclean?=)
Date: Tue, 1 Jul 2003 17:32:22 +1200 (NZST)
Subject: [R] crossed random effects
Message-ID: <20030701053222.7353.qmail@web40005.mail.yahoo.com>

Hi,

if I have posted this twice, please ignore this. I'm
not sure if I sent it to the correct e-mail address
the first time.

I have a data set on germination and plant growth with
the following variables:

dataset=fm
mass (response)
sub (fixed effect)
moist (fixed effect)
pop (fixed effect)
mum (random effect nested within population)
iheight (covariate)
plot (random effect- whole plot factor for split-plot
design).

I want to see if moist or sub interacts with mum for
any of the pops, but I am getting an error message. 

This is the formula I used:
fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
fm$grp = as.factor(rep(1,nrow(fm)))
fm$pl <- getGroups(fm, ~1|plot)
fm$mo <- getGroups(fm, ~1|moist)
fm$su <- getGroups(fm, ~1|sub)
> fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
data=fm, random=list(grp=pdBlocked(list(pdIdent(~pl -
1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
pdIdent(~pmu:mo - 1)))))
Error in chol((value + t(value))/2) : non-positive
definite matrix in chol

I know the problem is with the random interaction
terms, but I don't know how to overcome this.

Any advice would be greatly appreciated. I'm new to R
and analysis such as this.

Thank you,

Sarah Mclean
sarahmclean9 at yahoo.co.nz


http://mobile.yahoo.com.au - Yahoo! Mobile
- Check & compose your email via SMS on your Telstra or Vodafone mobile.



From gblevins at mn.rr.com  Tue Jul  1 08:08:33 2003
From: gblevins at mn.rr.com (Greg Blevins)
Date: Tue, 1 Jul 2003 01:08:33 -0500
Subject: [R] Creating a co-occurence matrix
Message-ID: <02bd01c33f97$33b94fa0$1c361d41@mn.rr.com>

Hello R experts,

I have a data.frame which has a series of 30 variables that are each coded
1,0--1
if a behavior is engaged in, 0 otherwise.

The data.frame looks like this:

                        var1 var2 ...var30
Respondent 1
Respondent 2
etc

I would like to create a matrix (or at least a half-matrix) as follows:

              var1  var2 ...var30
var1
var 2
.
.
var 30

where the number at each intersection is the number of people who engaged in
both behaviors.  I could run a whole bunch of tables and input the data by
hand into a new matrix, but I was wondering if there is a way to do this via
a program/function (I looked at "daisy," but concluded this function would
not handle this).

Any help would be appreciated!

Greg Blevins
The Market Solutions Group



From j.b.bremnes at met.no  Tue Jul  1 09:13:44 2003
From: j.b.bremnes at met.no (=?ISO-8859-1?Q?John_Bj=F8rnar_Bremnes?=)
Date: Tue, 01 Jul 2003 07:13:44 +0000
Subject: [R] R CMD check
In-Reply-To: <Pine.LNX.4.44.0306301537370.4454-100000@gannet.stats>
References: <Pine.LNX.4.44.0306301537370.4454-100000@gannet.stats>
Message-ID: <3F0134A8.6010807@met.no>

The error was simply a missing end-of-line character in the last line in 
one of the R files.

Thanks to Brian Ripley and Roger Bivand.


-- 
John Bjornar Bremnes
Norwegian Meteorological Institute (met.no)
Research and Development Department
P.O.Box 43 Blindern, N-0313 Oslo, Norway
Phone: (+47) 2296 3326. Fax: (+47) 2269 6355

Prof Brian Ripley wrote:
> You have an error in the R files of your package.  Before you even do
> R CMD check, do try loading the package in R.
> 
> On Mon, 30 Jun 2003, John Bj?rnar Bremnes wrote:
> 
> 
>>when using R CMD check mypkg I get the error message
>>...
>>* checking R files for library.dynam ... OK
>>* checking generic/method consistency ... WARNING
>>Error in .loadPackageQuietly(package, lib.loc) :
>>         Error in parse(file, n, text, prompt) : syntax error on line 95
>>Execution halted
>>* checking for assignment functions with final arg not named 'value' ... 
>>WARNING
>>Error in .loadPackageQuietly(package, lib.loc) :
>>         Error in parse(file, n, text, prompt) : syntax error on line 95
>>Execution halted
>>...
>>
>>What can I do to avoid this? I use R-1.7.0 on Linux RedHat.
>>
>>Any suggestions are appreciated.
>>
>>
> 
> 


-- 
John Bjornar Bremnes
Norwegian Meteorological Institute (met.no)
Research and Development Department
P.O.Box 43 Blindern, N-0313 Oslo, Norway
Phone: (+47) 2296 3326. Fax: (+47) 2269 6355



From ligges at statistik.uni-dortmund.de  Tue Jul  1 09:19:42 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jul 2003 09:19:42 +0200
Subject: [R] Deciphering an error message
In-Reply-To: <64F2BE2E.3667B780.0D1322AF@netscape.net>
References: <64F2BE2E.3667B780.0D1322AF@netscape.net>
Message-ID: <3F01360E.6020105@statistik.uni-dortmund.de>

Suzanne E. Blatt wrote:
> Hello,
> 
> I'm working in spatstat and having difficulty with the ppp objects.  I can get a ppp object for one set of data, but with the same code applied to a second data set (all I am changing is the field identifier), I get the following error message:
> 
> Error in switch(w$type, rectangle={: internal error: some total scores are neither 0 nor 1

What is a "ppp object"?
Which function has been applied and generated that error message?

The error message tells you some values a ought to sum up to 0 or 1 but 
they don't. I (and perhaps others as well) cannot help when you don't 
specify some relevant information.

Uwe Ligges

> Any thoughts on what this means and how to correct it would be most appreciated.
> 
> Thanks,
> Suzanne
> 
> __________________________________________________________________
> McAfee VirusScan Online from the Netscape Network.
> Comprehensive protection for your entire computer. Get your free trial today!
> http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397
> 
> Get AOL Instant Messenger 5.1 free of charge.  Download Now!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From athanasia.kamariotis at epfl.ch  Tue Jul  1 09:24:43 2003
From: athanasia.kamariotis at epfl.ch (ATHANASIA KAMARIOTIS)
Date: Tue, 01 Jul 2003 09:24:43 +0200
Subject: [R] masked objects
Message-ID: <4422f460d8.460d84422f@imap.epfl.ch>

When opening the software R it appears this message:



-----------------------------------------------------------------
Attaching package 'methods':


        The following object(s) are masked _by_ .GlobalEnv :

         new 


------------------------------------------------------------------


May you please answer to the following question: How can I restore the 
object new?

Merci d'avance,
Cordialement,
Athanasia



From ligges at statistik.uni-dortmund.de  Tue Jul  1 09:36:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jul 2003 09:36:05 +0200
Subject: [R] Creating a loop that works....
In-Reply-To: <1057025460.3f00edb46b6d9@webmail.utm.utoronto.ca>
References: <1057025460.3f00edb46b6d9@webmail.utm.utoronto.ca>
Message-ID: <3F0139E5.6040309@statistik.uni-dortmund.de>

Please don't send messages twice (to r-help at ... and R-help at ...).
I'm not going to read all your code, understand your problems and write 
your programs. That's probably the right job for a consultant, or a 
programmer at your department.

Uwe Ligges


Michael Rennie wrote:

> Hi there,
> 
> First off, thanks to everyone who has helped me so far.  Sorry to keep 
> pestering you all.
> 
> I'm including my code here, and I will comment down it where it is that I am 
> having problems figuring out how to write this damn thing.
> 
> 
> 
>>temper <- scan("temp2.dat", na.strings = ".", list(Day=0, Temp=0))
> 
> Read 366 records
> 
>>Day <- temper$Day ; Temp<-temper$Temp ; 
>>
>>temp<- cbind (Day, Temp)
>>#Day = number of days modelled, Temp = daily avg. temp.
>>#temp [,2]
>>
>>p<- 0.558626306252032
>>ACT <- 1.66764519286918
>>
>>Vc<-((CTM-temp[,2])/(CTM-CTO))
>>Vr<-((RTM-temp[,2])/(RTM-RTO))
>>
>>
>>comp<- cbind (Day, Temp, Vc, Vr)
>>
>>
>>bio<-NULL
>>M<- length(Day) #number of days iterated
>>for (i in 1:M)
> 
> + {
> + 
> + weight<- function(Day)
> + {
> + W<-NULL
> + 	if (Day[i]==1) {W[i] <- Wo}
> + else    if (Day[i]>1) {W[i] <- ((bio[i-1,1]*bio[i-1,9])/Ef)
> + 	}
> + 	W
> + }
> + 
> + W<-weight(Day)
> 
> The problem, as many of you have already identified, is right here. I hope I 
> finally have the syntax right, but even if the "if else" is coded properly, I 
> don't think R can find the values in the second condition I list. I need W in 
> each step of the iteration to change slightly, based on the mess of 
> calculations below (which are using parameters that I have already specified).  
> After all the calculations are made, I hope to get values in bio[i,1] and bio
> [i,9] corresponding to the iteration that just occured, then return to the top 
> of the loop to combine them into the value of W for the next iteration. What I 
> think is happening here is that R is looking for values in the condition before 
> they are actually there- the way I've written it, they can't be there until I 
> get past the conditional step.  Which means I am coding this all wrong.  That 
> said, I'm not sure how else to do it;  the value of W in the next iteration is 
> dependent on the values of Gr and W in the previous iteration, with the 
> exception of the first one (Day=1).  I've tried defining "bio" as 
> 
> bio<-matrix(NA, ncol=9, nrow=366)
> 
> but that doesn't help either.
> 
> Perhaps my rbind at the end of the file is incorrect? I think maybe I'm getting 
> mixed up between calculating vectors and values-- should I be specifying [i] 
> for everything below where I am now specifying vecotrs?  
> 
> + #W<-Wo
> + 
> + C<- p*CA*(W^CB)*((comp[,3]^Xc)*(exp(Xc*(1-comp[,3]))))*Pc
> + 
> + ASMR<- (ACT*RA*(W^(RB))*((comp[,4]^Xa)*(exp(Xa*(1-comp[,4])))))
> + 
> + SMR<- (ASMR/ACT)
> + 
> + A<- (ASMR-SMR)
> + 
> + F<- (FA*(comp[,2]^FB)*(exp(FG*p))*C)
> + 
> + U<- (UA*(comp[,2]^UB)*(exp(UG*p))*(C-F))
> + 
> + SDA<- (S*(C-F))
> + 
> + Gr<- (C-(ASMR+F+U+SDA))
> + #Day, Temp, Vc, Vr, W, C, ASMR, SMR, A, F, U, SDA, Gr)
> + 
> + bio<- rbind(c(W, C, ASMR, SMR, A, F, U, SDA, Gr))
> + 
> + dimnames (bio) <-list(NULL, c
> ("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))
> + 
> + }
> Error: length of dimnames[2] not equal to array extent
> Execution halted
> 
>



From ripley at stats.ox.ac.uk  Tue Jul  1 09:43:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 08:43:48 +0100 (BST)
Subject: [R] masked objects
In-Reply-To: <4422f460d8.460d84422f@imap.epfl.ch>
Message-ID: <Pine.LNX.4.44.0307010842090.8435-100000@gannet.stats>

On Tue, 1 Jul 2003, ATHANASIA KAMARIOTIS wrote:

> When opening the software R it appears this message:
> 
> 
> 
> -----------------------------------------------------------------
> Attaching package 'methods':
> 
> 
>         The following object(s) are masked _by_ .GlobalEnv :
> 
>          new 
> 
> 
> ------------------------------------------------------------------
> 
> 
> May you please answer to the following question: How can I restore the 
> object new?

Your object `new' is there: the system one is being masked.  This does not
matter if you are not using formal classes (from package `methods').

To avoid the message, rename your object

> mynew <- new
> rm(new)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s010592 at student.dtu.dk  Tue Jul  1 10:00:08 2003
From: s010592 at student.dtu.dk (Laurent Gautier)
Date: Tue, 1 Jul 2003 10:00:08 +0200
Subject: [R] namespaces and saved objects
Message-ID: <20030701080008.GD11803@bohr.gbar.dtu.dk>

Hi,

While saving (function 'save') a 'lmList' object (pack nlme),
R issues the warning messages:
1: namespaces may not be available when loading 
2: names in persistent strings are currently ignored 
3: namespaces may not be available when loading 
4: names in persistent strings are currently ignored 

Any pitfall I should be aware of ?


L.


PS: I use 
> R.version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status   Patched          
major    1                
minor    7.1              
year     2003             
month    06               
day      23               
language R



From azzalini at stat.unipd.it  Tue Jul  1 10:04:01 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 1 Jul 2003 10:04:01 +0200
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <001101c33f7f$1d456730$73c463d8@plaza.ds.adp.com>
References: <001101c33f7f$1d456730$73c463d8@plaza.ds.adp.com>
Message-ID: <20030701080401.7B5807CA825@tango.stat.unipd.it>

On Tuesday 01 July 2003 05:16, M. Edward Borasky wrote:
> Unfortunately, the data are *non-negative*, not strictly positive. Zero is
> a valid and frequent inter-arrival time. It is, IIRC, the most likely value
> of a (negative) exponential distribution.

Not really. Zero+ is the value with highest density in a (negative) exponential 
distribution, which implies that you should have *no* observed zero's from that
distribution.

If you have a non-negligible fraction of 0 values, then your data are reasonably 
described as  having a mixed distribution: 
  (1) a discrete component at 0, and 
  (2) a continuous positive component.

Kernel (or similar) density estimation is appropriate for the continuous component
only.  Notice that the same remark applies to any procedure (parametric or 
non-parametric, using mixtures, etc.) which is based on continuous components only. 

It *looks* that a wise procedure is to separate out the discrete and the continuos
component of your data, and handle them separately.  At the end you can "merge"
the two parts into
     Y = p * 0 + (1-p) * X
where p is the proportion of 0's, and X represents the  continuous component of
the random variable.

best wishes,

Adelchi Azzalini

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From ripley at stats.ox.ac.uk  Tue Jul  1 10:13:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 09:13:45 +0100 (BST)
Subject: [R] namespaces and saved objects
In-Reply-To: <20030701080008.GD11803@bohr.gbar.dtu.dk>
Message-ID: <Pine.LNX.4.44.0307010907270.5566-100000@gannet.stats>

On Tue, 1 Jul 2003, Laurent Gautier wrote:

> Hi,
> 
> While saving (function 'save') a 'lmList' object (pack nlme),
> R issues the warning messages:
> 1: namespaces may not be available when loading 
> 2: names in persistent strings are currently ignored 
> 3: namespaces may not be available when loading 
> 4: names in persistent strings are currently ignored 
> 
> Any pitfall I should be aware of ?

Only that you will need to have package nlme available to re-load the 
workspace.  I think these warnings have been removed in the development 
version of R.

I've removed the cause of such messages with the MASS function glm.nb() 
recently, and I guess nlme is doing similar things.  The cause was that 
negative.binomial() created a family of functions, which thereby had the 
body of negative.binomial as their environment and hence the MASS 
namespace in its environment.  As the latter was a minor nuisance, I have
manipulated the environments.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From azzalini at stat.unipd.it  Tue Jul  1 10:32:08 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 1 Jul 2003 10:32:08 +0200
Subject: [R] Fitting inter-arrival time data
In-Reply-To: <001201c33f81$87bc6df0$73c463d8@plaza.ds.adp.com>
References: <001201c33f81$87bc6df0$73c463d8@plaza.ds.adp.com>
Message-ID: <20030701083208.A85FF7CA825@tango.stat.unipd.it>

> the two parts into
> ? ? ?Y = p * 0 + (1-p) * X
> where p is the proportion of 0's, and X represents the ?continuous
> component of the random variable.

I must amend myself... what I should have written is
   Y = I * 0 + (1-I) * X
where I is a Bernoulli random variable with probability p of "success" (i.e. 1)
and X represents the ?continuous component of the random variable.

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From SuzieBlatt at netscape.net  Tue Jul  1 11:59:05 2003
From: SuzieBlatt at netscape.net (Suzanne E. Blatt)
Date: Tue, 01 Jul 2003 05:59:05 -0400
Subject: [R] Deciphering an error message
Message-ID: <451DD248.603D15C9.0D1322AF@netscape.net>





Hello again,

Below is the code I am using which generates the error message.

Any comments greatly appreciated,
S.


------------------------------------

#####################################################################
# Function to create a PPP object for all the points in a particular
# include.  Mark the points with their species name (or a number
# indicating it)
#
# Usage:
# x <- getBigPPP(year, include, species, number=FALSE)
#
# Where:
# x = ppp object with coordinates and bounds for indicated species
#      and plots.
# year = year(s) to include data for.  Either a single number or a
#        vector of numbers
# include = name(s) of plots to include data from.  Either a single
#           character expression or a vector of characters.
# species = name(s) of species to include data for.  Either a single
#           species name or a vector of characters.
# number = flag indicating whether species names or numbers are used
#          as marks in ppp object.  Species names are default.  To
#          use numbers, set number=TRUE.  This argument is not needed
#          unless numbers are desired.
#
# Example:
# source("spat1.R")                      # source file to install functions
# getBigPPP(1992, "G1", "apple")         # print PPP object (summary of)
#                                        # for apple in G1 in 1992.  Marks
#                                        # will be species name 'apple'
# x <- getBigPPP(1992, c("P1","P2","P3"), c("apple","hawthorn"), number=TRUE)
#                                        # x is defined as PPP object with
#                                        # coordinates and plot information
#                                        # for apple and hawthorn species
#                                        # in plots P1, P2, and P3, with
#                                        # marks set to numbers.
# plot(x)                                # plot locations of species, with
#                                        # different marks for each one, and
#                                        # plot boundaries.
#
getBigPPP <- function(year=NULL, include=NULL, species=NULL, number=FALSE, spColNam="Species") {
    # message about usage
    if(is.null(year) || is.null(include)) {
        return("ERROR !")
    }
    # create species selection if it is now null
    sp"Col <- t(trees[spColNam])
    # print(spCol)
    if(is.null(species)) {
        species <- as.character(unique(spCol))
    }
    # create the bounds for this object
    bounds <- plotBounds(include)
    # selection based on include and year
    select <- (trees$Plot %in% include) & (trees$Year %in% year) &
        (spCol %in% species) & (!is.na(trees$Northing)) &
        (!is.na(trees$Easting)) & (trees$Present == 1)
    # mark with numbers or with species name
    if(!number) {
        return(ppp(x=trees$Easting[select], y=trees$Northing[select],
            marks=as.factor(spCol[select]), poly=bounds))
    }
    specs <- unique(spCol[select])
    marks <- apply(as.matrix(spCol[select]), 1, function(x) {
        which(specs == x)[1]
    })
    return(ppp(x=trees$Easting[select], y=trees$Northing[select],
        marks=as.factor(marks), poly=bounds))
}
--------------------------------------------------







Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

>Suzanne E. Blatt wrote:
>> Hello,
>> 
>> I'm working in spatstat and having difficulty with the ppp objects.  I can get a ppp object for one set of data, but with the same code applied to a second data set (all I am changing is the field identifier), I get the following error message:
>> 
>> Error in switch(w$type, rectangle={: internal error: some total scores are neither 0 nor 1
>
>What is a "ppp object"?
>Which function has been applied and generated that error message?
>
>The error message tells you some values a ought to sum up to 0 or 1 but 
>they don't. I (and perhaps others as well) cannot help when you don't 
>specify some relevant information.
>
>Uwe Ligges
>
>> Any thoughts on what this means and how to correct it would be most appreciated.
>> 
>> Thanks,
>> Suzanne
>> 
>> __________________________________________________________________
>> McAfee VirusScan Online from the Netscape Network.
>> Comprehensive protection for your entire computer. Get your free trial today!
>> http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397
>> 
>> Get AOL Instant Messenger 5.1 free of charge.  Download Now!
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>

__________________________________________________________________
McAfee VirusScan Online from the Netscape Network.
Comprehensive protection for your entire computer. Get your free trial today!
http://channels.netscape.com/ns/computing/mcafee/index.jsp?promo=393397

Get AOL Instant Messenger 5.1 free of charge.  Download Now!



From Neil.Chriss at sac.com  Tue Jul  1 12:22:40 2003
From: Neil.Chriss at sac.com (Chriss, Neil)
Date: Tue, 1 Jul 2003 06:22:40 -0400 
Subject: [R] X-emacs and R for Windows
Message-ID: <7326095FBE833F46BA7D951C141CC7A003B3D7E5@mailisct2.saccapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030701/1d290296/attachment.pl

From athanasia.kamariotis at epfl.ch  Tue Jul  1 12:34:54 2003
From: athanasia.kamariotis at epfl.ch (ATHANASIA KAMARIOTIS)
Date: Tue, 01 Jul 2003 12:34:54 +0200
Subject: [R] Step - wise
Message-ID: <43d53451c8.451c843d53@imap.epfl.ch>

Madame , Monsieur,
J'aimerais utiliser la fonction step dans une boucle "for" pour pouvoir 
prédire des valeurs (à chaque pas) suivant le modèle proposé par step.
Est-il possible de le faire ?

Merci d'avance,
Cordialement
Athanasia



From glaziou at pasteur-kh.org  Tue Jul  1 12:48:38 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 1 Jul 2003 17:48:38 +0700
Subject: [R] X-emacs and R for Windows
In-Reply-To: <7326095FBE833F46BA7D951C141CC7A003B3D7E5@mailisct2.saccapital.com>
References: <7326095FBE833F46BA7D951C141CC7A003B3D7E5@mailisct2.saccapital.com>
Message-ID: <20030701104837.GK2929@pasteur-kh.org>

Chriss, Neil <Neil.Chriss at sac.com> wrote:
> Is there any way to make XEmacs (or any other editor) the default editor for
> R so that when I type 
>  
> > sample <- function(x,y) {
> + z<-x+y
> + }
> + edit(sample)
>  
> the XEmacs (or other editor) is the editor for this function (the default
> seems to be MS Notepad, blech).


See ?options

> options()$editor
[1] "/usr/bin/vim"
> options(editor="emacs")
> options()$editor
[1] "emacs"

-- 
Philippe



From ligges at statistik.uni-dortmund.de  Tue Jul  1 13:11:22 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jul 2003 13:11:22 +0200
Subject: [R] Deciphering an error message
In-Reply-To: <451DD248.603D15C9.0D1322AF@netscape.net>
References: <451DD248.603D15C9.0D1322AF@netscape.net>
Message-ID: <3F016C5A.4030000@statistik.uni-dortmund.de>

Suzanne E. Blatt wrote:

> 
> 
> 
> Hello again,
> 
> Below is the code I am using which generates the error message.
> 
> Any comments greatly appreciated,
> S.

Well, my questions were:

  - 'What is a "ppp object"?'

Still not answered yet, AFAICS.

  - 'Which function has been applied and generated that error message?'

Still not answered yet, AFAICS.

You have send us a self written function, but not a reproducible example 
that helps to discover the error message. And this function does *not* 
generate the error message, but perhaps a function called within your 
function. Further on:
- You are using a function ppp() I don't know anything about.
- (At least) The line
      sp"Col <- t(trees[spColNam])
    should generate a syntax error ...


So, please use the debugging tools to find out which function produces 
the error message (e.g. via traceback()) and specify *reproducible* 
examples that are as short as possible.

We cannot help as long as you are concealing the relevant information.

Uwe


> 
> ------------------------------------
> 
> #####################################################################
> # Function to create a PPP object for all the points in a particular
> # include.  Mark the points with their species name (or a number
> # indicating it)
> #
> # Usage:
> # x <- getBigPPP(year, include, species, number=FALSE)
> #
> # Where:
> # x = ppp object with coordinates and bounds for indicated species
> #      and plots.
> # year = year(s) to include data for.  Either a single number or a
> #        vector of numbers
> # include = name(s) of plots to include data from.  Either a single
> #           character expression or a vector of characters.
> # species = name(s) of species to include data for.  Either a single
> #           species name or a vector of characters.
> # number = flag indicating whether species names or numbers are used
> #          as marks in ppp object.  Species names are default.  To
> #          use numbers, set number=TRUE.  This argument is not needed
> #          unless numbers are desired.
> #
> # Example:
> # source("spat1.R")                      # source file to install functions
> # getBigPPP(1992, "G1", "apple")         # print PPP object (summary of)
> #                                        # for apple in G1 in 1992.  Marks
> #                                        # will be species name 'apple'
> # x <- getBigPPP(1992, c("P1","P2","P3"), c("apple","hawthorn"), number=TRUE)
> #                                        # x is defined as PPP object with
> #                                        # coordinates and plot information
> #                                        # for apple and hawthorn species
> #                                        # in plots P1, P2, and P3, with
> #                                        # marks set to numbers.
> # plot(x)                                # plot locations of species, with
> #                                        # different marks for each one, and
> #                                        # plot boundaries.
> #
> getBigPPP <- function(year=NULL, include=NULL, species=NULL, number=FALSE, spColNam="Species") {
>     # message about usage
>     if(is.null(year) || is.null(include)) {
>         return("ERROR !")
>     }
>     # create species selection if it is now null
>     sp"Col <- t(trees[spColNam])
>     # print(spCol)
>     if(is.null(species)) {
>         species <- as.character(unique(spCol))
>     }
>     # create the bounds for this object
>     bounds <- plotBounds(include)
>     # selection based on include and year
>     select <- (trees$Plot %in% include) & (trees$Year %in% year) &
>         (spCol %in% species) & (!is.na(trees$Northing)) &
>         (!is.na(trees$Easting)) & (trees$Present == 1)
>     # mark with numbers or with species name
>     if(!number) {
>         return(ppp(x=trees$Easting[select], y=trees$Northing[select],
>             marks=as.factor(spCol[select]), poly=bounds))
>     }
>     specs <- unique(spCol[select])
>     marks <- apply(as.matrix(spCol[select]), 1, function(x) {
>         which(specs == x)[1]
>     })
>     return(ppp(x=trees$Easting[select], y=trees$Northing[select],
>         marks=as.factor(marks), poly=bounds))
> }
> --------------------------------------------------
> 
> 
> 
> 
> 
> 
> 
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
> 
>>Suzanne E. Blatt wrote:
>>
>>>Hello,
>>>
>>>I'm working in spatstat and having difficulty with the ppp objects.  I can get a ppp object for one set of data, but with the same code applied to a second data set (all I am changing is the field identifier), I get the following error message:
>>>
>>>Error in switch(w$type, rectangle={: internal error: some total scores are neither 0 nor 1
>>
>>What is a "ppp object"?
>>Which function has been applied and generated that error message?
>>
>>The error message tells you some values a ought to sum up to 0 or 1 but 
>>they don't. I (and perhaps others as well) cannot help when you don't 
>>specify some relevant information.
>>
>>Uwe Ligges
>>
>>
>>>Any thoughts on what this means and how to correct it would be most appreciated.
>>>
>>>Thanks,
>>>Suzanne
>>>



From andy_liaw at merck.com  Tue Jul  1 13:57:56 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 01 Jul 2003 07:57:56 -0400
Subject: [R] Creating a co-occurence matrix
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7F3@usrymx25.merck.com>

> From: Greg Blevins [mailto:gblevins at mn.rr.com] 
> 
> Hello R experts,
> 
> I have a data.frame which has a series of 30 variables that 
> are each coded 1,0--1 if a behavior is engaged in, 0 otherwise.
> 
> The data.frame looks like this:
> 
>                         var1 var2 ...var30
> Respondent 1
> Respondent 2
> etc
> 
> I would like to create a matrix (or at least a half-matrix) 
> as follows:
> 
>               var1  var2 ...var30
> var1
> var 2
> .
> .
> var 30
> 
> where the number at each intersection is the number of people 
> who engaged in both behaviors.  I could run a whole bunch of 
> tables and input the data by hand into a new matrix, but I 
> was wondering if there is a way to do this via a 
> program/function (I looked at "daisy," but concluded this 
> function would not handle this).

If I understood you correctly, the cross product of the matrix should do it.
If your data frame is named "df", then you can do

  answer <- crossprod(data.matrix(df))

Hth,
Andy

 
> Any help would be appreciated!
> 
> Greg Blevins
> The Market Solutions Group
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From erwan.barret at wanadoo.fr  Tue Jul  1 15:08:32 2003
From: erwan.barret at wanadoo.fr (Erwan BARRET)
Date: Tue,  1 Jul 2003 15:08:32 +0200 (CEST)
Subject: [R] help for barchart(lattice)
Message-ID: <17773523.1057064912660.JavaMail.www@wwinf0402>

I'd like to plot coefficients from glm result.
with barchart, coefficients are ordered by the names of factors and I'd like to order them by values (or abs(values)).
to do that, I've ordered the vector of coefficients but it's plotting the same thing.

I'm working R 1.6.2 on win98.
Thanks

is it possible to do that with standard graphical function?



From athanasia.kamariotis at epfl.ch  Tue Jul  1 15:36:02 2003
From: athanasia.kamariotis at epfl.ch (ATHANASIA KAMARIOTIS)
Date: Tue, 01 Jul 2003 15:36:02 +0200
Subject: [R] step-wise and prediction
Message-ID: <41a7645370.4537041a76@imap.epfl.ch>

I would like to know how to use the function step with "for" in order 
to predict.
Id est:

Let M , A, B ,C be vectors of length 15

for(i in 1:5)
{

regrM<-lm(M[(1+i):(10+i)]~A[(1+i):(10+i)]+B[(1+i):(10+i)]+C[(1+i):
(10+i)])

sregrM<-step(regrM)

and then I would like to compute the predicted value : M* of M, using 
the predictors selected by the function step.
For example if i=1 and step gives us the model M~A,
then in the "for", I want to compute M* by the means of A[11], and 
compare this value to the real value M[11] and so on.



Merci d'avance ,
Cordialement 
Athanasia



From Huiqin.Yang at noaa.gov  Tue Jul  1 15:55:39 2003
From: Huiqin.Yang at noaa.gov (Huiqin Yang)
Date: Tue, 01 Jul 2003 09:55:39 -0400
Subject: [R] Computations slow in spite of large amounts of RAM. 
Message-ID: <3F0192DB.6B1D252D@noaa.gov>

Hi all,

I am a beginner trying to use R to work with large amounts of
oceanographic data, and I find that computations can be VERY slow.  In
particular, computational speed seems to depend strongly on the number
and size of the objects that are loaded (when R starts up).  The same
computations are significantly faster when all but the essential
objects are removed.  I am running R on a machine with 16 GB of RAM,
and our unix system manager assures me that there is memory available
to my R process that has not been used.

1.  Is the problem associated with how R uses memory?  If so, is there
some way to increase the amount of memory used by my R process to get
better performance?

The computations that are particularly slow involve looping with
by().  The data are measurements of vertical profiles of pressure,
temperature, and salinity at a number of stations, which are organized
into a dataframe p.1 (1925930 rows, 8 columns: id, p, t, and s, etc.),
and the objective is to get a much smaller dataframe and the unique 
values for ID is 1409 with the minimum and maximum pressure for each
profile.  The slow part is:

h.maxmin <- by(p.1,p.1$id,function(x){
             data.frame(id=x$id[1],
                      maxp=max(x$p),
                      minp=min(x$p))})

2.  Even with unneeded data objects removed, this is very slow.  Is
there a faster way to get the maximum and minimum values?

platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    7.0                 
year     2003                
month    04                  
day      16                  
language R             

Thank you for your time.

Helen



From spencer.graves at pdf.com  Tue Jul  1 16:22:22 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jul 2003 07:22:22 -0700
Subject: [R] step-wise and prediction
References: <41a7645370.4537041a76@imap.epfl.ch>
Message-ID: <3F01991E.3090201@pdf.com>

Have you considered "predict(regrM)"?

J'espere que ces mots vous aideront.
Spencer Graves

ATHANASIA KAMARIOTIS wrote:
> I would like to know how to use the function step with "for" in order 
> to predict.
> Id est:
> 
> Let M , A, B ,C be vectors of length 15
> 
> for(i in 1:5)
> {
> 
> regrM<-lm(M[(1+i):(10+i)]~A[(1+i):(10+i)]+B[(1+i):(10+i)]+C[(1+i):
> (10+i)])
> 
> sregrM<-step(regrM)
> 
> and then I would like to compute the predicted value : M* of M, using 
> the predictors selected by the function step.
> For example if i=1 and step gives us the model M~A,
> then in the "for", I want to compute M* by the means of A[11], and 
> compare this value to the real value M[11] and so on.
> 
> 
> 
> Merci d'avance ,
> Cordialement 
> Athanasia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Jul  1 16:23:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 1 Jul 2003 15:23:48 +0100 (GMT Daylight Time)
Subject: [R] help for barchart(lattice)
In-Reply-To: <17773523.1057064912660.JavaMail.www@wwinf0402>
Message-ID: <Pine.WNT.4.44.0307011518190.2980-100000@gannet.stats.ox.ac.uk>

On Tue, 1 Jul 2003, Erwan BARRET wrote:

> I'd like to plot coefficients from glm result. with barchart,
> coefficients are ordered by the names of factors and I'd like to order
> them by values (or abs(values)).

You need to set up a factor for the grouping you want: the plot order is
along the levels of the grouping factor.  Take a look at the barchart
example for barley: neither year nor site are in alphabetical order.

> to do that, I've ordered the vector of coefficients but it's plotting the
> same thing.

An implicit call to factor will use alphabetical order, so use an explict
one.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Tue Jul  1 16:31:13 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 01 Jul 2003 14:31:13 -0000
Subject: [R] Computations slow in spite of large amounts of RAM.
In-Reply-To: <3F0192DB.6B1D252D@noaa.gov>
References: <3F0192DB.6B1D252D@noaa.gov>
Message-ID: <6ry8zi5nf5.fsf@bates4.stat.wisc.edu>

"Huiqin Yang" <Huiqin.Yang at noaa.gov> writes:

> Hi all,
> 
> I am a beginner trying to use R to work with large amounts of
> oceanographic data, and I find that computations can be VERY slow.  In
> particular, computational speed seems to depend strongly on the number
> and size of the objects that are loaded (when R starts up).  The same
> computations are significantly faster when all but the essential
> objects are removed.  I am running R on a machine with 16 GB of RAM,
> and our unix system manager assures me that there is memory available
> to my R process that has not been used.
> 
> 1.  Is the problem associated with how R uses memory?  If so, is there
> some way to increase the amount of memory used by my R process to get
> better performance?

You could try setting a large nsize and vsize using 

 mem.limits

See the description in ?Memory

> The computations that are particularly slow involve looping with
> by().  The data are measurements of vertical profiles of pressure,
> temperature, and salinity at a number of stations, which are organized
> into a dataframe p.1 (1925930 rows, 8 columns: id, p, t, and s, etc.),
> and the objective is to get a much smaller dataframe and the unique 
> values for ID is 1409 with the minimum and maximum pressure for each
> profile.  The slow part is:
> 
> h.maxmin <- by(p.1,p.1$id,function(x){
>              data.frame(id=x$id[1],
>                       maxp=max(x$p),
>                       minp=min(x$p))})

I think it would be faster to use

h.maxmin <- tapply(p.1$p, p.1$id, range)

In the call to by you are subsetting the entire data frame and that
probably means taking at least one copy of that frame.  If you use
tapply on only the relevant columns you will use much less space.

> 2.  Even with unneeded data objects removed, this is very slow.  Is
> there a faster way to get the maximum and minimum values?

See above.


-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From andy_liaw at merck.com  Tue Jul  1 16:37:24 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 01 Jul 2003 10:37:24 -0400
Subject: [R] Computations slow in spite of large amounts of RAM.
Message-ID: <3A822319EB35174CA3714066D590DCD50205C7F7@usrymx25.merck.com>

> From: Huiqin Yang [mailto:Huiqin.Yang at noaa.gov] 
> 
> Hi all,
> 
> I am a beginner trying to use R to work with large amounts of 
> oceanographic data, and I find that computations can be VERY 
> slow.  In particular, computational speed seems to depend 
> strongly on the number and size of the objects that are 
> loaded (when R starts up).  The same computations are 
> significantly faster when all but the essential objects are 
> removed.  I am running R on a machine with 16 GB of RAM, and 
> our unix system manager assures me that there is memory 
> available to my R process that has not been used.
> 
> 1.  Is the problem associated with how R uses memory?  If so, 
> is there some way to increase the amount of memory used by my 
> R process to get better performance?

Is R compiled as 64-bit?  If not, it won't be able to use more than 4GB of
RAM (that's my understanding, anyway).

R keeps objects in memory, so if you are working with large amount of data,
it's a good habit to keep only the absolute essential objects in the
workspace, and save() and rm() things you don't need for the computation.

> 
> The computations that are particularly slow involve looping 
> with by().  The data are measurements of vertical profiles of 
> pressure, temperature, and salinity at a number of stations, 
> which are organized into a dataframe p.1 (1925930 rows, 8 
> columns: id, p, t, and s, etc.), and the objective is to get 
> a much smaller dataframe and the unique 
> values for ID is 1409 with the minimum and maximum pressure 
> for each profile.  The slow part is:
> 
> h.maxmin <- by(p.1,p.1$id,function(x){
>              data.frame(id=x$id[1],
>                       maxp=max(x$p),
>                       minp=min(x$p))})
> 
> 2.  Even with unneeded data objects removed, this is very 
> slow.  Is there a faster way to get the maximum and minimum values?

Why do you need to use by(), and why have the function return a data frame
containing only one row?  Here's an experiment on my 900MHz PIII laptop:

> n <- 1e5
> dat <- data.frame(id = sort(sample(LETTERS, n, replace=TRUE)),
+                   p = rnorm(n))
> 
> 
> system.time(h.maxmin <- by(dat, dat$id,function(x) {
+   data.frame(id=x$id[1], maxp=max(x$p), minp=min(x$p))}))
[1] 2.75 0.01 2.78   NA   NA
> system.time(junk <- tapply(dat$p, dat$id, function(x) range(x)))
[1] 0.12 0.01 0.13   NA   NA

If you want to coerce the result to a data frame with id as row names and
min and max as the two variables, you can do:

  junk.dat <- as.data.frame(do.call("rbind", junk))

HTH,
Andy


 
> platform sparc-sun-solaris2.9
> arch     sparc               
> os       solaris2.9          
> system   sparc, solaris2.9   
> status                       
> major    1                   
> minor    7.0                 
> year     2003                
> month    04                  
> day      16                  
> language R             
> 
> Thank you for your time.
> 
> Helen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From spencer.graves at pdf.com  Tue Jul  1 16:38:17 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jul 2003 07:38:17 -0700
Subject: [R] crossed random effects
References: <20030701053222.7353.qmail@web40005.mail.yahoo.com>
Message-ID: <3F019CD9.8070705@pdf.com>

	  Have you studied Pinhiero and Bates (2000) Mixed Effects Models in S 
and S-Plus (Springer)?

	  Also, have you tried simplifying your "lme" call until you get 
something that works, then start adding back terms in various 
configurations until it breaks?

	  Have you tried to compute how many coefficients are estimated in both 
fixed and random terms and evaluate whether all are estimable?  For 
example, with 2 factors at 2 levels each, if you don't have all 4 
possible combinations, you can't estimate the interaction -- even if you 
have thousands of replications of each.

	  Finally, you can always try to read the code.  I've learned a lot 
about S-Plus / R by doing that -- and solved a lot of my own problems 
that way.

hope this helps.  spencer graves

Sarah Mclean wrote:
> Hi,
> 
> if I have posted this twice, please ignore this. I'm
> not sure if I sent it to the correct e-mail address
> the first time.
> 
> I have a data set on germination and plant growth with
> the following variables:
> 
> dataset=fm
> mass (response)
> sub (fixed effect)
> moist (fixed effect)
> pop (fixed effect)
> mum (random effect nested within population)
> iheight (covariate)
> plot (random effect- whole plot factor for split-plot
> design).
> 
> I want to see if moist or sub interacts with mum for
> any of the pops, but I am getting an error message. 
> 
> This is the formula I used:
> fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
> fm$grp = as.factor(rep(1,nrow(fm)))
> fm$pl <- getGroups(fm, ~1|plot)
> fm$mo <- getGroups(fm, ~1|moist)
> fm$su <- getGroups(fm, ~1|sub)
> 
>>fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
> 
> data=fm, random=list(grp=pdBlocked(list(pdIdent(~pl -
> 1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
> pdIdent(~pmu:mo - 1)))))
> Error in chol((value + t(value))/2) : non-positive
> definite matrix in chol
> 
> I know the problem is with the random interaction
> terms, but I don't know how to overcome this.
> 
> Any advice would be greatly appreciated. I'm new to R
> and analysis such as this.
> 
> Thank you,
> 
> Sarah Mclean
> sarahmclean9 at yahoo.co.nz
> 
> 
> http://mobile.yahoo.com.au - Yahoo! Mobile
> - Check & compose your email via SMS on your Telstra or Vodafone mobile.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From gavin.simpson at ucl.ac.uk  Tue Jul  1 17:00:59 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 01 Jul 2003 16:00:59 +0100
Subject: [R] Warning message in scatter.smooth (modreg)
Message-ID: <3F01A22B.1020108@ucl.ac.uk>

Dear list,

In using the scatter.smooth() function (modreg) on a small data set (100 
obs) the following error was produced:

 > scatter.smooth(Na, S)
Warning message:
k-d tree limited by memory. ncmax= 200

I haven't used scatter.smooth much but when I have, I haven't seen this 
message before.

gc() returns

 > gc()
          used (Mb) gc trigger (Mb)
Ncells 417693 11.2     667722 17.9
Vcells 103949  0.8     786432  6.0

This is on a Win XP box with 512 MB RAM, with plenty of space still 
available (Task Manager Reports 216 MB of RAM and pagefile in use).

I have looked through the help files for loess and loess.control but 
didn't see anything about ncmax.  Can someone explain why I might be 
getting this warning?

Many thanks,

Gavin

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.1
year     2003
month    06
day      16
language R

I can send the data if required.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From bwmoore22 at yahoo.com  Tue Jul  1 17:02:36 2003
From: bwmoore22 at yahoo.com (Bruce Moore)
Date: Tue, 1 Jul 2003 08:02:36 -0700 (PDT)
Subject: [R] Novice Questions
In-Reply-To: <3F00955E.6090408@pdf.com>
Message-ID: <20030701150236.81901.qmail@web13705.mail.yahoo.com>

The print and echo delay from source() was caused by
the Windows RGUI  Misc->Buffer Output option, which is
turned on by default in 1.7.1 (don't know about other
versions).

Thank you for some very useful comments, and for
ending a couple of days of frustration.

Bruce Moore

--- Spencer Graves <spencer.graves at PDF.COM> wrote:
> Hi, Bruce:
> 
> 	  I'm overwhelmed.  Questioners seem to get
> quicker, more informative 
> responses to questions that are short, well written
> and easily and 
> quickly understood, and preferably include a toy
> example that someone 
> else can run quickly to reproduce the problem and to
> evaluate 
> alternative solutions.
> 
> 	  I see you are using "source" and would like to
> get more output.  Did 
> you check "?source"?  This function in R has
> arguments "echo" and 
> "verbose", which may produce what you want.  If
> you've already tried 
> that, then the problem may be output buffering, and
> I don't know how to 
> modify that parameter.  A search of
> "www.r-project.org" -> search -> "R 
> Site Search" might help.  Or ask a question focused
> specifically on 
> that, giving also which version of R you are using
> under which operating 
> system.
> 
> 	  I rarely use "source".  More often, I have R
> commands in another file 
> and copy and paste into R the commands I want to
> run.  That makes it 
> easier for me to isolate errors, etc.
> 
> 	  This does not address all your questions, but
> it's a start.
> 
> Hope this helps.
> spencer graves
> 
> Bruce Moore wrote:
> > I'm writing a program to perform linear
> regressions to
> > estimate the number of bank teller transactions
> per
> > hour of various types based upon day of week, time
> of
> > day, week of month and several prices.  I've got
> about
> > 25,000 records in my dataset, 85 columns of
> > transaction counts (used 1 at a time), about 50
> > columns of binary indicators (day, week, pay
> period,
> > hour, branch), and a half dozen real valued
> prices.
> > 
> > My program hangs on some regressions as I add
> > interactions, probably due to logic problems in my
> > code or collinearity problems in the data.
> > 
> > 1) I'm running my program via the source()
> command. 
> > It appears that source() does not print any
> messages
> > until it completes.  
> > 
> > ---->Is there a way to get diagnostic messages to
> > print immediately rather than when the source()
> > command has completed?
> > 
> > 2) I'm fairly certain that I've got some
> collinearity
> > in the data set and the interactions.  I've found
> an
> > append (Ott Toomet 5/30/2003) that talks about a
> > procedure to find collinearity problems using
> > model.matrix() to generate the dataset with
> > interactions and kappa() to determine the
> condition
> > number of the matrix.  
> > 
> > ---->Is there a more automated way to find
> collinear
> > variables?
> > 
> > 3) Is there a way to get lm() and/or step() or
> some
> > other package to give a model with only
> coefficients
> > that are significant at a particular level?
> > 
> > 4) Is there a way to suppress display of a
> password
> > when using the RODBC odbcConnect() function, or to
> get
> > the function to prompt for a password?
> > 
> > 5) What is the practical size limit on the number
> of
> > terms in model?  I know that I won't be able to
> > consider all interactions, but would like to have
> some
> > idea when to give up and go with what I've got.
> > 
> > 
> > 
> > =====
> > Bruce Moore
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 


=====
Bruce Moore



From P.W.vanRijn at uva.nl  Tue Jul  1 17:07:33 2003
From: P.W.vanRijn at uva.nl (Peter van Rijn)
Date: Tue, 1 Jul 2003 17:07:33 +0200
Subject: [R] Working memory full
Message-ID: <21F67EF0AFCF3F449AF94D739A0350352D10D8@rea05.fmg.uva.nl>

I have a question concerning the working memory used by R.

I am running a simulation job (a function which replicates a simulation) which calls several other functions that I have written. Something like this:

name1<-function(number of replications){
	for(i in 1:replication)
		name2(parameters)
		name3()
		# get results and aggregate
		open..
		write(..., append=T)
	}
}

name2<-function(parameters){
	# simulate data
	...	
	# save data and parameters
	write...
}

name3<-function(){
	# get data and parameters
	open...
	# analyse data
	...
	# save results
	write..	
}

name1(100)


However, after a couple of replications working memory seems to be full.

Error: cannot allocate vector of size ...Kb
In addition: Warning message: 
Reached total allocation of 126Mb: see help(memory.size)

This puzzles me, because all arrays (Note: large arrays, i.e., 500x500) are defined locally (within each function) and redefined every replication (the meta-function). So my question is: Why is R eating up my working memory while I think I am redefining the same (local) arrays? Any thoughts on how this can be circumvented?

Thanks,

Peter.



From rossini at blindglobe.net  Tue Jul  1 17:18:34 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 01 Jul 2003 08:18:34 -0700
Subject: [R] X-emacs and R for Windows
In-Reply-To: <20030701104837.GK2929@pasteur-kh.org> (Philippe Glaziou's
	message of "Tue, 1 Jul 2003 17:48:38 +0700")
References: <7326095FBE833F46BA7D951C141CC7A003B3D7E5@mailisct2.saccapital.com>
	<20030701104837.GK2929@pasteur-kh.org>
Message-ID: <87d6gu9sxh.fsf@jeeves.blindglobe.net>

Philippe Glaziou <glaziou at pasteur-kh.org> writes:

> Chriss, Neil <Neil.Chriss at sac.com> wrote:
>> Is there any way to make XEmacs (or any other editor) the default editor for
>> R so that when I type 
>>  
>> > sample <- function(x,y) {
>> + z<-x+y
>> + }
>> + edit(sample)
>>  
>> the XEmacs (or other editor) is the editor for this function (the default
>> seems to be MS Notepad, blech).
>
>
> See ?options
>
>> options()$editor
> [1] "/usr/bin/vim"
>> options(editor="emacs")
>> options()$editor
> [1] "emacs"

You might want to consider the emacs client variants (winclient, I
think, on XEmacs, not sure what it is on Emacs, gnuclient or
emacsclient); they require Emacs to be already running, and simply
provide an attached frame to work in.

best,
-tony
-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tapmehta at yahoo.com  Tue Jul  1 17:23:40 2003
From: tapmehta at yahoo.com (Tapan Mehta)
Date: Tue, 1 Jul 2003 08:23:40 -0700 (PDT)
Subject: [R] Memory problem:Failing to increase dynamic memory
Message-ID: <20030701152340.66170.qmail@web40614.mail.yahoo.com>

Hello,

I am trying to use R1.7 on Linux. I having some
problem with memory. The task has to handle 100 files
of 10MB (each file is .CEL file) and is related to
microarrays. I am trying to run this task on a 2GB or
a 4 GB node of a Bewoulf Linux based cluster. However
I am getting this error saying 'array size of 336000KB
cannot be intialized'.

 I have seen the help of R for memory but I have not
been able to increase the dynamic memory. I used to
use the memory.limit(size = ) function in the older
version of R(1.6.1) .In the latest version however I
am unable to achieve the task by using the function
mem.limits(). It would be nice if somebody could help
me out on this.

Thanks,

Tapan



From ripley at stats.ox.ac.uk  Tue Jul  1 17:38:08 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 16:38:08 +0100 (BST)
Subject: [R] X-emacs and R for Windows
In-Reply-To: <87d6gu9sxh.fsf@jeeves.blindglobe.net>
Message-ID: <Pine.LNX.4.44.0307011632180.1136-100000@gannet.stats>

On Tue, 1 Jul 2003, A.J. Rossini wrote:

> Philippe Glaziou <glaziou at pasteur-kh.org> writes:
> 
> > Chriss, Neil <Neil.Chriss at sac.com> wrote:
> >> Is there any way to make XEmacs (or any other editor) the default editor for
> >> R so that when I type 
> >>  
> >> > sample <- function(x,y) {
> >> + z<-x+y
> >> + }
> >> + edit(sample)
> >>  
> >> the XEmacs (or other editor) is the editor for this function (the default
> >> seems to be MS Notepad, blech).

Yes, it is: what other editor can you guarantee to be on all Windows 
systems?

BTW, I think you want to say fix(sample): edit returns a result you may 
want to assign.

> >
> > See ?options
> >
> >> options()$editor
> > [1] "/usr/bin/vim"
> >> options(editor="emacs")
> >> options()$editor
> > [1] "emacs"
> 
> You might want to consider the emacs client variants (winclient, I
> think, on XEmacs, not sure what it is on Emacs, gnuclient or
> emacsclient); they require Emacs to be already running, and simply
> provide an attached frame to work in.

On NTEmacs I use gnuclient, which BTW will start up an emacs if one is not 
already running, otherwise start a new frame (or, my choice) a new buffer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From DSmith2 at dhs.ca.gov  Tue Jul  1 18:06:16 2003
From: DSmith2 at dhs.ca.gov (Smith, Daniel (DHS-DEODC-EHIB))
Date: Tue, 1 Jul 2003 09:06:16 -0700 
Subject: [R] RE: question about spatial correlation with Xs and Ys
Message-ID: <535092585C2D2C48B698947567BA69B77EF05C@dhs-exc-msg-02.intra.dhs.ca.gov>

-----Original Message-----
From: r-help-request at stat.math.ethz.ch
[mailto:r-help-request at stat.math.ethz.ch]
Sent: Tuesday, July 01, 2003 3:09 AM
To: r-help at stat.math.ethz.ch
Subject: R-help Digest, Vol 5, Issue 1


At 18:48 30/06/2003, Martin Wegmann wrote:
>hello,
>
>I want to do a test for spatial correlation.
>I tried it with geary.test() but I don't understand the required input.
>x= a numeric vector the same length as the neighbours list in listw (my
>sampled data, I assume)
>listw= a listw object created for example by nb2listw (well when I check
>nb2listw() I get to "neighbours - an object of class nb" - but I couldn't
>figure out, what nb is or how I create such a class
>
>with sp.mantel.mc {spdep} I have the same problem: listw created by
nb2listw
>
>isn't there a more straight forward method ;-)  to check for spatial
>correlation? like x and y coordinates plus my sampled data?

Check out the notes from a short course in Spatial Epidemiology by Best et
al., at http://stats.ma.ic.ac.uk/~ngb30/.  They have S-plus code to run
Moran's I and Tango's test for spatial correlation that use disease counts
in small areas and their X and Y coordinates.  

Daniel Smith, Dr.P.H.
Environmental Health Investigations Branch
California Department of Health Services
1515 Clay Street, Suite 1700
Oakland, CA 94612



From ripley at stats.ox.ac.uk  Tue Jul  1 18:11:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 17:11:39 +0100 (BST)
Subject: [R] Warning message in scatter.smooth (modreg)
In-Reply-To: <3F01A22B.1020108@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0307011701550.1195-100000@gannet.stats>

On Tue, 1 Jul 2003, Gavin Simpson wrote:

> Dear list,
> 
> In using the scatter.smooth() function (modreg) on a small data set (100 
> obs) the following error was produced:
> 
>  > scatter.smooth(Na, S)
> Warning message:
> k-d tree limited by memory. ncmax= 200
> 
> I haven't used scatter.smooth much but when I have, I haven't seen this 
> message before.
> 
> gc() returns
> 
>  > gc()
>           used (Mb) gc trigger (Mb)
> Ncells 417693 11.2     667722 17.9
> Vcells 103949  0.8     786432  6.0
> 
> This is on a Win XP box with 512 MB RAM, with plenty of space still 
> available (Task Manager Reports 216 MB of RAM and pagefile in use).
> 
> I have looked through the help files for loess and loess.control but 
> didn't see anything about ncmax.  Can someone explain why I might be 
> getting this warning?

It's trying to fit a very complicated function, and running out of its own 
internal storage, which is set in the C routine loess_workspace.  However, 
200 cells should be ample for 100 obs, so I guess the algorithm is not
working properly in your example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul  1 18:15:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 17:15:22 +0100 (BST)
Subject: [R] Memory problem:Failing to increase dynamic memory
In-Reply-To: <20030701152340.66170.qmail@web40614.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0307011713190.1195-100000@gannet.stats>

On Tue, 1 Jul 2003, Tapan Mehta wrote:

> Hello,
> 
> I am trying to use R1.7 on Linux. I having some
> problem with memory. The task has to handle 100 files
> of 10MB (each file is .CEL file) and is related to
> microarrays. I am trying to run this task on a 2GB or
> a 4 GB node of a Bewoulf Linux based cluster. However
> I am getting this error saying 'array size of 336000KB
> cannot be intialized'.
> 
>  I have seen the help of R for memory but I have not
> been able to increase the dynamic memory. I used to
> use the memory.limit(size = ) function in the older
> version of R(1.6.1)

That function only exists on Windows under R, so you are not comparing 
like with like.  The Linux port has not preset memory limit.  You probably 
are just running out of memory and need to reorganize your computations.

> .In the latest version however I
> am unable to achieve the task by using the function
> mem.limits(). It would be nice if somebody could help
> me out on this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From A.B.Horne at exeter.ac.uk  Tue Jul  1 18:29:37 2003
From: A.B.Horne at exeter.ac.uk (Arun Horne)
Date: Tue, 1 Jul 2003 17:29:37 +0100
Subject: [R] Matrix to Vector
Message-ID: <HIEFIHMFNKEHBKCKIBGGKEENCCAA.a.b.horne@ex.ac.uk>

Hello,

I have a matrix of values that I want to convert to a vector, but remove
certain entries in the matrix first, i.e. in the vector I want every value
in the matrix that is greater than 1000. I would also like to find out how
many values are excluded in the transition from matrix to vector. Could
someone tell me how this can be done?

Thank you in advance
Arun Horne



From spencer.graves at pdf.com  Tue Jul  1 18:37:42 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jul 2003 09:37:42 -0700
Subject: [R] Matrix to Vector
References: <HIEFIHMFNKEHBKCKIBGGKEENCCAA.a.b.horne@ex.ac.uk>
Message-ID: <3F01B8D6.6010306@pdf.com>

A matrix is a vector with a dim attribute.  The following should do what 
I understand of you to be asking:

sel <- (Mat>1000)
Vec <- Mat[sel]
n.excl <- length(Mat)-length(Vec)

hope this helps.  spencer graves

Arun Horne wrote:
> Hello,
> 
> I have a matrix of values that I want to convert to a vector, but remove
> certain entries in the matrix first, i.e. in the vector I want every value
> in the matrix that is greater than 1000. I would also like to find out how
> many values are excluded in the transition from matrix to vector. Could
> someone tell me how this can be done?
> 
> Thank you in advance
> Arun Horne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Jul  1 18:48:51 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 1 Jul 2003 17:48:51 +0100 (BST)
Subject: [R] Matrix to Vector
In-Reply-To: <HIEFIHMFNKEHBKCKIBGGKEENCCAA.a.b.horne@ex.ac.uk>
Message-ID: <Pine.LNX.4.44.0307011745590.1535-100000@gannet.stats>

On Tue, 1 Jul 2003, Arun Horne wrote:

> I have a matrix of values that I want to convert to a vector, but remove
> certain entries in the matrix first, i.e. in the vector I want every value
> in the matrix that is greater than 1000. I would also like to find out how
> many values are excluded in the transition from matrix to vector. Could
> someone tell me how this can be done?

start with matrix A, and treat it like a vector (which it is)

vec <- A[A > 1000]  (reads down columns, BTW)
excluded <- sum(!(A > 1000))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mrennie at utm.utoronto.ca  Tue Jul  1 19:03:18 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Tue,  1 Jul 2003 13:03:18 -0400
Subject: [R] strange error message
Message-ID: <1057078998.3f01bed641ec2@webmail.utm.utoronto.ca>


Hi, there

I have a loop that is producing data, but is also generating an error message 
that I can't understand.

Here's the loop and the error message:

> bio<-matrix(NA, ncol=9, nrow=366)
> W<-NULL
> M<- length(Day) #number of days iterated
> 
> for (i in 1:M)
+ {
+ 
+ 
+ if (Day[i]==1) W[i] <- Wo else W[i] <- (W[i-1]+(Gr[i]/Ef))
+ 
+ 
+ C<- p*CA*(W^CB)*(((comp[,3])^Xc)*(exp(Xc*(1-(comp[,3])))))*Pc
+ 
+ ASMR<- ACT*RA*(W^RB)*(((comp[,4])^Xa)*(exp(Xa*(1-(comp[,4])))))
+ 
+ SMR<- (ASMR/ACT)
+ 
+ A<- (ASMR-SMR)
+ 
+ F<- (FA*((comp[,2])^FB)*(exp(FG*p))*C)
+ 
+ U<- (UA*((comp[,2])^UB)*(exp(UG*p))*(C-F))
+ 
+ SDA<- (S*(C-F))
+ 
+ Gr<- (C-(ASMR+F+U+SDA))
+ 
+ bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr)
+ 
+ }
There were 50 or more warnings (use warnings() to see the first 50)
> 
> warnings
function (...) 
{
    if (!(n <- length(last.warning))) 
        return()
    names <- names(last.warning)
    cat("Warning message", if (n > 1) 
        "s", ":\n", sep = "")
    for (i in 1:n) {
        out <- if (n == 1) 
            names[i]
        else paste(i, ": ", names[i], sep = "")
        if (length(last.warning[[i]])) {
            temp <- deparse(last.warning[[i]])
            out <- paste(out, "in:", temp[1], if (length(temp) > 
                1) 
                " ...")
        }
        cat(out, ..., fill = TRUE)
    }
}
> 
> dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))
> 
> 
> bio
               W        C     ASMR       SMR         A        F         U
  [1,]  9.200000 233.6647 107.5640  64.50050  43.06345 31.93755 15.840142


Also, does anyone know why I might be getting differences in the same 
calculation between R and Excel?  Is there any way to keep R from rounding your 
numbers, or to specify the # of decimal places you want for an element? 


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From p.dalgaard at biostat.ku.dk  Tue Jul  1 19:16:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 01 Jul 2003 17:16:53 -0000
Subject: [R] strange error message
In-Reply-To: <1057078998.3f01bed641ec2@webmail.utm.utoronto.ca>
References: <1057078998.3f01bed641ec2@webmail.utm.utoronto.ca>
Message-ID: <x28yri6u8u.fsf@biostat.ku.dk>

Michael Rennie <mrennie at utm.utoronto.ca> writes:

> + }
> There were 50 or more warnings (use warnings() to see the first 50)
> > 
> > warnings
> function (...) 
> {
>     if (!(n <- length(last.warning))) 
>         return()
>     names <- names(last.warning)
>     cat("Warning message", if (n > 1) 
>         "s", ":\n", sep = "")

So what were the warnings? We can figure out what the warnings
function looks like without your help.... (Read the message
*carefully*!) 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gavin.simpson at ucl.ac.uk  Tue Jul  1 19:22:40 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 01 Jul 2003 18:22:40 +0100
Subject: [R] strange error message
In-Reply-To: <1057078998.3f01bed641ec2@webmail.utm.utoronto.ca>
References: <1057078998.3f01bed641ec2@webmail.utm.utoronto.ca>
Message-ID: <3F01C360.8040207@ucl.ac.uk>

Michael

1) Those are not errors, but warnings.  You need to use warnings() (with 
the brackets) to see what they were.  By typing warnings (without the 
brackets) you are asking R to print out the source code for the 
warnings() function.

2) try ?options and see option digits in that help file

Hope that helps a bit,

Gavin

Michael Rennie wrote:
> Hi, there
> 
> I have a loop that is producing data, but is also generating an error message 
> that I can't understand.
> 
> Here's the loop and the error message:
> 
> 
>>bio<-matrix(NA, ncol=9, nrow=366)
>>W<-NULL
>>M<- length(Day) #number of days iterated
>>
>>for (i in 1:M)
> 
> + {
> + 
> + 
> + if (Day[i]==1) W[i] <- Wo else W[i] <- (W[i-1]+(Gr[i]/Ef))
> + 
> + 
> + C<- p*CA*(W^CB)*(((comp[,3])^Xc)*(exp(Xc*(1-(comp[,3])))))*Pc
> + 
> + ASMR<- ACT*RA*(W^RB)*(((comp[,4])^Xa)*(exp(Xa*(1-(comp[,4])))))
> + 
> + SMR<- (ASMR/ACT)
> + 
> + A<- (ASMR-SMR)
> + 
> + F<- (FA*((comp[,2])^FB)*(exp(FG*p))*C)
> + 
> + U<- (UA*((comp[,2])^UB)*(exp(UG*p))*(C-F))
> + 
> + SDA<- (S*(C-F))
> + 
> + Gr<- (C-(ASMR+F+U+SDA))
> + 
> + bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr)
> + 
> + }
> There were 50 or more warnings (use warnings() to see the first 50)
> 
>>warnings
> 
> function (...) 
> {
>     if (!(n <- length(last.warning))) 
>         return()
>     names <- names(last.warning)
>     cat("Warning message", if (n > 1) 
>         "s", ":\n", sep = "")
>     for (i in 1:n) {
>         out <- if (n == 1) 
>             names[i]
>         else paste(i, ": ", names[i], sep = "")
>         if (length(last.warning[[i]])) {
>             temp <- deparse(last.warning[[i]])
>             out <- paste(out, "in:", temp[1], if (length(temp) > 
>                 1) 
>                 " ...")
>         }
>         cat(out, ..., fill = TRUE)
>     }
> }
> 
>>dimnames (bio) <-list(NULL, c
> 
> ("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr"))
> 
>>
>>bio
> 
>                W        C     ASMR       SMR         A        F         U
>   [1,]  9.200000 233.6647 107.5640  64.50050  43.06345 31.93755 15.840142
> 
> 
> Also, does anyone know why I might be getting differences in the same 
> calculation between R and Excel?  Is there any way to keep R from rounding your 
> numbers, or to specify the # of decimal places you want for an element? 
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From anna at ptolemy.arc.nasa.gov  Tue Jul  1 19:45:27 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Tue, 1 Jul 2003 10:45:27 -0700
Subject: [R] lines and legend
Message-ID: <200307011045.27434.anna@ptolemy.arc.nasa.gov>

When I am trying to put a legend on a plot where I am using "lines", R just 
ignores it.  I can do it with boxplot or plot, but just not with lines.  Am I 
doing something wrong?  Maybe I am just making a mistake?

Anna



From anna at ptolemy.arc.nasa.gov  Tue Jul  1 20:23:55 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Tue, 1 Jul 2003 11:23:55 -0700
Subject: [R] lines and legend
In-Reply-To: <OF89268809.3B7D39BC-ON85256D56.006440BC@convergys.com>
References: <OF89268809.3B7D39BC-ON85256D56.006440BC@convergys.com>
Message-ID: <200307011123.55630.anna@ptolemy.arc.nasa.gov>

Yes, I am using plot and then lines.  The legend is just not appearing.  I am 
using the coordinates of the legend (150,4) which work on boxplot and plot.  
I have not looked at the output of par (I don't know how to) to see if they 
are in the region.  I assumed if they worked for plot and boxplot they would 
also for lines.

Anna




On Tuesday 01 July 2003 11:16, you wrote:
> I assume that you are calling 'plot' and then 'lines'.  Is the legend just
> not appearing?  what are you using for the coordinates of the legend?  Have
> you looked at the output from "par" to see if these values are within the
> plot region?
> __________________________________________________________
> James Holtman               "What is the problem you are trying to solve?"
> Executive Consultant  --  Office of Technology, Convergys
> james.holtman at convergys.com
> (513) 723-2929
>
>
>
>                       "Anna  H. Pryor"
>                       <anna at ptolemy.arc.nas        To:       R-help mailing
> list <r-help at stat.math.ethz.ch> a.gov>                       cc:
>                       Sent by:                     Subject:  [R] lines and
> legend r-help-bounces at stat.m
>                       ath.ethz.ch
>
>
>                       07/01/2003 13:45
>
>
>
>
>
>
> When I am trying to put a legend on a plot where I am using "lines", R just
>
> ignores it.  I can do it with boxplot or plot, but just not with lines.  Am
> I
> doing something wrong?  Maybe I am just making a mistake?
>
> Anna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sundar.dorai-raj at pdf.com  Tue Jul  1 20:36:54 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 01 Jul 2003 13:36:54 -0500
Subject: [R] lines and legend
References: <OF89268809.3B7D39BC-ON85256D56.006440BC@convergys.com>
	<200307011123.55630.anna@ptolemy.arc.nasa.gov>
Message-ID: <3F01D4C6.5080103@pdf.com>

Hi Anna,
What I often do before calling legend is reset the user-coordinates to 
the unit square. As in:

plot(x = 1:100, y = 1:100, col = "red")
par(usr = c(0, 1, 0, 1))
# now place legend in upper left hand corner
legend(x = 0.02, y = 0.98,
       legend = "test", col = "red", pch = 1)

Hope this is helpful,

Sundar


Anna H. Pryor wrote:
> Yes, I am using plot and then lines.  The legend is just not appearing.  I am 
> using the coordinates of the legend (150,4) which work on boxplot and plot.  
> I have not looked at the output of par (I don't know how to) to see if they 
> are in the region.  I assumed if they worked for plot and boxplot they would 
> also for lines.
> 
> Anna
> 
> 
> 
> 
> On Tuesday 01 July 2003 11:16, you wrote:
> 
>>I assume that you are calling 'plot' and then 'lines'.  Is the legend just
>>not appearing?  what are you using for the coordinates of the legend?  Have
>>you looked at the output from "par" to see if these values are within the
>>plot region?
>>__________________________________________________________
>>James Holtman               "What is the problem you are trying to solve?"
>>Executive Consultant  --  Office of Technology, Convergys
>>james.holtman at convergys.com
>>(513) 723-2929
>>
>>
>>
>>                      "Anna  H. Pryor"
>>                      <anna at ptolemy.arc.nas        To:       R-help mailing
>>list <r-help at stat.math.ethz.ch> a.gov>                       cc:
>>                      Sent by:                     Subject:  [R] lines and
>>legend r-help-bounces at stat.m
>>                      ath.ethz.ch
>>
>>
>>                      07/01/2003 13:45
>>
>>
>>
>>
>>
>>
>>When I am trying to put a legend on a plot where I am using "lines", R just
>>
>>ignores it.  I can do it with boxplot or plot, but just not with lines.  Am
>>I
>>doing something wrong?  Maybe I am just making a mistake?
>>
>>Anna
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kwan022 at stat.auckland.ac.nz  Tue Jul  1 20:41:46 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 2 Jul 2003 06:41:46 +1200 (NZST)
Subject: [R] lines and legend
In-Reply-To: <200307011123.55630.anna@ptolemy.arc.nasa.gov>
Message-ID: <Pine.LNX.4.44.0307020640160.7944-100000@stat55.stat.auckland.ac.nz>

Hi,

Have you specified the lty and/or col in legend?  For example, are the 
following codes work?
  plot(1:10, type = "n")
  lines(c(1, 10), c(1, 10))
  lines(c(1, 9), c(0, 10), col = "red")
  legend(8, 2, col = c("black", "red"), lty = 1, 
         legend = c("FOO", "FRED"))

On Tue, 1 Jul 2003, Anna  H. Pryor wrote:

> Date: Tue, 1 Jul 2003 11:23:55 -0700
> From: Anna  H. Pryor <anna at ptolemy.arc.nasa.gov>
> To: R-help mailing list <r-help at stat.math.ethz.ch>
> Subject: Re: [R] lines and legend
> 
> Yes, I am using plot and then lines.  The legend is just not appearing.  I am 
> using the coordinates of the legend (150,4) which work on boxplot and plot.  
> I have not looked at the output of par (I don't know how to) to see if they 
> are in the region.  I assumed if they worked for plot and boxplot they would 
> also for lines.
> 
> Anna
> 
> 
> 
> 
> On Tuesday 01 July 2003 11:16, you wrote:
> > I assume that you are calling 'plot' and then 'lines'.  Is the legend just
> > not appearing?  what are you using for the coordinates of the legend?  Have
> > you looked at the output from "par" to see if these values are within the
> > plot region?
> > __________________________________________________________
> > James Holtman               "What is the problem you are trying to solve?"
> > Executive Consultant  --  Office of Technology, Convergys
> > james.holtman at convergys.com
> > (513) 723-2929
> >
> >
> >
> >                       "Anna  H. Pryor"
> >                       <anna at ptolemy.arc.nas        To:       R-help mailing
> > list <r-help at stat.math.ethz.ch> a.gov>                       cc:
> >                       Sent by:                     Subject:  [R] lines and
> > legend r-help-bounces at stat.m
> >                       ath.ethz.ch
> >
> >
> >                       07/01/2003 13:45
> >
> >
> >
> >
> >
> >
> > When I am trying to put a legend on a plot where I am using "lines", R just
> >
> > ignores it.  I can do it with boxplot or plot, but just not with lines.  Am
> > I
> > doing something wrong?  Maybe I am just making a mistake?
> >
> > Anna
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From huxx0098 at umn.edu  Tue Jul  1 20:52:07 2003
From: huxx0098 at umn.edu (huxx0098)
Date: Tue, 01 Jul 2003 13:52:07 CDT
Subject: [R] upload the data
Message-ID: <200307011852.h61Iq7OQ018841@fantasy.software.umn.edu>

 Dear Sir,

I am trying to use the Rweb for the anova. But the first question is how to
load the data named with *.xle. You know, all of my data are input to
excel. Could you take time to tell me how to load the data from the local
excel based file.

I am very appreciate for your help.

Best wishes

Yang Hu
*****************************************************
Department of Entomology
219 Hodson Hall, 1980 Folwell Ave
University of Minnesota
St. Paul, Minnesota  55108  USA
(612) 624-3423
e-mail    huxx0098 at tc.umn.edu



From apjaworski at mmm.com  Tue Jul  1 21:56:33 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 1 Jul 2003 14:56:33 -0500
Subject: [R] lines and legend
Message-ID: <OFFB77B349.3AF7BDA8-ON86256D56.006D34C3@mmm.com>


Anna,

I found this quite useful in positioning a legend:

plot(1:10, 1:10)
legend(locator(1), "A legend")

Now, go to the plot window and click your left mouse where you want the
upper left corner of the legend to appear.  Ithink, this should work no
matter what your coordinate system is.

Hope this helps,

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "Anna  H. Pryor"     |
|         |           <anna at ptolemy.arc.nas|
|         |           a.gov>               |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           07/01/2003 12:45     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       R-help mailing list <r-help at stat.math.ethz.ch>                                                               |
  |      cc:                                                                                                                    |
  |      Subject:  [R] lines and legend                                                                                         |
  >-----------------------------------------------------------------------------------------------------------------------------|




When I am trying to put a legend on a plot where I am using "lines", R just

ignores it.  I can do it with boxplot or plot, but just not with lines.  Am
I
doing something wrong?  Maybe I am just making a mistake?

Anna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sarahmclean9 at yahoo.co.nz  Tue Jul  1 23:35:38 2003
From: sarahmclean9 at yahoo.co.nz (=?iso-8859-1?q?Sarah=20Mclean?=)
Date: Wed, 2 Jul 2003 09:35:38 +1200 (NZST)
Subject: [R] crossed random effects
In-Reply-To: <3F019CD9.8070705@pdf.com>
Message-ID: <20030701213538.8311.qmail@web40005.mail.yahoo.com>

Hi,

thanks for the advice. I have looked at the Pinheiro
and Bates book and I've tried simplifying my model.

I've narrowed the problem down to having mum nested
within pop. If I run the analysis on each population
separately, the interaction between mo and su with mum
works fine.

If I could analyse all of the pops at once this would
be preferable because I have multiple responses and
pops to test so it would take a bit of time. I'm
hoping there is any easier way.

Thanks 
Sarah

 --- Spencer Graves <spencer.graves at PDF.COM> wrote: > 
  Have you studied Pinhiero and Bates (2000) Mixed
> Effects Models in S 
> and S-Plus (Springer)?
> 
> 	  Also, have you tried simplifying your "lme" call
> until you get 
> something that works, then start adding back terms
> in various 
> configurations until it breaks?
> 
> 	  Have you tried to compute how many coefficients
> are estimated in both 
> fixed and random terms and evaluate whether all are
> estimable?  For 
> example, with 2 factors at 2 levels each, if you
> don't have all 4 
> possible combinations, you can't estimate the
> interaction -- even if you 
> have thousands of replications of each.
> 
> 	  Finally, you can always try to read the code. 
> I've learned a lot 
> about S-Plus / R by doing that -- and solved a lot
> of my own problems 
> that way.
> 
> hope this helps.  spencer graves
> 
> Sarah Mclean wrote:
> > Hi,
> > 
> > if I have posted this twice, please ignore this.
> I'm
> > not sure if I sent it to the correct e-mail
> address
> > the first time.
> > 
> > I have a data set on germination and plant growth
> with
> > the following variables:
> > 
> > dataset=fm
> > mass (response)
> > sub (fixed effect)
> > moist (fixed effect)
> > pop (fixed effect)
> > mum (random effect nested within population)
> > iheight (covariate)
> > plot (random effect- whole plot factor for
> split-plot
> > design).
> > 
> > I want to see if moist or sub interacts with mum
> for
> > any of the pops, but I am getting an error
> message. 
> > 
> > This is the formula I used:
> > fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
> > fm$grp = as.factor(rep(1,nrow(fm)))
> > fm$pl <- getGroups(fm, ~1|plot)
> > fm$mo <- getGroups(fm, ~1|moist)
> > fm$su <- getGroups(fm, ~1|sub)
> > 
> >>fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
> > 
> > data=fm,
> random=list(grp=pdBlocked(list(pdIdent(~pl -
> > 1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
> > pdIdent(~pmu:mo - 1)))))
> > Error in chol((value + t(value))/2) : non-positive
> > definite matrix in chol
> > 
> > I know the problem is with the random interaction
> > terms, but I don't know how to overcome this.
> > 
> > Any advice would be greatly appreciated. I'm new
> to R
> > and analysis such as this.
> > 
> > Thank you,
> > 
> > Sarah Mclean
> > sarahmclean9 at yahoo.co.nz
> > 
> > 
> > http://mobile.yahoo.com.au - Yahoo! Mobile
> > - Check & compose your email via SMS on your
> Telstra or Vodafone mobile.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>  

http://mobile.yahoo.com.au - Yahoo! Mobile
- Check & compose your email via SMS on your Telstra or Vodafone mobile.



From p.murrell at auckland.ac.nz  Wed Jul  2 00:36:57 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 02 Jul 2003 10:36:57 +1200
Subject: [R] symbol size on a plot
References: <3F000887.9020100@curie.fr>
Message-ID: <3F020D09.4020509@stat.auckland.ac.nz>

Hi


Philippe Hup? wrote:
 > Hi,
 >
 > I would like to get from a plot the size of the symbols plotted.
 > Imagine I have the following plot function :
 > plot(1:2,1:2, pch=15, cex=4)
 > I would like the get the values SIZE1 and SIZE2 so that if I plot the
 > following rectangle :
 > rect(1.5,1.5, 1.5+SIZE1, 1.5+SIZE2) then the size of this square is
 > exactely the same as the one of the symbols that have been plotted.
 >
 > Thanks for any idea.


This is a bit tricky.  The size of plotting symbols is loosely based on 
the current cex setting (i.e., the size of text), BUT there are some 
"magic multipliers" applied within the C code to determine the exact 
size so that they look "nice".

There are some standard calculations you can perform to get close to the 
symbol size, but you have to do some fiddling to get exact and it will 
depend on which symbol you are plotting.  Here's an example for the 
default pch=1 circles ...

	plot(1:10)
	size1 <- (par("cin")[2]/par("pin")[1])*
	  (par("usr")[2] - par("usr")[1]) * 0.5 * par("cex") * 0.375
	size2 <- (par("cin")[2]/par("pin")[2])*
	  (par("usr")[4] - par("usr")[3]) * 0.5 * par("cex") * 0.375
	rect(5 - size1, 5 - size2, 5 + size1, 5 + size2)

... the 0.375 multiplier is an example of a "magic multiplier".  I got 
it from the following bits of C code in R/src/main/graphics.c

	#define RADIUS	0.375

	#define CMAG	1.0				

	#define GSTR_0  Rf_dpptr(dd)->cra[1] * 0.5 *
		Rf_gpptr(dd)->ipr[0] * Rf_gpptr(dd)->cex

	case 1: /* S octahedron ( circle) */
	    xc = CMAG * RADIUS * GSTR_0;

Further inspection of the C code would give you similar minor fiddles 
for the other plotting symbols.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz



From spencer.graves at pdf.com  Wed Jul  2 02:07:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jul 2003 17:07:26 -0700
Subject: [R] crossed random effects
References: <20030701213538.8311.qmail@web40005.mail.yahoo.com>
Message-ID: <3F02223E.8050404@pdf.com>

	  How many mum's and pop's do you have, and how many observations do 
you have of each mum-pop combination?  If you want mum nested within 
pop, do I infer correctly that each mum has mated with only one pop, but 
that each pop may have offspring by multiple mums?  The table of mum-pop 
combinations might help explain why you got, "Error in chol((value + 
t(value))/2) : non-positive definite matrix in chol".

	  If you can get an answer ignoring pop, then you might be able to get 
an answer with pop as a separate random term without specifying mum 
nested within pop.  Also, I'd check very carefully the specification of 
nesting:  I've messed that up more than once, and I'm bald now, because 
I tore all my hair out before I figured out what I was doing wrong. 
(Well, there is a slight exageration there.)  Have you tried a very 
simple toy problem (or a published example) with nesting to make sure 
you can get the correct answer?

hope this helps.
spencer graves

Sarah Mclean wrote:
> Hi,
> 
> thanks for the advice. I have looked at the Pinheiro
> and Bates book and I've tried simplifying my model.
> 
> I've narrowed the problem down to having mum nested
> within pop. If I run the analysis on each population
> separately, the interaction between mo and su with mum
> works fine.
> 
> If I could analyse all of the pops at once this would
> be preferable because I have multiple responses and
> pops to test so it would take a bit of time. I'm
> hoping there is any easier way.
> 
> Thanks 
> Sarah
> 
>  --- Spencer Graves <spencer.graves at PDF.COM> wrote: > 
>   Have you studied Pinhiero and Bates (2000) Mixed
> 
>>Effects Models in S 
>>and S-Plus (Springer)?
>>
>>	  Also, have you tried simplifying your "lme" call
>>until you get 
>>something that works, then start adding back terms
>>in various 
>>configurations until it breaks?
>>
>>	  Have you tried to compute how many coefficients
>>are estimated in both 
>>fixed and random terms and evaluate whether all are
>>estimable?  For 
>>example, with 2 factors at 2 levels each, if you
>>don't have all 4 
>>possible combinations, you can't estimate the
>>interaction -- even if you 
>>have thousands of replications of each.
>>
>>	  Finally, you can always try to read the code. 
>>I've learned a lot 
>>about S-Plus / R by doing that -- and solved a lot
>>of my own problems 
>>that way.
>>
>>hope this helps.  spencer graves
>>
>>Sarah Mclean wrote:
>>
>>>Hi,
>>>
>>>if I have posted this twice, please ignore this.
>>
>>I'm
>>
>>>not sure if I sent it to the correct e-mail
>>
>>address
>>
>>>the first time.
>>>
>>>I have a data set on germination and plant growth
>>
>>with
>>
>>>the following variables:
>>>
>>>dataset=fm
>>>mass (response)
>>>sub (fixed effect)
>>>moist (fixed effect)
>>>pop (fixed effect)
>>>mum (random effect nested within population)
>>>iheight (covariate)
>>>plot (random effect- whole plot factor for
>>
>>split-plot
>>
>>>design).
>>>
>>>I want to see if moist or sub interacts with mum
>>
>>for
>>
>>>any of the pops, but I am getting an error
>>
>>message. 
>>
>>>This is the formula I used:
>>>fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
>>>fm$grp = as.factor(rep(1,nrow(fm)))
>>>fm$pl <- getGroups(fm, ~1|plot)
>>>fm$mo <- getGroups(fm, ~1|moist)
>>>fm$su <- getGroups(fm, ~1|sub)
>>>
>>>
>>>>fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
>>>
>>>data=fm,
>>
>>random=list(grp=pdBlocked(list(pdIdent(~pl -
>>
>>>1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
>>>pdIdent(~pmu:mo - 1)))))
>>>Error in chol((value + t(value))/2) : non-positive
>>>definite matrix in chol
>>>
>>>I know the problem is with the random interaction
>>>terms, but I don't know how to overcome this.
>>>
>>>Any advice would be greatly appreciated. I'm new
>>
>>to R
>>
>>>and analysis such as this.
>>>
>>>Thank you,
>>>
>>>Sarah Mclean
>>>sarahmclean9 at yahoo.co.nz
>>>
>>>
>>>http://mobile.yahoo.com.au - Yahoo! Mobile
>>>- Check & compose your email via SMS on your
>>
>>Telstra or Vodafone mobile.
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>
>>
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>> 
> 
> 
> http://mobile.yahoo.com.au - Yahoo! Mobile
> - Check & compose your email via SMS on your Telstra or Vodafone mobile.



From clists at perrin.socsci.unc.edu  Wed Jul  2 02:52:22 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Tue, 1 Jul 2003 20:52:22 -0400 (EDT)
Subject: [R] [ibiblio.org #1674] Mirroring request  (fwd)
Message-ID: <Pine.LNX.4.53.0307012051310.7739@perrin.socsci.unc.edu>

R users in the southeastern US (and those on the Internet-2 backbone) will
be happy to hear of the new CRAN mirror at ibiblio.org aka
metalab.unc.edu.

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


---------- Forwarded message ----------
Date: Tue,  1 Jul 2003 17:54:30 -0400 (EDT)
From: Nancy via RT <help-comment at ibiblio.org>
To: andrew_perrin at unc.edu
Cc: john at ibiblio.org, fred at ibiblio.org, dls at ibiblio.org, pjones at ibiblio.org,
     sbw at ibiblio.org, adriane at ibiblio.org, zulick at ibiblio.org,
     squealer at ibiblio.org
Subject: [ibiblio.org #1674] Mirroring request
Resent-Date: Tue, 1 Jul 2003 19:34:05 -0400 (EDT)
Resent-From: Andrew Perrin <andrew_perrin at unc.edu>
Resent-To: clists at perrin.socsci.unc.edu
Resent-Subject: [ibiblio.org #1674] Mirroring request


Dear Andy,

>
> You can find info on R at http://www.us.r-project.org and the CRAN at
>cran.r-project.org.  It's not proprietary in the slightest - GPL all
>the way.
>

Great!  We have set up the new mirror at
ftp://ftp.ibiblio.org/pub/languages/R/CRAN/.  I am writing them now to
have the link added to their list.  Thanks for letting us know!

Please let us know if you have any other questions.
Thank you,
Nancy

-- 
Nancy C. Wilson
http://www.ibiblio.org



From laimonis at maths.otago.ac.nz  Wed Jul  2 03:29:18 2003
From: laimonis at maths.otago.ac.nz (Laimonis Kavalieris)
Date: Wed, 02 Jul 2003 13:29:18 +1200
Subject: [R] How long is a day?
Message-ID: <3F02356E.20802@maths.otago.ac.nz>

Why is 19 March, 1947 a little longer than one day?

 x <-  as.POSIXct("1947-04-16")

 julian(x, origin = as.POSIXct("1947-03-20"))
Time difference of 27 days

 julian(x, origin = as.POSIXct("1947-03-19"))
Time difference of 28.04167 days

 > julian(x, origin = as.POSIXct("1947-03-18"))
Time difference of 29.04167 days

I am running R-1.7.1 compiled on RedHat 9.0

Laimonis

-- 
Dr Laimonis Kavalieris
Department of Mathematics and Statistics
University of Otago
PO Box 56 Dunedin
New Zealand

Tel (64)(3)479 7780
Fax (64)(3)479 8427



From laimonis at maths.otago.ac.nz  Wed Jul  2 03:42:09 2003
From: laimonis at maths.otago.ac.nz (Laimonis Kavalieris)
Date: Wed, 02 Jul 2003 13:42:09 +1200
Subject: [R] How long is a day?
Message-ID: <3F023871.5070807@maths.otago.ac.nz>

Sorry about wasing time -

Apparently NZ daylisght saving time finished on March 20, 1947 , and on 
March 3, 1980, and March 20, 2000 .  I'm impressed that R knows all of this

Laimonis

-- 
Dr Laimonis Kavalieris
Department of Mathematics and Statistics
University of Otago
PO Box 56 Dunedin
New Zealand

Tel (64)(3)479 7780
Fax (64)(3)479 8427



From white_n at usp.ac.fj  Wed Jul  2 04:11:18 2003
From: white_n at usp.ac.fj (Neil White)
Date: Wed, 02 Jul 2003 14:11:18 +1200
Subject: [R] Swedish characters in data frames
In-Reply-To: <3F02356E.20802@maths.otago.ac.nz>
References: <3F02356E.20802@maths.otago.ac.nz>
Message-ID: <3F023F46.2090403@usp.ac.fj>

Hi

I have some data that was prepared while I was Sweden. The columns 
labelled using ? or ? are fine, but for some reason ? is changed to a 
period.

so Bl?b?r stay the same, but D?dved becomes D.dved

I'd like to keep to original variable names

the data is read in using
vegFreqFull <- read.csv("/dat/neil/voles/veg/FrequentFull.csv", header=TRUE)


Using R 1.70 on Win 2K SP4


Regards

Neil

-- 
Dr Neil A. White
Senior Lecturer
Department of Biology
University of the South Pacific
PO Box 1168, Suva
Fiji Islands
Tel: +679 321 2409
Fax: +679 331 5601
www.usp.ac.fj/biology



From ripley at stats.ox.ac.uk  Wed Jul  2 08:15:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Jul 2003 07:15:20 +0100 (BST)
Subject: [R] How long is a day?
In-Reply-To: <3F02356E.20802@maths.otago.ac.nz>
Message-ID: <Pine.LNX.4.44.0307020710490.2651-100000@gannet.stats>

On Wed, 2 Jul 2003, Laimonis Kavalieris wrote:

> Why is 19 March, 1947 a little longer than one day?

A change from Daylight Savings Time in your time zone (unstated),
according to your OS.  Note, 0.04167 is exactly one hour, not `a little 
longer'.

>  x <-  as.POSIXct("1947-04-16")
> 
>  julian(x, origin = as.POSIXct("1947-03-20"))
> Time difference of 27 days
> 
>  julian(x, origin = as.POSIXct("1947-03-19"))
> Time difference of 28.04167 days
> 
>  > julian(x, origin = as.POSIXct("1947-03-18"))
> Time difference of 29.04167 days

You can just subtract dates.  In the UK

> x - as.POSIXct("1947-03-19")
Time difference of 27.95833 days

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jul  2 08:19:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Jul 2003 07:19:30 +0100 (BST)
Subject: [R] Swedish characters in data frames
In-Reply-To: <3F023F46.2090403@usp.ac.fj>
Message-ID: <Pine.LNX.4.44.0307020715580.2651-100000@gannet.stats>

On Wed, 2 Jul 2003, Neil White wrote:

> I have some data that was prepared while I was Sweden. The columns 
> labelled using ? or ? are fine, but for some reason ? is changed to a 
> period.

The reason is in ?data.frame and ?read.table

> so Bl?b?r stay the same, but D?dved becomes D.dved
> 
> I'd like to keep to original variable names
> 
> the data is read in using
> vegFreqFull <- read.csv("/dat/neil/voles/veg/FrequentFull.csv", header=TRUE)

Use check.names = FALSE.  You should also do this by setting a Swedish 
locale.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From P.Lemmens at nici.kun.nl  Wed Jul  2 08:47:42 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 02 Jul 2003 08:47:42 +0200
Subject: Unit of legend() coordinates (was: Re: [R] lines and legend)
In-Reply-To: <200307011123.55630.anna@ptolemy.arc.nasa.gov>
References: <OF89268809.3B7D39BC-ON85256D56.006440BC@convergys.com>
	<200307011123.55630.anna@ptolemy.arc.nasa.gov>
Message-ID: <2165234.1057135662@lemmens.socsci.kun.nl>

Dear all,

I would like to make a suggestion for an improvement (IMHO) of ?legend.

Please add a remark that the x,y positioning coordinates are in the units 
of the plot() itself, *not* in pixels or anything the like.

It would have saved me a lot of time figuring out why my legend just 
wouldn't appear if ?legend would have told me to set the coordinates in the 
units of the x and y axis. I haven't found it anywhere on the list or 
manuals (but I may have missed something). Now that I know this, it seems a 
logic choice, however, refering to a 'graphics device' seems to imply a 
coordinate system in pixels starting from the upper/lower left of the 
device (the usual choice in graphical manipulations in programming 
languages). This assumption is false and IMHO is nowhere in FAQ/manuals 
contradicted.

regards,
Paul

--On dinsdag 1 juli 2003 11:23 -0700 "Anna  H. Pryor" 
<anna at ptolemy.arc.nasa.gov> wrote:

> Yes, I am using plot and then lines.  The legend is just not appearing.
> I am  using the coordinates of the legend (150,4) which work on boxplot
> and plot.   I have not looked at the output of par (I don't know how to)
> to see if they  are in the region.  I assumed if they worked for plot and
> boxplot they would  also for lines.
>
> On Tuesday 01 July 2003 11:16, you wrote:
>> I assume that you are calling 'plot' and then 'lines'.  Is the legend
>> just not appearing?  what are you using for the coordinates of the
>> legend?  Have you looked at the output from "par" to see if these values
>> are within the plot region?
>>
>> When I am trying to put a legend on a plot where I am using "lines", R
>> just
>>
>> ignores it.  I can do it with boxplot or plot, but just not with lines.
>> Am I
>> doing something wrong?  Maybe I am just making a mistake?




-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From ripley at stats.ox.ac.uk  Wed Jul  2 09:17:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Jul 2003 08:17:30 +0100 (BST)
Subject: Unit of legend() coordinates (was: Re: [R] lines and legend)
In-Reply-To: <2165234.1057135662@lemmens.socsci.kun.nl>
Message-ID: <Pine.LNX.4.44.0307020757270.2826-100000@gannet.stats>

On Wed, 2 Jul 2003, Paul Lemmens wrote:

> I would like to make a suggestion for an improvement (IMHO) of ?legend.
> 
> Please add a remark that the x,y positioning coordinates are in the units 
> of the plot() itself, *not* in pixels or anything the like.

That's true of all x-y coordinates of all R plotting functions.  We don't
say it in ?points, for example.

> It would have saved me a lot of time figuring out why my legend just 
> wouldn't appear if ?legend would have told me to set the coordinates in the 
> units of the x and y axis. I haven't found it anywhere on the list or 
> manuals (but I may have missed something). Now that I know this, it seems a 
> logic choice, however, refering to a 'graphics device' seems to imply a 
> coordinate system in pixels starting from the upper/lower left of the 
> device (the usual choice in graphical manipulations in programming 
> languages). This assumption is false and IMHO is nowhere in FAQ/manuals 
> contradicted.

There are several examples on the help page for legend which contradict
that assumption, so I don't see how you can have maintained it after 
reading them, let alone after trying them out.

The coordinate systems are explained in detail in `An Introduction to R':
The phrase `graphics device' does not appear on the help page for legend,
so you must have found that somewhere else.


Is it reasonable to expect the help system/manuals to contradict all
possible misconceptions of all users?  If we attempted to do that, the
help pages would get impossibly cluttered by even those misconceptions
which come to light.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lun.li at glg.ed.ac.uk  Wed Jul  2 10:49:01 2003
From: lun.li at glg.ed.ac.uk (Lun Li)
Date: Wed, 02 Jul 2003 09:49:01 +0100 (BST)
Subject: [R] How to send a function to a slave from master and let the slave
	run using rmpi?
Message-ID: <1057135741.3f029c7ddee5f@staffmail.ed.ac.uk>

Dear All,

Can anyone help in how to send a function to a slave (not all slaves)from master 
and let the slave run using rmpi?

Thanks.


Lun Li



From andrejk at zrc-sazu.si  Wed Jul  2 11:40:46 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Wed, 2 Jul 2003 11:40:46 +0200
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <6rllvjsg1z.fsf@bates4.stat.wisc.edu>
Message-ID: <FHEEJBDDCNPPNJEACDJAGEAGCPAA.andrejk@zrc-sazu.si>

Firstly let me thank all for your answers and suggestions. I would like to
followup on your comments.
Currently I'm implementing multilevel models within a simulation run. So I
was looking for an estimate weather a covariate varies across second level
units or not. I was using HLM before so I knew about the test implemented in
the package and tried to reporoduce it in R. However I think I can follow
your logic about not using that.

I would appreciate your reflection on the following. I need a quantitative
figure to evaluate weather the covariate varies across second level units in
the process of simulation. Of course I will be running thousands of them and
would need to program the condition in code. In one of the previous
questions to the group dr. Bates suggested to use the CI estmates, however
he warned me about their very conservative nature (I got the same tip from
the book). I thought about using the lower bound of the CI as an estimate
with the rule "if above 0 then the covariate varies". Would that be a sound
think to do? Do you have any other suggestions? I would really appreciate
the feedback.

thanks

Andrej



-----Original Message-----
From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu]On Behalf Of Douglas
Bates
Sent: Monday, June 30, 2003 6:09 PM
To: J.R. Lockwood
Cc: Harold Doran; R-Help; Andrej Kveder
Subject: Re: [R] within group variance of the coeficients in LME


"J.R. Lockwood" <lockwood at rand.org> writes:

> >
> > 	Dear listers,
> >
> > 	I can't find the variance or se of the coefficients in a multilevel
model
> > 	using lme.
> >
>
> The component of an lme() object called "apVar" provides the estimated
> asymptotic covariance matrix of a particular transformation of the
> variance components. Dr. Bates can correct me if I'm wrong but I
> believe it is the matrix logarithm of Cholesky decomposition of the
> covariance matrix of the random effects.  I believe the details are in
> the book by Pinheiro and Bates.  Once you know the transformation you
> can use the "apVar" elements to get estimated asympotic standard
> errors for your variance components estimates using the delta method.
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/

First, thanks to those who answered the question.  I have been away
from my email for about a week and am just now catching up on the
r-help list.

As I understand the original question from Andrej he wants to obtain
the standard errors for coefficients in the fixed effects part of the
model.  Those are calculated in the summary method for lme objects and
returned as the component called 'tTable'.  Try

library(nlme)
example(lme)
summary(fm2)$tTable

to see the raw values.

Other software for fitting mixed-effects models, such as SAS PROC
MIXED and HLM, return standard errors along with the estimates of the
variances and covariances of the random effects.  We don't return
standard errors of estimated variances because we don't think they are
useful.  A standard error for a parameter estimate is most useful when
the distribution of the estimator is approximately symmetric, and
these are not.

Instead we feel that the variances and covariances should be converted
to an unconstrained scale, and preferably a scale for which the
log-likelihood is approximately quadratic.  The apVar component that
you mention is an approximate variance-covariance matrix of the
variance components on an unbounded parameterization that uses the
logarithm of any standard deviation and Fisher's z transformation of
any correlations.  If all variance-covariance matrices being estimated
are 1x1 or 2x2 then this parameterization is both unbounded and
unconstrained.  If any are 3x3 or larger then this parameterization
must be further constrained to ensure positive definiteness.
Nevertheless, once we have finished the optimization we convert to
this 'natural' parameterization to assess the variability of the
estimates because these parameters are easily interpreted.

The actual optimization of the profiled log-likelihood is done using
the log-Cholesky parameterization that you mentioned because it is
always unbounded and unconstrained.  Interpreting elements of this
parameter vector is complicated.

I hope this isn't too confusing.



From pankaj at tropmet.res.in  Wed Jul  2 12:58:58 2003
From: pankaj at tropmet.res.in (Pankaj Kumar Srivastav)
Date: Wed, 02 Jul 2003 16:28:58 +0530
Subject: [R] user mannual
Message-ID: <3F02BAF2.9010404@tropmet.res.in>

Dear Sir
I have successfuly downloaded R package. 
Kindly let me know where i will get _/*user mannual*/_ for the same.
awaiting a prompt reply
with regards
Pankaj

@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@
Pankaj Kumar
Research Scholar
Climatology & Hydrometeorology Divison
Indian Institute Of Tropical Meteorology
Homi Bhabha Road, Pune-411008
India

Phone No. +91-20-5893600  Ext. 361 (Work)
          +91-20-5897381 (Home)
Fax:      +91-20-5893825
E-mail:   pankaj at tropmet.res.in
          pinku_1975 at yahoo.com



From kwan022 at stat.auckland.ac.nz  Wed Jul  2 13:10:20 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 2 Jul 2003 23:10:20 +1200 (NZST)
Subject: [R] user mannual
In-Reply-To: <3F02BAF2.9010404@tropmet.res.in>
Message-ID: <Pine.LNX.4.44.0307022306520.11585-100000@stat55.stat.auckland.ac.nz>

Hi,

On Wed, 2 Jul 2003, Pankaj Kumar Srivastav wrote:

> Kindly let me know where i will get _/*user mannual*/_ for the same.

Where did you download R?  I'm assuming you downloaded it from a CRAN 
mirror, if so then there is a section titled "Documentation" on the left 
panel.  Perhaps it is an obvious place to start looking?

> awaiting a prompt reply

The helpers on the list usually respond within 24 hours, but it may not be 
a "prompt reply".  

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From jasont at indigoindustrial.co.nz  Wed Jul  2 13:19:05 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 02 Jul 2003 23:19:05 +1200
Subject: [R] user mannual
In-Reply-To: <3F02BAF2.9010404@tropmet.res.in>
References: <3F02BAF2.9010404@tropmet.res.in>
Message-ID: <3F02BFA9.3080302@indigoindustrial.co.nz>

Pankaj Kumar Srivastav wrote:
> Dear Sir
> I have successfuly downloaded R package. Kindly let me know where i will 
> get _/*user mannual*/_ for the same.

Please check the FAQ for further references.  You'll find many helpful 
hints there, to this and other questions.  Particularly:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#What%20documentation%20exists%20for%20R%3f

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From hdoran at nasdc.org  Wed Jul  2 15:20:21 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Wed, 2 Jul 2003 09:20:21 -0400
Subject: [R] within group variance of the coeficients in LME
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FB5@ernesto.NASDC.ORG>


I think using the metrics provided (standard deviation units) one can assess the degree of variability and determine whether differences are large enough to be considered practically significant. I suppose Dr. Bates and others can comment further, but making decisions solely on the basis of statistical significance is not always the most reasonable method for making research decisions, especially when it is largely a function of sample size.

In my HLM to R experience, I have found the lmList function to provide the best answer to your question, although it is not a "quantitative" test like the chi-square in HLM. You can fit a linear model for each subject, use the intervals function, and plot the intercepts and the slopes for all units with 95% CIs. You can visually see whether the intercepts and slopes vary across units. I think this is a very strong method for examining variability.

However, you can eliminate the random slope (or intercept, or both) and compare the models using the ANOVA command. This will give you an empirical test of the overall model fit.

I was reading in Kirk (Experimental Design) last night and was reminded that examining descriptives, raw data, and visual displays are too often overlooked and dismissed, but are extremely important.

Best,

HCD



 
------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628
<http://www.edperform.net>  
 
 


-----Original Message-----
From: Andrej Kveder [mailto:andrejk at zrc-sazu.si]
Sent: Wednesday, July 02, 2003 5:41 AM
To: Douglas Bates; J.R. Lockwood
Cc: Harold Doran; R-Help
Subject: RE: [R] within group variance of the coeficients in LME


Firstly let me thank all for your answers and suggestions. I would like to
followup on your comments.
Currently I'm implementing multilevel models within a simulation run. So I
was looking for an estimate weather a covariate varies across second level
units or not. I was using HLM before so I knew about the test implemented in
the package and tried to reporoduce it in R. However I think I can follow
your logic about not using that.

I would appreciate your reflection on the following. I need a quantitative
figure to evaluate weather the covariate varies across second level units in
the process of simulation. Of course I will be running thousands of them and
would need to program the condition in code. In one of the previous
questions to the group dr. Bates suggested to use the CI estmates, however
he warned me about their very conservative nature (I got the same tip from
the book). I thought about using the lower bound of the CI as an estimate
with the rule "if above 0 then the covariate varies". Would that be a sound
think to do? Do you have any other suggestions? I would really appreciate
the feedback.

thanks

Andrej



-----Original Message-----
From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu]On Behalf Of Douglas
Bates
Sent: Monday, June 30, 2003 6:09 PM
To: J.R. Lockwood
Cc: Harold Doran; R-Help; Andrej Kveder
Subject: Re: [R] within group variance of the coeficients in LME


"J.R. Lockwood" <lockwood at rand.org> writes:

> >
> > 	Dear listers,
> >
> > 	I can't find the variance or se of the coefficients in a multilevel
model
> > 	using lme.
> >
>
> The component of an lme() object called "apVar" provides the estimated
> asymptotic covariance matrix of a particular transformation of the
> variance components. Dr. Bates can correct me if I'm wrong but I
> believe it is the matrix logarithm of Cholesky decomposition of the
> covariance matrix of the random effects.  I believe the details are in
> the book by Pinheiro and Bates.  Once you know the transformation you
> can use the "apVar" elements to get estimated asympotic standard
> errors for your variance components estimates using the delta method.
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/

First, thanks to those who answered the question.  I have been away
from my email for about a week and am just now catching up on the
r-help list.

As I understand the original question from Andrej he wants to obtain
the standard errors for coefficients in the fixed effects part of the
model.  Those are calculated in the summary method for lme objects and
returned as the component called 'tTable'.  Try

library(nlme)
example(lme)
summary(fm2)$tTable

to see the raw values.

Other software for fitting mixed-effects models, such as SAS PROC
MIXED and HLM, return standard errors along with the estimates of the
variances and covariances of the random effects.  We don't return
standard errors of estimated variances because we don't think they are
useful.  A standard error for a parameter estimate is most useful when
the distribution of the estimator is approximately symmetric, and
these are not.

Instead we feel that the variances and covariances should be converted
to an unconstrained scale, and preferably a scale for which the
log-likelihood is approximately quadratic.  The apVar component that
you mention is an approximate variance-covariance matrix of the
variance components on an unbounded parameterization that uses the
logarithm of any standard deviation and Fisher's z transformation of
any correlations.  If all variance-covariance matrices being estimated
are 1x1 or 2x2 then this parameterization is both unbounded and
unconstrained.  If any are 3x3 or larger then this parameterization
must be further constrained to ensure positive definiteness.
Nevertheless, once we have finished the optimization we convert to
this 'natural' parameterization to assess the variability of the
estimates because these parameters are easily interpreted.

The actual optimization of the profiled log-likelihood is done using
the log-Cholesky parameterization that you mentioned because it is
always unbounded and unconstrained.  Interpreting elements of this
parameter vector is complicated.

I hope this isn't too confusing.



From bob.ohara at helsinki.fi  Wed Jul  2 15:29:43 2003
From: bob.ohara at helsinki.fi (Anon.)
Date: Wed, 02 Jul 2003 16:29:43 +0300
Subject: [R] Maximisation of likelihood of a discrete parameter
Message-ID: <3F02DE47.1020905@helsinki.fi>

Moi!

I have a problem where I want to find the ML estimate of a discrete 
parameter.  I just want a function like optim that finds the max/min 
value for a function.  Does anyone know of such a function for R?

Thanks.

Bob

-- 
Bob O'Hara

Rolf Nevanlinna Institute
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/



From andrejk at zrc-sazu.si  Wed Jul  2 15:39:54 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Wed, 2 Jul 2003 15:39:54 +0200
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FB5@ernesto.NASDC.ORG>
Message-ID: <FHEEJBDDCNPPNJEACDJAAEAKCPAA.andrejk@zrc-sazu.si>

I definetly agree on the use of the graphical tools to assess the importance
of statistical results. However in my case I can't use the graphical tools,
since it is part of a simulation run and the assessment of the results
should be programmed into the code of the simulation.

Therefore I'm looking at the best possible solution to do it quantatively.

Andrej



-----Original Message-----
From: Harold Doran [mailto:hdoran at NASDC.org]
Sent: Wednesday, July 02, 2003 3:20 PM
To: Andrej Kveder; Douglas Bates; J.R. Lockwood
Cc: R-Help
Subject: RE: [R] within group variance of the coeficients in LME



I think using the metrics provided (standard deviation units) one can assess
the degree of variability and determine whether differences are large enough
to be considered practically significant. I suppose Dr. Bates and others can
comment further, but making decisions solely on the basis of statistical
significance is not always the most reasonable method for making research
decisions, especially when it is largely a function of sample size.

In my HLM to R experience, I have found the lmList function to provide the
best answer to your question, although it is not a "quantitative" test like
the chi-square in HLM. You can fit a linear model for each subject, use the
intervals function, and plot the intercepts and the slopes for all units
with 95% CIs. You can visually see whether the intercepts and slopes vary
across units. I think this is a very strong method for examining
variability.

However, you can eliminate the random slope (or intercept, or both) and
compare the models using the ANOVA command. This will give you an empirical
test of the overall model fit.

I was reading in Kirk (Experimental Design) last night and was reminded that
examining descriptives, raw data, and visual displays are too often
overlooked and dismissed, but are extremely important.

Best,

HCD




------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628
<http://www.edperform.net>




-----Original Message-----
From: Andrej Kveder [mailto:andrejk at zrc-sazu.si]
Sent: Wednesday, July 02, 2003 5:41 AM
To: Douglas Bates; J.R. Lockwood
Cc: Harold Doran; R-Help
Subject: RE: [R] within group variance of the coeficients in LME


Firstly let me thank all for your answers and suggestions. I would like to
followup on your comments.
Currently I'm implementing multilevel models within a simulation run. So I
was looking for an estimate weather a covariate varies across second level
units or not. I was using HLM before so I knew about the test implemented in
the package and tried to reporoduce it in R. However I think I can follow
your logic about not using that.

I would appreciate your reflection on the following. I need a quantitative
figure to evaluate weather the covariate varies across second level units in
the process of simulation. Of course I will be running thousands of them and
would need to program the condition in code. In one of the previous
questions to the group dr. Bates suggested to use the CI estmates, however
he warned me about their very conservative nature (I got the same tip from
the book). I thought about using the lower bound of the CI as an estimate
with the rule "if above 0 then the covariate varies". Would that be a sound
think to do? Do you have any other suggestions? I would really appreciate
the feedback.

thanks

Andrej



-----Original Message-----
From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu]On Behalf Of Douglas
Bates
Sent: Monday, June 30, 2003 6:09 PM
To: J.R. Lockwood
Cc: Harold Doran; R-Help; Andrej Kveder
Subject: Re: [R] within group variance of the coeficients in LME


"J.R. Lockwood" <lockwood at rand.org> writes:

> >
> > 	Dear listers,
> >
> > 	I can't find the variance or se of the coefficients in a multilevel
model
> > 	using lme.
> >
>
> The component of an lme() object called "apVar" provides the estimated
> asymptotic covariance matrix of a particular transformation of the
> variance components. Dr. Bates can correct me if I'm wrong but I
> believe it is the matrix logarithm of Cholesky decomposition of the
> covariance matrix of the random effects.  I believe the details are in
> the book by Pinheiro and Bates.  Once you know the transformation you
> can use the "apVar" elements to get estimated asympotic standard
> errors for your variance components estimates using the delta method.
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/

First, thanks to those who answered the question.  I have been away
from my email for about a week and am just now catching up on the
r-help list.

As I understand the original question from Andrej he wants to obtain
the standard errors for coefficients in the fixed effects part of the
model.  Those are calculated in the summary method for lme objects and
returned as the component called 'tTable'.  Try

library(nlme)
example(lme)
summary(fm2)$tTable

to see the raw values.

Other software for fitting mixed-effects models, such as SAS PROC
MIXED and HLM, return standard errors along with the estimates of the
variances and covariances of the random effects.  We don't return
standard errors of estimated variances because we don't think they are
useful.  A standard error for a parameter estimate is most useful when
the distribution of the estimator is approximately symmetric, and
these are not.

Instead we feel that the variances and covariances should be converted
to an unconstrained scale, and preferably a scale for which the
log-likelihood is approximately quadratic.  The apVar component that
you mention is an approximate variance-covariance matrix of the
variance components on an unbounded parameterization that uses the
logarithm of any standard deviation and Fisher's z transformation of
any correlations.  If all variance-covariance matrices being estimated
are 1x1 or 2x2 then this parameterization is both unbounded and
unconstrained.  If any are 3x3 or larger then this parameterization
must be further constrained to ensure positive definiteness.
Nevertheless, once we have finished the optimization we convert to
this 'natural' parameterization to assess the variability of the
estimates because these parameters are easily interpreted.

The actual optimization of the profiled log-likelihood is done using
the log-Cholesky parameterization that you mentioned because it is
always unbounded and unconstrained.  Interpreting elements of this
parameter vector is complicated.

I hope this isn't too confusing.



From berislav.bosnjak at pliva.hr  Wed Jul  2 15:47:45 2003
From: berislav.bosnjak at pliva.hr (Berislav Bosnjak)
Date: Wed, 2 Jul 2003 15:47:45 +0200
Subject: [R] error while runing Bioconductro install script
Message-ID: <BEEHKHAKBAIGLMFFMHEHKEEMCCAA.berislav.bosnjak@pliva.hr>

Dear all!

I had installed R 1.6.2. and I tried to download Bioconductor using
Bioconductor install script. However, when I run the script in R it reports
following:

Error in getBioC(all) : Your R is not currently configured to allow HTTP
connections, which is required for getBioC to work properly.

Can you help me how to solve this problem and successfully download
Bioconductor packages.
Thank you very much in advance.

Sincerely,

Berislav Bosnjak, BSc



From lockwood at rand.org  Wed Jul  2 15:48:52 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Wed, 2 Jul 2003 09:48:52 -0400 (EDT)
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <FHEEJBDDCNPPNJEACDJAAEAKCPAA.andrejk@zrc-sazu.si>
Message-ID: <Pine.LNX.4.33.0307020938580.27455-100000@penguin.rand.org>

> I would appreciate your reflection on the following. I need a quantitative
> figure to evaluate weather the covariate varies across second level units in
> the process of simulation. Of course I will be running thousands of them and
> would need to program the condition in code. In one of the previous
> questions to the group dr. Bates suggested to use the CI estmates, however
> he warned me about their very conservative nature (I got the same tip from
> the book). I thought about using the lower bound of the CI as an estimate
> with the rule "if above 0 then the covariate varies". Would that be a sound
> think to do? Do you have any other suggestions? I would really appreciate
> the feedback.

I somehow missed Dr. Bates useful clarification regarding the apVar
component of the lme object (I had forgotten that the optimization and
apVar used different transformations).  I agree with him that even
though it is possible to obtain standard errors for your variance
components using an appropriate transformation of the apVar component,
you probably don't want to use that because the Wald statistics on
this scale will be ill-behaved. I would second the notion that the CIs
obtained from intervals() on your lme object (using a more
well-behaved scale) during the simulation is the best way to get at
what you want, even if they are conservative.  I think what you want
to do is think about why you are simulating -- presumably to
understand the properties of some operation on real data.  If you had
real data and wanted to test the variance components, you would
probably use the CIs from intervals().

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From skhadhar at cdcixis-cm.com  Wed Jul  2 15:56:48 2003
From: skhadhar at cdcixis-cm.com (Khadhar, Sofiene)
Date: Wed, 2 Jul 2003 15:56:48 +0200 
Subject: [R] Using mva.dll
Message-ID: <3E44E5AEE949DF4FA8E736AF13A8B31D8FB5BF@mspereire.cm.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030702/b0db7d0f/attachment.pl

From bates at stat.wisc.edu  Wed Jul  2 15:59:11 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 02 Jul 2003 13:59:11 -0000
Subject: [R] within group variance of the coeficients in LME
In-Reply-To: <FHEEJBDDCNPPNJEACDJAGEAGCPAA.andrejk@zrc-sazu.si>
References: <FHEEJBDDCNPPNJEACDJAGEAGCPAA.andrejk@zrc-sazu.si>
Message-ID: <6radbx9gih.fsf@bates4.stat.wisc.edu>

"Andrej Kveder" <andrejk at zrc-sazu.si> writes:

> I would appreciate your reflection on the following. I need a quantitative
> figure to evaluate weather the covariate varies across second level units in
> the process of simulation. Of course I will be running thousands of them and
> would need to program the condition in code. In one of the previous
> questions to the group dr. Bates suggested to use the CI estmates, however
> he warned me about their very conservative nature (I got the same tip from
> the book). I thought about using the lower bound of the CI as an estimate
> with the rule "if above 0 then the covariate varies". Would that be a sound
> think to do? Do you have any other suggestions? I would really appreciate
> the feedback.

I don't think that will work.  The confidence intervals are evaluated
from the standard error on the scale of the logarithm of the standard
deviation then transformed back to the standard deviation or variance
scale.  The lower end point on the standard deviation or variance
scale will always be greater than zero.



From spencer.graves at pdf.com  Wed Jul  2 16:22:22 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Jul 2003 07:22:22 -0700
Subject: [R] Maximisation of likelihood of a discrete parameter
References: <3F02DE47.1020905@helsinki.fi>
Message-ID: <3F02EA9E.6060000@pdf.com>

You may already know this, but in case you don't get a canned solutions, 
I'll mention that this is an integer programming problem.  If you can 
first get a solution  that it is not an integer, then you do an 
exhaustive search in a neighborhood near the maximum.  In one dimension, 
this is easy:  Just check floor and ceiling and pick the winner.  In 
multiple dimensions, this is harder, but the same principle should 
apply.  For elegant algorithms, see the literature on integer programming.

hope this helps.  spencer graves

Anon. wrote:
> Moi!
> 
> I have a problem where I want to find the ML estimate of a discrete 
> parameter.  I just want a function like optim that finds the max/min 
> value for a function.  Does anyone know of such a function for R?
> 
> Thanks.
> 
> Bob
>



From Robert.Schick at noaa.gov  Wed Jul  2 17:27:41 2003
From: Robert.Schick at noaa.gov (Robert Schick)
Date: Wed, 02 Jul 2003 08:27:41 -0700
Subject: [R] using [i] to plot & label all vector elements
Message-ID: <3F02F9ED.1562345D@noaa.gov>

I'm using 1.6.2 on a win 2k box. (I know I'm due for an upgrade.)

I've used brute force to label a series of vectors, and I'm wondering if
there's a better way to do this. I wanted to create a biplot of the 2nd
& 3rd PCs, and did this:

test <- edit(loadings(cv.prc.spr))
test.1 <- test[,1]
test.2 <- test[,2]
test.3 <- test[,3]
x0 <- 0
y0 <- 0
i <- order(test.1)
x <- test.1[i]
y <- test.2[i]
z <- test.3[i]

# and added the arrows to the pc plot with the following brute force
labeling.

arrows(x0, y0, x[i],y[i], length=0.1, lty=3)
text(x[i],y[i],labels=c("srnv", "ann.precip", "area.gt500", "modc" ,
"cscd","nwca", "range.ann.T", "mean.ann.T", "min.ann.T", "sedi", "gran",
"aluv", "cwca1", "elev", "volc", "gcv", "max.ann.T", "peak.flow.mnth"),
cex=.75)

When I tried using labels=rownames(x[i]) I get NULL. Three questions: 
1. Why is that statement NULL?
2. Is there a way to plot the labels without having to hard code them as
above. 
3. Even if I hard code them, there's something that seems to change in
the order of that that is eluding me. I suspect it's a combination of
[i] and order, but I don't understand it. Advice?

FWIW, test.1 looks like:
> test.1
     max.ann.T     mean.ann.T      min.ann.T           aluv          
cscd 
    -2.2162273     -2.1489203     -1.9744394     -1.2975221    
-1.1924917 
<snip>

-- 
Rob Schick
Ecologist
NOAA Fisheries
Santa Cruz Lab
110 Shaffer Road
Santa Cruz, CA 95060
Phone: 831.420.3960



From ernesto at ipimar.pt  Wed Jul  2 18:02:24 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 02 Jul 2003 16:02:24 -0000
Subject: [R] order in lapply
Message-ID: <1057161737.4305.62.camel@gandalf.ipimar.pt>

Hi

When using "split" is it possible to keep then same ordering of the
factor on the output list ?

Thanks

EJ

-- 
Ernesto Jardim <ernesto at ipimar.pt>
Bi?logo Marinho/Marine Biologist
IPIMAR - Instituto Nacional de Investiga??o Agr?ria e das Pescas
IPIMAR - National Research Institute for Agriculture and Fisheries
Av. Brasilia, 1400-006
Lisboa, Portugal
Tel: +351 213 027 000
Fax: +351 213 015 948
http://ernesto.freezope.org



From ripley at stats.ox.ac.uk  Wed Jul  2 18:19:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 2 Jul 2003 17:19:06 +0100 (BST)
Subject: [R] using [i] to plot & label all vector elements
In-Reply-To: <3F02F9ED.1562345D@noaa.gov>
Message-ID: <Pine.LNX.4.44.0307021708020.17536-100000@gannet.stats>

On Wed, 2 Jul 2003, Robert Schick wrote:

> I'm using 1.6.2 on a win 2k box. (I know I'm due for an upgrade.)
> 
> I've used brute force to label a series of vectors, and I'm wondering if
> there's a better way to do this. I wanted to create a biplot of the 2nd
> & 3rd PCs, and did this:

You seem to be using the 1st and 2nd.  What's wrong with biplot()'s 
princomp method (which can do any pair)?

> test <- edit(loadings(cv.prc.spr))
> test.1 <- test[,1]
> test.2 <- test[,2]
> test.3 <- test[,3]
> x0 <- 0
> y0 <- 0
> i <- order(test.1)
> x <- test.1[i]
> y <- test.2[i]
> z <- test.3[i]
> 
> # and added the arrows to the pc plot with the following brute force
> labeling.
> 
> arrows(x0, y0, x[i],y[i], length=0.1, lty=3)
> text(x[i],y[i],labels=c("srnv", "ann.precip", "area.gt500", "modc" ,
> "cscd","nwca", "range.ann.T", "mean.ann.T", "min.ann.T", "sedi", "gran",
> "aluv", "cwca1", "elev", "volc", "gcv", "max.ann.T", "peak.flow.mnth"),
> cex=.75)
> 
> When I tried using labels=rownames(x[i]) I get NULL. Three questions: 
> 1. Why is that statement NULL?

x[i] is a single vector, not a matrix.  It is matrices which have 
rownames.  names() would have worked.

> 2. Is there a way to plot the labels without having to hard code them as
> above..

Yes.

> 3. Even if I hard code them, there's something that seems to change in
> the order of that that is eluding me. I suspect it's a combination of
> [i] and order, but I don't understand it. Advice?

We can't see that from the details you have given.

Try working with the matrix directly:

test2 <- test[sort.list(test[, 1], ]
arrows(0, 0, test2[, 1], test2[, 2], length=0.1, lty=3)
text(test2[, 1:2], labels=rownames(test2), cex=0.75-

However, in this case there is no point in permuting the rows, so what 
exactly did you intend?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From djw1005 at cam.ac.uk  Wed Jul  2 18:36:44 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Wed, 2 Jul 2003 17:36:44 +0100 (BST)
Subject: [R] using [i] to plot & label all vector elements
In-Reply-To: <3F02F9ED.1562345D@noaa.gov>
Message-ID: <Pine.SOL.3.96.1030702172433.3605A-100000@virgo.cus.cam.ac.uk>


Robert Schick wrote:
> I've used brute force to label a series of vectors, and I'm wondering if
> there's a better way to do this. I wanted to create a biplot of the 2nd
> & 3rd PCs ...

I was unhappy with the standard biplot routine (the arrows didn't have
tick marks) so I wrote my own, a lattice version. It may be of some help,
but I should warn you that it is not wrapped up in a package, and there
are undoubtedly bugs. 
  http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/biplot.html

If m is a matrix with named rows and columns, you would invoke it by

res <- princompfull(m)

xyaplot (y~x, data=res$observations,
         axes=y~x, data.axes=res$variables, 
         subset.axes=c(FALSE,TRUE,TRUE,...), # whichever vectors to plot
         axes.spec=list(origin="o",min="min",max="max",ticks=TRUE),
         label="label", label.axes="label")

(The last two lines are a silly incantation which I will eventually tidy
up.)

Damon.



From lockwood at rand.org  Wed Jul  2 20:07:09 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Wed, 2 Jul 2003 14:07:09 -0400 (EDT)
Subject: [R] Re: Fitting particular repeated measures model with lme()
In-Reply-To: <Pine.LNX.4.33.0306201104410.11307-100000@penguin.rand.org>
Message-ID: <Pine.LNX.4.33.0307021404090.27455-100000@penguin.rand.org>

Hello,

A couple weeks ago I posted a message about using lme() to fit a model
based on the following: Students are tested in two years, and are
linked to teachers in the second year only.  Thus students are not
nested within teachers in the traditional sense.  The model for
student j in class i is:

Y_{ij0} = a_0 + e_{ij0}
Y_{ij1} = a_1 + b_i + e_{ij1}

with Var(b_i) the teacher variance component and Cov(e_{ij0},e_{ij1})
unstructured.  Thus if the data are organized by student, the "Z"
matrix in the usual linear mixed model notation has every other row
equal to a row of zeros.  For reference I include at the end of this
message a function "generate.data()" that simulates (balanced) data
according to this model.

I think I figured out a way to fit this model in lme() but I have some
questions about it.  My strategy was essentially to brute-force create
the Z matrix with columns containing indicators of the second year
teacher interleaved with zeros for the first year scores, and then
force the covariance matrix of the random effects to be a constant
times the identity.  Here is the code I am using:

############
library(nlme)
library(MASS)
ntch<-30
d<-generate.data(k=ntch)
varnames<-paste("tchid",1:ntch,sep="")
fmla<-as.formula(paste("~",paste(varnames,collapse="+"),"-1"))

lme.u2a<-lme(fixed=Y~time,data=d,random=list(tchid=pdIdent(form=fmla)),weights=varIdent(form=~1|time),correlation=corSymm(form = ~1|tchid/studid))

lme.u2b<-lme(fixed=Y~time,data=d,random=list(dummy.group=pdIdent(form=fmla)),weights=varIdent(form=~1|time),correlation=corSymm(form = ~1|dummy.group/studid))
############

I have one set of questions and one observation:

1) The two model fits provide (essentially) the same estimates, and
these estimates seem reasonable.  However, they provide different DF
in the table of fixed effects, reflecting the fact that the nesting
structures specified in the two models are different.  In the first
case, I specify that students are nested within teachers which is not
exactly true.  In the second, I use a dummy grouping variable
"dummy.group" which is identically equal to 1 for all records. 

My major questions are why/how these specifications fit the same
model, whether it is possible for me to fit the model without
specifying names to the list components in the random statement, and
more generally, whether lme() *requires* a nesting structure at all.
I guess what it comes down to is that I am having trouble
understanding how exactly specifying the random statement as a list of
formulas works.

2) intervals() on either of these objects fails with the following
   error message:

Error in structure(exp(as.vector(object)), names = c(paste("sd(", deparse(formula(object)[[2]]),  : 
	names attribute must be the same length as the vector

Any insights would be greatly appreciated.

best regards,

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/

## Function for generating data
generate.data<-function(k=50,n=25,mu=c(0,10),tau=sqrt(2),esd=sqrt(c(2,4)),corr=0.8){
  ## k=number of teachers
  ## n=number of students per teacher
  Sigma<-diag(esd)%*%matrix(c(1,corr,corr,1),ncol=2)%*%diag(esd)
  theta<-rnorm(k,sd=tau)
  theta<-cbind(0,rep(theta,each=n))
  e<-mvrnorm(n*k,mu=mu,Sigma=Sigma)
  Y<-c(t(theta))+c(t(e))
  tchid<-gl(k,2*n)
  z<-model.matrix(~tchid-1)
  s<-seq(from=1,to=(2*k*n-1),by=2)
  z[s,]<-rep(0,k)
  studid<-gl(n*k,2)
  time<-rep(c(0,1),times=n*k)
  dummy.group<-factor(rep(1,2*n*k))
  return(data.frame(studid,Y,time,dummy.group,tchid,z))
}



From Jo.Hardin at pomona.edu  Wed Jul  2 20:23:45 2003
From: Jo.Hardin at pomona.edu (Johanna Hardin)
Date: Wed, 2 Jul 2003 11:23:45 -0700 
Subject: [R] Batch files in R
Message-ID: <5B526011BAC3D511BFBA00B0D020615A0327393A@excsrv-acd1.pomona.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030702/770dc7e6/attachment.pl

From replies-discarded at sas.com  Wed Jul  2 20:28:29 2003
From: replies-discarded at sas.com (SAS Postmaster 62)
Date: Wed, 02 Jul 2003 14:28:29 -0400
Subject: [R] Notification of Virus Prevention Measures Taken - Please Read
	for D etails
Message-ID: <200307021828.h62ISHUu015795@stat.math.ethz.ch>

Virus prevention measures at SAS Institute have removed a suspicious attachment from an email message that appears to have been sent from the address named below. (Note that some "From" addresses on mail messages are forged, so it is possible that this address was not actually involved.)  The attachment has been permanently deleted, and did not reach the recipient. Viruses can range from extremely destructive ones that mass propagate to very minor ones that are intended as a "joke".

If you are the sender, the possibility exists that your PC is infected and you should investigate this immediately.  Please disconnect your PC from the network immediately and call your Help Desk for support. 

SAS staff can refer to http://sww.sas.com/helpdesk/virusdoc.htm for general virus information.

++++++++++++++++++++++++++++++++++++++
Sender: r-announce at lists.r-project.org
Recipient(s):  <Yi.Liao at sas.com>
Received:  Wed Jul 02 14:28:29 2003
Subject:  Re: Movie

Virus Detected and Action Taken: The file (your_details.zip/details.pif) was infected with the WORM_SOBIG.E computer virus. The following action has been taken: remove.
++++++++++++++++++++++++++++++++++++++



From sundar.dorai-raj at pdf.com  Wed Jul  2 20:46:05 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 02 Jul 2003 13:46:05 -0500
Subject: [R] order in lapply
References: <1057161737.4305.62.camel@gandalf.ipimar.pt>
Message-ID: <3F03286D.8000201@pdf.com>



Ernesto Jardim wrote:
> Hi
> 
> When using "split" is it possible to keep then same ordering of the
> factor on the output list ?
> 
> Thanks
> 
> EJ
> 

There may be an easier way to do this but I would make the split 
variable ordered before calling split. As in:

 > d = data.frame(a = 1:4, b = c("d", "c"))
 > sapply(d, data.class)
         a         b
"numeric"  "factor"
 > split(d$a, d$b)
$c
[1] 2 4

$d
[1] 1 3

 > d$b = ordered(d$b, c("d", "c"))
 > sapply(d, data.class)
         a         b
"numeric" "ordered"
 > split(d$a, d$b)
$d
[1] 1 3

$c
[1] 2 4

 >

Regards,
Sundar



From rossini at blindglobe.net  Wed Jul  2 20:59:23 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 02 Jul 2003 11:59:23 -0700
Subject: [R] Batch files in R
In-Reply-To: <5B526011BAC3D511BFBA00B0D020615A0327393A@excsrv-acd1.pomona.edu>
	(Johanna Hardin's message of "Wed, 2 Jul 2003 11:23:45 -0700")
References: <5B526011BAC3D511BFBA00B0D020615A0327393A@excsrv-acd1.pomona.edu>
Message-ID: <87fzlosqk4.fsf@jeeves.blindglobe.net>


?save  

is your friend.  Write out dataobjects with different filenames.


Johanna Hardin <Jo.Hardin at pomona.edu> writes:

> When I submit more than one batch file (same programs, different parameter
> values, huge simulations, different result names) the only results that get
> saved are from the *last* batch file to finish.  They are all being run in
> the same subdirectory (so same .RData file?)
>
> I've done:
>
> R --save BATCH infile outfile
>
> and I've also put
>
> q(save="yes") 
>
> at the end of the program, but it will still only save results from one
> program.  Is there any way to get all the results to save without putting
> each of the batch files in a separate directory?
>
> Thanks, Jo
>
> Johanna Hardin
> Department of Mathematics & Computer Science
> Pomona College
> (909) 607-8717
> jo.hardin at pomona.edu
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From sarahmclean9 at yahoo.co.nz  Wed Jul  2 21:29:22 2003
From: sarahmclean9 at yahoo.co.nz (=?iso-8859-1?q?Sarah=20Mclean?=)
Date: Thu, 3 Jul 2003 07:29:22 +1200 (NZST)
Subject: [R] crossed random effects- clarified version
Message-ID: <20030702192922.69924.qmail@web40010.mail.yahoo.com>

Hi,

here is a clarified version of my problem.

I have a total of 4*74 observations on 74 different
mums in 5 different populations of mums, subject to 6
treatments (2 moisture levels*3 substrate types).

I want to know if mum interacts with moisture level or
substrate type.

Population, moisture and substrate are fixed effects
and mum is a random effect within population. Plot is
a random whole-plot error for a split-plot design. The
data set is called fm.

This is the formula I used and the error message I
got:

fm$pmu <- getGroups(fm, ~1|pop/mum, level=2)
fm$grp = as.factor(rep(1,nrow(fm)))
fm$pl <- getGroups(fm, ~1|plot)
fm$mo <- getGroups(fm, ~1|moist)
fm$su <- getGroups(fm, ~1|sub)
> fm1 <- lme(sqrt(mass) ~ iheight + moist*sub*pop,
data=fm, random=list(grp=pdBlocked(list(pdIdent(~pl -
1), pdIdent(~pmu - 1),  pdIdent(~pmu:su - 1),
pdIdent(~pmu:mo - 1)))))
Error in chol((value + t(value))/2) : non-positive
definite matrix in chol

The model works if the interaction terms:
pdIdent(~pmu:su - 1), pdIdent(~pmu:mo - 1), are
removed, so they are causing the problem. 

The model also works if I test mums from one
population at a time so that mum no longer needs to be
nested, i.e. if I replace pmu with mu:
fm$mu <- getGroups(fm, ~1|mum)
> fm1 <- lme(sqrt(mass) ~ iheight + moist*sub,
data=fm, random=list(grp=pdBlocked(list(pdIdent(~pl -
1), pdIdent(~pmu - 1),  pdIdent(~mu:su - 1),
pdIdent(~mu:mo - 1)))))

It would be a lot faster if I can test all of the
populations at once instead of individually.

Any help would be much appreciated.

Thanks,
Sarah

http://mobile.yahoo.com.au - Yahoo! Mobile
- Check & compose your email via SMS on your Telstra or Vodafone mobile.



From depinayj at mail.nih.gov  Wed Jul  2 22:05:13 2003
From: depinayj at mail.nih.gov (Depinay, Jean-marc (NIH/FIC))
Date: Wed, 2 Jul 2003 16:05:13 -0400 
Subject: [R] Second Y axis
Message-ID: <0E3E7E8F6E23DF4C8127A063568356B5010CF263@nihexchange12.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030702/ce39ef5d/attachment.pl

From spencer.graves at pdf.com  Wed Jul  2 22:29:07 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Jul 2003 13:29:07 -0700
Subject: [R] Second Y axis
References: <0E3E7E8F6E23DF4C8127A063568356B5010CF263@nihexchange12.nih.gov>
Message-ID: <3F034093.7000003@pdf.com>

?axis

hope this helps.  spencer graves

Depinay, Jean-marc (NIH/FIC) wrote:
> I would like to add a second graph on the right y axis. Is it a way to do so
> with R?
> Thank you for your help,
> jmd
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From f.calboli at ucl.ac.uk  Wed Jul  2 23:40:02 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Wed, 02 Jul 2003 22:40:02 +0100
Subject: [R] DF in LME
In-Reply-To: <6rhe67sezw.fsf@bates4.stat.wisc.edu>
References: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
	<3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
Message-ID: <3.0.6.32.20030702224002.02944278@pop-server.ucl.ac.uk>

Dear All,

I know I am quite obsessive and downright annoying (I apologize about that,
but it's the way I am), but I would like to get my understanding of the way
nlme calculates degrees of freedom straight.

For instance, on page 91 in Pinheiro and Bates (2000), on the examle of
anova(fm2Machie), how is the sum of DF *Pi* corresponding to the terms
estimated at level *i* calculated? In the example presenting
anova(fm2Machine), *P1 = 0*. I just fail to see why. Same thing for *P2 =
2* (although this seems intuitive, but intuitive could be miles off the
real reason) and *P3 = 0*.

Incidentally, should I change the grouping, putting *machine* outside and
*worker* inside, would anything change?

A second thing: is there any substantial difference between the classical
decomposition of DF for an ANOVA and the method used by lme for the
interaction between a fixed and a random effect, in case the random
variable is nominal and the fixed one continuous?

Best regards,
Federico Calboli


=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From bates at stat.wisc.edu  Thu Jul  3 00:08:17 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 02 Jul 2003 22:08:17 -0000
Subject: [R] DF in LME
In-Reply-To: <3.0.6.32.20030702224002.02944278@pop-server.ucl.ac.uk>
References: <3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
	<3.0.6.32.20030626155725.02665008@pop-server.ucl.ac.uk>
	<3.0.6.32.20030702224002.02944278@pop-server.ucl.ac.uk>
Message-ID: <6radbw1t0w.fsf@bates4.stat.wisc.edu>

Federico Calboli <f.calboli at ucl.ac.uk> writes:

> Dear All,
> 
> I know I am quite obsessive and downright annoying (I apologize about that,
> but it's the way I am), but I would like to get my understanding of the way
> nlme calculates degrees of freedom straight.
> 
> For instance, on page 91 in Pinheiro and Bates (2000), on the examle of
> anova(fm2Machie), how is the sum of DF *Pi* corresponding to the terms
> estimated at level *i* calculated? In the example presenting
> anova(fm2Machine), *P1 = 0*. I just fail to see why. Same thing for *P2 =
> 2* (although this seems intuitive, but intuitive could be miles off the
> real reason) and *P3 = 0*.

The $p_i$ are the number of degrees of freedom of fixed-effects terms
estimated at level $i$.  For the model fit to the Machines data, level
0 is the intercept (1 d.f.) and the only other fixed effect is for
Machine.  That factor has 3 levels and 2 d.f. when the model also
contains the intercept.

There are 3 possible levels, which we number starting with the level
that has the largest groups.  (Note that this is the reverse of the
numbering of the levels in the multilevel modeling literature.)  So
our level 1 would be Worker, level 2 is Machine %in% Worker, and level
3 is individual observations.   This is why the counts of the numbers
of groups are $m_1=6$, $m_2=18$ and $m_3=54$.

The Machine term varies within the groups determined by Worker but
does not vary within the groups determined by "Machine %in% Worker".
Hence it is estimated at level 2 (in our notation) from which you get
the corresponding degrees of freedom.

I should point out that the F- and t-tests are approximate tests at
best.  Both the random effects parameters and the fixed effects
parameters are being estimated in these models.  To get F-tests we are
conditioning on the values of the parameters determining the
variance-covariance of the random effects.  There are good reasons to
do this (the random-effects and the fixed-effects parameters are
asymptotically uncorrelated) but the tests are still based on an
approximation.  

>From a practical point of view, if the degrees of freedom are
reasonably large then they do not need to be precisely determined.

> Incidentally, should I change the grouping, putting *machine* outside and
> *worker* inside, would anything change?

Yes.  Try it and see.

> A second thing: is there any substantial difference between the classical
> decomposition of DF for an ANOVA and the method used by lme for the
> interaction between a fixed and a random effect, in case the random
> variable is nominal and the fixed one continuous?

As far as I know the classical decompositions become very difficult to
formulate for unbalanced data.  The theory of partitioning the
response space into orthogonal subspaces is very elegant but can be
easily upset by lack of balance.  I tell my students that methods that
only work with balanced data are interesting from a theoretical point
of view but not from a practical point of view.  Observational data is
almost always unbalanced and even data from a balanced, designed
experiment frequently ends up being unbalanced because of missing
data.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From steve.dutky at tfn.com  Thu Jul  3 01:05:47 2003
From: steve.dutky at tfn.com (Dutky, Steve)
Date: Wed, 2 Jul 2003 19:05:47 -0400 
Subject: [R] fractional seconds from format.POSIXct
Message-ID: <6EEA47532CD0D611887500B0D04943453FB918@TFSMDMSG7>

Is there a format that yields fractional seconds from format.POSIXct and/or
related methods?

I'm attempting to use irts with millisecond events.

Thanks, Steve Dutky
TF Rockville Network Services
301-545-4113 desk
301-325-8146 cell



From Simon.Blomberg at anu.edu.au  Thu Jul  3 02:53:22 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 3 Jul 2003 10:53:22 +1000
Subject: [R] beginner gls (nlme) question
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133D2@CASEVS02.cas.anu.edu.au>

Hi all,

I am trying to get a handle on gls (package nlme). I have a toy problem: 3 fixed factors (A, B, C), two levels each, 5 replicates per treatment. The response variable is continuous, normal. I have a correlation matrix of the form:

> mat
     A    B C
A 1.00 0.75 0
B 0.75 1.00 0
C 0.00 0.00 1

which is common to all observations.

How do I construct the call to gls? I think I need to use correlation=corSymm(), but I do not understand the precise syntax. I have read the relevant parts of Pinheiro and Bates, but they only talk about cases where the corSymm correlation structure is modelled, rather than known. I have also searched the R archives, but no luck.

I think it should be of the form gls(response~A*B*C, data=dat, correlation=corSymm(...?))
but I don't understand the arguments to corSymm.

Thanks in advance,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379



From gisar at nus.edu.sg  Thu Jul  3 05:32:21 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Thu, 3 Jul 2003 11:32:21 +0800
Subject: [R] Batch files in R
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FECF@MBXSRV03.stf.nus.edu.sg>

You can use save(object1, file="lalala.1"), save(object1,
file="lalala.2"), ... and the use load() to restore the object1 and
object2. Or if you have many objects in a simulation to save, you can
save all objects using save.image("sim1.result.R").

Another option is to use write.table or zz <- file("lalal1.txt"); cat(
... , file=zz) etc if you want a human-readable form of your output.

It would be a good idea to use the compress=TRUE option in save() and R
--no-save.


-----Original Message-----
From: Johanna Hardin [mailto:Jo.Hardin at pomona.edu] 
Sent: Thursday, July 03, 2003 2:24 AM
To: 'r-help at lists.R-project.org'
Subject: [R] Batch files in R



When I submit more than one batch file (same programs, different
parameter values, huge simulations, different result names) the only
results that get saved are from the *last* batch file to finish.  They
are all being run in the same subdirectory (so same .RData file?)

I've done:

R --save BATCH infile outfile

and I've also put

q(save="yes") 

at the end of the program, but it will still only save results from one
program.  Is there any way to get all the results to save without
putting each of the batch files in a separate directory?

Thanks, Jo

Johanna Hardin
Department of Mathematics & Computer Science
Pomona College
(909) 607-8717
jo.hardin at pomona.edu



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Jul  3 08:10:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 07:10:47 +0100 (BST)
Subject: [R] fractional seconds from format.POSIXct
In-Reply-To: <6EEA47532CD0D611887500B0D04943453FB918@TFSMDMSG7>
Message-ID: <Pine.LNX.4.44.0307030708330.19222-100000@gannet.stats>

On Wed, 2 Jul 2003, Dutky, Steve wrote:

> Is there a format that yields fractional seconds from format.POSIXct and/or
> related methods?

No, by definition of the class "POSIXct".  See my article on this in an 
early R Newsletter.

> I'm attempting to use irts with millisecond events.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Jul  3 11:18:22 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Jul 2003 11:18:22 +0200
Subject: [R] Batch files in R
In-Reply-To: <CDA8D2689259E444942B3CDED8DD912933FECF@MBXSRV03.stf.nus.edu.sg>
References: <CDA8D2689259E444942B3CDED8DD912933FECF@MBXSRV03.stf.nus.edu.sg>
Message-ID: <16131.62686.523339.317702@gargle.gargle.HOWL>

>>>>> "Adaikalavan" == Adaikalavan Ramasamy <gisar at nus.edu.sg>
>>>>>     on Thu, 3 Jul 2003 11:32:21 +0800 writes:

    Adaikalavan> You can use save(object1, file="lalala.1"),
    Adaikalavan> save(object1, file="lalala.2"), ... and the use
    Adaikalavan> load() to restore the object1 and object2. Or
    Adaikalavan> if you have many objects in a simulation to
    Adaikalavan> save, you can save all objects using
    Adaikalavan> save.image("sim1.result.R").

yes, but use more sensible filename endings, definitely not '.R'.
*The* recommended ending for such files really is '.rda',
see also help(data).

    Adaikalavan> Another option is to use write.table or zz <-
    Adaikalavan> file("lalal1.txt"); cat( ... , file=zz) etc if
    Adaikalavan> you want a human-readable form of your output.

    Adaikalavan> It would be a good idea to use the
    Adaikalavan> compress=TRUE option in save() and R --no-save.

definitely two good ideas.
I think the only reason  compress=TRUE  hasn't becomde the
default behavior is the fact that it wasn't available on all
platforms (at some time point at least), 
see  capabilities("libz")  and  ?capabilities

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From hennil at statisticon.se  Thu Jul  3 11:33:38 2003
From: hennil at statisticon.se (Henric Nilsson)
Date: Thu, 03 Jul 2003 09:33:38 +0000
Subject: [R] How to use quasibinomial?
Message-ID: <200307030933.h639XPUu002479@stat.math.ethz.ch>

Dear all,

I've got some questions, probably due to misunderstandings on my behalf, related
to fitting overdispersed binomial data using glm().

1. I can't seem to get the correct p-values from anova.glm() for the F-tests when
supplying the dispersion argument and having fitted the model using
family=quasibinomial. Actually the p-values for the F-tests seems identical to the
p-values for the Chi-squared tests. When not supplying the dispersion argument,
i.e. when anova.glm() uses the default scaled Pearson statistic from
family=quasibinomial, both tests returns the p-values I'd expect. What am I doing
wrong here and how can I make it work?

> fit.1<-glm(y/n~host*variety,family=quasibinomial,weights=n)
> dscale<-sum(residuals(fit.1,type="deviance")^2/fit.1$df.residual)
> dscale
[1] 1.957517

> anova(fit.1,test="F",dispersion=dscale)
Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: y/n
Terms added sequentially (first to last)
             Df Deviance Resid. Df Resid. Dev       F    Pr(>F)
NULL                            20     98.719
host          1   55.969        19     42.751 28.5916 8.937e-08
variety       1    3.065        18     39.686  1.5657    0.2108
host:variety  1    6.408        17     33.278  3.2736    0.0704

I expected:
> 1-pf(3.2736,1,17)
[1] 0.08812074

> anova(fit.1,test="Chisq",dispersion=dscale)
Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: y/n
Terms added sequentially (first to last)
             Df Deviance Resid. Df Resid. Dev P(>|Chi|)
NULL                            20     98.719
host          1   55.969        19     42.751 8.937e-08
variety       1    3.065        18     39.686     0.211
host:variety  1    6.408        17     33.278     0.070

As expected:
> 1-pchisq(6.408/dscale,1)
[1] 0.07040576

2. When using summary.glm() on a glm object fitted using family=quasibinomial the
reported tests are t-tests. Why?

Thanks,
Henric



From daniele.medri at libero.it  Thu Jul  3 11:53:06 2003
From: daniele.medri at libero.it (Daniele Medri)
Date: Thu, 3 Jul 2003 11:53:06 +0200
Subject: [R] AID and CHAID: news about?
Message-ID: <200307031153.07831.daniele.medri@libero.it>

Dear R-helpers,
I just search if is there something for AID and CHAID in R: one message in 
this list (last year) without success and no possibility.

News about? If "no" let's me know because some friends are looking for thesis 
and this could be a usefull job.

bye
-- 
Daniele Medri



From ripley at stats.ox.ac.uk  Thu Jul  3 13:02:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 12:02:42 +0100 (BST)
Subject: [R] AID and CHAID: news about?
In-Reply-To: <200307031153.07831.daniele.medri@libero.it>
Message-ID: <Pine.LNX.4.44.0307031148390.1197-100000@gannet.stats>

AID and CHAID are (as I understand the terms, e.g. Kass, 1980, Applied
Statistics)) more than 20 years old procedures with known pitfalls.  They
appeared to have been completely eclipsed by other techniques (e.g.
rpart).

On Thu, 3 Jul 2003, Daniele Medri wrote:

> I just search if is there something for AID and CHAID in R: one message in 
> this list (last year) without success and no possibility.
> 
> News about? If "no" let's me know because some friends are looking for thesis 
> and this could be a usefull job.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Thu Jul  3 13:26:35 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 3 Jul 2003 12:26:35 +0100 
Subject: [R] General X11 Problems
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0083F@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

You may remember a while ago I was having problems with creating jpegs using R over CGI.  I solved this on my LAPTOP (*grins*) by 

- executing "xhost +localhost" from the command line 
- putting "SetEnv DISPLAY=:0.0" in my httpd.conf (apache)

And all worked well.  Now I'm trying to move the whole system onto my shiny new server - and encountering yet more problems with X11() (why, god why???).

Quick rundown - SUSE Linux 8.1, R version 1.7.1, Apache 1.3.27.

>From the COMMAND LINE, as the user my webserver runs as, I get:

>jpeg(file="blah",height=480,width=480)
1633: X11 connection rejected because of wrong authentication.
X connection to localhost:12.0 broken (explicit kill or server shutdown)

>From the COMMAND LINE as root, it works fine :-D

Over CGI, running as webmstr and with SetEnv DISPLAY=:0.0, I get:

Xlib: connection to ":0.0" refused by server
Xlib: No protocol specified

Error in X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  : 
	unable to start device JPEG
In addition: Warning message: 
unable to open connection to X11 display`' 
Execution halted

Over CGI, running as webmstr and with SetEnv DISPLAY=:12.0, I get:

Error in X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  : 
	unable to start device JPEG
In addition: Warning message: 
unable to open connection to X11 display`' 
Execution halted

Over CGI, running as webmstr and with no SetEnv command, I get:

Error in X11(paste("jpeg::", quality, ":", filename, sep = ""), width,  : 
	unable to start device JPEG
In addition: Warning message: 
unable to open connection to X11 display`' 
Execution halted


Ahem... so I think the take home message is that I haven't a clue how X11 works or hy it is so awkward.  NOTE I HAVE THIS ALL WORKING ON MY LAPTOP which is an identical setup.  I think what is telling is that the user webmstr can't open X11 displays from R either through the command line or via CGI

Help?

Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7764 490236
E-mail: michael.watson at bbsrc.ac.uk



From michael.watson at bbsrc.ac.uk  Thu Jul  3 15:02:19 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 3 Jul 2003 14:02:19 +0100 
Subject: [R] Generating a vector for breaks in a histogram
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00843@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

I have two lots of numbers which I would like to histogram using the hist() function.  For comparative reasons, I want them to be on the same scale, which I can use the xlim and ylim options to achieve.

However, having them on the same scale is meaningless unless they have the same "breaks".  Consulting the documentation, there are 4 ways of defining the number of breaks, only one of which is "definite", the others merely form suggestions, which I have found is not good enough.

The only definite way is to provide a vector to the hist() function which is a vector of the break points for the histogram.  So I need to generate a vector that contains say, 500, numbers in it, equi-distance apart between a min and a max.  EG:

> myfunc(n=20,min=1,max=20) 

would provide a vector, length 20, with the numbers 1 through 20 in it.

Is there a function in R that can do this?

Thanks
Mick



From th50 at leicester.ac.uk  Thu Jul  3 15:16:16 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 3 Jul 2003 14:16:16 +0100
Subject: [R] Generating a vector for breaks in a histogram
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B670@saffron.cfs.le.ac.uk>

Dear Mick,

Have a look at ?seq - seq(1,20,length=20) should do it.

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> Sent: 03 July 2003 14:02
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Generating a vector for breaks in a histogram
> 
> 
> Hi
> 
> I have two lots of numbers which I would like to histogram 
> using the hist() function.  For comparative reasons, I want 
> them to be on the same scale, which I can use the xlim and 
> ylim options to achieve.
> 
> However, having them on the same scale is meaningless unless 
> they have the same "breaks".  Consulting the documentation, 
> there are 4 ways of defining the number of breaks, only one 
> of which is "definite", the others merely form suggestions, 
> which I have found is not good enough.
> 
> The only definite way is to provide a vector to the hist() 
> function which is a vector of the break points for the 
> histogram.  So I need to generate a vector that contains say, 
> 500, numbers in it, equi-distance apart between a min and a max.  EG:
> 
> > myfunc(n=20,min=1,max=20) 
> 
> would provide a vector, length 20, with the numbers 1 through 
> 20 in it.
> 
> Is there a function in R that can do this?
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From edd at debian.org  Thu Jul  3 15:44:36 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 3 Jul 2003 08:44:36 -0500
Subject: [R] General X11 Problems
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C0083F@cl-exsrv1.irad.bbsrc.ac.uk>
References: <20B7EB075F2D4542AFFAF813E98ACD9301C0083F@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <20030703134436.GA31903@sonny.eddelbuettel.com>


Michael,

To create (most) chart types in batch mode or on a headless terminal, you
need to set up a 'virtual' X11 server.

This has become easier using the additional XFree86 'virtual framebuffer
server'. The package is called Xfvb, and your Linux distro will most likely
have it. You can also get it for Solaris and the other commercial unices.
With a bit of luck and google-ing, you should find some examples and howtos.

People who know more about security than I do advise against modifying xhost
settings at almost all costs.

Hth, Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From michael.watson at bbsrc.ac.uk  Thu Jul  3 15:48:04 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 3 Jul 2003 14:48:04 +0100 
Subject: [R] Generating a vector for breaks in a histogram
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00845@cl-exsrv1.irad.bbsrc.ac.uk>

Fantastic.  You're right, I was looking for seq().

However, my plan for using it for hist() was foiled!  

I thought if I did something like:

> b <- seq(0,500,10)
> hist(myvble,breaks=b)

It would bin myvble into the bins 0-50,50-100,100-150 etc and in that way I could ensure that two histograms are on the same scale with the same bins!

I get the following error:

Error in hist.default(Cy5, breaks = s) : some 'x' not counted; maybe 'breaks' do not span range of 'x'

Now this makes sense of course, my bins probably DON'T span the entire range of X.  SOOOOO I am still left with the same problem:

1) two variables
2) I want to draw histograms of both
3) I want them to have the SAME x-y scale on the graph
4) I want them to have the SAME bin range

How do i do it?  Any suggestions?

Cheers
Mick

-----Original Message-----
From: Hotz, T. [mailto:th50 at leicester.ac.uk]
Sent: 03 July 2003 14:16
To: michael watson (IAH-C); r-help at stat.math.ethz.ch
Subject: RE: [R] Generating a vector for breaks in a histogram


Dear Mick,

Have a look at ?seq - seq(1,20,length=20) should do it.

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> Sent: 03 July 2003 14:02
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Generating a vector for breaks in a histogram
> 
> 
> Hi
> 
> I have two lots of numbers which I would like to histogram 
> using the hist() function.  For comparative reasons, I want 
> them to be on the same scale, which I can use the xlim and 
> ylim options to achieve.
> 
> However, having them on the same scale is meaningless unless 
> they have the same "breaks".  Consulting the documentation, 
> there are 4 ways of defining the number of breaks, only one 
> of which is "definite", the others merely form suggestions, 
> which I have found is not good enough.
> 
> The only definite way is to provide a vector to the hist() 
> function which is a vector of the break points for the 
> histogram.  So I need to generate a vector that contains say, 
> 500, numbers in it, equi-distance apart between a min and a max.  EG:
> 
> > myfunc(n=20,min=1,max=20) 
> 
> would provide a vector, length 20, with the numbers 1 through 
> 20 in it.
> 
> Is there a function in R that can do this?
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Thu Jul  3 16:00:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 15:00:07 +0100 (BST)
Subject: [R] Generating a vector for breaks in a histogram
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00845@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0307031458000.1887-100000@gannet.stats>

On Thu, 3 Jul 2003, michael watson (IAH-C) wrote:

> Fantastic.  You're right, I was looking for seq().
> 
> However, my plan for using it for hist() was foiled!  
> 
> I thought if I did something like:
> 
> > b <- seq(0,500,10)
> > hist(myvble,breaks=b)
> 
> It would bin myvble into the bins 0-50,50-100,100-150 etc and in that way I could ensure that two histograms are on the same scale with the same bins!
> 
> I get the following error:
> 
> Error in hist.default(Cy5, breaks = s) : some 'x' not counted; maybe 'breaks' do not span range of 'x'
> 
> Now this makes sense of course, my bins probably DON'T span the entire
> range of X.  SOOOOO I am still left with the same problem:
> 
> 1) two variables
> 2) I want to draw histograms of both
> 3) I want them to have the SAME x-y scale on the graph
> 4) I want them to have the SAME bin range
> 
> How do i do it?  Any suggestions?

SOOOOO choose a range of breaks to span the whole range of x!

Hint: hist(c(x1,  x2)) knows how to do it, and may even provide you a
suitable vector of breaks in its return value.

> Cheers
> Mick
> 
> -----Original Message-----
> From: Hotz, T. [mailto:th50 at leicester.ac.uk]
> Sent: 03 July 2003 14:16
> To: michael watson (IAH-C); r-help at stat.math.ethz.ch
> Subject: RE: [R] Generating a vector for breaks in a histogram
> 
> 
> Dear Mick,
> 
> Have a look at ?seq - seq(1,20,length=20) should do it.
> 
> HTH
> 
> Thomas
> 
> ---
> 
> Thomas Hotz
> Research Associate in Medical Statistics
> University of Leicester
> United Kingdom
> 
> Department of Epidemiology and Public Health
> 22-28 Princess Road West
> Leicester
> LE1 6TP
> Tel +44 116 252-5410
> Fax +44 116 252-5423
> 
> Division of Medicine for the Elderly
> Department of Medicine
> The Glenfield Hospital
> Leicester
> LE3 9QP
> Tel +44 116 256-3643
> Fax +44 116 232-2976
> 
> 
> > -----Original Message-----
> > From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> > Sent: 03 July 2003 14:02
> > To: 'r-help at stat.math.ethz.ch'
> > Subject: [R] Generating a vector for breaks in a histogram
> > 
> > 
> > Hi
> > 
> > I have two lots of numbers which I would like to histogram 
> > using the hist() function.  For comparative reasons, I want 
> > them to be on the same scale, which I can use the xlim and 
> > ylim options to achieve.
> > 
> > However, having them on the same scale is meaningless unless 
> > they have the same "breaks".  Consulting the documentation, 
> > there are 4 ways of defining the number of breaks, only one 
> > of which is "definite", the others merely form suggestions, 
> > which I have found is not good enough.
> > 
> > The only definite way is to provide a vector to the hist() 
> > function which is a vector of the break points for the 
> > histogram.  So I need to generate a vector that contains say, 
> > 500, numbers in it, equi-distance apart between a min and a max.  EG:
> > 
> > > myfunc(n=20,min=1,max=20) 
> > 
> > would provide a vector, length 20, with the numbers 1 through 
> > 20 in it.
> > 
> > Is there a function in R that can do this?
> > 
> > Thanks
> > Mick
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Thu Jul  3 16:19:56 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 03 Jul 2003 10:19:56 -0400
Subject: [R] Generating a vector for breaks in a histogram
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00845@cl-exsrv1.irad.bbsrc.ac.uk>
References: <20B7EB075F2D4542AFFAF813E98ACD9301C00845@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <l9e8gvsri5kifkol6m2dim50aa2lj9r6kb@4ax.com>

On Thu, 3 Jul 2003 14:48:04 +0100 , "michael watson (IAH-C)"
<michael.watson at bbsrc.ac.uk> wrote :

>Now this makes sense of course, my bins probably DON'T span the entire range of X.  SOOOOO I am still left with the same problem:
>
>1) two variables
>2) I want to draw histograms of both
>3) I want them to have the SAME x-y scale on the graph
>4) I want them to have the SAME bin range
>
>How do i do it?  Any suggestions?

I'd expand the range to cover both (otherwise your histogram will miss
some observations), but if you really want to do what you're asking,
use both xlim and breaks.  For example:

> x <- rnorm(100)

This gives the error you saw, because there are some negative values:

> hist(x, breaks=seq(0,5, len=6))
Error in hist.default(x, breaks = seq(0, 5, len = 6)) : 
        some `x' not counted; maybe `breaks' do not span range of `x'

But this works:

> hist(x, breaks=seq(-5,5, len=11),xlim=c(0,5))

It only shows the bins that are between 0 and 5, and a bit of the one
from -1 to 0.  If you don't even want that bit, then you could do a
histogram of a subset of your data:

> hist(x[x>0], breaks=seq(0,5,len=6))

You'll probably want to use ylim to force the vertical scales to match
between plots.

If none of these work, you should always be able construct the plot
you want using barplot(), where you do all the calculations for
positioning yourself.

Duncan Murdoch



From kwright at eskimo.com  Thu Jul  3 16:45:45 2003
From: kwright at eskimo.com (Kevin Wright)
Date: Thu, 3 Jul 2003 07:45:45 -0700 (PDT)
Subject: [R] Looking for graphics/par cheat sheet
Message-ID: <200307031445.HAA03387@eskimo.com>


I've searched for a while and have not been able to find a succinct, one-page
guide to the graphics parameters that control the layout of plots.  I was
thinking about creating one, but thought I would see if this has already
been done.  

I'm thinking more about a visual guide to the pare parameters than a text
description.

Kevin Wright



From jmagalhaes at oninetspeed.pt  Thu Jul  3 17:08:53 2003
From: jmagalhaes at oninetspeed.pt (Jorge =?iso-8859-15?q?Magalh=E3es?=)
Date: Thu, 3 Jul 2003 16:08:53 +0100
Subject: [R] Some max value in three parallel plots
Message-ID: <200307031608.53575.jmagalhaes@oninetspeed.pt>

Dear Sirs,

I need to use the same max value in three different parallel plots (lattice 
package). How i can do it?

Thanks in the advance,

Jorge M.



From dominik.grathwohl at rdls.nestle.com  Thu Jul  3 17:12:16 2003
From: dominik.grathwohl at rdls.nestle.com (Grathwohl,Dominik,LAUSANNE,NRC/BAS)
Date: Thu, 3 Jul 2003 17:12:16 +0200 
Subject: [R] Bug in plotting groupedData-objects
Message-ID: <89466355CEFE7244AC3A013E45641C1801C8164F@lsmail2.crn.nestrd.ch>

Dear Experts,

May be the problem is still solved, however I tried to find the answer in
the archives:
I use:
> R.version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R              


# a small program to show you the problem:

library(NLME)

n <- 300

id <- gl(n, 3, 3*n)

visit <- rep(c(1:3), n)

y <- rnorm(3*n)

weight <- data.frame(cbind(id, visit, y))

d <- groupedData(y ~ visit|id, data=weight, 
labels=list(visit="Week", y="DHA", id="ID"), 
units=list(visit="(weeks)", y="(%)"), order.groups=F)

plot(d) # save by hand: 1.5 MB

if(interactive()) {
	win.metafile(filename="c:/tmp/mist.emf")
	plot(d)
	dev.off()
}

# The file mist.emf is empty, however the plot command before works.


if(interactive()) {
	win.metafile(filename="c:/tmp/mist2.emf")
	plot(visit,y)
	dev.off()
}

# changing object, the file mist2.emf is not empty:

if(interactive()) {
	jpeg(filename="c:/tmp/mist.jpg", width=600, height=600,
pointsize=12, quality=100, bg="white")
	plot(d)
	dev.off()
}

# changing the driver, mist.jpg is empty!
# So the problem seems to be some interaction with groupedData objects and
graphic drivers.
# Any hind is appreciated

# Regards,

# Dominik

Dominik Grathwohl 
Biostatistician 
Nestl? Research Center 
PO Box 44, CH-1000 Lausanne 26 
Phone: + 41 21 785 8034 
Fax: + 41 21 785 8556 
e-mail: dominik.grathwohl at rdls.nestle.com



From ripley at stats.ox.ac.uk  Thu Jul  3 17:15:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 16:15:19 +0100 (BST)
Subject: [R] Looking for graphics/par cheat sheet
In-Reply-To: <200307031445.HAA03387@eskimo.com>
Message-ID: <Pine.LNX.4.44.0307031609310.2167-100000@gannet.stats>

There is one in MASS4 p.85.  It's a direct descendant from Bill Venables'
`Notes on S'.  `An Introduction to R' is another descendant, and that has
two less comprehensive figures, in sections 12.5.3/.4 in the version I 
just consulted.

On Thu, 3 Jul 2003, Kevin Wright wrote:

> I've searched for a while and have not been able to find a succinct, one-page
> guide to the graphics parameters that control the layout of plots.  I was
> thinking about creating one, but thought I would see if this has already
> been done.  
> 
> I'm thinking more about a visual guide to the pare parameters than a text
> description.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jul  3 17:25:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 16:25:34 +0100 (BST)
Subject: [R] Bug in plotting groupedData-objects
In-Reply-To: <89466355CEFE7244AC3A013E45641C1801C8164F@lsmail2.crn.nestrd.ch>
Message-ID: <Pine.LNX.4.44.0307031616090.2167-100000@gannet.stats>

Aren't these lattice plots that you have not print()-ed?  Auto-printing 
does not work except at top-level, and if(interactive() { ... } is not 
top-level.

Using print(plot(d)) works for me.


On Thu, 3 Jul 2003, Grathwohl,Dominik,LAUSANNE,NRC/BAS wrote:

> Dear Experts,
> 
> May be the problem is still solved, however I tried to find the answer in
> the archives:
> I use:
> > R.version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    7.1            
> year     2003           
> month    06             
> day      16             
> language R              
> 
> 
> # a small program to show you the problem:
> 
> library(NLME)

It's nlme, not NLME

> n <- 300
> 
> id <- gl(n, 3, 3*n)
> 
> visit <- rep(c(1:3), n)
> 
> y <- rnorm(3*n)
> 
> weight <- data.frame(cbind(id, visit, y))
> 
> d <- groupedData(y ~ visit|id, data=weight, 
> labels=list(visit="Week", y="DHA", id="ID"), 
> units=list(visit="(weeks)", y="(%)"), order.groups=F)
> 
> plot(d) # save by hand: 1.5 MB
> 
> if(interactive()) {
> 	win.metafile(filename="c:/tmp/mist.emf")
> 	plot(d)
> 	dev.off()
> }
> 
> # The file mist.emf is empty, however the plot command before works.
> 
> 
> if(interactive()) {
> 	win.metafile(filename="c:/tmp/mist2.emf")
> 	plot(visit,y)
> 	dev.off()
> }
> 
> # changing object, the file mist2.emf is not empty:
> 
> if(interactive()) {
> 	jpeg(filename="c:/tmp/mist.jpg", width=600, height=600,
> pointsize=12, quality=100, bg="white")
> 	plot(d)
> 	dev.off()
> }
> 
> # changing the driver, mist.jpg is empty!
> # So the problem seems to be some interaction with groupedData objects and
> graphic drivers.
> # Any hind is appreciated
> 
> # Regards,
> 
> # Dominik
> 
> Dominik Grathwohl 
> Biostatistician 
> Nestl? Research Center 
> PO Box 44, CH-1000 Lausanne 26 
> Phone: + 41 21 785 8034 
> Fax: + 41 21 785 8556 
> e-mail: dominik.grathwohl at rdls.nestle.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dominik.grathwohl at rdls.nestle.com  Thu Jul  3 17:32:56 2003
From: dominik.grathwohl at rdls.nestle.com (Grathwohl,Dominik,LAUSANNE,NRC/BAS)
Date: Thu, 3 Jul 2003 17:32:56 +0200 
Subject: [R] Bug in plotting groupedData-objects
Message-ID: <89466355CEFE7244AC3A013E45641C1801C81650@lsmail2.crn.nestrd.ch>

Thank you very much, problem solved!
For me it's a bit strange, because I worked several years in this way
and suddenly it did not work any more.
What are the appropriate top-level commands?

Kind regards,

Dominik

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: jeudi, 3. juillet 2003 17:26
> To: Grathwohl,Dominik,LAUSANNE,NRC/BAS
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Bug in plotting groupedData-objects
> 
> 
> Aren't these lattice plots that you have not print()-ed?  
> Auto-printing 
> does not work except at top-level, and if(interactive() { ... 
> } is not 
> top-level.
> 
> Using print(plot(d)) works for me.
> 
> 
> On Thu, 3 Jul 2003, Grathwohl,Dominik,LAUSANNE,NRC/BAS wrote:
> 
> > Dear Experts,
> > 
> > May be the problem is still solved, however I tried to find 
> the answer in
> > the archives:
> > I use:
> > > R.version
> >          _              
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status                  
> > major    1              
> > minor    7.1            
> > year     2003           
> > month    06             
> > day      16             
> > language R              
> > 
> > 
> > # a small program to show you the problem:
> > 
> > library(NLME)
> 
> It's nlme, not NLME
> 
> > n <- 300
> > 
> > id <- gl(n, 3, 3*n)
> > 
> > visit <- rep(c(1:3), n)
> > 
> > y <- rnorm(3*n)
> > 
> > weight <- data.frame(cbind(id, visit, y))
> > 
> > d <- groupedData(y ~ visit|id, data=weight, 
> > labels=list(visit="Week", y="DHA", id="ID"), 
> > units=list(visit="(weeks)", y="(%)"), order.groups=F)
> > 
> > plot(d) # save by hand: 1.5 MB
> > 
> > if(interactive()) {
> > 	win.metafile(filename="c:/tmp/mist.emf")
> > 	plot(d)
> > 	dev.off()
> > }
> > 
> > # The file mist.emf is empty, however the plot command before works.
> > 
> > 
> > if(interactive()) {
> > 	win.metafile(filename="c:/tmp/mist2.emf")
> > 	plot(visit,y)
> > 	dev.off()
> > }
> > 
> > # changing object, the file mist2.emf is not empty:
> > 
> > if(interactive()) {
> > 	jpeg(filename="c:/tmp/mist.jpg", width=600, height=600,
> > pointsize=12, quality=100, bg="white")
> > 	plot(d)
> > 	dev.off()
> > }
> > 
> > # changing the driver, mist.jpg is empty!
> > # So the problem seems to be some interaction with 
> groupedData objects and
> > graphic drivers.
> > # Any hind is appreciated
> > 
> > # Regards,
> > 
> > # Dominik
> > 
> > Dominik Grathwohl 
> > Biostatistician 
> > Nestl? Research Center 
> > PO Box 44, CH-1000 Lausanne 26 
> > Phone: + 41 21 785 8034 
> > Fax: + 41 21 785 8556 
> > e-mail: dominik.grathwohl at rdls.nestle.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Thu Jul  3 17:38:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 16:38:55 +0100 (BST)
Subject: [R] Bug in plotting groupedData-objects
In-Reply-To: <89466355CEFE7244AC3A013E45641C1801C81650@lsmail2.crn.nestrd.ch>
Message-ID: <Pine.LNX.4.44.0307031635090.2220-100000@gannet.stats>

On Thu, 3 Jul 2003, Grathwohl,Dominik,LAUSANNE,NRC/BAS wrote:

> Thank you very much, problem solved!
> For me it's a bit strange, because I worked several years in this way
> and suddenly it did not work any more.

I don't think those plots have worked in R for more than a couple of years
(definitely not several), and things do not suddenly change without user
intervention.

> What are the appropriate top-level commands?

I don't understand: however top-level is when the > prompt appears.

> 
> Kind regards,
> 
> Dominik
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: jeudi, 3. juillet 2003 17:26
> > To: Grathwohl,Dominik,LAUSANNE,NRC/BAS
> > Cc: 'r-help at stat.math.ethz.ch'
> > Subject: Re: [R] Bug in plotting groupedData-objects
> > 
> > 
> > Aren't these lattice plots that you have not print()-ed?  
> > Auto-printing 
> > does not work except at top-level, and if(interactive() { ... 
> > } is not 
> > top-level.
> > 
> > Using print(plot(d)) works for me.
> > 
> > 
> > On Thu, 3 Jul 2003, Grathwohl,Dominik,LAUSANNE,NRC/BAS wrote:
> > 
> > > Dear Experts,
> > > 
> > > May be the problem is still solved, however I tried to find 
> > the answer in
> > > the archives:
> > > I use:
> > > > R.version
> > >          _              
> > > platform i386-pc-mingw32
> > > arch     i386           
> > > os       mingw32        
> > > system   i386, mingw32  
> > > status                  
> > > major    1              
> > > minor    7.1            
> > > year     2003           
> > > month    06             
> > > day      16             
> > > language R              
> > > 
> > > 
> > > # a small program to show you the problem:
> > > 
> > > library(NLME)
> > 
> > It's nlme, not NLME
> > 
> > > n <- 300
> > > 
> > > id <- gl(n, 3, 3*n)
> > > 
> > > visit <- rep(c(1:3), n)
> > > 
> > > y <- rnorm(3*n)
> > > 
> > > weight <- data.frame(cbind(id, visit, y))
> > > 
> > > d <- groupedData(y ~ visit|id, data=weight, 
> > > labels=list(visit="Week", y="DHA", id="ID"), 
> > > units=list(visit="(weeks)", y="(%)"), order.groups=F)
> > > 
> > > plot(d) # save by hand: 1.5 MB
> > > 
> > > if(interactive()) {
> > > 	win.metafile(filename="c:/tmp/mist.emf")
> > > 	plot(d)
> > > 	dev.off()
> > > }
> > > 
> > > # The file mist.emf is empty, however the plot command before works.
> > > 
> > > 
> > > if(interactive()) {
> > > 	win.metafile(filename="c:/tmp/mist2.emf")
> > > 	plot(visit,y)
> > > 	dev.off()
> > > }
> > > 
> > > # changing object, the file mist2.emf is not empty:
> > > 
> > > if(interactive()) {
> > > 	jpeg(filename="c:/tmp/mist.jpg", width=600, height=600,
> > > pointsize=12, quality=100, bg="white")
> > > 	plot(d)
> > > 	dev.off()
> > > }
> > > 
> > > # changing the driver, mist.jpg is empty!
> > > # So the problem seems to be some interaction with 
> > groupedData objects and
> > > graphic drivers.
> > > # Any hind is appreciated
> > > 
> > > # Regards,
> > > 
> > > # Dominik
> > > 
> > > Dominik Grathwohl 
> > > Biostatistician 
> > > Nestl? Research Center 
> > > PO Box 44, CH-1000 Lausanne 26 
> > > Phone: + 41 21 785 8034 
> > > Fax: + 41 21 785 8556 
> > > e-mail: dominik.grathwohl at rdls.nestle.com
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ernesto at ipimar.pt  Thu Jul  3 18:37:41 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 03 Jul 2003 16:37:41 -0000
Subject: [R] unlist
Message-ID: <1057250255.2948.26.camel@gandalf.ipimar.pt>

Hi

I have a list with several data.frames, all with the same number of
colunms but different number of rows, and I'd like to transform this
list into a single dataframe. I need to mimic an rbind of all dataframes
...

Transform doesn't seem to work :-(

Thanks

EJ

-- 
Ernesto Jardim <ernesto at ipimar.pt>
Bi?logo Marinho/Marine Biologist
IPIMAR - Instituto Nacional de Investiga??o Agr?ria e das Pescas
IPIMAR - National Research Institute for Agriculture and Fisheries
Av. Brasilia, 1400-006
Lisboa, Portugal
Tel: +351 213 027 000
Fax: +351 213 015 948
http://ernesto.freezope.org



From p.dalgaard at biostat.ku.dk  Thu Jul  3 18:55:35 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 03 Jul 2003 16:55:35 -0000
Subject: [R] unlist
In-Reply-To: <1057250255.2948.26.camel@gandalf.ipimar.pt>
References: <1057250255.2948.26.camel@gandalf.ipimar.pt>
Message-ID: <x21xx7zgy4.fsf@biostat.ku.dk>

Ernesto Jardim <ernesto at ipimar.pt> writes:

> Hi
> 
> I have a list with several data.frames, all with the same number of
> colunms but different number of rows, and I'd like to transform this
> list into a single dataframe. I need to mimic an rbind of all dataframes
> ...
> 
> Transform doesn't seem to work :-(

No, why should it? It's designed to add transformed variables to a
single dataframe.

I think what you're looking for is simply

  do.call("rbind",list.of.frames)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ernesto at ipimar.pt  Thu Jul  3 19:03:25 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 03 Jul 2003 17:03:25 -0000
Subject: [R] unlist
In-Reply-To: <x21xx7zgy4.fsf@biostat.ku.dk>
References: <1057250255.2948.26.camel@gandalf.ipimar.pt>
	<x21xx7zgy4.fsf@biostat.ku.dk>
Message-ID: <1057251796.2975.28.camel@gandalf.ipimar.pt>

On Thu, 2003-07-03 at 17:57, Peter Dalgaard BSA wrote:
> Ernesto Jardim <ernesto at ipimar.pt> writes:
> 
> > Hi
> > 
> > I have a list with several data.frames, all with the same number of
> > colunms but different number of rows, and I'd like to transform this
> > list into a single dataframe. I need to mimic an rbind of all dataframes
> > ...
> > 
> > Transform doesn't seem to work :-(
> 
> No, why should it? It's designed to add transformed variables to a
> single dataframe.
> 
> I think what you're looking for is simply
> 
>   do.call("rbind",list.of.frames)

Hi

It worked, thanks for your help.

EJ
-- 
Ernesto Jardim <ernesto at ipimar.pt>
Bi?logo Marinho/Marine Biologist
IPIMAR - Instituto Nacional de Investiga??o Agr?ria e das Pescas
IPIMAR - National Research Institute for Agriculture and Fisheries
Av. Brasilia, 1400-006
Lisboa, Portugal
Tel: +351 213 027 000
Fax: +351 213 015 948
http://ernesto.freezope.org



From rvaradha at jhsph.edu  Thu Jul  3 19:09:02 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 03 Jul 2003 13:09:02 -0400
Subject: [R] SVD and spectral decompositions of a hermitian matrix
Message-ID: <13f152613ed13b.13ed13b13f1526@jhsph.edu>

Hi:

I create a hermitian matrix and then perform its singular value 
decomposition. But when I put it back, I don't get the original 
hermitian matrix.  I am having the same problem with spectral value 
decomposition as well.

I am using R 1.7.0 on Windows.  Here is my code:

X <- matrix(rnorm(16)+1i*rnorm(16),4)
X <- X + t(X)
X[upper.tri(X)] <- Conj(X[upper.tri(X)])
Y <- La.svd(X)
Y$u %*% diag(Y$d) %*% t(Y$v)   # this doesn't give back X
Y$u %*% diag(Y$d) %*% Y$v      # this works fine.
Z <- La.eigen(X)   # the eigen values should be real, but are not.
Z$vec %*% diag(Z$val) %*% t(Z$vec)   # this doesn't give back X

The help for "La.svd" says that the function return U, D, and V such 
that X = U D V'  Furthermore, the help for "La.eigen" says that if the 
argument "symmetric" is not specified, the matrix is inspected for 
symmetry, so I expect that I should get real eigen values to a 
hermitian matrix. Are there any problems with these 2 functions, or 
what is it that I am not understanding? 

thanks for your help,
Ravi.



From ripley at stats.ox.ac.uk  Thu Jul  3 20:21:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 19:21:27 +0100 (BST)
Subject: [R] SVD and spectral decompositions of a hermitian matrix
In-Reply-To: <13f152613ed13b.13ed13b13f1526@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0307031858150.2403-100000@gannet.stats>

On Thu, 3 Jul 2003, Ravi Varadhan wrote:

> I create a hermitian matrix 

You didn't succeed, if you meant Hermitian.

> and then perform its singular value 
> decomposition. But when I put it back, I don't get the original 
> hermitian matrix.  I am having the same problem with spectral value 
> decomposition as well.
> 
> I am using R 1.7.0 on Windows.  Here is my code:
> 
> X <- matrix(rnorm(16)+1i*rnorm(16),4)
> X <- X + t(X)
> X[upper.tri(X)] <- Conj(X[upper.tri(X)])

and I get 

> X - Conj(t(X))
            [,1]        [,2]        [,3]        [,4]
[1,] 0-7.044789i 0+0.000000i 0+0.000000i 0+0.000000i
[2,] 0+0.000000i 0+4.255175i 0+0.000000i 0+0.000000i
[3,] 0+0.000000i 0+0.000000i 0+6.163605i 0+0.000000i
[4,] 0+0.000000i 0+0.000000i 0+0.000000i 0+3.021553i

so X is not Hermitian.

> Y <- La.svd(X)
> Y$u %*% diag(Y$d) %*% t(Y$v)   # this doesn't give back X

The result has component vt, not v: you can't read the help page!

> Y$u %*% diag(Y$d) %*% Y$v      # this works fine.
but is really matching Y$u %*% diag(Y$d) %*% Y$vt

> Z <- La.eigen(X)   # the eigen values should be real, but are not.

The matrix is not Hermitian.

> Z$vec %*% diag(Z$val) %*% t(Z$vec)   # this doesn't give back X

Nor should it: for a Hermitian matrix try

Z$vec %*% diag(Z$val) %*% Conj(t(Z$vec))

> The help for "La.svd" says that the function return U, D, and V such 
> that X = U D V' 

It doesn't: please work on improving your reading skills.

> Furthermore, the help for "La.eigen" says that if the 
> argument "symmetric" is not specified, the matrix is inspected for 
> symmetry, so I expect that I should get real eigen values to a 
> hermitian matrix. 

Yes, so check your matrix!

> Are there any problems with these 2 functions, or 
> what is it that I am not understanding? 

There is now no real point in using La.svd() and La.eigen() rather than 
svd() and eigen().


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jul  3 20:24:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 19:24:00 +0100 (BST)
Subject: [R] unlist
In-Reply-To: <1057250255.2948.26.camel@gandalf.ipimar.pt>
Message-ID: <Pine.LNX.4.44.0307031922320.2403-100000@gannet.stats>

On 3 Jul 2003, Ernesto Jardim wrote:

> I have a list with several data.frames, all with the same number of
> colunms but different number of rows, and I'd like to transform this
> list into a single dataframe. I need to mimic an rbind of all dataframes

As in do.call("rbind", list.of.data.frames)?

> Transform doesn't seem to work :-(

for this job? or in general?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Carlisle.Thacker at noaa.gov  Thu Jul  3 20:24:27 2003
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Thu, 03 Jul 2003 14:24:27 -0400
Subject: [R] Error using Tsp
Message-ID: <3F0474DB.65CB3396@noaa.gov>

Hello all,

My first try with the fields library's Tsp function, trying to imitate 
the ozone example with my data, has resulted in the following error 
message:

> fit <- Tps(as.matrix(all.60[,5:6]),as.vector(all.60[,3]))
Error in if (den < 0) { : missing value where logical needed
In addition: Warning messages: 
1: GCV search gives a minumum at the endpoints of the grid search in: 

Krig.find.gcvmin(info, lambda.grid, gcv.grid$GCV.one, Krig.fgcv.one,  
2: no finite arguments to max; returning -Inf 

What does the error message mean?  What am I doing wrong?

Thanks,

Carlisle

PS:  all.60 is a dataframe containing oceanographic data from 60m
depth. 
 Columns 5 and 6 are longitude and latitude and column 3 is
temperature.

PPS:  I'm using Windows XP
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    6.1            
year     2002           
month    11             
day      01             
language R



From fharrell at virginia.edu  Thu Jul  3 21:59:57 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 3 Jul 2003 15:59:57 -0400
Subject: [R] Help files mismatches and R CMD check
Message-ID: <20030703155957.7c952d04.fharrell@virginia.edu>

I have a function t.test.cluster for which R CMD check results in a documentation warning because its argument names don't match those of t( ).  Is there a way around this error besides renaming the function?  Will this disqualify a package from admission to CRAN?  How did t.test avoid this warning?

Thanks,

Frank
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From dojoly at wisc.edu  Thu Jul  3 22:06:05 2003
From: dojoly at wisc.edu (Damien Joly)
Date: Thu, 3 Jul 2003 15:06:05 -0500
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
Message-ID: <200307031506.05016.dojoly@wisc.edu>

I getting compilation errors when trying to install GLMMGibbs (see below).  
I'm running R v 1.7.1 on SuSE Linux 8.2.

Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1 combination 
and it worked fine.  Any hints are greatly appreciated.


Thank you in advance,
Damien



>install.packages("GLMMGibbs")
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 111448 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
........
downloaded 108Kb

trying URL `http://cran.r-project.org/src/contrib/GLMMGibbs_0.5-1.tar.gz'
Content type `application/x-tar' length 346260 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... ........
downloaded 338Kb

* Installing *source* package 'GLMMGibbs' ...
** libs
gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  
-fPIC   -c ars.c -o ars.o
ars.c:497:10: missing terminating " character
ars.c: In function `dump_arse':
ars.c:498: error: parse error before "mylesj"
ars.c:498: error: syntax error at '@' token
ars.c:498: error: stray '\' in program
ars.c:498:33: missing terminating " character
ars.c:507: error: redeclaration of `s'
ars.c:446: error: `s' previously declared here
ars.c:508: error: redeclaration of `i'
ars.c:447: error: `i' previously declared here
make: *** [ars.o] Error 1
ERROR: compilation failed for package 'GLMMGibbs'

Delete downloaded files (y/N)?



From p.dalgaard at biostat.ku.dk  Thu Jul  3 22:19:58 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 03 Jul 2003 20:19:58 -0000
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
In-Reply-To: <200307031506.05016.dojoly@wisc.edu>
References: <200307031506.05016.dojoly@wisc.edu>
Message-ID: <x2fzlnbbtv.fsf@biostat.ku.dk>

Damien Joly <dojoly at wisc.edu> writes:

> I getting compilation errors when trying to install GLMMGibbs (see below).  
> I'm running R v 1.7.1 on SuSE Linux 8.2.
> 
> Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1 combination 
> and it worked fine.  Any hints are greatly appreciated.

Which GCC version. There is something with multiline strings being
disallowed now due to increased standard adherence...

> * Installing *source* package 'GLMMGibbs' ...
> ** libs
> gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  
> -fPIC   -c ars.c -o ars.o
> ars.c:497:10: missing terminating " character

...which that error message looks like it could be a symptom of.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Jul  3 22:22:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 21:22:19 +0100 (BST)
Subject: [R] Help files mismatches and R CMD check
In-Reply-To: <20030703155957.7c952d04.fharrell@virginia.edu>
Message-ID: <Pine.LNX.4.44.0307032117340.2803-100000@gannet.stats>

On Thu, 3 Jul 2003, Frank E Harrell Jr wrote:

> I have a function t.test.cluster for which R CMD check results in a
> documentation warning because its argument names don't match those of t(
> ).  Is there a way around this error besides renaming the function?  
> Will this disqualify a package from admission to CRAN?  How did t.test
> avoid this warning?

t.test is a generic, so it avoids the warning. Sounds as if we would need 
either to improve the logic (so it does identify t.test as the appropriate 
generic) or add this to the stop list of reasonable exception (in package 
tools/R/utils.R).  In any case, it's our not your problem.

Brian

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jul  3 22:24:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jul 2003 21:24:49 +0100 (BST)
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux
	8.2 (R v. 1.7.1)
In-Reply-To: <x2fzlnbbtv.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0307032122550.2803-100000@gannet.stats>

It's a known problem with that package and gcc 3.3 (but not the 3.2.3 used 
on Windows nor earlier).  The maintainer has been informed ....

On 3 Jul 2003, Peter Dalgaard BSA wrote:

> Damien Joly <dojoly at wisc.edu> writes:
> 
> > I getting compilation errors when trying to install GLMMGibbs (see below).  
> > I'm running R v 1.7.1 on SuSE Linux 8.2.
> > 
> > Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1 combination 
> > and it worked fine.  Any hints are greatly appreciated.
> 
> Which GCC version. There is something with multiline strings being
> disallowed now due to increased standard adherence...
> 
> > * Installing *source* package 'GLMMGibbs' ...
> > ** libs
> > gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  
> > -fPIC   -c ars.c -o ars.o
> > ars.c:497:10: missing terminating " character
> 
> ...which that error message looks like it could be a symptom of.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dojoly at wisc.edu  Thu Jul  3 22:27:24 2003
From: dojoly at wisc.edu (Damien Joly)
Date: Thu, 3 Jul 2003 15:27:24 -0500
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
In-Reply-To: <x2fzlnbbtv.fsf@biostat.ku.dk>
References: <200307031506.05016.dojoly@wisc.edu> <x2fzlnbbtv.fsf@biostat.ku.dk>
Message-ID: <200307031527.24075.dojoly@wisc.edu>

On Thursday 03 July 2003 3:21 pm, Peter Dalgaard BSA wrote:
> Damien Joly <dojoly at wisc.edu> writes:
> > I getting compilation errors when trying to install GLMMGibbs (see
> > below). I'm running R v 1.7.1 on SuSE Linux 8.2.
> >
> > Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1
> > combination and it worked fine.  Any hints are greatly appreciated.
>
> Which GCC version. There is something with multiline strings being
> disallowed now due to increased standard adherence...
>

GCC version 3.3-23:

toxoplasma:/home/damien/temp # rpm -qa | grep gcc
libgcc-3.3-23
gcc-3.3-23
gcc-c++-3.3-23
gcc-g77-3.3-23
 
That sound right?  Do you recommend I drop back a version or two?  (with the 
associated dependency nightmare!)

  .



From p.dalgaard at biostat.ku.dk  Thu Jul  3 22:45:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 03 Jul 2003 20:45:57 -0000
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
In-Reply-To: <200307031527.24075.dojoly@wisc.edu>
References: <200307031506.05016.dojoly@wisc.edu>
	<x2fzlnbbtv.fsf@biostat.ku.dk> <200307031527.24075.dojoly@wisc.edu>
Message-ID: <x2brwbbamk.fsf@biostat.ku.dk>

Damien Joly <dojoly at wisc.edu> writes:

> On Thursday 03 July 2003 3:21 pm, Peter Dalgaard BSA wrote:
> > Damien Joly <dojoly at wisc.edu> writes:
> > > I getting compilation errors when trying to install GLMMGibbs (see
> > > below). I'm running R v 1.7.1 on SuSE Linux 8.2.
> > >
> > > Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1
> > > combination and it worked fine.  Any hints are greatly appreciated.
> >
> > Which GCC version. There is something with multiline strings being
> > disallowed now due to increased standard adherence...
> >
> 
> GCC version 3.3-23:
> 
> toxoplasma:/home/damien/temp # rpm -qa | grep gcc
> libgcc-3.3-23
> gcc-3.3-23
> gcc-c++-3.3-23
> gcc-g77-3.3-23
>  
> That sound right?  Do you recommend I drop back a version or two?  (with the 
> associated dependency nightmare!)

Just fix the quotes in the sources, I think. (In case you've only
installed with the automated tools before: Fetch the tar file, unpack
it with tar xfz GLMMGibbs_0.5-1.tar.gz, fix the relevant .c file and
finally: R CMD INSTALL GLMMGibbs)


The way to modify the sources seems to be as follows:
(http://lists.suse.com/archive/suse-programming-e/2003-May/0075.html) 

From: Philipp Thomas <philipp.thomas at t-link.de>
Date: Tue, 27 May 2003 01:55:06 +0200
Subject: Re: [suse-programming-e] Gcc 3.3

Graham Murray <graham at gmurray.org.uk> [Mon, 26 May 2003 16:35:44
+0100]:

> 1) Close the quotes at the end of each line as 2 adjacent string
> literals are treated as one.
> 2) Put a backslash at the end of each line to make (lexically) one
> line.

 3) If you want the string to include the newline (that's most of
    the cases that use multiline strings), either put \n\ at the end
    of each line or add \n at the end of each adjacent string literal.

Philipp


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rvaradha at jhsph.edu  Thu Jul  3 22:54:12 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 03 Jul 2003 16:54:12 -0400
Subject: [R] SVD and spectral decompositions of a hermitian matrix
Message-ID: <142c8471428958.1428958142c847@jhsph.edu>


Many thanks to Prof. Ripley for the help.  The problem was that my 
matrix wasn't Hermitian since I didn't ensure that the diagonals were 
real.

Best,
Ravi.  

----- Original Message -----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Date: Thursday, July 3, 2003 2:21 pm
Subject: Re: [R] SVD and spectral decompositions of a hermitian matrix

> On Thu, 3 Jul 2003, Ravi Varadhan wrote:
> 
> > I create a hermitian matrix 
> 
> You didn't succeed, if you meant Hermitian.
> 
> > and then perform its singular value 
> > decomposition. But when I put it back, I don't get the original 
> > hermitian matrix.  I am having the same problem with spectral 
> value 
> > decomposition as well.
> > 
> > I am using R 1.7.0 on Windows.  Here is my code:
> > 
> > X <- matrix(rnorm(16)+1i*rnorm(16),4)
> > X <- X + t(X)
> > X[upper.tri(X)] <- Conj(X[upper.tri(X)])
> 
> and I get 
> 
> > X - Conj(t(X))
>            [,1]        [,2]        [,3]        [,4]
> [1,] 0-7.044789i 0+0.000000i 0+0.000000i 0+0.000000i
> [2,] 0+0.000000i 0+4.255175i 0+0.000000i 0+0.000000i
> [3,] 0+0.000000i 0+0.000000i 0+6.163605i 0+0.000000i
> [4,] 0+0.000000i 0+0.000000i 0+0.000000i 0+3.021553i
> 
> so X is not Hermitian.
> 
> > Y <- La.svd(X)
> > Y$u %*% diag(Y$d) %*% t(Y$v)   # this doesn't give back X
> 
> The result has component vt, not v: you can't read the help page!
> 
> > Y$u %*% diag(Y$d) %*% Y$v      # this works fine.
> but is really matching Y$u %*% diag(Y$d) %*% Y$vt
> 
> > Z <- La.eigen(X)   # the eigen values should be real, but are not.
> 
> The matrix is not Hermitian.
> 
> > Z$vec %*% diag(Z$val) %*% t(Z$vec)   # this doesn't give back X
> 
> Nor should it: for a Hermitian matrix try
> 
> Z$vec %*% diag(Z$val) %*% Conj(t(Z$vec))
> 
> > The help for "La.svd" says that the function return U, D, and V 
> such 
> > that X = U D V' 
> 
> It doesn't: please work on improving your reading skills.
> 
> > Furthermore, the help for "La.eigen" says that if the 
> > argument "symmetric" is not specified, the matrix is inspected 
> for 
> > symmetry, so I expect that I should get real eigen values to a 
> > hermitian matrix. 
> 
> Yes, so check your matrix!
> 
> > Are there any problems with these 2 functions, or 
> > what is it that I am not understanding? 
> 
> There is now no real point in using La.svd() and La.eigen() rather 
> than 
> svd() and eigen().
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dojoly at wisc.edu  Thu Jul  3 23:05:58 2003
From: dojoly at wisc.edu (Damien Joly)
Date: Thu, 3 Jul 2003 16:05:58 -0500
Subject: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
In-Reply-To: <x2brwbbamk.fsf@biostat.ku.dk>
References: <200307031506.05016.dojoly@wisc.edu>
	<200307031527.24075.dojoly@wisc.edu> <x2brwbbamk.fsf@biostat.ku.dk>
Message-ID: <200307031605.58435.dojoly@wisc.edu>

It worked.  I added a "\" to the end of line 497 and it compiled fine.

I'm not a C programmer, but I should have picked that up!  Thanks for the 
help,

Damien

On Thursday 03 July 2003 3:47 pm, Peter Dalgaard BSA wrote:
> Damien Joly <dojoly at wisc.edu> writes:
> > On Thursday 03 July 2003 3:21 pm, Peter Dalgaard BSA wrote:
> > > Damien Joly <dojoly at wisc.edu> writes:
> > > > I getting compilation errors when trying to install GLMMGibbs (see
> > > > below). I'm running R v 1.7.1 on SuSE Linux 8.2.
> > > >
> > > > Has anyone else had this problem?  I tried it on a Win2000/R 1.5.1
> > > > combination and it worked fine.  Any hints are greatly appreciated.
> > >
> > > Which GCC version. There is something with multiline strings being
> > > disallowed now due to increased standard adherence...
> >
> > GCC version 3.3-23:
> >
> > toxoplasma:/home/damien/temp # rpm -qa | grep gcc
> > libgcc-3.3-23
> > gcc-3.3-23
> > gcc-c++-3.3-23
> > gcc-g77-3.3-23
> >
> > That sound right?  Do you recommend I drop back a version or two?  (with
> > the associated dependency nightmare!)
>
> Just fix the quotes in the sources, I think. (In case you've only
> installed with the automated tools before: Fetch the tar file, unpack
> it with tar xfz GLMMGibbs_0.5-1.tar.gz, fix the relevant .c file and
> finally: R CMD INSTALL GLMMGibbs)
>
>
> The way to modify the sources seems to be as follows:
> (http://lists.suse.com/archive/suse-programming-e/2003-May/0075.html)
>
> From: Philipp Thomas <philipp.thomas at t-link.de>
> Date: Tue, 27 May 2003 01:55:06 +0200
> Subject: Re: [suse-programming-e] Gcc 3.3
>
> Graham Murray <graham at gmurray.org.uk> [Mon, 26 May 2003 16:35:44
>
> +0100]:
> > 1) Close the quotes at the end of each line as 2 adjacent string
> > literals are treated as one.
> > 2) Put a backslash at the end of each line to make (lexically) one
> > line.
>
>  3) If you want the string to include the newline (that's most of
>     the cases that use multiline strings), either put \n\ at the end
>     of each line or add \n at the end of each adjacent string literal.
>
> Philipp



From dojoly at wisc.edu  Thu Jul  3 23:09:06 2003
From: dojoly at wisc.edu (Damien Joly)
Date: Thu, 3 Jul 2003 16:09:06 -0500
Subject: Fwd: Re: [R] compilation error when installing GLMMGibbs on SuSE
	Linux 8.2 (R v. 1.7.1)
Message-ID: <200307031609.06263.dojoly@wisc.edu>

>It worked.  I added a "\" to the end of line 497 and it compiled fine.

>I'm not a C programmer, but I should have picked that up!  Thanks for the 
>help,

>Damien

Line 497 of ars.c that is,

Damien
-------------- next part --------------
An embedded message was scrubbed...
From: Damien Joly <dojoly at wisc.edu>
Subject: Re: [R] compilation error when installing GLMMGibbs on SuSE Linux 8.2
	(R v. 1.7.1)
Date: Thu, 3 Jul 2003 16:05:58 -0500
Size: 2944
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030703/92fe18f5/attachment.mht

From Tom.Mulholland at health.wa.gov.au  Fri Jul  4 03:04:55 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Fri, 4 Jul 2003 09:04:55 +0800
Subject: [R] Generating a vector for breaks in a histogram
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FB8@nt207mesep.health.wa.gov.au>

One of my discoveries while learning the art of R, is that time has
moved on since I did my basic statistics in school (although to my
dismay the teaching of statistics in school appears also to have not
noticed the movement.) I have seen a few references when people want to
pie chart something, for the advice to be "find a better way." I've been
reading some of the ash work (see package of same name and loads of
papers on the web), also some interesting work on dot plots as an
alternative to histograms. They make me feel that unless the data that
you have in both histograms accidentally works well with the same set of
bins you may not get the comparative assessment that you think you are
getting.

I am beginning to form the opinion that in most cases (if not all) there
are better alternatives to histograms.
_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
189 Royal St, East Perth, WA, 6004
 
Tel: (08) 9222 4062
e-mail: Tom.Mulholland at health.wa.gov.au
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From silika at access.unizh.ch  Fri Jul  4 08:39:07 2003
From: silika at access.unizh.ch (Silika Tereshchenko)
Date: Fri, 04 Jul 2003 08:39:07 +0200 (CEST)
Subject: [R] Help. Import data
Message-ID: <1057300747.3f05210b2e769@imp.access.unizh.ch>

Hoy,


I have the following problem. I have to import the dataset (survey) from SPSS 
into R-programm. By using the "read.spss" the all of variables have been readed 
as factor. I need the variables "income" and "age" as numeric for the 
regreesion analysis. Could you plese help me.


Thanks a lot,
best regards,
Silika



From ripley at stats.ox.ac.uk  Fri Jul  4 08:43:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 07:43:47 +0100 (BST)
Subject: [R] Generating a vector for breaks in a histogram
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D56FB8@nt207mesep.health.wa.gov.au>
Message-ID: <Pine.LNX.4.44.0307040733240.3588-100000@gannet.stats>

Things have moved on since the ASH work too, but I would agree that
density estimation is often a better way than histograms.  However, close 
to state-of-the-art density estimation is built into R (?density) and
packages `polspline', `KernSmooth' and `sm' are also much more advanced 
than `ash'. 

It was the advent of enough computing power that changed this, and the S 
language has been in the forefront of making the state of the art 
available.  You'll see that MASS (the book) covers histograms and 
alternatives in its chapter on Univariate Distributions, and it has since 
its 1994 first edition (when did you go to `school'?)

One often overlooked alternative is plot ECDFs.

If distributions are not really continuous other techniques may be 
appropriate -- such as dotplots.

On Fri, 4 Jul 2003, Mulholland, Tom wrote:

> One of my discoveries while learning the art of R, is that time has
> moved on since I did my basic statistics in school (although to my
> dismay the teaching of statistics in school appears also to have not
> noticed the movement.) I have seen a few references when people want to
> pie chart something, for the advice to be "find a better way." I've been
> reading some of the ash work (see package of same name and loads of
> papers on the web), also some interesting work on dot plots as an
> alternative to histograms. They make me feel that unless the data that
> you have in both histograms accidentally works well with the same set of
> bins you may not get the comparative assessment that you think you are
> getting.
> 
> I am beginning to form the opinion that in most cases (if not all) there
> are better alternatives to histograms.
> _________________________________________________
>  
> Tom Mulholland
> Senior Policy Officer
> WA Country Health Service
> 189 Royal St, East Perth, WA, 6004
>  
> Tel: (08) 9222 4062
> e-mail: Tom.Mulholland at health.wa.gov.au
>  
> The contents of this e-mail transmission are confidential an...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul  4 08:56:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 07:56:05 +0100 (BST)
Subject: [R] Help. Import data
In-Reply-To: <1057300747.3f05210b2e769@imp.access.unizh.ch>
Message-ID: <Pine.LNX.4.44.0307040752530.3588-100000@gannet.stats>

How to convert a factor to numeric is one of the questions (with answers) 
in R's FAQ.

Assuming you don't need to do this again, I would just convert what you 
have imported.  Otherwise, explore the max.value.labels argument of 
read.spss.

On Fri, 4 Jul 2003, Silika Tereshchenko wrote:

> I have the following problem. I have to import the dataset (survey) from SPSS 
> into R-programm. By using the "read.spss" the all of variables have been readed 
> as factor. I need the variables "income" and "age" as numeric for the 
> regreesion analysis. Could you plese help me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fredrik.karlsson at ling.umu.se  Fri Jul  4 09:05:01 2003
From: fredrik.karlsson at ling.umu.se (Fredrik Karlsson)
Date: Fri, 4 Jul 2003 09:05:01 +0200
Subject: [R] Help. Import data
In-Reply-To: <1057300747.3f05210b2e769@imp.access.unizh.ch>
References: <1057300747.3f05210b2e769@imp.access.unizh.ch>
Message-ID: <20030704070501.GB28259@ling.umu.se>

Hi Silika,

This is what I would do:

>df <- read.spss("datafile", to.data.frame=T)
>for(i in c("age","income")){ #Add more if you like
+ df[[i]] <- as.numeric(df[[i]])
+}


/Fredrik



On Fri, Jul 04, 2003 at 08:39:07AM +0200, Silika Tereshchenko wrote:
> Hoy,
> 
> 
> I have the following problem. I have to import the dataset (survey) from SPSS 
> into R-programm. By using the "read.spss" the all of variables have been readed 
> as factor. I need the variables "income" and "age" as numeric for the 
> regreesion analysis. Could you plese help me.
> 
> 
> Thanks a lot,
> best regards,
> Silika
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Jul  4 09:18:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 08:18:07 +0100 (BST)
Subject: [R] Help. Import data
In-Reply-To: <20030704070501.GB28259@ling.umu.se>
Message-ID: <Pine.LNX.4.44.0307040815040.3677-100000@gannet.stats>

On Fri, 4 Jul 2003, Fredrik Karlsson wrote:

> Hi Silika,
> 
> This is what I would do:
> 
> >df <- read.spss("datafile", to.data.frame=T)
> >for(i in c("age","income")){ #Add more if you like
> + df[[i]] <- as.numeric(df[[i]])
> +}

Please don't: that does not turn a factor into its numerical equivalent.
See the FAQ question I pointed Silika to (Q7.12 in the current version).  
It ends

   In any case, do not call `as.numeric()' or their likes directly.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dgiunchi at discau.unipi.it  Fri Jul  4 11:37:43 2003
From: dgiunchi at discau.unipi.it (Dimitri Giunchi)
Date: Fri, 4 Jul 2003 08:37:43 -0100 (GMT)
Subject: [R] configuration error when installing gtkDevice
Message-ID: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>

Dear all,

when I try to install gtkDevice on Mandrake Linux 9.1 (R v. 1.7.1), I get
the following comfiguration error (see below).

Has anyone else had this problem?
Any hints are greatly appreciated.

Thank you in advance,

Dimitri



install.packages("gtkDevice")
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 111935 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.........
downloaded 109Kb

trying URL `http://cran.r-project.org/src/contrib/gtkDevice_0.5-3.tar.gz'
Content type `application/x-tar' length 41535 bytes
opened URL
.......... .......... .......... ..........
downloaded 40Kb

* Installing *source* package 'gtkDevice' ...
checking for gtk-config... no
checking for gtk12-config... no
ERROR: Cannot find gtk-config.
ERROR: configuration failed for package 'gtkDevice'



From ripley at stats.ox.ac.uk  Fri Jul  4 09:56:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 08:56:43 +0100 (BST)
Subject: [R] configuration error when installing gtkDevice
In-Reply-To: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
Message-ID: <Pine.LNX.4.44.0307040855110.3820-100000@gannet.stats>

On Fri, 4 Jul 2003, Dimitri Giunchi wrote:

> when I try to install gtkDevice on Mandrake Linux 9.1 (R v. 1.7.1), I get
> the following comfiguration error (see below).
> 
> Has anyone else had this problem?

Yes.  Do you have the appropriate gtk development RPMs installed?
My current RH8.0 machine did not.

[...]

> * Installing *source* package 'gtkDevice' ...
> checking for gtk-config... no
> checking for gtk12-config... no
> ERROR: Cannot find gtk-config.
> ERROR: configuration failed for package 'gtkDevice'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From glaziou at pasteur-kh.org  Fri Jul  4 10:09:07 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Fri, 4 Jul 2003 15:09:07 +0700
Subject: [R] configuration error when installing gtkDevice
In-Reply-To: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
References: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
Message-ID: <20030704080907.GE580@pasteur-kh.org>

Dimitri Giunchi <dgiunchi at discau.unipi.it> wrote:
> when I try to install gtkDevice on Mandrake Linux 9.1 (R
> v. 1.7.1), I get the following comfiguration error (see
> below).
> 
> Has anyone else had this problem?
> Any hints are greatly appreciated.
>
> [...]
> 
> trying URL `http://cran.r-project.org/src/contrib/gtkDevice_0.5-3.tar.gz'
> Content type `application/x-tar' length 41535 bytes
> opened URL
> .......... .......... .......... ..........
> downloaded 40Kb
> 
> * Installing *source* package 'gtkDevice' ...
> checking for gtk-config... no
> checking for gtk12-config... no
> ERROR: Cannot find gtk-config.


The ./configure script could not find gtk-config, as stated
in the error message.  Hint: you may need to install that
tool for the script to find it on your system. A google
search will give you the directions to the right rpm
package(s).

-- 
Philippe



From maechler at stat.math.ethz.ch  Fri Jul  4 11:54:29 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 4 Jul 2003 11:54:29 +0200
Subject: [R] Looking for graphics/par cheat sheet
In-Reply-To: <Pine.LNX.4.44.0307031609310.2167-100000@gannet.stats>
References: <200307031445.HAA03387@eskimo.com>
	<Pine.LNX.4.44.0307031609310.2167-100000@gannet.stats>
Message-ID: <16133.20181.538960.967995@gargle.gargle.HOWL>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Thu, 3 Jul 2003 16:15:19 +0100 (BST) writes:

    BDR> There is one in MASS4 p.85.  It's a direct descendant
    BDR> from Bill Venables' `Notes on S'.  `An Introduction to
    BDR> R' is another descendant, and that has two less
    BDR> comprehensive figures, in sections 12.5.3/.4 in the
    BDR> version I just consulted.

Hence, if you feel like it, we'd be grateful if you'd improve on
the figures in the R-Intro and contribute them to the R project;
preferably with the R code to produce them.  For those figures,
(which were made by R as is seen in the *.eps file), the R code
has been lost to my knowledge.

    BDR> On Thu, 3 Jul 2003, Kevin Wright wrote:

    >> I've searched for a while and have not been able to find
    >> a succinct, one-page guide to the graphics parameters
    >> that control the layout of plots.  I was thinking about
    >> creating one, but thought I would see if this has already
    >> been done.
    >> 
    >> I'm thinking more about a visual guide to the pare
    >> parameters than a text description.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Jean.Marc.Fromentin at ifremer.fr  Fri Jul  4 13:01:21 2003
From: Jean.Marc.Fromentin at ifremer.fr (via the vacation program)
Date: Fri, 4 Jul 2003 13:01:21 +0200 (MEST)
Subject: [R] away from my mail, absent pour le moment
Message-ID: <200307041101.NAA07801@sete.ifremer.fr>

I will not be reading my mail from 30_juin_2003 to 09_juillet_2003 
Your mail regarding "Re: Application" will be read then.

Je ne lirai pas mon courrier du 30_juin_2003 au 09_juillet_2003 
Votre message concernant "Re: Application" sera lu a mon retour.



From duncan at research.bell-labs.com  Fri Jul  4 14:51:56 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Fri, 4 Jul 2003 08:51:56 -0400
Subject: [R] configuration error when installing gtkDevice
In-Reply-To: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>;
	from dgiunchi@discau.unipi.it on Fri, Jul 04, 2003 at 08:37:43AM
	-0100
References: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
Message-ID: <20030704085156.A8456@jessie.research.bell-labs.com>


When compiling the gtkDevice package, we need the gtk+ development
libraries, i.e. libgtk.so and header files such as gtk.h.  The
error message saying 'Cannot find gtk-config' means that we cannot
locate the script that is used to tell us where the header files
and libraries are located.

So this means that you more than likely don't have the development
libraries on your system.  You will need to ensure that these are
available before installing gtkDevice. There should be
ready-to-install binaries for gtk for the Mandrake packaging system

Then, hopefully all will proceed smoothly.

 D.

Dimitri Giunchi wrote:
> Dear all,
> 
> when I try to install gtkDevice on Mandrake Linux 9.1 (R v. 1.7.1), I get
> the following comfiguration error (see below).
> 
> Has anyone else had this problem?
> Any hints are greatly appreciated.
> 
> Thank you in advance,
> 
> Dimitri
> 
> 
> 
> install.packages("gtkDevice")
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 111935 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .........
> downloaded 109Kb
> 
> trying URL `http://cran.r-project.org/src/contrib/gtkDevice_0.5-3.tar.gz'
> Content type `application/x-tar' length 41535 bytes
> opened URL
> .......... .......... .......... ..........
> downloaded 40Kb
> 
> * Installing *source* package 'gtkDevice' ...
> checking for gtk-config... no
> checking for gtk12-config... no
> ERROR: Cannot find gtk-config.
> ERROR: configuration failed for package 'gtkDevice'
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From dgiunchi at discau.unipi.it  Fri Jul  4 15:11:53 2003
From: dgiunchi at discau.unipi.it (Dimitri Giunchi)
Date: Fri, 04 Jul 2003 15:11:53 +0200
Subject: [R] configuration error when installing gtkDevice
In-Reply-To: <20030704085156.A8456@jessie.research.bell-labs.com>
References: <32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
	<32872.131.114.187.207.1057311463.squirrel@www.serra.unipi.it>
Message-ID: <5.2.0.9.2.20030704150906.028de140@dial-up.unipi.it>

Thank you very much to everybody for the help: now it works fine.

Dimitri



At 08.51 04/07/2003 -0400, you wrote:

>When compiling the gtkDevice package, we need the gtk+ development
>libraries, i.e. libgtk.so and header files such as gtk.h.  The
>error message saying 'Cannot find gtk-config' means that we cannot
>locate the script that is used to tell us where the header files
>and libraries are located.
>
>So this means that you more than likely don't have the development
>libraries on your system.  You will need to ensure that these are
>available before installing gtkDevice. There should be
>ready-to-install binaries for gtk for the Mandrake packaging system
>
>Then, hopefully all will proceed smoothly.
>
>  D.
>
>Dimitri Giunchi wrote:
> > Dear all,
> >
> > when I try to install gtkDevice on Mandrake Linux 9.1 (R v. 1.7.1), I get
> > the following comfiguration error (see below).
> >
> > Has anyone else had this problem?
> > Any hints are greatly appreciated.
> >
> > Thank you in advance,
> >
> > Dimitri
> >
> >
> >
> > install.packages("gtkDevice")
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> > Content type `text/plain; charset=iso-8859-1' length 111935 bytes
> > opened URL
> > .......... .......... .......... .......... ..........
> > .......... .......... .......... .......... ..........
> > .........
> > downloaded 109Kb
> >
> > trying URL `http://cran.r-project.org/src/contrib/gtkDevice_0.5-3.tar.gz'
> > Content type `application/x-tar' length 41535 bytes
> > opened URL
> > .......... .......... .......... ..........
> > downloaded 40Kb
> >
> > * Installing *source* package 'gtkDevice' ...
> > checking for gtk-config... no
> > checking for gtk12-config... no
> > ERROR: Cannot find gtk-config.
> > ERROR: configuration failed for package 'gtkDevice'
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>--
>_______________________________________________________________
>
>Duncan Temple Lang                duncan at research.bell-labs.com
>Bell Labs, Lucent Technologies    office: (908)582-3217
>700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
>Murray Hill, NJ  07974-2070
>          http://cm.bell-labs.com/stat/duncan

Dimitri Giunchi
Dipartimento di Etologia Ecologia Evoluzione
Universit? di Pisa
Via A. Volta 6
I-56126 Pisa
ITALY
Phone: +39 050 21316-20255
Fax: +39 050 24653



From alobo at ija.csic.es  Fri Jul  4 18:16:19 2003
From: alobo at ija.csic.es (Agustin Lobo)
Date: Fri, 4 Jul 2003 18:16:19 +0200 (MET DST)
Subject: [R] Problem with fitdistr for beta
Message-ID: <Pine.OSF.3.91.1030704181501.28948R-100000@paleo.ija.csic.es>


I have the following problem:

I have a vector x of data (0<x<=1 ) with
a U-shaped histogram and try to fit a beta
distribution using  fitdistr. In fact,
hist(rbeta(100,0.1,0.1)) looks a lot like
my data.

The equivalent to
the example in the manual
sometimes work:

> a <- rbeta(100,0.1,0.1)
> fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
>      shape1       shape2
  0.09444627   0.12048753
 (0.01120670) (0.01550129)

but sometimes does not:
> a <- rbeta(100,0.1,0.1)
> fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
        Function cannot be evaluated at initial parameters

Unfortunately, my data fall in the second case

I've searched for any weird value that be present in the
cases in which fitdistr exits with the error message, but
could not find any.

Any help?
(please if anyone answers be sure to answer to my address as well,
I cannot subscribe to the list)

Thanks

Agus

Dr. Agustin Lobo
Instituto de Ciencias de la Tierra (CSIC)
Lluis Sole Sabaris s/n
08028 Barcelona SPAIN
tel 34 93409 5410
fax 34 93411 0012
alobo at ija.csic.es



From hennil at statisticon.se  Fri Jul  4 18:33:45 2003
From: hennil at statisticon.se (Henric Nilsson)
Date: Fri, 04 Jul 2003 16:33:45 +0000
Subject: [R] How to use quasibinomial?
Message-ID: <200307041633.h64GXVUu027124@stat.math.ethz.ch>

I think I can now answer my own questions:

Henric Nilsson (hennil at statisticon.se) wrote*:

>1. I can't seem to get the correct p-values from anova.glm() for the F-tests when
>supplying the dispersion argument and having fitted the model using
>family=quasibinomial.

Using family=quasibinomial does exactly what it says: It fits a quasi-likelihood
model with mean=mu and variance=mu(1-mu). Hence, supplying the dispersion argument
results in R treating this as a known value, i.e. not estimated from data, in which
case the F-test shouldn't be used and therefore returns the same p-value as the Chi-
squared test.

>2. When using summary.glm() on a glm object fitted using family=quasibinomial the
>reported tests are t-tests. Why?

This makes sense since we're estimating the dispersion parameter. Can't really
remember what confused me here...

//Henric



From ripley at stats.ox.ac.uk  Fri Jul  4 18:38:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 17:38:02 +0100 (BST)
Subject: [R] Problem with fitdistr for beta
In-Reply-To: <Pine.OSF.3.91.1030704181501.28948R-100000@paleo.ija.csic.es>
Message-ID: <Pine.LNX.4.44.0307041729510.15266-100000@gannet.stats>

rbeta(100,0.1,0.1) is generating samples which contain 1, an impossible 
value for a beta and hence the sample has an infinite log-likelihood.
It is clearly documented on the help page that the range is 0 < x < 1.
However, that is not so surprising as P(X > 1-1e-16) is about 1% and hence 
values will get rounded to one.

The same would happen for a value of 0.

Your code is syntactically incorrect, at least as received here.

On Fri, 4 Jul 2003, Agustin Lobo wrote:

> 
> I have the following problem:
> 
> I have a vector x of data (0<x<=1 ) with
> a U-shaped histogram and try to fit a beta
> distribution using  fitdistr. In fact,
> hist(rbeta(100,0.1,0.1)) looks a lot like
> my data.
> 
> The equivalent to
> the example in the manual
> sometimes work:
> 
> > a <- rbeta(100,0.1,0.1)
> > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> >      shape1       shape2
>   0.09444627   0.12048753
>  (0.01120670) (0.01550129)
> 
> but sometimes does not:
> > a <- rbeta(100,0.1,0.1)
> > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> > Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>         Function cannot be evaluated at initial parameters
> 
> Unfortunately, my data fall in the second case
> 
> I've searched for any weird value that be present in the
> cases in which fitdistr exits with the error message, but
> could not find any.
> 
> Any help?
> (please if anyone answers be sure to answer to my address as well,
> I cannot subscribe to the list)
> 
> Thanks
> 
> Agus
> 
> Dr. Agustin Lobo
> Instituto de Ciencias de la Tierra (CSIC)
> Lluis Sole Sabaris s/n
> 08028 Barcelona SPAIN
> tel 34 93409 5410
> fax 34 93411 0012
> alobo at ija.csic.es
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From solares at unsl.edu.ar  Fri Jul  4 19:03:16 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Fri, 4 Jul 2003 14:03:16 -0300 (ART)
Subject: [R] parametros
Message-ID: <55250.170.210.173.216.1057338196.squirrel@inter14.unsl.edu.ar>

hello, sorry i?m a beginners in R, i use the version 1.6.1 for windows and 
1.5.5 for linux (susse), my
question is about ?how i cant to knowledge the parameter of a command tcltk 
in R?, in Windows i write
tt<-tktoplevel()
t <- tkmenu(tt) 
tkconfigure(tt,menu=t) 
a<- tkmenu(t, tearoff=FALSE)

and return the parameter. ?How i can do this in R 1.5.1 for linux?. Sorry



From solares at unsl.edu.ar  Fri Jul  4 19:04:38 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Fri, 4 Jul 2003 14:04:38 -0300 (ART)
Subject: [R] parametros
Message-ID: <55382.170.210.173.216.1057338278.squirrel@inter14.unsl.edu.ar>

hello, sorry i?m a beginners in R, i use the version 1.6.1 for windows and 
1.5.5 for linux (susse), my
question is about ?how i cant to knowledge the parameter of a command tcltk 
in R?, in Windows i write
tt<-tktoplevel()
t <- tkmenu(tt) 
tkconfigure(tt,menu=t) 
a<- tkmenu(t, tearoff=FALSE)
tkconfigure(a)

and return the parameter. ?How i can do this in R 1.5.1 for linux?. Sorry 
for the previous mail,its wrong (tkconfigure(a))



From spencer.graves at pdf.com  Fri Jul  4 19:09:09 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 04 Jul 2003 10:09:09 -0700
Subject: [R] Problem with fitdistr for beta
References: <Pine.LNX.4.44.0307041729510.15266-100000@gannet.stats>
Message-ID: <3F05B4B5.7070700@pdf.com>

My standard work-around for the kind of problem you identified is to 
shrink the numbers just a little towards 0.5.  For example:

 > library(MASS)
 > a <- rbeta(100,0.1,0.1)
 > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))
Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
	Function cannot be evaluated at initial parameters

 > r.a <- range(a)
 > c0 <- 0
 > c1 <- 1
 > if(r.a[1]==0)c0 <- min(a[a>0])
 > if(r.a[2]==1)c1 <- max(a[a<1])
 > c. <- c(c0, 1-c1)
 > if(any(c.>0))c. <- min(c.[c.>0])
 > c.
[1] 2.509104e-14
 > fitdistr(x=0.5*c.[1] + (1-c.[1])*a, "beta", 
start=list(shape1=0.1,shape2=0.1))
      shape1       shape2
   0.08728921   0.10403875
  (0.01044863) (0.01320188)
 >
hope this helps.  spencer graves

Prof Brian Ripley wrote:
> rbeta(100,0.1,0.1) is generating samples which contain 1, an impossible 
> value for a beta and hence the sample has an infinite log-likelihood.
> It is clearly documented on the help page that the range is 0 < x < 1.
> However, that is not so surprising as P(X > 1-1e-16) is about 1% and hence 
> values will get rounded to one.
> 
> The same would happen for a value of 0.
> 
> Your code is syntactically incorrect, at least as received here.
> 
> On Fri, 4 Jul 2003, Agustin Lobo wrote:
> 
> 
>>I have the following problem:
>>
>>I have a vector x of data (0<x<=1 ) with
>>a U-shaped histogram and try to fit a beta
>>distribution using  fitdistr. In fact,
>>hist(rbeta(100,0.1,0.1)) looks a lot like
>>my data.
>>
>>The equivalent to
>>the example in the manual
>>sometimes work:
>>
>>
>>>a <- rbeta(100,0.1,0.1)
>>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
>>>     shape1       shape2
>>
>>  0.09444627   0.12048753
>> (0.01120670) (0.01550129)
>>
>>but sometimes does not:
>>
>>>a <- rbeta(100,0.1,0.1)
>>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
>>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>>
>>        Function cannot be evaluated at initial parameters
>>
>>Unfortunately, my data fall in the second case
>>
>>I've searched for any weird value that be present in the
>>cases in which fitdistr exits with the error message, but
>>could not find any.
>>
>>Any help?
>>(please if anyone answers be sure to answer to my address as well,
>>I cannot subscribe to the list)
>>
>>Thanks
>>
>>Agus
>>
>>Dr. Agustin Lobo
>>Instituto de Ciencias de la Tierra (CSIC)
>>Lluis Sole Sabaris s/n
>>08028 Barcelona SPAIN
>>tel 34 93409 5410
>>fax 34 93411 0012
>>alobo at ija.csic.es
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From hennil at statisticon.se  Fri Jul  4 19:24:27 2003
From: hennil at statisticon.se (Henric Nilsson)
Date: Fri, 04 Jul 2003 17:24:27 +0000
Subject: [R] Quasi AIC
Message-ID: <200307041724.h64HODUu009097@stat.math.ethz.ch>

Dear all,

Using the quasibinomial and quasipoisson families results in no AIC being
calculated. However, a quasi AIC has actually been defined by Lebreton et al
(1992). In the (in my opinon, at least) very interesting book by Burnham and
Anderson (1998,2002) this QAIC (and also QAICc) is covered. Maybe this is something
that could be implemented in R.

Take a look at page 23 in this pdf:

http://www.springer-ny.com/supplements/0387953647/Preface_Contents_and_Chapter_8.pdf

Lebreton, J-D et al. (1992) Modeling Survival and Testing Biological Hypotheses
Using Marked Anomals: A Unified Approach with Case Studies. Ecological Monographs,
62(1), 67-118.

Burnham, K.P. & Anderson, D.R. (2002) Model Selection and Multi-Model Inference - A
Practical Information-Theroretic Approach (2nd Ed). Springer Verlag.

//Henric



From hennil at statisticon.se  Fri Jul  4 19:39:47 2003
From: hennil at statisticon.se (Henric Nilsson)
Date: Fri, 04 Jul 2003 17:39:47 +0000
Subject: [R] anova.glm() and deviance/df
Message-ID: <200307041739.h64HdXUu013485@stat.math.ethz.ch>

Dear R-help,

Is seems to me that R doesn't allow correct F-tests to be computed in anova.glm
after estimating the dispersion parameter by deviance/df. Or does it?

//Henric



From ripley at stats.ox.ac.uk  Fri Jul  4 19:47:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 18:47:55 +0100 (BST)
Subject: [R] anova.glm() and deviance/df
In-Reply-To: <200307041739.h64HdXUu013485@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0307041846420.15382-100000@gannet.stats>

On Fri, 4 Jul 2003, Henric Nilsson wrote:

> Is seems to me that R doesn't allow correct F-tests to be computed in anova.glm
> after estimating the dispersion parameter by deviance/df. Or does it?

It does when they are correct, e.g. for a Gaussian family.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul  4 19:54:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 18:54:39 +0100 (BST)
Subject: [R] Problem with fitdistr for beta
In-Reply-To: <3F05B4B5.7070700@pdf.com>
Message-ID: <Pine.LNX.4.44.0307041849250.15382-100000@gannet.stats>

In this example shrinking by (1 - 2e-16) leads to a significant change in 
the distribution: see my probability calculation.  And you can't shrink by 
much less.  A beta(0.1, 0.1) is barely a continuous distribution.

On Fri, 4 Jul 2003, Spencer Graves wrote:

> My standard work-around for the kind of problem you identified is to 
> shrink the numbers just a little towards 0.5.  For example:
> 
>  > library(MASS)
>  > a <- rbeta(100,0.1,0.1)
>  > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))
> Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> 	Function cannot be evaluated at initial parameters
> 
>  > r.a <- range(a)
>  > c0 <- 0
>  > c1 <- 1
>  > if(r.a[1]==0)c0 <- min(a[a>0])
>  > if(r.a[2]==1)c1 <- max(a[a<1])
>  > c. <- c(c0, 1-c1)
>  > if(any(c.>0))c. <- min(c.[c.>0])
>  > c.
> [1] 2.509104e-14
>  > fitdistr(x=0.5*c.[1] + (1-c.[1])*a, "beta", 
> start=list(shape1=0.1,shape2=0.1))
>       shape1       shape2
>    0.08728921   0.10403875
>   (0.01044863) (0.01320188)
>  >
> hope this helps.  spencer graves
> 
> Prof Brian Ripley wrote:
> > rbeta(100,0.1,0.1) is generating samples which contain 1, an impossible 
> > value for a beta and hence the sample has an infinite log-likelihood.
> > It is clearly documented on the help page that the range is 0 < x < 1.
> > However, that is not so surprising as P(X > 1-1e-16) is about 1% and hence 
> > values will get rounded to one.
> > 
> > The same would happen for a value of 0.
> > 
> > Your code is syntactically incorrect, at least as received here.
> > 
> > On Fri, 4 Jul 2003, Agustin Lobo wrote:
> > 
> > 
> >>I have the following problem:
> >>
> >>I have a vector x of data (0<x<=1 ) with
> >>a U-shaped histogram and try to fit a beta
> >>distribution using  fitdistr. In fact,
> >>hist(rbeta(100,0.1,0.1)) looks a lot like
> >>my data.
> >>
> >>The equivalent to
> >>the example in the manual
> >>sometimes work:
> >>
> >>
> >>>a <- rbeta(100,0.1,0.1)
> >>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> >>>     shape1       shape2
> >>
> >>  0.09444627   0.12048753
> >> (0.01120670) (0.01550129)
> >>
> >>but sometimes does not:
> >>
> >>>a <- rbeta(100,0.1,0.1)
> >>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> >>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> >>
> >>        Function cannot be evaluated at initial parameters
> >>
> >>Unfortunately, my data fall in the second case
> >>
> >>I've searched for any weird value that be present in the
> >>cases in which fitdistr exits with the error message, but
> >>could not find any.
> >>
> >>Any help?
> >>(please if anyone answers be sure to answer to my address as well,
> >>I cannot subscribe to the list)
> >>
> >>Thanks
> >>
> >>Agus
> >>
> >>Dr. Agustin Lobo
> >>Instituto de Ciencias de la Tierra (CSIC)
> >>Lluis Sole Sabaris s/n
> >>08028 Barcelona SPAIN
> >>tel 34 93409 5410
> >>fax 34 93411 0012
> >>alobo at ija.csic.es
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Goodman at ioz.ac.uk  Fri Jul  4 20:34:06 2003
From: Simon.Goodman at ioz.ac.uk (Simon Goodman)
Date: Fri, 4 Jul 2003 19:34:06 +0100
Subject: [R] specifying dataframe column names in loops
Message-ID: <78545F8851BED511A03C0060088063C2015AAC59@zsl3.zsl.org>

Dear list members,

This is probably a naive question (from a new user), but I'm having problems
getting column 
names to be recognised in loop that is supposed to apply a set of
calculations in turn
to each column of a dataframe.

The dataframe has 33 columns of output from a simulation.
The columns each have a text name and I want to apply the same set of
calculations
to each column in turn. These include some self-defined functions that refer
back to the 
column in the current iteration.

Setting up a 'for' loop and using age[s] (where 'age' is the dataframe and
's' is the iterator) to specify
 each column works for things like 'max' and 'min'. However for functions
like 'density' or locfit, 
error messages are generated:

lf<-locfit(~age[s])
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

Similary if I try to use the 'names' function.

lf<-locfit(~names(age[s]))
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        invalid variable type

So the question is how do I get functions like density or locfit to 
work in the loop in the same they would for specifying the name
of the column directly e.g. d<-density(ACER), where ACER is the name of the 
first column. 

Many thanks, Simon Goodman....


-----------------------------------------------------------------------

_________________________________________________________________________
This e-mail has been sent in confidence to the named addressee(s).
If you are not the intended recipient, you must not disclose or distribute
it in any form, and you are asked to contact the sender immediately.
Views or opinions expressed in this communication may not be those
of The Zoological Society of London and, therefore, The Zoological
Society of London does not accept legal responsibility for the contents
of this message.  The recipient(s) must be aware that e-mail is not a
secure communication medium and that the contents of this mail may
have been altered by a third party in transit.
If you have any issues regarding this mail please contact:
administrator at zsl.org.



From ripley at stats.ox.ac.uk  Fri Jul  4 20:50:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jul 2003 19:50:32 +0100 (BST)
Subject: [R] specifying dataframe column names in loops
In-Reply-To: <78545F8851BED511A03C0060088063C2015AAC59@zsl3.zsl.org>
Message-ID: <Pine.LNX.4.44.0307041945470.15515-100000@gannet.stats>

Sounds like you need to use substitute(). Something like

eval(substitute(lf <- locfit(~s, data=age), list(s=s)))

Formulae are different: I don't see why density() should be.

On Fri, 4 Jul 2003, Simon Goodman wrote:

> Dear list members,
> 
> This is probably a naive question (from a new user), but I'm having problems
> getting column 
> names to be recognised in loop that is supposed to apply a set of
> calculations in turn
> to each column of a dataframe.
> 
> The dataframe has 33 columns of output from a simulation.
> The columns each have a text name and I want to apply the same set of
> calculations
> to each column in turn. These include some self-defined functions that refer
> back to the 
> column in the current iteration.
> 
> Setting up a 'for' loop and using age[s] (where 'age' is the dataframe and
> 's' is the iterator) to specify
>  each column works for things like 'max' and 'min'. However for functions
> like 'density' or locfit, 
> error messages are generated:
> 
> lf<-locfit(~age[s])
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
>         invalid variable type

...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Jul  4 22:54:44 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Jul 2003 22:54:44 +0200
Subject: [R] parametros
References: <55382.170.210.173.216.1057338278.squirrel@inter14.unsl.edu.ar>
Message-ID: <3F05E994.5B4BE1C0@statistik.uni-dortmund.de>

solares at unsl.edu.ar wrote:
> 
> hello, sorry i?m a beginners in R, i use the version 1.6.1 

This one is outdated.


> for windows and 1.5.5 

Such a version was never released, and the below mentioned R-1.5.1 is
outdated.


> This one for linux (susse),

I guess you mean SuSE?


> my question is about ?how i cant to knowledge the parameter of a command tcltk
> in R?, in Windows i write
> tt<-tktoplevel()
> t <- tkmenu(tt)
> tkconfigure(tt,menu=t)
> a<- tkmenu(t, tearoff=FALSE)
> tkconfigure(a)
> 
> and return the parameter. ?How i can do this in R 1.5.1 for linux?. Sorry
> for the previous mail,its wrong (tkconfigure(a))

I don't have an R-1.5.1 on a linux machine around, but 

1) Please upgrade to a recent version of R and the tcltk package.
2) Was the Linux version compiled with tcl/tk support (and are the
required libraries installed on the system?)? If not, recompiling would
be neccessary anyway.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Jul  4 23:16:18 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 04 Jul 2003 23:16:18 +0200
Subject: [R] Quasi AIC
References: <200307041724.h64HODUu009097@stat.math.ethz.ch>
Message-ID: <3F05EEA2.DFBAAAB8@statistik.uni-dortmund.de>

Henric Nilsson wrote:
> 
> Dear all,
> 
> Using the quasibinomial and quasipoisson families results in no AIC being
> calculated. However, a quasi AIC has actually been defined by Lebreton et al
> (1992). In the (in my opinon, at least) very interesting book by Burnham and
> Anderson (1998,2002) this QAIC (and also QAICc) is covered. Maybe this is something
> that could be implemented in R.
> 
> Take a look at page 23 in this pdf:
> 
> http://www.springer-ny.com/supplements/0387953647/Preface_Contents_and_Chapter_8.pdf
> 
> Lebreton, J-D et al. (1992) Modeling Survival and Testing Biological Hypotheses
> Using Marked Anomals: A Unified Approach with Case Studies. Ecological Monographs,
> 62(1), 67-118.
> 
> Burnham, K.P. & Anderson, D.R. (2002) Model Selection and Multi-Model Inference - A
> Practical Information-Theroretic Approach (2nd Ed). Springer Verlag.


Well, I tried to send a private message, but the e-mail address you have
specified seems to be unknown for your mail server ... Please fix it
before posting messages.

If you think QAIC is useful and there's no implemented solution yet (I
haven't found any in a *quick* search through CRAN packages), patches
and suggestions including code and documentation are welcome as usual (I
don't know that much about QAIC, and cannot judge its worth, though).

Uwe Ligges



From gyc at stat.berkeley.edu  Sat Jul  5 01:06:18 2003
From: gyc at stat.berkeley.edu (Yongchao Ge)
Date: Fri, 4 Jul 2003 16:06:18 -0700 (PDT)
Subject: [R] the huge postscript plot
Message-ID: <Pine.SOL.4.50.0307041542200.7038-100000@toto.Berkeley.EDU>

Hi,

I'm just wondering how I can do to make a huge postscript plot be
manageable. My question is that I have to draw around 60,000 points which
makes it painfully slow to print or view in gv or put it into latex
document, though it is very fast to produce the postscript file.

A simple example is in the attachment.

Well, I found that if I use png or jpeg. It is much faster to view
the figure. The only problem is that the x label, y label are using
bitmapped font so it doesn't look as beautiful as postscript. Sometimes it
is just hard to read the legend which consists of information how to
interpret the figure.

My question is that if there are some options or packages such that when
we are plotting many points or lines, the plot uses the bitmapped (raster)
format, but for the characters, like the x label, y label, and the title,
it uses the native font.

pdf seems use the above approach and is very fast, but I have to use
pdf2ps to convert the pdf file to ps file. It is still slow. If you know
other good converter of pdf file to ps file, i will also very appreciate
it.


Thanks,

Yongchao

x<-1:2000
y<-matrix(rnorm(2000*12),2000,12)
for(i in 1:12){
    y[,i]<-sort(y[,i]+i*0.5)
}
postscript("try.ps")
matplot(x,y,type="l",lwd=5)
dev.off()



From glaziou at pasteur-kh.org  Sat Jul  5 03:46:55 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Sat, 5 Jul 2003 08:46:55 +0700
Subject: [R] the huge postscript plot
In-Reply-To: <Pine.SOL.4.50.0307041542200.7038-100000@toto.Berkeley.EDU>
References: <Pine.SOL.4.50.0307041542200.7038-100000@toto.Berkeley.EDU>
Message-ID: <20030705014655.GB579@pasteur-kh.org>

Yongchao Ge <gyc at stat.berkeley.edu> wrote:
> I'm just wondering how I can do to make a huge postscript plot be
> manageable. My question is that I have to draw around 60,000 points which
> makes it painfully slow to print or view in gv or put it into latex
> document, though it is very fast to produce the postscript file.
> 
> A simple example is in the attachment.
>
> 
> x<-1:2000
> y<-matrix(rnorm(2000*12),2000,12)
> for(i in 1:12){
>     y[,i]<-sort(y[,i]+i*0.5)
> }
> postscript("try.ps")
> matplot(x,y,type="l",lwd=5)
> dev.off()


How about sampling a subset of the data:

  sub=sort(sample(x,200, replace=F))
  postscript("try.ps")
  matplot(x[sub],y[sub,],type="l",lwd=5)     
  dev.off()


-- 
Philippe



From gisar at nus.edu.sg  Sat Jul  5 07:38:28 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Sat, 5 Jul 2003 13:38:28 +0800
Subject: [R] the huge postscript plot
Message-ID: <CDA8D2689259E444942B3CDED8DD912932D49E@MBXSRV03.stf.nus.edu.sg>

How about pdf(), jpeg() or png() ?

-----Original Message-----
From: Philippe Glaziou [mailto:glaziou at pasteur-kh.org] 
Sent: Saturday, July 05, 2003 9:47 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] the huge postscript plot


Yongchao Ge <gyc at stat.berkeley.edu> wrote:
> I'm just wondering how I can do to make a huge postscript plot be 
> manageable. My question is that I have to draw around 60,000 points 
> which makes it painfully slow to print or view in gv or put it into 
> latex document, though it is very fast to produce the postscript file.
> 
> A simple example is in the attachment.
>
> 
> x<-1:2000
> y<-matrix(rnorm(2000*12),2000,12)
> for(i in 1:12){
>     y[,i]<-sort(y[,i]+i*0.5)
> }
> postscript("try.ps")
> matplot(x,y,type="l",lwd=5)
> dev.off()


How about sampling a subset of the data:

  sub=sort(sample(x,200, replace=F))
  postscript("try.ps")
  matplot(x[sub],y[sub,],type="l",lwd=5)     
  dev.off()


-- 
Philippe

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Sat Jul  5 08:59:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Jul 2003 07:59:26 +0100 (BST)
Subject: [R] the huge postscript plot
In-Reply-To: <Pine.SOL.4.50.0307041542200.7038-100000@toto.Berkeley.EDU>
Message-ID: <Pine.LNX.4.44.0307050737490.16499-100000@gannet.stats>

On Fri, 4 Jul 2003, Yongchao Ge wrote:

> Hi,
> 
> I'm just wondering how I can do to make a huge postscript plot be
> manageable. My question is that I have to draw around 60,000 points which
> makes it painfully slow to print or view in gv or put it into latex
> document, though it is very fast to produce the postscript file.
> 
> A simple example is in the attachment.
> 
> Well, I found that if I use png or jpeg. It is much faster to view
> the figure. The only problem is that the x label, y label are using
> bitmapped font so it doesn't look as beautiful as postscript. Sometimes it
> is just hard to read the legend which consists of information how to
> interpret the figure.

Only if scaling is involved: try setting the resolution so you view it at 
1:1.

> My question is that if there are some options or packages such that when
> we are plotting many points or lines, the plot uses the bitmapped (raster)
> format, but for the characters, like the x label, y label, and the title,
> it uses the native font.
> 
> pdf seems use the above approach and is very fast, but I have to use

It doesn't use that approach: PDF is primarily a vector format, and that's
what R's pdf() driver produces.

> pdf2ps to convert the pdf file to ps file. It is still slow. If you know
> other good converter of pdf file to ps file, i will also very appreciate
> it.

Which points up that the problem is in your .ps viewer, nowhere else.
That's not surprising; postscript is a *print* description language and
primarily intended for high-quality printing.  For a long time postscript
printers contained more powerful processors than workstations (let alone
PCs).  Your example is not `painfully slow' on my printer (nor to view 
with ghostscript, to give due credit to the engine behind gv), but it is 
also not an interesting plot, nor is it `huge'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gyc at stat.berkeley.edu  Sat Jul  5 09:06:15 2003
From: gyc at stat.berkeley.edu (Yongchao Ge)
Date: Sat, 5 Jul 2003 00:06:15 -0700 (PDT)
Subject: [R] the huge postscript plot
Message-ID: <Pine.SOL.4.50.0307050004120.7891-100000@toto.Berkeley.EDU>

Hi Matt,

Thanks for the help. I'm just wondering if some package or some
options in par or ps.options exists so that I  can just use it directly.
As we know, we can not just randomly sample 1000 of them as we may miss some
important extreme points (outliers). We need to define a small distance
d, when a clouds of points are within the distance of d,
then we can randomly sample a few points from each clouds so we  don't
lose any visual information. What it really needs is a fast algorithm
to group 60,000 points into 1000+ of clusters. Each cluster's radius must
be less than d.

We can surely work this way. Again, let's go back to my original question,
I'd like to find a fast way to plot the points into a bitmapped (raster)
format, say 600x800. This is just an informal way of
the sampling strategy defined in the first graph and the radius d is
defined by the dimension size of the plot (600x800).  The larger dimension
size, the smaller the radius d is.

If such package or options exists in R, please let me know as it can save
me enormous of time for the programming.

Yongchao


On Fri, 4 Jul 2003, Wiener, Matthew wrote:

> One possibility is to sample your 60,000 points, since you probably can't
> see them all distinctly anyway.  You could sample, say, 10000 of them.
>
> Hope this helps,
>
> Matt Wiener
>
> -----Original Message-----
> From: Yongchao Ge [mailto:gyc at stat.berkeley.edu]
> Sent: Friday, July 04, 2003 7:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] the huge postscript plot
>
>
> Hi,
>
> I'm just wondering how I can do to make a huge postscript plot be
> manageable. My question is that I have to draw around 60,000 points which
> makes it painfully slow to print or view in gv or put it into latex
> document, though it is very fast to produce the postscript file.
>
> A simple example is in the attachment.
>
> Well, I found that if I use png or jpeg. It is much faster to view
> the figure. The only problem is that the x label, y label are using
> bitmapped font so it doesn't look as beautiful as postscript. Sometimes it
> is just hard to read the legend which consists of information how to
> interpret the figure.
>
> My question is that if there are some options or packages such that when
> we are plotting many points or lines, the plot uses the bitmapped (raster)
> format, but for the characters, like the x label, y label, and the title,
> it uses the native font.
>
> pdf seems use the above approach and is very fast, but I have to use
> pdf2ps to convert the pdf file to ps file. It is still slow. If you know
> other good converter of pdf file to ps file, i will also very appreciate
> it.
>
>
> Thanks,
>
> Yongchao
>
> x<-1:2000
> y<-matrix(rnorm(2000*12),2000,12)
> for(i in 1:12){
>     y[,i]<-sort(y[,i]+i*0.5)
> }
> postscript("try.ps")
> matplot(x,y,type="l",lwd=5)
> dev.off()
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
> USA) that may be confidential, proprietary copyrighted and/or legally
> privileged, and is intended solely for the use of the individual or entity
> named on this message. If you are not the intended recipient, and
> have received this message in error, please immediately return this by
> e-mail and then delete it.
> ------------------------------------------------------------------------------
>



From gyc at stat.berkeley.edu  Sat Jul  5 09:15:10 2003
From: gyc at stat.berkeley.edu (Yongchao Ge)
Date: Sat, 5 Jul 2003 00:15:10 -0700 (PDT)
Subject: [R] the huge postscript plot
In-Reply-To: <20030705013305.GA9874@cc.umanitoba.ca>
References: <Pine.SOL.4.50.0307041542200.7038-100000@toto.Berkeley.EDU>
	<20030705013305.GA9874@cc.umanitoba.ca>
Message-ID: <Pine.SOL.4.50.0307050007410.7891-100000@toto.Berkeley.EDU>

> Another alternative is to insert pdf graphs into latex. The way you do this
> is to use pdflatex with the graphicx package. In the preamble of your latex
> document, put
>
> \usepackage[pdftex]{graphicx}  instead of \usepackage[dvips]{graphicx}
>
> somewhere. When you run the document, use pdflatex file.tex instead of
> latex. Otherwise, it's essentially the same process. The only headache is
> that pdflatex won't let you overwrite an existing pdf output file, but
> there are clearly many ways to handle that little annoyance.

Hi Dennis,

Sure, this works out but I need to produce ps and pdf documents so I
have to use dvips and ps2pdf, not pdflatex. I also need to convert all my
ps file to pdf file by epstopdf, which is OK, but may not
be perfect. I still like to have a solution that can work it on the level
of producing ps file from R environment.



From gyc at stat.berkeley.edu  Sat Jul  5 09:56:49 2003
From: gyc at stat.berkeley.edu (Yongchao Ge)
Date: Sat, 5 Jul 2003 00:56:49 -0700 (PDT)
Subject: [R] the huge postscript plot
In-Reply-To: <Pine.LNX.4.44.0307050737490.16499-100000@gannet.stats>
References: <Pine.LNX.4.44.0307050737490.16499-100000@gannet.stats>
Message-ID: <Pine.SOL.4.50.0307050016380.7891-100000@toto.Berkeley.EDU>

> Which points up that the problem is in your .ps viewer, nowhere else.
> That's not surprising; postscript is a *print* description language and
> primarily intended for high-quality printing.  For a long time postscript
> printers contained more powerful processors than workstations (let alone
> PCs).  Your example is not `painfully slow' on my printer (nor to view
> with ghostscript, to give due credit to the engine behind gv), but it is
> also not an interesting plot, nor is it `huge'.

Hi Prof Ripley

Thank you for this reply and clarifying the concepts for me. I am using
linux on a PC. It might be the reason that .ps viewer makes it slow. I
also tried the department workstation (kind of outdated). It's still
slow. "painful" might be an exaggeration, or it just happened on my PC,
sorry for using this word. The example itself is not interesting, but I
need to draw many figures with more than 60,000 points. The example is
just one plot. My latex document may contain many such plots. I guess I have to
live with the converting of png file to eps file by setting the ratio 1:1
as you said. For a letter size paper plot, it's 612x792.

Maybe a better  question might be: how to sample more points
for a letter size paper?

For example, just specifying png size  as 1224x1584 will get more sample
points to plot, but it doesn't look good as the character size, lwd, or
other par parameters have to increase twice as much.



From wolski at molgen.mpg.de  Sat Jul  5 17:08:41 2003
From: wolski at molgen.mpg.de (Eryk Wolski)
Date: Sat, 5 Jul 2003 17:08:41 +0200 (MET DST)
Subject: [R] Font size of xlab & ylab.
Message-ID: <Pine.OSF.4.31.0307051707180.16802-100000@molgix.molgen.mpg.de>

Hi!
How thy font size of xlab & ylab can be set?

Eryk



From ripley at stats.ox.ac.uk  Sat Jul  5 17:20:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Jul 2003 16:20:24 +0100 (BST)
Subject: [R] Font size of xlab & ylab.
In-Reply-To: <Pine.OSF.4.31.0307051707180.16802-100000@molgix.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0307051619300.17670-100000@gannet.stats>

?par: you are looking for cex.lab

On Sat, 5 Jul 2003, Eryk Wolski wrote:

> How thy font size of xlab & ylab can be set?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Timur.Elzhov at jinr.ru  Sat Jul  5 18:13:52 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Sat, 5 Jul 2003 20:13:52 +0400
Subject: [R] Second Y axis
In-Reply-To: <0E3E7E8F6E23DF4C8127A063568356B5010CF263@nihexchange12.nih.gov>
References: <0E3E7E8F6E23DF4C8127A063568356B5010CF263@nihexchange12.nih.gov>
Message-ID: <20030705161351.GA4256@pcf004.jinr.ru>

On Wed, Jul 02, 2003 at 04:05:13PM -0400, Depinay, Jean-marc (NIH/FIC) wrote:

> I would like to add a second graph on the right y axis. Is it a way to do so
> with R?
plot(...)        # you normally plot 1st graph
par(new = TRUE)  # 2nd graph won't clean the 1st
plot(..., axes = FALSE, xlab = "", ylab = "")  # plotting 2ng graph
axis(4)


--
WBR,
Timur.



From paulda at BATTELLE.ORG  Sun Jul  6 01:29:08 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Sat, 05 Jul 2003 19:29:08 -0400
Subject: [R] Multiple plots on same graph
Message-ID: <940250A9EB37A24CBE28D858EF07774967A9AD@ws-bco-mse3.milky-way.battelle.org>

R1.7, Win2k:

I have some Splus code that has allowed me, in the past,
to place multiple plots on the same graph:


plot(y1 ~ x1, data = foo1.frame, type = "l", xlab="",ylab="", 
     xlim=c(...,...), ylim=c(...,...), axes=F)
par(new=T, xaxs = "d", yaxs = "d")
plot(y2 ~ x2, data = foo2.frame, col = 2)


However, I get the error message
"Error in plot.new() : axis style "d" unimplemented"
Can anyone recommend a way for me to accomplish the same
thing as the code above?  If I simply use par(new=T),
the produced graph contains elements that do not correspond
to the same axis-scales.


Thanks in advance,
  david paul



From support at sierra.com  Sun Jul  6 03:21:55 2003
From: support at sierra.com (support@sierra.com)
Date: Sat, 5 Jul 2003 18:21:55 -0700 (PDT)
Subject: [R] Thank you for contacting Technical support /Customer service
In-Reply-To: <200307060121.h661LZBe022219@baron.sierra.com>
References: <200307060121.h661LZBe022219@baron.sierra.com>
Message-ID: <200307060121.h661Lt6B022232@baron.sierra.com>

Hello, thank you for contacting Technical support/Customer service.
Please click on the link below to go to our online support site.
http://support.vugames.com/
Do not respond to this message as we will not see it.



From znmeb at aracnet.com  Sun Jul  6 05:39:33 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Sat, 5 Jul 2003 20:39:33 -0700
Subject: [R] Generating a vector for breaks in a histogram
In-Reply-To: <Pine.LNX.4.44.0307040733240.3588-100000@gannet.stats>
Message-ID: <000001c34370$37ab09a0$73c463d8@plaza.ds.adp.com>

Well ... In my own recent example, it was plotting the raw data as a
histogram that finally directed me to the "truth" of what the data had to
say. As you may recall, the dataset was inter-arrival times of calls to a
computer routine, known only from timestamps truncated (not rounded) to the
nearest second. I started with kernel density (sm.density, with the default
parameters, to be precise) and was unsatisfied with the result. Yesterday,
when I plotted the raw counts (how many values were 0, how many 1, etc.) as
a histogram, I was struck by two things:

1. There really are only two peaks -- the "stuff" in between them is, for
the purpose of business decisions, irrelevant.

2. The inter-arrival time value "0" in such a dataset represents all the
values that are greater than or equal to zero and *less than 1*, and so on.
There is a natural "histogramming" going on via the timestamp truncation,
which implies to me that the *midpoint* of the "bin" -- say, for the 0
values, 0.5 -- is the "natural" value to choose for the "x-axis" in the
absence of any better information. This also rather neatly disposes of the
issue of zero-valued inter-arrival times. :)

Are the "old ways" best? Maybe not. Can I make reasonable business decisions
without histograms? I'm not convinced that's the case; it certainly wasn't
the case this time.

Finally, while I've never been fortunate enough to use S, the existence of R
has caused a revolution in the way I do the analysis of computer performance
data. Before R came along, the only tools I had available were Excel,
Minitab, and any special-purpose code I was willing to write to accomplish
tasks not in the vocabulary of Excel or Minitab. For example, it's
difficult, though not impossible, to do a non-linear regression or kernel
density estimation with either tool. In R, they're one-liners. If there was
a Nobel Prize for scientific software, I'd nominate R and its creators. (Of
course, there *is* a Nobel in Economics.) :)
-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net

> -----Original Message-----
> Things have moved on since the ASH work too, but I would 
> agree that density estimation is often a better way than 
> histograms.  However, close 
> to state-of-the-art density estimation is built into R 
> (?density) and packages `polspline', `KernSmooth' and `sm' 
> are also much more advanced 
> than `ash'. 
> 
> It was the advent of enough computing power that changed 
> this, and the S 
> language has been in the forefront of making the state of the art 
> available.  You'll see that MASS (the book) covers histograms and 
> alternatives in its chapter on Univariate Distributions, and 
> it has since 
> its 1994 first edition (when did you go to `school'?)



From ripley at stats.ox.ac.uk  Sun Jul  6 08:19:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Jul 2003 07:19:29 +0100 (BST)
Subject: [R] Multiple plots on same graph
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967A9AD@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <Pine.LNX.4.44.0307060718230.26266-100000@gannet.stats>

On Sat, 5 Jul 2003, Paul, David  A wrote:

> R1.7, Win2k:
> 
> I have some Splus code that has allowed me, in the past,
> to place multiple plots on the same graph:
> 
> 
> plot(y1 ~ x1, data = foo1.frame, type = "l", xlab="",ylab="", 
>      xlim=c(...,...), ylim=c(...,...), axes=F)
> par(new=T, xaxs = "d", yaxs = "d")
> plot(y2 ~ x2, data = foo2.frame, col = 2)
> 
> 
> However, I get the error message
> "Error in plot.new() : axis style "d" unimplemented"
> Can anyone recommend a way for me to accomplish the same
> thing as the code above?  If I simply use par(new=T),
> the produced graph contains elements that do not correspond
> to the same axis-scales.

Use lines() or points() to add to the existing graph.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alobo at ija.csic.es  Sun Jul  6 12:08:53 2003
From: alobo at ija.csic.es (alobo@ija.csic.es)
Date: Sun,  6 Jul 2003 12:08:53 +0200
Subject: [R] Problem with fitdistr for beta
In-Reply-To: <Pine.LNX.4.44.0307041849250.15382-100000@gannet.stats>
References: <Pine.LNX.4.44.0307041849250.15382-100000@gannet.stats>
Message-ID: <1057486133.3f07f5355e972@orogeno.ija.csic.es>

Thanks Prof. B. Ripley and
Spencer Graves for your help.

According to the last message from 
BR, perhaps I should use another
distribution or combination
of distributions to model my data,
which look like a beta(0.1,0.1)
but can have both 0 and 1. Any
suggestion?

(regarding the extra "1)" in my
original message, it was just a pasting
problem.)

Agus

Mensaje citado por Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> In this example shrinking by (1 - 2e-16) leads to a significant change in 
> the distribution: see my probability calculation.  And you can't shrink by 
> much less.  A beta(0.1, 0.1) is barely a continuous distribution.
> 
> On Fri, 4 Jul 2003, Spencer Graves wrote:
> 
> > My standard work-around for the kind of problem you identified is to 
> > shrink the numbers just a little towards 0.5.  For example:
> > 
> >  > library(MASS)
> >  > a <- rbeta(100,0.1,0.1)
> >  > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))
> > Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> > 	Function cannot be evaluated at initial parameters
> > 
> >  > r.a <- range(a)
> >  > c0 <- 0
> >  > c1 <- 1
> >  > if(r.a[1]==0)c0 <- min(a[a>0])
> >  > if(r.a[2]==1)c1 <- max(a[a<1])
> >  > c. <- c(c0, 1-c1)
> >  > if(any(c.>0))c. <- min(c.[c.>0])
> >  > c.
> > [1] 2.509104e-14
> >  > fitdistr(x=0.5*c.[1] + (1-c.[1])*a, "beta", 
> > start=list(shape1=0.1,shape2=0.1))
> >       shape1       shape2
> >    0.08728921   0.10403875
> >   (0.01044863) (0.01320188)
> >  >
> > hope this helps.  spencer graves
> > 
> > Prof Brian Ripley wrote:
> > > rbeta(100,0.1,0.1) is generating samples which contain 1, an impossible 
> > > value for a beta and hence the sample has an infinite log-likelihood.
> > > It is clearly documented on the help page that the range is 0 < x < 1.
> > > However, that is not so surprising as P(X > 1-1e-16) is about 1% and
> hence 
> > > values will get rounded to one.
> > > 
> > > The same would happen for a value of 0.
> > > 
> > > Your code is syntactically incorrect, at least as received here.
> > > 
> > > On Fri, 4 Jul 2003, Agustin Lobo wrote:
> > > 
> > > 
> > >>I have the following problem:
> > >>
> > >>I have a vector x of data (0<x<=1 ) with
> > >>a U-shaped histogram and try to fit a beta
> > >>distribution using  fitdistr. In fact,
> > >>hist(rbeta(100,0.1,0.1)) looks a lot like
> > >>my data.
> > >>
> > >>The equivalent to
> > >>the example in the manual
> > >>sometimes work:
> > >>
> > >>
> > >>>a <- rbeta(100,0.1,0.1)
> > >>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> > >>>     shape1       shape2
> > >>
> > >>  0.09444627   0.12048753
> > >> (0.01120670) (0.01550129)
> > >>
> > >>but sometimes does not:
> > >>
> > >>>a <- rbeta(100,0.1,0.1)
> > >>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
> > >>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
> > >>
> > >>        Function cannot be evaluated at initial parameters
> > >>
> > >>Unfortunately, my data fall in the second case
> > >>
> > >>I've searched for any weird value that be present in the
> > >>cases in which fitdistr exits with the error message, but
> > >>could not find any.
> > >>
> > >>Any help?
> > >>(please if anyone answers be sure to answer to my address as well,
> > >>I cannot subscribe to the list)
> > >>
> > >>Thanks
> > >>
> > >>Agus
> > >>
> > >>Dr. Agustin Lobo
> > >>Instituto de Ciencias de la Tierra (CSIC)
> > >>Lluis Sole Sabaris s/n
> > >>08028 Barcelona SPAIN
> > >>tel 34 93409 5410
> > >>fax 34 93411 0012
> > >>alobo at ija.csic.es
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> > >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 




-------------------------------------------------
This mail sent through IMP: http://horde.org/imp/



From laurent at cbs.dtu.dk  Sun Jul  6 16:53:37 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Sun, 6 Jul 2003 16:53:37 +0200
Subject: [R] cross-compiling for windows with linux
Message-ID: <20030706145337.GA4003562@genome.cbs.dtu.dk>

Dear List,

I am (still) trying to use the makefile advertised in R-News
(the Makefile is a wrapper around other tools).
When I try to build a package using the (fairly recent)
'install.R' mecanism, the output leaves me not so confident
about the the usability of the pack built for Windows
(I am currently unable to verify it):

---------- Making package splicegear ------------
  installing inst files
  adding build stamp to DESCRIPTION
  installing R files
  save image
/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 23: /home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/bin/Rterm: No such file or directory
/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 34:  5846 Broken pipe             cat ${lib}/${pkg}/R/${pkg}
Execution of package source for splicegear failed
mv: cannot stat `.RData': No such file or directory

Changing 'Rterm' by 'Rtem.exe' in 'install-save.sh' do not appear to fix
the problem (the error message changes to 'cannot execute').

Any pointer would be appreciated.


Thanks,



L.



From ligges at statistik.uni-dortmund.de  Sun Jul  6 17:15:39 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 06 Jul 2003 17:15:39 +0200
Subject: [R] cross-compiling for windows with linux
In-Reply-To: <20030706145337.GA4003562@genome.cbs.dtu.dk>
References: <20030706145337.GA4003562@genome.cbs.dtu.dk>
Message-ID: <3F083D1B.1030909@statistik.uni-dortmund.de>

Laurent Gautier wrote:

> Dear List,
> 
> I am (still) trying to use the makefile advertised in R-News
> (the Makefile is a wrapper around other tools).
> When I try to build a package using the (fairly recent)
> 'install.R' mecanism, the output leaves me not so confident
> about the the usability of the pack built for Windows
> (I am currently unable to verify it):
> 
> ---------- Making package splicegear ------------
>   installing inst files
>   adding build stamp to DESCRIPTION
>   installing R files
>   save image
> /home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 23: /home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/bin/Rterm: No such file or directory
> /home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 34:  5846 Broken pipe             cat ${lib}/${pkg}/R/${pkg}
> Execution of package source for splicegear failed
> mv: cannot stat `.RData': No such file or directory
> 
> Changing 'Rterm' by 'Rtem.exe' in 'install-save.sh' do not appear to fix
> the problem (the error message changes to 'cannot execute').
> 
> Any pointer would be appreciated.

Most readers of R-help won't know that script well enough in order to 
provide an answer... What about asking the authors of that article directly?

Anyway, you don't need that script (it might be very helpful given it 
works, though). Just read the section "Cross-building packages on Linux" 
in .../src/gnwuwin32/readme.packages on how to cross-compile packages.
If the package uses compiled code, you will have to set up an 
environment according to the Section "Cross-building on ix86 Linux" in 
.../src/gnwuwin32/install as well.
I've never tried it, since compiling on Windows for Windows works.

Uwe Ligges



From dmurdoch at pair.com  Sun Jul  6 17:36:15 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 06 Jul 2003 11:36:15 -0400
Subject: [R] cross-compiling for windows with linux
In-Reply-To: <20030706145337.GA4003562@genome.cbs.dtu.dk>
References: <20030706145337.GA4003562@genome.cbs.dtu.dk>
Message-ID: <74gggvs96da11nqsq8i536optphskdet7l@4ax.com>

On Sun, 6 Jul 2003 16:53:37 +0200, you wrote:

>---------- Making package splicegear ------------
>  installing inst files
>  adding build stamp to DESCRIPTION
>  installing R files
>  save image
>/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 23: /home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/bin/Rterm: No such file or directory
>/home/laurent/these/R/RCrossBuild/WinR/R-1.7.1/src/gnuwin32/install-save.sh: line 34:  5846 Broken pipe             cat ${lib}/${pkg}/R/${pkg}
>Execution of package source for splicegear failed
>mv: cannot stat `.RData': No such file or directory
>
>Changing 'Rterm' by 'Rtem.exe' in 'install-save.sh' do not appear to fix
>the problem (the error message changes to 'cannot execute').

Rterm in Windows is the equivalent of R in Linux.  I imagine changing
Rterm to R should get things to work, but I can't do cross-compiling
to verify, so I can't really support cross-compiling at all.  There
may well be other problems besides this one.

Duncan Murdoch



From spencer.graves at pdf.com  Sun Jul  6 18:07:41 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jul 2003 09:07:41 -0700
Subject: [R] Problem with fitdistr for beta
References: <Pine.LNX.4.44.0307041849250.15382-100000@gannet.stats>
	<1057486133.3f07f5355e972@orogeno.ija.csic.es>
Message-ID: <3F08494D.4040107@pdf.com>

The following adjusts only observations that are numerically exactly 0 
or 1.  It will doubtless introduce some bias.  However, the bias 
introduced will be substantially less than the bias introduced by my 
earlier suggestion.

 > a <- rbeta(100,0.1,0.1)
 > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))
Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
	Function cannot be evaluated at initial parameters
 >
 > range(a) - (0:1)
[1] 9.62532e-20 0.00000e+00
 > sum(a==1)
[1] 1
 >
 > a1 <- a
 > a1[a==0] <- .Machine$double.eps
 > a1[a==1] <- (1-.Machine$double.eps)
 > fitdistr(x=a1, "beta", start=list(shape1=0.1,shape2=0.1))
      shape1        shape2
   0.087937990   0.081524037
  (0.010950667) (0.009899447)

You could also program your own "densfun" argument to "fitdistr" for a 
mixture of a discrete distribution with point masses at 0 and 1 with the 
rest following a standard beta distribution.  However, I would not 
recommend that for what I understand of your application.

hope this helps.  spencer graves

alobo at ija.csic.es wrote:
> Thanks Prof. B. Ripley and
> Spencer Graves for your help.
> 
> According to the last message from 
> BR, perhaps I should use another
> distribution or combination
> of distributions to model my data,
> which look like a beta(0.1,0.1)
> but can have both 0 and 1. Any
> suggestion?
> 
> (regarding the extra "1)" in my
> original message, it was just a pasting
> problem.)
> 
> Agus
> 
> Mensaje citado por Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> 
> 
>>In this example shrinking by (1 - 2e-16) leads to a significant change in 
>>the distribution: see my probability calculation.  And you can't shrink by 
>>much less.  A beta(0.1, 0.1) is barely a continuous distribution.
>>
>>On Fri, 4 Jul 2003, Spencer Graves wrote:
>>
>>
>>>My standard work-around for the kind of problem you identified is to 
>>>shrink the numbers just a little towards 0.5.  For example:
>>>
>>> > library(MASS)
>>> > a <- rbeta(100,0.1,0.1)
>>> > fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))
>>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>>>	Function cannot be evaluated at initial parameters
>>>
>>> > r.a <- range(a)
>>> > c0 <- 0
>>> > c1 <- 1
>>> > if(r.a[1]==0)c0 <- min(a[a>0])
>>> > if(r.a[2]==1)c1 <- max(a[a<1])
>>> > c. <- c(c0, 1-c1)
>>> > if(any(c.>0))c. <- min(c.[c.>0])
>>> > c.
>>>[1] 2.509104e-14
>>> > fitdistr(x=0.5*c.[1] + (1-c.[1])*a, "beta", 
>>>start=list(shape1=0.1,shape2=0.1))
>>>      shape1       shape2
>>>   0.08728921   0.10403875
>>>  (0.01044863) (0.01320188)
>>> >
>>>hope this helps.  spencer graves
>>>
>>>Prof Brian Ripley wrote:
>>>
>>>>rbeta(100,0.1,0.1) is generating samples which contain 1, an impossible 
>>>>value for a beta and hence the sample has an infinite log-likelihood.
>>>>It is clearly documented on the help page that the range is 0 < x < 1.
>>>>However, that is not so surprising as P(X > 1-1e-16) is about 1% and
>>>
>>hence 
>>
>>>>values will get rounded to one.
>>>>
>>>>The same would happen for a value of 0.
>>>>
>>>>Your code is syntactically incorrect, at least as received here.
>>>>
>>>>On Fri, 4 Jul 2003, Agustin Lobo wrote:
>>>>
>>>>
>>>>
>>>>>I have the following problem:
>>>>>
>>>>>I have a vector x of data (0<x<=1 ) with
>>>>>a U-shaped histogram and try to fit a beta
>>>>>distribution using  fitdistr. In fact,
>>>>>hist(rbeta(100,0.1,0.1)) looks a lot like
>>>>>my data.
>>>>>
>>>>>The equivalent to
>>>>>the example in the manual
>>>>>sometimes work:
>>>>>
>>>>>
>>>>>
>>>>>>a <- rbeta(100,0.1,0.1)
>>>>>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
>>>>>>    shape1       shape2
>>>>>
>>>>> 0.09444627   0.12048753
>>>>>(0.01120670) (0.01550129)
>>>>>
>>>>>but sometimes does not:
>>>>>
>>>>>
>>>>>>a <- rbeta(100,0.1,0.1)
>>>>>>fitdistr(x=a, "beta", start=list(shape1=0.1,shape2=0.1))1)
>>>>>>Error in optim(start, mylogfn, x = x, hessian = TRUE, ...) :
>>>>>
>>>>>       Function cannot be evaluated at initial parameters
>>>>>
>>>>>Unfortunately, my data fall in the second case
>>>>>
>>>>>I've searched for any weird value that be present in the
>>>>>cases in which fitdistr exits with the error message, but
>>>>>could not find any.
>>>>>
>>>>>Any help?
>>>>>(please if anyone answers be sure to answer to my address as well,
>>>>>I cannot subscribe to the list)
>>>>>
>>>>>Thanks
>>>>>
>>>>>Agus
>>>>>
>>>>>Dr. Agustin Lobo
>>>>>Instituto de Ciencias de la Tierra (CSIC)
>>>>>Lluis Sole Sabaris s/n
>>>>>08028 Barcelona SPAIN
>>>>>tel 34 93409 5410
>>>>>fax 34 93411 0012
>>>>>alobo at ija.csic.es
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>>
>>>>
>>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>
>>-- 
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
> 
> 
> 
> 
> 
> -------------------------------------------------
> This mail sent through IMP: http://horde.org/imp/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Sun Jul  6 18:37:57 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 06 Jul 2003 17:37:57 +0100 (BST)
Subject: [R] Conditional Distribution of MVN variates
Message-ID: <XFMail.030706173757.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

Given k RVs with MVN distribution N(mu,S) (S a kxk covariance matrix),
let (w.l.o.g.) X1 denote the first r of them, and X2 the last (k-r).
Likewise, let mu1 and mu2 denote their respective expectations.

Then, of course, the expectation of X2 given X1=x1 is

  mu2 + S21*inv(S22)*(x1 - mu1)

and the covariance matrix of X2 given X1=x2 is

  S22 - S21*inv(X11)*S12

where Sij is the matrix derived from S by taking the rows corresponding
to the indices of Xi and the columns corresponding to the indices of Xj,
and these define the MVN conditional distribution of X2 given X1=x1.

While these are not difficult to write one's own functions for in R,
I feel almost sure that they already exist somewhere, if only I could
find them!

So, are there standard functions for this in R? If so, in what library?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 06-Jul-03                                       Time: 17:37:57
------------------------------ XFMail ------------------------------



From yanyu at cs.ucla.edu  Sun Jul  6 23:04:09 2003
From: yanyu at cs.ucla.edu (Yan Yu)
Date: Sun, 6 Jul 2003 14:04:09 -0700 (PDT)
Subject: [R] a Q re. read source files in spatial package.
Message-ID: <Pine.SOL.4.33.0307061350380.27810-100000@panther.cs.ucla.edu>

Hello,

I tried to read the source file related to variograms in the "spatial"
package.
I copied part of the source file of "spatial" package as follows,
in the follwing lines, I want to read "VR_variogram" function, from
wrapped in .C(), I suppose it is a C function..
I searched in various sub directories of R, but I could not seem to find
it..
DOes anyone know where can I find it?
thanks a lot in advance!
yan

-----------------------------------
    315 variogram <- function(krig, nint, plotit=TRUE, ...)
    316 {
    317   z <- .C("VR_variogram",
    318           xp = double(nint),
    319           yp = double(nint),
    320           nint = as.integer(nint),
    321           as.double(krig$x),
    322           as.double(krig$y),
    323           if(krig$np > 0) as.double(krig$wz) else
as.double(krig$z),
    324           as.integer(length(krig$x)),
    325           cnt = integer(nint),
    326           PACKAGE = "spatial"



From n.r.street at soton.ac.uk  Sun Jul  6 23:06:52 2003
From: n.r.street at soton.ac.uk (Nathaniel Street)
Date: Sun, 06 Jul 2003 22:06:52 +0100
Subject: [R] Printing multiple graphics devices
Message-ID: <3F088F6C.8050604@soton.ac.uk>

Hi

I'm using a function (arrayview from the maanova package) that produces 
12 plots, all in separate windows. How can I get each one to print to a 
jpeg file as the plots are produced? Alternatively, is it possible to 
get dev.print (jpeg.....) to print all open graphics devices to separate 
windows?

Thanks in advance,

Nat Street
-- 
Nathaniel Street
      University of Southampton
        Plants and Environment Lab
         School of Biological Sciences
     Basset Crescent East
    Southampton
       SO16 7PX
tel: +44 (0) 2380 594387
  fax: +44 (0) 2380 594269
          n.r.street at soton.ac.uk
     http://www.soton.ac.uk/~nrs298
    ICQ: 203465793



From eairoldi at stat.cmu.edu  Mon Jul  7 04:11:55 2003
From: eairoldi at stat.cmu.edu (Edoardo Airoldi)
Date: Mon, 07 Jul 2003 02:11:55 -0000
Subject: [R] 'postscript' command within a function
Message-ID: <E278F903-1F7F-11B2-899C-00039390FFC4@stat.cmu.edu>

hello all,
  I am trying to print a ps file as part of a function as in:

func <- function (..., filename="temp.ps") {
	# some stuff
	[...]

     # plot
     eval( cat("postscript(\"",filename,"\")\n", sep="") )
     plot(...)
     abline(...)
     dev.off()

	# more stuff
	[...]
}

but it does not work.  Nor it does with 'paste' instead of 'cat'.  In 
order to make it work I have to call:
 > postscript("temp.ps")
 > func(...)
 > dev.off()

I am wondering why is that?  How can I make my call to postscript within a 
function sort of 'global' ??
thanks
Edo



From Tom.Mulholland at health.wa.gov.au  Mon Jul  7 04:59:10 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Mon, 7 Jul 2003 10:59:10 +0800
Subject: [R] Generating a vector for breaks in a histogram
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FBA@nt207mesep.health.wa.gov.au>

My gut feeling is that stacked dotplots would have given you the same
insight. In general terms it's about getting the right tool for the
right job. My comment was about the order of choosing rather than
ignoring totally. If I recall correctly the article about dot plots was
about old fashioned hand drawn dot plots where dots were either stacked
above each other or if more appropriate next to each other as near as
possible to where they should be located on the axis. This results in a
pattern that looks very similar to the histogram. The argument being
made if I recall correctly is that if you choose the wrong bins for a
histogram you may well end up with the same type of result that you had
with the densityplot.

My practical way of looking at this is to look at what happens to the
overall shape of the histogram when you change the bins. The issue is
how quickly and reliably do you get to the "truth" using the various
techniques. As you've noted the density plot doesn't seem to deal with
some types of data as well as it does others. So when I am looking at
data I use a variety of methods, and histograms come later than rugplots
or density plots, but I tend to do both of those together.

I'm just learning and welcome guidance in a field that I do not claim
expertise in.

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
189 Royal St, East Perth, WA, 6004
 
Tel: (08) 9222 4062
e-mail: Tom.Mulholland at health.wa.gov.au
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From s195404 at student.uq.edu.au  Mon Jul  7 05:34:46 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon,  7 Jul 2003 03:34:46 +0000
Subject: [R] 'postscript' command within a function
In-Reply-To: <E278F903-1F7F-11B2-899C-00039390FFC4@stat.cmu.edu>
References: <E278F903-1F7F-11B2-899C-00039390FFC4@stat.cmu.edu>
Message-ID: <1057548886.3f08ea5669cee@my.uq.edu.au>

Edoardo,

I'm wondering why
   postscript(file=filename)
doesn't suffice, and you need to use "eval" instead?

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Edoardo Airoldi <eairoldi at stat.cmu.edu>:

> hello all,
>   I am trying to print a ps file as part of a function as in:
> 
> func <- function (..., filename="temp.ps") {
> 	# some stuff
> 	[...]
> 
>      # plot
>      eval( cat("postscript(\"",filename,"\")\n", sep="") )
>      plot(...)
>      abline(...)
>      dev.off()
> 
> 	# more stuff
> 	[...]
> }
> 
> but it does not work.  Nor it does with 'paste' instead of
> 'cat'.  In 
> order to make it work I have to call:
>  > postscript("temp.ps")
>  > func(...)
>  > dev.off()
> 
> I am wondering why is that?  How can I make my call to
> postscript within a 
> function sort of 'global' ??
> thanks
> Edo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From rob.hyndman at buseco.monash.edu.au  Mon Jul  7 05:49:01 2003
From: rob.hyndman at buseco.monash.edu.au (Rob Hyndman)
Date: Mon, 07 Jul 2003 13:49:01 +1000
Subject: [R] Problems with a dll under windows
Message-ID: <3F08EDAD.38C5B7F3@buseco.monash.edu.au>

I am trying to get a dll compiled for use with dyn.load. I use R.1.7.1
under Windows.

I have tried the following trivial example based on the "Writing R
extensions" manual.

rtest.h
--------
class X {
public:
    X ();
    ~X ();
    void Give7(double*);
};

class Y {
public: Y (); ~Y ();
};

rtest.cpp
---------
#include <iostream.h>
#include "rtest.h"

static Y y;


extern "C" {

    void X_main () {
            X x;
    }
}

X::X()   { std::cout << "construct X" << std::endl; }
X::~X()  { std::cout << "destruct X"  << std::endl; }
Y::Y()   { std::cout << "construct Y" << std::endl; }
Y::~Y()  { std::cout << "destruct Y"  << std::endl; }

//silly function to test returning a constant double
void X::Give7(double * a)   
{
    *a=7;
}
---- end rtest.cpp

Then I create the dll using the following DOS command
   Rcmd SHLIB rtest.cpp
(I have all the perl, MINGW and related stuff set up as
per the instructions at http://www.stats.ox.ac.uk/pub/Rtools/)

Then in R, I type the following.
> x <- 3
> dyn.load("rtest.dll")
So far, so good.

Then...PROBLEM 1
> .C("Give7",x)
Error in .C("Give7", x) : C/Fortran function name not in load table

How do you convince the compiler to use the name "Give7" as an entry
point?

And PROBLEM 2...
I checked the DLL and the names have been decorated.
If I use the decorated name I get the following:
> .C("_ZN1X5Give7EPd",x)
[[1]]
[1] 3

For some reason, it is not altering x as it should.

What am I doing wrong?

Thanks.
Rob Hyndman
___________________________________________________
Rob J Hyndman
Associate Professor & Director of Consulting
Department of Econometrics & Business Statistics
Monash University, VIC 3800, Australia.
http://www-personal.buseco.monash.edu.au/~hyndman/



From silika at access.unizh.ch  Sun Jul  6 22:25:46 2003
From: silika at access.unizh.ch (Silika Tereshchenko)
Date: Sun, 06 Jul 2003 22:25:46 +0200 (CEST)
Subject: [R] Help
Message-ID: <1057523146.3f0885cacc8ce@imp.access.unizh.ch>

Dear all,



I have the problem, please, help me. I tried to import the dataset from the SPSS
into the R. It is the panel (survey). The importing of the data show that my
variables are read as afactor in R. But I need the variable "income" as a
numeric one. I tried to use the following command:   

 for(i in
c("AGE99","AGE00","I99PTOT","I00PTOT","I99PTOTN","I00PTOTN","I99PTOTG","I00PTOTG"))
+ {df[[i]]<-as.numeric(df[[i]])}
 
and then I get the error message. 
Error in "[[<-.data.frame"(*tmp*, i, value = as.numeric(df[[i]])) : 
        replacement has 1 rows, data has 6335

What can I do? Could you please help me. I need it for my work.

Dear all,



I have the problem, please, help me. I tried to import the dataset from the SPSS
into the R. It is the panel (survey). The importing of the data show that my
variables are read as afactor in R. But I need the variable "income" as a
numeric one. I tried to use the following command:   

 for(i in
c("AGE99","AGE00","I99PTOT","I00PTOT","I99PTOTN","I00PTOTN","I99PTOTG","I00PTOTG"))
+ {df[[i]]<-as.numeric(df[[i]])}
 
and then I get the error message. 
Error in "[[<-.data.frame"(*tmp*, i, value = as.numeric(df[[i]])) : 
        replacement has 1 rows, data has 6335

What can I do? Could you please help me. I need it for my work.



From christoffer.tornoe at ferring.com  Mon Jul  7 10:16:30 2003
From: christoffer.tornoe at ferring.com (christoffer.tornoe@ferring.com)
Date: Mon, 7 Jul 2003 10:16:30 +0200 
Subject: [R] nlmeODE package combining NLME with a differential equation
	solve r
Message-ID: <924BFF6804049043980D90E6D47A4F212F864D@fedk0014.FERRINGWORLD.LOCAL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030707/9db28565/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jul  7 10:58:16 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Jul 2003 10:58:16 +0200
Subject: [R] Help
In-Reply-To: <1057523146.3f0885cacc8ce@imp.access.unizh.ch>
References: <1057523146.3f0885cacc8ce@imp.access.unizh.ch>
Message-ID: <3F093628.805@statistik.uni-dortmund.de>

Silika Tereshchenko wrote:
> Dear all,
> 
> 
> 
> I have the problem, please, help me. I tried to import the dataset from the SPSS
> into the R. It is the panel (survey). The importing of the data show that my
> variables are read as afactor in R. But I need the variable "income" as a
> numeric one. I tried to use the following command:   
> 
>  for(i in
> c("AGE99","AGE00","I99PTOT","I00PTOT","I99PTOTN","I00PTOTN","I99PTOTG","I00PTOTG"))
> + {df[[i]]<-as.numeric(df[[i]])}
>  
> and then I get the error message. 
> Error in "[[<-.data.frame"(*tmp*, i, value = as.numeric(df[[i]])) : 
>         replacement has 1 rows, data has 6335
> 
> What can I do? Could you please help me. I need it for my work.

Is df really a data.frame? Can you tell us about its structure using 
str(df)? What is your version of R?
Your statement works for me with a simple example of df[[i]].

Uwe Ligges



> Dear all,
> 
> 
> 
> I have the problem, please, help me. I tried to import the dataset from the SPSS
> into the R. It is the panel (survey). The importing of the data show that my
> variables are read as afactor in R. But I need the variable "income" as a
> numeric one. I tried to use the following command:   
> 
>  for(i in
> c("AGE99","AGE00","I99PTOT","I00PTOT","I99PTOTN","I00PTOTN","I99PTOTG","I00PTOTG"))
> + {df[[i]]<-as.numeric(df[[i]])}
>  
> and then I get the error message. 
> Error in "[[<-.data.frame"(*tmp*, i, value = as.numeric(df[[i]])) : 
>         replacement has 1 rows, data has 6335
> 
> What can I do? Could you please help me. I need it for my work.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Jul  7 11:11:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Jul 2003 11:11:01 +0200
Subject: [R] Printing multiple graphics devices
In-Reply-To: <3F088F6C.8050604@soton.ac.uk>
References: <3F088F6C.8050604@soton.ac.uk>
Message-ID: <3F093925.1090104@statistik.uni-dortmund.de>

Nathaniel Street wrote:

> Hi
> 
> I'm using a function (arrayview from the maanova package)

Presumably most of us don't know "the maanova" package, which is not on 
CRAN.


 > that produces
> 12 plots, all in separate windows. How can I get each one to print to a 
> jpeg file as the plots are produced? 

Well, in principle jpeg() and friends should work as is, given the 
function you are using doesn't print to an x11() or windows() device 
explicitly.


 > Alternatively, is it possible to
> get dev.print (jpeg.....) to print all open graphics devices to separate 
> windows?

I guess you are going to save all opened x11() devices into jpegs?
In that case just loop over the list of opened devices.

> Thanks in advance,
> 
> Nat Street


Uwe Ligges



From dmurdoch at pair.com  Mon Jul  7 12:53:28 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 07 Jul 2003 06:53:28 -0400
Subject: [R] Problems with a dll under windows
In-Reply-To: <3F08EDAD.38C5B7F3@buseco.monash.edu.au>
References: <3F08EDAD.38C5B7F3@buseco.monash.edu.au>
Message-ID: <mvjigvkrk0rasbkvqgjs44a77n3ntrq886@4ax.com>

On Mon, 07 Jul 2003 13:49:01 +1000, you wrote:

>//silly function to test returning a constant double
>void X::Give7(double * a)   
>{
>    *a=7;
>}
>---- end rtest.cpp
>
>Then I create the dll using the following DOS command
>   Rcmd SHLIB rtest.cpp
>(I have all the perl, MINGW and related stuff set up as
>per the instructions at http://www.stats.ox.ac.uk/pub/Rtools/)
>
>Then in R, I type the following.
>> x <- 3
>> dyn.load("rtest.dll")
>So far, so good.
>
>Then...PROBLEM 1
>> .C("Give7",x)
>Error in .C("Give7", x) : C/Fortran function name not in load table

A call to Give7 needs to pass both an instance of X and a.  You're
only passing a, because R doesn't know anything about instances of X.

Generally speaking, R can only call standard C functions, not C++
methods.  Your example seems to be based on the Writing R Extensions
section 4.6 example, but you miss the last part of it:  R can only
call X_main, not X::Give7.

Duncan Murdoch
>
>How do you convince the compiler to use the name "Give7" as an entry
>point?
>
>And PROBLEM 2...
>I checked the DLL and the names have been decorated.
>If I use the decorated name I get the following:
>> .C("_ZN1X5Give7EPd",x)
>[[1]]
>[1] 3
>
>For some reason, it is not altering x as it should.
>
>What am I doing wrong?
>
>Thanks.
>Rob Hyndman
>___________________________________________________
>Rob J Hyndman
>Associate Professor & Director of Consulting
>Department of Econometrics & Business Statistics
>Monash University, VIC 3800, Australia.
>http://www-personal.buseco.monash.edu.au/~hyndman/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul  7 13:36:33 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Jul 2003 12:36:33 +0100 (BST)
Subject: [R] Conditional Distribution of MVN variates
In-Reply-To: <XFMail.030706173757.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030707123633.Ted.Harding@nessie.mcc.ac.uk>

On 06-Jul-03 Ted Harding wrote:
> Given k RVs with MVN distribution N(mu,S) (S a kxk covariance matrix),
> let (w.l.o.g.) X1 denote the first r of them, and X2 the last (k-r).
> Likewise, let mu1 and mu2 denote their respective expectations.
> 
> Then, of course, the expectation of X2 given X1=x1 is
OOPS!! Typos:
>*** mu2 + S21*inv(S22)*(x1 - mu1)
should of course be

     mu2 + S21*inv(S11)*(x1 - mu1)

> and the covariance matrix of X2 given X1=x2 is
> 
>*** S22 - S21*inv(X11)*S12
should be

     S22 - S21*inv(S11)*S12

> [...]
> So, are there standard functions for this in R? If so, in what library?
> 
> With thanks,
> Ted.

Sorry about the typos.

Anyway, can R say "here's one I prepared earlier"?

Thanks again,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 07-Jul-03                                       Time: 12:36:33
------------------------------ XFMail ------------------------------



From joerg-burmester at gmx.net  Mon Jul  7 13:49:44 2003
From: joerg-burmester at gmx.net (joerg-burmester@gmx.net)
Date: Mon, 7 Jul 2003 13:49:44 +0200 (MEST)
Subject: [R] (no subject)
Message-ID: <17360.1057578584@www36.gmx.net>

hi,

can anyone help me about the simple data probability-value of the F-test,
respectively p-value, shown in the summary of a linear model?
somehow i can not retrieve this data out of R+ into my sql-dataset. i can
get all other data shown with the command: 
summary(speciallinearmodel) - will have a gui screen

Call:
lm(formula = y21 ~ x4 + x18 + x4:x18, data = clustertransformationstabelle)
Residuals:
Min       1Q   Median       3Q      Max 
-0.20915 -0.07354 -0.01823  0.04414  0.45191 
Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.003613   0.043142  -0.084 0.933450    
x4           0.142373   0.113036   1.260 0.211057    
x18          0.250261   0.127277   1.966 0.052314 .  
x4:x18       0.818618   0.218541   3.746 0.000315 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
Residual standard error: 0.1305 on 91 degrees of freedom
Multiple R-Squared: 0.7865,     Adjusted R-squared: 0.7795 
F-statistic: 111.7 on 3 and 91 DF,  p-value: < 2.2e-16 

but the desired p-value like in this example p-value: < 2.2e-16, i somehow
can not retrieve by any command like attributes(speciallinearmodel).
has anyone an idea to get this shown but not selectable respectively
exportable p-value?



From andy_liaw at merck.com  Mon Jul  7 14:03:43 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 07 Jul 2003 08:03:43 -0400
Subject: P-value for F from summary.lm (was RE: [R] (no subject))
Message-ID: <3A822319EB35174CA3714066D590DCD50205C811@usrymx25.merck.com>

[Please use the subject line!]

In the help page for summary.lm, the "Value" section says that the returned
object has a component called "fstatistic", which has the F-statistic and
the associated numerator and denominator degrees of freedom.  You can get
the p-value by something like:

  fstat <- summary(speciallinearmodel)$fstatistic
  pval <- pf(fstat[1], fstat[2], fstat[3], lower.tail=FALSE)

HTH,
Andy

> -----Original Message-----
> From: joerg-burmester at gmx.net [mailto:joerg-burmester at gmx.net] 
> Sent: Monday, July 07, 2003 7:50 AM
> To: R-help at stat.math.ethz.ch
> Cc: amaitour at pasteur.fr; bitwrit at ozemail.com.au; 
> p.dalgaard at biostat.ku.dk; gregor.gawron at rmf.ch; 
> mditzen at zedat.fu-berlin.de
> Subject: [R] (no subject)
> 
> 
> hi,
> 
> can anyone help me about the simple data probability-value of 
> the F-test, respectively p-value, shown in the summary of a 
> linear model? somehow i can not retrieve this data out of R+ 
> into my sql-dataset. i can get all other data shown with the command: 
> summary(speciallinearmodel) - will have a gui screen
> 
> Call:
> lm(formula = y21 ~ x4 + x18 + x4:x18, data = 
> clustertransformationstabelle)
> Residuals:
> Min       1Q   Median       3Q      Max 
> -0.20915 -0.07354 -0.01823  0.04414  0.45191 
> Coefficients:
> Estimate Std. Error t value Pr(>|t|)    
> (Intercept) -0.003613   0.043142  -0.084 0.933450    
> x4           0.142373   0.113036   1.260 0.211057    
> x18          0.250261   0.127277   1.966 0.052314 .  
> x4:x18       0.818618   0.218541   3.746 0.000315 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> Residual standard error: 0.1305 on 91 degrees of freedom
> Multiple R-Squared: 0.7865,     Adjusted R-squared: 0.7795 
> F-statistic: 111.7 on 3 and 91 DF,  p-value: < 2.2e-16 
> 
> but the desired p-value like in this example p-value: < 
> 2.2e-16, i somehow can not retrieve by any command like 
> attributes(speciallinearmodel). has anyone an idea to get 
> this shown but not selectable respectively exportable p-value?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From andy_liaw at merck.com  Mon Jul  7 14:24:12 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 07 Jul 2003 08:24:12 -0400
Subject: [R] the huge postscript plot
Message-ID: <3A822319EB35174CA3714066D590DCD50205C812@usrymx25.merck.com>

The sunflowerplot function in base, or the hexbin package in Bioconductor,
could possibly be useful for this.

Andy

> -----Original Message-----
> From: Yongchao Ge [mailto:gyc at stat.berkeley.edu] 
> Sent: Saturday, July 05, 2003 3:06 AM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] the huge postscript plot
> 
> 
> Hi Matt,
> 
> Thanks for the help. I'm just wondering if some package or 
> some options in par or ps.options exists so that I  can just 
> use it directly. As we know, we can not just randomly sample 
> 1000 of them as we may miss some important extreme points 
> (outliers). We need to define a small distance d, when a 
> clouds of points are within the distance of d, then we can 
> randomly sample a few points from each clouds so we  don't 
> lose any visual information. What it really needs is a fast 
> algorithm to group 60,000 points into 1000+ of clusters. Each 
> cluster's radius must be less than d.
> 
> We can surely work this way. Again, let's go back to my 
> original question, I'd like to find a fast way to plot the 
> points into a bitmapped (raster) format, say 600x800. This is 
> just an informal way of the sampling strategy defined in the 
> first graph and the radius d is defined by the dimension size 
> of the plot (600x800).  The larger dimension size, the 
> smaller the radius d is.
> 
> If such package or options exists in R, please let me know as 
> it can save me enormous of time for the programming.
> 
> Yongchao
> 
> 
> On Fri, 4 Jul 2003, Wiener, Matthew wrote:
> 
> > One possibility is to sample your 60,000 points, since you probably 
> > can't see them all distinctly anyway.  You could sample, 
> say, 10000 of 
> > them.
> >
> > Hope this helps,
> >
> > Matt Wiener
> >
> > -----Original Message-----
> > From: Yongchao Ge [mailto:gyc at stat.berkeley.edu]
> > Sent: Friday, July 04, 2003 7:06 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] the huge postscript plot
> >
> >
> > Hi,
> >
> > I'm just wondering how I can do to make a huge postscript plot be 
> > manageable. My question is that I have to draw around 60,000 points 
> > which makes it painfully slow to print or view in gv or put it into 
> > latex document, though it is very fast to produce the 
> postscript file.
> >
> > A simple example is in the attachment.
> >
> > Well, I found that if I use png or jpeg. It is much faster 
> to view the 
> > figure. The only problem is that the x label, y label are using 
> > bitmapped font so it doesn't look as beautiful as postscript. 
> > Sometimes it is just hard to read the legend which consists of 
> > information how to interpret the figure.
> >
> > My question is that if there are some options or packages such that 
> > when we are plotting many points or lines, the plot uses 
> the bitmapped 
> > (raster) format, but for the characters, like the x label, y label, 
> > and the title, it uses the native font.
> >
> > pdf seems use the above approach and is very fast, but I 
> have to use 
> > pdf2ps to convert the pdf file to ps file. It is still slow. If you 
> > know other good converter of pdf file to ps file, i will also very 
> > appreciate it.
> >
> >
> > Thanks,
> >
> > Yongchao
> >
> > x<-1:2000
> > y<-matrix(rnorm(2000*12),2000,12)
> > for(i in 1:12){
> >     y[,i]<-sort(y[,i]+i*0.5)
> > }
> > postscript("try.ps")
> > matplot(x,y,type="l",lwd=5)
> > dev.off()
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> > 
> ----------------------------------------------------------------------
> > --------
> > Notice: This e-mail message, together with any attachments, contains
> > information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
> > USA) that may be confidential, proprietary copyrighted 
> and/or legally
> > privileged, and is intended solely for the use of the 
> individual or entity
> > named on this message. If you are not the intended recipient, and
> > have received this message in error, please immediately 
> return this by
> > e-mail and then delete it.
> > 
> --------------------------------------------------------------
> ----------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From p.dalgaard at biostat.ku.dk  Mon Jul  7 14:51:56 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 07 Jul 2003 12:51:56 -0000
Subject: [R] Conditional Distribution of MVN variates
In-Reply-To: <XFMail.030707123633.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030707123633.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2vfueebvp.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 06-Jul-03 Ted Harding wrote:
> > Given k RVs with MVN distribution N(mu,S) (S a kxk covariance matrix),
> > let (w.l.o.g.) X1 denote the first r of them, and X2 the last (k-r).
> > Likewise, let mu1 and mu2 denote their respective expectations.
> > 
> > Then, of course, the expectation of X2 given X1=x1 is
> OOPS!! Typos:
> >*** mu2 + S21*inv(S22)*(x1 - mu1)
> should of course be
> 
>      mu2 + S21*inv(S11)*(x1 - mu1)
> 
> > and the covariance matrix of X2 given X1=x2 is
> > 
> >*** S22 - S21*inv(X11)*S12
> should be
> 
>      S22 - S21*inv(S11)*S12
> 
> > [...]
> > So, are there standard functions for this in R? If so, in what library?
> > 
> > With thanks,
> > Ted.
> 
> Sorry about the typos.

So it wasn't a typo that you wanted V(X2|X1=x2) but E(X2|X1=x1)?  >;-)

> Anyway, can R say "here's one I prepared earlier"?

Traditionally, this sort of thing has been handled using the SWEEP
operator (not to be confused with the sweep() function) although that
appears to have fallen from grace a bit, probably because the
numerical robustness is not all that good. I played with it back in
the stone age (1992, for a multivariate missing-data problem) and
still have these two functions in an S3 .Data dir:

> read.S(".Data/swp")
function (l = stop("Argument is missing"), G = stop("Argument is
missing"))
{
    for (i in l) G <- swp1(i, G)
}
> read.S(".Data/swp1")
function (k = stop("Argument is missing"), G = stop("Argument is
missing"))
{
    G[-k, -k] <- G[-k, -k] - G[-k, k] %o% G[k, -k]/G[k, k]
    G[-k, k] <- G[-k, k]/G[k, k]
    G[k, -k] <- G[k, -k]/G[k, k]
    G[k, k] <- -1/G[k, k]
    G
}

Check Little+Rubin for the exact details, but the gist of it is that
you sweep on the column numbers for X1 and after the sweep, the
X2 block is the conditional variance, the X1,X2 block has the
regression coefficients, and the X1 block has the inv(V(X1)) which you
use to get SE's of the regression coefficients. AFAIR, it works to
have G = C(X1,X2) although it is traditionally augmented with an extra
row/col containing the means of each variable and 1/n in the diagonal
element.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tanton at teknet.net.au  Mon Jul  7 14:54:33 2003
From: tanton at teknet.net.au (Robert Tanton)
Date: Mon, 7 Jul 2003 22:54:33 +1000
Subject: [R] Random effects in LME
Message-ID: <000401c34486$ea311ee0$0200a8c0@beth.bethandrob>

I have a model which has 3 levels (Year some indexes calculated (i); State
(j); and Year for which data were available (k)).

I want a random effect for the Year the Indexes are calculated within State
(i,j). I normally use MLWin, and its simple; I specify the three levels;
then introduce a constant, and in the drop down box for the constant, I have
3 check boxes; I click on j (State); and voila, a random effect u(ij). Note
this is the only random effect I want.

I haven't found how to do this in R. I have tried:

mlcgc1 <- lme(Factors ~ 1, data=cgc, random=~1+1+0|YOA/State/YOR)

but this introduces a random effect for all levels; I have tried

mlcgc2 <- lme(Factors ~ 1,data=cgc,random = list(YOA=~1,State=~1))

but this gives me a 2 level model. So I tried:

mlcgc3 <- lme(Factors ~ 1,data=cgc,random = list(YOA=~1,State=~1,YOR=~0))

but this still gives me random effects for all 3 levels.

Any ideas?

Thanks.

Robert Tanton.



From p.dalgaard at biostat.ku.dk  Mon Jul  7 14:58:44 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 07 Jul 2003 12:58:44 -0000
Subject: P-value for F from summary.lm (was RE: [R] (no subject))
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C811@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205C811@usrymx25.merck.com>
Message-ID: <x2r852ebl1.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> [Please use the subject line!]
> 
> In the help page for summary.lm, the "Value" section says that the returned
> object has a component called "fstatistic", which has the F-statistic and
> the associated numerator and denominator degrees of freedom.  You can get
> the p-value by something like:
> 
>   fstat <- summary(speciallinearmodel)$fstatistic
>   pval <- pf(fstat[1], fstat[2], fstat[3], lower.tail=FALSE)

Also, note that this gets printed using format.pval(), which avoids
misleading printouts if the p value underflows to zero:

> format.pval(0)
[1] "< 2.22e-16"


> > but the desired p-value like in this example p-value: < 
> > 2.2e-16, i somehow can not retrieve by any command like 
> > attributes(speciallinearmodel). has anyone an idea to get 
> > this shown but not selectable respectively exportable p-value?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul  7 15:38:32 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 07 Jul 2003 14:38:32 +0100 (BST)
Subject: [R] Conditional Distribution of MVN variates
In-Reply-To: <x2vfueebvp.fsf@biostat.ku.dk>
Message-ID: <XFMail.030707143832.Ted.Harding@nessie.mcc.ac.uk>

On 07-Jul-03 Peter Dalgaard BSA wrote:
> So it wasn't a typo that you wanted V(X2|X1=x2) but E(X2|X1=x1)?  >;-)

  OOPS <- function(n=61){if(n)print(c("OOPS!! ",OOPS(n-1)))}

Thanks, and also to J.R. Lockwood, for suggestions. In R, these things
can begin to look quite sophisticated!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 07-Jul-03                                       Time: 14:38:32
------------------------------ XFMail ------------------------------



From michael.watson at bbsrc.ac.uk  Mon Jul  7 16:12:08 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 7 Jul 2003 15:12:08 +0100 
Subject: [R] Xvfb and R
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00865@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

I have recently installed and implemented Xvfb (X virtual frame buffer) so that I can create jpegs using R over CGI (SUSE Linux 8.1 and Apache 1.3).

I have noticed that in order to do this, a file (Rplots.ps) is created in my cgi-bin directory everytime a cgi script is run.  This could cause problems though as I have a multi-user system where it is possible that two different users will run cgi scripts at the same time, and both will be writing/reading Rplots.ps at the same time - a conflict!

I am simply using the jpeg() function of R.  On a different server, where I'm not using Xvfb but the real X server, I do not get this fule Rplots.ps so I am guessing that there is some magic going on behind the scenes, though I may be wrong.

Can anyone explain what is happening, and if I can make R/Xvfb create a DIFFERENT file, say in /tmp/, with a unique name, everytime a cgi script is run?

Cheers
Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7764 490236
E-mail: michael.watson at bbsrc.ac.uk



From tlumley at u.washington.edu  Mon Jul  7 16:17:18 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 7 Jul 2003 07:17:18 -0700 (PDT)
Subject: [R] the huge postscript plot
In-Reply-To: <Pine.SOL.4.50.0307050004120.7891-100000@toto.Berkeley.EDU>
Message-ID: <Pine.A41.4.44.0307070716150.36720-100000@homer32.u.washington.edu>

On Sat, 5 Jul 2003, Yongchao Ge wrote:

> Hi Matt,
>
> Thanks for the help. I'm just wondering if some package or some
> options in par or ps.options exists so that I  can just use it directly.
> As we know, we can not just randomly sample 1000 of them as we may miss some
> important extreme points (outliers). We need to define a small distance
> d, when a clouds of points are within the distance of d,
> then we can randomly sample a few points from each clouds so we  don't
> lose any visual information. What it really needs is a fast algorithm
> to group 60,000 points into 1000+ of clusters. Each cluster's radius must
> be less than d.
>

I think the hexbin package (http://www.bioconductor.org) will do what you
want.

	-thomas



From michael.watson at bbsrc.ac.uk  Mon Jul  7 16:14:40 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 7 Jul 2003 15:14:40 +0100 
Subject: [R] RE: Xvfb and R
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00866@cl-exsrv1.irad.bbsrc.ac.uk>

Forgive that last mail, I have now realised it was all complete b*ll*cks ;-)

-----Original Message-----
From: michael watson (IAH-C) 
Sent: 07 July 2003 15:12
To: 'r-help at stat.math.ethz.ch'
Subject: Xvfb and R


Hi

I have recently installed and implemented Xvfb (X virtual frame buffer) so that I can create jpegs using R over CGI (SUSE Linux 8.1 and Apache 1.3).

I have noticed that in order to do this, a file (Rplots.ps) is created in my cgi-bin directory everytime a cgi script is run.  This could cause problems though as I have a multi-user system where it is possible that two different users will run cgi scripts at the same time, and both will be writing/reading Rplots.ps at the same time - a conflict!

I am simply using the jpeg() function of R.  On a different server, where I'm not using Xvfb but the real X server, I do not get this fule Rplots.ps so I am guessing that there is some magic going on behind the scenes, though I may be wrong.

Can anyone explain what is happening, and if I can make R/Xvfb create a DIFFERENT file, say in /tmp/, with a unique name, everytime a cgi script is run?

Cheers
Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7764 490236
E-mail: michael.watson at bbsrc.ac.uk



From carlos at zanaca.com  Mon Jul  7 16:24:04 2003
From: carlos at zanaca.com (Carlos Rios)
Date: Mon, 7 Jul 2003 11:24:04 -0300
Subject: [R] RMySQL in Windows
References: <200307071002.h67A2EUw015062@stat.math.ethz.ch>
Message-ID: <01a401c34493$6f0cb190$0a01010a@suporte1>

Hi Folks,
  I'm having some problens using the RMySQL under R 1.7.0 (1.7.1) for
Windows. Actualy, It's not available for download at Packages > Install
packages from CRAN. I searched and download a version, but I just can't
install it.
  I used tom use rmysql with an old version of R, and I remember it was
"suported" by CRAN. I have already downloaded the DBI package.
  Anyone can help me talk to MySQL?

[]'s
  Carlos Rios
  www.zanaca.com
  http://www.ence.ibge.gov.br



From tpaska2000 at yahoo.com  Mon Jul  7 16:45:50 2003
From: tpaska2000 at yahoo.com (Tom Paska)
Date: Mon, 7 Jul 2003 07:45:50 -0700 (PDT)
Subject: [R] baselining nnet
Message-ID: <20030707144550.51786.qmail@web41002.mail.yahoo.com>

I'm having some trouble with the nnet.  Everytime I
run it, it comes up with slightly different answers. 
I assume this is due to using a some sort of random
seed to calculate the weights.  Doesn't anyone have
any examples of how to set these weights so that you
can get consistent results?

Thanks,
Tom Paska



From spencer.graves at pdf.com  Mon Jul  7 17:46:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Jul 2003 08:46:23 -0700
Subject: [R] baselining nnet
References: <20030707144550.51786.qmail@web41002.mail.yahoo.com>
Message-ID: <3F0995CF.3010504@pdf.com>

If "nnet" uses standard R random number generators, then calling 
"set.seed" immediately before "nnet" should produce consistent results.

hope this helps.  spencer graves

Tom Paska wrote:
> I'm having some trouble with the nnet.  Everytime I
> run it, it comes up with slightly different answers. 
> I assume this is due to using a some sort of random
> seed to calculate the weights.  Doesn't anyone have
> any examples of how to set these weights so that you
> can get consistent results?
> 
> Thanks,
> Tom Paska
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Jul  7 17:51:54 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Jul 2003 17:51:54 +0200
Subject: [R] RMySQL in Windows
References: <200307071002.h67A2EUw015062@stat.math.ethz.ch>
	<01a401c34493$6f0cb190$0a01010a@suporte1>
Message-ID: <3F09971A.B236E28@statistik.uni-dortmund.de>



Carlos Rios wrote:
> 
> Hi Folks,
>   I'm having some problens using the RMySQL under R 1.7.0 (1.7.1) for
> Windows. Actualy, It's not available for download at Packages > Install
> packages from CRAN. I searched and download a version, but I just can't
> install it.
>   I used tom use rmysql with an old version of R, and I remember it was
> "suported" by CRAN. I have already downloaded the DBI package.
>   Anyone can help me talk to MySQL?

Binary packages for Windows were never supported, but provided, AFAIK.
Since there are some prerequisites for RMySQL and some version specific
things going on related to your MySQL installation, we do not publish a
binary version. It seems to be "dangerous" in a way to do so. Please
compile it from sources yourself and read the RMySQL docs before doing
so.

Uwe Ligges

> []'s
>   Carlos Rios
>   www.zanaca.com
>   http://www.ence.ibge.gov.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Jul  7 17:57:57 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 07 Jul 2003 17:57:57 +0200
Subject: [R] baselining nnet
References: <20030707144550.51786.qmail@web41002.mail.yahoo.com>
Message-ID: <3F099885.B78751A5@statistik.uni-dortmund.de>



Tom Paska wrote:
> 
> I'm having some trouble with the nnet.  Everytime I
> run it, it comes up with slightly different answers.
> I assume this is due to using a some sort of random
> seed to calculate the weights.  Doesn't anyone have
> any examples of how to set these weights so that you
> can get consistent results?

I guess you have already read the references given in ?nnet:
Ripley, B. D. (1996) Pattern Recognition and Neural Networks. Cambridge.
Venables, W. N. and Ripley, B. D. (2002) Modern Applied Statistics with
S. Fourth edition.  Springer.

If so, what about specifying some more details so that people can help?
It's also a good idea to ask the package authors / maintainers for help,
since he / she regularly knows the answer.

Uwe Ligges



From csillery at selway.umt.edu  Mon Jul  7 18:58:37 2003
From: csillery at selway.umt.edu (Katalin Csillery)
Date: Mon, 7 Jul 2003 10:58:37 -0600 (MDT)
Subject: [R] error with methods on R-1.7.0
Message-ID: <Pine.OSF.4.21.0307071052560.26988-100000@selway.umt.edu>


Dear All,

I just upgraded for R-1.7.0 and it starts with the error message:
Error in assign("__MethodMetaData", table, envir = where) : 
        unused argument(s) ( ...)

typing
> library(methods)
Error in assign("__MethodMetaData", table, envir = where) : 
        unused argument(s) ( ...)
Error in library(methods) : .First.lib failed

Any idea how to fix it?
machine: mac os x 10.2.4
upgrade from fink tree

Thanks! Katalin

___
Katalin Csillery
Division of Biological Sciences
University of Montana, Missoula MT 59801
Phone: 406 243 6106, E-mail: csillery at selway.umt.edu



From rpeng at stat.ucla.edu  Mon Jul  7 20:58:59 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 07 Jul 2003 11:58:59 -0700
Subject: [R] error with methods on R-1.7.0
In-Reply-To: <Pine.OSF.4.21.0307071052560.26988-100000@selway.umt.edu>
References: <Pine.OSF.4.21.0307071052560.26988-100000@selway.umt.edu>
Message-ID: <3F09C2F3.5030702@stat.ucla.edu>

Does this happen when you do R --vanilla?  You might be loading an 
outdated package on startup which needs to be reinstalled.

-roger

Katalin Csillery wrote:
> Dear All,
> 
> I just upgraded for R-1.7.0 and it starts with the error message:
> Error in assign("__MethodMetaData", table, envir = where) : 
>         unused argument(s) ( ...)
> 
> typing
> 
>>library(methods)
> 
> Error in assign("__MethodMetaData", table, envir = where) : 
>         unused argument(s) ( ...)
> Error in library(methods) : .First.lib failed
> 
> Any idea how to fix it?
> machine: mac os x 10.2.4
> upgrade from fink tree
> 
> Thanks! Katalin
> 
> ___
> Katalin Csillery
> Division of Biological Sciences
> University of Montana, Missoula MT 59801
> Phone: 406 243 6106, E-mail: csillery at selway.umt.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From hanxianglu at yahoo.com  Tue Jul  8 05:01:15 2003
From: hanxianglu at yahoo.com (Hanhan)
Date: Mon, 7 Jul 2003 20:01:15 -0700 (PDT)
Subject: [R] Questions about corARMA
Message-ID: <20030708030115.23545.qmail@web10602.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030707/6393836e/attachment.pl

From eiji at atr.co.jp  Wed Jul  9 00:37:45 2003
From: eiji at atr.co.jp (Norberto Eiji Nawa)
Date: Tue, 08 Jul 2003 13:37:45 -0900 (GMT+9)
Subject: [R] Problem with package hdf5-1.4.7 in R-1.7.1
Message-ID: <20030708.133745.106276841.eiji@atr.co.jp>

Hello:

Has anyone successfully installed the package hdf5-1.4.7 in R version
1.7.1? I get a fatal error with a core dump (see below) whenever I try
to use hdf5load.

Since I'm in the process of moving to a new machine, I'm not sure
whether this is due to (my installations of) the HDF5 libs or the hdf5
package for R. Just for the record, the HDF5 data files were correctly
generated in the new machine (therefore, I assume the HDF5 libs are
good at least for outputting data). I could read them in my old
machine (R version 1.2.2).

Thanks for any help!

Eiji

ps: Just in case, I left the test data file I used at
http://www.his.atr.co.jp/~eiji/R/ts3n0.hdf (240kB) 

--error log--

> library(hdf5)
> hdf5load("ts3n0.hdf")
R.bin: H5T.c:5729: H5T_set_size: Assertion `"not implemented yet" && 0' failed.



From mrennie at utm.utoronto.ca  Tue Jul  8 06:46:03 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Tue,  8 Jul 2003 00:46:03 -0400
Subject: [R] specifying multiple parameter starting values in nlm
Message-ID: <1057639563.3f0a4c8b1118b@webmail.utm.utoronto.ca>


Hi there

I am having trouble figuring out how to get an nlm function to report estimates 
for two parameter values in an estimation.

The way I've got it goes something like this:

f <- function (q, r)
{

here, I have a second loop which uses q, r to give me values for c, d below.  a 
and b are already specified; this loop is a mass-balance function where I am 
trying to find values of q, r to give me c and d. I want c and d (mass-balance 
model outputs) to approximate a and b (known endpoints that I want the model to 
attain), respecitvely.  Thus, the function I want to minimize is:

fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
}
nlm (f, 1, r=1)

This doesn't return estimates for each parameter, and I get error messages 
saying something like "repleaced missing value with maximum" or something.  All 
I did here was try (and fail, obviously) to model my needs against the examples 
given in help(nlm), but they are using vectors as inputs, and I only need 2 
values input, and 2 returned.  

I've also tried specifying the function as 

f <- function (q)
{
mass-balance loop
}
fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
nlm(f, c(1,1))

Specifying 1 as the starting values, and coding q[1] and q[2] in my mass 
balance loop instead of q, r.

But this doesn't work either, and I get messages that I think are indicating 
that my mass balance loop isn't computing properly ("number of items to replace 
is not a multiple of replacement length"- an error I don't get when I run the 
mass-balance loop outside the nlm function and just specify q, r).

Does anyone know how to specify two parameters in the function, and ask for 
estimates of both back from nlm? Alternatively, any hunches on how to make this 
thing do what I want it to?  

Thanks, 

Mike


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From spencer.graves at pdf.com  Tue Jul  8 09:28:40 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jul 2003 00:28:40 -0700
Subject: [R] specifying multiple parameter starting values in nlm
References: <1057639563.3f0a4c8b1118b@webmail.utm.utoronto.ca>
Message-ID: <3F0A72A8.6090302@pdf.com>

	  Did you work through the examples at the end of the "nlm" 
documentation?  The first example there shows that "nlm" expects a 
single vector argument over which f is to be minimized.  This suggests 
that your first construction should not work, while your second should 
be closer.

	  Also, did you try running your function by itself, checking to make 
sure it computed correctly?  The final line for the second function as I 
read it below appears OUTSIDE the function definition as a stand-alone 
expression to be executed once between the function definition and the 
"nlm" call.

	  Also, have you tried "optim"?  It seems to be more general and 
robust, requiring fewer assumptions to function.  The default method for 
"optim" does not require the function to be differentiable, while "nlm" 
does.

hope this helps.  spencer graves

Michael Rennie wrote:
> Hi there
> 
> I am having trouble figuring out how to get an nlm function to report estimates 
> for two parameter values in an estimation.
> 
> The way I've got it goes something like this:
> 
> f <- function (q, r)
> {
> 
> here, I have a second loop which uses q, r to give me values for c, d below.  a 
> and b are already specified; this loop is a mass-balance function where I am 
> trying to find values of q, r to give me c and d. I want c and d (mass-balance 
> model outputs) to approximate a and b (known endpoints that I want the model to 
> attain), respecitvely.  Thus, the function I want to minimize is:
> 
> fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
> }
> nlm (f, 1, r=1)
> 
> This doesn't return estimates for each parameter, and I get error messages 
> saying something like "repleaced missing value with maximum" or something.  All 
> I did here was try (and fail, obviously) to model my needs against the examples 
> given in help(nlm), but they are using vectors as inputs, and I only need 2 
> values input, and 2 returned.  
> 
> I've also tried specifying the function as 
> 
> f <- function (q)
> {
> mass-balance loop
> }
> fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
> nlm(f, c(1,1))
> 
> Specifying 1 as the starting values, and coding q[1] and q[2] in my mass 
> balance loop instead of q, r.
> 
> But this doesn't work either, and I get messages that I think are indicating 
> that my mass balance loop isn't computing properly ("number of items to replace 
> is not a multiple of replacement length"- an error I don't get when I run the 
> mass-balance loop outside the nlm function and just specify q, r).
> 
> Does anyone know how to specify two parameters in the function, and ask for 
> estimates of both back from nlm? Alternatively, any hunches on how to make this 
> thing do what I want it to?  
> 
> Thanks, 
> 
> Mike
> 
>



From spencer.graves at pdf.com  Tue Jul  8 09:51:17 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jul 2003 00:51:17 -0700
Subject: [R] Questions about corARMA
References: <20030708030115.23545.qmail@web10602.mail.yahoo.com>
Message-ID: <3F0A77F5.6030108@pdf.com>

	  a.  Have you studied "Mixed Effects Models in S and S-Plus" by Jose 
C. Pinheiro, Douglas M. Bates (2000;  Springer)?  That book contains 
examples that might answer your question.

	  b.  Since you wrote "y(t)" and you say you are new to this list, I 
feel compelled to confirm that you know that parentheses "(..)" signal a 
function call in S-Plus syntax.  I would write your expression "y[t] = 
0.03*x1[t]+1.5*x2[t]", etc.  (And I would avoid "t", because that is is 
the R function for matrix transpose.)

	  c.  I wouldn't try "stepAIC" with "lme" until I was reasonably 
confident of the "lme" noise model.

	  d.  From a pragmatic perspective, I would not play with the 
correlation structure until I felt I had a reasonable model for the 
fixed effects.  The reason is simple:  Lack of fit can mascarade as a 
virtually nonstationary process, even if it is just something 
deterministic plus a small amouth of independent noise.  For example, if 
x is slowly moving like, "x <- 1:99" or "x <- sin((1:99)/(20*pi))", then 
"y <- x + 0.01*rnorm(99)" fit without "x" will appear as nonstationary.

	  e.  To understand autocorrelation structure (though not the "lme" 
function), I have gotten a lot from the book by Box, Jenkins and Reinsel 
(1994) Time Series Analysis: Forecasting & Control, 3rd Edition 
(Prentice Hall).

hope this helps.  spencer graves

Hanhan wrote:
> Hi,
> I'm a new member here in the list. I am a graduate from 
University of Georgia. Recently in doing analysis using lme
on a dataset, I found several questions:
> 1. How to express the equation when the correlation 
structure is very complicated. For exmaple, if the fixed
is y(t)=0.03x1(t)+1.5x2(t)(I omitted "hat" and others). And
the model with corARMA(p=2,q=3) is proper. What will be the
complete equation?
> 2. Is is that any regression error will be stationary? 
(Forgive me for my poor math background. This may be a simple
question to most people.) Since corARIMA is not available.
> 3. Why not make a function to automatically select the best 
corARMA structure (setting max p and q and the computer takes
care of the rest)?
> 4. When the initial model (without considering correlation 
structure) has many variables and some have no significance,
should I use stepAIC first to eliminate some variables? Or,
try corARMA with different combination of p and q, which may
make more variables significant without having to reduce
variable. I prefer the latter.
> 5. If the best corARMA model out of a model still contain 
some insignificant variables, I would use drop1 (instead of
stepAIC) and then try all the possible corARMA structures
again. So my steps would be drop1, corARMA, drop1,corARMA,
til I get a model with all variables significant. If the
initial model has many variables, it would be a time-consuming
process. Is it proper to do so? If proper, it'll be wonderful
if a new function is developed to automatically do so.
> Thanks, Hanhan
> 
> 
> 
> Xianglu Han
> 
> 206 Environmental Health Science
> 
> University of Georgia 30602
> 
> Phone: 706 255 2308
> 
>  
> 
> 
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Rau at demogr.mpg.de  Tue Jul  8 11:13:28 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 8 Jul 2003 11:13:28 +0200 
Subject: [R] Characters and Numeric Values in One Matrix
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8114CBAF@hermes.demogr.mpg.de>

Dear R-Users,

I want to ask a question for a colleague of mine. He wants to put a
character vector and a numeric vector into one matrix and still have the old
character and numeric type for the respective columns.
Unfortunately, I am just starting using R and I could not help him.
Is there an easy and straightforward way to do this in R?
	
Maybe a little example facilitates understanding our problem:
names <- c("Marge", "Lisa", "Homer", "Bart", "Maggie")
ages <- c(38,10,41,8,1)
Now he wants to have 2 columns in a matrix which should look like this:
"Marge"     38
"Lisa"      10
"Homer"     41
"Bart"       8
"Maggie"     1

I thought about using either:
family1 <- matrix(c(names, ages), ncol=2, byrow=FALSE)
or
family2 <- data.frame(names,ages)
but this simply transformed either the numeric into character values
(family1) or the character values into factor levels (family2)
Anyone here who can give us some advice on this?

We are using R 1.7.0 on Windows NT.

Thanks,
Roland



From th50 at leicester.ac.uk  Tue Jul  8 11:30:55 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 8 Jul 2003 10:30:55 +0100
Subject: [R] Characters and Numeric Values in One Matrix
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46D1@saffron.cfs.le.ac.uk>

Dear Roland,

Matrices can only be made of values of the same type, so you can't them here.

data.frame is the right track. ?data.frame states:

     Character variables passed to `data.frame' are converted to factor
     columns unless protected by `I'. It also applies to adding columns
     to a data frame.

So, try 

> family2 <- data.frame(I(names),ages)
> family2$names
[1] "Marge"  "Lisa"   "Homer"  "Bart"   "Maggie"
> is.factor(family2$names)
[1] FALSE

HTH,

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Rau, Roland [mailto:Rau at demogr.mpg.de]
> Sent: 08 July 2003 10:13
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Characters and Numeric Values in One Matrix
> 
> 
> Dear R-Users,
> 
> I want to ask a question for a colleague of mine. He wants to put a
> character vector and a numeric vector into one matrix and 
> still have the old
> character and numeric type for the respective columns.
> Unfortunately, I am just starting using R and I could not help him.
> Is there an easy and straightforward way to do this in R?
> 	
> Maybe a little example facilitates understanding our problem:
> names <- c("Marge", "Lisa", "Homer", "Bart", "Maggie")
> ages <- c(38,10,41,8,1)
> Now he wants to have 2 columns in a matrix which should look 
> like this:
> "Marge"     38
> "Lisa"      10
> "Homer"     41
> "Bart"       8
> "Maggie"     1
> 
> I thought about using either:
> family1 <- matrix(c(names, ages), ncol=2, byrow=FALSE)
> or
> family2 <- data.frame(names,ages)
> but this simply transformed either the numeric into character values
> (family1) or the character values into factor levels (family2)
> Anyone here who can give us some advice on this?
> 
> We are using R 1.7.0 on Windows NT.
> 
> Thanks,
> Roland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From gavin.simpson at ucl.ac.uk  Tue Jul  8 11:32:07 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 08 Jul 2003 10:32:07 +0100
Subject: [R] Characters and Numeric Values in One Matrix
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8114CBAF@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8114CBAF@hermes.demogr.mpg.de>
Message-ID: <3F0A8F97.7020605@ucl.ac.uk>

Hi Roland,

Try the following

 > names <- c("Marge", "Lisa", "Homer", "Bart", "Maggie")
 > ages <- c(38,10,41,8,1)
 > simpsons <- data.frame(I(names), ages)
 > simpsons
    names ages
1  Marge   38
2   Lisa   10
3  Homer   41
4   Bart    8
5 Maggie    1
 > str(simpsons)
`data.frame':   5 obs. of  2 variables:
  $ names:Class 'AsIs'  chr [1:5] "Marge" "Lisa" "Homer" "Bart" ...
  $ ages : num  38 10 41 8 1

Note the use of I() to protect names from the implicit conversion from 
character to factor that data.frame() does.

See ?data.frame for more.

HTH

Gav

Rau, Roland wrote:

> Dear R-Users,
> 
> I want to ask a question for a colleague of mine. He wants to put a
> character vector and a numeric vector into one matrix and still have the old
> character and numeric type for the respective columns.
> Unfortunately, I am just starting using R and I could not help him.
> Is there an easy and straightforward way to do this in R?
> 	
> Maybe a little example facilitates understanding our problem:
> names <- c("Marge", "Lisa", "Homer", "Bart", "Maggie")
> ages <- c(38,10,41,8,1)
> Now he wants to have 2 columns in a matrix which should look like this:
> "Marge"     38
> "Lisa"      10
> "Homer"     41
> "Bart"       8
> "Maggie"     1
> 
> I thought about using either:
> family1 <- matrix(c(names, ages), ncol=2, byrow=FALSE)
> or
> family2 <- data.frame(names,ages)
> but this simply transformed either the numeric into character values
> (family1) or the character values into factor levels (family2)
> Anyone here who can give us some advice on this?
> 
> We are using R 1.7.0 on Windows NT.
> 
> Thanks,
> Roland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Rau at demogr.mpg.de  Tue Jul  8 11:56:36 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 8 Jul 2003 11:56:36 +0200 
Subject: [R] Characters and Numeric Values in One Matrix
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8114CBB3@hermes.demogr.mpg.de>

Thank you very much for the advice using 
data.frame(I(names), ages)
I received two solutions within 15 minutes of my initial request.
What a quick and nice counterexample for the often heard claim: "Free
Software does not give you any support!"

Thanks again,
Roland



From mrufino at cmima.csic.es  Tue Jul  8 12:22:46 2003
From: mrufino at cmima.csic.es (Marta Rufino)
Date: Tue, 08 Jul 2003 12:22:46 +0200
Subject: [R] Lattice graphs: lines, symbols and strips
Message-ID: <3.0.1.32.20030708122246.008fdaf0@cucafera.icm.csic.es>

Dear collegue,

I have some questions about lattice graphs:

1. How can I introduce different lines in different graphs, from the same
page? For example, in the upper row of graphs, I would like to have a line
at 0.5, in the middle one at 0.3, and in the lower at 0.1. I am using
panel.abline, but I can only make it appear in all the graphs, not in some
of tham only...

2. Is it possible that the "strips" only appear in the top of the graphics,
once per column? and once per row? This would be very nice, because it
could save lots of graph space...

3. How can I make that the symbol within the graph varies with two
variables, for example when it is good I want to put a open circle and when
it is bad a closed circle or for example, that each point gets the number
of samples..., but this changes throughtout the graphs...

4. I have two series of points, and would like tham to be separated, not
overlaped. I tried jitter, but it did not worked (I am using xyplot).

Could you please help me?
Thank you very much in advance,
Marta

    

><((((?>`?.??.???`?.?.???`?...?><((((?>`?.??.???`?.?.???`?...?><((((?>
`?.??.???`?.?.???`?...?><((((?>`?.??.???`?.?.???`?...?><((((?>`?.??.??

Marta Rufino

Institut de Ciencies del Mar
Centre Mediterrani d'Investigacions Marines i Ambientals (CSIC)
Passeig Maritim 37-49
08003  Barcelona
Catalunya, Spain

Tfno:00 34 93 230 95 40
Tfax:00 34 93 230 95 55

><((((?>`?.??.???`?.?.???`?...?><((((?>`?.??.???`?.?.???`?...?><((((?>
`?.??.???`?.?.???`?...?><((((?>`?.??.???`?.?.???`?...?><((((?>`?.??.??



From djw1005 at cam.ac.uk  Tue Jul  8 15:07:02 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Tue, 8 Jul 2003 14:07:02 +0100 (BST)
Subject: [R] Lattice graphs: lines, symbols and strips
In-Reply-To: <3.0.1.32.20030708122246.008fdaf0@cucafera.icm.csic.es>
Message-ID: <Pine.SOL.3.96.1030708135330.12979B-100000@draco.cus.cam.ac.uk>


> 3. How can I make that the symbol within the graph varies with two
> variables, for example when it is good I want to put a open circle and when
> it is bad a closed circle or for example, that each point gets the number
> of samples..., but this changes throughtout the graphs...

It's not clear to me what you mean. Perhaps you can achieve something like
this with groups and subscripts. The subscripts argument to the panel (or
panel.groups) function says which of the points are about to be plotted.
For example,

xyplot(y~x, groups=g, 
  panel.groups=function(x,y,groups,subscripts,pch,...) {
    currentGroup=unique(groups[subscripts])
    cat(paste("Drawing group",currentGroup,"\n"))
    pch = ifelse(currentGroup=="good","+","*")
    panel.xyplot(x,y,pch=pch,...)
    })

You should define the groups parameter g beforehand, to specify for each
point whether it is "good" or "bad".

> 1. How can I introduce different lines in different graphs, from the same
> page? For example, in the upper row of graphs, I would like to have a line
> at 0.5, in the middle one at 0.3, and in the lower at 0.1. I am using
> panel.abline, but I can only make it appear in all the graphs, not in some
> of tham only...

The subscripts argument can help again here.

df <- data.frame(x=rnorm(10),y=rnorm(10),p=1+rbinom(10,2,.6))
liney <- c(.5,.3,.1)

xyplot(y~x|p,
       panel=function(x,y,subscripts,...) {
         currentPanel = unique(df$p[subscripts])
         cat(paste("Drawing panel",currentPanel,"\n"))
         panel.superpose(x,y,subscripts=subscripts,...)
         panel.abline(h=liney[currentPanel])
         })

I've referred back to the original data frame, so that from the subscripts
argument I can work out which values of the panel variable I'm plotting.
(There is also a panel.number argument, but I don't like to use it,
because it's hard to remember how it relates to the value of the panel
variable.)

> 4. I have two series of points, and would like tham to be separated, not
> overlaped. I tried jitter, but it did not worked (I am using xyplot).

Something like 
  xyplot(jitter(y)~jitter(x))

> 2. Is it possible that the "strips" only appear in the top of the graphics,
> once per column? and once per row? This would be very nice, because it
> could save lots of graph space...

I'd like to find this out, too.

Damon.



From jeaneid at chass.utoronto.ca  Tue Jul  8 15:45:20 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 8 Jul 2003 09:45:20 -0400
Subject: [R] readline and R 
Message-ID: <002201c34557$2c9d6650$8c7ba8c0@Scorpion>

Dear R users,

I am trying to install R 1.7.1 on a sparc-sun-solaris2.8 system. although
gnu readline is installed and works fine on the parent directory, R is not
recognizing it.
I get the following in the config.log file.

    configure:11627: checking for rl_callback_read_char in -lreadline
    configure:11658: gcc -o conftest -g -O2  -L/usr/local/lib
conftest.c -lreadline  -ldl -ltermcap -lm  >&5
    ld: fatal: library -lreadline: not found
    ld: fatal: File processing errors. No output written to conftest
    collect2: ld returned 1 exit status
I also tried to install the most recent version of readline (version 4.3)
with no luck. as I do not have root access to the system I have included the
readline directory in my home path.
I beleive that I have to change the path of the readline library from
usr/local/lib to my own which is on /local/R/readline, however I do not want
to change the configure file without making sure I am doing the right thing,
Any help is appreciated,


Jean Eid



From Laurie.Sindlinger at noaa.gov  Tue Jul  8 15:46:25 2003
From: Laurie.Sindlinger at noaa.gov (Laurie Sindlinger)
Date: Tue, 08 Jul 2003 09:46:25 -0400
Subject: [R] Not able to save .Rhistory
Message-ID: <3F0ACB31.1010700@noaa.gov>

Hello all,

    I am an R beginner, and I have not been able to save my history when 
using R. When I try to use the command that is shown in the help pages:

savehistory(file = ".Rhistory")

I receive an error message:

Error in savehistory(file) : no history available to save

Do any of you have suggestions on what the problem may be? Are there a 
minimum number of lines that can be saved? A file called .Rhistory is 
created in my working directory, but there is nothing inside of this 
file. This is the set-up that I am using:

platform   sparc-sun-solaris2.9
arch         sparc
os            solaris2.9
system     sparc,solaris2.9
status
major       1
minor       7.0
year         2003
month       04
day           16
language    R

Thanks so much for your time!
Laurie Sindlinger



From andel at ifi.unizh.ch  Tue Jul  8 15:58:41 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Tue, 08 Jul 2003 13:58:41 -0000
Subject: [R] rbind question
Message-ID: <20030708155825.1.20977.qmail@ifi.unizh.ch>

Hi

I am trying to replicate a vector in n rows for comparison purposes with 
another matrix.

foo <- c(1,2,3)
bar <- rbind(foo,foo) # does the trick for 2 rows
bar <- rbind(rep(foo,2)) # does something else

How do I generate a matrix with all rows=foo without writing 'foo' n times as 
arg?

Thanks,
David



From andy_liaw at merck.com  Tue Jul  8 16:07:18 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 08 Jul 2003 10:07:18 -0400
Subject: [R] rbind question
Message-ID: <3A822319EB35174CA3714066D590DCD50205C824@usrymx25.merck.com>

Using the recyling rule:

bar <- matrix(foo, nrow=n, ncol=length(foo), byrow=TRUE)

HTH,
Andy

> -----Original Message-----
> From: David Andel [mailto:andel at ifi.unizh.ch] 
> Sent: Tuesday, July 08, 2003 9:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rbind question
> 
> 
> Hi
> 
> I am trying to replicate a vector in n rows for comparison 
> purposes with 
> another matrix.
> 
> foo <- c(1,2,3)
> bar <- rbind(foo,foo) # does the trick for 2 rows
> bar <- rbind(rep(foo,2)) # does something else
> 
> How do I generate a matrix with all rows=foo without writing 
> 'foo' n times as 
> arg?
> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From JonesW at kssg.com  Tue Jul  8 16:02:58 2003
From: JonesW at kssg.com (Wayne Jones)
Date: Tue, 8 Jul 2003 15:02:58 +0100 
Subject: [R] rbind question
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0D71@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030708/99d05020/attachment.pl

From jzhang at jimmy.harvard.edu  Tue Jul  8 16:11:09 2003
From: jzhang at jimmy.harvard.edu (John Zhang)
Date: Tue, 8 Jul 2003 10:11:09 -0400 (EDT)
Subject: [R] rbind question
Message-ID: <200307081411.KAA12268@blaise.dfci.harvard.edu>

How about 

foo <- c(1,2,3)
bar <- matrix(rep(foo, 5), ncol = length(foo), byrow = TRUE)

>Date: 8 Jul 2003 15:58:25 +0200
>From: "David Andel" <andel at ifi.unizh.ch>
>To: r-help at stat.math.ethz.ch
>MIME-Version: 1.0
>Content-Transfer-Encoding: 8bit
>X-Virus-Scanned: by amavisd-milter (http://amavis.org/)
>X-Virus-Scanned: by amavisd-milter (http://amavis.org/)
>X-Spam-Status: No, hits=-5.3 required=5.0 tests=BAYES_01, HAS_ORGANIZATION 
version=2.54
>X-Spam-Level: 
>X-Spam-Checker-Version: SpamAssassin 2.54 (1.174.2.17-2003-05-11-exp)
>Subject: [R] rbind question
>X-BeenThere: r-help at stat.math.ethz.ch
>X-Mailman-Version: 2.1.2
>List-Id: Main R Mailing List: Primary help  <r-help.stat.math.ethz.ch>
>List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
>List-Post: <mailto:r-help at stat.math.ethz.ch>
>List-Subscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>List-Archive: <https://www.stat.math.ethz.ch/pipermail/r-help>
>List-Unsubscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>
>Hi
>
>I am trying to replicate a vector in n rows for comparison purposes with 
>another matrix.
>
>foo <- c(1,2,3)
>bar <- rbind(foo,foo) # does the trick for 2 rows
>bar <- rbind(rep(foo,2)) # does something else
>
>How do I generate a matrix with all rows=foo without writing 'foo' n times as 
>arg?
>
>Thanks,
>David
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Jianhua Zhang
Department of Biostatistics
Dana-Farber Cancer Institute
44 Binney Street
Boston, MA 02115-6084



From sundar.dorai-raj at pdf.com  Tue Jul  8 16:11:30 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 08 Jul 2003 09:11:30 -0500
Subject: [R] rbind question
References: <20030708155825.1.20977.qmail@ifi.unizh.ch>
Message-ID: <3F0AD112.7050400@pdf.com>



David Andel wrote:
> Hi
> 
> I am trying to replicate a vector in n rows for comparison purposes with 
> another matrix.
> 
> foo <- c(1,2,3)
> bar <- rbind(foo,foo) # does the trick for 2 rows
> bar <- rbind(rep(foo,2)) # does something else
> 
> How do I generate a matrix with all rows=foo without writing 'foo' n times as 
> arg?
> 

How about:

foo <- 1:3
n <- 10 # number of rows
matrix(rep(foo, n), nr = n, byrow = TRUE)

Regards,
Sundar



From paradis at isem.univ-montp2.fr  Tue Jul  8 16:13:03 2003
From: paradis at isem.univ-montp2.fr (Emmanuel Paradis)
Date: Tue, 08 Jul 2003 16:13:03 +0200
Subject: [R] rbind question
In-Reply-To: <20030708155825.1.20977.qmail@ifi.unizh.ch>
Message-ID: <4.2.0.58.20030708160959.00b3f0d0@162.38.183.200>

Hi David,

At 15:58 08/07/2003 +0200, vous avez ?crit:
>Hi
>
>I am trying to replicate a vector in n rows for comparison purposes with
>another matrix.
>
>foo <- c(1,2,3)
>bar <- rbind(foo,foo) # does the trick for 2 rows
>bar <- rbind(rep(foo,2)) # does something else

This is because `rep()' is executed before `rbind()'.


>How do I generate a matrix with all rows=foo without writing 'foo' n times as
>arg?

I guess there are several possible ways to do what you want. Here's my 
solution (already posted by Andy Liaw...):

R> foo <- c(1,2,3)
R> N <- 10
R> bar <- matrix(foo, nr = N, nc = length(foo), byrow = TRUE)
R> bar
       [,1] [,2] [,3]
  [1,]    1    2    3
  [2,]    1    2    3
  [3,]    1    2    3
  [4,]    1    2    3
  [5,]    1    2    3
  [6,]    1    2    3
  [7,]    1    2    3
  [8,]    1    2    3
  [9,]    1    2    3
[10,]    1    2    3


Emmanuel Paradis



From gb at stat.umu.se  Tue Jul  8 16:15:14 2003
From: gb at stat.umu.se (=?ISO-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 8 Jul 2003 16:15:14 +0200 (CEST)
Subject: [R] rbind question
In-Reply-To: <20030708155825.1.20977.qmail@ifi.unizh.ch>
Message-ID: <Pine.LNX.4.44.0307081611350.6420-100000@tal.stat.umu.se>

On 8 Jul 2003, David Andel wrote:

> Hi
> 
> I am trying to replicate a vector in n rows for comparison purposes with 
> another matrix.
> 
> foo <- c(1,2,3)
> bar <- rbind(foo,foo) # does the trick for 2 rows
> bar <- rbind(rep(foo,2)) # does something else
> 
> How do I generate a matrix with all rows=foo without writing 'foo' n times as 
> arg?

Like this?

> foo <- c(1,2,3)
> n <- 5                                                           
> bar <- matrix(foo, nrow = n, ncol = length(foo), byrow = TRUE)
> bar
      [,1] [,2] [,3]
 [1,]    1    2    3
 [2,]    1    2    3
 [3,]    1    2    3
 [4,]    1    2    3
 [5,]    1    2    3

G.

> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
---
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From pburns at pburns.seanet.com  Tue Jul  8 16:14:34 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 08 Jul 2003 15:14:34 +0100
Subject: [R] rbind question
References: <20030708155825.1.20977.qmail@ifi.unizh.ch>
Message-ID: <3F0AD1CA.3040301@pburns.seanet.com>

do.call("rbind", rep(list(foo), n))

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

David Andel wrote:

>Hi
>
>I am trying to replicate a vector in n rows for comparison purposes with 
>another matrix.
>
>foo <- c(1,2,3)
>bar <- rbind(foo,foo) # does the trick for 2 rows
>bar <- rbind(rep(foo,2)) # does something else
>
>How do I generate a matrix with all rows=foo without writing 'foo' n times as 
>arg?
>
>Thanks,
>David
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From Ramlau at gmx.de  Tue Jul  8 16:49:43 2003
From: Ramlau at gmx.de (Ramlau@gmx.de)
Date: Tue, 8 Jul 2003 16:49:43 +0200 (MEST)
Subject: [R] (no subject)
Message-ID: <4709.1057675783@www8.gmx.net>

Dear R users,

I created a program for a simulation. It produces datasets and then it
conducts some fits with the function coxph. Sometimes this function produces
warnings, but I get only this warnings after finishing my program. Is there any
possibility to get these hints earlier, maybe at once after the fit? I tried to
store that during my program, but warnings() doesn't work.

Thanks for every answer


-- 
Peggy Ramlau



Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!



From hdoran at nasdc.org  Tue Jul  8 17:25:57 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 8 Jul 2003 11:25:57 -0400
Subject: [R] NLME Fitted Values
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FBE@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030708/27e785ce/attachment.pl

From maechler at stat.math.ethz.ch  Tue Jul  8 18:04:19 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 8 Jul 2003 18:04:19 +0200
Subject: [R] readline and R installation
In-Reply-To: <002201c34557$2c9d6650$8c7ba8c0@Scorpion>
References: <002201c34557$2c9d6650$8c7ba8c0@Scorpion>
Message-ID: <16138.60291.20932.189405@gargle.gargle.HOWL>

One way is setting the (or adding to an existing one)
environment variable LD_LIBRARY_PATH to your directory
(.../local/R/readline); there are other related ways described
in the "R Administration/Installation" manual which I'd warmly
recommend consulting here.

If that is really /local/R/... I guess the resulting R will only
run on your workstation which is a bit unfortunate.
Did you try to convince your sys.admin installing readline
"globally"?

Hoping this helps,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><

>>>>> "Jean" == Jean Eid <jeaneid at chass.utoronto.ca>
>>>>>     on Tue, 8 Jul 2003 09:45:20 -0400 writes:

    Jean> I am trying to install R 1.7.1 on a
    Jean> sparc-sun-solaris2.8 system. although gnu readline is
    Jean> installed and works fine on the parent directory, R is
    Jean> not recognizing it.  I get the following in the
    Jean> config.log file.

    Jean> configure:11627: checking for rl_callback_read_char in -lreadline
    Jean> configure:11658: gcc -o conftest -g -O2  -L/usr/local/lib
    Jean>                  conftest.c -lreadline  -ldl -ltermcap -lm  >&5
    Jean> ld: fatal: library -lreadline: not found
    Jean> ld: fatal: File processing errors. No output written to conftest
    Jean> collect2: ld returned 1 exit status

    Jean> I also tried to install the most recent version of
    Jean> readline (version 4.3) with no luck. as I do not have
    Jean> root access to the system I have included the readline
    Jean> directory in my home path.  I beleive that I have to
    Jean> change the path of the readline library from
    Jean> usr/local/lib to my own which is on /local/R/readline,
    Jean> however I do not want to change the configure file
    Jean> without making sure I am doing the right thing, Any
    Jean> help is appreciated,



From huan.huang at bnpparibas.com  Tue Jul  8 18:04:29 2003
From: huan.huang at bnpparibas.com (huan.huang@bnpparibas.com)
Date: Tue, 8 Jul 2003 17:04:29 +0100
Subject: [R] Variance Ratio Test
Message-ID: <OF3D63E5E1.760C1423-ON80256D5D.00584D24@bnpparibas.com>


Dear all,

I am programming on the *variance ratio test*, according to Lo and
MacKinlay, 1988. I am wondering if there has been some official codes
written for that.

Many thanks and regards,

Huan Huang




This message and any attachments (the "message") is\ intende...{{dropped}}



From spencer.graves at pdf.com  Tue Jul  8 18:20:42 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jul 2003 09:20:42 -0700
Subject: [R] Warnings (was:   (no subject))
References: <4709.1057675783@www8.gmx.net>
Message-ID: <3F0AEF5A.9080708@pdf.com>

1.  Please use a descriptive subject line.

2.  In "?warnings", "See also" refers to "warning".  "Details" for the 
latter state, " The result depends on the value of `options("warn")'.

      If `warn' is negative warnings are ignored; if it is zero they are
      stored and printed after the top-level function has completed; if
      it is one they are printed as they occur and if it is 2 (or
      larger) warnings are turned into errors."

3.  Another important search facility is provided by "www.r-project.org" 
-> search -> "R site search".  A search there for "warnings" gave me 
thousands of hits, but "warnings as they occur" gave me 29, one of which 
produced the information just provided in item 2 above.

hope this helps.  spencer graves

Ramlau at gmx.de wrote:
> Dear R users,
> 
> I created a program for a simulation. It produces datasets and then it
> conducts some fits with the function coxph. Sometimes this function produces
> warnings, but I get only this warnings after finishing my program. Is there any
> possibility to get these hints earlier, maybe at once after the fit? I tried to
> store that during my program, but warnings() doesn't work.
> 
> Thanks for every answer
> 
>



From andel at ifi.unizh.ch  Tue Jul  8 18:22:50 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Tue, 08 Jul 2003 16:22:50 -0000
Subject: [R] how to rename rows/columns of a matrix?
Message-ID: <20030708182253.1.6215.qmail@ifi.unizh.ch>

Hi

I get a matrix with the columns named like "X13 X22 X1 X14 ...", i.e. not 
successively. That has it's good reasons, but now how can I rename the 
columns to "X1 X2 X3 ..."?

Thanks a lot,
David



From dave at evocapital.com  Tue Jul  8 18:34:53 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 8 Jul 2003 17:34:53 +0100
Subject: [R] how to rename rows/columns of a matrix?
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D0124C9@mail.internal.net>

If A is your matrix try:

cnames(A) = paste("X", 1:ncol(A), sep="")


-----Original Message-----
From: David Andel [mailto:andel at ifi.unizh.ch] 
Sent: 08 July 2003 17:23
To: R-Project
Subject: [R] how to rename rows/columns of a matrix?


Hi

I get a matrix with the columns named like "X13 X22 X1 X14 ...", i.e.
not 
successively. That has it's good reasons, but now how can I rename the 
columns to "X1 X2 X3 ..."?

Thanks a lot,
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dave at evocapital.com  Tue Jul  8 18:37:30 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 8 Jul 2003 17:37:30 +0100
Subject: [R] how to rename rows/columns of a matrix?
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D0124CA@mail.internal.net>

Sorry, that should have been "colnames" rather than cnames; i.e.:

colnames(A) = paste("X", 1:ncol(A), sep="")


-----Original Message-----
From: David Andel [mailto:andel at ifi.unizh.ch] 
Sent: 08 July 2003 17:23
To: R-Project
Subject: [R] how to rename rows/columns of a matrix?


Hi

I get a matrix with the columns named like "X13 X22 X1 X14 ...", i.e.
not 
successively. That has it's good reasons, but now how can I rename the 
columns to "X1 X2 X3 ..."?

Thanks a lot,
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From david.barron at jesus.ox.ac.uk  Tue Jul  8 18:38:29 2003
From: david.barron at jesus.ox.ac.uk (David Barron)
Date: Tue, 8 Jul 2003 17:38:29 +0100
Subject: [R] how to rename rows/columns of a matrix?
In-Reply-To: <20030708182253.1.6215.qmail@ifi.unizh.ch>
Message-ID: <HOEELNPAKEPPEBNKGDBFMEEACAAA.david.barron@jesus.ox.ac.uk>

For a matrix m with k columns:

colnames(m) <- paste("X", 1:k, sep="")

David

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of David Andel
Sent: 08 July 2003 17:23
To: R-Project
Subject: [R] how to rename rows/columns of a matrix?


Hi

I get a matrix with the columns named like "X13 X22 X1 X14 ...", i.e. not 
successively. That has it's good reasons, but now how can I rename the 
columns to "X1 X2 X3 ..."?

Thanks a lot,
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From apjaworski at mmm.com  Tue Jul  8 20:13:42 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 8 Jul 2003 13:13:42 -0500
Subject: [R] how to rename rows/columns of a matrix?
Message-ID: <OF8C3A10C3.647C2C50-ON86256D5D.0063A712@mmm.com>


This is what I would do:

cc <- NULL
for(i in 1:ncol(x)) cc <- c(cc, paste("X", i, sep=''))
colnames(x) <- cc

where x is your matrix.

Hope this helps,

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "David Andel"        |
|         |           <andel at ifi.unizh.ch> |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           07/08/2003 11:22     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       "R-Project" <r-help at stat.math.ethz.ch>                                                                       |
  |      cc:                                                                                                                    |
  |      Subject:  [R] how to rename rows/columns of a matrix?                                                                  |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hi

I get a matrix with the columns named like "X13 X22 X1 X14 ...", i.e. not
successively. That has it's good reasons, but now how can I rename the
columns to "X1 X2 X3 ..."?

Thanks a lot,
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ashley.Minor at ssec.wisc.edu  Tue Jul  8 22:17:25 2003
From: Ashley.Minor at ssec.wisc.edu (Ashley Minor)
Date: Tue, 08 Jul 2003 15:17:25 -0500
Subject: [R] exporting help
Message-ID: <3F0B26D5.5CE712EB@ssec.wisc.edu>

i need step by step directions on how to export a text file into the r
file



From p.dalgaard at biostat.ku.dk  Tue Jul  8 22:18:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 08 Jul 2003 20:18:53 -0000
Subject: [R] how to rename rows/columns of a matrix?
In-Reply-To: <OF8C3A10C3.647C2C50-ON86256D5D.0063A712@mmm.com>
References: <OF8C3A10C3.647C2C50-ON86256D5D.0063A712@mmm.com>
Message-ID: <x2smpgdb22.fsf@biostat.ku.dk>

apjaworski at mmm.com writes:

> This is what I would do:
> 
> cc <- NULL
> for(i in 1:ncol(x)) cc <- c(cc, paste("X", i, sep=''))
> colnames(x) <- cc
> 
> where x is your matrix.

R is a vectorized language:

colnames(x) <- paste("X", 1:ncol(x), sep='')

will do.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kwan022 at stat.auckland.ac.nz  Tue Jul  8 22:28:39 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 9 Jul 2003 08:28:39 +1200 (NZST)
Subject: [R] exporting help
In-Reply-To: <3F0B26D5.5CE712EB@ssec.wisc.edu>
Message-ID: <Pine.LNX.4.44.0307090825510.8338-100000@stat55.stat.auckland.ac.nz>

Have you read "R Data Import/Export"?

On Tue, 8 Jul 2003, Ashley Minor wrote:

> Date: Tue, 08 Jul 2003 15:17:25 -0500
> From: Ashley Minor <Ashley.Minor at ssec.wisc.edu>
> To: R-help at stat.math.ethz.ch
> Subject: [R] exporting help
> 
> i need step by step directions on how to export a text file into the r
> file
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From matthew_wiener at merck.com  Tue Jul  8 22:30:15 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 08 Jul 2003 16:30:15 -0400
Subject: [R] exporting help
Message-ID: <AEBD81486231A343B1813FE62D3352250369A0B6@usrymx15.merck.com>

Perhaps you should read one of the introductory manuals in the "Contributed
Documentation" section of cran.us.r-project.org.  This will introduce many
of the basic commands, and save you time and frustration.

You will probably end up wanting to use either "read.table" or "scan" to get
your data into R.

Hope this helps,

Matt Wiener

-----Original Message-----
From: Ashley Minor [mailto:Ashley.Minor at ssec.wisc.edu] 
Sent: Tuesday, July 08, 2003 4:17 PM
To: R-help at stat.math.ethz.ch
Subject: [R] exporting help


i need step by step directions on how to export a text file into the r
file

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From mrennie at utm.utoronto.ca  Tue Jul  8 23:14:35 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Tue, 08 Jul 2003 17:14:35 -0400
Subject: [R] solving multiple parameter starting values in nlm
In-Reply-To: <3F0A72A8.6090302@pdf.com>
References: <1057639563.3f0a4c8b1118b@webmail.utm.utoronto.ca>
Message-ID: <5.1.0.14.0.20030708164916.00a8cdc0@mail.utm.utoronto.ca>


Hi, there


At 12:28 AM 7/8/03 -0700, Spencer Graves wrote:
>           Did you work through the examples at the end of the "nlm" 
> documentation?  The first example there shows that "nlm" expects a single 
> vector argument over which f is to be minimized.  This suggests that your 
> first construction should not work, while your second should be closer.

I've gone through and now attempted a number of varieties on this 
theme.  Stated generally, the newest variation I am hung on is;

x<-NULL
Y<-NULL

q<-(x,y)

f <- function (q)
{
{
for i in (1:length(day))
{
.........I then use x and y in my loop for daily iterations, where each 
element is dependent upon the value of the previous iteration, and obtain 
values for c, d, and try to minimize the following function........
}
f <- ((a^2 -c^2)^2 + (b^2 - d^2)^2)^2/2 ; f
}
nlm (f, 1, r=1)

I've squared all the results, the differences, and the overall value as the 
function to minimize such that the differences between the user input 
values (a, b) and the model output (b,d) are exaggerated so as to obtain a 
sufficient minimization.  This function works on it's own when performed 
after the daily iteration loop, but it doesn't even get that far in this 
version of the nlm loop.  Also, the loop with only my daily iterations 
works fine (without the nlm loop), no warnings, all is good.

But, once I embed it within the loop for function f, my program stalls on 
the daily iterations.  I get error messages that seem to indicate that it 
isn't even going through the first iteration properly.

If, however, I code my input variables as q[1] and q[2] in the loop, 
instead of x, y, it solves the loop correctly, but comes back with a 
billion warnings that "the number of items to replace is not a multiple of 
replacement length".  I think that it wants my q vector to be the same 
length as the iterations in the daily loop, 365, but gets mad when it 
isn't, because it possesses only 2 elements, (x ,y) or (q[1], q[2]).

As well, when I run the daily iteration loops OUTSIDE the nlm loop with the 
q[1] and q[2] coding, I get the same error messages ("the number of items 
to replace is not a multiple of replacement length").  Despite this, it 
goes through the program and reports a solution.

My fear is that even though I am getting the right answers from the daily 
loop in this last version, if I am getting errors, then I shouldn't just 
simply ignore them and continue.  Or should I?  Are the errors simply a 
result of a technicality?  Personally, I would feel better if I could do it 
without them.


>           Also, did you try running your function by itself, checking to 
> make sure it computed correctly?  The final line for the second function 
> as I read it below appears OUTSIDE the function definition as a 
> stand-alone expression to be executed once between the function 
> definition and the "nlm" call.

Yes, or at least a modified version of it. see above.

>           Also, have you tried "optim"?  It seems to be more general and 
> robust, requiring fewer assumptions to function.  The default method for 
> "optim" does not require the function to be differentiable, while "nlm" does.

Yes, and "optim" is stalling in the same places and generating the same 
errors as 'nlm'.

Any other thoughts or suggestions for what I'm doing wrong?


>hope this helps.  spencer graves
>
>Michael Rennie wrote:
>>Hi there
>>I am having trouble figuring out how to get an nlm function to report 
>>estimates for two parameter values in an estimation.
>>The way I've got it goes something like this:
>>f <- function (q, r)
>>{
>>here, I have a second loop which uses q, r to give me values for c, d 
>>below.  a and b are already specified; this loop is a mass-balance 
>>function where I am trying to find values of q, r to give me c and d. I 
>>want c and d (mass-balance model outputs) to approximate a and b (known 
>>endpoints that I want the model to attain), respecitvely.  Thus, the 
>>function I want to minimize is:
>>fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
>>}
>>nlm (f, 1, r=1)
>>This doesn't return estimates for each parameter, and I get error 
>>messages saying something like "repleaced missing value with maximum" or 
>>something.  All I did here was try (and fail, obviously) to model my 
>>needs against the examples given in help(nlm), but they are using vectors 
>>as inputs, and I only need 2 values input, and 2 returned.
>>I've also tried specifying the function as
>>f <- function (q)
>>{
>>mass-balance loop
>>}
>>fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
>>nlm(f, c(1,1))
>>Specifying 1 as the starting values, and coding q[1] and q[2] in my mass 
>>balance loop instead of q, r.
>>But this doesn't work either, and I get messages that I think are 
>>indicating that my mass balance loop isn't computing properly ("number of 
>>items to replace is not a multiple of replacement length"- an error I 
>>don't get when I run the mass-balance loop outside the nlm function and 
>>just specify q, r).
>>Does anyone know how to specify two parameters in the function, and ask 
>>for estimates of both back from nlm? Alternatively, any hunches on how to 
>>make this thing do what I want it to?
>>Thanks,
>>Mike
>

Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From spencer.graves at pdf.com  Tue Jul  8 23:27:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jul 2003 14:27:23 -0700
Subject: [R] solving multiple parameter starting values in nlm
References: <1057639563.3f0a4c8b1118b@webmail.utm.utoronto.ca>
	<5.1.0.14.0.20030708164916.00a8cdc0@mail.utm.utoronto.ca>
Message-ID: <3F0B373B.5030308@pdf.com>

1.  With a problem like that, I will typically try to step through my 
function one line at a time until I find the line that produces the 
warning, "the number of items to replace is not a multiple of 
replacement length".  At that point, it is typically fairly easy to 
figure out how to fix the problem.

2.  If that fails, could you try to strip the function down to something 
that still produces the problem but is small enough that someone else 
can read it in 20 seconds?  Often, if you try to do this, you will find 
the problem.  For example, from what I read, your current problem has 
nothing to do with "nlm" nor "optim";  it is "the number of items to 
replace is not ... ."  If so, the function "nlm" and "optim" are just 
part of the forest that make it difficult for you to see the trees.

3.  "q" is the name of a function in R.  R is smart enough to know the 
difference, but I'm not always so smart.  I suggest you use a different 
name, like "q." or anything else.

hope this helps.  spencer graves

Michael Rennie wrote:
> 
> Hi, there
> 
> 
> At 12:28 AM 7/8/03 -0700, Spencer Graves wrote:
> 
>>           Did you work through the examples at the end of the "nlm" 
>> documentation?  The first example there shows that "nlm" expects a 
>> single vector argument over which f is to be minimized.  This suggests 
>> that your first construction should not work, while your second should 
>> be closer.
> 
> 
> I've gone through and now attempted a number of varieties on this 
> theme.  Stated generally, the newest variation I am hung on is;
> 
> x<-NULL
> Y<-NULL
> 
> q<-(x,y)
> 
> f <- function (q)
> {
> {
> for i in (1:length(day))
> {
> .........I then use x and y in my loop for daily iterations, where each 
> element is dependent upon the value of the previous iteration, and 
> obtain values for c, d, and try to minimize the following function........
> }
> f <- ((a^2 -c^2)^2 + (b^2 - d^2)^2)^2/2 ; f
> }
> nlm (f, 1, r=1)
> 
> I've squared all the results, the differences, and the overall value as 
> the function to minimize such that the differences between the user 
> input values (a, b) and the model output (b,d) are exaggerated so as to 
> obtain a sufficient minimization.  This function works on it's own when 
> performed after the daily iteration loop, but it doesn't even get that 
> far in this version of the nlm loop.  Also, the loop with only my daily 
> iterations works fine (without the nlm loop), no warnings, all is good.
> 
> But, once I embed it within the loop for function f, my program stalls 
> on the daily iterations.  I get error messages that seem to indicate 
> that it isn't even going through the first iteration properly.
> 
> If, however, I code my input variables as q[1] and q[2] in the loop, 
> instead of x, y, it solves the loop correctly, but comes back with a 
> billion warnings that "the number of items to replace is not a multiple 
> of replacement length".  I think that it wants my q vector to be the 
> same length as the iterations in the daily loop, 365, but gets mad when 
> it isn't, because it possesses only 2 elements, (x ,y) or (q[1], q[2]).
> 
> As well, when I run the daily iteration loops OUTSIDE the nlm loop with 
> the q[1] and q[2] coding, I get the same error messages ("the number of 
> items to replace is not a multiple of replacement length").  Despite 
> this, it goes through the program and reports a solution.
> 
> My fear is that even though I am getting the right answers from the 
> daily loop in this last version, if I am getting errors, then I 
> shouldn't just simply ignore them and continue.  Or should I?  Are the 
> errors simply a result of a technicality?  Personally, I would feel 
> better if I could do it without them.
> 
> 
>>           Also, did you try running your function by itself, checking 
>> to make sure it computed correctly?  The final line for the second 
>> function as I read it below appears OUTSIDE the function definition as 
>> a stand-alone expression to be executed once between the function 
>> definition and the "nlm" call.
> 
> 
> Yes, or at least a modified version of it. see above.
> 
>>           Also, have you tried "optim"?  It seems to be more general 
>> and robust, requiring fewer assumptions to function.  The default 
>> method for "optim" does not require the function to be differentiable, 
>> while "nlm" does.
> 
> 
> Yes, and "optim" is stalling in the same places and generating the same 
> errors as 'nlm'.
> 
> Any other thoughts or suggestions for what I'm doing wrong?
> 
> 
>> hope this helps.  spencer graves
>>
>> Michael Rennie wrote:
>>
>>> Hi there
>>> I am having trouble figuring out how to get an nlm function to report 
>>> estimates for two parameter values in an estimation.
>>> The way I've got it goes something like this:
>>> f <- function (q, r)
>>> {
>>> here, I have a second loop which uses q, r to give me values for c, d 
>>> below.  a and b are already specified; this loop is a mass-balance 
>>> function where I am trying to find values of q, r to give me c and d. 
>>> I want c and d (mass-balance model outputs) to approximate a and b 
>>> (known endpoints that I want the model to attain), respecitvely.  
>>> Thus, the function I want to minimize is:
>>> fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
>>> }
>>> nlm (f, 1, r=1)
>>> This doesn't return estimates for each parameter, and I get error 
>>> messages saying something like "repleaced missing value with maximum" 
>>> or something.  All I did here was try (and fail, obviously) to model 
>>> my needs against the examples given in help(nlm), but they are using 
>>> vectors as inputs, and I only need 2 values input, and 2 returned.
>>> I've also tried specifying the function as
>>> f <- function (q)
>>> {
>>> mass-balance loop
>>> }
>>> fu <- ((a^2 - c^2) + (b^2 - d^2))/2 ; fu
>>> nlm(f, c(1,1))
>>> Specifying 1 as the starting values, and coding q[1] and q[2] in my 
>>> mass balance loop instead of q, r.
>>> But this doesn't work either, and I get messages that I think are 
>>> indicating that my mass balance loop isn't computing properly 
>>> ("number of items to replace is not a multiple of replacement 
>>> length"- an error I don't get when I run the mass-balance loop 
>>> outside the nlm function and just specify q, r).
>>> Does anyone know how to specify two parameters in the function, and 
>>> ask for estimates of both back from nlm? Alternatively, any hunches 
>>> on how to make this thing do what I want it to?
>>> Thanks,
>>> Mike
>>
>>
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
>



From Symantec_AntiVirus_for_SMTP_Gateways at eslite.com  Tue Jul  8 23:49:03 2003
From: Symantec_AntiVirus_for_SMTP_Gateways at eslite.com (Symantec_AntiVirus_for_SMTP_Gateways@eslite.com)
Date: Tue, 8 Jul 2003 23:49:03 +0200 (MEST)
Subject: [R] Virus found in a message you sent
Message-ID: <200307082149.h68LmxUv016244@stat.math.ethz.ch>

A virus was found in a message sent by this
account.

--- Scan information follows ---

Result: Virus Detected
File Attachment: your_details.zip/details.pif
Attachment Status: deleted

--- Original message information follows ---

From: <r-announce at lists.r-project.org>
To: <service at eslitebooks.com>
Date: Tue, 8 Jul 2003 15:02:16 --0700
Subject: Re: Movie
Received: (from MEY-402-8-KUEI [128.208.198.154])
 by tphqvp02.eslite.com (SAVSMTP 3.0.0.44) with SMTP id M2003070905512413738
 for <service at eslitebooks.com>; Wed, 09 Jul 2003 05:51:25 +0800



From bates at stat.wisc.edu  Tue Jul  8 23:52:52 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 08 Jul 2003 21:52:52 -0000
Subject: [R] NLME Fitted Values
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FBE@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE930139FBE@ernesto.NASDC.ORG>
Message-ID: <6rof04ofd2.fsf@bates4.stat.wisc.edu>

"Harold Doran" <hdoran at nasdc.org> writes:

> Dear List:
>  
> I am having difficulties with the fitted values at different levels
> of a multilevel model. My data set is a series of student test
> scores over time with a total of 7,280 observations, 1,720 students
> nested witin 60 schools. The data set is not balanced.
>  
> The model was fit using
>  
> eg.model.1<-lme(math~year, random=~year|schoolid/childid, data=single).
>  
> When I call the random effects at all levels using 
>  
> EB.1<-data.frame(ranef(eg.model1, level=1)) and
> EB.2<-data.frame(ranef(eg.model1, level=2)), I get the shrinkage
> estimators that I expect. That is, I get 2 random effects for each
> child (1 intercept and 1 slope) and 2 for each school (1 intercept
> and 1 slope).
>  
> However, when I call the fitted values using:
>  
> fitted<-data.frame(fitted(eg.model1, level=0:2)), I get 7,280 fitted
> values at the level of observation. This makes sense (one for each
> observed score). However, I also get 7,280 fitted values at the
> child and at the school level. This does not seem correct to me.
>  
> I should only have, I think, 60 fitted values at the school level
> (actually, 1 intercept and 1 slope for each of 60 schools) and 1,720
> fitted values at the child level (again, 1 intercept and one for the
> slope for each child).

I think you are confusing the random effects with the fitted values.
The fitted values will depend on the fixed-effects and, when level >
0, on the random effects.  Because the year term, which is associated
with the fixed effects, can change within school and within child we
always return one fitted value for each observation.

> Why am I always getting 7,280 fitted values? 

There is some redundancy but we cannot determine the redundancy
without knowing the exact form of both the fixed effects and the
random effects.  In your case where year is the only term in the fixed
effects when level = 0 the fitted values for the same year should be
the same.  When level = 1 the fitted values for the same year and same
school but different children should be the same.  When level = 2
potentially all the fitted values will be different.

However, it could be that a fixed effects term would have a unique
value at each row and then even the level = 0 fitted values could all
be distinct.

> I have tried
> fitted.1<-data.frame(fitted(eg.model1, level=1)) and fitted.2<-data.frame(fitted(eg.model1, level=2)), but this does not appear to be working either.

But it appears that what you want is what you already got from the
ranef extractor.  Is there some reason that you don't want to use
those values?

Regards,
Doug Bates



From jeaneid at chass.utoronto.ca  Wed Jul  9 01:27:36 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 8 Jul 2003 19:27:36 -0400
Subject: [R] readline and R installation
References: <002201c34557$2c9d6650$8c7ba8c0@Scorpion>
	<16138.60291.20932.189405@gargle.gargle.HOWL>
Message-ID: <000f01c345a8$83fec500$8c7ba8c0@Scorpion>

I have read and added the options in the config.site file. Although I get
the following message
R is now configured for sparc-sun-solaris2.8

      Source directory:          .
      Installation directory:    /usr/local

      C compiler:                gcc  -g -O2
      C++ compiler:              g++  -g -O2
      Fortran compiler:          g77  -g

      Interfaces supported:      X11
      External libraries:        readline
      Additional capabilities:   bzip2, PCRE
      Options enabled:           R profiling

      Recommended packages:      no



configure.log still shows missing readline.h file. It shows the following in
config.log


    | #include <readline/history.h>
    configure:12368: result: no
    configure:12404: checking for readline/history.h
    configure:12411: result: no
    configure:12296: checking readline/readline.h usability
    configure:12309:
gcc -c -g -O2 -I/usr/local/R/readline -I/local/R/readline conftest.c >&5
    configure:12379:31: readline/readline.h: No such file or directory
    configure:12312: $? = 1



and my config.site file contains the following
LDFLAGS="-L/usr/local/lib -L/usr/local/R/readline"
LIBS="-L/usr/local/lib -L/local/R/readline"
CPPFLAGS="-I/usr/local/include -I/usr/local/R/readline -I/local/R/readline"

when I envoke R and press C-r it writes
(reverse-i-search)`':

but C-l , M-b, and others work. the only keys that do not work are the
delete and arrow up and down.

Thank you for all the help

----- Original Message ----- 
From: "Martin Maechler" <maechler at stat.math.ethz.ch>
To: "Jean Eid" <jeaneid at chass.utoronto.ca>
Cc: <R-help at stat.math.ethz.ch>
Sent: Tuesday, July 08, 2003 12:04 PM
Subject: Re: [R] readline and R installation


> One way is setting the (or adding to an existing one)
> environment variable LD_LIBRARY_PATH to your directory
> (.../local/R/readline); there are other related ways described
> in the "R Administration/Installation" manual which I'd warmly
> recommend consulting here.
>
> If that is really /local/R/... I guess the resulting R will only
> run on your workstation which is a bit unfortunate.
> Did you try to convince your sys.admin installing readline
> "globally"?
>
> Hoping this helps,
>
> Martin Maechler <maechler at stat.math.ethz.ch>
http://stat.ethz.ch/~maechler/
> Seminar fuer Statistik, ETH-Zentrum  LEO C16 Leonhardstr. 27
> ETH (Federal Inst. Technology) 8092 Zurich SWITZERLAND
> phone: x-41-1-632-3408 fax: ...-1228 <><
>
> >>>>> "Jean" == Jean Eid <jeaneid at chass.utoronto.ca>
> >>>>>     on Tue, 8 Jul 2003 09:45:20 -0400 writes:
>
>     Jean> I am trying to install R 1.7.1 on a
>     Jean> sparc-sun-solaris2.8 system. although gnu readline is
>     Jean> installed and works fine on the parent directory, R is
>     Jean> not recognizing it.  I get the following in the
>     Jean> config.log file.
>
>     Jean> configure:11627: checking for rl_callback_read_char
in -lreadline
>     Jean> configure:11658: gcc -o conftest -g -O2  -L/usr/local/lib
>     Jean>                  conftest.c -lreadline  -ldl -ltermcap -lm  >&5
>     Jean> ld: fatal: library -lreadline: not found
>     Jean> ld: fatal: File processing errors. No output written to conftest
>     Jean> collect2: ld returned 1 exit status
>
>     Jean> I also tried to install the most recent version of
>     Jean> readline (version 4.3) with no luck. as I do not have
>     Jean> root access to the system I have included the readline
>     Jean> directory in my home path.  I beleive that I have to
>     Jean> change the path of the readline library from
>     Jean> usr/local/lib to my own which is on /local/R/readline,
>     Jean> however I do not want to change the configure file
>     Jean> without making sure I am doing the right thing, Any
>     Jean> help is appreciated,
>



From Simon.Blomberg at anu.edu.au  Wed Jul  9 01:48:25 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Wed, 9 Jul 2003 09:48:25 +1000
Subject: [R] beginner gls (nlme) question
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133D3@CASEVS02.cas.anu.edu.au>

Hi,

Well since I have gotten no responses to my question, I managed to figure it out for myself (I think.). Here is the answer, for the purposes of the archive:

Firstly, I was wrong in that it is not the factors that are correlated, but the observations (D'oh!). Therefore, the correlation matrix will have dimension 2^3 * 5 = 40. However, we can simplify the structure if we assume no correlation among replicates of the same treatment (using a grouping variable), reducing the matrix to dimension 2^3=8. ie the correlations among the observations of the interactions also have to be specified, not just the main effects. The data set looks like:

   A B C   response group
1  0 0 0  40.411581     1
2  1 0 0  19.926468     1
3  0 1 0  70.824970     1
4  1 1 0  37.298386     1
5  0 0 1 108.361849     1
6  1 0 1  67.315986     1
7  0 1 1   7.843482     1
8  1 1 1  45.648360     1
9  0 0 0 115.457913     2
10 1 0 0  42.650458     2
11 0 1 0  88.852955     2
12 1 1 0 116.738213     2
13 0 0 1 150.724565     2
14 1 0 1  81.888251     2
15 0 1 1  47.178701     2
16 1 1 1  28.822613     2 
...

and the correlation matrix mat is:

> mat
      Int    A    B C  A:B A:C B:C A:B:C
Int     1 0.00 0.00 0 0.00   0   0     0
A       0 1.00 0.75 0 0.75   0   0     0
B       0 0.75 1.00 0 0.75   0   0     0
C       0 0.00 0.00 1 0.00   0   0     0
A:B     0 0.75 0.75 0 1.00   0   0     0
A:C     0 0.00 0.00 0 0.00   1   0     0
B:C     0 0.00 0.00 0 0.00   0   1     0
A:B:C   0 0.00 0.00 0 0.00   0   0     1

Then the call to gls will be:

gls(response~A*B*C, data=dat, correlation=corSymm(mat[lower.tri(mat)], form= ~1|group))

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Simon Blomberg 
> Sent: Thursday, 3 July 2003 10:53 AM
> To: R-help mailing list
> Subject: [R] beginner gls (nlme) question
> 
> 
> Hi all,
> 
> I am trying to get a handle on gls (package nlme). I have a 
> toy problem: 3 fixed factors (A, B, C), two levels each, 5 
> replicates per treatment. The response variable is 
> continuous, normal. I have a correlation matrix of the form:
> 
> > mat
>      A    B C
> A 1.00 0.75 0
> B 0.75 1.00 0
> C 0.00 0.00 1
> 
> which is common to all observations.
> 
> How do I construct the call to gls? I think I need to use 
> correlation=corSymm(), but I do not understand the precise 
> syntax. I have read the relevant parts of Pinheiro and Bates, 
> but they only talk about cases where the corSymm correlation 
> structure is modelled, rather than known. I have also 
> searched the R archives, but no luck.
> 
> I think it should be of the form gls(response~A*B*C, 
> data=dat, correlation=corSymm(...?))
> but I don't understand the arguments to corSymm.
> 
> Thanks in advance,
> 
> Simon.
> 
> Simon Blomberg, PhD
> Depression & Anxiety Consumer Research Unit
> Centre for Mental Health Research
> Australian National University
> http://www.anu.edu.au/cmhr/
> Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Jul  9 01:49:36 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 09 Jul 2003 00:49:36 +0100 (BST)
Subject: [R] Programming tip: Operators "+.", ".+"
Message-ID: <XFMail.030709004936.Ted.Harding@nessie.mcc.ac.uk>

Since I have devised the following, they may be thought useful
for that secret crock of gold at the end of the rainbow (WHERE
is it??) full of people's programming tips.

  "%+.%"<-function(X,x){sweep(X,1,x,"+")}

  "%.+%"<-function(X,x){sweep(X,2,x,"+")}

  X %+.% x adds a vector x to each of the columns of the matrix X
           with x recycled down columns if length(x) != nrow(X)

  X %+.% x adds a vector x to each of the rows of the matrix X
           with x recycled along rows if length(x) != ncol(X)

Although "X+x" will work down columns, and "t(t(X)+x)" will work
along rows, it is handy to have a consistent (and mnemonic) notation;
and, in any case, neither of these will work if x is a kx1 or 1xk
matrix rather than a vector or a list c(x1,...,xk). The above will
work in all cases, and also X%+.%x = X%+.%t(x), X%.+%x = X%.+%t(x).

Example:
  > S
       [,1] [,2] [,3]
  [1,]  1.0  0.6  0.3
  [2,]  0.6  1.0  0.6
  [3,]  0.3  0.6  1.0
  > x
  [1] 0 1 2
  > S%+.%x
       [,1] [,2] [,3]
  [1,]  1.0  0.6  0.3
  [2,]  1.6  2.0  1.6
  [3,]  2.3  2.6  3.0
  > S%.+%x
       [,1] [,2] [,3]
  [1,]  1.0  1.6  2.3
  [2,]  0.6  2.0  2.6
  [3,]  0.3  1.6  3.0

Anyway,crock of gold or not, somebody may find them useful.
Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 09-Jul-03                                       Time: 00:49:36
------------------------------ XFMail ------------------------------



From hdoran at nasdc.org  Wed Jul  9 04:14:47 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 8 Jul 2003 22:14:47 -0400
Subject: [R] NLME Fitted Values
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FC0@ernesto.NASDC.ORG>

I would like to be able to add the random effect for the slope to the fitted value for the slope at each level (i.e, for each school and for each student). 
 
The fitted values at the population level should be the fixed effects (B_00 and B_01). So each school should have the same fitted values, but a unique random effect. Adding the two together results in the mean adjusted growth rate for each school. But when I use fitted() and set level=0, I get results that are not the fixed effects. I am unsure what the fitted values are that are produced.
 
At the child level, each child has a unique random effect, but each child within the same school should share the same fitted value. But the fitted values for children at different schools should be different. Adding these should result in the mean adjusted growth rate for each child. Although I get unique random effects for each child using ranef(), I do not get the fitted values that I think I should get wth fitted().
 
However, when I use 
 
level.1<-data.frame(coef(eg.model1), level=1) this gives me what HLM calls the fitted values for each child. So, I think I can add these to the random effects at the same level to get the mean growth rate for each child.
 
So, I am a little unclear why fitted() is not producing results that are similar to HLM fitted values, but coef() produces the values that HLM calls the level 2 fitted values. 
 
So, the R and HLM random effects are convergent at all levels. The HLM fitted values at the population level are the fixed effects, but this is not the case when I use fitted () at level =0 in R.
 
However, the fitted values at the observation level in R are the same fitted values at the observation level in HLM. 
 
In sum, HLM and R random effects are exactly the same at all levels. HLM and R fitted values are exactly the same at the level of observation. But, the fitted vaues at other levels are very different.
 
Am I using fitted incorrectly and should instead be using coef() to accomplish my goal?

	-----Original Message----- 
	From: Douglas Bates [mailto:bates at stat.wisc.edu] 
	Sent: Tue 7/8/2003 5:52 PM 
	To: Harold Doran 
	Cc: R-help at stat.math.ethz.ch 
	Subject: Re: [R] NLME Fitted Values
	
	

	"Harold Doran" <hdoran at nasdc.org> writes:
	
	> Dear List:
	> 
	> I am having difficulties with the fitted values at different levels
	> of a multilevel model. My data set is a series of student test
	> scores over time with a total of 7,280 observations, 1,720 students
	> nested witin 60 schools. The data set is not balanced.
	> 
	> The model was fit using
	> 
	> eg.model.1<-lme(math~year, random=~year|schoolid/childid, data=single).
	> 
	> When I call the random effects at all levels using
	> 
	> EB.1<-data.frame(ranef(eg.model1, level=1)) and
	> EB.2<-data.frame(ranef(eg.model1, level=2)), I get the shrinkage
	> estimators that I expect. That is, I get 2 random effects for each
	> child (1 intercept and 1 slope) and 2 for each school (1 intercept
	> and 1 slope).
	> 
	> However, when I call the fitted values using:
	> 
	> fitted<-data.frame(fitted(eg.model1, level=0:2)), I get 7,280 fitted
	> values at the level of observation. This makes sense (one for each
	> observed score). However, I also get 7,280 fitted values at the
	> child and at the school level. This does not seem correct to me.
	> 
	> I should only have, I think, 60 fitted values at the school level
	> (actually, 1 intercept and 1 slope for each of 60 schools) and 1,720
	> fitted values at the child level (again, 1 intercept and one for the
	> slope for each child).
	
	I think you are confusing the random effects with the fitted values.
	The fitted values will depend on the fixed-effects and, when level >
	0, on the random effects.  Because the year term, which is associated
	with the fixed effects, can change within school and within child we
	always return one fitted value for each observation.
	
	> Why am I always getting 7,280 fitted values?
	
	There is some redundancy but we cannot determine the redundancy
	without knowing the exact form of both the fixed effects and the
	random effects.  In your case where year is the only term in the fixed
	effects when level = 0 the fitted values for the same year should be
	the same.  When level = 1 the fitted values for the same year and same
	school but different children should be the same.  When level = 2
	potentially all the fitted values will be different.
	
	However, it could be that a fixed effects term would have a unique
	value at each row and then even the level = 0 fitted values could all
	be distinct.
	
	> I have tried
	> fitted.1<-data.frame(fitted(eg.model1, level=1)) and fitted.2<-data.frame(fitted(eg.model1, level=2)), but this does not appear to be working either.
	
	But it appears that what you want is what you already got from the
	ranef extractor.  Is there some reason that you don't want to use
	those values?
	
	Regards,
	Doug Bates



From david.firth at nuffield.oxford.ac.uk  Wed Jul  9 11:54:54 2003
From: david.firth at nuffield.oxford.ac.uk (David Firth)
Date: Wed, 9 Jul 2003 10:54:54 +0100
Subject: [R] packaged datasets in .csv format
Message-ID: <6410AE2E-B1F3-11D7-A8D2-0050E4C03977@nuffield.oxford.ac.uk>

A couple of questions in connection with using .csv format to include 
data in a package:

First, the background.  The data() function loads data from .csv 
("comma-separated values") files using

   read.table(..., header = TRUE, sep = ";")

But ?read.table says

      ## To write a CSV file for input to Excel one might use
      write.table(x, file = "foo.csv", sep = ",", col.names = NA)
      ## and to read this file back into R one needs
      read.table("file.csv", header = TRUE, sep = ",", row.names=1)

As a result, .csv files created by write.table() as above are not read 
in by data() in the way that might be expected [that is, expected by 
someone who had not read help(data)!]

Two questions, then:
-- is there some compelling reason for  the use of `sep = ";"' in place 
of `sep = ",", row.names=1'?
-- if I want to maintain a dataset in .csv format, for use both in R 
and in other systems such as Excel, SPSS, etc, what is the best way to 
go about it?

Any advice would be much appreciated.

Cheers,
David



From david.firth at nuffield.oxford.ac.uk  Wed Jul  9 11:53:27 2003
From: david.firth at nuffield.oxford.ac.uk (David Firth)
Date: Wed, 9 Jul 2003 10:53:27 +0100
Subject: [R] packaged datasets in .csv format
Message-ID: <307D34CE-B1F3-11D7-A8D2-0050E4C03977@nuffield.oxford.ac.uk>

A couple of questions in connection with using .csv format to include 
data in a package:

First, the background.  The data() function loads data from .csv 
("comma-separated values") files using

   read.table(..., header = TRUE, sep = ";")

But ?read.table says

      ## To write a CSV file for input to Excel one might use
      write.table(x, file = "foo.csv", sep = ",", col.names = NA)
      ## and to read this file back into R one needs
      read.table("file.csv", header = TRUE, sep = ",", row.names=1)

As a result, .csv files created by write.table() as above are not read 
in by data() in the way that might be expected [that is, expected by 
someone who had not read help(data)!]

Two questions, then:
-- is there some compelling reason for  the use of `sep = ";"' in place 
of `sep = ",", row.names=1'?
-- if I want to maintain a dataset in .csv format, for use both in R 
and in other systems such as Excel, SPSS, etc, what is the best way to 
go about it?

Any advice would be much appreciated.

Cheers,
David



From Harvey.Monder at pharma.com  Wed Jul  9 13:54:19 2003
From: Harvey.Monder at pharma.com (Monder, Harvey)
Date: Wed, 9 Jul 2003 07:54:19 -0400
Subject: [R] Showing limits on line graphs
Message-ID: <274EB594CE46DA489B915C6DDC3E67A3018D5351@moo01.us01.apmn.org>

Is there a function that would allow me to create a line graph of some central tendency, such as mean, with limits indicated by a vertical line at each data point, i.e. SD?  It's a fairly common type of graphic, but in looking through examples I have been able to find, I have found no function for such a graph.

	Thanks in advance,

	Harvey



From edd at debian.org  Wed Jul  9 13:57:50 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 9 Jul 2003 06:57:50 -0500
Subject: [R] packaged datasets in .csv format
In-Reply-To: <307D34CE-B1F3-11D7-A8D2-0050E4C03977@nuffield.oxford.ac.uk>
References: <307D34CE-B1F3-11D7-A8D2-0050E4C03977@nuffield.oxford.ac.uk>
Message-ID: <20030709115750.GA15942@sonny.eddelbuettel.com>

On Wed, Jul 09, 2003 at 10:53:27AM +0100, David Firth wrote:
> First, the background.  The data() function loads data from .csv 
> ("comma-separated values") files using
> 
>   read.table(..., header = TRUE, sep = ";")
> 
> But ?read.table says
> 
>      ## To write a CSV file for input to Excel one might use
>      write.table(x, file = "foo.csv", sep = ",", col.names = NA)
>      ## and to read this file back into R one needs
>      read.table("file.csv", header = TRUE, sep = ",", row.names=1)
> 
> As a result, .csv files created by write.table() as above are not read 
> in by data() in the way that might be expected [that is, expected by 
> someone who had not read help(data)!]
> 
> Two questions, then:
> -- is there some compelling reason for  the use of `sep = ";"' in place 
> of `sep = ",", row.names=1'?
> -- if I want to maintain a dataset in .csv format, for use both in R 
> and in other systems such as Excel, SPSS, etc, what is the best way to 
> go about it?

If you include a file  data/foo.csv, also include a file  data/foo.R  as
the .R extension (for code) is examined before the .csv extension for the
data file.  That way, in the .R file you can specify exactly which options
are to be used when the file is loaded.  

Dirk

-- 
Don't drink and derive. Alcohol and analysis don't mix.



From dmurdoch at pair.com  Wed Jul  9 14:46:28 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 09 Jul 2003 08:46:28 -0400
Subject: [R] Showing limits on line graphs
In-Reply-To: <274EB594CE46DA489B915C6DDC3E67A3018D5351@moo01.us01.apmn.org>
References: <274EB594CE46DA489B915C6DDC3E67A3018D5351@moo01.us01.apmn.org>
Message-ID: <kb3ogvorafmsnvlsgec4honb517tq5lgr1@4ax.com>

On Wed, 9 Jul 2003 07:54:19 -0400, you wrote:

>Is there a function that would allow me to create a line graph of some central tendency, such as mean, with limits indicated by a vertical line at each data point, i.e. SD?  It's a fairly common type of graphic, but in looking through examples I have been able to find, I have found no function for such a graph.

You can construct these yourself using arrows(angle=90).  There are
functions which package this; one I can think of right now is plotCI,
in the gregmisc package.

Duncan Murdoch



From aurora at ebi.ac.uk  Wed Jul  9 13:55:32 2003
From: aurora at ebi.ac.uk (Aurora)
Date: Wed, 09 Jul 2003 13:55:32 +0200
Subject: [R] lists
Message-ID: <3F0C02B4.2060909@ebi.ac.uk>

Is there a command similar to "dim" for lists, so that one can know how 
many elements it stores?
Is it possible to add elements to a list once it has been constructed?

Thanks in advance,

        Aurora



From Harvey.Monder at pharma.com  Wed Jul  9 14:58:32 2003
From: Harvey.Monder at pharma.com (Monder, Harvey)
Date: Wed, 9 Jul 2003 08:58:32 -0400
Subject: [R] Showing limits on line graphs
Message-ID: <274EB594CE46DA489B915C6DDC3E67A3018D5352@moo01.us01.apmn.org>

I had considered using arrows or line segments to write my own function, but I though that someone might have done it before me.  Thanks to all for their rapid response.  The graphics I need can be generated using plotmeans in the gregmisc package.

	Harvey

-----Original Message-----
From: Duncan Murdoch [mailto:dmurdoch at pair.com]
Sent: Wednesday, July 09, 2003 8:46 AM
To: Monder, Harvey
Cc: 'r-help at lists. r-project. org' (E-mail)
Subject: Re: [R] Showing limits on line graphs


On Wed, 9 Jul 2003 07:54:19 -0400, you wrote:

>Is there a function that would allow me to create a line graph of some central tendency, such as mean, with limits indicated by a vertical line at each data point, i.e. SD?  It's a fairly common type of graphic, but in looking through examples I have been able to find, I have found no function for such a graph.

You can construct these yourself using arrows(angle=90).  There are
functions which package this; one I can think of right now is plotCI,
in the gregmisc package.

Duncan Murdoch



From th50 at leicester.ac.uk  Wed Jul  9 15:14:43 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 9 Jul 2003 14:14:43 +0100
Subject: [R] lists
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46D6@saffron.cfs.le.ac.uk>

> my.list<-list(a=1,b=2)
> length(my.list)
[1] 2
> my.list$c<-3
> my.list[[length(my.list)+1]]<-4
> my.list
$a
[1] 1

$b
[1] 2

$c
[1] 3

[[4]]
[1] 4

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Aurora [mailto:aurora at ebi.ac.uk]
> Sent: 09 July 2003 12:56
> To: R-help at stat.math.ethz.ch
> Subject: [R] lists
> 
> 
> Is there a command similar to "dim" for lists, so that one 
> can know how 
> many elements it stores?
> Is it possible to add elements to a list once it has been constructed?
> 
> Thanks in advance,
> 
>         Aurora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From H.RINNER at tirol.gv.at  Wed Jul  9 15:33:32 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Wed, 9 Jul 2003 15:33:32 +0200 
Subject: [R] RODBC and Oracle: error "table does not exist"
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE263@xms1.tirol.gv.at>

Dear r-helpers!

I have trouble reading data from an Oracle data base using
RODBC Version 1.0-3,
R Version 1.7.1,
Windows XP,
Oracle8 ODBC Driver Version 8.1.6.4.0:

> library(RODBC)
> channel <- odbcConnect(dsn="PAV32", case="oracle", believeNRows=FALSE)
> # ok, this was succesful
> x <- sqlTables(channel)
> x[37, ]
     TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS
37        <NA>         TKF ABTGRNAMEN      TABLE    <NA>

> # ok, so the table I am looking for ("ABTGRNAMEN") is there, but:
> sqlFetch(channel, "ABTGRNAMEN")
[1] "[RODBC] ERROR: Could not SQLExecute"                                   
[2] "S0002 942 [Oracle][ODBC][Ora]ORA-00942: table or view does not exist\n"
> # I also tried:
> sqlFetch(channel, "TKF.ABTGRNAMEN")
Error in odbcTableExists(channel, sqtable) : 
        TKF.ABTGRNAMEN : table not found on channel

What am I doing wrong here?
It doesn't work with other tables as well; on the other hand, connecting to
the table(s) in MS Access works fine using the same ODBC driver.

Best regards,
Heinrich.



From ghosh at science.unitn.it  Wed Jul  9 15:48:54 2003
From: ghosh at science.unitn.it (Ghosh Mini)
Date: Wed, 9 Jul 2003 15:48:54 +0200 (MET DST)
Subject: [R] Constructing missing data
Message-ID: <Pine.OSF.4.44.0307091542570.697-100000@omega.science.unitn.it>

Dear R users,

I have complete data of some station (near by) and incomplete data of
desired station. I want to construct the complete data (to find the gaps)
by using the complete data of near by station (say temperature versus
day). When I am plotting Temp of both station along date it is following
similar pattern, I feel some sort of projection will do. Yes linear
regression is not applicable.

Is there any method to fix it by using R???

Thanking you,
regards,
mini

***************************************************************************
Mini Ghosh
Post Doctoral Fellow
Department of Mathematics,
University of Trento
Via Sommarive 14
38050 Povo (TN)



From spencer.graves at pdf.com  Wed Jul  9 16:37:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 07:37:31 -0700
Subject: [R] Constructing missing data
References: <Pine.OSF.4.44.0307091542570.697-100000@omega.science.unitn.it>
Message-ID: <3F0C28AB.2000509@pdf.com>

Why is linear regression not applicable?  Your answer to this question 
might help someone answer your initial question below.

spencer graves

Ghosh Mini wrote:
> Dear R users,
> 
> I have complete data of some station (near by) and incomplete data of
> desired station. I want to construct the complete data (to find the gaps)
> by using the complete data of near by station (say temperature versus
> day). When I am plotting Temp of both station along date it is following
> similar pattern, I feel some sort of projection will do. Yes linear
> regression is not applicable.
> 
> Is there any method to fix it by using R???
> 
> Thanking you,
> regards,
> mini
> 
> ***************************************************************************
> Mini Ghosh
> Post Doctoral Fellow
> Department of Mathematics,
> University of Trento
> Via Sommarive 14
> 38050 Povo (TN)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andel at ifi.unizh.ch  Wed Jul  9 17:39:31 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Wed, 09 Jul 2003 15:39:31 -0000
Subject: [R] how to sort columns in a matrix?
Message-ID: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>

Hi

I am struggling in finding a way to sort columns in a matrix. Can anyone help me, please?

Thanks a lot,
David



From stecalza at tiscali.it  Wed Jul  9 17:56:33 2003
From: stecalza at tiscali.it (Stefano Calza)
Date: Wed, 9 Jul 2003 17:56:33 +0200
Subject: [R] how to sort columns in a matrix?
In-Reply-To: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>
References: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>
Message-ID: <20030709155633.GB1867@med.unibs.it>

On Wed, Jul 09, 2003 at 05:39:31PM +0200, David Andel wrote:

Hi,

if you want to sort in alphabical order the columns of the matrix you colud do this

aaa <- aaa[,sort(colnames(aaa))]

Is it what you want?

HIH,

Stefano

> Hi
> 
> I am struggling in finding a way to sort columns in a matrix. Can anyone help me, please?
> 
> Thanks a lot,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jul  9 18:10:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 09:10:53 -0700
Subject: [R] how to sort columns in a matrix?
References: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>
	<20030709155633.GB1867@med.unibs.it>
Message-ID: <3F0C3E8D.8080103@pdf.com>

Alternatively, if you want to sort first on aaa$a and second on aaa$b, 
then try the following:

aaa[order(aaa$a, aaa$b),]

hope this helps.  spencer graves

Stefano Calza wrote:
> On Wed, Jul 09, 2003 at 05:39:31PM +0200, David Andel wrote:
> 
> Hi,
> 
> if you want to sort in alphabical order the columns of the matrix you colud do this
> 
> aaa <- aaa[,sort(colnames(aaa))]
> 
> Is it what you want?
> 
> HIH,
> 
> Stefano
> 
> 
>>Hi
>>
>>I am struggling in finding a way to sort columns in a matrix. Can anyone help me, please?
>>
>>Thanks a lot,
>>David
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Wed Jul  9 18:24:27 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 9 Jul 2003 09:24:27 -0700 (PDT)
Subject: [R] how to sort columns in a matrix?
In-Reply-To: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>
Message-ID: <Pine.A41.4.44.0307090924060.217838-100000@homer04.u.washington.edu>

On 9 Jul 2003, David Andel wrote:

> Hi
>
> I am struggling in finding a way to sort columns in a matrix. Can anyone
> help me, please?

You probably want order().

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From stecalza at tiscali.it  Wed Jul  9 18:43:18 2003
From: stecalza at tiscali.it (Stefano Calza)
Date: Wed, 9 Jul 2003 18:43:18 +0200
Subject: [R] how to sort columns in a matrix?
In-Reply-To: <3F0C3E8D.8080103@pdf.com>
References: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>
	<20030709155633.GB1867@med.unibs.it> <3F0C3E8D.8080103@pdf.com>
Message-ID: <20030709164318.GD1867@med.unibs.it>

On Wed, Jul 09, 2003 at 09:10:53AM -0700, Spencer Graves wrote:

Of course! I guess I misunderstood the question!

Stefano


> Alternatively, if you want to sort first on aaa$a and second on aaa$b, 
> then try the following:
> 
> aaa[order(aaa$a, aaa$b),]
> 
> hope this helps.  spencer graves
> 
> Stefano Calza wrote:
> >On Wed, Jul 09, 2003 at 05:39:31PM +0200, David Andel wrote:
> >
> >Hi,
> >
> >if you want to sort in alphabical order the columns of the matrix you 
> >colud do this
> >
> >aaa <- aaa[,sort(colnames(aaa))]
> >
> >Is it what you want?
> >
> >HIH,
> >
> >Stefano
> >
> >
> >>Hi
> >>
> >>I am struggling in finding a way to sort columns in a matrix. Can anyone 
> >>help me, please?
> >>
> >>Thanks a lot,
> >>David
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jul  9 18:52:25 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 09:52:25 -0700
Subject: [R] how to sort columns in a matrix?
References: <Pine.GSO.4.44.0307091738140.3816-100000@igor.ifi.unizh.ch>	<20030709155633.GB1867@med.unibs.it>
	<3F0C3E8D.8080103@pdf.com> <20030709164318.GD1867@med.unibs.it>
Message-ID: <3F0C4849.1060108@pdf.com>

I think the question was ambiguous.  Now he has answers to two different 
interpretations of the question.

Spencer Graves

Stefano Calza wrote:
> On Wed, Jul 09, 2003 at 09:10:53AM -0700, Spencer Graves wrote:
> 
> Of course! I guess I misunderstood the question!
> 
> Stefano
> 
> 
> 
>>Alternatively, if you want to sort first on aaa$a and second on aaa$b, 
>>then try the following:
>>
>>aaa[order(aaa$a, aaa$b),]
>>
>>hope this helps.  spencer graves
>>
>>Stefano Calza wrote:
>>
>>>On Wed, Jul 09, 2003 at 05:39:31PM +0200, David Andel wrote:
>>>
>>>Hi,
>>>
>>>if you want to sort in alphabical order the columns of the matrix you 
>>>colud do this
>>>
>>>aaa <- aaa[,sort(colnames(aaa))]
>>>
>>>Is it what you want?
>>>
>>>HIH,
>>>
>>>Stefano
>>>
>>>
>>>
>>>>Hi
>>>>
>>>>I am struggling in finding a way to sort columns in a matrix. Can anyone 
>>>>help me, please?
>>>>
>>>>Thanks a lot,
>>>>David
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hanxianglu at yahoo.com  Wed Jul  9 20:20:13 2003
From: hanxianglu at yahoo.com (Hanhan)
Date: Wed, 9 Jul 2003 11:20:13 -0700 (PDT)
Subject: [R] model selection in lme when corARMA is assumed
Message-ID: <20030709182013.64197.qmail@web10603.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030709/0bcf2220/attachment.pl

From tobias_verbeke at skynet.be  Wed Jul  9 21:04:26 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Wed, 9 Jul 2003 21:04:26 +0200
Subject: [R] `acting' on variables
Message-ID: <20030709210426.2f4d38eb.tobias_verbeke@skynet.be>

Dear list,

How can one "from within a function" act
on these variables ("outside" a function)
that were given as arguments.

If I have e.g. this beginning of a function:

foo <- function(tiro){
app.tiro <- 3 * tiro

[...]

}

How can I from within the curly brackets
e.g. concatenate the app.tiro to
the particular variable I passed as the
argument `tiro' ?

What is the technical name of this phenomenon ?

Thank you in advance,


Tobias

PS forgive my non-programmer lingo



From eairoldi at stat.cmu.edu  Wed Jul  9 21:33:12 2003
From: eairoldi at stat.cmu.edu (Edoardo Airoldi)
Date: Wed, 9 Jul 2003 15:33:12 -0400
Subject: [R] .Internal(optim)
Message-ID: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>

> hi all,
>  I am using optim.  I am getting the following error message:
>
> Error in optim(par = start.vals[, h], fn = post.func.pois, gr = post.grad.
> pois,  :
>         L-BFGS-B needs finite values of fn
>
> If I look at optim typing '> optim' it seems that the error comes from 
> inside .Internal(optim), so I wonder how can I see the code for .Internal(
> optim)?
>
> thanks
> Edo
>
>



From spencer.graves at pdf.com  Wed Jul  9 21:30:16 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 12:30:16 -0700
Subject: [R] `acting' on variables
References: <20030709210426.2f4d38eb.tobias_verbeke@skynet.be>
Message-ID: <3F0C6D48.5070609@pdf.com>

 > foo <- function(tiro){
+  tiro <- c(tiro, 3 * tiro)
+  tiro
+ }
 > foo(2)
[1] 2 6

Is this what you want?

spencer graves

Tobias Verbeke wrote:
> Dear list,
> 
> How can one "from within a function" act
> on these variables ("outside" a function)
> that were given as arguments.
> 
> If I have e.g. this beginning of a function:
> 
> foo <- function(tiro){
> app.tiro <- 3 * tiro
> 
> [...]
> 
> }
> 
> How can I from within the curly brackets
> e.g. concatenate the app.tiro to
> the particular variable I passed as the
> argument `tiro' ?
> 
> What is the technical name of this phenomenon ?
> 
> Thank you in advance,
> 
> 
> Tobias
> 
> PS forgive my non-programmer lingo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rvaradha at jhsph.edu  Wed Jul  9 21:33:11 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Wed, 09 Jul 2003 15:33:11 -0400
Subject: [R] A problem with using the "outer" function
Message-ID: <2bee062be09e.2be09e2bee06@jhsph.edu>

Hi:

I am using R 1.7.0 on Windows.  I am having trouble getting "outer" to 
work on one of my functions.  Here is a simple example illustrating my 
problem:

> b1 <- c(1.2,2.3)
> b2 <- c(0.5,0.6)
> x <- c(3e+01, 1e+02, 3e+02, 5e+02, 1e+03, 1e+04, 1e+05, 1e+06)
> y <- c(2,4,2,5,2,3,1,1)
> n <- c(5,8,3,6,2,3,1,1)

> outer(b1,b2,FUN=bpllkd,x,y,n)
         [,1]     [,2]
[1,] 17.78031 17.78031
[2,] 17.78031 17.78031

These values should all be different. What is the problem here?
The function "bpllkd" is given below:

thanks for any help,
Ravi.

> bpllkd 
function(t1,t2,x,y,n){
p <- 1 - (1+x/10^t1)^(-t2)
keep <- !((p==0 & y==0) | (p==1 & n==y))
llk <- sum(y[keep]*log(p[keep])+(n-y)[keep]*
log(1-p[keep]))
return(-llk)
}



From tobias_verbeke at skynet.be  Wed Jul  9 21:54:01 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Wed, 9 Jul 2003 21:54:01 +0200
Subject: [R] `acting' on variables
In-Reply-To: <200307091925.h69JPGlf015926@erdos.math.unb.ca>
References: <200307091925.h69JPGlf015926@erdos.math.unb.ca>
Message-ID: <20030709215401.344f34b6.tobias_verbeke@skynet.be>

Thanks to all of you for your answers,

The situation I was dangerously trying
to cope with was an incomplete life table,
with incomplete ages and incomplete survivors.
This is the table I want to complete using
a Gompertz function to estimate mortality
at high ages:

  age    lx
1  75 53803
2  80 37441
3  85 21134

This is the function I wrote 
(I warn you, this is not meant 
for programmer's eyes):

gompertz.estimates <- function(age, surv){
n <- age[length(age)] - age[length(age) - 1]
ly2n <- surv[length(surv)]
lyn <- surv[length(surv) - 1]
ly <- surv[length(surv) - 2]
y <- age[length(age) - 2]
lastage <- age[length(age)]
# list(n =n, ly2n = ly2n, lyn = lyn,
# ly = ly, y = y, lastage = lastage)   OK
b <- (log(ly2n / lyn) / log(lyn / ly)) ** (1 / n)
a <- exp(log(lyn / ly) / ((b ** y) * (b ** n - 1)))
C <- ly * exp(-(b ** y) * log(a))
equation <- function(d) C * (a ** (b ** d))
# return(equation(c(90,95,100,105,110))) OK
ageseq <- seq(lastage + n, lastage + (20 * n), by=n)
l.est <- equation(ageseq[1])
newl.est <- 1
i <- 1
while(newl.est >= 0.5){
newl.est <- equation(ageseq[i])
l.est <- c(l.est, newl.est)
i <- i + 1
}
app.age <- ageseq[1:length(l.est)]
list(app.age = app.age, l.est = round(l.est, digits=0))
}


The components in the list are the things I
would like to add to the columns in my incomplete
life table.

If this is too dangerous (quaint etc.), 
I will not do it that way.

If you can recommend me more elegant ways
to do what I wanted, I will be grateful and
promise never ever to post this kind of
functions on this list ;-)


Regards,


Tobias



From sundar.dorai-raj at pdf.com  Wed Jul  9 21:54:05 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 09 Jul 2003 14:54:05 -0500
Subject: [R] .Internal(optim)
References: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>
Message-ID: <3F0C72DD.70902@pdf.com>



Edoardo Airoldi wrote:
>> hi all,
>>  I am using optim.  I am getting the following error message:
>>
>> Error in optim(par = start.vals[, h], fn = post.func.pois, gr = 
>> post.grad.
>> pois,  :
>>         L-BFGS-B needs finite values of fn
>>
>> If I look at optim typing '> optim' it seems that the error comes from 
>> inside .Internal(optim), so I wonder how can I see the code for 
>> .Internal(
>> optim)?
>>
>> thanks
>> Edo
>>
>>
> 

Actually, the error means that your function (post.func.pois) is 
returning a infinite value. I usually put print statements in to see 
where this is happening. As in,

post.func.pois = function(par, ...) {
...
print(par)
print(some.operation(par)) # etc.
...
return(value) # if this is Inf, optim will stop
}

If you really think you need to see the optim source code, you can 
downdload the source from cran and look for optim.c.

Regards,
Sundar



From dmurdoch at pair.com  Wed Jul  9 22:09:24 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 09 Jul 2003 16:09:24 -0400
Subject: [R] .Internal(optim)
In-Reply-To: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>
References: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>
Message-ID: <m0togvo1vb5pnnt2rbon567m6947mho92q@4ax.com>

On Wed, 9 Jul 2003 15:33:12 -0400, Edoardo Airoldi
<eairoldi at stat.cmu.edu> wrote :

>> hi all,
>>  I am using optim.  I am getting the following error message:
>>
>> Error in optim(par = start.vals[, h], fn = post.func.pois, gr = post.grad.
>> pois,  :
>>         L-BFGS-B needs finite values of fn
>>
>> If I look at optim typing '> optim' it seems that the error comes from 
>> inside .Internal(optim), so I wonder how can I see the code for .Internal(
>> optim)?

Anything that's called by .Internal is C or Fortran code.  You need to
download the R source package, decompress it somewhere, and start
looking around the src directory.

Internal functions are all listed in a big array declared in the
src/main/names.c file.  For example, this line is in that file:

{"optim",	do_optim,	0,	11,	7,	{PP_FUNCALL,
PREC_FN,	0}},

This says that "do_optim" is the function that does all the work.  The
other numbers describe things like how many arguments it takes, how to
deparse it, etc.  You then need to search through all the source to
find do_optim; it's in the file src/main/optim.c.  Reading C code that
works with R objects (declared as SEXP) is not easy, but that's where
you'll need to look.

In the end, I don't think all that work is going to be very fruitful.
You'll just find out that your objective function needs to return
finite values.  You should try to figure out why it doesn't.  Perhaps
if you set the bounds on the region a little smaller you'll fix
things?

Duncan Murdoch



From spencer.graves at pdf.com  Wed Jul  9 22:09:14 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 13:09:14 -0700
Subject: [R] A problem with using the "outer" function
References: <2bee062be09e.2be09e2bee06@jhsph.edu>
Message-ID: <3F0C766A.8070901@pdf.com>

Have you tried:

	outer(b1,b2,FUN=bpllkd,x=x,y=y,n=n)

I don't think "outer" quite knows what to do with "x, y, n)".  However, 
"x=x1, y=y1, n=n1") clearly tells outer to pass the object x1 to the 
argument x of function bpllkd, etc.

hope this helps.  spencer graves
p.s.  I was just bitten by that snake last week.

Ravi Varadhan wrote:
> Hi:
> 
> I am using R 1.7.0 on Windows.  I am having trouble getting "outer" to 
> work on one of my functions.  Here is a simple example illustrating my 
> problem:
> 
> 
>>b1 <- c(1.2,2.3)
>>b2 <- c(0.5,0.6)
>>x <- c(3e+01, 1e+02, 3e+02, 5e+02, 1e+03, 1e+04, 1e+05, 1e+06)
>>y <- c(2,4,2,5,2,3,1,1)
>>n <- c(5,8,3,6,2,3,1,1)
> 
> 
>>outer(b1,b2,FUN=bpllkd,x,y,n)
> 
>          [,1]     [,2]
> [1,] 17.78031 17.78031
> [2,] 17.78031 17.78031
> 
> These values should all be different. What is the problem here?
> The function "bpllkd" is given below:
> 
> thanks for any help,
> Ravi.
> 
> 
>>bpllkd 
> 
> function(t1,t2,x,y,n){
> p <- 1 - (1+x/10^t1)^(-t2)
> keep <- !((p==0 & y==0) | (p==1 & n==y))
> llk <- sum(y[keep]*log(p[keep])+(n-y)[keep]*
> log(1-p[keep]))
> return(-llk)
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Wed Jul  9 22:15:19 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 09 Jul 2003 16:15:19 -0400
Subject: [R] A problem with using the "outer" function
In-Reply-To: <2bee062be09e.2be09e2bee06@jhsph.edu>
References: <2bee062be09e.2be09e2bee06@jhsph.edu>
Message-ID: <ultogv8n0a4o4hoqr3slhpqcn0kcp4cbvo@4ax.com>

On Wed, 09 Jul 2003 15:33:11 -0400, Ravi Varadhan <rvaradha at jhsph.edu>
wrote :

>Hi:
>
>I am using R 1.7.0 on Windows.  I am having trouble getting "outer" to 
>work on one of my functions. 

Most likely the problem is that the function you give doesn't work on
array arguments.  Your function needs to take two arrays of the same
shape as the first two arguments, and return an array of answers.
outer() doesn't work by looping, it works by constructing big arrays
of inputs and making just one function call.

Duncan Murdoch



From tobias_verbeke at skynet.be  Wed Jul  9 22:24:01 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Wed, 9 Jul 2003 22:24:01 +0200
Subject: [R] `acting' on variables
In-Reply-To: <OF22B54A8C.D2FA515E-ON85256D5E.006E2D2B@convergys.com>
References: <OF22B54A8C.D2FA515E-ON85256D5E.006E2D2B@convergys.com>
Message-ID: <20030709222401.107b9ef9.tobias_verbeke@skynet.be>

Favete linguis...

Thanks,
Tobias



From dmurdoch at pair.com  Wed Jul  9 22:25:38 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 09 Jul 2003 16:25:38 -0400
Subject: [R] .Internal(optim)
In-Reply-To: <m0togvo1vb5pnnt2rbon567m6947mho92q@4ax.com>
References: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>
	<m0togvo1vb5pnnt2rbon567m6947mho92q@4ax.com>
Message-ID: <ofuogvo6cpg3gfb2ragapmdq5a8di8v4hc@4ax.com>

On Wed, 09 Jul 2003 16:09:24 -0400, Duncan Murdoch <dmurdoch at pair.com>
wrote :

>Anything that's called by .Internal is C or Fortran code.  

Whoops, .Internal would only call C.  Fortran can't handle the R
objects.  The C code might call some Fortran to do the work.

Duncan Murdoch



From spencer.graves at pdf.com  Wed Jul  9 22:44:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jul 2003 13:44:20 -0700
Subject: [R] .Internal(optim)
References: <2D7D083D-B244-11D7-9F78-00039390FFC4@stat.cmu.edu>
	<m0togvo1vb5pnnt2rbon567m6947mho92q@4ax.com>
Message-ID: <3F0C7EA4.1000708@pdf.com>

I haven't used optim enough to know, but simular functions ostensibly 
with box constraints would still test values outside the box and quite 
if it got an NA or possibly Inf.  I've had good luck transforming the 
inputs to remove constraints.  For example, if you need x > 0, then 
parameterize the function in terms of log.x, then compute "x <- 
exp(log.x)" as a first step of the function.  Then 0 will never be 
tested by the function minimizer.  When I do this, the log(likelihood) 
surface is often more nearly parabolic in terms of log.x than in terms 
of x.  This means that any normal approximation theory is more likely to 
work better in terms of log.x than in terms of x.

hope this helps.  spencer graves

Duncan Murdoch wrote:
> On Wed, 9 Jul 2003 15:33:12 -0400, Edoardo Airoldi
> <eairoldi at stat.cmu.edu> wrote :
> 
> 
>>>hi all,
>>> I am using optim.  I am getting the following error message:
>>>
>>>Error in optim(par = start.vals[, h], fn = post.func.pois, gr = post.grad.
>>>pois,  :
>>>        L-BFGS-B needs finite values of fn
>>>
>>>If I look at optim typing '> optim' it seems that the error comes from 
>>>inside .Internal(optim), so I wonder how can I see the code for .Internal(
>>>optim)?
>>
> 
> Anything that's called by .Internal is C or Fortran code.  You need to
> download the R source package, decompress it somewhere, and start
> looking around the src directory.
> 
> Internal functions are all listed in a big array declared in the
> src/main/names.c file.  For example, this line is in that file:
> 
> {"optim",	do_optim,	0,	11,	7,	{PP_FUNCALL,
> PREC_FN,	0}},
> 
> This says that "do_optim" is the function that does all the work.  The
> other numbers describe things like how many arguments it takes, how to
> deparse it, etc.  You then need to search through all the source to
> find do_optim; it's in the file src/main/optim.c.  Reading C code that
> works with R objects (declared as SEXP) is not easy, but that's where
> you'll need to look.
> 
> In the end, I don't think all that work is going to be very fruitful.
> You'll just find out that your objective function needs to return
> finite values.  You should try to figure out why it doesn't.  Perhaps
> if you set the bounds on the region a little smaller you'll fix
> things?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lisas at salford-systems.com  Wed Jul  9 23:48:25 2003
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 09 Jul 2003 14:48:25 -0700
Subject: [R] CFP: CART Data Mining Conference 2004
Message-ID: <3F0C8DA9.6010908@salford-systems.com>

Apologies for cross posting....

---------------------------------------------------------------------
  CART Data Mining'04: First International CART(R) Conferences
         Focusing on the Data Mining technology of
   Leo Breiman, Jerome Friedman, Richard Olshen, Charles Stone
         (CART, MARS(R), TreeNet(tm), PRIM(tm)...)

                First Call For submissions
---------------------------------------------------------------------

       US Venue:  San Francisco, March 23-25, 2004
       EU Venue:  Madrid,        May   25-26, 2004

    Conference home page: http://www.cartdatamining.com

   ----------------------------------------------------------
   ---                                                    ---
   ---               CALL FOR SUBMISSIONS                 ---
   ---        Submission deadline: October 27, 2003       ---
   ---                                                    ---
   ----------------------------------------------------------

Keynote Speakers:

Leo Breiman,     University of California, Berkeley
Jerome Friedman, Stanford University
Richard Olshen,  Stanford University
Charles Stone,   University of California, Berkeley

Conference Sponsor:         Salford Systems

The conferences are intended to serve several functions:

o A festschrift and opportunity to honor the four
  authors of CART and meet with them in person. Each
  is planning to offer a keynote paper.

o A venue to exchange ideas and experiences focused
  on the practice of data mining.

o A networking opportunity leading to the creation of
local user  groups and the establishment of a user
newsletter.

o A place to learn about extensions to CART related
technology and anticipated future developments

o An opportunity to obtain both basic and advanced training
offered by practical and theoretical experts

The conference series will provide an opportunity for data mining
professionals to exchange ideas on the art and practice of the real
world analysis of complex data.  Contributed papers covering any
application of CART, MARS, PRIM, and TreeNet are encouraged,
including innovative and unusual applications.

Breiman's Random Forests will be the subject of an introductory
tutorial.  A workshop devoted solely to RF will be scheduled separately.

  ----------------------------------------------------------
      Topics of Interest
  ----------------------------------------------------------

We welcome applied data analysis papers from any industry or field of
study.
Illustrative industry sessions under consideration and requesting papers
include

Financial Services
  Targeted marketing, customer Acquisition
  Fraud Detection
  CRM: Customer Retention
  Risk management and score card development
  Loss management in insurance
Financial Markets Modeling
  Stock Selection and Portfolio Management
  Business Cycle Forecasting
Telecommunications
  Churn modeling
  Collections management
  Fraud detection
Bioinformatics, Healthcare and Medicine
  DNA Microarray Data Analysis
  Proteomics
  Drug Discovery
Web Mining and Text Mining
  User profiling
  Recommendation Systems
  Learning from Designed Experiments
          eCommerce
Engineering, Manufacturing, and Quality Control
  Semiconductor quality control
Public Sector, Defense, Security

Papers may have a methological focus and cover any key area of
data mining such data quality assessment, missing value
imputation, feature selection, model selection, ensemble methods,
but should be rooted in a substantive industrial data mining
context and report on real world data.

Conference Participation

If you have an interest in attending or presenting at this conference
please let us know via email at info at cartdatamining.com. The conference
web site will contain a form for indicating the topic of a presentation
you are considering presenting.

Submission and Format of Papers

Presentations may be in the form of technical papers or power point
slide shows and are expected to last about 45 minutes. Technical papers
should not exceed 6,000 words (approximately 20 A4 pages) and power
point presentations should not exceed 40 slides.  If submitting
powerpoint please include additional detailed discussion notes.

We welcome submissions for either venue and expect that only a few
papers will be presented at both locations.


-----------------------------------------------------------------------
IMPORTANT DATES
-----------------------------------------------------------------------

October 27,   2003    Submission deadline for contributed papers

December 22,  2003    Notification of paper/presentation acceptance

January  19,  2004   Camera-ready presentations due

March 23-24,  2004    main conference San Francisco
May   25-26   2004    Madrid conference



From lisas at salford-systems.com  Wed Jul  9 23:55:45 2003
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 09 Jul 2003 14:55:45 -0700
Subject: [R] CFP: CART Data Mining Conference 2004
Message-ID: <3F0C8F61.3010306@salford-systems.com>

Apologies for cross posting....

---------------------------------------------------------------------
 CART Data Mining'04: First International CART(R) Conferences
        Focusing on the Data Mining technology of
  Leo Breiman, Jerome Friedman, Richard Olshen, Charles Stone
        (CART, MARS(R), TreeNet(tm), PRIM(tm)...)

               First Call For submissions
---------------------------------------------------------------------

      US Venue:  San Francisco, March 23-25, 2004
      EU Venue:  Madrid,        May   25-26, 2004

   Conference home page: http://www.cartdatamining.com

  ----------------------------------------------------------
  ---                                                    ---
  ---               CALL FOR SUBMISSIONS                 ---
  ---        Submission deadline: October 27, 2003       ---
  ---                                                    ---
  ----------------------------------------------------------

Keynote Speakers:

Leo Breiman,     University of California, Berkeley
Jerome Friedman, Stanford University
Richard Olshen,  Stanford University
Charles Stone,   University of California, Berkeley

Conference Sponsor:         Salford Systems

The conferences are intended to serve several functions:

o A festschrift and opportunity to honor the four
 authors of CART and meet with them in person. Each
 is planning to offer a keynote paper.

o A venue to exchange ideas and experiences focused
 on the practice of data mining.

o A networking opportunity leading to the creation of
local user  groups and the establishment of a user
newsletter.

o A place to learn about extensions to CART related
technology and anticipated future developments

o An opportunity to obtain both basic and advanced training
offered by practical and theoretical experts

The conference series will provide an opportunity for data mining
professionals to exchange ideas on the art and practice of the real
world analysis of complex data.  Contributed papers covering any
application of CART, MARS, PRIM, and TreeNet are encouraged,
including innovative and unusual applications.

Breiman's Random Forests will be the subject of an introductory
tutorial.  A workshop devoted solely to RF will be scheduled separately.

 ----------------------------------------------------------
     Topics of Interest
 ----------------------------------------------------------

We welcome applied data analysis papers from any industry or field of
study.
Illustrative industry sessions under consideration and requesting papers
include

Financial Services
 Targeted marketing, customer Acquisition
 Fraud Detection
 CRM: Customer Retention
 Risk management and score card development
 Loss management in insurance
Financial Markets Modeling
 Stock Selection and Portfolio Management
 Business Cycle Forecasting
Telecommunications
 Churn modeling
 Collections management
 Fraud detection
Bioinformatics, Healthcare and Medicine
 DNA Microarray Data Analysis
 Proteomics
 Drug Discovery
Web Mining and Text Mining
 User profiling
 Recommendation Systems
 Learning from Designed Experiments
         eCommerce
Engineering, Manufacturing, and Quality Control
 Semiconductor quality control
Public Sector, Defense, Security

Papers may have a methological focus and cover any key area of
data mining such data quality assessment, missing value
imputation, feature selection, model selection, ensemble methods,
but should be rooted in a substantive industrial data mining
context and report on real world data.

Conference Participation

If you have an interest in attending or presenting at this conference
please let us know via email at info at cartdatamining.com. The conference
web site will contain a form for indicating the topic of a presentation
you are considering presenting.

Submission and Format of Papers

Presentations may be in the form of technical papers or power point
slide shows and are expected to last about 45 minutes. Technical papers
should not exceed 6,000 words (approximately 20 A4 pages) and power
point presentations should not exceed 40 slides.  If submitting
powerpoint please include additional detailed discussion notes.

We welcome submissions for either venue and expect that only a few
papers will be presented at both locations.


-----------------------------------------------------------------------
IMPORTANT DATES
-----------------------------------------------------------------------

October 27,   2003    Submission deadline for contributed papers

December 22,  2003    Notification of paper/presentation acceptance

January  19,  2004   Camera-ready presentations due

March 23-24,  2004    main conference San Francisco
May   25-26   2004    Madrid conference



From lisas at salford-systems.com  Thu Jul 10 00:01:29 2003
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 09 Jul 2003 15:01:29 -0700
Subject: [R] CFP: CART Data Mining Conference 2004
Message-ID: <3F0C90B9.7040003@salford-systems.com>

Apologies for cross posting....

---------------------------------------------------------------------
CART Data Mining'04: First International CART(R) Conferences
       Focusing on the Data Mining technology of
 Leo Breiman, Jerome Friedman, Richard Olshen, Charles Stone
       (CART, MARS(R), TreeNet(tm), PRIM(tm)...)

              First Call For submissions
---------------------------------------------------------------------

     US Venue:  San Francisco, March 23-25, 2004
     EU Venue:  Madrid,        May   25-26, 2004

  Conference home page: http://www.cartdatamining.com

 ----------------------------------------------------------
 ---                                                    ---
 ---               CALL FOR SUBMISSIONS                 ---
 ---        Submission deadline: October 27, 2003       ---
 ---                                                    ---
 ----------------------------------------------------------

Keynote Speakers:

Leo Breiman,     University of California, Berkeley
Jerome Friedman, Stanford University
Richard Olshen,  Stanford University
Charles Stone,   University of California, Berkeley

Conference Sponsor:         Salford Systems

The conferences are intended to serve several functions:

o A festschrift and opportunity to honor the four
authors of CART and meet with them in person. Each
is planning to offer a keynote paper.

o A venue to exchange ideas and experiences focused
on the practice of data mining.

o A networking opportunity leading to the creation of
local user  groups and the establishment of a user
newsletter.

o A place to learn about extensions to CART related
technology and anticipated future developments

o An opportunity to obtain both basic and advanced training
offered by practical and theoretical experts

The conference series will provide an opportunity for data mining
professionals to exchange ideas on the art and practice of the real
world analysis of complex data.  Contributed papers covering any
application of CART, MARS, PRIM, and TreeNet are encouraged,
including innovative and unusual applications.

Breiman's Random Forests will be the subject of an introductory
tutorial.  A workshop devoted solely to RF will be scheduled separately.

----------------------------------------------------------
    Topics of Interest
----------------------------------------------------------

We welcome applied data analysis papers from any industry or field of
study.
Illustrative industry sessions under consideration and requesting papers
include

Financial Services
Targeted marketing, customer Acquisition
Fraud Detection
CRM: Customer Retention
Risk management and score card development
Loss management in insurance
Financial Markets Modeling
Stock Selection and Portfolio Management
Business Cycle Forecasting
Telecommunications
Churn modeling
Collections management
Fraud detection
Bioinformatics, Healthcare and Medicine
DNA Microarray Data Analysis
Proteomics
Drug Discovery
Web Mining and Text Mining
User profiling
Recommendation Systems
Learning from Designed Experiments
        eCommerce
Engineering, Manufacturing, and Quality Control
Semiconductor quality control
Public Sector, Defense, Security

Papers may have a methological focus and cover any key area of
data mining such data quality assessment, missing value
imputation, feature selection, model selection, ensemble methods,
but should be rooted in a substantive industrial data mining
context and report on real world data.

Conference Participation

If you have an interest in attending or presenting at this conference
please let us know via email at info at cartdatamining.com. The conference
web site will contain a form for indicating the topic of a presentation
you are considering presenting.

Submission and Format of Papers

Presentations may be in the form of technical papers or power point
slide shows and are expected to last about 45 minutes. Technical papers
should not exceed 6,000 words (approximately 20 A4 pages) and power
point presentations should not exceed 40 slides.  If submitting
powerpoint please include additional detailed discussion notes.

We welcome submissions for either venue and expect that only a few
papers will be presented at both locations.


-----------------------------------------------------------------------
IMPORTANT DATES
-----------------------------------------------------------------------

October 27,   2003    Submission deadline for contributed papers

December 22,  2003    Notification of paper/presentation acceptance

January  19,  2004   Camera-ready presentations due

March 23-24,  2004    main conference San Francisco
May   25-26   2004    Madrid conference



From djw1005 at cam.ac.uk  Thu Jul 10 01:00:24 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Thu, 10 Jul 2003 00:00:24 +0100 (BST)
Subject: [R] `acting' on variables
In-Reply-To: <20030709215401.344f34b6.tobias_verbeke@skynet.be>
Message-ID: <Pine.SOL.3.96.1030709233947.13551B-100000@libra.cus.cam.ac.uk>


> gompertz.estimates <- function(age, surv){
>   ...
>   app.age <- ageseq[1:length(l.est)]
>   list(app.age = app.age, l.est = round(l.est, digits=0))
>   }
> The components in the list are the things I
> would like to add to the columns in my incomplete
> life table.

The way I would do this is by adding to the function:
  gompertz.estimates <- function(age,surv) {
    ...
    list(age=c(age,app.age), surv=c(surv,l.est))
    }
and then invoke it by
  res <- gompertz.estimates(age,surv)
  age <- res$age
  surv <- res$surv

A dangerous way to do what I think you asked can be programmed along these
lines:
  f <- function(i) {
    nameOfParam <- deparse(substitute(i))
    j <- i*3
    assign(nameOfParam, j, inherits=TRUE)
    "I'm an evil function with side-effects"
    }
  val <- 3
  f(val)
  val
You'll see that val has been altered behind your back. One trouble with
this function is that it demands to be called with a straight variable. If
you do 
  f(val+3)
it won't work. I can't imagine any situation where this approach is
useful. I have on one occasion needed side-effects, and the <<- operator
was useful in that case. 

Damon.



From Toby.Patterson at csiro.au  Thu Jul 10 03:41:33 2003
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Thu, 10 Jul 2003 11:41:33 +1000
Subject: [R] Add a legend to a pairs plot??
Message-ID: <C4178DC99E08604EA5E2BDB989F09380241F57@extas2-hba.tas.csiro.au>

All, 
Can someone tell me if it is possible to add a legend to a plot produced
with pairs()? 

I have an example akin to plotting the iris data given in the pairs help
and would like to show what the symbols refer to somewhere. 

Thanks.



From alobo at ija.csic.es  Thu Jul 10 10:18:56 2003
From: alobo at ija.csic.es (Agustin Lobo)
Date: Thu, 10 Jul 2003 10:18:56 +0200 (MET DST)
Subject: [R] clim.pact (fwd)
Message-ID: <Pine.OSF.3.91.1030710093405.30732A-100000@paleo.ija.csic.es>


I'm trying to use the clim.pact
package but I cannot find the
descritions of "map object or "field object". For example,
according to the man page of function "map":

"
Description 
	Produces maps. 

Usage 
	map(x,y=NULL,col="black",lwd=1,lty=1,sym=TRUE, plot=TRUE,inv.col=FALSE) 

Arguments x A map object.
...
"

and according to the man page of plotField:

"
Usage 
	plotField(x,lon=NULL,lat=NULL,tim=NULL,mon=NULL, 
	col="black",lty=1,lwd=1,what="ano") 

Arguments x A field object.
"

retrieve.nc might be a clue, but the
files mentioned in the man page are not included
in the package:

"
Description 
	Reads a netCDF file and picks out vectors that look like 
	lngitude, latitude and time. Returns the first 3-D field in the file.
Usage
	retrieve.nc(f.name="data/ncep_t2m.nc",..
.../...

Examples
	X.1 <- retrieve.nc("data/mpi-gsdio_t2m.nc", 
		x.rng=c(-60,40),y.rng=c(50,75)) 
	X.2 <- retrieve.nc("data/mpi-gsdio_slp.nc", 
		x.rng=c(-60,40),y.rng=c(50,75))
"

I've downloaded a sample nc file from 
http://www.epic.noaa.gov/java/ncBrowse/
but don't get to use plotField.

Does anyone have any experience using this 
package?

Thanks

Agus

Dr. Agustin Lobo
Instituto de Ciencias de la Tierra (CSIC)
Lluis Sole Sabaris s/n
08028 Barcelona SPAIN
tel 34 93409 5410
fax 34 93411 0012
alobo at ija.csic.es



From joerg-burmester at gmx.net  Thu Jul 10 10:52:47 2003
From: joerg-burmester at gmx.net (joerg-burmester@gmx.net)
Date: Thu, 10 Jul 2003 10:52:47 +0200 (MEST)
Subject: [R] please help on frag polynoms
Message-ID: <21347.1057827167@www47.gmx.net>

hi there,

can anyone help me on the topic of frag polynoms?

i just heard of a friend of mine, that i could build in a functioon called
fragpoly (he was talking of such a function in the 'stata' language) in order
to improve my process of finding an optimal linear model.

instead of trying a vast amount of transformed inputdata to find the best
fit and then step backwards down to e.g. rank 5 in order to get a smooth curve,
i could start right from the beginning with only very few but therefore very
flexible funktions (litle similar to box-plot) which adopt themselve
automaticaly to an optimal fitt and r-squered.

can anyone tell me if such a flexible adoptive function also exists in r+?
(i could not find anything on the r+ pages with these search words).

waiting desperately for help
joerg



From alain.guerreau at fnac.net  Thu Jul 10 11:03:34 2003
From: alain.guerreau at fnac.net (guerreau)
Date: Thu, 10 Jul 2003 11:03:34 +0200
Subject: [R] size of graphics device
Message-ID: <001501c346c2$3df482a0$78e1273e@jetzt0>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030710/fe63e0bc/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jul 10 11:34:31 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jul 2003 11:34:31 +0200
Subject: [R] Add a legend to a pairs plot??
In-Reply-To: <C4178DC99E08604EA5E2BDB989F09380241F57@extas2-hba.tas.csiro.au>
References: <C4178DC99E08604EA5E2BDB989F09380241F57@extas2-hba.tas.csiro.au>
Message-ID: <3F0D3327.7070108@statistik.uni-dortmund.de>

Toby.Patterson at csiro.au wrote:

> All, 
> Can someone tell me if it is possible to add a legend to a plot produced
> with pairs()? 

It is possible, but it depends on where you want the legend(s) to be 
placed. One for each variable (argument diag.panel helps inthis case)? 
Just one for the whole pairs() plot (Divide the plot or just plot the 
lower triangle and use the upper for a legend, e.g.)?

Uwe Ligges


> I have an example akin to plotting the iris data given in the pairs help
> and would like to show what the symbols refer to somewhere. 
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From christmann at statistik.uni-dortmund.de  Thu Jul 10 12:39:27 2003
From: christmann at statistik.uni-dortmund.de (Andreas Christmann)
Date: Thu, 10 Jul 2003 12:39:27 +0200
Subject: [R] RE: packaged datasets in .csv format (David Firth)
References: <200307101012.h6AA42eR002818@stat.math.ethz.ch>
Message-ID: <3F0D425F.30104@statistik.uni-dortmund.de>

> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Wed, 9 Jul 2003 10:53:27 +0100
> From: David Firth <david.firth at nuffield.oxford.ac.uk>
> Subject: [R] packaged datasets in .csv format
> To: r-help at stat.math.ethz.ch
> Message-ID:
> 	<307D34CE-B1F3-11D7-A8D2-0050E4C03977 at nuffield.oxford.ac.uk>
> Content-Type: text/plain; charset=US-ASCII; format=flowed
> 
> A couple of questions in connection with using .csv format to include 
> data in a package:
> 
> First, the background.  The data() function loads data from .csv 
> ("comma-separated values") files using
> 
>    read.table(..., header = TRUE, sep = ";")
> 
> But ?read.table says
> 
>       ## To write a CSV file for input to Excel one might use
>       write.table(x, file = "foo.csv", sep = ",", col.names = NA)
>       ## and to read this file back into R one needs
>       read.table("file.csv", header = TRUE, sep = ",", row.names=1)
> 
> As a result, .csv files created by write.table() as above are not read 
> in by data() in the way that might be expected [that is, expected by 
> someone who had not read help(data)!]
> 
> Two questions, then:
> -- is there some compelling reason for  the use of `sep = ";"' in place 
> of `sep = ",", row.names=1'?

I prefer ";" instead of "," , because in text variables there are often 
",".


> -- if I want to maintain a dataset in .csv format, for use both in R 
> and in other systems such as Excel, SPSS, etc, what is the best way to 
> go about it?

Depends. Perhaps it is best to check it out for the software packages
and the versions of the software packages you are using.

Andreas Christmann

> 
> Any advice would be much appreciated.
> 
> Cheers,
> David



From gbrumen at student.ethz.ch  Thu Jul 10 12:54:33 2003
From: gbrumen at student.ethz.ch (Gorazd Brumen)
Date: Thu, 10 Jul 2003 10:54:33 -0000
Subject: [R] Simple linear regression
Message-ID: <1057834486.5275.3.camel@misko.homelinux.net>

Dear all,

My friend wants to fit a model of the type

z = a x^n y^m + b,

where x, y, z are data and a, b, n, m are unknown parameters. 

How can he transform this to fit in the linear regression framework? 
Any help would be appreciated.

With regards, 
Gorazd Brumen

-- 
Mail 1: gbrumen at student.ethz.ch
Mail 2: gorazd.brumen at fmf.uni-lj.si
Tel.: +41 (0)1 63 34906
Homepage: valjhun.fmf.uni-lj.si/~brumen



From kwan022 at stat.auckland.ac.nz  Thu Jul 10 13:00:00 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 10 Jul 2003 23:00:00 +1200 (NZST)
Subject: [R] Simple linear regression
In-Reply-To: <1057834486.5275.3.camel@misko.homelinux.net>
Message-ID: <Pine.LNX.4.44.0307102259410.18163-100000@stat55.stat.auckland.ac.nz>

Try:
  ?lm

On 10 Jul 2003, Gorazd Brumen wrote:

> Date: 10 Jul 2003 12:54:46 +0200
> From: Gorazd Brumen <gbrumen at student.ethz.ch>
> To: R-help at stat.math.ethz.ch
> Subject: [R] Simple linear regression
> 
> Dear all,
> 
> My friend wants to fit a model of the type
> 
> z = a x^n y^m + b,
> 
> where x, y, z are data and a, b, n, m are unknown parameters. 
> 
> How can he transform this to fit in the linear regression framework? 
> Any help would be appreciated.

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ligges at statistik.uni-dortmund.de  Thu Jul 10 13:25:32 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jul 2003 13:25:32 +0200
Subject: [R] RE: packaged datasets in .csv format (David Firth)
In-Reply-To: <3F0D425F.30104@statistik.uni-dortmund.de>
References: <200307101012.h6AA42eR002818@stat.math.ethz.ch>
	<3F0D425F.30104@statistik.uni-dortmund.de>
Message-ID: <3F0D4D2C.1030204@statistik.uni-dortmund.de>

Andreas Christmann wrote:
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Wed, 9 Jul 2003 10:53:27 +0100
>> From: David Firth <david.firth at nuffield.oxford.ac.uk>
>> Subject: [R] packaged datasets in .csv format
>> To: r-help at stat.math.ethz.ch
>> Message-ID:
>>     <307D34CE-B1F3-11D7-A8D2-0050E4C03977 at nuffield.oxford.ac.uk>
>> Content-Type: text/plain; charset=US-ASCII; format=flowed
>>
>> A couple of questions in connection with using .csv format to include 
>> data in a package:
>>
>> First, the background.  The data() function loads data from .csv 
>> ("comma-separated values") files using
>>
>>    read.table(..., header = TRUE, sep = ";")
>>
>> But ?read.table says
>>
>>       ## To write a CSV file for input to Excel one might use
>>       write.table(x, file = "foo.csv", sep = ",", col.names = NA)
>>       ## and to read this file back into R one needs
>>       read.table("file.csv", header = TRUE, sep = ",", row.names=1)
>>
>> As a result, .csv files created by write.table() as above are not read 
>> in by data() in the way that might be expected [that is, expected by 
>> someone who had not read help(data)!]
>>
>> Two questions, then:
>> -- is there some compelling reason for  the use of `sep = ";"' in 
>> place of `sep = ",", row.names=1'?

Do you really want an answer?
Today, one reason is compatibility to all the other packages on CRAN.


> I prefer ";" instead of "," , because in text variables there are often 
> ",".

That's why text variables can be quoted.


> 
>> -- if I want to maintain a dataset in .csv format, for use both in R 
>> and in other systems such as Excel, SPSS, etc, what is the best way to 
>> go about it?

When regularly using that many systems on the same data sets, it might 
be worth using a database system, e.g. MySQL.

BTW: R *and* Excel *and* (for sure, but I haven't tested) also SPSS can 
read a couple of different ASCII formatted files, so there are quite a 
lot possible formats.

Uwe Ligges


> Depends. Perhaps it is best to check it out for the software packages
> and the versions of the software packages you are using.
 >
> Andreas Christmann
> 
>>
>> Any advice would be much appreciated.
>>
>> Cheers,
>> David



From wolski at molgen.mpg.de  Thu Jul 10 13:54:37 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 10 Jul 2003 13:54:37 +0200
Subject: [R] XML Package.
Message-ID: <200307101354370866.061AF50E@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030710/6776d92a/attachment.pl

From upton at mitre.org  Thu Jul 10 14:09:03 2003
From: upton at mitre.org (Stephen C. Upton)
Date: Thu, 10 Jul 2003 08:09:03 -0400
Subject: [R] XML Package.
In-Reply-To: <200307101354370866.061AF50E@harry.molgen.mpg.de>
References: <200307101354370866.061AF50E@harry.molgen.mpg.de>
Message-ID: <3F0D575F.7020503@mitre.org>

Eryk,

If you go here on the CRAN, you should be able to find an XML.zip


  /pub/languages/R/CRAN/bin/windows/contrib

HTH
steve

Wolski wrote:

>Hi!
>I have installed the new R on windows.
>I wanted to reinstall the XML package. I am not able to find the XML.zip anymore. I am quite shure that they where a windows binary version.
>Has anyone old XML windows binary?
>Eryk
>
>Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics
>Ihnestrasse 73 14195 Berlin          'v'
>tel: 0049-30-84131285               /   \
>mail: wolski at molgen.mpg.de        ---W-W----
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From duncan at research.bell-labs.com  Thu Jul 10 14:09:58 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Thu, 10 Jul 2003 08:09:58 -0400
Subject: [R] XML Package.
In-Reply-To: <200307101354370866.061AF50E@harry.molgen.mpg.de>;
	from wolski@molgen.mpg.de on Thu, Jul 10, 2003 at 01:54:37PM +0200
References: <200307101354370866.061AF50E@harry.molgen.mpg.de>
Message-ID: <20030710080958.B23503@jessie.research.bell-labs.com>


Brian Ripley has kindly compiled the package for Windows and 
has made it available at 

  http://www.stats.ox.ac.uk/pub/RWin

along with selected other packages.

  D.

Wolski wrote:
> Hi!
> I have installed the new R on windows.
> I wanted to reinstall the XML package. I am not able to find the XML.zip anymore. I am quite shure that they where a windows binary version.
> Has anyone old XML windows binary?
> Eryk
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate Genomics
> Ihnestrasse 73 14195 Berlin          'v'
> tel: 0049-30-84131285               /   \
> mail: wolski at molgen.mpg.de        ---W-W----
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From vandemem at gmx.de  Thu Jul 10 14:13:20 2003
From: vandemem at gmx.de (Marc Vandemeulebroecke)
Date: Thu, 10 Jul 2003 14:13:20 +0200 (MEST)
Subject: [R] group sequential and adaptive designs
Message-ID: <13419.1057839200@www47.gmx.net>

Hello R users,

I am looking for R (or S) code related to group sequential or adaptive
designs for clinical trials. (The most prominent examples are the designs of
Pocock or O'Brien/Fleming, the alpha-spending function approach, or Fisher's
combination test and the inverse normal method.) I am particularly interested in
the calculation of the critical boundaries, the handling of spending functions
and, most of all, power calculations. All I could find were announcements of
courses or postings related to the S-Plus SeqTrial module. No code. Can
anyone give a hint?

Many thanks in advance,
Marc

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!



From maechler at stat.math.ethz.ch  Thu Jul 10 14:20:22 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 10 Jul 2003 14:20:22 +0200
Subject: [R] Simple linear regression
In-Reply-To: <Pine.LNX.4.44.0307102259410.18163-100000@stat55.stat.auckland.ac.nz>
References: <1057834486.5275.3.camel@misko.homelinux.net>
	<Pine.LNX.4.44.0307102259410.18163-100000@stat55.stat.auckland.ac.nz>
Message-ID: <16141.23046.756015.965088@gargle.gargle.HOWL>

>>>>> "KKWa" == Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz>
>>>>>     on Thu, 10 Jul 2003 23:00:00 +1200 (NZST) writes:

    KKWa> Try: ?lm

no.  see below

    KKWa> On 10 Jul 2003, Gorazd Brumen wrote:

    >> Date: 10 Jul 2003 12:54:46 +0200 From: Gorazd Brumen
    >> <gbrumen at student.ethz.ch> To: R-help at stat.math.ethz.ch
    >> Subject: [R] Simple linear regression
    >> 
    >> Dear all,
    >> 
    >> My friend wants to fit a model of the type
    >> 
    >> z = a x^n y^m + b,
    >> 
    >> where x, y, z are data and a, b, n, m are unknown
    >> parameters.
    >> 
    >> How can he transform this to fit in the linear regression
    >> framework?  Any help would be appreciated.

He can't.  When all 4   a, b, n, m  are parameters, this is a
non-linear regression problem.  --> Function  nls()

Now, effectively 2 of the 4 are linear, 2 are non linear;
such a problem is denoted as  `` partially linear least-squares ''
In such a case it's quite important (for efficiency and
inference reasons) to make use of this fact.

 ---> use  nls(...., method = "plinear" , ....)

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From hi_ono2001 at ybb.ne.jp  Thu Jul 10 15:03:47 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Thu, 10 Jul 2003 22:03:47 +0900
Subject: [R] Does anyone know R interface for METIS(graph partioning
	program)? 
Message-ID: <001a01c346e3$b3823000$818001db@webgis>

Hi.

  Does anyone know availability of R interface for
METIS(http://www-users.cs.umn.edu/~karypis/metis/index.html, graph
partioning program)?

 I think METIS is very useful for routing analysis(finding shortest path)
for more than 10 thousands nodes.

 Regards.



From hi_ono2001 at ybb.ne.jp  Thu Jul 10 15:11:02 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Thu, 10 Jul 2003 22:11:02 +0900
Subject: [R] R and Spatial index
Message-ID: <002c01c346e4$b6c075a0$818001db@webgis>

Hi.

 S-Plus has a function "quad.tree" which  performs a recursive partitioning
of a numeric matrix.

 Does R have similar function or functions support other spatial index(ex.
R-tree, K-D tree, etc.)?

  Regards.



From ligges at statistik.uni-dortmund.de  Thu Jul 10 15:26:40 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jul 2003 15:26:40 +0200
Subject: [R] XML Package.
In-Reply-To: <3F0D575F.7020503@mitre.org>
References: <200307101354370866.061AF50E@harry.molgen.mpg.de>
	<3F0D575F.7020503@mitre.org>
Message-ID: <3F0D6990.3020409@statistik.uni-dortmund.de>

Stephen C. Upton wrote:

> Eryk,
> 
> If you go here on the CRAN, you should be able to find an XML.zip
> 
> 
>  /pub/languages/R/CRAN/bin/windows/contrib

a) That directory is of your local CRAN mirror, please use soemthing 
like "CRAN/bin/windows/contrib" as a mirror-independent way to specify 
the location.

b) That locations contains only binary versions for R < 1.7.0, so it is 
*not appropriate* for a recent installation.
For R-1.7.x, CRAN/bin/windows/contrib/1.7 is the right place to look into.

c) Everything is clear when reading the ReadMe which points out the 
answer Duncan Temple Lang already has given:

"The packages
   SJava, XML, netCDF, and xgobi
are available at
   http://www.stats.ox.ac.uk/pub/RWin
kindly provided by Professor Brian D. Ripley."

These packages are not available from CRAN, because they don't compile 
out of the box with my automated scripts on my machine.

Uwe Ligges




> HTH
> steve
> 
> Wolski wrote:
> 
>> Hi!
>> I have installed the new R on windows.
>> I wanted to reinstall the XML package. I am not able to find the 
>> XML.zip anymore. I am quite shure that they where a windows binary 
>> version.
>> Has anyone old XML windows binary?
>> Eryk
>>
>> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. Vertebrate 
>> Genomics
>> Ihnestrasse 73 14195 Berlin          'v'
>> tel: 0049-30-84131285               /   \
>> mail: wolski at molgen.mpg.de        ---W-W----
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sola at stat.uni-muenchen.de  Thu Jul 10 15:45:19 2003
From: sola at stat.uni-muenchen.de (Adejumo Adebowale Olusola)
Date: Thu, 10 Jul 2003 15:45:19 +0200
Subject: [R] The question is on Symmetry model for square table.
Message-ID: <3F0D6DEF.9050303@stat.uni-muenchen.de>

Please help,
I tried a program on S-plus, and it worked. Also I tried the same 
program on R but not worked. Here is the programme. I put it in a 
function form. The model and assumption are at the bottom.

where
counts<-c(22,2,2,0,5,7,14,0,0,2,36,0,0,1,17,10)
which is name.data, i is row size and j is the column size.

symmetry
function(i, j, name.data)
{
	row <- (c(1:i))
	col <- (c(1:j))
	name.data <- expand.grid(A = row, B = col)
	name.data$counts <- c(counts)
	name.data$symm <- paste(pmin(as.numeric(name.data$A),   		 
as.numeric(name.data$B)), pmax(as.numeric(name.data$A), 		 
as.numeric(name.data$B)), sep = ",")
	symmetry <- glm(counts ~ symm, family = poisson(link = log), 			data = 
name.data)
}
 > summary(symmetry(4,4,counts))

Call: glm(formula = counts ~ symm, family = poisson(link = log), data = 
name.data)
Deviance Residuals:
        Min         1Q         Median        3Q      Max
  -4.123106 -0.9044956 -3.846197e-008 0.6543513 2.562617

Coefficients:
                  Value Std. Error    t value
(Intercept)  0.7912419  2.1128915  0.3744830
       symm1 -0.9191397  0.2169745 -4.2361653
       symm2 -0.7239676  0.2465491 -2.9364038
       symm3 -2.3094242  5.2703608 -0.4381909
       symm4  0.5614798  1.0575027  0.5309488
       symm5  0.3965751  0.7045443  0.5628817
       symm6 -0.1128162  0.5223163 -0.2159920
       symm7  0.4499711  0.3777980  1.1910362
       symm8  0.1895939  0.2946399  0.6434767
       symm9  0.1679270  0.2368599  0.7089720

(Dispersion Parameter for Poisson family taken to be 1 )

     Null Deviance: 190.398 on 15 degrees of freedom

Residual Deviance: 39.17989 on 6 degrees of freedom

Number of Fisher Scoring Iterations: 6

_________________________________________________________________________
Also in R. program, here is the same program together with the complain.

name.data=counts (above).

 > symmetry
function(i,j,name.data){
A<-(c(1:i))
B<-(c(1:j))
name.data<-expand.grid(A=A,B=B)
name.data$counts<-(c(counts))
name.data$symm<-paste(pmin(as.numeric(name.data$A),as.numeric(name.data$B)),
pmax(as.numeric(name.data$A),as.numeric(name.data$B)),sep=",")
symmetry<-glm(counts~symm,data=name.data,family=poisson(link=log))
}

 > symmetry(4,4,counts)
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  : invalid variable type

I tried to print out the table with symm pathern. and the function for 
symm below.

 > i<-4
 > j<-4
 > A<-(c(1:i))
 > B<-(c(1:j))
 > name.data<-expand.grid(A=A,B=B)
 > name.data$counts<-(c(counts))
 >name.data$symm<-paste(pmin(as.numeric(name.data$A),as.numeric(name.data$B)),
+ pmax(as.numeric(name.data$A),as.numeric(name.data$B)),sep=",")
 > name.data
    A B counts symm
1  1 1     22  1,1
2  2 1      2  1,2
3  3 1      2  1,3
4  4 1      0  1,4
5  1 2      5  1,2
6  2 2      7  2,2
7  3 2     14  2,3
8  4 2      0  2,4
9  1 3      0  1,3
10 2 3      2  2,3
11 3 3     36  3,3
12 4 3      0  3,4
13 1 4      0  1,4
14 2 4      1  2,4
15 3 4     17  3,4
16 4 4     10  4,4
 >

 > symm
function (x, levels = sort(unique.default(x), na.last = TRUE),
     labels = levels, exclude = NA, ordered = is.ordered(x))
{
     if (is.null(x))
         x <- list()
     exclude <- as.vector(exclude, typeof(x))
     levels <- levels[is.na(match(levels, exclude))]
     f <- match(x, levels)
     names(f) <- names(x)
     nl <- length(labels)
     attr(f, "levels") <- if (nl == length(levels))
         as.character(labels)
     else if (nl == 1)
         paste(labels, seq(along = levels), sep = "")
     else stop(paste("invalid labels; length", nl, "should be 1 or",
         length(levels)))
     class(f) <- c(if (ordered) "ordered", "factor")
     f
}

-----------------------------------------------------------------------------
The model and Assumptions

log(m_ij)= lambda + lambda_i + lambda_j + lambda_ij

where,
lambda_ij = lambda_ji for i not equal to j
and lambda_i(A) = lambda_i(B)
Likelihood equation is

m_ij =(n_ij + n_ji)/2

For symmetry m_(ij)=m_(ji)

"R program" does not recognised "symm" pathern, that is (1,1), (1,2) and 
so on but "S-plus" do. So please I need your assistance.

Thanks for your usual contibutions.

Yours

Sola.


-- 
---------------------------------
Adebowale Olusola Adejumo
Department of Statistics
LMU,University of Muenchen
Ludwigstra?e 33/III
D - 80539 M?nchen
Germany.

Tel:  ++49 89 2180 3165
Fax:  ++49 89 2180 5042
http://www.stat.uni-muenchen.de/



From fharrell at virginia.edu  Thu Jul 10 15:43:30 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 10 Jul 2003 09:43:30 -0400
Subject: [R] group sequential and adaptive designs
In-Reply-To: <13419.1057839200@www47.gmx.net>
References: <13419.1057839200@www47.gmx.net>
Message-ID: <20030710094330.5e473a21.fharrell@virginia.edu>

On Thu, 10 Jul 2003 14:13:20 +0200 (MEST)
Marc Vandemeulebroecke <vandemem at gmx.de> wrote:

> Hello R users,
> 
> I am looking for R (or S) code related to group sequential or adaptive
> designs for clinical trials. (The most prominent examples are the designs of
> Pocock or O'Brien/Fleming, the alpha-spending function approach, or Fisher's
> combination test and the inverse normal method.) I am particularly interested in
> the calculation of the critical boundaries, the handling of spending functions
> and, most of all, power calculations. All I could find were announcements of
> courses or postings related to the S-Plus SeqTrial module. No code. Can
> anyone give a hint?
> 
> Many thanks in advance,
> Marc
> 

If you get the U. of Wisconsin Biostatistics program ld98 executable, which implements several spending functions in the Lan DeMets paradigm, you can use the S function ldBands and its plot and summary methods, which call ld98.  ldBands is in the Hmisc package at http://hesweb1.med.virginia.edu/biostat/s/Hmisc.html

ldBands works under Linux/Unix but has not been tested under Windows yet.
Get ld98 and its documentation from http://www.medsch.wisc.edu/landemets

ldBands makes ld98 easier to use.  Examples show how to do power calculations.
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From david.firth at nuffield.oxford.ac.uk  Thu Jul 10 15:44:22 2003
From: david.firth at nuffield.oxford.ac.uk (David Firth)
Date: Thu, 10 Jul 2003 14:44:22 +0100
Subject: [R] packaged datasets in .csv format (David Firth)
In-Reply-To: <3F0D4D2C.1030204@statistik.uni-dortmund.de>
Message-ID: <9C91DFD2-B2DC-11D7-8F15-0050E4C03977@nuffield.oxford.ac.uk>

Many thanks to those who replied to my question.

Dirk's suggestion, to use a .R file in the "data" directory of the  
package, specifying how the .csv should be read, works fine as an  
answer to the question about making comma-separated files available.

Uwe's answer to my other question (; vs ,), ie compatibility with  
existing R packages, is well taken!

Cheers,
David

On Thursday, Jul 10, 2003, at 12:25 Europe/London, Uwe Ligges wrote:

> Andreas Christmann wrote:
>>> --------------------------------------------------------------------- 
>>> -
>>>
>>> Message: 1
>>> Date: Wed, 9 Jul 2003 10:53:27 +0100
>>> From: David Firth <david.firth at nuffield.oxford.ac.uk>
>>> Subject: [R] packaged datasets in .csv format
>>> To: r-help at stat.math.ethz.ch
>>> Message-ID:
>>>     <307D34CE-B1F3-11D7-A8D2-0050E4C03977 at nuffield.oxford.ac.uk>
>>> Content-Type: text/plain; charset=US-ASCII; format=flowed
>>>
>>> A couple of questions in connection with using .csv format to  
>>> include data in a package:
>>>
>>> First, the background.  The data() function loads data from .csv  
>>> ("comma-separated values") files using
>>>
>>>    read.table(..., header = TRUE, sep = ";")
>>>
>>> But ?read.table says
>>>
>>>       ## To write a CSV file for input to Excel one might use
>>>       write.table(x, file = "foo.csv", sep = ",", col.names = NA)
>>>       ## and to read this file back into R one needs
>>>       read.table("file.csv", header = TRUE, sep = ",", row.names=1)
>>>
>>> As a result, .csv files created by write.table() as above are not  
>>> read in by data() in the way that might be expected [that is,  
>>> expected by someone who had not read help(data)!]
>>>
>>> Two questions, then:
>>> -- is there some compelling reason for  the use of `sep = ";"' in  
>>> place of `sep = ",", row.names=1'?
>
> Do you really want an answer?
> Today, one reason is compatibility to all the other packages on CRAN.
>
>
>> I prefer ";" instead of "," , because in text variables there are  
>> often ",".
>
> That's why text variables can be quoted.
>
>
>>> -- if I want to maintain a dataset in .csv format, for use both in R  
>>> and in other systems such as Excel, SPSS, etc, what is the best way  
>>> to go about it?
>
> When regularly using that many systems on the same data sets, it might  
> be worth using a database system, e.g. MySQL.
>
> BTW: R *and* Excel *and* (for sure, but I haven't tested) also SPSS  
> can read a couple of different ASCII formatted files, so there are  
> quite a lot possible formats.
>
> Uwe Ligges
>
>
>> Depends. Perhaps it is best to check it out for the software packages
>> and the versions of the software packages you are using.
> >
>> Andreas Christmann
>>>
>>> Any advice would be much appreciated.
>>>
>>> Cheers,
>>> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From andy_liaw at merck.com  Thu Jul 10 16:08:23 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Jul 2003 10:08:23 -0400
Subject: [R] please help on frag polynoms
Message-ID: <3A822319EB35174CA3714066D590DCD50205C83F@usrymx25.merck.com>

I'm guessing you meant "fracpoly" in Stata, which is fractional polynomial
smoothing.  If that's the case, check out
http://www.homepages.ucl.ac.uk/~ucakgam/r/.

Andy

> -----Original Message-----
> From: joerg-burmester at gmx.net [mailto:joerg-burmester at gmx.net] 
> Sent: Thursday, July 10, 2003 4:53 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] please help on frag polynoms
> 
> 
> hi there,
> 
> can anyone help me on the topic of frag polynoms?
> 
> i just heard of a friend of mine, that i could build in a 
> functioon called fragpoly (he was talking of such a function 
> in the 'stata' language) in order to improve my process of 
> finding an optimal linear model.
> 
> instead of trying a vast amount of transformed inputdata to 
> find the best fit and then step backwards down to e.g. rank 5 
> in order to get a smooth curve, i could start right from the 
> beginning with only very few but therefore very flexible 
> funktions (litle similar to box-plot) which adopt themselve 
> automaticaly to an optimal fitt and r-squered.
> 
> can anyone tell me if such a flexible adoptive function also 
> exists in r+? (i could not find anything on the r+ pages with 
> these search words).
> 
> waiting desperately for help
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From john.janmaat at acadiau.ca  Thu Jul 10 16:14:46 2003
From: john.janmaat at acadiau.ca (John Janmaat)
Date: Thu, 10 Jul 2003 11:14:46 -0300
Subject: [R] GLS in R
Message-ID: <3F0D74D6.2060202@acadiau.ca>

Hello All,

Looking for an easy way to feed a non-identity covariance matrix to a 
regression.  Is there a function to do this, or do I choleski decompose 
the inverse of the covariance matrix and weight the observations - 
risking precision loss.

Thanks,

John.
-- 
--------------------------------------------------------------------------
Dr. John Janmaat
Department of Economics, Acadia University, Wolfville, NS, B4P 2R6
E-mail: jjanmaat at acadiau.ca        Web: http://ace.acadiau.ca/~jjanmaat
Tel: 902-585-1461		   Fax: 902-585-1070



From marc.fohr at first-private.de  Thu Jul 10 16:16:38 2003
From: marc.fohr at first-private.de (Fohr, Marc [AM])
Date: Thu, 10 Jul 2003 15:16:38 +0100
Subject: [R] Maximum Likelihood Estimation and Optimisation
Message-ID: <05D9367E7F42D711A0D40002A5F3BE8601276DA6@exchuk08.eur.nsroot.net>

Hello,

I want to calculate a maximum likelihood funktion in R in order to solve for the parameters of an estimator. Is there an easy way to do this in R? How do I get the parameters and the value of the maximum likelihood funktion. 

More, I want to specify the algorithm of the optimisation above: BHHH (Berndt Hall Hall Hausman). Is this possible?

Thanks a lot for your help and best regards

Marc

-----------------------------------------------------------------------------
Marc Fohr, CFA
Equity Portfolio Manager
First Private Investment Management
Neue Mainzer Strasse 75
D-60311 Frankfurt/Main
Phone: ++49 - 69 - 2607 5424
Fax: ++49 - 69 - 2607 5440
Email: marc.fohr at first-private.de



From bates at stat.wisc.edu  Thu Jul 10 16:42:46 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 10 Jul 2003 14:42:46 -0000
Subject: [R] Simple linear regression
In-Reply-To: <16141.23046.756015.965088@gargle.gargle.HOWL>
References: <1057834486.5275.3.camel@misko.homelinux.net>
	<Pine.LNX.4.44.0307102259410.18163-100000@stat55.stat.auckland.ac.nz>
	<16141.23046.756015.965088@gargle.gargle.HOWL>
Message-ID: <6r1xwyfno6.fsf@bates4.stat.wisc.edu>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> >>>>> "KKWa" == Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz>
> >>>>>     on Thu, 10 Jul 2003 23:00:00 +1200 (NZST) writes:
> 
>     KKWa> Try: ?lm
> 
> no.  see below
> 
>     KKWa> On 10 Jul 2003, Gorazd Brumen wrote:
> 
>     >> Date: 10 Jul 2003 12:54:46 +0200 From: Gorazd Brumen
>     >> <gbrumen at student.ethz.ch> To: R-help at stat.math.ethz.ch
>     >> Subject: [R] Simple linear regression
>     >> 
>     >> Dear all,
>     >> 
>     >> My friend wants to fit a model of the type
>     >> 
>     >> z = a x^n y^m + b,
>     >> 
>     >> where x, y, z are data and a, b, n, m are unknown
>     >> parameters.
>     >> 
>     >> How can he transform this to fit in the linear regression
>     >> framework?  Any help would be appreciated.
> 
> He can't.  When all 4   a, b, n, m  are parameters, this is a
> non-linear regression problem.  --> Function  nls()
> 
> Now, effectively 2 of the 4 are linear, 2 are non linear;
> such a problem is denoted as  `` partially linear least-squares ''
> In such a case it's quite important (for efficiency and
> inference reasons) to make use of this fact.
> 
>  ---> use  nls(...., method = "plinear" , ....)

I think it should be 'algorithm = "plinear"'

The full call would be something like

nls(z ~ cbind(x^n*y^m, 1), data = mydata, start=c(n = 1.0, m = 2.0),
    algorithm = "plinear")

Must the exponents n and m be positive?  If so, I recommend using the
logarithm of the exponents as the parameters in the optimization

nls(z ~ cbind(x^exp(logn)*y^exp(logm), 1), data = mydata,
   start=c(logn = 0., logm = log(2.0)),  algorithm = "plinear")



From hdoran at nasdc.org  Thu Jul 10 16:43:11 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Thu, 10 Jul 2003 10:43:11 -0400
Subject: [R] Maximum Likelihood Estimation and Optimisation
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FC5@ernesto.NASDC.ORG>

Well, lm() produces an OLS solution, which are also MLE solutions for the fixed effects. I think this is an easy way, although maybe not the best. 

BHHH is a numerical approximation that can be used when a closed form solution is not available. It is less sophisticated than Newton-Raphson.

Is this helpful?

 
------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628 
 
 


-----Original Message-----
From: Fohr, Marc [AM] [mailto:marc.fohr at first-private.de]
Sent: Thursday, July 10, 2003 10:17 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Maximum Likelihood Estimation and Optimisation


Hello,

I want to calculate a maximum likelihood funktion in R in order to solve for the parameters of an estimator. Is there an easy way to do this in R? How do I get the parameters and the value of the maximum likelihood funktion. 

More, I want to specify the algorithm of the optimisation above: BHHH (Berndt Hall Hall Hausman). Is this possible?

Thanks a lot for your help and best regards

Marc

-----------------------------------------------------------------------------
Marc Fohr, CFA
Equity Portfolio Manager
First Private Investment Management
Neue Mainzer Strasse 75
D-60311 Frankfurt/Main
Phone: ++49 - 69 - 2607 5424
Fax: ++49 - 69 - 2607 5440
Email: marc.fohr at first-private.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From david.barron at jesus.ox.ac.uk  Thu Jul 10 16:55:38 2003
From: david.barron at jesus.ox.ac.uk (David Barron)
Date: Thu, 10 Jul 2003 15:55:38 +0100
Subject: FW: [R] Maximum Likelihood Estimation and Optimisation
Message-ID: <HOEELNPAKEPPEBNKGDBFCEEECAAA.david.barron@jesus.ox.ac.uk>


Have a look at ?optim.  I don't think it has the BHHH algorithm as an
option, though.

===========================================
David Barron
Jesus College
University of Oxford


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Harold Doran
Sent: 10 July 2003 15:43
To: Fohr, Marc [AM]; R-help at stat.math.ethz.ch
Subject: RE: [R] Maximum Likelihood Estimation and Optimisation


Well, lm() produces an OLS solution, which are also MLE solutions for the
fixed effects. I think this is an easy way, although maybe not the best.

BHHH is a numerical approximation that can be used when a closed form
solution is not available. It is less sophisticated than Newton-Raphson.

Is this helpful?


------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628




-----Original Message-----
From: Fohr, Marc [AM] [mailto:marc.fohr at first-private.de]
Sent: Thursday, July 10, 2003 10:17 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Maximum Likelihood Estimation and Optimisation


Hello,

I want to calculate a maximum likelihood funktion in R in order to solve for
the parameters of an estimator. Is there an easy way to do this in R? How do
I get the parameters and the value of the maximum likelihood funktion.

More, I want to specify the algorithm of the optimisation above: BHHH
(Berndt Hall Hall Hausman). Is this possible?

Thanks a lot for your help and best regards

Marc

----------------------------------------------------------------------------
-
Marc Fohr, CFA
Equity Portfolio Manager
First Private Investment Management
Neue Mainzer Strasse 75
D-60311 Frankfurt/Main
Phone: ++49 - 69 - 2607 5424
Fax: ++49 - 69 - 2607 5440
Email: marc.fohr at first-private.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From simon at stats.gla.ac.uk  Thu Jul 10 16:58:00 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Thu, 10 Jul 2003 15:58:00 +0100 (BST)
Subject: [R] GLS in R
In-Reply-To: <3F0D74D6.2060202@acadiau.ca>
Message-ID: <Pine.SOL.3.96.1030710154902.2939B-100000@jupiter.stats.gla.ac.uk>

> do I choleski decompose 
> the inverse of the covariance matrix and weight the observations - 
> risking precision loss.
- I think you'd be better off choleski decomposing the cov matrix itself
wouldn't you? e.g. if V is the covariance matrix use chol() to get
V=L^T L
and then form L^{-T}y and L^{-T}X using solve() (assuming model is
y=Xb+e).
Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From spencer.graves at pdf.com  Thu Jul 10 17:01:52 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Jul 2003 08:01:52 -0700
Subject: [R] Maximum Likelihood Estimation and Optimisation
References: <66578BFC0BA55348B5907A0F798EE930139FC5@ernesto.NASDC.ORG>
Message-ID: <3F0D7FE0.6050708@pdf.com>

It is not obvious to me what parameters in what model you want to fit. 
Function "optim" does very well with many different kinds of problems. 
If you just want to estimate parameters of a probability distribution, 
function "fitdistr" in library(MASS) will do that.  A couple of days 
ago, I needed to fit a "Pareto distribution of the first kind."  A 
search of "www.r-project.org" -> search -> "R site search" uncovered 
functions for a Pareto distribtion of a different kind.  So, I wrote the 
following and used them to check "fitdistr" and then to actually fit the 
distribution to data.

hope this helps.  spencer graves
#####################################################
dpareto <-
function(x, shape, x0, log=FALSE){
     dp <- if(log) (log(shape-1)-shape*log(x/x0)-log(x0))
     else ((shape-1)*((x0/x)^shape)/x0)
     dp[x<x0] <- 0
     dp
}

ppareto <-
function(q, shape, x0, lower.tail = TRUE, log.p=FALSE){
     q <- pmax(x0, q)
     if(log.p){
         if(lower.tail){
             return(log(ppareto(q, shape, x0)))
         }
         else return((shape-1)*log(x0/q))
     }
     else{
         S.q <- (x0/q)^(shape-1)
         if(lower.tail)return(1-S.q)
         else return(S.q)
     }
}

qpareto <-
function(p, shape, x0, lower.tail=TRUE){
     if(lower.tail) p <- (1-p)
     x0*exp(-log(p)/(shape-1))
}

rpareto <-
function(n, shape, x0)
qpareto(runif(n), shape, x0, lower.tail=FALSE)

fitdistr(rpareto(10000, 3, 1), dpareto, list(shape=2.5), x0=1)

########################################################
Harold Doran wrote:
> Well, lm() produces an OLS solution, which are also MLE 
solutions for the fixed effects. I think this is an easy
way, although maybe not the best.
> 
> BHHH is a numerical approximation that can be used when 
a closed form solution is not available. It is less
sophisticated than Newton-Raphson.
> 
> Is this helpful?
> 
>  
> ------
> Harold C. Doran
> Director of Research and Evaluation
> New American Schools
> 675 N. Washington Street, Suite 220
> Alexandria, Virginia 22314
> 703.647.1628 
> 
> -----Original Message-----
> From: Fohr, Marc [AM] [mailto:marc.fohr at first-private.de]
> Sent: Thursday, July 10, 2003 10:17 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Maximum Likelihood Estimation and Optimisation
> 
> 
> Hello,
> 
> I want to calculate a maximum likelihood funktion in R in 
order to solve for the parameters of an estimator. Is there
an easy way to do this in R? How do I get the parameters and
the value of the maximum likelihood funktion.
> 
> More, I want to specify the algorithm of the optimisation 
above: BHHH (Berndt Hall Hall Hausman). Is this possible?
> 
> Thanks a lot for your help and best regards
> 
> Marc
> 
> -----------------------------------------------------------------------------
> Marc Fohr, CFA
> Equity Portfolio Manager
> First Private Investment Management
> Neue Mainzer Strasse 75
> D-60311 Frankfurt/Main
> Phone: ++49 - 69 - 2607 5424
> Fax: ++49 - 69 - 2607 5440
> Email: marc.fohr at first-private.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Jul 10 17:08:17 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 10 Jul 2003 08:08:17 -0700
Subject: [R] The question is on Symmetry model for square table.
References: <3F0D6DEF.9050303@stat.uni-muenchen.de>
Message-ID: <3F0D8161.3060209@pdf.com>

	  Did you try "traceback()"?  This might help you identify the 
offending line in your function.

	  If that doesn't help, I step through the function one line at a time 
(copy and paste from an editor) until R bombs on me.  If it doesn't 
bomb, then there is a "scoping" problem:  Are you using global variables 
in a function?  If yes, pass them explicitly as arguments.  I recently 
potentially similar problems this way.

hope this helps.  spencer graves

Adejumo Adebowale Olusola wrote:
> Please help,
> I tried a program on S-plus, and it worked. Also I tried the same 
> program on R but not worked. Here is the programme. I put it in a 
> function form. The model and assumption are at the bottom.
> 
> where
> counts<-c(22,2,2,0,5,7,14,0,0,2,36,0,0,1,17,10)
> which is name.data, i is row size and j is the column size.
> 
> symmetry
> function(i, j, name.data)
> {
>     row <- (c(1:i))
>     col <- (c(1:j))
>     name.data <- expand.grid(A = row, B = col)
>     name.data$counts <- c(counts)
>     name.data$symm <- paste(pmin(as.numeric(name.data$A),            
> as.numeric(name.data$B)), pmax(as.numeric(name.data$A),          
> as.numeric(name.data$B)), sep = ",")
>     symmetry <- glm(counts ~ symm, family = poisson(link = 
> log),             data = name.data)
> }
>  > summary(symmetry(4,4,counts))
> 
> Call: glm(formula = counts ~ symm, family = poisson(link = log), data = 
> name.data)
> Deviance Residuals:
>        Min         1Q         Median        3Q      Max
>  -4.123106 -0.9044956 -3.846197e-008 0.6543513 2.562617
> 
> Coefficients:
>                  Value Std. Error    t value
> (Intercept)  0.7912419  2.1128915  0.3744830
>       symm1 -0.9191397  0.2169745 -4.2361653
>       symm2 -0.7239676  0.2465491 -2.9364038
>       symm3 -2.3094242  5.2703608 -0.4381909
>       symm4  0.5614798  1.0575027  0.5309488
>       symm5  0.3965751  0.7045443  0.5628817
>       symm6 -0.1128162  0.5223163 -0.2159920
>       symm7  0.4499711  0.3777980  1.1910362
>       symm8  0.1895939  0.2946399  0.6434767
>       symm9  0.1679270  0.2368599  0.7089720
> 
> (Dispersion Parameter for Poisson family taken to be 1 )
> 
>     Null Deviance: 190.398 on 15 degrees of freedom
> 
> Residual Deviance: 39.17989 on 6 degrees of freedom
> 
> Number of Fisher Scoring Iterations: 6
> 
> _________________________________________________________________________
> Also in R. program, here is the same program together with the complain.
> 
> name.data=counts (above).
> 
>  > symmetry
> function(i,j,name.data){
> A<-(c(1:i))
> B<-(c(1:j))
> name.data<-expand.grid(A=A,B=B)
> name.data$counts<-(c(counts))
> name.data$symm<-paste(pmin(as.numeric(name.data$A),as.numeric(name.data$B)), 
> 
> pmax(as.numeric(name.data$A),as.numeric(name.data$B)),sep=",")
> symmetry<-glm(counts~symm,data=name.data,family=poisson(link=log))
> }
> 
>  > symmetry(4,4,counts)
> Error in model.frame(formula, rownames, variables, varnames, extras, 
> extranames,  : invalid variable type
> 
> I tried to print out the table with symm pathern. and the function for 
> symm below.
> 
>  > i<-4
>  > j<-4
>  > A<-(c(1:i))
>  > B<-(c(1:j))
>  > name.data<-expand.grid(A=A,B=B)
>  > name.data$counts<-(c(counts))
>  >name.data$symm<-paste(pmin(as.numeric(name.data$A),as.numeric(name.data$B)),
> + pmax(as.numeric(name.data$A),as.numeric(name.data$B)),sep=",")
>  > name.data
>    A B counts symm
> 1  1 1     22  1,1
> 2  2 1      2  1,2
> 3  3 1      2  1,3
> 4  4 1      0  1,4
> 5  1 2      5  1,2
> 6  2 2      7  2,2
> 7  3 2     14  2,3
> 8  4 2      0  2,4
> 9  1 3      0  1,3
> 10 2 3      2  2,3
> 11 3 3     36  3,3
> 12 4 3      0  3,4
> 13 1 4      0  1,4
> 14 2 4      1  2,4
> 15 3 4     17  3,4
> 16 4 4     10  4,4
>  >
> 
>  > symm
> function (x, levels = sort(unique.default(x), na.last = TRUE),
>     labels = levels, exclude = NA, ordered = is.ordered(x))
> {
>     if (is.null(x))
>         x <- list()
>     exclude <- as.vector(exclude, typeof(x))
>     levels <- levels[is.na(match(levels, exclude))]
>     f <- match(x, levels)
>     names(f) <- names(x)
>     nl <- length(labels)
>     attr(f, "levels") <- if (nl == length(levels))
>         as.character(labels)
>     else if (nl == 1)
>         paste(labels, seq(along = levels), sep = "")
>     else stop(paste("invalid labels; length", nl, "should be 1 or",
>         length(levels)))
>     class(f) <- c(if (ordered) "ordered", "factor")
>     f
> }
> 
> ----------------------------------------------------------------------------- 
> 
> The model and Assumptions
> 
> log(m_ij)= lambda + lambda_i + lambda_j + lambda_ij
> 
> where,
> lambda_ij = lambda_ji for i not equal to j
> and lambda_i(A) = lambda_i(B)
> Likelihood equation is
> 
> m_ij =(n_ij + n_ji)/2
> 
> For symmetry m_(ij)=m_(ji)
> 
> "R program" does not recognised "symm" pathern, that is (1,1), (1,2) and 
> so on but "S-plus" do. So please I need your assistance.
> 
> Thanks for your usual contibutions.
> 
> Yours
> 
> Sola.
> 
>



From gbrumen at student.ethz.ch  Thu Jul 10 17:31:45 2003
From: gbrumen at student.ethz.ch (Gorazd Brumen)
Date: Thu, 10 Jul 2003 15:31:45 -0000
Subject: [R] Simple linear regression
In-Reply-To: <6r1xwyfno6.fsf@bates4.stat.wisc.edu>
References: <1057834486.5275.3.camel@misko.homelinux.net>
	<Pine.LNX.4.44.0307102259410.18163-100000@stat55.stat.auckland.ac.nz>
	<16141.23046.756015.965088@gargle.gargle.HOWL>
	<6r1xwyfno6.fsf@bates4.stat.wisc.edu>
Message-ID: <1057851114.5284.495.camel@misko.homelinux.net>

Dear all,

Thank you all a lot for the help. The commands given by prof. Bates 
were the most direct way to the solution of the problem. 

Once again thank you all,
Gorazd Brumen

V ?et, 10.07.2003 ob 16:42, je Douglas Bates poslal(a):
> Martin Maechler <maechler at stat.math.ethz.ch> writes:
> 
> > >>>>> "KKWa" == Ko-Kang Kevin Wang <kwan022 at stat.auckland.ac.nz>
> > >>>>>     on Thu, 10 Jul 2003 23:00:00 +1200 (NZST) writes:
> > 
> >     KKWa> Try: ?lm
> > 
> > no.  see below
> > 
> >     KKWa> On 10 Jul 2003, Gorazd Brumen wrote:
> > 
> >     >> Date: 10 Jul 2003 12:54:46 +0200 From: Gorazd Brumen
> >     >> <gbrumen at student.ethz.ch> To: R-help at stat.math.ethz.ch
> >     >> Subject: [R] Simple linear regression
> >     >> 
> >     >> Dear all,
> >     >> 
> >     >> My friend wants to fit a model of the type
> >     >> 
> >     >> z = a x^n y^m + b,
> >     >> 
> >     >> where x, y, z are data and a, b, n, m are unknown
> >     >> parameters.
> >     >> 
> >     >> How can he transform this to fit in the linear regression
> >     >> framework?  Any help would be appreciated.
> > 
> > He can't.  When all 4   a, b, n, m  are parameters, this is a
> > non-linear regression problem.  --> Function  nls()
> > 
> > Now, effectively 2 of the 4 are linear, 2 are non linear;
> > such a problem is denoted as  `` partially linear least-squares ''
> > In such a case it's quite important (for efficiency and
> > inference reasons) to make use of this fact.
> > 
> >  ---> use  nls(...., method = "plinear" , ....)
> 
> I think it should be 'algorithm = "plinear"'
> 
> The full call would be something like
> 
> nls(z ~ cbind(x^n*y^m, 1), data = mydata, start=c(n = 1.0, m = 2.0),
>     algorithm = "plinear")
> 
> Must the exponents n and m be positive?  If so, I recommend using the
> logarithm of the exponents as the parameters in the optimization
> 
> nls(z ~ cbind(x^exp(logn)*y^exp(logm), 1), data = mydata,
>    start=c(logn = 0., logm = log(2.0)),  algorithm = "plinear")
> 
-- 
Mail 1: gbrumen at student.ethz.ch
Mail 2: gorazd.brumen at fmf.uni-lj.si
Tel.: +41 (0)1 63 34906
Homepage: valjhun.fmf.uni-lj.si/~brumen



From tlumley at u.washington.edu  Thu Jul 10 17:33:52 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Jul 2003 08:33:52 -0700 (PDT)
Subject: [R] A problem with using the "outer" function
In-Reply-To: <ultogv8n0a4o4hoqr3slhpqcn0kcp4cbvo@4ax.com>
Message-ID: <Pine.A41.4.44.0307100832290.46186-100000@homer14.u.washington.edu>

On Wed, 9 Jul 2003, Duncan Murdoch wrote:

> On Wed, 09 Jul 2003 15:33:11 -0400, Ravi Varadhan <rvaradha at jhsph.edu>
> wrote :
>
> >Hi:
> >
> >I am using R 1.7.0 on Windows.  I am having trouble getting "outer" to
> >work on one of my functions.
>
> Most likely the problem is that the function you give doesn't work on
> array arguments.  Your function needs to take two arrays of the same
> shape as the first two arguments, and return an array of answers.
> outer() doesn't work by looping, it works by constructing big arrays
> of inputs and making just one function call.


Two further notes:

1/ This is in the FAQ.

2/ It is possible to use mapply() to vectorise an arbitrary function.
There isn't any speed advantage in doing so, but it will then work with
outer().

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Thu Jul 10 17:37:04 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Jul 2003 08:37:04 -0700 (PDT)
Subject: [R] .Internal(optim)
In-Reply-To: <3F0C7EA4.1000708@pdf.com>
Message-ID: <Pine.A41.4.44.0307100834291.46186-100000@homer14.u.washington.edu>

On Wed, 9 Jul 2003, Spencer Graves wrote:

> I haven't used optim enough to know, but simular functions ostensibly
> with box constraints would still test values outside the box and quite
> if it got an NA or possibly Inf.  I've had good luck transforming the
> inputs to remove constraints.  For example, if you need x > 0, then


optim() doesn't test values outside the box (even using finite difference
approximations for derivatives), but it does test values on the boundary.

Box constraints are for x>=0, which you can't get with transformations.
For x>0 transformations may well be indicated.

	-thomas



From tlumley at u.washington.edu  Thu Jul 10 17:40:07 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Jul 2003 08:40:07 -0700 (PDT)
Subject: [R] please help on frag polynoms
In-Reply-To: <21347.1057827167@www47.gmx.net>
Message-ID: <Pine.A41.4.44.0307100837370.46186-100000@homer14.u.washington.edu>

On Thu, 10 Jul 2003 joerg-burmester at gmx.net wrote:

> hi there,
>
> can anyone help me on the topic of frag polynoms?

I don't think there is anything currently available for fractional
polynomials along the lines of -fracpoly- in Stata.

You could construct an equivalent fairly easily using step(), or you could
use local polynomials or splines.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From shashank at bmb-fs1.biochem.okstate.edu  Thu Jul 10 17:47:58 2003
From: shashank at bmb-fs1.biochem.okstate.edu (Shashank Bhide)
Date: Thu, 10 Jul 2003 10:47:58 -0500
Subject: [R] Help with R Installation on Debian 2.2.19 (old stable/potato)
Message-ID: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu>

Hi all,
    I hope this is the correct list to post such a question.
I was trying to install the R-project on Debian and encountered significant 
problems with the same.
   The main problem is the installation of the libc6 package. I need this 
package in order to install the R-core package. However, the libc6 is 
dependent on the libdb1-compat package, which just refuses to install on my 
server.
   I tried to install it yesterday and it ended up crashing my system and 
messing up the old Apache. Does anyone have a procedure to install 
R-project on Debian linux?
The libdb1-compat package version is 2.1.3-7 and the libc6 is 2.3.1-17
Please advise,
TIA
Shashank

Shashank Bhide
Oklahoma State University
405 744 7103 (Off)
405 744 7799 (Fax)



From ma at ne.su.se  Thu Jul 10 18:15:49 2003
From: ma at ne.su.se (Mahmood ARAI)
Date: Thu, 10 Jul 2003 18:15:49 +0200
Subject: [R] Re: Help with R Installation on Debian 2.2.19 (old
	stable/potato)
In-Reply-To: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu> 
References: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu>
Message-ID: <20030710161549.625DA3803B@mbox1.su.se>

Shashank Bhide writes: 

> Hi all,
>    I hope this is the correct list to post such a question.
> I was trying to install the R-project on Debian and encountered 
> significant problems with the same.
>   The main problem is the installation of the libc6 package. I need this 
> package in order to install the R-core package. However, the libc6 is 
> dependent on the libdb1-compat package, which just refuses to install on 
> my server.
>   I tried to install it yesterday and it ended up crashing my system and 
> messing up the old Apache. Does anyone have a procedure to install 
> R-project on Debian linux?
> The libdb1-compat package version is 2.1.3-7 and the libc6 is 2.3.1-17
> Please advise,
 

one way is to add the following line (for woody as an ex.)
"deb http://cran.r-project.org/bin/linux/debian woody main"
to your "/etc/apt/sources.list" and use apt-get install r-base .. .etc. 


> TIA
> Shashank 
> 
> Shashank Bhide
> Oklahoma State University
> 405 744 7103 (Off)
> 405 744 7799 (Fax) 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Jul 10 18:25:11 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 10 Jul 2003 16:25:11 -0000
Subject: [R] GLS in R
In-Reply-To: <Pine.SOL.3.96.1030710154902.2939B-100000@jupiter.stats.gla.ac.uk>
References: <Pine.SOL.3.96.1030710154902.2939B-100000@jupiter.stats.gla.ac.uk>
Message-ID: <x2ptkibb4d.fsf@biostat.ku.dk>

Simon Wood <simon at stats.gla.ac.uk> writes:

> > do I choleski decompose 
> > the inverse of the covariance matrix and weight the observations - 
> > risking precision loss.
> - I think you'd be better off choleski decomposing the cov matrix itself
> wouldn't you? e.g. if V is the covariance matrix use chol() to get
> V=L^T L
> and then form L^{-T}y and L^{-T}X using solve() (assuming model is
> y=Xb+e).
> Simon

If you supply a user-defined residual correlation structure to gls() I
think that's what you end up doing, and you have additional options
for modelling variance structure and you can estimate parameters
for either of them. 

gls() is in library(nlme) and included with the system by default.

Writing user-supplied correlation structures is quite possible but
somewhat involved since you need to write out about eight methods for
each structure (apropos("corCAR1") should give you an idea of what it
takes). Unfortunately, the examples of how to do this at the R level
seem to have been optimized out of existence; all of that stuff is in
.C calls now.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pcboutro at engmail.uwaterloo.ca  Thu Jul 10 18:59:32 2003
From: pcboutro at engmail.uwaterloo.ca (Paul Boutros)
Date: Thu, 10 Jul 2003 12:59:32 -0400
Subject: [R] Version Number of a Package
Message-ID: <1057856372.3f0d9b742e0de@www.nexusmail.uwaterloo.ca>

Hello,

Apologies if this is a stupid question.  I googled and skimmed the archives and 
didn't see anything specific about this.

I am documenting an analysis procedure in a DB and I would like to know the 
specific version number of each package that I use.  Is there a standardized 
way of getting that information out from R, or should I parse it out from the 
source-code files?  Ideally I would like a function like this:
myVerNum <- Version(package.names);

Any guidance, direction, or suggestions are very much appreciated!
Paul



----------------------------------------
This mail sent through www.mywaterloo.ca



From ozric at web.de  Thu Jul 10 19:25:38 2003
From: ozric at web.de (Christian Schulz)
Date: Thu, 10 Jul 2003 19:25:38 +0200
Subject: [R] Version Number of a Package
References: <1057856372.3f0d9b742e0de@www.nexusmail.uwaterloo.ca>
Message-ID: <000d01c34708$4851d9a0$9e0aebd9@pc>

pack <- installed.packages()

christian

----- Original Message -----
From: "Paul Boutros" <pcboutro at engmail.uwaterloo.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 10, 2003 6:59 PM
Subject: [R] Version Number of a Package


> Hello,
>
> Apologies if this is a stupid question.  I googled and skimmed the
archives and
> didn't see anything specific about this.
>
> I am documenting an analysis procedure in a DB and I would like to know
the
> specific version number of each package that I use.  Is there a
standardized
> way of getting that information out from R, or should I parse it out from
the
> source-code files?  Ideally I would like a function like this:
> myVerNum <- Version(package.names);
>
> Any guidance, direction, or suggestions are very much appreciated!
> Paul
>
>
>
> ----------------------------------------
> This mail sent through www.mywaterloo.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jameson at coost.com  Thu Jul 10 19:50:59 2003
From: jameson at coost.com (Jameson C. Burt)
Date: Thu, 10 Jul 2003 13:50:59 -0400
Subject: [R] Help with R Installation on Debian 2.2.19 (old stable/potato)
In-Reply-To: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu>
References: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu>
Message-ID: <20030710175059.GA12038@coost.com>

I have avoided crossing Debian versions 
(eg, installing woody packages over a Debian potato distribution) 
unless I do a full upgrade to, eg, woody.
However, R was and is available for the Debian potato version, 
but it is version 0.90.
If you would be satisfied with that version, and I got good use out of
that version, and even out of the earlier version 0.61,
try an ordinary Debian package insallation with "apt".
   apt-get update	#updates available Debian packages
   apt-get install   r-base
This adds any necessary libraries, but only libraries consistent
with the Debian potato version.
It presumes you already have a working  /etc/apt/sources.list , eg, with a line like
   deb http://debian.rutgers.edu   potato   main contrib non-free

Alternatively, you can get this potato package directly at
   http://debian.rutgers.edu/dists/potato/main/binary-i386/math/
looking for packages beginning with  "r-".
For example, you could
   cd /tmp
   wget  http://debian.rutgers.edu/dists/potato/main/binary-i386/math/r-base_0.90.1-2.deb
   dpkg -i  r-base_0.90.1-2.deb


In the next version of Debian, woody, all the following package names are available,
but not in potato. 
You should be able to install (dpkg -i package-name.deb) 
documentation packages like  "r-doc-pdf" from
later Debian versions into you potato version,
but you can also get that documentation directly from R's webpages.
   * r-gnome         Gnome gui for statistical computing system
     r-mathlib       standalone mathematics library
     r-recommended   collection of recommended packages
     r-base-dev      installation of auxiliary GNU R packages
     r-base-html     html docs for statistical computing system functions
     r-base-latex    LaTeX docs for statistical computing system functions
     r-doc-html      html manuals for statistical computing system
     r-doc-info      info manuals statistical computing system
     r-doc-pdf       pdf manuals for statistical computing system





On Thu, Jul 10, 2003 at 10:47:58AM -0500, Shashank Bhide wrote:
> Hi all,
>    I hope this is the correct list to post such a question.
> I was trying to install the R-project on Debian and encountered significant 
> problems with the same.
>   The main problem is the installation of the libc6 package. I need this 
> package in order to install the R-core package. However, the libc6 is 
> dependent on the libdb1-compat package, which just refuses to install on my 
> server.
>   I tried to install it yesterday and it ended up crashing my system and 
> messing up the old Apache. Does anyone have a procedure to install 
> R-project on Debian linux?
> The libdb1-compat package version is 2.1.3-7 and the libc6 is 2.3.1-17

-- 
Jameson C. Burt, NJ9L   Fairfax, Virginia, USA
jameson at coost.com       http://www.coost.com
(202) 690-0380 (work)

LTSP.org:  magic "mysterious and awe-inspiring even though
                  we know they are real and not supernatural"



From alpha_white_wolf at hotmail.com  Thu Jul 10 19:53:56 2003
From: alpha_white_wolf at hotmail.com (Riley Metzger)
Date: Thu, 10 Jul 2003 13:53:56 -0400
Subject: [R] Contour Plots
Message-ID: <BAY1-F117kqkOzV8Q3i00003f95@hotmail.com>

Hello,

I'm a grad. student in statistics and am looking for some information on how 
R draws its contours.  I suspect you are using a Bezier spline.  I have the 
C code but am curious about how it works.

Riley A. Metzger
University of Waterloo
Waterloo, Ontario, N2L 3G1
(519) 888-4567 Ext. 3715
rileymetzger at alumni.uwaterloo.ca



From jgentry at jimmy.harvard.edu  Thu Jul 10 22:23:47 2003
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Thu, 10 Jul 2003 16:23:47 -0400 (EDT)
Subject: [R] Version Number of a Package
In-Reply-To: <1057856372.3f0d9b742e0de@www.nexusmail.uwaterloo.ca>
Message-ID: <Pine.SOL.4.20.0307101619490.16143-100000@santiam.dfci.harvard.edu>

> I am documenting an analysis procedure in a DB and I would like to know the 
> specific version number of each package that I use.  Is there a standardized 
> way of getting that information out from R, or should I parse it out from the 
> source-code files?  Ideally I would like a function like this:
> myVerNum <- Version(package.names);

There are a several ways to skin that cat.

A few that come to mind are using package.description():
> package.description("Biobase")["Version"]
 Version 
"1.3.27" 

You can also use installed.packages():
> z <- installed.packages()
> z[1,"Version"]
[1] "1.2.28"

Also, in the Biobase package in Bioconductor (www.bioconductor.org),
there's a function package.version():

> package.version("Biobase")
[1] "1.3.27"

-J



From bb2 at duke.edu  Thu Jul 10 22:33:43 2003
From: bb2 at duke.edu (bb2@duke.edu)
Date: Thu, 10 Jul 2003 16:33:43 -0400
Subject: [R] HTML Help
Message-ID: <p05210600bb337cd401b6@[160.36.251.70]>

Hi,

I'm new to R having recently migrated from Splus 2000.  I'm having 
trouble using help.start().

When I launch R help using help.start(), the index.html page comes up 
with the various manuals, etc.  If I click on one of the links, e.g. 
Search Engine and Keywords, the next page (Keywords by Topic) comes 
up.  However, if I click on any item, e.g. attributes, nothing at all 
happens.

Any ideas on what is wrong and how to fix it?  I'm using R version 
1.7.0 on Mac OSX.  R was installed using Fink.

Thanks in advance,
Brian



From kschlauc at vt.edu  Thu Jul 10 22:37:33 2003
From: kschlauc at vt.edu (kschlauc)
Date: Thu, 10 Jul 2003 16:37:33 -0400
Subject: [R] ordering of alphanumeric strings
Message-ID: <3F1ACE12@zathras>

Can someone tell me which version of R began to order 
alpha-numeric strings in this manner:
"ABC 10" < "ABC 2"
rather than 
"ABC 2" < "ABC 10" ?

And, is there a way to force "ABC 2"
to be ordered as a value less than "ABC 10"?

thank you,
Karen
kschlauc at vt.edu



From tplate at blackmesacapital.com  Thu Jul 10 23:03:37 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 10 Jul 2003 15:03:37 -0600
Subject: [R] ordering of alphanumeric strings
In-Reply-To: <3F1ACE12@zathras>
Message-ID: <5.2.1.1.2.20030710150001.03fdc9f0@mailhost.blackmesacapital.com>

I would be very surprised if any version of R ever ordered strings in the 
manner you want.  R has no way of knowing that some digit strings nestled 
amongst alphabetic characters should be treated as numbers.

To achieve what you want you need to parse the strings yourself, e.g.:
 > x <- c("ABC 10", "ABC 2")
 > x1 <- do.call("rbind", strsplit(x, " "))
 > x2 <- list(x1[,1], as.numeric(as.character(x1[,2])))
 > do.call("order", x2)
[1] 2 1
 >

The above will only work if each element of 'x' has the same number of 
spaces in it (i.e., splitting on a space breaks each element into the same 
number of components).

hope this helps,

Tony Plate

At Thursday 04:37 PM 7/10/2003 -0400, kschlauc wrote:
>Can someone tell me which version of R began to order
>alpha-numeric strings in this manner:
>"ABC 10" < "ABC 2"
>rather than
>"ABC 2" < "ABC 10" ?
>
>And, is there a way to force "ABC 2"
>to be ordered as a value less than "ABC 10"?
>
>thank you,
>Karen
>kschlauc at vt.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From s195404 at student.uq.edu.au  Fri Jul 11 00:39:29 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Thu, 10 Jul 2003 22:39:29 +0000
Subject: [R] packaged datasets in .csv format (David Firth)
In-Reply-To: <9C91DFD2-B2DC-11D7-8F15-0050E4C03977@nuffield.oxford.ac.uk>
References: <9C91DFD2-B2DC-11D7-8F15-0050E4C03977@nuffield.oxford.ac.uk>
Message-ID: <1057876769.3f0deb21164b6@my.uq.edu.au>

This information and advice is indeed very useful.

Some would wonder, however, whether a file delimited with semi-
colons can still be called a CSV file. Excel Help has "CSV (Comma
delimited) format") ;-)

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting David Firth <david.firth at nuffield.oxford.ac.uk>:

> Many thanks to those who replied to my question.
> 
> Dirk's suggestion, to use a .R file in the "data" directory of
> the  
> package, specifying how the .csv should be read, works fine as
> an  
> answer to the question about making comma-separated files
> available.
> 
> Uwe's answer to my other question (; vs ,), ie compatibility
> with  
> existing R packages, is well taken!
> 
> Cheers,
> David
> 
> On Thursday, Jul 10, 2003, at 12:25 Europe/London, Uwe Ligges
> wrote:
> 
> > Andreas Christmann wrote:
> >>>
> ----------------------------------------------------------------
-----
> 
> >>> -
> >>>
> >>> Message: 1
> >>> Date: Wed, 9 Jul 2003 10:53:27 +0100
> >>> From: David Firth <david.firth at nuffield.oxford.ac.uk>
> >>> Subject: [R] packaged datasets in .csv format
> >>> To: r-help at stat.math.ethz.ch
> >>> Message-ID:
> >>>    
> <307D34CE-B1F3-11D7-A8D2-0050E4C03977 at nuffield.oxford.ac.uk>
> >>> Content-Type: text/plain; charset=US-ASCII; format=flowed
> >>>
> >>> A couple of questions in connection with using .csv format
> to  
> >>> include data in a package:
> >>>
> >>> First, the background.  The data() function loads data from
> .csv  
> >>> ("comma-separated values") files using
> >>>
> >>>    read.table(..., header = TRUE, sep = ";")
> >>>
> >>> But ?read.table says
> >>>
> >>>       ## To write a CSV file for input to Excel one might
> use
> >>>       write.table(x, file = "foo.csv", sep = ",", col.names
> = NA)
> >>>       ## and to read this file back into R one needs
> >>>       read.table("file.csv", header = TRUE, sep = ",",
> row.names=1)
> >>>
> >>> As a result, .csv files created by write.table() as above
> are not  
> >>> read in by data() in the way that might be expected [that
> is,  
> >>> expected by someone who had not read help(data)!]
> >>>
> >>> Two questions, then:
> >>> -- is there some compelling reason for  the use of `sep =
> ";"' in  
> >>> place of `sep = ",", row.names=1'?
> >
> > Do you really want an answer?
> > Today, one reason is compatibility to all the other packages
> on CRAN.
> >
> >
> >> I prefer ";" instead of "," , because in text variables
> there are  
> >> often ",".
> >
> > That's why text variables can be quoted.
> >
> >
> >>> -- if I want to maintain a dataset in .csv format, for use
> both in R  
> >>> and in other systems such as Excel, SPSS, etc, what is the
> best way  
> >>> to go about it?
> >
> > When regularly using that many systems on the same data sets,
> it might  
> > be worth using a database system, e.g. MySQL.
> >
> > BTW: R *and* Excel *and* (for sure, but I haven't tested)
> also SPSS  
> > can read a couple of different ASCII formatted files, so
> there are  
> > quite a lot possible formats.
> >
> > Uwe Ligges
> >
> >
> >> Depends. Perhaps it is best to check it out for the software
> packages
> >> and the versions of the software packages you are using.
> > >
> >> Andreas Christmann
> >>>
> >>> Any advice would be much appreciated.
> >>>
> >>> Cheers,
> >>> David
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Fri Jul 11 01:27:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 10 Jul 2003 23:27:30 -0000
Subject: [R] packaged datasets in .csv format (David Firth)
In-Reply-To: <1057876769.3f0deb21164b6@my.uq.edu.au>
References: <9C91DFD2-B2DC-11D7-8F15-0050E4C03977@nuffield.oxford.ac.uk>
	<1057876769.3f0deb21164b6@my.uq.edu.au>
Message-ID: <x2llv6ark2.fsf@biostat.ku.dk>

"Andrew C. Ward" <s195404 at student.uq.edu.au> writes:

> This information and advice is indeed very useful.
> 
> Some would wonder, however, whether a file delimited with semi-
> colons can still be called a CSV file. Excel Help has "CSV (Comma
> delimited) format") ;-)

Well, Excel will itself generate CSV files separated with semicolons
in some locales, as far as I know. It doesn't work well to have commas
as decimal separator *and* field separator (although I've heard that
a version of Paradox did exactly that...) 

I do often wonder which idiot made CSV files locale dependent, and for
what possible reason.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dcum007 at ec.auckland.ac.nz  Fri Jul 11 02:35:24 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Fri, 11 Jul 2003 12:35:24 +1200
Subject: [R] Random Numbers
Message-ID: <1057883724.3f0e064c2181e@webmail1.ec.auckland.ac.nz>

I thank all who replied to my first post regarding random number gereration in 
R. I have taken all your advice and read much about the gereration of random 
numbers. I have managed to find a few pieces of code that I have converted for 
my Java program and am now at the stage of testing these systems. I have found 
a number of articles on the testing of random numbers and some nice tests that 
can be done. Are any of these in-built in R (such as the serial correlation, 
birthday spacings...).
Any help would be much appreciated.
Thank you
David
dcum007 at ec.auckland.ac.nz



From bart.rossel at anu.edu.au  Fri Jul 11 02:58:35 2003
From: bart.rossel at anu.edu.au (bart rossel)
Date: Fri, 11 Jul 2003 10:58:35 +1000
Subject: [R] R and XP
Message-ID: <000801c34747$8ec82f80$ae29cb96@bartpc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030711/c68b34af/attachment.pl

From Paul.Boutros at utoronto.ca  Fri Jul 11 03:22:47 2003
From: Paul.Boutros at utoronto.ca (Paul Boutros)
Date: Thu, 10 Jul 2003 21:22:47 -0400
Subject: [R] Version Number of a Package
In-Reply-To: <Pine.SOL.4.20.0307101619490.16143-100000@santiam.dfci.harvard.edu>
Message-ID: <CPEAKHBKLBNIKJDIELLCAEDJCDAA.Paul.Boutros@utoronto.ca>

Thanks to all for the help: works like a charm now. :)
Paul

> -----Original Message-----
> From: Jeff Gentry [mailto:jgentry at jimmy.harvard.edu]
> Sent: Thursday, July 10, 2003 4:24 PM
> To: Paul.Boutros at utoronto.ca
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Version Number of a Package
> 
> 
> > I am documenting an analysis procedure in a DB and I would like 
> to know the 
> > specific version number of each package that I use.  Is there a 
> standardized 
> > way of getting that information out from R, or should I parse 
> it out from the 
> > source-code files?  Ideally I would like a function like this:
> > myVerNum <- Version(package.names);
> 
> There are a several ways to skin that cat.
> 
> A few that come to mind are using package.description():
> > package.description("Biobase")["Version"]
>  Version 
> "1.3.27" 
> 
> You can also use installed.packages():
> > z <- installed.packages()
> > z[1,"Version"]
> [1] "1.2.28"
> 
> Also, in the Biobase package in Bioconductor (www.bioconductor.org),
> there's a function package.version():
> 
> > package.version("Biobase")
> [1] "1.3.27"
> 
> -J



From s195404 at student.uq.edu.au  Fri Jul 11 03:43:29 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Fri, 11 Jul 2003 01:43:29 +0000
Subject: [R] R and XP
In-Reply-To: <000801c34747$8ec82f80$ae29cb96@bartpc>
References: <000801c34747$8ec82f80$ae29cb96@bartpc>
Message-ID: <1057887809.3f0e16411bf88@my.uq.edu.au>

Jan, perhaps you could give an example of the syntax and output so
we can see what the problem is. There are lots of users of R under
Windows, so I don't anticipate a major compatibility issue. There
have been a number of question on the list about directing input
to and from R, so searching the archive may be an option as well.
 
Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting bart rossel <bart.rossel at anu.edu.au>:

> Dear whom this may concern,
> 
> I am having problems running R under windows XP. I can source
> files and get all the functions loaded, but when directing it
> to a file to carry out analyses it comes up with an error
> message. I am using R for analyses of gpr files generated from
> microarray slides using Axon genepix 2000.
> I hope you have a solution to my problems.
> 
> Kind regards,
> 
> 
> Jan Bart Rossel (PhD Candidate)
> The Australian National University
> School of Biochemistry and Molecular Biology
> Linneaus way, building 41
> Canberra, ACT 0200
> Australia
> email: bart.rossel at anu.edu.au
> phone: +61 02 61252663
> FAX: +61 02 61250313
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From npk at astro.ucsc.edu  Fri Jul 11 04:09:16 2003
From: npk at astro.ucsc.edu (Nicholas Konidaris)
Date: Thu, 10 Jul 2003 19:09:16 -0700 (PDT)
Subject: [R] FITS File Reader
Message-ID: <Pine.GSO.4.44.0307101908210.14764-100000@astro.ucsc.edu>

Dear R users,

	I have searched the web and CRAN fairly carefully.  Does a FITS
format file reader for R currently exist that I can download?

Thank you!
n



From dmurdoch at pair.com  Fri Jul 11 04:23:50 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 10 Jul 2003 22:23:50 -0400
Subject: [R] R and XP
In-Reply-To: <000801c34747$8ec82f80$ae29cb96@bartpc>
References: <000801c34747$8ec82f80$ae29cb96@bartpc>
Message-ID: <7f7sgvg2sc33c91gk0auhku20ntmeoms9b@4ax.com>

On Fri, 11 Jul 2003 10:58:35 +1000, you wrote:

>Dear whom this may concern,
>
>I am having problems running R under windows XP. I can source files and get all the functions loaded, but when directing it to a file to carry out analyses it comes up with an error message. I am using R for analyses of gpr files generated from microarray slides using Axon genepix 2000.
>I hope you have a solution to my problems.

Generally if you want help, you need to give enough information to
people to understand your problem. For example, you need to tell us
what you were doing when you saw the error, and what the error message
was, in enough detail that we can do the same thing on our machines
and see it ourselves.  

Simplify the task down to something very simple that you think should
work but doesn't:  often in doing that, you'll figure out what's wrong
with what you're doing, and won't need to ask anyone for help.

Duncan Murdoch



From drf5n at mug.sys.virginia.edu  Fri Jul 11 07:17:05 2003
From: drf5n at mug.sys.virginia.edu (drf5n@mug.sys.virginia.edu)
Date: Fri, 11 Jul 2003 01:17:05 -0400 (EDT)
Subject: [R] postscript/eps label clipping
Message-ID: <Pine.LNX.4.44.0307110025410.31120-100000@mug.sys.virginia.edu>

The following code produces an eps file with the tops of each of the ylabs
clipped off.

par(mfrow=c(2,2))
  plot(runif(10),
     ylab="Function(Lengthy Expression)",xlab="Prediction")
  plot(runif(10),
     ylab=expression(Delta * Beta^2),xlab="Prediction")
  plot(runif(10),
     ylab="Function(Lengthy Expression)",xlab="Prediction")
  plot(runif(10),
     ylab=expression(Delta * Beta^2),xlab="Prediction")
 dev.print(postscript,file="foo.eps",
  horizontal=FALSE,onefile=FALSE,paper="special",
  pointsize=7, width=5,height=4)

?postscript seems to indicate paper="special", width=, height=, and
pointsize= are the recommended way to produce nice latex graphics.

If I don't set a pointsize, the letters aren't clipped, but the graphs
are tiny with respect to the x/y labels.  Is there something else I should
be adjusting instead?

Thanks for your time,
Dave
-- 
 Dave Forrest    (434)924-3954w(111B) (804)642-0662h (804)695-2026p
 drf5n at maplepark.com            http://mug.sys.virginia.edu/~drf5n/



From Tom.Mulholland at health.wa.gov.au  Fri Jul 11 07:38:18 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Fri, 11 Jul 2003 13:38:18 +0800
Subject: [R] postscript/eps label clipping
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FC3@nt207mesep.health.wa.gov.au>

Never having used postscript as an output method I looked to see what
you were talking about. I  noted that "ps.options needs to be called
before calling postscript". ps.options does have pointsize within it and
silly though it may seem, its what I would do next.
_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
189 Royal St, East Perth, WA, 6004
 
Tel: (08) 9222 4062
e-mail: Tom.Mulholland at health.wa.gov.au
 
The contents of this e-mail transmission are confidential and may be
protected by professional privilege. The contents are intended only for
the named recipients of this e-mail. If you are not the intended
recipient, you are hereby notified that any use, reproduction,
disclosure or distribution of the information contained in this e-mail
is prohibited. Please notify the sender immediately.


-----Original Message-----
From: drf5n at mug.sys.virginia.edu [mailto:drf5n at mug.sys.virginia.edu] 
Sent: Friday, 11 July 2003 1:17 PM
To: R-help at stat.math.ethz.ch
Subject: [R] postscript/eps label clipping


The following code produces an eps file with the tops of each of the
ylabs clipped off.

par(mfrow=c(2,2))
  plot(runif(10),
     ylab="Function(Lengthy Expression)",xlab="Prediction")
  plot(runif(10),
     ylab=expression(Delta * Beta^2),xlab="Prediction")
  plot(runif(10),
     ylab="Function(Lengthy Expression)",xlab="Prediction")
  plot(runif(10),
     ylab=expression(Delta * Beta^2),xlab="Prediction")
dev.print(postscript,file="foo.eps",
  horizontal=FALSE,onefile=FALSE,paper="special",
  pointsize=7, width=5,height=4)

?postscript seems to indicate paper="special", width=, height=, and
pointsize= are the recommended way to produce nice latex graphics.

If I don't set a pointsize, the letters aren't clipped, but the graphs
are tiny with respect to the x/y labels.  Is there something else I
should be adjusting instead?

Thanks for your time,
Dave
-- 
 Dave Forrest    (434)924-3954w(111B) (804)642-0662h (804)695-2026p
 drf5n at maplepark.com            http://mug.sys.virginia.edu/~drf5n/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Tom.Mulholland at health.wa.gov.au  Fri Jul 11 08:02:20 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Fri, 11 Jul 2003 14:02:20 +0800
Subject: [R] postscript/eps label clipping
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D55C2E@nt207mesep.health.wa.gov.au>

I guess I was wrong there. However it does seem that it will come down
to fontsize 9 without clipping (or if it does I find it hard to see).

-----Original Message-----
From: Mulholland, Tom 
Sent: Friday, 11 July 2003 1:38 PM
To: David Forrest; R-help at stat.math.ethz.ch
Subject: RE: [R] postscript/eps label clipping


Never having used postscript as an output method I looked to see what
you were talking about. I  noted that "ps.options needs to be called
before calling postscript". ps.options does have pointsize within it and
silly though it may seem, its what I would do next.



From rob.hyndman at buseco.monash.edu.au  Fri Jul 11 08:21:50 2003
From: rob.hyndman at buseco.monash.edu.au (Rob Hyndman)
Date: Fri, 11 Jul 2003 16:21:50 +1000
Subject: [R] Sections for help files
Message-ID: <3F0E577E.31DA9B55@buseco.monash.edu.au>

When compiling a package, is there any way of making the help files
(html or chtml) have separate sections for functions and data sets? When
looking at the help file for a package with a large number of help
pages, it would be nice to have the functions appear first and the data
sets appear second rather than have them jumbled together
alphabetically.

Currently, under Windows, chtml files have an tree index with object
names under one node and object Titles under another node. Something
like this would work with one node for functions and one node for data
sets. 

Can this be done? Would it be possible to incorporate such a feature
into future releases, either optionally or as the standard for all
packages?

Rob Hyndman 
___________________________________________________
Rob J Hyndman
Associate Professor & Director of Consulting
Department of Econometrics & Business Statistics
Monash University, VIC 3800, Australia.
http://www-personal.buseco.monash.edu.au/~hyndman/



From a.CALANDRA at mclink.it  Thu Jul 10 17:39:07 2003
From: a.CALANDRA at mclink.it (Andrea Calandra)
Date: Thu, 10 Jul 2003 17:39:07 +0200 (CEST)
Subject: [R] info
Message-ID: <1.0.2.200307101736.51309@mclink.it>

HI

I'm a student in chemical engineering, and i have to implement an algoritm about FIVE PARAMETERS INTERPOLATION for a calibration curve (dose, optical density)

y = a + (c - a) /(1+ e[-b(x-m])

where
x = ln(analyte dose + 1)
y = the optical absorbance data
a = the curves top asymptote
b = the slope of the curve
c = the curves bottom asymptote
m = the curve X intercept

Have you never seen this formula, because i don't fine information or
lecterature about solution of this!!!

Can i help me

Hi 
Mr. Calandra



From maechler at stat.math.ethz.ch  Fri Jul 11 10:26:38 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 11 Jul 2003 10:26:38 +0200
Subject: [R] Contour Plots
In-Reply-To: <BAY1-F117kqkOzV8Q3i00003f95@hotmail.com>
References: <BAY1-F117kqkOzV8Q3i00003f95@hotmail.com>
Message-ID: <16142.29886.363893.668672@gargle.gargle.HOWL>

>>>>> "Riley" == Riley Metzger <alpha_white_wolf at hotmail.com>
>>>>>     on Thu, 10 Jul 2003 13:53:56 -0400 writes:

    Riley> Hello, I'm a grad. student in statistics and am
    Riley> looking for some information on how R draws its
    Riley> contours.  I suspect you are using a Bezier spline.

I don't know if what we do can be called a Bezier spline (it
would be the most simple, i.e. "linear" one...), but in any
case, we simply do linear interpolation :

contour() uses data on a grid.
The only thing you have to do is drawing line segments in
rectangles. In the following case, given the z-values in points
A & B, find the "*" intersection on the AB line
A & C, find the "+" intersection on the AC line
now draw the segment from "*" to "+"

  A ____*____ B 
   |   .     |
   |  .      |
   | .       |
   |.        |
   +         |
   |_________| 
  C           D

(and all you guys with silly proportional fonts in your e-mail
 reader will not see a nice picture above ....)

Regards,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Fri Jul 11 10:31:58 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 11 Jul 2003 10:31:58 +0200
Subject: [R] HTML Help "Search Engine & Keywords" (Mac OS X)
In-Reply-To: <p05210600bb337cd401b6@[160.36.251.70]>
References: <p05210600bb337cd401b6@[160.36.251.70]>
Message-ID: <16142.30206.128537.399976@gargle.gargle.HOWL>

>>>>> "bb2" == bb2  <bb2 at duke.edu>
>>>>>     on Thu, 10 Jul 2003 16:33:43 -0400 writes:

    bb2> Hi, I'm new to R having recently migrated from Splus
    bb2> 2000.  I'm having trouble using help.start().

    bb2> When I launch R help using help.start(), the index.html
    bb2> page comes up with the various manuals, etc.  If I
    bb2> click on one of the links, e.g.  Search Engine and
    bb2> Keywords, the next page (Keywords by Topic) comes up.
    bb2> However, if I click on any item, e.g. attributes,
    bb2> nothing at all happens.

    bb2> Any ideas on what is wrong and how to fix it?  I'm
    bb2> using R version 1.7.0 on Mac OSX.  R was installed
    bb2> using Fink.

Search engine things only work when your browser properly
supports Java.  This used to give problems for several versions
of Mozilla in the past, since that required to have a "JRE"
(Java runtime engine) being installed on your system and
properly ``plugged into'' the browser.

I have no idea how this looks on Mac OS X; note that you can
choose the kind of browser to be used as an argument to
help.start() or change its default using 
options(browser = "......").

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From C.E.Marshall at newcastle.ac.uk  Fri Jul 11 10:58:44 2003
From: C.E.Marshall at newcastle.ac.uk (C.E.Marshall)
Date: Fri, 11 Jul 2003 09:58:44 +0100
Subject: [R] Exporting data
Message-ID: <3F0EF6B5@sws.ncl.ac.uk>

Dear All

I am a new user to R and so I have a question which I hope you can help me 
with.

I am running simulations calculating correlation coefficients from bivariate 
data and I was wondering whether there is a way of exporting 1000 simulation 
results from R to a text file or to another file for further manipulation. I 
am having difficulty I think because of the text combined in with the 
numerical results.

Many Thanks

Carolyn Marshall



From baron at psych.upenn.edu  Fri Jul 11 11:38:54 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 11 Jul 2003 05:38:54 -0400
Subject: [R] Exporting data
In-Reply-To: <3F0EF6B5@sws.ncl.ac.uk>
References: <3F0EF6B5@sws.ncl.ac.uk>
Message-ID: <20030711093854.GA15448@mail2.sas.upenn.edu>

On 07/11/03 09:58, C.E.Marshall wrote:
>I am running simulations calculating correlation coefficients from bivariate 
>data and I was wondering whether there is a way of exporting 1000 simulation 
>results from R to a text file or to another file for further manipulation. I 
>am having difficulty I think because of the text combined in with the 
>numerical results.

If I understand you, it would seem that the trick is to put the
results of your simulation into a matrix or data frame, possibly
with one row or column per result.  I assume that each result
consists of a few numbers, each representing some variable.  Then
use write.table, write.matrix, or write.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From spencer.graves at pdf.com  Fri Jul 11 11:43:29 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 02:43:29 -0700
Subject: [R] info
References: <1.0.2.200307101736.51309@mclink.it>
Message-ID: <3F0E86C1.1040702@pdf.com>

I assume you mean the following:

chemYield <-
function(a, x)(a[1]+(a[3]-a[2])/(1+exp(-a[2]*(x-a[4]))

	  If you want to estimate parameters a[1:4] from data on pairs of (x, 
y=chemYield), create a data.frame(x, y), and estimate the parameter 
vector "a" using "nls".

	  If you have trouble getting "nls" to converge, I would plot the data 
and make a serious effort to get good starting values for "a" from the 
plot.  If I still have trouble, I'd try "optim", then feed the output 
from "optim" into "nls".

	  I seem to recall having seen problems like this discussed in Bates 
and Watts (1988) Nonlinear Regression Analysis and Its Applications 
(Wiley).  I don't have the book in hand at the moment, so I can't give 
you a page reference, but they discuss problems of this nature.  Bates 
was a pioneer in developing measures of intrinsic vs. parameter effects 
curvature.  Bates and Watts studied many published data sets and found 
that in nearly all cases, the parameter effects curvature was at least 
an order of magnitude larger than the intrinsic curvature.  That means 
that numerical difficulties can often (usually?) be improved by trying 
different parameterizations for the same problem.

	  The function "nls" and similar functions are described among other 
places in Venables and Ripley (2002) Modern Applied Statistics with S, 
4th ed. (Springer, ch. 8).

hope this helps.  spencer graves

Andrea Calandra wrote:
> HI
> 
> I'm a student in chemical engineering, and i have 
to implement an algoritm about FIVE PARAMETERS
INTERPOLATION for a calibration curve (dose, optical density)
> 
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> Have you never seen this formula, because i don't fine information or
> lecterature about solution of this!!!
> 
> Can i help me
> 
> Hi 
> Mr. Calandra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From fwhd3550 at mb.infoweb.ne.jp  Fri Jul 11 11:56:58 2003
From: fwhd3550 at mb.infoweb.ne.jp (Yukihiro Ishii)
Date: Fri, 11 Jul 2003 18:56:58 +0900
Subject: [R] Nonliner Rgression using Neural Nnetworks
Message-ID: <20030711185443.0379.FWHD3550@mb.infoweb.ne.jp>

Hi, 
I am an old hand at chemistry but a complete beginner at statistics
 including R computations.
My question is whether you can carry out nonlinear
multivariate regression  analysis in  R using neural networks, where the
output variable can range from -Inf to  + Inf., unlike discriminant 
analysis where the output is confined to one  or zero. The library nnet
seems to work only in the latter case but then I could  be wrong. 

Please help me there.

Thanks in advance.

Y.Ishii <yukiasais at ybb.ne.jp>
2-3-28?Tsurumaki-minami, Hadano
257-0002 Japan



From spencer.graves at pdf.com  Fri Jul 11 11:51:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 02:51:38 -0700
Subject: [R] Exporting data
References: <3F0EF6B5@sws.ncl.ac.uk>
Message-ID: <3F0E88AA.8080503@pdf.com>

	  Do you want "write(..., append=TRUE)"?

	  If not, please provide a toy problem that shows clearly what you 
thought should work and why it is not satisfactory.  You are more likely 
to get what you want from this list if someone else can pick up your 
scrap of code and try something in 30 seconds than if it takes them 10 
minutes to figure out what you are even asking.  Please also include the 
version of R you are using under which operating system.

hope this helps.  spencer graves

C.E.Marshall wrote:
> Dear All
> 
> I am a new user to R and so I have a question which I hope you can help me 
> with.
> 
> I am running simulations calculating correlation coefficients from bivariate 
> data and I was wondering whether there is a way of exporting 1000 simulation 
> results from R to a text file or to another file for further manipulation. I 
> am having difficulty I think because of the text combined in with the 
> numerical results.
> 
> Many Thanks
> 
> Carolyn Marshall
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sackur at heraclite.ens.fr  Fri Jul 11 12:17:33 2003
From: sackur at heraclite.ens.fr (Jerome Sackur)
Date: Fri, 11 Jul 2003 12:17:33 +0200 (MET DST)
Subject: [R] unimodality test
Message-ID: <Pine.SOL.3.95.1030711115545.3318A-100000@heraclite>

Dear R users,

I am interested in uni- bi- multimodality tests, for analysing reaction
times data. I was lead to Hartigan's dip test (Ann. Statistics, 13, 1985,
pp. 70-84, Applied Statistics, 34, 1985, 320-325). Not being a programmer
I am unable to translate the Fortran code given in ref. 2 into a R
function. I'd be glad to learn that someone already did it, or has devised
a better solution for this kind of problem..



Thanks a lot in advance,

J. Sackur
Inserm U562
Orsay, France



From John.Marsland at CommerzbankIB.com  Fri Jul 11 12:43:43 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Fri, 11 Jul 2003 11:43:43 +0100
Subject: [R] unz()
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E83A@xmx8lonib.lonib.commerzbank.com>


I am having problems getting the unz() function to work as a connection to
start reading a file...

z <- unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r")
readLines(z,2)

yields the following problems:

> z <- unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r")
Error in unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r") : 
        unable to open connection
In addition: Warning message: 
cannot locate file `close_tmi_components.txt' in zip file
`c:/temp/stoxx.zip' 
> readLines(z,2)
Error in readLines(z, 2) : cannot open the connection
In addition: Warning message: 
cannot locate file `close_tmi_components.txt' in zip file
`c:/temp/stoxx.zip' 

can anybody offer any advice?

Regards,

John Marsland


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From p.dalgaard at biostat.ku.dk  Fri Jul 11 12:47:16 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 11 Jul 2003 10:47:16 -0000
Subject: [R] Contour Plots
In-Reply-To: <16142.29886.363893.668672@gargle.gargle.HOWL>
References: <BAY1-F117kqkOzV8Q3i00003f95@hotmail.com>
	<16142.29886.363893.668672@gargle.gargle.HOWL>
Message-ID: <x2he5tbanj.fsf@biostat.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> >>>>> "Riley" == Riley Metzger <alpha_white_wolf at hotmail.com>
> >>>>>     on Thu, 10 Jul 2003 13:53:56 -0400 writes:
> 
>     Riley> Hello, I'm a grad. student in statistics and am
>     Riley> looking for some information on how R draws its
>     Riley> contours.  I suspect you are using a Bezier spline.
> 
> I don't know if what we do can be called a Bezier spline (it
> would be the most simple, i.e. "linear" one...), but in any
> case, we simply do linear interpolation :
> 
> contour() uses data on a grid.
> The only thing you have to do is drawing line segments in
> rectangles. In the following case, given the z-values in points
> A & B, find the "*" intersection on the AB line
> A & C, find the "+" intersection on the AC line
> now draw the segment from "*" to "+"
> 
>   A ____*____ B 
>    |   .     |
>    |  .      |
>    | .       |
>    |.        |
>    +         |
>    |_________| 
>   C           D
> 
> (and all you guys with silly proportional fonts in your e-mail
>  reader will not see a nice picture above ....)

The actual source is available... (the contourLines function in
src/main/plot3d.c). It's a little more elaborate than Martin says
because there can be intersections on all four sides, in which case
you have to decide whether to connect them like

+--+      +--+
|/ |  or  | \|
| /|      |\ |
+--+      +--+

(in principle, they could - and sometimes theoretically should - also
cross, but I don't think we allow them to do that).

There are more elaborate algorithms in existence, but from what I
gather, they tend to be sensitive to the smoothness of the surface and
can give wild results occasionally (e.g. have contour lines for
different levels intersect eachother).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vandemem at gmx.de  Fri Jul 11 13:47:44 2003
From: vandemem at gmx.de (Marc Vandemeulebroecke)
Date: Fri, 11 Jul 2003 13:47:44 +0200 (MEST)
Subject: [R] short puzzles
Message-ID: <19033.1057924064@www1.gmx.net>

Dear R users,

can someone help with these short puzzles?

1) Is there a function like outer() that evaluates a three-argument function
on a threedimensional grid - or else how to define such a function, say,
outer.3()? E.g., calculate (x/y)^z on (x,y,z) element of {1,2,3}x{3,4}x{4,5} and
return the results in a 3-dimensional array. I would naively use outer() on
two of the arguments within a for() loop for the third argument and somehow
glue the array together. Is there a better way? What about outer.4(), or even
outer.n(), generalizing outer() to functions with an arbitrary number of
arguments?

2)
Define a function dimnames.outer() such that dimnames.outer(x, y, "*")
returns, for x <- 1:2, y <- 2:3, the following matrix:

   y
x   2 3
  1 2 3
  2 4 6

(Or does such such a function already exist?)

3)

How to combine puzzle 1 and puzzle 2? A function dimnames.outer.n() would be
a nice little tool.

4)

How can I access, within a function, the name of a variable that I have
passed to the function? E.g., letting a <- 2, and subsequently calling function
f(a) as defined below,

f <- function (x) {
  # How can I get "a" out of x?
}

5)

Finally: Letting x <- 2, how can I transform "x+y" into "2+y" (as some
suitable object), or generally "func(x,y)" into "func(2,y)"?

Many thanks,
Marc

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!



From dmurdoch at pair.com  Fri Jul 11 14:31:20 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 11 Jul 2003 08:31:20 -0400
Subject: [R] FITS File Reader
In-Reply-To: <Pine.GSO.4.44.0307101908210.14764-100000@astro.ucsc.edu>
References: <Pine.GSO.4.44.0307101908210.14764-100000@astro.ucsc.edu>
Message-ID: <v4btgvoa6p6c0abbdj0sefbgte575g0892@4ax.com>

On Thu, 10 Jul 2003 19:09:16 -0700 (PDT), Nicholas Konidaris
<npk at astro.ucsc.edu> wrote :

>Dear R users,
>
>	I have searched the web and CRAN fairly carefully.  Does a FITS
>format file reader for R currently exist that I can download?

www.wotsit.org has a 13 year old document describing FITS, which seems
to be a fairly open-ended format,  so it may not cover what you need.
However, it looks reasonably straightforward to read it using the
stream functions:  look at the help topics ?file, ?readLines, and
?readBin.

If you do locate code to read it, or you end up writing some yourself,
you should consider contributing it to the foreign package.

Duncan Murdoch



From sundar.dorai-raj at pdf.com  Fri Jul 11 14:42:49 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 11 Jul 2003 07:42:49 -0500
Subject: [R] short puzzles
References: <19033.1057924064@www1.gmx.net>
Message-ID: <3F0EB0C9.5040101@pdf.com>



Marc Vandemeulebroecke wrote:
> Dear R users,
> 
> can someone help with these short puzzles?
> 
> 1) Is there a function like outer() that evaluates a three-argument function
> on a threedimensional grid - or else how to define such a function, say,
> outer.3()? E.g., calculate (x/y)^z on (x,y,z) element of {1,2,3}x{3,4}x{4,5} and
> return the results in a 3-dimensional array. I would naively use outer() on
> two of the arguments within a for() loop for the third argument and somehow
> glue the array together. Is there a better way? What about outer.4(), or even
> outer.n(), generalizing outer() to functions with an arbitrary number of
> arguments?
> 
> 2)
> Define a function dimnames.outer() such that dimnames.outer(x, y, "*")
> returns, for x <- 1:2, y <- 2:3, the following matrix:
> 
>    y
> x   2 3
>   1 2 3
>   2 4 6
> 
> (Or does such such a function already exist?)
> 
> 3)
> 
> How to combine puzzle 1 and puzzle 2? A function dimnames.outer.n() would be
> a nice little tool.
> 

Here's what I came up with. If you need the other functions you 
mentioned, you can extract them from this example.

outer.3 <- function(x, y, z, FUN, ...) {
   print(deparse(substitute(x))) # for question 2
   n.x <- NROW(x)
   n.y <- NROW(y)
   n.z <- NROW(z)
   nm.x <- if(is.array(x)) dimnames(x)[[1]] else names(x)
   nm.y <- if(is.array(y)) dimnames(y)[[1]] else names(y)
   nm.z <- if(is.array(z)) dimnames(z)[[1]] else names(z)
   X <- expand.grid(x = x, y = y, z = z)
   f <- FUN(X$x, X$y, X$z, ...)
   array(f, dim = c(n.x, n.y, n.z),
         dimnames = list(nm.x, nm.y, nm.z))
}

a <- 1:3
b <- 3:4
c <- 4:5
names(a) <- a
names(b) <- b
names(c) <- c
outer.3(a, b, c, function(x, y, z) (x/y)^z)
outer.3(as.matrix(a), as.matrix(b), as.matrix(c),
         function(x, y, z) (x/y)^z)

> 4)
> 
> How can I access, within a function, the name of a variable that I have
> passed to the function? E.g., letting a <- 2, and subsequently calling function
> f(a) as defined below,
> 
> f <- function (x) {
>   # How can I get "a" out of x?
> }
> 

Use deparse(substitute(x)). See example above.

> 5)
> 
> Finally: Letting x <- 2, how can I transform "x+y" into "2+y" (as some
> suitable object), or generally "func(x,y)" into "func(2,y)"?
> 

Use substitute(func(x, y), list(x = 2)).

Hope this is useful,

Sundar



From B.Rowlingson at lancaster.ac.uk  Fri Jul 11 14:48:21 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 11 Jul 2003 13:48:21 +0100
Subject: [R] FITS File Reader
In-Reply-To: <v4btgvoa6p6c0abbdj0sefbgte575g0892@4ax.com>
References: <Pine.GSO.4.44.0307101908210.14764-100000@astro.ucsc.edu>
	<v4btgvoa6p6c0abbdj0sefbgte575g0892@4ax.com>
Message-ID: <3F0EB215.3070109@lancaster.ac.uk>

Duncan Murdoch wrote:

> If you do locate code to read it, or you end up writing some yourself,
> you should consider contributing it to the foreign package.

There's a C (and fortran)-level library for reading FITS files here:

http://heasarc.gsfc.nasa.gov/docs/software/fitsio/fitsio.html

  - together with addons for Perl, Python, C++ etc. Coding an R 
interface would be a nice exercise in R coding.

  I would have already written this if not for the fact that most of the 
high-dimensional data that I deal with gets written with some awful 
unformatted fortran IO rubbish that needs reverse-engineering and 
byte-swapping. Haven't these people heard of HDF or FITS?? *sigh*

  Also, I notice that SciLab can read FITS files, by using ImageMagick, 
which can read FITS files.... ImageMagick could convert it to something 
readable by R, I guess.... This would be the dirty hack solution.

Baz



From matthew_wiener at merck.com  Fri Jul 11 14:49:30 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 11 Jul 2003 08:49:30 -0400
Subject: [R] short puzzles
Message-ID: <AEBD81486231A343B1813FE62D3352250369A0D4@usrymx15.merck.com>

I'll try #1.  You can use outer twice:

outer(outer(x,y, "/"),z,function(a,b),"^")

In general I suppose you could write an "outer.n" function that would take
the vectors and the functions to be applied and loop through them.  And I
think that outer will then take care of the dimnames for you.  (Taking care
of #2, and, I think, #3.)

Hope this helps.

Matt Wiener

-----Original Message-----
From: Marc Vandemeulebroecke [mailto:vandemem at gmx.de] 
Sent: Friday, July 11, 2003 7:48 AM
To: r-help at stat.math.ethz.ch
Subject: [R] short puzzles


Dear R users,

can someone help with these short puzzles?

1) Is there a function like outer() that evaluates a three-argument function
on a threedimensional grid - or else how to define such a function, say,
outer.3()? E.g., calculate (x/y)^z on (x,y,z) element of {1,2,3}x{3,4}x{4,5}
and
return the results in a 3-dimensional array. I would naively use outer() on
two of the arguments within a for() loop for the third argument and somehow
glue the array together. Is there a better way? What about outer.4(), or
even
outer.n(), generalizing outer() to functions with an arbitrary number of
arguments?

2)
Define a function dimnames.outer() such that dimnames.outer(x, y, "*")
returns, for x <- 1:2, y <- 2:3, the following matrix:

   y
x   2 3
  1 2 3
  2 4 6

(Or does such such a function already exist?)

3)

How to combine puzzle 1 and puzzle 2? A function dimnames.outer.n() would be
a nice little tool.

4)

How can I access, within a function, the name of a variable that I have
passed to the function? E.g., letting a <- 2, and subsequently calling
function
f(a) as defined below,

f <- function (x) {
  # How can I get "a" out of x?
}

5)

Finally: Letting x <- 2, how can I transform "x+y" into "2+y" (as some
suitable object), or generally "func(x,y)" into "func(2,y)"?

Many thanks,
Marc

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From fharrell at virginia.edu  Fri Jul 11 12:37:35 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 11 Jul 2003 06:37:35 -0400
Subject: [R] Nonliner Rgression using Neural Nnetworks
In-Reply-To: <20030711185443.0379.FWHD3550@mb.infoweb.ne.jp>
References: <20030711185443.0379.FWHD3550@mb.infoweb.ne.jp>
Message-ID: <20030711063735.57d82797.fharrell@virginia.edu>

On Fri, 11 Jul 2003 18:56:58 +0900
Yukihiro Ishii <fwhd3550 at mb.infoweb.ne.jp> wrote:

> Hi, 
> I am an old hand at chemistry but a complete beginner at statistics
>  including R computations.
> My question is whether you can carry out nonlinear
> multivariate regression  analysis in  R using neural networks, where the
> output variable can range from -Inf to  + Inf., unlike discriminant 
> analysis where the output is confined to one  or zero. The library nnet
> seems to work only in the latter case but then I could  be wrong. 
> 
> Please help me there.
> 
> Thanks in advance.
> 
> Y.Ishii <yukiasais at ybb.ne.jp>
> 257-0002 Japan

You might want to look at the paper at

http://brain.cs.unr.edu/publications/goodman.ann_advantages.jasa99.pdf

The work was done using a nice standalone neural net program Nevprop by Goodman and colleagues, which is intended for binary outcomes and incorporates bootstrapping for estimating predictive accuracy of the network.

You may obtain Nevprop at http://brain.cs.unr.edu
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From paulda at BATTELLE.ORG  Fri Jul 11 15:48:03 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 11 Jul 2003 09:48:03 -0400
Subject: [R] info
Message-ID: <940250A9EB37A24CBE28D858EF07774967A9C4@ws-bco-mse3.milky-way.battelle.org>

The most commonly used dose-response functions for nonlinear calibration 
curves are the four- and five-parameter logistic functions.  The four-
parameter logistic is specified as

F(z) = delta + (alpha - delta)/(1 + (z/gamma)^beta)

so I'm not sure where you are getting your dose-response functional form
from.  In any case, you can fit this model using either nls( ) or nlme( ),
depending on whether or not you want to fit a random-effects model.
For references related to the four- and five-parameter logistic functions,
you can read

1.  Rodbard, D., and Frazier, G.R. (1975) "Statistical analysis of
radioligand
assay data," Methods Enzymol., vol. 37, p. 3 - 22.

2.  Dudley, R.A., Edwards, P., and Ekins, R.P.  (1985)  "Guidelines for 
immunoassay data processing," Clin. Chem., vol. 31, no. 8, p. 1264 - 1271

The first of these articles introduces the four-parameter logistic, and the
second refines its parametrization as well as introduces the five-parameter
logistic for use in situations where the calibration curve is asymmetric.
You should also acquire "Mixed Effects Models in S and Splus", by Drs.
Pinheiro and Bates if you intend to do anything with mixed effects models.


Best,
 
 david paul



-----Original Message-----
From: Andrea Calandra [mailto:a.CALANDRA at mclink.it] 
Sent: Thursday, July 10, 2003 11:39 AM
To: R-help at stat.math.ethz.ch
Subject: [R] info


HI

I'm a student in chemical engineering, and i have to implement an algoritm 
about FIVE PARAMETERS INTERPOLATION for a calibration curve (dose, optical
density)

y = a + (c - a) /(1+ e[-b(x-m])

where
x = ln(analyte dose + 1)
y = the optical absorbance data
a = the curves top asymptote
b = the slope of the curve
c = the curves bottom asymptote
m = the curve X intercept

Have you never seen this formula, because i don't fine information or 
lecterature about solution of this!!!

Can i help me

Hi 
Mr. Calandra



From ArneSaatkamp at gmx.de  Fri Jul 11 16:01:15 2003
From: ArneSaatkamp at gmx.de (ArneSaatkamp@gmx.de)
Date: Fri, 11 Jul 2003 16:01:15 +0200 (MEST)
Subject: [R] correlation, import of large tables,
	test for point-biserial c.c.?
Message-ID: <22534.1057932075@www52.gmx.net>

Dear R help community,

I want to calculate correlations between environment parameters and species
abundance data. When I use the cor() for my table (121 columns 91 rows) R
generates a dataset with the correlations between all columns; 

1) How can I limit the calculations to the correlations of only the first
column with every other ? (Or:) How can I extract the line/row in question from
the cor() dataset produced by R ?

2) I assume that with one continuous factor and the other binary (0/1), 
cor() gives the point-biserial correlation coefficient, but how can I find the
method used by R ?

3) I was not able to import (from "Excel") the whole 121x67 table, for
instance I divided it into pieces. Is there a simple solution to import the whole
file ?

4) In the end I want to test the correlation coefficients. Where do I find
an appropriated test for the point biserial correlation ? Can R calculate the
coefficient and test it for all data in one step ?

I just started working & learning with R, but even after reading the R-help
and Introduction to R, I still have big difficulties, so
Thanks in advance for your help !!

Arne Saatkamp

Arne Saatkamp
Inst. f. Biol. II - Abt. f. Geobotanik
Sch?nzlestr. 1
79104 Freiburg
Germany

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!



From peter.schlattmann at medizin.fu-berlin.de  Fri Jul 11 16:01:48 2003
From: peter.schlattmann at medizin.fu-berlin.de (Dr. Peter Schlattmann)
Date: Fri, 11 Jul 2003 16:01:48 +0200 (CEST)
Subject: [R] hazard estimate
Message-ID: <2229.160.45.172.237.1057932108.squirrel@www.medizin.fu-berlin.de>

Dear list,

is there a function available which provides an estimate of the hazard
function
based on a cox proportional hazard model? I only found the cumulative
hazard and the survival function as survfit options.

Thanks for your help
Peter



From maechler at stat.math.ethz.ch  Fri Jul 11 16:04:07 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 11 Jul 2003 16:04:07 +0200
Subject: [R] unimodality test
In-Reply-To: <Pine.SOL.3.95.1030711115545.3318A-100000@heraclite>
References: <Pine.SOL.3.95.1030711115545.3318A-100000@heraclite>
Message-ID: <16142.50135.660168.923694@gargle.gargle.HOWL>

>>>>> "Jerome" == Jerome Sackur <sackur at heraclite.ens.fr>
>>>>>     on Fri, 11 Jul 2003 12:17:33 +0200 (MET DST) writes:

    Jerome> Dear R users, I am interested in uni- bi-
    Jerome> multimodality tests, for analysing reaction times
    Jerome> data. I was lead to Hartigan's dip test
    Jerome> (Ann. Statistics, 13, 1985, pp. 70-84, Applied
    Jerome> Statistics, 34, 1985, 320-325). Not being a
    Jerome> programmer I am unable to translate the Fortran code
    Jerome> given in ref. 2 into a R function. I'd be glad to
    Jerome> learn that someone already did it, or has devised a
    Jerome> better solution for this kind of problem..

I had got a version with Fortran and S-plus from Dario Ringach
(@ NYU.edu) in 1994 (from what I see) and had worked on it in 2000,
made it into an R package back then.
The reason it hasn't made its way to CRAN was that the Fortran code
(which I f2c'ed to C) still has bugs (leading to segmentation
faults)  that I've not yet found time to debug.

Let me have a look at it before making it available (not on
CRAN) but via FTP as a source package.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From gisar at nus.edu.sg  Fri Jul 11 16:16:49 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Fri, 11 Jul 2003 22:16:49 +0800
Subject: [R] Exporting data
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FEDF@MBXSRV03.stf.nus.edu.sg>

This really depends on what your output is. As previously suggested
save() and write() are excellent suggestions.

for(ii in 1:1000){
	out <- cor( x[ii, ], y[ii, ] ) # or whatever 
	cat(ii, "\t", out, "\n", append=TRUE, file="output.txt")
}

This method is really not worth it for small simulations but I found
this to be extremely useful with large simulations. 

The output file can act both as log/progress report file and provide
partial results even if your program crashes. More sophisticated method
can be found under connections().


-----Original Message-----
From: C.E.Marshall [mailto:C.E.Marshall at newcastle.ac.uk] 
Sent: Friday, July 11, 2003 4:59 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Exporting data


Dear All

I am a new user to R and so I have a question which I hope you can help
me 
with.

I am running simulations calculating correlation coefficients from
bivariate 
data and I was wondering whether there is a way of exporting 1000
simulation 
results from R to a text file or to another file for further
manipulation. I 
am having difficulty I think because of the text combined in with the 
numerical results.

Many Thanks

Carolyn Marshall

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bob.ohara at helsinki.fi  Fri Jul 11 16:17:40 2003
From: bob.ohara at helsinki.fi (Anon.)
Date: Fri, 11 Jul 2003 17:17:40 +0300
Subject: [R] Offsets in glmmPQL?
Message-ID: <3F0EC704.701@helsinki.fi>

I've got a colleague who's using a GLMM to analyse her data, and I've 
told her that she needs to include an offset.  However, glmmPQL doesn't 
seem to allow one to be included.  Is there anyway of doing this?

Bob

-- 
Bob O'Hara

Rolf Nevanlinna Institute
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/



From spencer.graves at pdf.com  Fri Jul 11 16:25:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 07:25:28 -0700
Subject: [R] info
References: <940250A9EB37A24CBE28D858EF07774967A9C4@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3F0EC8D8.30506@pdf.com>

	  Calandra's dose-response function is very close to what you wrote: 
She has x = ln(z+1), while x = ln(z) and m = ln(gamma) would give what 
you wrote.  I would guess that your comments and references should help 
her.

Spencer Graves	

Paul, David A wrote:
> The most commonly used dose-response functions for nonlinear calibration 
> curves are the four- and five-parameter logistic functions.  The four-
> parameter logistic is specified as
> 
> F(z) = delta + (alpha - delta)/(1 + (z/gamma)^beta)
> 
> so I'm not sure where you are getting your dose-response functional form
> from.  In any case, you can fit this model using either nls( ) or nlme( ),
> depending on whether or not you want to fit a random-effects model.
> For references related to the four- and five-parameter logistic functions,
> you can read
> 
> 1.  Rodbard, D., and Frazier, G.R. (1975) "Statistical analysis of
> radioligand
> assay data," Methods Enzymol., vol. 37, p. 3 - 22.
> 
> 2.  Dudley, R.A., Edwards, P., and Ekins, R.P.  (1985)  "Guidelines for 
> immunoassay data processing," Clin. Chem., vol. 31, no. 8, p. 1264 - 1271
> 
> The first of these articles introduces the four-parameter logistic, and the
> second refines its parametrization as well as introduces the five-parameter
> logistic for use in situations where the calibration curve is asymmetric.
> You should also acquire "Mixed Effects Models in S and Splus", by Drs.
> Pinheiro and Bates if you intend to do anything with mixed effects models.
> 
> 
> Best,
>  
>  david paul
> 
> 
> 
> -----Original Message-----
> From: Andrea Calandra [mailto:a.CALANDRA at mclink.it] 
> Sent: Thursday, July 10, 2003 11:39 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] info
> 
> 
> HI
> 
> I'm a student in chemical engineering, and i have to implement an algoritm 
> about FIVE PARAMETERS INTERPOLATION for a calibration curve (dose, optical
> density)
> 
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> Have you never seen this formula, because i don't fine information or 
> lecterature about solution of this!!!
> 
> Can i help me
> 
> Hi 
> Mr. Calandra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Fri Jul 11 16:32:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 07:32:28 -0700
Subject: [R] hazard estimate
References: <2229.160.45.172.237.1057932108.squirrel@www.medizin.fu-berlin.de>
Message-ID: <3F0ECA7C.5030200@pdf.com>

	  The hazard function is the derivative of the cumulate hazard.  There 
was a discussion only a few days ago on smoothing the hazard rate, which 
as I recall may have dealt with smoothing the cumulative hazard and then 
differentiating that.

	  Have you checked http://www.r-project.org/ -> search -> "R site 
search"?  I just got 24 hits for "smooth hazard".  If you don't get the 
answer from someone else here and an R site search does not produce what 
you want, then try r-help again.

hope this helps.  spencer graves

Dr. Peter Schlattmann wrote:
> Dear list,
> 
> is there a function available which provides an estimate of the hazard
> function
> based on a cox proportional hazard model? I only found the cumulative
> hazard and the survival function as survfit options.
> 
> Thanks for your help
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From gisar at nus.edu.sg  Fri Jul 11 16:39:05 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Fri, 11 Jul 2003 22:39:05 +0800
Subject: [R] correlation, import of large tables,
	test for point-biserial c.c.?
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FEE0@MBXSRV03.stf.nus.edu.sg>

1) Calculating a 121 x 121 correlation matrix and then extracting the relevant correlating is extremely inefficient and slow. Instead try this :

data  <- as.matrix( data )                           # data is your 91 x 121 matrix or dataframe
colInterest   <- data[ ,1]
apply( data, 2, function(x) cor(x , colInterest) )


Here you take each column of data (at which point it becomes a vector called x) and calculates its correlation. apply() is an efficient form of for() loop.


3) I have imported files of much bigger dimensions without any problem. First of all ensure that the data is in tab delimited or comma seperated not .xls. Next use read.delim or read.csv to read in the file. 

If the file is only partially loaded or garbled up, then check for special characters. Most often the culprit is the comment character #. Sometimes % @ etc can also cause a problem. 

I have no idea about the other questions. If you type in help.start(), you will get a help page where you can do a keyword search etc.


-----Original Message-----
From: ArneSaatkamp at gmx.de [mailto:ArneSaatkamp at gmx.de] 
Sent: Friday, July 11, 2003 10:01 PM
To: r-help at stat.math.ethz.ch
Subject: [R] correlation, import of large tables,test for point-biserial c.c.?


Dear R help community,

I want to calculate correlations between environment parameters and species abundance data. When I use the cor() for my table (121 columns 91 rows) R generates a dataset with the correlations between all columns; 

1) How can I limit the calculations to the correlations of only the first column with every other ? (Or:) How can I extract the line/row in question from the cor() dataset produced by R ?

2) I assume that with one continuous factor and the other binary (0/1), 
cor() gives the point-biserial correlation coefficient, but how can I find the method used by R ?

3) I was not able to import (from "Excel") the whole 121x67 table, for instance I divided it into pieces. Is there a simple solution to import the whole file ?

4) In the end I want to test the correlation coefficients. Where do I find an appropriated test for the point biserial correlation ? Can R calculate the coefficient and test it for all data in one step ?

I just started working & learning with R, but even after reading the R-help and Introduction to R, I still have big difficulties, so Thanks in advance for your help !!

Arne Saatkamp

Arne Saatkamp
Inst. f. Biol. II - Abt. f. Geobotanik
Sch?nzlestr. 1
79104 Freiburg
Germany

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Fri Jul 11 16:39:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 07:39:59 -0700
Subject: [R] correlation, import of large tables, test for point-biserial
	c.c.?
References: <22534.1057932075@www52.gmx.net>
Message-ID: <3F0ECC3F.3090506@pdf.com>

Did you look at "?cor"?

The documentation observes that cor accepts an optional second argument. 
  The following works:
 > df1 <- data.frame(a=1:8, b=rep(c(-1, 1), 4),
+  c=rep(c(-1, 1), each=4))
 > cor(df1[,1], df1[, -1])
              b         c
[1,] 0.2182179 0.8728716
 >
hope this helps.  spencer graves

ArneSaatkamp at gmx.de wrote:
> Dear R help community,
> 
> I want to calculate correlations between environment parameters and species
> abundance data. When I use the cor() for my table (121 columns 91 rows) R
> generates a dataset with the correlations between all columns; 
> 
> 1) How can I limit the calculations to the correlations of only the first
> column with every other ? (Or:) How can I extract the line/row in question from
> the cor() dataset produced by R ?
> 
> 2) I assume that with one continuous factor and the other binary (0/1), 
> cor() gives the point-biserial correlation coefficient, but how can I find the
> method used by R ?
> 
> 3) I was not able to import (from "Excel") the whole 121x67 table, for
> instance I divided it into pieces. Is there a simple solution to import the whole
> file ?
> 
> 4) In the end I want to test the correlation coefficients. Where do I find
> an appropriated test for the point biserial correlation ? Can R calculate the
> coefficient and test it for all data in one step ?
> 
> I just started working & learning with R, but even after reading the R-help
> and Introduction to R, I still have big difficulties, so
> Thanks in advance for your help !!
> 
> Arne Saatkamp
> 
> Arne Saatkamp
> Inst. f. Biol. II - Abt. f. Geobotanik
> Sch?nzlestr. 1
> 79104 Freiburg
> Germany
>



From info at rhkoning.com  Fri Jul 11 16:42:08 2003
From: info at rhkoning.com (Ruud H. Koning)
Date: Fri, 11 Jul 2003 16:42:08 +0200
Subject: [R] hazard estimate
In-Reply-To: <2229.160.45.172.237.1057932108.squirrel@www.medizin.fu-berlin.de>
References: <2229.160.45.172.237.1057932108.squirrel@www.medizin.fu-berlin.de>
Message-ID: <200307111642080875.01912FDB@192.168.1.66>

library(survival)
?basehaz

gives

  For `basehaz', a dataframe with the baseline hazard, times, and
     strata.


Ruud 

*********** REPLY SEPARATOR  ***********

On 7/11/2003 at 4:01  Dr. Peter Schlattmann wrote:

>Dear list,
>
>is there a function available which provides an estimate of the hazard
>function
>based on a cox proportional hazard model? I only found the cumulative
>hazard and the survival function as survfit options.
>
>Thanks for your help
>Peter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From vandemem at gmx.de  Fri Jul 11 17:12:54 2003
From: vandemem at gmx.de (Marc Vandemeulebroecke)
Date: Fri, 11 Jul 2003 17:12:54 +0200 (MEST)
Subject: Thanks: [R] short puzzles
References: <AEBD81486231A343B1813FE62D3352250369A0D4@usrymx15.merck.com>
Message-ID: <18992.1057936374@www61.gmx.net>

Thanks to Andy Liaw, Patrick Burns, Sundar Dorai-Raj and Matthiew Wiener for
the answers to my puzzles. Here is a summary:

****************** The original question: ******************

> Dear R users,
> 
> can someone help with these short puzzles?
> 
> 1) Is there a function like outer() that evaluates a three-argument
> function
> on a threedimensional grid - or else how to define such a function, say,
> outer.3()? E.g., calculate (x/y)^z on (x,y,z) element of
> {1,2,3}x{3,4}x{4,5}
> and
> return the results in a 3-dimensional array. I would naively use outer()
> on
> two of the arguments within a for() loop for the third argument and
> somehow
> glue the array together. Is there a better way? What about outer.4(), or
> even
> outer.n(), generalizing outer() to functions with an arbitrary number of
> arguments?
> 
> 2)
> Define a function dimnames.outer() such that dimnames.outer(x, y, "*")
> returns, for x <- 1:2, y <- 2:3, the following matrix:
> 
>    y
> x   2 3
>   1 2 3
>   2 4 6
> 
> (Or does such such a function already exist?)
> 
> 3)
> 
> How to combine puzzle 1 and puzzle 2? A function dimnames.outer.n() would
> be
> a nice little tool.
> 
> 4)
> 
> How can I access, within a function, the name of a variable that I have
> passed to the function? E.g., letting a <- 2, and subsequently calling
> function
> f(a) as defined below,
> 
> f <- function (x) {
>   # How can I get "a" out of x?
> }
> 
> 5)
> 
> Finally: Letting x <- 2, how can I transform "x+y" into "2+y" (as some
> suitable object), or generally "func(x,y)" into "func(2,y)"?
> 
> Many thanks,
> Marc
> 

******************** Answer to 5 **********************

The solution is of course 

substitute(func(x, y), list(x = 2))

******************** Answer to 4 **********************

This was easy, too:

deparse(substitute(x))

************* Answer to 1 in easy situations ****************

Where the three arguments are easily isolated, outer() can be used twice:

outer(outer(a,b, "/"),c,"^")

**************** Answer to 1, 2 and 3 ******************

A valuable idea came from Sundar Dorai-Raj who uses expand.grid() and then
transforms the grid into a matrix. Here is his code:

outer.3 <- function(x, y, z, FUN, ...) {
   print(deparse(substitute(x))) # for question 2
   n.x <- NROW(x)
   n.y <- NROW(y)
   n.z <- NROW(z)
   nm.x <- if(is.array(x)) dimnames(x)[[1]] else names(x)
   nm.y <- if(is.array(y)) dimnames(y)[[1]] else names(y)
   nm.z <- if(is.array(z)) dimnames(z)[[1]] else names(z)
   X <- expand.grid(x = x, y = y, z = z)
   f <- FUN(X$x, X$y, X$z, ...)
   array(f, dim = c(n.x, n.y, n.z),
         dimnames = list(nm.x, nm.y, nm.z))
}

a <- 1:3
b <- 3:4
c <- 4:5
names(a) <- a
names(b) <- b
names(c) <- c
outer.3(a, b, c, function(x, y, z) (x/y)^z)
outer.3(as.matrix(a), as.matrix(b), as.matrix(c),
         function(x, y, z) (x/y)^z)

Finally, I have included the following code in my Rprofile. Here only vector
arguments are allowed, the dimnames are handeled in a slightly different
manner, and the choice of creating dimnames is controlled by the logical
argument dn.

outer.2 <- function (x, y, f, dn=TRUE, ...) {
  if (!(is.vector(x) && is.vector(y) && is.numeric(x) && is.numeric(y))) {
    stop("arguments not numeric vectors")
  }
  ### The suitability of f is not checked ###
  result <- outer(x, y, f, ...)
  if (dn) {
    lab.x                   <- deparse(substitute(x))
    lab.y                   <- deparse(substitute(y))
    dimnames(result)        <- list(x, y)
    names(dimnames(result)) <- c(lab.x, lab.y)
  }
  result
}

outer.3 <- function(x, y, z, f, dn=TRUE, ...) {
  if (!(is.vector(x) && is.vector(y) && is.vector(z) 
        && is.numeric(x) && is.numeric(y) && is.numeric(z))) {
          stop("arguments not numeric vectors")
  }
  ### The suitability of f is not checked ###
  X <- expand.grid(x=x, y=y, z=z)
  temp <- f(X$x, X$y, X$z, ...)
  result <- array(temp, dim = c(length(x), length(y), length(z)))
  if (dn) {
     lab.x                   <- deparse(substitute(x))
     lab.y                   <- deparse(substitute(y))
     lab.z                   <- deparse(substitute(z))
     dimnames(result)        <- list(x, y, z)
     names(dimnames(result)) <- c(lab.x, lab.y, lab.z)
  }
  result
}

A similar function outer.4() is straightforward.

-- 


Jetzt ein- oder umsteigen und USB-Speicheruhr als Pr?mie sichern!



From tpapp at axelero.hu  Fri Jul 11 17:13:56 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 11 Jul 2003 17:13:56 +0200
Subject: [R] metapost device in R (again ;-)
Message-ID: <20030711151356.GA1656@localhost>

Hi,

I read the 2000 thread on a MetaPost device in R. If I understand
correctly, the main problem with the concept is that R wants the device
driver to give back information on the size of strings/labels.

To the bet of my knowledge, MetaPost _does_ make it possible to
measure the bounding box of text (see section 7.3: Measuring text in
the MetaPost manual). For example, one could get the size of the
bounding box of btex $\int_a^b x^2$ etex -- would that be enough to
make an implementation possible? Or are the size of individual
characters and kerning information necessary?

Another question: can graphics devices be implemented solely in R (ie
without writing C code)? I realize that it will be much slower, but
first I would like to see how it works before writing in C. What
source files should I be looking at?

You may ask why I should bother about using Metapost. Well, I'd like
my TeX documents to be more consistent typographically, and MP has
quite a lot of useful features (such as the possibility to include its
eps output in LaTeX directly, EVEN when generating PDF files with
latexpdf). But the biggest bonus would clearly be the ability to
typeset math formulas nicely. (I realize that this would require one
to start a MetaPost process, but IMO the benefits would be worth the
overhead).

Is anyone else interested in a MetaPost device?

Thanks,

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From John.Marsland at CommerzbankIB.com  Fri Jul 11 17:18:52 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Fri, 11 Jul 2003 16:18:52 +0100
Subject: [R] unz()
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E83C@xmx8lonib.lonib.commerzbank.com>

I've solve my own problem! apologies.

For the record:

the "filename" argument should include the full path of the file within the
directory structure of the zip file.

but there seeks to be an issue with readLines:

> readLines(z,2)
Error in readLines(z, 2) : seek not enabled for this connection

readLines(z) works fine for the whole file, but unfortunately you cannot use
things like read.table etc.


> -----Original Message-----
> From: Marsland, John [mailto:John.Marsland at commerzbankib.com]
> Sent: 11 July 2003 11:44
> To: 
> Subject: [R] unz()
> 
> 
> 
> I am having problems getting the unz() function to work as a 
> connection to
> start reading a file...
> 
> z <- unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r")
> readLines(z,2)
> 
> yields the following problems:
> 
> > z <- unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r")
> Error in unz("c:/temp/stoxx.zip", "close_tmi_components.txt", "r") : 
>         unable to open connection
> In addition: Warning message: 
> cannot locate file `close_tmi_components.txt' in zip file
> `c:/temp/stoxx.zip' 
> > readLines(z,2)
> Error in readLines(z, 2) : cannot open the connection
> In addition: Warning message: 
> cannot locate file `close_tmi_components.txt' in zip file
> `c:/temp/stoxx.zip' 
> 
> can anybody offer any advice?
> 
> Regards,
> 
> John Marsland
> 
> 
> **************************************************************
> ******** 
> This is a commercial communication from Commerzbank AG.\ \ 
> T...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From f0z6305 at labs.tamu.edu  Fri Jul 11 17:26:26 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Fri, 11 Jul 2003 10:26:26 -0500
Subject: [R] How to plot a scatter-plot matrix?
References: <Pine.LNX.4.44.0307020640160.7944-100000@stat55.stat.auckland.ac.nz>
Message-ID: <005b01c347c0$cc942660$fe445ba5@f0z6305>

Hey, R-listers

I am going to plot a scatter-plot matrix using R.
For example, give a matrix X=[x1, x2, ..., xn]
where each xi is a column vector, how to plot
all the pair scatter-plots between two different
xi and xj?

Is PAIRS able to achieve this function?

Thanks for your help.

Fred



From tpapp at axelero.hu  Fri Jul 11 17:26:02 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 11 Jul 2003 17:26:02 +0200
Subject: [R] 3d plot with different levels done in different colors
Message-ID: <20030711152602.GB1656@localhost>

I would like a 3d plot of a matrix such that individual trapezoids
that make up the surface are colored according to the z-value of that
point (or preferably the midpoint of its four corners, or something
similar). MS Excel has something like that.

I know that persp can have an nx by ny matrix its "col" argument, I
just don't know how to generate that matrix. To calculate the midpoint
of the tiles for the matrix z, I could use something like

zz <- (z[-1,-1] + z[-1,-nrow(z)] + z[-ncol(z),-1] + z[-ncol(z),-nrow(z)])/4

but I don't know how to assign a color to that value. For example if I
have

boundaries <- c(0, .5, 1, 1.5)

and 

colors <- c("red", "green", "blue")

I am looking for a function that assigns red to the elements of zz
between 0 and .5, etc. Alternative solutions are welcome, maybe
somebody has already wrote a library that does the whole thing neatly.

Thanks,

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From bates at stat.wisc.edu  Fri Jul 11 17:42:43 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jul 2003 15:42:43 -0000
Subject: [R] info
In-Reply-To: <1.0.2.200307101736.51309@mclink.it>
References: <1.0.2.200307101736.51309@mclink.it>
Message-ID: <6rllv5oyrs.fsf@bates4.stat.wisc.edu>

Andrea Calandra <a.CALANDRA at mclink.it> writes:

> I'm a student in chemical engineering, and i have to implement an algoritm about FIVE PARAMETERS INTERPOLATION for a calibration curve (dose, optical density)
> 
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> Have you never seen this formula, because i don't fine information or
> lecterature about solution of this!!!

This is one parameterization of the four-parameter logistic growth
curve.  A slightly different version is available as the selfStart
model SSfpl in the nls package.  In R try

library(nls)
?SSfpl
example(SSfpl)

to see how nls and SSfpl can be used.  The example even produces a
figure for you showing what the SSfpl parameters represent.

A literature reference for the SSfpl form of the four-parameter
logistic is Appendix C.6 in Pinheiro and Bates (2000), "Mixed-effects
Models in S and S-PLUS", Springer.



From tlumley at u.washington.edu  Fri Jul 11 17:47:15 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 11 Jul 2003 08:47:15 -0700 (PDT)
Subject: [R] hazard estimate
In-Reply-To: <200307111642080875.01912FDB@192.168.1.66>
Message-ID: <Pine.A41.4.44.0307110846490.205988-100000@homer07.u.washington.edu>

On Fri, 11 Jul 2003, Ruud H. Koning wrote:

> library(survival)
> ?basehaz
>
> gives
>
>   For `basehaz', a dataframe with the baseline hazard, times, and
>      strata.
>
>

Yes, but that's the cumulative hazard, not the hazard rate.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tothri2000 at yahoo.ca  Fri Jul 11 17:49:51 2003
From: tothri2000 at yahoo.ca (ge yreyt)
Date: Fri, 11 Jul 2003 11:49:51 -0400 (EDT)
Subject: [R] using SVD to get an inverse matrix of covariance matrix
Message-ID: <20030711154951.83023.qmail@web41610.mail.yahoo.com>

Dear R-users,

I have one question about using SVD to get an inverse
matrix of covariance matrix

Sometimes I met many singular values d are close to 0:
look this example

$d
 [1] 4.178853e+00 2.722005e+00 2.139863e+00
1.867628e+00 1.588967e+00
 [6] 1.401554e+00 1.256964e+00 1.185750e+00
1.060692e+00 9.932592e-01
[11] 9.412768e-01 8.530497e-01 8.211395e-01
8.077817e-01 7.706618e-01
[16] 7.007119e-01 6.237449e-01 5.709922e-01
5.550645e-01 5.062633e-01
[21] 4.792278e-01 4.222183e-01 3.660419e-01
3.293667e-01 3.026312e-01
[26] 2.942821e-01 2.811098e-01 2.626359e-01
2.199134e-01 1.943776e-01
[31] 1.712359e-01 1.561616e-01 1.359116e-01
1.280704e-01 1.099847e-01
[36] 1.013633e-01 9.622151e-02 8.396722e-02
7.083654e-02 6.755967e-02
[41] 5.392306e-02 3.807169e-02 2.942905e-02
2.726249e-02 4.555067e-16
[46] 3.095299e-16 2.918951e-16 2.672369e-16
2.336190e-16 2.239488e-16
[51] 2.089471e-16 1.970283e-16 1.863823e-16
1.775903e-16 1.698164e-16
[56] 1.594850e-16 1.500927e-16 1.469157e-16
1.406057e-16 1.366468e-16
[61] 1.319553e-16 1.252144e-16 1.193341e-16
1.142526e-16 1.064905e-16
[66] 1.040117e-16 1.005124e-16 9.310727e-17
8.995158e-17 8.529797e-17
[71] 8.204344e-17 7.759612e-17 7.478445e-17
7.225679e-17 6.709050e-17
[76] 5.996665e-17 5.830386e-17 5.687619e-17
5.121094e-17 4.848857e-17
[81] 4.549679e-17 4.307547e-17 3.830520e-17
3.450571e-17 3.312035e-17
[86] 3.260300e-17 2.399392e-17 2.141970e-17
1.996962e-17 1.881993e-17
[91] 1.567323e-17 1.062695e-17 6.730278e-18
2.118570e-18 4.991002e-19

Since the inverse matrix = u * inverse(d) * v',
If I calculate inverse d based on formula : 1/d, then
most values of inverse matrix
will be huge. This must be not a good way. MOre
special case, if a single value is 0, then
we can not calculate inverse d based on 1/d.

Therefore, my question is how I can calculate inverse
d (that is inverse diag(d) more efficiently???


Thanks

ping


______________________________________________________________________ 
Post your free ad now! http://personals.yahoo.ca



From gavin.simpson at ucl.ac.uk  Fri Jul 11 17:56:48 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 11 Jul 2003 16:56:48 +0100
Subject: [R] How to plot a scatter-plot matrix?
In-Reply-To: <005b01c347c0$cc942660$fe445ba5@f0z6305>
References: <Pine.LNX.4.44.0307020640160.7944-100000@stat55.stat.auckland.ac.nz>
	<005b01c347c0$cc942660$fe445ba5@f0z6305>
Message-ID: <3F0EDE40.4080009@ucl.ac.uk>

Fred,

[from help on pairs() ]:

...
Arguments:

        x: the coordinates of points given as columns of a matrix.

So yes, pairs will do what you ask.  See ?pairs for more info.

Also you might consider the alternative function from the lattice package:

 > library(lattice)	#load lattice graphics package
 > ?splom		#help for splom()

Which is called differently using a formula interface.

HTH

G

Feng Zhang wrote:

> Hey, R-listers
> 
> I am going to plot a scatter-plot matrix using R.
> For example, give a matrix X=[x1, x2, ..., xn]
> where each xi is a column vector, how to plot
> all the pair scatter-plots between two different
> xi and xj?
> 
> Is PAIRS able to achieve this function?
> 
> Thanks for your help.
> 
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From bates at stat.wisc.edu  Fri Jul 11 18:11:06 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 11 Jul 2003 16:11:06 -0000
Subject: [R] Offsets in glmmPQL?
In-Reply-To: <3F0EC704.701@helsinki.fi>
References: <3F0EC704.701@helsinki.fi>
Message-ID: <6rhe5toxi2.fsf@bates4.stat.wisc.edu>

"Anon." <bob.ohara at helsinki.fi> writes:

> I've got a colleague who's using a GLMM to analyse her data, and I've
> told her that she needs to include an offset.  However, glmmPQL
> doesn't seem to allow one to be included.  Is there anyway of doing
> this?

We just discovered and fixed a similar problem in the GLMM function in
the lme4 package.  I have uploaded the 0.2-4 release of lme4 to CRAN.
Please wait a few days for it to be transferred to the packages
directory and for a Windows package to be created then try it.
Remember to check for version 0.2-4 or later.



From tlumley at u.washington.edu  Fri Jul 11 18:16:36 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 11 Jul 2003 09:16:36 -0700 (PDT)
Subject: [R] using SVD to get an inverse matrix of covariance matrix
In-Reply-To: <20030711154951.83023.qmail@web41610.mail.yahoo.com>
Message-ID: <Pine.A41.4.44.0307110912240.205988-100000@homer07.u.washington.edu>

On Fri, 11 Jul 2003, ge yreyt wrote:

> Dear R-users,
>
> I have one question about using SVD to get an inverse
> matrix of covariance matrix
>
> Sometimes I met many singular values d are close to 0:
> look this example

<snip>

> most values of inverse matrix
> will be huge. This must be not a good way. MOre
> special case, if a single value is 0, then
> we can not calculate inverse d based on 1/d.
>
> Therefore, my question is how I can calculate inverse
> d (that is inverse diag(d) more efficiently???
>

If singular values are zero the matrix doesn't have an inverse: that is,
the equation   Mx=b  will have multiple solutions for any given b.

It is possible to get a pseudoinverse, a matrix M that picks out one of
the solutions.  One way to do this is to set the diagonal to 1/d where d
is not (nearly) zero and to 0 when d is (nearly) zero. One place to find a
discussion of this is `Matrix Computations' by Golub and van Loan.


	-thomas



From jerome at hivnet.ubc.ca  Fri Jul 11 18:24:34 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Fri, 11 Jul 2003 09:24:34 -0700
Subject: [R] using SVD to get an inverse matrix of covariance matrix
In-Reply-To: <20030711154951.83023.qmail@web41610.mail.yahoo.com>
References: <20030711154951.83023.qmail@web41610.mail.yahoo.com>
Message-ID: <200307111630.JAA16743@hivnet.ubc.ca>


If some of the eigenvalues of a square matrix are (close to) zero, then 
it's inverse does not exist. However, you can always calculate it's 
generalized inverse ginv().

library(MASS)
help(ginv)

It'll allow you to specify a "tol" argument:
     tol: A relative tolerance to detect zero singular values.

Hope that helps,
Jerome

On July 11, 2003 08:49 am, ge yreyt wrote:
> Content-Length: 2154
> Status: R
> X-Status: N
>
> Dear R-users,
>
> I have one question about using SVD to get an inverse
> matrix of covariance matrix
>
> Sometimes I met many singular values d are close to 0:
> look this example
>

<snip>

>
> Since the inverse matrix = u * inverse(d) * v',
> If I calculate inverse d based on formula : 1/d, then
> most values of inverse matrix
> will be huge. This must be not a good way. MOre
> special case, if a single value is 0, then
> we can not calculate inverse d based on 1/d.
>
> Therefore, my question is how I can calculate inverse
> d (that is inverse diag(d) more efficiently???
>
>
> Thanks
>
> ping
>
>
> ______________________________________________________________________
> Post your free ad now! http://personals.yahoo.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From emb7 at st-andrews.ac.uk  Fri Jul 11 18:26:19 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Fri, 11 Jul 2003 17:26:19 +0100
Subject: [R] Constraining asymptote in SSasymp
Message-ID: <oprr5ml5ak8xmvrg@gatty.st-and.ac.uk>

Hi all,
Is there a simple way to constrain the Asym argument in the SSasymp 
function so that it does not exceed some maximum value?

Thanks!

Martin

-- 
Martin Biuw
Sea Mammal Research Unit
Gatty Marine Laboratory, University of St Andrews
St Andrews, Fife KY16 8PA
Scotland
Ph: +44-(0)1334-462637
Fax: +44-(0)1334-462632
Web: http://smub.st.and.ac.uk



From ted.harding at nessie.mcc.ac.uk  Fri Jul 11 18:29:50 2003
From: ted.harding at nessie.mcc.ac.uk (ted.harding@nessie.mcc.ac.uk)
Date: Fri, 11 Jul 2003 17:29:50 +0100 (BST)
Subject: [R] Indexing with NA as FALSE??
Message-ID: <200307111629.h6BGTo367875@nessie.mcc.ac.uk>

Hi Folks,

Example:
t<-c(1,2,3,4,5,6,7,8,9)
u<-c(1,NA,3,NA,5,NA,7,NA,9)
t[u==5]
--> NA NA 5 NA NA

Now, if I could somehow set things so that "NA" was FALSE for
indexing, then
t[u==5]
--> 5

I know I can do it with
t[(u==5)&(!is.na(u))]
but in the situation I am dealing with this leads to massively
cumbersome, typo-prone and hard-to-read code.

Also, as an extra, it would be very useful if, for instance,
t[u==NA] --> 2 4 6 8
(I realise that working round this is less cumbersome, but even so).

What I'm really trying to work round is the "don't know" way that R
handles NA. Reasonable in the logical sence, in that in the first
example R is saying "Can't tell whether u[2], u[4], u[6], u[8]
are equal to 5 or not, so will return a result which represents
this uncertainty", and in the second "You're telling me you don't
know what value you want to match, so ... ".

Instead of that, since NA is one of the three values TRUE, FALSE, NA
of a logical, I'd like to be able to (a) treat NA as FALSE, (b) test
for a match between NA (as specified by me) and NA (as the value of
a logical variable).

Is there any non-cumbersome way to achieve this?

With thanks,
Ted



From B.Rowlingson at lancaster.ac.uk  Fri Jul 11 19:03:04 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 11 Jul 2003 18:03:04 +0100
Subject: [R] Indexing with NA as FALSE??
In-Reply-To: <200307111629.h6BGTo367875@nessie.mcc.ac.uk>
References: <200307111629.h6BGTo367875@nessie.mcc.ac.uk>
Message-ID: <3F0EEDC8.6030809@lancaster.ac.uk>

ted.harding at nessie.mcc.ac.uk wrote:

> I know I can do it with
> t[(u==5)&(!is.na(u))]
> but in the situation I am dealing with this leads to massively
> cumbersome, typo-prone and hard-to-read code.

  You could redefine '[' or '==', but that would lead to massively 
dangerous code. Anything could happen. Anyone who writes code that 
redefines such basic stuff may need their head examined.

  I think you are going to have to work round it with the !is.na(u) 
thing, but you could wrap it up in a function:

true4sure<-function(v){v & !is.na(v)}

then

 > t[true4sure(u==5)]
[1] 5

  although perhaps you could give it a less whimsical name....

> Also, as an extra, it would be very useful if, for instance,
> t[u==NA] --> 2 4 6 8
> (I realise that working round this is less cumbersome, but even so).

  Here is a way of doing that. It redefines '=='. It will break things 
that depend on NA's remaining NA's in comparisons. Do not use this code. 
Do not even let it pollute your files. Consider it a dangerous virus:

> assign("==",function(a,b){a[is.na(a)]<-FALSE; b[is.na(b)]<-FALSE; get("==","package:base")(a,b)})

  and then you get:

> c(1,2,3,NA,NA,NA) == c(1,NA,2,NA,NA,4)
[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE

> Instead of that, since NA is one of the three values TRUE, FALSE, NA
> of a logical, I'd like to be able to (a) treat NA as FALSE, (b) test
> for a match between NA (as specified by me) and NA (as the value of
> a logical variable).

  Thats what it does. Of course it has a bug/feature in that NA is now 
== to FALSE.... But then you arent going to use that code.

  Safer would be to define a new binary operator:

 > assign("%=na%",function(a,b){a[is.na(a)]<-FALSE; b[is.na(b)]<-FALSE; 
  get("==","package:base")(a,b)})

  Then you can do:

 > c(1,2,3,NA,NA,NA) %=na% c(1,NA,2,NA,NA,4)
[1]  TRUE FALSE FALSE  TRUE  TRUE FALSE

  again this has the same NA==FALSE property.

  Here's a truth table for that operator:

 > outer(c(T,F,NA),c(T,F,NA),"%=na%")
       [,1]  [,2]  [,3]
[1,]  TRUE FALSE FALSE
[2,] FALSE  TRUE  TRUE
[3,] FALSE  TRUE  TRUE

  You just need to write an operator that returns TRUE on the diagonal 
only.... Easy modification of %=na% but its late on a Friday and I have 
a poker game to attend...

  Did I say not to use my code that redefines '=='? Well dont use it. Ever.

Baz



From tlumley at u.washington.edu  Fri Jul 11 19:07:29 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 11 Jul 2003 10:07:29 -0700 (PDT)
Subject: [R] Indexing with NA as FALSE??
In-Reply-To: <200307111629.h6BGTo367875@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.44.0307111005320.205988-100000@homer07.u.washington.edu>

On Fri, 11 Jul 2003 ted.harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
>
> Example:
> t<-c(1,2,3,4,5,6,7,8,9)
> u<-c(1,NA,3,NA,5,NA,7,NA,9)
> t[u==5]
> --> NA NA 5 NA NA
>
> Now, if I could somehow set things so that "NA" was FALSE for
> indexing, then
> t[u==5]
> --> 5

t[u %in% 5]


> I know I can do it with
> t[(u==5)&(!is.na(u))]
> but in the situation I am dealing with this leads to massively
> cumbersome, typo-prone and hard-to-read code.
>
> Also, as an extra, it would be very useful if, for instance,
> t[u==NA] --> 2 4 6 8

t[ u %in% NA]


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From rn001 at cebas.csic.es  Fri Jul 11 19:18:30 2003
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Fri, 11 Jul 2003 19:18:30 +0200
Subject: [R] three short questions
Message-ID: <200307111703.h6BH3YI24955@natura.cebas.csic.es>

Hi all;

This is my first message to the list, and I've got three "basic" questions:

How could I insert comments in a file with commands to be used as source in R?

Is it possible to quickly display a window with all the colors available in 
colors()? How?

I'm displaying points, but they overlap, wether points() uses triangles, 
bullets or whatever. Is it possible to change (diminish) the size of the 
symbols? 

Thanks all,

Javier



From ltorgo at liacc.up.pt  Fri Jul 11 20:29:29 2003
From: ltorgo at liacc.up.pt (Luis Torgo)
Date: Fri, 11 Jul 2003 18:29:29 +0000
Subject: [R] Question regarding irts class
Message-ID: <200307111829.29254.ltorgo@liacc.up.pt>

I'm using the new irts class from package tseries which I find quite useful.
However, I have data of different type being sample at irregular times (i.e. 
my data is more of a data frame than a matrix).
Function irts that is used to create irts objects demands that the value 
component is either a vector or a matrix. Is there any reason for not 
allowing it to be a data frame? Looking at the code of this function it seems 
straightforward to allow the function to accept data frames, but maybe there 
is some reason for not allowing this that I'm not aware.
Thanks,
Luis Torgo

-- 
Luis Torgo
    FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
    Machine Learning Group           Fax   : (+351) 22 600 36 54
    R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
    4150 PORTO   -  PORTUGAL         WWW   : http://www.liacc.up.pt/~ltorgo



From lockwood at rand.org  Fri Jul 11 19:27:37 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Fri, 11 Jul 2003 13:27:37 -0400 (EDT)
Subject: [R] three short questions
In-Reply-To: <200307111703.h6BH3YI24955@natura.cebas.csic.es>
Message-ID: <Pine.LNX.4.33.0307111325440.13878-100000@penguin.rand.org>

> 
> This is my first message to the list, and I've got three "basic" questions:
> 
> How could I insert comments in a file with commands to be used as source in R?

use the pound sign "#"

> 
> Is it possible to quickly display a window with all the colors available in 
> colors()? How?
> 

I've got such a thing on my web page, though it may be dated

http://www.rand.org/methodology/stat/members/lockwood/downloads/R-built-in-colors.pdf


> I'm displaying points, but they overlap, wether points() uses triangles, 
> bullets or whatever. Is it possible to change (diminish) the size of the 
> symbols? 
> 

yes; use "pch" to change symbols and "cex" to change sizes of symbols.
See the help page for "par"

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From dcojoc at clemson.edu  Fri Jul 11 22:29:20 2003
From: dcojoc at clemson.edu (Doru Cojoc)
Date: Fri, 11 Jul 2003 13:29:20 -0700
Subject: [R] Reading data from the console
Message-ID: <3F0F1E20.2487B53E@clemson.edu>

I want to be able to write a program in R that does the following:
- it allows the user to enter the dimensions of the matrix from the
console
- it allows the user then to enter each element of the matrix from the
console.

I am looking for an equivalent for the C++ command read, or read.ln.
read.table would not work, since the data is not in a table and,
furthermore, since the data does not exist prior to the execution of the
program; the user has to be able to introduce the data from the console.
scan would not work either. For example, in the following bit of code

cat('Number of populations:', '\n')
m<-scan("",n=1, quiet=TRUE)
cat('Number of categories:', '\n')
k<-scan("",n=1, quiet=TRUE)
N<-matrix(0,m,k)
for(i in 1:m) for(j in 1:k) {
 N[i,j]<-scan("",n=1, quiet=TRUE)
 }

scan will take m to be the next command rather than wait for me to
introduce the number I want for m.

Any ideas? Thanks a lot!

Doru Cojoc



From jerome at hivnet.ubc.ca  Fri Jul 11 19:45:33 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Fri, 11 Jul 2003 10:45:33 -0700
Subject: [R] Reading data from the console
In-Reply-To: <3F0F1E20.2487B53E@clemson.edu>
References: <3F0F1E20.2487B53E@clemson.edu>
Message-ID: <200307111751.KAA19536@hivnet.ubc.ca>


Put your code in a function. See below.

Cheers,
Jerome

mat <- function()
{
cat('Number of populations:', '\n')
m<-scan("",n=1, quiet=TRUE)
cat('Number of categories:', '\n')
k<-scan("",n=1, quiet=TRUE)
N<-matrix(0,m,k)
for(i in 1:m) for(j in 1:k) {
 N[i,j]<-scan("",n=1, quiet=TRUE)
 }
N
}

> mat()
Number of populations:
1: 2
Number of categories:
1: 2
1: 1
1: 2
1: 3
1: 4
     [,1] [,2]
[1,]    1    2
[2,]    3    4



On July 11, 2003 01:29 pm, Doru Cojoc wrote:
> I want to be able to write a program in R that does the following:
> - it allows the user to enter the dimensions of the matrix from the
> console
> - it allows the user then to enter each element of the matrix from the
> console.
>
> I am looking for an equivalent for the C++ command read, or read.ln.
> read.table would not work, since the data is not in a table and,
> furthermore, since the data does not exist prior to the execution of the
> program; the user has to be able to introduce the data from the console.
> scan would not work either. For example, in the following bit of code
>
> cat('Number of populations:', '\n')
> m<-scan("",n=1, quiet=TRUE)
> cat('Number of categories:', '\n')
> k<-scan("",n=1, quiet=TRUE)
> N<-matrix(0,m,k)
> for(i in 1:m) for(j in 1:k) {
>  N[i,j]<-scan("",n=1, quiet=TRUE)
>  }
>
> scan will take m to be the next command rather than wait for me to
> introduce the number I want for m.
>
> Any ideas? Thanks a lot!
>
> Doru Cojoc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Fri Jul 11 20:05:30 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 11 Jul 2003 18:05:30 -0000
Subject: [R] three short questions
In-Reply-To: <Pine.LNX.4.33.0307111325440.13878-100000@penguin.rand.org>
References: <Pine.LNX.4.33.0307111325440.13878-100000@penguin.rand.org>
Message-ID: <1057946706.19814.43.camel@localhost>

On Fri, 2003-07-11 at 12:27, J.R. Lockwood wrote:
> > 
> > This is my first message to the list, and I've got three "basic" questions:
> > 
> > How could I insert comments in a file with commands to be used as source in R?
> 
> use the pound sign "#"
> 
> > 
> > Is it possible to quickly display a window with all the colors available in 
> > colors()? How?
> > 
> 
> I've got such a thing on my web page, though it may be dated
> 
> http://www.rand.org/methodology/stat/members/lockwood/downloads/R-built-in-colors.pdf
> 
> 
> > I'm displaying points, but they overlap, wether points() uses triangles, 
> > bullets or whatever. Is it possible to change (diminish) the size of the 
> > symbols? 
> > 
> 
> yes; use "pch" to change symbols and "cex" to change sizes of symbols.
> See the help page for "par"


A couple other possibilities on your third query, depending upon the
nature of your data and the type of plot you are using:

1. Adjust the limits of the x and/or y axis in your plot to enhance the
separation of the points. Generally, this can be done with the 'xlim'
and/or 'ylim' arguments to your plotting function. For example, see
?plot.default for more information.

2. You can try to use jitter() [ie. plot(jitter(x), jitter(y))] if you
have a lot of points that overlap at common coordinates. This introduces
a level of 'noise' to the x and/or y values and can result in a level of
dispersion of these points. See ?jitter for more information. This can
result in a visual "clustering" of points, so keep that in mind.

Be aware that each of the above can impact in a positive or deleterious
fashion, the visual presentation and interpretation of your data so
proceed with caution.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Fri Jul 11 21:23:45 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 12:23:45 -0700
Subject: [R] using SVD to get an inverse matrix of covariance matrix
References: <Pine.A41.4.44.0307110912240.205988-100000@homer07.u.washington.edu>
Message-ID: <3F0F0EC1.5080509@pdf.com>

	  More presicely, if M is singular, then M%*%x = b will have multiple 
solutions only if "b" is in the subspace spanned by columns of M.

	  Example:

 > M <- array(1:2, dim=c(2,2))
 > (svdM <- svd(M))
$d
[1] 3.162278 0.000000

$u
            [,1]       [,2]
[1,] -0.4472136 -0.8944272
[2,] -0.8944272  0.4472136

$v
            [,1]       [,2]
[1,] -0.7071068 -0.7071068
[2,] -0.7071068  0.7071068

	  This "M" is not symmetrical and so cannot be a covariance matrix, but 
you can get the same effect with a symmetrical matrix.  I'm using this 
example because it is the simplest thing that comes to mind to 
illustrate the point.

	  By the definition of the singular value decomposition, M = svdM$u %*% 
diag(svdM$d) %*% t(svdM$v).  Since svdM$d[2] == 0,

	  M%*%x = svdM$u[, 1]*svdM$d[1]*t(svdM$v[,1])%*%x
	    = x1 * svdM$u[,1],
where
	  x1 = svdM$d[1]*t(svdM$v[,1])%*%x

	  Thus, if b is proportional to svdM$u[,1], the system has a solution. 
  That solution will be proportional to svdM$v[,1] + x2*svdM$v[,2], for 
any arbitrary value of x2.

hope this helps.
spencer graves

Thomas Lumley wrote:
> On Fri, 11 Jul 2003, ge yreyt wrote:
> 
> 
>>Dear R-users,
>>
>>I have one question about using SVD to get an inverse
>>matrix of covariance matrix
>>
>>Sometimes I met many singular values d are close to 0:
>>look this example
> 
> 
> <snip>
> 
>>most values of inverse matrix
>>will be huge. This must be not a good way. MOre
>>special case, if a single value is 0, then
>>we can not calculate inverse d based on 1/d.
>>
>>Therefore, my question is how I can calculate inverse
>>d (that is inverse diag(d) more efficiently???
>>
> 
> 
> If singular values are zero the matrix doesn't have an inverse: that is,
> the equation   Mx=b  will have multiple solutions for any given b.
> 
> It is possible to get a pseudoinverse, a matrix M that picks out one of
> the solutions.  One way to do this is to set the diagonal to 1/d where d
> is not (nearly) zero and to 0 when d is (nearly) zero. One place to find a
> discussion of this is `Matrix Computations' by Golub and van Loan.
> 
> 
> 	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From drf5n at mug.sys.virginia.edu  Fri Jul 11 21:23:15 2003
From: drf5n at mug.sys.virginia.edu (drf5n@mug.sys.virginia.edu)
Date: Fri, 11 Jul 2003 15:23:15 -0400 (EDT)
Subject: [R] postscript/eps label clipping
In-Reply-To: <74E242B6968AA0469B632C5A3EFC1EFD03D55C2E@nt207mesep.health.wa.gov.au>
Message-ID: <Pine.LNX.4.44.0307111434120.31120-100000@mug.sys.virginia.edu>

On Fri, 11 Jul 2003, Mulholland, Tom wrote:

> I guess I was wrong there. However it does seem that it will come down
> to fontsize 9 without clipping (or if it does I find it hard to see).

Thanks.  It seemed like that is the way it was working, but it also seems
counterintuitive: reduced pointsizes in postscript output make the graphs
bigger, up to a point, after which the letters are too big(small?) to fit
without clipping.

Thanks again,
Dave.
-- 
 Dave Forrest    (434)924-3954w(111B) (804)642-0662h (804)695-2026p
 drf5n at maplepark.com            http://mug.sys.virginia.edu/~drf5n/



From cliff at ms.washington.edu  Fri Jul 11 21:33:34 2003
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Fri, 11 Jul 2003 12:33:34 -0700
Subject: [R] getAnyhwhere behavior
Message-ID: <004901c347e3$52163040$52b2eb0c@C56909A>

I would have expected the function getAnywhere to have behaved
differently in the following:

> search()
 [1] ".GlobalEnv"                  "file:C:/R/Rdata/miya/.Rdata"
 [3] "package:boot"                "package:methods"
 [5] "package:ctest"               "package:mva"
 [7] "package:modreg"              "package:nls"
 [9] "package:ts"                  "Autoloads"
[11] "package:base"

> getAnywhere(basic.ci)
Error in getAnywhere(basic.ci) : Object "basic.ci" not found

> basic.ci<-get("basic.ci",environment(boot))

> getAnywhere(basic.ci)
A single object matching `basic.ci' was found
It was found in the following places
  .GlobalEnv
  registered S3 method for basic from namespace boot
  namespace:boot
with value

function (t0, t, conf = 0.95, hinv = function(t) t)
{
    qq <- norm.inter(t, (1 + c(conf, -conf))/2)
    out <- cbind(conf, matrix(qq[, 1], ncol = 2), matrix(hinv(2 *
        t0 - qq[, 2]), ncol = 2))
    out
}
<environment: namespace:boot>

Why did getAnywhere not "see" basic.ci in the environment from which I
"got" it?

**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From mpiorecky at hotmail.com  Fri Jul 11 22:39:09 2003
From: mpiorecky at hotmail.com (Mark Piorecky)
Date: Fri, 11 Jul 2003 14:39:09 -0600
Subject: [R] spdep
Message-ID: <BAY8-DAV41oFLaqSOc700000be4@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030711/03b9c911/attachment.pl

From jerome at hivnet.ubc.ca  Fri Jul 11 22:49:37 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Fri, 11 Jul 2003 13:49:37 -0700
Subject: [R] 3d plot with different levels done in different colors
In-Reply-To: <20030711152602.GB1656@localhost>
References: <20030711152602.GB1656@localhost>
Message-ID: <200307112056.NAA25117@hivnet.ubc.ca>


Hi,

Consider this example which I have modified from the persp() help file.
It uses topo.colors() to create a series of colors.

Cheers,
Jerome

     x <- seq(-10, 10, length= 30)
     y <- x
     f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
     z <- outer(x, y, f)
     z[is.na(z)] <- 1
     op <- par(bg = "white")
     zz <- (z[-1,-1] + z[-1,-ncol(z)] + z[-nrow(z),-1] + 
              z[-nrow(z),-ncol(z)])/4
     cols <- topo.colors(length(zz))
     cols[order(zz)] <- cols
     persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = cols)


On July 11, 2003 08:26 am, Tamas Papp wrote:
> Content-Length: 1223
> Status: R
> X-Status: N
>
> I would like a 3d plot of a matrix such that individual trapezoids
> that make up the surface are colored according to the z-value of that
> point (or preferably the midpoint of its four corners, or something
> similar). MS Excel has something like that.
>
> I know that persp can have an nx by ny matrix its "col" argument, I
> just don't know how to generate that matrix. To calculate the midpoint
> of the tiles for the matrix z, I could use something like
>
> zz <- (z[-1,-1] + z[-1,-nrow(z)] + z[-ncol(z),-1] +
> z[-ncol(z),-nrow(z)])/4
>
> but I don't know how to assign a color to that value. For example if I
> have
>
> boundaries <- c(0, .5, 1, 1.5)
>
> and
>
> colors <- c("red", "green", "blue")
>
> I am looking for a function that assigns red to the elements of zz
> between 0 and .5, etc. Alternative solutions are welcome, maybe
> somebody has already wrote a library that does the whole thing neatly.
>
> Thanks,
>
> Tamas



From rwang at math.ucalgary.ca  Fri Jul 11 23:18:03 2003
From: rwang at math.ucalgary.ca (rui)
Date: Fri, 11 Jul 2003 15:18:03 -0600
Subject: [R] How to generate regression matrix with correlation matrix
Message-ID: <002c01c347f1$ea1cc3f0$bf3d9f88@pcrui>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030711/1d5195b6/attachment.pl

From spencer.graves at pdf.com  Fri Jul 11 23:53:50 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 14:53:50 -0700
Subject: [R] How to generate regression matrix with correlation matrix
References: <002c01c347f1$ea1cc3f0$bf3d9f88@pcrui>
Message-ID: <3F0F31EE.3070509@pdf.com>

	  What problem are you really trying to solve?  The problem statement 
as I read it contains two logical contradictions that I see:

	  1.  Orthonormal means X'X = Identity matrix (10 x 10).  That means 
the pairwise correlation coefficients can NOT be different from 0.

	  2.  Not all symmetric matrices with 1's on the diagonal and random 
numbers U(-1, 1) on the off diagonal are correlation matrices.  Consider 
the following example:

  Cormat <- array(c(1, -0.9, -0.9, -0.9, 1, -0.9, -0.9, -0.9, 1), 
dim=c(3,3))
 > Cormat
      [,1] [,2] [,3]
[1,]  1.0 -0.9 -0.9
[2,] -0.9  1.0 -0.9
[3,] -0.9 -0.9  1.0
 > eigen(Cormat)
$values
[1]  1.9  1.9 -0.8

The fact that one eigenvalue is negative means that this "Cormat" is not 
positive definite.

hope this helps.  spencer graves

rui wrote:
> Dear R community:
> 
> I want to simulate a regression matrix which 
is generated from an orthonormal matrix X of
dimension 30*10 with different between-column
pairwise correlation coefficients generated from
uniform distribution U(-1,1).
> 
> Thanks in advance!
> 
> Rui
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jc at or.psychology.dal.ca  Sat Jul 12 02:20:15 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 11 Jul 2003 21:20:15 -0300
Subject: [R] ss's are incorrect from aov with multiple factors
Message-ID: <9BFDA490-B3FE-11D7-81B9-000A9566473A@or.psychology.dal.ca>

Hi,
	I have been trying to work with error terms given back from aov to 
make confidence intervals.  However, the numbers seem to be incorrect.  
If there is more than one term in the ANOVA then the error terms can be 
inflated by the number of factors in the extra terms.  The F's are 
correct so it is right back to the SS.  I was wondering if this is 
standard practice for stats programs or unique to R?



From rwang at math.ucalgary.ca  Sat Jul 12 02:27:32 2003
From: rwang at math.ucalgary.ca (rui)
Date: Fri, 11 Jul 2003 18:27:32 -0600
Subject: [R] More clear statement about the question of how to generate
	regression matrix with correlation matrix
Message-ID: <006301c3480c$6269e3a0$bf3d9f88@pcrui>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030711/3e01abe3/attachment.pl

From loesljrg at accucom.net  Sat Jul 12 02:38:13 2003
From: loesljrg at accucom.net (JRG)
Date: Fri, 11 Jul 2003 20:38:13 -0400
Subject: [R] ss's are incorrect from aov with multiple factors
In-Reply-To: <9BFDA490-B3FE-11D7-81B9-000A9566473A@or.psychology.dal.ca>
Message-ID: <B0023490435@netserv1.accucom.net>

On 11 Jul 03, at 21:20, John Christie wrote:

> Hi,
> 	I have been trying to work with error terms given back from aov to 
> make confidence intervals.  However, the numbers seem to be incorrect.  
> If there is more than one term in the ANOVA then the error terms can be 
> inflated by the number of factors in the extra terms.  The F's are 
> correct so it is right back to the SS.  I was wondering if this is 
> standard practice for stats programs or unique to R?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

In what sense are the SSs "incorrect", exactly?  And what do you think the "correct" values should be?

---JRG


John R. Gleason
Associate Professor

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers



From spencer.graves at pdf.com  Sat Jul 12 02:52:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 17:52:26 -0700
Subject: [R] More clear statement about the question of how to generate
	regression matrix with correlation matrix
References: <006301c3480c$6269e3a0$bf3d9f88@pcrui>
Message-ID: <3F0F5BCA.4030001@pdf.com>

Dear Rui:

	  If noone else responds, I suggest you forward my earlier comments to 
Fu and ask him.

Spencer Graves

rui wrote:
> Dear R community:
> 
> I am trying to do a simulation study mentioned by Fu (1998), 
Journal of Computational and Graphical Statistics, Volume7,
Number 3, Page 397-416. In order to give a clear statement of
quesion I copy the following paragraph from the article: We
compare the bridge model with the OLS, the lasso and the ridge
in a simulation of a linear regression model of 30 observations
and 10 regressors Y = beta0 + beta1*x1 + ... + beta10*x10 +
epsilon, where epsilon follows a normal distribution with mean
mu and standard deviation sigma. Ten regression matrices {X}m,
m=1,...,10, are generated from an orthonormal matrix X of
dimension 30*10 with different between-column pairwise
correlation coefficients {rho}m generated from uniform
distribution U(-1, 1).
> 
> Thanks in advance.
> 
> Rui
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Sat Jul 12 02:54:01 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 11 Jul 2003 20:54:01 -0400
Subject: [R] Nonliner Rgression using Neural Nnetworks
In-Reply-To: <20030711185443.0379.FWHD3550@mb.infoweb.ne.jp>
Message-ID: <3F0F23E9.29425.5E4A7D@localhost>

On 11 Jul 2003 at 18:56, Yukihiro Ishii wrote:

> Hi, 
> I am an old hand at chemistry but a complete beginner at statistics
>  including R computations.
> My question is whether you can carry out nonlinear
> multivariate regression  analysis in  R using neural networks, where the
> output variable can range from -Inf to  + Inf., unlike discriminant 
> analysis where the output is confined to one  or zero. The library nnet
> seems to work only in the latter case but then I could  be wrong. 

You are wrong. nnet can be used to predict a continous variable, for 
instance  by setting the arguments linout=TRUE. 

For ways to set different types of networks, see 
?nnet

ans especially the arguments
linout
entropy
softmax
censored

Kjetil Halvorsen

> 
> Please help me there.
> 
> Thanks in advance.
> 
> Y.Ishii <yukiasais at ybb.ne.jp>
> 2-3-28 $B!! (BTsurumaki-minami, Hadano
> 257-0002 Japan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jc at or.psychology.dal.ca  Sat Jul 12 02:57:36 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 11 Jul 2003 21:57:36 -0300
Subject: [R] ss's are incorrect from aov with multiple factors
In-Reply-To: <B0023490435@netserv1.accucom.net>
Message-ID: <D423413E-B403-11D7-917C-000A9566473A@or.psychology.dal.ca>


On Friday, July 11, 2003, at 09:38  PM, JRG wrote:

> On 11 Jul 03, at 21:20, John Christie wrote:
>
> In what sense are the SSs "incorrect", exactly?  And what do you think 
> the "correct" values should be?

Well, if I take the residuals for one of the main effects I should be 
able to calculate a confidence interval from it that has some 
relationship to the actual values for that effect so that an accurate 
plot can be made.  The transformation isn't too hard, I just need to 
divide them by the number of factors in the other term.  But, my 
recollection is that this isn't true for other stats packages and may 
lead people who are new and trying to calculate them for the first time 
to incorrect conclusions.  The reported values should be corrected.  
Although, my recollection could be wrong.



From spencer.graves at pdf.com  Sat Jul 12 03:00:52 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jul 2003 18:00:52 -0700
Subject: [R] ss's are incorrect from aov with multiple factors
References: <B0023490435@netserv1.accucom.net>
Message-ID: <3F0F5DC4.8000600@pdf.com>

Dear John Christie:

	  People tend to get the quickest and most helpful responses when they 
provide a toy problem that produces what they think are anamolous 
results.  This increases the chances that someone will be able to 
provide a sensible answer in the few seconds they have available for 
your question.  Often, the process of preparing a toy problem leads them 
to an answer to their question.

	  Sorry I couldn't be more helpful.  spencer graves

JRG wrote:
> On 11 Jul 03, at 21:20, John Christie wrote:
> 
> 
>>Hi,
>>	I have been trying to work with error terms given back from aov to 
>>make confidence intervals.  However, the numbers seem to be incorrect.  
>>If there is more than one term in the ANOVA then the error terms can be 
>>inflated by the number of factors in the extra terms.  The F's are 
>>correct so it is right back to the SS.  I was wondering if this is 
>>standard practice for stats programs or unique to R?
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> In what sense are the SSs "incorrect", exactly?  And what do you think the "correct" values should be?
> 
> ---JRG
> 
> 
> John R. Gleason
> Associate Professor
> 
> Syracuse University
> 430 Huntington Hall                      Voice:   315-443-3107
> Syracuse, NY 13244-2340  USA             FAX:     315-443-4085
> 
> PGP public key at keyservers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From liping66 at hotmail.com  Sat Jul 12 03:33:22 2003
From: liping66 at hotmail.com (liping)
Date: Fri, 11 Jul 2003 21:33:22 -0400
Subject: [R] help with bivariate density plot question
Message-ID: <BAY1-DAV12jIL2jpWkf00000d44@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030711/c5cdcd17/attachment.pl

From jc at or.psychology.dal.ca  Sat Jul 12 04:33:14 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 11 Jul 2003 23:33:14 -0300
Subject: [R] ss's are incorrect from aov with multiple factors (EXAMPLE!)
In-Reply-To: <3F0F5DC4.8000600@pdf.com>
Message-ID: <3042A3D1-B411-11D7-917C-000A9566473A@or.psychology.dal.ca>

OK, I do see that there is a problem in my first email.  I have noticed 
this with repeated measures designs.  Otherwise, of course, there is 
only one error term for all factors.  But, with repeated measures 
designs this is not the case.


On Friday, July 11, 2003, at 10:00  PM, Spencer Graves wrote:

> 	  People tend to get the quickest and most helpful responses when 
> they provide a toy problem that produces what they think are anamolous 
> results

here is an admittedly poor example with factors a and b and s subjects.

a<-factor(rep(c(0,1),12))
b<-factor(rep(c(0,0,1,1),6))
s<- factor(rep(1:6,each=4))
  x <- c(49.5, 62.8, 46.8, 57, 59.8, 58.5, 55.5, 56, 62.8, 55.8, 69.5, 
55, 62, 48.8, 45.5, 44.2, 52, 51.5, 49.8, 48.8, 57.2, 59, 53.2, 56)

now

summary(aov(x~a*b+Error(s/(a*b))))

gives a table of results
but, if one wanted to generate a confidence interval for factor b one 
needs to reanalyze the results thusly

ss<-aggregate(x, list(s=s, b=b), mean)
summary(aov(x~b+Error(s/b), data=ss))

This yields an error term half the size as that reported for b in the 
combined ANOVA.  I would suggest that the way the ss and MSE are 
reported is erroneous since they should be able to be used to directly 
calculate confidence intervals or make mean comparisons without having 
to collapse and reanalyze for every effect.

Furthermore, I am guessing that this problem makes it impossible to get 
a correct average MSE that includes the interaction term.  OK, far from 
impossible, but very difficult to verify that the term is correct.

NOTE  F for b is the same in the first ANOVA and the second.



From johns2326 at hotmail.com  Sat Jul 12 09:55:38 2003
From: johns2326 at hotmail.com (John Santander)
Date: Sat, 12 Jul 2003 00:55:38 -0700
Subject: [R] Problem with library "car"
Message-ID: <Law9-F87ZguRmewP9TQ00005b65@hotmail.com>

I am using the Unix version of R (version 1.7.0), installed via fink on a G4 
Macintosh. I recently upgraded from version 1.6.0 and found that the "car" 
library now has a problem:

---Begin transcript---

>library(car)

Attaching package 'car':


        The following object(s) are masked from package:base :

         dfbeta dfbeta.lm dfbetas dfbetas.lm hatvalues hatvalues.lm 
influence influence.glm influence.lm rstudent rstudent.glm rstudent.lm

>x <- 1:10
>y <- 2*x+3+2*rnorm(10)
>model<-lm(y~x)
>qq.plot(model)
Bus error

---End transcript---

No plot appears. After the bus error, R no longer operates normally, and I 
have to kill the terminal window in order to quit.

i just upgraded from R version 1.6, under which "car" had no problems.

Has anyone else seen this problem? Any ideas?

John



From tpapp at axelero.hu  Sat Jul 12 11:53:26 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 12 Jul 2003 11:53:26 +0200
Subject: [R] using cut on matrices
Message-ID: <20030712095326.GA720@localhost>

Dear list,

I'd like to use the function cut() on matrices, ie that when I apply
it to a matrix, it would return a matrix of the same dimensions
instead of a vector.

I wonder if there is a better (more elegant) solution than

matrix(cut(a, ...), ncol=ncol(a), nrow=nrow(a))

because I would like to use cut on both vectors and matrices and avoid
testing whether a is a matrix.

Thanks,

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From p.dalgaard at biostat.ku.dk  Sat Jul 12 12:37:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 12 Jul 2003 10:37:20 -0000
Subject: [R] ss's are incorrect from aov with multiple factors (EXAMPLE!)
In-Reply-To: <3042A3D1-B411-11D7-917C-000A9566473A@or.psychology.dal.ca>
References: <3042A3D1-B411-11D7-917C-000A9566473A@or.psychology.dal.ca>
Message-ID: <x2d6ggav0f.fsf@biostat.ku.dk>

John Christie <jc at or.psychology.dal.ca> writes:

> OK, I do see that there is a problem in my first email.  I have
> noticed this with repeated measures designs.  Otherwise, of course,
> there is only one error term for all factors.  But, with repeated
> measures designs this is not the case.
> 
> 
> On Friday, July 11, 2003, at 10:00  PM, Spencer Graves wrote:
> 
> > 	  People tend to get the quickest and most helpful responses
> > when they provide a toy problem that produces what they think are
> > anamolous results
> 
> here is an admittedly poor example with factors a and b and s subjects.
> 
> a<-factor(rep(c(0,1),12))
> b<-factor(rep(c(0,0,1,1),6))
> s<- factor(rep(1:6,each=4))
>   x <- c(49.5, 62.8, 46.8, 57, 59.8, 58.5, 55.5, 56, 62.8, 55.8, 69.5,
> 55, 62, 48.8, 45.5, 44.2, 52, 51.5, 49.8, 48.8, 57.2, 59, 53.2, 56)
> 
> now
> 
> summary(aov(x~a*b+Error(s/(a*b))))
> 
> gives a table of results
> but, if one wanted to generate a confidence interval for factor b one
> needs to reanalyze the results thusly
> 
> ss<-aggregate(x, list(s=s, b=b), mean)
> summary(aov(x~b+Error(s/b), data=ss))
> 
> This yields an error term half the size as that reported for b in the
> combined ANOVA.  I would suggest that the way the ss and MSE are
> reported is erroneous since they should be able to be used to directly
> calculate confidence intervals or make mean comparisons without having
> to collapse and reanalyze for every effect.
> 
> Furthermore, I am guessing that this problem makes it impossible to
> get a correct average MSE that includes the interaction term.  OK, far
> from impossible, but very difficult to verify that the term is correct.
> 
> NOTE  F for b is the same in the first ANOVA and the second.

As far as I can tell, yes, you get different results if you analyse
the original data than if you collapse by taking means over the a
factor, and no, you should not expect otherwise. The various SS in the
full analysis are distance measures in 24-dim space, whereas in the
aggregated analysis you get a distance in 12-space. The relation is
that every value entering in the b and s:b terms will be duplicated in
the former, hence the SS is twice as big. 

This is standard procedure, and R does the same as e.g. Genstat in
this respect. It is also necessary to ensure that the residual MS are
comparable, e.g. that you can test for a significant s:b random effect
by comparing with the residual MS to that of the s:a:b stratum.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sat Jul 12 12:39:48 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 12 Jul 2003 10:39:48 -0000
Subject: [R] using cut on matrices
In-Reply-To: <20030712095326.GA720@localhost>
References: <20030712095326.GA720@localhost>
Message-ID: <x28yr4auw1.fsf@biostat.ku.dk>

Tamas Papp <tpapp at axelero.hu> writes:

> Dear list,
> 
> I'd like to use the function cut() on matrices, ie that when I apply
> it to a matrix, it would return a matrix of the same dimensions
> instead of a vector.
> 
> I wonder if there is a better (more elegant) solution than
> 
> matrix(cut(a, ...), ncol=ncol(a), nrow=nrow(a))
> 
> because I would like to use cut on both vectors and matrices and avoid
> testing whether a is a matrix.

Will this not work?:

ac <- cut(a)
dim(ac) <- dim(a)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tpapp at axelero.hu  Sat Jul 12 13:20:51 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 12 Jul 2003 13:20:51 +0200
Subject: [R] 3d plot with different levels done in different colors
	(solution)
In-Reply-To: <200307112056.NAA25117@hivnet.ubc.ca>
References: <20030711152602.GB1656@localhost>
	<200307112056.NAA25117@hivnet.ubc.ca>
Message-ID: <20030712112051.GA1043@localhost>

On Fri, Jul 11, 2003 at 01:49:37PM -0700, 

I finally managed to do what I originally wanted. I would like to
thank the help I received from Jerome Asselin and Peter Dalgaard. Here
it is:

levelpersp <- function(x, y, z, colors=topo.colors, ...) {
  ## getting the value of the midpoint
  zz <- (z[-1,-1] + z[-1,-ncol(z)] + z[-nrow(z),-1] + z[-nrow(z),-ncol(z)])/4
  ## calculating the breaks
  breaks <- hist(zz, plot=FALSE)$breaks
  ## cutting up zz
  cols <- colors(length(breaks)-1)
  zzz <- cut(zz, breaks=breaks, labels=cols)
  ## plotting
  persp(x, y, z, col=as.character(zzz), ...)
  ## return breaks and colors for the legend
  list(breaks=breaks, colors=cols)
}


## Example
x <- seq(-10, 10, length=60)
y <- x
f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
levelpersp(x, y, z, theta = 30, phi = 30, expand = 0.5)

Regards,

Tamas

> 
> Hi,
> 
> Consider this example which I have modified from the persp() help file.
> It uses topo.colors() to create a series of colors.
> 
> Cheers,
> Jerome
> 
>      x <- seq(-10, 10, length= 30)
>      y <- x
>      f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
>      z <- outer(x, y, f)
>      z[is.na(z)] <- 1
>      op <- par(bg = "white")
>      zz <- (z[-1,-1] + z[-1,-ncol(z)] + z[-nrow(z),-1] + 
>               z[-nrow(z),-ncol(z)])/4
>      cols <- topo.colors(length(zz))
>      cols[order(zz)] <- cols
>      persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = cols)
> 
> 
> On July 11, 2003 08:26 am, Tamas Papp wrote:
> > Content-Length: 1223
> > Status: R
> > X-Status: N
> >
> > I would like a 3d plot of a matrix such that individual trapezoids
> > that make up the surface are colored according to the z-value of that
> > point (or preferably the midpoint of its four corners, or something
> > similar). MS Excel has something like that.
> >
> > I know that persp can have an nx by ny matrix its "col" argument, I
> > just don't know how to generate that matrix. To calculate the midpoint
> > of the tiles for the matrix z, I could use something like
> >
> > zz <- (z[-1,-1] + z[-1,-nrow(z)] + z[-ncol(z),-1] +
> > z[-ncol(z),-nrow(z)])/4
> >
> > but I don't know how to assign a color to that value. For example if I
> > have
> >
> > boundaries <- c(0, .5, 1, 1.5)
> >
> > and
> >
> > colors <- c("red", "green", "blue")
> >
> > I am looking for a function that assigns red to the elements of zz
> > between 0 and .5, etc. Alternative solutions are welcome, maybe
> > somebody has already wrote a library that does the whole thing neatly.
> >
> > Thanks,
> >
> > Tamas
> 

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ligges at statistik.uni-dortmund.de  Sat Jul 12 14:41:22 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Jul 2003 14:41:22 +0200
Subject: [R] Problem with library "car"
In-Reply-To: <Law9-F87ZguRmewP9TQ00005b65@hotmail.com>
References: <Law9-F87ZguRmewP9TQ00005b65@hotmail.com>
Message-ID: <3F1001F2.4000505@statistik.uni-dortmund.de>

John Santander wrote:
> I am using the Unix version of R (version 1.7.0), installed via fink on 
> a G4 Macintosh. I recently upgraded from version 1.6.0 and found that 
> the "car" library now has a problem:
> 
> ---Begin transcript---
> 
>> library(car)
> 
> 
> Attaching package 'car':
> 
> 
>        The following object(s) are masked from package:base :
> 
>         dfbeta dfbeta.lm dfbetas dfbetas.lm hatvalues hatvalues.lm 
> influence influence.glm influence.lm rstudent rstudent.glm rstudent.lm
> 
>> x <- 1:10
>> y <- 2*x+3+2*rnorm(10)
>> model<-lm(y~x)
>> qq.plot(model)
> 
> Bus error
> 
> ---End transcript---
> 
> No plot appears. After the bus error, R no longer operates normally, and 
> I have to kill the terminal window in order to quit.
> 
> i just upgraded from R version 1.6, under which "car" had no problems.
> 
> Has anyone else seen this problem? Any ideas?


Two ideas:

a) Reinstall a recent version of "car".
b) Upgrading to R-1.7.1, which is recent, is always a good idea.

Uwe Ligges

> John



From spencer.graves at pdf.com  Sat Jul 12 17:12:01 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Jul 2003 08:12:01 -0700
Subject: [R] using cut on matrices
References: <20030712095326.GA720@localhost> <x28yr4auw1.fsf@biostat.ku.dk>
Message-ID: <3F102541.9010401@pdf.com>

Peter's solution led me to an apparent bug in R.

 > a <- 1:9
 > cut(a)
Error in cut.default(a) : Argument "breaks" is missing, with no default

######################
That didn't work, so I read the documentation, found that a second 
argument was required.  Result:
 > cut(a, 2)
[1] (0.992,5] (0.992,5] (0.992,5] (0.992,5] (5,9.01]  (5,9.01]  (5,9.01]
[8] (5,9.01]  (5,9.01]
Levels: (0.992,5] (5,9.01]

##### LOOK VERY CAREFULLY:
##### R 1.6.2 coded 5 as (5, 9.01].
##### (I know I need to upgrade to R 1.7.1.)
##### S-Plus 6.1 for Windows 2000 produced the following:
 > cut(a, 2)
[1] 1 1 1 1 1 2 2 2 2
attr(, "levels"):
[1] "0.92+ thru 5.00" "5.00+ thru 9.08"

##### NOTE:  S-Plus 6.1 did it correctly.
##### I'm sorry to report an error without a fix,
##### but I'm out of time for this now.

##### Before I found the bug, I wrote a function
##### to retain dimnames:

Cut <-
  function(a, ...){
   ac <- cut(a, ...)
   if(is.array(a)){
    dim(ac) <- dim(a)
    dimnames(ac) <- dimnames(a)
   }
   else names(ac) <- names(a)
   ac
}

 > Cut(a, 2)
[1] (0.992,5] (0.992,5] (0.992,5] (0.992,5] (5,9.01]  (5,9.01]  (5,9.01]
[8] (5,9.01]  (5,9.01]
Levels: (0.992,5] (5,9.01]

##### That worked fine.  What about a vector with names?
 > a1 <- a
 > names(a1) <- letters[1:9]
 > Cut(a1, 2)
[1] (0.992,5] (0.992,5] (0.992,5] (0.992,5] (5,9.01]  (5,9.01]  (5,9.01]
[8] (5,9.01]  (5,9.01]
Levels: (0.992,5] (5,9.01]

##### What happened to the names?
 > names(Cut(a1,2))
[1] "a" "b" "c" "d" "e" "f" "g" "h" "i"

##### The names were there, but R chose not to display them.
##### S-Plus 6.1 under Win2000 produced the following:
 > Cut(a1, 2)
  a b c d e f g h i
  1 1 1 1 1 2 2 2 2
attr(, "levels"):
[1] "0.92+ thru 5.00" "5.00+ thru 9.08"

##### The names appear with codes and a translate table
##### Something similar happens with an array:
##### In R 1.6.2:
 > a2 <- a
 > dim(a2) <- c(3,3)
 > dimnames(a2) <- list(LETTERS[1:3], c("ab","bc","cd"))
 > Cut(a2, 2)
[1] (0.992,5] (0.992,5] (0.992,5] (0.992,5] (5,9.01]  (5,9.01]  (5,9.01]
[8] (5,9.01]  (5,9.01]
Levels: (0.992,5] (5,9.01]
 > dimnames(Cut(a2,2))
[[1]]
[1] "A" "B" "C"

[[2]]
[1] "ab" "bc" "cd"

##### S-Plus produced the following:
 > Cut(a2, 2)
   ab bc cd
A  1  1  2
B  1  1  2
C  1  2  2
attr(, "levels"):
[1] "0.92+ thru 5.00" "5.00+ thru 9.08"

#####
hope this helps.
spencer graves

Peter Dalgaard BSA wrote:
> Tamas Papp <tpapp at axelero.hu> writes:
> 
> 
>>Dear list,
>>
>>I'd like to use the function cut() on matrices, ie that when I apply
>>it to a matrix, it would return a matrix of the same dimensions
>>instead of a vector.
>>
>>I wonder if there is a better (more elegant) solution than
>>
>>matrix(cut(a, ...), ncol=ncol(a), nrow=nrow(a))
>>
>>because I would like to use cut on both vectors and matrices and avoid
>>testing whether a is a matrix.
> 
> 
> Will this not work?:
> 
> ac <- cut(a)
> dim(ac) <- dim(a)
>



From spencer.graves at pdf.com  Sat Jul 12 17:17:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Jul 2003 08:17:48 -0700
Subject: [R] help with bivariate density plot question
References: <BAY1-DAV12jIL2jpWkf00000d44@hotmail.com>
Message-ID: <3F10269C.1010008@pdf.com>

Did you look at "?contour"?  The "contour" command has a "levels" 
argument.  To translate op into percentage, you could produce an 
empirical CDF of op$zden.

hope this helps.  spencer graves

liping wrote:
> Dear R users:
> 
> I have a dataset with two variables (>20000 observations, two samples from same subject) and I used "kernSur" from library(Genkern) to 
> get a estimated bivariate density and corresponding plots as follows:
> 
> new.data.normal<-data.normal[!is.na(data.normal[,2]),]
> x<-new.data.normal[,2]
> y<-new.data.normal[,3]
> 
> op <- KernSur(x,y, xgridsize=50, ygridsize=50, correlation=0.4968023, 
>               xbandwidth=1, ybandwidth=1)
> 
> #3D density plot
> persp(op$xvals, op$yvals, op$zden,
>         theta=30,phi=10,expand=0.5,ltheta=120,
>         xlab="TECH3661.A",ylab="TECH3661.B",zlab="Prob",col="pink",
>       , main="3D DENSITY PLOT-TECH3661 ", sub=" TECH3661.A AND TECH3661.B",
>         box = T, axes = TRUE,ticktype = "detailed", )
> 
> #countour plot
> image(op$xvals, op$yvals, op$zden, col=terrain.colors(100), axes=TRUE,xlab="TECH3661.A",ylab="TECH3661.B")
> points(x,y,pch="*")
>     
> Now after above step, how can I use 'contour' or other commands to draw ellipse curves over above plots indicating "including about 68% data", "including about 84% data", etc. similar to the (-std,std), (-2*std,2*std),(-3*std, 3*std) intervals for univariate variable.
> 
> any suggestin will be appreciated.
> 
> liping
> 
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Tor.Strand at cih.uib.no  Sat Jul 12 18:50:21 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Sat, 12 Jul 2003 18:50:21 +0200
Subject: [R] question regarding GAM from a novice (in GAM as well as in R)
Message-ID: <BB3608ED.7202%Tor.Strand@cih.uib.no>

Need to use generalized additive models and have therefore obtained R

I am able to do the analysis but I have problems understanding the syntax
and the options

Can someone explain what some of the terms do in this model do:?

c<-gam(depvar~var1+var2+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
data=xdataset ) 

I do not use the terms including var4- var8 in my model, just want to know
what they do. 

+s(var4, by=var5)
+s(var6, var7)
+s(var8,3)

Furthermore, the results become rather different when I change the model to:

c<-gam(depvar~var1+var2-1+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
data=xdataset ) 
# note just adding a -1

Why.

Please forgive my ignorance.


Dr. Tor A Strand   
Centre for International Health
Haukeland Hospital
University of Bergen
5021 Bergen    
Norway     
Phone: (country prefix 47)
Residence:56 51 10 88, office: 55 97 49 80,
fax: 55 97 49 79, cellular:  90 97 10 86



From Mark.Lamias at grizzard.com  Sat Jul 12 18:51:04 2003
From: Mark.Lamias at grizzard.com (Mark Lamias)
Date: Sat, 12 Jul 2003 12:51:04 -0400
Subject: [R] Row Reduction
Message-ID: <BF3C804D59EE4C40BF5376A0FABA4AA62F6287@atl_mail.griz-main.com>

Hi,

I haven't worked with R is quite a while, so I apologize if this seems a
little basic.

I've looked in the documentation, but cannot find how to convert a matrix
into row reduced echelon form.  Does an R function exist that will do this?

Thanks.

Sincerely yours,

Mark J. Lamias



From yanyu at cs.ucla.edu  Sat Jul 12 20:19:17 2003
From: yanyu at cs.ucla.edu (Yan Yu)
Date: Sat, 12 Jul 2003 11:19:17 -0700 (PDT)
Subject: [R] a Q re. Kriging package 
Message-ID: <Pine.SOL.4.33.0307121113530.12233-100000@panther.cs.ucla.edu>

Hello,
   Based on a couple of textbooks on geostatistics I read, Kriging is an
exact interpolator, which means it reconstructs sample data values at
known locations.
I wonder which Kriging package in R is an exact interpolator?
does anyone has experience with that?

THANKS A LOT in advance,
have a nice weekend,
yan



From csillery at selway.umt.edu  Sat Jul 12 23:51:10 2003
From: csillery at selway.umt.edu (Katalin Csillery)
Date: Sat, 12 Jul 2003 15:51:10 -0600 (MDT)
Subject: [R] poisson regression nested effects
Message-ID: <Pine.OSF.4.21.0307121527390.14474-100000@selway.umt.edu>


Dear All,

I have two treatment levels and multinomial behaviors as response
and also a covariate (size). The individuals were split to the two
treatment levels on the first day and switched on the second day. 
I am interested not only the treatmen effect but the effect of the day,
such that the individuals experinced treatment 1 first, or treatment 2
first.

I conditioned on the most frequent behavior and than run separate poisson
models on the rest of the behaviors (proportions of time when they did a 
behavior given that they do not do the most frequent).

glm(behav1~treatment*day+size, family=poisson(), data=data)

I wonder that it is not correct because the day effect is nested in the
treatment effect but I do not know how set it up in glm().

Also do I need to do any corrections because of the separate tests on
each beahvior? It is more a statistical question.

Thanks for any help in advance!

Katalin



From kjetil at entelnet.bo  Sun Jul 13 00:22:00 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Sat, 12 Jul 2003 18:22:00 -0400
Subject: [R] a Q re. Kriging package 
In-Reply-To: <Pine.SOL.4.33.0307121113530.12233-100000@panther.cs.ucla.edu>
Message-ID: <3F1051C8.3350.1F02EE3@localhost>

On 12 Jul 2003 at 11:19, Yan Yu wrote:

Kriging is an exact interpolator PROVIDED you have a variogram model 
with no nugget effect. If there is an nugget effect, kriging is not 
an exact interpolator. If any package implementing kriging in R 
do not give results according to this (modulo rounding error), it is 
in error. I have never seen problems with this, but have not tried 
all kriging implementations in R!

Kjetil Halvorsen

> Hello,
>    Based on a couple of textbooks on geostatistics I read, Kriging is an
> exact interpolator, which means it reconstructs sample data values at
> known locations.
> I wonder which Kriging package in R is an exact interpolator?
> does anyone has experience with that?
> 
> THANKS A LOT in advance,
> have a nice weekend,
> yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Sun Jul 13 00:22:01 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Sat, 12 Jul 2003 18:22:01 -0400
Subject: [R] question regarding GAM from a novice (in GAM as well as in R)
In-Reply-To: <BB3608ED.7202%Tor.Strand@cih.uib.no>
Message-ID: <3F1051C9.28293.1F0325E@localhost>

On 12 Jul 2003 at 18:50, Tor A  Strand wrote:

Hei!

> Need to use generalized additive models and have therefore obtained R
> 
> I am able to do the analysis but I have problems understanding the syntax
> and the options
> 
> Can someone explain what some of the terms do in this model do:?
> 
> c<-gam(depvar~var1+var2+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
> data=xdataset ) 
> 
> I do not use the terms including var4- var8 in my model, just want to know
> what they do. 
> 
> +s(var4, by=var5)

Presumably var5 is a factor, separate smooths are calculated for each 
level of the factor. This is a kind of interaction.

> +s(var6, var7)

This looks like a bivariate smooth, that is , a surface. Never tried 
that.

> +s(var8,3)

second argument is the number of degrees of freedom.

> 
> Furthermore, the results become rather different when I change the model to:
> 
> c<-gam(depvar~var1+var2-1+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
> data=xdataset ) 
> # note just adding a -1
> 
> Why. 

This is removing the intercept from the model, and as all smooths are 
centered at zero in some way, that should have more or less the same 
effect as removing the intercept from a linear model. That can make a 
HUGE difference.

> 
> Please forgive my ignorance.

You probably have a local expert at Haukeland: Try to call
Tore Wenzel-Larsen if you need more help.

Kjetil Halvorsen

> 
> 
> Dr. Tor A Strand   
> Centre for International Health
> Haukeland Hospital
> University of Bergen
> 5021 Bergen    
> Norway     
> Phone: (country prefix 47)
> Residence:56 51 10 88, office: 55 97 49 80,
> fax: 55 97 49 79, cellular:  90 97 10 86
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jc at or.psychology.dal.ca  Sun Jul 13 00:47:32 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Sat, 12 Jul 2003 19:47:32 -0300
Subject: [R] ss's are incorrect from aov with multiple factors (EXAMPLE!)
In-Reply-To: <x2d6ggav0f.fsf@biostat.ku.dk>
Message-ID: <D2A001B2-B4BA-11D7-917C-000A9566473A@or.psychology.dal.ca>


On Saturday, July 12, 2003, at 07:40  AM, Peter Dalgaard BSA wrote:

> factor, and no, you should not expect otherwise. The various SS in the
> full analysis are distance measures in 24-dim space, whereas in the
> aggregated analysis you get a distance in 12-space. The relation is
> that every value entering in the b and s:b terms will be duplicated in
> the former, hence the SS is twice as big.
>
> This is standard procedure, and R does the same as e.g. Genstat in
> this respect. It is also necessary to ensure that the residual MS are
> comparable, e.g. that you can test for a significant s:b random effect
> by comparing with the residual MS to that of the s:a:b stratum.

OK, perhaps I need a little help then.  Suppose I do an interaction 
plot of a*b and I want to see what it looks like with 95%CI error bars. 
  Following Loftus & Masson (1995) there would be one of two ways. I 
could generate an error bar for the main effect I was interested in and 
stress in the description that the error bars only apply across that 
main effect.  I take it from what you have said that I would collapse 
the data in order to generate a proper error bar for only one effect.  
Or, I could generate one from a weighted average of the MSE from a, b, 
and a:b.  The question I have is, would I get each of the main effects 
in that from separate analyses?

BTW, Statview seems to generate the same MSE for me whether I collapse 
the data or not.



From spencer.graves at pdf.com  Sun Jul 13 01:16:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Jul 2003 16:16:57 -0700
Subject: [R] help with bivariate density plot question
References: <BAY1-DAV12jIL2jpWkf00000d44@hotmail.com>
Message-ID: <3F1096E9.2060907@pdf.com>

I've never used KernSur, but the documentation says it, "returns two 
vectors and a matrix:

xvals vector of ordinates at which the density
	has been estimated in the x dimension
yvals vector of ordinates at which the density
	has been estimated in the y dimension
zden matrix of density for f(x,y) with
	dimensions xgridsize, ygridsize"

	  If we can ignore the probability outside the plot is negligible, then 
the following should produce what you want:

zd <- rev(sort(op$zden))
Surv.zd <- cumsum(zd)
Surv.zd <- (Surv.zd/Surv.zd[lenght(zd)])

plot(zd, Surv.zd)

Now pick the probability levels you want from Surv.zd and translate them 
into levels for zd and use those numbers for the "levels" argument in a 
call to

	 countour(op$xvals, op$yvals, op$zden, levels = ... )

hope this helps.
spencer graves

liping wrote:
 > Dear Spencer Graves:
 >
 > thank you for your help and actually I did something but I
do not know if what I did is fine. I know op$zden is the
estimated density so I  first sort op$zden( I doubt this
step but if i did not sort op$zden, the following step will
go wrong) and then I will sum op$zden to reach the limit i
set up(.68,0.84,.97... ) and output the last value from
op$zden, take it as the contour curve level.  thanks again
for your response.
 >
 > regards,
 >
 > liping
 >
 >
 >
 > xval<-op$xvals
 > yval<-op$yvals
 > z.den<-sort(op$zden)
 >
 > z.distr<-0
 > prob<-c(0.68, 0.84, 0.97, 0.99)
 >
 > for ( j in 1:4) {
 >       for (i in 1:length(op$zden)){
 >           z.distr<-z.den[i]+z.distr
 >           if (z.distr>prob[j]) {
 > 
c<-list(iteration=i,z.distr=z.distr,zden=z.den[i])
 >                         print(c)
 >                         break
 >
 >                              }
 >                  }
 > label<-c("within 68%", "within 84%", "within 97%", "within 99%")
 > contour(op$xvals, op$yvals, op$zden,
 > levels=z.den[i],col=j+5,labels=label[j],add=TRUE)
 > }
 >
 > legend(2,6,legend=c("WITHIN 68%", "WITHIN 84%","WITHIN 97%", "WITHIN 
99%"),
 >                lty=c(1,2,3,4),col=c(6,7,8,9))
########################################################################
Did you look at "?contour"?  The "contour" command has a "levels"
argument.  To translate op into percentage, you could produce an
empirical CDF of op$zden.

hope this helps.  spencer graves

liping wrote:
 > Dear R users:
 >
 > I have a dataset with two variables (>20000 observations, two samples 
from same subject) and I used "kernSur" from library(Genkern) to
 > get a estimated bivariate density and corresponding plots as follows:
 >
 > new.data.normal<-data.normal[!is.na(data.normal[,2]),]
 > x<-new.data.normal[,2]
 > y<-new.data.normal[,3]
 >
 > op <- KernSur(x,y, xgridsize=50, ygridsize=50, correlation=0.4968023,
 >               xbandwidth=1, ybandwidth=1)
 >
 > #3D density plot
 > persp(op$xvals, op$yvals, op$zden,
 >         theta=30,phi=10,expand=0.5,ltheta=120,
 >         xlab="TECH3661.A",ylab="TECH3661.B",zlab="Prob",col="pink",
 >       , main="3D DENSITY PLOT-TECH3661 ", sub=" TECH3661.A AND 
TECH3661.B",
 >         box = T, axes = TRUE,ticktype = "detailed", )
 >
 > #countour plot
 > image(op$xvals, op$yvals, op$zden, col=terrain.colors(100), 
axes=TRUE,xlab="TECH3661.A",ylab="TECH3661.B")
 > points(x,y,pch="*")
 >
 > Now after above step, how can I use 'contour' or other commands to 
draw ellipse curves over above plots indicating "including about 68% 
data", "including about 84% data", etc. similar to the (-std,std), 
(-2*std,2*std),(-3*std, 3*std) intervals for univariate variable.
 >
 > any suggestin will be appreciated.
 >
 > liping
 >
 >
 >
 >
 >
 >
 >
 >
 > 	[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Sun Jul 13 02:14:55 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Jul 2003 17:14:55 -0700
Subject: [R] Row Reduction
References: <BF3C804D59EE4C40BF5376A0FABA4AA62F6287@atl_mail.griz-main.com>
Message-ID: <3F10A47F.9020801@pdf.com>

Have you looked at "qr"?  This decomposes a matrix into a product of an 
orthogonal matrix, Q, and an upper triangular matrix, R.

hope this helps.  spencer graves

Mark Lamias wrote:
> Hi,
> 
> I haven't worked with R is quite a while, so I apologize if this seems a
> little basic.
> 
> I've looked in the documentation, but cannot find how to convert a matrix
> into row reduced echelon form.  Does an R function exist that will do this?
> 
> Thanks.
> 
> Sincerely yours,
> 
> Mark J. Lamias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rwang at math.ucalgary.ca  Sun Jul 13 03:42:13 2003
From: rwang at math.ucalgary.ca (rui)
Date: Sat, 12 Jul 2003 19:42:13 -0600
Subject: [R] How to generate a matrix with a specific correlation (matrix)
Message-ID: <001001c348df$fbeee470$bf3d9f88@pcrui>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030712/afac224f/attachment.pl

From spencer.graves at pdf.com  Sun Jul 13 04:39:41 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 12 Jul 2003 19:39:41 -0700
Subject: [R] How to generate a matrix with a specific correlation (matrix)
References: <001001c348df$fbeee470$bf3d9f88@pcrui>
Message-ID: <3F10C66D.4090001@pdf.com>

	  If var(X) = diag(n) = n x n identity matrix, then var(A%*%X) = A %*% 
t(A).

	  If you can generate vector(s) X of uncorrelated observations with 
whatever distribution you want, then let A = t(chol(Correl)), where 
Correl = the desired correlation matrix.

	  If you want multivariate normal or t distributions, the package 
mvtnorm downloadable from CRAN will do this for you.

	  If this does not solve your problem, I suggest you provide a toy 
example that explains very succinctly what you want, what you've tried, 
and the deficiencies in what you've tried.

hope this helps.  spencer graves

rui wrote:
> Dear R community:
> 
> How to generate a matrix with a specific correlation (matrix)?
> Thanks in advance.
> 
> Rui
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hp3000al at yahoo.com  Sun Jul 13 07:09:59 2003
From: hp3000al at yahoo.com (dg gdf)
Date: Sat, 12 Jul 2003 22:09:59 -0700 (PDT)
Subject: [R] r-question
Message-ID: <20030713050959.29975.qmail@web14914.mail.yahoo.com>

I am student in Iran(IUT) that work on R software as
my project. I need to some data frames in version
1.7.0, but these are not available. please help me.



From feldesmanm at pdx.edu  Sun Jul 13 07:34:59 2003
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Sat, 12 Jul 2003 22:34:59 -0700
Subject: [R] r-question
In-Reply-To: <20030713050959.29975.qmail@web14914.mail.yahoo.com>
Message-ID: <6.0.0.12.2.20030712223402.01b8ddd0@pop4.attglobal.net>

At 10:09 PM 7/12/2003, dg gdf wrote:
 >I am student in Iran(IUT) that work on R software as
 >my project. I need to some data frames in version
 >1.7.0, but these are not available. please help me.
 >
 >______________________________________________

It isn't clear exactly what help you need.  We are most apt to be able to 
help if you can formulate a precise question.  Unfortunately, you haven't 
asked a question that anyone here can answer.



From kwan022 at stat.auckland.ac.nz  Sun Jul 13 07:37:02 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Sun, 13 Jul 2003 17:37:02 +1200 (NZST)
Subject: Data Frame (was: Re: [R] r-question
In-Reply-To: <20030713050959.29975.qmail@web14914.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0307131735330.13965-100000@stat55.stat.auckland.ac.nz>

(Please use a more meaningful subject)

Can you be more specific with your question?  Do you want to access an 
example data set and if so, which one?  Or do you want to create a data 
frame within R?  Or do you want to read in a data set as a data frame from 
an external package?

On Sat, 12 Jul 2003, dg gdf wrote:

> Date: Sat, 12 Jul 2003 22:09:59 -0700 (PDT)
> From: dg gdf <hp3000al at yahoo.com>
> To: R-help at stat.math.ethz.ch
> Subject: [R] r-question
> 
> I am student in Iran(IUT) that work on R software as
> my project. I need to some data frames in version
> 1.7.0, but these are not available. please help me.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Bill.Venables at csiro.au  Sun Jul 13 09:21:09 2003
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Sun, 13 Jul 2003 17:21:09 +1000
Subject: [R] using cut on matrices
Message-ID: <E09E527B56BE2D438A3D6A246DDD27A9165E00@roper-cv.qld.cmis.CSIRO.AU>

One solution to this problem is to use a function like

Cut <- function(x, ...) {
	m <- match.call()
	m[[1]] <- as.name("cut")
	v <- eval(m, sys.frame(sys.parent()))
	if(is.matrix(x)) matrix(v, nrow(x), ncol(x)) else v
}

but it is an odd sort of thing to want to do.  cut(...) in R produces a
factor result.  If you feed this back into a matrix

matrix(cut(v, breaks = pretty(v)), nrow(v), ncol(v))

you get a character matrix.  

If you adopt Peter's solution below

ac <- cut(a, ......)
dim(ac) <- dim(a)

and a is genuine matrix, you get a factor with a dim attribute which is not
of class matrix.  I found this a bit odd, but don't actually class it as a
bug.  It appears to be another unforseen consequence of the new classes and
methods, I suspect.

Bill Venables.


-----Original Message-----
From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk]
Sent: Saturday, July 12, 2003 8:43 PM
To: tpapp at axelero.hu
Cc: R-help mailing list
Subject: Re: [R] using cut on matrices


Tamas Papp <tpapp at axelero.hu> writes:

> Dear list,
> 
> I'd like to use the function cut() on matrices, ie that when I apply
> it to a matrix, it would return a matrix of the same dimensions
> instead of a vector.
> 
> I wonder if there is a better (more elegant) solution than
> 
> matrix(cut(a, ...), ncol=ncol(a), nrow=nrow(a))
> 
> because I would like to use cut on both vectors and matrices and avoid
> testing whether a is a matrix.

Will this not work?:

ac <- cut(a)
dim(ac) <- dim(a)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From silika at access.unizh.ch  Sun Jul 13 14:54:55 2003
From: silika at access.unizh.ch (Silika Tereshchenko)
Date: Sun, 13 Jul 2003 14:54:55 +0200 (CEST)
Subject: [R] Memory size
Message-ID: <1058100895.3f11569fea332@imp.access.unizh.ch>


Daer all,

I have the problem. I could not run the regression, because I have always the
warning message "memory.size". from the help file I learned that it is possible
to increase the memory size, but I did not undestand how could I do it. Could
you please explaine it to me. I would be very grateful for it.


The second question: I obtained from the regression the coefficient "6.003e-3"
and "0.0345e+3". What daos it mean?



Thanks a lot,
Silika



From silika at access.unizh.ch  Sun Jul 13 14:57:49 2003
From: silika at access.unizh.ch (Silika Tereshchenko)
Date: Sun, 13 Jul 2003 14:57:49 +0200 (CEST)
Subject: [R] Memory size
Message-ID: <1058101069.3f11574ddb054@imp.access.unizh.ch>

Daer all,

I have the problem. I could not run the regression, because I have always the
warning message "memory.size". from the help file I learned that it is possible
to increase the memory size, but I did not undestand how could I do it. Could
you please explaine it to me. I would be very grateful for it.



Thanks a lot,
Silika



From ligges at statistik.uni-dortmund.de  Sun Jul 13 15:14:29 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 13 Jul 2003 15:14:29 +0200
Subject: [R] Memory size
In-Reply-To: <1058100895.3f11569fea332@imp.access.unizh.ch>
References: <1058100895.3f11569fea332@imp.access.unizh.ch>
Message-ID: <3F115B35.2080500@statistik.uni-dortmund.de>

Silika Tereshchenko wrote:

> Daer all,
> 
> I have the problem. I could not run the regression, because I have always the
> warning message "memory.size". from the help file I learned that it is possible
> to increase the memory size, but I did not undestand how could I do it. Could
> you please explaine it to me. I would be very grateful for it.

So you have read ?Memory, and hopefully the manuals as well?
Which part of ?Memory is unclear?
Additionally, tell us the Operating System and your version of R as well 
(assuming the recent R-1.7.1).

BTW: Your problem might be too big to be easily solvable with the amount 
of RAM in your machine.


> The second question: I obtained from the regression the coefficient "6.003e-3"
> and "0.0345e+3". What daos it mean?

scientific notation:
6.003e-3  = 6.003  * 10^(-3) =  0.006003
0.0345e+3 = 0.0345 * 10^(3)  = 34.5

Uwe Ligges


> 
> 
> Thanks a lot,
> Silika
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Sun Jul 13 15:46:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 13 Jul 2003 06:46:13 -0700
Subject: Data Frame (was: Re: [R] r-question
References: <Pine.LNX.4.44.0307131735330.13965-100000@stat55.stat.auckland.ac.nz>
Message-ID: <3F1162A5.1070106@pdf.com>

Have you read "?data"?

hope this helps.  spencer graves

Ko-Kang Kevin Wang wrote:
> (Please use a more meaningful subject)
> 
> Can you be more specific with your question?  Do you want to access an 
> example data set and if so, which one?  Or do you want to create a data 
> frame within R?  Or do you want to read in a data set as a data frame from 
> an external package?
> 
> On Sat, 12 Jul 2003, dg gdf wrote:
> 
> 
>>Date: Sat, 12 Jul 2003 22:09:59 -0700 (PDT)
>>From: dg gdf <hp3000al at yahoo.com>
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] r-question
>>
>>I am student in Iran(IUT) that work on R software as
>>my project. I need to some data frames in version
>>1.7.0, but these are not available. please help me.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From peterm at andrew.cmu.edu  Sun Jul 13 16:55:04 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Sun, 13 Jul 2003 10:55:04 -0400
Subject: [R] How robust is mle in R?  
In-Reply-To: <200307131037.h6DA2Jeh007432@stat.math.ethz.ch>
Message-ID: <BB36EB08.590C%peterm@andrew.cmu.edu>

A newbie question:  I'm trying to decide whether to run a maximum likelihood
estimation in R or Stata and am wondering if the R mle routine is reasonably
robust.  I'm fairly certain that, with this data, in Stata I would get a lot
of complaints about non-concave functions and unproductive steps attempted,
but would eventually have a successful ML estimate.  I believe that, with
the 'unproductive step' at least, Stata gets around the problem by switching
to some alternative estimation method in difficult cases.  Does anyone know
how robust mle is in R?

Thanks,
Peter



From lsophir at wisemail.weizmann.ac.il  Sun Jul 13 21:10:18 2003
From: lsophir at wisemail.weizmann.ac.il (Ron Ophir)
Date: Sun, 13 Jul 2003 22:10:18 +0300
Subject: [R] bootstrap for hclust
Message-ID: <sf11d8d8.080@wisemail.weizmann.ac.il>

dear group members,
I am looking for a function that assess the stability of cluster. The result of hclust function is an hclust object which can  be plot as a dendrogram. However to have confidence in the tree topology usualy bootstap is applied. I understand that I can apply bootstarp on the original data and then run hclust(dist() ) as much as I resampled but how to comapre the topologies the I don't know. Is there a bootstarp or permutation for cluster?
thanks,
Ron



From spencer.graves at pdf.com  Sun Jul 13 21:59:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 13 Jul 2003 12:59:31 -0700
Subject: [R] How robust is mle in R?
References: <BB36EB08.590C%peterm@andrew.cmu.edu>
Message-ID: <3F11BA23.5080801@pdf.com>

	  R does not have one "mle routine".  Many statistical procedures do 
maximum likelihood estimation (mle) either by default or as an option. 
The "robustness" would depend on the likelihood and what you want to do 
and what you mean by "robustness.  Read the help files and check 
"www.r-project.org" -> search -> "R site search" for functions you might 
want to use.

	  S (of which R is an implementation) is a object-oriented language for 
statistics.  If you want to do standard analyses, Stata and other 
"statistical packages" may be easier to use.  If you application(s) 
involve a substantial amount of custom scripting, I know of nothing that 
beats R.  Many new statistical procedures are developed first in R and 
only later ported to other languages.  I expect this to be even more 
true in the future than it is today.

hope this helps.  spencer graves

Peter Muhlberger wrote:
> A newbie question:  I'm trying to decide whether to run a maximum likelihood
> estimation in R or Stata and am wondering if the R mle routine is reasonably
> robust.  I'm fairly certain that, with this data, in Stata I would get a lot
> of complaints about non-concave functions and unproductive steps attempted,
> but would eventually have a successful ML estimate.  I believe that, with
> the 'unproductive step' at least, Stata gets around the problem by switching
> to some alternative estimation method in difficult cases.  Does anyone know
> how robust mle is in R?
> 
> Thanks,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From franklin at polisci.wisc.edu  Sun Jul 13 22:30:18 2003
From: franklin at polisci.wisc.edu (Charles H. Franklin)
Date: Sun, 13 Jul 2003 15:30:18 -0500
Subject: [R] How robust is mle in R?  
In-Reply-To: <BB36EB08.590C%peterm@andrew.cmu.edu>
Message-ID: <PDEMINECFJGAIJGDGKGOEENEECAA.franklin@polisci.wisc.edu>

Peter,

  The R "optim" function is what you probably want to read up on for ML in
R. It may or may not be less "complaining" than the Stata ML functions.
Optim provides several alternative algorithms which may help you find one
that is best for your problem. On the other hand, it sounds like you have a
difficult likelihood and/or recalcitrant data.  It seems likely that your
likelihood and data are going to cause more problems than are due to any
peculiarities of the algorithms in Stata OR R. No magic bullets in either,
of course. For what it is worth, I've not managed to "break" the optim
function in R.

Comparatively, I think the Stata ML functions make access to the variables
and specification of the model a little easier and require fewer lines of
code. Stata also then provides standard post estimation commands for the ML
results. R (optim, actually) will return an object with the usual components
that you can also use for any post estimation purposes. But in R you'll
probably write a few more lines of code to specify the model and manipulate
the returned results. You probably need to learn a bit more to program this
effectively in R than you need to learn to do the same thing in Stata. Also,
Stata's "ml check" provides a nice test of your code before you loose it on
the data!

Gauss's "maxlik" routines would be another possibility, if you have or are
able to acquire Gauss.

Charles


/******************************************
** Charles H. Franklin
** Professor, Political Science
** University of Wisconsin, Madison
** 1050 Bascom Mall
** Madison, WI 53706
** 608-263-2022 Office
** 608-265-2663 Fax
** mailto:franklin at polisci.wisc.edu (best)
** mailto:chfrankl at facstaff.wisc.edu (alt)
** http://www.polisci.wisc.edu/~franklin
******************************************/

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peter Muhlberger
Sent: Sunday, July 13, 2003 9:55 AM
To: rhelp
Subject: [R] How robust is mle in R?


A newbie question:  I'm trying to decide whether to run a maximum likelihood
estimation in R or Stata and am wondering if the R mle routine is reasonably
robust.  I'm fairly certain that, with this data, in Stata I would get a lot
of complaints about non-concave functions and unproductive steps attempted,
but would eventually have a successful ML estimate.  I believe that, with
the 'unproductive step' at least, Stata gets around the problem by switching
to some alternative estimation method in difficult cases.  Does anyone know
how robust mle is in R?

Thanks,
Peter

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at stat.ucla.edu  Sun Jul 13 22:38:28 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Sun, 13 Jul 2003 13:38:28 -0700
Subject: [R] How robust is mle in R?
In-Reply-To: <BB36EB08.590C%peterm@andrew.cmu.edu>
References: <BB36EB08.590C%peterm@andrew.cmu.edu>
Message-ID: <3F11C344.9020201@stat.ucla.edu>

There's no 'mle' routine in R.  For doing general maximum likelihood 
estimation I often use 'optim' or 'nlm'.  I find 'optim' to be very 
useful, although you have to become familiar with all of the 
options/arguments in order to use it successfully.  For example, you 
often have to provide scaling information (via 'parscale') to 'optim' in 
order for a good solution to be found. 

If you have a very complicated likelihood surface, than finding a good 
solution will likely be the exception rather than the rule, and I think 
that's independent of what software you use.  R's 'optim' function 
provides four different procedures for optimizing a function, each of 
which has its advantages and disadvantages.  I suggest checking out the 
help page for 'optim', which is very detailed. 

-roger

Peter Muhlberger wrote:

>A newbie question:  I'm trying to decide whether to run a maximum likelihood
>estimation in R or Stata and am wondering if the R mle routine is reasonably
>robust.  I'm fairly certain that, with this data, in Stata I would get a lot
>of complaints about non-concave functions and unproductive steps attempted,
>but would eventually have a successful ML estimate.  I believe that, with
>the 'unproductive step' at least, Stata gets around the problem by switching
>to some alternative estimation method in difficult cases.  Does anyone know
>how robust mle is in R?
>
>Thanks,
>Peter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From rwang at math.ucalgary.ca  Sun Jul 13 23:38:54 2003
From: rwang at math.ucalgary.ca (rui)
Date: Sun, 13 Jul 2003 15:38:54 -0600
Subject: [R] How to install a package
Message-ID: <000801c34987$292c0830$bf3d9f88@pcrui>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030713/9fe94db5/attachment.pl

From J.McBroom at griffith.edu.au  Mon Jul 14 00:35:04 2003
From: J.McBroom at griffith.edu.au (James McBroom)
Date: Mon, 14 Jul 2003 08:35:04 +1000
Subject: [R] Offsets in glmmPQL?
Message-ID: <OF7185815E.C7D5ADEB-ON4A256D62.007B7710-4A256D62.007C0F55@domino.gu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030714/97e42603/attachment.pl

From Tor.Strand at cih.uib.no  Mon Jul 14 00:45:14 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Mon, 14 Jul 2003 00:45:14 +0200
Subject: [R] question regarding GAM from a novice (in GAM as well as in R)
In-Reply-To: <3F1051C9.28293.1F0325E@localhost>
Message-ID: <BB37AD9A.722B%Tor.Strand@cih.uib.no>

On 7/13/03 12:22 AM, "kjetil brinchmann halvorsen" <kjetil at entelnet.bo>
wrote:

> On 12 Jul 2003 at 18:50, Tor A  Strand wrote:
> 
> Hei!
> 
>> Need to use generalized additive models and have therefore obtained R
>> 
>> I am able to do the analysis but I have problems understanding the syntax
>> and the options
>> 
>> Can someone explain what some of the terms do in this model do:?
>> 
>> c<-gam(depvar~var1+var2+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
>> data=xdataset ) 
>> 
>> I do not use the terms including var4- var8 in my model, just want to know
>> what they do. 
>> 
>> +s(var4, by=var5)
> 
> Presumably var5 is a factor, separate smooths are calculated for each
> level of the factor. This is a kind of interaction.
> 
>> +s(var6, var7)
> 
> This looks like a bivariate smooth, that is , a surface. Never tried
> that.
> 
>> +s(var8,3)
> 
> second argument is the number of degrees of freedom.
> 
>> 
>> Furthermore, the results become rather different when I change the model to:
>> 
>> c<-gam(depvar~var1+var2-1+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
>> data=xdataset ) 
>> # note just adding a -1
>> 
>> Why. 
> 
> This is removing the intercept from the model, and as all smooths are
> centered at zero in some way, that should have more or less the same
> effect as removing the intercept from a linear model. That can make a
> HUGE difference.
> 
>> 
>> Please forgive my ignorance.
> 
> You probably have a local expert at Haukeland: Try to call
> Tore Wenzel-Larsen if you need more help.
> 
> Kjetil Halvorsen
> 
All of the above were extremely useful. I will also ask Tore.

Thanks



From Tor.Strand at cih.uib.no  Mon Jul 14 00:48:26 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Mon, 14 Jul 2003 00:48:26 +0200
Subject: [R] automatically saving plots and images
Message-ID: <BB37AE5A.722C%Tor.Strand@cih.uib.no>

I am a novice

Is there any way of automatically save a plot to an image file. I want to
generate images of plots during a procedure and I do not want to manually
save the files through the menu.

Thanks

Tor A Strand       
Centre for International Health
University of Bergen



From dmurdoch at pair.com  Mon Jul 14 00:51:49 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 13 Jul 2003 18:51:49 -0400
Subject: [R] How to install a package
In-Reply-To: <000801c34987$292c0830$bf3d9f88@pcrui>
References: <000801c34987$292c0830$bf3d9f88@pcrui>
Message-ID: <seo3hvoioladlcvp0cd9vsu3575dajh66f@4ax.com>

On Sun, 13 Jul 2003 15:38:54 -0600, you wrote:

>Dear R community:
>
>My platform: R 1.7.0 + windows2000.
>
>I am trying to install the package "lasso2" which I saw in the following web address: http://cran.us.r-project.org/src/contrib/PACKAGES.html#emplik. However, I failed to install it from R menu "Packages| Install package(s) from CRAN" since I 
>could not find this item in the list.

The list in the menu consists of packages that have been compiled for
Windows.  It looks as though lasso2 won't compile for Windows without
modifications.  If you want to use it, you'll need to find a compiled
copy somewhere else, or compile it yourself.

Duncan Murdoch



From ok at cs.otago.ac.nz  Mon Jul 14 00:53:11 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 14 Jul 2003 10:53:11 +1200 (NZST)
Subject: [R] Indexing with NA as FALSE??
Message-ID: <200307132253.h6DMrBUL089556@atlas.otago.ac.nz>

	> Also, as an extra, it would be very useful if, for instance,
	> t[u==NA] --> 2 4 6 8
	> (I realise that working round this is less cumbersome, but even so).
	
What's wrong with writing t[is.na(u)]?

Or you could define
    eq <- function(x,y) {
	(is.na(x) & is.na(y)) | (!is.na(x) & !is.na(y) & x == y)
    }
and use
    t[eq(u,NA)]



From pkraft at zoology.uq.edu.au  Mon Jul 14 01:22:33 2003
From: pkraft at zoology.uq.edu.au (Peter Kraft)
Date: Mon, 14 Jul 2003 09:22:33 +1000
Subject: [R] Coloured 3d surface
Message-ID: <3F11E9B9.8000209@zoology.uq.edu.au>

Hello,

I created a 3d surface (persp) with some points overlaid on it, which is 
fine. Now I have a second set of z-values(x,y-values same as the first 
surface), which I would like to make visible on the same graph, however, 
not as a surface, but rather as coloured contour on the first surface, 
so that the resulting graph will consist of the original surface having 
the colour of the second set of z-values; I managed to create a coloured 
contour plot, whereby the the contours are based on the first set of 
z-values and the second set is a colour-coded overlay. If I could do 
this in 3-d I would be happy for any hint you could give me.

Thanks for any help

Peter
-- 




Peter Kraft
Department of Zoology and Entomology
University of Queensland



From jerome at hivnet.ubc.ca  Mon Jul 14 01:33:14 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Sun, 13 Jul 2003 16:33:14 -0700
Subject: [R] automatically saving plots and images
In-Reply-To: <BB37AE5A.722C%Tor.Strand@cih.uib.no>
References: <BB37AE5A.722C%Tor.Strand@cih.uib.no>
Message-ID: <200307132339.QAA13154@hivnet.ubc.ca>


See ?device .

Hope this helps,
Jerome

On July 13, 2003 03:48 pm, Tor A  Strand wrote:
> I am a novice
>
> Is there any way of automatically save a plot to an image file. I want
> to generate images of plots during a procedure and I do not want to
> manually save the files through the menu.
>
> Thanks
>
> Tor A Strand
> Centre for International Health
> University of Bergen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Simon.Blomberg at anu.edu.au  Mon Jul 14 02:28:40 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 14 Jul 2003 10:28:40 +1000
Subject: [R] bootstrap for hclust
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133D6@CASEVS02.cas.anu.edu.au>

We had this question a couple of weeks ago (see the archive). No, there is currently no function for bootstrap support indices on hclust objects. However, code contributions are welcome. ;-) A long-winded way to do it could be to convert your hclust objects to phylo objects, using as.phylo (package ape), then write them to a file as bracket-format trees using write.tree (package ape), then analyse the results in another package e.g. PHYLIP.

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Ron Ophir [mailto:lsophir at wisemail.weizmann.ac.il]
> Sent: Monday, 14 July 2003 5:10 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] bootstrap for hclust
> 
> 
> dear group members,
> I am looking for a function that assess the stability of 
> cluster. The result of hclust function is an hclust object 
> which can  be plot as a dendrogram. However to have 
> confidence in the tree topology usualy bootstap is applied. I 
> understand that I can apply bootstarp on the original data 
> and then run hclust(dist() ) as much as I resampled but how 
> to comapre the topologies the I don't know. Is there a 
> bootstarp or permutation for cluster?
> thanks,
> Ron
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.murrell at auckland.ac.nz  Mon Jul 14 05:02:31 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 14 Jul 2003 15:02:31 +1200
Subject: [R] metapost device in R (again ;-)
References: <20030711151356.GA1656@localhost>
Message-ID: <3F121D47.1050808@stat.auckland.ac.nz>

Hi


Tamas Papp wrote:
> Hi,
> 
> I read the 2000 thread on a MetaPost device in R. If I understand
> correctly, the main problem with the concept is that R wants the device
> driver to give back information on the size of strings/labels.
> 
> To the bet of my knowledge, MetaPost _does_ make it possible to
> measure the bounding box of text (see section 7.3: Measuring text in
> the MetaPost manual). For example, one could get the size of the
> bounding box of btex $\int_a^b x^2$ etex -- would that be enough to
> make an implementation possible? Or are the size of individual
> characters and kerning information necessary?


Kerning information is not required, but a device driver does need to 
provide information on the size of individual characters (or return 
zeroes which in some cases will prompt a warning that such information 
is not available, but most stuff will still work it just may look a bit 
nastier).


> Another question: can graphics devices be implemented solely in R (ie
> without writing C code)? I realize that it will be much slower, but
> first I would like to see how it works before writing in C. What
> source files should I be looking at?


Unfortunately, you can't write a device driver solely in R -- all the 
hooks are at the C-level.  You need to look at ...

R/src/include/R_ext/GraphicsDevice.h

... for a description of the device structures and function calls you 
have to fill in, and see one of the existing device drivers, e.g., ...

R/src/modules/X11/devX11.c

... for an example of how an existing device does the filling in.


> You may ask why I should bother about using Metapost. Well, I'd like
> my TeX documents to be more consistent typographically, and MP has
> quite a lot of useful features (such as the possibility to include its
> eps output in LaTeX directly, EVEN when generating PDF files with
> latexpdf). But the biggest bonus would clearly be the ability to
> typeset math formulas nicely. (I realize that this would require one
> to start a MetaPost process, but IMO the benefits would be worth the
> overhead).


I assume that you know about the mathematical annotation facilities in R 
-- help(plotmath) -- and want something "nicer" ...

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz



From hp3000al at yahoo.com  Mon Jul 14 07:21:09 2003
From: hp3000al at yahoo.com (dg gdf)
Date: Sun, 13 Jul 2003 22:21:09 -0700 (PDT)
Subject: [R] I am not found som data frames
Message-ID: <20030714052109.51732.qmail@web14910.mail.yahoo.com>

In the name of Allah.
hello, how are you?
I sent a mail for you,yesterday but I didn't recieve
to my aim.
I need to some data frames in version 1.7.0 such as
"florida","Barley","rainforest" and some others.
I don't know these data frames are in which library.
I typed "data(package = .packages(all.available =
TRUE))" that was in "data()" command, but thay were
not available again.
please say me how I can found them.
thanks very very much.
bye.



From ligges at statistik.uni-dortmund.de  Mon Jul 14 08:43:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jul 2003 08:43:46 +0200
Subject: [R] I am not found som data frames
In-Reply-To: <20030714052109.51732.qmail@web14910.mail.yahoo.com>
References: <20030714052109.51732.qmail@web14910.mail.yahoo.com>
Message-ID: <3F125122.8060105@statistik.uni-dortmund.de>

dg gdf wrote:

> In the name of Allah.
> hello, how are you?
> I sent a mail for you,yesterday but I didn't recieve
> to my aim.
> I need to some data frames in version 1.7.0 such as
> "florida",

help.search("florida")
--> "Florida" is in package "car"

 > "Barley",

help.search("Barley")
--> "barley" in package "lattice"

> "rainforest" and some others.

help.search("rainforest")

On my machine, having installed all CRAN packages:
"No help files found with alias or title matching 'rainforest' using 
fuzzy matching."

So either the name is wrong or the data is from a not-on-CRAN package.

Uwe Ligges



> I don't know these data frames are in which library.
> I typed "data(package = .packages(all.available =
> TRUE))" that was in "data()" command, but thay were
> not available again.
> please say me how I can found them.
> thanks very very much.
> bye.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Jul 14 08:51:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jul 2003 08:51:43 +0200
Subject: [R] Coloured 3d surface
In-Reply-To: <3F11E9B9.8000209@zoology.uq.edu.au>
References: <3F11E9B9.8000209@zoology.uq.edu.au>
Message-ID: <3F1252FF.7040008@statistik.uni-dortmund.de>

Peter Kraft wrote:

> Hello,
> 
> I created a 3d surface (persp) with some points overlaid on it, which is 
> fine. Now I have a second set of z-values(x,y-values same as the first 
> surface), which I would like to make visible on the same graph, however, 
> not as a surface, but rather as coloured contour on the first surface, 
> so that the resulting graph will consist of the original surface having 
> the colour of the second set of z-values; I managed to create a coloured 
> contour plot, whereby the the contours are based on the first set of 
> z-values and the second set is a colour-coded overlay. If I could do 
> this in 3-d I would be happy for any hint you could give me.
> 
> Thanks for any help
> 
> Peter

You can specify an argument "col" in persp(), see ?persp. However, the 
granularity is given by the facets produced by persp() - each facet has 
exactly one colour.

Uwe Ligges



From vandemem at gmx.de  Mon Jul 14 09:29:50 2003
From: vandemem at gmx.de (Marc Vandemeulebroecke)
Date: Mon, 14 Jul 2003 09:29:50 +0200 (MEST)
Subject: [R] bug?
Message-ID: <21011.1058167790@www13.gmx.net>

Dear R programmers,

is there a sensible explanation for the following behaviour? The second
command seems not to be interpreted correctly.

> seq(0.6, 0.9, by=0.1) == 0.8
[1] FALSE FALSE  TRUE FALSE
> seq(0.7, 0.9, by=0.1) == 0.8
[1] FALSE FALSE FALSE
> c(0.7, 0.8, 0.9) == 0.8
[1] FALSE  TRUE FALSE
> seq(0.9, 0.7, by=-0.1) == 0.8
[1] FALSE  TRUE FALSE

I am running R version 1.7.1 on XP and NT.

Thanks,
Marc

--



From ligges at statistik.uni-dortmund.de  Mon Jul 14 10:10:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jul 2003 10:10:14 +0200
Subject: [R] bug?
In-Reply-To: <21011.1058167790@www13.gmx.net>
References: <21011.1058167790@www13.gmx.net>
Message-ID: <3F126566.5010802@statistik.uni-dortmund.de>

Marc Vandemeulebroecke wrote:

> Dear R programmers,
> 
> is there a sensible explanation for the following behaviour? The second
> command seems not to be interpreted correctly.
> 
> 
>>seq(0.6, 0.9, by=0.1) == 0.8
> 
> [1] FALSE FALSE  TRUE FALSE
> 
>>seq(0.7, 0.9, by=0.1) == 0.8
> 
> [1] FALSE FALSE FALSE
> 
>>c(0.7, 0.8, 0.9) == 0.8
> 
> [1] FALSE  TRUE FALSE
> 
>>seq(0.9, 0.7, by=-0.1) == 0.8
> 
> [1] FALSE  TRUE FALSE
> 
> I am running R version 1.7.1 on XP and NT.
> 
> Thanks,
> Marc
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


It is correct, just an instability of the representation of that 
floating point number, because (regularly) floating point numbers cannot 
be represented exactly.

Uwe Ligges



From tobias_verbeke at skynet.be  Mon Jul 14 10:32:57 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Mon, 14 Jul 2003 10:32:57 +0200
Subject: [R] for loop problem
Message-ID: <20030714103257.15cf8491.tobias_verbeke@skynet.be>

Dear list,

Here's a function that works fine
when I assign one value to i.
qx, ax and gr are vectors.
 
ax.to.nax <- function(qx, ax, gr=c(0,1,5,10)){
px <- 1 - qx
last.n <- gr[length(gr)] - gr[length(gr) - 1]
all.bounds <- c(gr, gr[length(gr)] + last.n)
n <- diff(all.bounds)
#
i <- 1 # this i should loop through the values of
       # gr: i=0, i=1, i=5 and i=10.
#
testprod <- prod(px[(i+1):(i+n[i+1])])
return(1 - testprod)
}

Every attempt to loop through
the values of gr, gives the following
error message:

Error in (i + 1):(i + n[i + 1]) : NA/NaN argument


Here is an example of the deplorably 
erroneous code that causes the message
to appear:

ax.to.nax.for <- function(qx, ax, gr=c(0,1,5,10)){
px <- 1 - qx
last.n <- gr[length(gr)] - gr[length(gr) - 1]
all.bounds <- c(gr, gr[length(gr)] + last.n)
n <- diff(all.bounds)
testprod <- numeric(length(gr))
for (j in 1:length(gr)){
  i <- gr[j]
  testprod[j] <- prod(px[(i+1):(i+n[i+1])])
  }
return(1 - testprod)
}


In what way am I ill-treating R ?



Thanks in advance,

Tobias



From p.dalgaard at biostat.ku.dk  Mon Jul 14 11:56:27 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 14 Jul 2003 09:56:27 -0000
Subject: [R] for loop problem
In-Reply-To: <20030714103257.15cf8491.tobias_verbeke@skynet.be>
References: <20030714103257.15cf8491.tobias_verbeke@skynet.be>
Message-ID: <x2r84ta0oh.fsf@biostat.ku.dk>

Tobias Verbeke <tobias_verbeke at skynet.be> writes:

> Dear list,
> 
> Here's a function that works fine
> when I assign one value to i.
> qx, ax and gr are vectors.
>  
> ax.to.nax <- function(qx, ax, gr=c(0,1,5,10)){
> px <- 1 - qx
> last.n <- gr[length(gr)] - gr[length(gr) - 1]
> all.bounds <- c(gr, gr[length(gr)] + last.n)
> n <- diff(all.bounds)
> #
> i <- 1 # this i should loop through the values of
>        # gr: i=0, i=1, i=5 and i=10.
> #
> testprod <- prod(px[(i+1):(i+n[i+1])])
> return(1 - testprod)
> }
> 
> Every attempt to loop through
> the values of gr, gives the following
> error message:
> 
> Error in (i + 1):(i + n[i + 1]) : NA/NaN argument
> 
> 
> Here is an example of the deplorably 
> erroneous code that causes the message
> to appear:
> 
> ax.to.nax.for <- function(qx, ax, gr=c(0,1,5,10)){
> px <- 1 - qx
> last.n <- gr[length(gr)] - gr[length(gr) - 1]
> all.bounds <- c(gr, gr[length(gr)] + last.n)
> n <- diff(all.bounds)
> testprod <- numeric(length(gr))
> for (j in 1:length(gr)){
>   i <- gr[j]
>   testprod[j] <- prod(px[(i+1):(i+n[i+1])])
>   }
> return(1 - testprod)
> }
> 
> 
> In what way am I ill-treating R ?

You're ill-treating the readers by not giving a full example of a call
to the function... However: in the for loop, i will be one of 0,1,5,10
and n is the vector c(1,4,5,5) so n[i+1] is indexing out of bounds and
e.g. 

> n[6]
[1] NA

and the : operator subsequently objects. Did you mean (i+1):(i+n[j]+1)
or so? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 11:59:04 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 10:59:04 +0100 (BST)
Subject: [R] Subsetting a matrix
Message-ID: <XFMail.030714105904.Ted.Harding@nessie.mcc.ac.uk>

I'd welcome some comments or advice regarding the situation described
below.

The following illustrates what seems to me to be an inconsistency
in the behaviour of matrix subsetting:

  > Z<-matrix(c(1.1,2.1,3.1,1.2,2.2,3.2,1.3,2.3,3.3),nrow=3)
  > Z
       [,1] [,2] [,3]
  [1,]  1.1  1.2  1.3
  [2,]  2.1  2.2  2.3
  [3,]  3.1  3.2  3.3
  > dim(Z)
  [1] 3 3

  > Z0<-Z[c(T,F,F),c(F,T,T)]
  > Z0
  [1] 1.2 1.3
  > dim(Z0)
  NULL

whereas, of course, with

  > Z1<-Z[c(T,T,F),c(F,T,T)]
  > Z1
       [,1] [,2]
  [1,]  1.2  1.3
  [2,]  2.2  2.3
  > dim(Z1)
  [1] 2 2

i.e. a fully-paid-up matrix.

What I would have expected is that Z0 should come out as a 1x2 matrix:

         [,1] [,2]
  [1,]  1.2  1.3

with dim(Z0) --> [1] 1 2

but it does not -- it does not have matrix status. I know it will try
to behave politely if forced into matrix-algebra society, but it betrays
its lack of true poise in solecisms like the following:

  > Z0%*%Z1[1:2,1]
       [,1]
  [1,]  4.3
  > Z1[1:2,1]%*%Z0
       [,1]
  [1,]  4.3

One of these should be a scalar (in fact the first, given how Z0 was
created), and the other a 2x2 matrix. Z0 simply does not know that you
have to behave differently if seated on someone's right rather than
on their left.

Now of course one can send Z0 away for remedial training ( t() ):

  > Z0<-t(Z0)
  > Z0
       [,1] [,2]
  [1,]  1.2  1.3
  > Z0%*%Z1[1:2,1]
       [,1]
  [1,]  4.3
  > Z1[1:2,1]%*%Z0
       [,1] [,2]
  [1,] 1.44 1.56
  [2,] 2.64 2.86

and now it does know how to behave. But it would save some bother (and
having to worry about this sort of thing in the midst of complex matrix
algebra) if matrix subsetting worked in a consistent way for all possible
subsets including cases which should result in 1xk or kx1 matrices
(where, by the way, we could have k=1 and get a 1x1 matrix).

Is there perhaps some global option which regulates this sort of
behaviour? Or is the underclass always with us?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 10:59:04
------------------------------ XFMail ------------------------------



From simon at stats.gla.ac.uk  Mon Jul 14 12:14:26 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 14 Jul 2003 11:14:26 +0100 (BST)
Subject: [R] question regarding GAM from a novice (in GAM as well as in R)
In-Reply-To: <BB3608ED.7202%Tor.Strand@cih.uib.no>
Message-ID: <Pine.SOL.3.96.1030714110542.27260E-100000@moon.stats.gla.ac.uk>

> 
> Can someone explain what some of the terms do in this model do:?
> 
> c<-gam(depvar~var1+var2+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
> data=xdataset ) 
> 
> I do not use the terms including var4- var8 in my model, just want to know
> what they do. 
> 
> +s(var4, by=var5)
- var5 is a variable multiplying this smooth of var4. i.e. the model is
something like:

E(depvar_i) = .... f(var4_i)var5_i + ... e_i

where f is a smooth function. Models like this are sometimes called
variable coefficient models (see Hastie and Tibhirani JRSSB 1993?)

> +s(var6, var7)
- A smooth function of two variables: var6 and var7 (you can, in
principle have smooths of any number of variables.)

> +s(var8,3)
old form of s(var8,k=3,bs="cr") uses a cubic regression spline basis with
3 knots to represent the smooth function of var8. Note that default k for
a 1-d smooth is 10, and default basis is "tp" - a thin plate regression
spline. The default basis is usually slightly better, and admits smooths
of several variables, but the "cr" basis is much quicker computatioanlly.

> Furthermore, the results become rather different when I change the model to:
> 
> c<-gam(depvar~var1+var2-1+s(var3)+s(var4, by=var5)+s(var6, var7)+s(var8,3),
> data=xdataset ) 

- *iff* var1 and var2 are not factors, then this is a model with no
intercept term, and the mean of the fitted values will be zero. Hence the
big change!

Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From maechler at stat.math.ethz.ch  Mon Jul 14 12:17:43 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Jul 2003 12:17:43 +0200
Subject: [R] unimodality test
In-Reply-To: <16142.50135.660168.923694@gargle.gargle.HOWL>
References: <Pine.SOL.3.95.1030711115545.3318A-100000@heraclite>
	<16142.50135.660168.923694@gargle.gargle.HOWL>
Message-ID: <16146.33607.410270.541532@gargle.gargle.HOWL>

replying to my own posting of last Friday:
>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch> writes:

>>>>> "Jerome" == Jerome Sackur <sackur at heraclite.ens.fr>
>>>>>     on Fri, 11 Jul 2003 12:17:33 +0200 (MET DST) writes:

    Jerome> Dear R users, I am interested in uni- bi-
    Jerome> multimodality tests, for analysing reaction times
    Jerome> data. I was lead to Hartigan's dip test
    Jerome> (Ann. Statistics, 13, 1985, pp. 70-84, Applied
    Jerome> Statistics, 34, 1985, 320-325). Not being a
    Jerome> programmer I am unable to translate the Fortran code
    Jerome> given in ref. 2 into a R function. I'd be glad to
    Jerome> learn that someone already did it, or has devised a
    Jerome> better solution for this kind of problem..

    MM> I had got a version with Fortran and S-plus from Dario
    MM> Ringach (@ NYU.edu) in 1994 (from what I see) and had
    MM> worked on it in 2000, made it into an R package back then.
(yes)

    MM> The reason it hasn't made its way to CRAN was
    MM> that the Fortran code (which I f2c'ed to C) still has
    MM> bugs (leading to segmentation faults) that I've not yet
    MM> found time to debug.

not quite true.

    MM> Let me have a look at it before making it available  <....>

I've put a new source package diptest (0.9-1) on CRAN's incoming.
For the interested ones, there's a version of it also available
as
	ftp://stat.ethz.ch/U/maechler/R/diptest_0.9-1.tar.gz

	(yes, only for those who can install packages from source!)

Note that because of vacation and conferences, it may take a
bit longer before this will be available from CRAN and its mirrors 
(and even a bit longer before "pre-compiled" availability for Windows).

Regards,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From azzalini at stat.unipd.it  Mon Jul 14 12:24:36 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 14 Jul 2003 12:24:36 +0200
Subject: [R] Subsetting a matrix
In-Reply-To: <XFMail.030714105904.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030714105904.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20030714102436.EA8F67CA831@tango.stat.unipd.it>

On Monday 14 July 2003 11:59, you wrote:
> I'd welcome some comments or advice regarding the situation described
> below.
>
> The following illustrates what seems to me to be an inconsistency
> in the behaviour of matrix subsetting:
>
> ? > Z<-matrix(c(1.1,2.1,3.1,1.2,2.2,3.2,1.3,2.3,3.3),nrow=3)
> ? > Z
> ? ? ? ?[,1] [,2] [,3]
> ? [1,] ?1.1 ?1.2 ?1.3
> ? [2,] ?2.1 ?2.2 ?2.3
> ? [3,] ?3.1 ?3.2 ?3.3
> ? > dim(Z)
> ? [1] 3 3
>
> ? > Z0<-Z[c(T,F,F),c(F,T,T)]
> ? > Z0
> ? [1] 1.2 1.3
> ? > dim(Z0)
> ? NULL
>
> whereas, of course, with
>
> ? > Z1<-Z[c(T,T,F),c(F,T,T)]
> ? > Z1
> ? ? ? ?[,1] [,2]
> ? [1,] ?1.2 ?1.3
> ? [2,] ?2.2 ?2.3
> ? > dim(Z1)
> ? [1] 2 2
>
> i.e. a fully-paid-up matrix.
>
> What I would have expected is that Z0 should come out as a 1x2 matrix:
>
> ? ? ? ? ?[,1] [,2]
> ? [1,] ?1.2 ?1.3
>
> with dim(Z0) --> [1] 1 2

This seems to me yet another example of the side-effects caused by
the automatic conversion of matrix to "vector" (in Splus/R sense)
when the one of the dimension is 1.  There are many examples of this 
sort.

Of course the remedy is to do
    Z0 <- Z[c(T,F,F),c(F,T,T), drop=FALSE]

Personally, I find this automatic conversion to "vector" a somewhat confusing 
feature (although I can see its reasons),  and I know of many people that would 
have preferred  that drop=FALSE was the default  behaviour, but surely now 
is difficult to change it.

regards
Adelchi Azzalini 


-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From B.Rowlingson at lancaster.ac.uk  Mon Jul 14 12:45:56 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 14 Jul 2003 11:45:56 +0100
Subject: [R] bug?
In-Reply-To: <3F126566.5010802@statistik.uni-dortmund.de>
References: <21011.1058167790@www13.gmx.net>
	<3F126566.5010802@statistik.uni-dortmund.de>
Message-ID: <3F1289E4.8080504@lancaster.ac.uk>


> 
> It is correct, just an instability of the representation of that 
> floating point number, because (regularly) floating point numbers cannot 
> be represented exactly.


  A python developer friend of mine tells me there was a proposal for 
python that two floating-point numbers should _never_ return TRUE for 
'=='. Even if their binary representations are the same. I can see the 
merit in this, as long as its documented.

  I'm tempted to think that any argument for  float == float always 
returning false is almost like saying float == float is an invalid 
operation, and hence should be flagged as that at compile/interp time. 
Probably easier to do in a strongly-typed language.

  Of course the main problem is that us 'old-timers' who grew up 
deciding whether to use REAL*4 or REAL*8 understand what is going on 
'under the hood' with floating point numbers, but today's computers are 
being used by mathematicians and statisticians whose perception of 
numbers comes from a different direction to computer people. "0.7 + 0.1 
does not equal 0.8? Does 1+1 not equal 2? That's madness!", they cry.

  The solution is education: I'm hoping to teach all our postgraduates a 
short course on computer history and culture, from binary numbers up to 
software engineering, hardware, programming languages etc etc. Floating 
point formats are in there somewhere, for sure.

Baz



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 12:38:38 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 11:38:38 +0100 (BST)
Subject: [R] Subsetting a matrix
In-Reply-To: <20030714102436.EA8F67CA831@tango.stat.unipd.it>
Message-ID: <XFMail.030714113838.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jul-03 Adelchi Azzalini wrote:
> Personally, I find this automatic conversion to "vector" a somewhat
> confusing feature (although I can see its reasons),  and I know of
> many people that would  have preferred  that drop=FALSE was the
> default  behaviour, but surely now  is difficult to change it.

Thanks for pointing out "drop=FALSE", which I had overlooked!

Maybe it is reasonable to propose incorporating "drop" as one of the
things you can set with "options"? :--

     `options' allows the user to set and examine a variety of global
     ``options'' which affect the way in which R computes and displays
     its results.

Affecting the way in which R computes Z[c(T,F,F),c(F,T,T)] is just what
I would like to be able to do!

With thanks,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 11:38:38
------------------------------ XFMail ------------------------------



From azzalini at stat.unipd.it  Mon Jul 14 13:10:57 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Mon, 14 Jul 2003 13:10:57 +0200
Subject: [R] Subsetting a matrix
In-Reply-To: <XFMail.030714113838.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030714113838.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20030714111057.B765B7CA831@tango.stat.unipd.it>


> Maybe it is reasonable to propose incorporating "drop" as one of the
> things you can set with "options"? :--
>
>      `options' allows the user to set and examine a variety of global
>      ``options'' which affect the way in which R computes and displays
>      its results.
>

Perhaps this is a way-out. As usual, everything is feasible when we resort
entirely on our own code.  The danger is that we might make use of some 
package (may be written some years ago) which assumes drop=TRUE is the default
behaviour, and changing the default behaviour might (potentially) have 
disasterous effects.

best wishes, Adelchi

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From tobias_verbeke at skynet.be  Mon Jul 14 13:18:26 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Mon, 14 Jul 2003 13:18:26 +0200
Subject: [R] for loop problem
In-Reply-To: <x2r84ta0oh.fsf@biostat.ku.dk>
References: <20030714103257.15cf8491.tobias_verbeke@skynet.be>
	<x2r84ta0oh.fsf@biostat.ku.dk>
Message-ID: <20030714131826.5beff6c2.tobias_verbeke@skynet.be>

Sir Dalgaard,

Thank you for your kind reply.
 
> > In what way am I ill-treating R ?
> 
> You're ill-treating the readers by not 
> giving a full example of a
> call to the function... 

ax.to.nax.for(qx, ax)

> However: in the for loop, i will be one of
> 0,1,5,10 and n is the vector c(1,4,5,5) 
> so n[i+1] is indexing out of
> bounds and e.g. 
> 
> > n[6]
> [1] NA
> 
> and the : operator subsequently objects. 
> Did you mean (i+1):(i+n[j]+1) or so? 

Yes! I meant (i+1):(i+n[j])


Thanks again,

Tobias



From f.calboli at ucl.ac.uk  Mon Jul 14 13:51:23 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Mon, 14 Jul 2003 12:51:23 +0100
Subject: [R] methods help and glmmPQL
Message-ID: <3.0.6.32.20030714125123.028a7158@pop-server.ucl.ac.uk>

Dear All,

I would like to ask you to help me with my memeory. I remember using some
function that would list all the possible methods I could apply to an
object. Say, if I had an object of 

class=lme, 

it would tell me that that I could do stuff like 

qqnorm(myobjct), or VarCorr(myobject). In general, a very complete list.

I though this list of all possible methods would pop out by typing 

methods(class = lme) or, methods(class = whatever class my object is),

but, if I type: methods(class = lme), I get:

"anova.lme"    "plot.lme"     "simulate.lme" "vcov.lme"

nowhere as complete list as I would expect (no mention of VarCorr, for
instance)

So, what was the magical command I used to get a list of all the possible
ways of skinning an object? 

Secondly, how do I get to see the SS and MS of the fixed terms of a glmmPQL
fit ?

Regards,
Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From z_eric2000 at yahoo.com  Mon Jul 14 13:55:22 2003
From: z_eric2000 at yahoo.com (Z Eric)
Date: Mon, 14 Jul 2003 04:55:22 -0700 (PDT)
Subject: [R] qualitative response model
Message-ID: <20030714115522.35057.qmail@web40807.mail.yahoo.com>

Hi, I want to know is there other functions in R to
estimate qualitative response model besides multinom()
in library nnet, if this is the only possibility, I
have a question about the application:
for example:
there is three transportation choice : car, bus ,
subway.
each alternative has own characteristic variables,
I want to apply conditional logit model to analysis
the choice of three alternatives, but if some cases
face different choice set, i.e only car and bus are
available, how to use this function?
I am a newcomer in R, hope the simple question not to
bother you.
thanks for the help.



From andy_liaw at merck.com  Mon Jul 14 14:09:54 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Jul 2003 08:09:54 -0400
Subject: [R] ss's are incorrect from aov with multiple factors ( EXAMP	LE!)
Message-ID: <3A822319EB35174CA3714066D590DCD50205C868@usrymx25.merck.com>

> From: John Christie [mailto:jc at or.psychology.dal.ca] 

[snip]
 
> BTW, Statview seems to generate the same MSE for me whether I 
> collapse 
> the data or not.

StatView is being orphaned by you know who.  Not exactly the package to
compare to, IMHO.

Andy

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From spencer.graves at pdf.com  Mon Jul 14 14:29:46 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 05:29:46 -0700
Subject: [R] Coloured 3d surface
References: <3F11E9B9.8000209@zoology.uq.edu.au>
Message-ID: <3F12A23A.9080802@pdf.com>

If you provide a toy example, it is a lot easier for others to test your 
code and experient with possible solutions.  [Besides, the effort to 
develop a very simple example often leads me to solutions to my own 
problems.  In that case, (a) I learn more, (b) I often get a quicker 
answer than I would from the list and then I don't have to disturb 
others, and (c) if I go to the list, I'm more likely to get what I want.]

sorry I couldn't be more helpful.  spencer graves

Peter Kraft wrote:
> Hello,
> 
> I created a 3d surface (persp) with some points overlaid on it, which is 
> fine. Now I have a second set of z-values(x,y-values same as the first 
> surface), which I would like to make visible on the same graph, however, 
> not as a surface, but rather as coloured contour on the first surface, 
> so that the resulting graph will consist of the original surface having 
> the colour of the second set of z-values; I managed to create a coloured 
> contour plot, whereby the the contours are based on the first set of 
> z-values and the second set is a colour-coded overlay. If I could do 
> this in 3-d I would be happy for any hint you could give me.
> 
> Thanks for any help
> 
> Peter



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 14:50:35 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 13:50:35 +0100 (BST)
Subject: [R] Subsetting a matrix
In-Reply-To: <20030714111057.B765B7CA831@tango.stat.unipd.it>
Message-ID: <XFMail.030714135035.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jul-03 Adelchi Azzalini wrote:
>> Maybe it is reasonable to propose incorporating "drop" as one of the
>> things you can set with "options"? :--
>> [...]
> 
> Perhaps this is a way-out. As usual, everything is feasible when we
> resort entirely on our own code.  The danger is that we might make
> use of some package (may be written some years ago) which assumes
> drop=TRUE is the default behaviour, and changing the default behaviour
> might (potentially) have disasterous effects.

Indeed. However, it should be OK when embarking on a block or function
of matrix computations, written by oneself, to enclose it between

  options(drop=FALSE)
  ...
  options(drop=TRUE)

(unless some of the standard or "base" matrix functions which one
might use in "..." depend on the default. How could one tell?).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 13:50:35
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 14:44:45 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 13:44:45 +0100 (BST)
Subject: [R] bug?
In-Reply-To: <3F1289E4.8080504@lancaster.ac.uk>
Message-ID: <XFMail.030714134445.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jul-03 Barry Rowlingson wrote:
>   Of course the main problem is that us 'old-timers' who grew up 
> deciding whether to use REAL*4 or REAL*8 understand what is going on 
> 'under the hood' with floating point numbers, but today's computers are
> being used by mathematicians and statisticians whose perception of 
> numbers comes from a different direction to computer people. "0.7 + 0.1
> does not equal 0.8? Does 1+1 not equal 2? That's madness!", they cry.
> 
>   The solution is education: I'm hoping to teach all our postgraduates
> a short course on computer history and culture, from binary numbers
> up to software engineering, hardware, programming languages etc etc.
> Floating point formats are in there somewhere, for sure.

Spot on, Baz!

An intriguing example to set people is the following:

Write a program to generate a sequence of numbers x(0) ( = anything in
the range 0 <= x <= 1, chosen by user), x(1), x(2), ... , x(n), ...
where
   x(n+1) = f(x(n)), ...
and f() is defined by
   f(x) = 2*x if 0 <= x <= 1/2
   f(x) = 2*(1-x) if 1/2 <= x <= 1

Investigate this series both mathematically and by simulation.

1. Find by mathematics the period-1 equilibrium points [ x(n+1) = x(n) ].

Ans: Clearly: x=0 and x= 2/3

Test: verify these computationally. OOPS!!

2. Find by mathematics the period-2 equilibria [ x(n+2) = x(n) but not
   period-1 ]:

Ans: 2/5 and 4/5

Test: --> OOPS!!

...

k. Find the period-k equilibria

Ans: {2^1, 2^2, ... , 2^k}/(2^k + 1)

Test: --> OOPS!!

And so on. The mathematical equilibria form a set everywhere dense on
[0,1], yet apart from 0 none of them will show up as such on a computer
which is doing standard floating-point arithmetic. (It's a different
story with programs which do exact arithmetic on fractions, preserving
integer numerator and denominator.)

Starting values of the form x(0) = s/2^k head both mathematically and
computationally to 0, arriving in at most (k+1) steps.

Mathematically, all other numbers are not equilibria and the sequence
is "chaotic", yet this cannot be observed either. In fact the sequence
generated will arrive at 0 (and stay there) in at most (N+1) steps where
N is the number of bits in the abscissa of the floating-point
representation.

Every single remotely interesting property of this sequence is inaccesible
by standard arithmetic computation.

Example:
>
f<-function(x){ if((x==0)|(x==1)) stop();
                if(x<=1/2){y<-2*x} else {y<-2*(1-x)}; y }
> x<-2/3; z<-x; while(TRUE){ x<-f(x); z<-c(z,x) }
and then inspect z.

[I invented this example some time ago when bothered by some features of
 a simulated chaos; feel free to use it if it will help to bring home
 some facts of computational life!]

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 13:44:45
------------------------------ XFMail ------------------------------



From znmeb at aracnet.com  Mon Jul 14 15:02:49 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Mon, 14 Jul 2003 06:02:49 -0700
Subject: [R] bug?
In-Reply-To: <3F126566.5010802@statistik.uni-dortmund.de>
Message-ID: <001501c34a08$3e1e6900$74c463d8@plaza.ds.adp.com>

To be more precise, the decimal number 0.1 does not have an exact binary
equivalent. A long time ago, there was a book called, IIRC, "Pascal with
Style" or something of that ilk, which set out the warning "Never compare
floating point numbers for equality."

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: Monday, July 14, 2003 1:10 AM
> To: Marc Vandemeulebroecke
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] bug?
> 
> 
> Marc Vandemeulebroecke wrote:
> 
> > Dear R programmers,
> > 
> > is there a sensible explanation for the following behaviour? The 
> > second command seems not to be interpreted correctly.
> > 
> > 
> >>seq(0.6, 0.9, by=0.1) == 0.8
> > 
> > [1] FALSE FALSE  TRUE FALSE
> > 
> >>seq(0.7, 0.9, by=0.1) == 0.8
> > 
> > [1] FALSE FALSE FALSE
> > 
> >>c(0.7, 0.8, 0.9) == 0.8
> > 
> > [1] FALSE  TRUE FALSE
> > 
> >>seq(0.9, 0.7, by=-0.1) == 0.8
> > 
> > [1] FALSE  TRUE FALSE
> > 
> > I am running R version 1.7.1 on XP and NT.
> > 
> > Thanks,
> > Marc
> > 
> > --
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> It is correct, just an instability of the representation of that 
> floating point number, because (regularly) floating point 
> numbers cannot 
> be represented exactly.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 15:14:22 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 14:14:22 +0100 (BST)
Subject: [R] qualitative response model
In-Reply-To: <20030714115522.35057.qmail@web40807.mail.yahoo.com>
Message-ID: <XFMail.030714141422.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jul-03 Z Eric wrote:
> Hi, I want to know is there other functions in R to
> estimate qualitative response model besides multinom()
> in library nnet, if this is the only possibility, I
> have a question about the application:
> for example:
> there is three transportation choice : car, bus, subway.
> each alternative has own characteristic variables,
> I want to apply conditional logit model to analysis
> the choice of three alternatives, but if some cases
> face different choice set, i.e only car and bus are
> available, how to use this function?
> I am a newcomer in R, hope the simple question not to
> bother you.
> thanks for the help.

A totally naive idea would be to incorporate the "choice"
set as a set of three factors CAR=0/1, BUS=0/1, SUB=0/1
according to availability. Naively, since (e.g.) CAR=0
would lead to zero cases observed, one might expect that
the estimation would predict zero for these. However, in
conjunction with the logistic model the parameter estimates
would be improper (-> inf).

Nevertheless, there may be some mileage in an approach which
incorporated this idea, though I'm not seeing how!

If it could be got to work, though, it should lead to a convenient
representation of interesting interactions, e.g. how does propensity
to use "car" change if "subway" is not available?

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 14:14:22
------------------------------ XFMail ------------------------------



From baliola at riseup.net  Mon Jul 14 17:34:28 2003
From: baliola at riseup.net (Martin Wegmann)
Date: Mon, 14 Jul 2003 15:34:28 +0000
Subject: [R] gam and step
Message-ID: <200307141534.28101.baliola@riseup.net>

hello, 

I am looking for a step() function for GAM's.
In the book Statistical Computing by Crawley and a removal of predictors  has 
been done "by hand"

model <- gam(y ~s(x1) +s(x2) + s(x3))
summary(model) 
model2 <- gam(y ~s(x2) + s(x3)) # removal of the unsignificant variable
#then comparing these two models if an significant increase occurs.
anova(model, model2, test="F")

isn't there a way to drop and add variables automatically until the best model 
is received? like in step(lm(...))? 
Or as in grasp.step.gam() - but that doesn't work when I tried it outside 
GRASP-R.

thanks for your help, cheers Martin



From andy_liaw at merck.com  Mon Jul 14 15:33:31 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Jul 2003 09:33:31 -0400
Subject: [R] Memory size
Message-ID: <3A822319EB35174CA3714066D590DCD50205C86D@usrymx25.merck.com>

How *exactly* did you "run the regression" in R?  There are several ways,
and it can make a big difference for large data sets.  lm() would be the
most expensive option.  If I'm not mistaken, lsfit() is more "lean and
mean".  You can even do it more or less by hand, by calling qr() directly.
There's also a disussion in Venables & Ripley's "S Programming" on this
subject (for Splus).

Andy

> -----Original Message-----
> From: Silika Tereshchenko [mailto:silika at access.unizh.ch] 
> Sent: Sunday, July 13, 2003 8:55 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Memory size
> 
> 
> 
> Daer all,
> 
> I have the problem. I could not run the regression, because I 
> have always the warning message "memory.size". from the help 
> file I learned that it is possible to increase the memory 
> size, but I did not undestand how could I do it. Could you 
> please explaine it to me. I would be very grateful for it.
> 
> 
> The second question: I obtained from the regression the 
> coefficient "6.003e-3" and "0.0345e+3". What daos it mean?
> 
> 
> 
> Thanks a lot,
> Silika
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From MSchwartz at medanalytics.com  Mon Jul 14 15:58:48 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 14 Jul 2003 13:58:48 -0000
Subject: [R] bug?
In-Reply-To: <3F126566.5010802@statistik.uni-dortmund.de>
References: <21011.1058167790@www13.gmx.net>
	<3F126566.5010802@statistik.uni-dortmund.de>
Message-ID: <1058191114.23869.6.camel@localhost>

On Mon, 2003-07-14 at 03:10, Uwe Ligges wrote:
> Marc Vandemeulebroecke wrote:
> 
> > Dear R programmers,
> > 
> > is there a sensible explanation for the following behaviour? The second
> > command seems not to be interpreted correctly.
> > 
> > 
> >>seq(0.6, 0.9, by=0.1) == 0.8
> > 
> > [1] FALSE FALSE  TRUE FALSE
> > 
> >>seq(0.7, 0.9, by=0.1) == 0.8
> > 
> > [1] FALSE FALSE FALSE
> > 
> >>c(0.7, 0.8, 0.9) == 0.8
> > 
> > [1] FALSE  TRUE FALSE
> > 
> >>seq(0.9, 0.7, by=-0.1) == 0.8
> > 
> > [1] FALSE  TRUE FALSE
> > 
> > I am running R version 1.7.1 on XP and NT.
> > 
> > Thanks,
> > Marc
> > 
> > --
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> It is correct, just an instability of the representation of that 
> floating point number, because (regularly) floating point numbers cannot 
> be represented exactly.
> 
> Uwe Ligges


An good online reference for these issues is at:

http://grouper.ieee.org/groups/754/

Specifically, the article by David Goldberg entitled "What Every
Computer Scientist Should Know about Floating-Point Arithmetic", which
is listed toward the bottom of that page.

HTH,

Marc Schwartz



From emkiba at gmx.de  Mon Jul 14 16:15:07 2003
From: emkiba at gmx.de (Michael Kirschbaum)
Date: Mon, 14 Jul 2003 16:15:07 +0200
Subject: [R] Processing List-elements
Message-ID: <000001c34a12$5ce95c50$471fb2ac@tvpaxter>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030714/145f1c34/attachment.pl

From andy_liaw at merck.com  Mon Jul 14 16:22:46 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Jul 2003 10:22:46 -0400
Subject: [R] Processing List-elements
Message-ID: <3A822319EB35174CA3714066D590DCD50205C86E@usrymx25.merck.com>

If the list components are vectors, you can do something like:

  mylist2 <- lapply(mylist, function(x) x[-1])

Andy

> -----Original Message-----
> From: Michael Kirschbaum [mailto:emkiba at gmx.de] 
> Sent: Monday, July 14, 2003 10:15 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Processing List-elements
> 
> 
> Hello.
> I got a problem with processing a list:
> How can I kill the first row of each element of a list, 
> without using a loop? Is there a function that processes each 
> element of a list in a way I can choose? thank you for answering...
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From maechler at stat.math.ethz.ch  Mon Jul 14 16:35:10 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Jul 2003 16:35:10 +0200
Subject: [R] "lean and mean" regression {was "Memory size"}
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C86D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205C86D@usrymx25.merck.com>
Message-ID: <16146.49054.105727.288002@gargle.gargle.HOWL>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Mon, 14 Jul 2003 09:33:31 -0400 writes:

    AndyL> How *exactly* did you "run the regression" in R?
    AndyL> There are several ways, and it can make a big
    AndyL> difference for large data sets.  lm() would be the
    AndyL> most expensive option.  If I'm not mistaken, lsfit()
    AndyL> is more "lean and mean".  
as a matter of fact, rather use  lm.fit() which is the ``work horse'' 
of lm().  lm.fit() and lsfit() are very similar (relying on the
same Fortran QR decomposition, but lm.fit() has now been tested 
{by lm() usage} much more extensively.

    AndyL> You can even do it more or less by hand, by calling
    AndyL> qr() directly.  There's also a disussion in Venables
    AndyL> & Ripley's "S Programming" on this subject (for Splus).

Section 7.2, (actually the relevant code is not at all S-plus specific,
	      just the final "resources(.)" [CPU, Memory]
	      measuring of the solution.)
	      
It's for the case of one factor with many (107!)levels and continuous
covariates otherwise. There, one can solve without constructing
the large matrices that all of lm(), lsfit() or lm.fit() would use.

It becomes really "interesting" if you have (several) factors
with (many) levels...

Regards,
Martin

    >> -----Original Message----- From: Silika Tereshchenko
    >> [mailto:silika at access.unizh.ch] Sent: Sunday, July 13,
    >> 2003 8:55 AM To: R-help at stat.math.ethz.ch Subject: [R]
    >> Memory size
    >> 
    >> 
    >> 
    >> Daer all,
    >> 
    >> I have the problem. I could not run the regression,
    >> because I have always the warning message
    >> "memory.size". from the help file I learned that it is
    >> possible to increase the memory size, but I did not
    >> undestand how could I do it. Could you please explaine it
    >> to me. I would be very grateful for it.
    >> 
    >> 
    >> The second question: I obtained from the regression the
    >> coefficient "6.003e-3" and "0.0345e+3". What daos it
    >> mean?
    >> 
    >> 
    >> 
    >> Thanks a lot, Silika
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
    >> 

    AndyL> ------------------------------------------------------------------------------
    AndyL> Notice: This e-mail message, together with any
    AndyL> attachments, ...{{dropped}}

    AndyL> ______________________________________________
    AndyL> R-help at stat.math.ethz.ch mailing list
    AndyL> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From flom at ndri.org  Mon Jul 14 16:47:07 2003
From: flom at ndri.org (Peter Flom)
Date: Mon, 14 Jul 2003 10:47:07 -0400
Subject: [R] Problem importing an S+ file
Message-ID: <sf128a35.079@MAIL.NDRI.ORG>

Hello

I am a newbie to R, so excuse me if this is basic.

I am using R 1.7.1 on a Windows machine (and also S+ 6)

I have an S+ data frame called da.hh, which is stored in 

C:\Program Files\Insightful\splus6\users\Flom\.data

I want to read this file into R.  After reading the R Data
Import/Export file, I loaded the foreign package with library(foreign)
and then tried

read.S(C:\\Program
Files\\Insightful\\splus6\\users\\Flom\\.data("_data", "da.hh"))

I also tried it with single \

In both cases, I got "syntax error"


Thanks in advance

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From maechler at stat.math.ethz.ch  Mon Jul 14 16:49:12 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Jul 2003 16:49:12 +0200
Subject: [R] options(): "no no" {was `Subsetting a matrix'}
In-Reply-To: <XFMail.030714135035.Ted.Harding@nessie.mcc.ac.uk>
References: <20030714111057.B765B7CA831@tango.stat.unipd.it>
	<XFMail.030714135035.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <16146.49896.546026.194307@gargle.gargle.HOWL>

>>>>> "Ted" == Ted Harding <Ted.Harding at nessie.mcc.ac.uk>
>>>>>     on Mon, 14 Jul 2003 13:50:35 +0100 (BST) writes:

    Ted> On 14-Jul-03 Adelchi Azzalini wrote:
    >>> Maybe it is reasonable to propose incorporating "drop"
    >>> as one of the things you can set with "options"? :--
    >>> [...]
    >>  Perhaps this is a way-out. As usual, everything is
    >> feasible when we resort entirely on our own code.  The
    >> danger is that we might make use of some package (may be
    >> written some years ago) which assumes drop=TRUE is the
    >> default behaviour, and changing the default behaviour
    >> might (potentially) have disasterous effects.

    Ted> Indeed. However, it should be OK when embarking on a
    Ted> block or function of matrix computations, written by
    Ted> oneself, to enclose it between

    Ted>   options(drop=FALSE) ...  options(drop=TRUE)

    Ted> (unless some of the standard or "base" matrix functions
    Ted> which one might use in "..." depend on the default. How
    Ted> could one tell?).

First:  
   Yes, most if not all good S programmers I know agree that
   drop = FALSE should have been the default --- when S was created.
So you are right, here, Adelchi.

But I'm pretty confident that we, the R core team, pretty
unanimously believe that  options() that influence the result of
computations are  `X' ,  where X is in
  {"bad", "evil", "design flaw", "deserves death penalty"}   (;-)

Currently, the only official exception is the contrasts setting;
all other options don't influence computations, but ``output''
(printed and graphical) and interface to R-external tools
including `input' such as character encoding and `ts.eps' for
time-series rounding behavior.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From spencer.graves at pdf.com  Mon Jul 14 17:08:42 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 08:08:42 -0700
Subject: [R] "lean and mean" regression {was "Memory size"}
References: <3A822319EB35174CA3714066D590DCD50205C86D@usrymx25.merck.com>
	<16146.49054.105727.288002@gargle.gargle.HOWL>
Message-ID: <3F12C77A.6090207@pdf.com>

Dear Silika:

	  Do you know what makes the memory requirements so large?  Do you have 
many observations, or is it (as Martin just suggested) "several factors 
with many levels"?  If the latter, and if you have not already done 
this, I suggest you think very carefully if you want all those 
(unordered) level.  If you have many levels with only one observation 
per level, then I suggest you first just delete those observations. 
You'd get residuals == 0 for those observations, anyway, and you can 
just as well handle that part of the problem manually.  If you have many 
levels with more than 1 but still very few observations per level, 
appropriate preparation for the regression might be to convert unordered 
factor levels to an ordinal scale then to numerics and than regress on a 
low-order polynomial in the made-up scale.  That's old technology but 
can still be quite useful.

	  In some applications, science progresses like this:  Unordered 
categories get ordered then transformed to an ordinal scale, then to a 
quantitative scale.  Checking for outliers might reveal misplaced levels.

hope this helps.  spencer graves

Martin Maechler wrote:
>>>>>>"AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>>    on Mon, 14 Jul 2003 09:33:31 -0400 writes:
>>>>>
> 
>     AndyL> How *exactly* did you "run the regression" in R?
>     AndyL> There are several ways, and it can make a big
>     AndyL> difference for large data sets.  lm() would be the
>     AndyL> most expensive option.  If I'm not mistaken, lsfit()
>     AndyL> is more "lean and mean".  
> as a matter of fact, rather use  lm.fit() which is the ``work horse'' 
> of lm().  lm.fit() and lsfit() are very similar (relying on the
> same Fortran QR decomposition, but lm.fit() has now been tested 
> {by lm() usage} much more extensively.
> 
>     AndyL> You can even do it more or less by hand, by calling
>     AndyL> qr() directly.  There's also a disussion in Venables
>     AndyL> & Ripley's "S Programming" on this subject (for Splus).
> 
> Section 7.2, (actually the relevant code is not at all S-plus specific,
> 	      just the final "resources(.)" [CPU, Memory]
> 	      measuring of the solution.)
> 	      
> It's for the case of one factor with many (107!)levels and continuous
> covariates otherwise. There, one can solve without constructing
> the large matrices that all of lm(), lsfit() or lm.fit() would use.
> 
> It becomes really "interesting" if you have (several) factors
> with (many) levels...
> 
> Regards,
> Martin
> 
>     >> -----Original Message----- From: Silika Tereshchenko
>     >> [mailto:silika at access.unizh.ch] Sent: Sunday, July 13,
>     >> 2003 8:55 AM To: R-help at stat.math.ethz.ch Subject: [R]
>     >> Memory size
>     >> 
>     >> 
>     >> 
>     >> Daer all,
>     >> 
>     >> I have the problem. I could not run the regression,
>     >> because I have always the warning message
>     >> "memory.size". from the help file I learned that it is
>     >> possible to increase the memory size, but I did not
>     >> undestand how could I do it. Could you please explaine it
>     >> to me. I would be very grateful for it.
>     >> 
>     >> 
>     >> The second question: I obtained from the regression the
>     >> coefficient "6.003e-3" and "0.0345e+3". What daos it
>     >> mean?
>     >> 
>     >> 
>     >> 
>     >> Thanks a lot, Silika
>     >> 
>     >> ______________________________________________
>     >> R-help at stat.math.ethz.ch mailing list
>     >> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>     >> 
> 
>     AndyL> ------------------------------------------------------------------------------
>     AndyL> Notice: This e-mail message, together with any
>     AndyL> attachments, ...{{dropped}}
> 
>     AndyL> ______________________________________________
>     AndyL> R-help at stat.math.ethz.ch mailing list
>     AndyL> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jul 14 17:12:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 08:12:48 -0700
Subject: [R] Problem importing an S+ file
References: <sf128a35.079@MAIL.NDRI.ORG>
Message-ID: <3F12C870.7070400@pdf.com>

Have you considered using "dump" and "source"?  The "foreign" package 
functions should also work, but I have to set priorities on the things I 
do that consume time.

hope this helps.  spencer graves

Peter Flom wrote:
> Hello
> 
> I am a newbie to R, so excuse me if this is basic.
> 
> I am using R 1.7.1 on a Windows machine (and also S+ 6)
> 
> I have an S+ data frame called da.hh, which is stored in 
> 
> C:\Program Files\Insightful\splus6\users\Flom\.data
> 
> I want to read this file into R.  After reading the R Data
> Import/Export file, I loaded the foreign package with library(foreign)
> and then tried
> 
> read.S(C:\\Program
> Files\\Insightful\\splus6\\users\\Flom\\.data("_data", "da.hh"))
> 
> I also tried it with single \
> 
> In both cases, I got "syntax error"
> 
> 
> Thanks in advance
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From roger at ysidro.econ.uiuc.edu  Mon Jul 14 17:41:44 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Mon, 14 Jul 2003 10:41:44 -0500 (CDT)
Subject: [R] "lean and mean" regression {was "Memory size"}
In-Reply-To: <16146.49054.105727.288002@gargle.gargle.HOWL>
Message-ID: <Pine.SOL.4.30.0307140959240.1268-100000@ysidro.econ.uiuc.edu>

If you can get model.matrix to make an X matrix for you, then you could
try to use slm in SparseM to estimate the model.  Once again, I would
make a plea that it would be nice to have a version of model.matrix that
returned a sparse form of X, since there are bound to be problems for
which X itself creates memory problems even before one tries to hit
it with the QR hammer.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Mon, 14 Jul 2003, Martin Maechler wrote:

> >>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
> >>>>>     on Mon, 14 Jul 2003 09:33:31 -0400 writes:
>
>     AndyL> How *exactly* did you "run the regression" in R?
>     AndyL> There are several ways, and it can make a big
>     AndyL> difference for large data sets.  lm() would be the
>     AndyL> most expensive option.  If I'm not mistaken, lsfit()
>     AndyL> is more "lean and mean".
> as a matter of fact, rather use  lm.fit() which is the ``work horse''
> of lm().  lm.fit() and lsfit() are very similar (relying on the
> same Fortran QR decomposition, but lm.fit() has now been tested
> {by lm() usage} much more extensively.
>
>     AndyL> You can even do it more or less by hand, by calling
>     AndyL> qr() directly.  There's also a disussion in Venables
>     AndyL> & Ripley's "S Programming" on this subject (for Splus).
>
> Section 7.2, (actually the relevant code is not at all S-plus specific,
> 	      just the final "resources(.)" [CPU, Memory]
> 	      measuring of the solution.)
>
> It's for the case of one factor with many (107!)levels and continuous
> covariates otherwise. There, one can solve without constructing
> the large matrices that all of lm(), lsfit() or lm.fit() would use.
>
> It becomes really "interesting" if you have (several) factors
> with (many) levels...
>
> Regards,
> Martin
>
>     >> -----Original Message----- From: Silika Tereshchenko
>     >> [mailto:silika at access.unizh.ch] Sent: Sunday, July 13,
>     >> 2003 8:55 AM To: R-help at stat.math.ethz.ch Subject: [R]
>     >> Memory size
>     >>
>     >>
>     >>
>     >> Daer all,
>     >>
>     >> I have the problem. I could not run the regression,
>     >> because I have always the warning message
>     >> "memory.size". from the help file I learned that it is
>     >> possible to increase the memory size, but I did not
>     >> undestand how could I do it. Could you please explaine it
>     >> to me. I would be very grateful for it.
>     >>
>     >>
>     >> The second question: I obtained from the regression the
>     >> coefficient "6.003e-3" and "0.0345e+3". What daos it
>     >> mean?
>     >>
>     >>
>     >>
>     >> Thanks a lot, Silika
>     >>
>     >> ______________________________________________
>     >> R-help at stat.math.ethz.ch mailing list
>     >> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>     >>
>
>     AndyL> ------------------------------------------------------------------------------
>     AndyL> Notice: This e-mail message, together with any
>     AndyL> attachments, ...{{dropped}}
>
>     AndyL> ______________________________________________
>     AndyL> R-help at stat.math.ethz.ch mailing list
>     AndyL> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From J.Illian at abertay.ac.uk  Mon Jul 14 17:40:02 2003
From: J.Illian at abertay.ac.uk (J.Illian@abertay.ac.uk)
Date: Mon, 14 Jul 2003 16:40:02 +0100
Subject: [R] bootstrapping the lme model
Message-ID: <F934BF2710426B44833B8677AF47F2467489A5@mail3.tay.ac.uk>

Dear all,
I have a data set o which I'd like to fit lme model. There are three factors
one of whoich is nested. This should be easy to do using lme in R, but the
problem ist that the data is highly non-normal. I was thinking about
bootstrapping the distribution but don't have much experience of doing this
in R and most references I find don't seem to go beyond the
"two-sample-t-test" setting. 

Any suggestions are very welcome.

Thanks

Janine

------------------------------------------
Janine Illian
lecturer in statistics
SIMBIOS
School of Computing and Advanced Technologies
University of Abertay Dundee
Bell Street
Dundee, DD1 1HG 
Scotland, UK
Tel: +44-(0)1382-308488
Fax: +44-(0)1382-308537



From borgulya at gyer2.sote.hu  Mon Jul 14 17:49:33 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-2?Q?BORGULYA_G=E1bor?=)
Date: Mon, 14 Jul 2003 17:49:33 +0200
Subject: [R] install problem
Message-ID: <3F12D10D.8010801@gyer2.sote.hu>

Dear R experts,

I am in transition to the R system from other statistical systems I was 
using (SPSS, Statistica, etc.). I have used R for a few months on my 
Windows PC, but now I would like to install R on a Linux box with SuSE 8.1.

I downloaded the RPM for SuSE 8.1 from the CRAN mirror.
But when I run the INSTALL script using the midnight commander it gives 
the following error message:

# /usr/lib/mc/extfs/rpm run /tmp/R-base-1.7.1-1.i386.rpm INSTALL
Installing "/tmp/R-base-1.7.1-1.i386.rpm"
error: failed dependencies:
         libg2c.so.0 is needed by R-base-1.7.1-1

Could anyone tell me how to go on, where to find the missing 
file/package and what to do with it?

Thank you

G?bor

-- 
Gabor BORGULYA MD MSc
Semmelweis University of Budapest, 2nd Dept of Paediatrics
Hungarian Paediatric Cancer Registry
phone: +36 - 1 - 4591500 / 2834



From mkondrin at hppi.troitsk.ru  Tue Jul 15 04:53:19 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 14 Jul 2003 19:53:19 -0700
Subject: [R] Disgrace - R-package for interactive plotting.
Message-ID: <3F136C9F.9080301@hppi.troitsk.ru>

Hello!
I have just finished writing some script to do interactive plotting with 
grid package (thanks Paul Murrell). One can change plots attributes, 
zoom-in region of interest on the fly, shuffle axes on page etc. It also 
has nice GTK interface (thanks Duncan Temple Lang). It is not as 
powerful as standard R graphics, but may be useful to do every-day 
plotting or for novices and/or people addicted to Matlab5.0-style 
graphing (me for example).
There is no Rd or even README files for it, but some information in bad 
English about it as well as its sources may be found here:
http://www.hppi.toitsk.ru/Kondrin/disgrace.htm
Any suggestions,bug-reports,flames and so on are welcome.



From p.dalgaard at biostat.ku.dk  Mon Jul 14 17:58:24 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 14 Jul 2003 15:58:24 -0000
Subject: [R] install problem
In-Reply-To: <3F12D10D.8010801@gyer2.sote.hu>
References: <3F12D10D.8010801@gyer2.sote.hu>
Message-ID: <x2fzl99jy8.fsf@biostat.ku.dk>

BORGULYA G?bor <borgulya at gyer2.sote.hu> writes:

> Dear R experts,
> 
> I am in transition to the R system from other statistical systems I
> was using (SPSS, Statistica, etc.). I have used R for a few months on
> my Windows PC, but now I would like to install R on a Linux box with
> SuSE 8.1.
> 
> I downloaded the RPM for SuSE 8.1 from the CRAN mirror.
> But when I run the INSTALL script using the midnight commander it
> gives the following error message:
> 
> # /usr/lib/mc/extfs/rpm run /tmp/R-base-1.7.1-1.i386.rpm INSTALL
> Installing "/tmp/R-base-1.7.1-1.i386.rpm"
> error: failed dependencies:
>          libg2c.so.0 is needed by R-base-1.7.1-1
> 
> Could anyone tell me how to go on, where to find the missing
> file/package and what to do with it?

I only have a SuSE 8.0 to look at, and that only has a static libg2c,
but

$ rpm -qf /usr/lib/gcc-lib/i486-suse-linux/2.95.3/libg2c.a
g77-2.95.3-216

so a good guess is that you need to install g77 from the SuSE 8.1
install disks.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mkondrin at hppi.troitsk.ru  Tue Jul 15 05:01:39 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 14 Jul 2003 20:01:39 -0700
Subject: [R] Disgrace - R-package for interactive plotting.
In-Reply-To: <3F136C9F.9080301@hppi.troitsk.ru>
References: <3F136C9F.9080301@hppi.troitsk.ru>
Message-ID: <3F136E93.3040004@hppi.troitsk.ru>

M.Kondrin wrote:
> Hello!
> I have just finished writing some script to do interactive plotting with 
> grid package (thanks Paul Murrell). One can change plots attributes, 
> zoom-in region of interest on the fly, shuffle axes on page etc. It also 
> has nice GTK interface (thanks Duncan Temple Lang). It is not as 
> powerful as standard R graphics, but may be useful to do every-day 
> plotting or for novices and/or people addicted to Matlab5.0-style 
> graphing (me for example).
> There is no Rd or even README files for it, but some information in bad 
> English about it as well as its sources may be found here:
> http://www.hppi.toitsk.ru/Kondrin/disgrace.htm
> Any suggestions,bug-reports,flames and so on are welcome.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

Sorry - wrong http
http://www.hppi.troitsk.ru/Kondrin/disgrace.htm



From borgulya at gyer2.sote.hu  Mon Jul 14 18:11:37 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Mon, 14 Jul 2003 18:11:37 +0200
Subject: [R] install problem
In-Reply-To: <x2fzl99jy8.fsf@biostat.ku.dk>
References: <3F12D10D.8010801@gyer2.sote.hu> <x2fzl99jy8.fsf@biostat.ku.dk>
Message-ID: <3F12D639.4060303@gyer2.sote.hu>

Peter Dalgaard BSA ?rta:
> BORGULYA G?bor <borgulya at gyer2.sote.hu> writes:
>>But when I run the INSTALL script using the midnight commander it
>>gives the following error message:
>>
>># /usr/lib/mc/extfs/rpm run /tmp/R-base-1.7.1-1.i386.rpm INSTALL
>>Installing "/tmp/R-base-1.7.1-1.i386.rpm"
>>error: failed dependencies:
>>         libg2c.so.0 is needed by R-base-1.7.1-1
>>
> 
> $ rpm -qf /usr/lib/gcc-lib/i486-suse-linux/2.95.3/libg2c.a
> g77-2.95.3-216
> 
> so a good guess is that you need to install g77 from the SuSE 8.1
> install disks.

Thank you, installing g77 solved the problem! R is running !)
G?bor



From rpeng at stat.ucla.edu  Mon Jul 14 18:22:22 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 14 Jul 2003 09:22:22 -0700
Subject: [R] methods help and glmmPQL
In-Reply-To: <3.0.6.32.20030714125123.028a7158@pop-server.ucl.ac.uk>
References: <3.0.6.32.20030714125123.028a7158@pop-server.ucl.ac.uk>
Message-ID: <3F12D8BE.4040109@stat.ucla.edu>

I think with the introduction of namespaces in R version 1.7.0 many of 
the methods in packages were hidden from the user.  That's why (for 
example) VarCorr.lme does not show up.  methods(class =) only shows 
user-visible methods.  Notice the following:

>  methods(VarCorr)
[1] "VarCorr.lme"       "VarCorr.pdBlocked" "VarCorr.pdMat"   
>  VarCorr.lme
Error: Object "VarCorr.lme" not found
>  methods(class = "lme")
[1] "anova.lme"    "plot.lme"     "simulate.lme" "vcov.lme"   
>  vcov.lme
function (object, ...)
object$varFix
<environment: namespace:base>
>

For some reason, those other methods are not hidden from the user. 

-roger


Federico Calboli wrote:

>Dear All,
>
>I would like to ask you to help me with my memeory. I remember using some
>function that would list all the possible methods I could apply to an
>object. Say, if I had an object of 
>
>class=lme, 
>
>it would tell me that that I could do stuff like 
>
>qqnorm(myobjct), or VarCorr(myobject). In general, a very complete list.
>
>I though this list of all possible methods would pop out by typing 
>
>methods(class = lme) or, methods(class = whatever class my object is),
>
>but, if I type: methods(class = lme), I get:
>
>"anova.lme"    "plot.lme"     "simulate.lme" "vcov.lme"
>
>nowhere as complete list as I would expect (no mention of VarCorr, for
>instance)
>
>So, what was the magical command I used to get a list of all the possible
>ways of skinning an object? 
>
>Secondly, how do I get to see the SS and MS of the fixed terms of a glmmPQL
>fit ?
>
>Regards,
>Federico Calboli
>
>=========================
>
>Federico C.F. Calboli
>
>Department of Biology
>University College London
>Room 327
>Darwin Building
>Gower Street
>London
>WClE 6BT
>
>Tel: (+44) 020 7679 4395 
>Fax (+44) 020 7679 7096
>f.calboli at ucl.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From andel at ifi.unizh.ch  Mon Jul 14 18:31:29 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Mon, 14 Jul 2003 16:31:29 -0000
Subject: [R] "which" for NA's (and FALSE's)
Message-ID: <20030714183140.1.1570.qmail@ifi.unizh.ch>

I need to know the position of the NA's in a matrix but it is resisting my 
approaches.

The problem shows already with a simple vector:

> x <- c(TRUE,NA,FALSE,TRUE)
> x
[1]  TRUE    NA FALSE  TRUE
> which(x)
[1] 1 4
> x[x==T]
[1] TRUE   NA TRUE
> x[x==NA]
[1] NA NA NA NA
> x[x=="NA"]
[1] NA
> x[x==F]
[1]    NA FALSE

How can I get just the position (2) for NA and (3) for FALSE as I do with 
"which(x)" for TRUE?

Thanks,
David



From Zhongming.Yang at cchmc.org  Mon Jul 14 18:32:28 2003
From: Zhongming.Yang at cchmc.org (Zhongming Yang)
Date: Mon, 14 Jul 2003 12:32:28 -0400
Subject: [R] Fwd: how to make exprSet
Message-ID: <sf12a2e1.091@mailx.chmcc.org>

An embedded message was scrubbed...
From: "Zhongming Yang" <Zhongming.Yang at cchmc.org>
Subject: how to make exprSet
Date: Mon, 14 Jul 2003 12:16:33 -0400
Size: 683
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030714/6e4061d3/attachment.mht

From jerome at hivnet.ubc.ca  Mon Jul 14 18:42:21 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Mon, 14 Jul 2003 09:42:21 -0700
Subject: [R] "which" for NA's (and FALSE's)
In-Reply-To: <20030714183140.1.1570.qmail@ifi.unizh.ch>
References: <20030714183140.1.1570.qmail@ifi.unizh.ch>
Message-ID: <200307141648.JAA00363@hivnet.ubc.ca>


> which(is.na(x))
[1] 2
> which(!x)
[1] 3

HTH,
Jerome

On July 14, 2003 09:31 am, David Andel wrote:
> Content-Length: 572
> Status: R
> X-Status: N
>
> I need to know the position of the NA's in a matrix but it is resisting
> my approaches.
>
> The problem shows already with a simple vector:
> > x <- c(TRUE,NA,FALSE,TRUE)
> > x
>
> [1]  TRUE    NA FALSE  TRUE
>
> > which(x)
>
> [1] 1 4
>
> > x[x==T]
>
> [1] TRUE   NA TRUE
>
> > x[x==NA]
>
> [1] NA NA NA NA
>
> > x[x=="NA"]
>
> [1] NA
>
> > x[x==F]
>
> [1]    NA FALSE
>
> How can I get just the position (2) for NA and (3) for FALSE as I do
> with "which(x)" for TRUE?
>
> Thanks,
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jul 14 18:44:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 09:44:05 -0700
Subject: [R] "which" for NA's (and FALSE's)
References: <20030714183140.1.1570.qmail@ifi.unizh.ch>
Message-ID: <3F12DDD5.8050606@pdf.com>

which(is.na(x))

hope this helps.  spencer graves

David Andel wrote:
> I need to know the position of the NA's in a matrix but it is resisting my 
> approaches.
> 
> The problem shows already with a simple vector:
> 
> 
>>x <- c(TRUE,NA,FALSE,TRUE)
>>x
> 
> [1]  TRUE    NA FALSE  TRUE
> 
>>which(x)
> 
> [1] 1 4
> 
>>x[x==T]
> 
> [1] TRUE   NA TRUE
> 
>>x[x==NA]
> 
> [1] NA NA NA NA
> 
>>x[x=="NA"]
> 
> [1] NA
> 
>>x[x==F]
> 
> [1]    NA FALSE
> 
> How can I get just the position (2) for NA and (3) for FALSE as I do with 
> "which(x)" for TRUE?
> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Mon Jul 14 18:34:42 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 09:34:42 -0700
Subject: [R] bootstrapping the lme model
References: <F934BF2710426B44833B8677AF47F2467489A5@mail3.tay.ac.uk>
Message-ID: <3F12DBA2.3020508@pdf.com>

	  The real issue is not the distribution of the responses but the 
distribution of errors assumed to be additive and normal.  Before I 
tried bootstrapping, I'd do other things first:

	  1.  What are the response variable(s)?  What do they look like on a 
normal probability plot (qqnorm)?  Might a transformation make the 
hypothesis of additive normal errors more plausible?

	  2.  What do the residuals look like in a normal probability plot?

	  I do the simple things first.  If time and money permit and the 
problem seems sufficiently important, then I investigate other 
alternatives like bootstrapping.

	  Venables and Ripley, Modern Applied Statistics with S, discuss 
bootstrapping, as does Frank Harrell, Regression Modeling Strategies.

hope this helps.
spencer graves

J.Illian at abertay.ac.uk wrote:
> Dear all,
> I have a data set o which I'd like to fit lme model. There are three factors
> one of whoich is nested. This should be easy to do using lme in R, but the
> problem ist that the data is highly non-normal. I was thinking about
> bootstrapping the distribution but don't have much experience of doing this
> in R and most references I find don't seem to go beyond the
> "two-sample-t-test" setting. 
> 
> Any suggestions are very welcome.
> 
> Thanks
> 
> Janine
> 
> ------------------------------------------
> Janine Illian
> lecturer in statistics
> SIMBIOS
> School of Computing and Advanced Technologies
> University of Abertay Dundee
> Bell Street
> Dundee, DD1 1HG 
> Scotland, UK
> Tel: +44-(0)1382-308488
> Fax: +44-(0)1382-308537
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From borgulya at gyer2.sote.hu  Mon Jul 14 19:16:54 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-2?Q?BORGULYA_G=E1bor?=)
Date: Mon, 14 Jul 2003 19:16:54 +0200
Subject: [R] special characters
Message-ID: <3F12E586.5020709@gyer2.sote.hu>

Dear R-users,

Some hours ago I successfully installed R 1.7.1 on my Linux computer 
(SuSE 8.1). Now I am trying to get my scripts work which I have written 
for R/Windows.

* While with Windows I was able to use the vowels with the accents 
present in the Hungarian language:

 > d$crude.inc<-d$esetsz?m/d$?vk?zepi.l?leksz?m*1000000
 >

the same line leads to an error on Linux:

 > d$crude.inc<-d$esetsz?m/d$?vk?zepi.l?leksz?m*1000000
Error: syntax error
 >


* This must be a difference in the behaviour of R, not the operating 
system because the following tests passed (even with a filename!) :

 > t<-"?vk?zepi.l?leksz?m"
 > t
[1] "?vk?zepi.l?leksz?m"


 > filename<-"csoportok_l?lek_?s_esetsz?mai.csv"
 > d=read.csv2(filename)
 >

The special characters are accepted inside strings, but not as names of 
variables.
Could you tell me what to set in R to make it accept these special 
characters as names?

Thank you,

G?bor

-- 
Gabor BORGULYA MD MSc
Semmelweis University of Budapest, 2nd Dept of Paediatrics
Hungarian Paediatric Cancer Registry
phone: +36 - 1 - 4591500 / 2834



From ligges at statistik.uni-dortmund.de  Mon Jul 14 19:23:40 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jul 2003 19:23:40 +0200
Subject: [R] "lean and mean" regression {was "Memory size"}
In-Reply-To: <Pine.SOL.4.30.0307140959240.1268-100000@ysidro.econ.uiuc.edu>
References: <Pine.SOL.4.30.0307140959240.1268-100000@ysidro.econ.uiuc.edu>
Message-ID: <3F12E71C.5020708@statistik.uni-dortmund.de>

Roger Koenker wrote:
> If you can get model.matrix to make an X matrix for you, then you could
> try to use slm in SparseM to estimate the model.  Once again, I would
> make a plea that it would be nice to have a version of model.matrix that
> returned a sparse form of X, since there are bound to be problems for
> which X itself creates memory problems even before one tries to hit
> it with the QR hammer.
> 
> 

[SNIP]

Dear all, before this thread grows further on:
Silika told me in a private message of the size of the problem. When I 
guess that a simple additive linear model should be applied (I still 
don't know), it is solvable by lm() with less or than 512MB RAM (tested 
on my machine), but not with 128MB on his/her Laptop ...
Thus, resizing the amount of RAM for R might solve the problem in a 
reasonable amount of time given not too much swapping occurs.

Best,
Uwe



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 19:22:01 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 18:22:01 +0100 (BST)
Subject: [R] bug?
In-Reply-To: <XFMail.030714134445.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030714182201.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jul-03 Ted Harding wrote:
> Spot on, Baz!
> 
> An intriguing example to set people is the following:
> 
> Write a program to generate a sequence of numbers x(0) ( = anything in
> the range 0 <= x <= 1, chosen by user), x(1), x(2), ... , x(n), ...
> where
>    x(n+1) = f(x(n)), ...
> and f() is defined by
>    f(x) = 2*x if 0 <= x <= 1/2
>    f(x) = 2*(1-x) if 1/2 <= x <= 1
> [...]
> Mathematically, all other numbers are not equilibria and the sequence
> is "chaotic", yet this cannot be observed either. In fact the sequence
> generated will arrive at 0 (and stay there) in at most (N+1) steps
> where N is the number of bits in the abscissa of the floating-point
> representation.

Sorry! This is inaccurate (I was thinking in terms of a zero exponent
2^0 in the FP representation of x(0) -- and by the way of course it's
"mantissa" and not "abscissa"!). Anyway, the above is true, even with a
very small x(0) = 2^(-E)*0.1... , once the multplications by 2 have taken
it up to the crossover from < 1/2 to > 1/2.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 18:22:01
------------------------------ XFMail ------------------------------



From simon at stats.gla.ac.uk  Mon Jul 14 19:47:52 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 14 Jul 2003 18:47:52 +0100 (BST)
Subject: [R] gam and step
In-Reply-To: <200307141534.28101.baliola@riseup.net>
Message-ID: <Pine.SOL.3.96.1030714184326.5128A-100000@moon.stats.gla.ac.uk>

There isn't a step.gam() in mgcv yet.... it is one of the things that I'd
like to do eventually, although it will probably be based on comparison of
GCV/UBRE scores rather than H_0 testing. 

best,
Simon

> I am looking for a step() function for GAM's.
> In the book Statistical Computing by Crawley and a removal of predictors  has 
> been done "by hand"
> 
> model <- gam(y ~s(x1) +s(x2) + s(x3))
> summary(model) 
> model2 <- gam(y ~s(x2) + s(x3)) # removal of the unsignificant variable
> #then comparing these two models if an significant increase occurs.
> anova(model, model2, test="F")
> 
> isn't there a way to drop and add variables automatically until the best model 
> is received? like in step(lm(...))? 
> Or as in grasp.step.gam() - but that doesn't work when I tried it outside 
> GRASP-R.
> 
> thanks for your help, cheers Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From p.dalgaard at biostat.ku.dk  Mon Jul 14 19:53:22 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 14 Jul 2003 17:53:22 -0000
Subject: [R] special characters
In-Reply-To: <3F12E586.5020709@gyer2.sote.hu>
References: <3F12E586.5020709@gyer2.sote.hu>
Message-ID: <x2brvx9elr.fsf@biostat.ku.dk>

BORGULYA G?bor <borgulya at gyer2.sote.hu> writes:

> Dear R-users,
> 
> Some hours ago I successfully installed R 1.7.1 on my Linux computer
> (SuSE 8.1). Now I am trying to get my scripts work which I have
> written for R/Windows.
> 
> * While with Windows I was able to use the vowels with the accents
> present in the Hungarian language:
> 
>  > d$crude.inc<-d$esetsz?m/d$?vk?zepi.l?leksz?m*1000000
>  >
> 
> the same line leads to an error on Linux:
> 
>  > d$crude.inc<-d$esetsz?m/d$?vk?zepi.l?leksz?m*1000000
> Error: syntax error
>  >

This is generally a locale problem, specifically, LC_CTYPE needs to be
set so that (AFAIR) isprint() returns true on the printable characters
in Hungarian locale. Actually, most of the iso-8859 locales will do
(Does Hungarian use iso-8859-1 or -2, BTW?), e.g. for me

> d$crude.inc<-d$esetsz?m/d$?vk?zepi.l?leksz?m*1000000
Error: Object "d" not found
> Sys.getenv("LC_CTYPE")
LC_CTYPE
 "da_DK"

(NB: no syntax error in that, just a complaint that I didn't define
d). A fair guess is that you want to set 

export LC_CTYPE=hu_HU 

in your .profile, or systemwide wherever SuSE wants you to put it. You
can also set LANG or LC_ALL, although I choose not to because it
upsets other parts of the system (error messages, collating sequence,
etc.) 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Mike.Prager at noaa.gov  Mon Jul 14 20:47:45 2003
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Mon, 14 Jul 2003 14:47:45 -0400
Subject: [R] bug?
In-Reply-To: <21011.1058167790@www13.gmx.net>
Message-ID: <5.2.1.1.2.20030714144031.02655d70@hermes.nos.noaa.gov>

At 7/14/2003 at 03:29 AM, Marc Vandemeulebroecke wrote:
>Dear R programmers,
>
>is there a sensible explanation for the following behaviour?
>
> > seq(0.7, 0.9, by=0.1) == 0.8
>[1] FALSE FALSE FALSE


As Uwe Ligges pointed out, most floating point numbers are not exactly 
representable in most bases.  Therefore, most floating-point comparisons 
for equality will not yield the common-sense results.

A reasonably short and free article that describes this in a bit more detail is

http://www.lahey.com/float.htm

As a result, a better way of doing such comparisons is something like this:

 > eps = 1e-6
 > aa = seq(0.7, 0.9, by=0.1)
 > abs(aa-0.8) < eps
[1] FALSE  TRUE FALSE

If the scales of numbers vary in a given computation, it can be better to 
compare abs((a-b)/(a+b)) to some epsilon, rather than just abs(a-b).

Hope that helps.


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From kevin.brown at mail.mcgill.ca  Mon Jul 14 21:13:37 2003
From: kevin.brown at mail.mcgill.ca (Kevin Brown)
Date: Mon, 14 Jul 2003 15:13:37 -0400
Subject: [R] Multipanel weighted regression lines.
Message-ID: <3F1300E1.6080705@mail.mcgill.ca>

Dear R-users,

I am trying to create a multipanel scatter plot which displays weighted 
regression lines for each subgroup. I am having diffuculty figuring out 
how the Trellis multipanel plotting functions work, in general. Does 
someone know where there are some beginners examples on how to create 
and use  panel.xxx functions in conjunction with the xyplot functions?

Back to the matter at hand - There are two different ways that I could 
add these lines, I think.

The first, and simplest would be to first find the coeffecients of the 
weighted regression line for each panel ( I can do this). Then I would 
add these coeffecients into a vector/matrix (i'm not sure what form of 
vector would be suitable) and add them to each panel (I do not know how 
to do this).

The second way would be to fiddle around with the function below to add 
the "weights" to the lm function call. I do not know how to change this 
"plot.regression" function to do that.

plot.regression = function(x,y)
{
panel.xyplot(x,y)
panel.abline(lm(y~x))
}

xyplot(yg~xg | g, panel="plot.regression")

Any help would be great!
Thanks,
Kevin Brown.



From mrennie at utm.utoronto.ca  Mon Jul 14 22:09:38 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 14 Jul 2003 16:09:38 -0400
Subject: [R] problem with coding for 'optim' in R
Message-ID: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030714/7bb909ba/attachment.pl

From xam72001 at yahoo.com  Mon Jul 14 22:10:44 2003
From: xam72001 at yahoo.com (max)
Date: Mon, 14 Jul 2003 13:10:44 -0700 (PDT)
Subject: [R] Loading Workspaces from R-Excel
Message-ID: <20030714201044.89914.qmail@web41905.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030714/293ef824/attachment.pl

From rpeng at stat.ucla.edu  Mon Jul 14 22:37:26 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 14 Jul 2003 13:37:26 -0700
Subject: [R] problem with coding for 'optim' in R
In-Reply-To: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
Message-ID: <3F131486.1010605@stat.ucla.edu>

It's important to remember that in R functions return whatever happens 
to be the last element of the function block, unless there is an 
explicit 'return' statement.  Your function 'f' in the second example is 
written incorrectly and will not work in 'optim'.  The last element in 
the function block is:

write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = 
NA, col.names = TRUE)

which I assume is *not* the value you want the function return.  Your 
function 'f' is returning whatever 'write.table' returns, which is 
nothing useful.  My guess is that you want your function 'f' to return 
the value 'f' defined in the function as

f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2

So this statement should be the last line of your function.  

Also, your function 'f' (still from the second output) doesn't use the value 'q' at all, so I can't see how the optimizer can optimize a function that ignores its parameters.

-roger

Michael Rennie wrote:

[snip]

>OUTPUT 2- program that doesn't work, and gets screwed up in the daily 
>iterations- cqan't recognize starting conditions for q, even though it 
>worked fine before I placed it within the 'optim' function;
>
>R : Copyright 2001, The R Development Core Team
>Version 1.4.0  (2001-12-19)
>
>R is free software and comes with ABSOLUTELY NO WARRANTY.
>You are welcome to redistribute it under certain conditions.
>Type `license()' or `licence()' for distribution details.
>
>R is a collaborative project with many contributors.
>Type `contributors()' for more information.
>
>Type `demo()' for some demos, `help()' for on-line help, or
>`help.start()' for a HTML browser interface to help.
>Type `q()' to quit R.
>
> > ####################################
> > #    perch.R                       #
> > # Hewett and Johnson bioenergetics #
> > # model combined with              #
> > # Trudel MMBM to estimate          #
> > # Consumption in perch in R code   #
> > # Execute with                     #
> > # R --vanilla < perch.R > perch.out#
> > ####################################
> >
> > #USER INPUT BELOW
> >
> > #Weight at time 0
> > Wo<- 9.2
> >
> > #Hg concentration at time 0 (ugHg/g wet weight)
> > Hgo<- 0.08
> >
> > #Weight at time t
> > Wt<- 32.2
> >
> > #Hg concentration at time t (ugHg/g wet weight)
> > Hgt<- 0.110
> >
> > #Prey methylmercury concentration (as constant)
> > Hgp<- 0.033
> >
> > #Prey caloric value (as constant)
> > Pc<- 800
> >
> > #Energy density of fish (as constant, calories)
> > Ef <- 1000
> >
> > #Maturity status, 0=immature, 1=mature
> > Mat<- 0
> >
> > #Sex, 1=male, 2=female
> > Sex<- 1
> >
> > #USER INPUT ABOVE
> >
> > #Bioenergetics parameters for perch
> > CA <- 0.25
> > CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> > CQ <- 2.3
> > CTO <- 23
> > CTM <- 28
> > Zc<- (log(CQ))*(CTM-CTO)
> > Yc<- (log(CQ))*(CTM-CTO+2)
> > Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> >
> > RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> > RB <- 0.8   #same as 1+(-0.2) see above...
> > RQ <- 2.1
> > RTO <- 28
> > RTM <- 33
> > Za <- (log(RQ))*(RTM-RTO)
> > Ya<- (log(RQ))*(RTM-RTO+2)
> > Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> >
> > S <- 0.172
> >
> > FA <- 0.158
> > FB <- -0.222
> > FG <- 0.631
> >
> > UA<- 0.0253
> > UB<- 0.58
> > UG<- -0.299
> >
> > #Mass balance model parameters
> > EA <- 0.002938
> > EB <- -0.2
> > EQ <- 0.066
> > a <- 0.8
> >
> > #Specifying sex-specific parameters
> >
> > GSI<- NULL
> >
> > if (Sex==1) GSI<-0.05 else
>+ if (Sex==2) GSI<-0.17
> >
> > # Define margin of error functions
> > #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a proportion
> > #    {
> > #    z <- qnorm(1-alpha/2)
> > #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample size
> > #    merror
> > #    }
> >
> > #Bring in temp file
> >
> > temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
>Read 366 records
> >
> > Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
> >
> > temp<- cbind (Day, jday, Temp)
> > #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> > #temp [,2]
> >
> > Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> > Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> >
> > comp<- cbind (Day, jday, Temp, Vc, Vr)
> >
> > #comp
> >
> > bio<-matrix(NA, ncol=13, nrow=length(Day))
> > W<-NULL
> > C<-NULL
> > ASMR<-NULL
> > SMR<-NULL
> > A<-NULL
> > F<-NULL
> > U<-NULL
> > SDA<-NULL
> > Gr<-NULL
> > Hg<-NULL
> > Ed<-NULL
> > GHg<-NULL
> > K<-NULL
> > Expegk<-NULL
> > EGK<-NULL
> > p<-NULL
> > ACT<-NULL
> >
> >
> > #p <- 0.558626306252032
> > #ACT <- 1.66764519286918
> >
> > q<-c(p,ACT)
> >
> > #introduce function to solve
> > f <- function (q)
>+ {
>+
>+ M<- length(Day) #number of days iterated
>+
>+ for (i in 1:M)
>+ {
>+
>+ #Bioenergetics model
>+
>+ if (Day[i]==1) W[i] <- Wo else
>+ if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else
>+ W[i] <- (W[i-1]+(Gr[i-1]/Ef))
>+ #W
>+
>+ #W<-Wo
>+
>+ C[i]<- p*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
>+
>+ ASMR[i]<- ACT*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
>+
>+ SMR[i]<- (ASMR[i]/ACT)
>+
>+ A[i]<- (ASMR[i]-SMR[i])
>+
>+ F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
>+
>+ U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
>+
>+ SDA[i]<- (S*(C[i]-F[i]))
>+
>+ Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
>+
>+ #Trudel MMBM
>+
>+ if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- 
>a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
>+
>+ Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
>+
>+ GHg[i] <- Gr[i]/Ef/W[i]
>+
>+ if (Sex==1) 
>K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M else
>+ if (Sex==2) 
>K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
>+ # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g
>+ # then express as Q times GSI gives K / M gives daily K
>+
>+ EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
>+
>+ Expegk[i] <- exp(-1*EGK[i])
>+
>+ bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>+
>+ }
>+
>+ #warnings()
>+
>+ dimnames (bio) <-list(NULL, c("W", "C", "ASMR", "SMR", "A", "F", "U", 
>"SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
>+
>+ bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>+
>+ dimnames (bioday) <-list(NULL, c("jday", "W", "C", "ASMR", "SMR", "A", 
>"F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
>+
>+ #bioday
>+
>+ Wtmod<- bioday [length(W),2]
>+ Wtmod
>+
>+ Hgtmod<- bioday [length(Hg),14]
>+ Hgtmod
>+
>+ f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2 ; f
>+
>+ q
>+
>+ #warnings()
>+
>+ write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = 
>NA, col.names = TRUE)
>+
>+ }
> >
> > #nlm(f,c(1,1))
> >
> > optim(c(1,1), f, method = "L-BFGS-B",
>+       lower = c(0, 0), upper=c(2, 10))
>Error in "[<-"(*tmp*, i, value = (W[i - 1] + (Gr[i - 1]/Ef))) :
>         nothing to replace with
>Execution halted
>
>
>Michael Rennie
>M.Sc. Candidate
>University of Toronto at Mississauga
>3359 Mississauga Rd. N.
>Mississauga, ON  L5L 1C6
>Ph: 905-828-5452  Fax: 905-828-3792
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From mrennie at utm.utoronto.ca  Mon Jul 14 22:45:12 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 14 Jul 2003 16:45:12 -0400
Subject: [R] problem with coding for 'optim' in R
In-Reply-To: <3F131486.1010605@stat.ucla.edu>
References: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
Message-ID: <5.1.0.14.0.20030714164138.00ac2a50@mail.utm.utoronto.ca>


Hi, Roger,

At 01:37 PM 7/14/03 -0700, Roger D. Peng wrote:
>It's important to remember that in R functions return whatever happens to 
>be the last element of the function block, unless there is an explicit 
>'return' statement.  Your function 'f' in the second example is written 
>incorrectly and will not work in 'optim'.  The last element in the 
>function block is:
>
>write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = NA, 
>col.names = TRUE)
>
>>which I assume is *not* the value you want the function return.  Your 
>>function 'f' is returning whatever 'write.table' returns, which is 
>>nothing useful.  My guess is that you want your function 'f' to return 
>>the value 'f' defined in the function as
>>
>>f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2
>>
>>So this statement should be the last line of your function.

This is valuable information.  Thanks very much.  I'll move things about 
and see what happens.


>>Also, your function 'f' (still from the second output) doesn't use the 
>>value 'q' at all, so I can't see how the optimizer can optimize a 
>>function that ignores its parameters.

 From what I've read and the examples I've encountered, the 'optim' 
function expects the first entry in

optim(x, f, etc....)

To be the starting point for your variable that you specify earlier in the 
loop under

f<- function (x)

If I am wrong on this, then this could be giving me problems as well.

>>-roger

Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From borgulya at gyer2.sote.hu  Mon Jul 14 23:07:27 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Mon, 14 Jul 2003 23:07:27 +0200
Subject: [R] special characters
In-Reply-To: <20030714191719.GA709@localhost>
References: <3F12E586.5020709@gyer2.sote.hu> <20030714191719.GA709@localhost>
Message-ID: <3F131B8F.7030706@gyer2.sote.hu>

Dear Tam?s and Peter,

Thank you for your suggestions. Both of you wrote about modifying the 
value of the LC_ALL environment variable (or even the LANG or LC_ALL) in 
the .bashrc file.

First I modified the .bashrc, but it didn't change the result.
I guessed that modifying .profile might have a stronger influence on the 
system, but the problem is still unsolved:

------------ .profile ----------------
export LANG=hu_HU
export LC_ALL=hu_HU
--------------------------------------

gab at gyoh:~> locale
LANG=hu_HU
LC_CTYPE="hu_HU"
LC_NUMERIC="hu_HU"
LC_TIME="hu_HU"
LC_COLLATE="hu_HU"
LC_MONETARY="hu_HU"
LC_MESSAGES="hu_HU"
LC_PAPER="hu_HU"
LC_NAME="hu_HU"
LC_ADDRESS="hu_HU"
LC_TELEPHONE="hu_HU"
LC_MEASUREMENT="hu_HU"
LC_IDENTIFICATION="hu_HU"
LC_ALL=hu_HU

gab at gyoh:~> R

R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)

 > xy
Error: Object "xy" not found
 > ??
Error: syntax error
 >

Do you or anyone have any other idea?
Thank you

G?bor



From rpeng at stat.ucla.edu  Mon Jul 14 23:26:07 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Mon, 14 Jul 2003 14:26:07 -0700
Subject: [R] problem with coding for 'optim' in R
In-Reply-To: <5.1.0.14.0.20030714164138.00ac2a50@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030714164138.00ac2a50@mail.utm.utoronto.ca>
Message-ID: <3F131FEF.1090501@stat.ucla.edu>

Heuristically, 'optim' works by changing the imputs to your function 'f' 
and tries to find a minimum of the function.  But your 'f' function does 
not actually reference the imputs that are passed in (i.e. your 'q' 
parameter).  Therefore, changing the imput values will do nothing and I 
would be surprised if 'optim' worked at all.  You appear to have another 
variable 'q' defined in the global workspace but this is not related to 
the 'q' which is passed as an argument to 'f'.  'optim' is modifying the 
value of 'q' that is passed to 'f', not the one stored in the global 
workspace.

It seems your interpretation of how 'optim' works is correct, but you 
have to rewrite 'f' so that it actually uses the arguments passed to it. 

-roger


Michael Rennie wrote:

>
> Hi, Roger,
>
> At 01:37 PM 7/14/03 -0700, Roger D. Peng wrote:
>
>> It's important to remember that in R functions return whatever 
>> happens to be the last element of the function block, unless there is 
>> an explicit 'return' statement.  Your function 'f' in the second 
>> example is written incorrectly and will not work in 'optim'.  The 
>> last element in the function block is:
>>
>> write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na 
>> = NA, col.names = TRUE)
>>
>>> which I assume is *not* the value you want the function return.  
>>> Your function 'f' is returning whatever 'write.table' returns, which 
>>> is nothing useful.  My guess is that you want your function 'f' to 
>>> return the value 'f' defined in the function as
>>>
>>> f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2
>>>
>>> So this statement should be the last line of your function.
>>
>
> This is valuable information.  Thanks very much.  I'll move things 
> about and see what happens.
>
>
>>> Also, your function 'f' (still from the second output) doesn't use 
>>> the value 'q' at all, so I can't see how the optimizer can optimize 
>>> a function that ignores its parameters.
>>
>
> From what I've read and the examples I've encountered, the 'optim' 
> function expects the first entry in
>
> optim(x, f, etc....)
>
> To be the starting point for your variable that you specify earlier in 
> the loop under
>
> f<- function (x)
>
> If I am wrong on this, then this could be giving me problems as well.
>
>>> -roger
>>
>
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
>
>



From peterm at andrew.cmu.edu  Tue Jul 15 00:03:34 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Mon, 14 Jul 2003 18:03:34 -0400
Subject: [R] Hypothesis testing after optim
Message-ID: <BB38A0F6.5997%peterm@andrew.cmu.edu>

Hi folks:  Does anyone know of a way to do (linear) hypothesis tests of
parameters after fitting a maximum-likelihood model w/ optim?  I can't seem
to find anything like a Wald test whose documentation says it applies to
optim output.  

Also, thanks again to everyone who gave me feedback on the robustness of ML
estimation in R!

Peter


********************************
********************************
Peter Muhlberger
Visiting Professor of Political Science
Institute for the Study of Information Technology and Society (InSITeS)
Carnegie Mellon University



From Ted.Harding at nessie.mcc.ac.uk  Mon Jul 14 23:16:38 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jul 2003 22:16:38 +0100 (BST)
Subject: [R] bug?
In-Reply-To: <5.2.1.1.2.20030714144031.02655d70@hermes.nos.noaa.gov>
Message-ID: <XFMail.030714221638.Ted.Harding@nessie.mcc.ac.uk>

> At 7/14/2003 at 03:29 AM, Marc Vandemeulebroecke wrote:
>Dear R programmers,
>
>is there a sensible explanation for the following behaviour?
>
> > seq(0.7, 0.9, by=0.1) == 0.8
>[1] FALSE FALSE FALSE

Yet another %**% function ...

> "%~=%"<-function(x,y){abs(x-y)<1e-15}
> seq(0.7, 0.9, by=0.1) %~=% 0.8
[1] FALSE  TRUE FALSE

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jul-03                                       Time: 22:16:38
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Tue Jul 15 00:35:58 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 15:35:58 -0700
Subject: [R] Hypothesis testing after optim
References: <BB38A0F6.5997%peterm@andrew.cmu.edu>
Message-ID: <3F13304E.4070704@pdf.com>

For nested models, 2*log(likelihood ratio) is approximately chi-square 
with degrees of freedom = the number of parameters estimated for the 
larger model that were not in the smaller model minus any additional 
constraints.  You can compute 2*log(likelihood ratio) as twice the 
difference in the log(likelihoods) at the optimum and then use pchisq to 
conver that to a significance probability.

hope this helps.  spencer graves

Peter Muhlberger wrote:
> Hi folks:  Does anyone know of a way to do (linear) hypothesis tests of
> parameters after fitting a maximum-likelihood model w/ optim?  I can't seem
> to find anything like a Wald test whose documentation says it applies to
> optim output.  
> 
> Also, thanks again to everyone who gave me feedback on the robustness of ML
> estimation in R!
> 
> Peter
> 
> 
> ********************************
> ********************************
> Peter Muhlberger
> Visiting Professor of Political Science
> Institute for the Study of Information Technology and Society (InSITeS)
> Carnegie Mellon University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mrennie at utm.utoronto.ca  Tue Jul 15 01:04:36 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 14 Jul 2003 19:04:36 -0400
Subject: [R] problem with coding for 'optim' in R
In-Reply-To: <3F131FEF.1090501@stat.ucla.edu>
References: <5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030714160416.00aaab00@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030714164138.00ac2a50@mail.utm.utoronto.ca>
	<3F131FEF.1090501@stat.ucla.edu>
Message-ID: <1058223876.3f1337047a13c@webmail.utm.utoronto.ca>


Hi,

My argument q is related to "Wtmod" and "Hgtmod" in function f (see below).  
What I attempted to do was make function f the squared difference between my 
modelled inputs and my observed endpoints of the 365-day outcomes of the 
iteration process.  The modelled outcomes 'Wtmod' and 'Hgtmod' are dependent 
upon two particular variables, p and ACT. I've tried to combine the two of 
those parameters into q by q<-(c(p, ACT)).  Thus, the function f does rely on 
the argument q, but only after performing the iterative process; I am trying to 
minimize the difference between the outcome of the iterative process and the 
observed endpoints I have specified in function f.  Because there are 365 
calculations that use p, ACT that need to happen before I get the iterative 
endpoints, I need to get to the end of this iterative process before I can 
compare my endpoints, in order to figure out how to adjust p and ACT.  Upon 
doing so, it needs to return to the beginning of the iterative process and 
attempt different starting points for p, ACT (components of q).

It seems as though this should work, since when I specify the known solution 
set for p, ACT, I appear to get a solution back for function f (It takes a hell 
of a long time to figure out though, which is strange since I have literally 
given it the answer- see OUTPUT #1). However, it does give me a wierd message 
at the end, so something is still wrong.     

But, when I try a different set of starting points (1,1), it doesn't work. 
(OUTPUT # 2), and it gives me some strange error, stating that "L-BFGS-B needs 
finite values of fn", which it didn't seem to need when I gave it the values 
for p, ACT that I know yield the correct answer.

So, if the only time I can get 'optim' to work is when I give it the actual 
solution, then it's pretty much useless, and not terribly sucessful at 
optimizing.

Any ideas?

Mike

Quoting "Roger D. Peng" <rpeng at stat.ucla.edu>:

> Heuristically, 'optim' works by changing the imputs to your function 'f' 
> and tries to find a minimum of the function.  But your 'f' function does 
> not actually reference the imputs that are passed in (i.e. your 'q' 
> parameter).  Therefore, changing the imput values will do nothing and I 
> would be surprised if 'optim' worked at all.  You appear to have another 
> variable 'q' defined in the global workspace but this is not related to 
> the 'q' which is passed as an argument to 'f'.  'optim' is modifying the 
> value of 'q' that is passed to 'f', not the one stored in the global 
> workspace.
> 
> It seems your interpretation of how 'optim' works is correct, but you 
> have to rewrite 'f' so that it actually uses the arguments passed to it. 
> 
> -roger
> 
> 

OUTPUT 1- using known solution set for p, ACT, the 'optim' function appears to 
work (but not for any other starting point, see output 2 below), and I get a 
message back, which I'm not sure what it means ("CONVERGENCE: 
REL_REDUCTION_OF_F <= FACTR*EPSMCH").


R : Copyright 2001, The R Development Core Team
Version 1.4.0  (2001-12-19)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

> ####################################
> #    perch.R                       #
> # Hewett and Johnson bioenergetics #
> # model combined with              #
> # Trudel MMBM to estimate          #
> # Consumption in perch in R code   # 
> # Execute with                     #
> # R --vanilla < perch.R > perch.out#
> ####################################
> 
> #USER INPUT BELOW
> 
> #Weight at time 0
> Wo<- 9.2
> 
> #Hg concentration at time 0 (ugHg/g wet weight)
> Hgo<- 0.08 
> 
> #Weight at time t
> Wt<- 32.2
> 
> #Hg concentration at time t (ugHg/g wet weight) 
> Hgt<- 0.110
> 
> #Prey methylmercury concentration (as constant)
> Hgp<- 0.033
> 
> #Prey caloric value (as constant)
> Pc<- 800
> 
> #Energy density of fish (as constant, calories)
> Ef <- 1000
> 
> #Maturity status, 0=immature, 1=mature
> Mat<- 0
> 
> #Sex, 1=male, 2=female
> Sex<- 1
> 
> #USER INPUT ABOVE
> 
> #Bioenergetics parameters for perch
> CA <- 0.25
> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> CQ <- 2.3
> CTO <- 23
> CTM <- 28
> Zc<- (log(CQ))*(CTM-CTO)
> Yc<- (log(CQ))*(CTM-CTO+2)
> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> 
> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> RB <- 0.8   #same as 1+(-0.2) see above...
> RQ <- 2.1
> RTO <- 28
> RTM <- 33
> Za <- (log(RQ))*(RTM-RTO)
> Ya<- (log(RQ))*(RTM-RTO+2)
> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> 
> S <- 0.172
> 
> FA <- 0.158
> FB <- -0.222
> FG <- 0.631
> 
> UA<- 0.0253
> UB<- 0.58
> UG<- -0.299
> 
> #Mass balance model parameters
> EA <- 0.002938
> EB <- -0.2
> EQ <- 0.066
> a <- 0.8
> 
> #Specifying sex-specific parameters
> 
> GSI<- NULL
> 
> if (Sex==1) GSI<-0.05 else 
+ if (Sex==2) GSI<-0.17 
> 
> # Define margin of error functions
> #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a proportion
> #    {
> #    z <- qnorm(1-alpha/2)
> #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample size
> #    merror
> #    }
> 
> #Bring in temp file
> 
> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
Read 366 records
> 
> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ; 
> 
> temp<- cbind (Day, jday, Temp)
> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> #temp [,2]
> 
> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> 
> comp<- cbind (Day, jday, Temp, Vc, Vr)
> 
> #comp
> 
> bio<-matrix(NA, ncol=13, nrow=length(Day))
> W<-NULL
> C<-NULL
> ASMR<-NULL
> SMR<-NULL
> A<-NULL
> F<-NULL
> U<-NULL
> SDA<-NULL
> Gr<-NULL
> Hg<-NULL
> Ed<-NULL
> GHg<-NULL
> K<-NULL
> Expegk<-NULL
> EGK<-NULL
> p<-NULL
> ACT<-NULL
> 
> 
> p <- 0.558626306252032 
> ACT <- 1.66764519286918
> 
> q<-c(p,ACT)
> 
> #specify sttarting values
> #q0<-c(p = 1, ACT = 1)
> 
> #introduce function to solve
> f <- function (q)
+ {
+ 
+ 
+ M<- length(Day) #number of days iterated
+ 
+ for (i in 1:M)
+ {
+ 
+ #Bioenergetics model
+ 
+ if (Day[i]==1) W[i] <- Wo else
+ if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else 
+ W[i] <- (W[i-1]+(Gr[i-1]/Ef))
+ #W
+ 
+ #W<-Wo
+ 
+ C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
+ 
+ ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
+ 
+ SMR[i]<- (ASMR[i]/q[2])
+ 
+ A[i]<- (ASMR[i]-SMR[i])
+ 
+ F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
+ 
+ U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
+ 
+ SDA[i]<- (S*(C[i]-F[i]))
+ 
+ Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
+ 
+ #Trudel MMBM
+ 
+ if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*
(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
+ 
+ Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
+ 
+ GHg[i] <- Gr[i]/Ef/W[i]
+ 
+ if (Sex==1) K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])
*GSI)/M else
+ if (Sex==2) K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
+ # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g 
+ # then express as Q times GSI gives K / M gives daily K
+ 
+ EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
+ 
+ Expegk[i] <- exp(-1*EGK[i])
+ 
+ bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
+ 
+ }
+ 
+ #warnings()
+ 
+ dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
+ 
+ bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
+ 
+ dimnames (bioday) <-list(NULL, c
("jday", "W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK"
, "Hg"))
+ 
+ #bioday
+ 
+ Wtmod<- bioday [length(W),2]
+ Wtmod
+ 
+ Hgtmod<- bioday [length(Hg),14]
+ Hgtmod
+ 
+ q
+ 
+ f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2 ; f
+ 
+ #warnings()
+ 
+ #write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = NA, 
col.names = TRUE)
+ 
+ 
+ 
+ #nlm(f,c(1,1))
+ }
> 
> optim(q, f, method = "L-BFGS-B",
+ 	lower = c(0, 0), upper=c(2, 10))
$par
[1] 0.5586205 1.6676453

$value
[1] 8.703706e-10

$counts
function gradient 
      14       14 

$convergence
[1] 0

$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

> 
> write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = NA, 
col.names = TRUE)
Error in inherits(x, "data.frame") : Object "bioday" not found
Execution halted


OUTPUT 2- trying starting points of p, ACT = 1, the program no longer works.

R : Copyright 2001, The R Development Core Team
Version 1.4.0  (2001-12-19)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

> ####################################
> #    perch.R                       #
> # Hewett and Johnson bioenergetics #
> # model combined with              #
> # Trudel MMBM to estimate          #
> # Consumption in perch in R code   # 
> # Execute with                     #
> # R --vanilla < perch.R > perch.out#
> ####################################
> 
> #USER INPUT BELOW
> 
> #Weight at time 0
> Wo<- 9.2
> 
> #Hg concentration at time 0 (ugHg/g wet weight)
> Hgo<- 0.08 
> 
> #Weight at time t
> Wt<- 32.2
> 
> #Hg concentration at time t (ugHg/g wet weight) 
> Hgt<- 0.110
> 
> #Prey methylmercury concentration (as constant)
> Hgp<- 0.033
> 
> #Prey caloric value (as constant)
> Pc<- 800
> 
> #Energy density of fish (as constant, calories)
> Ef <- 1000
> 
> #Maturity status, 0=immature, 1=mature
> Mat<- 0
> 
> #Sex, 1=male, 2=female
> Sex<- 1
> 
> #USER INPUT ABOVE
> 
> #Bioenergetics parameters for perch
> CA <- 0.25
> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> CQ <- 2.3
> CTO <- 23
> CTM <- 28
> Zc<- (log(CQ))*(CTM-CTO)
> Yc<- (log(CQ))*(CTM-CTO+2)
> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> 
> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> RB <- 0.8   #same as 1+(-0.2) see above...
> RQ <- 2.1
> RTO <- 28
> RTM <- 33
> Za <- (log(RQ))*(RTM-RTO)
> Ya<- (log(RQ))*(RTM-RTO+2)
> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> 
> S <- 0.172
> 
> FA <- 0.158
> FB <- -0.222
> FG <- 0.631
> 
> UA<- 0.0253
> UB<- 0.58
> UG<- -0.299
> 
> #Mass balance model parameters
> EA <- 0.002938
> EB <- -0.2
> EQ <- 0.066
> a <- 0.8
> 
> #Specifying sex-specific parameters
> 
> GSI<- NULL
> 
> if (Sex==1) GSI<-0.05 else 
+ if (Sex==2) GSI<-0.17 
> 
> # Define margin of error functions
> #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a proportion
> #    {
> #    z <- qnorm(1-alpha/2)
> #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample size
> #    merror
> #    }
> 
> #Bring in temp file
> 
> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
Read 366 records
> 
> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ; 
> 
> temp<- cbind (Day, jday, Temp)
> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> #temp [,2]
> 
> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> 
> comp<- cbind (Day, jday, Temp, Vc, Vr)
> 
> #comp
> 
> bio<-matrix(NA, ncol=13, nrow=length(Day))
> W<-NULL
> C<-NULL
> ASMR<-NULL
> SMR<-NULL
> A<-NULL
> F<-NULL
> U<-NULL
> SDA<-NULL
> Gr<-NULL
> Hg<-NULL
> Ed<-NULL
> GHg<-NULL
> K<-NULL
> Expegk<-NULL
> EGK<-NULL
> p<-NULL
> ACT<-NULL
> 
> 
> p <- 1 #0.558626306252032 
> ACT <- 1 #1.66764519286918
> 
> q<-c(p,ACT)
> 
> #specify sttarting values
> #q0<-c(p = 1, ACT = 1)
> 
> #introduce function to solve
> f <- function (q)
+ {
+ 
+ 
+ M<- length(Day) #number of days iterated
+ 
+ for (i in 1:M)
+ {
+ 
+ #Bioenergetics model
+ 
+ if (Day[i]==1) W[i] <- Wo else
+ if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else 
+ W[i] <- (W[i-1]+(Gr[i-1]/Ef))
+ #W
+ 
+ #W<-Wo
+ 
+ C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
+ 
+ ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
+ 
+ SMR[i]<- (ASMR[i]/q[2])
+ 
+ A[i]<- (ASMR[i]-SMR[i])
+ 
+ F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
+ 
+ U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
+ 
+ SDA[i]<- (S*(C[i]-F[i]))
+ 
+ Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
+ 
+ #Trudel MMBM
+ 
+ if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*
(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
+ 
+ Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
+ 
+ GHg[i] <- Gr[i]/Ef/W[i]
+ 
+ if (Sex==1) K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])
*GSI)/M else
+ if (Sex==2) K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
+ # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g 
+ # then express as Q times GSI gives K / M gives daily K
+ 
+ EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
+ 
+ Expegk[i] <- exp(-1*EGK[i])
+ 
+ bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
+ 
+ }
+ 
+ #warnings()
+ 
+ dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
+ 
+ bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
+ 
+ dimnames (bioday) <-list(NULL, c
("jday", "W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK"
, "Hg"))
+ 
+ #bioday
+ 
+ Wtmod<- bioday [length(W),2]
+ Wtmod
+ 
+ Hgtmod<- bioday [length(Hg),14]
+ Hgtmod
+ 
+ q
+ 
+ f <- (((Wt-Wtmod)^2 + (Hgt-Hgtmod)^2)/2)^2 ; f
+ 
+ #warnings()
+ 
+ #write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = NA, 
col.names = TRUE)
+ 
+ 
+ 
+ #nlm(f,c(1,1))
+ }
> 
> optim(q, f, method = "L-BFGS-B",
+ 	lower = c(0, 0), upper=c(2, 10))
Error in optim(q, f, method = "L-BFGS-B", lower = c(0, 0), upper = c(2,  : 
	L-BFGS-B needs finite values of fn
Execution halted

-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From spencer.graves at pdf.com  Tue Jul 15 01:38:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 14 Jul 2003 16:38:05 -0700
Subject: [R] bug?
References: <XFMail.030714221638.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3F133EDD.60908@pdf.com>

	  This may be picking nits, but from "%~=%" as written, I think I would 
get 1e-50 %~=% 5e-50.  With numbers ranging from 1e-51 to 1e-48, this is 
rarely what I would want.  Change "-" to "+" in the above example, and I 
get the same result.

	  Something like the following might be more appropriate:

"%~=%"<-function(x,y){
	if(is.numeric(x) & is.numeric(y)){
	   xpy <- (x+y)
	   xpy[xpy==0] <- 4*.Machine$double.eps
	   return((abs(x-y)/xpy)<(4*.Machine$double.eps))
	 }
	 else return(x==y)
}

	  This is close but not quite there yet, I don't think.  Consider the 
following:

 > 1e-50 %~=% 2e-50
[1] FALSE
 > 0 %~=% 2e-50
[1] FALSE
 > 1e+50 %~=% 2e+50
[1] FALSE
 > 0 %~=% 2e+50
[1] FALSE
 > 11e-50 %~=% 2e-50
[1] FALSE
 > 1 %~=% (1+.Machine$double.eps)
[1] TRUE
 > 1 %~=% (1+3*.Machine$double.eps)
[1] TRUE

?????? spencer graves

(Ted Harding) wrote:
>>At 7/14/2003 at 03:29 AM, Marc Vandemeulebroecke wrote:
>>Dear R programmers,
>>
>>is there a sensible explanation for the following behaviour?
>>
>>
>>>seq(0.7, 0.9, by=0.1) == 0.8
>>
>>[1] FALSE FALSE FALSE
> 
> 
> Yet another %**% function ...
> 
> 
>>"%~=%"<-function(x,y){abs(x-y)<1e-15}
>>seq(0.7, 0.9, by=0.1) %~=% 0.8
> 
> [1] FALSE  TRUE FALSE
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 14-Jul-03                                       Time: 22:16:38
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From djw1005 at cam.ac.uk  Tue Jul 15 02:06:33 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Tue, 15 Jul 2003 01:06:33 +0100 (BST)
Subject: [R] Multipanel weighted regression lines.
In-Reply-To: <3F1300E1.6080705@mail.mcgill.ca>
Message-ID: <Pine.SOL.3.96.1030715005527.19053A-100000@libra.cus.cam.ac.uk>


Kevin Brown asks:
> Does someone know where there are some beginners examples on how to
> create and use panel.xxx functions in conjunction with the xyplot
> functions? 

I don't know of any formal resources other than the help page. I worked
out about panel functions by experimenting with functions of the form
  panel=function(...) print(...)
!

> The second way would be to fiddle around with the function below to add 
> the "weights" to the lm function call. I do not know how to change this 
> "plot.regression" function to do that.

I would use the subscripts=TRUE argument to xyplot. Then your panel
function will receive an argument named subscripts, which indicates which
rows of your data are being plotted in this panel. For example, something
like

xyplot ( yg~xg | g, subscripts=TRUE, panel=function(x,y,subscripts,...) {
  panel.xyplot(x,y)
  thisPanel <- unique(g[subscripts])
  fit <- lm(y~x and something involving thisPanel)
  panel.abline(fit)
  })

Damon Wischik.



From kjetil at entelnet.bo  Tue Jul 15 03:26:04 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 14 Jul 2003 21:26:04 -0400
Subject: [R] qualitative response model
In-Reply-To: <XFMail.030714141422.Ted.Harding@nessie.mcc.ac.uk>
References: <20030714115522.35057.qmail@web40807.mail.yahoo.com>
Message-ID: <3F131FEC.16461.FA472A@localhost>

On 14 Jul 2003 at 14:14, Ted Harding wrote:

Another possibility could be to try MCMCprobit or MCMClogit 
in package MCMCPack on CRAN.

Kjetil Halvorsen

> On 14-Jul-03 Z Eric wrote:
> > Hi, I want to know is there other functions in R to
> > estimate qualitative response model besides multinom()
> > in library nnet, if this is the only possibility, I
> > have a question about the application:
> > for example:
> > there is three transportation choice : car, bus, subway.
> > each alternative has own characteristic variables,
> > I want to apply conditional logit model to analysis
> > the choice of three alternatives, but if some cases
> > face different choice set, i.e only car and bus are
> > available, how to use this function?
> > I am a newcomer in R, hope the simple question not to
> > bother you.
> > thanks for the help.
> 
> A totally naive idea would be to incorporate the "choice"
> set as a set of three factors CAR=0/1, BUS=0/1, SUB=0/1
> according to availability. Naively, since (e.g.) CAR=0
> would lead to zero cases observed, one might expect that
> the estimation would predict zero for these. However, in
> conjunction with the logistic model the parameter estimates
> would be improper (-> inf).
> 
> Nevertheless, there may be some mileage in an approach which
> incorporated this idea, though I'm not seeing how!
> 
> If it could be got to work, though, it should lead to a convenient
> representation of interesting interactions, e.g. how does propensity
> to use "car" change if "subway" is not available?
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 14-Jul-03                                       Time: 14:14:22
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From s195404 at student.uq.edu.au  Tue Jul 15 03:32:45 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Tue, 15 Jul 2003 01:32:45 +0000
Subject: [R] Multipanel weighted regression lines.
In-Reply-To: <3F1300E1.6080705@mail.mcgill.ca>
References: <3F1300E1.6080705@mail.mcgill.ca>
Message-ID: <1058232765.3f1359bdbbe11@my.uq.edu.au>

Dear Kevin,

If you look at the examples under ?xyplot, you'll get some idea
about using panel functions. In your case, assume your data frame
has variables x, y, w (for weight), G (for panel) and g (for sub-
group). You could plot this using something like the following
clumsy effort:

   x <- rnorm(500)
   y <- 0.9*x + sqrt(1-0.9*0.9)*rnorm(500)
   g <- sample(x=1:2, size=500, replace=TRUE)
   G <- sample(x=c("A", "B", "C"), size=500, replace=TRUE)
   w <- runif(500)
   tmp <- data.frame(x=x, y=y, w=w, g=g, G=G)
   #
   xyplot(y ~ x | G, data=tmp, groups=g, weights=w, 
          panel=function(x,y,groups,subscripts=subscripts,
                         weights,...) {
      t1 <- unique(groups)
      for (i in 1:length(t1)) {
         w <- is.element(groups[subscripts], t1[i])
         lpoints(x[w], y[w], pch=i, col=i)
         panel.abline(lm(y[w] ~ x[w], weights=(weights,
               [subscripts])[w]), lty=i, col=i)
      }
   }, key=list(space="top", columns=2, points=list(pch=1:2,
               col=1:2), text=list(paste("Group", 1:2),
               col=1:2)))
    


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Kevin Brown <kevin.brown at mail.mcgill.ca>:

> Dear R-users,
> 
> I am trying to create a multipanel scatter plot which displays
> weighted 
> regression lines for each subgroup. I am having diffuculty
> figuring out 
> how the Trellis multipanel plotting functions work, in general.
> Does 
> someone know where there are some beginners examples on how to
> create 
> and use  panel.xxx functions in conjunction with the xyplot
> functions?
> 
> Back to the matter at hand - There are two different ways that
> I could 
> add these lines, I think.
> 
> The first, and simplest would be to first find the coeffecients
> of the 
> weighted regression line for each panel ( I can do this). Then
> I would 
> add these coeffecients into a vector/matrix (i'm not sure what
> form of 
> vector would be suitable) and add them to each panel (I do not
> know how 
> to do this).
> 
> The second way would be to fiddle around with the function
> below to add 
> the "weights" to the lm function call. I do not know how to
> change this 
> "plot.regression" function to do that.
> 
> plot.regression = function(x,y)
> {
> panel.xyplot(x,y)
> panel.abline(lm(y~x))
> }
> 
> xyplot(yg~xg | g, panel="plot.regression")
> 
> Any help would be great!
> Thanks,
> Kevin Brown.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ok at cs.otago.ac.nz  Tue Jul 15 03:39:08 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 15 Jul 2003 13:39:08 +1200 (NZST)
Subject: [R] bug?
Message-ID: <200307150139.h6F1d8Jt195091@atlas.otago.ac.nz>

Marc Vandemeulebroecke <vandemem at gmx.de> asked:
	is there a sensible explanation for the following behaviour?

	> seq(0.6, 0.9, by=0.1) == 0.8
	[1] FALSE FALSE  TRUE FALSE
	> seq(0.7, 0.9, by=0.1) == 0.8
	[1] FALSE FALSE FALSE

Yes.  It's called "floating-point arithmetic".  The problem is that
only computers using decimal floating-point arithmetic can represent
0.1 exactly; computers using binary floating-point can only represent
numbers of the form (whole number) * (power of 2) {plus other stuff you
probably don't want to know about, like NaNs, which aren't relevant here}.

Let's see what you got:
    > seq(0.6, 0.9, by=0.1) - 0.8
    [1] -0.2 -0.1  0.0  0.1
    > seq(0.7, 0.9, by=0.1) - 0.8 
    [1] -1.000000e-01 -1.110223e-16  1.000000e-01
                      ^^^^^^^^^^^^^

This difference isn't 0; it's about one unit in the last place.

The best way to work around this is only to use by=x when x is a
whole number times a power of two.  For example,

    > seq(6, 9, by=1)*0.1 == 0.8
    [1] FALSE FALSE  TRUE FALSE
    > seq(7, 9, by=1)*0.1 == 0.8
    [1] FALSE  TRUE FALSE

This is the reason why DO-loops with REAL control variables are
deprecated in Fortran 90; they often give you very nasty surprises.



From ok at cs.otago.ac.nz  Tue Jul 15 05:54:53 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 15 Jul 2003 15:54:53 +1200 (NZST)
Subject: [R] R, geochemistry, ternary diagrams
Message-ID: <200307150354.h6F3srpA170664@atlas.otago.ac.nz>

I'm in e-mail contact with a geochemist who maintains a well regarded
geochemistry web site.  He's drawing diagrams either with a Turbo Pascal
program running in a DOS window or with Excel.  I'm trying to persuade
him that R would be a better choice.  Something he's particularly keen
on is ternary diagrams.  I think he is talking about ternary phase
diagrams, and if so, it looks as though it should be doable in R, but
I don't know enough about the lower levels of R graphics to do it myself
(yet).  Looking at his speed and volume requirements, R can handle them
easily, even on a machine about 1/4 of the speed he has in mind.

Are there enough geochemists using R already that he'd find like-minded
people to discuss technical issues with if he _did_ switch to R?
Is there a package somewhere already that does ternary and other
geochemistry diagrams?



From Tom.Mulholland at health.wa.gov.au  Tue Jul 15 06:13:03 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Tue, 15 Jul 2003 12:13:03 +0800
Subject: [R] R, geochemistry, ternary diagrams
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D55C30@nt207mesep.health.wa.gov.au>

Try ternaryplot in the vcd package

-----Original Message-----
From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz] 
Sent: Tuesday, 15 July 2003 11:55 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R, geochemistry, ternary diagrams


I'm in e-mail contact with a geochemist who maintains a well regarded
geochemistry web site.  He's drawing diagrams either with a Turbo Pascal
program running in a DOS window or with Excel.  I'm trying to persuade
him that R would be a better choice.  Something he's particularly keen
on is ternary diagrams.  I think he is talking about ternary phase
diagrams, and if so, it looks as though it should be doable in R, but I
don't know enough about the lower levels of R graphics to do it myself
(yet).  Looking at his speed and volume requirements, R can handle them
easily, even on a machine about 1/4 of the speed he has in mind.

Are there enough geochemists using R already that he'd find like-minded
people to discuss technical issues with if he _did_ switch to R? Is
there a package somewhere already that does ternary and other
geochemistry diagrams?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From r.darnell at uq.edu.au  Tue Jul 15 07:14:50 2003
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Tue, 15 Jul 2003 15:14:50 +1000
Subject: [R] Specifying an lme model
Message-ID: <7k6k9xrp.fsf@uq.edu.au>

I would like some advice on how if possible, to test the following

 I have subjects each measured several times. The subjects are sampled
 from 3 subpopulations (groups). The question is "Is the 
 between-subject variance the same for the three groups?"

The "null" model is 

lme0 <- lme(y~group,random=~1|subject)

I did think that the model that defined a specific between-subject
variance for each group was

update(lme0,.~., weights=varIdent(form=~1|group))

but I am not sure.

I would appreciate any help.

Ross Darnell



From cmetcs at nus.edu.sg  Tue Jul 15 08:58:10 2003
From: cmetcs at nus.edu.sg (Tan Chuen Seng)
Date: Tue, 15 Jul 2003 14:58:10 +0800
Subject: [R] Keeping track of occurrence of warning message
Message-ID: <C23CE79231BD7F458FABA63FC8509BC818B7A5@MBXSRV03.stf.nus.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/574551ae/attachment.pl

From Tor.Strand at cih.uib.no  Tue Jul 15 09:08:56 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Tue, 15 Jul 2003 09:08:56 +0200
Subject: [R] More questions about gam and plot
Message-ID: <BB397528.7289%Tor.Strand@cih.uib.no>

When I use a "gam" command
c <- gam(depvar~var1+var2+s(var3)+s(var4)........)

Followed by a plot command
plot(c,....)   

The standard errors cross in the middle of the graph, (This variable is
almost linearly related with the outcome variable) why?

When I use the "persp" command after the gam fit, it does not graph the
smooth terms but usually two of the other terms in the model, why?

Dr. Tor A Strand   
Centre for International Health
Haukeland Hospital
University of Bergen
5021 Bergen    
Norway     
Phone: (country prefix 47)
Residence:56 51 10 88, office: 55 97 49 80,
fax: 55 97 49 79, cellular:  90 97 10 86



From laurent at cbs.dtu.dk  Tue Jul 15 09:17:28 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Tue, 15 Jul 2003 09:17:28 +0200
Subject: [R] Keeping track of occurrence of warning message
In-Reply-To: <C23CE79231BD7F458FABA63FC8509BC818B7A5@MBXSRV03.stf.nus.edu.sg>
References: <C23CE79231BD7F458FABA63FC8509BC818B7A5@MBXSRV03.stf.nus.edu.sg>
Message-ID: <20030715071728.GB11221209@genome.cbs.dtu.dk>

On Tue, Jul 15, 2003 at 02:58:10PM +0800, Tan Chuen Seng wrote:
> Hi there,
> 
> I am interested if there is anyway to keep track of the occurrence of
> warning message. 
> 
> I know that warnings will only be printed out at the end of the program
> if warn=0. However I am also interested at which particular set of data
> does the warnings occur too. This is because I am running 1000 data, so
> if there are 2 or 3 data that give warnings, I would like to know which
> are the ones out of the 1000 data.
> 
> I tried using the following code in the program to indicate where the
> warning occur but was unable to get anything recorded although the
> warnings() gave me 12 messages.
> 
> track.warning<-NULL
> ....
> if(options("warn")$warn>=0){ 
> track.warning<-c(track.warning,data.no)
> }
> 
> 
> Your help is greatly appreciated. Thanks.
> 
> >From chuen seng
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


One way to do things is to have a list to store warnings has you
hit them in you loop: you can access what is the list last.warning.



Hopin' it helps,



L.



From tobias_verbeke at skynet.be  Tue Jul 15 09:33:37 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Tue, 15 Jul 2003 09:33:37 +0200
Subject: [R] R, geochemistry, ternary diagrams
In-Reply-To: <200307150354.h6F3srpA170664@atlas.otago.ac.nz>
References: <200307150354.h6F3srpA170664@atlas.otago.ac.nz>
Message-ID: <20030715093337.176ff84d.tobias_verbeke@skynet.be>


> Are there enough geochemists using R already that he'd find
> like-minded people to discuss technical issues with if he _did_
> switch to R? Is there a package somewhere already that does ternary
> and other geochemistry diagrams?

Another possibility for a ternary plot
was mentioned by Prof Ripley in

http://maths.newcastle.edu.au/~rking/R/help/02b/3637.html

> library(MASS)
> example(Skye)

gives code and an example


HTH,

Tobias



From maechler at stat.math.ethz.ch  Tue Jul 15 09:44:22 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Jul 2003 09:44:22 +0200
Subject: [R] bug?
In-Reply-To: <3F133EDD.60908@pdf.com>
References: <XFMail.030714221638.Ted.Harding@nessie.mcc.ac.uk>
	<3F133EDD.60908@pdf.com>
Message-ID: <16147.45270.466573.890354@gargle.gargle.HOWL>


Before everyone re-invents the wheel:

All current versions of the S language (including R for several
years) have the generic function   all.equal()
with a method for simple numeric vectors (but with many other methods too).

The point is that you need a *combination* of relative and
absolute difference.  Look at all.equal.numeric() if you want to
learn ..

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From runge at plan.auc.dk  Tue Jul 15 10:53:56 2003
From: runge at plan.auc.dk (Jesper Runge Madsen)
Date: Tue, 15 Jul 2003 10:53:56 +0200
Subject: [R] dbApply (R newbee)
Message-ID: <000e01c34aae$a0502e00$0501a8c0@rungef112>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/9087bbbd/attachment.pl

From simon at stats.gla.ac.uk  Tue Jul 15 12:12:14 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 15 Jul 2003 11:12:14 +0100 (BST)
Subject: [R] More questions about gam and plot
In-Reply-To: <BB397528.7289%Tor.Strand@cih.uib.no>
Message-ID: <Pine.SOL.3.96.1030715110920.28448A-100000@jupiter.stats.gla.ac.uk>

> When I use a "gam" command
> c <- gam(depvar~var1+var2+s(var3)+s(var4)........)
> 
> Followed by a plot command
> plot(c,....)   
> 
> The standard errors cross in the middle of the graph, (This variable is
> almost linearly related with the outcome variable) why?

- Smooth terms are "centred" - which means that they are constrained to
sum to zero over the covariate values. For a straight line this means that
there must be one point that is "known" exactly. 

> 
> When I use the "persp" command after the gam fit, it does not graph the
> smooth terms but usually two of the other terms in the model, why?

- By default, persp picks up the first two covariates in your model. If
you want to plot against other variables you need to tell persp this...
see ?persp.gam.

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 12:33:24 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 11:33:24 +0100 (BST)
Subject: [R] printf and friends in R?
Message-ID: <XFMail.030715113239.Ted.Harding@nessie.mcc.ac.uk>

Hi folks

Does R have anything straightforwardly resembling the commands
fprintf, sprintf, printf (derived from C, and present in octave
and matlab)?

As in   printf(format_string, ... )
where "format_string" defines the print format (including any
fixed text) and "..." is a list of variables whose values are
to be inserted into the line.

Example:

  printf("Case %d#%.2f#%.2f%.4f\n", n,x1,x2,x3 )

which would output a line like

  Case 10#3.21#7.65#0.4321

this particular case being a line in the right format for processing
by groff's 'tbl' table-formatter. In fact you could write the whole
table definition using printf:

  printf(".TS\ntab(#);\nr n n n.\n"
  for(i in (1:nrow(X))){
    printf("Case %d#%.2f#%.2f%.4f\n", n,X[i,1],X[i,2],X[i,3] )
  }
  printf(".TE\n")

Other variants could be used for similar purposes.

(If something like this is not already around in R, I suppose it
wouldn't be too difficult for me to write it; but it's worth asking
first!)

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 11:32:39
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 12:14:05 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 11:14:05 +0100 (BST)
Subject: [R] Subsetting a matrix [Again!]
In-Reply-To: <XFMail.030714105904.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030715111405.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

People's suggestion of "drop=FALSE" seemed to do the trick
(preserving "matrix" character when subestting to a row,
i.e. creating 1xk matrix).

However, I seem to have encountered a case where even this does
not work:

> mu<-c(1,2,3)
> mu<-matrix(mu,nrow=1)
> mu
     [,1] [,2] [,3]
     [1,]    1    2    3

> iX1<-c(T,F,F); iX2<- !iX1

> mu1<-mu[iX1,drop=FALSE]; mu2<-mu[iX2,drop=FALSE];
> mu1
[1] 1
> mu2
[1] 2 3

So now I still don't get my 1xk matrices, even though mu is a
matrix and I've used "drop=FALSE". Why?

(I'm getting a bit bewildered by all this, and must get it pinned
down: the code this will go into is too complicated to allow easy
debugging if the subsetting does unpredicted things.)

[BTW: Just in case anyone gets the thought that it might work if you
 matrify iX1, iX2 e.g. iX1<-matrix(iX1,nrow=1) -- well, it doesn't!]

Best wishes to all,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 11:14:05
------------------------------ XFMail ------------------------------



From borgulya at gyer2.sote.hu  Tue Jul 15 12:41:05 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Tue, 15 Jul 2003 12:41:05 +0200
Subject: [R] Hypothesis testing after optim
In-Reply-To: <BB38A0F6.5997%peterm@andrew.cmu.edu>
References: <BB38A0F6.5997%peterm@andrew.cmu.edu>
Message-ID: <3F13DA41.1010105@gyer2.sote.hu>

Hi Peter,

Here are two links that may be useful:

ML hypothesis tests:
http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_5.html

ML confidence bounds:
http://www.weibull.com/LifeDataWeb/likelihood_ratio_confidence_bounds.htm


Yours,

G?bor
-- 
Gabor BORGULYA MD MSc
Semmelweis University of Budapest, 2nd Dept of Paediatrics
Hungarian Paediatric Cancer Registry
phone: +36 - 1 - 4591500 / 2834



From zeileis at ci.tuwien.ac.at  Tue Jul 15 12:53:14 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Tue, 15 Jul 2003 12:53:14 +0200
Subject: [R] Subsetting a matrix [Again!]
In-Reply-To: <XFMail.030715111405.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030715111405.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200307151053.h6FArE1D013922@thorin.ci.tuwien.ac.at>

On Tuesday 15 July 2003 12:14, Ted Harding wrote:

> Hi Folks,
>
> People's suggestion of "drop=FALSE" seemed to do the trick
> (preserving "matrix" character when subestting to a row,
> i.e. creating 1xk matrix).
>
> However, I seem to have encountered a case where even this does
>
> not work:
> > mu<-c(1,2,3)
> > mu<-matrix(mu,nrow=1)
> > mu
>
>      [,1] [,2] [,3]
>      [1,]    1    2    3
>
> > iX1<-c(T,F,F); iX2<- !iX1
> >
> > mu1<-mu[iX1,drop=FALSE]; mu2<-mu[iX2,drop=FALSE];
> > mu1
>
> [1] 1
>
> > mu2
>
> [1] 2 3
>
> So now I still don't get my 1xk matrices, even though mu is a
> matrix and I've used "drop=FALSE". Why?

Because you are subsetting mu like a vector not like a matrix. The 
following produces the desired output:

R> mu1<-mu[,iX1,drop=FALSE]; mu2<-mu[,iX2,drop=FALSE];
R> mu1
     [,1]
[1,]    1
R> mu2
     [,1] [,2]
[1,]    2    3

Thus, iX1 and iX2 may only be used as the column index.
Z


> (I'm getting a bit bewildered by all this, and must get it pinned
> down: the code this will go into is too complicated to allow easy
> debugging if the subsetting does unpredicted things.)
>
> [BTW: Just in case anyone gets the thought that it might work if you
>  matrify iX1, iX2 e.g. iX1<-matrix(iX1,nrow=1) -- well, it doesn't!]
>
> Best wishes to all,
> Ted.
>
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 15-Jul-03                                       Time: 11:14:05
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From david.meyer at ci.tuwien.ac.at  Tue Jul 15 13:07:34 2003
From: david.meyer at ci.tuwien.ac.at (David Meyer)
Date: Tue, 15 Jul 2003 13:07:34 +0200
Subject: [R] R, geochemistry, ternary diagrams
In-Reply-To: <20030715093337.176ff84d.tobias_verbeke@skynet.be>;
	from tobias_verbeke@skynet.be on Tue, Jul 15, 2003 at 09:33:37
	+0200
References: <200307150354.h6F3srpA170664@atlas.otago.ac.nz>
	<20030715093337.176ff84d.tobias_verbeke@skynet.be>
Message-ID: <20030715110734.GA21879@boromir.ci.tuwien.ac.at>

And in package vcd function ternaryplot().

g.,
-d

On 2003.07.15 09:33, Tobias Verbeke wrote:
> 
> > Are there enough geochemists using R already that he'd find
> > like-minded people to discuss technical issues with if he _did_
> > switch to R? Is there a package somewhere already that does ternary
> > and other geochemistry diagrams?
> 
> Another possibility for a ternary plot
> was mentioned by Prof Ripley in
> 
> http://maths.newcastle.edu.au/~rking/R/help/02b/3637.html
> 
> > library(MASS)
> > example(Skye)
> 
> gives code and an example
> 
> 
> HTH,
> 
> Tobias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 13:01:02 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 12:01:02 +0100 (BST)
Subject: [R] Subsetting a matrix [Again!]
In-Reply-To: <200307151053.h6FArE1D013922@thorin.ci.tuwien.ac.at>
Message-ID: <XFMail.030715120102.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jul-03 Achim Zeileis wrote:
>> >[...]
>> > mu1<-mu[iX1,drop=FALSE]; mu2<-mu[iX2,drop=FALSE];
>> > mu1
>>
>> [1] 1
>>
>> > mu2
>>
>> [1] 2 3
>>
>> So now I still don't get my 1xk matrices, even though mu is a
>> matrix and I've used "drop=FALSE". Why?
> 
> Because you are subsetting mu like a vector not like a matrix. The 
> following produces the desired output:
> 
> R> mu1<-mu[,iX1,drop=FALSE]; mu2<-mu[,iX2,drop=FALSE];

Thanks!! (Must have some coffee).
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 12:01:02
------------------------------ XFMail ------------------------------



From e6p at bigfoot.com  Tue Jul 15 13:17:58 2003
From: e6p at bigfoot.com (DJ)
Date: Tue, 15 Jul 2003 12:17:58 +0100
Subject: [R] Multinomial Logit with multiple groups
Message-ID: <002c01c34ac2$bf7ce4d0$0100a8c0@Cybercom>

Hi,

I am inexperienced with ML and R so please be tolerant :)

I am trying to replicate the method I have seen in a paper without success.
If my understanding is correct (a big 'IF') it seems to use Multinomial
Logit on multiple groups of
various sizes, with 'nature' selecting the choice (the winner) - then uses
Maximum Likelihood to optimise the parameters to produce a model for
prediction.

I have not found any examples which use this technique. What is worse the
paper only really provides a summary of the method. So I  am stuck!

Here is an short summary extract from the paper describing the method:


    **************************************************
    Suppose horse h* is observed to win a race.
    The multinomial logit model gives:

                    exp(Vh*)
          Ph*=  ----------            for h* = 1,2,...,H.
                    H
                    'Sigma'exp(Vh)
                    h=1

    A linear-in-parameters specification leads to:

                        N
            Vh =   'Sigma' An*Zhn
                        n=1

    where Zhn=Zhn(Xh,Yn) is the measured value of attribute n for horse h in
a race.
    The 'A' values in the equation are the parameters of the stochastic
utility model that must                         be estimated from a sample
of races.

    The likelihiood function can be written:

                    j
    exp(L) = 'Product' Pjh*
                    j=1

    where j denetes a race, h* is the horse observed to win race j, and L is
the log-likelihood function.
    *********************************************

In an ideal world I would hope for the R code to solve a toy problem using
the above method.
I can provide a jpg of the paper and a dataset if required.

But really, *any* help you could give to help me get to grips with it woul
be great.

Thanks,
David



From B.Rowlingson at lancaster.ac.uk  Tue Jul 15 13:25:44 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 15 Jul 2003 12:25:44 +0100
Subject: [R] printf and friends in R?
In-Reply-To: <XFMail.030715113239.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030715113239.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3F13E4B8.5050809@lancaster.ac.uk>

(Ted Harding) wrote:

help.search("printf") reveals the sprintf function (amongst others).

> Example:
> 
>   printf("Case %d#%.2f#%.2f%.4f\n", n,x1,x2,x3 )
> 
> which would output a line like
> 
>   Case 10#3.21#7.65#0.4321

 > sprintf("Case %d#%.2f#%.2f%.4f\n", as.integer(n),x1,x2,x3 )
[1] "Case 10#3.21#7.650.4321\n"

  Note it returns a formatted string, which you can then print or cat if 
you want it output to screen. If you cat it then the \n will be a 
carriage return.


Baz



From elsawy at ysbl.york.ac.uk  Tue Jul 15 14:16:24 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Tue, 15 Jul 2003 13:16:24 +0100
Subject: [R] eigen vector sign reversal
Message-ID: <3F13F098.2462CC82@ysbl.york.ac.uk>

I've just installed R 1.7.1 under linux red hat
I noticed sign reversal of eigen vectors ,some of them not all, upon
using diag function relative to those obtained using R 1.4.1
this is gonna miss up lots of my previous scripts
I wonder if there is a way to avoid this.
best regards
karim



From gregr at rand.org  Tue Jul 15 01:08:11 2003
From: gregr at rand.org (Greg Ridgeway)
Date: Mon, 14 Jul 2003 16:08:11 -0700
Subject: [R] package announcement: Generalized Boosted Models (gbm)
Message-ID: <056e01c34a5c$cbe699c0$3f039a82@rand.org>

Generalized Boosted Models (gbm)

This package implements extensions to Y. Freund and R. Schapire's AdaBoost
algorithm and J. Friedman's gradient boosting machine (aka multivariate
adaptive regression trees, MART). It includes regression methods for least
squares, absolute loss, logistic, Poisson, Cox proportional hazards/partial
likelihood, and the AdaBoost exponential loss. It handles continuous,
nominal, ordinal covariates as well as those containing missing values. This
package also includes a preliminary out-of-bag estimator for the optimal
number of iterations, graphical tools for lower dimensional projections of
the fitted surface, and a few demos of example gbm sessions.

gbm 1.0 will soon appear on CRAN. Earlier versions have been up for a few
months and the latest includes many of the suggestions and fixes sent to me
by the early adopters.

Enjoy!

Greg

_______________________________________________________________
Greg Ridgeway, Ph.D.
Statistician
RAND
http://www.rand.org/methodology/stat/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 14:23:25 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 13:23:25 +0100 (BST)
Subject: [R] Multivariate regression method
Message-ID: <XFMail.030715132325.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,
Thanks to several people's suggestions and clarifications,
I think I have implemented a function which computes the
conditional mean and covariance matrix of a subset of the
dimensions of an MV-normal variable, given the values on the
other dimensions (these conditioning value can be presented
as a matrix, to deal with several cases at once).

The code is below, for anyone who would like to use it.
Comments will be welcome.

Two auxiliary functions ".+". and ixX are defined as well as the main
function MV.regn

Example:
  U1<-rnorm(10);U2<-rnorm(10);U3<-rnorm(10);
  X<-cbind(U2+U3.U1+U3,U1+U2); mu<-matrix(c(0,0,0),nrow=1);
  S<-matrix(c(2,1,1, 1,2,1, 1,1,2),nrow=3);
#Ex 1
  MV.regn(S,mu,X[,1,drop=FALSE],1)
#Ex 2
  MV.regn(S,mu,X[c(1,3,5,7),1:2],1,2)

==================================================================

"%.+%"<-function(X,x){sweep(X,2,x,"+")} ## Adds x to rows of X
ixX<-function(X,...){(1:ncol(X))%in%c( ... )}
MV.regn<-function(S,mu,x1,...){
### NB NB The k-variate MV variables etc are ROW vectors throughout
###       (as in nxk matrix of data on n cases with k variables observed).
### NB NB S,mu,x1 MUST be arrays (matrices): create with "drop=FALSE" 
###       or specify when entering arguments, e.g.
###       MV.regn(S,mu,X[,1,drop=FALSE],1)
### Arguments: S  is the covariance matrix of MV X
###            mu is the ROW (1xk matrix) expectation(X)
###            x1 is matrix: rows are conditioning values for selected
###               columns of X (NB if a single column make sure it's
###               a matrix).
###            ... is an indexing vector or comma-list of numbers
###               selecting the conditioning columns of X for the
###               conditioning variable X1 (implies complementary set of
###               columns of X for the variable X2 whose conditional
###               distribution (X2 | X1=x1) is to be found).
  iX1<-ixX(S, ... ); iX2<-!iX1;
  s11<-solve(S[iX1,iX1,drop=FALSE]); s12<-S[iX1,iX2,drop=FALSE];
  s21<-S[iX2,iX1,drop=FALSE]; s22<-S[iX2,iX2,drop=FALSE];
  mu1<-mu[,iX1,drop=FALSE]; mu2<-mu[,iX2,drop=FALSE];

  Cmu  <- (x1%.+%(-mu1))%*%s11%*%s12 %.+% mu2;
  Cvar <- s22 - s21%*%s11%*%s12;
  list(Cmu=Cmu,Cvar=Cvar,iX1=iX1,iX2=iX2)
}

=================================================================

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 13:23:25
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 14:34:54 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 13:34:54 +0100 (BST)
Subject: [R] printf and friends in R?
In-Reply-To: <3F13E4B8.5050809@lancaster.ac.uk>
Message-ID: <XFMail.030715133454.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jul-03 Barry Rowlingson wrote:
> help.search("printf") reveals the sprintf function (amongst others).
Thanks! (For some reason this drew a blank when I tried it before ... ).

All best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 13:34:54
------------------------------ XFMail ------------------------------



From B.Rowlingson at lancaster.ac.uk  Tue Jul 15 15:27:13 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 15 Jul 2003 14:27:13 +0100
Subject: [R] printf and friends in R?
In-Reply-To: <XFMail.030715133454.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030715133454.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3F140131.9080200@lancaster.ac.uk>

(Ted Harding) wrote:
> On 15-Jul-03 Barry Rowlingson wrote:
> 
>>help.search("printf") reveals the sprintf function (amongst others).
> 
> Thanks! (For some reason this drew a blank when I tried it before ... ).

  perhaps because help.search does approximate matching and found 
everything with 'print' in as well? I get about 12 screens of matches to 
that, but because I knew sprintf was in there _somewhere_ I found it.

  You can turn off approximate matching with:

    help.search("printf",agrep=F)

  which gives the one true match.

Baz



From vincent.stoliaroff at sgcib.com  Tue Jul 15 15:31:27 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Tue, 15 Jul 2003 15:31:27 +0200
Subject: [R] clearing some part of a graph
Message-ID: <OFD0E63799.2377FBF4-ONC1256D64.0049C283@ges.marc.societe-generale.fr>

Hi R lovers

2 questions:

1) I'd like to know how to clean the title, the sub title or the labels of
a graph. I know how to redefine it with the function title() but it
overwrites the previous title and do not replace it

2) How could I clear a whole plot (for example in a multiple figure
environment)

thanks for your help
vincent






******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From vincent.stoliaroff at sgcib.com  Tue Jul 15 15:36:08 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Tue, 15 Jul 2003 15:36:08 +0200
Subject: [R] function acf in package ts
Message-ID: <OF8FCBA396.CECC9627-ONC1256D64.00495113@ges.marc.societe-generale.fr>

Hi R lovers!



I'd like to know if the 2 blue lines in the graph produced by the function
acf in the package ts represents the level for the test of significance of
the autocorrelation

thanks for help
vincent




******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From dj at research.bell-labs.com  Tue Jul 15 15:36:35 2003
From: dj at research.bell-labs.com (David James)
Date: Tue, 15 Jul 2003 09:36:35 -0400
Subject: [R] dbApply (R newbee)
In-Reply-To: <000e01c34aae$a0502e00$0501a8c0@rungef112>;
	from runge@plan.auc.dk on Tue, Jul 15, 2003 at 10:53:56AM +0200
References: <000e01c34aae$a0502e00$0501a8c0@rungef112>
Message-ID: <20030715093635.A11409@jessie.research.bell-labs.com>

Hello Jesper,

My mistake.  I should have replaced the call to dbGetConnection(res)
with as(res, "MySQLConnection").  Thus, the easiest workaround is
for you to define the following trivial function

   dbGetConnection <- function(res) as(res, "MySQLConnection")

and re-run your example.

Thanks for the report.

--
David


Jesper Runge Madsen wrote:
> I am trying to use R interfaced with MySQL. Present goal is that R should
> calculate the 85% quantile of AvgSpeed for each LinieID. Looking through
> documentation of the RMySQL Package, I guessed that dbApply would do the
> trick due to this example
> 
>  
> 
> ## compute quanitiles for each network agent
> con <- dbConnect(MySQL(), group="vitalAnalysis")
> res <- dbSendQuery(con, 
              "select Agent, ip_addr, DATA from pseudo_data order by Agent")
> out <- dbApply(res, INDEX = "Agent", 
>         FUN = function(x, grp) quantile(x$DATA, names=FALSE))
>  
> 
> But when I try I get this:
> 
>  
> 
> > con = dbConnect(MySQL(),group="Speed")
> 
> > res <- dbSendQuery(con, "Select LinieID, AvgSpeed from speedlinietur order
> by LinieID, AvgSpeed")
> 
> > out <- dbApply(res, INDEX = "LinieID",FUN = function(x, grp)
> quantile(x$AvgSpeed, names=FALSE))
> 
> Error in mysqlDBApply(res, ...) : couldn't find function "dbGetConnection"
> 
> >
> 
>  
> 
> I saw in an earlier posting that:
> 
> There has been a change in function names in the new RMySQL 0.5-0 
> version (also in the new ROracle and RSQLite). The reason is that 
> the R-SIG-DB has agreed on a common interface to all databases, 
> and as part of this new common interface, most functions have been 
> renamed. The following simple name mapping may help you upgrade 
> existing code to the new DBI: 
> 
> pre-DBI DBI 0.1-4 
> 
> 
..
> 
> dbGetConnection = getConnection
> 
> 
..
> 
>  
> 
>  
> 
> when I do this I get this error message
> 
> > dbGetConnection = getConnection
> 
> > out <- dbApply(res, INDEX = "LinieID",FUN = function(x, grp)
> quantile(x$AvgSpeed, names=FALSE))
> 
> Error in if (what %in% set) structure(what, class = "connection") else NULL
> : 
> 
>         argument is of length zero
> 
> >
> 
>  
> 
> Can anyone please help me
> 
>  
> 
>  
> 
>  
> 
>  
> 
> Jesper Runge Madsen
> 
> Ph.D.
> 
> Trafikforskningsgruppen
> 
> Institut for Samfundsudvikling og Planl?gning
> 
> Aalborg Universitet
> 
>  <mailto:runge at plan.auc.dk> runge at plan.auc.dk
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636



From MSchwartz at medanalytics.com  Tue Jul 15 15:46:57 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 15 Jul 2003 13:46:57 -0000
Subject: [R] printf and friends in R?
In-Reply-To: <3F140131.9080200@lancaster.ac.uk>
References: <XFMail.030715133454.Ted.Harding@nessie.mcc.ac.uk>
	<3F140131.9080200@lancaster.ac.uk>
Message-ID: <1058276810.23869.72.camel@localhost>

On Tue, 2003-07-15 at 08:27, Barry Rowlingson wrote:
> (Ted Harding) wrote:
> > On 15-Jul-03 Barry Rowlingson wrote:
> > 
> >>help.search("printf") reveals the sprintf function (amongst others).
> > 
> > Thanks! (For some reason this drew a blank when I tried it before ... ).
> 
>   perhaps because help.search does approximate matching and found 
> everything with 'print' in as well? I get about 12 screens of matches to 
> that, but because I knew sprintf was in there _somewhere_ I found it.
> 
>   You can turn off approximate matching with:
> 
>     help.search("printf",agrep=F)
> 
>   which gives the one true match.
> 
> Baz


Yet another option, which I forget about myself sometimes, is apropos().

For example:

> apropos("printf")
[1] "sprintf"

See ?apropos for more info.

HTH,

Marc Schwartz



From th50 at leicester.ac.uk  Tue Jul 15 15:53:15 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 15 Jul 2003 14:53:15 +0100
Subject: [R] clearing some part of a graph
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B676@saffron.cfs.le.ac.uk>

Dear Vincent,

You can't clear a title only. The idea is to plot without one, like

plot(...,main="",sub="",xlab="",ylab="")

and add the title by hand (as you mentioned).

Moreover, AFAIK you can't clear one plot in a multiple figure environment,
since you can only clear the graphics device (not the plot),
but you can put a new plot onto a specific part of your m.f.e. using

par(mfg=c(i,j))

see ?par and "An Introduction to R" for more details.
(I have to admit that I am unsure whether that will remove a plot there,
or create the new one on top of it - hiding the old one. That matters
with respect to dev.copy() I assume.)

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: vincent.stoliaroff at sgcib.com 
> [mailto:vincent.stoliaroff at sgcib.com]
> Sent: 15 July 2003 14:31
> To: r-help at stat.math.ethz.ch
> Subject: [R] clearing some part of a graph
> 
> 
> Hi R lovers
> 
> 2 questions:
> 
> 1) I'd like to know how to clean the title, the sub title or 
> the labels of
> a graph. I know how to redefine it with the function title() but it
> overwrites the previous title and do not replace it
> 
> 2) How could I clear a whole plot (for example in a multiple figure
> environment)
> 
> thanks for your help
> vincent
> 
> 
> 
> 
> 
> 
> ******************************************************************
> The sender's email address has changed to 
> firstname.lastname@ sgcib.com. You may want to update your 
> personal address book. Please see http://www.sgcib.com for more 
> information.
>                                **
> This message and any attachments (the "message") are 
> confide...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kirschenmichel at gmx.de  Tue Jul 15 15:55:36 2003
From: kirschenmichel at gmx.de (michael kirschbaum)
Date: Tue, 15 Jul 2003 15:55:36 +0200 (MEST)
Subject: [R] (no subject)
Message-ID: <27323.1058277336@www54.gmx.net>

Hi 
I got a problem with creating a textfile:
how can I create a textfile, which has a headline like:

#data1
1 2 3 4 
5 6 7 8

the problem is, how to bind text and matrix, so that the
"read.table"-function
will ignore the text and read the numbers.

Thank you for help
Michael 

-- 
2



From vincent.stoliaroff at sgcib.com  Tue Jul 15 16:00:47 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Tue, 15 Jul 2003 16:00:47 +0200
Subject: [R] clearing some part of a graph
Message-ID: <OFA25F5B26.0429E649-ONC1256D64.004CA321@ges.marc.societe-generale.fr>


Many thanks

I don't see any other solution than what you've proposed.
I have tried the par(mfg=c(i,j,k,l))
But it overwrites rather than replaces the previous graph

take care
vincnet



|---------+---------------------------->
|         |           th50 at leicester.ac|
|         |           .uk              |
|         |                            |
|         |           07/15/03 03:53 PM|
|         |                            |
|---------+---------------------------->
  >------------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                              |
  |       To:       Vincent STOLIAROFF/fr/socgen at socgen, r-help at stat.math.ethz.ch                                                |
  |       cc:                                                                                                                    |
  |       Subject:  RE: [R] clearing some part of a graph                                                                        |
  >------------------------------------------------------------------------------------------------------------------------------|




Dear Vincent,

You can't clear a title only. The idea is to plot without one, like

plot(...,main="",sub="",xlab="",ylab="")

and add the title by hand (as you mentioned).

Moreover, AFAIK you can't clear one plot in a multiple figure environment,
since you can only clear the graphics device (not the plot),
but you can put a new plot onto a specific part of your m.f.e. using

par(mfg=c(i,j))

see ?par and "An Introduction to R" for more details.
(I have to admit that I am unsure whether that will remove a plot there,
or create the new one on top of it - hiding the old one. That matters
with respect to dev.copy() I assume.)

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: vincent.stoliaroff at sgcib.com
> [mailto:vincent.stoliaroff at sgcib.com]
> Sent: 15 July 2003 14:31
> To: r-help at stat.math.ethz.ch
> Subject: [R] clearing some part of a graph
>
>
> Hi R lovers
>
> 2 questions:
>
> 1) I'd like to know how to clean the title, the sub title or
> the labels of
> a graph. I know how to redefine it with the function title() but it
> overwrites the previous title and do not replace it
>
> 2) How could I clear a whole plot (for example in a multiple figure
> environment)
>
> thanks for your help
> vincent
>
>
>
>
>
>
> ******************************************************************
> The sender's email address has changed to
> firstname.lastname@ sgcib.com. You may want to update your
> personal address book. Please see http://www.sgcib.com for more
> information.
>                                **
> This message and any attachments (the "message") are
> confide...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>







******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From th50 at leicester.ac.uk  Tue Jul 15 16:17:21 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 15 Jul 2003 15:17:21 +0100
Subject: [R] skip first line with read.table (was: no subject)
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46E5@saffron.cfs.le.ac.uk>

Dear Michael,

There was an email thread midst June 2003 on a related issue. 

The suggestions made there will certainly help you

Have a look for thread "Programcode and data in the same textfile"
(just type it into the R Site Search, and the thread will come up).

I recall that read.table() has an argument skip which allows the 
first lines to be skipped, see ?read.table for details.

The manual "R Data Import/Export" is very useful as well.

Please provide a meaningful subject that makes searching the archives easier.

HTH

Thomas


---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: michael kirschbaum [mailto:kirschenmichel at gmx.de]
> Sent: 15 July 2003 14:56
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> 
> Hi 
> I got a problem with creating a textfile:
> how can I create a textfile, which has a headline like:
> 
> #data1
> 1 2 3 4 
> 5 6 7 8
> 
> the problem is, how to bind text and matrix, so that the
> "read.table"-function
> will ignore the text and read the numbers.
> 
> Thank you for help
> Michael 
> 
> -- 
> 2
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Tue Jul 15 16:17:02 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 15 Jul 2003 07:17:02 -0700
Subject: [R] (no subject)
References: <27323.1058277336@www54.gmx.net>
Message-ID: <3F140CDE.2030302@pdf.com>

"sink" may be the simplest but won't give you the control you may want.

For full control, have you considered "cat" and "write.table"?  Both 
have "append" arguments plus others.

hope this helps.  spencer graves

michael kirschbaum wrote:
> Hi 
> I got a problem with creating a textfile:
> how can I create a textfile, which has a headline like:
> 
> #data1
> 1 2 3 4 
> 5 6 7 8
> 
> the problem is, how to bind text and matrix, so that the
> "read.table"-function
> will ignore the text and read the numbers.
> 
> Thank you for help
> Michael 
>



From kny1116 at yahoo.com.tw  Tue Jul 15 16:25:28 2003
From: kny1116 at yahoo.com.tw (kny)
Date: Tue, 15 Jul 2003 22:25:28 +0800
Subject: [R] How to use fortrane compiler to install unix R???
Message-ID: <001101c34adc$f14a3a20$be00a8c0@kny>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/f7de041b/attachment.pl

From tobias_verbeke at skynet.be  Tue Jul 15 16:32:12 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Tue, 15 Jul 2003 16:32:12 +0200
Subject: [R] (no subject)
In-Reply-To: <27323.1058277336@www54.gmx.net>
References: <27323.1058277336@www54.gmx.net>
Message-ID: <20030715163212.648a78dc.tobias_verbeke@skynet.be>

[read text file :]

> #data1
> 1 2 3 4 
> 5 6 7 8

Is this what you mean ?

> b <- read.table("filewithyourdata", header=F, sep=" ")
> b
  V1 V2 V3 V4
1  1  2  3  4
2  5  6  7  8


HTH,

Tobias



From Anne.Olga.Piotet at omsv.vd.ch  Tue Jul 15 16:31:23 2003
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Tue, 15 Jul 2003 14:31:23 -0000
Subject: [R] How to read in data
Message-ID: <002a01c3633b$652500d0$84dad10a@RENOVA4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/57785457/attachment.pl

From Anne.Olga.Piotet at omsv.vd.ch  Tue Jul 15 16:31:54 2003
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Tue, 15 Jul 2003 14:31:54 -0000
Subject: [R] (no subject)
Message-ID: <003701c3633b$78416dc0$84dad10a@RENOVA4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/2158038c/attachment.pl

From tobias_verbeke at skynet.be  Tue Jul 15 16:40:51 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Tue, 15 Jul 2003 16:40:51 +0200
Subject: [R] (no subject)
In-Reply-To: <27323.1058277336@www54.gmx.net>
References: <27323.1058277336@www54.gmx.net>
Message-ID: <20030715164051.76d15992.tobias_verbeke@skynet.be>

Sorry Michael,
I should read more carefully.
You asked to create the file,
not to read it.

Tobias



From jmacdon at med.umich.edu  Tue Jul 15 16:56:40 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 15 Jul 2003 10:56:40 -0400
Subject: [R] How to read in data
Message-ID: <sf13de04.096@mail-02.med.umich.edu>

You probably need to change the directory to the one that contains the
text file you are reading in. This is done under the File menu.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Anne Piotet" <Anne.Olga.Piotet at omsv.vd.ch> 08/15/03 10:42AM >>>
Hello,
I'm new to R and in the process of testing it
My first question: I fail to read in my data (ANSI toto.txt file, tab
separated)
            > test <-read.table("toto.txt")
            Error in file(file, "r") : unable to open connection
            In addition: Warning message: 
            cannot open file `toto.txt'
            > test <- scan("C:\\toto.txt")
            Error in scan("C:\\toto.txt") : "scan" expected a real, got
"No_D"
            > test <-scan("test.dat")
            Error in file(file, "r") : unable to open connection
            In addition: Warning message: 
            cannot open file `toto.txt
(and no, it is not read only or locked or whatever) I use Windows
2000/XP 

second question...what are the size limits of statistical files I can
handle? I plan to analize plant datas (up to 500'000 records, from which
I will analize a restrictive set of variates ) Even when broken down by
some chracteristics, the data to analize can have 50'000-100'000
records

Well thank for the help
Anne

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mros at autan.toulouse.inra.fr  Tue Jul 15 16:58:28 2003
From: mros at autan.toulouse.inra.fr (Mathieu Ros)
Date: Tue, 15 Jul 2003 16:58:28 +0200 (MEST)
Subject: [R] How to read in data
In-Reply-To: <002a01c3633b$652500d0$84dad10a@RENOVA4>
References: <002a01c3633b$652500d0$84dad10a@RENOVA4>
Message-ID: <16148.5409.59928.239325@autan.toulouse.inra.fr>

>>>>> "AP" == Anne Piotet <Anne.Olga.Piotet at omsv.vd.ch> disait:

    AP> Hello, I'm new to R and in the process of testing it My first
    AP> question: I fail to read in my data (ANSI toto.txt file, tab
    AP> separated)
    >> test <-read.table("toto.txt")
    AP>             Error in file(file, "r") : unable to open
    AP> connection In addition: Warning message: cannot open file
    AP> `toto.txt'
    >> test <- scan("C:\\toto.txt")
    AP>             Error in scan("C:\\toto.txt") : "scan" expected a
    AP> real, got "No_D"
    >> test <-scan("test.dat")
    AP>             Error in file(file, "r") : unable to open
    AP> connection In addition: Warning message: cannot open file
    AP> `toto.txt (and no, it is not read only or locked or whatever)
    AP> I use Windows 2000/XP

I think
read.table("C:\\toto.txt",header=TRUE)
will do the job : the message you got on your first and third attempts
means that you gave a wrong path to your file.
otherwise, read the help for read.table carefully (header and skip parameters).

    AP> second question...what are the size limits of statistical
    AP> files I can handle? I plan to analize plant datas (up to
    AP> 500'000 records, from which I will analize a restrictive set
    AP> of variates ) Even when broken down by some chracteristics,
    AP> the data to analize can have 50'000-100'000 records

    AP> Well thank for the help Anne
de rien ;)

regards,

--Mathieu



From B.Rowlingson at lancaster.ac.uk  Tue Jul 15 17:05:21 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 15 Jul 2003 16:05:21 +0100
Subject: [R] How to read in data
In-Reply-To: <002a01c3633b$652500d0$84dad10a@RENOVA4>
References: <002a01c3633b$652500d0$84dad10a@RENOVA4>
Message-ID: <3F141831.4030308@lancaster.ac.uk>

Anne Piotet wrote:
> Hello,
> I'm new to R and in the process of testing it
> My first question: I fail to read in my data (ANSI toto.txt file, tab separated)
>             > test <-read.table("toto.txt")
>             Error in file(file, "r") : unable to open connection
>             In addition: Warning message: 
>             cannot open file `toto.txt'

  - that's because it didnt find the file in that location.

>             > test <- scan("C:\\toto.txt")
>             Error in scan("C:\\toto.txt") : "scan" expected a real, got "No_D"

  - that's because it did find the file, but there was the text "No_D" 
in it. scan() will only read numbers unless you tell it otherwise.

>             > test <-scan("test.dat")
>             Error in file(file, "r") : unable to open connection
>             In addition: Warning message: 
>             cannot open file `toto.txt

  again, its not looking in c:\, so it doesn't find it. Funny how 
scan("test.dat") brings up an error about "toto.txt" :)

  R has a working directory which is where scan() and read.file() will 
start looking for files without a full path - type getwd() to see where 
that is at any time.

  You didnt try the other option:

   test <- read.table("c:\\toto.txt", sep='\t')

- I give a full path to toto.txt and tell it the columns are separated 
with tabs ('\t'). You may need other options - popular ones are as.is=T 
which keeps character variables as text rather than converting to 
categorical data (factors), and head=T if the first line of the file is 
a header with column names.

  If this works, then do names(test) and summary(test) to see what 
you've got.

> second question...what are the size limits of statistical files I can handle? I plan to analize plant datas (up to 500'000 records, from which I will analize a restrictive set of variates ) Even when broken down by some chracteristics, the data to analize can have 50'000-100'000 records

  Depends - whats the size of the machine you are using (and dont say 
its a small box that fits under my monitor). How much RAM and disk space 
does it have?

Baz



From bates at stat.wisc.edu  Tue Jul 15 17:12:54 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 15 Jul 2003 15:12:54 -0000
Subject: [R] Specifying an lme model
In-Reply-To: <7k6k9xrp.fsf@uq.edu.au>
References: <7k6k9xrp.fsf@uq.edu.au>
Message-ID: <6rsmp76cxw.fsf@bates4.stat.wisc.edu>

Ross Darnell <r.darnell at uq.edu.au> writes:

> I would like some advice on how if possible, to test the following
> 
>  I have subjects each measured several times. The subjects are sampled
>  from 3 subpopulations (groups). The question is "Is the 
>  between-subject variance the same for the three groups?"
> 
> The "null" model is 
> 
> lme0 <- lme(y~group,random=~1|subject)
> 
> I did think that the model that defined a specific between-subject
> variance for each group was
> 
> update(lme0,.~., weights=varIdent(form=~1|group))
> 
> but I am not sure.

I think you have it right.  You should then compare the two fitted
models using the anova generic, which will provide a likelihood ratio
test statistic and a p-value based on a chi-squared reference
distribution.  Regard the p-value as an approximation.



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 16:41:28 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 15:41:28 +0100 (BST)
Subject: [R] How to use fortrane compiler to install unix R???
In-Reply-To: <001101c34adc$f14a3a20$be00a8c0@kny>
Message-ID: <XFMail.030715154128.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jul-03 kny wrote:
> Hi:
>   I am a R beginner,and I had a problem about installing R on
> unix(Dec-Tru64 alpha)
> my system had been installed fortrane complier,and I also
> follow manual document to do following procedure.
> ./configure
> make
> make install
> 
> but ,when I do 'make' there are something wrong,and 'make' procedure
> will be stopped,and also 'make install'.
> please help me to solve this problem.thanks.

Did you get any apparent error messages from './configure'? If you did,
this will indicate whether your system lacks something essential, or
is not properly set up for the job.

If not:
An indication of where things went wrong, and possibly of what went
wrong, should be available in the output from 'make'.

However, this is often voluminous, so I suggest you try

  make 2>somewhere/make.errors

where "somewhere" is a convenient directory to store the stuff. Then
page through this, and try to identify indications of problems.

Good luck,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 15:41:28
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Tue Jul 15 17:18:06 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 11:18:06 -0400
Subject: [R] writing data to file (was: no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD50205C882@usrymx25.merck.com>

1. Please use an informative subject line.

2. Try something like the following:

> m <- matrix(1:8, 2, 4, byrow=TRUE)
> outfile <- file("test.txt", open="w")
> writeLines("#data1", outfile)
> write(t(m), outfile, ncol=ncol(m))
> close(outfile)
> file.show("test.txt")

Consult the relevant help pages for the functions used above.

HTH,
Andy


> -----Original Message-----
> From: michael kirschbaum [mailto:kirschenmichel at gmx.de] 
> Sent: Tuesday, July 15, 2003 9:56 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> 
> Hi 
> I got a problem with creating a textfile:
> how can I create a textfile, which has a headline like:
> 
> #data1
> 1 2 3 4 
> 5 6 7 8
> 
> the problem is, how to bind text and matrix, so that the 
> "read.table"-function will ignore the text and read the numbers.
> 
> Thank you for help
> Michael 
> 
> -- 
> 2
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From andy_liaw at merck.com  Tue Jul 15 17:23:38 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 11:23:38 -0400
Subject: [R] function acf in package ts
Message-ID: <3A822319EB35174CA3714066D590DCD50205C883@usrymx25.merck.com>

>From ?plot.acf:

Note:

     The confidence interval plotted in `plot.acf' is based on an
     uncorrelated series and should be treated with appropriate
     caution.  Using `ci.type = "ma"' may be less potentially
     misleading.

Andy


> -----Original Message-----
> From: vincent.stoliaroff at sgcib.com 
> [mailto:vincent.stoliaroff at sgcib.com] 
> Sent: Tuesday, July 15, 2003 9:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] function acf in package ts
> 
> 
> Hi R lovers!
> 
> 
> 
> I'd like to know if the 2 blue lines in the graph produced by 
> the function acf in the package ts represents the level for 
> the test of significance of the autocorrelation
> 
> thanks for help
> vincent
> 
> 
> 
> 
> ******************************************************************
> The sender's email address has changed to 
> firstname.lastname@ sgcib.com. You may want to update your 
> personal address book. Please see http://www.sgcib.com for more 
> information.
>                                **
> This message and any attachments (the "message") are 
> confide...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From andy_liaw at merck.com  Tue Jul 15 17:30:27 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 11:30:27 -0400
Subject: [R] clearing some part of a graph
Message-ID: <3A822319EB35174CA3714066D590DCD50205C884@usrymx25.merck.com>

> From: vincent.stoliaroff at sgcib.com 
> 
> Many thanks
> 
> I don't see any other solution than what you've proposed.
> I have tried the par(mfg=c(i,j,k,l))
> But it overwrites rather than replaces the previous graph

Set the background color of the new plot to something like "white" so the
previous graph will be completely covered.

HTH,
Andy
 
> take care
> vincnet
> 
> Dear Vincent,
> 
> You can't clear a title only. The idea is to plot without one, like
> 
> plot(...,main="",sub="",xlab="",ylab="")
> 
> and add the title by hand (as you mentioned).
> 
> Moreover, AFAIK you can't clear one plot in a multiple figure 
> environment, since you can only clear the graphics device 
> (not the plot), but you can put a new plot onto a specific 
> part of your m.f.e. using
> 
> par(mfg=c(i,j))
> 
> see ?par and "An Introduction to R" for more details.
> (I have to admit that I am unsure whether that will remove a 
> plot there, or create the new one on top of it - hiding the 
> old one. That matters with respect to dev.copy() I assume.)
> 
> HTH
> 
> Thomas
> 
> ---
> 
> Thomas Hotz
> Research Associate in Medical Statistics
> University of Leicester
> United Kingdom
> 
> Department of Epidemiology and Public Health
> 22-28 Princess Road West
> Leicester
> LE1 6TP
> Tel +44 116 252-5410
> Fax +44 116 252-5423
> 
> Division of Medicine for the Elderly
> Department of Medicine
> The Glenfield Hospital
> Leicester
> LE3 9QP
> Tel +44 116 256-3643
> Fax +44 116 232-2976
> 
> 
> > -----Original Message-----
> > From: vincent.stoliaroff at sgcib.com 
> > [mailto:vincent.stoliaroff at sgcib.com]
> > Sent: 15 July 2003 14:31
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] clearing some part of a graph
> >
> >
> > Hi R lovers
> >
> > 2 questions:
> >
> > 1) I'd like to know how to clean the title, the sub title or the 
> > labels of a graph. I know how to redefine it with the 
> function title() 
> > but it overwrites the previous title and do not replace it
> >
> > 2) How could I clear a whole plot (for example in a multiple figure
> > environment)
> >
> > thanks for your help
> > vincent
> >
> >
> >
> >
> >
> >
> > ******************************************************************
> > The sender's email address has changed to
> > firstname.lastname@ sgcib.com. You may want to update your personal 
> > address book. Please see http://www.sgcib.com for more information.
> >                                **
> > This message and any attachments (the "message") are
> > confide...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> 
> 
> 
> 
> 
> 
> ******************************************************************
> The sender's email address has changed to 
> firstname.lastname@ sgcib.com. You may want to update your 
> personal address book. Please see http://www.sgcib.com for more 
> information.
>                                **
> This message and any attachments (the "message") are 
> confide...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From athanasia.kamariotis at epfl.ch  Tue Jul 15 17:36:24 2003
From: athanasia.kamariotis at epfl.ch (ATHANASIA KAMARIOTIS)
Date: Tue, 15 Jul 2003 17:36:24 +0200
Subject: [R] "predict"
Message-ID: <71d457520a.7520a71d45@imap.epfl.ch>

Good afternoon,

Can you please tell me how R computes : " predict(ar.x)$pred "
in :

#let x be a vector

ar.x<-ar(x)
predict(ar.x)$pred

Thank you

Bye



From andel at ifi.unizh.ch  Tue Jul 15 18:51:43 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Tue, 15 Jul 2003 16:51:43 -0000
Subject: [R] matrix manipulations
Message-ID: <Pine.GSO.4.44.0307151836170.5691-100000@igor>

Hi

cor(x,apply(x,1,sum)) gives me the correlations of each column with the sums of each row (correct me if I'm wrong, please).

What I need are the correlations of each column with the sums of each row except the entry in the given column. It seems that for any one column i I get it by doing:

cor(x[,i],apply(x[,-i],1,sum))

But I struggle to get it for all the columns. I was trying things like:

for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum))

which doesn't generate any output at all, and

> rbind(for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum)))
          [,1]
[1,] 0.1880237

outputs just the result of the very last column.

I know that it shouldn't be necessary to use for(), but I couldn't figure out a way how to do the task using e.g. apply().

How do you get the results of all columns?

Thank you,
David



From rpeng at stat.ucla.edu  Tue Jul 15 19:19:38 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Tue, 15 Jul 2003 10:19:38 -0700
Subject: [R] eigen vector sign reversal
In-Reply-To: <3F13F098.2462CC82@ysbl.york.ac.uk>
References: <3F13F098.2462CC82@ysbl.york.ac.uk>
Message-ID: <3F1437AA.6000608@stat.ucla.edu>

I think at version 1.7.0 R started using LAPACK for its eigen/svd 
routines.  I think using `eigen(x, EISPACK = TRUE)' uses the previous 
version.

-roger

Karim Elsawy wrote:

>I've just installed R 1.7.1 under linux red hat
>I noticed sign reversal of eigen vectors ,some of them not all, upon
>using diag function relative to those obtained using R 1.4.1
>this is gonna miss up lots of my previous scripts
>I wonder if there is a way to avoid this.
>best regards
>karim
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From andy_liaw at merck.com  Tue Jul 15 19:21:21 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 13:21:21 -0400
Subject: [R] matrix manipulations
Message-ID: <3A822319EB35174CA3714066D590DCD50205C886@usrymx25.merck.com>

I don't think for loop is so bad here, but if you insist on not using it,
try:

> x<-matrix(rnorm(25), 5, 5)
> sapply(1:5, function(i) cor(x[,i], rowSums(x[,-i])))
[1] -0.04179336 -0.08613796  0.48194936  0.38317629 -0.22081706

HTH,
Andy

> -----Original Message-----
> From: David Andel [mailto:andel at ifi.unizh.ch] 
> Sent: Tuesday, July 15, 2003 12:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] matrix manipulations
> 
> 
> Hi
> 
> cor(x,apply(x,1,sum)) gives me the correlations of each 
> column with the sums of each row (correct me if I'm wrong, please).
> 
> What I need are the correlations of each column with the sums 
> of each row except the entry in the given column. It seems 
> that for any one column i I get it by doing:
> 
> cor(x[,i],apply(x[,-i],1,sum))
> 
> But I struggle to get it for all the columns. I was trying 
> things like:
> 
> for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum))
> 
> which doesn't generate any output at all, and
> 
> > rbind(for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum)))
>           [,1]
> [1,] 0.1880237
> 
> outputs just the result of the very last column.
> 
> I know that it shouldn't be necessary to use for(), but I 
> couldn't figure out a way how to do the task using e.g. apply().
> 
> How do you get the results of all columns?
> 
> Thank you,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From rpeng at stat.ucla.edu  Tue Jul 15 19:27:26 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Tue, 15 Jul 2003 10:27:26 -0700
Subject: [R] (no subject)
In-Reply-To: <27323.1058277336@www54.gmx.net>
References: <27323.1058277336@www54.gmx.net>
Message-ID: <3F14397E.30400@stat.ucla.edu>

One way might be to use a connection and `writeLines'.  For example:

>  a <- matrix(1:8, byrow = TRUE, ncol = 4)
>  a
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
>  con <- file("testfile.txt", "w")
>  writeLines("#data", con)
>  write(a, con, ncol = 4)
>  close(con)
>

-roger

michael kirschbaum wrote:

>Hi 
>I got a problem with creating a textfile:
>how can I create a textfile, which has a headline like:
>
>#data1
>1 2 3 4 
>5 6 7 8
>
>the problem is, how to bind text and matrix, so that the
>"read.table"-function
>will ignore the text and read the numbers.
>
>Thank you for help
>Michael 
>
>  
>



From apjaworski at mmm.com  Tue Jul 15 19:28:10 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 15 Jul 2003 12:28:10 -0500
Subject: [R] matrix manipulations
Message-ID: <OF9669C76F.20330BE0-ON86256D64.005FCBFC@mmm.com>


David,

I am not sure if it can be done in a vectorized form, but this should work

y <- NULL
for(i in 1:ncol(x)) y <- c(y, cor(x[,i],apply(x[,-i],1,sum)))

Cheers,

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "David Andel"        |
|         |           <andel at ifi.unizh.ch> |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           07/15/2003 11:51     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       r-help at stat.math.ethz.ch                                                                                     |
  |      cc:                                                                                                                    |
  |      Subject:  [R] matrix manipulations                                                                                     |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hi

cor(x,apply(x,1,sum)) gives me the correlations of each column with the
sums of each row (correct me if I'm wrong, please).

What I need are the correlations of each column with the sums of each row
except the entry in the given column. It seems that for any one column i I
get it by doing:

cor(x[,i],apply(x[,-i],1,sum))

But I struggle to get it for all the columns. I was trying things like:

for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum))

which doesn't generate any output at all, and

> rbind(for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum)))
          [,1]
[1,] 0.1880237

outputs just the result of the very last column.

I know that it shouldn't be necessary to use for(), but I couldn't figure
out a way how to do the task using e.g. apply().

How do you get the results of all columns?

Thank you,
David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mrennie at utm.utoronto.ca  Tue Jul 15 19:46:34 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Tue, 15 Jul 2003 13:46:34 -0400
Subject: [R] Excel can do what R can't?????
Message-ID: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/a309fb4f/attachment.pl

From Ramlau at gmx.de  Tue Jul 15 19:50:53 2003
From: Ramlau at gmx.de (Ramlau@gmx.de)
Date: Tue, 15 Jul 2003 19:50:53 +0200 (MEST)
Subject: [R] Problem with my simulation
Message-ID: <32336.1058291453@www62.gmx.net>

Dear R-users,

thanks for your help last time!

But now I've got a new problem with my simulation program.
In order to save time I decided to divide my program in 
different parts. So I can use several computers (at the 
moment 3). On all these computers I installed the R-version 
1.7.1. One works without problems. The other 2 don't work. 
During program`s running Rgui.exe causes an error, so the 
computer interrupts and closes R. Sometimes I don't get any
error message and in the other case it seems useless.
Could anybody give me hints?

Thanks a lot!

-- 
Peggy Ramlau



From andy_liaw at merck.com  Tue Jul 15 20:05:37 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 14:05:37 -0400
Subject: [R] Problem with my simulation
Message-ID: <3A822319EB35174CA3714066D590DCD50205C889@usrymx25.merck.com>

Seems like you need to get help here:
http://www.catb.org/~esr/faqs/smart-questions.html first before anyone here
can help you.

Andy

> -----Original Message-----
> From: Ramlau at gmx.de [mailto:Ramlau at gmx.de] 
> Sent: Tuesday, July 15, 2003 1:51 PM
> To: R-Project
> Subject: [R] Problem with my simulation
> 
> 
> Dear R-users,
> 
> thanks for your help last time!
> 
> But now I've got a new problem with my simulation program.
> In order to save time I decided to divide my program in 
> different parts. So I can use several computers (at the 
> moment 3). On all these computers I installed the R-version 
> 1.7.1. One works without problems. The other 2 don't work. 
> During program`s running Rgui.exe causes an error, so the 
> computer interrupts and closes R. Sometimes I don't get any 
> error message and in the other case it seems useless. Could 
> anybody give me hints?
> 
> Thanks a lot!
> 
> -- 
> Peggy Ramlau
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From spencer.graves at pdf.com  Tue Jul 15 20:09:18 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 15 Jul 2003 11:09:18 -0700
Subject: [R] matrix manipulations
References: <OF9669C76F.20330BE0-ON86256D64.005FCBFC@mmm.com>
Message-ID: <3F14434E.8030407@pdf.com>

What about the following:

 > x <- array(1:15, dim=c(5,3))
 > k <- dim(x)[2]
 > (P.k <- outer(rep(1,k), rep(1,k))-diag(k))
      [,1] [,2] [,3]
[1,]    0    1    1
[2,]    1    0    1
[3,]    1    1    0
 > (x.1 <- (x %*%P.k))
      [,1] [,2] [,3]
[1,]   17   12    7
[2,]   19   14    9
[3,]   21   16   11
[4,]   23   18   13
[5,]   25   20   15
 > cor(x, x.1)
      [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    1    1    1

This has the right pieces and the correct answer in this case and 
presumably in others.

hope this helps.  spencer graves

apjaworski at mmm.com wrote:
> David,
> 
> I am not sure if it can be done in a vectorized form, but this should work
> 
> y <- NULL
> for(i in 1:ncol(x)) y <- c(y, cor(x[,i],apply(x[,-i],1,sum)))
> 
> Cheers,
> 
> Andy
> 
> __________________________________
> Andy Jaworski
> Engineering Systems Technology Center
> 3M Center, 518-1-01
> St. Paul, MN 55144-1000
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
> 
> 
> |---------+-------------------------------->
> |         |           "David Andel"        |
> |         |           <andel at ifi.unizh.ch> |
> |         |           Sent by:             |
> |         |           r-help-bounces at stat.m|
> |         |           ath.ethz.ch          |
> |         |                                |
> |         |                                |
> |         |           07/15/2003 11:51     |
> |         |                                |
> |---------+-------------------------------->
>   >-----------------------------------------------------------------------------------------------------------------------------|
>   |                                                                                                                             |
>   |      To:       r-help at stat.math.ethz.ch                                                                                     |
>   |      cc:                                                                                                                    |
>   |      Subject:  [R] matrix manipulations                                                                                     |
>   >-----------------------------------------------------------------------------------------------------------------------------|
> 
> 
> 
> 
> Hi
> 
> cor(x,apply(x,1,sum)) gives me the correlations of each column with the
> sums of each row (correct me if I'm wrong, please).
> 
> What I need are the correlations of each column with the sums of each row
> except the entry in the given column. It seems that for any one column i I
> get it by doing:
> 
> cor(x[,i],apply(x[,-i],1,sum))
> 
> But I struggle to get it for all the columns. I was trying things like:
> 
> for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum))
> 
> which doesn't generate any output at all, and
> 
> 
>>rbind(for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum)))
> 
>           [,1]
> [1,] 0.1880237
> 
> outputs just the result of the very last column.
> 
> I know that it shouldn't be necessary to use for(), but I couldn't figure
> out a way how to do the task using e.g. apply().
> 
> How do you get the results of all columns?
> 
> Thank you,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Tue Jul 15 20:25:55 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 15 Jul 2003 11:25:55 -0700
Subject: [R] Excel can do what R can't?????
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
Message-ID: <3F144733.4050002@pdf.com>

	  I've programmed many things like this in both Excel and R.  When I 
did not get the same answer from both, it was because I had an error in 
one (or both).  I do this routinely as part of debugging:  I catch many 
mistakes this way, and I often feel I can not trust my answers without 
this level of checking.

	  I use Excel with Solver if I only need one solution or if I'm working 
with someone who doesn't have R or S-Plus.   Otherwise, I prefer S-Plus 
of R.

	  First forget about "optim":  Do you get the same numbers from your 
function "f" and from Excel?  Have you plotted the function to be sure 
you even have local minima?  Naively, I would expect Excel to be more 
likely to get stuck in local minima than "optim".

	  I'm sorry you've had such a frustrating experience with R.  The S 
language is very powerful but does have a steep learning curve.  I had 
S-Plus for 3-5 years before I was finally able to do anything useful 
with it.  Now, it is an integral part of how I do much of what I do.

hope this helps.
spencer graves

Michael Rennie wrote:
> Hi there
> 
> I thought this would be of particular interest to people using 'optim' 
> functions and perhaps people involved with R development.
> 
> I've been beaten down by R trying to get it to perform an optimization on a 
> mass-balance model.  I've written the same program in excel, and using the 
> 'solver' function, it comes up with an answer for my variables (p, ACT, 
> which I've assigned to q in R) that gives a solution to the function "f" in 
> about 3 seconds, with a value of the function around 0.0004. R, on the 
> other hand, appears to get stuck in local minima, and spits back an 
> approximation that is close the the p, ACT values excel does, but not 
> nearly precise enough for my needs, and not nearly as precise as excel, and 
> it takes about 3 minutes.  Also, the solution for the value it returns for 
> the function is about 8 orders of magnitude greater than the excel version, 
> so I can't really say the function is approximating zero.  I was able to 
> determine this using a  "trace" command on function f, which is listed below.
> 
> This is very likely due to the fact that I've made some coding error along 
> the way, or have done something else wrong, but I don't know.  Either way, 
> I am shocked and surprised that a program like excel is outperforming 
> R.  I've attached my command file and the dataset "temp.dat" at the bottom 
> of this e-mail for anyone who would like to fiddle around with it, and if 
> you come up with something, PLEASE let me know- In the meantime, I've got 
> to start fiddling with excel and figuring out how to automate the solver 
> calculation.....
> 
> Briefly, the point of the program is to approximate the model output from 
> an iterative calculation, Wtmod and Hgtmod, to user-specified endpoints Wt 
> and Hgt, by seeking the optimal values of p, ACT involved in the iterative 
> process.
> 
> Also, if your interested in recent correspondence that explains the point 
> of the program a bit, and how the function ties in to the iterative 
> process, search the R help forum for e-mails entitled "[R] problem with 
> coding for 'optim' in R".  Thanks also to Roger Peng and numerous others 
> for helping me get this far.
> 
> The whole point of me doing this in R was because it's supposed to be 
> spectacularly fast at automating complex loops, but seems to be falling 
> short for this application.  Hopefully it's something wrong with my coding 
> and not with R itself.
> 
> Mike
> 
> R COMMAND FILE:
> 
> ####################################
> #    perch.R                       #
> # Hewett and Johnson bioenergetics #
> # model combined with              #
> # Trudel MMBM to estimate          #
> # Consumption in perch in R code   #
> # Execute with                     #
> # R --vanilla < perch.R > perch.out#
> ####################################
> 
> #USER INPUT BELOW
> 
> #Weight at time 0
> Wo<- 9.2
> 
> #Hg concentration at time 0 (ugHg/g wet weight)
> Hgo<- 0.08
> 
> #Weight at time t
> Wt<- 32.2
> 
> #Hg concentration at time t (ugHg/g wet weight)
> Hgt<- 0.110
> 
> #Prey methylmercury concentration (as constant)
> Hgp<- 0.033
> 
> #Prey caloric value (as constant)
> Pc<- 800
> 
> #Energy density of fish (as constant, calories)
> Ef <- 1000
> 
> #Maturity status, 0=immature, 1=mature
> Mat<- 0
> 
> #Sex, 1=male, 2=female
> Sex<- 1
> 
> #USER INPUT ABOVE
> 
> #Bioenergetics parameters for perch
> CA <- 0.25
> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> CQ <- 2.3
> CTO <- 23
> CTM <- 28
> Zc<- (log(CQ))*(CTM-CTO)
> Yc<- (log(CQ))*(CTM-CTO+2)
> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> 
> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> RB <- 0.8   #same as 1+(-0.2) see above...
> RQ <- 2.1
> RTO <- 28
> RTM <- 33
> Za <- (log(RQ))*(RTM-RTO)
> Ya<- (log(RQ))*(RTM-RTO+2)
> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> 
> S <- 0.172
> 
> FA <- 0.158
> FB <- -0.222
> FG <- 0.631
> 
> UA<- 0.0253
> UB<- 0.58
> UG<- -0.299
> 
> #Mass balance model parameters
> EA <- 0.002938
> EB <- -0.2
> EQ <- 0.066
> a <- 0.8
> 
> #Specifying sex-specific parameters
> 
> GSI<- NULL
> 
> if (Sex==1) GSI<-0.05 else
> if (Sex==2) GSI<-0.17
> 
> # Define margin of error functions
> #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a proportion
> #    {
> #    z <- qnorm(1-alpha/2)
> #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample size
> #    merror
> #    }
> 
> #Bring in temp file
> 
> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
> 
> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
> 
> temp<- cbind (Day, jday, Temp)
> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> #temp [,2]
> 
> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> 
> comp<- cbind (Day, jday, Temp, Vc, Vr)
> 
> #comp
> 
> bio<-matrix(NA, ncol=13, nrow=length(Day))
> W<-NULL
> C<-NULL
> ASMR<-NULL
> SMR<-NULL
> A<-NULL
> F<-NULL
> U<-NULL
> SDA<-NULL
> Gr<-NULL
> Hg<-NULL
> Ed<-NULL
> GHg<-NULL
> K<-NULL
> Expegk<-NULL
> EGK<-NULL
> p<-NULL
> ACT<-NULL
> 
> #starting values for p, ACT
> p <- 1 #  0.558626306252032 #solution set for p, ACT from excel 'solver' f'n
> ACT <- 2 #  1.66764519286918
> 
> q<-c(p,ACT)
> 
> #specify sttarting values
> #q0<-c(p = 1, ACT = 1)
> 
> #introduce function to solve
> f <- function (q)
> {
> 
> 
> M<- length(Day) #number of days iterated
> 
> for (i in 1:M)
> {
> 
> #Bioenergetics model
> 
> if (Day[i]==1) W[i] <- Wo else
> if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else
> W[i] <- (W[i-1]+(Gr[i-1]/Ef))
> #W
> 
> #W<-Wo
> 
> C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
> 
> ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
> 
> SMR[i]<- (ASMR[i]/q[2])
> 
> A[i]<- (ASMR[i]-SMR[i])
> 
> F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
> 
> U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
> 
> SDA[i]<- (S*(C[i]-F[i]))
> 
> Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
> 
> #Trudel MMBM
> 
> if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- 
> a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
> 
> Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
> 
> GHg[i] <- Gr[i]/Ef/W[i]
> 
> if (Sex==1) 
> K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M else
> if (Sex==2) K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
> # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g
> # then express as Q times GSI gives K / M gives daily K
> 
> EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
> 
> Expegk[i] <- exp(-1*EGK[i])
> 
> bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> 
> }
> 
> #warnings()
> 
> dimnames (bio) <-list(NULL, c("W", "C", "ASMR", "SMR", "A", "F", "U", 
> "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> 
> bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> 
> dimnames (bioday) <-list(NULL, c("jday", "W", "C", "ASMR", "SMR", "A", "F", 
> "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> 
> #bioday
> 
> Wtmod<- bioday [length(W),2]
> Wtmod
> 
> Hgtmod<- bioday [length(Hg),14]
> Hgtmod
> 
> q
> 
> f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))^2) ; f
> 
> #warnings()
> 
> #write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na = NA, 
> col.names = TRUE)
> 
> 
> 
> #nlm(f,c(1,1))
> }
> 
> optim(q, f, method = "L-BFGS-B",
>          lower = c(0.2, 1), upper=c(2, 3),
>          control = list(fnscale = 0.001))
> 
> 
> TRACE FUNCTION USED TO DETERMINE WHERE R IS GETTING STUCK;
> 
>   trace("f", quote(print(q)), at = 1, print = FALSE)
> 
> o <- optim(c(1,2), f, method = "L-BFGS-B", lower = c(0.2,1), upper = c(2,3))
> 
> DATA FOR TEMP.DAT:
> 
> 1       153     9.4
> 2       154     9.6
> 3       155     9.8
> 4       156     10
> 5       157     10.2
> 6       158     10.4
> 7       159     10.6
> 8       160     10.8
> 9       161     11
> 10      162     11.2
> 11      163     11.4
> 12      164     11.6
> 13      165     11.8
> 14      166     12
> 15      167     12.3
> 16      168     12.5
> 17      169     12.7
> 18      170     12.9
> 19      171     13.1
> 20      172     13.4
> 21      173     13.6
> 22      174     13.8
> 23      175     14
> 24      176     14.2
> 25      177     14.5
> 26      178     14.7
> 27      179     14.9
> 28      180     15.1
> 29      181     15.4
> 30      182     15.6
> 31      183     15.8
> 32      184     16
> 33      185     16.2
> 34      186     16.5
> 35      187     16.7
> 36      188     16.9
> 37      189     17.1
> 38      190     17.3
> 39      191     17.5
> 40      192     17.7
> 41      193     17.9
> 42      194     18.1
> 43      195     18.3
> 44      196     18.5
> 45      197     18.7
> 46      198     18.9
> 47      199     19
> 48      200     19.2
> 49      201     19.4
> 50      202     19.5
> 51      203     19.7
> 52      204     19.9
> 53      205     20
> 54      206     20.2
> 55      207     20.3
> 56      208     20.4
> 57      209     20.5
> 58      210     20.7
> 59      211     20.8
> 60      212     20.9
> 61      213     21
> 62      214     21.1
> 63      215     21.2
> 64      216     21.3
> 65      217     21.3
> 66      218     21.4
> 67      219     21.5
> 68      220     21.5
> 69      221     21.6
> 70      222     21.6
> 71      223     21.6
> 72      224     21.7
> 73      225     21.7
> 74      226     21.7
> 75      227     21.7
> 76      228     21.7
> 77      229     21.7
> 78      230     21.7
> 79      231     21.6
> 80      232     21.6
> 81      233     21.6
> 82      234     21.5
> 83      235     21.5
> 84      236     21.4
> 85      237     21.3
> 86      238     21.3
> 87      239     21.2
> 88      240     21.1
> 89      241     21
> 90      242     20.9
> 91      243     20.8
> 92      244     20.7
> 93      245     20.5
> 94      246     20.4
> 95      247     20.3
> 96      248     20.2
> 97      249     20
> 98      250     19.9
> 99      251     19.7
> 100     252     19.5
> 101     253     19.4
> 102     254     19.2
> 103     255     19
> 104     256     18.9
> 105     257     18.7
> 106     258     18.5
> 107     259     18.3
> 108     260     18.1
> 109     261     17.9
> 110     262     17.7
> 111     263     17.5
> 112     264     17.3
> 113     265     17.1
> 114     266     16.9
> 115     267     16.7
> 116     268     16.5
> 117     269     16.2
> 118     270     16
> 119     271     15.8
> 120     272     15.6
> 121     273     15.4
> 122     274     15.1
> 123     275     14.9
> 124     276     14.7
> 125     277     14.5
> 126     278     14.2
> 127     279     14
> 128     280     13.8
> 129     281     13.6
> 130     282     13.4
> 131     283     13.1
> 132     284     12.9
> 133     285     12.7
> 134     286     12.5
> 135     287     12.3
> 136     288     12
> 137     289     11.8
> 138     290     11.6
> 139     291     11.4
> 140     292     11.2
> 141     293     11
> 142     294     10.8
> 143     295     10.6
> 144     296     10.4
> 145     297     10.2
> 146     298     10
> 147     299     9.8
> 148     300     9.6
> 149     301     9.4
> 150     302     9.3
> 151     303     9.1
> 152     304     8.9
> 153     305     8.7
> 154     306     8.6
> 155     307     8.4
> 156     308     8.2
> 157     309     8.1
> 158     310     7.9
> 159     311     7.8
> 160     312     7.6
> 161     313     7.5
> 162     314     7.3
> 163     315     7.2
> 164     316     7
> 165     317     6.9
> 166     318     6.8
> 167     319     6.7
> 168     320     6.5
> 169     321     6.4
> 170     322     6.3
> 171     323     6.2
> 172     324     6.1
> 173     325     6
> 174     326     5.8
> 175     327     5.7
> 176     328     5.6
> 177     329     5.5
> 178     330     5.5
> 179     331     5.4
> 180     332     5.3
> 181     333     5.2
> 182     334     5.1
> 183     335     5
> 184     336     5
> 185     337     4.9
> 186     338     4.8
> 187     339     4.7
> 188     340     4.7
> 189     341     4.6
> 190     342     4.5
> 191     343     4.5
> 192     344     4.4
> 193     345     4.4
> 194     346     4.3
> 195     347     4.3
> 196     348     4.2
> 197     349     4.2
> 198     350     4.1
> 199     351     4.1
> 200     352     4
> 201     353     4
> 202     354     4
> 203     355     3.9
> 204     356     3.9
> 205     357     3.8
> 206     358     3.8
> 207     359     3.8
> 208     360     3.8
> 209     361     3.7
> 210     362     3.7
> 211     363     3.7
> 212     364     3.6
> 213     365     3.6
> 214     366     3.6
> 215     1       3.2
> 216     2       3.2
> 217     3       3.2
> 218     4       3.2
> 219     5       3.2
> 220     6       3.2
> 221     7       3.2
> 222     8       3.2
> 223     9       3.2
> 224     10      3.2
> 225     11      3.2
> 226     12      3.2
> 227     13      3.2
> 228     14      3.2
> 229     15      3.2
> 230     16      3.2
> 231     17      3.2
> 232     18      3.2
> 233     19      3.2
> 234     20      3.2
> 235     21      3.2
> 236     22      3.2
> 237     23      3.2
> 238     24      3.2
> 239     25      3.2
> 240     26      3.2
> 241     27      3.2
> 242     28      3.2
> 243     29      3.2
> 244     30      3.2
> 245     31      3.2
> 246     32      3.2
> 247     33      3.2
> 248     34      3.2
> 249     35      3.2
> 250     36      3.2
> 251     37      3.2
> 252     38      3.2
> 253     39      3.2
> 254     40      3.2
> 255     41      3.2
> 256     42      3.2
> 257     43      3.2
> 258     44      3.2
> 259     45      3.2
> 260     46      3.2
> 261     47      3.2
> 262     48      3.2
> 263     49      3.2
> 264     50      3.2
> 265     51      3.2
> 266     52      3.2
> 267     53      3.2
> 268     54      3.3
> 269     55      3.3
> 270     56      3.3
> 271     57      3.3
> 272     58      3.3
> 273     59      3.3
> 274     60      3.3
> 275     61      3.3
> 276     62      3.3
> 277     63      3.3
> 278     64      3.3
> 279     65      3.3
> 280     66      3.3
> 281     67      3.3
> 282     68      3.3
> 283     69      3.3
> 284     70      3.3
> 285     71      3.4
> 286     72      3.4
> 287     73      3.4
> 288     74      3.4
> 289     75      3.4
> 290     76      3.4
> 291     77      3.4
> 292     78      3.4
> 293     79      3.5
> 294     80      3.5
> 295     81      3.5
> 296     82      3.5
> 297     83      3.5
> 298     84      3.5
> 299     85      3.6
> 300     86      3.6
> 301     87      3.6
> 302     88      3.6
> 303     89      3.6
> 304     90      3.7
> 305     91      3.7
> 306     92      3.7
> 307     93      3.8
> 308     94      3.8
> 309     95      3.8
> 310     96      3.8
> 311     97      3.9
> 312     98      3.9
> 313     99      4
> 314     100     4
> 315     101     4
> 316     102     4.1
> 317     103     4.1
> 318     104     4.2
> 319     105     4.2
> 320     106     4.3
> 321     107     4.3
> 322     108     4.4
> 323     109     4.4
> 324     110     4.5
> 325     111     4.5
> 326     112     4.6
> 327     113     4.7
> 328     114     4.7
> 329     115     4.8
> 330     116     4.9
> 331     117     5
> 332     118     5
> 333     119     5.1
> 334     120     5.2
> 335     121     5.3
> 336     122     5.4
> 337     123     5.5
> 338     124     5.5
> 339     125     5.6
> 340     126     5.7
> 341     127     5.8
> 342     128     6
> 343     129     6.1
> 344     130     6.2
> 345     131     6.3
> 346     132     6.4
> 347     133     6.5
> 348     134     6.7
> 349     135     6.8
> 350     136     6.9
> 351     137     7
> 352     138     7.2
> 353     139     7.3
> 354     140     7.5
> 355     141     7.6
> 356     142     7.8
> 357     143     7.9
> 358     144     8.1
> 359     145     8.2
> 360     146     8.4
> 361     147     8.6
> 362     148     8.7
> 363     149     8.9
> 364     150     9.1
> 365     151     9.3
> 366     152     9.3
> 
> 
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From flom at ndri.org  Tue Jul 15 20:44:29 2003
From: flom at ndri.org (Peter Flom)
Date: Tue, 15 Jul 2003 14:44:29 -0400
Subject: [R] Tree question
Message-ID: <sf141364.040@MAIL.NDRI.ORG>

I was under the impression that the tree method (e.g. as implemented in
rpart) was insensitive to monotonic transformations of the dependent
variable.  e.g. Breiman Olshen et al. Classification and Regression
Trees  state "In a standard data structure [a tree] is invariant under
all monotone transformations of individual ordered varaibles" (p. 57)

However, I get very different results from
tr.hh.pri <- rpart((log(YPRISX+1)~AGE+DRUGUSEY+SEX+OBSXNUM))

and 

tr.hh.pri <- rpart(YPRISX~AGE+DRUGUSEY+SEX+OBSXNUM)

the former gives more splits and different splits.

Some notes:
The DV is a count variable, and highly skew, with some 0s, many 1s, and
a long right tail out to 99.
AGE ranges from 18-25
DRUGUSEY is ordered (hardest drug used)
and 
OBSXNUM is also ordered (proportion of your friends who object to your
having 'casual sex')

printing the first tree gives

 1) root 307 23.472040 0.7114605  
   2) AGE>=19.5 196 13.811070 0.6857971  
     4) OBSXNUM< 2.5 69  5.712526 0.6338252  
       8) DRUGUSEY>=1.5 15  2.261203 0.5161601 *
       9) DRUGUSEY< 1.5 54  3.185960 0.6665100 *
     5) OBSXNUM>=2.5 127  7.810911 0.7140339 *
   3) AGE< 19.5 111  9.303947 0.7567761  
     6) DRUGUSEY< 0.5 48  1.105266 0.6727132 *
     7) DRUGUSEY>=0.5 63  7.601052 0.8208239  
      14) SEX>=1.5 21  1.258395 0.7317629 *
      15) SEX< 1.5 42  6.092803 0.8653544 *


printing the second tree gives

 1) root 307 144.540700 1.1205210  
   2) AGE>=19.5 196  68.382650 1.0561220 *
   3) AGE< 19.5 111  73.909910 1.2342340  
     6) DRUGUSEY< 0.5 48   2.979167 0.9791667 *
     7) DRUGUSEY>=0.5 63  65.428570 1.4285710  
      14) SEX>=1.5 21   6.571429 1.1428570 *
      15) SEX< 1.5 42  56.285710 1.5714290 *


So, is this the 'exception that proves the rule'? Have I done something
wrong?  Or what?

Any ideas or thoughts?

Thanks in advance


Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From jerome at hivnet.ubc.ca  Tue Jul 15 20:47:47 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 15 Jul 2003 11:47:47 -0700
Subject: [R] Excel can do what R can't?????
In-Reply-To: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
Message-ID: <200307151854.LAA23105@hivnet.ubc.ca>


Mike,

The definition of your function f() seems quite inefficient. You could 
vectorize it, which would shorten and speed up your code, especially if M 
is large. See the R introduction file available online to learn how to do 
it if you don't already know how. Also, you have to return only one 
argument. Unless I'm wrong, your function wants to return Wtmod, Hgtmod, q 
and f. I'm don't think this would change anything in this case, but you 
should definitely clean this up!

Another advice... If you can simplify your example into a few lines of 
"ready-to-execute" code with a toy dataset, then it's easy for everyone to 
try it out and you can get more feedback. The code you've included is 
quite large and cumbersome. For one thing, you could easily have removed 
the lines of code that were commented out.

Meanwhile, I would suggest that you go back to the basics of R to clean up 
your code.

Sorry I can't be more helpful.
Jerome

On July 15, 2003 10:46 am, Michael Rennie wrote:
> Hi there
>
> I thought this would be of particular interest to people using 'optim'
> functions and perhaps people involved with R development.
>
> I've been beaten down by R trying to get it to perform an optimization
> on a mass-balance model.  I've written the same program in excel, and
> using the 'solver' function, it comes up with an answer for my variables
> (p, ACT, which I've assigned to q in R) that gives a solution to the
> function "f" in about 3 seconds, with a value of the function around
> 0.0004. R, on the other hand, appears to get stuck in local minima, and
> spits back an approximation that is close the the p, ACT values excel
> does, but not nearly precise enough for my needs, and not nearly as
> precise as excel, and it takes about 3 minutes.  Also, the solution for
> the value it returns for the function is about 8 orders of magnitude
> greater than the excel version, so I can't really say the function is
> approximating zero.  I was able to determine this using a  "trace"
> command on function f, which is listed below.
>
> This is very likely due to the fact that I've made some coding error
> along the way, or have done something else wrong, but I don't know. 
> Either way, I am shocked and surprised that a program like excel is
> outperforming R.  I've attached my command file and the dataset
> "temp.dat" at the bottom of this e-mail for anyone who would like to
> fiddle around with it, and if you come up with something, PLEASE let me
> know- In the meantime, I've got to start fiddling with excel and
> figuring out how to automate the solver calculation.....
>
> Briefly, the point of the program is to approximate the model output
> from an iterative calculation, Wtmod and Hgtmod, to user-specified
> endpoints Wt and Hgt, by seeking the optimal values of p, ACT involved
> in the iterative process.
>
> Also, if your interested in recent correspondence that explains the
> point of the program a bit, and how the function ties in to the
> iterative process, search the R help forum for e-mails entitled "[R]
> problem with coding for 'optim' in R".  Thanks also to Roger Peng and
> numerous others for helping me get this far.
>
> The whole point of me doing this in R was because it's supposed to be
> spectacularly fast at automating complex loops, but seems to be falling
> short for this application.  Hopefully it's something wrong with my
> coding and not with R itself.
>
> Mike
>
> R COMMAND FILE:
>
> ####################################
> #    perch.R                       #
> # Hewett and Johnson bioenergetics #
> # model combined with              #
> # Trudel MMBM to estimate          #
> # Consumption in perch in R code   #
> # Execute with                     #
> # R --vanilla < perch.R > perch.out#
> ####################################
>
> #USER INPUT BELOW
>
> #Weight at time 0
> Wo<- 9.2
>
> #Hg concentration at time 0 (ugHg/g wet weight)
> Hgo<- 0.08
>
> #Weight at time t
> Wt<- 32.2
>
> #Hg concentration at time t (ugHg/g wet weight)
> Hgt<- 0.110
>
> #Prey methylmercury concentration (as constant)
> Hgp<- 0.033
>
> #Prey caloric value (as constant)
> Pc<- 800
>
> #Energy density of fish (as constant, calories)
> Ef <- 1000
>
> #Maturity status, 0=immature, 1=mature
> Mat<- 0
>
> #Sex, 1=male, 2=female
> Sex<- 1
>
> #USER INPUT ABOVE
>
> #Bioenergetics parameters for perch
> CA <- 0.25
> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> CQ <- 2.3
> CTO <- 23
> CTM <- 28
> Zc<- (log(CQ))*(CTM-CTO)
> Yc<- (log(CQ))*(CTM-CTO+2)
> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
>
> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> RB <- 0.8   #same as 1+(-0.2) see above...
> RQ <- 2.1
> RTO <- 28
> RTM <- 33
> Za <- (log(RQ))*(RTM-RTO)
> Ya<- (log(RQ))*(RTM-RTO+2)
> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
>
> S <- 0.172
>
> FA <- 0.158
> FB <- -0.222
> FG <- 0.631
>
> UA<- 0.0253
> UB<- 0.58
> UG<- -0.299
>
> #Mass balance model parameters
> EA <- 0.002938
> EB <- -0.2
> EQ <- 0.066
> a <- 0.8
>
> #Specifying sex-specific parameters
>
> GSI<- NULL
>
> if (Sex==1) GSI<-0.05 else
> if (Sex==2) GSI<-0.17
>
> # Define margin of error functions
> #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a
> proportion #    {
> #    z <- qnorm(1-alpha/2)
> #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample
> size #    merror
> #    }
>
> #Bring in temp file
>
> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0,
> Temp=0))
>
> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
>
> temp<- cbind (Day, jday, Temp)
> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> #temp [,2]
>
> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
>
> comp<- cbind (Day, jday, Temp, Vc, Vr)
>
> #comp
>
> bio<-matrix(NA, ncol=13, nrow=length(Day))
> W<-NULL
> C<-NULL
> ASMR<-NULL
> SMR<-NULL
> A<-NULL
> F<-NULL
> U<-NULL
> SDA<-NULL
> Gr<-NULL
> Hg<-NULL
> Ed<-NULL
> GHg<-NULL
> K<-NULL
> Expegk<-NULL
> EGK<-NULL
> p<-NULL
> ACT<-NULL
>
> #starting values for p, ACT
> p <- 1 #  0.558626306252032 #solution set for p, ACT from excel 'solver'
> f'n ACT <- 2 #  1.66764519286918
>
> q<-c(p,ACT)
>
> #specify sttarting values
> #q0<-c(p = 1, ACT = 1)
>
> #introduce function to solve
> f <- function (q)
> {
>
>
> M<- length(Day) #number of days iterated
>
> for (i in 1:M)
> {
>
> #Bioenergetics model
>
> if (Day[i]==1) W[i] <- Wo else
> if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else
> W[i] <- (W[i-1]+(Gr[i-1]/Ef))
> #W
>
> #W<-Wo
>
> C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
>
> ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
>
> SMR[i]<- (ASMR[i]/q[2])
>
> A[i]<- (ASMR[i]-SMR[i])
>
> F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
>
> U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
>
> SDA[i]<- (S*(C[i]-F[i]))
>
> Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
>
> #Trudel MMBM
>
> if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <-
> a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
>
> Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
>
> GHg[i] <- Gr[i]/Ef/W[i]
>
> if (Sex==1)
> K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M else
> if (Sex==2)
> K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M # =
> dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g
> # then express as Q times GSI gives K / M gives daily K
>
> EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
>
> Expegk[i] <- exp(-1*EGK[i])
>
> bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>
> }
>
> #warnings()
>
> dimnames (bio) <-list(NULL, c("W", "C", "ASMR", "SMR", "A", "F", "U",
> "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
>
> bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>
> dimnames (bioday) <-list(NULL, c("jday", "W", "C", "ASMR", "SMR", "A",
> "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
>
> #bioday
>
> Wtmod<- bioday [length(W),2]
> Wtmod
>
> Hgtmod<- bioday [length(Hg),14]
> Hgtmod
>
> q
>
> f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))^2) ; f
>
> #warnings()
>
> #write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na =
> NA, col.names = TRUE)
>
>
>
> #nlm(f,c(1,1))
> }
>
> optim(q, f, method = "L-BFGS-B",
>          lower = c(0.2, 1), upper=c(2, 3),
>          control = list(fnscale = 0.001))
>
>
> TRACE FUNCTION USED TO DETERMINE WHERE R IS GETTING STUCK;
>
>   trace("f", quote(print(q)), at = 1, print = FALSE)
>
> o <- optim(c(1,2), f, method = "L-BFGS-B", lower = c(0.2,1), upper =
> c(2,3))
>
> DATA FOR TEMP.DAT:
>
> 1       153     9.4
> 2       154     9.6
> 3       155     9.8
> 4       156     10
> 5       157     10.2
> 6       158     10.4
> 7       159     10.6
> 8       160     10.8
> 9       161     11
> 10      162     11.2
> 11      163     11.4
> 12      164     11.6
> 13      165     11.8
> 14      166     12
> 15      167     12.3
> 16      168     12.5
> 17      169     12.7
> 18      170     12.9
> 19      171     13.1
> 20      172     13.4
> 21      173     13.6
> 22      174     13.8
> 23      175     14
> 24      176     14.2
> 25      177     14.5
> 26      178     14.7
> 27      179     14.9
> 28      180     15.1
> 29      181     15.4
> 30      182     15.6
> 31      183     15.8
> 32      184     16
> 33      185     16.2
> 34      186     16.5
> 35      187     16.7
> 36      188     16.9
> 37      189     17.1
> 38      190     17.3
> 39      191     17.5
> 40      192     17.7
> 41      193     17.9
> 42      194     18.1
> 43      195     18.3
> 44      196     18.5
> 45      197     18.7
> 46      198     18.9
> 47      199     19
> 48      200     19.2
> 49      201     19.4
> 50      202     19.5
> 51      203     19.7
> 52      204     19.9
> 53      205     20
> 54      206     20.2
> 55      207     20.3
> 56      208     20.4
> 57      209     20.5
> 58      210     20.7
> 59      211     20.8
> 60      212     20.9
> 61      213     21
> 62      214     21.1
> 63      215     21.2
> 64      216     21.3
> 65      217     21.3
> 66      218     21.4
> 67      219     21.5
> 68      220     21.5
> 69      221     21.6
> 70      222     21.6
> 71      223     21.6
> 72      224     21.7
> 73      225     21.7
> 74      226     21.7
> 75      227     21.7
> 76      228     21.7
> 77      229     21.7
> 78      230     21.7
> 79      231     21.6
> 80      232     21.6
> 81      233     21.6
> 82      234     21.5
> 83      235     21.5
> 84      236     21.4
> 85      237     21.3
> 86      238     21.3
> 87      239     21.2
> 88      240     21.1
> 89      241     21
> 90      242     20.9
> 91      243     20.8
> 92      244     20.7
> 93      245     20.5
> 94      246     20.4
> 95      247     20.3
> 96      248     20.2
> 97      249     20
> 98      250     19.9
> 99      251     19.7
> 100     252     19.5
> 101     253     19.4
> 102     254     19.2
> 103     255     19
> 104     256     18.9
> 105     257     18.7
> 106     258     18.5
> 107     259     18.3
> 108     260     18.1
> 109     261     17.9
> 110     262     17.7
> 111     263     17.5
> 112     264     17.3
> 113     265     17.1
> 114     266     16.9
> 115     267     16.7
> 116     268     16.5
> 117     269     16.2
> 118     270     16
> 119     271     15.8
> 120     272     15.6
> 121     273     15.4
> 122     274     15.1
> 123     275     14.9
> 124     276     14.7
> 125     277     14.5
> 126     278     14.2
> 127     279     14
> 128     280     13.8
> 129     281     13.6
> 130     282     13.4
> 131     283     13.1
> 132     284     12.9
> 133     285     12.7
> 134     286     12.5
> 135     287     12.3
> 136     288     12
> 137     289     11.8
> 138     290     11.6
> 139     291     11.4
> 140     292     11.2
> 141     293     11
> 142     294     10.8
> 143     295     10.6
> 144     296     10.4
> 145     297     10.2
> 146     298     10
> 147     299     9.8
> 148     300     9.6
> 149     301     9.4
> 150     302     9.3
> 151     303     9.1
> 152     304     8.9
> 153     305     8.7
> 154     306     8.6
> 155     307     8.4
> 156     308     8.2
> 157     309     8.1
> 158     310     7.9
> 159     311     7.8
> 160     312     7.6
> 161     313     7.5
> 162     314     7.3
> 163     315     7.2
> 164     316     7
> 165     317     6.9
> 166     318     6.8
> 167     319     6.7
> 168     320     6.5
> 169     321     6.4
> 170     322     6.3
> 171     323     6.2
> 172     324     6.1
> 173     325     6
> 174     326     5.8
> 175     327     5.7
> 176     328     5.6
> 177     329     5.5
> 178     330     5.5
> 179     331     5.4
> 180     332     5.3
> 181     333     5.2
> 182     334     5.1
> 183     335     5
> 184     336     5
> 185     337     4.9
> 186     338     4.8
> 187     339     4.7
> 188     340     4.7
> 189     341     4.6
> 190     342     4.5
> 191     343     4.5
> 192     344     4.4
> 193     345     4.4
> 194     346     4.3
> 195     347     4.3
> 196     348     4.2
> 197     349     4.2
> 198     350     4.1
> 199     351     4.1
> 200     352     4
> 201     353     4
> 202     354     4
> 203     355     3.9
> 204     356     3.9
> 205     357     3.8
> 206     358     3.8
> 207     359     3.8
> 208     360     3.8
> 209     361     3.7
> 210     362     3.7
> 211     363     3.7
> 212     364     3.6
> 213     365     3.6
> 214     366     3.6
> 215     1       3.2
> 216     2       3.2
> 217     3       3.2
> 218     4       3.2
> 219     5       3.2
> 220     6       3.2
> 221     7       3.2
> 222     8       3.2
> 223     9       3.2
> 224     10      3.2
> 225     11      3.2
> 226     12      3.2
> 227     13      3.2
> 228     14      3.2
> 229     15      3.2
> 230     16      3.2
> 231     17      3.2
> 232     18      3.2
> 233     19      3.2
> 234     20      3.2
> 235     21      3.2
> 236     22      3.2
> 237     23      3.2
> 238     24      3.2
> 239     25      3.2
> 240     26      3.2
> 241     27      3.2
> 242     28      3.2
> 243     29      3.2
> 244     30      3.2
> 245     31      3.2
> 246     32      3.2
> 247     33      3.2
> 248     34      3.2
> 249     35      3.2
> 250     36      3.2
> 251     37      3.2
> 252     38      3.2
> 253     39      3.2
> 254     40      3.2
> 255     41      3.2
> 256     42      3.2
> 257     43      3.2
> 258     44      3.2
> 259     45      3.2
> 260     46      3.2
> 261     47      3.2
> 262     48      3.2
> 263     49      3.2
> 264     50      3.2
> 265     51      3.2
> 266     52      3.2
> 267     53      3.2
> 268     54      3.3
> 269     55      3.3
> 270     56      3.3
> 271     57      3.3
> 272     58      3.3
> 273     59      3.3
> 274     60      3.3
> 275     61      3.3
> 276     62      3.3
> 277     63      3.3
> 278     64      3.3
> 279     65      3.3
> 280     66      3.3
> 281     67      3.3
> 282     68      3.3
> 283     69      3.3
> 284     70      3.3
> 285     71      3.4
> 286     72      3.4
> 287     73      3.4
> 288     74      3.4
> 289     75      3.4
> 290     76      3.4
> 291     77      3.4
> 292     78      3.4
> 293     79      3.5
> 294     80      3.5
> 295     81      3.5
> 296     82      3.5
> 297     83      3.5
> 298     84      3.5
> 299     85      3.6
> 300     86      3.6
> 301     87      3.6
> 302     88      3.6
> 303     89      3.6
> 304     90      3.7
> 305     91      3.7
> 306     92      3.7
> 307     93      3.8
> 308     94      3.8
> 309     95      3.8
> 310     96      3.8
> 311     97      3.9
> 312     98      3.9
> 313     99      4
> 314     100     4
> 315     101     4
> 316     102     4.1
> 317     103     4.1
> 318     104     4.2
> 319     105     4.2
> 320     106     4.3
> 321     107     4.3
> 322     108     4.4
> 323     109     4.4
> 324     110     4.5
> 325     111     4.5
> 326     112     4.6
> 327     113     4.7
> 328     114     4.7
> 329     115     4.8
> 330     116     4.9
> 331     117     5
> 332     118     5
> 333     119     5.1
> 334     120     5.2
> 335     121     5.3
> 336     122     5.4
> 337     123     5.5
> 338     124     5.5
> 339     125     5.6
> 340     126     5.7
> 341     127     5.8
> 342     128     6
> 343     129     6.1
> 344     130     6.2
> 345     131     6.3
> 346     132     6.4
> 347     133     6.5
> 348     134     6.7
> 349     135     6.8
> 350     136     6.9
> 351     137     7
> 352     138     7.2
> 353     139     7.3
> 354     140     7.5
> 355     141     7.6
> 356     142     7.8
> 357     143     7.9
> 358     144     8.1
> 359     145     8.2
> 360     146     8.4
> 361     147     8.6
> 362     148     8.7
> 363     149     8.9
> 364     150     9.1
> 365     151     9.3
> 366     152     9.3
>
>
>
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Tue Jul 15 20:59:03 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 14:59:03 -0400
Subject: [R] Tree question
Message-ID: <3A822319EB35174CA3714066D590DCD50205C88C@usrymx25.merck.com>

That should be "in"dependent variable.  The CART book, in the sentence you
quoted, did not make this clear.  But the following paragraph clearly
indicate that they are talking about the predictor variables, not the
response.

This is because the tree algorithm doesn't work on the original predictor
variables, but rather just the ranks of their unique values.  The possible
splits are all the "gaps" between the unique values, so the algorithm only
need the ranks.  Ranks are clearly invariant to monotone transformation.

Andy

> -----Original Message-----
> From: Peter Flom [mailto:flom at ndri.org] 
> Sent: Tuesday, July 15, 2003 2:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Tree question
> 
> 
> I was under the impression that the tree method (e.g. as 
> implemented in
> rpart) was insensitive to monotonic transformations of the 
> dependent variable.  e.g. Breiman Olshen et al. 
> Classification and Regression Trees  state "In a standard 
> data structure [a tree] is invariant under all monotone 
> transformations of individual ordered varaibles" (p. 57)
> 
> However, I get very different results from
> tr.hh.pri <- rpart((log(YPRISX+1)~AGE+DRUGUSEY+SEX+OBSXNUM))
> 
> and 
> 
> tr.hh.pri <- rpart(YPRISX~AGE+DRUGUSEY+SEX+OBSXNUM)
> 
> the former gives more splits and different splits.
> 
> Some notes:
> The DV is a count variable, and highly skew, with some 0s, 
> many 1s, and a long right tail out to 99. AGE ranges from 
> 18-25 DRUGUSEY is ordered (hardest drug used) and 
> OBSXNUM is also ordered (proportion of your friends who 
> object to your having 'casual sex')
> 
> printing the first tree gives
> 
>  1) root 307 23.472040 0.7114605  
>    2) AGE>=19.5 196 13.811070 0.6857971  
>      4) OBSXNUM< 2.5 69  5.712526 0.6338252  
>        8) DRUGUSEY>=1.5 15  2.261203 0.5161601 *
>        9) DRUGUSEY< 1.5 54  3.185960 0.6665100 *
>      5) OBSXNUM>=2.5 127  7.810911 0.7140339 *
>    3) AGE< 19.5 111  9.303947 0.7567761  
>      6) DRUGUSEY< 0.5 48  1.105266 0.6727132 *
>      7) DRUGUSEY>=0.5 63  7.601052 0.8208239  
>       14) SEX>=1.5 21  1.258395 0.7317629 *
>       15) SEX< 1.5 42  6.092803 0.8653544 *
> 
> 
> printing the second tree gives
> 
>  1) root 307 144.540700 1.1205210  
>    2) AGE>=19.5 196  68.382650 1.0561220 *
>    3) AGE< 19.5 111  73.909910 1.2342340  
>      6) DRUGUSEY< 0.5 48   2.979167 0.9791667 *
>      7) DRUGUSEY>=0.5 63  65.428570 1.4285710  
>       14) SEX>=1.5 21   6.571429 1.1428570 *
>       15) SEX< 1.5 42  56.285710 1.5714290 *
> 
> 
> So, is this the 'exception that proves the rule'? Have I done 
> something wrong?  Or what?
> 
> Any ideas or thoughts?
> 
> Thanks in advance
> 
> 
> Peter
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From flom at ndri.org  Tue Jul 15 21:12:42 2003
From: flom at ndri.org (Peter Flom)
Date: Tue, 15 Jul 2003 15:12:42 -0400
Subject: [R] tree problem solved
Message-ID: <sf1419f8.079@MAIL.NDRI.ORG>

You guys are fast!

Several people pointed out that the tree method  is insenstive to
monotone transforms of  the independent variables, not the dependent
variable

Thanks

Peter



From shitao at hotmail.com  Tue Jul 15 21:18:59 2003
From: shitao at hotmail.com (Tao Shi)
Date: Tue, 15 Jul 2003 19:18:59 +0000
Subject: [R] Why two chisq.test p values differ when the contingency table
	is transposed?
Message-ID: <Sea2-F34Jx0EI0DEgAk0000fc3e@hotmail.com>

I'm using R1.7.0 runing with Win XP.  Thanks,

...Tao


????????????????????????????????????????????????????????

>x
     [,1] [,2]
[1,]  149  151
[2,]    1    8
>t(x)
     [,1] [,2]
[1,]  149    1
[2,]  151    8
>chisq.test(x, simulate.p.value=T, B=100000)

        Pearson's Chi-squared test with simulated p-value (based on 1e+05 
replicates)

data:  x
X-squared = 5.2001, df = NA, p-value = 0.03774

>chisq.test(t(x), simulate.p.value=T, B=100000)

        Pearson's Chi-squared test with simulated p-value (based on 1e+05 
replicates)

data:  t(x)
X-squared = 5.2001, df = NA, p-value = 0.01642



From johnson.2060 at osu.edu  Tue Jul 15 21:42:02 2003
From: johnson.2060 at osu.edu (Andrew Johnson)
Date: Tue, 15 Jul 2003 15:42:02 -0400
Subject: [R] Plotting a graph of many lines between groups of points...
Message-ID: <0HI300FC8023Y8@mail-mta5.service.ohio-state.edu>

I have a data file read into a data frame.

For example,

	V1	V2	V3	V4
1	1	1	3	4
2	2	3	5	10
.	.	.	.	.
.	.	.	.	.
n          V1[n]     V2[n]     V3[n]     V4[n]

to n=many thousand

I want to plot a graph with many line segments, where v1[i]=x1, v2[i]=y1, 
v3[i]=x2, v4[i]=y2 for i=1,n.

This seems relatively simple in theory but I've spent quite a bit of time 
trying to make it happen with plot(type=), points(x,y), or lines(x,y) to no 
avail.

Do I need to turn these into vectors before plotting them?

Any help would be greatly appreciated.

Thanks,
Andrew



From Ted.Harding at nessie.mcc.ac.uk  Tue Jul 15 22:00:04 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jul 2003 21:00:04 +0100 (BST)
Subject: [R] Why two chisq.test p values differ when the contingency 
In-Reply-To: <Sea2-F34Jx0EI0DEgAk0000fc3e@hotmail.com>
Message-ID: <XFMail.030715210004.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jul-03 Tao Shi wrote:
>>x
>      [,1] [,2]
> [1,]  149  151
> [2,]    1    8
>>t(x)
>      [,1] [,2]
> [1,]  149    1
> [2,]  151    8
>>chisq.test(x, simulate.p.value=T, B=100000)
>         Pearson's Chi-squared test with simulated p-value (based on
> 1e+05 replicates)
> data:  x
> X-squared = 5.2001, df = NA, p-value = 0.03774
> 
>>chisq.test(t(x), simulate.p.value=T, B=100000)
>         Pearson's Chi-squared test with simulated p-value (based on
> 1e+05 replicates)
> data:  t(x)
> X-squared = 5.2001, df = NA, p-value = 0.01642

Possibly you may just have been unlucky, though the 0.03774 seems large:

c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
for(i in (1:9)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
                       B=100000)$p.value)}
c2tx<-chisq.test(tx, simulate.p.value=T, B=100000)$p.value
for(i in (1:9)){c2tx<-c(c2tx,chisq.test(tx, simulate.p.value=T,
                        B=100000)$p.value)}
cbind(c2x,c2tx)
          c2x    c2tx
 [1,] 0.01627 0.01720
 [2,] 0.01672 0.01690
 [3,] 0.01662 0.01669
 [4,] 0.01733 0.01656
 [5,] 0.01679 0.01777
 [6,] 0.01715 0.01769
 [7,] 0.01765 0.01769
 [8,] 0.01703 0.01740
 [9,] 0.01704 0.01708
[10,] 0.01669 0.01655

sd(c2x)
 [1] 0.0003946715
sd(c2tx)
 [1] 0.0004737099

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jul-03                                       Time: 21:00:04
------------------------------ XFMail ------------------------------



From jfkincaid at salisbury.edu  Tue Jul 15 22:21:07 2003
From: jfkincaid at salisbury.edu (Joel Kincaid)
Date: Tue, 15 Jul 2003 16:21:07 -0400
Subject: [R] Plotting a graph of many lines between groups of
	points...
Message-ID: <sf1429f5.085@mail2.salisbury.edu>

johnson.2060 at osu.edu wrote:
> I have a data file read into a data frame.
> 
> For example,
> 
>     V1  V2  V3  V4
> 1   1   1   3   4
> 2   2   3   5   10
> .   .   .   .   .
> .   .   .   .   .
> n          V1[n]     V2[n]     V3[n]     V4[n]
> 
> to n=many thousand
> 
is this stored as a data.frame? a matrix? In any case there seems to be
a problem with your indexing ...
> I want to plot a graph with many line segments, where v1[i]=x1,
v2[i]=y1, 
> v3[i]=x2, v4[i]=y2 for i=1,n.
indexing is ok if you have df$v3[i] ....
> 
> This seems relatively simple in theory but I've spent quite a bit of
time 
> trying to make it happen with plot(type=), points(x,y), or lines(x,y)
to no 
> avail.
could you send the actual code for your plot? Perhaps some way to
construct the data. at this point things are a bit murky.
> 
> Do I need to turn these into vectors before plotting them?
Thus my question about how the data are stored...
> 
> Any help would be greatly appreciated.
> 
> Thanks,
> Andrew
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers, 
Joel


Joel F. Kincaid, Ph. D.
Assistant Professor
Department of Economics and Finance
Franklin P. Perdue School of Business
Salisbury University
Salisbury Maryland, 21801
Phone: (410) 548-4416
Email:   jfkincaid at salisbury.edu



From mrennie at utm.utoronto.ca  Tue Jul 15 22:24:47 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Tue, 15 Jul 2003 16:24:47 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <200307151854.LAA23105@hivnet.ubc.ca>
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
Message-ID: <5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>

At 11:47 AM 7/15/03 -0700, Jerome Asselin wrote:

>Mike,
>
>The definition of your function f() seems quite inefficient. You could
>vectorize it, which would shorten and speed up your code, especially if M
>is large.

Hi, Jerome

I don;t think I can vectorize it, since in the iteration loop, the value 
for each [i] is dependent on the value of [i-1], so I require the loop to 
go through each [i] before I can get my values for any particular vector 
(variable).  I actually had my program operating this way in the first 
place, but I get all sorts of warnings and the 'optim' function especially 
doesn't seem to appreciate it.

>See the R introduction file available online to learn how to do
>it if you don't already know how. Also, you have to return only one
>argument. Unless I'm wrong, your function wants to return Wtmod, Hgtmod, q
>and f. I'm don't think this would change anything in this case, but you
>should definitely clean this up!

The calls to Wtmod, q, and Hgtmod are all just residual from the 
development of the loop inside function f.  I would like to get the last 
line of 'bioday' reported from within the loop, had I figured out the 
optimization, but that point is rather moot unless I can get the 
optimization functioning.

>Another advice... If you can simplify your example into a few lines of
>"ready-to-execute" code with a toy dataset, then it's easy for everyone to
>try it out and you can get more feedback. The code you've included is
>quite large and cumbersome. For one thing, you could easily have removed
>the lines of code that were commented out.
>
>Meanwhile, I would suggest that you go back to the basics of R to clean up
>your code.

Thanks for the advice- every bit helps if I eventually get this thing to 
work.....

Mike

>Sorry I can't be more helpful.
>Jerome
>
>On July 15, 2003 10:46 am, Michael Rennie wrote:
> > Hi there
> >
> > I thought this would be of particular interest to people using 'optim'
> > functions and perhaps people involved with R development.
> >
> > I've been beaten down by R trying to get it to perform an optimization
> > on a mass-balance model.  I've written the same program in excel, and
> > using the 'solver' function, it comes up with an answer for my variables
> > (p, ACT, which I've assigned to q in R) that gives a solution to the
> > function "f" in about 3 seconds, with a value of the function around
> > 0.0004. R, on the other hand, appears to get stuck in local minima, and
> > spits back an approximation that is close the the p, ACT values excel
> > does, but not nearly precise enough for my needs, and not nearly as
> > precise as excel, and it takes about 3 minutes.  Also, the solution for
> > the value it returns for the function is about 8 orders of magnitude
> > greater than the excel version, so I can't really say the function is
> > approximating zero.  I was able to determine this using a  "trace"
> > command on function f, which is listed below.
> >
> > This is very likely due to the fact that I've made some coding error
> > along the way, or have done something else wrong, but I don't know.
> > Either way, I am shocked and surprised that a program like excel is
> > outperforming R.  I've attached my command file and the dataset
> > "temp.dat" at the bottom of this e-mail for anyone who would like to
> > fiddle around with it, and if you come up with something, PLEASE let me
> > know- In the meantime, I've got to start fiddling with excel and
> > figuring out how to automate the solver calculation.....
> >
> > Briefly, the point of the program is to approximate the model output
> > from an iterative calculation, Wtmod and Hgtmod, to user-specified
> > endpoints Wt and Hgt, by seeking the optimal values of p, ACT involved
> > in the iterative process.
> >
> > Also, if your interested in recent correspondence that explains the
> > point of the program a bit, and how the function ties in to the
> > iterative process, search the R help forum for e-mails entitled "[R]
> > problem with coding for 'optim' in R".  Thanks also to Roger Peng and
> > numerous others for helping me get this far.
> >
> > The whole point of me doing this in R was because it's supposed to be
> > spectacularly fast at automating complex loops, but seems to be falling
> > short for this application.  Hopefully it's something wrong with my
> > coding and not with R itself.
> >
> > Mike
> >
> > R COMMAND FILE:
> >
> > ####################################
> > #    perch.R                       #
> > # Hewett and Johnson bioenergetics #
> > # model combined with              #
> > # Trudel MMBM to estimate          #
> > # Consumption in perch in R code   #
> > # Execute with                     #
> > # R --vanilla < perch.R > perch.out#
> > ####################################
> >
> > #USER INPUT BELOW
> >
> > #Weight at time 0
> > Wo<- 9.2
> >
> > #Hg concentration at time 0 (ugHg/g wet weight)
> > Hgo<- 0.08
> >
> > #Weight at time t
> > Wt<- 32.2
> >
> > #Hg concentration at time t (ugHg/g wet weight)
> > Hgt<- 0.110
> >
> > #Prey methylmercury concentration (as constant)
> > Hgp<- 0.033
> >
> > #Prey caloric value (as constant)
> > Pc<- 800
> >
> > #Energy density of fish (as constant, calories)
> > Ef <- 1000
> >
> > #Maturity status, 0=immature, 1=mature
> > Mat<- 0
> >
> > #Sex, 1=male, 2=female
> > Sex<- 1
> >
> > #USER INPUT ABOVE
> >
> > #Bioenergetics parameters for perch
> > CA <- 0.25
> > CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> > CQ <- 2.3
> > CTO <- 23
> > CTM <- 28
> > Zc<- (log(CQ))*(CTM-CTO)
> > Yc<- (log(CQ))*(CTM-CTO+2)
> > Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> >
> > RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> > RB <- 0.8   #same as 1+(-0.2) see above...
> > RQ <- 2.1
> > RTO <- 28
> > RTM <- 33
> > Za <- (log(RQ))*(RTM-RTO)
> > Ya<- (log(RQ))*(RTM-RTO+2)
> > Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> >
> > S <- 0.172
> >
> > FA <- 0.158
> > FB <- -0.222
> > FG <- 0.631
> >
> > UA<- 0.0253
> > UB<- 0.58
> > UG<- -0.299
> >
> > #Mass balance model parameters
> > EA <- 0.002938
> > EB <- -0.2
> > EQ <- 0.066
> > a <- 0.8
> >
> > #Specifying sex-specific parameters
> >
> > GSI<- NULL
> >
> > if (Sex==1) GSI<-0.05 else
> > if (Sex==2) GSI<-0.17
> >
> > # Define margin of error functions
> > #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a
> > proportion #    {
> > #    z <- qnorm(1-alpha/2)
> > #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo) sample
> > size #    merror
> > #    }
> >
> > #Bring in temp file
> >
> > temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0,
> > Temp=0))
> >
> > Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
> >
> > temp<- cbind (Day, jday, Temp)
> > #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> > #temp [,2]
> >
> > Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> > Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> >
> > comp<- cbind (Day, jday, Temp, Vc, Vr)
> >
> > #comp
> >
> > bio<-matrix(NA, ncol=13, nrow=length(Day))
> > W<-NULL
> > C<-NULL
> > ASMR<-NULL
> > SMR<-NULL
> > A<-NULL
> > F<-NULL
> > U<-NULL
> > SDA<-NULL
> > Gr<-NULL
> > Hg<-NULL
> > Ed<-NULL
> > GHg<-NULL
> > K<-NULL
> > Expegk<-NULL
> > EGK<-NULL
> > p<-NULL
> > ACT<-NULL
> >
> > #starting values for p, ACT
> > p <- 1 #  0.558626306252032 #solution set for p, ACT from excel 'solver'
> > f'n ACT <- 2 #  1.66764519286918
> >
> > q<-c(p,ACT)
> >
> > #specify sttarting values
> > #q0<-c(p = 1, ACT = 1)
> >
> > #introduce function to solve
> > f <- function (q)
> > {
> >
> >
> > M<- length(Day) #number of days iterated
> >
> > for (i in 1:M)
> > {
> >
> > #Bioenergetics model
> >
> > if (Day[i]==1) W[i] <- Wo else
> > if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else
> > W[i] <- (W[i-1]+(Gr[i-1]/Ef))
> > #W
> >
> > #W<-Wo
> >
> > C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
> >
> > ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
> >
> > SMR[i]<- (ASMR[i]/q[2])
> >
> > A[i]<- (ASMR[i]-SMR[i])
> >
> > F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
> >
> > U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
> >
> > SDA[i]<- (S*(C[i]-F[i]))
> >
> > Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
> >
> > #Trudel MMBM
> >
> > if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <-
> > a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
> >
> > Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
> >
> > GHg[i] <- Gr[i]/Ef/W[i]
> >
> > if (Sex==1)
> > K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M else
> > if (Sex==2)
> > K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M # =
> > dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g
> > # then express as Q times GSI gives K / M gives daily K
> >
> > EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
> >
> > Expegk[i] <- exp(-1*EGK[i])
> >
> > bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> >
> > }
> >
> > #warnings()
> >
> > dimnames (bio) <-list(NULL, c("W", "C", "ASMR", "SMR", "A", "F", "U",
> > "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> >
> > bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> >
> > dimnames (bioday) <-list(NULL, c("jday", "W", "C", "ASMR", "SMR", "A",
> > "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> >
> > #bioday
> >
> > Wtmod<- bioday [length(W),2]
> > Wtmod
> >
> > Hgtmod<- bioday [length(Hg),14]
> > Hgtmod
> >
> > q
> >
> > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))^2) ; f
> >
> > #warnings()
> >
> > #write.table (bioday, file = "perch.csv", append = FALSE, sep=",", na =
> > NA, col.names = TRUE)
> >
> >
> >
> > #nlm(f,c(1,1))
> > }
> >
> > optim(q, f, method = "L-BFGS-B",
> >          lower = c(0.2, 1), upper=c(2, 3),
> >          control = list(fnscale = 0.001))
> >
> >
> > TRACE FUNCTION USED TO DETERMINE WHERE R IS GETTING STUCK;
> >
> >   trace("f", quote(print(q)), at = 1, print = FALSE)
> >
> > o <- optim(c(1,2), f, method = "L-BFGS-B", lower = c(0.2,1), upper =
> > c(2,3))
> >
> > DATA FOR TEMP.DAT:
> >
> > 1       153     9.4
> > 2       154     9.6
> > 3       155     9.8
> > 4       156     10
> > 5       157     10.2
> > 6       158     10.4
> > 7       159     10.6
> > 8       160     10.8
> > 9       161     11
> > 10      162     11.2
> > 11      163     11.4
> > 12      164     11.6
> > 13      165     11.8
> > 14      166     12
> > 15      167     12.3
> > 16      168     12.5
> > 17      169     12.7
> > 18      170     12.9
> > 19      171     13.1
> > 20      172     13.4
> > 21      173     13.6
> > 22      174     13.8
> > 23      175     14
> > 24      176     14.2
> > 25      177     14.5
> > 26      178     14.7
> > 27      179     14.9
> > 28      180     15.1
> > 29      181     15.4
> > 30      182     15.6
> > 31      183     15.8
> > 32      184     16
> > 33      185     16.2
> > 34      186     16.5
> > 35      187     16.7
> > 36      188     16.9
> > 37      189     17.1
> > 38      190     17.3
> > 39      191     17.5
> > 40      192     17.7
> > 41      193     17.9
> > 42      194     18.1
> > 43      195     18.3
> > 44      196     18.5
> > 45      197     18.7
> > 46      198     18.9
> > 47      199     19
> > 48      200     19.2
> > 49      201     19.4
> > 50      202     19.5
> > 51      203     19.7
> > 52      204     19.9
> > 53      205     20
> > 54      206     20.2
> > 55      207     20.3
> > 56      208     20.4
> > 57      209     20.5
> > 58      210     20.7
> > 59      211     20.8
> > 60      212     20.9
> > 61      213     21
> > 62      214     21.1
> > 63      215     21.2
> > 64      216     21.3
> > 65      217     21.3
> > 66      218     21.4
> > 67      219     21.5
> > 68      220     21.5
> > 69      221     21.6
> > 70      222     21.6
> > 71      223     21.6
> > 72      224     21.7
> > 73      225     21.7
> > 74      226     21.7
> > 75      227     21.7
> > 76      228     21.7
> > 77      229     21.7
> > 78      230     21.7
> > 79      231     21.6
> > 80      232     21.6
> > 81      233     21.6
> > 82      234     21.5
> > 83      235     21.5
> > 84      236     21.4
> > 85      237     21.3
> > 86      238     21.3
> > 87      239     21.2
> > 88      240     21.1
> > 89      241     21
> > 90      242     20.9
> > 91      243     20.8
> > 92      244     20.7
> > 93      245     20.5
> > 94      246     20.4
> > 95      247     20.3
> > 96      248     20.2
> > 97      249     20
> > 98      250     19.9
> > 99      251     19.7
> > 100     252     19.5
> > 101     253     19.4
> > 102     254     19.2
> > 103     255     19
> > 104     256     18.9
> > 105     257     18.7
> > 106     258     18.5
> > 107     259     18.3
> > 108     260     18.1
> > 109     261     17.9
> > 110     262     17.7
> > 111     263     17.5
> > 112     264     17.3
> > 113     265     17.1
> > 114     266     16.9
> > 115     267     16.7
> > 116     268     16.5
> > 117     269     16.2
> > 118     270     16
> > 119     271     15.8
> > 120     272     15.6
> > 121     273     15.4
> > 122     274     15.1
> > 123     275     14.9
> > 124     276     14.7
> > 125     277     14.5
> > 126     278     14.2
> > 127     279     14
> > 128     280     13.8
> > 129     281     13.6
> > 130     282     13.4
> > 131     283     13.1
> > 132     284     12.9
> > 133     285     12.7
> > 134     286     12.5
> > 135     287     12.3
> > 136     288     12
> > 137     289     11.8
> > 138     290     11.6
> > 139     291     11.4
> > 140     292     11.2
> > 141     293     11
> > 142     294     10.8
> > 143     295     10.6
> > 144     296     10.4
> > 145     297     10.2
> > 146     298     10
> > 147     299     9.8
> > 148     300     9.6
> > 149     301     9.4
> > 150     302     9.3
> > 151     303     9.1
> > 152     304     8.9
> > 153     305     8.7
> > 154     306     8.6
> > 155     307     8.4
> > 156     308     8.2
> > 157     309     8.1
> > 158     310     7.9
> > 159     311     7.8
> > 160     312     7.6
> > 161     313     7.5
> > 162     314     7.3
> > 163     315     7.2
> > 164     316     7
> > 165     317     6.9
> > 166     318     6.8
> > 167     319     6.7
> > 168     320     6.5
> > 169     321     6.4
> > 170     322     6.3
> > 171     323     6.2
> > 172     324     6.1
> > 173     325     6
> > 174     326     5.8
> > 175     327     5.7
> > 176     328     5.6
> > 177     329     5.5
> > 178     330     5.5
> > 179     331     5.4
> > 180     332     5.3
> > 181     333     5.2
> > 182     334     5.1
> > 183     335     5
> > 184     336     5
> > 185     337     4.9
> > 186     338     4.8
> > 187     339     4.7
> > 188     340     4.7
> > 189     341     4.6
> > 190     342     4.5
> > 191     343     4.5
> > 192     344     4.4
> > 193     345     4.4
> > 194     346     4.3
> > 195     347     4.3
> > 196     348     4.2
> > 197     349     4.2
> > 198     350     4.1
> > 199     351     4.1
> > 200     352     4
> > 201     353     4
> > 202     354     4
> > 203     355     3.9
> > 204     356     3.9
> > 205     357     3.8
> > 206     358     3.8
> > 207     359     3.8
> > 208     360     3.8
> > 209     361     3.7
> > 210     362     3.7
> > 211     363     3.7
> > 212     364     3.6
> > 213     365     3.6
> > 214     366     3.6
> > 215     1       3.2
> > 216     2       3.2
> > 217     3       3.2
> > 218     4       3.2
> > 219     5       3.2
> > 220     6       3.2
> > 221     7       3.2
> > 222     8       3.2
> > 223     9       3.2
> > 224     10      3.2
> > 225     11      3.2
> > 226     12      3.2
> > 227     13      3.2
> > 228     14      3.2
> > 229     15      3.2
> > 230     16      3.2
> > 231     17      3.2
> > 232     18      3.2
> > 233     19      3.2
> > 234     20      3.2
> > 235     21      3.2
> > 236     22      3.2
> > 237     23      3.2
> > 238     24      3.2
> > 239     25      3.2
> > 240     26      3.2
> > 241     27      3.2
> > 242     28      3.2
> > 243     29      3.2
> > 244     30      3.2
> > 245     31      3.2
> > 246     32      3.2
> > 247     33      3.2
> > 248     34      3.2
> > 249     35      3.2
> > 250     36      3.2
> > 251     37      3.2
> > 252     38      3.2
> > 253     39      3.2
> > 254     40      3.2
> > 255     41      3.2
> > 256     42      3.2
> > 257     43      3.2
> > 258     44      3.2
> > 259     45      3.2
> > 260     46      3.2
> > 261     47      3.2
> > 262     48      3.2
> > 263     49      3.2
> > 264     50      3.2
> > 265     51      3.2
> > 266     52      3.2
> > 267     53      3.2
> > 268     54      3.3
> > 269     55      3.3
> > 270     56      3.3
> > 271     57      3.3
> > 272     58      3.3
> > 273     59      3.3
> > 274     60      3.3
> > 275     61      3.3
> > 276     62      3.3
> > 277     63      3.3
> > 278     64      3.3
> > 279     65      3.3
> > 280     66      3.3
> > 281     67      3.3
> > 282     68      3.3
> > 283     69      3.3
> > 284     70      3.3
> > 285     71      3.4
> > 286     72      3.4
> > 287     73      3.4
> > 288     74      3.4
> > 289     75      3.4
> > 290     76      3.4
> > 291     77      3.4
> > 292     78      3.4
> > 293     79      3.5
> > 294     80      3.5
> > 295     81      3.5
> > 296     82      3.5
> > 297     83      3.5
> > 298     84      3.5
> > 299     85      3.6
> > 300     86      3.6
> > 301     87      3.6
> > 302     88      3.6
> > 303     89      3.6
> > 304     90      3.7
> > 305     91      3.7
> > 306     92      3.7
> > 307     93      3.8
> > 308     94      3.8
> > 309     95      3.8
> > 310     96      3.8
> > 311     97      3.9
> > 312     98      3.9
> > 313     99      4
> > 314     100     4
> > 315     101     4
> > 316     102     4.1
> > 317     103     4.1
> > 318     104     4.2
> > 319     105     4.2
> > 320     106     4.3
> > 321     107     4.3
> > 322     108     4.4
> > 323     109     4.4
> > 324     110     4.5
> > 325     111     4.5
> > 326     112     4.6
> > 327     113     4.7
> > 328     114     4.7
> > 329     115     4.8
> > 330     116     4.9
> > 331     117     5
> > 332     118     5
> > 333     119     5.1
> > 334     120     5.2
> > 335     121     5.3
> > 336     122     5.4
> > 337     123     5.5
> > 338     124     5.5
> > 339     125     5.6
> > 340     126     5.7
> > 341     127     5.8
> > 342     128     6
> > 343     129     6.1
> > 344     130     6.2
> > 345     131     6.3
> > 346     132     6.4
> > 347     133     6.5
> > 348     134     6.7
> > 349     135     6.8
> > 350     136     6.9
> > 351     137     7
> > 352     138     7.2
> > 353     139     7.3
> > 354     140     7.5
> > 355     141     7.6
> > 356     142     7.8
> > 357     143     7.9
> > 358     144     8.1
> > 359     145     8.2
> > 360     146     8.4
> > 361     147     8.6
> > 362     148     8.7
> > 363     149     8.9
> > 364     150     9.1
> > 365     151     9.3
> > 366     152     9.3
> >
> >
> >
> > Michael Rennie
> > M.Sc. Candidate
> > University of Toronto at Mississauga
> > 3359 Mississauga Rd. N.
> > Mississauga, ON  L5L 1C6
> > Ph: 905-828-5452  Fax: 905-828-3792
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From jfkincaidsu at netscape.net  Tue Jul 15 22:30:05 2003
From: jfkincaidsu at netscape.net (Joel Kincaid)
Date: Tue, 15 Jul 2003 16:30:05 -0400
Subject: [R] Plotting a graph of many lines between groups of points...
References: <0HI300FC8023Y8@mail-mta5.service.ohio-state.edu>
Message-ID: <3F14644D.6010903@netscape.net>

johnson.2060 at osu.edu wrote:
 > I have a data file read into a data frame.
 >
 > For example,
 >
 >     V1  V2  V3  V4
 > 1   1   1   3   4
 > 2   2   3   5   10
 > .   .   .   .   .
 > .   .   .   .   .
 > n          V1[n]     V2[n]     V3[n]     V4[n]
 >
 > to n=many thousand
 >
is this stored as a data.frame? a matrix? In any case there seems to be 
a problem with your indexing ...
 > I want to plot a graph with many line segments, where v1[i]=x1, v2[i]=y1,
 > v3[i]=x2, v4[i]=y2 for i=1,n.
indexing is ok if you have df$v3[i] ....
 >
 > This seems relatively simple in theory but I've spent quite a bit of time
 > trying to make it happen with plot(type=), points(x,y), or lines(x,y) 
to no
 > avail.
could you send the actual code for your plot? Perhaps some way to 
construct the data. at this point things are a bit murky.
 >
 > Do I need to turn these into vectors before plotting them?
Thus my question about how the data are stored...
 >
 > Any help would be greatly appreciated.
 >
 > Thanks,
 > Andrew
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers,
Joel

Joel F. Kincaid, Ph. D.
Assistant Professor
Department of Economics and Finance
Franklin P. Perdue School of Business
Salisbury University
Salisbury Maryland, 21801
Phone: (410) 548-4416
Email:   jfkincaid at salisbury.edu



From shidaxia at yahoo.com  Tue Jul 15 22:37:05 2003
From: shidaxia at yahoo.com (Shi, Tao)
Date: Tue, 15 Jul 2003 13:37:05 -0700 (PDT)
Subject: [R] Why two chisq.test p values differ when the contingency 
In-Reply-To: <XFMail.030715210004.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20030715203705.25726.qmail@web13408.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/b6ceb3dd/attachment.pl

From jerome at hivnet.ubc.ca  Tue Jul 15 22:41:59 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 15 Jul 2003 13:41:59 -0700
Subject: [R] Excel can do what R can't?????
In-Reply-To: <5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
Message-ID: <200307152048.NAA28133@hivnet.ubc.ca>


I thought that you could simplify your code by using something like 
c(0,W[-length(W)]) as opposed to W[i-1] in a loop, but now I understand 
it's not that easy. Unless you can analytically simplify the calculation 
of W in order to vectorize it, it's going to be slow.

However, many of the lines don't depend on [i] and not on [i-1]. Therefore 
you could simplify those as they don't need to be calculated within the 
loop.

HTH,
Jerome

On July 15, 2003 01:24 pm, Michael Rennie wrote:
> At 11:47 AM 7/15/03 -0700, Jerome Asselin wrote:
> >Mike,
> >
> >The definition of your function f() seems quite inefficient. You could
> >vectorize it, which would shorten and speed up your code, especially if
> > M is large.
>
> Hi, Jerome
>
> I don;t think I can vectorize it, since in the iteration loop, the value
> for each [i] is dependent on the value of [i-1], so I require the loop
> to go through each [i] before I can get my values for any particular
> vector (variable).  I actually had my program operating this way in the
> first place, but I get all sorts of warnings and the 'optim' function
> especially doesn't seem to appreciate it.
>
> >See the R introduction file available online to learn how to do
> >it if you don't already know how. Also, you have to return only one
> >argument. Unless I'm wrong, your function wants to return Wtmod,
> > Hgtmod, q and f. I'm don't think this would change anything in this
> > case, but you should definitely clean this up!
>
> The calls to Wtmod, q, and Hgtmod are all just residual from the
> development of the loop inside function f.  I would like to get the last
> line of 'bioday' reported from within the loop, had I figured out the
> optimization, but that point is rather moot unless I can get the
> optimization functioning.
>
> >Another advice... If you can simplify your example into a few lines of
> >"ready-to-execute" code with a toy dataset, then it's easy for everyone
> > to try it out and you can get more feedback. The code you've included
> > is quite large and cumbersome. For one thing, you could easily have
> > removed the lines of code that were commented out.
> >
> >Meanwhile, I would suggest that you go back to the basics of R to clean
> > up your code.
>
> Thanks for the advice- every bit helps if I eventually get this thing to
> work.....
>
> Mike
>
> >Sorry I can't be more helpful.
> >Jerome
> >
> >On July 15, 2003 10:46 am, Michael Rennie wrote:
> > > Hi there
> > >
> > > I thought this would be of particular interest to people using
> > > 'optim' functions and perhaps people involved with R development.
> > >
> > > I've been beaten down by R trying to get it to perform an
> > > optimization on a mass-balance model.  I've written the same program
> > > in excel, and using the 'solver' function, it comes up with an
> > > answer for my variables (p, ACT, which I've assigned to q in R) that
> > > gives a solution to the function "f" in about 3 seconds, with a
> > > value of the function around 0.0004. R, on the other hand, appears
> > > to get stuck in local minima, and spits back an approximation that
> > > is close the the p, ACT values excel does, but not nearly precise
> > > enough for my needs, and not nearly as precise as excel, and it
> > > takes about 3 minutes.  Also, the solution for the value it returns
> > > for the function is about 8 orders of magnitude greater than the
> > > excel version, so I can't really say the function is approximating
> > > zero.  I was able to determine this using a  "trace" command on
> > > function f, which is listed below.
> > >
> > > This is very likely due to the fact that I've made some coding error
> > > along the way, or have done something else wrong, but I don't know.
> > > Either way, I am shocked and surprised that a program like excel is
> > > outperforming R.  I've attached my command file and the dataset
> > > "temp.dat" at the bottom of this e-mail for anyone who would like to
> > > fiddle around with it, and if you come up with something, PLEASE let
> > > me know- In the meantime, I've got to start fiddling with excel and
> > > figuring out how to automate the solver calculation.....
> > >
> > > Briefly, the point of the program is to approximate the model output
> > > from an iterative calculation, Wtmod and Hgtmod, to user-specified
> > > endpoints Wt and Hgt, by seeking the optimal values of p, ACT
> > > involved in the iterative process.
> > >
> > > Also, if your interested in recent correspondence that explains the
> > > point of the program a bit, and how the function ties in to the
> > > iterative process, search the R help forum for e-mails entitled "[R]
> > > problem with coding for 'optim' in R".  Thanks also to Roger Peng
> > > and numerous others for helping me get this far.
> > >
> > > The whole point of me doing this in R was because it's supposed to
> > > be spectacularly fast at automating complex loops, but seems to be
> > > falling short for this application.  Hopefully it's something wrong
> > > with my coding and not with R itself.
> > >
> > > Mike
> > >
> > > R COMMAND FILE:
> > >
> > > ####################################
> > > #    perch.R                       #
> > > # Hewett and Johnson bioenergetics #
> > > # model combined with              #
> > > # Trudel MMBM to estimate          #
> > > # Consumption in perch in R code   #
> > > # Execute with                     #
> > > # R --vanilla < perch.R > perch.out#
> > > ####################################
> > >
> > > #USER INPUT BELOW
> > >
> > > #Weight at time 0
> > > Wo<- 9.2
> > >
> > > #Hg concentration at time 0 (ugHg/g wet weight)
> > > Hgo<- 0.08
> > >
> > > #Weight at time t
> > > Wt<- 32.2
> > >
> > > #Hg concentration at time t (ugHg/g wet weight)
> > > Hgt<- 0.110
> > >
> > > #Prey methylmercury concentration (as constant)
> > > Hgp<- 0.033
> > >
> > > #Prey caloric value (as constant)
> > > Pc<- 800
> > >
> > > #Energy density of fish (as constant, calories)
> > > Ef <- 1000
> > >
> > > #Maturity status, 0=immature, 1=mature
> > > Mat<- 0
> > >
> > > #Sex, 1=male, 2=female
> > > Sex<- 1
> > >
> > > #USER INPUT ABOVE
> > >
> > > #Bioenergetics parameters for perch
> > > CA <- 0.25
> > > CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get
> > > cal/d CQ <- 2.3
> > > CTO <- 23
> > > CTM <- 28
> > > Zc<- (log(CQ))*(CTM-CTO)
> > > Yc<- (log(CQ))*(CTM-CTO+2)
> > > Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> > >
> > > RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> > > RB <- 0.8   #same as 1+(-0.2) see above...
> > > RQ <- 2.1
> > > RTO <- 28
> > > RTM <- 33
> > > Za <- (log(RQ))*(RTM-RTO)
> > > Ya<- (log(RQ))*(RTM-RTO+2)
> > > Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> > >
> > > S <- 0.172
> > >
> > > FA <- 0.158
> > > FB <- -0.222
> > > FG <- 0.631
> > >
> > > UA<- 0.0253
> > > UB<- 0.58
> > > UG<- -0.299
> > >
> > > #Mass balance model parameters
> > > EA <- 0.002938
> > > EB <- -0.2
> > > EQ <- 0.066
> > > a <- 0.8
> > >
> > > #Specifying sex-specific parameters
> > >
> > > GSI<- NULL
> > >
> > > if (Sex==1) GSI<-0.05 else
> > > if (Sex==2) GSI<-0.17
> > >
> > > # Define margin of error functions
> > > #merror <- function(phat,M,alpha) # (1-alpha)*100% merror for a
> > > proportion #    {
> > > #    z <- qnorm(1-alpha/2)
> > > #    merror <- z * sqrt(phat*(1-phat)/M)  # M is (Monte Carlo)
> > > sample size #    merror
> > > #    }
> > >
> > > #Bring in temp file
> > >
> > > temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0,
> > > Temp=0))
> > >
> > > Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
> > >
> > > temp<- cbind (Day, jday, Temp)
> > > #Day = number of days modelled, jday=julian day, Temp = daily avg.
> > > temp. #temp [,2]
> > >
> > > Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> > > Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> > >
> > > comp<- cbind (Day, jday, Temp, Vc, Vr)
> > >
> > > #comp
> > >
> > > bio<-matrix(NA, ncol=13, nrow=length(Day))
> > > W<-NULL
> > > C<-NULL
> > > ASMR<-NULL
> > > SMR<-NULL
> > > A<-NULL
> > > F<-NULL
> > > U<-NULL
> > > SDA<-NULL
> > > Gr<-NULL
> > > Hg<-NULL
> > > Ed<-NULL
> > > GHg<-NULL
> > > K<-NULL
> > > Expegk<-NULL
> > > EGK<-NULL
> > > p<-NULL
> > > ACT<-NULL
> > >
> > > #starting values for p, ACT
> > > p <- 1 #  0.558626306252032 #solution set for p, ACT from excel
> > > 'solver' f'n ACT <- 2 #  1.66764519286918
> > >
> > > q<-c(p,ACT)
> > >
> > > #specify sttarting values
> > > #q0<-c(p = 1, ACT = 1)
> > >
> > > #introduce function to solve
> > > f <- function (q)
> > > {
> > >
> > >
> > > M<- length(Day) #number of days iterated
> > >
> > > for (i in 1:M)
> > > {
> > >
> > > #Bioenergetics model
> > >
> > > if (Day[i]==1) W[i] <- Wo else
> > > if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else
> > > W[i] <- (W[i-1]+(Gr[i-1]/Ef))
> > > #W
> > >
> > > #W<-Wo
> > >
> > > C[i]<-
> > > q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
> > >
> > > ASMR[i]<-
> > > q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
> > >
> > > SMR[i]<- (ASMR[i]/q[2])
> > >
> > > A[i]<- (ASMR[i]-SMR[i])
> > >
> > > F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
> > >
> > > U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
> > >
> > > SDA[i]<- (S*(C[i]-F[i]))
> > >
> > > Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
> > >
> > > #Trudel MMBM
> > >
> > > if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <-
> > > a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-Expegk[i-1])+(Hg[i-1]*Expegk[i-
> > >1])
> > >
> > > Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
> > >
> > > GHg[i] <- Gr[i]/Ef/W[i]
> > >
> > > if (Sex==1)
> > > K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M
> > > else if (Sex==2)
> > > K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M #
> > > = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to
> > > ug/g # then express as Q times GSI gives K / M gives daily K
> > >
> > > EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
> > >
> > > Expegk[i] <- exp(-1*EGK[i])
> > >
> > > bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> > >
> > > }
> > >
> > > #warnings()
> > >
> > > dimnames (bio) <-list(NULL, c("W", "C", "ASMR", "SMR", "A", "F",
> > > "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> > >
> > > bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK,
> > > Hg)
> > >
> > > dimnames (bioday) <-list(NULL, c("jday", "W", "C", "ASMR", "SMR",
> > > "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> > >
> > > #bioday
> > >
> > > Wtmod<- bioday [length(W),2]
> > > Wtmod
> > >
> > > Hgtmod<- bioday [length(Hg),14]
> > > Hgtmod
> > >
> > > q
> > >
> > > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))^2) ;
> > > f
> > >
> > > #warnings()
> > >
> > > #write.table (bioday, file = "perch.csv", append = FALSE, sep=",",
> > > na = NA, col.names = TRUE)
> > >
> > >
> > >
> > > #nlm(f,c(1,1))
> > > }
> > >
> > > optim(q, f, method = "L-BFGS-B",
> > >          lower = c(0.2, 1), upper=c(2, 3),
> > >          control = list(fnscale = 0.001))
> > >
> > >
> > > TRACE FUNCTION USED TO DETERMINE WHERE R IS GETTING STUCK;
> > >
> > >   trace("f", quote(print(q)), at = 1, print = FALSE)
> > >
> > > o <- optim(c(1,2), f, method = "L-BFGS-B", lower = c(0.2,1), upper =
> > > c(2,3))
> > >
> > > DATA FOR TEMP.DAT:
> > >
> > > 1       153     9.4
> > > 2       154     9.6
> > > 3       155     9.8
> > > 4       156     10
> > > 5       157     10.2
> > > 6       158     10.4
> > > 7       159     10.6
> > > 8       160     10.8
> > > 9       161     11
> > > 10      162     11.2
> > > 11      163     11.4
> > > 12      164     11.6
> > > 13      165     11.8
> > > 14      166     12
> > > 15      167     12.3
> > > 16      168     12.5
> > > 17      169     12.7
> > > 18      170     12.9
> > > 19      171     13.1
> > > 20      172     13.4
> > > 21      173     13.6
> > > 22      174     13.8
> > > 23      175     14
> > > 24      176     14.2
> > > 25      177     14.5
> > > 26      178     14.7
> > > 27      179     14.9
> > > 28      180     15.1
> > > 29      181     15.4
> > > 30      182     15.6
> > > 31      183     15.8
> > > 32      184     16
> > > 33      185     16.2
> > > 34      186     16.5
> > > 35      187     16.7
> > > 36      188     16.9
> > > 37      189     17.1
> > > 38      190     17.3
> > > 39      191     17.5
> > > 40      192     17.7
> > > 41      193     17.9
> > > 42      194     18.1
> > > 43      195     18.3
> > > 44      196     18.5
> > > 45      197     18.7
> > > 46      198     18.9
> > > 47      199     19
> > > 48      200     19.2
> > > 49      201     19.4
> > > 50      202     19.5
> > > 51      203     19.7
> > > 52      204     19.9
> > > 53      205     20
> > > 54      206     20.2
> > > 55      207     20.3
> > > 56      208     20.4
> > > 57      209     20.5
> > > 58      210     20.7
> > > 59      211     20.8
> > > 60      212     20.9
> > > 61      213     21
> > > 62      214     21.1
> > > 63      215     21.2
> > > 64      216     21.3
> > > 65      217     21.3
> > > 66      218     21.4
> > > 67      219     21.5
> > > 68      220     21.5
> > > 69      221     21.6
> > > 70      222     21.6
> > > 71      223     21.6
> > > 72      224     21.7
> > > 73      225     21.7
> > > 74      226     21.7
> > > 75      227     21.7
> > > 76      228     21.7
> > > 77      229     21.7
> > > 78      230     21.7
> > > 79      231     21.6
> > > 80      232     21.6
> > > 81      233     21.6
> > > 82      234     21.5
> > > 83      235     21.5
> > > 84      236     21.4
> > > 85      237     21.3
> > > 86      238     21.3
> > > 87      239     21.2
> > > 88      240     21.1
> > > 89      241     21
> > > 90      242     20.9
> > > 91      243     20.8
> > > 92      244     20.7
> > > 93      245     20.5
> > > 94      246     20.4
> > > 95      247     20.3
> > > 96      248     20.2
> > > 97      249     20
> > > 98      250     19.9
> > > 99      251     19.7
> > > 100     252     19.5
> > > 101     253     19.4
> > > 102     254     19.2
> > > 103     255     19
> > > 104     256     18.9
> > > 105     257     18.7
> > > 106     258     18.5
> > > 107     259     18.3
> > > 108     260     18.1
> > > 109     261     17.9
> > > 110     262     17.7
> > > 111     263     17.5
> > > 112     264     17.3
> > > 113     265     17.1
> > > 114     266     16.9
> > > 115     267     16.7
> > > 116     268     16.5
> > > 117     269     16.2
> > > 118     270     16
> > > 119     271     15.8
> > > 120     272     15.6
> > > 121     273     15.4
> > > 122     274     15.1
> > > 123     275     14.9
> > > 124     276     14.7
> > > 125     277     14.5
> > > 126     278     14.2
> > > 127     279     14
> > > 128     280     13.8
> > > 129     281     13.6
> > > 130     282     13.4
> > > 131     283     13.1
> > > 132     284     12.9
> > > 133     285     12.7
> > > 134     286     12.5
> > > 135     287     12.3
> > > 136     288     12
> > > 137     289     11.8
> > > 138     290     11.6
> > > 139     291     11.4
> > > 140     292     11.2
> > > 141     293     11
> > > 142     294     10.8
> > > 143     295     10.6
> > > 144     296     10.4
> > > 145     297     10.2
> > > 146     298     10
> > > 147     299     9.8
> > > 148     300     9.6
> > > 149     301     9.4
> > > 150     302     9.3
> > > 151     303     9.1
> > > 152     304     8.9
> > > 153     305     8.7
> > > 154     306     8.6
> > > 155     307     8.4
> > > 156     308     8.2
> > > 157     309     8.1
> > > 158     310     7.9
> > > 159     311     7.8
> > > 160     312     7.6
> > > 161     313     7.5
> > > 162     314     7.3
> > > 163     315     7.2
> > > 164     316     7
> > > 165     317     6.9
> > > 166     318     6.8
> > > 167     319     6.7
> > > 168     320     6.5
> > > 169     321     6.4
> > > 170     322     6.3
> > > 171     323     6.2
> > > 172     324     6.1
> > > 173     325     6
> > > 174     326     5.8
> > > 175     327     5.7
> > > 176     328     5.6
> > > 177     329     5.5
> > > 178     330     5.5
> > > 179     331     5.4
> > > 180     332     5.3
> > > 181     333     5.2
> > > 182     334     5.1
> > > 183     335     5
> > > 184     336     5
> > > 185     337     4.9
> > > 186     338     4.8
> > > 187     339     4.7
> > > 188     340     4.7
> > > 189     341     4.6
> > > 190     342     4.5
> > > 191     343     4.5
> > > 192     344     4.4
> > > 193     345     4.4
> > > 194     346     4.3
> > > 195     347     4.3
> > > 196     348     4.2
> > > 197     349     4.2
> > > 198     350     4.1
> > > 199     351     4.1
> > > 200     352     4
> > > 201     353     4
> > > 202     354     4
> > > 203     355     3.9
> > > 204     356     3.9
> > > 205     357     3.8
> > > 206     358     3.8
> > > 207     359     3.8
> > > 208     360     3.8
> > > 209     361     3.7
> > > 210     362     3.7
> > > 211     363     3.7
> > > 212     364     3.6
> > > 213     365     3.6
> > > 214     366     3.6
> > > 215     1       3.2
> > > 216     2       3.2
> > > 217     3       3.2
> > > 218     4       3.2
> > > 219     5       3.2
> > > 220     6       3.2
> > > 221     7       3.2
> > > 222     8       3.2
> > > 223     9       3.2
> > > 224     10      3.2
> > > 225     11      3.2
> > > 226     12      3.2
> > > 227     13      3.2
> > > 228     14      3.2
> > > 229     15      3.2
> > > 230     16      3.2
> > > 231     17      3.2
> > > 232     18      3.2
> > > 233     19      3.2
> > > 234     20      3.2
> > > 235     21      3.2
> > > 236     22      3.2
> > > 237     23      3.2
> > > 238     24      3.2
> > > 239     25      3.2
> > > 240     26      3.2
> > > 241     27      3.2
> > > 242     28      3.2
> > > 243     29      3.2
> > > 244     30      3.2
> > > 245     31      3.2
> > > 246     32      3.2
> > > 247     33      3.2
> > > 248     34      3.2
> > > 249     35      3.2
> > > 250     36      3.2
> > > 251     37      3.2
> > > 252     38      3.2
> > > 253     39      3.2
> > > 254     40      3.2
> > > 255     41      3.2
> > > 256     42      3.2
> > > 257     43      3.2
> > > 258     44      3.2
> > > 259     45      3.2
> > > 260     46      3.2
> > > 261     47      3.2
> > > 262     48      3.2
> > > 263     49      3.2
> > > 264     50      3.2
> > > 265     51      3.2
> > > 266     52      3.2
> > > 267     53      3.2
> > > 268     54      3.3
> > > 269     55      3.3
> > > 270     56      3.3
> > > 271     57      3.3
> > > 272     58      3.3
> > > 273     59      3.3
> > > 274     60      3.3
> > > 275     61      3.3
> > > 276     62      3.3
> > > 277     63      3.3
> > > 278     64      3.3
> > > 279     65      3.3
> > > 280     66      3.3
> > > 281     67      3.3
> > > 282     68      3.3
> > > 283     69      3.3
> > > 284     70      3.3
> > > 285     71      3.4
> > > 286     72      3.4
> > > 287     73      3.4
> > > 288     74      3.4
> > > 289     75      3.4
> > > 290     76      3.4
> > > 291     77      3.4
> > > 292     78      3.4
> > > 293     79      3.5
> > > 294     80      3.5
> > > 295     81      3.5
> > > 296     82      3.5
> > > 297     83      3.5
> > > 298     84      3.5
> > > 299     85      3.6
> > > 300     86      3.6
> > > 301     87      3.6
> > > 302     88      3.6
> > > 303     89      3.6
> > > 304     90      3.7
> > > 305     91      3.7
> > > 306     92      3.7
> > > 307     93      3.8
> > > 308     94      3.8
> > > 309     95      3.8
> > > 310     96      3.8
> > > 311     97      3.9
> > > 312     98      3.9
> > > 313     99      4
> > > 314     100     4
> > > 315     101     4
> > > 316     102     4.1
> > > 317     103     4.1
> > > 318     104     4.2
> > > 319     105     4.2
> > > 320     106     4.3
> > > 321     107     4.3
> > > 322     108     4.4
> > > 323     109     4.4
> > > 324     110     4.5
> > > 325     111     4.5
> > > 326     112     4.6
> > > 327     113     4.7
> > > 328     114     4.7
> > > 329     115     4.8
> > > 330     116     4.9
> > > 331     117     5
> > > 332     118     5
> > > 333     119     5.1
> > > 334     120     5.2
> > > 335     121     5.3
> > > 336     122     5.4
> > > 337     123     5.5
> > > 338     124     5.5
> > > 339     125     5.6
> > > 340     126     5.7
> > > 341     127     5.8
> > > 342     128     6
> > > 343     129     6.1
> > > 344     130     6.2
> > > 345     131     6.3
> > > 346     132     6.4
> > > 347     133     6.5
> > > 348     134     6.7
> > > 349     135     6.8
> > > 350     136     6.9
> > > 351     137     7
> > > 352     138     7.2
> > > 353     139     7.3
> > > 354     140     7.5
> > > 355     141     7.6
> > > 356     142     7.8
> > > 357     143     7.9
> > > 358     144     8.1
> > > 359     145     8.2
> > > 360     146     8.4
> > > 361     147     8.6
> > > 362     148     8.7
> > > 363     149     8.9
> > > 364     150     9.1
> > > 365     151     9.3
> > > 366     152     9.3
> > >
> > >
> > >
> > > Michael Rennie
> > > M.Sc. Candidate
> > > University of Toronto at Mississauga
> > > 3359 Mississauga Rd. N.
> > > Mississauga, ON  L5L 1C6
> > > Ph: 905-828-5452  Fax: 905-828-3792
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792



From andy_liaw at merck.com  Tue Jul 15 22:46:19 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jul 2003 16:46:19 -0400
Subject: [R] Plotting a graph of many lines between groups of
 points..	.
Message-ID: <3A822319EB35174CA3714066D590DCD50205C890@usrymx25.merck.com>

> From: Andrew Johnson [mailto:johnson.2060 at osu.edu] 
> 
> I have a data file read into a data frame.
> 
> For example,
> 
> 	V1	V2	V3	V4
> 1	1	1	3	4
> 2	2	3	5	10
> .	.	.	.	.
> .	.	.	.	.
> n          V1[n]     V2[n]     V3[n]     V4[n]
> 
> to n=many thousand
> 
> I want to plot a graph with many line segments, where 
> v1[i]=x1, v2[i]=y1, 
> v3[i]=x2, v4[i]=y2 for i=1,n.
> 
> This seems relatively simple in theory but I've spent quite a 
> bit of time 
> trying to make it happen with plot(type=), points(x,y), or 
> lines(x,y) to no 
> avail.
> 
> Do I need to turn these into vectors before plotting them?

See if matplot(mydata[,c(1,3)], mydata[,c(2,4)]) does what you want.  If so,
check the help page for matplot for other options.

HTH,
Andy


 
> Any help would be greatly appreciated.
> 
> Thanks,
> Andrew
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From rvaradha at jhsph.edu  Tue Jul 15 23:02:05 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Tue, 15 Jul 2003 17:02:05 -0400
Subject: [R] Why two chisq.test p values differ when the contingency
Message-ID: <2095f420abb7.20abb72095f4@jhsph.edu>

Hi Tao:

The P-values for 2x2 table are generated based on a random (discrete 
uniform distribution) sampling of all possible 2x2 tables, conditioning 
on the observed margin totals. If one of the cells is extremely small, 
as in your case, you get a big difference in P-values. Suppose, you 
changed the cell with value 1 to, say, 5 or 6, then the two P-values 
are nearly the same. However, I don't understand why they should be so 
different, since the set of all possible 2x2 tables will be the same in 
both cases. I would be interested in knowing how this happens.

Ravi.


----- Original Message -----
From: "Shi, Tao" <shidaxia at yahoo.com>
Date: Tuesday, July 15, 2003 4:37 pm
Subject: RE: [R] Why two chisq.test p values differ when the contingency

> Hi, Ted and Dennis:
> 
> Thanks for your speedy replies!  I don't think this happens just 
> randomly, rather, I'm thinking it may be due to the way chisq.test 
> function handles simulation.  Here shows why: (Ted, I think there 
> is an error in your code, "tx" should be t(x)  )
> 
> > x
>     [,1] [,2]
> [1,]  149  151
> [2,]    1    8
> > c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
> > for(i in (1:20)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
> +                        B=100000)$p.value)}
> > c2tx<-chisq.test(t(x), simulate.p.value=T, B=100000)$p.value
> > for(i in (1:20)){c2tx<-c(c2tx,chisq.test(t(x), simulate.p.value=T,
> +                         B=100000)$p.value)}
> > cbind(c2x,c2tx)
>          c2x    c2tx
> [1,] 0.03727 0.01629
> [2,] 0.03682 0.01662
> [3,] 0.03671 0.01665
> [4,] 0.03788 0.01745
> [5,] 0.03706 0.01646
> [6,] 0.03715 0.01728
> [7,] 0.03664 0.01683
> [8,] 0.03681 0.01720
> [9,] 0.03742 0.01758
> [10,] 0.03712 0.01685
> [11,] 0.03739 0.01615
> [12,] 0.03811 0.01653
> [13,] 0.03711 0.01673
> [14,] 0.03639 0.01678
> [15,] 0.03714 0.01719
> [16,] 0.03774 0.01780
> [17,] 0.03574 0.01707
> [18,] 0.03661 0.01705
> [19,] 0.03751 0.01711
> [20,] 0.03683 0.01718
> [21,] 0.03678 0.01653
> 
> 
> 
> ...Tao
> 
> ============================================================
> Ted.Harding at nessie.mcc.ac.uk wrote:
> On 15-Jul-03 Tao Shi wrote:
> >>x
> > [,1] [,2]
> > [1,] 149 151
> > [2,] 1 8
> >>t(x)
> > [,1] [,2]
> > [1,] 149 1
> > [2,] 151 8
> >>chisq.test(x, simulate.p.value=T, B=100000)
> > Pearson's Chi-squared test with simulated p-value (based on
> > 1e+05 replicates)
> > data: x
> > X-squared = 5.2001, df = NA, p-value = 0.03774
> > 
> >>chisq.test(t(x), simulate.p.value=T, B=100000)
> > Pearson's Chi-squared test with simulated p-value (based on
> > 1e+05 replicates)
> > data: t(x)
> > X-squared = 5.2001, df = NA, p-value = 0.01642
> 
> Possibly you may just have been unlucky, though the 0.03774 seems 
> large:
> c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
> for(i in (1:9)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
> B=100000)$p.value)}
> c2tx<-chisq.test(tx, simulate.p.value=T, B=100000)$p.value
> for(i in (1:9)){c2tx<-c(c2tx,chisq.test(tx, simulate.p.value=T,
> B=100000)$p.value)}
> cbind(c2x,c2tx)
> c2x c2tx
> [1,] 0.01627 0.01720
> [2,] 0.01672 0.01690
> [3,] 0.01662 0.01669
> [4,] 0.01733 0.01656
> [5,] 0.01679 0.01777
> [6,] 0.01715 0.01769
> [7,] 0.01765 0.01769
> [8,] 0.01703 0.01740
> [9,] 0.01704 0.01708
> [10,] 0.01669 0.01655
> 
> sd(c2x)
> [1] 0.0003946715
> sd(c2tx)
> [1] 0.0004737099
> 
> Ted.
> 
> 
> -------------------------------------------------------------------
> -
> E-Mail: (Ted Harding) 
> Fax-to-email: +44 (0)870 167 1972
> Date: 15-Jul-03 Time: 21:00:04
> ------------------------------ XFMail -----------------------------
> -
> 
> 
> 
> 
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From vasileios_p at yahoo.gr  Tue Jul 15 23:14:45 2003
From: vasileios_p at yahoo.gr (=?iso-8859-7?q?vasilis=20pappas?=)
Date: Tue, 15 Jul 2003 22:14:45 +0100 (BST)
Subject: [R] greek problem
Message-ID: <20030715211445.98245.qmail@web12908.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/bfa4fa39/attachment.pl

From andel at ifi.unizh.ch  Tue Jul 15 23:27:50 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Tue, 15 Jul 2003 21:27:50 -0000
Subject: [R] matrix manipulations
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C886@usrymx25.merck.com>
Message-ID: <Pine.GSO.4.44.0307152326220.5735-100000@igor>

Thanks a lot, this does exactly what I was looking for.

On Tue, 15 Jul 2003, Liaw, Andy wrote:

> I don't think for loop is so bad here, but if you insist on not using it,
> try:
>
> > x<-matrix(rnorm(25), 5, 5)
> > sapply(1:5, function(i) cor(x[,i], rowSums(x[,-i])))
> [1] -0.04179336 -0.08613796  0.48194936  0.38317629 -0.22081706
>
> HTH,
> Andy
>
> > -----Original Message-----
> > From: David Andel [mailto:andel at ifi.unizh.ch]
> > Sent: Tuesday, July 15, 2003 12:52 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] matrix manipulations
> >
> >
> > Hi
> >
> > cor(x,apply(x,1,sum)) gives me the correlations of each
> > column with the sums of each row (correct me if I'm wrong, please).
> >
> > What I need are the correlations of each column with the sums
> > of each row except the entry in the given column. It seems
> > that for any one column i I get it by doing:
> >
> > cor(x[,i],apply(x[,-i],1,sum))
> >
> > But I struggle to get it for all the columns. I was trying
> > things like:
> >
> > for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum))
> >
> > which doesn't generate any output at all, and
> >
> > > rbind(for(i in 1:ncol(x)) cor(x[,i],apply(x[,-i],1,sum)))
> >           [,1]
> > [1,] 0.1880237
> >
> > outputs just the result of the very last column.
> >
> > I know that it shouldn't be necessary to use for(), but I
> > couldn't figure out a way how to do the task using e.g. apply().
> >
> > How do you get the results of all columns?
> >
> > Thank you,
> > David
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> >



From r.darnell at uq.edu.au  Wed Jul 16 00:19:38 2003
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Wed, 16 Jul 2003 08:19:38 +1000
Subject: [R] Specifying an lme model
In-Reply-To: <6rsmp76cxw.fsf@bates4.stat.wisc.edu> (Douglas Bates's message
	of "15 Jul 2003 10:12:59 -0500")
References: <7k6k9xrp.fsf@uq.edu.au> <6rsmp76cxw.fsf@bates4.stat.wisc.edu>
Message-ID: <he5nv3et.fsf@uq.edu.au>

Thanks for the help.

Sorry but now comes the second question. What we have now assumes
equal within-subject variances. How can I fit separate (pooled)
within-subject variances for each group (similar to an unequal
variance t-test) and test if this is better than the existing constant
within-subject variance.

Many thanks

Ross
 
Douglas Bates <bates at stat.wisc.edu> writes:

> Ross Darnell <r.darnell at uq.edu.au> writes:
>
>> I would like some advice on how if possible, to test the following
>> 
>>  I have subjects each measured several times. The subjects are sampled
>>  from 3 subpopulations (groups). The question is "Is the 
>>  between-subject variance the same for the three groups?"
>> 
>> The "null" model is 
>> 
>> lme0 <- lme(y~group,random=~1|subject)
>> 
>> I did think that the model that defined a specific between-subject
>> variance for each group was
>> 
>> update(lme0,.~., weights=varIdent(form=~1|group))
>> 
>> but I am not sure.
>
> I think you have it right.  You should then compare the two fitted
> models using the anova generic, which will provide a likelihood ratio
> test statistic and a p-value based on a chi-squared reference
> distribution.  Regard the p-value as an approximation.
>
>
>

-- 
Ross Darnell
School of Health and Rehabilitation Sciences
University of Queensland, Brisbane QLD 4067 AUSTRALIA
Email: <r.darnell at uq.edu.au>
Phone +61 7 3365 6087
http://www.uq.edu.au/~uqrdarne/



From p.dalgaard at biostat.ku.dk  Wed Jul 16 01:03:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 15 Jul 2003 23:03:38 -0000
Subject: [R] Why two chisq.test p values differ when the contingency
In-Reply-To: <20030715203705.25726.qmail@web13408.mail.yahoo.com>
References: <20030715203705.25726.qmail@web13408.mail.yahoo.com>
Message-ID: <x2lluz8k5t.fsf@biostat.ku.dk>

"Shi, Tao" <shidaxia at yahoo.com> writes:

> Hi, Ted and Dennis:
>  
> Thanks for your speedy replies!  I don't think this happens just randomly, rather, I'm thinking it may be due to the way chisq.test function handles simulation.  Here shows why: (Ted, I think there is an error in your code, "tx" should be t(x)  )
> 
> > x
>      [,1] [,2]
> [1,]  149  151
> [2,]    1    8
> > c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
> > for(i in (1:20)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
> +                        B=100000)$p.value)}
> > c2tx<-chisq.test(t(x), simulate.p.value=T, B=100000)$p.value
> > for(i in (1:20)){c2tx<-c(c2tx,chisq.test(t(x), simulate.p.value=T,
> +                         B=100000)$p.value)}
> > cbind(c2x,c2tx)
>           c2x    c2tx
>  [1,] 0.03727 0.01629
>  [2,] 0.03682 0.01662

I agree that this looks dodgy. The simulation is by taking samples of
tables consistent with the given marginals, so should be invariant
under transpose operations. I venture a guess that the algorithm is
somehow forgetting to count tables that are identical to the current
table. (Notice that there are really only ten tables consistent with
those marginals, with probabilities

> dhyper(0:9,150,159,9)
 [1] 0.002258834 0.020194876 0.079185170 0.178727311 0.255905014
 0.241046013
 [7] 0.149366119 0.058713525 0.013284864 0.001318274

and the differences between c2x and c2tx look suspiciously close to
0.020194876...)

You might want to file this as a bug report.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Wed Jul 16 02:30:59 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Jul 2003 01:30:59 +0100 (BST)
Subject: [R] Why two chisq.test p values differ when the contingency
In-Reply-To: <20030715203705.25726.qmail@web13408.mail.yahoo.com>
Message-ID: <XFMail.030716013059.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jul-03 Shi, Tao wrote:
> Hi, Ted and Dennis:
>  
> Thanks for your speedy replies!  I don't think this happens just
> randomly, rather, I'm thinking it may be due to the way chisq.test
> function handles simulation.  Here shows why: (Ted, I think there is an
> error in your code, "tx" should be t(x)  )

Not so -- I had already transposed them:
> x
     [,1] [,2]
[1,]  149  151
[2,]    1    8
> tx
     [,1] [,2]
[1,]  149    1
[2,]  151    8

Anyway, just to check, a third run (which as it happens follows straight
on from the ones previously reported since I had not used that instance
of R since) amd putting the new results alongside the previous ones:

> c2trx<-chisq.test(t(x), simulate.p.value=T, B=100000)$p.value
> for(i in (1:9)){c2trx<-c(c2trx,chisq.test(t(x), simulate.p.value=T,
B=100000)$p.value)}
> cbind(c2x,c2tx,c2trx)
          c2x    c2tx   c2trx
 [1,] 0.01627 0.01720 0.01628
 [2,] 0.01672 0.01690 0.01686
 [3,] 0.01662 0.01669 0.01706
 [4,] 0.01733 0.01656 0.01705
 [5,] 0.01679 0.01777 0.01633
 [6,] 0.01715 0.01769 0.01782
 [7,] 0.01765 0.01769 0.01688
 [8,] 0.01703 0.01740 0.01683
 [9,] 0.01704 0.01708 0.01689
[10,] 0.01669 0.01655 0.01721

Maybe Peter Dalgaard's suspicion is true, that one case is not being
counted. But this must be R-implementation/version-dependent. In my
case:
> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    6.1              
year     2002             
month    11               
day      01               
language R                

Ted.


> 
>> x
>      [,1] [,2]
> [1,]  149  151
> [2,]    1    8
>> c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
>> for(i in (1:20)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
> +                        B=100000)$p.value)}
>> c2tx<-chisq.test(t(x), simulate.p.value=T, B=100000)$p.value
>> for(i in (1:20)){c2tx<-c(c2tx,chisq.test(t(x), simulate.p.value=T,
> +                         B=100000)$p.value)}
>> cbind(c2x,c2tx)
>           c2x    c2tx
>  [1,] 0.03727 0.01629
>  [2,] 0.03682 0.01662
>  [3,] 0.03671 0.01665
>  [4,] 0.03788 0.01745
>  [5,] 0.03706 0.01646
>  [6,] 0.03715 0.01728
>  [7,] 0.03664 0.01683
>  [8,] 0.03681 0.01720
>  [9,] 0.03742 0.01758
> [10,] 0.03712 0.01685
> [11,] 0.03739 0.01615
> [12,] 0.03811 0.01653
> [13,] 0.03711 0.01673
> [14,] 0.03639 0.01678
> [15,] 0.03714 0.01719
> [16,] 0.03774 0.01780
> [17,] 0.03574 0.01707
> [18,] 0.03661 0.01705
> [19,] 0.03751 0.01711
> [20,] 0.03683 0.01718
> [21,] 0.03678 0.01653
>  
>  
>  
> ...Tao
>  
> ============================================================
> Ted.Harding at nessie.mcc.ac.uk wrote:
> On 15-Jul-03 Tao Shi wrote:
>>>x
>> [,1] [,2]
>> [1,] 149 151
>> [2,] 1 8
>>>t(x)
>> [,1] [,2]
>> [1,] 149 1
>> [2,] 151 8
>>>chisq.test(x, simulate.p.value=T, B=100000)
>> Pearson's Chi-squared test with simulated p-value (based on
>> 1e+05 replicates)
>> data: x
>> X-squared = 5.2001, df = NA, p-value = 0.03774
>> 
>>>chisq.test(t(x), simulate.p.value=T, B=100000)
>> Pearson's Chi-squared test with simulated p-value (based on
>> 1e+05 replicates)
>> data: t(x)
>> X-squared = 5.2001, df = NA, p-value = 0.01642
> 
> Possibly you may just have been unlucky, though the 0.03774 seems
> large:
> 
> c2x<-chisq.test(x, simulate.p.value=T, B=100000)$p.value
> for(i in (1:9)){c2x<-c(c2x,chisq.test(x, simulate.p.value=T,
> B=100000)$p.value)}
> c2tx<-chisq.test(tx, simulate.p.value=T, B=100000)$p.value
> for(i in (1:9)){c2tx<-c(c2tx,chisq.test(tx, simulate.p.value=T,
> B=100000)$p.value)}
> cbind(c2x,c2tx)
> c2x c2tx
> [1,] 0.01627 0.01720
> [2,] 0.01672 0.01690
> [3,] 0.01662 0.01669
> [4,] 0.01733 0.01656
> [5,] 0.01679 0.01777
> [6,] 0.01715 0.01769
> [7,] 0.01765 0.01769
> [8,] 0.01703 0.01740
> [9,] 0.01704 0.01708
> [10,] 0.01669 0.01655
> 
> sd(c2x)
> [1] 0.0003946715
> sd(c2tx)
> [1] 0.0004737099
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) 
> Fax-to-email: +44 (0)870 167 1972
> Date: 15-Jul-03 Time: 21:00:04
> ------------------------------ XFMail ------------------------------
> 
> 
> 
> 
> 
> ---------------------------------
> Do you Yahoo!?


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 16-Jul-03                                       Time: 01:30:59
------------------------------ XFMail ------------------------------



From shidaxia at yahoo.com  Wed Jul 16 02:59:14 2003
From: shidaxia at yahoo.com (Shi, Tao)
Date: Tue, 15 Jul 2003 17:59:14 -0700 (PDT)
Subject: [R] Why two chisq.test p values differ when the contingency
In-Reply-To: <XFMail.030716013059.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20030716005914.56444.qmail@web13409.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/b33423b4/attachment.pl

From mr_james_ireland at sbcglobal.net  Wed Jul 16 07:52:58 2003
From: mr_james_ireland at sbcglobal.net (J Ireland)
Date: Tue, 15 Jul 2003 22:52:58 -0700 (PDT)
Subject: [R] wireframe question
Message-ID: <20030716055258.72188.qmail@web80211.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030715/cdfea7b0/attachment.pl

From ok at cs.otago.ac.nz  Wed Jul 16 08:08:37 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 16 Jul 2003 18:08:37 +1200 (NZST)
Subject: [R] Stem and leaf display?
Message-ID: <200307160608.h6G68b53207562@atlas.otago.ac.nz>

I would like to do some fairly basic stem-and-leaf displays in R.
I am aware (I might even say painfully aware) of stem(base) and
have tried it.  That's why I'm hoping someone has a usable stem-
and-leaf display for R so that I don't have to write my own.

r-project.org > Search > R Site Search > "stem and leaf display"
finds nothing.

I also tried the mail archive search, but couldn't figure out how
to search the bodies of the messages.

For the record, there are some less important things that I would
like to have (like depths and outlier lines), and it is a pity that
the "atom" argument of stem() is effectively undocumented (it's a
tolerance, fine, but what _of_ and how used?), but I'd put up with
stem() the way it is if only it didn't do things like displaying
5.67 and 5.78 as
    56 | 78
which makes it effectively unusable.  What good is a stem and leaf
plot where you can't recover the leading digits of the numbers correctly?



From Simon.Blomberg at anu.edu.au  Wed Jul 16 08:21:20 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Wed, 16 Jul 2003 16:21:20 +1000
Subject: [R] wireframe question
Message-ID: <7A3A13F416B40842BD2C1753E044B359B0995E@CASEVS02.cas.anu.edu.au>

try

wireframe(z ~ x * y, data, drape=T, colorkey=F, scales=list(arrows=FALSE))

see the help for scales under ?xyplot for other goodies.

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: J Ireland [mailto:mr_james_ireland at sbcglobal.net]
> Sent: Wednesday, 16 July 2003 3:53 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] wireframe question
> 
> 
> Hi,
>  
> I'm probably being dense, but could somebody tell me how I 
> can get tick marks on the axes of my wireframe plot to show up?
>  
> This gets me a beautiful looking plot, but no ticks:
> 
> wireframe(z ~ x * y, data, drape=T, colorkey=F)
> 
> Thanks in advance,
> 
>  -James
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mkondrin at hppi.troitsk.ru  Wed Jul 16 20:21:12 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Wed, 16 Jul 2003 11:21:12 -0700
Subject: [R] Excel can do what R can't?????
In-Reply-To: <5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>	<5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
Message-ID: <3F159798.8030504@hppi.troitsk.ru>

 >?optim

optim(par, fn, gr = NULL,
            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"),
            lower = -Inf, upper = Inf,
            control = list(), hessian = FALSE, ...)

.....
       fn: A function to be minimized (or maximized), with first
           argument the vector of parameters over which minimization is
           to take place. It should return a scalar result.

Your fn defined as:
f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
What is its first argument I wonder?
I think you have just an ill-defined R function (although for Excel it 
may be OK - do not know) and optim just chokes on it.



From maechler at stat.math.ethz.ch  Wed Jul 16 09:21:56 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 16 Jul 2003 09:21:56 +0200
Subject: [R] eigen vector sign reversal
In-Reply-To: <3F1437AA.6000608@stat.ucla.edu>
References: <3F13F098.2462CC82@ysbl.york.ac.uk>
	<3F1437AA.6000608@stat.ucla.edu>
Message-ID: <16148.64788.333443.824815@gargle.gargle.HOWL>

>>>>> "Roger" == Roger D Peng <rpeng at stat.ucla.edu>
>>>>>     on Tue, 15 Jul 2003 10:19:38 -0700 writes:

    Roger> I think at version 1.7.0 R started using LAPACK for
    Roger> its eigen/svd routines.  I think using `eigen(x,
    Roger> EISPACK = TRUE)' uses the previous version.

Yes (2 x).

But it the eigen vectors *are* determined only upto
multiplication with +/- 1.
So I think Karim should consider improving the R scripts that 
have been dependent on particular signs of the eigen vectors.

    Roger> Karim Elsawy wrote:

    >> I've just installed R 1.7.1 under linux red hat I noticed
    >> sign reversal of eigen vectors ,some of them not all,
    >> upon using diag function relative to those obtained using
    >> R 1.4.1 this is gonna miss up lots of my previous scripts
    >> I wonder if there is a way to avoid this.  best regards
    >> karim

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Richard.Rowe at jcu.edu.au  Wed Jul 16 09:11:23 2003
From: Richard.Rowe at jcu.edu.au (Richard Rowe)
Date: Wed, 16 Jul 2003 17:11:23 +1000
Subject: [R] Stem and leaf display?
In-Reply-To: <200307160608.h6G68b53207562@atlas.otago.ac.nz>
Message-ID: <5.0.0.25.1.20030716165614.031dc550@pop.jcu.edu.au>

At 18:08 16/07/03 +1200, Richard A. O'Keefe wrote:
>I would like to do some fairly basic stem-and-leaf displays in R.
>I am aware (I might even say painfully aware) of stem(base) and
>have tried it.  That's why I'm hoping someone has a usable stem-
>and-leaf display for R so that I don't have to write my own.
>
>r-project.org > Search > R Site Search > "stem and leaf display"
>finds nothing.
>
>I also tried the mail archive search, but couldn't figure out how
>to search the bodies of the messages.
>
>For the record, there are some less important things that I would
>like to have (like depths and outlier lines), and it is a pity that
>the "atom" argument of stem() is effectively undocumented (it's a
>tolerance, fine, but what _of_ and how used?),

Source is available ...

>  but I'd put up with stem() the way it is if only it didn't do things 
> like displaying
>5.67 and 5.78 as
>     56 | 78
>which makes it effectively unusable.  What good is a stem and leaf
>plot where you can't recover the leading digits of the numbers correctly?


?????

 > x<-c(5.67, 5.78)
 > stem(x)

   The decimal point is 1 digit(s) to the left of the |

   56 | 7
   57 |
   57 | 8


What are you doing?


Richard Rowe
Senior Lecturer
Department of Zoology and Tropical Ecology, James Cook University
Townsville, Queensland 4811, Australia
fax (61)7 47 25 1570
phone (61)7 47 81 4851
e-mail: Richard.Rowe at jcu.edu.au
http://www.jcu.edu.au/school/tbiol/zoology/homepage.html



From anson at bgumail.bgu.ac.il  Wed Jul 16 10:04:58 2003
From: anson at bgumail.bgu.ac.il (Yonathan (Jon) Anson)
Date: Wed, 16 Jul 2003 11:04:58 +0300
Subject: [R] Weighted SUR, 2SLS regressions
Message-ID: <3F15072A.9070406@bgumail.bgu.ac.il>

Is there an option for running SUR and 2SLS regressions with weighting 
(I am analysing mortality in towns, hence want to weight by population size)

Many thanks

Jon Anson

-- 
Yonathan (Jon) Anson
Department of Social Work
Ben Gurion University of the Negev
84105 Be'er Sheva, Israel.

Tel: +972 8 647 93 14(w) +972 8 6489286 (h) 067 233279 (m)
Fax: +972 8 647 29 33



From th50 at leicester.ac.uk  Wed Jul 16 10:41:22 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 16 Jul 2003 09:41:22 +0100
Subject: [R] Plotting a graph of many lines between groups of points...
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46EA@saffron.cfs.le.ac.uk>

Dear Andrew,

Assuming your variables are called V1 to V4, and are vectors, 
I'd use something like

plot(V1,V2,xlim=range(V1,V3),ylim=range(V2,V4),type="n")
segments(V1,V2,V3,V4)

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Andrew Johnson [mailto:johnson.2060 at osu.edu]
> Sent: 15 July 2003 20:42
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plotting a graph of many lines between groups of 
> points...
> 
> 
> I have a data file read into a data frame.
> 
> For example,
> 
> 	V1	V2	V3	V4
> 1	1	1	3	4
> 2	2	3	5	10
> .	.	.	.	.
> .	.	.	.	.
> n          V1[n]     V2[n]     V3[n]     V4[n]
> 
> to n=many thousand
> 
> I want to plot a graph with many line segments, where 
> v1[i]=x1, v2[i]=y1, 
> v3[i]=x2, v4[i]=y2 for i=1,n.
> 
> This seems relatively simple in theory but I've spent quite a 
> bit of time 
> trying to make it happen with plot(type=), points(x,y), or 
> lines(x,y) to no 
> avail.
> 
> Do I need to turn these into vectors before plotting them?
> 
> Any help would be greatly appreciated.
> 
> Thanks,
> Andrew
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From MOORE_ROGER at lilly.com  Wed Jul 16 10:42:16 2003
From: MOORE_ROGER at lilly.com (MOORE_ROGER@lilly.com)
Date: Wed, 16 Jul 2003 09:42:16 +0100
Subject: [R] Missing or Corrupted Files
Message-ID: <OF8B4EF9F1.2F4132AF-ON80256D65.002F1BEB@d49.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030716/3f68bf34/attachment.pl

From Friedrich.Leisch at ci.tuwien.ac.at  Wed Jul 16 11:13:46 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Wed, 16 Jul 2003 11:13:46 +0200
Subject: [R] CRAN package orphanizing process
Message-ID: <16149.5962.212995.750145@galadriel.ci.tuwien.ac.at>


As the list of packages on CRAN becomes longer almost on a weekly
basis, we need a formal mechanism to handle the case when somebody
wants to resign from maintaining a package.


Possible reasons for orphanizing a package:

1) The current maintainer actively wants to orphanize the package,
   e.g., because he has no longer the time or interest to act as
   package maintainer.

2) The current maintainer does not answer to emails by the CRAN admins
   for longer periods of time.


The orphanizing process:

1) The package gets the special keyword "ORPHANED" as maintainer in
   the DESCRIPTION file, the patch level of the version number is
   increased.

2) The package is moved from $CRAN/src/contrib to
   $CRAN/src/contrin/Orphaned.

3) As long as the package passes "R CMD check" for the current release
   version of R, it additionally remains in $CRAN/src/contrib/.


Everybody is more than welcome to take over as maintainer of an
orphaned package. Simply download the package sources, make changes if
necessary (respecting original author and license!) and resubmit the
package to CRAN with your name as maintainer in the DESCRIPTION file
of the package.

For the CRAN admins,
Fritz Leisch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From a.CALANDRA at mclink.it  Wed Jul 16 11:24:18 2003
From: a.CALANDRA at mclink.it (Andrea Calandra)
Date: Wed, 16 Jul 2003 11:24:18 +0200 (CEST)
Subject: [R] Re: Info
Message-ID: <1.0.2.200307161121.84734@mclink.it>


Sorry

I'm student in biomedical engineer and i have to solve this formula
for immuno-assay. I need to design a calibration curve

But i don't understand How can i write this formula in R language:
y = a + (c - a) /(1+ e[-b(x-m])

where
x = ln(analyte dose + 1)
y = the optical absorbance data
a = the curves top asymptote
b = the slope of the curve
c = the curves bottom asymptote
m = the curve X intercept

I have to calculate the parameters (a,b,c,m).After with X that i know
i calculate the Y.

> ==========================
> Date: Tue, 15 Jul 2003 19:30:31 -0700 (PDT)
> From: Rick Fletcher <fletcher at uidaho.edu>
> To: Andrea Calandra <a.CALANDRA at mclink.it>
> Subject: Re: Info
> ==========================
> 
> 
> Andrea, if you only have five data points, it will be very difficult 
> to
> fit all those parameters so they give a unique solution.  If 
> you want to
> fit all of your parameters at once, you need many more data points.
> 
> On Tue, 15 Jul 2003, Andrea Calandra wrote:
> 
> >  downloaded the R Language, it's very good!
> > but i have a question
> >
> > i have Five point with X and Y that know (concentration and 
> optical density) how can interpolate the curve
> > with folowing FIVE INTERPOLATION
> >
> > y = a + (c - a) /(1+ e[-b(x-m])
> > > >
> > > > where
> > > > x = ln(analyte dose + 1)
> > > > y = the optical absorbance data
> > > > a = the curves top asymptote
> > > > b = the slope of the curve
> > > > c = the curves bottom asymptote
> > > > m = the curve X intercept
> >
> >
> >
> >
> >
> >
> >
> > > ==========================
> > > Date: Mon, 07 Jul 2003 11:11:28 -0700 (PDT)
> > > From: Rick Fletcher <fletcher at uidaho.edu>
> > > To: Andrea Calandra <a.CALANDRA at mclink.it>
> > > Subject: Re: Info
> > > ==========================
> > >
> > > Did you find a solution yet?  I am sorry for the delay in 
> response,
> > > but
> > > the July 4th weekend is a big holiday here.  It is the day 
> we
> > > celebrate
> > > our independence from England.
> > >
> > > Do you still need help?
> > >
> > >
> > > On Fri, 4 Jul 2003, Andrea Calandra wrote:
> > >
> > > > i take your name on the web!
> > > > thank you
> > > >
> > > > > ==========================
> > > > > Date: Thu, 03 Jul 2003 11:05:24 -0700 (PDT)
> > > > > From: Rick Fletcher <fletcher at uidaho.edu>
> > > > > To: Andrea Calandra <a.CALANDRA at mclink.it>
> > > > > Subject: Re: Info
> > > > > ==========================
> > > > >
> > > > >
> > > > > Hello,
> > > > >
> > > > > May I ask how you got my name?  Thank you.
> > > > >
> > > > >
> > > > > On Thu, 3 Jul 2003, Andrea Calandra wrote:
> > > > >
> > > > > > HI
> > > > > >
> > > > > > I'm a student in chemical engineering at university 
> of
> > > Rome,
> > > > > and i have to implement an algoritm about FIVE PARAMETERS
> > > INTERPOLATION
> > > > > for a calibration curve (dose, optical density)
> > > > > >
> > > > > > y = a + (c - a) /(1+ e[-b(x-m])
> > > > > >
> > > > > > where
> > > > > > x = ln(analyte dose + 1)
> > > > > > y = the optical absorbance data
> > > > > > a = the curves top asymptote
> > > > > > b = the slope of the curve
> > > > > > c = the curves bottom asymptote
> > > > > > m = the curve X intercept
> > > > > >
> > > > > > Have you never seen this formula, because i don't fine
> > > information
> > > > > or
> > > > > > lecterature about solution of this!!!
> > > > > >
> > > > > > Can i help me
> > > > > > Andrea Calandra
> > > > > >
> > > > > > thank you
> > > > > >
> > > > > >
> > > > > >
> > > > >
> > > > > Rick
> > > > > T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
> > > > > Associate professor of chemistry  |  That's Idaho, not 
> Iowa.
> > > > >    | ad hominem
> > > > > University of Idaho               |  Upper Left Hand 
> Corner.
> > > > >    | ad hominem
> > > > > Moscow, ID 83844-2343             |  No, I don't grow 
> potatoes.
> > > > > | ad hominem
> > > >
> > > >
> > > >
> > >
> > > Rick
> > > T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
> > > Associate professor of chemistry  |  That's Idaho, not Iowa.
> > >    | ad hominem
> > > University of Idaho               |  Upper Left Hand Corner.
> > >    | ad hominem
> > > Moscow, ID 83844-2343             |  No, I don't grow potatoes.
> > > | ad hominem
> >
> >
> >
> 
> Rick
> T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
> Associate professor of chemistry  |  That's Idaho, not Iowa. 
>    | ad hominem
> University of Idaho               |  Upper Left Hand Corner. 
>    | ad hominem
> Moscow, ID 83844-2343             |  No, I don't grow potatoes. 
> | ad hominem



From ligges at statistik.uni-dortmund.de  Wed Jul 16 11:28:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Jul 2003 11:28:37 +0200
Subject: [R] Missing or Corrupted Files
In-Reply-To: <OF8B4EF9F1.2F4132AF-ON80256D65.002F1BEB@d49.lilly.com>
References: <OF8B4EF9F1.2F4132AF-ON80256D65.002F1BEB@d49.lilly.com>
Message-ID: <3F151AC5.3010407@statistik.uni-dortmund.de>

MOORE_ROGER at lilly.com wrote:

> I'd appreciate any advice people can give me on the following problem :-
> 
> I've recently started using R, as support for WinBugs software, on my PC 
> which runs Windows XP.
> Whatever I've done has resulted in the loss of the GUI display, although 
> other functionality appears to be okay. This occurred without using 
> WinBugs.

What does "loss of the GUI" mean?
What happens when you start Rgui.exe, nothing?

Uwe Ligges


> Both reinstallation of R and the use of the System Restore capability of 
> Windows XP (to return system files to the state they were in before the 
> problem) have not solved the fault.
> 
> Any thoughts please ?
> 
>                         Thanks,
>                         Roger M.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Tor.Strand at cih.uib.no  Wed Jul 16 12:11:32 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Wed, 16 Jul 2003 12:11:32 +0200
Subject: [R] how to handle missing values
Message-ID: <BB3AF174.7319%Tor.Strand@cih.uib.no>

This group impresses me, so far I have been helped with all my questions
within 24 hours. Thanks.

Therefore another one.

I am used to programs (such as STATA) where observations with missing values
that are included in a model are simply ignored in the analysis. So far I
have not been able to figure out how to deal with missing values in R and
have solved the problem by deleting observations with missing values before
loading them into R.

Can anyone give me a hint on how to do this in a simpler way?

Sincerely,


Dr. Tor A Strand   
Centre for International Health
Haukeland Hospital
University of Bergen
5021 Bergen    
Norway     
Phone: (country prefix 47)
Residence:56 51 10 88, office: 55 97 49 80,
fax: 55 97 49 79, cellular:  90 97 10 86



From zeileis at ci.tuwien.ac.at  Wed Jul 16 12:19:31 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 16 Jul 2003 12:19:31 +0200
Subject: [R] how to handle missing values
In-Reply-To: <BB3AF174.7319%Tor.Strand@cih.uib.no>
References: <BB3AF174.7319%Tor.Strand@cih.uib.no>
Message-ID: <200307161019.h6GAJWU8010073@thorin.ci.tuwien.ac.at>

On Wednesday 16 July 2003 12:11, Tor A  Strand wrote:

> This group impresses me, so far I have been helped with all my
> questions within 24 hours. Thanks.
>
> Therefore another one.
>
> I am used to programs (such as STATA) where observations with
> missing values that are included in a model are simply ignored in
> the analysis. So far I have not been able to figure out how to deal
> with missing values in R and have solved the problem by deleting
> observations with missing values before loading them into R.
>
> Can anyone give me a hint on how to do this in a simpler way?

R can deal with NAs in several ways, look at
  help(na.omit)

Many methods, e.g., lm(), also take an na.action argument so that you 
can keep your data frame with the NAs in R, but fit linear models only 
on the complete cases for example.

Best,
Z


> Sincerely,
>
>
> Dr. Tor A Strand
> Centre for International Health
> Haukeland Hospital
> University of Bergen
> 5021 Bergen
> Norway
> Phone: (country prefix 47)
> Residence:56 51 10 88, office: 55 97 49 80,
> fax: 55 97 49 79, cellular:  90 97 10 86
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kwan022 at stat.auckland.ac.nz  Wed Jul 16 12:19:50 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 16 Jul 2003 22:19:50 +1200 (NZST)
Subject: [R] how to handle missing values
In-Reply-To: <BB3AF174.7319%Tor.Strand@cih.uib.no>
Message-ID: <Pine.LNX.4.44.0307162218350.19076-100000@stat55.stat.auckland.ac.nz>

There are many packages that handles missing values, aka imputations.

>From memory Hmisc is one of them.  But you can certainly search through 
CRAN.

On Wed, 16 Jul 2003, Tor A  Strand wrote:

> Date: Wed, 16 Jul 2003 12:11:32 +0200
> From: Tor A  Strand <Tor.Strand at cih.uib.no>
> To: R-list <r-help at stat.math.ethz.ch>
> Subject: [R] how to handle missing values
> 
> This group impresses me, so far I have been helped with all my questions
> within 24 hours. Thanks.
> 
> Therefore another one.
> 
> I am used to programs (such as STATA) where observations with missing values
> that are included in a model are simply ignored in the analysis. So far I
> have not been able to figure out how to deal with missing values in R and
> have solved the problem by deleting observations with missing values before
> loading them into R.
> 
> Can anyone give me a hint on how to do this in a simpler way?
> 
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From borgulya at gyer2.sote.hu  Wed Jul 16 12:21:40 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-2?Q?BORGULYA_G=E1bor?=)
Date: Wed, 16 Jul 2003 12:21:40 +0200
Subject: [R] numerical differentiation in R? (for optim "SANN" parscale)
Message-ID: <3F152734.2010603@gyer2.sote.hu>

Dear R users,

I am running a maximum likelihood model with optim. I chose the 
simulated annealing method (method="SANN").

SANN is not performing bad, but I guess it would be much more effecive 
if I could set the `parscale' parameter.

The help sais:
`parscale' A vector of scaling values for the parameters.
           Optimization is performed on `par/parscale' and these should
           be comparable in the sense that a unit change in any element
           produces about a unit change in the scaled value.

Since I know the approximate optimal parameters of the function to 
optimise I could use these values to calculate `parscale'.
If I understand the role of `parscale' well, I have to differentiate my 
function numerically.

How can I perform the numerical differentiation in R? I thought about 
writing a small function, but I am sure it is already written. It must 
be present at least in some of the optimisation algorithms.
Anyway, I couln't find it neither in the help, nor in the non-internal, 
displayable source of optim.

Could anyone tell me where to find such a function?
And if it really is what I need for `parscale'?

Thank you!

G?bor
-- 
Gabor BORGULYA MD MSc
Semmelweis University of Budapest, 2nd Dept of Paediatrics
Hungarian Paediatric Cancer Registry
phone: +36 - 1 - 4591500 / 2834



From p.dalgaard at biostat.ku.dk  Wed Jul 16 12:33:28 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 16 Jul 2003 10:33:28 -0000
Subject: [R] Why two chisq.test p values differ when the contingency
In-Reply-To: <20030716005914.56444.qmail@web13409.mail.yahoo.com>
References: <20030716005914.56444.qmail@web13409.mail.yahoo.com>
Message-ID: <x2d6ga92s4.fsf@biostat.ku.dk>

"Shi, Tao" <shidaxia at yahoo.com> writes:

> Hi, Ted:
>  
> I guess this problem is platform-dependent. I just tied it on a R
> 1.6.1 runing on Win2K, it gave me two different p values. But when I
> tried it on R1.7.0 on a Linux Server, I got the similar result as
> you did. I have filed a bug-report as Peter suggested.

And to add to the confusion, I checked it on r-devel (1.8.0-pre) on
Redhat Linux, so it is likely to be compiler dependent too. 

[As we have seen in another thread, you cannot trust "==" with floating
point operations. There are cases where you don't even get x==y
when x and y are computed in the exact same way! Optimizing compilers
will do that to you quite easily. With Intel chips, the typical issue
is that a double is stored in 8 bytes but the FPU uses 10 bytes
internally. Generally, this increases precision, but you can end up
comparing a value to a truncated value of itself if one instance is
stored in memory and the other is in an FPU register.]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gisar at nus.edu.sg  Wed Jul 16 12:56:58 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Wed, 16 Jul 2003 18:56:58 +0800
Subject: [R] how to handle missing values
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FEF9@MBXSRV03.stf.nus.edu.sg>

This really depends on what you want to do. I will try to give some
example below.


1. Coding the missing values

But you definitely do not need to delete observations BEFORE loading
them into R. 
By default any empty cells or "NA" is treated as NA, when you load the
data using read.delim(). You can adjust the na.string option in
read.delim() to change this default behaviour.

Ensure the coding is ok before you proceed. You can check using is.na()
for example to see if R will treat them as missing values.


2. Perform calculations with missing values as defined by na.action()

X <- c(1,2,3,NA, 4)
sum(X, na.rm=T)     # gives you 10
See ?na.action for more interestin detail.

Some algorithms are capable of automatically handle missing values. In
the classification context, rpart can handle missing values.


3. Missing value imputation

There are many imputation methods (eg. EMV, e1071, hmisc, norm, permax,
pamr libraries). The type of imputation depends on your application,
area of research and type of missingness (if at missing completely at
random, missing/observed at random, 
informative missing).

Good luck.


-----Original Message-----
From: Tor A Strand [mailto:Tor.Strand at cih.uib.no] 
Sent: Wednesday, July 16, 2003 6:12 PM
To: R-list
Subject: [R] how to handle missing values


This group impresses me, so far I have been helped with all my questions
within 24 hours. Thanks.

Therefore another one.

I am used to programs (such as STATA) where observations with missing
values that are included in a model are simply ignored in the analysis.
So far I have not been able to figure out how to deal with missing
values in R and have solved the problem by deleting observations with
missing values before loading them into R.

Can anyone give me a hint on how to do this in a simpler way?

Sincerely,


Dr. Tor A Strand   
Centre for International Health
Haukeland Hospital
University of Bergen
5021 Bergen    
Norway     
Phone: (country prefix 47)
Residence:56 51 10 88, office: 55 97 49 80,
fax: 55 97 49 79, cellular:  90 97 10 86

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bates at stat.wisc.edu  Wed Jul 16 14:42:15 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Jul 2003 12:42:15 -0000
Subject: [R] Re: Info
In-Reply-To: <1.0.2.200307161121.84734@mclink.it>
References: <1.0.2.200307161121.84734@mclink.it>
Message-ID: <6rwueive1e.fsf@bates4.stat.wisc.edu>

Andrea Calandra <a.CALANDRA at mclink.it> writes:

> I'm student in biomedical engineer and i have to solve this formula
> for immuno-assay. I need to design a calibration curve
> 
> But i don't understand How can i write this formula in R language:
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> I have to calculate the parameters (a,b,c,m).After with X that i know
> i calculate the Y.

You have asked essentially the same question three or four times and
have gotten the same answer each time.  You tell us IN ALL CAPS that
you are doing five-point interpolation and we point out that you have
only four parameters in that formula.  Asking the same question over
and over and over again will not change that.  Perhaps you should
accept the fact that you cannot interpolate five points with a
four-parameter model then reconsider your question.

If you wish to do a least-squares fit of those parameters you can use
nls and SSfpl as I suggested in an earlier response.



From spencer.graves at pdf.com  Wed Jul 16 15:12:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 06:12:51 -0700
Subject: [R] Re: Info
References: <1.0.2.200307161121.84734@mclink.it>
Message-ID: <3F154F53.7010305@pdf.com>

On 7/11, I replied to one of your earlier posts on this problem wiht the 
following:

chemYield <-
function(a, x)(a[1]+(a[3]-a[2])/(1+exp(-a[2]*(x-a[4]))

This can be used in "optim" but not "nls".  Mimicking an example in the 
documentation for "nls" (package nls), I suspect the following should 
work:

yield.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
                        data = yield.data.frame,
                        start = list( a= 0, c.=2, b= 1, m=4 ),
                        trace = TRUE )

where "y" and "x" are columns of "yield.data.frame.  [Note:  I suggest 
you avoid using reserved words like "c":  "c" is a function in R.  R is 
smart enough to distinguish between a function "c" and a non-function 
object in many contexts.  However, I try to avoid relying on this.  Note 
that here I changed your "c" to "c.".]

	  Have you tried something like these two?  I suggest you try them both 
with a very simple toy example with 4 or 5 observations.  If you can't 
get both of them to work, please submit the data with your failed 
attempts and the resulting error message(s).  With that detail, someone 
else will likely be able to respond in seconds.  Without it, we may no 
be able to help you.  We've been at this almost a week and still do not 
have the problem solved largely because people like me are shooting in 
the dark:  We can't figure out what you are missing, and you aren't 
providing enough detail to help us see the gap.

hope this helps.  spencer graves

Andrea Calandra wrote:
> Sorry
> 
> I'm student in biomedical engineer and i have to solve this formula
> for immuno-assay. I need to design a calibration curve
> 
> But i don't understand How can i write this formula in R language:
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> I have to calculate the parameters (a,b,c,m).After with X that i know
> i calculate the Y.
> 
> 
>>==========================
>>Date: Tue, 15 Jul 2003 19:30:31 -0700 (PDT)
>>From: Rick Fletcher <fletcher at uidaho.edu>
>>To: Andrea Calandra <a.CALANDRA at mclink.it>
>>Subject: Re: Info
>>==========================
>>
>>
>>Andrea, if you only have five data points, it will be very difficult 
>>to
>>fit all those parameters so they give a unique solution.  If 
>>you want to
>>fit all of your parameters at once, you need many more data points.
>>
>>On Tue, 15 Jul 2003, Andrea Calandra wrote:
>>
>>
>>> downloaded the R Language, it's very good!
>>>but i have a question
>>>
>>>i have Five point with X and Y that know (concentration and 
>>
>>optical density) how can interpolate the curve
>>
>>>with folowing FIVE INTERPOLATION
>>>
>>>y = a + (c - a) /(1+ e[-b(x-m])
>>>
>>>>>where
>>>>>x = ln(analyte dose + 1)
>>>>>y = the optical absorbance data
>>>>>a = the curves top asymptote
>>>>>b = the slope of the curve
>>>>>c = the curves bottom asymptote
>>>>>m = the curve X intercept
>>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>>==========================
>>>>Date: Mon, 07 Jul 2003 11:11:28 -0700 (PDT)
>>>>From: Rick Fletcher <fletcher at uidaho.edu>
>>>>To: Andrea Calandra <a.CALANDRA at mclink.it>
>>>>Subject: Re: Info
>>>>==========================
>>>>
>>>>Did you find a solution yet?  I am sorry for the delay in 
>>>
>>response,
>>
>>>>but
>>>>the July 4th weekend is a big holiday here.  It is the day 
>>>
>>we
>>
>>>>celebrate
>>>>our independence from England.
>>>>
>>>>Do you still need help?
>>>>
>>>>
>>>>On Fri, 4 Jul 2003, Andrea Calandra wrote:
>>>>
>>>>
>>>>>i take your name on the web!
>>>>>thank you
>>>>>
>>>>>
>>>>>>==========================
>>>>>>Date: Thu, 03 Jul 2003 11:05:24 -0700 (PDT)
>>>>>>From: Rick Fletcher <fletcher at uidaho.edu>
>>>>>>To: Andrea Calandra <a.CALANDRA at mclink.it>
>>>>>>Subject: Re: Info
>>>>>>==========================
>>>>>>
>>>>>>
>>>>>>Hello,
>>>>>>
>>>>>>May I ask how you got my name?  Thank you.
>>>>>>
>>>>>>
>>>>>>On Thu, 3 Jul 2003, Andrea Calandra wrote:
>>>>>>
>>>>>>
>>>>>>>HI
>>>>>>>
>>>>>>>I'm a student in chemical engineering at university 
>>>>>>
>>of
>>
>>>>Rome,
>>>>
>>>>>>and i have to implement an algoritm about FIVE PARAMETERS
>>>>>
>>>>INTERPOLATION
>>>>
>>>>>>for a calibration curve (dose, optical density)
>>>>>>
>>>>>>>y = a + (c - a) /(1+ e[-b(x-m])
>>>>>>>
>>>>>>>where
>>>>>>>x = ln(analyte dose + 1)
>>>>>>>y = the optical absorbance data
>>>>>>>a = the curves top asymptote
>>>>>>>b = the slope of the curve
>>>>>>>c = the curves bottom asymptote
>>>>>>>m = the curve X intercept
>>>>>>>
>>>>>>>Have you never seen this formula, because i don't fine
>>>>>>
>>>>information
>>>>
>>>>>>or
>>>>>>
>>>>>>>lecterature about solution of this!!!
>>>>>>>
>>>>>>>Can i help me
>>>>>>>Andrea Calandra
>>>>>>>
>>>>>>>thank you
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>Rick
>>>>>>T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
>>>>>>Associate professor of chemistry  |  That's Idaho, not 
>>>>>
>>Iowa.
>>
>>>>>>   | ad hominem
>>>>>>University of Idaho               |  Upper Left Hand 
>>>>>
>>Corner.
>>
>>>>>>   | ad hominem
>>>>>>Moscow, ID 83844-2343             |  No, I don't grow 
>>>>>
>>potatoes.
>>
>>>>>>| ad hominem
>>>>>
>>>>>
>>>>>
>>>>Rick
>>>>T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
>>>>Associate professor of chemistry  |  That's Idaho, not Iowa.
>>>>   | ad hominem
>>>>University of Idaho               |  Upper Left Hand Corner.
>>>>   | ad hominem
>>>>Moscow, ID 83844-2343             |  No, I don't grow potatoes.
>>>>| ad hominem
>>>
>>>
>>>
>>Rick
>>T. Rick Fletcher   -   http://www.chem.uidaho.edu/~fletcher/
>>Associate professor of chemistry  |  That's Idaho, not Iowa. 
>>   | ad hominem
>>University of Idaho               |  Upper Left Hand Corner. 
>>   | ad hominem
>>Moscow, ID 83844-2343             |  No, I don't grow potatoes. 
>>| ad hominem
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jul 16 15:29:22 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 06:29:22 -0700
Subject: [R] Excel can do what R can't?????
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>	<5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>	<5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
	<3F159798.8030504@hppi.troitsk.ru>
Message-ID: <3F155332.4060708@pdf.com>

The phrase:

    f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f

is an immediate computation, not a function.  If you want a function, 
try something like the following:

    f <- function(x){
	  Wt <- x[1]
	  Wtmod <- x[2]
	  Hgt <- x[3]
	  Hgtmod <- x[4]
      1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
    }

OR

    f <- function(x, X){
	  Wt <- X[,1]
	  Hgt <- X[,2]
	  Wtmod <- x[1]
	  Hgtmod <- x[2]
	1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
    }

"par" in "optim" is the starting values for "x".  Pass "X" to "f" via 
"..." in the call to "optim".

	  If you can't make this work, please submit a toy example with the 
code and error messages.  Please limit your example to 3 observations, 
preferably whole numbers so someone else can read your question in 
seconds.  If it is any longer than that, it should be ignored.

hope this helps.
Spencer Graves

M.Kondrin wrote:
>  >?optim
> 
> optim(par, fn, gr = NULL,
>            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"),
>            lower = -Inf, upper = Inf,
>            control = list(), hessian = FALSE, ...)
> 
> .....
>       fn: A function to be minimized (or maximized), with first
>           argument the vector of parameters over which minimization is
>           to take place. It should return a scalar result.
> 
> Your fn defined as:
> f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> What is its first argument I wonder?
> I think you have just an ill-defined R function (although for Excel it 
> may be OK - do not know) and optim just chokes on it.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From JonesW at kssg.com  Wed Jul 16 15:42:09 2003
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 16 Jul 2003 14:42:09 +0100
Subject: [R] Sorting a data frame
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030716/bd29fedf/attachment.pl

From emkiba at gmx.de  Wed Jul 16 16:24:27 2003
From: emkiba at gmx.de (Michael Kirschbaum)
Date: Wed, 16 Jul 2003 16:24:27 +0200
Subject: [R] turning list-elements into a vector
Message-ID: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030716/c4342c02/attachment.pl

From A.Barnes at ed.sac.ac.uk  Wed Jul 16 17:26:08 2003
From: A.Barnes at ed.sac.ac.uk (Andrew Barnes)
Date: Wed, 16 Jul 2003 15:26:08 +0000
Subject: [R] Tobit analysis
Message-ID: <3F156E90.3447.139A0BA@localhost>

Having read previous correspondance on this topic, am I right in using a 
gaussian distribution for a tobit model, one article suggests a normal distribution?

Also, I want to censure at the upper bound, so, using the survival5 package I use:

survreg(Surv(y,y<c,type="right")~x) for a censored regression.

Could anybody who's had experience of this, confirm whether I'm in the ballpark?

Grateful thanks
Andrew Barnes
=======================================
Dr Andrew Barnes
Agricultural Management Economist

Modelling and Strategy Group
Land Economy Research Department
Research Division
SAC
West Mains Road
Edinburgh
EH9 3JG

Tel: 0131 535 4042    E.Mail:  A.Barnes at ed.sac.ac.uk
Fax: 0131 667 2601    Mobile:  07811 935 022

Website: http://www.sac.ac.uk/management/external/default.htm



From spencer.graves at pdf.com  Wed Jul 16 16:33:14 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 07:33:14 -0700
Subject: [R] Sorting a data frame
References: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
Message-ID: <3F15622A.5020108@pdf.com>

?order

hope this helps.  spencer graves

Wayne Jones wrote:
> Hi there R-Helpers, 
> 
> Does anyone know if it is possible to sort a dataframe?
> 
> I.e. Sort alphabetically column 1 ( which has some reocurring elements) then
> sort alphabetically column2 but keeping the order of column 1 constant; 
> much the same way that the sort function works in Excel.
> 
> Regards, 
> 
> Wayne
> 
> 
> Dr Wayne R. Jones
> Statistician / Research Analyst
> KSS Group plc
> St James's Buildings
> 79 Oxford Street
> Manchester M1 6SS
> Tel: +44(0)161 609 4084
> Mob: +44(0)7810 523 713
> 
> 
> 
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
> 
> 
> The information in this Internet email is confidential and m...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bates at stat.wisc.edu  Wed Jul 16 16:39:11 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Jul 2003 14:39:11 -0000
Subject: [R] Re: Four-parameter logistic [was Re: Info]
In-Reply-To: <3F154F53.7010305@pdf.com>
References: <1.0.2.200307161121.84734@mclink.it> <3F154F53.7010305@pdf.com>
Message-ID: <6roezuh6y3.fsf_-_@bates4.stat.wisc.edu>

Spencer Graves <spencer.graves at pdf.com> writes:

> On 7/11, I replied to one of your earlier posts on this problem wiht
> the following:
> 
> 
> chemYield <-
> function(a, x)(a[1]+(a[3]-a[2])/(1+exp(-a[2]*(x-a[4]))
> 
> This can be used in "optim" but not "nls".  Mimicking an example in
> the documentation for "nls" (package nls), I suspect the following
> should work:
> 
> 
> yield.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
>                         data = yield.data.frame,
>                         start = list( a= 0, c.=2, b= 1, m=4 ),
>                         trace = TRUE )
> 
> where "y" and "x" are columns of "yield.data.frame.  [Note:  I suggest
> you avoid using reserved words like "c":  "c" is a function in R.  R
> is smart enough to distinguish between a function "c" and a
> non-function object in many contexts.  However, I try to avoid relying
> on this.  Note that here I changed your "c" to "c.".]
> 
> 
> 	  Have you tried something like these two?  I suggest you try
> them both with a very simple toy example with 4 or 5 observations.  If
> you can't get both of them to work, please submit the data with your
> failed attempts and the resulting error message(s).  With that detail,
> someone else will likely be able to respond in seconds.  Without it,
> we may no be able to help you.  We've been at this almost a week and
> still do not have the problem solved largely because people like me
> are shooting in the dark:  We can't figure out what you are missing,
> and you aren't providing enough detail to help us see the gap.
> 
> 
> hope this helps.  spencer graves

Thanks for the reply Spencer.  I would like to offer a few additional
comments, not in criticism but in the spirit in which you offer your
comments - to assist the community.

It is not a good idea to use nls to fit a four-parameter model to 4
data points.  The relative offset convergence criterion used in nls
cannot be applied to cases with 0 degrees of freedom for residuals.

In simulating data for test fits by nls you must add noise to the
response.  The relative offset convergence criterion used in nls will
not, in general, declare convergence on artificial 'perfect fit' data.
The author of nls does not regard this as a bug :-).

To get a reasonable fit of a four parameter logistic model you need
data across a wide range of x values.  In particular you need data on
both the 'toe' of the curve and the 'shoulder' of the curve.  A common
reason for failure to fit the four-parameter logistic is the
availability of data only on the 'toe' and the linear portion of
the curve.  Without some data past the linear region you cannot
determine an upper asymptote and the two parameters written as m and a
in the above equation cannot be separately determined.

I would recommend using the 'plinear' algorithm in nls for this
model.  Two of the four parameters are conditionally linear.  Reducing
an optimization problem from four parameters to two parameters is a
big win.

The SSfpl self-starting model can relieve the user of the need to form
starting estimates.  See

?SSfpl

and 

example(SSfpl)

The SSfpl model uses a slightly different parameterization of the
model, which we (Don Watts and I) have found to be more stable than
the one given above.  It can be difficult to estimate b in the above
formulation.  We find it better to use a location-scale form of b and
m and to converge on the logarithm of scale parameter.

Successful use of nonlinear regression depends on understanding the
nature of the model and your data.  In particular, I recommend
plotting the data before ever trying to fit any model.  You may recall
that I go a little further than that in some courses and inform
students that if I catch them fitting models without plotting the data
first they will be in danger of failing the course. :-)


Thanks for your efforts in responding to many, many questions on this
list.  I hope my comments are helpful.



From dmurdoch at pair.com  Wed Jul 16 16:44:05 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 16 Jul 2003 10:44:05 -0400
Subject: [R] Sorting a data frame
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
Message-ID: <u3pahv03tvj9i43jma9n03jkppg0kh8ja3@4ax.com>

On Wed, 16 Jul 2003 14:42:09 +0100, Wayne Jones <JonesW at kssg.com>
wrote :

>
>Hi there R-Helpers, 
>
>Does anyone know if it is possible to sort a dataframe?
>
>I.e. Sort alphabetically column 1 ( which has some reocurring elements) then
>sort alphabetically column2 but keeping the order of column 1 constant; 
>much the same way that the sort function works in Excel.

The general idea is to use order():

sorted <- unsorted[order(unsorted$col1, unsorted$col2),]

Duncan Murdoch



From tlumley at u.washington.edu  Wed Jul 16 16:41:12 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 16 Jul 2003 07:41:12 -0700 (PDT)
Subject: [R] eigen vector sign reversal
In-Reply-To: <3F1437AA.6000608@stat.ucla.edu>
Message-ID: <Pine.A41.4.44.0307160740000.263342-100000@homer01.u.washington.edu>

On Tue, 15 Jul 2003, Roger D. Peng wrote:

> I think at version 1.7.0 R started using LAPACK for its eigen/svd
> routines.  I think using `eigen(x, EISPACK = TRUE)' uses the previous
> version.

Yes, but as the sign of eigenvectors is not well-defined it may be better
to fix the scripts so that they don't care.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From MSchwartz at medanalytics.com  Wed Jul 16 16:41:24 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 16 Jul 2003 14:41:24 -0000
Subject: [R] Sorting a data frame
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
Message-ID: <1058366471.2399.108.camel@localhost>

On Wed, 2003-07-16 at 08:42, Wayne Jones wrote:
> Hi there R-Helpers, 
> 
> Does anyone know if it is possible to sort a dataframe?
> 
> I.e. Sort alphabetically column 1 ( which has some reocurring elements) then
> sort alphabetically column2 but keeping the order of column 1 constant; 
> much the same way that the sort function works in Excel.
> 
> Regards, 
> 
> Wayne


Wayne,

Use order() to sort the columns. This allows for multi-level keys. See
?order for more information.

Example:

# Create two column df
> df <- data.frame(V1 = c("W","A", "A", "B", ""), V2 = c("E", "M", "B",
"O", "Q"))

# show df unsorted
> df
  V1 V2
1  W  E
2  A  M
3  A  B
4  B  O
5     Q

# now sort df, by V1, then V2
> df[order(df$V1, df$V2), ]
  V1 V2
5     Q
3  A  B
2  A  M
4  B  O
1  W  E

Note that rows 2 and 3 are subsorted on V2.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Wed Jul 16 16:45:50 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 16 Jul 2003 14:45:50 -0000
Subject: [R] turning list-elements into a vector
In-Reply-To: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>
References: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>
Message-ID: <6rd6gah6my.fsf@bates4.stat.wisc.edu>

"Michael Kirschbaum" <emkiba at gmx.de> writes:

> I want to create a vector from specific matrix-elements( e.g.[1,1])
> these matrices are elements of a list.

> Is there any possibility to work without a loop? (e.g. with "lapply"?)

Sounds like you want

lapply(mlist, "[", i = 1, j = 1)

where mlist is your list of matrices.



From david.firth at nuffield.oxford.ac.uk  Wed Jul 16 16:47:31 2003
From: david.firth at nuffield.oxford.ac.uk (David Firth)
Date: Wed, 16 Jul 2003 15:47:31 +0100
Subject: [R] Tobit analysis
In-Reply-To: <3F156E90.3447.139A0BA@localhost>
Message-ID: <6D87B5BE-B79C-11D7-8DB2-0050E4C03977@nuffield.oxford.ac.uk>

Gaussian and normal are two different words for the same thing.

The example at
http://www.biostat.wustl.edu/archives/html/s-news/1999-06/msg00125.html
should give you plenty of clues.

David

On Wednesday, Jul 16, 2003, at 16:26 Europe/London, Andrew Barnes wrote:

> Having read previous correspondance on this topic, am I right in using 
> a
> gaussian distribution for a tobit model, one article suggests a normal 
> distribution?
>
> Also, I want to censure at the upper bound, so, using the survival5 
> package I use:
>
> survreg(Surv(y,y<c,type="right")~x) for a censored regression.
>
> Could anybody who's had experience of this, confirm whether I'm in the 
> ballpark?
>
> Grateful thanks
> Andrew Barnes
> =======================================
> Dr Andrew Barnes
> Agricultural Management Economist
>
> Modelling and Strategy Group
> Land Economy Research Department
> Research Division
> SAC
> West Mains Road
> Edinburgh
> EH9 3JG
>
> Tel: 0131 535 4042    E.Mail:  A.Barnes at ed.sac.ac.uk
> Fax: 0131 667 2601    Mobile:  07811 935 022
>
> Website: http://www.sac.ac.uk/management/external/default.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Wed Jul 16 16:49:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 07:49:26 -0700
Subject: [R] Re: Four-parameter logistic [was Re: Info]
References: <1.0.2.200307161121.84734@mclink.it> <3F154F53.7010305@pdf.com>
	<6roezuh6y3.fsf_-_@bates4.stat.wisc.edu>
Message-ID: <3F1565F6.7040409@pdf.com>

Dear Doug:

	  Thanks for your reply and clarifications.  I greatly appreciate them. 
  I preach the Gospel of Bates & Watts every chance I get, but life has 
given me so many other problems to solve that I don't know your works as 
well as I would like.

Best Wishes,
Spencer Graves

Douglas Bates wrote:
> Spencer Graves <spencer.graves at pdf.com> writes:
> 
> 
>>On 7/11, I replied to one of your earlier posts on this problem wiht
>>the following:
>>
>>
>>chemYield <-
>>function(a, x)(a[1]+(a[3]-a[2])/(1+exp(-a[2]*(x-a[4]))
>>
>>This can be used in "optim" but not "nls".  Mimicking an example in
>>the documentation for "nls" (package nls), I suspect the following
>>should work:
>>
>>
>>yield.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
>>                        data = yield.data.frame,
>>                        start = list( a= 0, c.=2, b= 1, m=4 ),
>>                        trace = TRUE )
>>
>>where "y" and "x" are columns of "yield.data.frame.  [Note:  I suggest
>>you avoid using reserved words like "c":  "c" is a function in R.  R
>>is smart enough to distinguish between a function "c" and a
>>non-function object in many contexts.  However, I try to avoid relying
>>on this.  Note that here I changed your "c" to "c.".]
>>
>>
>>	  Have you tried something like these two?  I suggest you try
>>them both with a very simple toy example with 4 or 5 observations.  If
>>you can't get both of them to work, please submit the data with your
>>failed attempts and the resulting error message(s).  With that detail,
>>someone else will likely be able to respond in seconds.  Without it,
>>we may no be able to help you.  We've been at this almost a week and
>>still do not have the problem solved largely because people like me
>>are shooting in the dark:  We can't figure out what you are missing,
>>and you aren't providing enough detail to help us see the gap.
>>
>>
>>hope this helps.  spencer graves
> 
> 
> Thanks for the reply Spencer.  I would like to offer a few additional
> comments, not in criticism but in the spirit in which you offer your
> comments - to assist the community.
> 
> It is not a good idea to use nls to fit a four-parameter model to 4
> data points.  The relative offset convergence criterion used in nls
> cannot be applied to cases with 0 degrees of freedom for residuals.
> 
> In simulating data for test fits by nls you must add noise to the
> response.  The relative offset convergence criterion used in nls will
> not, in general, declare convergence on artificial 'perfect fit' data.
> The author of nls does not regard this as a bug :-).
> 
> To get a reasonable fit of a four parameter logistic model you need
> data across a wide range of x values.  In particular you need data on
> both the 'toe' of the curve and the 'shoulder' of the curve.  A common
> reason for failure to fit the four-parameter logistic is the
> availability of data only on the 'toe' and the linear portion of
> the curve.  Without some data past the linear region you cannot
> determine an upper asymptote and the two parameters written as m and a
> in the above equation cannot be separately determined.
> 
> I would recommend using the 'plinear' algorithm in nls for this
> model.  Two of the four parameters are conditionally linear.  Reducing
> an optimization problem from four parameters to two parameters is a
> big win.
> 
> The SSfpl self-starting model can relieve the user of the need to form
> starting estimates.  See
> 
> ?SSfpl
> 
> and 
> 
> example(SSfpl)
> 
> The SSfpl model uses a slightly different parameterization of the
> model, which we (Don Watts and I) have found to be more stable than
> the one given above.  It can be difficult to estimate b in the above
> formulation.  We find it better to use a location-scale form of b and
> m and to converge on the logarithm of scale parameter.
> 
> Successful use of nonlinear regression depends on understanding the
> nature of the model and your data.  In particular, I recommend
> plotting the data before ever trying to fit any model.  You may recall
> that I go a little further than that in some courses and inform
> students that if I catch them fitting models without plotting the data
> first they will be in danger of failing the course. :-)
> 
> 
> Thanks for your efforts in responding to many, many questions on this
> list.  I hope my comments are helpful.
>



From spencer.graves at pdf.com  Wed Jul 16 16:50:30 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 07:50:30 -0700
Subject: [R] turning list-elements into a vector
References: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>
Message-ID: <3F156636.9090409@pdf.com>

Have you considere "unlist"?  Also, note that a matrix is a vector with 
a dim attribute, and a data.frame is actually a list with some other 
attributes.

hope this helps.
spencer graves

Michael Kirschbaum wrote:
> Hi.
> Can anyone help me?
> I want to create a vector from specific matrix-elements( e.g.[1,1]) these matrices are elements of a list.
> Is there any possibility to work without a loop? (e.g. with "lapply"?)
> hope, you can help me
> thank you
> Michael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Wed Jul 16 16:55:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Jul 2003 16:55:05 +0200
Subject: [R] turning list-elements into a vector
In-Reply-To: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>
References: <000801c34ba5$f77cbca0$82e6b1ac@tvpaxter>
Message-ID: <3F156749.4060902@statistik.uni-dortmund.de>

Michael Kirschbaum wrote:

> Hi.
> Can anyone help me?
> I want to create a vector from specific matrix-elements( e.g.[1,1]) these matrices are elements of a list.
> Is there any possibility to work without a loop? (e.g. with "lapply"?)
> hope, you can help me
> thank you
> Michael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


  sapply(list, function(x) x[1,1])

Uwe Ligges



From andy_liaw at merck.com  Wed Jul 16 16:59:31 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Jul 2003 10:59:31 -0400
Subject: [R] turning list-elements into a vector
Message-ID: <3A822319EB35174CA3714066D590DCD50205C89D@usrymx25.merck.com>

Something like this?

  myvec <- sapply(list.of.mat, function(x) x[i, j])

If all the matrices are of the same dimension, it might be easier to make
them into an array, and just "slice" through the array.

Andy



> -----Original Message-----
> From: Michael Kirschbaum [mailto:emkiba at gmx.de] 
> Sent: Wednesday, July 16, 2003 10:24 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] turning list-elements into a vector
> 
> 
> Hi.
> Can anyone help me?
> I want to create a vector from specific matrix-elements( 
> e.g.[1,1]) these matrices are elements of a list. Is there 
> any possibility to work without a loop? (e.g. with "lapply"?) 
> hope, you can help me thank you Michael
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From huan.huang at bnpparibas.com  Wed Jul 16 17:00:42 2003
From: huan.huang at bnpparibas.com (huan.huang@bnpparibas.com)
Date: Wed, 16 Jul 2003 16:00:42 +0100
Subject: [R] turning list-elements into a vector
Message-ID: <OF9D1527EE.C171A31B-ON80256D65.0052766A@bnpparibas.com>


hi Emkiba, try this:

x <- matrix(1:12, 4, 3)
y <- matrix(x, 12, 1)
yy <- y[,1]

regards,

Huan



Internet
emkiba at gmx.de@stat.math.ethz.ch - 07/16/2003 03:24 PM


Sent by:    r-help-bounces at stat.math.ethz.ch

To:    r-help

cc:


Subject:    [R] turning list-elements into a vector


Hi.
Can anyone help me?
I want to create a vector from specific matrix-elements( e.g.[1,1]) these
matrices are elements of a list.
Is there any possibility to work without a loop? (e.g. with "lapply"?)
hope, you can help me
thank you
Michael

 [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help






This message and any attachments (the "message") is\ intende...{{dropped}}



From petr.pikal at precheza.cz  Wed Jul 16 17:13:57 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 16 Jul 2003 17:13:57 +0200
Subject: [R] bwplot does something weird with Hmisc library attached
Message-ID: <3F1587D5.12646.B5EE64@localhost>

Dear all

I would like to ask you about possible bug in using bwplot (from lattice) together 
with Hmisc library attached. I found it in my actual data, but here is a toy 
example. It appears only when some levels are missing.

library(lattice)
library(Hmisc)

# preparing data
x1<-rnorm(10,5,1)
x2<-rnorm(10,5,5)
x3<-rnorm(10,1,1)
x4<-rnorm(10,1,5)
x<-c(x1,x2,x3,x4)
x<-c(x,x+5)
f2<-as.factor(rep(c("a","b","a","b"),c(20,20,20,20)))
f3<-as.factor(rep(rep(c("c","d"),each=10),4))
f1<-as.factor(rep(c(1,2),c(40,40)))
data<-data.frame(x,f1,f2,f3)
rm(x,f1,f2,f3)
attach(data)

# 1st plot
bwplot(f1~x|f2)

# 2 is bigger than 1 **OK**

# 2nd plot
bwplot(f1~x|f2*f3)

# again 2 is bigger than 1 **OK**

# removing some levels
detach("data")
data<-data[-c(1:10,21:30),]
attach(data)

# 3rd plot
bwplot(f1~x|f2)

# 2 is bigger than 1 **OK**

# 4th plot
bwplot(f1~x|f2*f3)

### OOOPS  1 is bigger than 2 ***FALSE*** 

after detaching Hmisc 
detach("package:Hmisc")

#5th plot
bwplot(f1~x|f2*f3)

# again 2 is bigger than 1 **OK**

The levels for factor f1 are in the OOOPS case reversed, maybe it has something 
to do with factor redefinition as stated when attaching Hmisc library.

Is it a bug or am I doing something wrong?

R 1.7.1 WNT

Package: Hmisc
Version: 1.6-1
Date: 2003-06-21

Package: lattice
Version: 0.7-14
Date: 2003/06/07

Thank you for any response.

Petr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From mrennie at utm.utoronto.ca  Wed Jul 16 17:18:28 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Wed, 16 Jul 2003 11:18:28 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <3F155332.4060708@pdf.com>
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
	<3F159798.8030504@hppi.troitsk.ru> <3F155332.4060708@pdf.com>
Message-ID: <1058368708.3f156cc41d4e2@webmail.utm.utoronto.ca>


Hi, Spencer

I know I submitted a beastly ammount of code, but I'm not sure how to simplify 
it much further, and still sucessfully address the problem that i am having.  
The reason being is that the funciton begins

f<- function (q) 

At the top of the iterative loop.  This is what takes q and generates Wtmod, 
Hgtmod at the end of the iterative loop. the assignment to f occurs at the 
bottom of the iterative loop. So, yes, the call to f is performing an immediate 
computation, but based on arguments that are coming out of the iterative loop 
above it, arguments which depend on q<-(p, ACT).  Maybe this is the problem; 
I've got too much going on between my function defenition and it's assignment, 
but I don't know how to get around it.

So, I'm not sure if your example will work- the output from the iterative 
process is Wtmod, Hgtmod, and I want to minimize the difference between them 
and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach this 
optimization are in the iterative loop (p, ACT), so re-defining these outputs 
as x's and getting it to vary these doesn't do me much good unless they are 
directly linked to the output of the iterative loop above it.  

Last, it's not even that I'm getting error messages anymore- I just can't get 
the solution that I get from Excel.  If I try to let R find the solution, and 
give it starting values of c(1,2), it gives me an optimization sulution, but an 
extremely poor one.  However, if I give it the answer I got from excel, it 
comes right back with the same answer and solutions I get from excel.  

Using the 'trace' function, I can see that R gets stuck in a specific region of 
parameter space in looking for the optimization and just appears to give up.  
Even when it re-set itself, it keeps going back to this region, and thus 
doesn't even try a full range of the parameter space I've defined before it 
stops and gives me the wrong answer.

I can try cleaning up the code and see if I can re-submit it, but what I am 
trying to program is so parameter heavy that 90% of it is just defining these 
at the top of the file.

Thank you for the suggestions,

Mike
  

Quoting Spencer Graves <spencer.graves at PDF.COM>:

> The phrase:
> 
>     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> 
> is an immediate computation, not a function.  If you want a function, 
> try something like the following:
> 
>     f <- function(x){
> 	  Wt <- x[1]
> 	  Wtmod <- x[2]
> 	  Hgt <- x[3]
> 	  Hgtmod <- x[4]
>       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>     }
> 
> OR
> 
>     f <- function(x, X){
> 	  Wt <- X[,1]
> 	  Hgt <- X[,2]
> 	  Wtmod <- x[1]
> 	  Hgtmod <- x[2]
> 	1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>     }
> 
> "par" in "optim" is the starting values for "x".  Pass "X" to "f" via 
> "..." in the call to "optim".
> 
> 	  If you can't make this work, please submit a toy example with the 
> code and error messages.  Please limit your example to 3 observations, 
> preferably whole numbers so someone else can read your question in 
> seconds.  If it is any longer than that, it should be ignored.
> 
> hope this helps.
> Spencer Graves
> 
> M.Kondrin wrote:
> >  >?optim
> > 
> > optim(par, fn, gr = NULL,
> >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"),
> >            lower = -Inf, upper = Inf,
> >            control = list(), hessian = FALSE, ...)
> > 
> > .....
> >       fn: A function to be minimized (or maximized), with first
> >           argument the vector of parameters over which minimization is
> >           to take place. It should return a scalar result.
> > 
> > Your fn defined as:
> > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> > What is its first argument I wonder?
> > I think you have just an ill-defined R function (although for Excel it 
> > may be OK - do not know) and optim just chokes on it.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From spencer.graves at pdf.com  Wed Jul 16 17:45:10 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 08:45:10 -0700
Subject: [R] Excel can do what R can't?????
References: <5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715131359.00a79a30@mail.utm.utoronto.ca>
	<5.1.0.14.0.20030715161906.00a7b830@mail.utm.utoronto.ca>
	<3F159798.8030504@hppi.troitsk.ru> <3F155332.4060708@pdf.com>
	<1058368708.3f156cc41d4e2@webmail.utm.utoronto.ca>
Message-ID: <3F157306.2050802@pdf.com>

1.  If I have an answer that works, I typically go to something else.

2.  If your Excel solution is still inadequate, have you considered 
trying to modularize your function "f", so "f" is 5-10 lines, several of 
which call other functions, f1, f2, ..., f6, say, and each of these do a 
piece of the computations that can be checked by comparison with 
intermediate results in Excel.

hope this helps.  spencer graves

Michael Rennie wrote:
> Hi, Spencer
> 
> I know I submitted a beastly ammount of code, but I'm not sure how to simplify 
> it much further, and still sucessfully address the problem that i am having.  
> The reason being is that the funciton begins
> 
> f<- function (q) 
> 
> At the top of the iterative loop.  This is what takes q and generates Wtmod, 
> Hgtmod at the end of the iterative loop. the assignment to f occurs at the 
> bottom of the iterative loop. So, yes, the call to f is performing an immediate 
> computation, but based on arguments that are coming out of the iterative loop 
> above it, arguments which depend on q<-(p, ACT).  Maybe this is the problem; 
> I've got too much going on between my function defenition and it's assignment, 
> but I don't know how to get around it.
> 
> So, I'm not sure if your example will work- the output from the iterative 
> process is Wtmod, Hgtmod, and I want to minimize the difference between them 
> and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach this 
> optimization are in the iterative loop (p, ACT), so re-defining these outputs 
> as x's and getting it to vary these doesn't do me much good unless they are 
> directly linked to the output of the iterative loop above it.  
> 
> Last, it's not even that I'm getting error messages anymore- I just can't get 
> the solution that I get from Excel.  If I try to let R find the solution, and 
> give it starting values of c(1,2), it gives me an optimization sulution, but an 
> extremely poor one.  However, if I give it the answer I got from excel, it 
> comes right back with the same answer and solutions I get from excel.  
> 
> Using the 'trace' function, I can see that R gets stuck in a specific region of 
> parameter space in looking for the optimization and just appears to give up.  
> Even when it re-set itself, it keeps going back to this region, and thus 
> doesn't even try a full range of the parameter space I've defined before it 
> stops and gives me the wrong answer.
> 
> I can try cleaning up the code and see if I can re-submit it, but what I am 
> trying to program is so parameter heavy that 90% of it is just defining these 
> at the top of the file.
> 
> Thank you for the suggestions,
> 
> Mike
>   
> 
> Quoting Spencer Graves <spencer.graves at PDF.COM>:
> 
> 
>>The phrase:
>>
>>    f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
>>
>>is an immediate computation, not a function.  If you want a function, 
>>try something like the following:
>>
>>    f <- function(x){
>>	  Wt <- x[1]
>>	  Wtmod <- x[2]
>>	  Hgt <- x[3]
>>	  Hgtmod <- x[4]
>>      1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>>    }
>>
>>OR
>>
>>    f <- function(x, X){
>>	  Wt <- X[,1]
>>	  Hgt <- X[,2]
>>	  Wtmod <- x[1]
>>	  Hgtmod <- x[2]
>>	1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>>    }
>>
>>"par" in "optim" is the starting values for "x".  Pass "X" to "f" via 
>>"..." in the call to "optim".
>>
>>	  If you can't make this work, please submit a toy example with the 
>>code and error messages.  Please limit your example to 3 observations, 
>>preferably whole numbers so someone else can read your question in 
>>seconds.  If it is any longer than that, it should be ignored.
>>
>>hope this helps.
>>Spencer Graves
>>
>>M.Kondrin wrote:
>>
>>> >?optim
>>>
>>>optim(par, fn, gr = NULL,
>>>           method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"),
>>>           lower = -Inf, upper = Inf,
>>>           control = list(), hessian = FALSE, ...)
>>>
>>>.....
>>>      fn: A function to be minimized (or maximized), with first
>>>          argument the vector of parameters over which minimization is
>>>          to take place. It should return a scalar result.
>>>
>>>Your fn defined as:
>>>f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
>>>What is its first argument I wonder?
>>>I think you have just an ill-defined R function (although for Excel it 
>>>may be OK - do not know) and optim just chokes on it.
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
> 
>



From rpeng at stat.ucla.edu  Wed Jul 16 18:23:29 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Wed, 16 Jul 2003 09:23:29 -0700
Subject: [R] numerical differentiation in R? (for optim "SANN" parscale)
In-Reply-To: <3F152734.2010603@gyer2.sote.hu>
References: <3F152734.2010603@gyer2.sote.hu>
Message-ID: <3F157C00.1020105@stat.ucla.edu>

'optim' does not require any differentiation of the objective function 
for the "SANN" method.  For the other four methods 'optim' will do 
numerical differentiation for you if a gradient is not provided.  
Furthermore, the 'parscale' argument has nothing to do with 
differentiation.  As far as I know, it is used to scale the values of 
the parameters before choosing candidates (so that they are roughly 
comparable). 

-roger

BORGULYA G?bor wrote:

> Dear R users,
>
> I am running a maximum likelihood model with optim. I chose the 
> simulated annealing method (method="SANN").
>
> SANN is not performing bad, but I guess it would be much more effecive 
> if I could set the `parscale' parameter.
>
> The help sais:
> `parscale' A vector of scaling values for the parameters.
>           Optimization is performed on `par/parscale' and these should
>           be comparable in the sense that a unit change in any element
>           produces about a unit change in the scaled value.
>
> Since I know the approximate optimal parameters of the function to 
> optimise I could use these values to calculate `parscale'.
> If I understand the role of `parscale' well, I have to differentiate 
> my function numerically.
>
> How can I perform the numerical differentiation in R? I thought about 
> writing a small function, but I am sure it is already written. It must 
> be present at least in some of the optimisation algorithms.
> Anyway, I couln't find it neither in the help, nor in the 
> non-internal, displayable source of optim.
>
> Could anyone tell me where to find such a function?
> And if it really is what I need for `parscale'?
>
> Thank you!
>
> G?bor



From carlos at zanaca.com  Wed Jul 16 18:26:04 2003
From: carlos at zanaca.com (Carlos Rios)
Date: Wed, 16 Jul 2003 13:26:04 -0300
Subject: [R] Line plot help
Message-ID: <006c01c34bb6$f4edebb0$0a01010a@suporte1>

I was wondering if is there any way of plotting  line segments on a plot.

Actually I need a graph that shows the advantage of using a different method
for estimate the parameter of my databse, so I've plotted the N estimatives
for M areas in my database and then I've plotted the new estimation (N'
estimatives) and I need a line to link those paired points (N'(1)-N(1),
N'(2)-N(2)) vertically, for every area.   I made a functions that draws
points trougth the diference of N(*)'-N(*) put it consume alot of CPU and
RAM...

Better, is there any plot that does what I want to do? :)



 Thanks,
  Carlos Rios
  http://www.ence.ibge.gov.br/



From spencer.graves at pdf.com  Wed Jul 16 18:43:14 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 09:43:14 -0700
Subject: [R] Line plot help
References: <006c01c34bb6$f4edebb0$0a01010a@suporte1>
Message-ID: <3F1580A2.1030103@pdf.com>

Standard advice:  "plot" to set "xlim" and "ylim", then a loop with a 
separate call to "lines" in a loop for each line desired.  Someone else 
may have something better for your current needs, but this works for me.

hope this helps.  spencer graves

Carlos Rios wrote:
> I was wondering if is there any way of plotting  line segments on a plot.
> 
> Actually I need a graph that shows the advantage of using a different method
> for estimate the parameter of my databse, so I've plotted the N estimatives
> for M areas in my database and then I've plotted the new estimation (N'
> estimatives) and I need a line to link those paired points (N'(1)-N(1),
> N'(2)-N(2)) vertically, for every area.   I made a functions that draws
> points trougth the diference of N(*)'-N(*) put it consume alot of CPU and
> RAM...
> 
> Better, is there any plot that does what I want to do? :)
> 
> 
> 
>  Thanks,
>   Carlos Rios
>   http://www.ence.ibge.gov.br/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jmacdon at med.umich.edu  Wed Jul 16 18:45:12 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 16 Jul 2003 12:45:12 -0400
Subject: [R] Line plot help
Message-ID: <sf1548f3.043@mail-01.med.umich.edu>

Segments should do what you want. Try ?segments for help.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Carlos Rios" <carlos at zanaca.com> 07/16/03 12:26PM >>>
I was wondering if is there any way of plotting  line segments on a
plot.

Actually I need a graph that shows the advantage of using a different
method
for estimate the parameter of my databse, so I've plotted the N
estimatives
for M areas in my database and then I've plotted the new estimation
(N'
estimatives) and I need a line to link those paired points
(N'(1)-N(1),
N'(2)-N(2)) vertically, for every area.   I made a functions that
draws
points trougth the diference of N(*)'-N(*) put it consume alot of CPU
and
RAM...

Better, is there any plot that does what I want to do? :)



 Thanks,
  Carlos Rios
  http://www.ence.ibge.gov.br/ 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mikem at salter-point.com  Wed Jul 16 18:54:39 2003
From: mikem at salter-point.com (Mike Meyer)
Date: Wed, 16 Jul 2003 09:54:39 -0700
Subject: [R] Is there a bug in qr(..,LAPACK=T)
Message-ID: <20030716095439.1be17b0c.mikem@salter-point.com>

The following snippet suggests that there is either a bug in qr(,LAPACK=T), or some bug in my understanding.   Note that the detected rank is correct (= 2) using the default LINPACK qr, but incorrect (=3) using LAPACK.   This is running on Linux Redhat 9.0, using the lapack library that comes with the Redhat distribution.   I'm running R 1.7.1 compiled from the source.  If the bug is in my understanding (or in the Redhat 9.0 libraries or compiler) I would much appreciate some enlightenment.
Thanks, --Mike

> X
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    2    1
[3,]    1    3    1
[4,]    1    4    1
> qr(X)
$qr
     [,1]       [,2] [,3]
[1,] -2.0 -5.0000000   -2
[2,]  0.5 -2.2360680    0
[3,]  0.5  0.4472136    0
[4,]  0.5  0.8944272    0

$rank
[1] 2

$qraux
[1] 1.5 1.0 0.0

$pivot
[1] 1 2 3

attr(,"class")
[1] "qr"

> qr(X,LAPACK=T)
$qr
           [,1]       [,2]          [,3]
[1,] -5.4772256 -1.8257419 -1.825742e+00
[2,]  0.3087742 -0.8164966 -8.164966e-01
[3,]  0.4631613 -0.3270981 -1.378276e-16
[4,]  0.6175484 -0.7892454  9.055216e-01

$rank
[1] 3

$qraux
[1] 1.182574 1.156135 1.098920

$pivot
[1] 2 1 3

attr(,"useLAPACK")
[1] TRUE
attr(,"class")
[1] "qr"
>

-- 

Mike Meyer,  Seattle WA



From rvaradha at jhsph.edu  Wed Jul 16 20:01:13 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Wed, 16 Jul 2003 14:01:13 -0400
Subject: [R] Is there a bug in qr(..,LAPACK=T)
Message-ID: <2bd9402bc1f3.2bc1f32bd940@jhsph.edu>


As the help page for "qr" says, LAPACK does not attempt to detect 
linear dependencies or rank deficiencies, so you should not use the 
value of "rank" obtained with argument, LAPACK = TRUE. Computing the 
rank of a matrix using finite precision is difficult, as the example on 
the help page for "qr" shows using Hilbert matrix order 9.  The rank 
can change depending on "tolerance" option, which is actually not used 
if LAPACK = TRUE.

Ravi.

----- Original Message -----
From: Mike Meyer <mikem at salter-point.com>
Date: Wednesday, July 16, 2003 12:54 pm
Subject: [R] Is there a bug in qr(..,LAPACK=T)

> The following snippet suggests that there is either a bug in 
> qr(,LAPACK=T), or some bug in my understanding.   Note that the 
> detected rank is correct (= 2) using the default LINPACK qr, but 
> incorrect (=3) using LAPACK.   This is running on Linux Redhat 
> 9.0, using the lapack library that comes with the Redhat 
> distribution.   I'm running R 1.7.1 compiled from the source.  If 
> the bug is in my understanding (or in the Redhat 9.0 libraries or 
> compiler) I would much appreciate some enlightenment.
> Thanks, --Mike
> 
> > X
>     [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    2    1
> [3,]    1    3    1
> [4,]    1    4    1
> > qr(X)
> $qr
>     [,1]       [,2] [,3]
> [1,] -2.0 -5.0000000   -2
> [2,]  0.5 -2.2360680    0
> [3,]  0.5  0.4472136    0
> [4,]  0.5  0.8944272    0
> 
> $rank
> [1] 2
> 
> $qraux
> [1] 1.5 1.0 0.0
> 
> $pivot
> [1] 1 2 3
> 
> attr(,"class")
> [1] "qr"
> 
> > qr(X,LAPACK=T)
> $qr
>           [,1]       [,2]          [,3]
> [1,] -5.4772256 -1.8257419 -1.825742e+00
> [2,]  0.3087742 -0.8164966 -8.164966e-01
> [3,]  0.4631613 -0.3270981 -1.378276e-16
> [4,]  0.6175484 -0.7892454  9.055216e-01
> 
> $rank
> [1] 3
> 
> $qraux
> [1] 1.182574 1.156135 1.098920
> 
> $pivot
> [1] 2 1 3
> 
> attr(,"useLAPACK")
> [1] TRUE
> attr(,"class")
> [1] "qr"
> >
> 
> -- 
> 
> Mike Meyer,  Seattle WA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From reid_huntsinger at merck.com  Wed Jul 16 20:09:58 2003
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 16 Jul 2003 14:09:58 -0400
Subject: [R] Excel can do what R can't?????
Message-ID: <2C23DE2983BE034CB1CB90DB6B813FD60612537E@uswpmx11.merck.com>

R is good at automating specific kinds of complex loops, namely those that
can be vectorized, or that can be written to draw on otherwise built-in
facilities. It's usually reasonable for other kinds of loops but not
spectacularly fast. You can write this part in C, though, quite easily, and
R provides very convenient utilities for this.

As for your code: You seem to have a system of equations that relates W and
Hg to their one-period-ago values. It might clarify things if you coded this
as a function: input time t values and q, output time t + 1 values. (You
wouldn't need any arrays.) Then f would just iterate this function and
calculate the criterion.

Does the trajectory of (W, Hg) for given q in R seem correct? Does it agree
with Excel? What does the criterion function look like? You could plot it in
R and perhaps see if the surface is complicated, in which case a simple grid
search might work for you.

Reid Huntsinger





-----Original Message-----
From: Michael Rennie [mailto:mrennie at utm.utoronto.ca] 
Sent: Wednesday, July 16, 2003 11:18 AM
To: Spencer Graves
Cc: R-Help; M.Kondrin
Subject: Re: [R] Excel can do what R can't?????



Hi, Spencer

I know I submitted a beastly ammount of code, but I'm not sure how to
simplify 
it much further, and still sucessfully address the problem that i am having.

The reason being is that the funciton begins

f<- function (q) 

At the top of the iterative loop.  This is what takes q and generates Wtmod,

Hgtmod at the end of the iterative loop. the assignment to f occurs at the 
bottom of the iterative loop. So, yes, the call to f is performing an
immediate 
computation, but based on arguments that are coming out of the iterative
loop 
above it, arguments which depend on q<-(p, ACT).  Maybe this is the problem;

I've got too much going on between my function defenition and it's
assignment, 
but I don't know how to get around it.

So, I'm not sure if your example will work- the output from the iterative 
process is Wtmod, Hgtmod, and I want to minimize the difference between them

and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach this

optimization are in the iterative loop (p, ACT), so re-defining these
outputs 
as x's and getting it to vary these doesn't do me much good unless they are 
directly linked to the output of the iterative loop above it.  

Last, it's not even that I'm getting error messages anymore- I just can't
get 
the solution that I get from Excel.  If I try to let R find the solution,
and 
give it starting values of c(1,2), it gives me an optimization sulution, but
an 
extremely poor one.  However, if I give it the answer I got from excel, it 
comes right back with the same answer and solutions I get from excel.  

Using the 'trace' function, I can see that R gets stuck in a specific region
of 
parameter space in looking for the optimization and just appears to give up.

Even when it re-set itself, it keeps going back to this region, and thus 
doesn't even try a full range of the parameter space I've defined before it 
stops and gives me the wrong answer.

I can try cleaning up the code and see if I can re-submit it, but what I am 
trying to program is so parameter heavy that 90% of it is just defining
these 
at the top of the file.

Thank you for the suggestions,

Mike
  

Quoting Spencer Graves <spencer.graves at PDF.COM>:

> The phrase:
> 
>     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> 
> is an immediate computation, not a function.  If you want a function, 
> try something like the following:
> 
>     f <- function(x){
> 	  Wt <- x[1]
> 	  Wtmod <- x[2]
> 	  Hgt <- x[3]
> 	  Hgtmod <- x[4]
>       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>     }
> 
> OR
> 
>     f <- function(x, X){
> 	  Wt <- X[,1]
> 	  Hgt <- X[,2]
> 	  Wtmod <- x[1]
> 	  Hgtmod <- x[2]
> 	1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>     }
> 
> "par" in "optim" is the starting values for "x".  Pass "X" to "f" via 
> "..." in the call to "optim".
> 
> 	  If you can't make this work, please submit a toy example with the 
> code and error messages.  Please limit your example to 3 observations, 
> preferably whole numbers so someone else can read your question in 
> seconds.  If it is any longer than that, it should be ignored.
> 
> hope this helps.
> Spencer Graves
> 
> M.Kondrin wrote:
> >  >?optim
> > 
> > optim(par, fn, gr = NULL,
> >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"),
> >            lower = -Inf, upper = Inf,
> >            control = list(), hessian = FALSE, ...)
> > 
> > .....
> >       fn: A function to be minimized (or maximized), with first
> >           argument the vector of parameters over which minimization is
> >           to take place. It should return a scalar result.
> > 
> > Your fn defined as:
> > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> > What is its first argument I wonder?
> > I think you have just an ill-defined R function (although for Excel it 
> > may be OK - do not know) and optim just chokes on it.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From mvalborn at artsci.wustl.edu  Wed Jul 16 20:38:22 2003
From: mvalborn at artsci.wustl.edu (Vicky Albornoz)
Date: Wed, 16 Jul 2003 13:38:22 -0500
Subject: [R] Fatal error in SJava.
Message-ID: <3F159B9E.8000306@artsci.wustl.edu>

Dear r-helpers,

I have been trying to invoke R from Java in a Windows 2000 computer 
(unfortunately). All my environment variables seem to be properly set, 
everything seems to be in order, but I obtaining a

Fatal error: unable to open the base package

error window.

Also, the output of the invoker is

Loading RInterpreter library
R_HOME: R_HOME=C:/Programas/R
RVersion: R_VERSION=1.6.1

whereas I have installed the 1.7.1 version.

Some time ago I found a number of messages by another person facing the 
same problems and he said that when R was invoked directly, it could 
properly read the R_HOME variable, but when invoked from Java, it 
searched for the base package in the wrong place (a subdirectory of the 
jre, if my memory is to be trusted). I believe he finally solved his 
problem fixing this "bug", but, I insist, I have not been able to find 
his message again anywhere and I could be totally misled at this point.

Has anybody faced the same problem and knows a way to fix it?

Sincerely,

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com



From Steve.Chriss at state.or.us  Wed Jul 16 20:45:38 2003
From: Steve.Chriss at state.or.us (CHRISS Steve)
Date: Wed, 16 Jul 2003 11:45:38 -0700
Subject: [R] AR in R
Message-ID: <4062462B678F084ABE97BBCC8705D3FE01889D79@opuc-email.puc.state.or.us>

R Help,
How does one do a least squares regression with an AR component (for any
order) in R?

Thanks,
Steve Chriss

________________________________
Steve W. Chriss
Oregon Public Utility Commission
P: 503.378.3778
F: 503.373.7752
E: steve.chriss at state.or.us



From mikem at salter-point.com  Wed Jul 16 20:50:55 2003
From: mikem at salter-point.com (Mike Meyer)
Date: Wed, 16 Jul 2003 11:50:55 -0700
Subject: [R] Is there a bug in qr(..,LAPACK=T)
In-Reply-To: <20030716095439.1be17b0c.mikem@salter-point.com>
References: <20030716095439.1be17b0c.mikem@salter-point.com>
Message-ID: <20030716115055.713ad33e.mikem@salter-point.com>

Several people have kindly (and gently) pointed out that the ?qr documentation states that rank detection does not work for the LAPACK case.  Its my fault for assuming that rank detection did work. --Mike


On Wed, 16 Jul 2003 09:54:39 -0700
Mike Meyer <mikem at salter-point.com> wrote:

> The following snippet suggests that there is either a bug in qr(,LAPACK=T), or some bug in my understanding.   Note that the detected rank is correct (= 2) using the default LINPACK qr, but incorrect (=3) using LAPACK.   This is running on Linux Redhat 9.0, using the lapack library that comes with the Redhat distribution.   I'm running R 1.7.1 compiled from the source.  If the bug is in my understanding (or in the Redhat 9.0 libraries or compiler) I would much appreciate some enlightenment.
> Thanks, --Mike
> 
> > X
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    2    1
> [3,]    1    3    1
> [4,]    1    4    1
> > qr(X)
> $qr
>      [,1]       [,2] [,3]
> [1,] -2.0 -5.0000000   -2
> [2,]  0.5 -2.2360680    0
> [3,]  0.5  0.4472136    0
> [4,]  0.5  0.8944272    0
> 
> $rank
> [1] 2
> 
> $qraux
> [1] 1.5 1.0 0.0
> 
> $pivot
> [1] 1 2 3
> 
> attr(,"class")
> [1] "qr"
> 
> > qr(X,LAPACK=T)
> $qr
>            [,1]       [,2]          [,3]
> [1,] -5.4772256 -1.8257419 -1.825742e+00
> [2,]  0.3087742 -0.8164966 -8.164966e-01
> [3,]  0.4631613 -0.3270981 -1.378276e-16
> [4,]  0.6175484 -0.7892454  9.055216e-01
> 
> $rank
> [1] 3
> 
> $qraux
> [1] 1.182574 1.156135 1.098920
> 
> $pivot
> [1] 2 1 3
> 
> attr(,"useLAPACK")
> [1] TRUE
> attr(,"class")
> [1] "qr"
> >
> 
> -- 
> 
> Mike Meyer,  Seattle WA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 

Mike Meyer,  Seattle WA



From ozric at web.de  Wed Jul 16 21:05:22 2003
From: ozric at web.de (Christian Schulz)
Date: Wed, 16 Jul 2003 21:05:22 +0200
Subject: [R] Fatal error in SJava.
References: <3F159B9E.8000306@artsci.wustl.edu>
Message-ID: <003a01c34bcd$36128c70$c50aebd9@pc>

Perhaps you forget the entry in your  /rw1071/.Renviron  like:
JAVA_HOME=c:/yourPath/Java/j2re1.4.1_02  ?

>>library(SJava)
using JAVA_HOME = c:/Programme/Java/j2re1.4.1_02

regards,christian


----- Original Message -----
From: "Vicky Albornoz" <mvalborn at artsci.wustl.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, July 16, 2003 8:38 PM
Subject: [R] Fatal error in SJava.


Dear r-helpers,

I have been trying to invoke R from Java in a Windows 2000 computer
(unfortunately). All my environment variables seem to be properly set,
everything seems to be in order, but I obtaining a

Fatal error: unable to open the base package

error window.

Also, the output of the invoker is

Loading RInterpreter library
R_HOME: R_HOME=C:/Programas/R
RVersion: R_VERSION=1.6.1

whereas I have installed the 1.7.1 version.

Some time ago I found a number of messages by another person facing the
same problems and he said that when R was invoked directly, it could
properly read the R_HOME variable, but when invoked from Java, it
searched for the base package in the wrong place (a subdirectory of the
jre, if my memory is to be trusted). I believe he finally solved his
problem fixing this "bug", but, I insist, I have not been able to find
his message again anywhere and I could be totally misled at this point.

Has anybody faced the same problem and knows a way to fix it?

Sincerely,

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jfox at mcmaster.ca  Wed Jul 16 21:17:13 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Jul 2003 15:17:13 -0400
Subject: [R] AR in R
In-Reply-To: <4062462B678F084ABE97BBCC8705D3FE01889D79@opuc-email.puc.st
	ate.or.us>
Message-ID: <5.1.0.14.2.20030716151426.01e75168@127.0.0.1>

Dear Steve,

At 11:45 AM 7/16/2003 -0700, CHRISS Steve wrote:
>R Help,
>How does one do a least squares regression with an AR component (for any
>order) in R?
>
>Thanks,
>Steve Chriss

Do you mean a model with AR errors? If so, you can use the gls (generalized 
least squares) function in the nlme package. Some details and an example 
with AR(2) errors are in 
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix-timeseries-regression.pdf>.

I hope that this helps,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From joerg.evermann at vuw.ac.nz  Wed Jul 16 22:56:47 2003
From: joerg.evermann at vuw.ac.nz (Joerg Evermann)
Date: Wed, 16 Jul 2003 13:56:47 -0700
Subject: [R] Q re Linear Models
Message-ID: <03844792-B7D0-11D7-A44D-000A959A3508@vuw.ac.nz>

Hello,

I apologize in advance if what I'm about to ask is trivial or has been 
answered before. In the latter case I would appreciate a pointer to the 
right list/location

I'm trying to model the following experimental design with groupedData 
and lme in R:

Subjects were measured on two tasks (continuous outcome variable is 
"Prob", tasks are coded as within-subjects factor "Domain"). Subjects 
are recruited from three different groups ("Group") and assigned to 
three different conditions ("Rules"). These all appear to be fixed 
between-subjects factors. The effect I'm after is that of "Rules".

Additionally, I've measured for each observation (for each subject for 
each task) four continuous variables ("SelfAssessment", "Usefulness", 
"Information" and "Ease") that I'd like to include as covariates. These 
of course may partially depend on the subject. Additionally, in a 
post-test I measure yet another continuous variable for each subject 
("UMLTTL").

The problem I have is that I'm not sure how to properly include these 
continuous covariates in the groupedData formulas and the lme formulas.

Any help will be appreciated, either on the list of by email.

Many many thanks in advance.

  Joerg



From mrennie at utm.utoronto.ca  Wed Jul 16 22:54:42 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Wed, 16 Jul 2003 16:54:42 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <2C23DE2983BE034CB1CB90DB6B813FD606125381@uswpmx11.merck.co
 m>
Message-ID: <5.1.0.14.0.20030716164605.0239a0a0@mail.utm.utoronto.ca>

Hi, Reid

>Do the values of W and Hg over time for a given q agree between R and Excel?
>Not the optimal value of q, just the trajectories for fixed q (trying
>several values for q).

If I take the iterative loop out of the function, and ask for values of 
Hgmod, Hgtmod, and f, then I get EXACTLY what I get out of Excel.  It's the 
optimization that seems to be the problem.  If I trace the solutions, R 
isn't even exploring the full parameter space I tell it to look in.  SO, 
the iterative loop is correct, and doing what it's supposed to, since 
values of p, ACT match exactly what they do in excel- it's something about 
how R is examining the possibilities in  the optimization process that is 
giving me different answers between the two.

I dunno- I'm going to tinker with it some more tonight.

Mike

>Reid
>
>-----Original Message-----
>From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>Sent: Wednesday, July 16, 2003 2:47 PM
>To: Huntsinger, Reid
>Subject: RE: [R] Excel can do what R can't?????
>
>
>
>Hi, Reid
>
>At 02:09 PM 7/16/03 -0400, you wrote:
> >R is good at automating specific kinds of complex loops, namely those that
> >can be vectorized, or that can be written to draw on otherwise built-in
> >facilities. It's usually reasonable for other kinds of loops but not
> >spectacularly fast. You can write this part in C, though, quite easily, and
> >R provides very convenient utilities for this.
> >
> >As for your code: You seem to have a system of equations that relates W and
> >Hg to their one-period-ago values. It might clarify things if you coded
>this
> >as a function: input time t values and q, output time t + 1 values. (You
> >wouldn't need any arrays.) Then f would just iterate this function and
> >calculate the criterion.
>
>Wouldn't I still need to loop this function to get it through 365 days?  Is
>there a big difference, then, between this and what I've got?
>
> >Does the trajectory of (W, Hg) for given q in R seem correct? Does it agree
> >with Excel? What does the criterion function look like? You could plot it
>in
> >R and perhaps see if the surface is complicated, in which case a simple
>grid
> >search might work for you.
>
>When I give R the values that I get in excel for p, ACT, the function
>solution is actually more precise than what I get in Excel; the values for
>p, ACT come back identical (then again, they are exactly what I
>assigned..)  But, if I leave R on it's own to find the solution, it keeps
>getting jammed in a particular region.  I've never done any function
>plotting in R, but it would help if I could see what kind of surface I get
>for f as a function of p, ACT- this would at least force R to examine the
>full range of values specified by the upper and lower limits I've set
>(which it isn't doing under the 'optim' command).
>
>Mike
>
>
> >Reid Huntsinger
> >
> >
> >
> >
> >
> >-----Original Message-----
> >From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
> >Sent: Wednesday, July 16, 2003 11:18 AM
> >To: Spencer Graves
> >Cc: R-Help; M.Kondrin
> >Subject: Re: [R] Excel can do what R can't?????
> >
> >
> >
> >Hi, Spencer
> >
> >I know I submitted a beastly ammount of code, but I'm not sure how to
> >simplify
> >it much further, and still sucessfully address the problem that i am
>having.
> >
> >The reason being is that the funciton begins
> >
> >f<- function (q)
> >
> >At the top of the iterative loop.  This is what takes q and generates
>Wtmod,
> >
> >Hgtmod at the end of the iterative loop. the assignment to f occurs at the
> >bottom of the iterative loop. So, yes, the call to f is performing an
> >immediate
> >computation, but based on arguments that are coming out of the iterative
> >loop
> >above it, arguments which depend on q<-(p, ACT).  Maybe this is the
>problem;
> >
> >I've got too much going on between my function defenition and it's
> >assignment,
> >but I don't know how to get around it.
> >
> >So, I'm not sure if your example will work- the output from the iterative
> >process is Wtmod, Hgtmod, and I want to minimize the difference between
>them
> >
> >and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach
>this
> >
> >optimization are in the iterative loop (p, ACT), so re-defining these
> >outputs
> >as x's and getting it to vary these doesn't do me much good unless they are
> >directly linked to the output of the iterative loop above it.
> >
> >Last, it's not even that I'm getting error messages anymore- I just can't
> >get
> >the solution that I get from Excel.  If I try to let R find the solution,
> >and
> >give it starting values of c(1,2), it gives me an optimization sulution,
>but
> >an
> >extremely poor one.  However, if I give it the answer I got from excel, it
> >comes right back with the same answer and solutions I get from excel.
> >
> >Using the 'trace' function, I can see that R gets stuck in a specific
>region
> >of
> >parameter space in looking for the optimization and just appears to give
>up.
> >
> >Even when it re-set itself, it keeps going back to this region, and thus
> >doesn't even try a full range of the parameter space I've defined before it
> >stops and gives me the wrong answer.
> >
> >I can try cleaning up the code and see if I can re-submit it, but what I am
> >trying to program is so parameter heavy that 90% of it is just defining
> >these
> >at the top of the file.
> >
> >Thank you for the suggestions,
> >
> >Mike
> >
> >
> >Quoting Spencer Graves <spencer.graves at PDF.COM>:
> >
> > > The phrase:
> > >
> > >     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ;
>f
> > >
> > > is an immediate computation, not a function.  If you want a function,
> > > try something like the following:
> > >
> > >     f <- function(x){
> > >         Wt <- x[1]
> > >         Wtmod <- x[2]
> > >         Hgt <- x[3]
> > >         Hgtmod <- x[4]
> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
> > >     }
> > >
> > > OR
> > >
> > >     f <- function(x, X){
> > >         Wt <- X[,1]
> > >         Hgt <- X[,2]
> > >         Wtmod <- x[1]
> > >         Hgtmod <- x[2]
> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
> > >     }
> > >
> > > "par" in "optim" is the starting values for "x".  Pass "X" to "f" via
> > > "..." in the call to "optim".
> > >
> > >         If you can't make this work, please submit a toy example with
>the
> > > code and error messages.  Please limit your example to 3 observations,
> > > preferably whole numbers so someone else can read your question in
> > > seconds.  If it is any longer than that, it should be ignored.
> > >
> > > hope this helps.
> > > Spencer Graves
> > >
> > > M.Kondrin wrote:
> > > >  >?optim
> > > >
> > > > optim(par, fn, gr = NULL,
> > > >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B",
>"SANN"),
> > > >            lower = -Inf, upper = Inf,
> > > >            control = list(), hessian = FALSE, ...)
> > > >
> > > > .....
> > > >       fn: A function to be minimized (or maximized), with first
> > > >           argument the vector of parameters over which minimization is
> > > >           to take place. It should return a scalar result.
> > > >
> > > > Your fn defined as:
> > > > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> > > > What is its first argument I wonder?
> > > > I think you have just an ill-defined R function (although for Excel it
> > > > may be OK - do not know) and optim just chokes on it.
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > >
> >
> >
> >--
> >Michael Rennie
> >M.Sc. Candidate
> >University of Toronto at Mississauga
> >3359 Mississauga Rd. N.
> >Mississauga ON  L5L 1C6
> >Ph: 905-828-5452  Fax: 905-828-3792
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >---------------------------------------------------------------------------
>---
> >Notice: This e-mail message, together with any attachments, contains
> >information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
> >USA) that may be confidential, proprietary copyrighted and/or legally
> >privileged, and is intended solely for the use of the individual or entity
> >named on this message. If you are not the intended recipient, and
> >have received this message in error, please immediately return this by
> >e-mail and then delete it.
> >---------------------------------------------------------------------------
>---
>
>Michael Rennie
>M.Sc. Candidate
>University of Toronto at Mississauga
>3359 Mississauga Rd. N.
>Mississauga, ON  L5L 1C6
>Ph: 905-828-5452  Fax: 905-828-3792
>
>
>------------------------------------------------------------------------------
>Notice: This e-mail message, together with any attachments, contains
>information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>USA) that may be confidential, proprietary copyrighted and/or legally
>privileged, and is intended solely for the use of the individual or entity
>named on this message. If you are not the intended recipient, and
>have received this message in error, please immediately return this by
>e-mail and then delete it.
>------------------------------------------------------------------------------

Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From spencer.graves at pdf.com  Wed Jul 16 23:20:30 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 14:20:30 -0700
Subject: [R] Excel can do what R can't?????
References: <5.1.0.14.0.20030716164605.0239a0a0@mail.utm.utoronto.ca>
Message-ID: <3F15C19E.9010302@pdf.com>

I'm confused:

	  I've done this type of thing by programming the same objective 
function in R (or S-Plus)  and Excel.  After the answers from my 
objective function in R match the answers in Excel, then I pass that 
objective function to something like "optim", which then finds the same 
answers as "solver" in Excel.  Your latest description makes me wonder 
if the function you pass to "optim" tries to do some of the optimization 
that "optim" is supposed to do. ???

hope this helps.  spencer graves

Michael Rennie wrote:
> Hi, Reid
> 
>> Do the values of W and Hg over time for a given q agree between R and 
>> Excel?
>> Not the optimal value of q, just the trajectories for fixed q (trying
>> several values for q).
> 
> 
> If I take the iterative loop out of the function, and ask for values of 
> Hgmod, Hgtmod, and f, then I get EXACTLY what I get out of Excel.  It's 
> the optimization that seems to be the problem.  If I trace the 
> solutions, R isn't even exploring the full parameter space I tell it to 
> look in.  SO, the iterative loop is correct, and doing what it's 
> supposed to, since values of p, ACT match exactly what they do in excel- 
> it's something about how R is examining the possibilities in  the 
> optimization process that is giving me different answers between the two.
> 
> I dunno- I'm going to tinker with it some more tonight.
> 
> Mike
> 
>> Reid
>>
>> -----Original Message-----
>> From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>> Sent: Wednesday, July 16, 2003 2:47 PM
>> To: Huntsinger, Reid
>> Subject: RE: [R] Excel can do what R can't?????
>>
>>
>>
>> Hi, Reid
>>
>> At 02:09 PM 7/16/03 -0400, you wrote:
>> >R is good at automating specific kinds of complex loops, namely those 
>> that
>> >can be vectorized, or that can be written to draw on otherwise built-in
>> >facilities. It's usually reasonable for other kinds of loops but not
>> >spectacularly fast. You can write this part in C, though, quite 
>> easily, and
>> >R provides very convenient utilities for this.
>> >
>> >As for your code: You seem to have a system of equations that relates 
>> W and
>> >Hg to their one-period-ago values. It might clarify things if you coded
>> this
>> >as a function: input time t values and q, output time t + 1 values. (You
>> >wouldn't need any arrays.) Then f would just iterate this function and
>> >calculate the criterion.
>>
>> Wouldn't I still need to loop this function to get it through 365 
>> days?  Is
>> there a big difference, then, between this and what I've got?
>>
>> >Does the trajectory of (W, Hg) for given q in R seem correct? Does it 
>> agree
>> >with Excel? What does the criterion function look like? You could 
>> plot it
>> in
>> >R and perhaps see if the surface is complicated, in which case a simple
>> grid
>> >search might work for you.
>>
>> When I give R the values that I get in excel for p, ACT, the function
>> solution is actually more precise than what I get in Excel; the values 
>> for
>> p, ACT come back identical (then again, they are exactly what I
>> assigned..)  But, if I leave R on it's own to find the solution, it keeps
>> getting jammed in a particular region.  I've never done any function
>> plotting in R, but it would help if I could see what kind of surface I 
>> get
>> for f as a function of p, ACT- this would at least force R to examine the
>> full range of values specified by the upper and lower limits I've set
>> (which it isn't doing under the 'optim' command).
>>
>> Mike
>>
>>
>> >Reid Huntsinger
>> >
>> >
>> >
>> >
>> >
>> >-----Original Message-----
>> >From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>> >Sent: Wednesday, July 16, 2003 11:18 AM
>> >To: Spencer Graves
>> >Cc: R-Help; M.Kondrin
>> >Subject: Re: [R] Excel can do what R can't?????
>> >
>> >
>> >
>> >Hi, Spencer
>> >
>> >I know I submitted a beastly ammount of code, but I'm not sure how to
>> >simplify
>> >it much further, and still sucessfully address the problem that i am
>> having.
>> >
>> >The reason being is that the funciton begins
>> >
>> >f<- function (q)
>> >
>> >At the top of the iterative loop.  This is what takes q and generates
>> Wtmod,
>> >
>> >Hgtmod at the end of the iterative loop. the assignment to f occurs 
>> at the
>> >bottom of the iterative loop. So, yes, the call to f is performing an
>> >immediate
>> >computation, but based on arguments that are coming out of the iterative
>> >loop
>> >above it, arguments which depend on q<-(p, ACT).  Maybe this is the
>> problem;
>> >
>> >I've got too much going on between my function defenition and it's
>> >assignment,
>> >but I don't know how to get around it.
>> >
>> >So, I'm not sure if your example will work- the output from the 
>> iterative
>> >process is Wtmod, Hgtmod, and I want to minimize the difference between
>> them
>> >
>> >and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach
>> this
>> >
>> >optimization are in the iterative loop (p, ACT), so re-defining these
>> >outputs
>> >as x's and getting it to vary these doesn't do me much good unless 
>> they are
>> >directly linked to the output of the iterative loop above it.
>> >
>> >Last, it's not even that I'm getting error messages anymore- I just 
>> can't
>> >get
>> >the solution that I get from Excel.  If I try to let R find the 
>> solution,
>> >and
>> >give it starting values of c(1,2), it gives me an optimization sulution,
>> but
>> >an
>> >extremely poor one.  However, if I give it the answer I got from 
>> excel, it
>> >comes right back with the same answer and solutions I get from excel.
>> >
>> >Using the 'trace' function, I can see that R gets stuck in a specific
>> region
>> >of
>> >parameter space in looking for the optimization and just appears to give
>> up.
>> >
>> >Even when it re-set itself, it keeps going back to this region, and thus
>> >doesn't even try a full range of the parameter space I've defined 
>> before it
>> >stops and gives me the wrong answer.
>> >
>> >I can try cleaning up the code and see if I can re-submit it, but 
>> what I am
>> >trying to program is so parameter heavy that 90% of it is just defining
>> >these
>> >at the top of the file.
>> >
>> >Thank you for the suggestions,
>> >
>> >Mike
>> >
>> >
>> >Quoting Spencer Graves <spencer.graves at PDF.COM>:
>> >
>> > > The phrase:
>> > >
>> > >     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>> (((Hgt-Hgtmod)^2)/Hgt))2) ;
>> f
>> > >
>> > > is an immediate computation, not a function.  If you want a function,
>> > > try something like the following:
>> > >
>> > >     f <- function(x){
>> > >         Wt <- x[1]
>> > >         Wtmod <- x[2]
>> > >         Hgt <- x[3]
>> > >         Hgtmod <- x[4]
>> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>> > >     }
>> > >
>> > > OR
>> > >
>> > >     f <- function(x, X){
>> > >         Wt <- X[,1]
>> > >         Hgt <- X[,2]
>> > >         Wtmod <- x[1]
>> > >         Hgtmod <- x[2]
>> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>> > >     }
>> > >
>> > > "par" in "optim" is the starting values for "x".  Pass "X" to "f" via
>> > > "..." in the call to "optim".
>> > >
>> > >         If you can't make this work, please submit a toy example with
>> the
>> > > code and error messages.  Please limit your example to 3 
>> observations,
>> > > preferably whole numbers so someone else can read your question in
>> > > seconds.  If it is any longer than that, it should be ignored.
>> > >
>> > > hope this helps.
>> > > Spencer Graves
>> > >
>> > > M.Kondrin wrote:
>> > > >  >?optim
>> > > >
>> > > > optim(par, fn, gr = NULL,
>> > > >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B",
>> "SANN"),
>> > > >            lower = -Inf, upper = Inf,
>> > > >            control = list(), hessian = FALSE, ...)
>> > > >
>> > > > .....
>> > > >       fn: A function to be minimized (or maximized), with first
>> > > >           argument the vector of parameters over which 
>> minimization is
>> > > >           to take place. It should return a scalar result.
>> > > >
>> > > > Your fn defined as:
>> > > > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>> (((Hgt-Hgtmod)^2)/Hgt))2) ; f
>> > > > What is its first argument I wonder?
>> > > > I think you have just an ill-defined R function (although for 
>> Excel it
>> > > > may be OK - do not know) and optim just chokes on it.
>> > > >
>> > > > ______________________________________________
>> > > > R-help at stat.math.ethz.ch mailing list
>> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > >
>> > >
>> >
>> >
>> >--
>> >Michael Rennie
>> >M.Sc. Candidate
>> >University of Toronto at Mississauga
>> >3359 Mississauga Rd. N.
>> >Mississauga ON  L5L 1C6
>> >Ph: 905-828-5452  Fax: 905-828-3792
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >
>> >--------------------------------------------------------------------------- 
>>
>> ---
>> >Notice: This e-mail message, together with any attachments, contains
>> >information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>> >USA) that may be confidential, proprietary copyrighted and/or legally
>> >privileged, and is intended solely for the use of the individual or 
>> entity
>> >named on this message. If you are not the intended recipient, and
>> >have received this message in error, please immediately return this by
>> >e-mail and then delete it.
>> >--------------------------------------------------------------------------- 
>>
>> ---
>>
>> Michael Rennie
>> M.Sc. Candidate
>> University of Toronto at Mississauga
>> 3359 Mississauga Rd. N.
>> Mississauga, ON  L5L 1C6
>> Ph: 905-828-5452  Fax: 905-828-3792
>>
>>
>> ------------------------------------------------------------------------------ 
>>
>> Notice: This e-mail message, together with any attachments, contains
>> information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>> USA) that may be confidential, proprietary copyrighted and/or legally
>> privileged, and is intended solely for the use of the individual or 
>> entity
>> named on this message. If you are not the intended recipient, and
>> have received this message in error, please immediately return this by
>> e-mail and then delete it.
>> ------------------------------------------------------------------------------ 
>>
> 
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jul 16 23:23:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 14:23:59 -0700
Subject: [R] Q re Linear Models
References: <03844792-B7D0-11D7-A44D-000A959A3508@vuw.ac.nz>
Message-ID: <3F15C26F.9060805@pdf.com>

Do you have Pinhiero and Bates (2000) Mixed-Effects Models in S and 
S-Plus (Springer)?  For me, "lme" is a Ferrari not a Volkswagen, and I 
had to read a portion of this book before I could get much of anything 
out of "lme".

hope this helps.  spencer graves

Joerg Evermann wrote:
> Hello,
> 
> I apologize in advance if what I'm about to ask is trivial or has been 
> answered before. In the latter case I would appreciate a pointer to the 
> right list/location
> 
> I'm trying to model the following experimental design with groupedData 
> and lme in R:
> 
> Subjects were measured on two tasks (continuous outcome variable is 
> "Prob", tasks are coded as within-subjects factor "Domain"). Subjects 
> are recruited from three different groups ("Group") and assigned to 
> three different conditions ("Rules"). These all appear to be fixed 
> between-subjects factors. The effect I'm after is that of "Rules".
> 
> Additionally, I've measured for each observation (for each subject for 
> each task) four continuous variables ("SelfAssessment", "Usefulness", 
> "Information" and "Ease") that I'd like to include as covariates. These 
> of course may partially depend on the subject. Additionally, in a 
> post-test I measure yet another continuous variable for each subject 
> ("UMLTTL").
> 
> The problem I have is that I'm not sure how to properly include these 
> continuous covariates in the groupedData formulas and the lme formulas.
> 
> Any help will be appreciated, either on the list of by email.
> 
> Many many thanks in advance.
> 
>  Joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From reid_huntsinger at merck.com  Wed Jul 16 23:36:00 2003
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 16 Jul 2003 17:36:00 -0400
Subject: [R] Excel can do what R can't?????
Message-ID: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>

I don't understand how you can take the loop out of the function and still
get
values for the final timepoint. And whether the optimal parameter values
agree wasn't
my question. First I'd like to determine whether Hg and W (in your code)
have the same
value in R as they do in Excel, for a few possible values of the parameter
q. Then,
if so, whether the surface over which you're minimizing is complicated or
not. As I said, if it is you can't expect local optimizers to work very well
without good starting values,
and they really don't explore the whole parameter space--that would be too
slow for their
usual applications. Optimization approaches like grid search or simulated
annealing do try
to cover the parameter space and may be better suited to your use. I would
certainly try plotting and grid search just to see what's happening since
that's clearly possible with
two parameters.

Reid Huntsinger

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at PDF.COM] 
Sent: Wednesday, July 16, 2003 5:20 PM
To: Michael Rennie
Cc: Huntsinger, Reid; R-Help
Subject: Re: [R] Excel can do what R can't?????


I'm confused:

	  I've done this type of thing by programming the same objective 
function in R (or S-Plus)  and Excel.  After the answers from my 
objective function in R match the answers in Excel, then I pass that 
objective function to something like "optim", which then finds the same 
answers as "solver" in Excel.  Your latest description makes me wonder 
if the function you pass to "optim" tries to do some of the optimization 
that "optim" is supposed to do. ???

hope this helps.  spencer graves

Michael Rennie wrote:
> Hi, Reid
> 
>> Do the values of W and Hg over time for a given q agree between R and 
>> Excel?
>> Not the optimal value of q, just the trajectories for fixed q (trying
>> several values for q).
> 
> 
> If I take the iterative loop out of the function, and ask for values of 
> Hgmod, Hgtmod, and f, then I get EXACTLY what I get out of Excel.  It's 
> the optimization that seems to be the problem.  If I trace the 
> solutions, R isn't even exploring the full parameter space I tell it to 
> look in.  SO, the iterative loop is correct, and doing what it's 
> supposed to, since values of p, ACT match exactly what they do in excel- 
> it's something about how R is examining the possibilities in  the 
> optimization process that is giving me different answers between the two.
> 
> I dunno- I'm going to tinker with it some more tonight.
> 
> Mike
> 
>> Reid
>>
>> -----Original Message-----
>> From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>> Sent: Wednesday, July 16, 2003 2:47 PM
>> To: Huntsinger, Reid
>> Subject: RE: [R] Excel can do what R can't?????
>>
>>
>>
>> Hi, Reid
>>
>> At 02:09 PM 7/16/03 -0400, you wrote:
>> >R is good at automating specific kinds of complex loops, namely those 
>> that
>> >can be vectorized, or that can be written to draw on otherwise built-in
>> >facilities. It's usually reasonable for other kinds of loops but not
>> >spectacularly fast. You can write this part in C, though, quite 
>> easily, and
>> >R provides very convenient utilities for this.
>> >
>> >As for your code: You seem to have a system of equations that relates 
>> W and
>> >Hg to their one-period-ago values. It might clarify things if you coded
>> this
>> >as a function: input time t values and q, output time t + 1 values. (You
>> >wouldn't need any arrays.) Then f would just iterate this function and
>> >calculate the criterion.
>>
>> Wouldn't I still need to loop this function to get it through 365 
>> days?  Is
>> there a big difference, then, between this and what I've got?
>>
>> >Does the trajectory of (W, Hg) for given q in R seem correct? Does it 
>> agree
>> >with Excel? What does the criterion function look like? You could 
>> plot it
>> in
>> >R and perhaps see if the surface is complicated, in which case a simple
>> grid
>> >search might work for you.
>>
>> When I give R the values that I get in excel for p, ACT, the function
>> solution is actually more precise than what I get in Excel; the values 
>> for
>> p, ACT come back identical (then again, they are exactly what I
>> assigned..)  But, if I leave R on it's own to find the solution, it keeps
>> getting jammed in a particular region.  I've never done any function
>> plotting in R, but it would help if I could see what kind of surface I 
>> get
>> for f as a function of p, ACT- this would at least force R to examine the
>> full range of values specified by the upper and lower limits I've set
>> (which it isn't doing under the 'optim' command).
>>
>> Mike
>>
>>
>> >Reid Huntsinger
>> >
>> >
>> >
>> >
>> >
>> >-----Original Message-----
>> >From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>> >Sent: Wednesday, July 16, 2003 11:18 AM
>> >To: Spencer Graves
>> >Cc: R-Help; M.Kondrin
>> >Subject: Re: [R] Excel can do what R can't?????
>> >
>> >
>> >
>> >Hi, Spencer
>> >
>> >I know I submitted a beastly ammount of code, but I'm not sure how to
>> >simplify
>> >it much further, and still sucessfully address the problem that i am
>> having.
>> >
>> >The reason being is that the funciton begins
>> >
>> >f<- function (q)
>> >
>> >At the top of the iterative loop.  This is what takes q and generates
>> Wtmod,
>> >
>> >Hgtmod at the end of the iterative loop. the assignment to f occurs 
>> at the
>> >bottom of the iterative loop. So, yes, the call to f is performing an
>> >immediate
>> >computation, but based on arguments that are coming out of the iterative
>> >loop
>> >above it, arguments which depend on q<-(p, ACT).  Maybe this is the
>> problem;
>> >
>> >I've got too much going on between my function defenition and it's
>> >assignment,
>> >but I don't know how to get around it.
>> >
>> >So, I'm not sure if your example will work- the output from the 
>> iterative
>> >process is Wtmod, Hgtmod, and I want to minimize the difference between
>> them
>> >
>> >and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach
>> this
>> >
>> >optimization are in the iterative loop (p, ACT), so re-defining these
>> >outputs
>> >as x's and getting it to vary these doesn't do me much good unless 
>> they are
>> >directly linked to the output of the iterative loop above it.
>> >
>> >Last, it's not even that I'm getting error messages anymore- I just 
>> can't
>> >get
>> >the solution that I get from Excel.  If I try to let R find the 
>> solution,
>> >and
>> >give it starting values of c(1,2), it gives me an optimization sulution,
>> but
>> >an
>> >extremely poor one.  However, if I give it the answer I got from 
>> excel, it
>> >comes right back with the same answer and solutions I get from excel.
>> >
>> >Using the 'trace' function, I can see that R gets stuck in a specific
>> region
>> >of
>> >parameter space in looking for the optimization and just appears to give
>> up.
>> >
>> >Even when it re-set itself, it keeps going back to this region, and thus
>> >doesn't even try a full range of the parameter space I've defined 
>> before it
>> >stops and gives me the wrong answer.
>> >
>> >I can try cleaning up the code and see if I can re-submit it, but 
>> what I am
>> >trying to program is so parameter heavy that 90% of it is just defining
>> >these
>> >at the top of the file.
>> >
>> >Thank you for the suggestions,
>> >
>> >Mike
>> >
>> >
>> >Quoting Spencer Graves <spencer.graves at PDF.COM>:
>> >
>> > > The phrase:
>> > >
>> > >     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>> (((Hgt-Hgtmod)^2)/Hgt))2) ;
>> f
>> > >
>> > > is an immediate computation, not a function.  If you want a function,
>> > > try something like the following:
>> > >
>> > >     f <- function(x){
>> > >         Wt <- x[1]
>> > >         Wtmod <- x[2]
>> > >         Hgt <- x[3]
>> > >         Hgtmod <- x[4]
>> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>> > >     }
>> > >
>> > > OR
>> > >
>> > >     f <- function(x, X){
>> > >         Wt <- X[,1]
>> > >         Hgt <- X[,2]
>> > >         Wtmod <- x[1]
>> > >         Hgtmod <- x[2]
>> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>> > >     }
>> > >
>> > > "par" in "optim" is the starting values for "x".  Pass "X" to "f" via
>> > > "..." in the call to "optim".
>> > >
>> > >         If you can't make this work, please submit a toy example with
>> the
>> > > code and error messages.  Please limit your example to 3 
>> observations,
>> > > preferably whole numbers so someone else can read your question in
>> > > seconds.  If it is any longer than that, it should be ignored.
>> > >
>> > > hope this helps.
>> > > Spencer Graves
>> > >
>> > > M.Kondrin wrote:
>> > > >  >?optim
>> > > >
>> > > > optim(par, fn, gr = NULL,
>> > > >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B",
>> "SANN"),
>> > > >            lower = -Inf, upper = Inf,
>> > > >            control = list(), hessian = FALSE, ...)
>> > > >
>> > > > .....
>> > > >       fn: A function to be minimized (or maximized), with first
>> > > >           argument the vector of parameters over which 
>> minimization is
>> > > >           to take place. It should return a scalar result.
>> > > >
>> > > > Your fn defined as:
>> > > > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>> (((Hgt-Hgtmod)^2)/Hgt))2) ; f
>> > > > What is its first argument I wonder?
>> > > > I think you have just an ill-defined R function (although for 
>> Excel it
>> > > > may be OK - do not know) and optim just chokes on it.
>> > > >
>> > > > ______________________________________________
>> > > > R-help at stat.math.ethz.ch mailing list
>> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > >
>> > >
>> >
>> >
>> >--
>> >Michael Rennie
>> >M.Sc. Candidate
>> >University of Toronto at Mississauga
>> >3359 Mississauga Rd. N.
>> >Mississauga ON  L5L 1C6
>> >Ph: 905-828-5452  Fax: 905-828-3792
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> >
>>
>---------------------------------------------------------------------------

>>
>> ---
>> >Notice: This e-mail message, together with any attachments, contains
>> >information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>> >USA) that may be confidential, proprietary copyrighted and/or legally
>> >privileged, and is intended solely for the use of the individual or 
>> entity
>> >named on this message. If you are not the intended recipient, and
>> >have received this message in error, please immediately return this by
>> >e-mail and then delete it.
>>
>---------------------------------------------------------------------------

>>
>> ---
>>
>> Michael Rennie
>> M.Sc. Candidate
>> University of Toronto at Mississauga
>> 3359 Mississauga Rd. N.
>> Mississauga, ON  L5L 1C6
>> Ph: 905-828-5452  Fax: 905-828-3792
>>
>>
>>
----------------------------------------------------------------------------
-- 
>>
>> Notice: This e-mail message, together with any attachments, contains
>> information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>> USA) that may be confidential, proprietary copyrighted and/or legally
>> privileged, and is intended solely for the use of the individual or 
>> entity
>> named on this message. If you are not the intended recipient, and
>> have received this message in error, please immediately return this by
>> e-mail and then delete it.
>>
----------------------------------------------------------------------------
-- 
>>
> 
> 
> Michael Rennie
> M.Sc. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From runge at plan.auc.dk  Wed Jul 16 23:41:04 2003
From: runge at plan.auc.dk (Jesper Runge Madsen)
Date: Wed, 16 Jul 2003 23:41:04 +0200 (CEST)
Subject: [R] list to data frame
Message-ID: <4957.62.79.113.27.1058391664.squirrel@webmail.plan.auc.dk>

Dear R helpers
I am trying to convert a list into a data frame but when I try, I get a
stack overflow error (Error: protect(): stack overflow). My list contains
about 17000 rows and looks like shown at the bottom. The reason that I
want to convert it in to a data frame is that I want to export it to a
mysql database with the dbWriteTable function.

The function that I use is
As.data.frame(listname)

I hope someone can help me



Cut out from the list
structure(list("-9.000000" = 187, "9754.000000" = 130, "9755.000000" = 129,
    "9756.000000" = 125.5, "9757.000000" = 118.1111, "9762.000000" =
132.6667,
    "9763.000000" = 133, "9764.000000" = 130, "9766.000000" = 130.5,
    "9780.000000" = 160, "9787.000000" = 154, "9808.000000" = 147.8,
    "9811.000000" = 156.5, "9812.000000" = 154.3333, "9815.000000" = 141,
    "9819.000000" = 135, "9820.000000" = 141, "9821.000000" = 140.5,
.
.
.
.
    "525965.000000" = 76.4545), .Names = c("-9.000000", "9754.000000",
    "9755.000000", "9756.000000", "9757.000000", "9762.000000",
"9763.000000",
    "9764.000000", "9766.000000", "9780.000000", "9787.000000",
"9808.000000",
    "9811.000000", "9812.000000", "9815.000000", "9819.000000",
"9820.000000",
    "9821.000000", "9822.000000", "9823.000000", "9824.000000",
"9825.000000",
.
.
.
.
    "525863.000000", "525866.000000", "525867.000000", "525868.000000",
    "525869.000000", "525870.000000", "525951.000000", "525952.000000",
    "525954.000000", "525962.000000", "525964.000000", "525965.000000"
))

/Jesper Runge Madsen
Aalborg Universitet
Denmark



From borgulya at gyer2.sote.hu  Thu Jul 17 00:06:12 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Thu, 17 Jul 2003 00:06:12 +0200
Subject: [R] Specifying an lme model
In-Reply-To: <he5nv3et.fsf@uq.edu.au>
References: <7k6k9xrp.fsf@uq.edu.au> <6rsmp76cxw.fsf@bates4.stat.wisc.edu>
	<he5nv3et.fsf@uq.edu.au>
Message-ID: <3F15CC54.8070203@gyer2.sote.hu>

Dear Ross,

I think I can not answer your question, but I have three ideas that 
might be useful:

1. Cluster randomized trials often ask similar questions. Having a look 
at CRT methods may suggest you a solution.
2. Similar questions may arise in time series analysis. I wouldn't be 
surprised if a standard tool for time series could solve your problem.
3. I like maximum likelihood methods so I find it interesting to treat 
the question as minimising three likelihood functions. You could use the 
likelihood-ratio test to test significance.

I hope you find something useful from the above.

G?bor



From p.dalgaard at biostat.ku.dk  Thu Jul 17 00:25:17 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 16 Jul 2003 22:25:17 -0000
Subject: [R] list to data frame
In-Reply-To: <4957.62.79.113.27.1058391664.squirrel@webmail.plan.auc.dk>
References: <4957.62.79.113.27.1058391664.squirrel@webmail.plan.auc.dk>
Message-ID: <x27k6i85uk.fsf@biostat.ku.dk>

"Jesper Runge Madsen" <runge at plan.auc.dk> writes:

> Dear R helpers
> I am trying to convert a list into a data frame but when I try, I get a
> stack overflow error (Error: protect(): stack overflow). My list contains
> about 17000 rows and looks like shown at the bottom. The reason that I
> want to convert it in to a data frame is that I want to export it to a
> mysql database with the dbWriteTable function.
> 
> The function that I use is
> As.data.frame(listname)
> 
> I hope someone can help me
> 
> 
> 
> Cut out from the list
> structure(list("-9.000000" = 187, "9754.000000" = 130, "9755.000000" = 129,
>     "9756.000000" = 125.5, "9757.000000" = 118.1111, "9762.000000" =
> 132.6667,
>     "9763.000000" = 133, "9764.000000" = 130, "9766.000000" = 130.5,
>     "9780.000000" = 160, "9787.000000" = 154, "9808.000000" = 147.8,
>     "9811.000000" = 156.5, "9812.000000" = 154.3333, "9815.000000" = 141,
>     "9819.000000" = 135, "9820.000000" = 141, "9821.000000" = 140.5,
> .
> .
> .
> .
>     "525965.000000" = 76.4545), .Names = c("-9.000000", "9754.000000",
>     "9755.000000", "9756.000000", "9757.000000", "9762.000000",
> "9763.000000",
>     "9764.000000", "9766.000000", "9780.000000", "9787.000000",
> "9808.000000",
>     "9811.000000", "9812.000000", "9815.000000", "9819.000000",
> "9820.000000",
>     "9821.000000", "9822.000000", "9823.000000", "9824.000000",
> "9825.000000",
> .
> .
> .
> .
>     "525863.000000", "525866.000000", "525867.000000", "525868.000000",
>     "525869.000000", "525870.000000", "525951.000000", "525952.000000",
>     "525954.000000", "525962.000000", "525964.000000", "525965.000000"
> ))
> 
> /Jesper Runge Madsen
> Aalborg Universitet
> Denmark

There may or may not be a bug in R, but are you sure that the list is
in the format you want? Those numeric names look suspicious and the
whole thing would be converted to a data frame with *1* row and a
helluva lot of columns... If you really have a long list of length-1
vectors, you may want to do something like

data.frame(x=unlist(l))

or perhaps

data.frame(x=unlist(l),row.names=names(l))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sbinny at sina.com  Thu Jul 17 00:25:15 2003
From: sbinny at sina.com (sbinny)
Date: Thu, 17 Jul 2003 06:25:15 +0800
Subject: [R] Help on NNET
Message-ID: <20030716222515.16744.qmail@sina.com>

Hi, Dear all,

I am just starting using R in my work and got some trouble to figure out some of the errors. Can anybody help me?

The following is the script:

read.csv('pupil.txt',header=TRUE,sep='\t')->pupil
samp<-c(1:50, 112:162, 171:220, 228:278)
pupil.nn2 <- nnet(Type ~ ., data = pupil, subset = samp, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
table(pupil$Type[-samp], predict(pupil.nn2, pupil[-samp,], type = "class"))

After running this, I got error information as

#Error in switch(type, raw = z, class = { : 
#        inappropriate fit for class

BTW, pupil.txt
X1   X2  Type
0.2  0.5  0
...........
..........
.........1
.........
.........
..........2
.......
........3
........
there are totally 351 records. 
My objective is to classify them into 4 classes.

Thanks a lot for your help!



From spencer.graves at pdf.com  Thu Jul 17 00:38:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 15:38:26 -0700
Subject: [R] Excel can do what R can't?????
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
Message-ID: <3F15D3E2.2090105@pdf.com>

	  Clearly there is much I don't understand about your problem.  I 
suspect you are doing something that isn't correct in R, and the 
complexity of the problem makes it difficult for you or anyone else to 
isolate the likely error.

	  Have you tried discarding Hg (or W) and simplifying the problem to 
estimating a single parameter Hgmod with only 3 observations, say?  Then 
you can program those three numbers in Excel and R and have a better 
chance of finding the problem.  If you still can't find it, give that to 
the world and maybe someone else can see it quickly.  You can't expect 
others to search a huge haystack for your needle, but if you cut it to a 
small handful of straw, it is much easier to find the needle just by 
squeezing a little.

	  If you get the toy problem to work and still can't see why the full 
problem doesn't, add complexity back into your function a little at a 
time until you figure it out.

	  In this process, I suggest you modularize your code:  Don't write one 
huge complicated function.  Write a series of relatively simple 
functions, with the more complicated tasks being done by simpler 
functions.

	  Example:
Define the following functions:

   X1 <- function(X, Xmod)(((X-Xmod)^2)/X)

   Wt.Hg <- function(Xmod, Wt, Hgt)(X1(Wt, Xmod[1])+X1(Hgt, Xmod[2]))

Then (1e9*Wt.Hg(Xmod, Wt, Hgt)) computes the following line:

 >>>>>      1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)

	  This may not be a good example, but you need to invent concepts for 
portions of your code.

hope this helps.  spencer graves

Huntsinger, Reid wrote:
> I don't understand how you can take the loop out of the function and still
> get
> values for the final timepoint. And whether the optimal parameter values
> agree wasn't
> my question. First I'd like to determine whether Hg and W (in your code)
> have the same
> value in R as they do in Excel, for a few possible values of the parameter
> q. Then,
> if so, whether the surface over which you're minimizing is complicated or
> not. As I said, if it is you can't expect local optimizers to work very well
> without good starting values,
> and they really don't explore the whole parameter space--that would be too
> slow for their
> usual applications. Optimization approaches like grid search or simulated
> annealing do try
> to cover the parameter space and may be better suited to your use. I would
> certainly try plotting and grid search just to see what's happening since
> that's clearly possible with
> two parameters.
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at PDF.COM] 
> Sent: Wednesday, July 16, 2003 5:20 PM
> To: Michael Rennie
> Cc: Huntsinger, Reid; R-Help
> Subject: Re: [R] Excel can do what R can't?????
> 
> 
> I'm confused:
> 
> 	  I've done this type of thing by programming the same objective 
> function in R (or S-Plus)  and Excel.  After the answers from my 
> objective function in R match the answers in Excel, then I pass that 
> objective function to something like "optim", which then finds the same 
> answers as "solver" in Excel.  Your latest description makes me wonder 
> if the function you pass to "optim" tries to do some of the optimization 
> that "optim" is supposed to do. ???
> 
> hope this helps.  spencer graves
> 
> Michael Rennie wrote:
> 
>>Hi, Reid
>>
>>
>>>Do the values of W and Hg over time for a given q agree between R and 
>>>Excel?
>>>Not the optimal value of q, just the trajectories for fixed q (trying
>>>several values for q).
>>
>>
>>If I take the iterative loop out of the function, and ask for values of 
>>Hgmod, Hgtmod, and f, then I get EXACTLY what I get out of Excel.  It's 
>>the optimization that seems to be the problem.  If I trace the 
>>solutions, R isn't even exploring the full parameter space I tell it to 
>>look in.  SO, the iterative loop is correct, and doing what it's 
>>supposed to, since values of p, ACT match exactly what they do in excel- 
>>it's something about how R is examining the possibilities in  the 
>>optimization process that is giving me different answers between the two.
>>
>>I dunno- I'm going to tinker with it some more tonight.
>>
>>Mike
>>
>>
>>>Reid
>>>
>>>-----Original Message-----
>>>From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>>>Sent: Wednesday, July 16, 2003 2:47 PM
>>>To: Huntsinger, Reid
>>>Subject: RE: [R] Excel can do what R can't?????
>>>
>>>
>>>
>>>Hi, Reid
>>>
>>>At 02:09 PM 7/16/03 -0400, you wrote:
>>>
>>>>R is good at automating specific kinds of complex loops, namely those 
>>>
>>>that
>>>
>>>>can be vectorized, or that can be written to draw on otherwise built-in
>>>>facilities. It's usually reasonable for other kinds of loops but not
>>>>spectacularly fast. You can write this part in C, though, quite 
>>>
>>>easily, and
>>>
>>>>R provides very convenient utilities for this.
>>>>
>>>>As for your code: You seem to have a system of equations that relates 
>>>
>>>W and
>>>
>>>>Hg to their one-period-ago values. It might clarify things if you coded
>>>
>>>this
>>>
>>>>as a function: input time t values and q, output time t + 1 values. (You
>>>>wouldn't need any arrays.) Then f would just iterate this function and
>>>>calculate the criterion.
>>>
>>>Wouldn't I still need to loop this function to get it through 365 
>>>days?  Is
>>>there a big difference, then, between this and what I've got?
>>>
>>>
>>>>Does the trajectory of (W, Hg) for given q in R seem correct? Does it 
>>>
>>>agree
>>>
>>>>with Excel? What does the criterion function look like? You could 
>>>
>>>plot it
>>>in
>>>
>>>>R and perhaps see if the surface is complicated, in which case a simple
>>>
>>>grid
>>>
>>>>search might work for you.
>>>
>>>When I give R the values that I get in excel for p, ACT, the function
>>>solution is actually more precise than what I get in Excel; the values 
>>>for
>>>p, ACT come back identical (then again, they are exactly what I
>>>assigned..)  But, if I leave R on it's own to find the solution, it keeps
>>>getting jammed in a particular region.  I've never done any function
>>>plotting in R, but it would help if I could see what kind of surface I 
>>>get
>>>for f as a function of p, ACT- this would at least force R to examine the
>>>full range of values specified by the upper and lower limits I've set
>>>(which it isn't doing under the 'optim' command).
>>>
>>>Mike
>>>
>>>
>>>
>>>>Reid Huntsinger
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>-----Original Message-----
>>>>From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
>>>>Sent: Wednesday, July 16, 2003 11:18 AM
>>>>To: Spencer Graves
>>>>Cc: R-Help; M.Kondrin
>>>>Subject: Re: [R] Excel can do what R can't?????
>>>>
>>>>
>>>>
>>>>Hi, Spencer
>>>>
>>>>I know I submitted a beastly ammount of code, but I'm not sure how to
>>>>simplify
>>>>it much further, and still sucessfully address the problem that i am
>>>
>>>having.
>>>
>>>>The reason being is that the funciton begins
>>>>
>>>>f<- function (q)
>>>>
>>>>At the top of the iterative loop.  This is what takes q and generates
>>>
>>>Wtmod,
>>>
>>>>Hgtmod at the end of the iterative loop. the assignment to f occurs 
>>>
>>>at the
>>>
>>>>bottom of the iterative loop. So, yes, the call to f is performing an
>>>>immediate
>>>>computation, but based on arguments that are coming out of the iterative
>>>>loop
>>>>above it, arguments which depend on q<-(p, ACT).  Maybe this is the
>>>
>>>problem;
>>>
>>>>I've got too much going on between my function defenition and it's
>>>>assignment,
>>>>but I don't know how to get around it.
>>>>
>>>>So, I'm not sure if your example will work- the output from the 
>>>
>>>iterative
>>>
>>>>process is Wtmod, Hgtmod, and I want to minimize the difference between
>>>
>>>them
>>>
>>>>and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach
>>>
>>>this
>>>
>>>>optimization are in the iterative loop (p, ACT), so re-defining these
>>>>outputs
>>>>as x's and getting it to vary these doesn't do me much good unless 
>>>
>>>they are
>>>
>>>>directly linked to the output of the iterative loop above it.
>>>>
>>>>Last, it's not even that I'm getting error messages anymore- I just 
>>>
>>>can't
>>>
>>>>get
>>>>the solution that I get from Excel.  If I try to let R find the 
>>>
>>>solution,
>>>
>>>>and
>>>>give it starting values of c(1,2), it gives me an optimization sulution,
>>>
>>>but
>>>
>>>>an
>>>>extremely poor one.  However, if I give it the answer I got from 
>>>
>>>excel, it
>>>
>>>>comes right back with the same answer and solutions I get from excel.
>>>>
>>>>Using the 'trace' function, I can see that R gets stuck in a specific
>>>
>>>region
>>>
>>>>of
>>>>parameter space in looking for the optimization and just appears to give
>>>
>>>up.
>>>
>>>>Even when it re-set itself, it keeps going back to this region, and thus
>>>>doesn't even try a full range of the parameter space I've defined 
>>>
>>>before it
>>>
>>>>stops and gives me the wrong answer.
>>>>
>>>>I can try cleaning up the code and see if I can re-submit it, but 
>>>
>>>what I am
>>>
>>>>trying to program is so parameter heavy that 90% of it is just defining
>>>>these
>>>>at the top of the file.
>>>>
>>>>Thank you for the suggestions,
>>>>
>>>>Mike
>>>>
>>>>
>>>>Quoting Spencer Graves <spencer.graves at PDF.COM>:
>>>>
>>>>
>>>>>The phrase:
>>>>>
>>>>>    f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>>>>
>>>(((Hgt-Hgtmod)^2)/Hgt))2) ;
>>>f
>>>
>>>>>is an immediate computation, not a function.  If you want a function,
>>>>>try something like the following:
>>>>>
>>>>>    f <- function(x){
>>>>>        Wt <- x[1]
>>>>>        Wtmod <- x[2]
>>>>>        Hgt <- x[3]
>>>>>        Hgtmod <- x[4]
>>>>>      1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>>>>>    }
>>>>>
>>>>>OR
>>>>>
>>>>>    f <- function(x, X){
>>>>>        Wt <- X[,1]
>>>>>        Hgt <- X[,2]
>>>>>        Wtmod <- x[1]
>>>>>        Hgtmod <- x[2]
>>>>>      1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
>>>>>    }
>>>>>
>>>>>"par" in "optim" is the starting values for "x".  Pass "X" to "f" via
>>>>>"..." in the call to "optim".
>>>>>
>>>>>        If you can't make this work, please submit a toy example with
>>>>
>>>the
>>>
>>>>>code and error messages.  Please limit your example to 3 
>>>>
>>>observations,
>>>
>>>>>preferably whole numbers so someone else can read your question in
>>>>>seconds.  If it is any longer than that, it should be ignored.
>>>>>
>>>>>hope this helps.
>>>>>Spencer Graves
>>>>>
>>>>>M.Kondrin wrote:
>>>>>
>>>>>> >?optim
>>>>>>
>>>>>>optim(par, fn, gr = NULL,
>>>>>>           method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B",
>>>>>
>>>"SANN"),
>>>
>>>>>>           lower = -Inf, upper = Inf,
>>>>>>           control = list(), hessian = FALSE, ...)
>>>>>>
>>>>>>.....
>>>>>>      fn: A function to be minimized (or maximized), with first
>>>>>>          argument the vector of parameters over which 
>>>>>
>>>minimization is
>>>
>>>>>>          to take place. It should return a scalar result.
>>>>>>
>>>>>>Your fn defined as:
>>>>>>f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
>>>>>
>>>(((Hgt-Hgtmod)^2)/Hgt))2) ; f
>>>
>>>>>>What is its first argument I wonder?
>>>>>>I think you have just an ill-defined R function (although for 
>>>>>
>>>Excel it
>>>
>>>>>>may be OK - do not know) and optim just chokes on it.
>>>>>>
>>>>>>______________________________________________
>>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>>
>>>>>
>>>>
>>>>--
>>>>Michael Rennie
>>>>M.Sc. Candidate
>>>>University of Toronto at Mississauga
>>>>3359 Mississauga Rd. N.
>>>>Mississauga ON  L5L 1C6
>>>>Ph: 905-828-5452  Fax: 905-828-3792
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>
>>>
>>---------------------------------------------------------------------------
> 
> 
>>>---
>>>
>>>>Notice: This e-mail message, together with any attachments, contains
>>>>information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
>>>>USA) that may be confidential, proprietary copyrighted and/or legally
>>>>privileged, and is intended solely for the use of the individual or 
>>>
>>>entity
>>>
>>>>named on this message. If you are not the intended recipient, and
>>>>have received this message in error, please immediately return this by
>>>>e-mail and then delete it.
>>>
>>---------------------------------------------------------------------------
> 
> 
>>>---
>>>
>>>Michael Rennie
>>>M.Sc. Candidate
>>>University of Toronto at Mississauga
>>>3359 Mississauga Rd. N.
>>>Mississauga, ON  L5L 1C6
>>>Ph: 905-828-5452  Fax: 905-828-3792
>>>
>>>
>>>
>>
> ----------------------------------------------------------------------------



From mrennie at utm.utoronto.ca  Thu Jul 17 01:30:59 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Wed, 16 Jul 2003 19:30:59 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
Message-ID: <1058398259.3f15e0335a583@webmail.utm.utoronto.ca>


Hi, Reid and Spencer- 

I think I've figured something out pretty critical to the problem.  

Loking at my 'solver' options, I have a condition added that 'Hgtmod = Hgt'.  
Without this conditional statement, I have to run solver 3-4 times before I get 
a final solution. MEANING- solver and R, when left to their own devices, suck 
equally at finding a solution with similar starting points.  BUT, given a 
conditional statement that demands Hgt = Hgtmod, it gives it somewhere to look 
withing the given parameter space. 

So, the millon dollar quesiton: Is there any way of setting up a contitional 
statement like this in 'optim', to specify a solution such that Hgtmod = Hgt?  
Or, write it into function f?  The control statements 'fnscale' and 'parscale' 
look like candidates, but will only help me if I build Hgtmod into the 
optimization statement- can I do that?  How do you specify multiple 
parameters?  Or should I specify two functions to optimize- one, fucntion f, 
and a second, something like

g<- (Hgt = Hgtmod)^2  

Can I do that? If this is all I need to do, I am off to the races, and owe you 
both a beer!  I'm going to try some stuff. All is not lost! If you have any 
ideas, I'd love to hear them.

THanks again for everything.....

Quoting "Huntsinger, Reid" <reid_huntsinger at merck.com>:

> I don't understand how you can take the loop out of the function and still
> get
> values for the final timepoint. And whether the optimal parameter values
> agree wasn't
> my question. First I'd like to determine whether Hg and W (in your code)
> have the same
> value in R as they do in Excel, for a few possible values of the parameter
> q. 

Yes- this was what I was trying to communicate (albeit unsucessfully) in my 
last e-mail.  values of 1,2 or 1.5,1.5 or whatever I try, I get the same 
answers for f, Wtmod, Hgtmod in Excel as I do in R.  Understand, though, that 
in order to determine the agreement for these values, I'm not using an 
optimization function of any sort- just plugging in numbers, and seeing what I 
get back.  THat's what I meant about taking the 365-day loop out of the 
optimization function; even though I am still calculating a value for f, I am 
no longer defining it at the top of the loop with 'f<-function(q)'; simply 
performing the calculation 

f <- 1000000000*((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt)) ; f.  

In order to find out if the value of f is the same in excel and in R given the 
same starting parameters of p, ACT. Sorry- I didn't mean to be unclear.

Then,
> if so, whether the surface over which you're minimizing is complicated or
> not. As I said, if it is you can't expect local optimizers to work very
> well
> without good starting values,
> and they really don't explore the whole parameter space--that would be too
> slow for their
> usual applications. Optimization approaches like grid search or simulated
> annealing do try
> to cover the parameter space and may be better suited to your use. I would
> certainly try plotting and grid search just to see what's happening since
> that's clearly possible with
> two parameters.

This is next on the list- I suspect that this is where I may be running into 
problems. But, if this is were I am running into problems, then does that mean 
that Excel is using a better optimization process than R?  Since they are 
recognizing the same parameter space over p, ACT, as evidenced that they offer 
identical solutions for the same values of p, ACT, then shouldn't they be 
reaching the same solution?

AH! Ureka moment- see top of e-mail........
  

Thanks again for your suggestions.

Mike

> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at PDF.COM] 
> Sent: Wednesday, July 16, 2003 5:20 PM
> To: Michael Rennie
> Cc: Huntsinger, Reid; R-Help
> Subject: Re: [R] Excel can do what R can't?????
> 
> 
> I'm confused:
> 
> 	  I've done this type of thing by programming the same objective 
> function in R (or S-Plus)  and Excel.  After the answers from my 
> objective function in R match the answers in Excel, then I pass that 
> objective function to something like "optim", which then finds the same 
> answers as "solver" in Excel.  Your latest description makes me wonder 
> if the function you pass to "optim" tries to do some of the optimization 
> that "optim" is supposed to do. ???
> 
> hope this helps.  spencer graves
> 
> Michael Rennie wrote:
> > Hi, Reid
> > 
> >> Do the values of W and Hg over time for a given q agree between R and 
> >> Excel?
> >> Not the optimal value of q, just the trajectories for fixed q (trying
> >> several values for q).
> > 
> > 
> > If I take the iterative loop out of the function, and ask for values of 
> > Hgmod, Hgtmod, and f, then I get EXACTLY what I get out of Excel.  It's 
> > the optimization that seems to be the problem.  If I trace the 
> > solutions, R isn't even exploring the full parameter space I tell it to 
> > look in.  SO, the iterative loop is correct, and doing what it's 
> > supposed to, since values of p, ACT match exactly what they do in excel- 
> > it's something about how R is examining the possibilities in  the 
> > optimization process that is giving me different answers between the two.
> > 
> > I dunno- I'm going to tinker with it some more tonight.
> > 
> > Mike
> > 
> >> Reid
> >>
> >> -----Original Message-----
> >> From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
> >> Sent: Wednesday, July 16, 2003 2:47 PM
> >> To: Huntsinger, Reid
> >> Subject: RE: [R] Excel can do what R can't?????
> >>
> >>
> >>
> >> Hi, Reid
> >>
> >> At 02:09 PM 7/16/03 -0400, you wrote:
> >> >R is good at automating specific kinds of complex loops, namely those 
> >> that
> >> >can be vectorized, or that can be written to draw on otherwise built-in
> >> >facilities. It's usually reasonable for other kinds of loops but not
> >> >spectacularly fast. You can write this part in C, though, quite 
> >> easily, and
> >> >R provides very convenient utilities for this.
> >> >
> >> >As for your code: You seem to have a system of equations that relates 
> >> W and
> >> >Hg to their one-period-ago values. It might clarify things if you coded
> >> this
> >> >as a function: input time t values and q, output time t + 1 values.
> (You
> >> >wouldn't need any arrays.) Then f would just iterate this function and
> >> >calculate the criterion.
> >>
> >> Wouldn't I still need to loop this function to get it through 365 
> >> days?  Is
> >> there a big difference, then, between this and what I've got?
> >>
> >> >Does the trajectory of (W, Hg) for given q in R seem correct? Does it 
> >> agree
> >> >with Excel? What does the criterion function look like? You could 
> >> plot it
> >> in
> >> >R and perhaps see if the surface is complicated, in which case a simple
> >> grid
> >> >search might work for you.
> >>
> >> When I give R the values that I get in excel for p, ACT, the function
> >> solution is actually more precise than what I get in Excel; the values 
> >> for
> >> p, ACT come back identical (then again, they are exactly what I
> >> assigned..)  But, if I leave R on it's own to find the solution, it
> keeps
> >> getting jammed in a particular region.  I've never done any function
> >> plotting in R, but it would help if I could see what kind of surface I 
> >> get
> >> for f as a function of p, ACT- this would at least force R to examine
> the
> >> full range of values specified by the upper and lower limits I've set
> >> (which it isn't doing under the 'optim' command).
> >>
> >> Mike
> >>
> >>
> >> >Reid Huntsinger
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >-----Original Message-----
> >> >From: Michael Rennie [mailto:mrennie at utm.utoronto.ca]
> >> >Sent: Wednesday, July 16, 2003 11:18 AM
> >> >To: Spencer Graves
> >> >Cc: R-Help; M.Kondrin
> >> >Subject: Re: [R] Excel can do what R can't?????
> >> >
> >> >
> >> >
> >> >Hi, Spencer
> >> >
> >> >I know I submitted a beastly ammount of code, but I'm not sure how to
> >> >simplify
> >> >it much further, and still sucessfully address the problem that i am
> >> having.
> >> >
> >> >The reason being is that the funciton begins
> >> >
> >> >f<- function (q)
> >> >
> >> >At the top of the iterative loop.  This is what takes q and generates
> >> Wtmod,
> >> >
> >> >Hgtmod at the end of the iterative loop. the assignment to f occurs 
> >> at the
> >> >bottom of the iterative loop. So, yes, the call to f is performing an
> >> >immediate
> >> >computation, but based on arguments that are coming out of the
> iterative
> >> >loop
> >> >above it, arguments which depend on q<-(p, ACT).  Maybe this is the
> >> problem;
> >> >
> >> >I've got too much going on between my function defenition and it's
> >> >assignment,
> >> >but I don't know how to get around it.
> >> >
> >> >So, I'm not sure if your example will work- the output from the 
> >> iterative
> >> >process is Wtmod, Hgtmod, and I want to minimize the difference between
> >> them
> >> >
> >> >and my observed endpoints (Wt, Hgt).  The numbers I am varying to reach
> >> this
> >> >
> >> >optimization are in the iterative loop (p, ACT), so re-defining these
> >> >outputs
> >> >as x's and getting it to vary these doesn't do me much good unless 
> >> they are
> >> >directly linked to the output of the iterative loop above it.
> >> >
> >> >Last, it's not even that I'm getting error messages anymore- I just 
> >> can't
> >> >get
> >> >the solution that I get from Excel.  If I try to let R find the 
> >> solution,
> >> >and
> >> >give it starting values of c(1,2), it gives me an optimization
> sulution,
> >> but
> >> >an
> >> >extremely poor one.  However, if I give it the answer I got from 
> >> excel, it
> >> >comes right back with the same answer and solutions I get from excel.
> >> >
> >> >Using the 'trace' function, I can see that R gets stuck in a specific
> >> region
> >> >of
> >> >parameter space in looking for the optimization and just appears to
> give
> >> up.
> >> >
> >> >Even when it re-set itself, it keeps going back to this region, and
> thus
> >> >doesn't even try a full range of the parameter space I've defined 
> >> before it
> >> >stops and gives me the wrong answer.
> >> >
> >> >I can try cleaning up the code and see if I can re-submit it, but 
> >> what I am
> >> >trying to program is so parameter heavy that 90% of it is just defining
> >> >these
> >> >at the top of the file.
> >> >
> >> >Thank you for the suggestions,
> >> >
> >> >Mike
> >> >
> >> >
> >> >Quoting Spencer Graves <spencer.graves at PDF.COM>:
> >> >
> >> > > The phrase:
> >> > >
> >> > >     f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
> >> (((Hgt-Hgtmod)^2)/Hgt))2) ;
> >> f
> >> > >
> >> > > is an immediate computation, not a function.  If you want a
> function,
> >> > > try something like the following:
> >> > >
> >> > >     f <- function(x){
> >> > >         Wt <- x[1]
> >> > >         Wtmod <- x[2]
> >> > >         Hgt <- x[3]
> >> > >         Hgtmod <- x[4]
> >> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
> >> > >     }
> >> > >
> >> > > OR
> >> > >
> >> > >     f <- function(x, X){
> >> > >         Wt <- X[,1]
> >> > >         Hgt <- X[,2]
> >> > >         Wtmod <- x[1]
> >> > >         Hgtmod <- x[2]
> >> > >       1000000000*(((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt))2)
> >> > >     }
> >> > >
> >> > > "par" in "optim" is the starting values for "x".  Pass "X" to "f"
> via
> >> > > "..." in the call to "optim".
> >> > >
> >> > >         If you can't make this work, please submit a toy example
> with
> >> the
> >> > > code and error messages.  Please limit your example to 3 
> >> observations,
> >> > > preferably whole numbers so someone else can read your question in
> >> > > seconds.  If it is any longer than that, it should be ignored.
> >> > >
> >> > > hope this helps.
> >> > > Spencer Graves
> >> > >
> >> > > M.Kondrin wrote:
> >> > > >  >?optim
> >> > > >
> >> > > > optim(par, fn, gr = NULL,
> >> > > >            method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B",
> >> "SANN"),
> >> > > >            lower = -Inf, upper = Inf,
> >> > > >            control = list(), hessian = FALSE, ...)
> >> > > >
> >> > > > .....
> >> > > >       fn: A function to be minimized (or maximized), with first
> >> > > >           argument the vector of parameters over which 
> >> minimization is
> >> > > >           to take place. It should return a scalar result.
> >> > > >
> >> > > > Your fn defined as:
> >> > > > f <- 1000000000*(((((Wt-Wtmod)^2)/Wt) + 
> >> (((Hgt-Hgtmod)^2)/Hgt))2) ; f
> >> > > > What is its first argument I wonder?
> >> > > > I think you have just an ill-defined R function (although for 
> >> Excel it
> >> > > > may be OK - do not know) and optim just chokes on it.
> >> > > >
> >> > > > ______________________________________________
> >> > > > R-help at stat.math.ethz.ch mailing list
> >> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> > >
> >> > >
> >> >
> >> >
> >> >--
> >> >Michael Rennie
> >> >M.Sc. Candidate
> >> >University of Toronto at Mississauga
> >> >3359 Mississauga Rd. N.
> >> >Mississauga ON  L5L 1C6
> >> >Ph: 905-828-5452  Fax: 905-828-3792
> >> >
> >> >______________________________________________
> >> >R-help at stat.math.ethz.ch mailing list
> >> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> >
> >>
> >---------------------------------------------------------------------------
> 
> >>
> >> ---
> >> >Notice: This e-mail message, together with any attachments, contains
> >> >information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
> >> >USA) that may be confidential, proprietary copyrighted and/or legally
> >> >privileged, and is intended solely for the use of the individual or 
> >> entity
> >> >named on this message. If you are not the intended recipient, and
> >> >have received this message in error, please immediately return this by
> >> >e-mail and then delete it.
> >>
> >---------------------------------------------------------------------------
> 
> >>
> >> ---
> >>
> >> Michael Rennie
> >> M.Sc. Candidate
> >> University of Toronto at Mississauga
> >> 3359 Mississauga Rd. N.
> >> Mississauga, ON  L5L 1C6
> >> Ph: 905-828-5452  Fax: 905-828-3792
> >>
> >>
> >>
> ----------------------------------------------------------------------------
> -- 
> >>
> >> Notice: This e-mail message, together with any attachments, contains
> >> information of Merck & Co., Inc. (Whitehouse Station, New Jersey,
> >> USA) that may be confidential, proprietary copyrighted and/or legally
> >> privileged, and is intended solely for the use of the individual or 
> >> entity
> >> named on this message. If you are not the intended recipient, and
> >> have received this message in error, please immediately return this by
> >> e-mail and then delete it.
> >>
> ----------------------------------------------------------------------------
> -- 
> >>
> > 
> > 
> > Michael Rennie
> > M.Sc. Candidate
> > University of Toronto at Mississauga
> > 3359 Mississauga Rd. N.
> > Mississauga, ON  L5L 1C6
> > Ph: 905-828-5452  Fax: 905-828-3792
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
> USA) that may be confidential, proprietary copyrighted and/or legally 
> privileged, and is intended solely for the use of the individual or entity
> named on this message. If you are not the intended recipient, and
> have received this message in error, please immediately return this by 
> e-mail and then delete it.
> ------------------------------------------------------------------------------
> 


-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From JSmith at telicmanagement.com  Thu Jul 17 02:06:04 2003
From: JSmith at telicmanagement.com (John Smith)
Date: Wed, 16 Jul 2003 20:06:04 -0400
Subject: [R] duplicate row.names 
Message-ID: <3F62097DBC5C0C439F06B1DD2C393E035F28CE@saturn5.paloma.com>

I am looping over many data files and reading in the data with	 F <-
read.table(filename)	to read in a 22000 by 15 matrix.  Works fine on the
first matrix F, but I get the following error when the second file is read
into F:

Error in "row.names<-.data.frame"(*tmp*, value = row.names) : 
        duplicate row.names are not allowed

I have tried picking a column of the matrix and making that my rownames by
doing		rownames <- as.vector(F[,4])
-but that does not work.

Each row in my matrix is not unique.

Any suggestions greatly appreciated.

John



From mrennie at utm.utoronto.ca  Thu Jul 17 02:51:23 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Wed, 16 Jul 2003 20:51:23 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <3F15D3E2.2090105@pdf.com>
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
	<3F15D3E2.2090105@pdf.com>
Message-ID: <1058403083.3f15f30b8b2b2@webmail.utm.utoronto.ca>

Hmmm.

I tried entering 'Hgtmod = Hgt' at the end of my 'optim' function, but that 
didn't help me any- still getting poor optimizations.  Perhaps this isn't 
working to set a condition, as I was hoping it to.  I think that if I can set 
the condition Hgmod = Hgt, then it should be able to find reasonable solutions 
to this problem set, since that seems to be the trick in my Excel 'solver' 
function.

Also, I'm a little hesitant to simplify this too much in terms of reducing the 
model, becuase I need this thing to work over 365 iterations, as it does in 
excel.  I've at least cleaned up some of the commenting so it should be a bit 
more straightforward.  I tried making the arrays more clear with tabs, but lost 
them upon pasting them into this file.

I've included below the code I am currently using with my temp.dat file 
attached (it's also below so someone can copy and paste it into a text file 
named 'temp.dat')- that's all anyone should need to play around with this if 
they are feeling so inclined by cutting and pasting into R.

Thanks again,

Mike


#Weight at time 0
Wo<- 9.2

#Hg concentration at time 0 (ugHg/g wet weight)
Hgo<- 0.08 

#Weight at time t
Wt<- 32.2

#Hg concentration at time t (ugHg/g wet weight) 
Hgt<- 0.110

#Prey methylmercury concentration (as constant)
Hgp<- 0.033

#Prey caloric value (as constant)
Pc<- 800

#Energy density of fish (as constant, calories)
Ef <- 1000

#Maturity status, 0=immature, 1=mature
Mat<- 0

#Sex, 1=male, 2=female
Sex<- 1

#USER INPUT ABOVE

#Bioenergetics parameters for perch
CA <- 0.25
CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
CQ <- 2.3
CTO <- 23
CTM <- 28
Zc<- (log(CQ))*(CTM-CTO)
Yc<- (log(CQ))*(CTM-CTO+2)
Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400

RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
RB <- 0.8   #same as 1+(-0.2) see above...
RQ <- 2.1
RTO <- 28
RTM <- 33
Za <- (log(RQ))*(RTM-RTO)
Ya<- (log(RQ))*(RTM-RTO+2)
Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400

S <- 0.172

FA <- 0.158
FB <- -0.222
FG <- 0.631

UA<- 0.0253
UB<- 0.58
UG<- -0.299

#Mass balance model parameters
EA <- 0.002938
EB <- -0.2
EQ <- 0.066
a <- 0.8

#Specifying sex-specific parameters

GSI<- NULL

if (Sex==1) GSI<-0.05 else 
if (Sex==2) GSI<-0.17 

#Bring in temp file

temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))

Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ; 

temp<- cbind (Day, jday, Temp)
#Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
#temp [,2]

Vc<-(CTM-(temp[,3]))/(CTM-CTO)
Vr<-(RTM-(temp[,3]))/(RTM-RTO)

comp<- cbind (Day, jday, Temp, Vc, Vr)

#comp

bio<-matrix(NA, ncol=13, nrow=length(Day))
W<-NULL
C<-NULL
ASMR<-NULL
SMR<-NULL
A<-NULL
F<-NULL
U<-NULL
SDA<-NULL
Gr<-NULL
Hg<-NULL
Ed<-NULL
GHg<-NULL
K<-NULL
Expegk<-NULL
EGK<-NULL
p<-NULL
ACT<-NULL


p <- 1 #  0.558626306252032 
ACT <- 2 #  1.66764519286918

q<-c(p,ACT)
 
#introduce function to solve
f <- function (q, Hgtmod)
{

M<- length(Day) #number of days iterated

for (i in 1:M)
{

#Bioenergetics model
if (Day[i]==1) W[i] <- Wo else
if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else 
W[i] <- (W[i-1]+(Gr[i-1]/Ef))

C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc

ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))

SMR[i]<- (ASMR[i]/q[2])

A[i]<- (ASMR[i]-SMR[i])

F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])

U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))

SDA[i]<- (S*(C[i]-F[i]))

Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))

#Trudel MMBM

if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-
Expegk[i-1])+(Hg[i-1]*Expegk[i-1])

Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))

GHg[i] <- Gr[i]/Ef/W[i]

if (Sex==1) K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M 
else
if (Sex==2) K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
# = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g 
# then express as Q times GSI gives K / M gives daily K

EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))

Expegk[i] <- exp(-1*EGK[i])

bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)

}

dimnames (bio) <-list(NULL, c
("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))

bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)

dimnames (bioday) <-list(NULL, c
("jday", "W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK"
, "Hg"))


Wtmod<- bioday [length(W),2]
Wtmod

Hgtmod<- bioday [length(Hg),14]
Hgtmod

q

f <- 1000000000*((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt)) ; f
}

optim(q, f, method = "L-BFGS-B",
	lower = c(0.2, 2), upper=c(2, 3),
Hgtmod = Hgt)

#-----------------------------

Temp.dat:

1       153     9.4
2       154     9.6
3       155     9.8
4       156     10
5       157     10.2
6       158     10.4
7       159     10.6
8       160     10.8
9       161     11
10      162     11.2
11      163     11.4
12      164     11.6
13      165     11.8
14      166     12
15      167     12.3
16      168     12.5
17      169     12.7
18      170     12.9
19      171     13.1
20      172     13.4
21      173     13.6
22      174     13.8
23      175     14
24      176     14.2
25      177     14.5
26      178     14.7
27      179     14.9
28      180     15.1
29      181     15.4
30      182     15.6
31      183     15.8
32      184     16
33      185     16.2
34      186     16.5
35      187     16.7
36      188     16.9
37      189     17.1
38      190     17.3
39      191     17.5
40      192     17.7
41      193     17.9
42      194     18.1
43      195     18.3
44      196     18.5
45      197     18.7
46      198     18.9
47      199     19
48      200     19.2
49      201     19.4
50      202     19.5
51      203     19.7
52      204     19.9
53      205     20
54      206     20.2
55      207     20.3
56      208     20.4
57      209     20.5
58      210     20.7
59      211     20.8
60      212     20.9
61      213     21
62      214     21.1
63      215     21.2
64      216     21.3
65      217     21.3
66      218     21.4
67      219     21.5
68      220     21.5
69      221     21.6
70      222     21.6
71      223     21.6
72      224     21.7
73      225     21.7
74      226     21.7
75      227     21.7
76      228     21.7
77      229     21.7
78      230     21.7
79      231     21.6
80      232     21.6
81      233     21.6
82      234     21.5
83      235     21.5
84      236     21.4
85      237     21.3
86      238     21.3
87      239     21.2
88      240     21.1
89      241     21
90      242     20.9
91      243     20.8
92      244     20.7
93      245     20.5
94      246     20.4
95      247     20.3
96      248     20.2
97      249     20
98      250     19.9
99      251     19.7
100     252     19.5
101     253     19.4
102     254     19.2
103     255     19
104     256     18.9
105     257     18.7
106     258     18.5
107     259     18.3
108     260     18.1
109     261     17.9
110     262     17.7
111     263     17.5
112     264     17.3
113     265     17.1
114     266     16.9
115     267     16.7
116     268     16.5
117     269     16.2
118     270     16
119     271     15.8
120     272     15.6
121     273     15.4
122     274     15.1
123     275     14.9
124     276     14.7
125     277     14.5
126     278     14.2
127     279     14
128     280     13.8
129     281     13.6
130     282     13.4
131     283     13.1
132     284     12.9
133     285     12.7
134     286     12.5
135     287     12.3
136     288     12
137     289     11.8
138     290     11.6
139     291     11.4
140     292     11.2
141     293     11
142     294     10.8
143     295     10.6
144     296     10.4
145     297     10.2
146     298     10
147     299     9.8
148     300     9.6
149     301     9.4
150     302     9.3
151     303     9.1
152     304     8.9
153     305     8.7
154     306     8.6
155     307     8.4
156     308     8.2
157     309     8.1
158     310     7.9
159     311     7.8
160     312     7.6
161     313     7.5
162     314     7.3
163     315     7.2
164     316     7
165     317     6.9
166     318     6.8
167     319     6.7
168     320     6.5
169     321     6.4
170     322     6.3
171     323     6.2
172     324     6.1
173     325     6
174     326     5.8
175     327     5.7
176     328     5.6
177     329     5.5
178     330     5.5
179     331     5.4
180     332     5.3
181     333     5.2
182     334     5.1
183     335     5
184     336     5
185     337     4.9
186     338     4.8
187     339     4.7
188     340     4.7
189     341     4.6
190     342     4.5
191     343     4.5
192     344     4.4
193     345     4.4
194     346     4.3
195     347     4.3
196     348     4.2
197     349     4.2
198     350     4.1
199     351     4.1
200     352     4
201     353     4
202     354     4
203     355     3.9
204     356     3.9
205     357     3.8
206     358     3.8
207     359     3.8
208     360     3.8
209     361     3.7
210     362     3.7
211     363     3.7
212     364     3.6
213     365     3.6
214     366     3.6
215     1       3.2
216     2       3.2
217     3       3.2
218     4       3.2
219     5       3.2
220     6       3.2
221     7       3.2
222     8       3.2
223     9       3.2
224     10      3.2
225     11      3.2
226     12      3.2
227     13      3.2
228     14      3.2
229     15      3.2
230     16      3.2
231     17      3.2
232     18      3.2
233     19      3.2
234     20      3.2
235     21      3.2
236     22      3.2
237     23      3.2
238     24      3.2
239     25      3.2
240     26      3.2
241     27      3.2
242     28      3.2
243     29      3.2
244     30      3.2
245     31      3.2
246     32      3.2
247     33      3.2
248     34      3.2
249     35      3.2
250     36      3.2
251     37      3.2
252     38      3.2
253     39      3.2
254     40      3.2
255     41      3.2
256     42      3.2
257     43      3.2
258     44      3.2
259     45      3.2
260     46      3.2
261     47      3.2
262     48      3.2
263     49      3.2
264     50      3.2
265     51      3.2
266     52      3.2
267     53      3.2
268     54      3.3
269     55      3.3
270     56      3.3
271     57      3.3
272     58      3.3
273     59      3.3
274     60      3.3
275     61      3.3
276     62      3.3
277     63      3.3
278     64      3.3
279     65      3.3
280     66      3.3
281     67      3.3
282     68      3.3
283     69      3.3
284     70      3.3
285     71      3.4
286     72      3.4
287     73      3.4
288     74      3.4
289     75      3.4
290     76      3.4
291     77      3.4
292     78      3.4
293     79      3.5
294     80      3.5
295     81      3.5
296     82      3.5
297     83      3.5
298     84      3.5
299     85      3.6
300     86      3.6
301     87      3.6
302     88      3.6
303     89      3.6
304     90      3.7
305     91      3.7
306     92      3.7
307     93      3.8
308     94      3.8
309     95      3.8
310     96      3.8
311     97      3.9
312     98      3.9
313     99      4
314     100     4
315     101     4
316     102     4.1
317     103     4.1
318     104     4.2
319     105     4.2
320     106     4.3
321     107     4.3
322     108     4.4
323     109     4.4
324     110     4.5
325     111     4.5
326     112     4.6
327     113     4.7
328     114     4.7
329     115     4.8
330     116     4.9
331     117     5
332     118     5
333     119     5.1
334     120     5.2
335     121     5.3
336     122     5.4
337     123     5.5
338     124     5.5
339     125     5.6
340     126     5.7
341     127     5.8
342     128     6
343     129     6.1
344     130     6.2
345     131     6.3
346     132     6.4
347     133     6.5
348     134     6.7
349     135     6.8
350     136     6.9
351     137     7
352     138     7.2
353     139     7.3
354     140     7.5
355     141     7.6
356     142     7.8
357     143     7.9
358     144     8.1
359     145     8.2
360     146     8.4
361     147     8.6
362     148     8.7
363     149     8.9
364     150     9.1
365     151     9.3
366     152     9.3

-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: temp.dat
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030716/1285b704/temp.pl

From spencer.graves at pdf.com  Thu Jul 17 03:48:52 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 18:48:52 -0700
Subject: [R] Excel can do what R can't?????
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
	<3F15D3E2.2090105@pdf.com>
	<1058403083.3f15f30b8b2b2@webmail.utm.utoronto.ca>
Message-ID: <3F160084.7080906@pdf.com>

I don't expect you to have a complete solution from the simplifications. 
  I expect you to learn something from the toy problems that can help 
you solve the real problems.

Michael Rennie wrote:
> Hmmm.
> 
> I tried entering 'Hgtmod = Hgt' at the end of my 'optim' function, but that 
> didn't help me any- still getting poor optimizations.  Perhaps this isn't 
> working to set a condition, as I was hoping it to.  I think that if I can set 
> the condition Hgmod = Hgt, then it should be able to find reasonable solutions 
> to this problem set, since that seems to be the trick in my Excel 'solver' 
> function.
> 
> Also, I'm a little hesitant to simplify this too much in terms of reducing the 
> model, becuase I need this thing to work over 365 iterations, as it does in 
> excel.  I've at least cleaned up some of the commenting so it should be a bit 
> more straightforward.  I tried making the arrays more clear with tabs, but lost 
> them upon pasting them into this file.
> 
> I've included below the code I am currently using with my temp.dat file 
> attached (it's also below so someone can copy and paste it into a text file 
> named 'temp.dat')- that's all anyone should need to play around with this if 
> they are feeling so inclined by cutting and pasting into R.
> 
> Thanks again,
> 
> Mike
> 
> 
> #Weight at time 0
> Wo<- 9.2
> 
> #Hg concentration at time 0 (ugHg/g wet weight)
> Hgo<- 0.08 
> 
> #Weight at time t
> Wt<- 32.2
> 
> #Hg concentration at time t (ugHg/g wet weight) 
> Hgt<- 0.110
> 
> #Prey methylmercury concentration (as constant)
> Hgp<- 0.033
> 
> #Prey caloric value (as constant)
> Pc<- 800
> 
> #Energy density of fish (as constant, calories)
> Ef <- 1000
> 
> #Maturity status, 0=immature, 1=mature
> Mat<- 0
> 
> #Sex, 1=male, 2=female
> Sex<- 1
> 
> #USER INPUT ABOVE
> 
> #Bioenergetics parameters for perch
> CA <- 0.25
> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
> CQ <- 2.3
> CTO <- 23
> CTM <- 28
> Zc<- (log(CQ))*(CTM-CTO)
> Yc<- (log(CQ))*(CTM-CTO+2)
> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
> 
> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
> RB <- 0.8   #same as 1+(-0.2) see above...
> RQ <- 2.1
> RTO <- 28
> RTM <- 33
> Za <- (log(RQ))*(RTM-RTO)
> Ya<- (log(RQ))*(RTM-RTO+2)
> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
> 
> S <- 0.172
> 
> FA <- 0.158
> FB <- -0.222
> FG <- 0.631
> 
> UA<- 0.0253
> UB<- 0.58
> UG<- -0.299
> 
> #Mass balance model parameters
> EA <- 0.002938
> EB <- -0.2
> EQ <- 0.066
> a <- 0.8
> 
> #Specifying sex-specific parameters
> 
> GSI<- NULL
> 
> if (Sex==1) GSI<-0.05 else 
> if (Sex==2) GSI<-0.17 
> 
> #Bring in temp file
> 
> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
> 
> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ; 
> 
> temp<- cbind (Day, jday, Temp)
> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
> #temp [,2]
> 
> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
> 
> comp<- cbind (Day, jday, Temp, Vc, Vr)
> 
> #comp
> 
> bio<-matrix(NA, ncol=13, nrow=length(Day))
> W<-NULL
> C<-NULL
> ASMR<-NULL
> SMR<-NULL
> A<-NULL
> F<-NULL
> U<-NULL
> SDA<-NULL
> Gr<-NULL
> Hg<-NULL
> Ed<-NULL
> GHg<-NULL
> K<-NULL
> Expegk<-NULL
> EGK<-NULL
> p<-NULL
> ACT<-NULL
> 
> 
> p <- 1 #  0.558626306252032 
> ACT <- 2 #  1.66764519286918
> 
> q<-c(p,ACT)
>  
> #introduce function to solve
> f <- function (q, Hgtmod)
> {
> 
> M<- length(Day) #number of days iterated
> 
> for (i in 1:M)
> {
> 
> #Bioenergetics model
> if (Day[i]==1) W[i] <- Wo else
> if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else 
> W[i] <- (W[i-1]+(Gr[i-1]/Ef))
> 
> C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
> 
> ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
> 
> SMR[i]<- (ASMR[i]/q[2])
> 
> A[i]<- (ASMR[i]-SMR[i])
> 
> F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
> 
> U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
> 
> SDA[i]<- (S*(C[i]-F[i]))
> 
> Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
> 
> #Trudel MMBM
> 
> if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-
> Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
> 
> Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
> 
> GHg[i] <- Gr[i]/Ef/W[i]
> 
> if (Sex==1) K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M 
> else
> if (Sex==2) K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
> # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to ug/g 
> # then express as Q times GSI gives K / M gives daily K
> 
> EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
> 
> Expegk[i] <- exp(-1*EGK[i])
> 
> bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> 
> }
> 
> dimnames (bio) <-list(NULL, c
> ("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK", "Hg"))
> 
> bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> 
> dimnames (bioday) <-list(NULL, c
> ("jday", "W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", "EGK"
> , "Hg"))
> 
> 
> Wtmod<- bioday [length(W),2]
> Wtmod
> 
> Hgtmod<- bioday [length(Hg),14]
> Hgtmod
> 
> q
> 
> f <- 1000000000*((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt)) ; f
> }
> 
> optim(q, f, method = "L-BFGS-B",
> 	lower = c(0.2, 2), upper=c(2, 3),
> Hgtmod = Hgt)
> 
> #-----------------------------
> 
> Temp.dat:
> 
> 1       153     9.4
> 2       154     9.6
> 3       155     9.8
> 4       156     10
> 5       157     10.2
> 6       158     10.4
> 7       159     10.6
> 8       160     10.8
> 9       161     11
> 10      162     11.2
> 11      163     11.4
> 12      164     11.6
> 13      165     11.8
> 14      166     12
> 15      167     12.3
> 16      168     12.5
> 17      169     12.7
> 18      170     12.9
> 19      171     13.1
> 20      172     13.4
> 21      173     13.6
> 22      174     13.8
> 23      175     14
> 24      176     14.2
> 25      177     14.5
> 26      178     14.7
> 27      179     14.9
> 28      180     15.1
> 29      181     15.4
> 30      182     15.6
> 31      183     15.8
> 32      184     16
> 33      185     16.2
> 34      186     16.5
> 35      187     16.7
> 36      188     16.9
> 37      189     17.1
> 38      190     17.3
> 39      191     17.5
> 40      192     17.7
> 41      193     17.9
> 42      194     18.1
> 43      195     18.3
> 44      196     18.5
> 45      197     18.7
> 46      198     18.9
> 47      199     19
> 48      200     19.2
> 49      201     19.4
> 50      202     19.5
> 51      203     19.7
> 52      204     19.9
> 53      205     20
> 54      206     20.2
> 55      207     20.3
> 56      208     20.4
> 57      209     20.5
> 58      210     20.7
> 59      211     20.8
> 60      212     20.9
> 61      213     21
> 62      214     21.1
> 63      215     21.2
> 64      216     21.3
> 65      217     21.3
> 66      218     21.4
> 67      219     21.5
> 68      220     21.5
> 69      221     21.6
> 70      222     21.6
> 71      223     21.6
> 72      224     21.7
> 73      225     21.7
> 74      226     21.7
> 75      227     21.7
> 76      228     21.7
> 77      229     21.7
> 78      230     21.7
> 79      231     21.6
> 80      232     21.6
> 81      233     21.6
> 82      234     21.5
> 83      235     21.5
> 84      236     21.4
> 85      237     21.3
> 86      238     21.3
> 87      239     21.2
> 88      240     21.1
> 89      241     21
> 90      242     20.9
> 91      243     20.8
> 92      244     20.7
> 93      245     20.5
> 94      246     20.4
> 95      247     20.3
> 96      248     20.2
> 97      249     20
> 98      250     19.9
> 99      251     19.7
> 100     252     19.5
> 101     253     19.4
> 102     254     19.2
> 103     255     19
> 104     256     18.9
> 105     257     18.7
> 106     258     18.5
> 107     259     18.3
> 108     260     18.1
> 109     261     17.9
> 110     262     17.7
> 111     263     17.5
> 112     264     17.3
> 113     265     17.1
> 114     266     16.9
> 115     267     16.7
> 116     268     16.5
> 117     269     16.2
> 118     270     16
> 119     271     15.8
> 120     272     15.6
> 121     273     15.4
> 122     274     15.1
> 123     275     14.9
> 124     276     14.7
> 125     277     14.5
> 126     278     14.2
> 127     279     14
> 128     280     13.8
> 129     281     13.6
> 130     282     13.4
> 131     283     13.1
> 132     284     12.9
> 133     285     12.7
> 134     286     12.5
> 135     287     12.3
> 136     288     12
> 137     289     11.8
> 138     290     11.6
> 139     291     11.4
> 140     292     11.2
> 141     293     11
> 142     294     10.8
> 143     295     10.6
> 144     296     10.4
> 145     297     10.2
> 146     298     10
> 147     299     9.8
> 148     300     9.6
> 149     301     9.4
> 150     302     9.3
> 151     303     9.1
> 152     304     8.9
> 153     305     8.7
> 154     306     8.6
> 155     307     8.4
> 156     308     8.2
> 157     309     8.1
> 158     310     7.9
> 159     311     7.8
> 160     312     7.6
> 161     313     7.5
> 162     314     7.3
> 163     315     7.2
> 164     316     7
> 165     317     6.9
> 166     318     6.8
> 167     319     6.7
> 168     320     6.5
> 169     321     6.4
> 170     322     6.3
> 171     323     6.2
> 172     324     6.1
> 173     325     6
> 174     326     5.8
> 175     327     5.7
> 176     328     5.6
> 177     329     5.5
> 178     330     5.5
> 179     331     5.4
> 180     332     5.3
> 181     333     5.2
> 182     334     5.1
> 183     335     5
> 184     336     5
> 185     337     4.9
> 186     338     4.8
> 187     339     4.7
> 188     340     4.7
> 189     341     4.6
> 190     342     4.5
> 191     343     4.5
> 192     344     4.4
> 193     345     4.4
> 194     346     4.3
> 195     347     4.3
> 196     348     4.2
> 197     349     4.2
> 198     350     4.1
> 199     351     4.1
> 200     352     4
> 201     353     4
> 202     354     4
> 203     355     3.9
> 204     356     3.9
> 205     357     3.8
> 206     358     3.8
> 207     359     3.8
> 208     360     3.8
> 209     361     3.7
> 210     362     3.7
> 211     363     3.7
> 212     364     3.6
> 213     365     3.6
> 214     366     3.6
> 215     1       3.2
> 216     2       3.2
> 217     3       3.2
> 218     4       3.2
> 219     5       3.2
> 220     6       3.2
> 221     7       3.2
> 222     8       3.2
> 223     9       3.2
> 224     10      3.2
> 225     11      3.2
> 226     12      3.2
> 227     13      3.2
> 228     14      3.2
> 229     15      3.2
> 230     16      3.2
> 231     17      3.2
> 232     18      3.2
> 233     19      3.2
> 234     20      3.2
> 235     21      3.2
> 236     22      3.2
> 237     23      3.2
> 238     24      3.2
> 239     25      3.2
> 240     26      3.2
> 241     27      3.2
> 242     28      3.2
> 243     29      3.2
> 244     30      3.2
> 245     31      3.2
> 246     32      3.2
> 247     33      3.2
> 248     34      3.2
> 249     35      3.2
> 250     36      3.2
> 251     37      3.2
> 252     38      3.2
> 253     39      3.2
> 254     40      3.2
> 255     41      3.2
> 256     42      3.2
> 257     43      3.2
> 258     44      3.2
> 259     45      3.2
> 260     46      3.2
> 261     47      3.2
> 262     48      3.2
> 263     49      3.2
> 264     50      3.2
> 265     51      3.2
> 266     52      3.2
> 267     53      3.2
> 268     54      3.3
> 269     55      3.3
> 270     56      3.3
> 271     57      3.3
> 272     58      3.3
> 273     59      3.3
> 274     60      3.3
> 275     61      3.3
> 276     62      3.3
> 277     63      3.3
> 278     64      3.3
> 279     65      3.3
> 280     66      3.3
> 281     67      3.3
> 282     68      3.3
> 283     69      3.3
> 284     70      3.3
> 285     71      3.4
> 286     72      3.4
> 287     73      3.4
> 288     74      3.4
> 289     75      3.4
> 290     76      3.4
> 291     77      3.4
> 292     78      3.4
> 293     79      3.5
> 294     80      3.5
> 295     81      3.5
> 296     82      3.5
> 297     83      3.5
> 298     84      3.5
> 299     85      3.6
> 300     86      3.6
> 301     87      3.6
> 302     88      3.6
> 303     89      3.6
> 304     90      3.7
> 305     91      3.7
> 306     92      3.7
> 307     93      3.8
> 308     94      3.8
> 309     95      3.8
> 310     96      3.8
> 311     97      3.9
> 312     98      3.9
> 313     99      4
> 314     100     4
> 315     101     4
> 316     102     4.1
> 317     103     4.1
> 318     104     4.2
> 319     105     4.2
> 320     106     4.3
> 321     107     4.3
> 322     108     4.4
> 323     109     4.4
> 324     110     4.5
> 325     111     4.5
> 326     112     4.6
> 327     113     4.7
> 328     114     4.7
> 329     115     4.8
> 330     116     4.9
> 331     117     5
> 332     118     5
> 333     119     5.1
> 334     120     5.2
> 335     121     5.3
> 336     122     5.4
> 337     123     5.5
> 338     124     5.5
> 339     125     5.6
> 340     126     5.7
> 341     127     5.8
> 342     128     6
> 343     129     6.1
> 344     130     6.2
> 345     131     6.3
> 346     132     6.4
> 347     133     6.5
> 348     134     6.7
> 349     135     6.8
> 350     136     6.9
> 351     137     7
> 352     138     7.2
> 353     139     7.3
> 354     140     7.5
> 355     141     7.6
> 356     142     7.8
> 357     143     7.9
> 358     144     8.1
> 359     145     8.2
> 360     146     8.4
> 361     147     8.6
> 362     148     8.7
> 363     149     8.9
> 364     150     9.1
> 365     151     9.3
> 366     152     9.3
> 
> 
> 
> ------------------------------------------------------------------------
> 
> 1	153	9.4
> 2	154	9.6
> 3	155	9.8
> 4	156	10
> 5	157	10.2
> 6	158	10.4
> 7	159	10.6
> 8	160	10.8
> 9	161	11
> 10	162	11.2
> 11	163	11.4
> 12	164	11.6
> 13	165	11.8
> 14	166	12
> 15	167	12.3
> 16	168	12.5
> 17	169	12.7
> 18	170	12.9
> 19	171	13.1
> 20	172	13.4
> 21	173	13.6
> 22	174	13.8
> 23	175	14
> 24	176	14.2
> 25	177	14.5
> 26	178	14.7
> 27	179	14.9
> 28	180	15.1
> 29	181	15.4
> 30	182	15.6
> 31	183	15.8
> 32	184	16
> 33	185	16.2
> 34	186	16.5
> 35	187	16.7
> 36	188	16.9
> 37	189	17.1
> 38	190	17.3
> 39	191	17.5
> 40	192	17.7
> 41	193	17.9
> 42	194	18.1
> 43	195	18.3
> 44	196	18.5
> 45	197	18.7
> 46	198	18.9
> 47	199	19
> 48	200	19.2
> 49	201	19.4
> 50	202	19.5
> 51	203	19.7
> 52	204	19.9
> 53	205	20
> 54	206	20.2
> 55	207	20.3
> 56	208	20.4
> 57	209	20.5
> 58	210	20.7
> 59	211	20.8
> 60	212	20.9
> 61	213	21
> 62	214	21.1
> 63	215	21.2
> 64	216	21.3
> 65	217	21.3
> 66	218	21.4
> 67	219	21.5
> 68	220	21.5
> 69	221	21.6
> 70	222	21.6
> 71	223	21.6
> 72	224	21.7
> 73	225	21.7
> 74	226	21.7
> 75	227	21.7
> 76	228	21.7
> 77	229	21.7
> 78	230	21.7
> 79	231	21.6
> 80	232	21.6
> 81	233	21.6
> 82	234	21.5
> 83	235	21.5
> 84	236	21.4
> 85	237	21.3
> 86	238	21.3
> 87	239	21.2
> 88	240	21.1
> 89	241	21
> 90	242	20.9
> 91	243	20.8
> 92	244	20.7
> 93	245	20.5
> 94	246	20.4
> 95	247	20.3
> 96	248	20.2
> 97	249	20
> 98	250	19.9
> 99	251	19.7
> 100	252	19.5
> 101	253	19.4
> 102	254	19.2
> 103	255	19
> 104	256	18.9
> 105	257	18.7
> 106	258	18.5
> 107	259	18.3
> 108	260	18.1
> 109	261	17.9
> 110	262	17.7
> 111	263	17.5
> 112	264	17.3
> 113	265	17.1
> 114	266	16.9
> 115	267	16.7
> 116	268	16.5
> 117	269	16.2
> 118	270	16
> 119	271	15.8
> 120	272	15.6
> 121	273	15.4
> 122	274	15.1
> 123	275	14.9
> 124	276	14.7
> 125	277	14.5
> 126	278	14.2
> 127	279	14
> 128	280	13.8
> 129	281	13.6
> 130	282	13.4
> 131	283	13.1
> 132	284	12.9
> 133	285	12.7
> 134	286	12.5
> 135	287	12.3
> 136	288	12
> 137	289	11.8
> 138	290	11.6
> 139	291	11.4
> 140	292	11.2
> 141	293	11
> 142	294	10.8
> 143	295	10.6
> 144	296	10.4
> 145	297	10.2
> 146	298	10
> 147	299	9.8
> 148	300	9.6
> 149	301	9.4
> 150	302	9.3
> 151	303	9.1
> 152	304	8.9
> 153	305	8.7
> 154	306	8.6
> 155	307	8.4
> 156	308	8.2
> 157	309	8.1
> 158	310	7.9
> 159	311	7.8
> 160	312	7.6
> 161	313	7.5
> 162	314	7.3
> 163	315	7.2
> 164	316	7
> 165	317	6.9
> 166	318	6.8
> 167	319	6.7
> 168	320	6.5
> 169	321	6.4
> 170	322	6.3
> 171	323	6.2
> 172	324	6.1
> 173	325	6
> 174	326	5.8
> 175	327	5.7
> 176	328	5.6
> 177	329	5.5
> 178	330	5.5
> 179	331	5.4
> 180	332	5.3
> 181	333	5.2
> 182	334	5.1
> 183	335	5
> 184	336	5
> 185	337	4.9
> 186	338	4.8
> 187	339	4.7
> 188	340	4.7
> 189	341	4.6
> 190	342	4.5
> 191	343	4.5
> 192	344	4.4
> 193	345	4.4
> 194	346	4.3
> 195	347	4.3
> 196	348	4.2
> 197	349	4.2
> 198	350	4.1
> 199	351	4.1
> 200	352	4
> 201	353	4
> 202	354	4
> 203	355	3.9
> 204	356	3.9
> 205	357	3.8
> 206	358	3.8
> 207	359	3.8
> 208	360	3.8
> 209	361	3.7
> 210	362	3.7
> 211	363	3.7
> 212	364	3.6
> 213	365	3.6
> 214	366	3.6
> 215	1	3.2
> 216	2	3.2
> 217	3	3.2
> 218	4	3.2
> 219	5	3.2
> 220	6	3.2
> 221	7	3.2
> 222	8	3.2
> 223	9	3.2
> 224	10	3.2
> 225	11	3.2
> 226	12	3.2
> 227	13	3.2
> 228	14	3.2
> 229	15	3.2
> 230	16	3.2
> 231	17	3.2
> 232	18	3.2
> 233	19	3.2
> 234	20	3.2
> 235	21	3.2
> 236	22	3.2
> 237	23	3.2
> 238	24	3.2
> 239	25	3.2
> 240	26	3.2
> 241	27	3.2
> 242	28	3.2
> 243	29	3.2
> 244	30	3.2
> 245	31	3.2
> 246	32	3.2
> 247	33	3.2
> 248	34	3.2
> 249	35	3.2
> 250	36	3.2
> 251	37	3.2
> 252	38	3.2
> 253	39	3.2
> 254	40	3.2
> 255	41	3.2
> 256	42	3.2
> 257	43	3.2
> 258	44	3.2
> 259	45	3.2
> 260	46	3.2
> 261	47	3.2
> 262	48	3.2
> 263	49	3.2
> 264	50	3.2
> 265	51	3.2
> 266	52	3.2
> 267	53	3.2
> 268	54	3.3
> 269	55	3.3
> 270	56	3.3
> 271	57	3.3
> 272	58	3.3
> 273	59	3.3
> 274	60	3.3
> 275	61	3.3
> 276	62	3.3
> 277	63	3.3
> 278	64	3.3
> 279	65	3.3
> 280	66	3.3
> 281	67	3.3
> 282	68	3.3
> 283	69	3.3
> 284	70	3.3
> 285	71	3.4
> 286	72	3.4
> 287	73	3.4
> 288	74	3.4
> 289	75	3.4
> 290	76	3.4
> 291	77	3.4
> 292	78	3.4
> 293	79	3.5
> 294	80	3.5
> 295	81	3.5
> 296	82	3.5
> 297	83	3.5
> 298	84	3.5
> 299	85	3.6
> 300	86	3.6
> 301	87	3.6
> 302	88	3.6
> 303	89	3.6
> 304	90	3.7
> 305	91	3.7
> 306	92	3.7
> 307	93	3.8
> 308	94	3.8
> 309	95	3.8
> 310	96	3.8
> 311	97	3.9
> 312	98	3.9
> 313	99	4
> 314	100	4
> 315	101	4
> 316	102	4.1
> 317	103	4.1
> 318	104	4.2
> 319	105	4.2
> 320	106	4.3
> 321	107	4.3
> 322	108	4.4
> 323	109	4.4
> 324	110	4.5
> 325	111	4.5
> 326	112	4.6
> 327	113	4.7
> 328	114	4.7
> 329	115	4.8
> 330	116	4.9
> 331	117	5
> 332	118	5
> 333	119	5.1
> 334	120	5.2
> 335	121	5.3
> 336	122	5.4
> 337	123	5.5
> 338	124	5.5
> 339	125	5.6
> 340	126	5.7
> 341	127	5.8
> 342	128	6
> 343	129	6.1
> 344	130	6.2
> 345	131	6.3
> 346	132	6.4
> 347	133	6.5
> 348	134	6.7
> 349	135	6.8
> 350	136	6.9
> 351	137	7
> 352	138	7.2
> 353	139	7.3
> 354	140	7.5
> 355	141	7.6
> 356	142	7.8
> 357	143	7.9
> 358	144	8.1
> 359	145	8.2
> 360	146	8.4
> 361	147	8.6
> 362	148	8.7
> 363	149	8.9
> 364	150	9.1
> 365	151	9.3
> 366	152	9.3



From Simon.Blomberg at anu.edu.au  Thu Jul 17 03:50:16 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 17 Jul 2003 11:50:16 +1000
Subject: [R] duplicate row.names 
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133DA@CASEVS02.cas.anu.edu.au>

I think you need to be a bit more specific for us to be able to help you. At least provide a toy problem which replicates the error. Does this do what you want?

dat1 <- data.frame (x=rnorm(100), y=rnorm(100)) #make up some data
dat2 <- data.frame (x=rnorm(100), y=rnorm(100))
write.table(dat1, file="test1.txt") #write data to files
write.table(dat2, file="test2.txt")
F<- NULL # Initialise
for (i in 1:2) F <- rbind(F, read.table(paste("test", i, ".txt", sep="")))
F <- data.frame(F, row.names=NULL) # clean up row names.

Hope this helps,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: John Smith [mailto:JSmith at telicmanagement.com]
> Sent: Thursday, 17 July 2003 10:06 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] duplicate row.names 
> 
> 
> I am looping over many data files and reading in the data 
> with	 F <-
> read.table(filename)	to read in a 22000 by 15 matrix.  Works 
> fine on the
> first matrix F, but I get the following error when the second 
> file is read
> into F:
> 
> Error in "row.names<-.data.frame"(*tmp*, value = row.names) : 
>         duplicate row.names are not allowed
> 
> I have tried picking a column of the matrix and making that 
> my rownames by
> doing		rownames <- as.vector(F[,4])
> -but that does not work.
> 
> Each row in my matrix is not unique.
> 
> Any suggestions greatly appreciated.
> 
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From gisar at nus.edu.sg  Thu Jul 17 05:31:29 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Thu, 17 Jul 2003 11:31:29 +0800
Subject: [R] duplicate row.names 
Message-ID: <CDA8D2689259E444942B3CDED8DD912932D4E9@MBXSRV03.stf.nus.edu.sg>

The row.names argument (if present) defines which column is to be used
as the row names. The default in read.table() is missing row.names and
hence row.names is not detected. The row names (if present) need to be
unique.

Giving us your code would have been helpful, but I am guessing you have
set the row.names to some number and hence it would be complain if it
not unique. Try :

1. Setting row.names=FALSE 
2. Assigning a unique identifier to each row
2. Manually rename the redundant rows

Your problem is with reading the data itself, and ` rownames <-
as.vector(F[,4]) ` just creates another unrelated variable.

Regards, Adai.


-----Original Message-----
From: John Smith [mailto:JSmith at telicmanagement.com] 
Sent: Thursday, July 17, 2003 8:06 AM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] duplicate row.names 


I am looping over many data files and reading in the data with	 F <-
read.table(filename)	to read in a 22000 by 15 matrix.  Works fine on
the
first matrix F, but I get the following error when the second file is
read into F:

Error in "row.names<-.data.frame"(*tmp*, value = row.names) : 
        duplicate row.names are not allowed

I have tried picking a column of the matrix and making that my rownames
by
doing		rownames <- as.vector(F[,4])
-but that does not work.

Each row in my matrix is not unique.

Any suggestions greatly appreciated.

John

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Jul 17 05:38:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jul 2003 20:38:48 -0700
Subject: [R] Excel can do what R can't?????
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>	<3F15D3E2.2090105@pdf.com>	<1058403083.3f15f30b8b2b2@webmail.utm.utoronto.ca>
	<3F160084.7080906@pdf.com>
Message-ID: <3F161A48.8000605@pdf.com>

Are you familiar with "How to Solve It" by George Polya?  Among other 
things, Polya discusses the value of solving hypersimplified versions of 
a problem, because the solutions to the simple problems may help you see 
how you can solve the real problem.  Am I correct that prior to the 
completion of the transcontinental railroad, the quickest path from 
Toronto to Vancouver was by way of Tierra del Fuego?

spencer graves

Spencer Graves wrote:
> I don't expect you to have a complete solution from the simplifications. 
>  I expect you to learn something from the toy problems that can help you 
> solve the real problems.
> 
> Michael Rennie wrote:
> 
>> Hmmm.
>>
>> I tried entering 'Hgtmod = Hgt' at the end of my 'optim' function, but 
>> that didn't help me any- still getting poor optimizations.  Perhaps 
>> this isn't working to set a condition, as I was hoping it to.  I think 
>> that if I can set the condition Hgmod = Hgt, then it should be able to 
>> find reasonable solutions to this problem set, since that seems to be 
>> the trick in my Excel 'solver' function.
>>
>> Also, I'm a little hesitant to simplify this too much in terms of 
>> reducing the model, becuase I need this thing to work over 365 
>> iterations, as it does in excel.  I've at least cleaned up some of the 
>> commenting so it should be a bit more straightforward.  I tried making 
>> the arrays more clear with tabs, but lost them upon pasting them into 
>> this file.
>>
>> I've included below the code I am currently using with my temp.dat 
>> file attached (it's also below so someone can copy and paste it into a 
>> text file named 'temp.dat')- that's all anyone should need to play 
>> around with this if they are feeling so inclined by cutting and 
>> pasting into R.
>>
>> Thanks again,
>>
>> Mike
>>
>>
>> #Weight at time 0
>> Wo<- 9.2
>>
>> #Hg concentration at time 0 (ugHg/g wet weight)
>> Hgo<- 0.08
>> #Weight at time t
>> Wt<- 32.2
>>
>> #Hg concentration at time t (ugHg/g wet weight) Hgt<- 0.110
>>
>> #Prey methylmercury concentration (as constant)
>> Hgp<- 0.033
>>
>> #Prey caloric value (as constant)
>> Pc<- 800
>>
>> #Energy density of fish (as constant, calories)
>> Ef <- 1000
>>
>> #Maturity status, 0=immature, 1=mature
>> Mat<- 0
>>
>> #Sex, 1=male, 2=female
>> Sex<- 1
>>
>> #USER INPUT ABOVE
>>
>> #Bioenergetics parameters for perch
>> CA <- 0.25
>> CB <- 0.73  #same as 1+(-0.27)- convert g/g/d to g/d * Pc to get cal/d
>> CQ <- 2.3
>> CTO <- 23
>> CTM <- 28
>> Zc<- (log(CQ))*(CTM-CTO)
>> Yc<- (log(CQ))*(CTM-CTO+2)
>> Xc<- ((Zc^2)*(1+(1+40/Yc)^0.5)^2)/400
>>
>> RA <- 34.992  #0.0108*3240 cal/g 02, converting weight of 02 to cal
>> RB <- 0.8   #same as 1+(-0.2) see above...
>> RQ <- 2.1
>> RTO <- 28
>> RTM <- 33
>> Za <- (log(RQ))*(RTM-RTO)
>> Ya<- (log(RQ))*(RTM-RTO+2)
>> Xa<- ((Za^2)*(1+(1+40/Ya)^0.5)^2)/400
>>
>> S <- 0.172
>>
>> FA <- 0.158
>> FB <- -0.222
>> FG <- 0.631
>>
>> UA<- 0.0253
>> UB<- 0.58
>> UG<- -0.299
>>
>> #Mass balance model parameters
>> EA <- 0.002938
>> EB <- -0.2
>> EQ <- 0.066
>> a <- 0.8
>>
>> #Specifying sex-specific parameters
>>
>> GSI<- NULL
>>
>> if (Sex==1) GSI<-0.05 else if (Sex==2) GSI<-0.17
>> #Bring in temp file
>>
>> temper <- scan("temp.dat", na.strings = ".", list(Day=0, jday=0, Temp=0))
>>
>> Day<-temper$Day ; jday<-temper$jday ; Temp<-temper$Temp ;
>> temp<- cbind (Day, jday, Temp)
>> #Day = number of days modelled, jday=julian day, Temp = daily avg. temp.
>> #temp [,2]
>>
>> Vc<-(CTM-(temp[,3]))/(CTM-CTO)
>> Vr<-(RTM-(temp[,3]))/(RTM-RTO)
>>
>> comp<- cbind (Day, jday, Temp, Vc, Vr)
>>
>> #comp
>>
>> bio<-matrix(NA, ncol=13, nrow=length(Day))
>> W<-NULL
>> C<-NULL
>> ASMR<-NULL
>> SMR<-NULL
>> A<-NULL
>> F<-NULL
>> U<-NULL
>> SDA<-NULL
>> Gr<-NULL
>> Hg<-NULL
>> Ed<-NULL
>> GHg<-NULL
>> K<-NULL
>> Expegk<-NULL
>> EGK<-NULL
>> p<-NULL
>> ACT<-NULL
>>
>>
>> p <- 1 #  0.558626306252032 ACT <- 2 #  1.66764519286918
>>
>> q<-c(p,ACT)
>>  
>> #introduce function to solve
>> f <- function (q, Hgtmod)
>> {
>>
>> M<- length(Day) #number of days iterated
>>
>> for (i in 1:M)
>> {
>>
>> #Bioenergetics model
>> if (Day[i]==1) W[i] <- Wo else
>> if (jday[i]==121 && Mat==1) W[i] <- (W[i-1]-(W[i-1]*GSI*1.2)) else 
>> W[i] <- (W[i-1]+(Gr[i-1]/Ef))
>>
>> C[i]<- q[1]*CA*(W[i]^CB)*((comp[i,4])^Xc)*(exp(Xc*(1-(comp[i,4]))))*Pc
>>
>> ASMR[i]<- q[2]*RA*(W[i]^RB)*((comp[i,5])^Xa)*(exp(Xa*(1-(comp[i,5]))))
>>
>> SMR[i]<- (ASMR[i]/q[2])
>>
>> A[i]<- (ASMR[i]-SMR[i])
>>
>> F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
>>
>> U[i]<- (UA*((comp[i,3])^UB)*(exp(UG*p))*(C[i]-F[i]))
>>
>> SDA[i]<- (S*(C[i]-F[i]))
>>
>> Gr[i]<- (C[i]-(ASMR[i]+F[i]+U[i]+SDA[i]))
>>
>> #Trudel MMBM
>>
>> if (Day[i]==1) Hg[i] <- Hgo else Hg[i] <- 
>> a*Hgp*(C[i-1]/Pc/W[i-1])/EGK[i-1]*(1-
>> Expegk[i-1])+(Hg[i-1]*Expegk[i-1])
>>
>> Ed[i]<- EA*(W[i]^EB)*(exp(EQ*(comp[i,3])))
>>
>> GHg[i] <- Gr[i]/Ef/W[i]
>>
>> if (Sex==1) 
>> K[i]<-(((0.1681*(10^(1.3324+(0.000453*Hg[i])))/1000)/Hg[i])*GSI)/M else
>> if (Sex==2) 
>> K[i]<-(((0.1500*(10^(0.8840+(0.000903*Hg[i])))/1000)/Hg[i])*GSI)/M
>> # = dw/ww conversion * gonad ~ body conc'n function(ng/g) / convert to 
>> ug/g # then express as Q times GSI gives K / M gives daily K
>>
>> EGK[i] <- (Ed[i] + GHg[i] + (K[i]*Mat))
>>
>> Expegk[i] <- exp(-1*EGK[i])
>>
>> bio<- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>>
>> }
>>
>> dimnames (bio) <-list(NULL, c
>> ("W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", "GHg", 
>> "EGK", "Hg"))
>>
>> bioday<-cbind(jday, W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
>>
>> dimnames (bioday) <-list(NULL, c
>> ("jday", "W", "C", "ASMR", "SMR", "A", "F", "U", "SDA", "Gr", "Ed", 
>> "GHg", "EGK"
>> , "Hg"))
>>
>>
>> Wtmod<- bioday [length(W),2]
>> Wtmod
>>
>> Hgtmod<- bioday [length(Hg),14]
>> Hgtmod
>>
>> q
>>
>> f <- 1000000000*((((Wt-Wtmod)^2)/Wt) + (((Hgt-Hgtmod)^2)/Hgt)) ; f
>> }
>>
>> optim(q, f, method = "L-BFGS-B",
>>     lower = c(0.2, 2), upper=c(2, 3),
>> Hgtmod = Hgt)
>>
>> #-----------------------------
>>
>> Temp.dat:
>>
>> 1       153     9.4
>> 2       154     9.6
>> 3       155     9.8
>> 4       156     10
>> 5       157     10.2
>> 6       158     10.4
>> 7       159     10.6
>> 8       160     10.8
>> 9       161     11
>> 10      162     11.2
>> 11      163     11.4
>> 12      164     11.6
>> 13      165     11.8
>> 14      166     12
>> 15      167     12.3
>> 16      168     12.5
>> 17      169     12.7
>> 18      170     12.9
>> 19      171     13.1
>> 20      172     13.4
>> 21      173     13.6
>> 22      174     13.8
>> 23      175     14
>> 24      176     14.2
>> 25      177     14.5
>> 26      178     14.7
>> 27      179     14.9
>> 28      180     15.1
>> 29      181     15.4
>> 30      182     15.6
>> 31      183     15.8
>> 32      184     16
>> 33      185     16.2
>> 34      186     16.5
>> 35      187     16.7
>> 36      188     16.9
>> 37      189     17.1
>> 38      190     17.3
>> 39      191     17.5
>> 40      192     17.7
>> 41      193     17.9
>> 42      194     18.1
>> 43      195     18.3
>> 44      196     18.5
>> 45      197     18.7
>> 46      198     18.9
>> 47      199     19
>> 48      200     19.2
>> 49      201     19.4
>> 50      202     19.5
>> 51      203     19.7
>> 52      204     19.9
>> 53      205     20
>> 54      206     20.2
>> 55      207     20.3
>> 56      208     20.4
>> 57      209     20.5
>> 58      210     20.7
>> 59      211     20.8
>> 60      212     20.9
>> 61      213     21
>> 62      214     21.1
>> 63      215     21.2
>> 64      216     21.3
>> 65      217     21.3
>> 66      218     21.4
>> 67      219     21.5
>> 68      220     21.5
>> 69      221     21.6
>> 70      222     21.6
>> 71      223     21.6
>> 72      224     21.7
>> 73      225     21.7
>> 74      226     21.7
>> 75      227     21.7
>> 76      228     21.7
>> 77      229     21.7
>> 78      230     21.7
>> 79      231     21.6
>> 80      232     21.6
>> 81      233     21.6
>> 82      234     21.5
>> 83      235     21.5
>> 84      236     21.4
>> 85      237     21.3
>> 86      238     21.3
>> 87      239     21.2
>> 88      240     21.1
>> 89      241     21
>> 90      242     20.9
>> 91      243     20.8
>> 92      244     20.7
>> 93      245     20.5
>> 94      246     20.4
>> 95      247     20.3
>> 96      248     20.2
>> 97      249     20
>> 98      250     19.9
>> 99      251     19.7
>> 100     252     19.5
>> 101     253     19.4
>> 102     254     19.2
>> 103     255     19
>> 104     256     18.9
>> 105     257     18.7
>> 106     258     18.5
>> 107     259     18.3
>> 108     260     18.1
>> 109     261     17.9
>> 110     262     17.7
>> 111     263     17.5
>> 112     264     17.3
>> 113     265     17.1
>> 114     266     16.9
>> 115     267     16.7
>> 116     268     16.5
>> 117     269     16.2
>> 118     270     16
>> 119     271     15.8
>> 120     272     15.6
>> 121     273     15.4
>> 122     274     15.1
>> 123     275     14.9
>> 124     276     14.7
>> 125     277     14.5
>> 126     278     14.2
>> 127     279     14
>> 128     280     13.8
>> 129     281     13.6
>> 130     282     13.4
>> 131     283     13.1
>> 132     284     12.9
>> 133     285     12.7
>> 134     286     12.5
>> 135     287     12.3
>> 136     288     12
>> 137     289     11.8
>> 138     290     11.6
>> 139     291     11.4
>> 140     292     11.2
>> 141     293     11
>> 142     294     10.8
>> 143     295     10.6
>> 144     296     10.4
>> 145     297     10.2
>> 146     298     10
>> 147     299     9.8
>> 148     300     9.6
>> 149     301     9.4
>> 150     302     9.3
>> 151     303     9.1
>> 152     304     8.9
>> 153     305     8.7
>> 154     306     8.6
>> 155     307     8.4
>> 156     308     8.2
>> 157     309     8.1
>> 158     310     7.9
>> 159     311     7.8
>> 160     312     7.6
>> 161     313     7.5
>> 162     314     7.3
>> 163     315     7.2
>> 164     316     7
>> 165     317     6.9
>> 166     318     6.8
>> 167     319     6.7
>> 168     320     6.5
>> 169     321     6.4
>> 170     322     6.3
>> 171     323     6.2
>> 172     324     6.1
>> 173     325     6
>> 174     326     5.8
>> 175     327     5.7
>> 176     328     5.6
>> 177     329     5.5
>> 178     330     5.5
>> 179     331     5.4
>> 180     332     5.3
>> 181     333     5.2
>> 182     334     5.1
>> 183     335     5
>> 184     336     5
>> 185     337     4.9
>> 186     338     4.8
>> 187     339     4.7
>> 188     340     4.7
>> 189     341     4.6
>> 190     342     4.5
>> 191     343     4.5
>> 192     344     4.4
>> 193     345     4.4
>> 194     346     4.3
>> 195     347     4.3
>> 196     348     4.2
>> 197     349     4.2
>> 198     350     4.1
>> 199     351     4.1
>> 200     352     4
>> 201     353     4
>> 202     354     4
>> 203     355     3.9
>> 204     356     3.9
>> 205     357     3.8
>> 206     358     3.8
>> 207     359     3.8
>> 208     360     3.8
>> 209     361     3.7
>> 210     362     3.7
>> 211     363     3.7
>> 212     364     3.6
>> 213     365     3.6
>> 214     366     3.6
>> 215     1       3.2
>> 216     2       3.2
>> 217     3       3.2
>> 218     4       3.2
>> 219     5       3.2
>> 220     6       3.2
>> 221     7       3.2
>> 222     8       3.2
>> 223     9       3.2
>> 224     10      3.2
>> 225     11      3.2
>> 226     12      3.2
>> 227     13      3.2
>> 228     14      3.2
>> 229     15      3.2
>> 230     16      3.2
>> 231     17      3.2
>> 232     18      3.2
>> 233     19      3.2
>> 234     20      3.2
>> 235     21      3.2
>> 236     22      3.2
>> 237     23      3.2
>> 238     24      3.2
>> 239     25      3.2
>> 240     26      3.2
>> 241     27      3.2
>> 242     28      3.2
>> 243     29      3.2
>> 244     30      3.2
>> 245     31      3.2
>> 246     32      3.2
>> 247     33      3.2
>> 248     34      3.2
>> 249     35      3.2
>> 250     36      3.2
>> 251     37      3.2
>> 252     38      3.2
>> 253     39      3.2
>> 254     40      3.2
>> 255     41      3.2
>> 256     42      3.2
>> 257     43      3.2
>> 258     44      3.2
>> 259     45      3.2
>> 260     46      3.2
>> 261     47      3.2
>> 262     48      3.2
>> 263     49      3.2
>> 264     50      3.2
>> 265     51      3.2
>> 266     52      3.2
>> 267     53      3.2
>> 268     54      3.3
>> 269     55      3.3
>> 270     56      3.3
>> 271     57      3.3
>> 272     58      3.3
>> 273     59      3.3
>> 274     60      3.3
>> 275     61      3.3
>> 276     62      3.3
>> 277     63      3.3
>> 278     64      3.3
>> 279     65      3.3
>> 280     66      3.3
>> 281     67      3.3
>> 282     68      3.3
>> 283     69      3.3
>> 284     70      3.3
>> 285     71      3.4
>> 286     72      3.4
>> 287     73      3.4
>> 288     74      3.4
>> 289     75      3.4
>> 290     76      3.4
>> 291     77      3.4
>> 292     78      3.4
>> 293     79      3.5
>> 294     80      3.5
>> 295     81      3.5
>> 296     82      3.5
>> 297     83      3.5
>> 298     84      3.5
>> 299     85      3.6
>> 300     86      3.6
>> 301     87      3.6
>> 302     88      3.6
>> 303     89      3.6
>> 304     90      3.7
>> 305     91      3.7
>> 306     92      3.7
>> 307     93      3.8
>> 308     94      3.8
>> 309     95      3.8
>> 310     96      3.8
>> 311     97      3.9
>> 312     98      3.9
>> 313     99      4
>> 314     100     4
>> 315     101     4
>> 316     102     4.1
>> 317     103     4.1
>> 318     104     4.2
>> 319     105     4.2
>> 320     106     4.3
>> 321     107     4.3
>> 322     108     4.4
>> 323     109     4.4
>> 324     110     4.5
>> 325     111     4.5
>> 326     112     4.6
>> 327     113     4.7
>> 328     114     4.7
>> 329     115     4.8
>> 330     116     4.9
>> 331     117     5
>> 332     118     5
>> 333     119     5.1
>> 334     120     5.2
>> 335     121     5.3
>> 336     122     5.4
>> 337     123     5.5
>> 338     124     5.5
>> 339     125     5.6
>> 340     126     5.7
>> 341     127     5.8
>> 342     128     6
>> 343     129     6.1
>> 344     130     6.2
>> 345     131     6.3
>> 346     132     6.4
>> 347     133     6.5
>> 348     134     6.7
>> 349     135     6.8
>> 350     136     6.9
>> 351     137     7
>> 352     138     7.2
>> 353     139     7.3
>> 354     140     7.5
>> 355     141     7.6
>> 356     142     7.8
>> 357     143     7.9
>> 358     144     8.1
>> 359     145     8.2
>> 360     146     8.4
>> 361     147     8.6
>> 362     148     8.7
>> 363     149     8.9
>> 364     150     9.1
>> 365     151     9.3
>> 366     152     9.3
>>
>>
>>
>> ------------------------------------------------------------------------
>>
>> 1    153    9.4
>> 2    154    9.6
>> 3    155    9.8
>> 4    156    10
>> 5    157    10.2
>> 6    158    10.4
>> 7    159    10.6
>> 8    160    10.8
>> 9    161    11
>> 10    162    11.2
>> 11    163    11.4
>> 12    164    11.6
>> 13    165    11.8
>> 14    166    12
>> 15    167    12.3
>> 16    168    12.5
>> 17    169    12.7
>> 18    170    12.9
>> 19    171    13.1
>> 20    172    13.4
>> 21    173    13.6
>> 22    174    13.8
>> 23    175    14
>> 24    176    14.2
>> 25    177    14.5
>> 26    178    14.7
>> 27    179    14.9
>> 28    180    15.1
>> 29    181    15.4
>> 30    182    15.6
>> 31    183    15.8
>> 32    184    16
>> 33    185    16.2
>> 34    186    16.5
>> 35    187    16.7
>> 36    188    16.9
>> 37    189    17.1
>> 38    190    17.3
>> 39    191    17.5
>> 40    192    17.7
>> 41    193    17.9
>> 42    194    18.1
>> 43    195    18.3
>> 44    196    18.5
>> 45    197    18.7
>> 46    198    18.9
>> 47    199    19
>> 48    200    19.2
>> 49    201    19.4
>> 50    202    19.5
>> 51    203    19.7
>> 52    204    19.9
>> 53    205    20
>> 54    206    20.2
>> 55    207    20.3
>> 56    208    20.4
>> 57    209    20.5
>> 58    210    20.7
>> 59    211    20.8
>> 60    212    20.9
>> 61    213    21
>> 62    214    21.1
>> 63    215    21.2
>> 64    216    21.3
>> 65    217    21.3
>> 66    218    21.4
>> 67    219    21.5
>> 68    220    21.5
>> 69    221    21.6
>> 70    222    21.6
>> 71    223    21.6
>> 72    224    21.7
>> 73    225    21.7
>> 74    226    21.7
>> 75    227    21.7
>> 76    228    21.7
>> 77    229    21.7
>> 78    230    21.7
>> 79    231    21.6
>> 80    232    21.6
>> 81    233    21.6
>> 82    234    21.5
>> 83    235    21.5
>> 84    236    21.4
>> 85    237    21.3
>> 86    238    21.3
>> 87    239    21.2
>> 88    240    21.1
>> 89    241    21
>> 90    242    20.9
>> 91    243    20.8
>> 92    244    20.7
>> 93    245    20.5
>> 94    246    20.4
>> 95    247    20.3
>> 96    248    20.2
>> 97    249    20
>> 98    250    19.9
>> 99    251    19.7
>> 100    252    19.5
>> 101    253    19.4
>> 102    254    19.2
>> 103    255    19
>> 104    256    18.9
>> 105    257    18.7
>> 106    258    18.5
>> 107    259    18.3
>> 108    260    18.1
>> 109    261    17.9
>> 110    262    17.7
>> 111    263    17.5
>> 112    264    17.3
>> 113    265    17.1
>> 114    266    16.9
>> 115    267    16.7
>> 116    268    16.5
>> 117    269    16.2
>> 118    270    16
>> 119    271    15.8
>> 120    272    15.6
>> 121    273    15.4
>> 122    274    15.1
>> 123    275    14.9
>> 124    276    14.7
>> 125    277    14.5
>> 126    278    14.2
>> 127    279    14
>> 128    280    13.8
>> 129    281    13.6
>> 130    282    13.4
>> 131    283    13.1
>> 132    284    12.9
>> 133    285    12.7
>> 134    286    12.5
>> 135    287    12.3
>> 136    288    12
>> 137    289    11.8
>> 138    290    11.6
>> 139    291    11.4
>> 140    292    11.2
>> 141    293    11
>> 142    294    10.8
>> 143    295    10.6
>> 144    296    10.4
>> 145    297    10.2
>> 146    298    10
>> 147    299    9.8
>> 148    300    9.6
>> 149    301    9.4
>> 150    302    9.3
>> 151    303    9.1
>> 152    304    8.9
>> 153    305    8.7
>> 154    306    8.6
>> 155    307    8.4
>> 156    308    8.2
>> 157    309    8.1
>> 158    310    7.9
>> 159    311    7.8
>> 160    312    7.6
>> 161    313    7.5
>> 162    314    7.3
>> 163    315    7.2
>> 164    316    7
>> 165    317    6.9
>> 166    318    6.8
>> 167    319    6.7
>> 168    320    6.5
>> 169    321    6.4
>> 170    322    6.3
>> 171    323    6.2
>> 172    324    6.1
>> 173    325    6
>> 174    326    5.8
>> 175    327    5.7
>> 176    328    5.6
>> 177    329    5.5
>> 178    330    5.5
>> 179    331    5.4
>> 180    332    5.3
>> 181    333    5.2
>> 182    334    5.1
>> 183    335    5
>> 184    336    5
>> 185    337    4.9
>> 186    338    4.8
>> 187    339    4.7
>> 188    340    4.7
>> 189    341    4.6
>> 190    342    4.5
>> 191    343    4.5
>> 192    344    4.4
>> 193    345    4.4
>> 194    346    4.3
>> 195    347    4.3
>> 196    348    4.2
>> 197    349    4.2
>> 198    350    4.1
>> 199    351    4.1
>> 200    352    4
>> 201    353    4
>> 202    354    4
>> 203    355    3.9
>> 204    356    3.9
>> 205    357    3.8
>> 206    358    3.8
>> 207    359    3.8
>> 208    360    3.8
>> 209    361    3.7
>> 210    362    3.7
>> 211    363    3.7
>> 212    364    3.6
>> 213    365    3.6
>> 214    366    3.6
>> 215    1    3.2
>> 216    2    3.2
>> 217    3    3.2
>> 218    4    3.2
>> 219    5    3.2
>> 220    6    3.2
>> 221    7    3.2
>> 222    8    3.2
>> 223    9    3.2
>> 224    10    3.2
>> 225    11    3.2
>> 226    12    3.2
>> 227    13    3.2
>> 228    14    3.2
>> 229    15    3.2
>> 230    16    3.2
>> 231    17    3.2
>> 232    18    3.2
>> 233    19    3.2
>> 234    20    3.2
>> 235    21    3.2
>> 236    22    3.2
>> 237    23    3.2
>> 238    24    3.2
>> 239    25    3.2
>> 240    26    3.2
>> 241    27    3.2
>> 242    28    3.2
>> 243    29    3.2
>> 244    30    3.2
>> 245    31    3.2
>> 246    32    3.2
>> 247    33    3.2
>> 248    34    3.2
>> 249    35    3.2
>> 250    36    3.2
>> 251    37    3.2
>> 252    38    3.2
>> 253    39    3.2
>> 254    40    3.2
>> 255    41    3.2
>> 256    42    3.2
>> 257    43    3.2
>> 258    44    3.2
>> 259    45    3.2
>> 260    46    3.2
>> 261    47    3.2
>> 262    48    3.2
>> 263    49    3.2
>> 264    50    3.2
>> 265    51    3.2
>> 266    52    3.2
>> 267    53    3.2
>> 268    54    3.3
>> 269    55    3.3
>> 270    56    3.3
>> 271    57    3.3
>> 272    58    3.3
>> 273    59    3.3
>> 274    60    3.3
>> 275    61    3.3
>> 276    62    3.3
>> 277    63    3.3
>> 278    64    3.3
>> 279    65    3.3
>> 280    66    3.3
>> 281    67    3.3
>> 282    68    3.3
>> 283    69    3.3
>> 284    70    3.3
>> 285    71    3.4
>> 286    72    3.4
>> 287    73    3.4
>> 288    74    3.4
>> 289    75    3.4
>> 290    76    3.4
>> 291    77    3.4
>> 292    78    3.4
>> 293    79    3.5
>> 294    80    3.5
>> 295    81    3.5
>> 296    82    3.5
>> 297    83    3.5
>> 298    84    3.5
>> 299    85    3.6
>> 300    86    3.6
>> 301    87    3.6
>> 302    88    3.6
>> 303    89    3.6
>> 304    90    3.7
>> 305    91    3.7
>> 306    92    3.7
>> 307    93    3.8
>> 308    94    3.8
>> 309    95    3.8
>> 310    96    3.8
>> 311    97    3.9
>> 312    98    3.9
>> 313    99    4
>> 314    100    4
>> 315    101    4
>> 316    102    4.1
>> 317    103    4.1
>> 318    104    4.2
>> 319    105    4.2
>> 320    106    4.3
>> 321    107    4.3
>> 322    108    4.4
>> 323    109    4.4
>> 324    110    4.5
>> 325    111    4.5
>> 326    112    4.6
>> 327    113    4.7
>> 328    114    4.7
>> 329    115    4.8
>> 330    116    4.9
>> 331    117    5
>> 332    118    5
>> 333    119    5.1
>> 334    120    5.2
>> 335    121    5.3
>> 336    122    5.4
>> 337    123    5.5
>> 338    124    5.5
>> 339    125    5.6
>> 340    126    5.7
>> 341    127    5.8
>> 342    128    6
>> 343    129    6.1
>> 344    130    6.2
>> 345    131    6.3
>> 346    132    6.4
>> 347    133    6.5
>> 348    134    6.7
>> 349    135    6.8
>> 350    136    6.9
>> 351    137    7
>> 352    138    7.2
>> 353    139    7.3
>> 354    140    7.5
>> 355    141    7.6
>> 356    142    7.8
>> 357    143    7.9
>> 358    144    8.1
>> 359    145    8.2
>> 360    146    8.4
>> 361    147    8.6
>> 362    148    8.7
>> 363    149    8.9
>> 364    150    9.1
>> 365    151    9.3
>> 366    152    9.3
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at stat.ucla.edu  Thu Jul 17 07:42:32 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Wed, 16 Jul 2003 22:42:32 -0700
Subject: [R] Excel can do what R can't?????
In-Reply-To: <1058398259.3f15e0335a583@webmail.utm.utoronto.ca>
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
	<1058398259.3f15e0335a583@webmail.utm.utoronto.ca>
Message-ID: <3F163748.5020100@stat.ucla.edu>

I'm having a little difficulty understanding this thread.  If Excel can 
do the job correctly and suits your needs, why not just use Excel? 

As far as I know, 'optim' cannot optimize a function subject to 
arbitrary equality constraints.  The 'constrOptim' function allows for 
linear inequality constraints but in general, you will either have to 
reformulate the problem or add your own penalty into the objective 
function.  Also, just a small note, but using lexical scoping in your 
problem would eliminate the need to have all those variables defined in 
the global environment (but otherwise it won't change anything). 

-roger

Michael Rennie wrote:

>Hi, Reid and Spencer- 
>
>I think I've figured something out pretty critical to the problem.  
>
>Loking at my 'solver' options, I have a condition added that 'Hgtmod = Hgt'.  
>Without this conditional statement, I have to run solver 3-4 times before I get 
>a final solution. MEANING- solver and R, when left to their own devices, suck 
>equally at finding a solution with similar starting points.  BUT, given a 
>conditional statement that demands Hgt = Hgtmod, it gives it somewhere to look 
>withing the given parameter space. 
>
>So, the millon dollar quesiton: Is there any way of setting up a contitional 
>statement like this in 'optim', to specify a solution such that Hgtmod = Hgt?  
>Or, write it into function f?  The control statements 'fnscale' and 'parscale' 
>

I don't think 'fnscale' or 'parscale' will help you here at all.  If you 
want a constraint you need to specify it in the objective function 
directly (something like an additive penalty). 

>look like candidates, but will only help me if I build Hgtmod into the 
>optimization statement- can I do that?  How do you specify multiple 
>parameters?  Or should I specify two functions to optimize- one, fucntion f, 
>and a second, something like
>
>g<- (Hgt = Hgtmod)^2  
>
>Can I do that? If this is all I need to do, I am off to the races, and owe you 
>both a beer!  I'm going to try some stuff. All is not lost! If you have any 
>ideas, I'd love to hear them.
>
>THanks again for everything.....
>
>
>  
>
>  
>



From mkondrin at hppi.troitsk.ru  Thu Jul 17 18:48:26 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 17 Jul 2003 09:48:26 -0700
Subject: [R] list to data frame
In-Reply-To: <4957.62.79.113.27.1058391664.squirrel@webmail.plan.auc.dk>
References: <4957.62.79.113.27.1058391664.squirrel@webmail.plan.auc.dk>
Message-ID: <3F16D35A.8010405@hppi.troitsk.ru>

Jesper Runge Madsen wrote:
> Dear R helpers
> I am trying to convert a list into a data frame but when I try, I get a
> stack overflow error (Error: protect(): stack overflow). My list contains
> about 17000 rows and looks like shown at the bottom. The reason that I
> want to convert it in to a data frame is that I want to export it to a
> mysql database with the dbWriteTable function.
> 
> The function that I use is
> As.data.frame(listname)
> 
> I hope someone can help me
> 
> 
> 
> Cut out from the list
> structure(list("-9.000000" = 187, "9754.000000" = 130, "9755.000000" = 129,
>     "9756.000000" = 125.5, "9757.000000" = 118.1111, "9762.000000" =
> 132.6667,
>     "9763.000000" = 133, "9764.000000" = 130, "9766.000000" = 130.5,
>     "9780.000000" = 160, "9787.000000" = 154, "9808.000000" = 147.8,
>     "9811.000000" = 156.5, "9812.000000" = 154.3333, "9815.000000" = 141,
>     "9819.000000" = 135, "9820.000000" = 141, "9821.000000" = 140.5,
> .
> .
> .
> .
>     "525965.000000" = 76.4545), .Names = c("-9.000000", "9754.000000",
>     "9755.000000", "9756.000000", "9757.000000", "9762.000000",
> "9763.000000",
>     "9764.000000", "9766.000000", "9780.000000", "9787.000000",
> "9808.000000",
>     "9811.000000", "9812.000000", "9815.000000", "9819.000000",
> "9820.000000",
>     "9821.000000", "9822.000000", "9823.000000", "9824.000000",
> "9825.000000",
> .
> .
> .
> .
>     "525863.000000", "525866.000000", "525867.000000", "525868.000000",
>     "525869.000000", "525870.000000", "525951.000000", "525952.000000",
>     "525954.000000", "525962.000000", "525964.000000", "525965.000000"
> ))
> 
> /Jesper Runge Madsen
> Aalborg Universitet
> Denmark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
If your list were formatted like this one
 >list("ColId1"=c(12,13,14),"ColId2"=c(34,56,78))->k
then
 >as.data.frame(k)->g
would give desirable result
 > as.data.frame(k)
    a  b
1 12 34
2 13 56
3 14 78
If it is related to the question you have asked earlier and you want to 
convert matrix back to data.frame
 > k<-cbind(c(1,2,3,4),c(1,2,3,4))
 > k
      [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    3    3
[4,]    4    4
 > class(k)
[1] "matrix"
 > as.data.frame(k)->k2
 > names(k2)<-c("ColId1","ColId2")
to insert suitable  column names instead default ones
 > k2
   ColId1 ColId2
1      1      1
2      2      2
3      3      3
4      4      4
 > class(k2)
[1] "data.frame"



From mrennie at utm.utoronto.ca  Thu Jul 17 07:54:30 2003
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Thu, 17 Jul 2003 01:54:30 -0400
Subject: [R] Excel can do what R can't?????
In-Reply-To: <3F163748.5020100@stat.ucla.edu>
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>
	<1058398259.3f15e0335a583@webmail.utm.utoronto.ca>
	<3F163748.5020100@stat.ucla.edu>
Message-ID: <1058421270.3f163a16e09d8@webmail.utm.utoronto.ca>

Quoting "Roger D. Peng" <rpeng at stat.ucla.edu>:

> I'm having a little difficulty understanding this thread.  If Excel can 
> do the job correctly and suits your needs, why not just use Excel? 
> 

Primarily because I don't know how to automate this in excel.  The reason for 
me doing this is I eventually need it to go through 1000 rows of input 
variables so that I can get output with error associated with it. Plus, people 
think your cool if you can do it in R. 

> As far as I know, 'optim' cannot optimize a function subject to 
> arbitrary equality constraints.  The 'constrOptim' function allows for 
> linear inequality constraints but in general, you will either have to 
> reformulate the problem or add your own penalty into the objective 
> function.  Also, just a small note, but using lexical scoping in your 
> problem would eliminate the need to have all those variables defined in 
> the global environment (but otherwise it won't change anything). 
> 

What's lexical scoping?

Mike

-- 
Michael Rennie
M.Sc. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From azzalini at stat.unipd.it  Thu Jul 17 08:11:17 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Thu, 17 Jul 2003 08:11:17 +0200
Subject: [R] Is there a bug in qr(..,LAPACK=T)
In-Reply-To: <20030716115055.713ad33e.mikem@salter-point.com>
References: <20030716095439.1be17b0c.mikem@salter-point.com>
	<20030716115055.713ad33e.mikem@salter-point.com>
Message-ID: <20030717061117.4F5AF7CA831@tango.stat.unipd.it>

On Wednesday 16 July 2003 20:50, Mike Meyer wrote:
> Several people have kindly (and gently) pointed out that the ?qr
> documentation states that rank detection does not work for the LAPACK case.
> ?Its my fault for assuming that rank detection did work. --Mike

sprictly speaking is your "fault",
however it seems sensible that qr(..) returns the rank value as NULL or NA 
when LAPACK=TRUE -- since it does not try to evaluate it -- instead of
always returning `full rank'.

regards, Adelchi
-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/



From mkondrin at hppi.troitsk.ru  Thu Jul 17 19:43:53 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 17 Jul 2003 10:43:53 -0700
Subject: [R] Formal definitions of R-language.
Message-ID: <3F16E059.8020700@hppi.troitsk.ru>

Hello!
Some CS-guys (the type who knows what Church formalism is) keep asking 
me questions about formal definitions of R-language that I can not 
answer (or even understand). Is there some freely available papers which 
I can throw at them where it would be explained is R 
functional/OOP/procedural language, does it use weak/strong, 
dynamic/static typization, does it use lazy or ...(do not know what) 
evaluation, what sort of garbage collector it uses?
Thanks.



From Tor.Strand at cih.uib.no  Thu Jul 17 08:29:06 2003
From: Tor.Strand at cih.uib.no (Tor A Strand)
Date: Thu, 17 Jul 2003 08:29:06 +0200
Subject: [R] missing values and gam (was: how to handle missing values)
In-Reply-To: <BB3AF174.7319%Tor.Strand@cih.uib.no>
Message-ID: <BB3C0ED2.735D%Tor.Strand@cih.uib.no>

Thank you for all the responses on generalized additive models(gam) and
missing values. I am now able set up a model using gam and have a certain
understanding of how R deals with missing values.

The problem is, however, I am still not able to a gam model that is from a
dataset that contains missing values.

The function

C<-gam(depvar~var1+var2+s(var3), data=dataset)
Returns the errors

Error in na.omit.default() : Argument "object" is missing, with no default

Again, can anyone help a newbie.

Tor



From ligges at statistik.uni-dortmund.de  Thu Jul 17 09:07:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Jul 2003 09:07:52 +0200
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F16E059.8020700@hppi.troitsk.ru>
References: <3F16E059.8020700@hppi.troitsk.ru>
Message-ID: <3F164B48.8050403@statistik.uni-dortmund.de>

M.Kondrin wrote:

> Hello!
> Some CS-guys (the type who knows what Church formalism is) keep asking 
> me questions about formal definitions of R-language that I can not 
> answer (or even understand). Is there some freely available papers which 
> I can throw at them where it would be explained is R 
> functional/OOP/procedural language, does it use weak/strong, 
> dynamic/static typization, does it use lazy or ...(do not know what) 
> evaluation, what sort of garbage collector it uses?
> Thanks.

R ships with a draft version of the manual "R Language Definition".
Another source is Venables & Ripley (2000): S Programming, Springer.

Uwe Ligges



From cmetcs at nus.edu.sg  Thu Jul 17 09:27:49 2003
From: cmetcs at nus.edu.sg (Tan Chuen Seng)
Date: Thu, 17 Jul 2003 15:27:49 +0800
Subject: [R] Keeping track of occurrence of warning message
Message-ID: <C23CE79231BD7F458FABA63FC8509BC818B7B4@MBXSRV03.stf.nus.edu.sg>

Hi Laurent,

Sorry to trouble you again. I have little idea how to get R to know if a
warning has taken place. I tried using last.warning as I think it was
what you suggested. But still track.warning is NULL although there are
warnings. The code goes something like this:

w1<-0
track.warning<-NULL
....
if(length(last.warning)>w1){
track.warning<-c(track.warning,data.no)
}

It seems to me that length(last.warning)==0 throughout the running of
the program although in the end there are still errors.

Please advice. Thanks for your help.

>From chuen seng


-----Original Message-----
From: Laurent Gautier [mailto:laurent at cbs.dtu.dk] 
Sent: Wednesday, July 16, 2003 3:34 PM
To: Tan Chuen Seng
Subject: Re: [R] Keeping track of occurrence of warning message


On Tue, Jul 15, 2003 at 05:51:32PM +0800, Tan Chuen Seng wrote:
> Hi Laurent,
> 
> Thanks for the suggestion. However I am not too sure how to access
> this last.warning while in the loop. From my understanding, I can't 
> get dimensions from a list.

l <- list(a=1:2, b=letters[1:5])
length(l)
lapply(l, length)

...if it answers your question....



> 
> Please advice.
> 
> >From chuen seng
> 
> -----Original Message-----
> From: Laurent Gautier [mailto:laurent at cbs.dtu.dk]
> Sent: Tuesday, July 15, 2003 3:17 PM
> To: Tan Chuen Seng
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Keeping track of occurrence of warning message
> 
> 
> On Tue, Jul 15, 2003 at 02:58:10PM +0800, Tan Chuen Seng wrote:
> > Hi there,
> > 
> > I am interested if there is anyway to keep track of the occurrence
> > of
> > warning message.
> > 
> > I know that warnings will only be printed out at the end of the 
> > program if warn=0. However I am also interested at which particular 
> > set of data does the warnings occur too. This is because I am 
> > running 1000 data, so if there are 2 or 3 data that give warnings, I

> > would like to know which are the ones out of the 1000 data.
> > 
> > I tried using the following code in the program to indicate where
> > the
> > warning occur but was unable to get anything recorded although the
> > warnings() gave me 12 messages.
> > 
> > track.warning<-NULL
> > ....
> > if(options("warn")$warn>=0){
> > track.warning<-c(track.warning,data.no)
> > }
> > 
> > 
> > Your help is greatly appreciated. Thanks.
> > 
> > >From chuen seng
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> One way to do things is to have a list to store warnings has you hit
> them in you loop: you can access what is the list last.warning.
> 
> 
> 
> Hopin' it helps,
> 
> 
> 
> L.
> 
> 

-- 
--------------------------------------------------------------
currently at the National Yang-Ming University in Taipei, Taiwan
--------------------------------------------------------------
Laurent Gautier			CBS, Building 208, DTU
PhD. Student			DK-2800 Lyngby,Denmark	
tel: +45 45 25 24 89		http://www.cbs.dtu.dk/laurent



From Tom.Mulholland at health.wa.gov.au  Thu Jul 17 10:17:33 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Thu, 17 Jul 2003 16:17:33 +0800
Subject: [R] Recode from 2 variables
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FC6@nt207mesep.health.wa.gov.au>

I am trying to create a new variable which uses the suburb names if HR
and HRRES are the same but which uses HRRES if they are different. Any
assistance would be appreciated as my brain has just packed up. I'm not
sure I can teach myself anymore new tricks this afternoon. 


              HR                   HRRES                         SUBURB
What I am trying to get
954    Wheatbelt          Great Southern ALBANY
Great Southern
3177   Wheatbelt               Wheatbelt ARDATH
Ardath
3564   Wheatbelt                   Metro ARMADALE
Metro
3825   Wheatbelt               Wheatbelt ARTHUR RIVER
Arthur River
5049   Wheatbelt              South West AUSTRALIND
SouthWest
5445   Wheatbelt               Wheatbelt BABAKIN
Babakin
5769   Wheatbelt               Wheatbelt BADGINGARRA
Bagingarra
6093   Wheatbelt               Wheatbelt BAKERS HILL
Bakers Hill
7065   Wheatbelt               Wheatbelt BALLIDU
Ballidu
9396   Wheatbelt                   Metro BAYSWATER
Metro
9657   Wheatbelt               Wheatbelt BEACON
Beacon
12492  Wheatbelt               Wheatbelt BENCUBBIN
Bencubbin
13122  Metro                       Metro BENTLEY
Bentley
13788  Metro                       Metro BEVERLEY
Beverley
14436  Metro                       Metro BINDI BINDI
Bindi Bindi
14517  Metro                       Metro BINDOON
Bindoon
16218  Metro                   Wheatbelt BODALLIN
Wheatbelt
_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
189 Royal St, East Perth, WA, 6004
 
Tel: (08) 9222 4062
e-mail: Tom.Mulholland at health.wa.gov.au
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From Soren.Hojsgaard at agrsci.dk  Thu Jul 17 10:16:28 2003
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 17 Jul 2003 10:16:28 +0200
Subject: [R] Testing for numeric(0)
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC6BF9D0@DJFPOST01.djf.agrsci.dk>

Hi all, Can anyone tell me how to test for numeric(0) ?
Best regards
S?ren H?jsgaard



From maechler at stat.math.ethz.ch  Thu Jul 17 10:24:51 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Jul 2003 10:24:51 +0200
Subject: [R] bwplot does something weird with Hmisc library attached
In-Reply-To: <3F1587D5.12646.B5EE64@localhost>
References: <3F1587D5.12646.B5EE64@localhost>
Message-ID: <16150.23891.143208.15537@gargle.gargle.HOWL>

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Wed, 16 Jul 2003 17:13:57 +0200 writes:

    Petr> Dear all I would like to ask you about possible bug in
    Petr> using bwplot (from lattice) together with Hmisc
    Petr> library attached. I found it in my actual data, but
    Petr> here is a toy example. It appears only when some
    Petr> levels are missing.

    Petr> library(lattice) 
    Petr> library(Hmisc)

    ..............

    Petr> The levels for factor f1 are in the OOOPS case
    Petr> reversed, maybe it has something to do with factor
    Petr> redefinition as stated when attaching Hmisc library.

exactly.
Because of this:

  >> > library(Hmisc)
  >> Hmisc library by Frank E Harrell Jr
  >> 
  >> Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
  >> to see overall documentation.
  >> 
  >> Hmisc redefines [.factor to drop unused levels of factor variables
  >> when subscripting. To prevent this behaviour, issue the command
  >> options(drop.unused.levels=F).
  >> 
  >> Attaching package 'Hmisc':
  >> 
  >> 
  >> 	The following object(s) are masked from package:survival :
  >> 
  >> 	 as.data.frame.Surv untangle.specials 
  >> 
  >> 
  >> 	The following object(s) are masked from package:stepfun :
  >> 
  >> 	 ecdf 
  >> 
  >> 
  >> 	The following object(s) are masked from package:base :
  >> 
  >> 	 [.factor %in% interaction [.terms 

where only the last line is the real crucial problem,
(and "%in%" is I think identical to R base's definition)
using Hmisc unfortunately can break anything that internally
uses  "[.factor"(), interaction(), or  "[.terms"()
where the first is your problem.

If you look closer at the text above, you see

  >>> To prevent this behaviour, issue the command 
  >>    options(drop.unused.levels=F).

and that's your solution: Issue

    options(drop.unused.levels = FALSE)

    ## "FALSE", not "F", since "F" can be anything: it's valid
                variable name which initially is set to FALSE!

I would set this option everytime when using Hmisc, because
otherwise `anything' else can be broken by the changed factor
subsetting behavior.

---

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From B.Rowlingson at lancaster.ac.uk  Thu Jul 17 10:33:22 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 17 Jul 2003 09:33:22 +0100
Subject: [R] Testing for numeric(0)
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC6BF9D0@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC6BF9D0@DJFPOST01.djf.agrsci.dk>
Message-ID: <3F165F52.1070901@lancaster.ac.uk>

S?ren H?jsgaard wrote:
> Hi all, Can anyone tell me how to test for numeric(0) ?

 > foo <- numeric(0)
 > length(foo)
[1] 0
 > is.numeric(foo)
[1] TRUE

  so:

  is.numeric0 <- function(x){length(x)==0 & is.numeric(x)}

seems to do the trick....

Baz



From trainor at transborder.org  Thu Jul 17 10:35:33 2003
From: trainor at transborder.org (Douglas Trainor)
Date: Thu, 17 Jul 2003 03:35:33 -0500
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F164B48.8050403@statistik.uni-dortmund.de>
References: <3F16E059.8020700@hppi.troitsk.ru>
	<3F164B48.8050403@statistik.uni-dortmund.de>
Message-ID: <3F165FD5.1090907@transborder.org>

Uwe Ligges wrote:

> M.Kondrin wrote:
>
>> Hello!
>> Some CS-guys (the type who knows what Church formalism is) keep 
>> asking me questions about formal definitions of R-language that I can 
>> not answer (or even understand). Is there some freely available 
>> papers which I can throw at them where it would be explained is R 
>> functional/OOP/procedural language, does it use weak/strong, 
>> dynamic/static typization, does it use lazy or ...(do not know what) 
>> evaluation, what sort of garbage collector it uses?
>> Thanks.
>
>
> R ships with a draft version of the manual "R Language Definition".
> Another source is Venables & Ripley (2000): S Programming, Springer. 


Tell the "CS-guys" to grab the source code and chew on the LALR 
context-free grammar stuff in the file "gram.y" as in:

    R-1.7.1/src/main/gram.y

"R Language Definition" lives at:

    http://cran.r-project.org/doc/manuals/R-lang.pdf



From laurent at cbs.dtu.dk  Thu Jul 17 10:50:04 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Thu, 17 Jul 2003 10:50:04 +0200
Subject: [R] Testing for numeric(0)
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC6BF9D0@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC6BF9D0@DJFPOST01.djf.agrsci.dk>
Message-ID: <20030717085004.GB13734341@genome.cbs.dtu.dk>

On Thu, Jul 17, 2003 at 10:16:28AM +0200, S?ren H?jsgaard wrote:
> Hi all, Can anyone tell me how to test for numeric(0) ?
> Best regards
> S?ren H?jsgaard



foo <- numeric(0)
identical(foo, numeric(0))



Hopin' it helps,


L.



> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
--------------------------------------------------------------
currently at the National Yang-Ming University in Taipei, Taiwan
--------------------------------------------------------------
Laurent Gautier			CBS, Building 208, DTU
PhD. Student			DK-2800 Lyngby,Denmark	
tel: +45 45 25 24 89		http://www.cbs.dtu.dk/laurent



From s-plus at wiwi.uni-bielefeld.de  Thu Jul 17 10:49:52 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 17 Jul 2003 10:49:52 +0200
Subject: [R] Stem and leaf display?
References: <200307160608.h6G68b53207562@atlas.otago.ac.nz>
Message-ID: <3F166330.6070802@wiwi.uni-bielefeld.de>

Some weeks ago I wrote a function to  construct a stem-and-leaf-display
because we want to have some more parameters to control the result.
D.Trenkler remind me of

/Velleman/Hoaglin: ABC of EDA, page 15: "It is easy to construct a 
Stem-and-Leaf-Display by hand...
It is not nearly as easy to write a general computer program to produce 
Stem-and-Leaf-Displays./"

so the programming was a great challange also.


Here is the link to the code:

   http://www.wiwi.uni-bielefeld.de/~wolf/software/mystem/ms.sch

German documentation:

   http://www.wiwi.uni-bielefeld.de/~wolf/software/mystem/ms.ps
   http://www.wiwi.uni-bielefeld.de/~wolf/software/mystem/ms.html

Syntax of stem.leaf

##################################################################

#Description:                                                    #
#   stem.leaf  produces a stem-and-leaf-display of a data set    #
#                                                                #
#Usage:                                                          #
#   stem.leaf(data)                                              #
#   stem.leaf(data,unit=100,m=5,Min=50,Max=1000,rule.line="Dixon"#
#                                                                #
#Arguments:                                                      #
#   data:      vector of input data                              #
#   unit:      unit of leafs in:  { ...,100,10,1,.1,.01,... }    #
#   m:         1, 2 or 5 -- 10/m=number of possible leaf digits  #
#   Min:       minimum of stem                                   #
#   Max:       maximum of stem                                   #
#   rule.line: = "Dixon"    => number of lines <- 10*log(n,10)   #
#              = "Velleman" => number of lines <- 2*sqrt(n)      #
#              = "Sturges"  => number of lines <- 1 + log(n,2)   #
#   style:     = "UREDA"    => UREDA like stem ( m = 2, 5 )      #
#                                                                #
#Author:                                                         #
#   Peter Wolf 05/2003                                           #
##################################################################

Hope it helps

Peter Wolf


Richard A. O'Keefe wrote:

>I would like to do some fairly basic stem-and-leaf displays in R.
>I am aware (I might even say painfully aware) of stem(base) and
>have tried it.  That's why I'm hoping someone has a usable stem-
>and-leaf display for R so that I don't have to write my own.
>
>r-project.org > Search > R Site Search > "stem and leaf display"
>finds nothing.
>
>I also tried the mail archive search, but couldn't figure out how
>to search the bodies of the messages.
>
>For the record, there are some less important things that I would
>like to have (like depths and outlier lines), and it is a pity that
>the "atom" argument of stem() is effectively undocumented (it's a
>tolerance, fine, but what _of_ and how used?), but I'd put up with
>stem() the way it is if only it didn't do things like displaying
>5.67 and 5.78 as
>    56 | 78
>which makes it effectively unusable.  What good is a stem and leaf
>plot where you can't recover the leading digits of the numbers correctly?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
-----------------------------------------------------------------------------------
Peter Wolf,   Department of Economics and Business Administration,
University of Bielefeld
 pwolf at wiwi.uni-bielefeld.de, 
http://www.wiwi.uni-bielefeld.de/~wolf/wolf.html



From Tom.Mulholland at health.wa.gov.au  Thu Jul 17 10:55:23 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Thu, 17 Jul 2003 16:55:23 +0800
Subject: [R] Recode from 2 variables
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D55C33@nt207mesep.health.wa.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030717/9c2b7217/attachment.pl

From spencer.graves at pdf.com  Thu Jul 17 11:23:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Jul 2003 02:23:05 -0700
Subject: [R] Keeping track of occurrence of warning message
References: <C23CE79231BD7F458FABA63FC8509BC818B7B4@MBXSRV03.stf.nus.edu.sg>
Message-ID: <3F166AF9.7080605@pdf.com>

Have you tried setting "options(warn=1)"?  I don't know if this will 
help, but I saw it on a related thread recently.

spencer graves

Tan Chuen Seng wrote:
> Hi Laurent,
> 
> Sorry to trouble you again. I have little idea how to get R to know if a
> warning has taken place. I tried using last.warning as I think it was
> what you suggested. But still track.warning is NULL although there are
> warnings. The code goes something like this:
> 
> w1<-0
> track.warning<-NULL
> ....
> if(length(last.warning)>w1){
> track.warning<-c(track.warning,data.no)
> }
> 
> It seems to me that length(last.warning)==0 throughout the running of
> the program although in the end there are still errors.
> 
> Please advice. Thanks for your help.
> 
>>From chuen seng
> 
> 
> -----Original Message-----
> From: Laurent Gautier [mailto:laurent at cbs.dtu.dk] 
> Sent: Wednesday, July 16, 2003 3:34 PM
> To: Tan Chuen Seng
> Subject: Re: [R] Keeping track of occurrence of warning message
> 
> 
> On Tue, Jul 15, 2003 at 05:51:32PM +0800, Tan Chuen Seng wrote:
> 
>>Hi Laurent,
>>
>>Thanks for the suggestion. However I am not too sure how to access
>>this last.warning while in the loop. From my understanding, I can't 
>>get dimensions from a list.
> 
> 
> l <- list(a=1:2, b=letters[1:5])
> length(l)
> lapply(l, length)
> 
> ...if it answers your question....
> 
> 
> 
> 
>>Please advice.
>>
>>>From chuen seng
>>
>>-----Original Message-----
>>From: Laurent Gautier [mailto:laurent at cbs.dtu.dk]
>>Sent: Tuesday, July 15, 2003 3:17 PM
>>To: Tan Chuen Seng
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Keeping track of occurrence of warning message
>>
>>
>>On Tue, Jul 15, 2003 at 02:58:10PM +0800, Tan Chuen Seng wrote:
>>
>>>Hi there,
>>>
>>>I am interested if there is anyway to keep track of the occurrence
>>>of
>>>warning message.
>>>
>>>I know that warnings will only be printed out at the end of the 
>>>program if warn=0. However I am also interested at which particular 
>>>set of data does the warnings occur too. This is because I am 
>>>running 1000 data, so if there are 2 or 3 data that give warnings, I
>>
> 
>>>would like to know which are the ones out of the 1000 data.
>>>
>>>I tried using the following code in the program to indicate where
>>>the
>>>warning occur but was unable to get anything recorded although the
>>>warnings() gave me 12 messages.
>>>
>>>track.warning<-NULL
>>>....
>>>if(options("warn")$warn>=0){
>>>track.warning<-c(track.warning,data.no)
>>>}
>>>
>>>
>>>Your help is greatly appreciated. Thanks.
>>>
>>>>From chuen seng
>>>
>>>	[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list 
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>>One way to do things is to have a list to store warnings has you hit
>>them in you loop: you can access what is the list last.warning.
>>
>>
>>
>>Hopin' it helps,
>>
>>
>>
>>L.
>>
>>
> 
>



From spencer.graves at pdf.com  Thu Jul 17 11:28:36 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Jul 2003 02:28:36 -0700
Subject: [R] Excel can do what R can't?????
References: <2C23DE2983BE034CB1CB90DB6B813FD606125383@uswpmx11.merck.com>	<1058398259.3f15e0335a583@webmail.utm.utoronto.ca>	<3F163748.5020100@stat.ucla.edu>
	<1058421270.3f163a16e09d8@webmail.utm.utoronto.ca>
Message-ID: <3F166C44.4000003@pdf.com>

For "lexical scoping", read some of the better documentation on R.  At 
minimum, check "www.r-project.org" -> search -> "R site search".

spencer graves

Michael Rennie wrote:
> Quoting "Roger D. Peng" <rpeng at stat.ucla.edu>:
> 
> 
>>I'm having a little difficulty understanding this thread.  If Excel can 
>>do the job correctly and suits your needs, why not just use Excel? 
>>
> 
> 
> Primarily because I don't know how to automate this in excel.  The reason for 
> me doing this is I eventually need it to go through 1000 rows of input 
> variables so that I can get output with error associated with it. Plus, people 
> think your cool if you can do it in R. 
> 
> 
>>As far as I know, 'optim' cannot optimize a function subject to 
>>arbitrary equality constraints.  The 'constrOptim' function allows for 
>>linear inequality constraints but in general, you will either have to 
>>reformulate the problem or add your own penalty into the objective 
>>function.  Also, just a small note, but using lexical scoping in your 
>>problem would eliminate the need to have all those variables defined in 
>>the global environment (but otherwise it won't change anything). 
>>
> 
> 
> What's lexical scoping?
> 
> Mike
>



From simon at stats.gla.ac.uk  Thu Jul 17 12:00:54 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Thu, 17 Jul 2003 11:00:54 +0100 (BST)
Subject: [R] missing values and gam (was: how to handle missing values)
In-Reply-To: <BB3C0ED2.735D%Tor.Strand@cih.uib.no>
Message-ID: <Pine.SOL.3.96.1030717105736.10309A-100000@jupiter.stats.gla.ac.uk>

mgcv 0.9 will handle missing values properly (provided you are happy that 
dropping them is 'proper'). There is a pre-release version at:

www.stats.gla.ac.uk/~simon/simon/mgcv.html

(it is a pre-release version, so there will be bugs, reports of which
gratefully received!)

simon

> Thank you for all the responses on generalized additive models(gam) and
> missing values. I am now able set up a model using gam and have a certain
> understanding of how R deals with missing values.
> 
> The problem is, however, I am still not able to a gam model that is from a
> dataset that contains missing values.
> 
> The function
> 
> C<-gam(depvar~var1+var2+s(var3), data=dataset)
> Returns the errors
> 
> Error in na.omit.default() : Argument "object" is missing, with no default
> 
> Again, can anyone help a newbie.
> 
> Tor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mkondrin at hppi.troitsk.ru  Thu Jul 17 23:07:34 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 17 Jul 2003 14:07:34 -0700
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F165FD5.1090907@transborder.org>
References: <3F16E059.8020700@hppi.troitsk.ru>
	<3F164B48.8050403@statistik.uni-dortmund.de>
	<3F165FD5.1090907@transborder.org>
Message-ID: <3F171016.5040201@hppi.troitsk.ru>

Douglas Trainor wrote:
> Uwe Ligges wrote:
> 
>> M.Kondrin wrote:
>>
>>> Hello!
>>> Some CS-guys (the type who knows what Church formalism is) keep 
>>> asking me questions about formal definitions of R-language that I can 
>>> not answer (or even understand). Is there some freely available 
>>> papers which I can throw at them where it would be explained is R 
>>> functional/OOP/procedural language, does it use weak/strong, 
>>> dynamic/static typization, does it use lazy or ...(do not know what) 
>>> evaluation, what sort of garbage collector it uses?
>>> Thanks.
>>
>>
>>
>> R ships with a draft version of the manual "R Language Definition".
>> Another source is Venables & Ripley (2000): S Programming, Springer. 
> 
> 
> 
> Tell the "CS-guys" to grab the source code and chew on the LALR 
> context-free grammar stuff in the file "gram.y" as in:
> 
>    R-1.7.1/src/main/gram.y
> 
> "R Language Definition" lives at:
> 
>    http://cran.r-project.org/doc/manuals/R-lang.pdf
> 
> 
> 
> 
> 
Thanks for your answers!
I have already read "R Language Definition" but it is rather hmmm... 
evasive (?). For example:

  R belongs to a class of programming languages in which subroutines
have the ability to modify or construct other subroutines and evaluate
the result as an integral part of the language itself.  This is similar
to Lisp and Scheme and other languages of the "functional programming"
variety, but in contrast to FORTRAN and the ALGOL family.

Being similar to other functional programming language does not mean 
being the one itself, or does it? About typing it (not this passage I 
mean but "RLD" in a whole) said nothing.
This thing about LALR is very good. I will try it on them.



From angel_lul at hotmail.com  Thu Jul 17 12:57:48 2003
From: angel_lul at hotmail.com (Angel)
Date: Thu, 17 Jul 2003 12:57:48 +0200
Subject: [R] remove and put back elements in vector
Message-ID: <Law11-OE69kiBgoGtV200009071@hotmail.com>


Hi,
How can I remove elements from a vector and them put them back in place??
An example (very simple, my vector/operations are much larger/complicated):
Got a vector, lets say:
 a<-c(1,2,40,10,3,20,6);
# I remove values larger than 10
a<-a[a<10]
# Do some operations on the new a "1 2 3 6"
b<-a^2
# Now b is "[1]  1  4  9 36"
# Now I want to insert the elements I removed in a back into a and b
# so I get: 
# a       "1 2 40 10 3 20 6"
#and b "1 4 40 10 9 20 36"

The only thing I've found on the archives is explanations on how to insert:
http://maths.newcastle.edu.au/~rking/R/help/03a/1794.html
Thanks,
Angel



From hothorn at ci.tuwien.ac.at  Thu Jul 17 13:03:01 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Thu, 17 Jul 2003 13:03:01 +0200 (CEST)
Subject: [R] remove and put back elements in vector
In-Reply-To: <Law11-OE69kiBgoGtV200009071@hotmail.com>
Message-ID: <Pine.LNX.3.96.1030717125951.10761C-100000@thorin.ci.tuwien.ac.at>



On Thu, 17 Jul 2003, Angel wrote:

> 
> Hi,
> How can I remove elements from a vector and them put them back in place??
> An example (very simple, my vector/operations are much larger/complicated):
> Got a vector, lets say:
>  a<-c(1,2,40,10,3,20,6);


you need to save the index of the elements of interest:

R> a<-c(1,2,40,10,3,20,6)
R> indx <- which(a < 10)
R> b <- a[indx]
R> b <- b^2 
R> a[indx] <- b
R> a
[1]  1  4 40 10  9 20 36

and similar with b

Torsten

> # I remove values larger than 10
> a<-a[a<10]
> # Do some operations on the new a "1 2 3 6"
> b<-a^2
> # Now b is "[1]  1  4  9 36"
> # Now I want to insert the elements I removed in a back into a and b
> # so I get: 
> # a       "1 2 40 10 3 20 6"
> #and b "1 4 40 10 9 20 36"
> 
> The only thing I've found on the archives is explanations on how to insert:
> http://maths.newcastle.edu.au/~rking/R/help/03a/1794.html
> Thanks,
> Angel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jul 17 13:07:54 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 17 Jul 2003 13:07:54 +0200
Subject: [R] remove and put back elements in vector
In-Reply-To: <Law11-OE69kiBgoGtV200009071@hotmail.com>
References: <Law11-OE69kiBgoGtV200009071@hotmail.com>
Message-ID: <3F16838A.6080005@statistik.uni-dortmund.de>

Angel wrote:
> Hi,
> How can I remove elements from a vector and them put them back in place??
> An example (very simple, my vector/operations are much larger/complicated):
> Got a vector, lets say:
>  a<-c(1,2,40,10,3,20,6);
> # I remove values larger than 10
> a<-a[a<10]
> # Do some operations on the new a "1 2 3 6"
> b<-a^2
> # Now b is "[1]  1  4  9 36"
> # Now I want to insert the elements I removed in a back into a and b
> # so I get: 
> # a       "1 2 40 10 3 20 6"
> #and b "1 4 40 10 9 20 36"


In this case, you don't want to remove the elements, but calculate only 
on the others as in:

   b <- ifelse(a < 10, a^2, a)

or

   b <- a
   b[b < 10] <- b[b < 10]^2

Uwe Ligges

> The only thing I've found on the archives is explanations on how to insert:
> http://maths.newcastle.edu.au/~rking/R/help/03a/1794.html
> Thanks,
> Angel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From adai at nus.edu.sg  Thu Jul 17 13:28:29 2003
From: adai at nus.edu.sg (Adaikalavan Ramasamy)
Date: Thu, 17 Jul 2003 19:28:29 +0800 (SGT)
Subject: [R] remove and put back elements in vector
In-Reply-To: <Pine.LNX.3.96.1030717125951.10761C-100000@thorin.ci.tuwien.ac.at>
Message-ID: <Pine.LNX.4.33.0307171926510.30819-100000@adai>


Would a simple ifelse() do ?

a <- ifelse( a < 10, a^2, a )




On Thu, 17 Jul 2003, Torsten Hothorn wrote:

> 
> 
> On Thu, 17 Jul 2003, Angel wrote:
> 
> > 
> > Hi,
> > How can I remove elements from a vector and them put them back in place??
> > An example (very simple, my vector/operations are much larger/complicated):
> > Got a vector, lets say:
> >  a<-c(1,2,40,10,3,20,6);
> 
> 
> you need to save the index of the elements of interest:
> 
> R> a<-c(1,2,40,10,3,20,6)
> R> indx <- which(a < 10)
> R> b <- a[indx]
> R> b <- b^2 
> R> a[indx] <- b
> R> a
> [1]  1  4 40 10  9 20 36
> 
> and similar with b
> 
> Torsten
> 
> > # I remove values larger than 10
> > a<-a[a<10]
> > # Do some operations on the new a "1 2 3 6"
> > b<-a^2
> > # Now b is "[1]  1  4  9 36"
> > # Now I want to insert the elements I removed in a back into a and b
> > # so I get: 
> > # a       "1 2 40 10 3 20 6"
> > #and b "1 4 40 10 9 20 36"
> > 
> > The only thing I've found on the archives is explanations on how to insert:
> > http://maths.newcastle.edu.au/~rking/R/help/03a/1794.html
> > Thanks,
> > Angel
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From sackur at heraclite.ens.fr  Thu Jul 17 13:20:44 2003
From: sackur at heraclite.ens.fr (Jerome Sackur)
Date: Thu, 17 Jul 2003 13:20:44 +0200 (MET DST)
Subject: [R] Silverman modality test
Message-ID: <Pine.SOL.3.95.1030717131750.18602A-100000@heraclite>

Dear R users,

I've written the following functions to implement Silverman's modality
test ("Using kernel density estimates to investigate multimodality", J.R.
Stat. Soc. B, 43, (1981), 97-99.), and I tested them on the Chondrite data
set (Good & Gaskins, J. Amer. Stat. Ass., 75, (1980), 42-56). Values for
the critical window width seem OK, which is not the case for the
significance levels.

If someone could give me a hint about what is wrong...

Or perhaps someone has already done a real implementation of this test?

Thanks,



Jerome Sackur

Inserm U562

Orsay, France



nbmodes <- function (x, h)
# returns how many modes there are in the kernel density estimate of
# vector x, with window width h.
  {
    modes <- density (x, bw=h)
    modes <- diff (diff (modes$y) / abs (diff (modes$y)))
    modes <- rep(1, length(modes))[modes==-2]
    modes <- sum (modes)
    return (modes)
  }

hcrit <- function (x, n, e=.0001)
# Returns the minimal window width such that the kernel density estimate 
# of x has n modes. 
{
  minw <- min (abs (diff (x)))
  maxw <- (max (x) - min (x))/2
  winN <- maxw
  winN1 <- minw
  while (abs(winN-winN1)>e)
    {
      modes <- nbmodes (x, winN)
      winN1 <- winN
      if (modes > n)
        {
          minw <- winN
          winN <- (winN + maxw)/2
        }
      else
        {
          maxw <- winN
          winN <- (winN - minw)/2
        }
    }
return (winN)
}

silversignif <- function (x, m, nboot=1000)
# Silverman's significance for the null that x has at
# most  modes.
{
  h <- hcrit (x, m)
  h0 <- h
  n <- 0
  theta <- function (y, h0=h, sigma2=var(y))
    {
     (y + rnorm (1, mean=0, sd=h0^2)) / sqrt ( 1+ (h0^2/sigma2))
    }
  for (i in 1:nboot) {
    boot <- theta(sample(x, replace=T))
    nb <- nbmodes (boot, h0)
    if (nb > m) {
      n <- n+1
    }
  }
  return (n/nboot)
}


# Chondrite meteor data
chond <- c(20.77, 22.56, 22.71, 22.99, 26.39, 27.08, 27.32, 27.33, 27.57,
       27.81, 28.69, 29.36, 30.25, 31.89, 32.88, 33.23, 33.28, 33.40,
       33.52, 33.83, 33.95, 34.82)



From Ted.Harding at nessie.mcc.ac.uk  Thu Jul 17 13:30:19 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 17 Jul 2003 12:30:19 +0100 (BST)
Subject: [R] remove and put back elements in vector
In-Reply-To: <Law11-OE69kiBgoGtV200009071@hotmail.com>
Message-ID: <XFMail.030717123019.Ted.Harding@nessie.mcc.ac.uk>

On 17-Jul-03 Angel wrote:
> Got a vector, lets say:
>  a<-c(1,2,40,10,3,20,6);
># I remove values larger than 10
> a<-a[a<10]
># Do some operations on the new a "1 2 3 6"
> b<-a^2
># Now b is "[1]  1  4  9 36"
># Now I want to insert the elements I removed in a back into a and b
># so I get: 
># a       "1 2 40 10 3 20 6"
>#and b "1 4 40 10 9 20 36"

For the above example, step-by-step:
> a<-c(1,2,40,10,3,20,6);
> ix<-(a<10)
> b<-a[ix]
> b<-b^2
> a[ix]<-b
> a
[1]  1  4 40 10  9 20 36

However, in this particuarly simple case it could easily be done
in one go with

  a[a<10]<-(a[a<10])^2

> a<-c(1,2,40,10,3,20,6);
> a[a<10]<-(a[a<10])^2
> a
[1]  1  4 40 10  9 20 36

But if what you want to do to these numbers is much more complicated
then the previous (step-by-step) method may be best.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 17-Jul-03                                       Time: 12:30:19
------------------------------ XFMail ------------------------------



From M.Mamin at intershop.de  Thu Jul 17 13:50:29 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Thu, 17 Jul 2003 13:50:29 +0200
Subject: [R] line colors in lattice.xyplot with png device.
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF053@jena03.net.j.ad.intershop.net>


Hi,

R is very new for me, so excuse if my questions are too basic...

	BTW, are there any forum  where new R users could get help without 
	annoying this huge mailing list ?
	
	
In following code, I'd like to choose the color for each of the curve
diplayed.

png(filename = filename, width = 950, height = 600, pointsize = 10,  bg =
"white")
xyplot(HITS+MS1*3+FREQ~TIME   |SERVER*LOCAL, data=myd,allow.multiple = TRUE,
scales = "same", type="l")
dev.off()

another problem is that I don't get a white backgroud, which is quite
problematic when printing the output.
(I works on WIN2K)


and last but not least, is there a way to use 2 different y axis, i.e. curve
1& 2 => left axis, curve 3 => right axis ?


Sample codes would be welcome as I'm not yet used with the R syntax


Thanks,

Marc Mamin



From lutz.thieme at amd.com  Thu Jul 17 14:09:50 2003
From: lutz.thieme at amd.com (lutz.thieme@amd.com)
Date: Thu, 17 Jul 2003 14:09:50 +0200
Subject: [R] line length limitation in ROracle
Message-ID: <E540DF203FFED21182EB0008C728756010628D79@deexmta4.amd.com>

Hello everybody,

I found that queries (send by "dbExecStatement" ) with more than 4000 characters
length produces an error in ROracle (ver 0.3-3). Maybe there is a limitation of 4kB.... 
Is this a bug? If yes, is this problem solved in the latest version of ROracle (ver 0.5-0)?

My system information:

platform sparc-sun-solaris2.8
arch     sparc               
os       solaris2.8          
system   sparc, solaris2.8   
status                       
major    1                   
minor    5.1                 
year     2002                
month    06                  
day      17                  
language R      

Thank you in advance for your support!

	Kind regards,

	Lutz


	Lutz Thieme
	Product Engineering
	AMD Saxony Limited Liability Company & Co. KG
	M/S E22-PE, Wilschdorfer Landstr. 101
	D-01109 Dresden, Gemany
	phone:	+ 49-351-277 -  4269
	fax:		+ 49-351-277-9-4269



From JSmith at telicmanagement.com  Thu Jul 17 14:27:08 2003
From: JSmith at telicmanagement.com (John Smith)
Date: Thu, 17 Jul 2003 08:27:08 -0400
Subject: [R] duplicate row.names 
Message-ID: <3F62097DBC5C0C439F06B1DD2C393E035F28D0@saturn5.paloma.com>

row.names=NULL seems to have fixed my problem.  strange that NULL uses row
numbers and not specifying uses row numbers, but NULL poses no problems and
the latter does.

Thanks to you and everyone else who responded.

John

-----Original Message-----
From: Simon Blomberg [mailto:Simon.Blomberg at anu.edu.au]
Sent: Wednesday, July 16, 2003 9:50 PM
To: John Smith; r-help at stat.math.ethz.ch
Subject: RE: [R] duplicate row.names 


I think you need to be a bit more specific for us to be able to help you. At
least provide a toy problem which replicates the error. Does this do what
you want?

dat1 <- data.frame (x=rnorm(100), y=rnorm(100)) #make up some data
dat2 <- data.frame (x=rnorm(100), y=rnorm(100))
write.table(dat1, file="test1.txt") #write data to files
write.table(dat2, file="test2.txt")
F<- NULL # Initialise
for (i in 1:2) F <- rbind(F, read.table(paste("test", i, ".txt", sep="")))
F <- data.frame(F, row.names=NULL) # clean up row names.

Hope this helps,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: John Smith [mailto:JSmith at telicmanagement.com]
> Sent: Thursday, 17 July 2003 10:06 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] duplicate row.names 
> 
> 
> I am looping over many data files and reading in the data 
> with	 F <-
> read.table(filename)	to read in a 22000 by 15 matrix.  Works 
> fine on the
> first matrix F, but I get the following error when the second 
> file is read
> into F:
> 
> Error in "row.names<-.data.frame"(*tmp*, value = row.names) : 
>         duplicate row.names are not allowed
> 
> I have tried picking a column of the matrix and making that 
> my rownames by
> doing		rownames <- as.vector(F[,4])
> -but that does not work.
> 
> Each row in my matrix is not unique.
> 
> Any suggestions greatly appreciated.
> 
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dj at research.bell-labs.com  Thu Jul 17 14:57:17 2003
From: dj at research.bell-labs.com (David James)
Date: Thu, 17 Jul 2003 08:57:17 -0400
Subject: [R] line length limitation in ROracle
In-Reply-To: <E540DF203FFED21182EB0008C728756010628D79@deexmta4.amd.com>;
	from lutz.thieme@amd.com on Thu, Jul 17, 2003 at 02:09:50PM +0200
References: <E540DF203FFED21182EB0008C728756010628D79@deexmta4.amd.com>
Message-ID: <20030717085717.A28381@jessie.research.bell-labs.com>

Hi,

The message is

   > junk <- paste(sample(letters, size=5000, replace=T), collapse="")

   > rs <- dbSendQuery(con, junk)
   Error in oraPrepareStatement(con, statement, bind = NULL) : 
      RS-DBI driver: (too long a statement -- it must has less than 4000 chars)

so I wouldn't call it a bug -- the message clearly states what's causing
the problem.  You can easily change line 58 in RS-Oracle.h,

   #define RS_ORA_STATEMENT_LEN  4000    /* dynamic statement length */

to some other constant and re-install.  But make sure that the new limit
you pick doesn't overflow any Oracle's own buffers -- for this you'll
need to look at the Oracle Pro C/C++ Programmer's Reference Manual.

Regards,

--
David

lutz.thieme at amd.com wrote:
> Hello everybody,
> 
> I found that queries (send by "dbExecStatement" ) with more than 4000 characters
> length produces an error in ROracle (ver 0.3-3). Maybe there is a limitation of 4kB.... 
> Is this a bug? If yes, is this problem solved in the latest version of ROracle (ver 0.5-0)?
> 
> My system information:
> 
> platform sparc-sun-solaris2.8
> arch     sparc               
> os       solaris2.8          
> system   sparc, solaris2.8   
> status                       
> major    1                   
> minor    5.1                 
> year     2002                
> month    06                  
> day      17                  
> language R      
> 
> Thank you in advance for your support!
> 
> 	Kind regards,
> 
> 	Lutz
> 
> 
> 	Lutz Thieme
> 	Product Engineering
> 	AMD Saxony Limited Liability Company & Co. KG
> 	M/S E22-PE, Wilschdorfer Landstr. 101
> 	D-01109 Dresden, Gemany
> 	phone:	+ 49-351-277 -  4269
> 	fax:		+ 49-351-277-9-4269
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636



From a.CALANDRA at mclink.it  Thu Jul 17 15:21:05 2003
From: a.CALANDRA at mclink.it (Andrea Calandra)
Date: Thu, 17 Jul 2003 15:21:05 +0200 (CEST)
Subject: [R] Info
Message-ID: <1.0.2.200307171519.27963@mclink.it>

Sorry

I'm student in biomedical engineer and i have to solve this formula
for immuno-assay. I need to design a calibration curve

But i don't understand How can i write this formula in R language:
y = a + (c - a) /(1+ e[-b(x-m])

where
x = ln(analyte dose + 1)
y = the optical absorbance data
a = the curves top asymptote
b = the slope of the curve
c = the curves bottom asymptote
m = the curve X intercept

I have to calculate the parameters (a,b,c,m).After with X that i know
i calculate the Y.


i try:


yeld.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
 data = yeld,
 start = list( a= 0, c.=2, b= 1, m=4 ),
 trace = TRUE )


where yeld is a data.frame
  x y
1 1 1
2 2 2
3 3 3
4 4 4
5 5 5


but give me an error: << exceeded number of itwerations>>

thank you
Andrea



From tblackw at umich.edu  Thu Jul 17 15:55:12 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 17 Jul 2003 09:55:12 -0400 (EDT)
Subject: [R] Info
In-Reply-To: <1.0.2.200307171519.27963@mclink.it>
Message-ID: <Pine.SOL.4.44.0307170954040.28117-100000@robotron.gpcc.itd.umich.edu>

Please DO READ the section "Details" in help("nls").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From felber at slf.ch  Thu Jul 17 15:53:59 2003
From: felber at slf.ch (Andi Felber)
Date: Thu, 17 Jul 2003 15:53:59 +0200
Subject: [R] Hosmer- Lemeshow test
Message-ID: <5.1.0.14.1.20030717154838.028de6f0@mail.slf.ch>

dear all,

i made some analysis with logistic regression. do you know if there is 
already a function available for the hosmer-lemeshow fitness test?

thank you very much for the help

best wishes

andi



From grimm.heinz at rcc.ch  Thu Jul 17 15:58:20 2003
From: grimm.heinz at rcc.ch (Heinz Grimm)
Date: Thu, 17 Jul 2003 15:58:20 +0200
Subject: [R] Fatal error in SJava.
Message-ID: <3F16AB7C.F3168A24@rcc.ch>

>Dear r-helpers,

>I have been trying to invoke R from Java in a Windows 2000 computer 
>(unfortunately). All my environment variables seem to be properly set, 
>everything seems to be in order, but I obtaining a

>Fatal error: unable to open the base package

>error window.

[...]

>Some time ago I found a number of messages by another person facing the 
>same problems and he said that when R was invoked directly, it could 
>properly read the R_HOME variable, but when invoked from Java, it 
>searched for the base package in the wrong place (a subdirectory of the 
>jre, if my memory is to be trusted).

This was filed as bug 2003. I have exchanged some emails with Professor
Ripley and he convinced me, that it's not a bug in R. The problem is
caused
by SJava, that doesn't invoke R as described in src/gnuwin32/front-ends.
I have appended a new version of REmbedWin.c, based on the samples in
src/gnuwin32/front-ends, that solves this problem. You just have to
recompile
SJava with the new version of REmbedWin.c. I have tested it with
JDK1.3.1 on
Windows 2000, not with JDK1.4.1.


Regards,
Heinz Grimm




/* REmbedWin.C
 * avoids problem with loading base package
 */
/* 27/03/2000 win32-api needs this */
#define NONAMELESSUNION
#include <windows.h>
#include <stdio.h>
#include <config.h>
#include <Rversion.h>
#include <Startup.h>
/* for askok and askyesnocancel */
#include <graphapp/graphapp.h>

/* for signal-handling code */
#include <psignal.h>

/* one way to allow user interrupts: called in ProcessEvents */
#ifdef _MSC_VER
__declspec(dllimport) int UserBreak;
#else
#define UserBreak     (*_imp__UserBreak)
extern int UserBreak;
#endif

/* calls into the R DLL */
extern char *getDLLVersion();
extern void R_DefParams(Rstart);
extern void R_SetParams(Rstart);
extern void setup_term_ui(void);
extern void ProcessEvents(void);
extern void end_Rmainloop(void), R_ReplDLLinit(void);
extern int R_ReplDLLdo1();
extern void run_Rmainloop(void);


/* simple input, simple output */

/* This version blocks all events: a real one needs to call
ProcessEvents
   frequently. See rterm.c and ../system.c for one approach using
   a separate thread for input */
int myReadConsole(char *prompt, char *buf, int len, int addtohistory)
{
    fputs(prompt, stdout);
    fflush(stdout);
    if(fgets(buf, len, stdin)) return 1;
    else return 0;
}

void myWriteConsole(char *buf, int len)
{
    printf("%s", buf);
}

void myCallBack()
{
    /* called during i/o, eval, graphics in ProcessEvents */
}

void myBusy(int which)
{
    /* set a busy cursor ... if which = 1, unset if which = 0 */
}

static void my_onintr(int sig)
{
    UserBreak = 1;
}

void winInit_R()
{
  char *argv[] = {"SJava", "--no-save", "--silent"};
  int argc;
  argc = sizeof(argv)/sizeof(argv[0]);

  structRstart rp;
  Rstart Rp = &rp;
  char Rversion[25], RUser[MAX_PATH], RHome[MAX_PATH], *p;

  sprintf(Rversion, "%s.%s", R_MAJOR, R_MINOR);
  if(strcmp(getDLLVersion(), Rversion) != 0)
  {
    fprintf(stderr, "Error: R.DLL version does not match\n");
    exit(1);
  }

  R_DefParams(Rp);
  if(getenv("R_HOME"))
  {
    strcpy(RHome, getenv("R_HOME"));
  }
  else
  {
    fprintf(stderr, "R_HOME must be set\n");
    exit(1);
  }
  Rp->rhome = RHome;
  /*
   * try R_USER then HOME then working directory
   */
  if (getenv("R_USER"))
  {
    strcpy(RUser, getenv("R_USER"));
  } else
  if (getenv("HOME"))
  {
    strcpy(RUser, getenv("HOME"));
  } else
  if (getenv("HOMEDIR"))
  {
    strcpy(RUser, getenv("HOMEDIR"));
    strcat(RUser, getenv("HOMEPATH"));
  } else
  {
    GetCurrentDirectory(MAX_PATH, RUser);
  }
  p = RUser + (strlen(RUser) - 1);
  if (*p == '/' || *p == '\\') *p = '\0';
  Rp->home = RUser;
  Rp->CharacterMode = LinkDLL;
  Rp->ReadConsole = myReadConsole;
  Rp->WriteConsole = myWriteConsole;
  Rp->CallBack = myCallBack;
  Rp->message = askok;
  Rp->yesnocancel = askyesnocancel;
  Rp->busy = myBusy;

  Rp->R_Quiet = TRUE;
  Rp->R_Interactive = FALSE;
  Rp->RestoreAction = SA_RESTORE;
  Rp->SaveAction = SA_NOSAVE;
  Rp->CommandLineArgs = NULL;
  Rp->NumCommandLineArgs = 0;
  /* Rp->nsize = 300000;
     Rp->vsize = 6e6; */
  R_SetParams(Rp); /* so R_ShowMessage is set */
  R_SizeFromEnv(Rp);
  R_SetParams(Rp);

  FlushConsoleInputBuffer(GetStdHandle(STD_INPUT_HANDLE));

  signal(SIGBREAK, my_onintr);
  setup_term_ui();
  setup_Rmainloop();
  return;
}

This e-mail transmission contains confidential or legally privileged
information that is intended for the addressee(s) only. You are hereby
notified that any disclosure, copying, distribution or use of the
contents of this e-mail is strictly prohibited if you are not the
intended recipient. Please inform the sender and delete the message from
your system if you have received this e-mail transmission in error.
Thank you.



From mn216 at columbia.edu  Thu Jul 17 15:55:53 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 17 Jul 2003 09:55:53 -0400
Subject: [R] Formal definitions of R-language.
References: <3F16E059.8020700@hppi.troitsk.ru>
Message-ID: <3F16AAE9.CEF96C11@columbia.edu>



some comments. I am still learning S/R so please let me know if I am
missing something.

"M.Kondrin" wrote:
> 
> Hello!
> Some CS-guys (the type who knows what Church formalism is) keep asking
> me questions about formal definitions of R-language that I can not
> answer (or even understand). Is there some freely available papers which
> I can throw at them where it would be explained is R
> functional/OOP/procedural language,

R is object oriented in the sense that the basic entities of the
language (data elements, language elements etc.) are complex entities
with type and defined behavior (objects). it has inheritance,
polymorphism, operator overloading etc. you can define constructors for
objects, but as far as I know no destructors (you can define clean up
routines on libraries though).

it has has elements of being a functional language like the fact that
programs are composed of expressions that are turned into function
objects that get evaluated. it also supports lambda expressions. it is
not a pure functional language because functions can and do have side
effects, it has persistent state and assignments, and it has flow of
control statements. also, recursion, as far as I know, is inefficient in
S/R. which tend to discourage purely functional programming.

> does it use weak/strong,

weak typing. variables are not typed and keep type information once
constructed. conversion between types is often automatic or can be
programmed to be so, hence operations on disparate types can often be
carried out.

> dynamic/static typization,

it is dynamically typed. objects carry and supply type information at
run time. types (as well as behavior) can (only) be defined at run time.
the S-evaluator has to start first, it then constructs class and
function definitions in the run-time environment. object type can be
changed, modified and augmented at run time, at least with old style
classes, (can you add or remove slots in the new style classes?).

> does it use lazy or ...(do not know what)
> evaluation,

uses lazy evaluation of expressions. expressions are constructed by the
S-evaluator, but not evaluated until needed.

> what sort of garbage collector it uses?

No garbage collector. uses reference counting to discard objects that
are no longer needed.


> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From tlumley at u.washington.edu  Thu Jul 17 15:59:32 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 17 Jul 2003 06:59:32 -0700 (PDT)
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F171016.5040201@hppi.troitsk.ru>
Message-ID: <Pine.A41.4.44.0307170633470.124274-100000@homer07.u.washington.edu>

On Thu, 17 Jul 2003, M.Kondrin wrote:

> Douglas Trainor wrote:
> >
> > Tell the "CS-guys" to grab the source code and chew on the LALR
> > context-free grammar stuff in the file "gram.y" as in:
> >
> >    R-1.7.1/src/main/gram.y
> >
> > "R Language Definition" lives at:
> >
> >    http://cran.r-project.org/doc/manuals/R-lang.pdf
> >
> >
> Thanks for your answers!

I don't think the grammar will actually answer their questions -- it
doesn't specify a lot of the things you (they) are interested in.

R doesn't really conform to purist classifications of languages (I tell
C++ programmers that they would be happier if they think of the old method
system as providing operator overloading rather objects). Luke Tierney
would probably be the best person to reply but here's a stab at what they
want:

R has dynamic typing, and static lexical scoping.  Functions are
first-class objects but assignments to change variables are allowed and
widely used.  It has lazy evaluation of arguments. It has a non-copying
garbage collector but does not have continuations or tail recursion
optimisation.  Operations that would naturally be written using dynamic
scope or macros are faked by constructing expressions and evaluating them
in other than the default environment.

There are two object systems.  Both provide polymorphism and (multiple)
inheritance but do not restrict access to the internal structure of
objects.

Documentation for some things (methods, namespaces, the garbage collector)
is at http://developer.r-project.org


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.dalgaard at biostat.ku.dk  Thu Jul 17 16:08:54 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 17 Jul 2003 14:08:54 -0000
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F165FD5.1090907@transborder.org>
References: <3F16E059.8020700@hppi.troitsk.ru>
	<3F164B48.8050403@statistik.uni-dortmund.de>
	<3F165FD5.1090907@transborder.org>
Message-ID: <x2llux6y45.fsf@biostat.ku.dk>

[Oops. Accidentally sent only to D.Trainor first time around, even
though it was intended mainly for M.Kondrin and the list]

Douglas Trainor <trainor at transborder.org> writes:

> Uwe Ligges wrote:
> 
> > M.Kondrin wrote:
> >
> >> Hello!
> >> Some CS-guys (the type who knows what Church formalism is) keep
> >> asking me questions about formal definitions of R-language that I
> >> can not answer (or even understand). Is there some freely available
> >> papers which I can throw at them where it would be explained is R
> >> functional/OOP/procedural language, does it use weak/strong,
> >> dynamic/static typization, does it use lazy or ...(do not know
> >> what) evaluation, what sort of garbage collector it uses?
> >> Thanks.
> >
> >
> > R ships with a draft version of the manual "R Language Definition".
> > Another source is Venables & Ripley (2000): S Programming, Springer.
> 
> 
> Tell the "CS-guys" to grab the source code and chew on the LALR
> context-free grammar stuff in the file "gram.y" as in:
> 
>     R-1.7.1/src/main/gram.y

Don't. You'll get laughed out... The grammar of the language is
essentially unrelated to the kind of categorizations CS people are
looking for.

Probably, Robert Gentleman or Luke Tierney are the guys best qualified
to answer the question, but I can give a try:

R is quite close to being Scheme plus "syntactic sugar". Lexical
scoping and function closures comes directly from Scheme and the
internal structure of calls and functions (i.e. that they are
equivalent to lists so that you can do things like
 
  e <- quote(2+2) ; lapply(e,deparse)

R is a functional language, with lazy evaluation and weak dynamic
typing (a variable can change type at will: a <- 1 ; a <- "a" is
allowed). Semantically, everything is copy-on-modify although some
optimization tricks are used in the implementation to avoid the worst
inefficiencies.

Parameter passing is according to the "pass-by-value illusion", i.e.
what is really getting passed down to a function is a "promise", which
embodies the expression used in the call. This is at the core of the
lazy evaluation mechanism: The result of the expression is not
computed until needed. It also allows a function to get hold of the
the expression itself: This is useful for labeling plots but it also
allows some variants of "pass-by-name"-like semantics via evaluation
in the environment of the caller.

R is not object oriented in the same sense as Java or C++. We don't
(generally) have methods that are semantically part of objects. However,
we do have class-based function dispatch and generic functions, so
that a function can do different things of different kind of objects,
and - with the S4 class system - also for combinations of objects.

Other distinctive features are that basic operations are vectorized,
and that (somewhat perversly to LISP programmers) the traditional
"dotted-pair" list lives quite deeply hidden from users, whereas the
thing called "list" in R is really a generic vector where each element
can be of different type. [This is not the only place where we have
some rather unfortunate clashes in terminology between the S language
(which is historically of APL descent) and the Scheme-like engine
underneath.]

Some implementation stuff is documented in sketches on
developer.r-project.org (do read the opening paragraph!) and the
intention is to have it documented in the R Language Definition,
although that is still rather incomplete (and as Thomas Lumley once
put it, "it would be desirable if one could truthfully say that it is
a work in progress"). Also of course, there is the paper by Ross and
Robert in JCGS.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Jul 17 16:20:50 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 17 Jul 2003 14:20:50 -0000
Subject: [R] Formal definitions of R-language.
In-Reply-To: <3F16AAE9.CEF96C11@columbia.edu>
References: <3F16E059.8020700@hppi.troitsk.ru> <3F16AAE9.CEF96C11@columbia.edu>
Message-ID: <x2he5l6xk6.fsf@biostat.ku.dk>

Murad Nayal <mn216 at columbia.edu> writes:

[Sensible stuff, except:]
> > what sort of garbage collector it uses?
> 
> No garbage collector. uses reference counting to discard objects that
> are no longer needed.

No, R uses garbage collection with a "generational" scheme which looks
more frequently at younger objects. The vector heap is handled with
standard malloc-style memory allocation techniques (we once had code
that would compact the heap during garbage collection, but it was
found to be logically inconsistent: if you called a C routine and it
called back into R and triggered a GC, it could happen that the C
pointers were no longer pointing to the vectors they originally did).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tblackw at umich.edu  Thu Jul 17 16:25:21 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 17 Jul 2003 10:25:21 -0400 (EDT)
Subject: [R] Help on NNET
In-Reply-To: <20030716222515.16744.qmail@sina.com>
Message-ID: <Pine.SOL.4.44.0307171017450.28117-100000@robotron.gpcc.itd.umich.edu>


I'm not a nnet() user, but after reading the help, there are
two things you might try, both in the model formula passed to
nnet() in the third line of code below.  Alternate version:

pupil.nn2 <- nnet( as.factor(Type) ~ X1 + X2, ...
	 	(with all the remaining arguments as before).

I'm not sure which (if either) of these two variants might do
the trick.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 17 Jul 2003, sbinny wrote:

> The following is the script:
>
> read.csv('pupil.txt',header=TRUE,sep='\t')->pupil
> samp<-c(1:50, 112:162, 171:220, 228:278)
> pupil.nn2 <- nnet(Type ~ ., data = pupil, subset = samp, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
> table(pupil$Type[-samp], predict(pupil.nn2, pupil[-samp,], type = "class"))
>
> There are totally 351 observations.
> My objective is to classify them into 4 classes.
>
> Thanks a lot for your help!



From vito.muggeo at giustizia.it  Thu Jul 17 16:32:05 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 17 Jul 2003 16:32:05 +0200
Subject: R: [R] Hosmer- Lemeshow test
References: <5.1.0.14.1.20030717154838.028de6f0@mail.slf.ch>
Message-ID: <011201c34c70$3537bfc0$5c13070a@GIUSTIZIA.IT>

As far as I know, the Hosmer-Lemeshow test is not a good choice, as it
depends on the groups to be formed to compute the statistic.
The design and/or hmisch  packages by F.Harrell should include some
alternative GoF statistics, but I didn't try.

Some weeks ago there has been a similar message about GoF test with sparse
date in logistic regression model. You could look for it and contact the
author....

best,
vito



----- Original Message -----
From: Andi Felber <felber at slf.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 17, 2003 3:53 PM
Subject: [R] Hosmer- Lemeshow test


> dear all,
>
> i made some analysis with logistic regression. do you know if there is
> already a function available for the hosmer-lemeshow fitness test?
>
> thank you very much for the help
>
> best wishes
>
> andi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ebrington at hotmail.com  Thu Jul 17 16:58:09 2003
From: ebrington at hotmail.com (Steve Moore)
Date: Thu, 17 Jul 2003 15:58:09 +0100
Subject: [R] Rather open question
Message-ID: <Law12-F93PPu1OOLMgn00019e54@hotmail.com>

Dear All,

I have a question that I was hoping someone may be able to shed some light 
on.  If I wanted to analyse 1000-2000 arrays in R, how much computing power 
would I need in order for the program to run O.K.  Any help that anybody 
could give me would be greatly appreciated.

Many Thanks

Steve.



Dr. Stephen Moore
Dept. of Oncology
Queens University of Belfast
U-Floor
Belfast City Hospital
Belfast
N.Ireland
BT9 7AB

_________________________________________________________________
On the move? Get Hotmail on your mobile phone http://www.msn.co.uk/msnmobile



From spencer.graves at pdf.com  Thu Jul 17 17:02:00 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Jul 2003 08:02:00 -0700
Subject: [R] Info
References: <1.0.2.200307171519.27963@mclink.it>
Message-ID: <3F16BA68.8070306@pdf.com>

	  Thanks for providing data and sample command.  (In the future, please 
provide the data in the form of a "data.frame" commmand.  It is not as 
easy to read visually, but it is easier for others to copy into R.)

	  Please follow Doug Bates' advice:

	  1.  Plot the data.

	  2.  Play with the nonlinear formula to understand what the particular 
coefficients do.

	  3.  Use algorithm = "plinear".

	  ** If you did the above, you would find out that you "data" are 
entirely linear.  What values for the parameters to you need to make 
your nonlinear formula (approximately) linear?  I don't have time to 
analyze it right now, but you may need to send b to 0 and c. to Inf. 
Compute a second-order Taylor approximation to your formula to find out.

	  Also, please set "trace=TRUE":  Then the algorithm will tell you what 
it is trying to do with the parameter estimates.

	  You are persistent, Andrea:  You will get an answer.

Best Wishes,
Spencer Graves

Andrea Calandra wrote:
> Sorry
> 
> I'm student in biomedical engineer and i have to solve this formula
> for immuno-assay. I need to design a calibration curve
> 
> But i don't understand How can i write this formula in R language:
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> I have to calculate the parameters (a,b,c,m).After with X that i know
> i calculate the Y.
> 
> 
> i try:
> 
> 
> yeld.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
>  data = yeld,
>  start = list( a= 0, c.=2, b= 1, m=4 ),
>  trace = TRUE )
> 
> 
> where yeld is a data.frame
>   x y
> 1 1 1
> 2 2 2
> 3 3 3
> 4 4 4
> 5 5 5
> 
> 
> but give me an error: << exceeded number of itwerations>>
> 
> thank you
> Andrea
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From cafa at ime.unicamp.br  Thu Jul 17 17:04:35 2003
From: cafa at ime.unicamp.br (Cezar Augusto de Freitas Anselmo)
Date: Thu, 17 Jul 2003 12:04:35 -0300 (BRT)
Subject: [R] C compiler to R
Message-ID: <Pine.GSO.4.05.10307171203560.23871-100000@athenas.ime.unicamp.br>

Hi, all.
I'd like know which is the more appropriate C compiler to use with R
programs.
Thanks,
C.

========================================
Cezar Freitas (ICQ 109128967)
IMECC - UNICAMP
Campinas, SP - Brasil



From sigma at consultoresestadisticos.com  Thu Jul 17 17:28:45 2003
From: sigma at consultoresestadisticos.com (Carlos J. Gil Bellosta)
Date: Thu, 17 Jul 2003 10:28:45 -0500
Subject: [R] C compiler to R
References: <Pine.GSO.4.05.10307171203560.23871-100000@athenas.ime.unicamp.br>
Message-ID: <3F16C0AD.9040406@consultoresestadisticos.com>

gcc (I just need to write that; I tried Borland's and I only got trouble).

Sincerely,

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com


Cezar Augusto de Freitas Anselmo wrote:

>Hi, all.
>I'd like know which is the more appropriate C compiler to use with R
>programs.
>Thanks,
>C.
>
>========================================
>Cezar Freitas (ICQ 109128967)
>IMECC - UNICAMP
>Campinas, SP - Brasil
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From djw1005 at cam.ac.uk  Thu Jul 17 17:34:22 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Thu, 17 Jul 2003 16:34:22 +0100 (BST)
Subject: [R] Excel can do what R can't?????
In-Reply-To: <1058368708.3f156cc41d4e2@webmail.utm.utoronto.ca>
Message-ID: <Pine.SOL.3.96.1030717155817.5099A-100000@virgo.cus.cam.ac.uk>


Michael Rennie wrote:
> Last, it's not even that I'm getting error messages anymore- I just
> can't get the solution that I get from Excel.  If I try to let R find
> the solution, and give it starting values of c(1,2), it gives me an
> optimization solution, but an extremely poor one.  However, if I give it
> the answer I got from excel, it comes right back with the same answer
> and solutions I get from excel. 
> 
> Using the 'trace' function, I can see that R gets stuck in a specific
> region of parameter space in looking for the optimization and just
> appears to give up.  Even when it re-set itself, it keeps going back to
> this region, and thus doesn't even try a full range of the parameter
> space I've defined before it stops and gives me the wrong answer. 

1. Either your function or the Excel solver is wrong. I executed your
source code (which defines f), then ran it over a grid of points, and
plotted the answer, using this code:

xvals <- seq(.2,2,by=.2)
yvals <- seq(1,3,by=.2)
z <- matrix(NA,nrow=length(xvals),ncol=length(yvals))
for (i in 1:length(xvals)) for (j in 1:length(yvals)) {
  x <- xvals[i]
  y <- yvals[j]
  z[i,j] <- f(c(x,y))
  }
filled.contour(x=xvals,y=yvals,z=log(z))

Your "solution" from Excel evaluates to
  f(c(.558626306252032,1.66764519286918)) == 0.3866079
while I easily found a point which was much better,
  f(c(.4,1)) = 7.83029e-05

You should have tried executing your function over a grid of points, and
plotting the results in a contour plot, to see if optim was working
sensibly. You could do the same grid in Excel and R to verify that the
function you've defined does the same thing in each.

Since your optimization is only over a 2D parameter space, it is easy for
you to plot the results, to see at a glance what the optimum is, and to
work out what is going wrong.

2. Your code executes very slowly because it is programmed inefficiently.
You need to iterate a function to get your final solution, but you don't
need to keep track of all the states you visit on the way. The way R
works, whenever you assign a value to a certain index in a vector, as in 
  A[i] <- 10,
the system actually copies the entire vector. So, in every iteration, you
are copying very many vectors, and this is needlessly slowing down the
program. Also, at the end of each iteration, you define
  bio <- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
which creates a matrix. But you only ever use this matrix right at the
end, and so there is no need to create this 365*14 matrix at every single
iteration.

It looks to me as if you took some Excel code and translated it directly
into R. This will not produce efficient R code. Your iterative loop would
be more naturally expressed in R as

f <- function(q) {
  p <- q[[1]]
  ACT <- q[[2]]
  # cat(paste("Trying p=",p," ACT=",ACT,"\n",sep=""))
  state <- c(W=Wo,Hg=Hgo)
  numdays <- length(temps)
  for (i in 1:numdays)
    state <- updateState(state,
                         jday=temps$jday[i],temp=temps$Temp[i],M=numdays,
                         p=p,ACT=ACT)
  Wtmod <- state[["W"]]
  Hgtmod <- state[["Hg"]]
  (Wt-Wtmod)^2/Wt + (Hgt-Hgtmod)^2/Hgt
  }

updateState <- function(state,jday,temp,M,p,ACT) {
  # Given W[i-1] and Hg[i-1], want to compute W[i] and Hg[i]
  W <- state[["W"]]
  Hg <- state[["Hg"]]
  # First compute certain parameters: Vc[i-1] ... Expegk[i-1]
  Vc <- (CTM-temp)/(CTM-CTO)
  Vr <- (RTM-temp)/(RTM-RTO)
  C <-      p * CA * W^CB * Vc^Xc * exp(Xc*(1-Vc)) * Pc
  ASMR <- ACT * RA * W^RB * Vr^Xa * exp(Xa*(1-Vr))
  ...
  # Now find W[i] and Hg[i]
  Wnew <- if (!(jday==121 && Mat==1)) W+Gr/Ef
          else                        W * (1-GSI*1.2)
  Hgnew <- a*Hgp*C*(1-Expegk)/(Pc*W*EGK) + Hg*Expegk
  c(W=Wnew,Hg=Hgnew)
  }

In this code, I do not attempt to keep the entire array in memory. All I
need to know at each iteration is the value of state=(W,Hg) at time i-1,
and from this I compute the new value at time i.

3. You use some thoroughly weird code to read in a table. You should add a
row to the top of your table with variable names, then just use
  temps <- read.table("TEMP.DAT", header=TRUE)
  temps$Vc <- (CTM-temps$temp)/(CTM-CTO)
This would also avoid leaving global variables (like Day) hanging around
the place. Global variables cause confusion: see the next point.

4. Here are some lines taken from your code.

p <- NULL
ACT <- NULL

#starting values for p, ACT
p <- 1
ACT <- 2

f <- function (q)
  {
  F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
  # (and ACT is never referred to)
  }

Why did you define p<-NULL and ACT<-NULL at the top? Those definitions are
irrelevant, because they are overridden by p<-1 and ACT<-2.

In the body of your function f, in defining F[i], you refer to the
variable p. The only assignment to p is in the line p<-1. I strongly
suspect this is an error. Probably you want to refer to q[1]. The best way
to do this (as you can see from my code above) is to define p and ACT at
the beginning of f.

5. Some minor comments on code. It's unwise to use T or F as variable
names in R, because of the potential for confusion with S-Plus, which uses
them for TRUE and False. Also, you don't need all those brackets: A*(B*C)
is the same as A*B*C, and ((A/B)/C) is more transparently written as
A/(B*C). Also, you should indent your code, since otherwise you'll just
confuse yourself and other people.

6. I've written a version of the code which takes all these comments into
account. It doesn't agree with your Excel solution. You haven't given us
enough real data for me to work out if there's a bug in my code or if the
Excel solution is wrong. Once you have worked out a function f which you
know to be correct (checked by drawing a contour plot), if you have any
more problems, share it with us and we may be able to help. 

Damon Wischik.



From dmurdoch at pair.com  Thu Jul 17 18:00:57 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 17 Jul 2003 12:00:57 -0400
Subject: [R] Rather open question
In-Reply-To: <Law12-F93PPu1OOLMgn00019e54@hotmail.com>
References: <Law12-F93PPu1OOLMgn00019e54@hotmail.com>
Message-ID: <kuhdhvonsle9danqkcj4cl3ulq0vlbrg95@4ax.com>

On Thu, 17 Jul 2003 15:58:09 +0100, "Steve Moore"
<ebrington at hotmail.com> wrote :

>Dear All,
>
>I have a question that I was hoping someone may be able to shed some light 
>on.  If I wanted to analyse 1000-2000 arrays in R, how much computing power 
>would I need in order for the program to run O.K.  Any help that anybody 
>could give me would be greatly appreciated.

I think the question is *too* open.  What kind of arrays are you
looking at?  What analysis do you want to do?   Probably the easiest
way to answer your question is to analyze a few arrays, and
extrapolate.  

Duncan Murdoch



From bates at stat.wisc.edu  Thu Jul 17 18:11:28 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jul 2003 16:11:28 -0000
Subject: [R] Rather open question
In-Reply-To: <Law12-F93PPu1OOLMgn00019e54@hotmail.com>
References: <Law12-F93PPu1OOLMgn00019e54@hotmail.com>
Message-ID: <6ru19l16bt.fsf@bates4.stat.wisc.edu>

"Steve Moore" <ebrington at hotmail.com> writes:

> I have a question that I was hoping someone may be able to shed some
> light on.  If I wanted to analyse 1000-2000 arrays in R, how much
> computing power would I need in order for the program to run O.K.  Any
> help that anybody could give me would be greatly appreciated.

Are the arrays that you speak of microarrays, such as manufactured by
Affymetrix?  If so, it may be a good idea also to send your question to
the bioconductor mailing list <bioconductor at stat.math.ethz.ch>.

I expect that any answers to your question will require you to be more
specific about how you plan to analyze your data.  Because R works "in
memory" the primary bottleneck in using R on large datasets is the
amount of memory that you have available on the computer.  A typical
Linux or Windows workstation or server can address up to 4GB of memory
and you could expect to buy computers with 3GB or 4GB memory for a
small fraction of what 1000-2000 Affymetrix chips and the preparation
of your samples will cost.  More than 4GB will require switching to
processors other than Pentium and Athlon.  One interesting possibility
is the newly introduced AMD Opteron which is a 64-bit processor that
uses an extension of the x86 instruction set.

This is not to say that memory will be the only issue.  As I said
above, it will be necessary to have some idea of what you plan to do
before meaningful advice can be offered.



From Joke.Allemeersch at esat.kuleuven.ac.be  Thu Jul 17 17:58:25 2003
From: Joke.Allemeersch at esat.kuleuven.ac.be (Joke Allemeersch)
Date: Thu, 17 Jul 2003 17:58:25 +0200
Subject: [R] univariate normal mixtures 
Message-ID: <3F16C7A1.9000306@esat.kuleuven.ac.be>

Hello,

I have a concrete statistical question:
I have a sample of an univariate mixture of an unknown number (k) of 
normal distributions, each time with an unknown mean `m_i' and a 
standard deviation `k * m_i', where k is known factor constant for all 
the normal distributions. (The `i' is a subscript.)
Is there a function in R that can estimate the number of normal 
distributions k and the means `m_i' for the different normal 
distributions from a sample?  Or evt. a function that can estimate the 
`m_i', when the number of distributions `k' is known?
So far I only found a package, called `normix'.  But at first sight it 
only provides methods to sample from such distributions and to estimate 
the densities; but not to fit such a distribution.
Can someone indicate where I can find an elegant solution?

Thank you in advance

Joke Allemeersch

Katholieke universiteit Leuven.
Belgium.



From B.Rowlingson at lancaster.ac.uk  Thu Jul 17 18:22:05 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 17 Jul 2003 17:22:05 +0100
Subject: [R] C compiler to R
In-Reply-To: <3F16C0AD.9040406@consultoresestadisticos.com>
References: <Pine.GSO.4.05.10307171203560.23871-100000@athenas.ime.unicamp.br>
	<3F16C0AD.9040406@consultoresestadisticos.com>
Message-ID: <3F16CD2D.8040000@lancaster.ac.uk>

Carlos J. Gil Bellosta wrote:

> gcc (I just need to write that; I tried Borland's and I only got trouble).

  Correct but possibly not sufficiently precise (where's Brian Ripley 
when we need him?).

  You should use the same C compiler that was used to compiler R in the 
first place.

  On Windows platforms this is the gcc compiler from the Mingw port of gcc.

  See the file src/gnuwin32/INSTALL in the R source directory.

Baz



From bates at stat.wisc.edu  Thu Jul 17 18:39:04 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jul 2003 16:39:04 -0000
Subject: [R] Info
In-Reply-To: <1.0.2.200307171519.27963@mclink.it>
References: <1.0.2.200307171519.27963@mclink.it>
Message-ID: <6r65m1151q.fsf@bates4.stat.wisc.edu>

1) Please plot your data.  Notice that they are of the form y = x.
   That is, they fall on a straight line.

2) Please plot the curve that you expect to fit.  You could do this,
   as I have said in previous replies, by

library(nls)
example(SSfpl)

3) Please plot the four-parameter logistic with the values of the
   parameters that you are using as starting estimates.  You could do
   this by 

x = seq(0, 5, by = 0.1)
plot(x, 0+(2-0)/(1+exp(-1.*(x-4.))), type = 'l')

4) Please look at this plot.  Notice where the points (1,1), (2,2),
   (3,3), (4,4), and (5,5) would fall.  The curve that you are using
   as a starting estimate is nowhere close to your data.  

5) Please plot your data.  Print out the plot and draw by eye a
   four-parameter logistic curve that you would feel would be a
   reasonable interpolation curve.  Pay particular attention to the
   position of the asymptotes.  Because your data fall exactly on a
   straight line there is no possible way of determining the positions
   of asymptotes.

6) Please read the replies that you have received from me and from
   others to your previous statements of this same question.  We spent
   our time writing those explanations for you.  Please do us the
   courtesy of reading these explanations.

In this restatement of your question you state that nls could not fit
this model to these data.  As I am trying to get you to realize, the
reason for this is that there is no best fit of a four-parameter
logistic model to such data.  As you yourself write, the curve that
you are trying to fit has a "top asymptote" and a "bottom asymptote".
Your data are of the form y = x.  They have no curvature.  They fall
exactly on a straight line.  As a consequence there is absolutely no
information available from these data that can be used to estimate the
positions of asymptotes.  In fact I believe that the theoretical
values of a and c are -Inf and +Inf and the value of m is undefined.
(I think you have the designations of "top asymptote" and "bottom
asymptote" backwards.)

It is not a deficiency in R and in the abilities of the people who
respond to you on this list that we have not been able to give you a
piece of R code that does the impossible.  Continuing to ask the same
question several more times will not change this.  Instead of sending
the same question again please consult with someone at your
institution to have them explain why you cannot estimate four
parameters from data that fall on a straight line.

Andrea Calandra <a.CALANDRA at mclink.it> writes:

> Sorry
> 
> I'm student in biomedical engineer and i have to solve this formula
> for immuno-assay. I need to design a calibration curve
> 
> But i don't understand How can i write this formula in R language:
> y = a + (c - a) /(1+ e[-b(x-m])
> 
> where
> x = ln(analyte dose + 1)
> y = the optical absorbance data
> a = the curves top asymptote
> b = the slope of the curve
> c = the curves bottom asymptote
> m = the curve X intercept
> 
> I have to calculate the parameters (a,b,c,m).After with X that i know
> i calculate the Y.
> 
> 
> i try:
> 
> 
> yeld.fit <- nls( y ~ a + (c.-a)/(1+exp(-b*(x-m))),
>  data = yeld,
>  start = list( a= 0, c.=2, b= 1, m=4 ),
>  trace = TRUE )
> 
> 
> where yeld is a data.frame
>   x y
> 1 1 1
> 2 2 2
> 3 3 3
> 4 4 4
> 5 5 5
> 
> 
> but give me an error: << exceeded number of itwerations>>



From pbrunno at yahoo.co.uk  Thu Jul 17 18:51:12 2003
From: pbrunno at yahoo.co.uk (=?iso-8859-1?q?Bruno=20Paul=20Mmbando?=)
Date: Thu, 17 Jul 2003 17:51:12 +0100 (BST)
Subject: [R] glm.nb
Message-ID: <20030717165112.58139.qmail@web10108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030717/bd61f389/attachment.pl

From sigma at consultoresestadisticos.com  Thu Jul 17 18:52:05 2003
From: sigma at consultoresestadisticos.com (Carlos J. Gil Bellosta)
Date: Thu, 17 Jul 2003 11:52:05 -0500
Subject: [R] univariate normal mixtures
References: <3F16C7A1.9000306@esat.kuleuven.ac.be>
Message-ID: <3F16D435.7080306@consultoresestadisticos.com>

Well,

If k is known, you can use maximun likelihood to fit the weights, means, 
and sd's. The EM algorithm can be of help to solve the optimization 
problem. You would have to implement it yourself for your particular 
case, but I do not think it is big trouble.

Then you could estimate k using Bayesian formalism: from a reasonable a 
priory distribution on k=1, 2,... compute the posterior distributions 
using the densities obtained above, etc.

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com

Joke Allemeersch wrote:

> Hello,
>
> I have a concrete statistical question:
> I have a sample of an univariate mixture of an unknown number (k) of 
> normal distributions, each time with an unknown mean `m_i' and a 
> standard deviation `k * m_i', where k is known factor constant for all 
> the normal distributions. (The `i' is a subscript.)
> Is there a function in R that can estimate the number of normal 
> distributions k and the means `m_i' for the different normal 
> distributions from a sample?  Or evt. a function that can estimate the 
> `m_i', when the number of distributions `k' is known?
> So far I only found a package, called `normix'.  But at first sight it 
> only provides methods to sample from such distributions and to 
> estimate the densities; but not to fit such a distribution.
> Can someone indicate where I can find an elegant solution?
>
> Thank you in advance
>
> Joke Allemeersch
>
> Katholieke universiteit Leuven.
> Belgium.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From keef9490 at uidaho.edu  Thu Jul 17 19:13:33 2003
From: keef9490 at uidaho.edu (Robert Keefe)
Date: Thu, 17 Jul 2003 10:13:33 -0700 (PDT)
Subject: [R] univariate normal mixtures
In-Reply-To: <3F16D435.7080306@consultoresestadisticos.com>
References: <3F16C7A1.9000306@esat.kuleuven.ac.be>
	<3F16D435.7080306@consultoresestadisticos.com>
Message-ID: <Pine.GHP.4.51.0307171011570.12564@raptor.csrv.uidaho.edu>


I'll be in the lab in roughly 45 minutes, so I'll
boot Frothie back up. Egads, no one can access my
394 website!! What will they do??

R

_____________________________________________________

Motorcycles are everywhere!
Check twice; save a life.

Rob Keefe                 Lab: (208) 885-5165
M.S. student              Home: (208) 882-9749
University of Idaho
_____________________________________________________

On Thu, 17 Jul 2003, Carlos J. Gil Bellosta wrote:

> Well,
>
> If k is known, you can use maximun likelihood to fit the weights, means,
> and sd's. The EM algorithm can be of help to solve the optimization
> problem. You would have to implement it yourself for your particular
> case, but I do not think it is big trouble.
>
> Then you could estimate k using Bayesian formalism: from a reasonable a
> priory distribution on k=1, 2,... compute the posterior distributions
> using the densities obtained above, etc.
>
> Carlos J. Gil Bellosta
> Sigma Consultores Estad?sticos
> http://www.consultoresestadisticos.com
>
> Joke Allemeersch wrote:
>
> > Hello,
> >
> > I have a concrete statistical question:
> > I have a sample of an univariate mixture of an unknown number (k) of
> > normal distributions, each time with an unknown mean `m_i' and a
> > standard deviation `k * m_i', where k is known factor constant for all
> > the normal distributions. (The `i' is a subscript.)
> > Is there a function in R that can estimate the number of normal
> > distributions k and the means `m_i' for the different normal
> > distributions from a sample?  Or evt. a function that can estimate the
> > `m_i', when the number of distributions `k' is known?
> > So far I only found a package, called `normix'.  But at first sight it
> > only provides methods to sample from such distributions and to
> > estimate the densities; but not to fit such a distribution.
> > Can someone indicate where I can find an elegant solution?
> >
> > Thank you in advance
> >
> > Joke Allemeersch
> >
> > Katholieke universiteit Leuven.
> > Belgium.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From fharrell at virginia.edu  Thu Jul 17 19:20:37 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 17 Jul 2003 13:20:37 -0400
Subject: R: [R] Hosmer- Lemeshow test
In-Reply-To: <011201c34c70$3537bfc0$5c13070a@GIUSTIZIA.IT>
References: <5.1.0.14.1.20030717154838.028de6f0@mail.slf.ch>
	<011201c34c70$3537bfc0$5c13070a@GIUSTIZIA.IT>
Message-ID: <20030717132037.5787de74.fharrell@virginia.edu>

On Thu, 17 Jul 2003 16:32:05 +0200
Vito Muggeo <vito.muggeo at giustizia.it> wrote:

> As far as I know, the Hosmer-Lemeshow test is not a good choice, as it
> depends on the groups to be formed to compute the statistic.
> The design and/or hmisch  packages by F.Harrell should include some
> alternative GoF statistics, but I didn't try.

Correct.  Use the more powerful one degree of freedom test of Hosmer, le Cessie, et al:

f <- lrm( . . ., x=T, y=T)
resid(f, 'gof')

Frank

> 
> Some weeks ago there has been a similar message about GoF test with sparse
> date in logistic regression model. You could look for it and contact the
> author....
> 
> best,
> vito
> 
> 
> 
> ----- Original Message -----
> From: Andi Felber <felber at slf.ch>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, July 17, 2003 3:53 PM
> Subject: [R] Hosmer- Lemeshow test
> 
> 
> > dear all,
> >
> > i made some analysis with logistic regression. do you know if there is
> > already a function available for the hosmer-lemeshow fitness test?
> >
> > thank you very much for the help
> >
> > best wishes
> >
> > andi
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From spencer.graves at pdf.com  Thu Jul 17 19:34:43 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Jul 2003 10:34:43 -0700
Subject: [R] univariate normal mixtures
References: <3F16C7A1.9000306@esat.kuleuven.ac.be>
	<3F16D435.7080306@consultoresestadisticos.com>
Message-ID: <3F16DE33.3040604@pdf.com>

	  Did you search "www.r-help.org" -> Search -> "R Site Search" and 
S-News (Google -> S-News -> S-News Mailing List Archives)?

	  I recently estimated a normal mixture, 2 components, mean 0, 
different variances.  I had computational difficulties until I took the 
time to avoid under and overflows, preserving numerical precision.  I 
got strange answers in my application, which convinced me that I had a 
3-component mixture, not 2, but it was not sufficiently important for me 
to modify my code to allow more than 2 components.

	  The code I used follows, in case it might be of interest to someone.

hope this helps.  spencer graves
#######################
dnorm2.0 <-
function(x, log.sd1=0.5*log(var(x)/10),
	log.sd2=0.5*log(10*var(x)), logit.w2=0,
	output.all=F, log.p=T){
# 	log(dnorm(...)) for both densities
	lg.dn1 <- ((-0.5)*(log(2*pi)+(x/exp(log.sd1))^2)-log.sd1)
	lg.dn2 <- ((-0.5)*(log(2*pi)+(x/exp(log.sd2))^2)-log.sd2)
#	Adjust one to use a negative exponential
#  in the denominator for
#  w2 = exp(logit.w2)/(1+exp(logit.w2))
#     = 1/(1+exp(-logit.w2))
	if(logit.w2>=0){
		lg.dn1 <- (lg.dn1-logit.w2)
		logit.w2 <- (-logit.w2)
	}
	else
		lg.dn2 <- (lg.dn2+logit.w2)
#	Now write log(exp(lg.dn1)+exp(lg.dn2))
#   = lg12 + log(1+exp(lg.12-lg12))
	lg12 <- pmax(lg.dn1, lg.dn2)
	lg.12 <- pmin(lg.dn1, lg.dn2)
#	Put it all together	
	lg.d2 <- (lg12 + log(1+exp(lg.12-lg12))
		 + logit.w2)
	if(log.p)lg.d2 else exp(lg.d2)
}

lglk.dn2 <-
function(x=c(log.sd1=log(1e-15), log.sd2=0, logit.w2=log(35/(127-35))),
	data.){
#		
	dn <- dnorm2.0(data., log.sd1=x[1],
		 log.sd2=x[2], logit.w2=x[3])
	(-sum(dn))
}


Carlos J. Gil Bellosta wrote:
> Well,
> 
> If k is known, you can use maximun likelihood to fit the weights, means, 
> and sd's. The EM algorithm can be of help to solve the optimization 
> problem. You would have to implement it yourself for your particular 
> case, but I do not think it is big trouble.
> 
> Then you could estimate k using Bayesian formalism: from a reasonable a 
> priory distribution on k=1, 2,... compute the posterior distributions 
> using the densities obtained above, etc.
> 
> Carlos J. Gil Bellosta
> Sigma Consultores Estad?sticos
> http://www.consultoresestadisticos.com
> 
> Joke Allemeersch wrote:
> 
>> Hello,
>>
>> I have a concrete statistical question:
>> I have a sample of an univariate mixture of an unknown number (k) of 
>> normal distributions, each time with an unknown mean `m_i' and a 
>> standard deviation `k * m_i', where k is known factor constant for all 
>> the normal distributions. (The `i' is a subscript.)
>> Is there a function in R that can estimate the number of normal 
>> distributions k and the means `m_i' for the different normal 
>> distributions from a sample?  Or evt. a function that can estimate the 
>> `m_i', when the number of distributions `k' is known?
>> So far I only found a package, called `normix'.  But at first sight it 
>> only provides methods to sample from such distributions and to 
>> estimate the densities; but not to fit such a distribution.
>> Can someone indicate where I can find an elegant solution?
>>
>> Thank you in advance
>>
>> Joke Allemeersch
>>
>> Katholieke universiteit Leuven.
>> Belgium.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Thu Jul 17 18:42:40 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 17 Jul 2003 12:42:40 -0400
Subject: [R] Minor nuisance with rw1071
Message-ID: <3F1699C0.9556.9977B8@localhost>

Hola!

Starting with rw1071, just after starting Rgui the scope is not with 
Rconsole, but some other place. This means I have to do a mouse click 
in the Rconsole before starting to write the first command. Since I 
always forget this, I end up writing the first command twice. 

This is of course only a minor nuisance, but why is focus no longer 
set to Rconsole window when start-up?

This is on windows XP. 
(pre-compiled binary from CRAN)

Kjetil Halvorsen



From runge at plan.auc.dk  Thu Jul 17 21:24:03 2003
From: runge at plan.auc.dk (Jesper Runge Madsen)
Date: Thu, 17 Jul 2003 21:24:03 +0200
Subject: [R] dbApply and data.frame
Message-ID: <000a01c34c98$fbb26ba0$0501a8c0@rungef112>

Hallo again

I now succeeded in using dbApply on my data and I can convert it into a
data.frame. But as Peter Dalgaard pointed out I his answer to my earlier
question (Re: [R] list to data frame, 17.07.2003) I get one row and 10000
columns instead of  what I want two columns and 10000 rows when I convert
the list that dbApply returns to a Data frame.

The list I want to convert looks like this

structure(list("10000" = 123.85, "100003" = 59.7, "100004" = 61.5, 
    "100005" = 61.525, "10001" = 124.525, "100016" = 64.93848, 
    "100018" = 64.55, "10002" = 125.435, "100020" = 61.5714, 
.
.
.
    "9998" = 124.75, "99987" = 58.86503, "99989" = 61.93, "9999" = 123.8625,

    "99993" = 59.850025, "99995" = 36.853585), .Names = c("10000", 
"100003", "100004", "100005", "10001", "100016", "100018", "10002", 
.
.
"99962", "9997", "99973", "99978", "9998", "99987", "99989", 
"9999", "99993", "99995"))
 

what I get when I use as.data.frame(fraktil) is as said
 

X10000           X100003          X100004
123.85            59.7                61,5
 
I have tried this
fraktil.df <-
data.frame(LinieID=names(fraktil),"quantile_85"=unlist(fraktil))
but when I do this R shutsdown with no warning( R 1071, on a winXP system)
the same happens when I try write.table or dbWriteTable(con, "fraktil",
fraktil)
What I would like to end up with is a table like this
 
LinieID	quantile_85
100005	61.525
10001      124.525
100016     64.93848 
100018     64.55
10002      125.435
100020     61.5714
.                     .
.                     .
.                     .

Now at last my question J, is  there a way to make my list/data frame into
the table shown above. And does anybody know why R is crashing. 

Jesper Runge Madsen
Ph.D.
Trafikforskningsgruppen
Institut for Samfundsudvikling og Planl?gning
Aalborg Universitet
runge at plan.auc.dk
Tel: 9635 9800



From rpeng at stat.ucla.edu  Thu Jul 17 21:34:39 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Thu, 17 Jul 2003 12:34:39 -0700
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <3F1699C0.9556.9977B8@localhost>
References: <3F1699C0.9556.9977B8@localhost>
Message-ID: <3F16FA4F.4000402@stat.ucla.edu>

This doesn't happen with me.  When I startup the focus is on the 
console.  I'm running

 > version
         _             
platform i386-pc-mingw32
arch     i386          
os       mingw32       
system   i386, mingw32 
status                 
major    1             
minor    7.1           
year     2003          
month    06            
day      16            
language R             
 >

on Windows XP Home.  Maybe you have something else running in the 
background?

-roger

kjetil brinchmann halvorsen wrote:

>Hola!
>
>Starting with rw1071, just after starting Rgui the scope is not with 
>Rconsole, but some other place. This means I have to do a mouse click 
>in the Rconsole before starting to write the first command. Since I 
>always forget this, I end up writing the first command twice. 
>
>This is of course only a minor nuisance, but why is focus no longer 
>set to Rconsole window when start-up?
>
>This is on windows XP. 
>(pre-compiled binary from CRAN)
>
>Kjetil Halvorsen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From flom at ndri.org  Thu Jul 17 21:56:54 2003
From: flom at ndri.org (Peter Flom)
Date: Thu, 17 Jul 2003 15:56:54 -0400
Subject: [R] Rpart question - labeling nodes with something not in x$frame
Message-ID: <sf16c758.039@MAIL.NDRI.ORG>

I have a tree created with

tr.hh.logcas <- rpart(log(YCASSX + 1)~AGE+DRUGUSEY+SEX+OBSXNUM +WINDLE,
xval = 10)

I would like to label the nodes with YCASSX rather than log(YCASSX +
1).  But the help file for text in library rpart says that you can only
use labels that are part of x$frame, which YCASSX is not.

Is there a way to do what I want?

Thanks in advance

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ripley at stats.ox.ac.uk  Thu Jul 17 21:59:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jul 2003 20:59:16 +0100 (BST)
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <3F16FA4F.4000402@stat.ucla.edu>
Message-ID: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>

Can we check whether you are each running R in MDI or SDI mode (not that
I have a problem with either, but then I use focus-follows-mouse)?

Duncan M has been removing show(RConsole) calls and it is quite possible 
that this is the cause, but we do need to be able to reproduce the 
problem.

On Thu, 17 Jul 2003, Roger D. Peng wrote:

> This doesn't happen with me.  When I startup the focus is on the 
> console.  I'm running
> 
>  > version
>          _             
> platform i386-pc-mingw32
> arch     i386          
> os       mingw32       
> system   i386, mingw32 
> status                 
> major    1             
> minor    7.1           
> year     2003          
> month    06            
> day      16            
> language R             
>  >
> 
> on Windows XP Home.  Maybe you have something else running in the 
> background?
> 
> -roger
> 
> kjetil brinchmann halvorsen wrote:
> 
> >Hola!
> >
> >Starting with rw1071, just after starting Rgui the scope is not with 
> >Rconsole, but some other place. This means I have to do a mouse click 
> >in the Rconsole before starting to write the first command. Since I 
> >always forget this, I end up writing the first command twice. 
> >
> >This is of course only a minor nuisance, but why is focus no longer 
> >set to Rconsole window when start-up?
> >
> >This is on windows XP. 
> >(pre-compiled binary from CRAN)
> >
> >Kjetil Halvorsen
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tblackw at umich.edu  Thu Jul 17 22:09:07 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 17 Jul 2003 16:09:07 -0400 (EDT)
Subject: [R] dbApply and data.frame
In-Reply-To: <000a01c34c98$fbb26ba0$0501a8c0@rungef112>
Message-ID: <Pine.SOL.4.44.0307171603150.22493-100000@mspacman.gpcc.itd.umich.edu>

Runge  -

I haven't tried it out, but half a guess says that R might not like
using underscore in a variable name.  Please try exactly the same
command without quotes and without the underscore:

fraktil.df <- data.frame(LinieID=as.numeric(names(fraktil)),
	 	quantile85=unlist(fraktil))

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 17 Jul 2003, Jesper Runge Madsen wrote:

> Hallo again
>
> I now succeeded in using dbApply on my data and I can convert it into a
> data.frame. But as Peter Dalgaard pointed out I his answer to my earlier
> question (Re: [R] list to data frame, 17.07.2003) I get one row and 10000
> columns instead of  what I want two columns and 10000 rows when I convert
> the list that dbApply returns to a Data frame.
>
> The list I want to convert looks like this
>
> structure(list("10000" = 123.85, "100003" = 59.7, "100004" = 61.5,
>     "100005" = 61.525, "10001" = 124.525, "100016" = 64.93848,
>     "100018" = 64.55, "10002" = 125.435, "100020" = 61.5714,
> .
> .
> .
>     "9998" = 124.75, "99987" = 58.86503, "99989" = 61.93, "9999" = 123.8625,
>
>     "99993" = 59.850025, "99995" = 36.853585), .Names = c("10000",
> "100003", "100004", "100005", "10001", "100016", "100018", "10002",
> .
> .
> "99962", "9997", "99973", "99978", "9998", "99987", "99989",
> "9999", "99993", "99995"))
>
>
> what I get when I use as.data.frame(fraktil) is as said
>
>
> X10000           X100003          X100004
> 123.85            59.7                61,5
>
> I have tried this
> fraktil.df <-
> data.frame(LinieID=names(fraktil),"quantile_85"=unlist(fraktil))
> but when I do this R shutsdown with no warning( R 1071, on a winXP system)
> the same happens when I try write.table or dbWriteTable(con, "fraktil",
> fraktil)
> What I would like to end up with is a table like this
>
> LinieID	quantile_85
> 100005	61.525
> 10001      124.525
> 100016     64.93848
> 100018     64.55
> 10002      125.435
> 100020     61.5714
> .                     .
> .                     .
> .                     .
>
> Now at last my question J, is  there a way to make my list/data frame into
> the table shown above. And does anybody know why R is crashing.
>
> Jesper Runge Madsen
> Ph.D.
> Trafikforskningsgruppen
> Institut for Samfundsudvikling og Planlægning
> Aalborg Universitet
> runge at plan.auc.dk
> Tel: 9635 9800
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From emma042 at yahoo.com  Thu Jul 17 22:11:05 2003
From: emma042 at yahoo.com (=?iso-8859-1?q?Emma=20Tan?=)
Date: Thu, 17 Jul 2003 21:11:05 +0100 (BST)
Subject: [R] glmmPQL with crossed random effects
Message-ID: <20030717201105.24265.qmail@web14205.mail.yahoo.com>

Is it possible to fit a crossed random effects model
using glmmPQL and, e.g.
random=pdBlocked(list(pdIdent(~a-1),pdIdent(~b-1))) as
the random part of the model?  I seem to be getting
errors on R using glmmPQL in this way, although if I
try something very similar using 'lme' instead of
'glmmPQL', it works fine.  Does the data have to be of
a groupedData form for it to work (as it has to be for
lme) because glmmPQL doesn't seem to work with data of
the form groupedData?



From dmurdoch at pair.com  Thu Jul 17 22:29:08 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 17 Jul 2003 16:29:08 -0400
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <3F1699C0.9556.9977B8@localhost>
References: <3F1699C0.9556.9977B8@localhost>
Message-ID: <ai1ehvo2lj2ufsgtq8g2pkmes2mrtmf2ai@4ax.com>

On Thu, 17 Jul 2003 12:42:40 -0400, "kjetil brinchmann halvorsen"
<kjetil at entelnet.bo> wrote :

>Hola!
>
>Starting with rw1071, just after starting Rgui the scope is not with 
>Rconsole, but some other place. This means I have to do a mouse click 
>in the Rconsole before starting to write the first command. Since I 
>always forget this, I end up writing the first command twice. 

Do you have any code in your Rprofile that's playing with the GUI?
This sounds vaguely like a bug in the handling of the speedbuttons; I
can't think how that bug would cause this effect, but it might be
something related.

Duncan Murdoch



From kwan022 at stat.auckland.ac.nz  Thu Jul 17 22:27:44 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 18 Jul 2003 08:27:44 +1200 (NZST)
Subject: [R] Rpart question - labeling nodes with something not in x$frame
In-Reply-To: <sf16c758.039@MAIL.NDRI.ORG>
Message-ID: <Pine.LNX.4.44.0307180823540.26492-100000@stat61.stat.auckland.ac.nz>

On Thu, 17 Jul 2003, Peter Flom wrote:

> I have a tree created with
> 
> tr.hh.logcas <- rpart(log(YCASSX + 1)~AGE+DRUGUSEY+SEX+OBSXNUM +WINDLE,
> xval = 10)
> 
> I would like to label the nodes with YCASSX rather than log(YCASSX +
> 1).  But the help file for text in library rpart says that you can only
> use labels that are part of x$frame, which YCASSX is not.

This may not be the best solution, but what I have done once is to add 
another column into the data frame with the labels I want.

For example:
  data(iris)
  library(rpart)
  # Recoding the response:
  #    s: setosa
  #    c: versicolor
  #    v: virginica
  ir <- iris[, -5]
  Species <- rep(c("s", "c", "v"), rep(50, 3))
  ir <- as.data.frame(cbind(ir, Species))
  ir.rp <- rpart(Species ~ ., data = ir)
  plot(ir.rp)
  text(ir.rp)

This is probably the long/silly way, but it works ;-D

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Thu Jul 17 22:39:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jul 2003 21:39:53 +0100 (BST)
Subject: [R] getAnyhwhere behavior
In-Reply-To: <004901c347e3$52163040$52b2eb0c@C56909A>
Message-ID: <Pine.LNX.4.44.0307172137500.5979-100000@gannet.stats>

On Fri, 11 Jul 2003, Cliff Lunneborg wrote:

> I would have expected the function getAnywhere to have behaved
> differently in the following:
> 
> > search()
>  [1] ".GlobalEnv"                  "file:C:/R/Rdata/miya/.Rdata"
>  [3] "package:boot"                "package:methods"
>  [5] "package:ctest"               "package:mva"
>  [7] "package:modreg"              "package:nls"
>  [9] "package:ts"                  "Autoloads"
> [11] "package:base"
> 
> > getAnywhere(basic.ci)
> Error in getAnywhere(basic.ci) : Object "basic.ci" not found
> 
> > basic.ci<-get("basic.ci",environment(boot))
> 
> > getAnywhere(basic.ci)
> A single object matching `basic.ci' was found
> It was found in the following places
>   .GlobalEnv
>   registered S3 method for basic from namespace boot
>   namespace:boot
> with value
> 
> function (t0, t, conf = 0.95, hinv = function(t) t)
> {
>     qq <- norm.inter(t, (1 + c(conf, -conf))/2)
>     out <- cbind(conf, matrix(qq[, 1], ncol = 2), matrix(hinv(2 *
>         t0 - qq[, 2]), ncol = 2))
>     out
> }
> <environment: namespace:boot>
> 
> Why did getAnywhere not "see" basic.ci in the environment from which I
> "got" it?

Try reading the help page: your first usage is incorrect, and you get an
error message, as your should.  You seem to be blind to one of the
differences in your two calls.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at stat.ucla.edu  Thu Jul 17 22:41:59 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Thu, 17 Jul 2003 13:41:59 -0700
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
References: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
Message-ID: <3F170A17.3090100@stat.ucla.edu>

I was running in MDI mode, but I don't have the problem in either mode.

-roger

Prof Brian Ripley wrote:

>Can we check whether you are each running R in MDI or SDI mode (not that
>I have a problem with either, but then I use focus-follows-mouse)?
>
>Duncan M has been removing show(RConsole) calls and it is quite possible 
>that this is the cause, but we do need to be able to reproduce the 
>problem.
>
>On Thu, 17 Jul 2003, Roger D. Peng wrote:
>
>  
>
>>This doesn't happen with me.  When I startup the focus is on the 
>>console.  I'm running
>>
>> > version
>>         _             
>>platform i386-pc-mingw32
>>arch     i386          
>>os       mingw32       
>>system   i386, mingw32 
>>status                 
>>major    1             
>>minor    7.1           
>>year     2003          
>>month    06            
>>day      16            
>>language R             
>> >
>>
>>on Windows XP Home.  Maybe you have something else running in the 
>>background?
>>
>>-roger
>>
>>kjetil brinchmann halvorsen wrote:
>>
>>    
>>
>>>Hola!
>>>
>>>Starting with rw1071, just after starting Rgui the scope is not with 
>>>Rconsole, but some other place. This means I have to do a mouse click 
>>>in the Rconsole before starting to write the first command. Since I 
>>>always forget this, I end up writing the first command twice. 
>>>
>>>This is of course only a minor nuisance, but why is focus no longer 
>>>set to Rconsole window when start-up?
>>>
>>>This is on windows XP. 
>>>(pre-compiled binary from CRAN)
>>>
>>>Kjetil Halvorsen
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>    
>>
>
>  
>



From ross at biostat.ucsf.edu  Thu Jul 17 23:10:01 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 17 Jul 2003 21:10:01 -0000
Subject: [R] Looking to maximize a conditional likelihood
Message-ID: <1058476166.2296.37.camel@epibiosun115-4>

I want to maximize a conditional likelihood function that is basically
logistic conditional on the number of successes within strata.  What
would be a good starting place for this?  A complication is that the
denominator includes a term that is the sum over all permutations.

Although there is no time dimension to the problem, it's possible a
degenerate use of the Cox proportional hazards model (in the survival
package of 1.7) will do what I want.

Here's a little more detail on the function, though this is still quite
terse.  Pseudo Tex notation:

We have cases in clusters.  y_ij is the outcome (0 or 1) for the j'th
case in the i'th cluster  It has vector covariates X_ij. c_ij is a
transform of y_ij and is in (0, 1) (it's actually the probability of
being in cluster j).  It may be specified a priori or to be estimated.

Maximize the product over i of

c_ij exp(sum_j y_ij X_ij b)
----------------------------
sum_K c_ij exp(sum_K y_ij X_ij b)

Where sum_K means we are summing over all possible subsets of the strata
that contain the observed number of successes.
b, and perhaps c, are the parameters to estimate.

The more vanilla case has no c_ij terms.  That says we maximize the
probability of the observed outcomes, given the total number of
successes within strata--i.e., we condition out on strata-specific
effects.  I kind of expected there would already be a routine that does
this, but I can't find it.

Thanks for any help you can offer.
          
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From spencer.graves at pdf.com  Thu Jul 17 23:27:37 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 17 Jul 2003 14:27:37 -0700
Subject: [R] Looking to maximize a conditional likelihood
References: <1058476166.2296.37.camel@epibiosun115-4>
Message-ID: <3F1714C9.5060900@pdf.com>

Have you considered "optim"?

spencer graves

Ross Boylan wrote:
> I want to maximize a conditional likelihood function that is basically
> logistic conditional on the number of successes within strata.  What
> would be a good starting place for this?  A complication is that the
> denominator includes a term that is the sum over all permutations.
> 
> Although there is no time dimension to the problem, it's possible a
> degenerate use of the Cox proportional hazards model (in the survival
> package of 1.7) will do what I want.
> 
> Here's a little more detail on the function, though this is still quite
> terse.  Pseudo Tex notation:
> 
> We have cases in clusters.  y_ij is the outcome (0 or 1) for the j'th
> case in the i'th cluster  It has vector covariates X_ij. c_ij is a
> transform of y_ij and is in (0, 1) (it's actually the probability of
> being in cluster j).  It may be specified a priori or to be estimated.
> 
> Maximize the product over i of
> 
> c_ij exp(sum_j y_ij X_ij b)
> ----------------------------
> sum_K c_ij exp(sum_K y_ij X_ij b)
> 
> Where sum_K means we are summing over all possible subsets of the strata
> that contain the observed number of successes.
> b, and perhaps c, are the parameters to estimate.
> 
> The more vanilla case has no c_ij terms.  That says we maximize the
> probability of the observed outcomes, given the total number of
> successes within strata--i.e., we condition out on strata-specific
> effects.  I kind of expected there would already be a routine that does
> this, but I can't find it.
> 
> Thanks for any help you can offer.
>



From p.dalgaard at biostat.ku.dk  Thu Jul 17 23:28:58 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 17 Jul 2003 21:28:58 -0000
Subject: [R] dbApply and data.frame
In-Reply-To: <Pine.SOL.4.44.0307171603150.22493-100000@mspacman.gpcc.itd.umich.edu>
References: <Pine.SOL.4.44.0307171603150.22493-100000@mspacman.gpcc.itd.umich.edu>
Message-ID: <x27k6g7sbr.fsf@biostat.ku.dk>

Thomas W Blackwell <tblackw at umich.edu> writes:

> Runge  -
> 
> I haven't tried it out, but half a guess says that R might not like
> using underscore in a variable name.  Please try exactly the same
> command without quotes and without the underscore:
> 
> fraktil.df <- data.frame(LinieID=as.numeric(names(fraktil)),
> 	 	quantile85=unlist(fraktil))


Should work when quoted, so I wouldn't have too high hopes that it
works (but it's worth a try of course). 

The same thing works with synthetic data:

> l <- as.list(rnorm(200000))
> names(l) <- 1:200000
> fraktil <- l
> fraktil.df <- data.frame(LinieID=as.numeric(names(fraktil)),"quantile_85"=unlist(fraktil))
> fraktil.df[1:20,]
   LinieID quantile.85
1        1  0.78609143
2        2  1.16852825
3        3  1.11639102
4        4 -1.09446630
5        5  0.95239745
6        6 -1.15613682
7        7  1.04672917
8        8 -1.66155659
9        9 -0.47110949
10      10  0.08956315
11      11 -0.58068630
12      12 -1.15674753
13      13  1.70977126
14      14  0.52023438
15      15 -1.99154007
16      16  2.11717771
17      17 -0.18951885
18      18  0.19108247
19      19  0.39874107
20      20  0.97474058

(You actually need to add check.names=FALSE to prevent it from
removing the underscore automatically).

I.e. R is not *supposed* to crash, so either there is a platform
dependency  or (more likely) it is triggered by an earlier memory
corruption in dbApply() or the database backend. What kind of machine
is this happening on? Would it be possible for you to run under the
debugger so that we could get a bit more information.
 
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pfaffman at relaxpc.com  Thu Jul 17 23:32:16 2003
From: pfaffman at relaxpc.com (Jay Pfaffman)
Date: Thu, 17 Jul 2003 14:32:16 -0700
Subject: [R] confused about histograms
Message-ID: <200307172132.h6HLWGvJ004318@aaalab.Stanford.EDU>

I've got a data set with integer codes from 0--3.  I'd like a
histogram with a single bar for 0, 1, 2 and 3.  I'd like each of the 4
bars centered over a label.

    hist(mydata,  breaks=4, main="Simulation") 

gives me three bars.  The best I've been able to do is do something
like 

   print(hist((wexp),  breaks=25, main="Simulation"))

This gives me something close to what I want, but seems like a silly
way to do it.  I'm about to give up and have someone do it in Excel
(which peeves me to no end).

To further complicate matters, one data set has no items in the zero
cell.  I'd like that graph to look like the others (using my stupid
hack above it has bars that are a different width).

Thanks.

-- 
Jay Pfaffman                           pfaffman at relaxpc.com
+1-415-821-7507 (H)                    +1-415-812-5047 (M)



From ross at biostat.ucsf.edu  Thu Jul 17 23:33:55 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 17 Jul 2003 21:33:55 -0000
Subject: [R] Looking to maximize a conditional likelihood
In-Reply-To: <3F1714C9.5060900@pdf.com>
References: <1058476166.2296.37.camel@epibiosun115-4>
	<3F1714C9.5060900@pdf.com>
Message-ID: <1058477595.2296.58.camel@epibiosun115-4>

On Thu, 2003-07-17 at 14:27, Spencer Graves wrote:
> Have you considered "optim"?
> 
> spencer graves
> 
Thank you for drawing that to my attention.  I take it that's the best general
purpose optimizer to use?

My hope is to find something that knows a bit more about the structure of the
problem, including particularly the sum of permutations bit.  After looking
at the coxph documentation 
http://stat.ethz.ch/R-alpha/R-patched/library/survival/html/coxph.html
I'm getting more optimistic it's what I need (particularly with 
method=exact, which it says is equivalent to the conditional logistic model, 
which is my case (at least without the c_ij's).



From p.dalgaard at biostat.ku.dk  Thu Jul 17 23:34:13 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 17 Jul 2003 21:34:13 -0000
Subject: [R] Looking to maximize a conditional likelihood
In-Reply-To: <1058476166.2296.37.camel@epibiosun115-4>
References: <1058476166.2296.37.camel@epibiosun115-4>
Message-ID: <x23ch47s2p.fsf@biostat.ku.dk>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> I want to maximize a conditional likelihood function that is basically
> logistic conditional on the number of successes within strata.  What
> would be a good starting place for this?  A complication is that the
> denominator includes a term that is the sum over all permutations.
> 
> Although there is no time dimension to the problem, it's possible a
> degenerate use of the Cox proportional hazards model (in the survival
> package of 1.7) will do what I want.

Did you check out clogit from the same package?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.murrell at auckland.ac.nz  Thu Jul 17 23:41:44 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 18 Jul 2003 09:41:44 +1200
Subject: [R] confused about histograms
References: <200307172132.h6HLWGvJ004318@aaalab.Stanford.EDU>
Message-ID: <3F171818.9040309@stat.auckland.ac.nz>

Hi

Sounds like a barplot may provide a more appropriate visualisation; see 
?barplot

Paul


Jay Pfaffman wrote:
> I've got a data set with integer codes from 0--3.  I'd like a
> histogram with a single bar for 0, 1, 2 and 3.  I'd like each of the 4
> bars centered over a label.
> 
>     hist(mydata,  breaks=4, main="Simulation") 
> 
> gives me three bars.  The best I've been able to do is do something
> like 
> 
>    print(hist((wexp),  breaks=25, main="Simulation"))
> 
> This gives me something close to what I want, but seems like a silly
> way to do it.  I'm about to give up and have someone do it in Excel
> (which peeves me to no end).
> 
> To further complicate matters, one data set has no items in the zero
> cell.  I'd like that graph to look like the others (using my stupid
> hack above it has bars that are a different width).
> 
> Thanks.
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ross at biostat.ucsf.edu  Thu Jul 17 23:42:58 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 17 Jul 2003 21:42:58 -0000
Subject: [R] Looking to maximize a conditional likelihood
In-Reply-To: <x23ch47s2p.fsf@biostat.ku.dk>
References: <1058476166.2296.37.camel@epibiosun115-4>
	<x23ch47s2p.fsf@biostat.ku.dk>
Message-ID: <1058478125.2296.69.camel@epibiosun115-4>

On Thu, 2003-07-17 at 14:37, Peter Dalgaard BSA wrote:
> Ross Boylan <ross at biostat.ucsf.edu> writes:
> 
> > I want to maximize a conditional likelihood function that is basically
> > logistic conditional on the number of successes within strata.  What
> > would be a good starting place for this?  A complication is that the
> > denominator includes a term that is the sum over all permutations.
> > 
> > Although there is no time dimension to the problem, it's possible a
> > degenerate use of the Cox proportional hazards model (in the survival
> > package of 1.7) will do what I want.
> 
> Did you check out clogit from the same package?
Wow, that looks like it exactly!  Thanks.  I was so sure this was a
time-series package I didn't think to look there.

This leads to a general search question: is there a way I could have
done a search on "conditional logit" and found this?  I tried various
searches like help.search("condition").  My system doesn't have this
module available, so was that the only problem?  I'm not sure if the
usual searches look through all the available packages.

-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From tplate at blackmesacapital.com  Thu Jul 17 23:50:32 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 17 Jul 2003 15:50:32 -0600
Subject: [R] confused about histograms
In-Reply-To: <200307172132.h6HLWGvJ004318@aaalab.Stanford.EDU>
Message-ID: <5.2.1.1.2.20030717154911.04424970@mailhost.blackmesacapital.com>

 > hist(sample(0:3, 10, T), breaks=(0:4)-0.5)

At Thursday 02:32 PM 7/17/2003 -0700, Jay Pfaffman wrote:
>I've got a data set with integer codes from 0--3.  I'd like a
>histogram with a single bar for 0, 1, 2 and 3.  I'd like each of the 4
>bars centered over a label.
>
>     hist(mydata,  breaks=4, main="Simulation")
>
>gives me three bars.  The best I've been able to do is do something
>like
>
>    print(hist((wexp),  breaks=25, main="Simulation"))
>
>This gives me something close to what I want, but seems like a silly
>way to do it.  I'm about to give up and have someone do it in Excel
>(which peeves me to no end).
>
>To further complicate matters, one data set has no items in the zero
>cell.  I'd like that graph to look like the others (using my stupid
>hack above it has bars that are a different width).
>
>Thanks.
>
>--
>Jay Pfaffman                           pfaffman at relaxpc.com
>+1-415-821-7507 (H)                    +1-415-812-5047 (M)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Thu Jul 17 23:55:03 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 17 Jul 2003 17:55:03 -0400
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
References: <3F16FA4F.4000402@stat.ucla.edu>
	<Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
Message-ID: <eh6ehvoroi7v433qns7jn282abhqso38gs@4ax.com>

On Thu, 17 Jul 2003 20:59:16 +0100 (BST), you wrote:

>Duncan M has been removing show(RConsole) calls 

Wrong tense; that should be "did remove", and it all happened before
1.7.0.  Though I'm tempted to take out some more, to stop the
irritating change to the console display mode (loss of minimization or
maximization) when closing help or graphics windows.

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Fri Jul 18 00:03:08 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 17 Jul 2003 22:03:08 -0000
Subject: [R] Looking to maximize a conditional likelihood
In-Reply-To: <1058478125.2296.69.camel@epibiosun115-4>
References: <1058476166.2296.37.camel@epibiosun115-4>
	<x23ch47s2p.fsf@biostat.ku.dk>
	<1058478125.2296.69.camel@epibiosun115-4>
Message-ID: <x2y8yw6c5o.fsf@biostat.ku.dk>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> > Did you check out clogit from the same package?

> Wow, that looks like it exactly!  Thanks.  I was so sure this was a
> time-series package I didn't think to look there.

> This leads to a general search question: is there a way I could have
> done a search on "conditional logit" and found this? 

In R itself, probably not. The CRAN search engine at

http://finzi.psych.upenn.edu/search.html

will get you there if you search for "conditional logistic". 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andel at ifi.unizh.ch  Fri Jul 18 00:07:43 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Thu, 17 Jul 2003 22:07:43 -0000
Subject: [R] how to divide a string into characters? - for comparing strings
 that is
Message-ID: <Pine.GSO.4.44.0307172359190.22758-100000@igor>

Hi

I am searching for a way to do something like "ABC" -> c("A","B","C"). How can this be accomplished?

I tried cut() and split(), but they do something else, it seems.

The purpose for doing this is to find the number of common (and uncommon) characters, i.e. ultimately I want something like this:

> foo("ABD","ADE")
c(2,1) # 2 in x are in y, 1 in y is not in x
> foo("AB","ADE")
c(1,2) # 1 in x is in y, 2 in y are not in x

Maybe I even do not need the string splitting?

I hope I was clear in stating my problem.

Thank you for your valuable input,
David



From rpeng at stat.ucla.edu  Fri Jul 18 00:21:08 2003
From: rpeng at stat.ucla.edu (Roger D. Peng)
Date: Thu, 17 Jul 2003 15:21:08 -0700
Subject: [R] how to divide a string into characters? - for comparing
	strings that is
In-Reply-To: <Pine.GSO.4.44.0307172359190.22758-100000@igor>
References: <Pine.GSO.4.44.0307172359190.22758-100000@igor>
Message-ID: <3F172154.90903@stat.ucla.edu>



David Andel wrote:

>Hi
>
>I am searching for a way to do something like "ABC" -> c("A","B","C"). How can this be accomplished?
>  
>
Try

strsplit("ABC", "")[[1]]

>I tried cut() and split(), but they do something else, it seems.
>
>The purpose for doing this is to find the number of common (and uncommon) characters, i.e. ultimately I want something like this:
>
>  
>
>>foo("ABD","ADE")
>>    
>>
>c(2,1) # 2 in x are in y, 1 in y is not in x
>  
>
>>foo("AB","ADE")
>>    
>>
>c(1,2) # 1 in x is in y, 2 in y are not in x
>
>Maybe I even do not need the string splitting?
>
>I hope I was clear in stating my problem.
>
>Thank you for your valuable input,
>David
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From fjmolina at lbl.gov  Fri Jul 18 00:30:14 2003
From: fjmolina at lbl.gov (Francisco J Molina)
Date: Thu, 17 Jul 2003 15:30:14 -0700
Subject: [R] R matrices in memory
Message-ID: <16151.9078.642535.987245@dhcp-63-193.cse.ucsc.edu>


Acording to the documentation in gsl 

"The physical row dimension tda, or trailing dimension, specifies the size
of a row of the matrix as laid out in memory"

I think that if I pass a matrix to C++ through .C as single ( or.double ),
that is, 

.C ( as.single ( matrix ))

then the tda is simply the number of elements of that matrix.
Am I right?

Thank you.



From jerome at hivnet.ubc.ca  Fri Jul 18 00:29:01 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 17 Jul 2003 15:29:01 -0700
Subject: [R] how to divide a string into characters? - for comparing
	strings that is
In-Reply-To: <Pine.GSO.4.44.0307172359190.22758-100000@igor>
References: <Pine.GSO.4.44.0307172359190.22758-100000@igor>
Message-ID: <200307172235.PAA05433@hivnet.ubc.ca>


Here is a way of doing by splitting the character strings.

x <- "ABD"
y <- "ADE"
x <- unlist(strsplit(x,""))
y <- unlist(strsplit(y,""))
c(sum(as.logical(match(x,y,nomatch=0))),
sum(!as.logical(match(y,x,nomatch=0))))

HTH,
Jerome

On July 17, 2003 03:07 pm, David Andel wrote:
> Hi
>
> I am searching for a way to do something like "ABC" -> c("A","B","C").
> How can this be accomplished?
>
> I tried cut() and split(), but they do something else, it seems.
>
> The purpose for doing this is to find the number of common (and 
uncommon) characters, i.e. ultimately I want something like this:
> > foo("ABD","ADE")
>
> c(2,1) # 2 in x are in y, 1 in y is not in x
>
> > foo("AB","ADE")
>
> c(1,2) # 1 in x is in y, 2 in y are not in x
>
> Maybe I even do not need the string splitting?
>
> I hope I was clear in stating my problem.
>
> Thank you for your valuable input,
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ross at biostat.ucsf.edu  Fri Jul 18 00:30:29 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 17 Jul 2003 22:30:29 -0000
Subject: [R] searching (was Looking to maximize a conditional likelihood)
In-Reply-To: <x2y8yw6c5o.fsf@biostat.ku.dk>
References: <1058476166.2296.37.camel@epibiosun115-4>
	<x23ch47s2p.fsf@biostat.ku.dk>
	<1058478125.2296.69.camel@epibiosun115-4>
	<x2y8yw6c5o.fsf@biostat.ku.dk>
Message-ID: <1058480981.2296.101.camel@epibiosun115-4>

On Thu, 2003-07-17 at 15:06, Peter Dalgaard BSA wrote:
> Ross Boylan <ross at biostat.ucsf.edu> writes:
> 

> > This leads to a general search question: is there a way I could have
> > done a search on "conditional logit" and found this? 
> 
> In R itself, probably not. The CRAN search engine at
> 
> http://finzi.psych.upenn.edu/search.html
> 
> will get you there if you search for "conditional logistic". 

Hmm... if I limit the search to "functions" and enter "conditional
logistic" the clogit function is the 17th of 18 hits.  It's good that it
found it, but sort of weird it ranks it so low.

Among other things the search engine seems not to give points for the
search words being in the page title (I think it's the only one that
meets this requirement) or to their adjacency.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From jfox at mcmaster.ca  Fri Jul 18 00:55:14 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 17 Jul 2003 18:55:14 -0400
Subject: [R] how to divide a string into characters? - for
	comparing strings that is
In-Reply-To: <Pine.GSO.4.44.0307172359190.22758-100000@igor>
Message-ID: <5.1.0.14.2.20030717185241.01fd0668@127.0.0.1>

Dear David

At 12:07 AM 7/18/2003 +0200, David Andel wrote:

>I am searching for a way to do something like "ABC" -> c("A","B","C"). How 
>can this be accomplished?
>
>I tried cut() and split(), but they do something else, it seems.
>
>The purpose for doing this is to find the number of common (and uncommon) 
>characters, i.e. ultimately I want something like this:
>
> > foo("ABD","ADE")
>c(2,1) # 2 in x are in y, 1 in y is not in x
> > foo("AB","ADE")
>c(1,2) # 1 in x is in y, 2 in y are not in x
>
>Maybe I even do not need the string splitting?
>
>I hope I was clear in stating my problem.

Here's a solution based on splitting the strings:

 > foo <- function(a, b){
+     a <- strsplit(a, "")[[1]]
+     b <- strsplit(b, "")[[1]]
+     nmatch <- sum(outer(a, b, "=="))
+     c(nmatch, length(b) - nmatch)
+     }
 > foo("ABD","ADE")
[1] 2 1
 > foo("AB","ADE")
[1] 1 2

By the way, apropos() turns up strsplit():

 > apropos("split")
  [1] "split"              "split.data.frame"   "split.data.frame<-"
  [4] "split.default"      "split.screen"       "split<-"
  [7] "split<-.data.frame" "split<-.default"    "strsplit"
[10] "unsplit"

I hope that this helps,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From rgazaffi at uol.com.br  Fri Jul 18 00:59:40 2003
From: rgazaffi at uol.com.br (Rodrigo)
Date: Thu, 17 Jul 2003 19:59:40 -0300
Subject: [R] i need help in cluster analyse
Message-ID: <005a01c34cb7$1ba41490$de9162c8@rodrigog>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030717/ce243e95/attachment.pl

From AdamL at spc.int  Fri Jul 18 01:03:51 2003
From: AdamL at spc.int (Adam Langley)
Date: Fri, 18 Jul 2003 10:03:51 +1100
Subject: [R] pie charts as symbols
Message-ID: <27DF1E0087070642B128C853E74FCCAF0218C7CB@tazar.spc.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030718/a8ac0217/attachment.pl

From paulda at BATTELLE.ORG  Fri Jul 18 01:13:45 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 17 Jul 2003 19:13:45 -0400
Subject: [R] Matrix Multiplication
Message-ID: <940250A9EB37A24CBE28D858EF07774967A9EA@ws-bco-mse3.milky-way.battelle.org>

R1.7.1/Win2k:

Apologies if this posts twice - the first message
was not in plain text.

I have looked in help.start() and tried typing
"crossprod" and "%*%" into the RGui to get an idea 
for what R is using as internal algorithms for 
its matrix computations/manipulations... to no 
avail.

Could someone point me in the direction of some
documentation?  All I get for "crossprod" is

	function (x, y = NULL) 
	.Internal(crossprod(x, y))
	<environment: namespace:base>

Much thanks in advance,
	david paul



From bates at stat.wisc.edu  Fri Jul 18 01:15:29 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 17 Jul 2003 23:15:29 -0000
Subject: [R] R matrices in memory
In-Reply-To: <16151.9078.642535.987245@dhcp-63-193.cse.ucsc.edu>
References: <16151.9078.642535.987245@dhcp-63-193.cse.ucsc.edu>
Message-ID: <6rfzl4ycbp.fsf@bates4.stat.wisc.edu>

Francisco J Molina <fjmolina at lbl.gov> writes:

> Acording to the documentation in gsl 
> 
> "The physical row dimension tda, or trailing dimension, specifies the size
> of a row of the matrix as laid out in memory"

I think you may have problems here.  Arrays in R are stored in
column-major ordering, as in Fortran.  In other words, the elements of
a row of a matrix are not adjacent in memory so "the size of a row of
the matrix as laid out in memory" is not meaningful.  

For R matrices the elements of columns are adjacent in memory.

You may need to write conversion routines to copy the contents of the
R matrix into the form that GSL expects.

> I think that if I pass a matrix to C++ through .C as single ( or.double ),
> that is, 
> 
> .C ( as.single ( matrix ))
> 
> then the tda is simply the number of elements of that matrix.



From kjetil at entelnet.bo  Fri Jul 18 01:21:06 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 17 Jul 2003 19:21:06 -0400
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
References: <3F16FA4F.4000402@stat.ucla.edu>
Message-ID: <3F16F722.25794.6EE3C1@localhost>

On 17 Jul 2003 at 20:59, Prof Brian Ripley wrote:

> Can we check whether you are each running R in MDI or SDI mode (not that
> I have a problem with either, but then I use focus-follows-mouse)?

I am running in MDI mode. Switching to SDI, problem disappears. 

But happens only with one of my many R startup icons, not with the 
others! It happens when starting in a directory where I experimented 
with the package Rcmdr by John Fox, and I guess that must have to do 
with the problem. First, the startup message now includes
Loading required package: foreign 
Loading required package: mva 
Loading required package: ctest 
Loading required package: tcltk 
Loading required package: car 
Loading required package: lattice 
Loading required package: grid 
Loading required package: nls 

loading extra packages compared with factory-fresh standard, 
including car. I did'nt change any startup files, so this must have 
been set by Rcmdr, but I don't know where. 

The value of options("defaultPackages") have not been changed. 

When typing ls(all=TRUE) I get a lot of variables names starting with 
. , most of them must have been defined by Rcmdr, but none seems
to contain package name info. I have none .Last() or .First() 
function anywhere. 

Incidentally, one of the variables defined by Rcmdr, .logFont, if I 
try to print its value, Rgui bombs, repeatedly!

This is all the info I have been able to find.
I have no .Renviron file, anywhere.
The only changes i made to .Rprofile was to get html help, and 
redefing the default editor. 

Kjetil Halvorsen

> 
> Duncan M has been removing show(RConsole) calls and it is quite possible 
> that this is the cause, but we do need to be able to reproduce the 
> problem.
> 
> On Thu, 17 Jul 2003, Roger D. Peng wrote:
> 
> > This doesn't happen with me.  When I startup the focus is on the 
> > console.  I'm running
> > 
> >  > version
> >          _             
> > platform i386-pc-mingw32
> > arch     i386          
> > os       mingw32       
> > system   i386, mingw32 
> > status                 
> > major    1             
> > minor    7.1           
> > year     2003          
> > month    06            
> > day      16            
> > language R             
> >  >
> > 
> > on Windows XP Home.  Maybe you have something else running in the 
> > background?
> > 
> > -roger
> > 
> > kjetil brinchmann halvorsen wrote:
> > 
> > >Hola!
> > >
> > >Starting with rw1071, just after starting Rgui the scope is not with 
> > >Rconsole, but some other place. This means I have to do a mouse click 
> > >in the Rconsole before starting to write the first command. Since I 
> > >always forget this, I end up writing the first command twice. 
> > >
> > >This is of course only a minor nuisance, but why is focus no longer 
> > >set to Rconsole window when start-up?
> > >
> > >This is on windows XP. 
> > >(pre-compiled binary from CRAN)
> > >
> > >Kjetil Halvorsen
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > >
> > >  
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Fri Jul 18 01:57:33 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Jul 2003 19:57:33 -0400
Subject: [R] Matrix Multiplication
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8C1@usrymx25.merck.com>

You have/can get the source, no?  I believe the answer is in
R-1.7.1/src/main/array.c.  Looks like it depends on what BLAS you have or
don't have.

Andy

> -----Original Message-----
> From: Paul, David A [mailto:paulda at BATTELLE.ORG] 
> Sent: Thursday, July 17, 2003 7:14 PM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Matrix Multiplication
> 
> 
> R1.7.1/Win2k:
> 
> Apologies if this posts twice - the first message
> was not in plain text.
> 
> I have looked in help.start() and tried typing
> "crossprod" and "%*%" into the RGui to get an idea 
> for what R is using as internal algorithms for 
> its matrix computations/manipulations... to no 
> avail.
> 
> Could someone point me in the direction of some
> documentation?  All I get for "crossprod" is
> 
> 	function (x, y = NULL) 
> 	.Internal(crossprod(x, y))
> 	<environment: namespace:base>
> 
> Much thanks in advance,
> 	david paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From Simon.Blomberg at anu.edu.au  Fri Jul 18 02:03:05 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 18 Jul 2003 10:03:05 +1000
Subject: [R] i need help in cluster analyse
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133DC@CASEVS02.cas.anu.edu.au>

Rodrigo,

You need to think about what type of clustering algorithm you intend to use. There are many choices. A simple example to get you started is:

dist <- read.table("matrix.txt", header=TRUE) # read in your matrix from a text file
dend <- hclust(as.dist(dist), method="average") # do clustering, using UPGMA method
plot(dend) #plot your dendrogram

See ?hclust for more methods. There are also other packages on CRAN that do cluster analysis.

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Rodrigo [mailto:rgazaffi at uol.com.br]
> Sent: Friday, 18 July 2003 9:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] i need help in cluster analyse
> 
> 
> Hello,
> My name is Rodrigo, I am using R program and I have a trouble.
>  
> I am trying to do a dendrogram with genetics information.
> Let me explain...
> The Similarity Matrix was already did, and with this matrix I 
> want to construct a dendrogram.
> So, the distance is done. I need to transform this matrix 
> (that I have) in a dendrogram,
>  
> I woud be very grateful if someone could help me.
> PS: I am sending a example together, for the matrix I have 
> and want to transform in a dendrogram.
> 
> best wish to all.
> Thank you for the attention.
> 
> ##############################################################
> ##################################
>      1         2         3         4         5         6      
>    7         8         9         10         11         12     
>     13     14
> 01 1.0000 0.4500 0.2174 0.3636 0.3810 0.2632 0.5000 0.4167 
> 0.2500 0.4286 0.3182 0.3333 0.3684 0.2778 
> 02 0.4500 1.0000 0.3750 0.4583 0.4167 0.4737 0.4074 0.3929 
> 0.3600 0.4000 0.2593 0.3846 0.4762 0.3500 
> 03 0.2174 0.3750 1.0000 0.4167 0.4348 0.3684 0.3214 0.4074 
> 0.3333 0.4167 0.3200 0.4783 0.3636 0.3889 
> 04 0.3636 0.4583 0.4167 1.0000 0.5217 0.3478 0.4444 0.5385 
> 0.5417 0.5000 0.4583 0.5600 0.5238 0.4286 
> 05 0.3810 0.4167 0.4348 0.5217 1.0000 0.4000 0.5833 0.5000 
> 0.5455 0.5217 0.4167 0.5652 0.3478 0.2273 
> 06 0.2632 0.4737 0.3684 0.3478 0.4000 1.0000 0.2800 0.2593 
> 0.3913 0.3182 0.3043 0.3077 0.3000 0.4706 
> 07 0.5000 0.4074 0.3214 0.4444 0.5833 0.2800 1.0000 0.5926 
> 0.5200 0.5600 0.4615 0.4815 0.3462 0.2917 
> 08 0.4167 0.3929 0.4074 0.5385 0.5000 0.2593 0.5926 1.0000 
> 0.4815 0.4815 0.4444 0.5556 0.3846 0.3200 
> 09 0.2500 0.3600 0.3333 0.5417 0.5455 0.3913 0.5200 0.4815 
> 1.0000 0.5909 0.5652 0.4815 0.4545 0.3478 
> 10 0.4286 0.4000 0.4167 0.5000 0.5217 0.3182 0.5600 0.4815 
> 0.5909 1.0000 0.4583 0.6087 0.4545 0.2727 
> 11 0.3182 0.2593 0.3200 0.4583 0.4167 0.3043 0.4615 0.4444 
> 0.5652 0.4583 1.0000 0.5200 0.3478 0.2609 
> 12 0.3333 0.3846 0.4783 0.5600 0.5652 0.3077 0.4815 0.5556 
> 0.4815 0.6087 0.5200 1.0000 0.4167 0.2222 
> 13 0.3684 0.4762 0.3636 0.5238 0.3478 0.3000 0.3462 0.3846 
> 0.4545 0.4545 0.3478 0.4167 1.0000 0.3158 
> 14 0.2778 0.3500 0.3889 0.4286 0.2273 0.4706 0.2917 0.3200 
> 0.3478 0.2727 0.2609 0.2222 0.3158 1.0000 
> 
> 
> This matrix have 14 individuals and the analyse of similarity 
> (the range of statistic is 0 to 1).
> I want to use it in that way to the cluster analyse.
>  
> 
> 
> 
> 
> ---
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tmulholl at bigpond.net.au  Fri Jul 18 02:37:49 2003
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Fri, 18 Jul 2003 08:37:49 +0800
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <eh6ehvoroi7v433qns7jn282abhqso38gs@4ax.com>
Message-ID: <001401c34cc4$d170fdd0$2202a8c0@ACER>

It is indeed irritating, I thought it was another of my finger problems.
Feel free to continue and make the tense correct.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
Sent: Friday, July 18, 2003 5:55 AM
To: Prof Brian Ripley
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Minor nuisance with rw1071

On Thu, 17 Jul 2003 20:59:16 +0100 (BST), you wrote:

>Duncan M has been removing show(RConsole) calls

Wrong tense; that should be "did remove", and it all happened before
1.7.0.  Though I'm tempted to take out some more, to stop the
irritating change to the console display mode (loss of minimization or
maximization) when closing help or graphics windows.

Duncan Murdoch


---



From Simon.Blomberg at anu.edu.au  Fri Jul 18 02:55:46 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Fri, 18 Jul 2003 10:55:46 +1000
Subject: [R] i need help in cluster analyse
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133DD@CASEVS02.cas.anu.edu.au>

> > The Similarity Matrix was already did, and with this matrix I 
> > want to construct a dendrogram.


> > This matrix have 14 individuals and the analyse of similarity 
> > (the range of statistic is 0 to 1).
> > I want to use it in that way to the cluster analyse.


I forgot to mention that hclust expects a DISsimilarity matrix. So you may want to calculate one minus your similarity matrix to turn it into a dissimilarity matrix.

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379



From jfox at mcmaster.ca  Fri Jul 18 02:42:27 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 17 Jul 2003 20:42:27 -0400
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <3F16F722.25794.6EE3C1@localhost>
References: <Pine.LNX.4.44.0307172056540.5920-100000@gannet.stats>
	<3F16FA4F.4000402@stat.ucla.edu>
Message-ID: <5.1.0.14.2.20030717202741.01f765d0@127.0.0.1>

Dear Kjetil , Roger,and Brian,

I'm not sure that this will shed any light on Kjetil's problems, but the 
startup for Rcmdr is

.onAttach <- function(...){
     cat("\nRcmdr Version 0.9-0\n")
     Commander()
     }

.onLoad <- function(...){
     save.options <- options(warn=-1)
     on.exit(options(save.options))
     foreign <- require(foreign)
     mva <- require(mva)
     ctest <- require(ctest)
     tcltk <- require(tcltk)
     car <- require(car)
     absent <- !c(foreign, mva, ctest, tcltk, car)
     if (any(absent)) {
         cat("\nThe following packages required by Rcmdr are missing:\n")
         cat(paste(c("foreign", "mva", "ctest", "tcltk", "car")[absent], 
collapse=", "))
         cat("\n")
         }
     }

In a vanilla installation, mva and ctest are among the default packages, so 
what one should normally see on loading Rcmdr is

 > library(Rcmdr)
Loading required package: foreign
Loading required package: tcltk
Loading required package: car

Rcmdr Version 0.9-0
 >

Another, possibly relevant, piece of information: .logFont is a Tcl 
variable containing a font definition:

 > .logFont
<Tcl> font1

If trying to print this variable causes Rgui to bomb, then perhaps 
something is wrong with either the installation of the tcltk package or of 
Tcl/Tk itself.

I'm aware as well that Kjetil has been unable to get Rcmdr to work 
properly, for reasons that I've been unable to discover. By the way, Rcmdr 
requires the SDI.

I hope that this is useful.
  John


At 07:21 PM 7/17/2003 -0400, kjetil brinchmann halvorsen wrote:
>On 17 Jul 2003 at 20:59, Prof Brian Ripley wrote:
>
> > Can we check whether you are each running R in MDI or SDI mode (not that
> > I have a problem with either, but then I use focus-follows-mouse)?
>
>I am running in MDI mode. Switching to SDI, problem disappears.
>
>But happens only with one of my many R startup icons, not with the
>others! It happens when starting in a directory where I experimented
>with the package Rcmdr by John Fox, and I guess that must have to do
>with the problem. First, the startup message now includes
>Loading required package: foreign
>Loading required package: mva
>Loading required package: ctest
>Loading required package: tcltk
>Loading required package: car
>Loading required package: lattice
>Loading required package: grid
>Loading required package: nls
>
>loading extra packages compared with factory-fresh standard,
>including car. I did'nt change any startup files, so this must have
>been set by Rcmdr, but I don't know where.
>
>The value of options("defaultPackages") have not been changed.
>
>When typing ls(all=TRUE) I get a lot of variables names starting with
>. , most of them must have been defined by Rcmdr, but none seems
>to contain package name info. I have none .Last() or .First()
>function anywhere.
>
>Incidentally, one of the variables defined by Rcmdr, .logFont, if I
>try to print its value, Rgui bombs, repeatedly!
>
>This is all the info I have been able to find.
>I have no .Renviron file, anywhere.
>The only changes i made to .Rprofile was to get html help, and
>redefing the default editor.
>
>Kjetil Halvorsen
>
> >
> > Duncan M has been removing show(RConsole) calls and it is quite possible
> > that this is the cause, but we do need to be able to reproduce the
> > problem.
> >
> > On Thu, 17 Jul 2003, Roger D. Peng wrote:
> >
> > > This doesn't happen with me.  When I startup the focus is on the
> > > console.  I'm running
> > >
> > >  > version
> > >          _
> > > platform i386-pc-mingw32
> > > arch     i386
> > > os       mingw32
> > > system   i386, mingw32
> > > status
> > > major    1
> > > minor    7.1
> > > year     2003
> > > month    06
> > > day      16
> > > language R
> > >  >
> > >
> > > on Windows XP Home.  Maybe you have something else running in the
> > > background?
> > >
> > > -roger
> > >
> > > kjetil brinchmann halvorsen wrote:
> > >
> > > >Hola!
> > > >
> > > >Starting with rw1071, just after starting Rgui the scope is not with
> > > >Rconsole, but some other place. This means I have to do a mouse click
> > > >in the Rconsole before starting to write the first command. Since I
> > > >always forget this, I end up writing the first command twice.
> > > >
> > > >This is of course only a minor nuisance, but why is focus no longer
> > > >set to Rconsole window when start-up?
> > > >
> > > >This is on windows XP.
> > > >(pre-compiled binary from CRAN)
> > > >
> > > >Kjetil Halvorsen
> > > >
> > > >______________________________________________
> > > >R-help at stat.math.ethz.ch mailing list
> > > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > >
> > > >
> > > >
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ok at cs.otago.ac.nz  Fri Jul 18 04:25:55 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 18 Jul 2003 14:25:55 +1200 (NZST)
Subject: [R] confused about histograms
Message-ID: <200307180225.h6I2PtZs225914@atlas.otago.ac.nz>

Jay Pfaffman <pfaffman at relaxpc.com> wrote:
	I've got a data set with integer codes from 0--3.  I'd like a
	histogram with a single bar for 0, 1, 2 and 3.  I'd like each of the 4
	bars centered over a label.
	
> x <- rbinom(100, 3, 0.5)
> barplot(table(x))

This returns a vector of bar locations, which you can use to place
your labels.



From hp3000al at yahoo.com  Fri Jul 18 08:29:44 2003
From: hp3000al at yahoo.com (dg gdf)
Date: Thu, 17 Jul 2003 23:29:44 -0700 (PDT)
Subject: [R] what is "fuzzy matching"?
Message-ID: <20030718062944.11234.qmail@web14914.mail.yahoo.com>

hello.
I am a student that work on R.
I need to some data frames such as
"dolphins","kiwishade","cabbages","beams",... .
I typed 'help.seaech("dolphins")' but the response was
"No help files found with alias or title matching
'dolphins' using fuzzy matching.".
what is "fuzzy matching" and how can I find these data
frames?
bye.



From andel at ifi.unizh.ch  Fri Jul 18 09:25:34 2003
From: andel at ifi.unizh.ch (David Andel)
Date: Fri, 18 Jul 2003 07:25:34 -0000
Subject: [R] how to divide a string into characters? - for  comparing
	strings that is
In-Reply-To: <5.1.0.14.2.20030717185241.01fd0668@127.0.0.1>
Message-ID: <Pine.GSO.4.44.0307180924210.583-100000@igor>

Cool, thanks a lot!

On Thu, 17 Jul 2003, John Fox wrote:

> Dear David
>
> At 12:07 AM 7/18/2003 +0200, David Andel wrote:
>
> >I am searching for a way to do something like "ABC" -> c("A","B","C"). How
> >can this be accomplished?
> >
> >I tried cut() and split(), but they do something else, it seems.
> >
> >The purpose for doing this is to find the number of common (and uncommon)
> >characters, i.e. ultimately I want something like this:
> >
> > > foo("ABD","ADE")
> >c(2,1) # 2 in x are in y, 1 in y is not in x
> > > foo("AB","ADE")
> >c(1,2) # 1 in x is in y, 2 in y are not in x
> >
> >Maybe I even do not need the string splitting?
> >
> >I hope I was clear in stating my problem.
>
> Here's a solution based on splitting the strings:
>
>  > foo <- function(a, b){
> +     a <- strsplit(a, "")[[1]]
> +     b <- strsplit(b, "")[[1]]
> +     nmatch <- sum(outer(a, b, "=="))
> +     c(nmatch, length(b) - nmatch)
> +     }
>  > foo("ABD","ADE")
> [1] 2 1
>  > foo("AB","ADE")
> [1] 1 2
>
> By the way, apropos() turns up strsplit():
>
>  > apropos("split")
>   [1] "split"              "split.data.frame"   "split.data.frame<-"
>   [4] "split.default"      "split.screen"       "split<-"
>   [7] "split<-.data.frame" "split<-.default"    "strsplit"
> [10] "unsplit"
>
> I hope that this helps,
>   John
>
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>

-- 
David Andel, MD-PhD Student, Artificial Intelligence Laboratory
Institute of Information Technology, University of Zurich
Andreasstrasse 15, 8050 Zurich, Switzerland
Phone: +41-1-635 45 75, Fax: +41-1-635 68 09, e-mail: andel at ifi.unizh.ch
Office: AND 2.18, Homepage: http://www.ifi.unizh.ch/~andel
--
"Real knowledge is to know the extent of ones ignorance." - Confucius
"Two wrongs don't make a right, but three lefts do."
"Democracy is not a spectator sport."



From info at rhkoning.com  Fri Jul 18 09:31:41 2003
From: info at rhkoning.com (Ruud H. Koning)
Date: Fri, 18 Jul 2003 09:31:41 +0200
Subject: [R] C compiler to R
In-Reply-To: <3F16CD2D.8040000@lancaster.ac.uk>
References: <Pine.GSO.4.05.10307171203560.23871-100000@athenas.ime.unicamp.br>
	<3F16C0AD.9040406@consultoresestadisticos.com>
	<3F16CD2D.8040000@lancaster.ac.uk>
Message-ID: <200307180931410343.00315B6F@192.168.1.66>

I posted a similar question some time ago, and got the answer below. Ruud

*********** REPLY SEPARATOR  ***********

On 7/17/2003 at 5:22  Barry Rowlingson wrote:

>Carlos J. Gil Bellosta wrote:

>
>  Correct but possibly not sufficiently precise (where's Brian Ripley 
>when we need him?).
>



>On Sun, 10 Nov 2002, Ruud H. Koning wrote:
>
>> Hello, which C++ compiler is recommended for writing code to be used in
>> S-Plus and R? I am using windows2000 as operating system. Thanks, Ruud
>
>S-PLUS <= 2000  Watcom C++ (not really available any more)
>S-PLUS 6.x: Microsoft VC++6 (possibly hard to get now)
>R: MinGW port of gcc-3.2.
>
>In each case the information is in the documentation.  In all cases if you
>know what you are doing you can use any C++ compiler that can create a
>DLL, but using the recommended compiler makes life a lot easier when you
>want to call entry points in the S-PLUS/R main DLL.
>
>You can see further details and worked examples in `S Programming' and its
>on-line complements.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272860 (secr)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From ripley at stats.ox.ac.uk  Fri Jul 18 09:32:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 08:32:40 +0100 (BST)
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <5.1.0.14.2.20030717202741.01f765d0@127.0.0.1>
Message-ID: <Pine.LNX.4.44.0307180826290.6655-100000@gannet.stats>

I believe it's a side effect of starting up tcltk.  That grabs focus for
its invisible window: there's a kludgy workaround in the tcltk startup
code, but that it looks like that is not working during the startup
(which here seems to be something in .RData).

On Thu, 17 Jul 2003, John Fox wrote:

> Dear Kjetil , Roger,and Brian,
> 
> I'm not sure that this will shed any light on Kjetil's problems, but the 
> startup for Rcmdr is
> 
> .onAttach <- function(...){
>      cat("\nRcmdr Version 0.9-0\n")
>      Commander()
>      }
> 
> .onLoad <- function(...){
>      save.options <- options(warn=-1)
>      on.exit(options(save.options))
>      foreign <- require(foreign)
>      mva <- require(mva)
>      ctest <- require(ctest)
>      tcltk <- require(tcltk)
>      car <- require(car)
>      absent <- !c(foreign, mva, ctest, tcltk, car)
>      if (any(absent)) {
>          cat("\nThe following packages required by Rcmdr are missing:\n")
>          cat(paste(c("foreign", "mva", "ctest", "tcltk", "car")[absent], 
> collapse=", "))
>          cat("\n")
>          }
>      }
> 
> In a vanilla installation, mva and ctest are among the default packages, so 
> what one should normally see on loading Rcmdr is
> 
>  > library(Rcmdr)
> Loading required package: foreign
> Loading required package: tcltk
> Loading required package: car
> 
> Rcmdr Version 0.9-0
>  >
> 
> Another, possibly relevant, piece of information: .logFont is a Tcl 
> variable containing a font definition:
> 
>  > .logFont
> <Tcl> font1
> 
> If trying to print this variable causes Rgui to bomb, then perhaps 
> something is wrong with either the installation of the tcltk package or of 
> Tcl/Tk itself.
> 
> I'm aware as well that Kjetil has been unable to get Rcmdr to work 
> properly, for reasons that I've been unable to discover. By the way, Rcmdr 
> requires the SDI.
> 
> I hope that this is useful.
>   John
> 
> 
> At 07:21 PM 7/17/2003 -0400, kjetil brinchmann halvorsen wrote:
> >On 17 Jul 2003 at 20:59, Prof Brian Ripley wrote:
> >
> > > Can we check whether you are each running R in MDI or SDI mode (not that
> > > I have a problem with either, but then I use focus-follows-mouse)?
> >
> >I am running in MDI mode. Switching to SDI, problem disappears.
> >
> >But happens only with one of my many R startup icons, not with the
> >others! It happens when starting in a directory where I experimented
> >with the package Rcmdr by John Fox, and I guess that must have to do
> >with the problem. First, the startup message now includes
> >Loading required package: foreign
> >Loading required package: mva
> >Loading required package: ctest
> >Loading required package: tcltk
> >Loading required package: car
> >Loading required package: lattice
> >Loading required package: grid
> >Loading required package: nls
> >
> >loading extra packages compared with factory-fresh standard,
> >including car. I did'nt change any startup files, so this must have
> >been set by Rcmdr, but I don't know where.
> >
> >The value of options("defaultPackages") have not been changed.
> >
> >When typing ls(all=TRUE) I get a lot of variables names starting with
> >. , most of them must have been defined by Rcmdr, but none seems
> >to contain package name info. I have none .Last() or .First()
> >function anywhere.
> >
> >Incidentally, one of the variables defined by Rcmdr, .logFont, if I
> >try to print its value, Rgui bombs, repeatedly!
> >
> >This is all the info I have been able to find.
> >I have no .Renviron file, anywhere.
> >The only changes i made to .Rprofile was to get html help, and
> >redefing the default editor.
> >
> >Kjetil Halvorsen
> >
> > >
> > > Duncan M has been removing show(RConsole) calls and it is quite possible
> > > that this is the cause, but we do need to be able to reproduce the
> > > problem.
> > >
> > > On Thu, 17 Jul 2003, Roger D. Peng wrote:
> > >
> > > > This doesn't happen with me.  When I startup the focus is on the
> > > > console.  I'm running
> > > >
> > > >  > version
> > > >          _
> > > > platform i386-pc-mingw32
> > > > arch     i386
> > > > os       mingw32
> > > > system   i386, mingw32
> > > > status
> > > > major    1
> > > > minor    7.1
> > > > year     2003
> > > > month    06
> > > > day      16
> > > > language R
> > > >  >
> > > >
> > > > on Windows XP Home.  Maybe you have something else running in the
> > > > background?
> > > >
> > > > -roger
> > > >
> > > > kjetil brinchmann halvorsen wrote:
> > > >
> > > > >Hola!
> > > > >
> > > > >Starting with rw1071, just after starting Rgui the scope is not with
> > > > >Rconsole, but some other place. This means I have to do a mouse click
> > > > >in the Rconsole before starting to write the first command. Since I
> > > > >always forget this, I end up writing the first command twice.
> > > > >
> > > > >This is of course only a minor nuisance, but why is focus no longer
> > > > >set to Rconsole window when start-up?
> > > > >
> > > > >This is on windows XP.
> > > > >(pre-compiled binary from CRAN)
> > > > >
> > > > >Kjetil Halvorsen
> > > > >
> > > > >______________________________________________
> > > > >R-help at stat.math.ethz.ch mailing list
> > > > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > >
> > > > >
> > > > >
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > >
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul 18 10:04:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 09:04:39 +0100 (BST)
Subject: [R] How to install a package
In-Reply-To: <seo3hvoioladlcvp0cd9vsu3575dajh66f@4ax.com>
Message-ID: <Pine.LNX.4.44.0307180901410.6711-100000@gannet.stats>

On Sun, 13 Jul 2003, Duncan Murdoch wrote:

> On Sun, 13 Jul 2003 15:38:54 -0600, you wrote:
> 
> >My platform: R 1.7.0 + windows2000.
> >
> >I am trying to install the package "lasso2" which I saw in the
> >following web address:
> >http://cran.us.r-project.org/src/contrib/PACKAGES.html#emplik. However,
> >I failed to install it from R menu "Packages| Install package(s) from
> >CRAN" since I could not find this item in the list.
> 
> The list in the menu consists of packages that have been compiled for
> Windows.  It looks as though lasso2 won't compile for Windows without
> modifications.  If you want to use it, you'll need to find a compiled
> copy somewhere else, or compile it yourself.

It's worse: it will compile but it will not pass its own tests.  That's 
true on all my platforms with current R, not just Windows.

There is a ReadMe at

http://cran.us.r-project.org/bin/windows/contrib/1.7/ReadMe

Please read it!  Please also update to R 1.7.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Anne.Olga.Piotet at omsv.vd.ch  Fri Jul 18 10:14:30 2003
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Fri, 18 Jul 2003 10:14:30 +0200
Subject: [R] How to read in data
References: <002a01c3633b$652500d0$84dad10a@RENOVA4>
	<16148.5409.59928.239325@autan.toulouse.inra.fr>
Message-ID: <005601c34d04$9d4a8360$84dad10a@RENOVA4>

Thanks! yes it WORKS
Anne

----- Original Message -----
From: "Mathieu Ros" <mros at autan.toulouse.inra.fr>
To: "Anne Piotet" <Anne.Olga.Piotet at omsv.vd.ch>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, July 15, 2003 4:58 PM
Subject: Re: [R] How to read in data


> >>>>> "AP" == Anne Piotet <Anne.Olga.Piotet at omsv.vd.ch> disait:
>
>     AP> Hello, I'm new to R and in the process of testing it My first
>     AP> question: I fail to read in my data (ANSI toto.txt file, tab
>     AP> separated)
>     >> test <-read.table("toto.txt")
>     AP>             Error in file(file, "r") : unable to open
>     AP> connection In addition: Warning message: cannot open file
>     AP> `toto.txt'
>     >> test <- scan("C:\\toto.txt")
>     AP>             Error in scan("C:\\toto.txt") : "scan" expected a
>     AP> real, got "No_D"
>     >> test <-scan("test.dat")
>     AP>             Error in file(file, "r") : unable to open
>     AP> connection In addition: Warning message: cannot open file
>     AP> `toto.txt (and no, it is not read only or locked or whatever)
>     AP> I use Windows 2000/XP
>
> I think
> read.table("C:\\toto.txt",header=TRUE)
> will do the job : the message you got on your first and third attempts
> means that you gave a wrong path to your file.
> otherwise, read the help for read.table carefully (header and skip
parameters).
>
>     AP> second question...what are the size limits of statistical
>     AP> files I can handle? I plan to analize plant datas (up to
>     AP> 500'000 records, from which I will analize a restrictive set
>     AP> of variates ) Even when broken down by some chracteristics,
>     AP> the data to analize can have 50'000-100'000 records
>
>     AP> Well thank for the help Anne
> de rien ;)
>
> regards,
>
> --Mathieu
>
>



From wouter.buytaert at yucom.be  Fri Jul 18 10:43:40 2003
From: wouter.buytaert at yucom.be (wouter buytaert)
Date: Fri, 18 Jul 2003 08:43:40 -0000
Subject: [R] stepwise regression analysis
Message-ID: <1058517871.1935.2.camel@dhcp-69-016.agr.kuleuven.be>


Hello,

is there a function in R to do stepwise regression analysis (e.g. for
backward elimination)?

thanks,

Wouter



From v_bill_pikounis at merck.com  Fri Jul 18 11:07:27 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Fri, 18 Jul 2003 05:07:27 -0400
Subject: [R] univariate normal mixtures
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0197D4AA@usrymx18.merck.com>

Joke,

Two other places to help you with your objectives in fitting univariate
normal mixtures are:

1) The mclust package by Raftery and Fraley (available at CRAN).  Their
EMclust() function, for example, lets you specify a range of "number of
components" to fit multiple models as well as the ability to specify whether
to assume equal variances or not.  The Schwarz (BIC / SBC) criterion is used
to help distinguish goodness-of-fit amongst the models fitted.  I have found
the fitting routines to be more-than-quick enough under Linux, but did run
into problems when running the same code under Windows.

2) The Venables & Ripley MASS book, Editions 4 and earlier, provide a very
educational and useful discussion of analyses of mixture models beyond the
fitting considerations (which are nicely covered as well).  I do not have my
book copy with me at the moment, but I believe in the 4th edition the
material is covered in the last chapter entitled "Optimization".

Hope that Helps.

Best Regards,
Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

v_bill_pikounis at merck.com

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: Joke Allemeersch [mailto:Joke.Allemeersch at esat.kuleuven.ac.be] 
> Sent: Thursday, July 17, 2003 11:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] univariate normal mixtures
> 
> 
> Hello,
> 
> I have a concrete statistical question:
> I have a sample of an univariate mixture of an unknown number (k) of 
> normal distributions, each time with an unknown mean `m_i' and a 
> standard deviation `k * m_i', where k is known factor 
> constant for all 
> the normal distributions. (The `i' is a subscript.)
> Is there a function in R that can estimate the number of normal 
> distributions k and the means `m_i' for the different normal 
> distributions from a sample?  Or evt. a function that can 
> estimate the 
> `m_i', when the number of distributions `k' is known?
> So far I only found a package, called `normix'.  But at first 
> sight it 
> only provides methods to sample from such distributions and 
> to estimate 
> the densities; but not to fit such a distribution.
> Can someone indicate where I can find an elegant solution?
> 
> Thank you in advance
> 
> Joke Allemeersch
> 
> Katholieke universiteit Leuven.
> Belgium.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From theis at statistik.uni-dortmund.de  Fri Jul 18 11:12:32 2003
From: theis at statistik.uni-dortmund.de (Winfried Theis)
Date: Fri, 18 Jul 2003 09:12:32 -0000
Subject: [R] stepwise regression analysis
In-Reply-To: <1058517871.1935.2.camel@dhcp-69-016.agr.kuleuven.be>
References: <1058517871.1935.2.camel@dhcp-69-016.agr.kuleuven.be>
Message-ID: <1058521311.2862.5.camel@malepartus>

Hello!
On Fri, 2003-07-18 at 10:44, wouter buytaert wrote:
> 
> Hello,
> 
> is there a function in R to do stepwise regression analysis (e.g. for
> backward elimination)?
Try ?step and look at the options there.

Cheers,

Winfried
> 
> thanks,
> 
> Wouter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
-- 
---------------------------------------------------------------------
Dipl.-Math. Winfried Theis
SFB 475, Fachbereich Statistik, Universit"at Dortmund, 44221 Dortmund
Tel.: +49-231-755-5903 FAX: +49-231-755-4387



From ripley at stats.ox.ac.uk  Fri Jul 18 11:16:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 10:16:00 +0100 (BST)
Subject: [R] eigen vector sign reversal
In-Reply-To: <3F1437AA.6000608@stat.ucla.edu>
Message-ID: <Pine.LNX.4.44.0307181013530.15007-100000@gannet.stats>

On Tue, 15 Jul 2003, Roger D. Peng wrote:

> I think at version 1.7.0 R started using LAPACK for its eigen/svd 
> routines.  I think using `eigen(x, EISPACK = TRUE)' uses the previous 
> version.

That is true, but using different compilers on the same machine and the 
same version of R may give different signs for the eigenvectors.  The 
moral is, don't rely on the signs of eigenvectors!  (This *is* on the help 
page.)

> Karim Elsawy wrote:
> 
> >I've just installed R 1.7.1 under linux red hat
> >I noticed sign reversal of eigen vectors ,some of them not all, upon
> >using diag function relative to those obtained using R 1.4.1
> >this is gonna miss up lots of my previous scripts
> >I wonder if there is a way to avoid this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Jul 18 11:19:26 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 18 Jul 2003 11:19:26 +0200
Subject: (Fwd) Re: [R] what is "fuzzy matching"?
Message-ID: <3F17D7BE.9246.CD692F@localhost>

Try Google

I presume these are datasets used in J.Maindonald's Using R... 
Introduction text.


On 17 Jul 2003 at 23:29, dg gdf wrote:

> hello.
> I am a student that work on R.
> I need to some data frames such as
> "dolphins","kiwishade","cabbages","beams",... .
> I typed 'help.seaech("dolphins")' but the response was
> "No help files found with alias or title matching
> 'dolphins' using fuzzy matching.".
> what is "fuzzy matching" and how can I find these data
> frames?
> bye.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers

PetrPetr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From michael.watson at bbsrc.ac.uk  Fri Jul 18 11:32:19 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 18 Jul 2003 10:32:19 +0100
Subject: [R] R won't connect to the internet
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0091A@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

I can't get R to connect to the internet.  I am running R 1.7.1 on Windows XP and whenever I try to download packages etc from within R using the internet, it fails.

OK so I am behind a firewall and use a proxy server....

SO, if I go to my MS-DOS prompt and type:

	RGui.exe --internet2

everything works....

BUT i can't set up a shortcut for this as Windows (I hate Windows) complains that RGui.exe --internet2 is invalid and won't save it

So do I have to go into MS-DOS everytime I want to start up R???

Thanks
Mick



From th50 at leicester.ac.uk  Fri Jul 18 11:57:14 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 18 Jul 2003 10:57:14 +0100
Subject: [R] R won't connect to the internet
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46F4@saffron.cfs.le.ac.uk>

Dear Mick,

Have you tried to change the properties of your shortcut 
(right-click on the shortcut); add "--internet2" for the
target (works at least on W2K).

If that fails, create a small *.bat file, containing the 
line

\yourpath\Rgui --internet2

and probably, before executing this, insert the line

cd \yourHome

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> Sent: 18 July 2003 10:32
> To: R-help at stat.math.ethz.ch
> Subject: [R] R won't connect to the internet
> 
> 
> Hi
> 
> I can't get R to connect to the internet.  I am running R 
> 1.7.1 on Windows XP and whenever I try to download packages 
> etc from within R using the internet, it fails.
> 
> OK so I am behind a firewall and use a proxy server....
> 
> SO, if I go to my MS-DOS prompt and type:
> 
> 	RGui.exe --internet2
> 
> everything works....
> 
> BUT i can't set up a shortcut for this as Windows (I hate 
> Windows) complains that RGui.exe --internet2 is invalid and 
> won't save it
> 
> So do I have to go into MS-DOS everytime I want to start up R???
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Fri Jul 18 12:01:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 11:01:02 +0100 (BST)
Subject: [R] R won't connect to the internet
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C0091A@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0307181058100.15087-100000@gannet.stats>

On Fri, 18 Jul 2003, michael watson (IAH-C) wrote:

> I can't get R to connect to the internet.  I am running R 1.7.1 on
> Windows XP and whenever I try to download packages etc from within R
> using the internet, it fails.
> 
> OK so I am behind a firewall and use a proxy server....
> 
> SO, if I go to my MS-DOS prompt and type:
> 
> 	RGui.exe --internet2
> 
> everything works....
> 
> BUT i can't set up a shortcut for this as Windows (I hate Windows)
> complains that RGui.exe --internet2 is invalid and won't save it

Please ask someone who has a lesser aversion to your operating system (if
you hate it, why use it?) to show you how to make a shortcut. It can be
done, but you do need to give the full path to the executable in the
Target field, and you may need quotes in the correct places.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jul 18 12:06:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 11:06:38 +0100 (BST)
Subject: [R] "predict"
In-Reply-To: <71d457520a.7520a71d45@imap.epfl.ch>
Message-ID: <Pine.LNX.4.44.0307181104470.15087-100000@gannet.stats>

It uses the appropriate method for the generic function predict().
In your case it is predict.ar(), and you can examine it by

> getS3method("predict", "ar")


On Tue, 15 Jul 2003, ATHANASIA KAMARIOTIS wrote:

> Can you please tell me how R computes : " predict(ar.x)$pred "
> in :
> 
> #let x be a vector
> 
> ar.x<-ar(x)
> predict(ar.x)$pred

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tomas.willebrand at szooek.slu.se  Fri Jul 18 12:27:11 2003
From: tomas.willebrand at szooek.slu.se (Tomas Willebrand)
Date: Fri, 18 Jul 2003 12:27:11 +0200
Subject: [R] Harmonic mean center of animal locations
Message-ID: <200307181227.11737.tomas.willebrand@szooek.slu.se>

Hello list;

I am working with a dataset containing animal locations over time for a large 
number of individuals. I would like to compute a center of activity by 
finding the minimum harmonic mean on a grid overlaying the points of an 
animal (a "standard" to express center of activity in animal ecology). 

I have searched the archives at Jonathan Barons server but have not been able 
to find any package that seem to include such a function.

Do anyone of you know of such a function?

Yours 

Tomas Willebrand
Wildlife Ecologist

PS  I am aware of the dedicated home range programs available but planned to 
do most analysis in R.



From ripley at stats.ox.ac.uk  Fri Jul 18 12:31:25 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 11:31:25 +0100 (BST)
Subject: [R] AR in R
In-Reply-To: <5.1.0.14.2.20030716151426.01e75168@127.0.0.1>
Message-ID: <Pine.LNX.4.44.0307181128560.15087-100000@gannet.stats>

On Wed, 16 Jul 2003, John Fox wrote:

> Dear Steve,
> 
> At 11:45 AM 7/16/2003 -0700, CHRISS Steve wrote:
> >R Help,
> >How does one do a least squares regression with an AR component (for any
> >order) in R?
> >
> >Thanks,
> >Steve Chriss
> 
> Do you mean a model with AR errors? If so, you can use the gls (generalized 
> least squares) function in the nlme package. Some details and an example 
> with AR(2) errors are in 
> <http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix-timeseries-regression.pdf>.

You can also use the function arima() in R itself, and that is sometimes
more flexible.

Examples of both are in Venables & Ripley's MASS (2002).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wouter.buytaert at yucom.be  Fri Jul 18 12:35:05 2003
From: wouter.buytaert at yucom.be (wouter buytaert)
Date: Fri, 18 Jul 2003 10:35:05 -0000
Subject: [R] repeated measures anova
Message-ID: <1058524460.6378.7.camel@dhcp-69-016.agr.kuleuven.be>

Hello,

what is the best way to do an ANOVA on two-factor experiments with
repeated measures on one of the factors (e.g. time) in R, (with
Greenhouse-Geisser Epsilon or Huynh-Feldt Epsilon calculation, if
possible (as described in Ott and Longnecker, 2001. Statistical Methods
and Data Analysis 5th ed., chapter 18))

Thanks,

Wouter



From John.Marsland at CommerzbankIB.com  Fri Jul 18 12:49:25 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Fri, 18 Jul 2003 11:49:25 +0100
Subject: [R] R won't connect to the internet
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E852@xmx8lonib.lonib.commerzbank.com>

I also have used R1.7.1 behind a proxy server.

I've not used the --internet2 flag - I've not found that necessary.

Simply set up an environment variable in you control panel system properties
called "http_proxy" and set it value to something like
"http://100.100.100.100:8080/". Everything should work as normal for the
standard installation of R. If your proxy requires authorisation, a dialog
box will pop up and ask you to type in your userid and password - this seems
be remembered, so you don't need to type it very often.

Regards,

John Marsland

> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> Sent: 18 July 2003 10:32
> To: R-help at stat.math.ethz.ch
> Subject: [R] R won't connect to the internet
> 
> 
> Hi
> 
> I can't get R to connect to the internet.  I am running R 
> 1.7.1 on Windows XP and whenever I try to download packages 
> etc from within R using the internet, it fails.
> 
> OK so I am behind a firewall and use a proxy server....
> 
> SO, if I go to my MS-DOS prompt and type:
> 
> 	RGui.exe --internet2
> 
> everything works....
> 
> BUT i can't set up a shortcut for this as Windows (I hate 
> Windows) complains that RGui.exe --internet2 is invalid and 
> won't save it
> 
> So do I have to go into MS-DOS everytime I want to start up R???
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From borgulya at gyer2.sote.hu  Fri Jul 18 12:53:37 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Fri, 18 Jul 2003 12:53:37 +0200
Subject: summary: [R] numerical differentiation in R? (for optim "SANN"
	parscale)
In-Reply-To: <200307161642.JAA06575@hivnet.ubc.ca>
References: <3F152734.2010603@gyer2.sote.hu>
	<200307161642.JAA06575@hivnet.ubc.ca>
Message-ID: <3F17D1B1.4020401@gyer2.sote.hu>

Dear Wayne Jones, Ravi Varadhan, Roger D. Peng and Jerome Asselin,

Thank you for the helpful answers! I summarise them below and add my 
experiences:

The numerical differentiation
-----------------------------
 > Check out ?fdHess and run the example!
This was the solution. help(fdHess, package="nlme")

 > help(numericDeriv,package="nls")
This is a good solution, too. For my case, the above was more practical.


Running optim(..., method="SANN")
---------------------------------
 > You don't need to do any numerical differentiation in "optim", by
 > default it will automatically compute the derivatives via numerical
 > differentiation.
I was experimenting with optim a lot, and I found that "SANN" does not 
calculate derivatives.

 > For the other four methods 'optim' will do
 > numerical differentiation for you if a gradient is not provided.
This agrees with my observations.

 > 'optim' does not require any differentiation of the objective function
 > for the "SANN" method.
True, however, providing a 'parscale' based on the derivatives for 
"SANN" vastly accelerated its convergence. See below.


Role of 'parscale' optim(..., control=list(parscale=g, ...))
------------------------------------------------------------

For my function to optimise this was the solution:
library(nlme)
fd<-fdHess(start.values, modell.2)
g <- 1/fd$gradient
out<-optim(start.values, modell.2, method="SANN", hessian=TRUE, 
control=list(trace=2, parscale=g))

 > the 'parscale' argument has nothing to do with
 > differentiation.  As far as I know, it is used to scale the values of
 > the parameters before choosing candidates (so that they are roughly
 > comparable).
Differentiation was useful to examine the scales.

 > The help sais:
 > `parscale' A vector of scaling values for the parameters.
 >           Optimization is performed on `par/parscale' and these should
 >           be comparable in the sense that a unit change in any element
 >           produces about a unit change in the scaled value.

So, yes, 'parscale' is used to scale the parameters before choosing 
candidates. But choosing candidates seems to be critical: setting 
parscale to the reciprocials of the gradient values calculated at a good 
guess of the optimal parameters accelerated the convergence immensely.

In my case parscale values were very diverse, ranging from 1e-07 to 
1e+05. Without letting the optimisation procedure know these differences 
in the scales, it generated poor candidates.

Thanks you once more, and I hope you found my experiences useful.

G?bor

-- 
Gabor BORGULYA MD MSc
Semmelweis University of Budapest, 2nd Dept of Paediatrics
Hungarian Paediatric Cancer Registry



From runge at plan.auc.dk  Fri Jul 18 12:59:02 2003
From: runge at plan.auc.dk (jesper Runge Madsen)
Date: Fri, 18 Jul 2003 12:59:02 +0200
Subject: SV: [R] dbApply and data.frame
In-Reply-To: <x27k6g7sbr.fsf@biostat.ku.dk>
Message-ID: <000e01c34d1b$997bf7c0$b23da8c0@RUNGEF113>



-----Oprindelig meddelelse-----
Fra: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk] 
Sendt: 17. juli 2003 23:32
Til: Thomas W Blackwell
Cc: Jesper Runge Madsen; 'r-help'
Emne: Re: [R] dbApply and data.frame


>I.e. R is not *supposed* to crash, so either there is a platform
>dependency  or (more likely) it is triggered by an earlier memory
>corruption in dbApply() or the database backend. What kind of machine
>is this happening on? Would it be possible for you to run under the
>debugger so that we could get a bit more information.
> 
>-- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>

I am using a 900 Mhz Athlon, with 512 mb ram, and windows XP. And an IBM
Thinkpad T23 also with XP and 512 mb ram. 

I don't know how to run under the debugger but I have found out that if
I save my workspace, shuts down R and reopen R with the saved workspace,
then I can run 

Data.frame(LinieID=as.numeric(names(fraktil)),"quantile_85"=unlist(frakt
il))

With no problems, and even write my data to the MySQL afterwards. 

Can it be caused by the packages RMySQL and DBI. I have tried just to
detach them before converting my list to a data.frame but R still
crashes them.

/Jesper



From e6p at bigfoot.com  Fri Jul 18 13:02:55 2003
From: e6p at bigfoot.com (DJ)
Date: Fri, 18 Jul 2003 12:02:55 +0100
Subject: [R] Looking to maximize a conditional likelihood
Message-ID: <000301c34d1c$249e8f20$0100a8c0@Cybercom>

Can I ask if this method would be suitable for application to my problem?

I recently posted a question to which no one has replied:

https://www.stat.math.ethz.ch/pipermail/r-help/2003-July/034777.html

Basically I have multiple unique choice sets of various sizes on which I
want to apply a multinomial logit to optimise the attributes.

eg
Chosen  AttrQ   AttrW   Choices set size
0             8            0            3
1            20         34            3
0             7            2            3

0             5            3            4
0             3            5            4
1           25          18            4
0            4             9            4

1          30           12            2
0            2            4             2

Any help would be greatly appreciated - including any R code.


Thanks



From John.Marsland at CommerzbankIB.com  Fri Jul 18 13:17:56 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Fri, 18 Jul 2003 12:17:56 +0100
Subject: [R] R won't connect to the internet
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E854@xmx8lonib.lonib.commerzbank.com>

I have had a play with the --internet2 flag under windows NT and XP

if you set the target on your shortcut to:

"C:\Program Files\R\rw1071\bin\Rgui.exe" --internet2

it should work. I think your problem is to do with the placement of the
quote marks - it's a wonderfully horrible idea to be able to put spaces in
filenames...

Regards,

John

> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk]
> Sent: 18 July 2003 10:32
> To: R-help at stat.math.ethz.ch
> Subject: [R] R won't connect to the internet
> 
> 
> Hi
> 
> I can't get R to connect to the internet.  I am running R 
> 1.7.1 on Windows XP and whenever I try to download packages 
> etc from within R using the internet, it fails.
> 
> OK so I am behind a firewall and use a proxy server....
> 
> SO, if I go to my MS-DOS prompt and type:
> 
> 	RGui.exe --internet2
> 
> everything works....
> 
> BUT i can't set up a shortcut for this as Windows (I hate 
> Windows) complains that RGui.exe --internet2 is invalid and 
> won't save it
> 
> So do I have to go into MS-DOS everytime I want to start up R???
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From v_bill_pikounis at merck.com  Fri Jul 18 13:42:53 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Fri, 18 Jul 2003 07:42:53 -0400
Subject: [R] repeated measures anova
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0197D4AE@usrymx18.merck.com>

Wouter,

> what is the best way to do an ANOVA on two-factor experiments with
> repeated measures on one of the factors (e.g. time) in R, (with
> Greenhouse-Geisser Epsilon or Huynh-Feldt Epsilon calculation, if

You could use aov() with an Error term on your subjects factor; this follows
the classical "split-plot" approach. But I do not believe you will find any
existing R code to do the G-G or H-F corrections.  And I doubt there ever
will be, because the better strategy is to use linear mixed-effect modeling
approaches.  See for example, Diggle et al (Analysis of Longitudinal Data,
2001); Pinheiro & Bates (Mixed Effects Model with S, 2000); and Venables &
Ripley (MASS 4th edition, Sitka Trees example.)  You get the split-plot
approach with a uniform correlation (also called compound symmetry)
specification for your error matrix, but other alternatives are available.
If your repeated measure is time, my experience (with biology data) finds
that a first order autoregression or something similar is better. The
correlation between timepoint pairs weakens as the distance between them
increases.

Hope that helps.

Best Regards,
Bill


----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

v_bill_pikounis at merck.com

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: wouter buytaert [mailto:wouter.buytaert at yucom.be] 
> Sent: Friday, July 18, 2003 6:34 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] repeated measures anova
> 
> 
> Hello,
> 
> what is the best way to do an ANOVA on two-factor experiments with
> repeated measures on one of the factors (e.g. time) in R, (with
> Greenhouse-Geisser Epsilon or Huynh-Feldt Epsilon calculation, if
> possible (as described in Ott and Longnecker, 2001. 
> Statistical Methods
> and Data Analysis 5th ed., chapter 18))
> 
> Thanks,
> 
> Wouter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From mkondrin at hppi.troitsk.ru  Sat Jul 19 00:46:52 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 18 Jul 2003 15:46:52 -0700
Subject: [R] Formal definitions of R-language.
In-Reply-To: <200307180029.h6I0TORL128221@atlas.otago.ac.nz>
References: <200307180029.h6I0TORL128221@atlas.otago.ac.nz>
Message-ID: <3F1878DC.1040900@hppi.troitsk.ru>

Dear Richard A. O'Keefy, Peter Dalaard BSA, Thomas Lumley, Murad Nayal, 
Douglas Trainor, Uwe Liggs!
Thanks you once more!
Your answers are very interesting and the referencies you have provided 
will keep quiet those CS-guys for a while. Your answers themselves are a 
very good informal introduction (even for non-specialist like me) to 
formal basic of R-language. Hope you would not mind if I publish this 
thread at my site (http://www.hppi.troitsk.ru/Kondrin/index.htm) with a 
title something like "What is R?"



From f.calboli at ucl.ac.uk  Fri Jul 18 14:13:26 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Fri, 18 Jul 2003 13:13:26 +0100
Subject: [R] line colors in lattice.xyplot with png device.
Message-ID: <3.0.6.32.20030718131326.00abd418@pop-server.ucl.ac.uk>

for the background try: 

# open a trellis device. If not open the next step will not work
trellis.device(device=windows) #x11 in linux

# set background to white. Only works on an open trellis device
background<-trellis.par.get("background")
background$col<-"white"
trellis.par.set("background",background)

# set the default line colour to black
plot.line<-trellis.par.get("plot.line")
plot.line$col<-"black"
trellis.par.set("plot.line",plot.line)

# set the default symbol colour to black
dot.symbol<-trellis.par.get("dot.symbol")
dot.symbol$col<-"black"
trellis.par.set("dot.symbol",dot.symbol)

# set strip background color
strip.background<-trellis.par.get("strip.background")
strip.background$col<-"white"
trellis.par.set("strip.background",strip.background)

should be self explanatory

try ?par to get na idea on how to get different colours for the line. the
lattice/trellis manual is avalable in pdf from bell labs

FC


=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From andy_liaw at merck.com  Fri Jul 18 14:06:24 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 18 Jul 2003 08:06:24 -0400
Subject: [R] How to install a package
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8C6@usrymx25.merck.com>

> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> 
> On Sun, 13 Jul 2003, Duncan Murdoch wrote:
> 
> > On Sun, 13 Jul 2003 15:38:54 -0600, you wrote:
> > 
> > >My platform: R 1.7.0 + windows2000.
> > >
> > >I am trying to install the package "lasso2" which I saw in the 
> > >following web address: 
> > >http://cran.us.r-project.org/src/contrib/PACKAGES.html#emplik. 
> > >However, I failed to install it from R menu "Packages| Install 
> > >package(s) from CRAN" since I could not find this item in the list.
> > 
> > The list in the menu consists of packages that have been 
> compiled for 
> > Windows.  It looks as though lasso2 won't compile for 
> Windows without 
> > modifications.  If you want to use it, you'll need to find 
> a compiled 
> > copy somewhere else, or compile it yourself.
> 
> It's worse: it will compile but it will not pass its own 
> tests.  That's 
> true on all my platforms with current R, not just Windows.

In place of lasso2, Prof. Hastie's lars package (also on CRAN) can be used
(and it's faster, too).

HTH,
Andy

> 
> There is a ReadMe at
> 
http://cran.us.r-project.org/bin/windows/contrib/1.7/ReadMe

Please read it!  Please also update to R 1.7.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From J.Illian at abertay.ac.uk  Fri Jul 18 14:20:07 2003
From: J.Illian at abertay.ac.uk (J.Illian@abertay.ac.uk)
Date: Fri, 18 Jul 2003 13:20:07 +0100
Subject: [R] Hidden Markov estimation
Message-ID: <F934BF2710426B44833B8677AF47F2467489C4@mail3.tay.ac.uk>

Dear all,
is there a package in R that can be used to estimate the parameters in a
Hidden markov Model?

Thanks

Janine 

------------------------------------------
Janine Illian
lecturer in statistics
SIMBIOS
School of Computing and Advanced Technologies
University of Abertay Dundee
Bell Street
Dundee, DD1 1HG 
Scotland, UK
Tel: +44-(0)1382-308488
Fax: +44-(0)1382-308537



From manuel.coello at uca.es  Fri Jul 18 14:30:49 2003
From: manuel.coello at uca.es (Manuel Lopez Coello)
Date: Fri, 18 Jul 2003 14:30:49 +0200 (CEST)
Subject: [R] rterm
Message-ID: <20030718123049.38A7F1EA0D@perceval.uca.es>

I would like to use Rterm but i don`t know its parameters. I have searched about this issue but i haven´t found anything. thank you.



From ripley at stats.ox.ac.uk  Fri Jul 18 14:43:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 18 Jul 2003 13:43:52 +0100 (GMT Daylight Time)
Subject: [R] rterm
In-Reply-To: <20030718123049.38A7F1EA0D@perceval.uca.es>
Message-ID: <Pine.WNT.4.44.0307181339360.3004-100000@gannet.stats.ox.ac.uk>

Well, there's a file  README.rterm in the top-level (binary) installation
directory, and the section `Invoking R under Windows' in `An Introduction
to R' applies equally to RGui and Rterm.  All the information is in one of
those two places.

On Fri, 18 Jul 2003, Manuel Lopez Coello wrote:

> I would like to use Rterm but i don`t know its parameters. I have
> searched about this issue but i haven´t found anything. thank you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From james.lindsey at luc.ac.be  Fri Jul 18 15:18:30 2003
From: james.lindsey at luc.ac.be (Jim Lindsey)
Date: Fri, 18 Jul 2003 15:18:30 +0200 (MET DST)
Subject: [R] Hidden Markov estimation
In-Reply-To: <F934BF2710426B44833B8677AF47F2467489C4@mail3.tay.ac.uk> from
	"J.Illian@abertay.ac.uk" at Jul 18, 2003 01:20:07 PM
Message-ID: <200307181318.PAA18939@luc.ac.be>

> 
> Dear all,
> is there a package in R that can be used to estimate the parameters in a
> Hidden markov Model?

You might try my functions hidden (discrete time), chidden (continuous
time), and cphidden (find a change-point) functions in my repeated
library at www.luc.ac.be/~jlindsey/rcode.html
  Jim

> 
> Thanks
> 
> Janine 
> 
> ------------------------------------------
> Janine Illian
> lecturer in statistics
> SIMBIOS
> School of Computing and Advanced Technologies
> University of Abertay Dundee
> Bell Street
> Dundee, DD1 1HG 
> Scotland, UK
> Tel: +44-(0)1382-308488
> Fax: +44-(0)1382-308537
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ayalahec at msu.edu  Fri Jul 18 15:45:03 2003
From: ayalahec at msu.edu (Hector L. Ayala-del-Rio)
Date: Fri, 18 Jul 2003 09:45:03 -0400
Subject: [R] Base package R Macintosh port
Message-ID: <0884ED67-B926-11D7-881F-000393DB5846@msu.edu>

Hi everybody
    I was trying the R binary (1.7.0) for Macintosh (Carbon) and I tried 
to install some packages using "install.packages" and apparently this 
function was not implemented.  I reloaded the base library since the 
"update.packages" function should be on it but did not work.  Also I 
tried read.table and did not work either.  Does anybody know if the 
base package from Macinthosh port is different to the regular base 
package from the Unix port??

Thanks

Hector


H?ctor L. Ayala-del-R?o, Ph.D.
Center for Microbial Ecology &
Center for Genomic and Evolutionary Studies
on Microbial Life at Low Temperatures
Michigan State University
545 Plant & Soil Sciences Building
East Lansing, MI 48824-1325
Phone: 517-353-9021
Fax: 517-353-2917



From jfox at mcmaster.ca  Fri Jul 18 15:54:40 2003
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 18 Jul 2003 09:54:40 -0400
Subject: [R] Minor nuisance with rw1071
In-Reply-To: <Pine.LNX.4.44.0307180826290.6655-100000@gannet.stats>
References: <5.1.0.14.2.20030717202741.01f765d0@127.0.0.1>
Message-ID: <5.1.0.14.2.20030718094312.020eb6f0@127.0.0.1>

Dear Brian,

At 08:32 AM 7/18/2003 +0100, Prof Brian Ripley wrote:
>I believe it's a side effect of starting up tcltk.  That grabs focus for
>its invisible window: there's a kludgy workaround in the tcltk startup
>code, but that it looks like that is not working during the startup
>(which here seems to be something in .RData).

That there's something in .RData causing the problem is a good guess. To 
provide a bit more context: Kjetil reports to me that he's unable to use 
the Rcmdr GUI without generating "Warning: The use of _ is soon to be 
removed: you will be warned repeatedly" and parse errors. Of course, I 
don't use underscores in any of my code.

The only other similar report that I received was from Martin Maechler 
using Rcmdr under Emacs/ESS on a Linux machine. That problem went away when 
he ran R from a terminal rather than from Emacs, but perhaps that was 
coincidence. (Rcmdr works for me under XEmacs/ESS under Windows, though 
it's not a natural marriage.)

As a more general matter, I've encountered but been able to work around a 
variety of focus problems using the tcltk package.

Regards,
  John


-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From M.Mamin at intershop.de  Fri Jul 18 15:56:16 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Fri, 18 Jul 2003 15:56:16 +0200
Subject: [R] line colors in lattice.xyplot with png device.
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF056@jena03.net.j.ad.intershop.net>


here is the solution:



col<-c("#cc0000", "#330099", "#66cc00","#ff6600" ,"#ff00cc", "#00000",
"#bo7080", "#7080bo")
lty<-c(1,1,1,1,1,1,1,1)
lwd<-c(1,1,1,1,1,1,1,1)
mylines<-list(col=col,lty=lty,lwd=lwd)

filename<- "c:\\temp\\test.png"
trellis.device(png,filename = filename, width = 940, height = 600, pointsize
= 10,  bg = "white")

trellis.par.set("superpose.line",mylines)

xyplot(HITS+FREQ~TIME   |SERVER*LOCAL, data=subset(myd,
TYPE=="xxx"),allow.multiple = TRUE, scales = "same", 
type="l",xlim=range(myd$TIME), ylim=c(0,80000),layout=c(3,2),
key = list(lines = Rows(trellis.par.get("superpose.line"),
      c(1:2)), 
      text = list(lab=c("total hits", "hits per day")),
      columns = 2),
dev.off()




Thanks to Andy Liaw and Federico C.F. Calboli


Marc Mamin



From bates at stat.wisc.edu  Fri Jul 18 16:11:44 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 18 Jul 2003 14:11:44 -0000
Subject: [R] what is "fuzzy matching"?
In-Reply-To: <20030718062944.11234.qmail@web14914.mail.yahoo.com>
References: <20030718062944.11234.qmail@web14914.mail.yahoo.com>
Message-ID: <6r1xwn53h3.fsf@bates4.stat.wisc.edu>

dg gdf <hp3000al at yahoo.com> writes:

> I am a student that work on R.

> I need to some data frames such as
> "dolphins","kiwishade","cabbages","beams",... .

> I typed 'help.search("dolphins")' but the response was
> "No help files found with alias or title matching
> 'dolphins' using fuzzy matching.".

> what is "fuzzy matching" 

An attempt to find the name or variations of the name.  I imagine,
though I haven't tested it, that help.search("dolphins") would also
find "dolphin" and "Dolphins", etc.

> and how can I find these data frames?

Ask your instructor?  Reread the instructions for the exercise?



From tlumley at u.washington.edu  Fri Jul 18 16:22:05 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 18 Jul 2003 07:22:05 -0700 (PDT)
Subject: [R] Base package R Macintosh port
In-Reply-To: <0884ED67-B926-11D7-881F-000393DB5846@msu.edu>
Message-ID: <Pine.A41.4.44.0307180717200.39600-100000@homer36.u.washington.edu>

On Fri, 18 Jul 2003, Hector L. Ayala-del-Rio wrote:

> Hi everybody
>     I was trying the R binary (1.7.0) for Macintosh (Carbon) and I tried
> to install some packages using "install.packages" and apparently this
> function was not implemented.  I reloaded the base library since the
> "update.packages" function should be on it but did not work.  Also I
> tried read.table and did not work either.  Does anybody know if the
> base package from Macinthosh port is different to the regular base
> package from the Unix port??

The Carbon port does not have the necessary low-level support for
connecting to CRAN directly (and never will, as it has been frozen).  The
Darwin port has more or less all the features of the Unix port. It doesn't
have a GUI yet, but soon will.

On the other hand, read.table works (for files, not for urls).

	-thomas



From dmarta at giub.unibe.ch  Fri Jul 18 16:29:41 2003
From: dmarta at giub.unibe.ch (Paul Della-Marta)
Date: Fri, 18 Jul 2003 16:29:41 +0200
Subject: [R] Non-Linear Principle Component Analysis using Neural Networks
Message-ID: <1058538581.3f1804554b2a6@www.cx.unibe.ch>

Dear R-help,

I am a new member of the R-help mailing list and would like to know if anyone 
has used R to perform a Non-linear PCA using a multiple layer feed forward 
neural networks? 


Could I modify the function nnet to contain multiple hidden layers? 

Kind Regards,
Paul
_____________________________________________________

Paul Della-Marta
PhD Student                   
Meteorology and Climatology   
Physical Geography            
University of Bern


--------------------------------------------------
This mail sent through IMP at http://mail.unibe.ch



From znmeb at aracnet.com  Fri Jul 18 16:32:21 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Fri, 18 Jul 2003 07:32:21 -0700
Subject: [R] R won't connect to the internet
In-Reply-To: <Pine.LNX.4.44.0307181058100.15087-100000@gannet.stats>
Message-ID: <000d01c34d39$6995de90$74c463d8@plaza.ds.adp.com>

Actually, what I had to do was go into the "Properties" of the shortcut. The
"Target" on my system looks like this:


"C:\Program Files\R\rw1071\bin\Rgui.exe" --internet2

In other words, the *path* is in quotes because of the space in "Program
Files" but the "--internet2" is *outside* the quotes!

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Friday, July 18, 2003 3:01 AM
> To: michael watson (IAH-C)
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] R won't connect to the internet
> 
> 
> On Fri, 18 Jul 2003, michael watson (IAH-C) wrote:
> 
> > I can't get R to connect to the internet.  I am running R 1.7.1 on 
> > Windows XP and whenever I try to download packages etc from 
> within R 
> > using the internet, it fails.
> > 
> > OK so I am behind a firewall and use a proxy server....
> > 
> > SO, if I go to my MS-DOS prompt and type:
> > 
> > 	RGui.exe --internet2
> > 
> > everything works....
> > 
> > BUT i can't set up a shortcut for this as Windows (I hate Windows) 
> > complains that RGui.exe --internet2 is invalid and won't save it
> 
> Please ask someone who has a lesser aversion to your 
> operating system (if you hate it, why use it?) to show you 
> how to make a shortcut. It can be done, but you do need to 
> give the full path to the executable in the Target field, and 
> you may need quotes in the correct places.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Jul 18 16:36:16 2003
From: Ted.Harding at nessie.mcc.ac.uk (Ted Harding)
Date: Fri, 18 Jul 2003 15:36:16 +0100 (BST)
Subject: [R] Problem indexing into array
Message-ID: <200307181436.h6IEaHG63551@nessie.mcc.ac.uk>

Hi Folks,

Can anyone give me the tip I've been groping for with the
following question:?

mu: kx2x2x2 array of reals corresponding to means of k RVs
            at the combinations of values (1,2)x(1,2)x(1,2)
            of dichotomous variables F1,F2,F3
    mu prints out as k rows (one for each Xi) of 8 numbers

M:  N x (3+k) matrix of cases. The first 3 cols are values
            of (F1,F2,F3) as above (and the remainder are values
            of (X1,...,Xk) but this doesn't really matter).

AIM: For each row of M, find from mu the set of k means
    mu_1,...,mu_k corresponding to the values of F1, F2, F3
    in that row. E.g. if M[1,1:3] -> 2,1,1 I want mu[,2,1,1]

BUT: Not to have to do it by mu[,M[1,1],M[1,2],M[1,3]] but more
    like mu[,M[1,1:3]] -- except that this kind of try does not
    work, leading to error: "incorrect number of dimensions".

So:
HOW To construct an object F from M[1,1:3] such that

    mu[,F] gives mu[,2,1,1]

NEXT QUESTION: If the above can be done, I'd really like to be
able to "vectorise" it so as to get ti all at once from all the
rows of M, rather than having to bind the results of a loop over
the rows of M, i.e. achieve what you'd naively expect

    mu[,M[,1:3]]

to do (except that even mu[,M[1,1:3]] won't work).

With thanks,
Ted.



From spencer.graves at pdf.com  Fri Jul 18 16:53:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Jul 2003 07:53:13 -0700
Subject: [R] stepwise regression analysis
References: <1058517871.1935.2.camel@dhcp-69-016.agr.kuleuven.be>
	<1058521311.2862.5.camel@malepartus>
Message-ID: <3F1809D9.6090704@pdf.com>

Or stepAIC in the MASS library.  If you are adventurouos, you can 
experiment with the poorly debugged stepAIC.c downloadable from 
www.prodsyse.com.

spencer graves

Winfried Theis wrote:
> Hello!
> On Fri, 2003-07-18 at 10:44, wouter buytaert wrote:
> 
>>Hello,
>>
>>is there a function in R to do stepwise regression analysis (e.g. for
>>backward elimination)?
> 
> Try ?step and look at the options there.
> 
> Cheers,
> 
> Winfried
> 
>>thanks,
>>
>>Wouter
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From gkoru at engr.smu.edu  Fri Jul 18 17:27:12 2003
From: gkoru at engr.smu.edu (A. Gunes Koru)
Date: Fri, 18 Jul 2003 10:27:12 -0500 (CDT)
Subject: [R] Probability plotting with R
Message-ID: <Pine.SOL.4.05.10307181021190.4784-100000@spot.seas.smu.edu>

Hello,

Our professor asked us to do probability plotting using weibull paper,
exponential paper, normal, log-normal paper, etc. I know I can create Q-Q
plot for normal dist. and see if all te points are on one line. How do I
go about other distributions?

I tried generating different samples and use the general qq function.
However, I could not do it since I don't know the population parameters
(In normal QQ, we assume that the sample mean and variance were population
parameters too)

I was wondering if there is any way of probability plotting with R instead
of using the papers. I appreciate your helps in advance.

Regards

A. Gunes Koru



From tlumley at u.washington.edu  Fri Jul 18 17:50:01 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 18 Jul 2003 08:50:01 -0700 (PDT)
Subject: [R] Probability plotting with R
In-Reply-To: <Pine.SOL.4.05.10307181021190.4784-100000@spot.seas.smu.edu>
Message-ID: <Pine.A41.4.44.0307180849050.102880-100000@homer25.u.washington.edu>

On Fri, 18 Jul 2003, A. Gunes Koru wrote:

> Hello,
>
> Our professor asked us to do probability plotting using weibull paper,
> exponential paper, normal, log-normal paper, etc. I know I can create Q-Q
> plot for normal dist. and see if all te points are on one line. How do I
> go about other distributions?
>

The qqmath() function in the lattice package can do plots like those from
qqnorm() for other distributions.

	-thomas



From ripley at stats.ox.ac.uk  Fri Jul 18 17:58:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 16:58:47 +0100 (BST)
Subject: [R] Non-Linear Principle Component Analysis using Neural Networks
In-Reply-To: <1058538581.3f1804554b2a6@www.cx.unibe.ch>
Message-ID: <Pine.LNX.4.44.0307181655200.1298-100000@gannet.stats>

On Fri, 18 Jul 2003, Paul Della-Marta wrote:

> I am a new member of the R-help mailing list and would like to know if anyone 
> has used R to perform a Non-linear PCA using a multiple layer feed forward 
> neural networks? 
> 
> 
> Could I modify the function nnet to contain multiple hidden layers? 

Quite easily: the code is general and there are R-level functions to
specify the net, norm.net and add.net.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From John.Marsland at CommerzbankIB.com  Fri Jul 18 18:54:32 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Fri, 18 Jul 2003 17:54:32 +0100
Subject: [R] unz()
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E863@xmx8lonib.lonib.commerzbank.com>

I am having some problems with unz() under R 1.7.1 using windows NT 4.0

download.file(url="http://www.test.com/test.zip",
destfile="c:/temp/test.zip", mode="wb")
z <- unz("c:/temp/test.zip", "home/test.txt", open="r")
test <- read.table(z,sep=";",skip=1,header=T,as.is=T)
close(z)

this code seems temperamental and seems to work if you play around with
open(z) and isOpen(z) eventually ...

otherwise I get the error:

Error in readLines(file, skip) : seek not enabled for this connection

If anybody could offer advice?

Regards,

John Marsland


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Fri Jul 18 18:54:55 2003
From: Ted.Harding at nessie.mcc.ac.uk (Ted Harding)
Date: Fri, 18 Jul 2003 17:54:55 +0100 (BST)
Subject: [R] Problem indexing into array
In-Reply-To: <no.id> from "Ted Harding" at Jul 18, 2003 03:36:16 PM
Message-ID: <200307181654.h6IGstQ75494@nessie.mcc.ac.uk>

> Hi Folks,
> Can anyone give me the tip I've been groping for with the
> following question:?
> 
> mu: kx2x2x2 array of reals corresponding to means of k RVs
>             at the combinations of values (1,2)x(1,2)x(1,2)
>             of dichotomous variables F1,F2,F3
>     mu prints out as k rows (one for each Xi) of 8 numbers
Sorry -- this line may have misled. Suppose originally mu is
as it describes:

       F3=1  F3=1  F3=1  F3=1  F3=2  F3=2  F3=2  F3=2
       F2=1  F2=1  F2=2  F2=2  F2=1  F2=1  F2=2  F2=2
       F1=1  F1=2  F1=1  F1=2  F1=1  F1=2  F1=1  F1=2
mu_1  1.111 1.211 1.121 1.221 1.112 1.212 1.122 1.222
mu_2  2.111 2.211 2.121 2.221 2.112 2.212 2.122 2.222
...
mu_k  k.111 k.211 k.121 k.221 k.112 k.212 k.122 k.222

and suppose it is converted into an array:

  mu<-array(data=mu,dim=c(k,2,2,2))

Now the rest, below, refers to the latter form of mu.
Apologies for any confusion, and thanks for any help!

> 
> M:  N x (3+k) matrix of cases. The first 3 cols are values
>             of (F1,F2,F3) as above (and the remainder are values
>             of (X1,...,Xk) but this doesn't really matter).
> 
> AIM: For each row of M, find from mu the set of k means
>     mu_1,...,mu_k corresponding to the values of F1, F2, F3
>     in that row. E.g. if M[1,1:3] -> 2,1,1 I want mu[,2,1,1]
> 
> BUT: Not to have to do it by mu[,M[1,1],M[1,2],M[1,3]] but more
>     like mu[,M[1,1:3]] -- except that this kind of try does not
>     work, leading to error: "incorrect number of dimensions".
> 
> So:
> HOW To construct an object F from M[1,1:3] such that
> 
>     mu[,F] gives mu[,2,1,1]
> 
> NEXT QUESTION: If the above can be done, I'd really like to be
> able to "vectorise" it so as to get ti all at once from all the
> rows of M, rather than having to bind the results of a loop over
> the rows of M, i.e. achieve what you'd naively expect
> 
>     mu[,M[,1:3]]
> 
> to do (except that even mu[,M[1,1:3]] won't work).
> 
> With thanks,
> Ted.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From malbani at fas.harvard.edu  Fri Jul 18 19:30:21 2003
From: malbani at fas.harvard.edu (Marco Albani)
Date: Fri, 18 Jul 2003 13:30:21 -0400
Subject: [R] Positioning legends outside the plot region
Message-ID: <3F182EAD.3020301@fas.harvard.edu>


Hello,

I understand this issue has already been discussed
http://maths.newcastle.edu.au/~rking/R/help/01c/3763.html
but there is one point I am trying to understand better.

I want to lace a legend below the plotting region.
Right now I do


## increase size of the right hand border (secondary axis)
## and bottom border (legend)
old.mar <- par("mar")
old.mar[1] <- 7.1
old.mar[4] <- 7.1
old.par <- par(mar=old.mar)

plot(data$t, data$w, type="l",
        xlab =x.lab, ylab = y.lab, col="black", lwd=2,
        ...)

   ## Plot the fluxes on a secondary axis

    x <- range(data$w)
    y <- range(c(data$rain,data$uptake,data$perc))
    K <-  x[1] - y[1]*(x[2]-x[1])/(y[2]-y[1])
    R <- (x[2]-x[1])/(y[2]-y[1])
    y.seq <- pretty(y)

  lines(data$t, (K + R * data$uptake) , col ="green")
   axis(4, at = (K + R * y.seq), labels = y.seq)
   mtext(y2.lab, side=4, line=2)

   title(main = paste("Soil Water dynamics for site\n",data$site[1]),
       sub = NULL, xlab = NULL, ylab = NULL,
       line = NA, outer = FALSE)

  ## Set xpd=TRUE, so all plotting is clipped to the figure
  ## (not the plot) region:
  old.par <- par(xpd=TRUE)

## add the legend outside the plot on the bottom,
## centered on the plot x axis

x <- mean(par("usr")[1:2])
y <- par("usr")[3] - 1

     legend(x,y,
            legend=c("Soil Water", "Precipitation","Percolation","Uptake"),
            col=c("black","blue","red","green"),
            lwd=c(2,1,1,1),
            bty="n", xjust=0.5, ncol=2,
      )

  par(old.par)


The part I am not happy with is the setting of the y coordinates for the 
legend.

Is there a good way to tie it in with the par("mar")[1] so that I know 
the legend is place below the title of the x axis of my plot?




-- 
Marco Albani, Ph.D.
Postdoctoral Fellow
Dept. of Organismic and Evolutionary Biology
Harvard University
HUH 22 Divinity Avenue
Cambridge, MA
02138-2094 USA

Tel: +1 617 495 1621
Fax: +1 617 495 9484

http://www.people.fas.harvard.edu/~malbani



From pfaffman at relaxpc.com  Fri Jul 18 20:41:52 2003
From: pfaffman at relaxpc.com (Jay Pfaffman)
Date: Fri, 18 Jul 2003 11:41:52 -0700
Subject: [R] confused about x-coordinates and bar charts
Message-ID: <200307181841.h6IIfqXW009447@aaalab.Stanford.EDU>

Thanks for all the help on my previous histogram problem.  I intend to
summarize the solutions back to the list Real Soon Now, but first, I've
got another problem.

I've made a bar chart that reports means.  I'd like to put the number
of observations on top of each bar.  Here's what I've got:

barplot((subset$x),
        col=grey(.5),
        ylab="Mean Engagement Rating",
        xlab="Comment Category",
        main="All Engagement Ratings",
        ylim=c(0,7),
        cex.names=.75,
        names.arg=mynames
        )


for (i in 1:7){
  text(i,as.numeric(subset$x[i])+.5,counts$x[i])
}


This would seem to do the trick, but the numbers are not centered over
the bars.  They're a bit to the right on the bars on the left and
almost right on the bars on the right.  I don't understand why the
scale is not the same.  

Thanks again for your help.

-- 
Jay Pfaffman                           pfaffman at relaxpc.com
+1-415-821-7507 (H)                    +1-415-812-5047 (M)



From adrian_humbert at yahoo.com  Fri Jul 18 20:45:01 2003
From: adrian_humbert at yahoo.com (Adi Humbert)
Date: Fri, 18 Jul 2003 11:45:01 -0700 (PDT)
Subject: [R] question about formulating a nls optimization
Message-ID: <20030718184501.69402.qmail@web12108.mail.yahoo.com>

Dear list, 

I'm migrating a project from Matlab to R, and I'm
facing a relatively complicated problem for nls.  My
objective function is below:

>> objFun <- function(yEx,xEx,tEx,gamma,theta,kappa){
     yTh <- pdfDY(xEx,tEx,gamma,theta,kappa)
     sum(log(yEx/yTh)^2)
   }

The equation is yTh=P(xEx,tEx) + noise.
I collect my data in:

>> data <- data.frame(xEx,tEx,yEx)

And I do the nls:
aux <- nls( ~ objFun(yEx,xEx,tEx,gamma,theta,kappa), 
           data=data,
   start=list(gamma=0.085, theta=8.62*10^(-5),
kappa=4.45*10^(-3)), trace=TRUE)

I get the following error: 
179.8911 :  8.50e-02 8.62e-05 4.45e-03 
Error in sum(rr[-(1:npar)]^2) : subscript out of
bounds

One problem that I identified is that the algorithm
does not change the values of the parameters at all. 
It calculates the objective function OK, but the
parameters are not updated.  

The function pdfDY is a Fourier integral with
parameters xEx, tEx, gamma, theta, kappa, that I 
calculate with a Filon algorithm.  The calculation of
pdfDY is correct.  

Can anybody help me with this?  Thank you very much, 

Adrian



From malbani at fas.harvard.edu  Fri Jul 18 20:54:43 2003
From: malbani at fas.harvard.edu (Marco Albani)
Date: Fri, 18 Jul 2003 14:54:43 -0400
Subject: [R] Re: Positioning legends outside the plot region
References: <3F182EAD.3020301@fas.harvard.edu>
Message-ID: <3F184273.3000707@fas.harvard.edu>

Marco Albani wrote:
> 
> The part I am not happy with is the setting of the y coordinates for the 
> legend.
> 
> Is there a good way to tie it in with the par("mar")[1] so that I know 
> the legend is place below the title of the x axis of my plot?
> 

I answer my own question:


## Set xpd=TRUE, so all plotting is clipped to the figure
## (not the plot) region:
old.par <- par(xpd=TRUE)

#add the legend outside the plot on the bottom, centered on the plot x axis
x <- mean(par("usr")[1:2])

## calculate the ratio of inches to y user coords
in2y <- (par("usr")[4] - par("usr")[3])/par("pin")[2]

## place y at the bottom of the margin
y <-  par("usr")[3] - par("mai")[1] * in2y

## justify the legend so that is centered in x (xjust=0.5)
## and on top of y (yjust=0)
legend(x,y,
            legend=c("Soil Water", "Precipitation","Percolation","Uptake"),
            col=c("black","blue","red","green"),
            lwd=c(2,1,1,1),
            bty="n", xjust=0.5, ncol=2, yjust=0
      )

par(old.par)


-- 
Marco Albani, Ph.D.
Postdoctoral Fellow
Dept. of Organismic and Evolutionary Biology
Harvard University
HUH 22 Divinity Avenue
Cambridge, MA
02138-2094 USA

Tel: +1 617 495 1621
Fax: +1 617 495 9484

http://www.people.fas.harvard.edu/~malbani



From p.dalgaard at biostat.ku.dk  Fri Jul 18 21:04:02 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 18 Jul 2003 19:04:02 -0000
Subject: [R] question about formulating a nls optimization
In-Reply-To: <20030718184501.69402.qmail@web12108.mail.yahoo.com>
References: <20030718184501.69402.qmail@web12108.mail.yahoo.com>
Message-ID: <x2d6g764c8.fsf@biostat.ku.dk>

Adi Humbert <adrian_humbert at yahoo.com> writes:

> Dear list, 
> 
> I'm migrating a project from Matlab to R, and I'm
> facing a relatively complicated problem for nls.  My
> objective function is below:
> 
> >> objFun <- function(yEx,xEx,tEx,gamma,theta,kappa){
>      yTh <- pdfDY(xEx,tEx,gamma,theta,kappa)
>      sum(log(yEx/yTh)^2)
>    }
> 
> The equation is yTh=P(xEx,tEx) + noise.
> I collect my data in:
> 
> >> data <- data.frame(xEx,tEx,yEx)
> 
> And I do the nls:
> aux <- nls( ~ objFun(yEx,xEx,tEx,gamma,theta,kappa), 
>            data=data,
>    start=list(gamma=0.085, theta=8.62*10^(-5),
> kappa=4.45*10^(-3)), trace=TRUE)
> 
> I get the following error: 
> 179.8911 :  8.50e-02 8.62e-05 4.45e-03 
> Error in sum(rr[-(1:npar)]^2) : subscript out of
> bounds
> 
> One problem that I identified is that the algorithm
> does not change the values of the parameters at all. 
> It calculates the objective function OK, but the
> parameters are not updated.  
> 
> The function pdfDY is a Fourier integral with
> parameters xEx, tEx, gamma, theta, kappa, that I 
> calculate with a Filon algorithm.  The calculation of
> pdfDY is correct.  
> 
> Can anybody help me with this?  Thank you very much, 

Your objFun() is returning a scalar where nls is expecting a vector of
residuals. Try making it return just log(yEx/yTh), or rewrite as 

log(yEx) ~ log(pdfDY(xEx,tEx,gamma,theta,kappa))

or use optim() to minimize general functions.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at medanalytics.com  Fri Jul 18 21:12:52 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 18 Jul 2003 19:12:52 -0000
Subject: [R] confused about x-coordinates and bar charts
In-Reply-To: <200307181841.h6IIfqXW009447@aaalab.Stanford.EDU>
References: <200307181841.h6IIfqXW009447@aaalab.Stanford.EDU>
Message-ID: <1058555575.9353.46.camel@localhost>

On Fri, 2003-07-18 at 13:41, Jay Pfaffman wrote:
> Thanks for all the help on my previous histogram problem.  I intend to
> summarize the solutions back to the list Real Soon Now, but first, I've
> got another problem.
> 
> I've made a bar chart that reports means.  I'd like to put the number
> of observations on top of each bar.  Here's what I've got:
> 
> barplot((subset$x),
>         col=grey(.5),
>         ylab="Mean Engagement Rating",
>         xlab="Comment Category",
>         main="All Engagement Ratings",
>         ylim=c(0,7),
>         cex.names=.75,
>         names.arg=mynames
>         )
> 
> 
> for (i in 1:7){
>   text(i,as.numeric(subset$x[i])+.5,counts$x[i])
> }
> 
> 
> This would seem to do the trick, but the numbers are not centered over
> the bars.  They're a bit to the right on the bars on the left and
> almost right on the bars on the right.  I don't understand why the
> scale is not the same.  
> 
> Thanks again for your help.


You need to get the bar midpoints (which are not integer values) and are
returned by barplot() 'silently'. You must assign these to a separate
variable to be used when calling barplot().  

Thus, change your code to:

mp <- barplot(subset$x,
        col=grey(.5),
        ylab="Mean Engagement Rating",
        xlab="Comment Category",
        main="All Engagement Ratings",
        ylim=c(0,7),
        cex.names=.75,
        names.arg=mynames
        )

Now to place the bar values above the bar tops use: 

text(mp, subset$x, labels = subset$x, pos = 3)

The 'pos = 3' argument, will place the text above the top of the bars. 

Also, note that you do not need a for loop here. Remember, R is vector
based. A single call to text() will cycle through the values.

See ?barplot and ?text for more information.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Fri Jul 18 21:15:51 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 18 Jul 2003 19:15:51 -0000
Subject: [R] question about formulating a nls optimization
In-Reply-To: <20030718184501.69402.qmail@web12108.mail.yahoo.com>
References: <20030718184501.69402.qmail@web12108.mail.yahoo.com>
Message-ID: <6rispzhci8.fsf@bates4.stat.wisc.edu>

Adi Humbert <adrian_humbert at yahoo.com> writes:

> I'm migrating a project from Matlab to R, and I'm
> facing a relatively complicated problem for nls.  My
> objective function is below:
> 
> >> objFun <- function(yEx,xEx,tEx,gamma,theta,kappa){
>      yTh <- pdfDY(xEx,tEx,gamma,theta,kappa)
>      sum(log(yEx/yTh)^2)
>    }
> 
> The equation is yTh=P(xEx,tEx) + noise.
> I collect my data in:
> 
> >> data <- data.frame(xEx,tEx,yEx)
> 
> And I do the nls:
> aux <- nls( ~ objFun(yEx,xEx,tEx,gamma,theta,kappa), 
>            data=data,
>    start=list(gamma=0.085, theta=8.62*10^(-5),
> kappa=4.45*10^(-3)), trace=TRUE)
> 
> I get the following error: 
> 179.8911 :  8.50e-02 8.62e-05 4.45e-03 
> Error in sum(rr[-(1:npar)]^2) : subscript out of
> bounds

Could you run traceback() at this point so we can see where the error
message is originating?  I would hope that npar is 3 and my guess is
that the error message is originating in a call to

           conv = function()
           {
               rr <- qr.qty( QR, resid ) # rotated residual vector
               sqrt( sum( rr[1:npar]^2 ) / sum( rr[ -(1:npar) ]^2 ) )
           },

How many rows are there in your data frame?

> One problem that I identified is that the algorithm
> does not change the values of the parameters at all. 
> It calculates the objective function OK, but the
> parameters are not updated.  

> The function pdfDY is a Fourier integral with
> parameters xEx, tEx, gamma, theta, kappa, that I 
> calculate with a Filon algorithm.  The calculation of
> pdfDY is correct.



From flom at ndri.org  Fri Jul 18 21:20:54 2003
From: flom at ndri.org (Peter Flom)
Date: Fri, 18 Jul 2003 15:20:54 -0400
Subject: [R] Tree question with ordinal IV and split labels
Message-ID: <sf181063.053@MAIL.NDRI.ORG>

Hi

I have a tree, created in rpart.  One of the IVs is an ordinal
variable.  I would like to get the splits labeled with the LEVELS of
this factor, rather than a,b,c, and d.  

Is there a way to do this?

Thanks in advance



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From MSchwartz at medanalytics.com  Fri Jul 18 21:25:56 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 18 Jul 2003 19:25:56 -0000
Subject: [R] confused about x-coordinates and bar charts
In-Reply-To: <1058555575.9353.46.camel@localhost>
References: <200307181841.h6IIfqXW009447@aaalab.Stanford.EDU>
	<1058555575.9353.46.camel@localhost>
Message-ID: <1058556358.9353.51.camel@localhost>

On Fri, 2003-07-18 at 14:12, Marc Schwartz wrote:
> On Fri, 2003-07-18 at 13:41, Jay Pfaffman wrote:
> > Thanks for all the help on my previous histogram problem.  I intend to
> > summarize the solutions back to the list Real Soon Now, but first, I've
> > got another problem.
> > 
> > I've made a bar chart that reports means.  I'd like to put the number
> > of observations on top of each bar.  Here's what I've got:
> > 
> > barplot((subset$x),
> >         col=grey(.5),
> >         ylab="Mean Engagement Rating",
> >         xlab="Comment Category",
> >         main="All Engagement Ratings",
> >         ylim=c(0,7),
> >         cex.names=.75,
> >         names.arg=mynames
> >         )
> > 
> > 
> > for (i in 1:7){
> >   text(i,as.numeric(subset$x[i])+.5,counts$x[i])
> > }
> > 
> > 
> > This would seem to do the trick, but the numbers are not centered over
> > the bars.  They're a bit to the right on the bars on the left and
> > almost right on the bars on the right.  I don't understand why the
> > scale is not the same.  
> > 
> > Thanks again for your help.
> 
> 
> You need to get the bar midpoints (which are not integer values) and are
> returned by barplot() 'silently'. You must assign these to a separate
> variable to be used when calling barplot().  
> 
> Thus, change your code to:
> 
> mp <- barplot(subset$x,
>         col=grey(.5),
>         ylab="Mean Engagement Rating",
>         xlab="Comment Category",
>         main="All Engagement Ratings",
>         ylim=c(0,7),
>         cex.names=.75,
>         names.arg=mynames
>         )
> 
> Now to place the bar values above the bar tops use: 
> 
> text(mp, subset$x, labels = subset$x, pos = 3)
> 
> The 'pos = 3' argument, will place the text above the top of the bars. 
> 
> Also, note that you do not need a for loop here. Remember, R is vector
> based. A single call to text() will cycle through the values.
> 
> See ?barplot and ?text for more information.
> 
> HTH,
> 
> Marc Schwartz


Quick correction to my own reply, which is that the values to be printed
above the bars were not the means but the counts.  Hence use:

text(mp, subset$x, labels = counts$x, pos = 3)

Sorry for the oversight.

Marc



From Iyue.Sung at ingenix.com  Fri Jul 18 21:35:17 2003
From: Iyue.Sung at ingenix.com (Sung, Iyue)
Date: Fri, 18 Jul 2003 15:35:17 -0400
Subject: [R] Tree question with ordinal IV and split labels
Message-ID: <98A6B2BB08F027408F2E8ADDC58412631087F7@mail.epidemiology.com>


Use "pretty" option in "text" function:

plot(mytree)
text(mytree, pretty=0)

> -----Original Message-----
> From: Peter Flom [mailto:flom at ndri.org]
> Sent: Friday, July 18, 2003 3:21 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Tree question with ordinal IV and split labels
> 
> 
> Hi
> 
> I have a tree, created in rpart.  One of the IVs is an ordinal
> variable.  I would like to get the splits labeled with the LEVELS of
> this factor, rather than a,b,c, and d.  
> 
> Is there a way to do this?
> 
> Thanks in advance
> 
> 
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

"Secure Server <ingenixps.com>" made the following
 annotations on 07/18/03 15:35:19
------------------------------------------------------------------------------
"This e-mail, including attachments, may include confidential and/or proprietary information, and may be used only by the person or entity to which it is addressed. If the reader of this e-mail is not the intended recipient or his or her authorized agent, the reader is hereby notified that any dissemination, distribution or copying of this e-mail is prohibited. If you have received this e-mail in error, please notify the sender by replying to this message and delete this e-mail immediately."



From bates at stat.wisc.edu  Fri Jul 18 21:41:37 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 18 Jul 2003 19:41:37 -0000
Subject: [R] question about formulating a nls optimization
In-Reply-To: <6rispzhci8.fsf@bates4.stat.wisc.edu>
References: <20030718184501.69402.qmail@web12108.mail.yahoo.com>
	<6rispzhci8.fsf@bates4.stat.wisc.edu>
Message-ID: <6rznjbfwqq.fsf@bates4.stat.wisc.edu>

No need to respond to my suggestion about traceback().  Peter has
detected the problem.



From jpmorgan at vt.edu  Fri Jul 18 21:56:31 2003
From: jpmorgan at vt.edu (J. P. Morgan)
Date: Fri, 18 Jul 2003 15:56:31 -0400
Subject: [R] lexicographic sort of ordered lists
Message-ID: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030718/bf043e54/attachment.pl

From liping66 at hotmail.com  Fri Jul 18 22:02:33 2003
From: liping66 at hotmail.com (liping)
Date: Fri, 18 Jul 2003 16:02:33 -0400
Subject: [R] problem with memory size in UNIX
Message-ID: <BAY1-DAV39hhIOp37Q600007094@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030718/371d8edc/attachment.pl

From Amer.Siddique at ssa.gov  Fri Jul 18 22:01:49 2003
From: Amer.Siddique at ssa.gov (Siddique, Amer)
Date: Fri, 18 Jul 2003 15:01:49 -0500
Subject: [R] create a vector looping over a frame 
Message-ID: <F151C5EFCA66314D9FE66CB7199F4AAD5A3451@sad6cd1.ch.ssa.gov>

Hello,

I have a data.frame

> names(popA)
 [1] "Year"   "Series" "Age"    "WM"     "WF"     "HM"     "HF"     "BM"    
 [9] "BF"     "IM"     "IF"     "AM"     "AF"     "Yr"  

how do i loop over a subset of variables in this frame to create a vector of
length equal to the number of variables in the subset such that the vector's
ith element is the result of an aggregate fncn applied to the ith variable?

eg,
sumvar<-c(sum(WM),...,sum(AF))

if i try

for(i in 4:13){
sumVar<- sum(popA[,i])
sumVar2<-c(sumVar) 
}

it returns 

> sumVar2
[1] 287567

> length(sumVar2)
[1] 1

which is only the value at the last spot. i cant quite figure this simple
loop. 

Thanks,

Amer Siddique



From Ted.Harding at nessie.mcc.ac.uk  Fri Jul 18 22:13:33 2003
From: Ted.Harding at nessie.mcc.ac.uk (Ted Harding)
Date: Fri, 18 Jul 2003 21:13:33 +0100 (BST)
Subject: [R] Problem indexing into array
In-Reply-To: <no.id> from "Patrick Burns" at Jul 18, 2003 08:11:40 PM
Message-ID: <200307182013.h6IKDX285480@nessie.mcc.ac.uk>

> mu[M[,1:3]]
> 
> should do what you want, I think.  See chapter 1 of S Poetry.
> 
> Patrick Burns

Thanks Patrick (and honoured to hear from the author of S-Poetry!).

This doesn't do what I want (I was probably not clear), though
Tony Plate's reply looks promising if maybe cumbersome.
(Thanks, Tony).

Basically: Given that mu was made by

  mu<-array(data=x, dim=c(9,2,2,2))

say, where x has 72 reals in it, and given that the first three columns
of M contain values (1 or 2) of p,q,r, I want to recover from mu
the 9 values mu[,p,q,r] where (p,q,r) is the result of M[i,1:3] for
some value of i.

I don't want to do this as mu[,M[,1],M[,2],M[,3]] because in fact
this is taking place inside a function where the dimensions of the
arrays thrown at the function will not be known beforehand.

And, if the result can be got for all rows of M in one swoop, instead
of looping through the rows of M, all the better!

Thanks!
Ted.



From dmurdoch at pair.com  Fri Jul 18 22:30:48 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 18 Jul 2003 16:30:48 -0400
Subject: [R] lexicographic sort of ordered lists
In-Reply-To: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>
References: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>
Message-ID: <2jlghvgsba5j2kttkhetjk5mel0njnl6gc@4ax.com>

On Fri, 18 Jul 2003 15:56:31 -0400, "J. P. Morgan" <jpmorgan at vt.edu>
wrote :

>Does anyone know how to execute the following sort problem in R? Matrix X
>has positive integer entries, and each column has been sorted in ascending
>order. The problem is now to order the columns lexicographically. For
>instance if the matrix X is
 
...

>but I need a method that will work regardless of k=number of rows of X. That
>is, the program must be able to accept any integer-valued matrix X for which
>each column is sorted, then permute columns accordingly. 

You could do it in a loop, if you had a "stable" order() function.  I
don't know if the standard order() is stable; here's one that is
stable:

stableorder <- function(x, ...) order(x, ...,1:length(x))

Then the idea is to loop from the last row to the first, sorting
columns in a stable way:

lexico <- function(X) {
  for (i in nrow(X):1) {
    X <- X[, stableorder(X[i,])]
  }
  X
}

(I just tried this with the regular order(), and got the same result,
so order() might be stable, but I wouldn't trust it to be...)

Duncan Murdoch



From spencer.graves at pdf.com  Fri Jul 18 22:30:15 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Jul 2003 13:30:15 -0700
Subject: [R] lexicographic sort of ordered lists
References: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>
Message-ID: <3F1858D7.9090102@pdf.com>

"?order"?

J. P. Morgan wrote:
> Does anyone know how to execute the following sort problem in R? Matrix X
> has positive integer entries, and each column has been sorted in ascending
> order. The problem is now to order the columns lexicographically. For
> instance if the matrix X is
> 
>  
> 
> 1 2 1 1 2
> 
> 2 2 3 3 2
> 
> 3 5 5 4 2
> 
>  
> 
> then the result should be
> 
>  
> 
> 1 1 1 2 2
> 
> 2 3 3 2 2
> 
> 3 4 5 2 5
> 
>  
> 
> Let ONE be a vector of 1's of length ncol(X). The result in this example can
> be accomplished by
> 
>  
> 
> X=X[,order(ONE,X[1,],X[,2],X[,3])]
> 
>  
> 
> but I need a method that will work regardless of k=number of rows of X. That
> is, the program must be able to accept any integer-valued matrix X for which
> each column is sorted, then permute columns accordingly. 
> 
>  
> 
> Thanks, JP
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Jul 18 22:52:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jul 2003 21:52:56 +0100 (BST)
Subject: [R] lexicographic sort of ordered lists
In-Reply-To: <2jlghvgsba5j2kttkhetjk5mel0njnl6gc@4ax.com>
Message-ID: <Pine.LNX.4.44.0307182150470.24819-100000@gannet.stats>

On Fri, 18 Jul 2003, Duncan Murdoch wrote:

> On Fri, 18 Jul 2003 15:56:31 -0400, "J. P. Morgan" <jpmorgan at vt.edu>
> wrote :
> 
> >Does anyone know how to execute the following sort problem in R? Matrix X
> >has positive integer entries, and each column has been sorted in ascending
> >order. The problem is now to order the columns lexicographically. For
> >instance if the matrix X is
>  
> ...
> 
> >but I need a method that will work regardless of k=number of rows of X. That
> >is, the program must be able to accept any integer-valued matrix X for which
> >each column is sorted, then permute columns accordingly. 
> 
> You could do it in a loop, if you had a "stable" order() function.  I
> don't know if the standard order() is stable; here's one that is
> stable:

It is: why is that not on the help page, I wonder (nor for sort)?

> 
> stableorder <- function(x, ...) order(x, ...,1:length(x))
> 
> Then the idea is to loop from the last row to the first, sorting
> columns in a stable way:
> 
> lexico <- function(X) {
>   for (i in nrow(X):1) {
>     X <- X[, stableorder(X[i,])]
>   }
>   X
> }
> 
> (I just tried this with the regular order(), and got the same result,
> so order() might be stable, but I wouldn't trust it to be...)
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From RBaskin at ahrq.gov  Fri Jul 18 23:28:13 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Fri, 18 Jul 2003 17:28:13 -0400
Subject: [R] create a vector looping over a frame 
Message-ID: <3598558AD728D41183350008C7CF291C0F16B8B2@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030718/2f2d2ec9/attachment.pl

From tanjinho at yahoo.com  Fri Jul 18 23:43:24 2003
From: tanjinho at yahoo.com (Jin Ho Tan)
Date: Fri, 18 Jul 2003 14:43:24 -0700 (PDT)
Subject: [R] Rserv.conf
Message-ID: <20030718214324.81711.qmail@web41113.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030718/12dc6ae1/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Jul 18 23:54:06 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 18 Jul 2003 21:54:06 -0000
Subject: [R] lexicographic sort of ordered lists
In-Reply-To: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>
References: <000001c34d66$af6dafb0$2546ad80@stat.vt.edu>
Message-ID: <x2znjb4hwb.fsf@biostat.ku.dk>

"J. P. Morgan" <jpmorgan at vt.edu> writes:

> Does anyone know how to execute the following sort problem in R? Matrix X
> has positive integer entries, and each column has been sorted in ascending
> order. The problem is now to order the columns lexicographically. For
> instance if the matrix X is
> 
>  
> 
> 1 2 1 1 2
> 
> 2 2 3 3 2
> 
> 3 5 5 4 2
> 
>  
> 
> then the result should be
> 
>  
> 
> 1 1 1 2 2
> 
> 2 3 3 2 2
> 
> 3 4 5 2 5
> 
>  
> 
> Let ONE be a vector of 1's of length ncol(X). The result in this example can
> be accomplished by
> 
>  
> 
> X=X[,order(ONE,X[1,],X[,2],X[,3])]

What's the ONE supposed to be good for??? Works just as well without
(after fixing the obvious typo):

> X[,order(X[1,],X[2,],X[3,])]
  V1 V4 V3 V5 V2
1  1  1  1  2  2
2  2  3  3  2  2
3  3  4  5  2  5


> but I need a method that will work regardless of k=number of rows of X. That
> is, the program must be able to accept any integer-valued matrix X for which
> each column is sorted, then permute columns accordingly. 

This should do it:

> X[,do.call("order",split(X,row(X)))]
  V1 V4 V3 V5 V2
1  1  1  1  2  2
2  2  3  3  2  2
3  3  4  5  2  5

(is there a better way of obtaining a list containing the rows of a
matrix?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sat Jul 19 00:05:11 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 18 Jul 2003 22:05:11 -0000
Subject: [R] Problem indexing into array
In-Reply-To: <200307182013.h6IKDX285480@nessie.mcc.ac.uk>
References: <200307182013.h6IKDX285480@nessie.mcc.ac.uk>
Message-ID: <x2smp34hdt.fsf@biostat.ku.dk>

[Argh. Forgot to cc: the list again...]

Ted Harding <Ted.Harding at nessie.mcc.ac.uk> writes:

> > mu[M[,1:3]]
> > 
> > should do what you want, I think.  See chapter 1 of S Poetry.
> > 
> > Patrick Burns
> 
> Thanks Patrick (and honoured to hear from the author of S-Poetry!).
> 
> This doesn't do what I want (I was probably not clear), though
> Tony Plate's reply looks promising if maybe cumbersome.
> (Thanks, Tony).
> 
> Basically: Given that mu was made by
> 
>   mu<-array(data=x, dim=c(9,2,2,2))
> 
> say, where x has 72 reals in it, and given that the first three columns
> of M contain values (1 or 2) of p,q,r, I want to recover from mu
> the 9 values mu[,p,q,r] where (p,q,r) is the result of M[i,1:3] for
> some value of i.
> 
> I don't want to do this as mu[,M[,1],M[,2],M[,3]] because in fact
> this is taking place inside a function where the dimensions of the
> arrays thrown at the function will not be known beforehand.
> 
> And, if the result can be got for all rows of M in one swoop, instead
> of looping through the rows of M, all the better!

OK, you've suckered me in...

Let's see, this probably gets easier if you first do 

aux <- array(1:8,c(2,2,2))
dim(mu) <- c(9,8)

then aux[M[,1:3]] will be a vector of numbers between 1 and 8 and

mu[,aux[M[,1:3]]

is what you want, I think. Another option would be along the lines of

apply(M[,1:3], function(r) do.call("[", c(alist(mu,), as.list(r)) )

where the use of alist() is a really filthy trick, but I know no other
way to insert a missing argument into do.call()...
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tblackw at umich.edu  Sat Jul 19 00:15:17 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 18 Jul 2003 18:15:17 -0400 (EDT)
Subject: [R] create a vector looping over a frame 
In-Reply-To: <F151C5EFCA66314D9FE66CB7199F4AAD5A3451@sad6cd1.ch.ssa.gov>
Message-ID: <Pine.SOL.4.44.0307181808290.7357-100000@rygar.gpcc.itd.umich.edu>

On Fri, 18 Jul 2003, Siddique, Amer wrote:

> I have a data.frame
>
> > names(popA)
>  [1] "Year"   "Series" "Age"    "WM"     "WF"     "HM"     "HF"     "BM"
>  [9] "BF"     "IM"     "IF"     "AM"     "AF"     "Yr"
>
> how do i loop over a subset of variables in this frame to create a vector of
> length equal to the number of variables in the subset such that the vector's
> ith element is the result of an aggregate fncn applied to the ith variable?
>
> eg,
> sumvar<-c(sum(WM),...,sum(AF))

sumvar <- sapply(popA[ ,4:13], sum)

You can use any function, even a user-written one, in place of sum().
When you see the help for  sapply(), remember that a data.frame is
also a list ("inherits" from list).

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From otter at otter-rsch.com  Fri Jul 18 00:35:47 2003
From: otter at otter-rsch.com (otter)
Date: Thu, 17 Jul 2003 15:35:47 -0700
Subject: [R] univariate normal mixtures
Message-ID: <3F1724C3.3070802@otter-rsch.com>

Hi,

Did you get this dealt with OK.

  Cheers,

   Dave



From dmurdoch at pair.com  Sat Jul 19 01:33:34 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 18 Jul 2003 19:33:34 -0400
Subject: [R] Probability plotting with R
In-Reply-To: <Pine.A41.4.44.0307180849050.102880-100000@homer25.u.washington.edu>
References: <Pine.SOL.4.05.10307181021190.4784-100000@spot.seas.smu.edu>
	<Pine.A41.4.44.0307180849050.102880-100000@homer25.u.washington.edu>
Message-ID: <hj0hhvklkauq7bt2698rp1ba0be5eg7tdk@4ax.com>

On Fri, 18 Jul 2003 08:50:01 -0700 (PDT), you wrote:

>On Fri, 18 Jul 2003, A. Gunes Koru wrote:
>
>> Hello,
>>
>> Our professor asked us to do probability plotting using weibull paper,
>> exponential paper, normal, log-normal paper, etc. I know I can create Q-Q
>> plot for normal dist. and see if all te points are on one line. How do I
>> go about other distributions?
>>
>
>The qqmath() function in the lattice package can do plots like those from
>qqnorm() for other distributions.

Also qqplot used with ppoints, e.g.

# make up some fake data
x <- rexp(100,10)

# calculate standard quantiles to plot against
stdexponential <- qexp(ppoints(100))

# do the plot
qqplot(stdexponential,x)



From markjenn at bigpond.net.au  Sat Jul 19 02:22:22 2003
From: markjenn at bigpond.net.au (Mark and Jenny)
Date: Sat, 19 Jul 2003 10:22:22 +1000
Subject: [R] R program for MacOS9.2.2
Message-ID: <BB3ECC5E.1ED%markjenn@bigpond.net.au>

Having trouble with this program which I have just downloaded for R program
for MacOS9.2.2. 

1. The download file on the CRAN site had a ? on the file to be downloaded.
Does this mean there is a problem with the file or does my computer have a
problem reading it?

2. On opening 'R' Initially comes up with Argument:____________ and choices
for console or file.    What am I supposed to enter here?

3. If I OK this and go to the 'copywright page, where it says to type
`demo()' for a demo it the comes up with a 'syntax error' What is a syntax
error?



If you cannot help with these questions, please could you email the program
with instructions how to install and use as I can not make any sense of the
FTP site.
Apologies for my ignorance.

Many Thanks
jenny



From hp3000al at yahoo.com  Sat Jul 19 07:51:49 2003
From: hp3000al at yahoo.com (dg gdf)
Date: Fri, 18 Jul 2003 22:51:49 -0700 (PDT)
Subject: [R] I don't find "fuzzy matching"
Message-ID: <20030719055149.86458.qmail@web14902.mail.yahoo.com>

hello.
I'm a student that work on R.(version 1.7.0)
I need to some data frames such as
"possum","Beams"'"kiwishade","dolphins",... .
I typed help.search("possum") and so on,but the
response was "No help files found with alias or title
matching 'possum' using fuzzy matching.".
I dont know what is "fuzzy matching" and I didn't find
it in google search.
please help me to find these data frames very very
directly.



From deepayansarkar at vsnl.net  Sat Jul 19 00:10:59 2003
From: deepayansarkar at vsnl.net (Deepayan Sarkar)
Date: Sat, 19 Jul 2003 00:10:59 +0200
Subject: [R] line colors in lattice.xyplot with png device
In-Reply-To: <200307181028.h6IA4vei011930@stat.math.ethz.ch>
References: <200307181028.h6IA4vei011930@stat.math.ethz.ch>
Message-ID: <200307190011.00097.deepayansarkar@vsnl.net>

On Friday 18 July 2003 12:28, r-help-request at stat.math.ethz.ch wrote:
> Message: 9
> Date: Thu, 17 Jul 2003 13:50:29 +0200
> From: Marc Mamin <M.Mamin at intershop.de>
> Subject: [R] line colors in lattice.xyplot with png device.
> To: r-help at stat.math.ethz.ch
>
> Hi,
>
> R is very new for me, so excuse if my questions are too basic...
>
> ????????BTW, are there any forum ?where new R users could get help without
> ????????annoying this huge mailing list ?
> ????????
> ????????
> In following code, I'd like to choose the color for each of the curve
> diplayed.
>
> png(filename = filename, width = 950, height = 600, pointsize = 10, ?bg =
> "white")
> xyplot(HITS+MS1*3+FREQ~TIME ? |SERVER*LOCAL, data=myd,allow.multiple =
> TRUE, scales = "same", type="l")
> dev.off()

With (simpler) simulated data:

y1 <- rnorm(50)
y2 <- rnorm(50)
x <- 1:50

xyplot(y1 + y2 ~ x, allow.m = TRUE, type = 'l', 
       col = c("red", "blue"))

There are more complicated ways, namely by changing the trellis parameter 
superpose.line (see ?trellis.par.get and ?lset).

> another problem is that I don't get a white backgroud, which is quite
> problematic when printing the output.
> (I works on WIN2K)

Try (see ?trellis.device)

trellis.device(png, ..., bg = "white")

Also, see ?col.whitebg

> and last but not least, is there a way to use 2 different y axis, i.e.
> curve 1& 2 => left axis, curve 3 => right axis ?

Not really, unless you are willing to write some complicated panel functions.

HTH,

Deepayan



From tobias_verbeke at skynet.be  Sat Jul 19 09:49:00 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Sat, 19 Jul 2003 09:49:00 +0200
Subject: [R] I don't find "fuzzy matching"
In-Reply-To: <20030719055149.86458.qmail@web14902.mail.yahoo.com>
References: <20030719055149.86458.qmail@web14902.mail.yahoo.com>
Message-ID: <20030719094900.2c47786a.tobias_verbeke@skynet.be>

I think the datasets you're talking
about can be downloaded at

http://wwwmaths.anu.edu.au/~johnm/r/dsets/

put these in your working directory
and type (in R), e.g.

> source("dolphins.R")

You can see you have the data now
by typing:

> dolphins
   wt heart logweight logheart species
1  35   245  3.555348 5.501258    styx
2  42   255  3.737670 5.541264    styx
3  71   525  4.262680 6.263398    styx
4  65   425  4.174387 6.052089    styx
5  63   425  4.143135 6.052089    styx
6  64   440  4.158883 6.086775    styx
7  45   350  3.806662 5.857933    styx
8  54   300  3.988984 5.703782   delph
9  59   350  4.077537 5.857933   delph
10 50   320  3.912023 5.768321   delph
11 42   240  3.737670 5.480639   delph
12 55   305  4.007333 5.720312   delph
13 37   220  3.610918 5.393628   delph
14 47   310  3.850148 5.736572   delph
15 40   210  3.688879 5.347108   delph
16 52   350  3.951244 5.857933   delph


HTH,

Tobias



From ozric at web.de  Sat Jul 19 09:44:23 2003
From: ozric at web.de (Christian Schulz)
Date: Sat, 19 Jul 2003 09:44:23 +0200
Subject: [R] I don't find "fuzzy matching"
References: <20030719055149.86458.qmail@web14902.mail.yahoo.com>
Message-ID: <001f01c34dc9$92c9a680$e105ebd9@pc>

Fuzzy Matching Identifying Words with Similar Spelling

Why you believe that the data.frames you mentioned
are in R-Project or any package - any literature?

>data(package = .packages(all.available = TRUE))
But it seems, that it works only for installed packages.

regards,christian

----- Original Message ----- 
From: "dg gdf" <hp3000al at yahoo.com>
To: <R-help at stat.math.ethz.ch>
Sent: Saturday, July 19, 2003 7:51 AM
Subject: [R] I don't find "fuzzy matching"


> hello.
> I'm a student that work on R.(version 1.7.0)
> I need to some data frames such as
> "possum","Beams"'"kiwishade","dolphins",... .
> I typed help.search("possum") and so on,but the
> response was "No help files found with alias or title
> matching 'possum' using fuzzy matching.".
> I dont know what is "fuzzy matching" and I didn't find
> it in google search.
> please help me to find these data frames very very
> directly.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 19 12:34:01 2003
From: Ted.Harding at nessie.mcc.ac.uk (Ted Harding)
Date: Sat, 19 Jul 2003 11:34:01 +0100 (BST)
Subject: [R] Problem indexing into array
In-Reply-To: <x2oezr4e0h.fsf@biostat.ku.dk> from "Peter Dalgaard BSA" at Jul
	19, 2003 01:21:50 AM
Message-ID: <200307191034.h6JAY2519782@nessie.mcc.ac.uk>

Well, thanks to all who have come up with ideas and suggestions,
and commments about how R and S work, which I must consider!

Meanwhile, since it was beginning to appear that there might be
no direct method of doing it in R, I went down the same road as
Tony Plate's suggestion and gratefully adapted his template to
do what I needed. His function was called 'array.pos', to find
the location in an array of the element with given coordinates.
I'm seeking lots, so

marray.pos<-function(i,d){
  sum(c(0,i-1)*c(1,cumprod(d[-length(d)]))) + (1:d[1]) }

Example:
x<-array((1:144),dim=c(6,2,2,3,2))
t<-rbind(c(1,2,2,1),c(2,2,3,1),c(1,2,1,2),...)

Then the following returns a matrix with 6 columns, in which
each row consists of all the 6 values in x whose coordinates
[?,p,q,r,s] satisfy (p=1,q=2,r=2,s=1), then (p=2,q=2,r=3,s=1),
etc., until tthe rows of t are exhausted:

t(matrix(x[apply(t,1,marray.pos,dim(x))],ncol=nrow(t)))

Once again, an interesting thread, and thanks to all who took
part!

Ted.



From mkondrin at hppi.troitsk.ru  Sat Jul 19 23:52:18 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Sat, 19 Jul 2003 14:52:18 -0700
Subject: [R] problem with memory size in UNIX
In-Reply-To: <BAY1-DAV39hhIOp37Q600007094@hotmail.com>
References: <BAY1-DAV39hhIOp37Q600007094@hotmail.com>
Message-ID: <3F19BD92.8000806@hppi.troitsk.ru>

liping wrote:
> Dear R/Biocondutor users:
> 
> I tried to merge two big affybatch data sets in R (under unix mainframe) and encounter the problems as following:
> 
> combine2.3<-merge(combine2.1,TALL)
> Error: cannot allocate vector of size 409600 Kb
> 
> I do not know what is the problem and how can i fix the problem. any suggustion will be appreciated. 
> 
> liping
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
Vector with 500MB are rather big. Are your shure you have enough 
physical memory (and system quotas allow you allocate that amount of 
memeory)? If yes, try to tune with mem.limits(1000000,500000000)



From r_stuff_online at hotmail.com  Sat Jul 19 17:39:37 2003
From: r_stuff_online at hotmail.com (Neil Osborne)
Date: Sat, 19 Jul 2003 15:39:37 +0000
Subject: [R] Method 'EvaluateNoReturn' of object 'IStatConnector' failed
Message-ID: <Law12-F560JxUZqU93T0000a158@hotmail.com>

Hi All,

I am using the R ActiveX component (v1.2). I have written a custom function 
in R which returns a dataframe. I want to be able to retrieve the contents 
of the returned dataframe in my application.

Here is a snippet of the code (line that throws exception). X has been 
correctly declared and init'ed earlier on in the code.


x.EvaluateNoReturn("results_frame <- MyFunction()")


I was intending to then invoke the GetSymbol method to retrieve the contents 
of results_frame into a local variable.

I notice that this still dosen't work if I try to use the Evaluate method to 
directly assign the returned dataframe to my local variable.


My questions can thus be summarised as ff:
1). How do I call a custom function written in R, using the R ActiveX 
library. ?
2). How do I access a dataframe returned from a function call, using the R 
ActiveX library. ?
3). Can I / How do I pass arguments into a function (including datframes) 
using the R ActiveX library. ?


Many Thanks



PS: I am relatively new to R, and I suspect that my function is not loaded 
into R's memory space. When using R in the normal way (i.e. via GUI), I 
initially used the source() command to load the function script. However, I 
have saved it in the R workspace (default startup directory), and is thus 
loaded everytime R is started. I am assuming the using the R ActiveX library 
fires up R, which loads up the default workspace - but I may be doing 
something wrong.

Any help will be greatly appreciated.

Thanks

_________________________________________________________________
Find a cheaper internet access deal - choose one to suit you.



From ripley at stats.ox.ac.uk  Sat Jul 19 19:01:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Jul 2003 18:01:44 +0100 (BST)
Subject: [R] Method 'EvaluateNoReturn' of object 'IStatConnector' failed
In-Reply-To: <Law12-F560JxUZqU93T0000a158@hotmail.com>
Message-ID: <Pine.LNX.4.44.0307191755370.32629-100000@gannet.stats>

On Sat, 19 Jul 2003, Neil Osborne wrote:

> I am using the R ActiveX component (v1.2). I have written a custom function 
> in R which returns a dataframe. I want to be able to retrieve the contents 
> of the returned dataframe in my application.
> 
> Here is a snippet of the code (line that throws exception). X has been 
> correctly declared and init'ed earlier on in the code.
> 
> 
> x.EvaluateNoReturn("results_frame <- MyFunction()")

That is not a valid R command: underline is not part of a syntactically
valid name and has been made defunct (in the development version which
will become 1.8.0).  Depending on your version of R this will be an error
or give a warning and mean

results <- frame <- MyFunction()

> I was intending to then invoke the GetSymbol method to retrieve the contents 
> of results_frame into a local variable.

Won't work as there cannot be such a variable name (unquoted).

> I notice that this still dosen't work if I try to use the Evaluate method to 
> directly assign the returned dataframe to my local variable.
> 
> 
> My questions can thus be summarised as ff:
> 1). How do I call a custom function written in R, using the R ActiveX 
> library. ?
> 2). How do I access a dataframe returned from a function call, using the R 
> ActiveX library. ?
> 3). Can I / How do I pass arguments into a function (including datframes) 
> using the R ActiveX library. ?

In all cases, the same way you do at R's command line.  So do check your 
scripts at the command line where you will get more informative error 
messages and recover options.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From umalvarez at fata.unam.mx  Sat Jul 19 18:35:18 2003
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Sat, 19 Jul 2003 11:35:18 -0500 (CDT)
Subject: [R] R program for MacOS9.2.2
In-Reply-To: <BB3ECC5E.1ED%markjenn@bigpond.net.au>
Message-ID: <Pine.LNX.4.44.0307191132280.11605-100000@fata.unam.mx>

Hi!

Since I'm not using the Carbon version of R, I haven't seen those errors 
before. But take a look at the 'R FAQ' and the 'R for Macintosh FAQ/DOC', 
booth available at cran.

Good look.


On Sat, 19 Jul 2003, Mark and Jenny wrote:

> Having trouble with this program which I have just downloaded for R program
> for MacOS9.2.2. 
> 
> 1. The download file on the CRAN site had a ? on the file to be downloaded.
> Does this mean there is a problem with the file or does my computer have a
> problem reading it?
> 
> 2. On opening 'R' Initially comes up with Argument:____________ and choices
> for console or file.    What am I supposed to enter here?
> 
> 3. If I OK this and go to the 'copywright page, where it says to type
> `demo()' for a demo it the comes up with a 'syntax error' What is a syntax
> error?
> 
> 
> 
> If you cannot help with these questions, please could you email the program
> with instructions how to install and use as I can not make any sense of the
> FTP site.
> Apologies for my ignorance.
> 
> Many Thanks
> jenny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From dcum007 at ec.auckland.ac.nz  Sun Jul 20 22:48:39 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Mon, 21 Jul 2003 08:48:39 +1200
Subject: [R] Random Numbers
Message-ID: <1058734119.3f1b0027be5b9@webmail1.ec.auckland.ac.nz>

Thank you to all who replied to my previous question regarding the generation 
of random numbers. 

I have read up on the literature and learnt a whole lot more. In my research I 
found code for many generators which I have translated into java for my 
project. I'm now at the stage of testing these generators and was wondering if 
there were any inbuilt (or if someone has written libraries for) testing 
commands in R. 

Are there any libraries for R that have the serial correlation test, the 
birthday spacing test or any of the other tests for randomness??
Thank you all
David



From edd at debian.org  Sun Jul 20 23:47:02 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Jul 2003 16:47:02 -0500
Subject: [R] Help with R Installation on Debian 2.2.19 (old stable/potato)
In-Reply-To: <20030710175059.GA12038@coost.com>
References: <5.2.0.9.2.20030710103921.00a754c8@biochem.okstate.edu>
	<20030710175059.GA12038@coost.com>
Message-ID: <20030720214701.GA18286@sonny.eddelbuettel.com>

On Thu, Jul 10, 2003 at 01:50:59PM -0400, Jameson C. Burt wrote:
> I have avoided crossing Debian versions 
> (eg, installing woody packages over a Debian potato distribution) 

Countless users have done that with success. It all depends on your comfort
level with your Debian system as you need some modest amount of knowledge to
know how and when you can do this.  The apt-get program "formally" supports
it, though I am not too sure if the one woody does.

But as suggested, there is a Debian "woody" repository of R on CRAN too. 

Hth, Dirk

-- 
    Sorry for replying late to your mail, but I have been been away 
    from home and email for about ten days and am slowly catching up.



From mario at unir.br  Mon Jul 21 00:41:35 2003
From: mario at unir.br (Mario Alberto Cozzuol)
Date: Sun, 20 Jul 2003 18:41:35 -0400
Subject: [R] Problem with packages...
Message-ID: <200307201841.35989.mario@unir.br>

Hi, I just installed the last version of R taken from the CRAN mirron in 
University of Vi?osa, Brasil. Since I use Mandrake 9.1 I get the proper .rpm 
file. I installed fine and all the test runs OK.
I tryied to install sevela packages ussing the 
install.packages("package.name")
and the installation runs OK... apparently.
However, when I try to use the new package I get the following message for all 
them (in this example for GRASS module):
Error: couldn't find function "GRASS.connect".
In this specific case I tryed with GRASS running and without as well as 
launching R from inside GRASS but nothing works.
The worst part is the tha very same happend with all the other packages, like 
GraspeR, geoR, akima, etc....
Well, I tryed to get something about that in the mailing list archives, but I 
found nothing up to now. Any one have any idea about where is the problem? (I 
guess it is sitting on my chair!)
My system: Mandrake 9.1, R 1.7.1-1mdk, tcl/tk 8.3.3-21.mdk...

Thnak you in advance!

Mario

-- 
Dr. Mario A. Cozzuol
Laborat?rio de Biologia Evolutiva
Universidade Federal de Rond?nia
BR 364, Km 9,5
78900-000 Porto Velho, RO
Brasil
Tel./Fax 55 69 217-8593 
E-mail mario at unir.br



From stoet at volition.wustl.edu  Mon Jul 21 03:12:54 2003
From: stoet at volition.wustl.edu (Gijsbert Stoet)
Date: Sun, 20 Jul 2003 20:12:54 -0500
Subject: [R] re: repeated measures
Message-ID: <16155.15894.79756.623810@volition.wustl.edu>

There is an excellent paper on how to use repeated measures in R. It
is called "Notes on the use of R for psychology experiments and
questionnaires" by Baron and Li, and if you can find it on the
internet as html or pdf document.



From ok at cs.otago.ac.nz  Mon Jul 21 03:46:07 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 21 Jul 2003 13:46:07 +1200 (NZST)
Subject: [R] Formal definitions of R-language.
Message-ID: <200307210146.h6L1k7JN249640@atlas.otago.ac.nz>

Murad Nayal <mn216 at columbia.edu> wrote:
	recursion, as far as I know, is inefficient in
	S/R. which tend to discourage purely functional programming.
	
It's not so much that _recursion_ is slow, as that function calling
is slow (whether recursive or not).  A large part of the reason is
that the S function calling convention was designed to favour
human ease-of-use much more than ease of compilation.



From mn216 at columbia.edu  Mon Jul 21 04:16:44 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Sun, 20 Jul 2003 22:16:44 -0400
Subject: [R] help on barplot
Message-ID: <3F1B4D0C.8666AED7@columbia.edu>



Hello,

I am trying to compare two histograms using barplot. the idea is to plot
the histograms as pairs of columns side by side for each x value. I was
able to do it using barplot before but I can't remember now for the life
of me now how I did it in the past:

> d
              [,1]         [,2]
-37.5 0.0000000000 2.789396e-05
-32.5 0.0001394700 5.578801e-05
-27.5 0.0019804742 1.732218e-02
-22.5 0.0217294282 1.380474e-01
-17.5 0.0938912134 4.005579e-02
-12.5 0.0630683403 4.351464e-03
-7.5  0.0163179916 8.368201e-05
-2.5  0.0025941423 5.578801e-05
2.5   0.0002789400 0.000000e+00
7.5   0.0000000000 0.000000e+00

> barplot(d,beside=TRUE)

barplot here plots two separate 'sets' of columns, on the left side a
bar plot of d[,1] is plotted while on the right side a separate bar plot
of d[,2] is plotted. how can I combine the two?

actually, while on the subject of histograms. is it possible to plot a
3D-histogram in R (a true 3D bar plot, without using image).

many thanks
Murad


-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From tlumley at u.washington.edu  Mon Jul 21 04:35:32 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 20 Jul 2003 19:35:32 -0700 (PDT)
Subject: [R] Formal definitions of R-language.
In-Reply-To: <200307210146.h6L1k7JN249640@atlas.otago.ac.nz>
Message-ID: <Pine.A41.4.44.0307201929060.49650-100000@homer33.u.washington.edu>

On Mon, 21 Jul 2003, Richard A. O'Keefe wrote:

> Murad Nayal <mn216 at columbia.edu> wrote:
> 	recursion, as far as I know, is inefficient in
> 	S/R. which tend to discourage purely functional programming.
>
> It's not so much that _recursion_ is slow, as that function calling
> is slow (whether recursive or not).  A large part of the reason is
> that the S function calling convention was designed to favour
> human ease-of-use much more than ease of compilation.
>

The biggest problem with recursion in comparison to languages like Scheme
and compiled LISPs may be that R can't eliminate any of the recursions.
Even tail-recursion elimination (which is compulsory in Scheme) isn't
possible without changing the semantics of the language, because functions
like sys.frame() give access to all the frames that would be optimised out
of existence.

In my experience recursive functions run reasonably well in R until they
hit the stack limit.

	-thomas



From stoet at volition.wustl.edu  Mon Jul 21 04:51:07 2003
From: stoet at volition.wustl.edu (Gijsbert Stoet)
Date: Sun, 20 Jul 2003 21:51:07 -0500
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <16155.21787.716116.428183@volition.wustl.edu>

Hi,

  suppose I do want to test whether the slopes (e.g. determined with
lsfit) of two different population are significantly different, how do
I test this (in R). Say for example, I found out what the slope
between age and number of books read per year is for two different
populations of subjects (e.g. 25 man and 25 woman), say using
lsfit. How can I tell whether the slopes are different in R. (And how
would I do it for regression coefficients?)

Thanks a lot for your help.



From bmagill at earthlink.net  Mon Jul 21 04:12:36 2003
From: bmagill at earthlink.net (Brett Magill)
Date: Sun, 20 Jul 2003 21:12:36 -0500 (GMT)
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <6616038.1058760754742.JavaMail.nobody@thecount.psp.pas.earthlink.net>

Not really r-specific:

Z = (b1 - b2) / SQRT ( SEb1^2 + SEb2^2)

-------Original Message-------
From: Gijsbert Stoet <stoet at volition.wustl.edu>
Sent: 07/20/03 09:51 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to test whether two slopes are sign. different?

> 
> Hi,

  suppose I do want to test whether the slopes (e.g. determined with
lsfit) of two different population are significantly different, how do
I test this (in R). Say for example, I found out what the slope
between age and number of books read per year is for two different
populations of subjects (e.g. 25 man and 25 woman), say using
lsfit. How can I tell whether the slopes are different in R. (And how
would I do it for regression coefficients?)

Thanks a lot for your help.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From hu at mshri.on.ca  Mon Jul 21 06:14:22 2003
From: hu at mshri.on.ca (Pingzhao Hu)
Date: Mon, 21 Jul 2003 00:14:22 -0400
Subject: [R] general location model
Message-ID: <490D0AFAF3D2D3119F6C00508B6FDF15026CE801@ex.mshri.on.ca>

Dear R-helpers,

I am trying to use general location model for a set of category and
continuous variables (medical dataset).

Does any one know where I can find this kind of  R source code????

Mix package (R code) has functions for 
restricted and unrestricted general location models, but it seems that
they can only handle datasets with missing values (????). My dataset has no
missing value, so I did not success in using this package to handle my
dataset. Any one has experience in this. By the way, if I assume that my
dataset has one missing value, then I can use the functions in mix package
to build up location model!

Thanks

ping



From MSchwartz at medanalytics.com  Mon Jul 21 06:34:50 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 21 Jul 2003 04:34:50 -0000
Subject: [R] help on barplot
In-Reply-To: <3F1B4D0C.8666AED7@columbia.edu>
References: <3F1B4D0C.8666AED7@columbia.edu>
Message-ID: <1058762088.9353.114.camel@localhost>

On Sun, 2003-07-20 at 21:16, Murad Nayal wrote:
> Hello,
> 
> I am trying to compare two histograms using barplot. the idea is to plot
> the histograms as pairs of columns side by side for each x value. I was
> able to do it using barplot before but I can't remember now for the life
> of me now how I did it in the past:
> 
> > d
>               [,1]         [,2]
> -37.5 0.0000000000 2.789396e-05
> -32.5 0.0001394700 5.578801e-05
> -27.5 0.0019804742 1.732218e-02
> -22.5 0.0217294282 1.380474e-01
> -17.5 0.0938912134 4.005579e-02
> -12.5 0.0630683403 4.351464e-03
> -7.5  0.0163179916 8.368201e-05
> -2.5  0.0025941423 5.578801e-05
> 2.5   0.0002789400 0.000000e+00
> 7.5   0.0000000000 0.000000e+00
> 
> > barplot(d,beside=TRUE)
> 
> barplot here plots two separate 'sets' of columns, on the left side a
> bar plot of d[,1] is plotted while on the right side a separate bar plot
> of d[,2] is plotted. how can I combine the two?
> 
> actually, while on the subject of histograms. is it possible to plot a
> 3D-histogram in R (a true 3D bar plot, without using image).
> 
> many thanks
> Murad


You need to restructure the data passed to barplot() so that the two
'height' related columns are converted to 2 rows of 10 columns. Each
column is then drawn as pairs of bars:

barplot(rbind(d[, 1], d[, 2]), beside = TRUE)

Take a look at the change in structure as a result of:

rbind(d[, 1], d[, 2])

In terms of 3d histograms, it would appear that Duncan Murdoch, Daniel
Adler et al are working on porting Duncan's Windows only DJMRGL package
(http://www.stats.uwo.ca/faculty/murdoch/software) to multiple platforms
at (http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl). If my
read is correct, it looks like they have some of the primitives ready to
go at this time, but perhaps have not yet converted Duncan's hist3d()
function.

HTH,

Marc Schwartz



From MHerzog at cabnr.unr.edu  Mon Jul 21 08:23:29 2003
From: MHerzog at cabnr.unr.edu (Herzog, Mark)
Date: Sun, 20 Jul 2003 23:23:29 -0700
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <6FD4F362992E394A902933580B7F6E8674F30A@agnt-mail.agnt.unr.edu>

Or better yet skip the whole "significantly" different all together, and figure out if a model with 2 slopes explains the data "better" than a model with 1------ AIC's!
 
Mark

	-----Original Message----- 
	From: Brett Magill [mailto:bmagill at earthlink.net] 
	Sent: Sun 7/20/2003 7:12 PM 
	To: Gijsbert Stoet; r-help at stat.math.ethz.ch 
	Cc: 
	Subject: Re: [R] how to test whether two slopes are sign. different?
	
	

	Not really r-specific:
	
	Z = (b1 - b2) / SQRT ( SEb1^2 + SEb2^2)
	
	-------Original Message-------
	From: Gijsbert Stoet <stoet at volition.wustl.edu>
	Sent: 07/20/03 09:51 PM
	To: r-help at stat.math.ethz.ch
	Subject: [R] how to test whether two slopes are sign. different?
	
	>
	> Hi,
	
	  suppose I do want to test whether the slopes (e.g. determined with
	lsfit) of two different population are significantly different, how do
	I test this (in R). Say for example, I found out what the slope
	between age and number of books read per year is for two different
	populations of subjects (e.g. 25 man and 25 woman), say using
	lsfit. How can I tell whether the slopes are different in R. (And how
	would I do it for regression coefficients?)
	
	Thanks a lot for your help.
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
	>
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Mon Jul 21 08:33:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jul 2003 07:33:42 +0100 (BST)
Subject: [R] how to test whether two slopes are sign. different?
In-Reply-To: <6FD4F362992E394A902933580B7F6E8674F30A@agnt-mail.agnt.unr.edu>
Message-ID: <Pine.LNX.4.44.0307210728470.20207-100000@gannet.stats>

On Sun, 20 Jul 2003, Herzog, Mark wrote:

> Or better yet skip the whole "significantly" different all together, and
> figure out if a model with 2 slopes explains the data "better" than a
> model with 1------ AIC's!

That's not what AIC is designed to do: it is about `prediction' not 
`explanation', as you will discover from the primary sources (if not from 
some of the secondary ones).

In this specific case there are is the question of whether the error
variances are the same to take into account, which makes it tricky to fit
a single model (especially with lsfit).

>  
> Mark
> 
> 	-----Original Message----- 
> 	From: Brett Magill [mailto:bmagill at earthlink.net] 
> 	Sent: Sun 7/20/2003 7:12 PM 
> 	To: Gijsbert Stoet; r-help at stat.math.ethz.ch 
> 	Cc: 
> 	Subject: Re: [R] how to test whether two slopes are sign. different?
> 	
> 	
> 
> 	Not really r-specific:
> 	
> 	Z = (b1 - b2) / SQRT ( SEb1^2 + SEb2^2)
> 	
> 	-------Original Message-------
> 	From: Gijsbert Stoet <stoet at volition.wustl.edu>
> 	Sent: 07/20/03 09:51 PM
> 	To: r-help at stat.math.ethz.ch
> 	Subject: [R] how to test whether two slopes are sign. different?
> 	
> 	>
> 	> Hi,
> 	
> 	  suppose I do want to test whether the slopes (e.g. determined with
> 	lsfit) of two different population are significantly different, how do
> 	I test this (in R). Say for example, I found out what the slope
> 	between age and number of books read per year is for two different
> 	populations of subjects (e.g. 25 man and 25 woman), say using
> 	lsfit. How can I tell whether the slopes are different in R. (And how
> 	would I do it for regression coefficients?)
> 	
> 	Thanks a lot for your help.
> 	
> 	______________________________________________
> 	R-help at stat.math.ethz.ch mailing list
> 	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 	>
> 	
> 	______________________________________________
> 	R-help at stat.math.ethz.ch mailing list
> 	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From temiz at deprem.gov.tr  Mon Jul 21 08:46:42 2003
From: temiz at deprem.gov.tr (orkun)
Date: Mon, 21 Jul 2003 09:46:42 +0300
Subject: [R] R commands from a text file ?
Message-ID: <3F1B8C52.6080802@deprem.gov.tr>

Hello

I was wondering if it was possible to enter R commands from an external
text file. If it is possible, it will be easy for repetetive tasks.

Does anyone have an idea ?

thanks in advance


Ahmet Temiz
   TURKEY


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From H.RINNER at tirol.gv.at  Mon Jul 21 08:53:14 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 21 Jul 2003 08:53:14 +0200
Subject: [R] RODBC and Oracle: error "table does not exist"
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE274@xms1.tirol.gv.at>

I am re-trying a question I asked 12 days ago, to which unfortunately I got
no answer so far. Maybe someone who has succesfully established ODBC
connections between R and Oracle can give a hint what I am doing wrong?

-----Urspr?ngliche Nachricht-----
Von: RINNER Heinrich [mailto:H.RINNER at tirol.gv.at] 
Gesendet: Mittwoch, 09. Juli 2003 15:34
An: 'r-help at stat.math.ethz.ch'
Betreff: [R] RODBC and Oracle: error "table does not exist"

Dear r-helpers!

I have trouble reading data from an Oracle data base using
RODBC Version 1.0-3,
R Version 1.7.1,
Windows XP,
Oracle8 ODBC Driver Version 8.1.6.4.0:

> library(RODBC)
> channel <- odbcConnect(dsn="PAV32", case="oracle", believeNRows=FALSE)
> # ok, this was succesful
> x <- sqlTables(channel)
> x[37, ]
     TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS
37        <NA>         TKF ABTGRNAMEN      TABLE    <NA>

> # ok, so the table I am looking for ("ABTGRNAMEN") is there, but:
> sqlFetch(channel, "ABTGRNAMEN")
[1] "[RODBC] ERROR: Could not SQLExecute"                                   
[2] "S0002 942 [Oracle][ODBC][Ora]ORA-00942: table or view does not exist\n"
> # I also tried:
> sqlFetch(channel, "TKF.ABTGRNAMEN")
Error in odbcTableExists(channel, sqtable) : 
        TKF.ABTGRNAMEN : table not found on channel

What am I doing wrong here?
It doesn't work with other tables as well; on the other hand, connecting to
the table(s) in MS Access works fine using the same ODBC driver.

Best regards,
Heinrich.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From H.RINNER at tirol.gv.at  Mon Jul 21 08:56:28 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 21 Jul 2003 08:56:28 +0200
Subject: AW: [R] R commands from a text file ?
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE275@xms1.tirol.gv.at>

You can use source():

For example
> source("C:\temp\foo.R")

wher "foo.R" is a text fiel containing R commands.

Regards,
Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: orkun [mailto:temiz at deprem.gov.tr] 
> Gesendet: Montag, 21. Juli 2003 08:47
> An: R-help
> Betreff: [R] R commands from a text file ?
> 
> 
> Hello
> 
> I was wondering if it was possible to enter R commands from 
> an external
> text file. If it is possible, it will be easy for repetetive tasks.
> 
> Does anyone have an idea ?
> 
> thanks in advance
> 
> 
> Ahmet Temiz
>    TURKEY
> 
> 
> ______________________________________
> 
> 
> 
> ______________________________________
> The views and opinions expressed in this e-mail message are 
> ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Simon.Blomberg at anu.edu.au  Mon Jul 21 09:01:35 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Mon, 21 Jul 2003 17:01:35 +1000
Subject: [R] R commands from a text file ?
Message-ID: <7A3A13F416B40842BD2C1753E044B359B0996C@CASEVS02.cas.anu.edu.au>

Or you may be interested in ?BATCH

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: RINNER Heinrich [mailto:H.RINNER at tirol.gv.at]
> Sent: Monday, 21 July 2003 4:56 PM
> To: 'orkun'
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: AW: [R] R commands from a text file ?
> 
> 
> You can use source():
> 
> For example
> > source("C:\temp\foo.R")
> 
> wher "foo.R" is a text fiel containing R commands.
> 
> Regards,
> Heinrich.
> 
> > -----Urspr?ngliche Nachricht-----
> > Von: orkun [mailto:temiz at deprem.gov.tr] 
> > Gesendet: Montag, 21. Juli 2003 08:47
> > An: R-help
> > Betreff: [R] R commands from a text file ?
> > 
> > 
> > Hello
> > 
> > I was wondering if it was possible to enter R commands from 
> > an external
> > text file. If it is possible, it will be easy for repetetive tasks.
> > 
> > Does anyone have an idea ?
> > 
> > thanks in advance
> > 
> > 
> > Ahmet Temiz
> >    TURKEY
> > 
> > 
> > ______________________________________
> > 
> > 
> > 
> > ______________________________________
> > The views and opinions expressed in this e-mail message are 
> > ...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mkondrin at hppi.troitsk.ru  Mon Jul 21 20:13:46 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 21 Jul 2003 11:13:46 -0700
Subject: [R] R commands from a text file ?
In-Reply-To: <3F1B8C52.6080802@deprem.gov.tr>
References: <3F1B8C52.6080802@deprem.gov.tr>
Message-ID: <3F1C2D5A.1010109@hppi.troitsk.ru>

orkun wrote:
> Hello
> 
> I was wondering if it was possible to enter R commands from an external
> text file. If it is possible, it will be easy for repetetive tasks.
> 
> Does anyone have an idea ?
> 
> thanks in advance
> 
> 
> Ahmet Temiz
>   TURKEY
> 
> 
> ______________________________________
> 
> 
> 
> ______________________________________
> The views and opinions expressed in this e-mail message are ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
?source



From ligges at statistik.uni-dortmund.de  Mon Jul 21 09:16:28 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 21 Jul 2003 09:16:28 +0200
Subject: AW: [R] R commands from a text file ?
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE275@xms1.tirol.gv.at>
References: <C4D44AB4CB62D311BA6500041202E886031EE275@xms1.tirol.gv.at>
Message-ID: <3F1B934C.8010603@statistik.uni-dortmund.de>

RINNER Heinrich wrote:

> You can use source():
> 
> For example
> 
>>source("C:\temp\foo.R")

No. source() is the right hint, but you cannot specify the file this 
way! On Unix-like sytems it is as usual, but on Windows either
"C:\\temp\\foo.R" or "C:/temp/foo.R".

Uwe Ligges


> 
> wher "foo.R" is a text fiel containing R commands.
> 
> Regards,
> Heinrich.
> 
> 
>>-----Urspr?ngliche Nachricht-----
>>Von: orkun [mailto:temiz at deprem.gov.tr] 
>>Gesendet: Montag, 21. Juli 2003 08:47
>>An: R-help
>>Betreff: [R] R commands from a text file ?
>>
>>
>>Hello
>>
>>I was wondering if it was possible to enter R commands from 
>>an external
>>text file. If it is possible, it will be easy for repetetive tasks.
>>
>>Does anyone have an idea ?
>>
>>thanks in advance
>>
>>
>>Ahmet Temiz
>>   TURKEY
>>
>>
>>______________________________________
>>
>>
>>
>>______________________________________
>>The views and opinions expressed in this e-mail message are 
>>...{{dropped}}
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mkondrin at hppi.troitsk.ru  Mon Jul 21 20:58:11 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 21 Jul 2003 11:58:11 -0700
Subject: [R] grid and gtkDevice package clipping bug.
Message-ID: <3F1C37C3.1050701@hppi.troitsk.ru>

Hello!
When I draw points on grid's viewport with clip set to TRUE clipping 
does not have effect on some symbols. It happens only when drawing over 
gtkDevice (GTK ver. 1.2) (with postscript device clipping works OK - 
take a look at this screenshot 
http://www.hppi.troitsk.ru/Kondrin/clipbug.png) and only with symbols 
that are drawn with commands other than gdk_draw_polygon() (i.e. circles 
- gdk_draw_arc() - and squares - gdk_draw_rect()).



From H.RINNER at tirol.gv.at  Mon Jul 21 10:31:20 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 21 Jul 2003 10:31:20 +0200
Subject: [R] RODBC and Oracle: error "table does not exist"
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE27A@xms1.tirol.gv.at>

Dear Marc,
thanks very much for your answer!!
Adding quotes to the table names didn't change anything (I had tried that
before),
but creating synonyms in Oracle did the trick!

Everything is working fine now, so thanks again.

-Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: Marc Mamin [mailto:M.Mamin at intershop.de] 
> Gesendet: Montag, 21. Juli 2003 09:11
> An: 'RINNER Heinrich'
> Betreff: RE: [R] RODBC and Oracle: error "table does not exist"
> 
> 
> 
> 
> hallo,
> I'm using following syntax: which is working fine (with 
> Oracle 8.1.7.2):
> 
> channel <- ....
> sh3 <- sqlQuery(channel, "select .....")
> odbcClose(channel)
> myd <- data.frame(sh3)
> rm(sh3)
> 
> 
> Some other things you may check or try:
> 
> - add quotes to the table names :
> 
>  sqlFetch(channel, "\"ABTGRNAMEN\"")
> 
> - if you are not connecting as the user TKF , you may try to 
> create synonyms
> in oracle:
> 
> sqlplus TKF/PWD at XXXX
> 
> SQL> create synonym ABTGRNAMEN for TKF.ABTGRNAMEN
> 
> 
> HTH,
> 
> Marc Mamin
> 
> 
> 
> 
> -----Original Message-----
> From: RINNER Heinrich [mailto:H.RINNER at tirol.gv.at]
> Sent: Monday, July 21, 2003 8:53 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: RE: [R] RODBC and Oracle: error "table does not exist"
> 
> 
> I am re-trying a question I asked 12 days ago, to which 
> unfortunately I got
> no answer so far. Maybe someone who has succesfully established ODBC
> connections between R and Oracle can give a hint what I am 
> doing wrong?
> 
> -----Urspr?ngliche Nachricht-----
> Von: RINNER Heinrich [mailto:H.RINNER at tirol.gv.at] 
> Gesendet: Mittwoch, 09. Juli 2003 15:34
> An: 'r-help at stat.math.ethz.ch'
> Betreff: [R] RODBC and Oracle: error "table does not exist"
> 
> Dear r-helpers!
> 
> I have trouble reading data from an Oracle data base using
> RODBC Version 1.0-3,
> R Version 1.7.1,
> Windows XP,
> Oracle8 ODBC Driver Version 8.1.6.4.0:
> 
> > library(RODBC)
> > channel <- odbcConnect(dsn="PAV32", case="oracle", 
> believeNRows=FALSE)
> > # ok, this was succesful
> > x <- sqlTables(channel)
> > x[37, ]
>      TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS
> 37        <NA>         TKF ABTGRNAMEN      TABLE    <NA>
> 
> > # ok, so the table I am looking for ("ABTGRNAMEN") is there, but:
> > sqlFetch(channel, "ABTGRNAMEN")
> [1] "[RODBC] ERROR: Could not SQLExecute"                     
>               
> [2] "S0002 942 [Oracle][ODBC][Ora]ORA-00942: table or view 
> does not exist\n"
> > # I also tried:
> > sqlFetch(channel, "TKF.ABTGRNAMEN")
> Error in odbcTableExists(channel, sqtable) : 
>         TKF.ABTGRNAMEN : table not found on channel
> 
> What am I doing wrong here?
> It doesn't work with other tables as well; on the other hand, 
> connecting to
> the table(s) in MS Access works fine using the same ODBC driver.
> 
> Best regards,
> Heinrich.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From huan.huang at bnpparibas.com  Mon Jul 21 12:17:07 2003
From: huan.huang at bnpparibas.com (huan.huang@bnpparibas.com)
Date: Mon, 21 Jul 2003 11:17:07 +0100
Subject: [R] correlated residuals in gls: Coefficient matrix not invertible
Message-ID: <OF7E98D35D.851FC6CE-ON80256D6A.00387FD5@bnpparibas.com>


Dear Rers,

I have threes series, x, y, z and I want to fit a model z ~ x + y. First of
all, I fit a lm. I found the residuals are correlated, by looking at the
acf() and pacf(). Then I tried to fit a gls model allowing residuals to be
correlated (correlation = corARMA(p=5, q=1)):

y.na <- as.data.frame(y[complete.cases(y),])
y.gls <- gls(z ~ x + y, data = y.na,  correlation=corARMA(p=5, q=1))

It gave this error message:
Error in "coef<-.corARMA"(*tmp*, value = c(188.077895742055, 180.123242661068,  :
      Coefficient matrix not invertible

I tried the gls allowing residuals following AR(1):

y.gls <- gls(z ~ x + y, data = y.na,  correlation=corAR1())

and it works. Actually I found in the correlation structure, as long as I set the p larger than 1 (ARMA(2, 1) OR ARMA(3,1)), I will have the error
message saying "Coefficient matrix not invertible".
But if I only set q (ARMA(0,2), ARMA(0,3)), it is working all right.

I did the above on some other data and had similar error messages:
Error in "coef<-.corARMA"(*tmp*, value = c(123.027732874371, 114.73028258271,  :
      Coefficient matrix not invertible

Could anybody give me some hint or point me a direction?

Thanks a lot.

Huan Huang





This message and any attachments (the "message") is\ intende...{{dropped}}



From f.calboli at ucl.ac.uk  Mon Jul 21 12:45:32 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Mon, 21 Jul 2003 11:45:32 +0100
Subject: [R] Problem with packages...
Message-ID: <3.0.6.32.20030721114532.00aad020@pop-server.ucl.ac.uk>

Have you tried to do:

library(" the library you want")

before using the function?

Federico

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From V.Khamenia at biovision-discovery.de  Mon Jul 21 11:11:58 2003
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 21 Jul 2003 11:11:58 +0200
Subject: [R] calling R from C
Message-ID: <D15343265276D31197BC00A024A6C11077412B@EXS_BDC>

Hi All,

  We'd like to use functions provided in R in our application.
  Our application is written in C/C++ and currently runs on 
  win32, Linux and Mac. We'd be happy to attach the whole
  R ( i.e. not just transfer some function by hand).
  The important detail is that we deal with big amount of data, so
  "command line"-like invocations won't be very interesting.
  We'd like to link R's code statically or dynamically.

  Any comments and/or links to docs on subj would be 
  highly appreciated. (Especially comments on Mac)

  Thank you in advance.

P.S. If this message appears twice -- I'm sorry (first one
  was sent in default HTML format)

kind regards,
Valery A.Khamenya
---------------------------------------------------------------------------
Bioinformatics Department
BioVisioN AG, Hannover



From V.Khamenia at biovision-discovery.de  Mon Jul 21 10:08:04 2003
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 21 Jul 2003 10:08:04 +0200
Subject: [R] calling R from C
Message-ID: <D15343265276D31197BC00A024A6C11077412A@EXS_BDC>

Hi All,

  We'd like to use functions provided in R in our application.
  Our application is written in C/C++ and currently runs on 
  win32, Linux and Mac. We'd be happy to attach the whole
  R ( i.e. not just transfer some function by hand).
  It is important that we deal with big amount of data, so
  "command line"-like invocations won't be very interesting.
  We'd like to link R's code statically or dynamically.

  Any comments and/or links to docs on subj would be 
  highly appreciated. (Especially comments on Mac)

  Thank you in advance.

kind regards,
Valery A.Khamenya
---------------------------------------------------------------------------
Bioinformatics Department
BioVisioN AG, Hannover



From temiz at deprem.gov.tr  Mon Jul 21 12:50:45 2003
From: temiz at deprem.gov.tr (orkun)
Date: Mon, 21 Jul 2003 13:50:45 +0300
Subject: [R] R commands from a text file ? THANKS
In-Reply-To: <3F1C2D5A.1010109@hppi.troitsk.ru>
References: <3F1B8C52.6080802@deprem.gov.tr> <3F1C2D5A.1010109@hppi.troitsk.ru>
Message-ID: <3F1BC585.10307@deprem.gov.tr>

M.Kondrin wrote:

> orkun wrote:
>
>> Hello
>>
>> I was wondering if it was possible to enter R commands from an external
>> text file. If it is possible, it will be easy for repetetive tasks.
>>
>> Does anyone have an idea ?
>>
>> thanks in advance
>>
>>
>> Ahmet Temiz
>>   TURKEY
>>
>>
>> ______________________________________
>>
>>
>>
>> ______________________________________
>> The views and opinions expressed in this e-mail message are 
>> ...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
> ?source
>
>
>
>
Thanks to all people who were good enough to answer my question.

regards



______________________________________



______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From cafa at ime.unicamp.br  Mon Jul 21 13:39:47 2003
From: cafa at ime.unicamp.br (Cezar Augusto de Freitas Anselmo)
Date: Mon, 21 Jul 2003 08:39:47 -0300 (BRT)
Subject: [R] C compiler to R
Message-ID: <Pine.GSO.4.05.10307210839110.6207-100000@athenas.ime.unicamp.br>

I'd like to know what the more appropriate C compiler to use with R.
Thanks,
C.

========================================
Cezar Freitas (ICQ 109128967)
IMECC - UNICAMP
Campinas, SP - Brasil



From hdoran at nasdc.org  Mon Jul 21 14:15:13 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Mon, 21 Jul 2003 08:15:13 -0400
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <66578BFC0BA55348B5907A0F798EE93029E888@ernesto.NASDC.ORG>

Dear Stoet

This can be handled well in using a mixed-effects model, library (nmle). You can use the lmList option to check whether the slopes differ across populations.

 
------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628
<http://www.edperform.net>  
 
 


-----Original Message-----
From: Gijsbert Stoet [mailto:stoet at volition.wustl.edu]
Sent: Sunday, July 20, 2003 10:51 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to test whether two slopes are sign. different?


Hi,

  suppose I do want to test whether the slopes (e.g. determined with
lsfit) of two different population are significantly different, how do
I test this (in R). Say for example, I found out what the slope
between age and number of books read per year is for two different
populations of subjects (e.g. 25 man and 25 woman), say using
lsfit. How can I tell whether the slopes are different in R. (And how
would I do it for regression coefficients?)

Thanks a lot for your help.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From flom at ndri.org  Mon Jul 21 14:50:53 2003
From: flom at ndri.org (Peter Flom)
Date: Mon, 21 Jul 2003 08:50:53 -0400
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <sf1ba98d.084@MAIL.NDRI.ORG>

A couple people gave you technical solutions already.

My question would be, after you've determined whether they are
significantly different, then what?  Isn't the better question whether
the two slopes are different in a substantively important way?

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)


>>> Gijsbert Stoet <stoet at volition.wustl.edu> 07/20/03 10:51PM >>>
Hi,

  suppose I do want to test whether the slopes (e.g. determined with
lsfit) of two different population are significantly different, how do
I test this (in R). Say for example, I found out what the slope
between age and number of books read per year is for two different
populations of subjects (e.g. 25 man and 25 woman), say using
lsfit. How can I tell whether the slopes are different in R. (And how
would I do it for regression coefficients?)

Thanks a lot for your help.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Mon Jul 21 14:58:36 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jul 2003 08:58:36 -0400
Subject: [R] how to test whether two slopes are sign. different?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8DC@usrymx25.merck.com>

Here's the "classical" (ANCOVA?) approach, as one would learn in an applied
regression course:

summary(lm(num.book ~ pop + age + pop:age, data=your.data))

Where your.data is a data frame with at least three columns: num.book, age,
and an indicator, pop, that tells which population the subject belongs (must
be a factor).  The interaction term, pop:age, gives you a test of equal
slope.

This, of course, assume that the residual variance is the same between the
two populations.  It gets trickier if they're not, as Ripley pointed out.

HTH,
Andy

> -----Original Message-----
> From: Gijsbert Stoet [mailto:stoet at volition.wustl.edu] 
> Sent: Sunday, July 20, 2003 10:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to test whether two slopes are sign. different?
> 
> 
> Hi,
> 
>   suppose I do want to test whether the slopes (e.g. determined with
> lsfit) of two different population are significantly 
> different, how do I test this (in R). Say for example, I 
> found out what the slope between age and number of books read 
> per year is for two different populations of subjects (e.g. 
> 25 man and 25 woman), say using lsfit. How can I tell whether 
> the slopes are different in R. (And how would I do it for 
> regression coefficients?)
> 
> Thanks a lot for your help.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From spencer.graves at pdf.com  Mon Jul 21 15:30:36 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 06:30:36 -0700
Subject: [R] help on barplot
References: <3F1B4D0C.8666AED7@columbia.edu>
	<1058762088.9353.114.camel@localhost>
Message-ID: <3F1BEAFC.5090802@pdf.com>

Please relieve me my ignorance on one point:  How does "rbind(d[, 1], 
d[, 2])" differe from "t(d)"?

Thanks, Spencer Graves

Marc Schwartz wrote:
> On Sun, 2003-07-20 at 21:16, Murad Nayal wrote:
> 
>>Hello,
>>
>>I am trying to compare two histograms using barplot. the idea is to plot
>>the histograms as pairs of columns side by side for each x value. I was
>>able to do it using barplot before but I can't remember now for the life
>>of me now how I did it in the past:
>>
>>
>>>d
>>
>>              [,1]         [,2]
>>-37.5 0.0000000000 2.789396e-05
>>-32.5 0.0001394700 5.578801e-05
>>-27.5 0.0019804742 1.732218e-02
>>-22.5 0.0217294282 1.380474e-01
>>-17.5 0.0938912134 4.005579e-02
>>-12.5 0.0630683403 4.351464e-03
>>-7.5  0.0163179916 8.368201e-05
>>-2.5  0.0025941423 5.578801e-05
>>2.5   0.0002789400 0.000000e+00
>>7.5   0.0000000000 0.000000e+00
>>
>>
>>>barplot(d,beside=TRUE)
>>
>>barplot here plots two separate 'sets' of columns, on the left side a
>>bar plot of d[,1] is plotted while on the right side a separate bar plot
>>of d[,2] is plotted. how can I combine the two?
>>
>>actually, while on the subject of histograms. is it possible to plot a
>>3D-histogram in R (a true 3D bar plot, without using image).
>>
>>many thanks
>>Murad
> 
> 
> 
> You need to restructure the data passed to barplot() so that the two
> 'height' related columns are converted to 2 rows of 10 columns. Each
> column is then drawn as pairs of bars:
> 
> barplot(rbind(d[, 1], d[, 2]), beside = TRUE)
> 
> Take a look at the change in structure as a result of:
> 
> rbind(d[, 1], d[, 2])
> 
> In terms of 3d histograms, it would appear that Duncan Murdoch, Daniel
> Adler et al are working on porting Duncan's Windows only DJMRGL package
> (http://www.stats.uwo.ca/faculty/murdoch/software) to multiple platforms
> at (http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl). If my
> read is correct, it looks like they have some of the primitives ready to
> go at this time, but perhaps have not yet converted Duncan's hist3d()
> function.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bates at stat.wisc.edu  Mon Jul 21 15:32:20 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 21 Jul 2003 13:32:20 -0000
Subject: [R] [OT] Modeling strategies
Message-ID: <6r7k6cgg3y.fsf@bates4.stat.wisc.edu>

I happened to look up Frank Harrell's book "Regression Modeling
Strategies" on Amazon.com today.  I was surprised to see that in
addition to the typical links to related books they had sponsored
links to sites about "How to become a model", "Try out for reality TV
shows", ...

There seems to be some confusion about the nature of the modeling that
Frank describes.



From boysen at math.uni-goettingen.de  Mon Jul 21 15:42:17 2003
From: boysen at math.uni-goettingen.de (Leif.Boysen)
Date: Mon, 21 Jul 2003 15:42:17 +0200 (MET DST)
Subject: [R] Confidence Band for empirical distribution function
Message-ID: <Pine.OSF.4.03.10307211525570.24908-100000@u35.num.math.uni-goettingen.de>

Hi,

I was trying to draw an empirical distribution function with uniform
confidence bands. So I tried to find a way to calculate values of the
Kolmogorov-Smirnov Distribution but failed.
I guess it must be hidden somewhere (since the ks-test is implemented),
but I was unable to find it. 

Is there any way to do this?

Thanks

Leif Boysen



From tlumley at u.washington.edu  Mon Jul 21 16:14:05 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Jul 2003 07:14:05 -0700 (PDT)
Subject: [R] C compiler to R
In-Reply-To: <Pine.GSO.4.05.10307210839110.6207-100000@athenas.ime.unicamp.br>
Message-ID: <Pine.A41.4.44.0307210712360.88596-100000@homer03.u.washington.edu>

On Mon, 21 Jul 2003, Cezar Augusto de Freitas Anselmo wrote:

> I'd like to know what the more appropriate C compiler to use with R.

If you have an already-compiled R then you should use the compiler that
was used to compile R.

If you want to compile R then any C compiler should work, but gcc gets the
most testing.

	-thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Mon Jul 21 16:21:12 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Jul 2003 07:21:12 -0700 (PDT)
Subject: [R] Re: [Rd] calling R from C
In-Reply-To: <D15343265276D31197BC00A024A6C11077412A@EXS_BDC>
Message-ID: <Pine.A41.4.44.0307210715020.88596-100000@homer03.u.washington.edu>

On Mon, 21 Jul 2003, Khamenia, Valery wrote:

> Hi All,
>
>   We'd like to use functions provided in R in our application.
>   Our application is written in C/C++ and currently runs on
>   win32, Linux and Mac. We'd be happy to attach the whole
>   R ( i.e. not just transfer some function by hand).
>   It is important that we deal with big amount of data, so
>   "command line"-like invocations won't be very interesting.
>   We'd like to link R's code statically or dynamically.
>

You can compile R as a shared library, which allows you to construct and
evaluate R expressions from C.

There's a recent thread on r-devel with a bit more information:
http://maths.newcastle.edu.au/~rking/R/devel/03b/0073.html


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From MSchwartz at medanalytics.com  Mon Jul 21 16:29:06 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 21 Jul 2003 14:29:06 -0000
Subject: [R] help on barplot
In-Reply-To: <3F1BEAFC.5090802@pdf.com>
References: <3F1B4D0C.8666AED7@columbia.edu>
	<1058762088.9353.114.camel@localhost>  <3F1BEAFC.5090802@pdf.com>
Message-ID: <1058797747.9353.124.camel@localhost>

No practical difference in this case. Either way you end up with the
matrix rotated 90 degrees.

Presumably t(d) for a large d would be faster, however at 11:30 last
night, using rbind() for some reason was the first approach that came to
mind...  ;-)

Marc


On Mon, 2003-07-21 at 08:30, Spencer Graves wrote:
> Please relieve me my ignorance on one point:  How does "rbind(d[, 1], 
> d[, 2])" differe from "t(d)"?
> 
> Thanks, Spencer Graves
> 
> Marc Schwartz wrote:
> > On Sun, 2003-07-20 at 21:16, Murad Nayal wrote:
> > 
> >>Hello,
> >>
> >>I am trying to compare two histograms using barplot. the idea is to plot
> >>the histograms as pairs of columns side by side for each x value. I was
> >>able to do it using barplot before but I can't remember now for the life
> >>of me now how I did it in the past:
> >>
> >>
> >>>d
> >>
> >>              [,1]         [,2]
> >>-37.5 0.0000000000 2.789396e-05
> >>-32.5 0.0001394700 5.578801e-05
> >>-27.5 0.0019804742 1.732218e-02
> >>-22.5 0.0217294282 1.380474e-01
> >>-17.5 0.0938912134 4.005579e-02
> >>-12.5 0.0630683403 4.351464e-03
> >>-7.5  0.0163179916 8.368201e-05
> >>-2.5  0.0025941423 5.578801e-05
> >>2.5   0.0002789400 0.000000e+00
> >>7.5   0.0000000000 0.000000e+00
> >>
> >>
> >>>barplot(d,beside=TRUE)
> >>
> >>barplot here plots two separate 'sets' of columns, on the left side a
> >>bar plot of d[,1] is plotted while on the right side a separate bar plot
> >>of d[,2] is plotted. how can I combine the two?
> >>
> >>actually, while on the subject of histograms. is it possible to plot a
> >>3D-histogram in R (a true 3D bar plot, without using image).
> >>
> >>many thanks
> >>Murad
> > 
> > 
> > 
> > You need to restructure the data passed to barplot() so that the two
> > 'height' related columns are converted to 2 rows of 10 columns. Each
> > column is then drawn as pairs of bars:
> > 
> > barplot(rbind(d[, 1], d[, 2]), beside = TRUE)
> > 
> > Take a look at the change in structure as a result of:
> > 
> > rbind(d[, 1], d[, 2])
> > 
> > In terms of 3d histograms, it would appear that Duncan Murdoch, Daniel
> > Adler et al are working on porting Duncan's Windows only DJMRGL package
> > (http://www.stats.uwo.ca/faculty/murdoch/software) to multiple platforms
> > at (http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl). If my
> > read is correct, it looks like they have some of the primitives ready to
> > go at this time, but perhaps have not yet converted Duncan's hist3d()
> > function.
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dave at evocapital.com  Mon Jul 21 16:30:00 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Mon, 21 Jul 2003 15:30:00 +0100
Subject: [R] RODBC: problem saving a new table in an "Excel database"
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063DE5@mail.internal.net>

Hi

I am using package RODBC version 1.0-1 under R version 1.7.1 on Windows
XP Pro. I am having problems writing a new table to an (Excel) database
using sqlSave.

I connect to an empty Excel spreadsheet using odbcConnectExcel (which, I
believe, uses the Microsoft Excel Driver DSN). Then I try and save a new
table to the database(spreadsheet) using SqlSave, but obtain an error
message. 

Below is some test code which reproduces the error (assuming that
c:\test.xls is a standard "blank" Excel spreadsheet; i.e. Sheet1,
Sheet2, Sheet3)

> con = odbcConnectExcel("c:\\test.xls")
> df = data.frame(x = rnorm(10), y = rnorm(10), z = letters[1:10])
> sqlSave(con, df)
Error in sqlSave(con, df) : [RODBC] ERROR: Could not SQLExecute

Where am I going wrong?

Thanks,

Dave



From wl at eimb.ru  Mon Jul 21 16:50:33 2003
From: wl at eimb.ru (Wladimir Eremeev)
Date: Mon, 21 Jul 2003 18:50:33 +0400
Subject: [R] Lattice: how to draw some text outside the panel?
In-Reply-To: <200307211014.h6LA1reQ007882@stat.math.ethz.ch>
References: <200307211014.h6LA1reQ007882@stat.math.ethz.ch>
Message-ID: <9785.030721@eimb.ru>

Dear r-help

   I draw plots with xyplot() function.

   Each plot contains also a line of regression.
   I want to write the trend value and its significance
   (obtained with lm()) below each panel.
   I use ltext() for this.
   But the text is cut, when it comes outside a panel.
   Moreover (obviously), it doesn't appear at all when its
   coordinates are outside a panel.

   Could you, please, be so kind to give me a direction to walk
   in order to have a text written below a panel.

   Thank you!

-- 
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist                                Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 954-5534



From flom at ndri.org  Mon Jul 21 16:43:54 2003
From: flom at ndri.org (Peter Flom)
Date: Mon, 21 Jul 2003 10:43:54 -0400
Subject: [R] Changing the labels on a regression tree (repeat post - with
	added clarity)
Message-ID: <sf1bc3f1.033@MAIL.NDRI.ORG>

Hello

I posted a very similar question last week, but the responses I
received indicated that my post was unclear....

I have a regression tree created in rpart with

tr.logypsx <- rpart(log(YPSX + 1)
~AGE+drugfact+sexfact+as.numeric(OBSX) +WINDLE + EABUSED     + PABAU +
positive.par + control.par + lenient.par,   xval = 10, method = 'anova',
cp = 0.0001, data = duhray2)

and then 

tr.logypsx.pruned <- prune(tr.logypsx, cp = .012)

The labels at the nodes are, naturally enough, the dependent variable
[log(YPSX + 1)].  This DV is better than YPSX because YPSX is extremely
skew.  However, the intuitively meaningful variable is YPSX.  Is there a
way to change the labels to reflect this?

Thanks in advance, and sorry for the repeat

Peter



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ripley at stats.ox.ac.uk  Mon Jul 21 16:44:41 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jul 2003 15:44:41 +0100 (BST)
Subject: [R] RODBC: problem saving a new table in an "Excel database"
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063DE5@mail.internal.net>
Message-ID: <Pine.LNX.4.44.0307211544150.20579-100000@gannet.stats>

The driver does not support writing to Excel: did you check?

On Mon, 21 Jul 2003, David Khabie-Zeitoune wrote:

> Hi
> 
> I am using package RODBC version 1.0-1 under R version 1.7.1 on Windows
> XP Pro. I am having problems writing a new table to an (Excel) database
> using sqlSave.
> 
> I connect to an empty Excel spreadsheet using odbcConnectExcel (which, I
> believe, uses the Microsoft Excel Driver DSN). Then I try and save a new
> table to the database(spreadsheet) using SqlSave, but obtain an error
> message. 
> 
> Below is some test code which reproduces the error (assuming that
> c:\test.xls is a standard "blank" Excel spreadsheet; i.e. Sheet1,
> Sheet2, Sheet3)
> 
> > con = odbcConnectExcel("c:\\test.xls")
> > df = data.frame(x = rnorm(10), y = rnorm(10), z = letters[1:10])
> > sqlSave(con, df)
> Error in sqlSave(con, df) : [RODBC] ERROR: Could not SQLExecute
> 
> Where am I going wrong?
> 
> Thanks,
> 
> Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chrysopa at insecta.ufv.br  Mon Jul 21 16:38:27 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Mon, 21 Jul 2003 11:38:27 -0300
Subject: [R] problem in pot with subset selection
Message-ID: <200307210906.16266.chrysopa@insecta.ufv.br>

Hi,

I try to make a selective plot, but it make an error. The subset only work 
with length < 4, it is correct?

> 
plot((ocorrencia/isca)~frag,subset=especieama==c("grupo1","grupo2","grupo3","AnoplotermesSp1","NeocapritermesOpacus"),pch=c(1,2,3,4,5),xlab="?rea 
(ha)",ylab="Propor??o de iscas ocupadas por t?rmitas")
Warning messages: 
1: longer object length
	is not a multiple of shorter object length in: is.na(e1) | is.na(e2) 
2: longer object length
	is not a multiple of shorter object length in: "==.default"(especieama, 
c("grupo1", "grupo2", "grupo3", "AnoplotermesSp1",  

I make the plot using points, but I want to know if this is a bug or real 
limitation in subset comparison.

I use R-1.7.1

Thanks
Ronaldo
-- 
I have discovered that all human evil comes from this, man's being unable
to sit still in a room.
		-- Blaise Pascal
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From chrysopa at insecta.ufv.br  Mon Jul 21 17:00:09 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Mon, 21 Jul 2003 12:00:09 -0300
Subject: [R] doubt about graphics
Message-ID: <200307211200.09426.chrysopa@insecta.ufv.br>

Hi,

I have an data like this

x <- c(1:11)
y <- seq(245,445,20)

I make a plot

plot(x,y)

The first y value in y axis is 250 and the last y valeu is 450, the spacing 
between values are 50, so the y values showed are: 250, 300, 350, 400, 450

I need that values in y axis are: 240, 280, 320, 360, 400, 440, 480.

I try

plot(x,y,yaxp=c(240,480,6))

dont work

I try

plot(x,y,yaxp=c(240,480,6),ylim=range(240,480))

dont work too.

How I make this control?

Thanks
Ronaldo

-- 
The turtle lives 'twixt plated decks
Which practically conceal its sex.
I think it clever of the turtle
In such a fix to be so fertile.
		-- Ogden Nash
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From spencer.graves at pdf.com  Mon Jul 21 17:27:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 08:27:51 -0700
Subject: [R] doubt about graphics
References: <200307211200.09426.chrysopa@insecta.ufv.br>
Message-ID: <3F1C0677.9010107@pdf.com>


x <- c(1:11)
y <- seq(245,445,20)

plot(x,y, axes=F, ylim=c(240, 480))
axis(1)
axis(2, at=seq(240, 480, 40))

Worked for me on R 1.7.1 and S-Plus 6.1 under Win2000.

hope this helps.  spencer graves

Ronaldo Reis Jr. wrote:
> Hi,
> 
> I have an data like this
> 
> x <- c(1:11)
> y <- seq(245,445,20)
> 
> I make a plot
> 
> plot(x,y)
> 
> The first y value in y axis is 250 and the last y valeu is 450, the spacing 
> between values are 50, so the y values showed are: 250, 300, 350, 400, 450
> 
> I need that values in y axis are: 240, 280, 320, 360, 400, 440, 480.
> 
> I try
> 
> plot(x,y,yaxp=c(240,480,6))
> 
> dont work
> 
> I try
> 
> plot(x,y,yaxp=c(240,480,6),ylim=range(240,480))
> 
> dont work too.
> 
> How I make this control?
> 
> Thanks
> Ronaldo
>



From p.dalgaard at biostat.ku.dk  Mon Jul 21 17:41:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 21 Jul 2003 15:41:38 -0000
Subject: [R] [OT] Modeling strategies
In-Reply-To: <6r7k6cgg3y.fsf@bates4.stat.wisc.edu>
References: <6r7k6cgg3y.fsf@bates4.stat.wisc.edu>
Message-ID: <x2brvn51f3.fsf@biostat.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> I happened to look up Frank Harrell's book "Regression Modeling
> Strategies" on Amazon.com today.  I was surprised to see that in
> addition to the typical links to related books they had sponsored
> links to sites about "How to become a model", "Try out for reality TV
> shows", ...
> 
> There seems to be some confusion about the nature of the modeling that
> Frank describes.

I noticed the same thing with Carson, Cobelli, and Finkelstein's book
on metabolic modeling.

The "Similar items" categories can be fun too, especially the "Loosely
related" group.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From chrysopa at insecta.ufv.br  Mon Jul 21 17:46:46 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Mon, 21 Jul 2003 12:46:46 -0300
Subject: [R] how to calculate Rsquare
Message-ID: <200307211241.57759.chrysopa@insecta.ufv.br>

Hi,

I have something like this:

> x <- 1:10
> y2 <- 30+5*x+rnorm(x,sd=3)
> y <- c(y1,y2)
> x <- c(x,x)
> plot(x,y)
> x <- 1:10
> y1 <- 1+5*x+rnorm(x,sd=2)
> y2 <- 30+5*x+rnorm(x,sd=5)
> y <- c(y1,y2)
> x <- c(x,x)
> f <- factor(rep(c("a","b"),c(10,10)))
> m <- lm(y~x+f)
> anova(m)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)    
x          1 4062.9  4062.9  400.04 2.990e-13 ***
f          1 4421.5  4421.5  435.35 1.496e-13 ***
Residuals 17  172.7    10.2                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> #rsquare of model
> (4062.9+4421.5)/(4062.9+4421.5+172.7)
[1] 0.980051

In this way I calculate the model rsquare, but how to calculate the rsquare of  
each levels "a" and "b"?

This is only an example, the model maybe glm, lme etc.

Thanks
Ronaldo 
-- 
NOWPRINT. NOWPRINT. Clemclone, back to the shadows again.
- The Firesign Theater
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From tlumley at u.washington.edu  Mon Jul 21 18:02:03 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Jul 2003 09:02:03 -0700 (PDT)
Subject: [R] problem in pot with subset selection
In-Reply-To: <200307210906.16266.chrysopa@insecta.ufv.br>
Message-ID: <Pine.A41.4.44.0307210859270.85788-100000@homer39.u.washington.edu>

On Mon, 21 Jul 2003, Ronaldo Reis Jr. wrote:

> Hi,
>
> I try to make a selective plot, but it make an error. The subset only work
> with length < 4, it is correct?
>
> >
>
> plot((ocorrencia/isca)~frag,
> subset=especieama==c("grupo1","grupo2","grupo3","AnoplotermesSp1","NeocapritermesOpacus"),
> pch=c(1,2,3,4,5),xlab="Área (ha)",ylab="Proporção de iscas ocupadas por
> térmitas")
> Warning messages:  1: longer object length
> 	is not a multiple of shorter object length in: is.na(e1) | is.na(e2)
> 2: longer object length
> 	is not a multiple of shorter object length in: "==.default"(especieama,
> c("grupo1", "grupo2", "grupo3", "AnoplotermesSp1",
>
> I make the plot using points, but I want to know if this is a bug or real
> limitation in subset comparison.
>

No.

You probably wanted
   especieama %in% c("grupo1","grupo2","grupo3","AnoplotermesSp1","NeocapritermesOpacus")
rather than ==


	-thomas



From fharrell at virginia.edu  Mon Jul 21 18:07:03 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 21 Jul 2003 12:07:03 -0400
Subject: [R] [OT] Modeling strategies
In-Reply-To: <6r7k6cgg3y.fsf@bates4.stat.wisc.edu>
References: <6r7k6cgg3y.fsf@bates4.stat.wisc.edu>
Message-ID: <20030721120703.01801f0a.fharrell@virginia.edu>

On 21 Jul 2003 08:32:33 -0500
Douglas Bates <bates at stat.wisc.edu> wrote:

> I happened to look up Frank Harrell's book "Regression Modeling
> Strategies" on Amazon.com today.  I was surprised to see that in
> addition to the typical links to related books they had sponsored
> links to sites about "How to become a model", "Try out for reality TV
> shows", ...
> 
> There seems to be some confusion about the nature of the modeling that
> Frank describes.

If Amazon is doing their job, they will link these also with 50/50 mixtures of bivariate normal distributions with equal variances and differences in means in only one coordinate.  :-)

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From mkondrin at hppi.troitsk.ru  Tue Jul 22 05:21:37 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 21 Jul 2003 20:21:37 -0700
Subject: [R] grid and gtkDevice package clipping bug.
In-Reply-To: <20030721115607.A3731@jessie.research.bell-labs.com>
References: <3F1C37C3.1050701@hppi.troitsk.ru>
	<20030721115607.A3731@jessie.research.bell-labs.com>
Message-ID: <3F1CADC1.9090609@hppi.troitsk.ru>

Duncan Temple Lang wrote:
> Hi there.
> 
>   What commands did you use to get this effect?  I can't seem
> to reproduce it on my machine with some simple commands to get
> the same display.
> 
>  Thanks,
>    D
> 
> M.Kondrin wrote:
> 
>>Hello!
>>When I draw points on grid's viewport with clip set to TRUE clipping 
>>does not have effect on some symbols. It happens only when drawing over 
>>gtkDevice (GTK ver. 1.2) (with postscript device clipping works OK - 
>>take a look at this screenshot 
>>http://www.hppi.troitsk.ru/Kondrin/clipbug.png) and only with symbols 
>>that are drawn with commands other than gdk_draw_polygon() (i.e. circles 
>>- gdk_draw_arc() - and squares - gdk_draw_rect()).
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>
I test this only with symbols with pch>20.
Here is an example
> gtk()
> viewport(h=0.8,w=0.8,clip=F)->v2
> grid.rect(vp=v2)->r
> v2$clip
[1] FALSE
> v2$clip<-T
> v2$clip
[1] TRUE
> grid.points(c(-0.1,0.5,1.1),c(-0.1,0.5,1.1),pch=21,vp=v2)
>
#clipping does not work - there are circles outside rectangle
> grid.points(c(-0.1,0.5,1.1),c(0,0.5,1),pch=24,vp=v2)
#now OK - have only one triangle in the center of rectangle



From spencer.graves at pdf.com  Mon Jul 21 18:41:10 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 09:41:10 -0700
Subject: [R] how to calculate Rsquare
References: <200307211241.57759.chrysopa@insecta.ufv.br>
Message-ID: <3F1C17A6.9070005@pdf.com>

	  It should be easy to get a separate R^2 from "lm" for each level of a 
factor:  Just split the data and run "lm" once for each level of f. 
I've done this with a "for" loop something like the following:

	dat <- data.frame(x=x, y=y, f=f)
	for(i.f in 1:2){
	   sel <- (f == c("a", "b")[i.f])
	   fit <- lm(y~x, data=dat[sel])
	 ...

	}

where "..." is replaced by appropriate commands to get R^2 from 
summary(fit).  It may also be possible to do with with tapply.

	  Getting an R^2 from glm and lme is harder and controversial:  Some 
people say simply, "Don't do it, because it doesn't mean anything."  For 
those who want to do it anyway, the following papers discuss alternate 
definitions of R^2 outside the standard normal linear model:

	  NagelKerke, N. J. D. (1991) "A note on a general definition of the 
coefficient of determination", Biometrika 78: 691-2.

	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of 
determination for binary responses", The American Statistician 46:  1-4.

	  Cameron, A. Colin and Windmeijer, F. A. G. (1997) "An R-squared 
measure of goodness of fit for some common nonlinear regression models", 
Journal of Econometrics 77:  329-342.

	  I'd be pleased to hear other comments on this issue.  Hope this 
helps.  spencer graves


Ronaldo Reis Jr. wrote:
> Hi,
> 
> I have something like this:
> 
> 
>>x <- 1:10
>>y2 <- 30+5*x+rnorm(x,sd=3)
>>y <- c(y1,y2)
>>x <- c(x,x)
>>plot(x,y)
>>x <- 1:10
>>y1 <- 1+5*x+rnorm(x,sd=2)
>>y2 <- 30+5*x+rnorm(x,sd=5)
>>y <- c(y1,y2)
>>x <- c(x,x)
>>f <- factor(rep(c("a","b"),c(10,10)))
>>m <- lm(y~x+f)
>>anova(m)
> 
> Analysis of Variance Table
> 
> Response: y
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> x          1 4062.9  4062.9  400.04 2.990e-13 ***
> f          1 4421.5  4421.5  435.35 1.496e-13 ***
> Residuals 17  172.7    10.2                      
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
>>#rsquare of model
>>(4062.9+4421.5)/(4062.9+4421.5+172.7)
> 
> [1] 0.980051
> 
> In this way I calculate the model rsquare, but how to calculate the rsquare of  
> each levels "a" and "b"?
> 
> This is only an example, the model maybe glm, lme etc.
> 
> Thanks
> Ronaldo



From lisa_junglov at merck.com  Mon Jul 21 19:36:39 2003
From: lisa_junglov at merck.com (Junglov, Lisa B)
Date: Mon, 21 Jul 2003 13:36:39 -0400
Subject: [R] Rosetta Inpharmatics position announcement
Message-ID: <176FF955CCA3D5118F6800508BB23CB302FE25D9@ussemx01.merck.com>

Apologies for cross-posting to both R-Help and S-News.

I am not a subscriber to this list so please direct all inquires to
rosettahr at merck.com.  

Senior Research Scientist - Data Analysis
The Senior Research Scientist Data Analyst position will be responsible for
mathematical modeling and analysis of gene expression and other data from
various biological systems. This position works closely with biologists at
Rosetta and Merck Research Laboratories to enable advanced methods of drug
discovery. Additionally, the Research Scientist Data Analyst will develop
and evaluate alternative algorithmic approaches, develop modes of display,
and will present results. Extensive experience with statistical data
modeling and familiarity with data mining algorithms are required.
Qualifications include: A Ph.D. with solid experience in Molecular Biology,
Biophysics, Bioinformatics or related field is required. Proficiency in
Matlab is required and familiarity with C++, Splus, SAS, PERL, NT and Unix
is preferred.
Rosetta Inpharmatics LLC, a wholly owned subsidiary of Merck & Co. Inc.
<../redirect/merck.htm> , develops and implements technologies that will
improve drug discovery. The company's leading-edge genomic research and data
analysis efforts focus on how medical compounds affect biology, enabling
more accurate selection of drug targets and more efficient drug development.


For more information about Rosetta and other position please visit our site
at www.rii.com <www.rii.com> .


Thank you,


Lisa Junglov
Human Resources
Rosetta Inpharmatics        
12040 115th Avenue NE         
Kirkland, WA 98034
lisa_junglov at merck.com 
(425) 636-6421 
www.rii.com <www.rii.com>         


------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From richard_raubertas at merck.com  Mon Jul 21 19:44:46 2003
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Mon, 21 Jul 2003 13:44:46 -0400
Subject: [R] Inconsistent handling of character NA?
Message-ID: <38C4C095FC35E5469BED686B42F40A1302846775@usrymx17.merck.com>

[R 1.7.1 on Windows XP Pro]

Since R allows missing values for character variables, why
are NA's not propagated by character manipulation functions?
For example:

> temp <- c("a", NA)
> temp
[1] "a" NA 
> is.na(temp)
[1] FALSE  TRUE
> paste(temp[1], temp[2])
[1] "a NA"
> substr(temp, 1, 1)
[1] "a" "N"
> sub("[aA]","b", temp)
[1] "b"  "Nb"

It seems to me that paste(temp[1], temp[2]) should return
a single character NA: it asks to concatenate "a" with
some unknown string, so the result should be unknown as
well.  This is certainly how numeric NA's are handled.

Rich Raubertas
Biometrics Research, RY33-300
Merck & Co.
 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From john.lewis at sympatico.ca  Mon Jul 21 19:45:39 2003
From: john.lewis at sympatico.ca (john lewis)
Date: Mon, 21 Jul 2003 13:45:39 -0400
Subject: [R] Contents of R-help digest.-contouring
Message-ID: <002201c34faf$e82f2220$8c00a8c0@max>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030721/6fa501e1/attachment.pl

From tord.snall at ebc.uu.se  Mon Jul 21 20:23:46 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Mon, 21 Jul 2003 20:23:46 +0200
Subject: [R] bold AND italic as font in text()
Message-ID: <3.0.6.32.20030721202346.00da7f88@mail.anst.uu.se>

Dear all,
Is it possible to somshow plot text as italic AND bold. I tried font=c(2,3)
in text(), but it doesn't work. It seems like the latter value is used.


Thanks in advance!

Sincerely,
Tord



-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From spencer.graves at pdf.com  Mon Jul 21 20:22:55 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 11:22:55 -0700
Subject: [R] Contents of R-help digest.-contouring
References: <002201c34faf$e82f2220$8c00a8c0@max>
Message-ID: <3F1C2F7F.1040307@pdf.com>

Comments inline:

john lewis wrote:
> R- Users:
> 
> Can someone indicate what I am during wrong? This is a script essentially from 
> Venerable & Ripley's text on interpolating a surface with loess function but I can not get it to run.
> 
> Thanks.
> John Lewis
> Professor
> McGill University
> Montreal
> 
> library(MASS)
> library(modreg)
> data(topo)
> par(mfcol=c(2,2), pty="s")
> topo.loess <- loess(z ~ x * y, topo, degree=2, span = 0.25, normalize=F)
> topo.mar <- list(x = seq(0, 6.5, 0.1), y=seq(0, 6.5, 0.1))
> topo.lo <- predict(topo.loess, expand.grid(topo.mar), se=T)
> lo.1 <- as.data.frame(topo.lo)
> lo.2 <- as.matrix(lo.1$fit)

	  I just listed "lo.2":  It was a 4356 x 1 matrix;  in S-Plus 6.1, it 
was a NULL matrix with 0 rows and 1 column.  I recently attempted to 
upgrade to R 1.7.1 and now I can't get graphics from R.  However, the 
following worked for me in S-Plus 6.1:

   lo.2 <- matrix(topo.lo$fit, nrow=66)

With this, I got a plot that looked sensible (though I didn't compare it 
with MASS).

hope this helps.  spencer graves
> 
> lo.3 <- as.matrix(lo.1$se.fit)
> 
> contour(topo.mar$x,topo.mar$y,lo.2, levels = seq(700,1000,25),
> 
>         xlab="fit", ylab="")
> 
> points(topo)
> 
> contour(topo.mar$x,topo.mar$y,lo.3, levels = seq(5, 25, 5),
> 
>         xlab="standard error", ylab="")
> 
> title("Loess degree = 2")
> 
> points(topo)
> 
> 
> This is the error I keep getting.
> 
> "Error in contour.default(topo.mar$x, topo.mar$y, lo.2, levels = seq(700,  : 
>         no proper `z' matrix specified" 
> 
> I received this same message when I ran the script first without changing topo.lo
> from a list to data.frame etc. as seen below.
> 
> contour(topo.mar$x,topo.mar$y,topo.lo$fit, levels = seq(700,1000,25),
> 
>         xlab="fit", ylab="")
> 
> 
> I checked the length of each variable and they are correct.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tplate at blackmesacapital.com  Mon Jul 21 20:26:43 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 21 Jul 2003 12:26:43 -0600
Subject: [R] Contents of R-help digest.-contouring
In-Reply-To: <002201c34faf$e82f2220$8c00a8c0@max>
Message-ID: <5.2.1.1.2.20030721121514.04a87760@mailhost.blackmesacapital.com>

The 'z' argument to contour must be a matrix with the appropriate 
dimensions -- lo.2 as defined originally has dimensions 4356 by 1.

You can create a matrix of appropriate dimensions as follows:

 > lo.2 <- array(lo.1$fit, dim=sapply(topo.mar, length))

It's also probably worth changing topo.mar so that the x and y elements 
have different lengths, to provide a quick check that the x and y 
dimensions were not swapped somewhere along the way.

hope this helps,

Tony Plate

At Monday 01:45 PM 7/21/2003 -0400, john lewis wrote:
>R- Users:
>
>Can someone indicate what I am during wrong? This is a script essentially 
>from
>
>Venerable & Ripley's text on interpolating a surface with loess function 
>but I can not get it to run.
>
>Thanks.
>
>John Lewis
>
>Professor
>
>McGill University
>
>Montreal
>
>
>
>library(MASS)
>
>library(modreg)
>
>data(topo)
>
>par(mfcol=c(2,2), pty="s")
>
>topo.loess <- loess(z ~ x * y, topo, degree=2, span = 0.25, normalize=F)
>
>topo.mar <- list(x = seq(0, 6.5, 0.1), y=seq(0, 6.5, 0.1))
>
>topo.lo <- predict(topo.loess, expand.grid(topo.mar), se=T)
>
>lo.1 <- as.data.frame(topo.lo)
>
>lo.2 <- as.matrix(lo.1$fit)
>
>lo.3 <- as.matrix(lo.1$se.fit)
>
>contour(topo.mar$x,topo.mar$y,lo.2, levels = seq(700,1000,25),
>
>         xlab="fit", ylab="")
>
>points(topo)
>
>contour(topo.mar$x,topo.mar$y,lo.3, levels = seq(5, 25, 5),
>
>         xlab="standard error", ylab="")
>
>title("Loess degree = 2")
>
>points(topo)
>
>
>This is the error I keep getting.
>
>"Error in contour.default(topo.mar$x, topo.mar$y, lo.2, levels = seq(700,  :
>         no proper `z' matrix specified"
>
>I received this same message when I ran the script first without changing 
>topo.lo
>from a list to data.frame etc. as seen below.
>
>contour(topo.mar$x,topo.mar$y,topo.lo$fit, levels = seq(700,1000,25),
>
>         xlab="fit", ylab="")
>
>
>I checked the length of each variable and they are correct.
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Tony Plate   tplate at acm.org



From tlumley at u.washington.edu  Mon Jul 21 20:29:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Jul 2003 11:29:50 -0700 (PDT)
Subject: [R] Inconsistent handling of character NA?
In-Reply-To: <38C4C095FC35E5469BED686B42F40A1302846775@usrymx17.merck.com>
Message-ID: <Pine.A41.4.44.0307211122190.85788-100000@homer39.u.washington.edu>

On Mon, 21 Jul 2003, Raubertas, Richard wrote:

> [R 1.7.1 on Windows XP Pro]
>
> Since R allows missing values for character variables, why
> are NA's not propagated by character manipulation functions?

They are in the development version.


> For example:
>
> > temp <- c("a", NA)
> > temp
> [1] "a" NA
> > is.na(temp)
> [1] FALSE  TRUE
> > paste(temp[1], temp[2])
> [1] "a NA"
> > substr(temp, 1, 1)
> [1] "a" "N"
> > sub("[aA]","b", temp)
> [1] "b"  "Nb"
>
> It seems to me that paste(temp[1], temp[2]) should return
> a single character NA: it asks to concatenate "a" with
> some unknown string, so the result should be unknown as
> well.  This is certainly how numeric NA's are handled.

paste() does not do what you want even in the development version. The
reason is that paste should be able to produce a printable string from NA
input.  This is certainly how numeric NA's are handled by paste(). You
could make a case that this behaviour is what deparse() is for, but I
don't think it's going to be successful.


Here's the regression tests from the development version so you can see
how the new stuff works.
a <- c("NA", NA, "BANANA")
na <- as.character(NA)
a1 <- substr(a,1,1)
stopifnot(is.na(a1)==is.na(a))
a2 <- substring(a,1,1)
stopifnot(is.na(a2)==is.na(a))
a3 <- sub("NA","na",a)
stopifnot(is.na(a3)==is.na(a))
a3 <- gsub("NA","na",a)
stopifnot(is.na(a3)==is.na(a))
substr(a3, 1, 2) <- "na"
stopifnot(is.na(a3)==is.na(a))
substr(a3, 1, 2) <- na
stopifnot(all(is.na(a3)))
stopifnot(agrep("NA", a) == c(1, 3))
stopifnot(grep("NA", a) == c(1, 3))
stopifnot(grep("NA", a, perl=TRUE) == c(1, 3))
stopifnot(agrep(na, a) == 2)
stopifnot(grep(na, a) == 2)
stopifnot(grep(na, a, perl=TRUE) == 2)
a4 <- abbreviate(a)
stopifnot(is.na(a4) == is.na(a))
a5 <- chartr("NA", "na", a)
stopifnot(is.na(a5) == is.na(a))
a6 <- gsub(na, "na", a)
stopifnot(all(!is.na(a6)))
a7 <- a; substr(a7, 1, 2) <- "na"
stopifnot(is.na(a7) == is.na(a))
a8 <- a; substr(a8, 1, 2) <- na
stopifnot(all(is.na(a8)))
stopifnot(identical(a, toupper(tolower(a))))
a9<-strsplit(a, "NA")
stopifnot(identical(a9, list("",na,c("BA",""))))
a10<-strsplit(a, na)
stopifnot(identical(a10, as.list(a)))
## but nchar doesn't fit this pattern
stopifnot(all(!is.na(nchar(a))))


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ahmlatif at yahoo.com  Mon Jul 21 20:32:22 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Mon, 21 Jul 2003 11:32:22 -0700 (PDT)
Subject: [R] bold AND italic as font in text()
In-Reply-To: <3.0.6.32.20030721202346.00da7f88@mail.anst.uu.se>
Message-ID: <20030721183222.49987.qmail@web41215.mail.yahoo.com>

use 
text(, , expression(bold("what you want to write")))

Mahbub.
--- Tord Snall <tord.snall at ebc.uu.se> wrote:
> Dear all,
> Is it possible to somshow plot text as italic AND
> bold. I tried font=c(2,3)
> in text(), but it doesn't work. It seems like the
> latter value is used.
> 
> 
> Thanks in advance!
> 
> Sincerely,
> Tord
> 
> 
> 
>
-----------------------------------------------------------------------
> Tord Snäll
> Avd. f växtekologi, Evolutionsbiologiskt centrum,
> Uppsala universitet
> Dept. of Plant Ecology, Evolutionary Biology Centre,
> Uppsala University
> Villavägen 14			
> SE-752 36 Uppsala, Sweden
> Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
> Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
> Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
> E-mail: Tord.Snall at ebc.uu.se
> Check this:
> http://www.vaxtbio.uu.se/resfold/snall.htm!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Mon Jul 21 20:49:01 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 21 Jul 2003 14:49:01 -0400
Subject: [R] Contents of R-help digest.-contouring
In-Reply-To: <002201c34faf$e82f2220$8c00a8c0@max>
References: <002201c34faf$e82f2220$8c00a8c0@max>
Message-ID: <90dohv8ajdf96evej5p0io1tl171po6d8t@4ax.com>

On Mon, 21 Jul 2003 13:45:39 -0400, "john lewis"
<john.lewis at sympatico.ca> wrote :

>R- Users:
>
>Can someone indicate what I am during wrong? This is a script essentially from 
>
>Venerable & Ripley's text on interpolating a surface with loess function but I can not get it to run.

That's Venables...

>topo.lo <- predict(topo.loess, expand.grid(topo.mar), se=T)
>
>lo.1 <- as.data.frame(topo.lo)
>
>lo.2 <- as.matrix(lo.1$fit)

The problem is here.  lo.2 ends up being a matrix with dimensions 4356
by 1, because you didn't say what dimensions to use.  Try this
instead:

lo.2 <- matrix(lo.1$fit, length(topo.mar$x), length(topo.mar$y))


Duncan Murdoch



From ripley at stats.ox.ac.uk  Mon Jul 21 20:57:54 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jul 2003 19:57:54 +0100 (BST)
Subject: [R] Contents of R-help digest.-contouring
In-Reply-To: <90dohv8ajdf96evej5p0io1tl171po6d8t@4ax.com>
Message-ID: <Pine.LNX.4.44.0307211952470.20972-100000@gannet.stats>

On Mon, 21 Jul 2003, Duncan Murdoch wrote:

> On Mon, 21 Jul 2003 13:45:39 -0400, "john lewis"
> <john.lewis at sympatico.ca> wrote :
> 
> >R- Users:
> >
> >Can someone indicate what I am during wrong? This is a script essentially from 
> >
> >Venerable & Ripley's text on interpolating a surface with loess function but I can not get it to run.
> 
> That's Venables...
> 
> >topo.lo <- predict(topo.loess, expand.grid(topo.mar), se=T)
> >
> >lo.1 <- as.data.frame(topo.lo)
> >
> >lo.2 <- as.matrix(lo.1$fit)
> 
> The problem is here.  lo.2 ends up being a matrix with dimensions 4356
> by 1, because you didn't say what dimensions to use.  Try this
> instead:
> 
> lo.2 <- matrix(lo.1$fit, length(topo.mar$x), length(topo.mar$y))

which is rather close to what the purported source actually has, including
in the on-line scripts installed with the MASS package under R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Jul 21 21:27:19 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 12:27:19 -0700
Subject: [R] Contents of R-help digest.-contouring
References: <002201c34faf$e82f2220$8c00a8c0@max>
	<90dohv8ajdf96evej5p0io1tl171po6d8t@4ax.com>
Message-ID: <3F1C3E97.6090503@pdf.com>

The reference to "Venerable and Ripley" reminds me that in the late 
1940s, George Box worked for Imperial Chemicals Industries, which 
appeared in at least one brochure I saw as "Empirical Chemicals 
Industries".  George invented response surface methods there, so the 
"typo" was not as inappropriate as one might otherwise think.

Enjoy, Spencer Graves

Duncan Murdoch wrote:
> On Mon, 21 Jul 2003 13:45:39 -0400, "john lewis"
> <john.lewis at sympatico.ca> wrote :
> 
> 
>>R- Users:
>>
>>Can someone indicate what I am during wrong? This is a script essentially from 
>>
>>Venerable & Ripley's text on interpolating a surface with loess function but I can not get it to run.
> 
> 
> That's Venables...
> 
> 
>>topo.lo <- predict(topo.loess, expand.grid(topo.mar), se=T)
>>
>>lo.1 <- as.data.frame(topo.lo)
>>
>>lo.2 <- as.matrix(lo.1$fit)
> 
> 
> The problem is here.  lo.2 ends up being a matrix with dimensions 4356
> by 1, because you didn't say what dimensions to use.  Try this
> instead:
> 
> lo.2 <- matrix(lo.1$fit, length(topo.mar$x), length(topo.mar$y))
> 
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From xiwu at uoguelph.ca  Mon Jul 21 21:54:24 2003
From: xiwu at uoguelph.ca (ximing wu)
Date: Mon, 21 Jul 2003 15:54:24 -0400
Subject: [R] generate a series of fucntion
Message-ID: <5.1.0.14.2.20030721155016.00bb2648@staff.mail.uoguelph.ca>

Hi there,

I want to generate a large amount of functions,

say f=function(x,t) exp(-t[1]-t[2]*g_1(x)-t[3]*g_2(1+x))

where g_1(x) and g_2(x) are from a long list of moments, such as x, x^2, 
log(x), log(1+x) .. and so on.

Any suggestions on how to do this efficiently?

thanks a lot.

x.w



From andrewr at uidaho.edu  Mon Jul 21 22:08:19 2003
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 21 Jul 2003 13:08:19 -0700
Subject: [R] Off topic: looking citations on stepwise regression
In-Reply-To: <200307211020.h6LA1req007882@stat.math.ethz.ch>
References: <200307211020.h6LA1req007882@stat.math.ethz.ch>
Message-ID: <200307211308.20992.andrewr@uidaho.edu>

I apologize for the off-topic question, but this is the largest and most 
responsive statistical community that I've been a part of.

I'm looking for citable references on stepwise regression, specifically those 
that cover when stepwise regession is a suitable fitting technique.  Two 
reviewers have responded to a manuscript where I fit a small dataset (n = 36) 
with a large model (p = 6) suggesting that I should use stepwise regession.  
I think it's a bad idea.  Can anyone help me find some ammunition?

Thanks

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From dcum007 at ec.auckland.ac.nz  Mon Jul 21 22:09:03 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Tue, 22 Jul 2003 08:09:03 +1200
Subject: [R] Random Numbers
Message-ID: <1058818143.3f1c485f9c84f@webmail2.ec.auckland.ac.nz>

Thank you to all who replied to my previous question regarding the generation 
of random numbers. 

I have read up on the literature and learnt a whole lot more. In my research I 
found code for many generators which I have translated into java for my 
project. I'm now at the stage of testing these generators and was wondering if 
there were any inbuilt (or if someone has written libraries for) testing 
commands in R. 

Are there any libraries for R that have the serial correlation test, the 
birthday spacing test or any of the other tests for randomness??
Thank you all
David



From andy_liaw at merck.com  Mon Jul 21 22:15:35 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jul 2003 16:15:35 -0400
Subject: [R] generate a series of fucntion
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8E3@usrymx25.merck.com>

Does this do sort of what you want?

> g1 <- list(function(x) log(x), function(x) log(x+1), function(x) x,
function(x) x^2)
> g1
[[1]]
function(x) log(x)

[[2]]
function(x) log(x+1)

[[3]]
function(x) x

[[4]]
function(x) x^2

> lapply(g1, function(g) {function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))})
[[1]]
function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
<environment: 022BCBD8>

[[2]]
function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
<environment: 022BBD08>

[[3]]
function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
<environment: 022BBDCC>

[[4]]
function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
<environment: 022BBE90>

You did not say what g1 and g2 actually are, but I suppose the above should
give you enough hint.

Andy

> -----Original Message-----
> From: ximing wu [mailto:xiwu at uoguelph.ca] 
> Sent: Monday, July 21, 2003 3:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] generate a series of fucntion
> 
> 
> Hi there,
> 
> I want to generate a large amount of functions,
> 
> say f=function(x,t) exp(-t[1]-t[2]*g_1(x)-t[3]*g_2(1+x))
> 
> where g_1(x) and g_2(x) are from a long list of moments, such 
> as x, x^2, 
> log(x), log(1+x) .. and so on.
> 
> Any suggestions on how to do this efficiently?
> 
> thanks a lot.
> 
> x.w
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From abunn at montana.edu  Mon Jul 21 22:19:54 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 21 Jul 2003 14:19:54 -0600
Subject: [R] Setting name attributes to a vector - join?
Message-ID: <000301c34fc5$8472e620$78f05a99@msu.montana.edu>

I have a vector of land cover class data from a GIS:

> landcov[1:10,2] #the first ten examples of a large (100k+) object
 1  2  3  4  5  6  7  8  9 10 
12 12 01 12 01 15 15 15 15 15

etc.

I also have a lookup table for the class data that gives the cover type
as a string:

> cnd.names #the look up table, i.e., landcov[3,2] == 1 == "Montane
Meadow"
   CndVal                                          Cnddbname
1       1                                     Montane Meadow
2       2                    Sierran Mixed Coniferous Forest
3       3                            Mixed Montane Chaparral
4       4                            Jeffrey Pine-Fir Forest
5       5                           Sierran White Fir Forest

etc.

How can I create an object that has the string as its value?

I.e., foo[3] <- "Montane Meadow"
I tried using apply and a custom function but couldn't get it to work.

I feel like this is trivial but am stuck fast.

Thanks, Andy



From dmurdoch at pair.com  Mon Jul 21 22:24:55 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 21 Jul 2003 16:24:55 -0400
Subject: [R] generate a series of fucntion
In-Reply-To: <5.1.0.14.2.20030721155016.00bb2648@staff.mail.uoguelph.ca>
References: <5.1.0.14.2.20030721155016.00bb2648@staff.mail.uoguelph.ca>
Message-ID: <5riohvs7hkl9e1q8rh778kb1phrvk5gc27@4ax.com>

On Mon, 21 Jul 2003 15:54:24 -0400, ximing wu <xiwu at uoguelph.ca> wrote
:

>Hi there,
>
>I want to generate a large amount of functions,
>
>say f=function(x,t) exp(-t[1]-t[2]*g_1(x)-t[3]*g_2(1+x))
>
>where g_1(x) and g_2(x) are from a long list of moments, such as x, x^2, 
>log(x), log(1+x) .. and so on.
>
>Any suggestions on how to do this efficiently?

I'd just have one function, with a definition like this:

f <- function(x, t, g1, g2)  exp(-t[1]-t[2]*g1(x)-t[3]*g2(1+x))

and pass different functions to it, e.g.

f(x, t, function(x) x, function(x) x^2)

If you really need separate functions, then still do what I suggest
above, but have your individual functions defined in terms of one,
e.g.

f1 <- function(x,t) f(x, t, function(x) x, function(x) x^2)

Duncan Murdoch



From tblackw at umich.edu  Mon Jul 21 22:35:36 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 21 Jul 2003 16:35:36 -0400 (EDT)
Subject: [R] Setting name attributes to a vector - join?
In-Reply-To: <000301c34fc5$8472e620$78f05a99@msu.montana.edu>
Message-ID: <Pine.SOL.4.44.0307211631400.18590-100000@tetris.gpcc.itd.umich.edu>


I assume that  cnd.names  is a vector of character strings and
landcov[ ,2] is a vector of integers in c(1:length(cnd.names).
Then
	foo <- cnd.names[landcov[ ,2]]

should be what you want.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 21 Jul 2003, Andy Bunn wrote:

> I have a vector of land cover class data from a GIS:
>
> > landcov[1:10,2] #the first ten examples of a large (100k+) object
>  1  2  3  4  5  6  7  8  9 10
> 12 12 01 12 01 15 15 15 15 15
>
> etc.
>
> I also have a lookup table for the class data that gives the cover type
> as a string:
>
> > cnd.names #the look up table, i.e., landcov[3,2] == 1 == "Montane
> Meadow"
>    CndVal                                          Cnddbname
> 1       1                                     Montane Meadow
> 2       2                    Sierran Mixed Coniferous Forest
> 3       3                            Mixed Montane Chaparral
> 4       4                            Jeffrey Pine-Fir Forest
> 5       5                           Sierran White Fir Forest
>
> etc.
>
> How can I create an object that has the string as its value?
>
> I.e., foo[3] <- "Montane Meadow"
> I tried using apply and a custom function but couldn't get it to work.
>
> I feel like this is trivial but am stuck fast.
>
> Thanks, Andy



From p.dalgaard at biostat.ku.dk  Mon Jul 21 23:19:34 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 21 Jul 2003 21:19:34 -0000
Subject: [R] Contents of R-help digest.-contouring
In-Reply-To: <3F1C3E97.6090503@pdf.com>
References: <002201c34faf$e82f2220$8c00a8c0@max>
	<90dohv8ajdf96evej5p0io1tl171po6d8t@4ax.com>
	<3F1C3E97.6090503@pdf.com>
Message-ID: <x23cgz4lra.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> The reference to "Venerable and Ripley" reminds me that in the late
> 1940s, George Box worked for Imperial Chemicals Industries, which
> appeared in at least one brochure I saw as "Empirical Chemicals
> Industries".  George invented response surface methods there, so the
> "typo" was not as inappropriate as one might otherwise think.

My old teacher, S?ren Johansen, did a set of lecture notes on
exponential families. The list of references includes "Baker, R.J. and
Nelder, D.A. (1978), The GLIM system, Rothamsted Exponential
Station" (it's *J*.A. Nelder, but that's not the issue...).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kjetil at entelnet.bo  Mon Jul 21 23:23:14 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 21 Jul 2003 17:23:14 -0400
Subject: [R] Confidence Band for empirical distribution function
In-Reply-To: <Pine.OSF.4.03.10307211525570.24908-100000@u35.num.math.uni-goettingen.de>
Message-ID: <3F1C2182.1539.B3532A@localhost>

On 21 Jul 2003 at 15:42, Leif.Boysen wrote:

Here are some functions doing this using the package stepfun:

ecdf.ksCI <- function(x, main = NULL, sub = NULL,
                      xlab = deparse(substitute(x)), ...)
{
    require(stepfun)
    xlab
    if(is.null(main))
        main <- paste("ecdf(",deparse(substitute(x)),") + 95% 
K.S.bands",
                      sep="")
    n <- length(x)
    if(is.null(sub))
        sub <- paste("n = ", n)
    ec <- ecdf(x)
    xx <- get("x", envir=environment(ec))# = sort(x)
    yy <- get("y", envir=environment(ec))
    D <- approx.ksD(n)
    yyu <- pmin(yy+D, 1)
    yyl <- pmax(yy-D, 0)
    ecu <- stepfun(xx, c(yyu, 1) )
    ecl <- stepfun(xx, c(yyl, yyl[n]) )

    ## Plots -- all calling  plot.stepfun

    plot(ec, main = main, sub = sub, xlab = xlab, ...)
    plot(ecu, add=TRUE, verticals=TRUE, do.points=FALSE,
         col.hor="red" , col.vert="red", ...)
    plot(ecl, add=TRUE, verticals=TRUE, do.points=FALSE,
         col.hor="red", col.vert="red", ...)
}


approx.ksD <- function(n)
{
    ## approximations for the critical level for Kolmogorov-Smirnov
    ## statistic D,
    ## for confidence level 0.95. Taken from Bickel & Doksum, table 
IX,
    ## p.483
    ## and Lienert G.A.(1975) who attributes to Miller,L.H.(1956), 
JASA
    ifelse(n > 80,
           1.358 /( sqrt(n) + .12 + .11/sqrt(n)),##Bickel&Doksum, 
table
                                                 ##IX,p.483

           splinefun(c(1:9, 10, 15, 10 * 2:8),# from Lienert
                     c(.975,   .84189, .70760, .62394, .56328,# 1:5
                       .51926, .48342, .45427, .43001, .40925,# 6:10
                       .33760, .29408, .24170, .21012,# 15,20,30,40
                       .18841, .17231, .15975, .14960)) (n))
}


\name{ecdf.ksCI}
\alias{ecdf.ksCI}

\title{ Plotting the empirical distribution function together with 
confidence
        curves. }
\description{ Plots the empirical distribution function for one-
dimensional
        data, together with upper and lower confidence curves. Always 
uses 
        pointwise confidence level of 95\%.
 
}
\usage{
ecdf.ksCI(x, main=NULL, sub=NULL, xlab = deparse(substitute(x)), ...)
}
%- maybe also `usage' for other objects documented here.
\arguments{
  \item{x}{ \code{x} numerical vector of observations.  }
  \item{\dots}{ \code{\dots} arguments given to 
\code{\link{plot.stepfun}}.}
}
\details{
  Uses the \code{\link{stepfun}} package. 
}
\value{
 Nothing. Used for its side effect, to produce a plot.
}
\references{ Peter J. Bickel & Kjell A. Doksum: Mathematical 
Statistics, Basic Ideas and Selected
      Topics. Holden-Day, 1977. }
\author{ Kjetil Halvorsen }




\seealso{ \code{\link{ecdf}} and \code{\link{plot.stepfun}} in 
package
          \code{\link{stepfun}}. }

\examples{
ecdf.ksCI( rchisq(50,3) )
}
\keyword{ hplot }
\keyword{nonparametric}


Kjetil Halvorsen



> Hi,
> 
> I was trying to draw an empirical distribution function with uniform
> confidence bands. So I tried to find a way to calculate values of the
> Kolmogorov-Smirnov Distribution but failed.
> I guess it must be hidden somewhere (since the ks-test is implemented),
> but I was unable to find it. 
> 
> Is there any way to do this?
> 
> Thanks
> 
> Leif Boysen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Sue_Paul at moh.govt.nz  Tue Jul 22 01:17:00 2003
From: Sue_Paul at moh.govt.nz (Sue_Paul@moh.govt.nz)
Date: Tue, 22 Jul 2003 11:17:00 +1200
Subject: [R] Analysis of Complex Survey Data
Message-ID: <OF3B9BF74E.E7FDF237-ONCC256D6A.007F748E@moh.govt.nz>

Hi all
I would like to perform a logistic regression analysis on some complex survey data with R, but am not sure if there are functions within R that will
enable me to do so.
Also, are there any extensions of the "cor" function that would enable me to incorporate survey weights when calculating correlation coefficients for
bivariate data.
Any help on this matter will be greatly appreciated.
Many thanks in advance.

Kind Regards
Sue Paul
Advisor (Statistics)
Public Health Intelligence
Ministry of Health
DDI: 04 460 4926
Mobile: 021 100 3340
Fax: 04 495 4401

http://www.moh.govt.nz/PHI
mailto:sue_paul at moh.govt.nz



From tlumley at u.washington.edu  Tue Jul 22 01:46:19 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 21 Jul 2003 16:46:19 -0700 (PDT)
Subject: [R] Analysis of Complex Survey Data
In-Reply-To: <OF3B9BF74E.E7FDF237-ONCC256D6A.007F748E@moh.govt.nz>
Message-ID: <Pine.A41.4.44.0307211638260.85788-100000@homer39.u.washington.edu>

On Tue, 22 Jul 2003 Sue_Paul at moh.govt.nz wrote:

> Hi all I would like to perform a logistic regression analysis on some
> complex survey data with R, but am not sure if there are functions
> within R that will enable me to do so.

svyglm() in the survey package will fit generalised linear models to
complex survey data.


> Also, are there any extensions of
> the "cor" function that would enable me to incorporate survey weights
> when calculating correlation coefficients for bivariate data. Any help
> on this matter will be greatly appreciated. Many thanks in advance.
>

There's nothing built in for correlations. You can use svyvar() to get the
covariance matrix and get the correlations from that
eg

   covmat<-svyvar(~var1+var2+var3, design=mysurvey)
   vars<-diag(covmat)
   corrmat<- covmat/sqrt(vars%*%t(vars))

What you can't get easily is standard errors for these correlations.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ok at cs.otago.ac.nz  Tue Jul 22 02:22:36 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 22 Jul 2003 12:22:36 +1200 (NZST)
Subject: [R] generate a series of fucntion
Message-ID: <200307220022.h6M0MalU259599@atlas.otago.ac.nz>

ximing wu <xiwu at uoguelph.ca> wrote:
	> I want to generate a large amount of functions,
	> 
	> say f=function(x,t) exp(-t[1]-t[2]*g_1(x)-t[3]*g_2(1+x))
	> 
	> where g_1(x) and g_2(x) are from a long list of moments, such 
	> as x, x^2, 
	> log(x), log(1+x) .. and so on.
	> 
	> Any suggestions on how to do this efficiently?

Unfortunately, outer() doesn't like function elements.

Let's suppose you have vectors g.1[1:n] and g.2[1:n],
and that you want to construct a list f[1:n] of functions.

f <- lapply(1:n, function(i) {
        p <- g.1[i]
        q <- g.2[i]
        function (x, t) exp(-t[1] - t[2]*p(x) - t[3]*q(x))
     })

will make a list of n functions.  Note that the function passed to lappy()
returns a function.  In R, it returns a *different* function each time,
and each of these points to an environment with its own copy of p and q.



From fjmolina at lbl.gov  Tue Jul 22 03:56:05 2003
From: fjmolina at lbl.gov (Francisco J Molina)
Date: Mon, 21 Jul 2003 18:56:05 -0700
Subject: [R] R and C++ compared with only C++
Message-ID: <16156.39349.973173.495231@dhcp-63-193.cse.ucsc.edu>


My computer is a pentium 4 running at 2.4 GHz.
My R is 1.7.1

I have written a program in R that calls C++. The program spends most of the
time in C++ ( > 90% ). R basically deals with output and input.
How slower can this be compared with the program I would get from rewriting
everything in C++?

Thank you.



From p.murrell at auckland.ac.nz  Tue Jul 22 04:04:46 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 22 Jul 2003 14:04:46 +1200
Subject: [R] bold AND italic as font in text()
References: <3.0.6.32.20030721202346.00da7f88@mail.anst.uu.se>
Message-ID: <3F1C9BBE.7040002@stat.auckland.ac.nz>

Hi


Tord Snall wrote:
> Dear all,
> Is it possible to somshow plot text as italic AND bold. I tried font=c(2,3)
> in text(), but it doesn't work. It seems like the latter value is used.


Looks like you want font=4.  Try the following:

	plot(0:5, 0:5, type="n")
	text(1:4, 1:4, paste("font = ", 1:4), font=1:4, cex=3)

[For mathematical annotation, you can use bolditalic(something).]

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ncroglio at iname.com  Tue Jul 22 04:06:21 2003
From: ncroglio at iname.com (Nicholas Croglio)
Date: Mon, 21 Jul 2003 19:06:21 -0700
Subject: [R] function default
Message-ID: <20030722020621.11470.qmail@iname.com>


Greetings,
Is there a way to create default arguments for functions?
If there is, then please explain or forward me to the instructions.
I could not find anything in the archives or in the manual under function or args.

Thanks
-- 
__________________________________________________________



CareerBuilder.com has over 400,000 jobs. Be smarter about your job search
http://corp.mail.com/careers



From Simon.Blomberg at anu.edu.au  Tue Jul 22 04:14:32 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Tue, 22 Jul 2003 12:14:32 +1000
Subject: [R] function default
Message-ID: <7A3A13F416B40842BD2C1753E044B359B09971@CASEVS02.cas.anu.edu.au>

Simply declare the defaults where you specify the function arguments. e.g. 

test <- function (x=4, y=5) x*y

> test()
[1] 20

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Nicholas Croglio [mailto:ncroglio at iname.com]
> Sent: Tuesday, 22 July 2003 12:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] function default
> 
> 
> 
> Greetings,
> Is there a way to create default arguments for functions?
> If there is, then please explain or forward me to the instructions.
> I could not find anything in the archives or in the manual 
> under function or args.
> 
> Thanks
> -- 
> __________________________________________________________
> 
> 
> 
> CareerBuilder.com has over 400,000 jobs. Be smarter about 
> your job search
> http://corp.mail.com/careers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Tue Jul 22 04:16:00 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Tue, 22 Jul 2003 02:16:00 +0000
Subject: [R] function default
In-Reply-To: <20030722020621.11470.qmail@iname.com>
References: <20030722020621.11470.qmail@iname.com>
Message-ID: <1058840160.3f1c9e60d8d11@my.uq.edu.au>

Most functions in R have default arguments, and these are
specified in the function definition. For instance, the
following function has a default argument of 10 which is
used if none is specified:

do.it <- function(n=10) { rnorm(n) }
do.it()

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Nicholas Croglio <ncroglio at iname.com>:

> 
> Greetings,
> Is there a way to create default arguments for
> functions?
> If there is, then please explain or forward me to the
> instructions.
> I could not find anything in the archives or in the
> manual under function or args.
> 
> Thanks
> -- 
> __________________________________________________________
> 
> 
> 
> CareerBuilder.com has over 400,000 jobs. Be smarter about
> your job search
> http://corp.mail.com/careers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Tue Jul 22 04:19:47 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Tue, 22 Jul 2003 02:19:47 +0000
Subject: [R] R and C++ compared with only C++
In-Reply-To: <16156.39349.973173.495231@dhcp-63-193.cse.ucsc.edu>
References: <16156.39349.973173.495231@dhcp-63-193.cse.ucsc.edu>
Message-ID: <1058840387.3f1c9f43631c9@my.uq.edu.au>

As always, it depends on your priorities and motivation. If
that extra 10% is important and if you can be bothered
writing the C++, then putting the whole thing into C++ may
be worth it. If not then you could let R deal with the
input and output.

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Francisco J Molina <fjmolina at lbl.gov>:

> 
> My computer is a pentium 4 running at 2.4 GHz.
> My R is 1.7.1
> 
> I have written a program in R that calls C++. The program
> spends most of the
> time in C++ ( > 90% ). R basically deals with output and
> input.
> How slower can this be compared with the program I would
> get from rewriting
> everything in C++?
> 
> Thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Tue Jul 22 04:20:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jul 2003 19:20:38 -0700
Subject: [R] function default
References: <20030722020621.11470.qmail@iname.com>
Message-ID: <3F1C9F76.7000907@pdf.com>

fun <-
function(x=2, y=3){
	c(x, y)
}

fun()

should return (2, 3);  "fun(1)" should return (1, 3);  "fun(y=9)" should 
return (2, 9), etc.

hope this helps.  spencer graves

Nicholas Croglio wrote:
> Greetings,
> Is there a way to create default arguments for functions?
> If there is, then please explain or forward me to the instructions.
> I could not find anything in the archives or in the manual under function or args.
> 
> Thanks



From p.murrell at auckland.ac.nz  Tue Jul 22 04:38:46 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 22 Jul 2003 14:38:46 +1200
Subject: [R] Lattice: how to draw some text outside the panel?
References: <200307211014.h6LA1reQ007882@stat.math.ethz.ch>
	<9785.030721@eimb.ru>
Message-ID: <3F1CA3B6.6010701@stat.auckland.ac.nz>

Hi


Wladimir Eremeev wrote:
> Dear r-help
> 
>    I draw plots with xyplot() function.
> 
>    Each plot contains also a line of regression.
>    I want to write the trend value and its significance
>    (obtained with lm()) below each panel.
>    I use ltext() for this.
>    But the text is cut, when it comes outside a panel.
>    Moreover (obviously), it doesn't appear at all when its
>    coordinates are outside a panel.
> 
>    Could you, please, be so kind to give me a direction to walk
>    in order to have a text written below a panel.


Lattice clips to the panel by default.  I think there is a plan to add a 
lattice argument to allow you to turn this clipping off in future versions.

In the meantime, there is a workaround.  The idea is to push a viewport 
bigger than the panel and set the clipping region using that larger 
viewport, then push another viewport which is the same as the original 
panel (but do not clip to that).  The following example shows what to do 
   (if you remove the push.viewport and pop.viewport bits the text is 
clipped to the panel):

      data(quakes)
      Depth <- equal.count(quakes$depth, number=8, overlap=.1)
      xyplot(lat ~ long | Depth, data = quakes,
             panel=function(x, y, ...) {
               panel.xyplot(x, y)

               # Set the clipping region four times larger
               xscale <- current.viewport()$xscale
               yscale <- current.viewport()$yscale
               push.viewport(viewport(width=2, height=2, clip=TRUE))
               push.viewport(viewport(width=.5, height=.5,
                                      xscale=xscale, yscale=yscale))

               ltext(rep(165, 4), seq(-10, -40, -10),
                     c("one", "two", "three", "four"))

               # Pop the extra viewports (VERY IMPORTANT!)
               pop.viewport(2)
             })

Hope that helps

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From kjetil at entelnet.bo  Tue Jul 22 05:04:57 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 21 Jul 2003 23:04:57 -0400
Subject: Rcmdr problems (was:Re: [R] Minor nuisance with rw1071)
In-Reply-To: <5.1.0.14.2.20030718094312.020eb6f0@127.0.0.1>
References: <Pine.LNX.4.44.0307180826290.6655-100000@gannet.stats>
Message-ID: <3F1C7199.1487.1EC3128@localhost>

On 18 Jul 2003 at 9:54, John Fox wrote:

Sorry for being so late coming back on this, I was travelling the 
weekend. I tried to remove as much as possible from the workspace 
(.RData file), but nothing created by Rcmdr. This did'nt change 
anything. 

Then I downloaded the latest Rcmdr (0.9-0, development version) from 
John Fox page, and compiled it myself for windows. The focus problem 
is unchanged, but now the problems with attaching a dataset has gone, 
the warnings about _ being deprecated has gone, happily. 

Also, the problem with printing the .logFont variable seems to have 
gone. In all but one trials it did'nt bomb Rgui.

Kjetil

> Dear Brian,
> 
> At 08:32 AM 7/18/2003 +0100, Prof Brian Ripley wrote:
> >I believe it's a side effect of starting up tcltk.  That grabs focus for
> >its invisible window: there's a kludgy workaround in the tcltk startup
> >code, but that it looks like that is not working during the startup
> >(which here seems to be something in .RData).
> 
> That there's something in .RData causing the problem is a good guess. To 
> provide a bit more context: Kjetil reports to me that he's unable to use 
> the Rcmdr GUI without generating "Warning: The use of _ is soon to be 
> removed: you will be warned repeatedly" and parse errors. Of course, I 
> don't use underscores in any of my code.
> 
> The only other similar report that I received was from Martin Maechler 
> using Rcmdr under Emacs/ESS on a Linux machine. That problem went away when 
> he ran R from a terminal rather than from Emacs, but perhaps that was 
> coincidence. (Rcmdr works for me under XEmacs/ESS under Windows, though 
> it's not a natural marriage.)
> 
> As a more general matter, I've encountered but been able to work around a 
> variety of focus problems using the tcltk package.
> 
> Regards,
>   John
> 
> 
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> -----------------------------------------------------
>



From ok at cs.otago.ac.nz  Tue Jul 22 05:30:30 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 22 Jul 2003 15:30:30 +1200 (NZST)
Subject: [R] R and C++ compared with only C++
Message-ID: <200307220330.h6M3UUDL298212@atlas.otago.ac.nz>

Francisco J Molina <fjmolina at lbl.gov> asked:
	I have written a program in R that calls C++.
	The program spends most of the time in C++ ( > 90% ). ...
	How slower can this be compared with the program I would get
	from rewriting everything in C++?
	
Suppose the program takes 1 minute in R and 9 minutes in C++.
Suppose you could make the R code *infinitely* faster by
rewriting it in C++.  Then the program would just take 9 minutes in C++.
Your program would then take 90% or the time, or run 11% faster.

If course, if your rewritten R code *wasn't* infinitely fast in C++,
the gain would be even less.  It could even be slower (because the
C++ code you wrote might not be as good as the C, C++, or Fortran
code that R was really spending its time in).



From kurt.neumann at chello.at  Tue Jul 22 08:39:53 2003
From: kurt.neumann at chello.at (Dipl. Ing. Kurt Neumann)
Date: Tue, 22 Jul 2003 08:39:53 +0200
Subject: [R] Help is sought for a small problem 
Message-ID: <000d01c3501c$0f068de0$377abad4@neumannpiv>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030722/6e0c814e/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jul 22 09:07:04 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Jul 2003 09:07:04 +0200
Subject: [R] Help is sought for a small problem
In-Reply-To: <000d01c3501c$0f068de0$377abad4@neumannpiv>
References: <000d01c3501c$0f068de0$377abad4@neumannpiv>
Message-ID: <3F1CE298.9010006@statistik.uni-dortmund.de>

Dipl. Ing. Kurt Neumann wrote:
> Dear there,
> 
>>I tried today within my R 1.7.0 under Windows XP the command
> 
> 
>>update.packages()
> 
> 
>>and then I answered y in all subsequent questions.
> 
> 
>>>From the software responses I understood that everythin went OK from VR to Rcmdr, but in survival it said

Are you sure you used R-1.7.0 for Windows? Installing package bundles 
like VR failed in that version, AFAIK. So the first recommendation is: 
Update to R-1.7.1.


>>Error in file(filem "r") : unable to open connection
> 
> Should read: Error in file(file, "r") : ....

Maybe an error has occured during the download?
Does the error occur again when you try
  install.packages("survival")
?

My guess: It's not survival but VR which fails. Attention: The 
installation starts after download of *all* packages has finished.
Please read http://cran.r-project.org/bin/windows/contrib/1.7/ReadMe.

Uwe Ligges


>>So I wonder, what I might have done wrong? Could you please help me?
> 
> 
> This may be a Windows specific issue.  Can you pls ask on the r-help
> mailing list?
> 
> Regards,
> -kh
> 
> 
>>Regards
>>                        Kurt Neumann
>



From angel_lul at hotmail.com  Tue Jul 22 11:41:11 2003
From: angel_lul at hotmail.com (Angel)
Date: Tue, 22 Jul 2003 11:41:11 +0200
Subject: [R] rank with ties
Message-ID: <Law11-OE36JrAncMvgR00010df9@hotmail.com>

Hi,
Is there a function like rank but that solves the ties by randomly assigning
a value (doesn't average ranks of ties).
This is what I actually need:
I want to make NA all elements of each column in an array that are ranked in
a position larger that rankmax for each column.
# Say I've got an array b:
b<-cbind(c(1:5,5:1),c(1,12,14,2,5,4:8))
#> b
 #     [,1] [,2]
 #[1,]    1    1
 #[2,]    2   12
 #[3,]    3   14
 #[4,]    4    2
 #[5,]    5    5
 #[6,]    5    4
 #[7,]    4    5
 #[8,]    3    6
 #[9,]    2    7
#[10,]    1    8

rankmax<-5   #  The maximum rank position

# I make the values ranked in a position larger than 5 NAs
b[which(apply(b,2,rank)>rankmax)]<-NA

# > b
#      [,1] [,2]
# [1,]    1    1
# [2,]    2   NA
# [3,]   NA   NA
# [4,]   NA    2
# [5,]   NA    5
# [6,]   NA    4
# [7,]   NA    5
# [8,]   NA   NA
# [9,]    2   NA
#[10,]    1   NA

###  What I want is one of the "3" in the first column not to be made NA.
(similar to the algorithm used in sort)


Thanks in advance for any help,
Angel



From ripley at stats.ox.ac.uk  Tue Jul 22 11:56:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jul 2003 10:56:26 +0100 (BST)
Subject: [R] rank with ties
In-Reply-To: <Law11-OE36JrAncMvgR00010df9@hotmail.com>
Message-ID: <Pine.LNX.4.44.0307221052250.13026-100000@gannet.stats>

sort.list(sort.list(x)) does ranks, with first occurrence winning (which 
is what sort does, BTW).

If you really want ties broken at random you could do something like

sort.list(order(x, runif(length(x)))

On Tue, 22 Jul 2003, Angel wrote:

> Hi,
> Is there a function like rank but that solves the ties by randomly assigning
> a value (doesn't average ranks of ties).
> This is what I actually need:
> I want to make NA all elements of each column in an array that are ranked in
> a position larger that rankmax for each column.
> # Say I've got an array b:
> b<-cbind(c(1:5,5:1),c(1,12,14,2,5,4:8))
> #> b
>  #     [,1] [,2]
>  #[1,]    1    1
>  #[2,]    2   12
>  #[3,]    3   14
>  #[4,]    4    2
>  #[5,]    5    5
>  #[6,]    5    4
>  #[7,]    4    5
>  #[8,]    3    6
>  #[9,]    2    7
> #[10,]    1    8
> 
> rankmax<-5   #  The maximum rank position
> 
> # I make the values ranked in a position larger than 5 NAs
> b[which(apply(b,2,rank)>rankmax)]<-NA
> 
> # > b
> #      [,1] [,2]
> # [1,]    1    1
> # [2,]    2   NA
> # [3,]   NA   NA
> # [4,]   NA    2
> # [5,]   NA    5
> # [6,]   NA    4
> # [7,]   NA    5
> # [8,]   NA   NA
> # [9,]    2   NA
> #[10,]    1   NA
> 
> ###  What I want is one of the "3" in the first column not to be made NA.
> (similar to the algorithm used in sort)
> 
> 
> Thanks in advance for any help,
> Angel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From johannes.fuernkranz at t-online.de  Tue Jul 22 12:10:12 2003
From: johannes.fuernkranz at t-online.de (Johannes Fuernkranz)
Date: Tue, 22 Jul 2003 12:10:12 +0200
Subject: [R] curves with shaded areas?
Message-ID: <3F1D0D84.1090102@ai.univie.ac.at>

Hi,

I want to make a plot with abline where the area below or above the 
curve is shaded. I can't find any documentation on that. Can anybody 
help me with that?

thanks, Juffi



From picklwolfgang at a1.net  Tue Jul 22 12:22:24 2003
From: picklwolfgang at a1.net (picklwolfgang@a1.net)
Date: Tue, 22 Jul 2003 12:22:24 +0200
Subject: [R] barplot
Message-ID: <5384151ade.51ade53841@a1.net>

is this the right mailinglist for my trivial "barplot()" problem?

mfg

wolfgang



From ripley at stats.ox.ac.uk  Tue Jul 22 12:24:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jul 2003 11:24:42 +0100 (BST)
Subject: [R] curves with shaded areas?
In-Reply-To: <3F1D0D84.1090102@ai.univie.ac.at>
Message-ID: <Pine.LNX.4.44.0307221123120.13179-100000@gannet.stats>

See ?polygon, ?rect for how to shade areas.

If you make a plot with abline, surely it has no curves (only straight 
lines)?

On Tue, 22 Jul 2003, Johannes Fuernkranz wrote:

> I want to make a plot with abline where the area below or above the 
> curve is shaded. I can't find any documentation on that. Can anybody 
> help me with that?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From th50 at leicester.ac.uk  Tue Jul 22 12:37:39 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 22 Jul 2003 11:37:39 +0100
Subject: [R] Confidence Band for empirical distribution function
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B67D@saffron.cfs.le.ac.uk>

Dear Leif,

If you look at the definition of ks.test, you'll find the lines

pkstwo <- function(x, tol = 1e-06) {
    if (is.numeric(x)) 
        x <- as.vector(x)
    else stop("Argument x must be numeric")
    p <- rep(0, length(x))
    p[is.na(x)] <- NA
    IND <- which(!is.na(x) & (x > 0))
    if (length(IND) > 0) {
        p[IND] <- .C("pkstwo", as.integer(length(x)), p = as.double(x[IND]), 
            as.double(tol), PACKAGE = "ctest")$p
    }
    return(p)
}

which calls C code to calculate the p-values given the test statistic.
You'll find explanations on what this function does in the original C file
src/library/ctest/src/ks.c

I haven't tried that but I assume that you could use it to calculate p-values
given the test-statistics yourself.

Please also note that ks.test() returns the p-value as well.

If you need quantiles, I assume you need to invert the cdf yourself,
e.g. using uniroot().

HTH

Thomas
      
---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Leif.Boysen [mailto:boysen at math.uni-goettingen.de]
> Sent: 21 July 2003 14:42
> To: r-help at stat.math.ethz.ch
> Subject: [R] Confidence Band for empirical distribution function
> 
> 
> Hi,
> 
> I was trying to draw an empirical distribution function with uniform
> confidence bands. So I tried to find a way to calculate values of the
> Kolmogorov-Smirnov Distribution but failed.
> I guess it must be hidden somewhere (since the ks-test is 
> implemented),
> but I was unable to find it. 
> 
> Is there any way to do this?
> 
> Thanks
> 
> Leif Boysen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From hb at maths.lth.se  Tue Jul 22 12:44:37 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 22 Jul 2003 12:44:37 +0200
Subject: [R] curves with shaded areas?
In-Reply-To: <3F1D0D84.1090102@ai.univie.ac.at>
Message-ID: <001a01c3503e$3f11a020$e502eb82@maths.lth.se>

polygon() together with par("usr") would be your friend here. As I do
not know if you want horisontal/vertical lines or any line, here is a
general example:

# Generate the data
x <- seq(from=1, to=3.5*pi, length=100)
y <- sin(x)

# Creates an empty plot of right size
plot(x,y, type="n")  

# Get plotting region
x0 <- par("usr")[1]; x1 <- par("usr")[2];
y0 <- par("usr")[3]; y1 <- par("usr")[4];

last <- length(y)

# Create the "upper" polygon
polygon(c(x0,x0,x,x1,x1), c(y1,y[1],y,y[last],y1), col="#ffcccc",
border=NA)
# Create the "lower" polygon
polygon(c(x0,x0,x,x1,x1), c(y0,y[1],y,y[last],y0), col="#ccccff",
border=NA)

# Plot the data on top
points(x,y)

As you see, as there are 4% margins by default, you have to specify what
you want to do at the margins. Depending on if you want regions defined
by abline(h)/abline(v) or by abline(a,b) you solution will be somewhat
different. 

Cheers

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Johannes Fuernkranz
> Sent: den 22 juli 2003 12:10
> To: r-help at stat.math.ethz.ch
> Subject: [R] curves with shaded areas?
> 
> 
> Hi,
> 
> I want to make a plot with abline where the area below or above the 
> curve is shaded. I can't find any documentation on that. Can anybody 
> help me with that?
> 
> thanks, Juffi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>



From cdeclercq at nordnet.fr  Tue Jul 22 14:06:38 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Tue, 22 Jul 2003 13:06:38 +0100
Subject: [R] curves with shaded areas?
In-Reply-To: <3F1D0D84.1090102@ai.univie.ac.at>
Message-ID: <NGBBKLJCOLPAFMJIEMHCEEEHCJAA.cdeclercq@nordnet.fr>

> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Johannes
> Fuernkranz
[...]
> I want to make a plot with abline where the area below or above the
> curve is shaded. I can't find any documentation on that. Can anybody
> help me with that?
[...]

You have to play with par("usr").

See the toy example below, perhaps a little naive, but it seems to do what
you want.

##
tmp<-rnorm(100)
mydf<-data.frame(x=tmp, y=0.5*tmp+rnorm(100))
plot(y~x, mydf)
myreg<-lm(y~x, mydf)
abline(myreg)
x1<-par("usr")[1:2]
y1<-par("usr")[3]
myxy<-rbind(
  cbind(x1, predict(myreg, data.frame(x=x1))),
  cbind(rev(x1), rep(y1,2))
)
polygon(myxy, col="grey")
##

I hope it helps.

Christophe
--
Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org



From johannes.fuernkranz at t-online.de  Tue Jul 22 13:00:00 2003
From: johannes.fuernkranz at t-online.de (Johannes Fuernkranz)
Date: Tue, 22 Jul 2003 13:00:00 +0200
Subject: [R] curves with shaded areas?
In-Reply-To: <Pine.LNX.4.44.0307221123120.13179-100000@gannet.stats>
References: <Pine.LNX.4.44.0307221123120.13179-100000@gannet.stats>
Message-ID: <3F1D1930.2050600@ai.univie.ac.at>

Prof Brian Ripley wrote:

> See ?polygon, ?rect for how to shade areas.

Thank you.
This covers most but not all of my problems. :-)

> If you make a plot with abline, surely it has no curves (only straight 
> lines)?

Right, sorry. I was a bit over-simplifying my problem, because I thought 
this can be done with a general plot parameter (something like lty), and 
most of the graphs I am plotting are actually straight regression lines.

The tougher cases are those wher I want to shade the area inside / 
outside a specific contour line (produced with 'contour'). Can this be 
done as well?

Thanks for your help,

						Juffi



From ripley at stats.ox.ac.uk  Tue Jul 22 13:18:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jul 2003 12:18:27 +0100 (BST)
Subject: [R] curves with shaded areas?
In-Reply-To: <3F1D1930.2050600@ai.univie.ac.at>
Message-ID: <Pine.LNX.4.44.0307221213100.15757-100000@gannet.stats>

On Tue, 22 Jul 2003, Johannes Fuernkranz wrote:

> Prof Brian Ripley wrote:
> 
> > See ?polygon, ?rect for how to shade areas.
> 
> Thank you.
> This covers most but not all of my problems. :-)
> 
> > If you make a plot with abline, surely it has no curves (only straight 
> > lines)?
> 
> Right, sorry. I was a bit over-simplifying my problem, because I thought 
> this can be done with a general plot parameter (something like lty), and 
> most of the graphs I am plotting are actually straight regression lines.
> 
> The tougher cases are those wher I want to shade the area inside / 
> outside a specific contour line (produced with 'contour'). Can this be 
> done as well?

?filled.contour with a single contour may help.  Otherwise I think you
need the clines package (from Paul Murrell,
http://www.stat.auckland.ac.nz/~paul/) and use polygon on its output.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Jul 22 13:57:45 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jul 2003 07:57:45 -0400
Subject: [R] R and C++ compared with only C++
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8E6@usrymx25.merck.com>

I must say that I agree with Andrew.  I recently did just that:  took an R
package with C code and turn it into a stand-alone C program.  Granted I'm
far from being a C expert, but not only that the code didn't run any faster
(because all the heavy computations were already done in C anyway), but I
had to spend lots and lots of time debugging the new code.  If I have the
choice, I wouldn't do it.  (No, I didn't have that choice...)

Cheers,
Andy

> -----Original Message-----
> From: Andrew C. Ward [mailto:s195404 at student.uq.edu.au] 
> Sent: Monday, July 21, 2003 10:20 PM
> To: fjmolina at lbl.gov
> Cc: r-help
> Subject: Re: [R] R and C++ compared with only C++
> 
> 
> As always, it depends on your priorities and motivation. If 
> that extra 10% is important and if you can be bothered 
> writing the C++, then putting the whole thing into C++ may be 
> worth it. If not then you could let R deal with the input and output.
> 
> Regards,
> 
> Andrew C. Ward
> 
> CAPE Centre
> Department of Chemical Engineering
> The University of Queensland
> Brisbane Qld 4072 Australia
> andreww at cheque.uq.edu.au
> 
> 
> Quoting Francisco J Molina <fjmolina at lbl.gov>:
> 
> > 
> > My computer is a pentium 4 running at 2.4 GHz.
> > My R is 1.7.1
> > 
> > I have written a program in R that calls C++. The program 
> spends most 
> > of the time in C++ ( > 90% ). R basically deals with output and
> > input.
> > How slower can this be compared with the program I would
> > get from rewriting
> > everything in C++?
> > 
> > Thank you.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From wettenhall at wehi.edu.au  Tue Jul 22 14:08:55 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 22 Jul 2003 22:08:55 +1000 (EST)
Subject: [R] Calling R from C
In-Reply-To: <200307221005.h6MA4weF014105@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0307222154010.17110-100000@unix28.alpha.wehi.edu.au>


Duncan Temple Lang's article "In Search of C/C++ & 
Fortran Routines" in R News Vol 1/3, September 2001 is 
definitely worth reading (even though it's mainly about calling 
C from R):
http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf

and you should have a look at "Writing R Extensions" :
http://cran.r-project.org/doc/manuals/R-exts.pdf

James



From jrogers at cantatapharm.com  Tue Jul 22 14:41:15 2003
From: jrogers at cantatapharm.com (Jim Rogers)
Date: Tue, 22 Jul 2003 08:41:15 -0400
Subject: [R] generate a series of fucntion
Message-ID: <99A12772DCDEEB458B996332957B0D53011808@mercury.cantatapharm.com>

You need to be careful if you take the approach below. Try it:

> g1 <- list(function(x) log(x), function(x) log(x+1), function(x) x,
function(x) x^2)
> g2 <- lapply(g1, function(g) {
+   function(x,t) exp(-t[1]-t[2]*g(x)-t[3]*g(1+x))
+ }
+              )
> lapply(g2, function(f) get("g", environment(f)))
[[1]]
function(x) x^2

[[2]]
function(x) x^2

[[3]]
function(x) x^2

[[4]]
function(x) x^2

Because of lazy evaluation + lexical scoping, "g" doesn't get evaluated
where you would naturally think.

To avoid this, you can do:

> g2 <- lapply(g1, function(g) {
+   g  # or, for readability, force(g), as of R 1.7.0 
+   function(x,t) exp(-t[1]-t[2]*g(x)-t[3]*g(1+x))
+ }
+              )
> lapply(g2, function(f) get("g", environment(f)))
[[1]]
function(x) log(x)

[[2]]
function(x) log(x+1)

[[3]]
function(x) x

[[4]]
function(x) x^2

I was first confused by this a few months back, and Luke Tierney, Robert
Gentleman, and Thomas Lumley were kind enough to explain it to me. If
you want to find that thread, do an R site search for "lexical scoping
Jim Rogers".

Cheers,
Jim

> Message: 43
> Date: Mon, 21 Jul 2003 16:15:35 -0400
> From: "Liaw, Andy" <andy_liaw at merck.com>
> Subject: RE: [R] generate a series of fucntion
> To: "'ximing wu'" <xiwu at uoguelph.ca>
> Cc: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
> Message-ID:
> 	<3A822319EB35174CA3714066D590DCD50205C8E3 at usrymx25.merck.com>
> Content-Type: text/plain
> 
> Does this do sort of what you want?
> 
> > g1 <- list(function(x) log(x), function(x) log(x+1), function(x) x,
> function(x) x^2)
> > g1
> [[1]]
> function(x) log(x)
> 
> [[2]]
> function(x) log(x+1)
> 
> [[3]]
> function(x) x
> 
> [[4]]
> function(x) x^2
> 
> > lapply(g1, function(g) {function(x,t)
exp(-t[1]-t2*g(x)-t[3]*g(1+x))})
> [[1]]
> function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
> <environment: 022BCBD8>
> 
> [[2]]
> function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
> <environment: 022BBD08>
> 
> [[3]]
> function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
> <environment: 022BBDCC>
> 
> [[4]]
> function(x,t) exp(-t[1]-t2*g(x)-t[3]*g(1+x))
> <environment: 022BBE90>
> 
> You did not say what g1 and g2 actually are, but I suppose the above
should
> give you enough hint.
> 
> Andy
> 
> > -----Original Message-----
> > From: ximing wu [mailto:xiwu at uoguelph.ca] 
> > Sent: Monday, July 21, 2003 3:54 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] generate a series of fucntion
> > 
> > 
> > Hi there,
> > 
> > I want to generate a large amount of functions,
> > 
> > say f=function(x,t) exp(-t[1]-t[2]*g_1(x)-t[3]*g_2(1+x))
> > 
> > where g_1(x) and g_2(x) are from a long list of moments, such 
> > as x, x^2, 
> > log(x), log(1+x) .. and so on.
> > 
> > Any suggestions on how to do this efficiently?
> > 
> > thanks a lot.
> > 
> > x.w
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 


James A. Rogers, Ph.D. <rogers at cantatapharm.com>
Statistical Scientist
Cantata Pharmaceuticals
300 Technology Square, 5th floor
Cambridge, MA  02139
617.225.9009 x312
Fax 617.225.9010



From jfox at mcmaster.ca  Tue Jul 22 15:36:26 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 22 Jul 2003 09:36:26 -0400
Subject: Rcmdr problems (was:Re: [R] Minor nuisance with rw1071)
In-Reply-To: <3F1C7199.1487.1EC3128@localhost>
References: <5.1.0.14.2.20030718094312.020eb6f0@127.0.0.1>
	<Pine.LNX.4.44.0307180826290.6655-100000@gannet.stats>
Message-ID: <5.1.0.14.2.20030722092316.01f89208@127.0.0.1>

Dear Kjetil,

At 11:04 PM 7/21/2003 -0400, kjetil brinchmann halvorsen wrote:
>On 18 Jul 2003 at 9:54, John Fox wrote:
>
>Sorry for being so late coming back on this, I was travelling the
>weekend. I tried to remove as much as possible from the workspace
>(.RData file), but nothing created by Rcmdr. This did'nt change
>anything.

Maybe I'm missing something here, but why not just remove the .RData file 
(or rename it temporarily, if it contains something important)?

>Then I downloaded the latest Rcmdr (0.9-0, development version) from
>John Fox page, and compiled it myself for windows. The focus problem
>is unchanged, but now the problems with attaching a dataset has gone,
>the warnings about _ being deprecated has gone, happily.

I'm glad that you finally got the package to work, but it's curious that 
you had to compile it yourself. Many people have used the Windows binary 
package from my web site without this difficulty, and the older Windows 
binary on CRAN wasn't compiled by me.

>Also, the problem with printing the .logFont variable seems to have
>gone. In all but one trials it did'nt bomb Rgui.

If Rcmdr brings down Rgui in a reproducible way, I'd be interested to learn 
about it. I haven't experienced this behaviour myself nor heard about it 
before.

I recall that you mentioned that you've been running Rcmdr in a separate 
directory. Do you observe the same problem when you load the package after 
starting R in another place? Is there anything unusual about your system 
that we haven't discussed? For example, are you using the Tcl/Tk 
distribution that comes with the R installer? I'm still at a loss to 
understand why you've been experiencing these difficulties.

Regards,
  John

>Kjetil
>
> > Dear Brian,
> >
> > At 08:32 AM 7/18/2003 +0100, Prof Brian Ripley wrote:
> > >I believe it's a side effect of starting up tcltk.  That grabs focus for
> > >its invisible window: there's a kludgy workaround in the tcltk startup
> > >code, but that it looks like that is not working during the startup
> > >(which here seems to be something in .RData).
> >
> > That there's something in .RData causing the problem is a good guess. To
> > provide a bit more context: Kjetil reports to me that he's unable to use
> > the Rcmdr GUI without generating "Warning: The use of _ is soon to be
> > removed: you will be warned repeatedly" and parse errors. Of course, I
> > don't use underscores in any of my code.
> >
> > The only other similar report that I received was from Martin Maechler
> > using Rcmdr under Emacs/ESS on a Linux machine. That problem went away 
> when
> > he ran R from a terminal rather than from Emacs, but perhaps that was
> > coincidence. (Rcmdr works for me under XEmacs/ESS under Windows, though
> > it's not a natural marriage.)
> >
> > As a more general matter, I've encountered but been able to work around a
> > variety of focus problems using the tcltk package.
> >
> > Regards,
> >   John
> >
> >
> > -----------------------------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario, Canada L8S 4M4
> > email: jfox at mcmaster.ca
> > phone: 905-525-9140x23604
> > web: www.socsci.mcmaster.ca/jfox
> > -----------------------------------------------------
> >

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From johannes.fuernkranz at t-online.de  Tue Jul 22 15:41:29 2003
From: johannes.fuernkranz at t-online.de (Johannes Fuernkranz)
Date: Tue, 22 Jul 2003 15:41:29 +0200
Subject: [R] curves with shaded areas?
In-Reply-To: <Pine.LNX.4.44.0307221213100.15757-100000@gannet.stats>
References: <Pine.LNX.4.44.0307221213100.15757-100000@gannet.stats>
Message-ID: <3F1D3F09.40902@ai.univie.ac.at>

Prof Brian Ripley wrote:
> 
> ?filled.contour with a single contour may help.  Otherwise I think you
> need the clines package (from Paul Murrell,
> http://www.stat.auckland.ac.nz/~paul/) and use polygon on its output.

filled.contour is just what I need for the remaining cases.

Many thanks to all who replied!

						Juffi



From lsilva at fc.up.pt  Tue Jul 22 15:48:33 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Tue, 22 Jul 2003 14:48:33 +0100
Subject: [R] variable names
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEAAA@MAIL.fc.up.pt>

Dear helpers
 
I want to use rpart several times in a loop to build a classification tree. My problem is that rpart needs a formula as argument and for that the variables need to have names and this doesn't happen in my case. Every iteration in the loop has a different dataset with several variables (ex. 38 or more) and so I can't type the names by hand every time. Is there any function that generates names for variables in a dataframe. If so, how can I use then the argument
 
rpart(classlabels~. ,.....)
 
thanks



From hdoran at nasdc.org  Tue Jul 22 15:50:20 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 22 Jul 2003 09:50:20 -0400
Subject: [R] Conditional Statements for Graphing
Message-ID: <66578BFC0BA55348B5907A0F798EE930139FF2@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030722/c8b7892d/attachment.pl

From hothorn at ci.tuwien.ac.at  Tue Jul 22 15:57:31 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Tue, 22 Jul 2003 15:57:31 +0200 (CEST)
Subject: [R] variable names
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAA@MAIL.fc.up.pt>
Message-ID: <Pine.LNX.3.96.1030722155343.16356B-100000@thorin.ci.tuwien.ac.at>


On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:

> Dear helpers
>  
> I want to use rpart several times in a loop to build a classification tree. My problem is that rpart needs a formula as argument and for that the variables need to have names and this doesn't happen in my case. Every iteration in the loop has a different dataset with several variables (ex. 38 or more) and so I can't type the names by hand every time. Is there any function that generates names for variables in a dataframe. If so, how can I use then the argument
>  

If your data is organised in a data.frame, (dummy) variable names are
available by default:

R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
R> mydata
          X1          X2         X3         X4          X5
1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
R> rpart(X1 ~ ., data = mydata)
n= 5 

node), split, n, deviance, yval
      * denotes terminal node

1) root 5 7.463698 -0.3589306 *

best,

Torsten

> rpart(classlabels~. ,.....)
>  
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From th50 at leicester.ac.uk  Tue Jul 22 16:07:10 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 22 Jul 2003 15:07:10 +0100
Subject: [R] variable names
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B67E@saffron.cfs.le.ac.uk>

Dear Luis,

You might want to have a look at

"Bill Venables. Programmer's niche. R News, 2(2):24-26, June 2002"
which you can find at http://cran.r-project.org/doc/Rnews/
or look into the manual "R Language Definition", chapter
"Computing on the language".

Assuming that in your case the variable to be classified is called
classLabels, and the names of your datasets are in a vector called
dataNames, you could use something like

lapply(dataNames,function(theName){
  eval(parse(text = paste("rpart(classLabels~., data =", theName, ")"))) 
})

which will return a list of results. This is achieved by generating the
command line you would type as a string using paste, which is then parsed
and explicitly evaluated.

See the corresponding help pages for more details.

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Luis Miguel Almeida da Silva [mailto:lsilva at fc.up.pt]
> Sent: 22 July 2003 14:49
> To: r-help at stat.math.ethz.ch
> Subject: [R] variable names
> 
> 
> Dear helpers
>  
> I want to use rpart several times in a loop to build a 
> classification tree. My problem is that rpart needs a formula 
> as argument and for that the variables need to have names and 
> this doesn't happen in my case. Every iteration in the loop 
> has a different dataset with several variables (ex. 38 or 
> more) and so I can't type the names by hand every time. Is 
> there any function that generates names for variables in a 
> dataframe. If so, how can I use then the argument
>  
> rpart(classlabels~. ,.....)
>  
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From lsilva at fc.up.pt  Tue Jul 22 16:10:47 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Tue, 22 Jul 2003 15:10:47 +0100
Subject: [R] variable names
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEAAB@MAIL.fc.up.pt>

I didn't noticed that fact. I've already found a way to do that
 
x <- 1:40
colnames(df.treino) <- paste("Ncp",x,sep=".")
 
and this generates names that I can relate with the variables. Thanks anyway
 
The problem is that I use rpart in a loop and the class labels are in the last column. For the above example I would "type"
 
rpart(Ncp.40~.,data=df.treino)
 
But in the next step of the loop I can have only 35 variables and the class labels would be at the Ncp.36. So I have to refresh the formula in rpart... and that is my problem

	-----Original Message----- 
	From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at] 
	Sent: Tue 22/07/2003 14:57 
	To: Luis Miguel Almeida da Silva 
	Cc: r-help at stat.math.ethz.ch 
	Subject: Re: [R] variable names
	
	


	On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
	
	> Dear helpers
	> 
	> I want to use rpart several times in a loop to build a classification tree. My problem is that rpart needs a formula as argument and for that the variables need to have names and this doesn't happen in my case. Every iteration in the loop has a different dataset with several variables (ex. 38 or more) and so I can't type the names by hand every time. Is there any function that generates names for variables in a dataframe. If so, how can I use then the argument
	> 
	
	If your data is organised in a data.frame, (dummy) variable names are
	available by default:
	
	R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
	R> mydata
	          X1          X2         X3         X4          X5
	1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
	2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
	3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
	4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
	5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
	R> rpart(X1 ~ ., data = mydata)
	n= 5
	
	node), split, n, deviance, yval
	      * denotes terminal node
	
	1) root 5 7.463698 -0.3589306 *
	
	best,
	
	Torsten
	
	> rpart(classlabels~. ,.....)
	> 
	> thanks
	>
	> ______________________________________________
	> R-help at stat.math.ethz.ch mailing list
	> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
	>
	>



From hothorn at ci.tuwien.ac.at  Tue Jul 22 16:16:59 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Tue, 22 Jul 2003 16:16:59 +0200 (CEST)
Subject: [R] variable names
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAB@MAIL.fc.up.pt>
Message-ID: <Pine.LNX.3.96.1030722161243.16356C-100000@thorin.ci.tuwien.ac.at>

On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:

> I didn't noticed that fact. I've already found a way to do that
>  
> x <- 1:40
> colnames(df.treino) <- paste("Ncp",x,sep=".")
>  
> and this generates names that I can relate with the variables. Thanks anyway
>  
> The problem is that I use rpart in a loop and the class labels are in the last column. For the above example I would "type"
>  
> rpart(Ncp.40~.,data=df.treino)
>  
> But in the next step of the loop I can have only 35 variables and the class labels would be at the Ncp.36. So I have to refresh the formula in rpart... and that is my problem
> 

R> df.treino <- data.frame(matrix(rnorm(25), ncol=5))
R> thisformula <- as.formula(paste(colnames(df.treino)[ncol(df.treino)],
"~ ."))
R> thisformula
X5 ~ .
R> rpart(thisformula, data = df.treino)
n= 5 

node), split, n, deviance, yval
      * denotes terminal node

1) root 5 3.032904 -0.3392065 *

Torsten


> 	-----Original Message----- 
> 	From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at] 
> 	Sent: Tue 22/07/2003 14:57 
> 	To: Luis Miguel Almeida da Silva 
> 	Cc: r-help at stat.math.ethz.ch 
> 	Subject: Re: [R] variable names
> 	
> 	
> 
> 
> 	On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
> 	
> 	> Dear helpers
> 	> 
> 	> I want to use rpart several times in a loop to build a classification tree. My problem is that rpart needs a formula as argument and for that the variables need to have names and this doesn't happen in my case. Every iteration in the loop has a different dataset with several variables (ex. 38 or more) and so I can't type the names by hand every time. Is there any function that generates names for variables in a dataframe. If so, how can I use then the argument
> 	> 
> 	
> 	If your data is organised in a data.frame, (dummy) variable names are
> 	available by default:
> 	
> 	R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
> 	R> mydata
> 	          X1          X2         X3         X4          X5
> 	1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
> 	2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
> 	3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
> 	4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
> 	5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
> 	R> rpart(X1 ~ ., data = mydata)
> 	n= 5
> 	
> 	node), split, n, deviance, yval
> 	      * denotes terminal node
> 	
> 	1) root 5 7.463698 -0.3589306 *
> 	
> 	best,
> 	
> 	Torsten
> 	
> 	> rpart(classlabels~. ,.....)
> 	> 
> 	> thanks
> 	>
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 	>
> 	>
> 	
> 	
> 
>



From andy_liaw at merck.com  Tue Jul 22 16:20:36 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jul 2003 10:20:36 -0400
Subject: [R] variable names
Message-ID: <3A822319EB35174CA3714066D590DCD50205C8EB@usrymx25.merck.com>

Read Bill Venables' column in R News issue 2/2, page 24.

Andy

> From: Luis Miguel Almeida da Silva [mailto:lsilva at fc.up.pt] 
> 
> I didn't noticed that fact. I've already found a way to do that
>  
> x <- 1:40
> colnames(df.treino) <- paste("Ncp",x,sep=".")
>  
> and this generates names that I can relate with the 
> variables. Thanks anyway
>  
> The problem is that I use rpart in a loop and the class 
> labels are in the last column. For the above example I would "type"
>  
> rpart(Ncp.40~.,data=df.treino)
>  
> But in the next step of the loop I can have only 35 variables 
> and the class labels would be at the Ncp.36. So I have to 
> refresh the formula in rpart... and that is my problem
> 
> 	-----Original Message----- 
> 	From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at] 
> 	Sent: Tue 22/07/2003 14:57 
> 	To: Luis Miguel Almeida da Silva 
> 	Cc: r-help at stat.math.ethz.ch 
> 	Subject: Re: [R] variable names
> 	
> 	
> 
> 
> 	On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
> 	
> 	> Dear helpers
> 	> 
> 	> I want to use rpart several times in a loop to build 
> a classification tree. My problem is that rpart needs a 
> formula as argument and for that the variables need to have 
> names and this doesn't happen in my case. Every iteration in 
> the loop has a different dataset with several variables (ex. 
> 38 or more) and so I can't type the names by hand every time. 
> Is there any function that generates names for variables in a 
> dataframe. If so, how can I use then the argument
> 	> 
> 	
> 	If your data is organised in a data.frame, (dummy) 
> variable names are
> 	available by default:
> 	
> 	R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
> 	R> mydata
> 	          X1          X2         X3         X4          X5
> 	1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
> 	2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
> 	3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
> 	4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
> 	5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
> 	R> rpart(X1 ~ ., data = mydata)
> 	n= 5
> 	
> 	node), split, n, deviance, yval
> 	      * denotes terminal node
> 	
> 	1) root 5 7.463698 -0.3589306 *
> 	
> 	best,
> 	
> 	Torsten
> 	
> 	> rpart(classlabels~. ,.....)
> 	> 
> 	> thanks
> 	>
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 	>
> 	>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From th50 at leicester.ac.uk  Tue Jul 22 16:24:33 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 22 Jul 2003 15:24:33 +0100
Subject: [R] variable names
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486015E46FB@saffron.cfs.le.ac.uk>

Dear Luis,

you could change my previous reply to something like

lapply(dataNames,function(theName){
  eval(parse(text = paste("rpart(X", ncol(get(theName)), "~., data =", theName, ")"))) 
})

making use of get(), ncol() and Torsten's suggestion.

HTH

Thomas

> -----Original Message-----
> From: Luis Miguel Almeida da Silva [mailto:lsilva at fc.up.pt]
> Sent: 22 July 2003 15:11
> To: Torsten Hothorn
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] variable names
> 
> 
> I didn't noticed that fact. I've already found a way to do that
>  
> x <- 1:40
> colnames(df.treino) <- paste("Ncp",x,sep=".")
>  
> and this generates names that I can relate with the 
> variables. Thanks anyway
>  
> The problem is that I use rpart in a loop and the class 
> labels are in the last column. For the above example I would "type"
>  
> rpart(Ncp.40~.,data=df.treino)
>  
> But in the next step of the loop I can have only 35 variables 
> and the class labels would be at the Ncp.36. So I have to 
> refresh the formula in rpart... and that is my problem
> 
> 	-----Original Message----- 
> 	From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at] 
> 	Sent: Tue 22/07/2003 14:57 
> 	To: Luis Miguel Almeida da Silva 
> 	Cc: r-help at stat.math.ethz.ch 
> 	Subject: Re: [R] variable names
> 	
> 	
> 
> 
> 	On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
> 	
> 	> Dear helpers
> 	> 
> 	> I want to use rpart several times in a loop to build 
> a classification tree. My problem is that rpart needs a 
> formula as argument and for that the variables need to have 
> names and this doesn't happen in my case. Every iteration in 
> the loop has a different dataset with several variables (ex. 
> 38 or more) and so I can't type the names by hand every time. 
> Is there any function that generates names for variables in a 
> dataframe. If so, how can I use then the argument
> 	> 
> 	
> 	If your data is organised in a data.frame, (dummy) 
> variable names are
> 	available by default:
> 	
> 	R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
> 	R> mydata
> 	          X1          X2         X3         X4          X5
> 	1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
> 	2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
> 	3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
> 	4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
> 	5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
> 	R> rpart(X1 ~ ., data = mydata)
> 	n= 5
> 	
> 	node), split, n, deviance, yval
> 	      * denotes terminal node
> 	
> 	1) root 5 7.463698 -0.3589306 *
> 	
> 	best,
> 	
> 	Torsten
> 	
> 	> rpart(classlabels~. ,.....)
> 	> 
> 	> thanks
> 	>
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 	>
> 	>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From lsilva at fc.up.pt  Tue Jul 22 16:40:21 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Tue, 22 Jul 2003 15:40:21 +0100
Subject: [R] variable names
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEAAC@MAIL.fc.up.pt>

It worked! thank you

	-----Original Message----- 
	From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at] 
	Sent: Tue 22/07/2003 15:16 
	To: Luis Miguel Almeida da Silva 
	Cc: r-help at stat.math.ethz.ch 
	Subject: RE: [R] variable names
	
	

	On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
	
	> I didn't noticed that fact. I've already found a way to do that
	> 
	> x <- 1:40
	> colnames(df.treino) <- paste("Ncp",x,sep=".")
	> 
	> and this generates names that I can relate with the variables. Thanks anyway
	> 
	> The problem is that I use rpart in a loop and the class labels are in the last column. For the above example I would "type"
	> 
	> rpart(Ncp.40~.,data=df.treino)
	> 
	> But in the next step of the loop I can have only 35 variables and the class labels would be at the Ncp.36. So I have to refresh the formula in rpart... and that is my problem
	>
	
	R> df.treino <- data.frame(matrix(rnorm(25), ncol=5))
	R> thisformula <- as.formula(paste(colnames(df.treino)[ncol(df.treino)],
	"~ ."))
	R> thisformula
	X5 ~ .
	R> rpart(thisformula, data = df.treino)
	n= 5
	
	node), split, n, deviance, yval
	      * denotes terminal node
	
	1) root 5 3.032904 -0.3392065 *
	
	Torsten
	
	
	>       -----Original Message-----
	>       From: Torsten Hothorn [mailto:hothorn at ci.tuwien.ac.at]
	>       Sent: Tue 22/07/2003 14:57
	>       To: Luis Miguel Almeida da Silva
	>       Cc: r-help at stat.math.ethz.ch
	>       Subject: Re: [R] variable names
	>      
	>      
	>
	>
	>       On Tue, 22 Jul 2003, Luis Miguel Almeida da Silva wrote:
	>      
	>       > Dear helpers
	>       >
	>       > I want to use rpart several times in a loop to build a classification tree. My problem is that rpart needs a formula as argument and for that the variables need to have names and this doesn't happen in my case. Every iteration in the loop has a different dataset with several variables (ex. 38 or more) and so I can't type the names by hand every time. Is there any function that generates names for variables in a dataframe. If so, how can I use then the argument
	>       >
	>      
	>       If your data is organised in a data.frame, (dummy) variable names are
	>       available by default:
	>      
	>       R> mydata <- data.frame(matrix(rnorm(25), ncol=5))
	>       R> mydata
	>                 X1          X2         X3         X4          X5
	>       1  1.3806313 -0.41827136  0.9591628 -1.3351038  0.02746110
	>       2  0.5114590 -1.34111439 -0.9617552 -0.8367088 -0.06913021
	>       3 -1.7508089 -0.49387076 -1.7597395  2.3899490 -0.15209650
	>       4 -1.6753809 -1.28381808 -1.0424903  0.1002998  0.27784949
	>       5 -0.2605535 -0.09035652 -2.5786418  1.0483400 -0.70445615
	>       R> rpart(X1 ~ ., data = mydata)
	>       n= 5
	>      
	>       node), split, n, deviance, yval
	>             * denotes terminal node
	>      
	>       1) root 5 7.463698 -0.3589306 *
	>      
	>       best,
	>      
	>       Torsten
	>      
	>       > rpart(classlabels~. ,.....)
	>       >
	>       > thanks
	>       >
	>       > ______________________________________________
	>       > R-help at stat.math.ethz.ch mailing list
	>       > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
	>       >
	>       >
	>      
	>      
	>
	>



From bates at stat.wisc.edu  Tue Jul 22 17:01:39 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 Jul 2003 15:01:39 -0000
Subject: [R] Conditional Statements for Graphing
In-Reply-To: <66578BFC0BA55348B5907A0F798EE930139FF2@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE930139FF2@ernesto.NASDC.ORG>
Message-ID: <6rr84iwqov.fsf@bates4.stat.wisc.edu>

"Harold Doran" <hdoran at nasdc.org> writes:

> Dear List
>  
> I have math test scores for male and female students where gender is
> a dummy code (female =1). I also have a variety of other demographic
> variables.
>  
> However to begin, I want to create a very simple stripchart where
> female math scores are a blue circle and male scores are a red
> triangle.
>  
> I am having difficulty using conditional statements to accomplish this. 

The basic plotting commands in R (plot, lines, points) take a col
argument that can be a vector.  Another argument that can be
vectorized is pch which selects the plot character in plot and points.

For example

> data(mtcars)
> str(mtcars)
`data.frame':	32 obs. of  11 variables:
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
 $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
 $ disp: num  160 160 108 258 360 ...
 $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
 $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
 $ qsec: num  16.5 17.0 18.6 19.4 17.0 ...
 $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
 $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
> unique(mtcars$cyl)
[1] 6 4 8
> plot(mpg ~ wt, data = mtcars, col = ifelse(cyl == 4, 'blue', ifelse(cyl == 6, 'green', 'red')))

If all else fails there is a trick that can be used to build up plots
in stages.  Begin by plotting all the data with type = 'n'.  This
suppresses both points and lines but does define the axes so they will
accomodate all the data.  Then add points or lines for selected
groups with pch and col set to scalars.  The same plot as above could
be generated with

> plot(mpg ~ wt, data = mtcars, type = 'n')
> points(mpg ~ wt, data = mtcars, subset = cyl == 4, col = 'blue')
> points(mpg ~ wt, data = mtcars, subset = cyl == 6, col = 'green')
> points(mpg ~ wt, data = mtcars, subset = cyl == 8, col = 'red')



From Oldradio69 at aol.com  Tue Jul 22 17:11:17 2003
From: Oldradio69 at aol.com (Oldradio69@aol.com)
Date: Tue, 22 Jul 2003 11:11:17 -0400
Subject: [R] greek in main title
Message-ID: <0F0F86E6.0A14F10F.0C4E5449@aol.com>

Hello,

I have written a function that demonstrates the CLT by
generating samples following the exponential distribution,
calculating the means, plotting the histogram, and drawing
the limiting normal curve as an overlay.  I have the title
of each histogram state the sample size and rate (1/theta)
for the exponential (the output is actually 4 histograms),
but I can't get the greek letter theta to appear in the
heading.  Is this possible?  I can get nontext for the xlab 
and ylab pretty easily.

Thanks, Jason

Below is my code.  Currently it just writes out "theta."

function (theta) 
{
  layout(matrix(c(1:4),2,2,byrow=TRUE))
  par(bg = "cornsilk")
  n <- c(1,10,25,50)
  for(i in 1:4)  {
     xbar <- rep(0, 1000)
     for (j in 1:1000)
        xbar[j] <- mean(rexp(n[i], rate = 1/theta))
     heading <- paste("Exp w/ theta =", theta, ", n = ", n[i])
     hist(xbar, prob = TRUE, breaks = "FD", main = title(heading, font.main = 6), 
          col = "lightgray", xlab = expression(bar(x)))
     xbar <- sort(xbar)
     points(xbar, dnorm(xbar, mean = theta, sd = theta/sqrt(n[i])), 
        type = "l", col = 2)
    }
}



From abunn at montana.edu  Tue Jul 22 17:41:56 2003
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 22 Jul 2003 09:41:56 -0600
Subject: [R] Making a group membership matrix
Message-ID: <000001c35067$da8a0dc0$78f05a99@msu.montana.edu>

Hi Helpers:

I have a factor object that has 314k entries of 39 land cover types.
(This object can be coerced to characters neatly should that be easier
to work with.) 
> length(foo)
[1] 314482
> foo[1:10]
 [1] Montane Chaparral Barren            Red Fir           Red Fir

 [5] Red Fir           Red Fir           Red Fir           Red Fir

 [9] Red Fir           Red Fir          
39 Levels: Alpine-Dwarf Shrub Annual Grassland Aspen Barren ... White
Fir
> summary(foo)
         Alpine-Dwarf Shrub            Annual Grassland
Aspen 
                       7402                           0
582 
                     Barren                 Bitterbrush      Blue
Oak-Foothill Pine 
                      69111                           9
0 
          Blue Oak Woodland  Chamise-Redshank Chaparral    Closed-Cone
Pine-Cypress 
                          0                           0
0 
                   Cropland                Desert Scrub
Douglas-Fir 
                          0                           0
0 
              Eastside Pine Freshwater Emergent Wetland
Jeffrey Pine 
                          0                           0
11342 
                Joshua Tree                     Juniper
Lacustrine 
                          0                        1293
501 
             Lodgepole Pine                    Low Sage
Mixed Chaparral 
                      60332                          31
1043 
          Montane Chaparral            Montane Hardwood    Montane
Hardwood-Conifer 
                       6648                         326
0 
           Montane Riparian        Orchard and Vineyard
Perennial Grassland 
                        180                           0
17 
             Pinyon-Juniper              Ponderosa Pine
Red Fir 
                        968                         708
66263 
                   Riverine                   Sagebrush       Sierran
Mixed Conifer 
                          0                        2292
14264 
          Subalpine Conifer                       Urban
Valley-Foothill Riparian 
                      66237                           0
0 
        Valley Oak Woodland                  Wet Meadow
White Fir 
                          0                        2216
2717 
>


I want to make a matrix that has the cover types as columns and
length(foo) rows. I want the matrix entities to be scored one if that
cover type else zero.

foo.mat <- matrix(data = 0, nrow = length(foo), 
                  ncol = nlevels(foo))

colnames(foo.mat) <- levels(foo)

That is easy enough but I'm at a loss as how to populate it properly.

In case I'm not being clear. This is what I want:

> foo[1]
[1] Montane Chaparral
39 Levels: Alpine-Dwarf Shrub Annual Grassland Aspen Barren ... White
Fir
> foo.mat[1,]
         Alpine-Dwarf Shrub            Annual Grassland
Aspen 
                         0                          0
0 
                     Barren                 Bitterbrush      Blue
Oak-Foothill Pine 
                         0                          0
0 
          Blue Oak Woodland  Chamise-Redshank Chaparral    Closed-Cone
Pine-Cypress 
                         0                          0
0 
                   Cropland                Desert Scrub
Douglas-Fir 
                         0                          0
0 
              Eastside Pine Freshwater Emergent Wetland
Jeffrey Pine 
                         0                          0
0 
                Joshua Tree                     Juniper
Lacustrine 
                         0                          0
0 
             Lodgepole Pine                    Low Sage
Mixed Chaparral 
                         0                          0
0 
          Montane Chaparral            Montane Hardwood    Montane
Hardwood-Conifer 
                         1                          0
0 
           Montane Riparian        Orchard and Vineyard
Perennial Grassland 
                         0                          0
0 
             Pinyon-Juniper              Ponderosa Pine
Red Fir 
                         0                          0
0 
                   Riverine                   Sagebrush       Sierran
Mixed Conifer 
                         0                          0
0 
          Subalpine Conifer                       Urban
Valley-Foothill Riparian 
                         0                          0
0 
        Valley Oak Woodland                  Wet Meadow
White Fir 
                         0                          0
0 
>


I'm sure that somebody will send me a cryptic oneliner but I have no
idea how to do this.

Thanks in advance, Andy



From ahmlatif at yahoo.com  Tue Jul 22 17:44:59 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Tue, 22 Jul 2003 08:44:59 -0700 (PDT)
Subject: [R] greek in main title
In-Reply-To: <0F0F86E6.0A14F10F.0C4E5449@aol.com>
Message-ID: <20030722154459.75563.qmail@web41201.mail.yahoo.com>

Hi,

change the name of function argument "theta" to
something else say "thta" and then use the following
.... 

heading <- expression(paste("Exp w/ ", theta, "=",
thta, ", n  = ", n[i]))

Hope it will help...

Mahbub.

--- Oldradio69 at aol.com wrote:
> Hello,
> 
> I have written a function that demonstrates the CLT
> by
> generating samples following the exponential
> distribution,
> calculating the means, plotting the histogram, and
> drawing
> the limiting normal curve as an overlay.  I have the
> title
> of each histogram state the sample size and rate
> (1/theta)
> for the exponential (the output is actually 4
> histograms),
> but I can't get the greek letter theta to appear in
> the
> heading.  Is this possible?  I can get nontext for
> the xlab 
> and ylab pretty easily.
> 
> Thanks, Jason
> 
> Below is my code.  Currently it just writes out
> "theta."
> 
> function (theta) 
> {
>   layout(matrix(c(1:4),2,2,byrow=TRUE))
>   par(bg = "cornsilk")
>   n <- c(1,10,25,50)
>   for(i in 1:4)  {
>      xbar <- rep(0, 1000)
>      for (j in 1:1000)
>         xbar[j] <- mean(rexp(n[i], rate = 1/theta))
>      heading <- paste("Exp w/ theta =", theta, ", n
> = ", n[i])
>      hist(xbar, prob = TRUE, breaks = "FD", main =
> title(heading, font.main = 6), 
>           col = "lightgray", xlab =
> expression(bar(x)))
>      xbar <- sort(xbar)
>      points(xbar, dnorm(xbar, mean = theta, sd =
> theta/sqrt(n[i])), 
>         type = "l", col = 2)
>     }
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help


__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From spencer.graves at pdf.com  Tue Jul 22 17:57:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 22 Jul 2003 08:57:13 -0700
Subject: [R] greek in main title
References: <0F0F86E6.0A14F10F.0C4E5449@aol.com>
Message-ID: <3F1D5ED9.9020703@pdf.com>

I was not able to get theta italicized, but I otherwise got something 
matching my understanding of what you wanted.  Consider the following:

plotExpNormal <-
function (theta)
{
   layout(matrix(c(1:4),2,2,byrow=TRUE))
   par(bg = "cornsilk")
   n <- c(1,10,25,50)
   for(i in 1:4)  {
      xbar <- rep(0, 1000)
      for (j in 1:1000)
         xbar[j] <- mean(rexp(n[i], rate = 1/theta))
      heading <- substitute(exp(italic(theta)) == th, list(th=theta))
      hist(xbar, prob = TRUE, breaks = "FD", main = title(heading, 
font.main = 6),
           col = "lightgray", xlab = expression(bar(x)))
      xbar <- sort(xbar)
      points(xbar, dnorm(xbar, mean = theta, sd = theta/sqrt(n[i])),
         type = "l", col = 2)
     }
}
plotExpNormal(2)
################
What do you think?
Spencer Graves
p.s.  I'm running R 1.7.1 under Windows 2000.

Oldradio69 at aol.com wrote:
> Hello,
> 
> I have written a function that demonstrates the CLT by
> generating samples following the exponential distribution,
> calculating the means, plotting the histogram, and drawing
> the limiting normal curve as an overlay.  I have the title
> of each histogram state the sample size and rate (1/theta)
> for the exponential (the output is actually 4 histograms),
> but I can't get the greek letter theta to appear in the
> heading.  Is this possible?  I can get nontext for the xlab 
> and ylab pretty easily.
> 
> Thanks, Jason
> 
> Below is my code.  Currently it just writes out "theta."
> 
> function (theta) 
> {
>   layout(matrix(c(1:4),2,2,byrow=TRUE))
>   par(bg = "cornsilk")
>   n <- c(1,10,25,50)
>   for(i in 1:4)  {
>      xbar <- rep(0, 1000)
>      for (j in 1:1000)
>         xbar[j] <- mean(rexp(n[i], rate = 1/theta))
>      heading <- paste("Exp w/ theta =", theta, ", n = ", n[i])
>      hist(xbar, prob = TRUE, breaks = "FD", main = title(heading, font.main = 6), 
>           col = "lightgray", xlab = expression(bar(x)))
>      xbar <- sort(xbar)
>      points(xbar, dnorm(xbar, mean = theta, sd = theta/sqrt(n[i])), 
>         type = "l", col = 2)
>     }
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lockwood at rand.org  Tue Jul 22 18:00:13 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Tue, 22 Jul 2003 12:00:13 -0400 (EDT)
Subject: [R] Making a group membership matrix
In-Reply-To: <000001c35067$da8a0dc0$78f05a99@msu.montana.edu>
Message-ID: <Pine.LNX.4.33.0307221154170.3633-100000@penguin.rand.org>

Hi,

The resulting matrix will be *really* large, so make sure you have
enough RAM.  You might reduce this by dropping all the levels of the
factor that have zero counts.  As far as the solution, it is a
one-liner, but not really a crypic one:

model.matrix(~foo-1)

(assuming that you haven't changed the default contrasts in options() )


> Hi Helpers:
> 
> I have a factor object that has 314k entries of 39 land cover types.
> (This object can be coerced to characters neatly should that be easier
> to work with.) 
> > length(foo)
> [1] 314482
> > foo[1:10]
>  [1] Montane Chaparral Barren            Red Fir           Red Fir
> 
>  [5] Red Fir           Red Fir           Red Fir           Red Fir
> 
>  [9] Red Fir           Red Fir          
> 39 Levels: Alpine-Dwarf Shrub Annual Grassland Aspen Barren ... White
> Fir
> > summary(foo)
>          Alpine-Dwarf Shrub            Annual Grassland
> Aspen 
>                        7402                           0
> 582 
>                      Barren                 Bitterbrush      Blue
> Oak-Foothill Pine 
>                       69111                           9
> 0 
>           Blue Oak Woodland  Chamise-Redshank Chaparral    Closed-Cone
> Pine-Cypress 
>                           0                           0
> 0 
>                    Cropland                Desert Scrub
> Douglas-Fir 
>                           0                           0
> 0 
>               Eastside Pine Freshwater Emergent Wetland
> Jeffrey Pine 
>                           0                           0
> 11342 
>                 Joshua Tree                     Juniper
> Lacustrine 
>                           0                        1293
> 501 
>              Lodgepole Pine                    Low Sage
> Mixed Chaparral 
>                       60332                          31
> 1043 
>           Montane Chaparral            Montane Hardwood    Montane
> Hardwood-Conifer 
>                        6648                         326
> 0 
>            Montane Riparian        Orchard and Vineyard
> Perennial Grassland 
>                         180                           0
> 17 
>              Pinyon-Juniper              Ponderosa Pine
> Red Fir 
>                         968                         708
> 66263 
>                    Riverine                   Sagebrush       Sierran
> Mixed Conifer 
>                           0                        2292
> 14264 
>           Subalpine Conifer                       Urban
> Valley-Foothill Riparian 
>                       66237                           0
> 0 
>         Valley Oak Woodland                  Wet Meadow
> White Fir 
>                           0                        2216
> 2717 
> >
> 
> 
> I want to make a matrix that has the cover types as columns and
> length(foo) rows. I want the matrix entities to be scored one if that
> cover type else zero.
> 
> foo.mat <- matrix(data = 0, nrow = length(foo), 
>                   ncol = nlevels(foo))
> 
> colnames(foo.mat) <- levels(foo)
> 
> That is easy enough but I'm at a loss as how to populate it properly.
> 
> In case I'm not being clear. This is what I want:
> 
> > foo[1]
> [1] Montane Chaparral
> 39 Levels: Alpine-Dwarf Shrub Annual Grassland Aspen Barren ... White
> Fir
> > foo.mat[1,]

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From Oldradio69 at aol.com  Tue Jul 22 18:02:53 2003
From: Oldradio69 at aol.com (Oldradio69@aol.com)
Date: Tue, 22 Jul 2003 12:02:53 -0400
Subject: [R] greek in main title
Message-ID: <0F90E202.3ED3E3FB.0C4E5449@aol.com>

Hi -- your solution looses the ability to assign the values of theta and n[i]
in the simulation... you get thta and n_i in the titles.


In a message dated 7/22/2003 10:44:59 AM Eastern Standard Time, ahmlatif at yahoo.com writes:

> Hi,
> 
> change the name of function argument "theta" to
> something else say "thta" and then use the following
> .... 
> 
> heading <- expression(paste("Exp w/ ", theta, "=",
> thta, ", n  = ", n[i]))
> 
> Hope it will help...
> 
> Mahbub.
> 
> --- Oldradio69 at aol.com wrote:
> > Hello,
> > 
> > I have written a function that demonstrates the CLT
> > by
> > generating samples following the exponential
> > distribution,
> > calculating the means, plotting the histogram, and
> > drawing
> > the limiting normal curve as an overlay.  I have the
> > title
> > of each histogram state the sample size and rate
> > (1/theta)
> > for the exponential (the output is actually 4
> > histograms),
> > but I can't get the greek letter theta to appear in
> > the
> > heading.  Is this possible?  I can get nontext for
> > the xlab 
> > and ylab pretty easily.
> > 
> > Thanks, Jason
> > 
> > Below is my code.  Currently it just writes out
> > "theta."
> > 
> > function (theta) 
> > {
> >   layout(matrix(c(1:4),2,2,byrow=TRUE))
> >   par(bg = "cornsilk")
> >   n <- c(1,10,25,50)
> >   for(i in 1:4)  {
> >      xbar <- rep(0, 1000)
> >      for (j in 1:1000)
> >         xbar[j] <- mean(rexp(n[i], rate = 1/theta))
> >      heading <- paste("Exp w/ theta =", theta, ", n
> > = ", n[i])
> >      hist(xbar, prob = TRUE, breaks = "FD", main =
> > title(heading, font.main = 6), 
> >           col = "lightgray", xlab =
> > expression(bar(x)))
> >      xbar <- sort(xbar)
> >      points(xbar, dnorm(xbar, mean = theta, sd =
> > theta/sqrt(n[i])), 
> >         type = "l", col = 2)
> >     }
> > }
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> __________________________________
> Do you Yahoo!?
> Yahoo! SiteBuilder - Free, easy-to-use web site design 
> software
> http://sitebuilder.yahoo.com



From jarrod.hadfield at imperial.ac.uk  Wed Jul 23 09:52:03 2003
From: jarrod.hadfield at imperial.ac.uk (Jarrod Hadfield)
Date: Wed, 23 Jul 2003 08:52:03 +0100
Subject: [R] animal models and lme
Message-ID: <a05100303bb43eef9888a@[129.31.3.147]>


-- 
Hi R users,

I'm trying to fit an animal model using lme and cannot fit the 
corelation matrix which describes the additive genetic correlation 
between individuals.

I have created a test data set (pedigree) where the first column 
specifies the id of each individual, and the second column contains 
simulated trait values. AGRM is the corresponding additive genetic 
relationship matrix whose elements are equal to the genetic 
correlation between individuals.

The test data is simulated to have a heritibility of 0.8 (i.e. 80% of 
the variation can be explained by genetic effects)

My initial attempt has been:

CM<-pdSymm(AGRM, ~1|animal)
lme(trait~1, data=pedigree, CM)

but which ever combination I use I always get "invalid formula for groups".

Does any one know how to code this sort of model?

Thanks for your help,

Jarrod Hadfield.

copy and paste from here to get the test data (Sorry the correlation 
matrix is so big)

##############################################################################################

animal<-c("Newbird1","Newbird2","a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z","aa","bb","cc","dd","ee","ff","gg","hh","ii","jj","kk","ll","mm")

trait<-c(-1.10988343,-0.95061690,-1.05596407,-0.20075671,-0.98747062,1.72132706,1.33973268,0.30333354,0.34969961,0.70681745,-0.15210172,-1.05918719,-0.32665059,0.55423824,-0.81324407,-0.61359451,,0.65936390,-0.92295239,0.87285686,1.11508781,1.30904325,0.83437615,,0.72948628,,0.63043694,-0.40078431,,1.20814060,,0.69233357,-1.51568342,-0.16235203,-2.26109886,1.21242313,0.04010594,0.06988673,-0.70644384,-0.69870290,-0.02043294,-0.57957532,-0.78726879,0.20574982,0.27469486,-0.25899545)

pedigree<-data.frame(cbind(animal, trait))

AGRM<-as.matrix(cbind(c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0),c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5),c(0,0,1,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0,1,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.75,0.75,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25),c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5, 
0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25),c(0,0,0.5,0.5,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.75,0.75,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0,0,0),c(0,0,0,0,0,0,0.5 
,0.5,0,0,0,0,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5),c(0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0.5,0,0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,1,0.5,0.5,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c(0.5,0,0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0 
,0,0,0,0,0.5,1,0.5,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c(0.5,0,0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,1,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c(0,0,0.25,0.25,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0.125,0.125,0.125,1,0.5,0.5,0.125,0.125,0.125,0.125,0.125,0.125,0,0,0),c(0,0,0.25,0.25,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,1,0.5,0.125,0.125,0.125,0.125,0.125,0.125,0,0,0),c(0,0,0.25,0.25,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,0.5,1,0.125,0.125,0.125,0.125,0.125,0.125,0,0,0),c(0,0,0,0,0.25,0.75,0,0,0,0,0,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,1.25,0.75,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.75,0,0,0,0,0,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.75,1.25,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.75,0,0,0,0,0,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0. 
125,0.75,0.75,1.25,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.25,0,0,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0.125,0.125,0.125,0.25,0.25,0.25,1,0.5,0.5,0.125,0.125,0.125),c(0,0,0,0,0.25,0.25,0,0,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0.125,0.125,0.125,0.25,0.25,0.25,0.5,1,0.5,0.125,0.125,0.125),c(0,0,0,0,0.25,0.25,0,0,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0.125,0.125,0.125,0.25,0.25,0.25,0.5,0.5,1,0.125,0.125,0.125),c(0,0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,1,0.5,0.5),c(0,0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,1,0.5),c(0,0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,0.5,1)))
colnames(AGRM)<-animal
rownames(AGRM)<-animal



From ripley at stats.ox.ac.uk  Tue Jul 22 18:13:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jul 2003 17:13:58 +0100 (BST)
Subject: [R] animal models and lme
In-Reply-To: <a05100303bb43eef9888a@[129.31.3.147]>
Message-ID: <Pine.LNX.4.44.0307221712280.19749-100000@gannet.stats>

On Wed, 23 Jul 2003, Jarrod Hadfield wrote:

I can't get your message included, for some reason.

Your lme model has no random effects: that is not allowed.  Either include 
a random effect or (more likely) use gls or lm.gls (MASS) if you really 
just want a GLS fit.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jerome at hivnet.ubc.ca  Tue Jul 22 18:32:06 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 22 Jul 2003 09:32:06 -0700
Subject: [R] greek in main title
In-Reply-To: <0F0F86E6.0A14F10F.0C4E5449@aol.com>
References: <0F0F86E6.0A14F10F.0C4E5449@aol.com>
Message-ID: <200307221638.JAA15483@hivnet.ubc.ca>


Here is another solution. I'm not sure what you are trying to accomplish 
with font.main=6, though. See ?par for details: have a closer look at the 
description for "font" option.

Cheers,
Jerome

  theta <- 2
  layout(matrix(c(1:4),2,2,byrow=TRUE))
  par(bg = "cornsilk")
  n <- c(1,10,25,50)
  for(i in 1:4)  {
     xbar <- rep(0, 1000)
     for (j in 1:1000)
        xbar[j] <- mean(rexp(n[i], rate = 1/theta))
     heading <- paste("paste(plain('Exp w/ '),theta,' = ", theta, 
                ", n = ", n[i],"')",sep="")
     hist(xbar, prob = TRUE, breaks = "FD", main=parse(text=heading),
          font.main = 6, 
          col = "lightgray", xlab = expression(bar(x)))
     xbar <- sort(xbar)
     points(xbar, dnorm(xbar, mean = theta, sd = theta/sqrt(n[i])), 
        type = "l", col = 2)
    }


On July 22, 2003 08:11 am, Oldradio69 at aol.com wrote:
> Content-Length: 1269
> Status: R
> X-Status: N
>
> Hello,
>
> I have written a function that demonstrates the CLT by
> generating samples following the exponential distribution,
> calculating the means, plotting the histogram, and drawing
> the limiting normal curve as an overlay.  I have the title
> of each histogram state the sample size and rate (1/theta)
> for the exponential (the output is actually 4 histograms),
> but I can't get the greek letter theta to appear in the
> heading.  Is this possible?  I can get nontext for the xlab
> and ylab pretty easily.
>
> Thanks, Jason
>
> Below is my code.  Currently it just writes out "theta."
>
> function (theta)
> {
>   layout(matrix(c(1:4),2,2,byrow=TRUE))
>   par(bg = "cornsilk")
>   n <- c(1,10,25,50)
>   for(i in 1:4)  {
>      xbar <- rep(0, 1000)
>      for (j in 1:1000)
>         xbar[j] <- mean(rexp(n[i], rate = 1/theta))
>      heading <- paste("Exp w/ theta =", theta, ", n = ", n[i])
>      hist(xbar, prob = TRUE, breaks = "FD", main = title(heading,
> font.main = 6), col = "lightgray", xlab = expression(bar(x)))
>      xbar <- sort(xbar)
>      points(xbar, dnorm(xbar, mean = theta, sd = theta/sqrt(n[i])),
>         type = "l", col = 2)
>     }
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lsilva at fc.up.pt  Tue Jul 22 18:59:24 2003
From: lsilva at fc.up.pt (Luis Miguel Almeida da Silva)
Date: Tue, 22 Jul 2003 17:59:24 +0100
Subject: [R] plot character
Message-ID: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>

Dear helpers
 
Is it possible to plot with a desired character? For example "tr" for training error and "te" for test error.
I tried 
 
plot(x,y,pch="tr")
 
but only appears "t" in the plot



From B.Rowlingson at lancaster.ac.uk  Tue Jul 22 19:10:05 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 22 Jul 2003 18:10:05 +0100
Subject: [R] plot character
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
References: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
Message-ID: <3F1D6FED.5000707@lancaster.ac.uk>

Luis Miguel Almeida da Silva wrote:
> Dear helpers
>  
> Is it possible to plot with a desired character? For example "tr" for training error and "te" for test error.
> I tried 
>  
> plot(x,y,pch="tr")
>  
> but only appears "t" in the plot
> 

  You probably want to use the 'text' function, but you have to set up 
the plot first, since 'text' adds to plots. EG:

plot(c(0,1),c(0,1),type='n')

text(runif(10),runif(10),'Random')

The text is by default centered on the x,y location, but you can change 
this with the adj parameter.

The text can be a vector, and it is recycled to the length of x,y:

text(runif(10),runif(10),c('Foo','Bar'))

see help(text) for more.

Baz



From jerome at hivnet.ubc.ca  Tue Jul 22 19:13:14 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 22 Jul 2003 10:13:14 -0700
Subject: [R] plot character
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
References: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
Message-ID: <200307221719.KAA17487@hivnet.ubc.ca>


Use text().

x <- rnorm(10); y <- rnorm(10)
plot(x,y,type="n")
text(x,y,rep(c("te","tr"),each=5))

Cheers,
Jerome

On July 22, 2003 09:59 am, Luis Miguel Almeida da Silva wrote:
> Dear helpers
>
> Is it possible to plot with a desired character? For example "tr" for
> training error and "te" for test error. I tried
>
> plot(x,y,pch="tr")
>
> but only appears "t" in the plot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Tue Jul 22 19:17:44 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 22 Jul 2003 17:17:44 -0000
Subject: [R] plot character
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
References: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
Message-ID: <1058894225.4164.13.camel@localhost>

On Tue, 2003-07-22 at 11:59, Luis Miguel Almeida da Silva wrote:
> Dear helpers
>  
> Is it possible to plot with a desired character? For example "tr" for training error and "te" for test error.
> I tried 
>  
> plot(x,y,pch="tr")
>  
> but only appears "t" in the plot


One option is to use text(x, y, "TextSymbol") for your point symbols.
This will by default place the "TextSymbol" in the plot region centered
on x,y.

You would first call:

plot(x, y, type = "n") 

so that no points are plotted.

Then use:

text(x, y, "tr")

and

text(x, y, "te")

for each set of x,y coordinates as appropriate.

You can use the 'cex' argument to text() to adjust text size and of
course others for color, etc.

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Tue Jul 22 19:21:24 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 22 Jul 2003 10:21:24 -0700
Subject: [R] plot character
References: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
Message-ID: <3F1D7294.2070906@pdf.com>

Have you considered "?text"?

hope this helps.  spencer graves

Luis Miguel Almeida da Silva wrote:
> Dear helpers
>  
> Is it possible to plot with a desired character? For example "tr" for training error and "te" for test error.
> I tried 
>  
> plot(x,y,pch="tr")
>  
> but only appears "t" in the plot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Tue Jul 22 19:31:21 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 22 Jul 2003 13:31:21 -0400
Subject: [R] plot character
In-Reply-To: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
References: <D52F84A2AE107848949A8C7E45F02D699DEAAE@MAIL.fc.up.pt>
Message-ID: <13tqhv0cajn9o1r78t94uonf39k3e81did@4ax.com>

On Tue, 22 Jul 2003 17:59:24 +0100, "Luis Miguel Almeida da Silva"
<lsilva at fc.up.pt> wrote :

>Dear helpers
> 
>Is it possible to plot with a desired character? For example "tr" for training error and "te" for test error.
>I tried 
> 
>plot(x,y,pch="tr")
> 
>but only appears "t" in the plot

Use text() instead:

 plot(x, y, type='n')
 text(x, y, "tr")

The first line sets up the axes, etc; the second one actually plots
the strings.  You can plot a whole character vector of different
values if you like.

Duncan Murdoch



From monkeychump at hushmail.com  Tue Jul 22 19:53:09 2003
From: monkeychump at hushmail.com (monkeychump@hushmail.com)
Date: Tue, 22 Jul 2003 10:53:09 -0700
Subject: [R] Matrix manipulation - dropping rows based on a query
Message-ID: <200307221753.h6MHr9lg034471@mailserver2.hushmail.com>


I have a matrix with some very funky data. I want to drop the columns
with a mean > 1 (for instance).
#Example:
my.mat <- matrix(rnorm(100, 0, 3), nrow = 10)

#I know I can get the row means:
apply(my.mat,1, mean)

I think I need to get a vector that has those rows > 1

my.mat[-rows>1,]

The actual querry is much more complicated but this gives an approximation.

Thanks.









Promote security and make money with the Hushmail Affiliate Program:



From jmacdon at med.umich.edu  Tue Jul 22 20:15:24 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 22 Jul 2003 14:15:24 -0400
Subject: [R] Matrix manipulation - dropping rows based on a query
Message-ID: <sf1d4705.086@mail-02.med.umich.edu>

tst <- apply(my.mat, 2, mean) < 1

my.mat[,tst] 

Will return only columns with mean < 1

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> <monkeychump at hushmail.com> 07/22/03 01:53PM >>>

I have a matrix with some very funky data. I want to drop the columns
with a mean > 1 (for instance).
#Example:
my.mat <- matrix(rnorm(100, 0, 3), nrow = 10)

#I know I can get the row means:
apply(my.mat,1, mean)

I think I need to get a vector that has those rows > 1

my.mat[-rows>1,]

The actual querry is much more complicated but this gives an
approximation.

Thanks.









Promote security and make money with the Hushmail Affiliate Program:

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From fjmolina at lbl.gov  Tue Jul 22 20:45:01 2003
From: fjmolina at lbl.gov (Francisco J Molina)
Date: Tue, 22 Jul 2003 11:45:01 -0700
Subject: [R] libblas.so.3
Message-ID: <16157.34349.570428.171052@dhcp-63-193.cse.ucsc.edu>


I am using a Pentium 4 and R 1.7.1

I removed libblas.so.3 to make sure that my programs are using ATLAS (
maching dependent implementation of BLAS ).

R is not working. Any suggestion?



From ligges at statistik.uni-dortmund.de  Tue Jul 22 21:40:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Jul 2003 21:40:52 +0200
Subject: [R] Matrix manipulation - dropping rows based on a query
In-Reply-To: <sf1d4705.086@mail-02.med.umich.edu>
References: <sf1d4705.086@mail-02.med.umich.edu>
Message-ID: <3F1D9344.7060208@statistik.uni-dortmund.de>

James MacDonald wrote:
> tst <- apply(my.mat, 2, mean) < 1

... and
  rowMeans(my.mat) < 1
will be faster, but perhaps not working for the requested "more 
complicated query".

Uwe Ligges

> my.mat[,tst] 
> 
> Will return only columns with mean < 1
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> 
>>>><monkeychump at hushmail.com> 07/22/03 01:53PM >>>
> 
> 
> I have a matrix with some very funky data. I want to drop the columns
> with a mean > 1 (for instance).
> #Example:
> my.mat <- matrix(rnorm(100, 0, 3), nrow = 10)
> 
> #I know I can get the row means:
> apply(my.mat,1, mean)
> 
> I think I need to get a vector that has those rows > 1
> 
> my.mat[-rows>1,]
> 
> The actual querry is much more complicated but this gives an
> approximation.
> 
> Thanks.
>



From ligges at statistik.uni-dortmund.de  Tue Jul 22 21:42:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Jul 2003 21:42:52 +0200
Subject: [R] libblas.so.3
In-Reply-To: <16157.34349.570428.171052@dhcp-63-193.cse.ucsc.edu>
References: <16157.34349.570428.171052@dhcp-63-193.cse.ucsc.edu>
Message-ID: <3F1D93BC.1010800@statistik.uni-dortmund.de>

Francisco J Molina wrote:

> I am using a Pentium 4 and R 1.7.1
> 
> I removed libblas.so.3 to make sure that my programs are using ATLAS (
> maching dependent implementation of BLAS ).
> 
> R is not working. Any suggestion?

First suggestion: Please tell us some details, in particular why you 
think R is not working. Error message etc. (fails the configure, the 
make or running R?) ...

The OS might be useful as well, we can only guess you are on a Unix-like 
platform.

Uwe Ligges



From fjmolina at lbl.gov  Tue Jul 22 21:52:27 2003
From: fjmolina at lbl.gov (Francisco J Molina)
Date: Tue, 22 Jul 2003 12:52:27 -0700
Subject: [R] libblas.so.3
In-Reply-To: <3F1D93BC.1010800@statistik.uni-dortmund.de>
References: <16157.34349.570428.171052@dhcp-63-193.cse.ucsc.edu>
	<3F1D93BC.1010800@statistik.uni-dortmund.de>
Message-ID: <16157.38395.175581.48312@dhcp-63-193.cse.ucsc.edu>


I am using linux rehat 9
When I try to run R I get

usr/lib/R/bin/R.bin: error while loading shared libraries: libblas.so.3: cannot open shared object file: No such file or directory

( because I removed it )

I thought of compiling R from source but I have read:

"R currently uses only level 1 blas and the most significant atlas
optimizations are for level 3 (and level 2 to some extent). 
The problem with using ATLAS is that its installation process does
not build the shared libraries by default and the whole build process is
rather complicated."

This is from a 3 years old message but I guess that the state of the art now
is similar.

Uwe Ligges writes:
 > Francisco J Molina wrote:
 > 
 > > I am using a Pentium 4 and R 1.7.1
 > > 
 > > I removed libblas.so.3 to make sure that my programs are using ATLAS (
 > > maching dependent implementation of BLAS ).
 > > 
 > > R is not working. Any suggestion?
 > 
 > First suggestion: Please tell us some details, in particular why you 
 > think R is not working. Error message etc. (fails the configure, the 
 > make or running R?) ...
 > 
 > The OS might be useful as well, we can only guess you are on a Unix-like 
 > platform.
 > 
 > Uwe Ligges
 >



From ligges at statistik.uni-dortmund.de  Tue Jul 22 22:04:16 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Jul 2003 22:04:16 +0200
Subject: [R] libblas.so.3
In-Reply-To: <16157.38395.175581.48312@dhcp-63-193.cse.ucsc.edu>
References: <16157.34349.570428.171052@dhcp-63-193.cse.ucsc.edu>	<3F1D93BC.1010800@statistik.uni-dortmund.de>
	<16157.38395.175581.48312@dhcp-63-193.cse.ucsc.edu>
Message-ID: <3F1D98C0.5080400@statistik.uni-dortmund.de>

Francisco J Molina wrote:
> I am using linux rehat 9
> When I try to run R I get
> 
> usr/lib/R/bin/R.bin: error while loading shared libraries: libblas.so.3: cannot open shared object file: No such file or directory
> 
> ( because I removed it )
> 
> I thought of compiling R from source but I have read:
> 
> "R currently uses only level 1 blas and the most significant atlas
> optimizations are for level 3 (and level 2 to some extent). 
> The problem with using ATLAS is that its installation process does
> not build the shared libraries by default and the whole build process is
> rather complicated."
> 
> This is from a 3 years old message but I guess that the state of the art now
> is similar.

You can specify --with-blas during the configuration step and don't need 
to remove any BLAS libraries. See, e.g., Appendix A.2.2 of the "R 
Installation and Administration" manual.

Uwe Ligges


> Uwe Ligges writes:
>  > Francisco J Molina wrote:
>  > 
>  > > I am using a Pentium 4 and R 1.7.1
>  > > 
>  > > I removed libblas.so.3 to make sure that my programs are using ATLAS (
>  > > maching dependent implementation of BLAS ).
>  > > 
>  > > R is not working. Any suggestion?
>  > 
>  > First suggestion: Please tell us some details, in particular why you 
>  > think R is not working. Error message etc. (fails the configure, the 
>  > make or running R?) ...
>  > 
>  > The OS might be useful as well, we can only guess you are on a Unix-like 
>  > platform.
>  > 
>  > Uwe Ligges
>  >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bbraumoeller at wcfia.harvard.edu  Tue Jul 22 23:19:42 2003
From: bbraumoeller at wcfia.harvard.edu (Bear F. Braumoeller)
Date: Tue, 22 Jul 2003 17:19:42 -0400
Subject: [R] Curious warning in R for OS X w/Xwindows
Message-ID: <359FD80C-BC8A-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>

I recently reformatted my TiBook and reinstalled everything from the 
system up.  After installing Apple's version of XWindows and the 
XWindows version of R 1.7.0, I ran the two and got a strange warning:

-----
R : Copyright 2003, The R Development Core Team
Version 1.7.0 Patched (2003-05-14)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

 > help("lm")
WARNING: terminal is not fully functional
-  (press RETURN)
-----

The main functional disabilities I've noticed after I press return are 
more or less cosmetic -- the arrow keys don't scroll, for example; 
rather, they produce ugly characters on the screen -- but are 
nevertheless annoying.  Moreover, I don't get this message in a regular 
terminal window when I use the "man" command; there, the viewer works 
exactly as it should.

Has anyone else encountered this, and does anyone have a fix?


Thanks,
-Bear


Bear F. Braumoeller
Assistant Professor
Department of Government
Harvard University
http://www.people.fas.harvard.edu/~bfbraum



From bates at stat.wisc.edu  Wed Jul 23 00:11:14 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 22 Jul 2003 22:11:14 -0000
Subject: [R] Processing a large number of files
Message-ID: <6rk7aarz3i.fsf@bates4.stat.wisc.edu>

I maintain the Devore5 package which contains the data sets from the
5th edition of Jay Devore's text "Probability and Statistics for
Engineering and the Sciences".  The 6th edition has now been published
and it includes several new data sets in exercises and examples.  In
addition, some exercises and examples from the 5th edition are
renumbered in the 6th edition.

I face the daunting task of adding and documenting the new data sets
and updating the numbering.  I had thought of going back to the text
files but discovered that it was easier to work from another form.

A CD-ROM with the book provides the data sets in several different
formats, including SPSS saved data sets.  I was pleasantly surprised
that I could write an R script that read the data from the .sav file,
converted it to an R data frame, converted the SPSS name such as
ex01-11.sav to an allowable R name (ex01.11), and saved the resulting
data set in a new directory.  In the past I would have written Python
or Perl scripts to do all the manipulations of iterating over files
but with the current facilities in R for listing file names, etc., I
can do the whole thing in R.  My script, which worked on the first
try, is

library(foreign)
SPSS = "/cdrom/Manual Install/Datasets/SPSS/"  # change as appropriate
Rdata = "/tmp/Devore6/data/"            # change as appropriate
chapters = c("CH01", "CH04", "CH06", "CH07", "CH08", "CH09",
    "CH10", "CH11", "CH12", "CH13", "CH15", "Ch14", "Ch16")
for (ch in chapters) {
    path = paste(SPSS, ch, sep = '')
    files = list.files(path = path, pattern = '*.sav')
    for (ff in files) {
        dsn = gsub('-', '.', gsub('\.sav$', '', ff))
        assign(dsn, data.frame(read.spss(paste(path, ff, sep = '/'))))
        save(list = dsn, file = paste(Rdata, dsn, ".rda", sep = ''))
    }
}

In fact this script processed the 326 files so quickly that I thought
I must have made a mistake and somehow missed most of the files.  I
had to look in the output directory to convince myself that it had
indeed run properly.

I would encourage others to consider using list.files, gsub,
etc. within R for such scripting applications.



From p.dalgaard at biostat.ku.dk  Wed Jul 23 00:20:34 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 22 Jul 2003 22:20:34 -0000
Subject: [R] Curious warning in R for OS X w/Xwindows
In-Reply-To: <359FD80C-BC8A-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>
References: <359FD80C-BC8A-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>
Message-ID: <x2ptk22o95.fsf@biostat.ku.dk>

"Bear F. Braumoeller" <bbraumoeller at wcfia.harvard.edu> writes:

> I recently reformatted my TiBook and reinstalled everything from the
> system up.  After installing Apple's version of XWindows and the
> XWindows version of R 1.7.0, I ran the two and got a strange warning:
> 
> -----
> R : Copyright 2003, The R Development Core Team
> Version 1.7.0 Patched (2003-05-14)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type `license()' or `licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type `contributors()' for more information.
> 
> Type `demo()' for some demos, `help()' for on-line help, or
> `help.start()' for a HTML browser interface to help.
> Type `q()' to quit R.
> 
>  > help("lm")
> WARNING: terminal is not fully functional
> -  (press RETURN)
> -----
> 
> The main functional disabilities I've noticed after I press return are
> more or less cosmetic -- the arrow keys don't scroll, for example;
> rather, they produce ugly characters on the screen -- but are
> nevertheless annoying.  Moreover, I don't get this message in a
> regular terminal window when I use the "man" command; there, the
> viewer works exactly as it should.
> 
> Has anyone else encountered this, and does anyone have a fix?

OS X is not quite Unix/Linux, so I don't really know, but I'd suspect
that somehow the environment variable that describes the terminal
capabilities (usually called TERM) got unset or not set properly for
the terminal emulator in which you are running R. How exactly are you
starting R? What is the output of 

Sys.getenv("TERM")
Sys.getenv("PAGER")
options("pager")

?
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bbraumoeller at wcfia.harvard.edu  Wed Jul 23 00:59:19 2003
From: bbraumoeller at wcfia.harvard.edu (Bear F. Braumoeller)
Date: Tue, 22 Jul 2003 18:59:19 -0400
Subject: [R] Curious warning in R for OS X w/Xwindows
In-Reply-To: <x2ptk22o95.fsf@biostat.ku.dk>
Message-ID: <2083464E-BC98-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>


On Tuesday, July 22, 2003, at 06:24 PM, Peter Dalgaard BSA wrote:

> OS X is not quite Unix/Linux, so I don't really know, but I'd suspect
> that somehow the environment variable that describes the terminal
> capabilities (usually called TERM) got unset or not set properly for
> the terminal emulator in which you are running R. How exactly are you
> starting R? What is the output of
>
> Sys.getenv("TERM")
> Sys.getenv("PAGER")
> options("pager")


I'm starting R with

    xterm -sb -rightbar -sl 1000 -bg black -fg blue -title R -e 
/usr/local/bin/R

-- but it also happens if I just start a vanilla terminal and type R.

As to the other questions,

 > Sys.getenv("TERM")
    TERM
"xterm"
 > Sys.getenv("PAGER")
           PAGER
"/usr/bin/less"
 > options("pager")
$pager
[1] "/usr/local/lib/R/bin/pager"

 >


-Bear

Bear F. Braumoeller
Assistant Professor
Department of Government
Harvard University
http://www.people.fas.harvard.edu/~bfbraum



From Simon.Blomberg at anu.edu.au  Wed Jul 23 01:33:18 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Wed, 23 Jul 2003 09:33:18 +1000
Subject: [R] animal models and lme
Message-ID: <7A3A13F416B40842BD2C1753E044B359B133E3@CASEVS02.cas.anu.edu.au>

Hi,

You should look at Pinheiro and Bates (2000) Mixed-effects models in S and S-Plus. It describes how to format the correlation matrix to pass to functions lme and gls. Basically, the correlation matrix has to be one of the corStruct classes, probably corSymm for your example. So in the call to lme (or gls if you really have no random effects), use something like: correlation=corSymm(AGRM[lower.tri(AGRM)], fixed=TRUE). Anyway, it's all there in the book.

HTH,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Jarrod Hadfield [mailto:jarrod.hadfield at imperial.ac.uk]
> Sent: Wednesday, 23 July 2003 5:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] animal models and lme
> 
> 
> 
> -- 
> Hi R users,
> 
> I'm trying to fit an animal model using lme and cannot fit the 
> corelation matrix which describes the additive genetic correlation 
> between individuals.
> 
> I have created a test data set (pedigree) where the first column 
> specifies the id of each individual, and the second column contains 
> simulated trait values. AGRM is the corresponding additive genetic 
> relationship matrix whose elements are equal to the genetic 
> correlation between individuals.
> 
> The test data is simulated to have a heritibility of 0.8 (i.e. 80% of 
> the variation can be explained by genetic effects)
> 
> My initial attempt has been:
> 
> CM<-pdSymm(AGRM, ~1|animal)
> lme(trait~1, data=pedigree, CM)
> 
> but which ever combination I use I always get "invalid 
> formula for groups".
> 
> Does any one know how to code this sort of model?
> 
> Thanks for your help,
> 
> Jarrod Hadfield.
> 
> copy and paste from here to get the test data (Sorry the correlation 
> matrix is so big)
> 
> ##############################################################
> ################################
> 
> animal<-c("Newbird1","Newbird2","a","b","c","d","e","f","g","h
> ","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w",
> "x","y","z","aa","bb","cc","dd","ee","ff","gg","hh","ii","jj",
> "kk","ll","mm")
> 
> trait<-c(-1.10988343,-0.95061690,-1.05596407,-0.20075671,-0.98
747062,1.72132706,1.33973268,0.30333354,0.34969961> ,0.70681745,-0.15210172,-1.05918719,-0.32665059,0.55423824,-0.
> 81324407,-0.61359451,,0.65936390,-0.92295239,0.87285686,1.1150
> 8781,1.30904325,0.83437615,,0.72948628,,0.63043694,-0.40078431
> ,,1.20814060,,0.69233357,-1.51568342,-0.16235203,-2.26109886,1
.21242313,0.04010594,0.06988673,-0.70644384,-> 0.69870290,-0.02043294,-0.57957532,-0.78726879,0.20574982,0.27
> 469486,-0.25899545)
> 
> pedigree<-data.frame(cbind(animal, trait))
> 
> AGRM<-as.matrix(cbind(c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0,0,0,0,0,0,0,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0),c(0,1,0,0,0
,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0> ,0,0,0,0,0,0,0,0,0.5,0.5,0.5),c(0,0,1,0,0,0,0,0,0,0,0.5,0.5,0.
> 5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0
> ,0,0,0,0,0,0),c(0,0,0,1,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,
> 0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0,
0,1,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,> 0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25,0.25,0.25,0.25,0,0,0),c(0
,0,0,0,0,1,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0> ,0,0,0,0,0,0,0,0,0,0,0.75,0.75,0.75,0.25,0.25,0.25,0,0,0),c(0,
0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,> 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0,1,0,0,0
> ,0,0,0,0,0,0,0,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
> ,0,0,0),c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,
> 0.5,0.5,0,0,0,0,0,0,0,0,0,0.5,0.5,0.5,0.25,0.25,0.25),c(0,0,0,
0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5!
>  , 
> 0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25),c(0,0,0.5,0.5,
> 0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.
> 25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0
> ,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.2
> 5,0.25,0,0,0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0,0.5,0.5,1,
> 0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0,0,
> 0,0,0,0,0,0,0),c(0,0,0.5,0.5,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0
> ,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0)
> ,c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0
> ,0,0,0,0,0.25,0.25,0.25,0.75,0.75,0.75,0.25,0.25,0.25,0,0,0),c
> (0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,1,0.5,0.5,0,0,0,0,0,0,0,0
> ,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0),c(0,0,
0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0,> 0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.25,0.25,0.25,0,0,0)
> ,c(0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0
> ,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0,0,0),c(0,0,0
> ,0,0,0,0.!
>  5 
> ,0.5,0,0,0,0,0,0,0,0,0,0,1,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0
> ,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0.5,
> 1,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0
,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0.5,0.5,1,0.5,0,0,0,0> ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),c(0,0,0,0,0,0,0.5,0.5,0,0,0,0,
> 0,0,0,0,0,0,0.5,0.5,0.5,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
> 0),c(0,0,0,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,1,0.5,0.5
> ,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0,0,0,
0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,1,0.> 5,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0,0,0
,0,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5> ,1,0.5,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.5,0.5,0.5),c(0,0,0,0
,0,0,0,0,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0.5,0> .5,1,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25),c(0.5,0,
0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0,0,0,> 0,0,0,0,0,0,1,0.5,0.5,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c(0
.5,0,0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0> ,0,0,0,!
>  0 
> ,0,0,0,0,0.5,1,0.5,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c(0.5,
0,0.5,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0,0,> 0,0,0,0,0,0,0,0.5,0.5,1,0.125,0.125,0.125,0,0,0,0,0,0,0,0,0),c
> (0,0,0.25,0.25,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0.25,0.25,0.25
> ,0.25,0,0,0,0,0,0,0,0,0.125,0.125,0.125,1,0.5,0.5,0.125,0.125,
> 0.125,0.125,0.125,0.125,0,0,0),c(0,0,0.25,0.25,0.5,0,0,0,0,0,0
> .25,0.25,0.25,0.5,0.25,0.25,0.25,0.25,0,0,0,0,0,0,0,0,0.125,0.
> 125,0.125,0.5,1,0.5,0.125,0.125,0.125,0.125,0.125,0.125,0,0,0)
> ,c(0,0,0.25,0.25,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0.25,0.25,0.
> 25,0.25,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,0.5,1,0.125,0.12
> 5,0.125,0.125,0.125,0.125,0,0,0),c(0,0,0,0,0.25,0.75,0,0,0,0,0
> ,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0.12
> 5,1.25,0.75,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.75,0,0
> ,0,0,0,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.125,0.12
> 5,0.125,0.75,1.25,0.75,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.
> 75,0,0,0,0,0,0,0,0,0.75,0.5,0.5,0.5,0,0,0,0,0,0,0,0,0,0,0,0.12
> 5,0.125,0!
>  . 
> 125,0.75,0.75,1.25,0.25,0.25,0.25,0,0,0),c(0,0,0,0,0.25,0.25,0
> ,0,0.5,0,0,0,0,0,0.25,0.25,0.25,0.5,0,0,0,0,0.25,0.25,0.25,0.2
> 5,0,0,0,0.125,0.125,0.125,0.25,0.25,0.25,1,0.5,0.5,0.125,0.125
> ,0.125),c(0,0,0,0,0.25,0.25,0,0,0.5,0,0,0,0,0,0.25,0.25,0.25,0
> .5,0,0,0,0,0.25,0.25,0.25,0.25,0,0,0,0.125,0.125,0.125,0.25,0.
> 25,0.25,0.5,1,0.5,0.125,0.125,0.125),c(0,0,0,0,0.25,0.25,0,0,0
> .5,0,0,0,0,0,0.25,0.25,0.25,0.5,0,0,0,0,0.25,0.25,0.25,0.25,0,
> 0,0,0.125,0.125,0.125,0.25,0.25,0.25,0.5,0.5,1,0.125,0.125,0.1
> 25),c(0,0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.25
> ,0.25,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,1,0.5,0.5),
> c(0,0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.2
> 5,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,1,0.5),c(0,
0.5,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0,0,0,0,0,0,> 0.25,0.25,0.5,0.25,0,0,0,0,0,0,0,0,0,0.125,0.125,0.125,0.5,0.5,1)))
> colnames(AGRM)<-animal
> rownames(AGRM)<-animal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From cmfoster at iastate.edu  Wed Jul 23 01:43:03 2003
From: cmfoster at iastate.edu (Carol Foster)
Date: Tue, 22 Jul 2003 18:43:03 -0500
Subject: [R] using getBioC()
Message-ID: <p05100300bb43791738fc@[129.186.12.222]>

Hello,
    I am trying to install R/Bioconductor on a G4 Mac running OS X.  I 
have successfully installed R so that a command window opens, but 
installation of the downloaded Bioconductor package is giving me 
trouble.  After copying/pasting the Bioconductor installation script 
in to the window and typing getBioC(), I get the following error 
message.

"Error in getBioC():  R not currently configured to allow HTTP 
connections, which is required for getBioC to work properly."

   Any suggestions would be greatly appreciated.

Sincerely,

Carol Foster


-- 
Carol M. Foster, Ph.D.
Postdoctoral Research Associate
Department of Genetics, Development, and Cell Biology
353 Bessey Hall
Iowa State University
Ames, Iowa  50011  USA
Phone: (515) 294-3509, Fax: (515) 294-1337
Email: cmfoster at iastate.edu



From monkeychump at hushmail.com  Wed Jul 23 02:09:47 2003
From: monkeychump at hushmail.com (monkeychump@hushmail.com)
Date: Tue, 22 Jul 2003 17:09:47 -0700
Subject: [R] Boosting,
	bagging and bumping. Questions about R tools and predictions.
Message-ID: <200307230009.h6N09lQ8056386@mailserver3.hushmail.com>


I'm interested in further understanding the differences in using many
classification trees to improve classification rates. I'm also interested
in finding out what I can do in R and which methods will allow prediction.
Can anybody point me to a citation or discussion?

Specifically, I want to classify remotely sensed imagery where training
data is extracted on class membership by the user. That training data
(usually spectral bands and categorical data - e.g., soil type) is classified
(using rpart for instance) and then the resulting tree is applied to
the entire image. This results in a classified image that can then be
checked for accuracy. Classification trees are increasingly used by the
remote sensing folks but it seems like finding optimal trees is an active
area of research in computational statistics.

I've seen great claims made by baggers and boosters (and just what is
bumping?) of increasing classification accuracy but aside from TreeNet
by Salford Systems I'm not aware of tools that can grow forests of trees
that can then be used to make predictions.

Can anybody help?










Promote security and make money with the Hushmail Affiliate Program:



From kwan022 at stat.auckland.ac.nz  Wed Jul 23 02:22:59 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 23 Jul 2003 12:22:59 +1200 (NZST)
Subject: [R] Boosting, bagging and bumping. Questions about R tools and
	predictions.
In-Reply-To: <200307230009.h6N09lQ8056386@mailserver3.hushmail.com>
Message-ID: <Pine.LNX.4.44.0307231220040.26346-100000@stat55.stat.auckland.ac.nz>

Hi,

If you want to learn the theory of boosting and bagging, and other 
classification techniques, then you will want to refer to Hastie, 
Tibshirani and Friedman's "The Element of Statistical Learning: Data 
Mining, Inference, and Prediction" by Springer.  It is the best book I 
have seen in these areas.

To apply them (or some of the techniques) in R, the book you want to look 
at is Venables and Ripley's MASS 4 (Modern Applied Statistics with S).

On Tue, 22 Jul 2003 monkeychump at hushmail.com wrote:

> Date: Tue, 22 Jul 2003 17:09:47 -0700
> From: monkeychump at hushmail.com
> To: r-help at stat.math.ethz.ch
> Subject: [R] Boosting,
>      bagging and bumping. Questions about R tools and predictions.
> 
> 
> I'm interested in further understanding the differences in using many
> classification trees to improve classification rates. I'm also interested
> in finding out what I can do in R and which methods will allow prediction.
> Can anybody point me to a citation or discussion?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Tom.Mulholland at health.wa.gov.au  Wed Jul 23 04:07:44 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Wed, 23 Jul 2003 10:07:44 +0800
Subject: [R] Boosting,
	bagging and bumping. Questions about R tools and predictions.
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D55C3B@nt207mesep.health.wa.gov.au>

http://www.boosting.org/publications.html I found some of the papers on
this page useful in understanding the concepts you refer to. I will
leave it to the better informed members of the group to talk about the
packages that relate to this field. 

-----Original Message-----
From: monkeychump at hushmail.com [mailto:monkeychump at hushmail.com] 
Sent: Wednesday, 23 July 2003 8:10 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Boosting,bagging and bumping. Questions about R tools and
predictions.



I'm interested in further understanding the differences in using many
classification trees to improve classification rates. I'm also
interested in finding out what I can do in R and which methods will
allow prediction. Can anybody point me to a citation or discussion?


_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
189 Royal St, East Perth, WA, 6004
 
Tel: (08) 9222 4062
e-mail: Tom.Mulholland at health.wa.gov.au
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From Arnaud.Dowkiw at dpi.qld.gov.au  Wed Jul 23 05:25:27 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Wed, 23 Jul 2003 13:25:27 +1000
Subject: [R] pls.pcr compared to Unscrambler
Message-ID: <C2C6EA6C4DADB348BFDF58894B03901201274224@kinsrv001.dpi.qld.gov.au>

Dear R-helpers,

Has anybody ever tried to compare pls regression outputs from the pls.pcr R-package developped by Wehrens with outputs from the Unscrambler software developped by CAMO company ?
I find very different outputs and wonder if this comes from differences between methods/algorithms SIMPLS (pls.pcr) and PLS1 (Unscrambler).

Arnaud

*************************
Arnaud DOWKIW
Department of Primary Industries
J. Bjelke-Petersen Research Station
KINGAROY, QLD 4610
Australia
T : + 61 7 41 600 700
T : + 61 7 41 600 728 (direct)
F : + 61 7 41 600 760
**************************
 

********************************DISCLAIMER******************...{{dropped}}



From d.scott at auckland.ac.nz  Wed Jul 23 06:16:08 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Wed, 23 Jul 2003 16:16:08 +1200 (NZST)
Subject: [R] Package submission---HyperbolicDist
Message-ID: <Pine.LNX.4.44.0307231611280.7534-100000@localhost.localdomain>


I have just uploaded a package for the hyperbolic distribution to CRAN. 

>From the help file:

This library provides a collection of functions for working with
the hyperbolic distribution. Included are the 
density function, distribution function, quantiles, 
random number generation and fitting the hyperbolic distribution
to data (fit.hyperb). The function hyperb.change.pars
will interchange parameter values between different parameterisations.
The mean, variance and mode of a given hyperbolic distribution are
given by hyperb.mean, hyperb.var, and hyperb.mode
respectively. For assessing the fit of the hyperbolic distribution to
a set of data, the log-histogram is useful.

David Scott

_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From ligges at statistik.uni-dortmund.de  Wed Jul 23 08:58:00 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jul 2003 08:58:00 +0200
Subject: [R] Processing a large number of files
In-Reply-To: <6rk7aarz3i.fsf@bates4.stat.wisc.edu>
References: <6rk7aarz3i.fsf@bates4.stat.wisc.edu>
Message-ID: <3F1E31F8.4030901@statistik.uni-dortmund.de>

Douglas Bates wrote:
> I maintain the Devore5 package which contains the data sets from the
> 5th edition of Jay Devore's text "Probability and Statistics for
> Engineering and the Sciences".  The 6th edition has now been published
> and it includes several new data sets in exercises and examples.  In
> addition, some exercises and examples from the 5th edition are
> renumbered in the 6th edition.
> 
> I face the daunting task of adding and documenting the new data sets
> and updating the numbering.  I had thought of going back to the text
> files but discovered that it was easier to work from another form.
> 
> A CD-ROM with the book provides the data sets in several different
> formats, including SPSS saved data sets.  I was pleasantly surprised
> that I could write an R script that read the data from the .sav file,
> converted it to an R data frame, converted the SPSS name such as
> ex01-11.sav to an allowable R name (ex01.11), and saved the resulting
> data set in a new directory.  In the past I would have written Python
> or Perl scripts to do all the manipulations of iterating over files
> but with the current facilities in R for listing file names, etc., I
> can do the whole thing in R.  My script, which worked on the first
> try, is
> 
> library(foreign)
> SPSS = "/cdrom/Manual Install/Datasets/SPSS/"  # change as appropriate
> Rdata = "/tmp/Devore6/data/"            # change as appropriate
> chapters = c("CH01", "CH04", "CH06", "CH07", "CH08", "CH09",
>     "CH10", "CH11", "CH12", "CH13", "CH15", "Ch14", "Ch16")
> for (ch in chapters) {
>     path = paste(SPSS, ch, sep = '')
>     files = list.files(path = path, pattern = '*.sav')
>     for (ff in files) {
>         dsn = gsub('-', '.', gsub('\.sav$', '', ff))
>         assign(dsn, data.frame(read.spss(paste(path, ff, sep = '/'))))
>         save(list = dsn, file = paste(Rdata, dsn, ".rda", sep = ''))
>     }
> }
> 
> In fact this script processed the 326 files so quickly that I thought
> I must have made a mistake and somehow missed most of the files.  I
> had to look in the output directory to convince myself that it had
> indeed run properly.
> 
> I would encourage others to consider using list.files, gsub,
> etc. within R for such scripting applications.

Doug, indeed, it's great. The main part of the current automated script 
files for compiling R binary packages for Windows is done in R including 
processing of files (e.g. checking which of the 2xx CRAN packages has 
been updated) and generation of Windows *.bat files for the final 
processing and upload steps.
In principle, the whole stuff could be done in a single R script (but 
would be more difficult to debug hence not implemented that way).

Uwe

Uwe Ligges



From bill at barnard-engineering.com  Wed Jul 23 09:28:51 2003
From: bill at barnard-engineering.com (Bill Barnard)
Date: Wed, 23 Jul 2003 07:28:51 -0000
Subject: [R] using getBioC()
In-Reply-To: <p05100300bb43791738fc@[129.186.12.222]>
References: <p05100300bb43791738fc@[129.186.12.222]>
Message-ID: <1058945334.31094.26.camel@tioga.barnard-engineering.com>

There was a post in the Bioconductor mailing list regarding the same
error message:

https://stat.ethz.ch/pipermail/bioconductor/2003-February/000854.html

One possible solution mentioned there was to remove the line in
getBioC() that checks the capabilities. Since it seems likely that you
do have access to http from your machine, even if the R version doesn't
know that, then you could try saving the script to a local file, edit it
to change the line that reads:

http <- as.logical(capabilities(what="http/ftp"))

to:
##http <- as.logical(capabilities(what="http/ftp"))
http <- TRUE

then source your local copy of the file, and finally try re-running the
function.

I don't understand why the capability would be detected as FALSE. I
built my version on Linux, but never made any explicit selection of the
capability. I do see some other items listed by the capabilities()
function that I did configure. (It wouldn't be some oddity of the Mac OS
would it?...)

Anyway, I hope this helps.

Cheers,

Bill Barnard

On Tue, 2003-07-22 at 16:43, Carol Foster wrote:
> Hello,
>     I am trying to install R/Bioconductor on a G4 Mac running OS X.  I 
> have successfully installed R so that a command window opens, but 
> installation of the downloaded Bioconductor package is giving me 
> trouble.  After copying/pasting the Bioconductor installation script 
> in to the window and typing getBioC(), I get the following error 
> message.
> 
> "Error in getBioC():  R not currently configured to allow HTTP 
> connections, which is required for getBioC to work properly."
> 
>    Any suggestions would be greatly appreciated.
> 
> Sincerely,
> 
> Carol Foster
>



From ripley at stats.ox.ac.uk  Wed Jul 23 09:40:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 08:40:31 +0100 (BST)
Subject: [R] libblas.so.3
In-Reply-To: <16157.38395.175581.48312@dhcp-63-193.cse.ucsc.edu>
Message-ID: <Pine.LNX.4.44.0307230838380.26690-100000@gannet.stats>

On Tue, 22 Jul 2003, Francisco J Molina wrote:

> 
> I am using linux rehat 9
> When I try to run R I get
> 
> usr/lib/R/bin/R.bin: error while loading shared libraries: libblas.so.3: cannot open shared object file: No such file or directory
> 
> ( because I removed it )
> 
> I thought of compiling R from source but I have read:
> 
> "R currently uses only level 1 blas and the most significant atlas
> optimizations are for level 3 (and level 2 to some extent). 
> The problem with using ATLAS is that its installation process does
> not build the shared libraries by default and the whole build process is
> rather complicated."
> 
> This is from a 3 years old message but I guess that the state of the art now
> is similar.

It's not.  R now makes heavy use of level 3 BLAS.  On RH9 ATLAS should 
build out of the box (with static libraries), and be well worth using.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wl at eimb.ru  Wed Jul 23 10:21:07 2003
From: wl at eimb.ru (Wladimir Eremeev)
Date: Wed, 23 Jul 2003 12:21:07 +0400
Subject: [R] lattice: how to format axis labels?
Message-ID: <15514.030723@eimb.ru>

Dear r-help,

  I draw graphics with xyplot() function.
  Labels on the y axis are appearing as follows: "1.5, 1, 0.5, 0"

  I'd like to have them to be "1.5, 1.0, 0.5, 0.0", i.e. with fixed
  number of digits after the dot (one in this case).

  Is there any way to do this without implicit specifying labels?
  
  And some questions about font.
  
  Unfortunately I cannot find in the documentation how to make the
  axis labels bold.

  What's the difference between fonts 1, 2, 3 and 4?
  I have tried them all (trellis.par.set("par.xlab.text",list(font=4));),
  but haven't seen any difference.

  I use R 1.7.1 on WindowsNT.

  Thank you!
  
-- 
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist                                Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 954-5534



From elsawy at ysbl.york.ac.uk  Wed Jul 23 10:40:34 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Wed, 23 Jul 2003 09:40:34 +0100
Subject: [R] Read trajectory file into R
Message-ID: <3F1E4A02.9DA45D76@ysbl.york.ac.uk>

dear helpers,
I wonder if there is a way to read a molecular dynamic trajectory file (
binary file) produced by CHARMM into R. Something like that in matlab. 
Actually this will save tremendous effort in post processing.
best regards
karim



From ripley at stats.ox.ac.uk  Wed Jul 23 10:49:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 09:49:00 +0100 (BST)
Subject: [R] Read trajectory file into R
In-Reply-To: <3F1E4A02.9DA45D76@ysbl.york.ac.uk>
Message-ID: <Pine.LNX.4.44.0307230946130.26690-100000@gannet.stats>

On Wed, 23 Jul 2003, Karim Elsawy wrote:

> I wonder if there is a way to read a molecular dynamic trajectory file (
> binary file) produced by CHARMM into R. Something like that in matlab. 
> Actually this will save tremendous effort in post processing.

If you know the file format, yes.  That's a main aim of connections and 
function readBin().   Function read.S (in package foreign) is an example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jul 23 10:58:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 23 Jul 2003 08:58:38 -0000
Subject: [R] Curious warning in R for OS X w/Xwindows
In-Reply-To: <2083464E-BC98-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>
References: <2083464E-BC98-11D7-B6FD-003065F6CEFA@wcfia.harvard.edu>
Message-ID: <x2llup39a0.fsf@biostat.ku.dk>

"Bear F. Braumoeller" <bbraumoeller at wcfia.harvard.edu> writes:

> I'm starting R with
> 
>     xterm -sb -rightbar -sl 1000 -bg black -fg blue -title R -e
> /usr/local/bin/R
> 
> -- but it also happens if I just start a vanilla terminal and type R.
> 
> As to the other questions,
> 
>  > Sys.getenv("TERM")
>     TERM
> "xterm"
>  > Sys.getenv("PAGER")
>            PAGER
> "/usr/bin/less"
>  > options("pager")
> $pager
> [1] "/usr/local/lib/R/bin/pager"

And less itself works OK in an xterm?  The above looks perfectly
normal to me. The whole procedure is external to R: R writes a file,
then fires up the pager on it, so it is difficult to imagine that
something in R itself should cause the problem. You said that "man"
works; does that use less as its pager too? "man whatever | less"
might be illuminating. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gavin.simpson at ucl.ac.uk  Wed Jul 23 11:28:27 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 23 Jul 2003 10:28:27 +0100
Subject: [R] Boosting,
	bagging and bumping. Questions about R tools and predictions.
In-Reply-To: <200307230009.h6N09lQ8056386@mailserver3.hushmail.com>
References: <200307230009.h6N09lQ8056386@mailserver3.hushmail.com>
Message-ID: <3F1E553B.1020708@ucl.ac.uk>

Take a look at the randomForest package on CRAN:

randomForest: Breiman's random forest for classification and regression

Classification and regression based on a forest of trees using random 
inputs.
Version:	3.9-6
Depends:	R (>= 1.7.0)
Author:		Fortran original by Leo Breiman and Adele Cutler, R port 		by 
Andy Liaw and Matthew Wiener.
Maintainer:	Andy Liaw <andy_liaw at merck.com>

which has a predict function

HTH

Gav

monkeychump wrote:

> I'm interested in further understanding the differences in using many
> classification trees to improve classification rates. I'm also interested
> in finding out what I can do in R and which methods will allow prediction.
> Can anybody point me to a citation or discussion?
> 
> Specifically, I want to classify remotely sensed imagery where training
> data is extracted on class membership by the user. That training data
> (usually spectral bands and categorical data - e.g., soil type) is classified
> (using rpart for instance) and then the resulting tree is applied to
> the entire image. This results in a classified image that can then be
> checked for accuracy. Classification trees are increasingly used by the
> remote sensing folks but it seems like finding optimal trees is an active
> area of research in computational statistics.
> 
> I've seen great claims made by baggers and boosters (and just what is
> bumping?) of increasing classification accuracy but aside from TreeNet
> by Salford Systems I'm not aware of tools that can grow forests of trees
> that can then be used to make predictions.
> 
> Can anybody help?
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Promote security and make money with the Hushmail Affiliate Program:
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From iwhite at staffmail.ed.ac.uk  Wed Jul 23 11:58:55 2003
From: iwhite at staffmail.ed.ac.uk (iwhite@staffmail.ed.ac.uk)
Date: Wed, 23 Jul 2003 10:58:55 +0100 (BST)
Subject: [R] animal models and lme
In-Reply-To: <a05100303bb43eef9888a@[129.31.3.147]>
Message-ID: <Pine.GSO.4.33.0307231037030.7256-100000@holyrood.ed.ac.uk>

Not convinced that responses so far have addressed the problem. The
model is

y = mu + U + e

where e is a vector of independendent errors with variance ve, and U
is a vector of random effects with covariance matrix va*A, where A is a
known matrix (which we can assume is a correlation matrix). If we know the
ratio (va/ve), this reduces to a GLS problem, but not otherwise. Usually
we have to estimate both ve and va.

======================================
I.White
ICAPB, University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Fax: 0131 650 6564  Tel: 0131 650 5490
E-mail: iwhite at staffmail.ed.ac.uk



From elsawy at ysbl.york.ac.uk  Wed Jul 23 12:39:05 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Wed, 23 Jul 2003 11:39:05 +0100
Subject: [R] Read trajectory file into R
Message-ID: <3F1E65C9.1D57A9C1@ysbl.york.ac.uk>


Prof Brian Ripley wrote:
> 
> On Wed, 23 Jul 2003, Karim Elsawy wrote:
> 
> > I wonder if there is a way to read a molecular dynamic trajectory file (
> > binary file) produced by CHARMM into R. Something like that in matlab.
> > Actually this will save tremendous effort in post processing.
> 
> If you know the file format, yes.  That's a main aim of connections and
> function readBin().   Function read.S (in package foreign) is an example.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


Thanks a lot for your help, actually I do not know the exact file format
at the moment
all what I know is :
"The DCD files (the trajectory files) are single precision binary
FORTRAN files, so are transportable
between computer architectures. They are not, unfortunately,
transportable between big-endian (most workstations) and little endian
(Intel) architectures "

is this enough????

best regards
karim



From adavis at saipan.com  Wed Jul 23 13:06:14 2003
From: adavis at saipan.com (Alan Davis)
Date: Wed, 23 Jul 2003 21:06:14 +1000
Subject: [R] category data (text categories)
Message-ID: <20030723210614.2385514b.adavis@saipan.com>

May I ask for a pointer to documentation of how to use category data
(text categories), which I would like to graph as present/absent at
stations along a transect.  

Thanks for any pointers,

Alan Davis

-- 
adavis at saipan.com                                     1-670-322-6580
    Alan E. Davis,  PMB 30, Box 10006, Saipan, MP 96950-8906, CNMI

I have steadily endeavored to keep my mind free, so as to give up any
hypothesis, however much beloved -- and I cannot resist forming one 
on every subject -- as soon as facts are shown to be opposed to it.  
                                  -- Charles Darwin (1809-1882)

The right to search for truth implies also a duty; one must not
conceal any part of what one has recognized to be true. 
                                  -- Albert Einstein 

As we enjoy great advantages from the inventions of others we should
be glad of an opportunity to serve others by any invention of
ours, and this we should do freely and generously.
                                  -- Benjamin Franklin



From e6p at bigfoot.com  Wed Jul 23 13:48:27 2003
From: e6p at bigfoot.com (DJ)
Date: Wed, 23 Jul 2003 12:48:27 +0100
Subject: [R] multinomial logit discrete choice model
Message-ID: <004501c35110$552cc5b0$0100a8c0@Cybercom>

Hi,

I'm struggling trying to specify a multinomial logit discrete choice model
in R.

Any help and/or code examples appreciated.

I am specifically interested in specifying a model where no universal choice
set exists and each choice set has a variable number of alternatives (one of
which is chosen) see data below.

Many Thanks,

David

PS I have asked earlier but without reply.

Is it because:
    a. it's a stupid question
    b. It's obvious
    c. No one knows the answer


Data:
> Chosen  AttrQ   AttrW  | Choices set 1
> 0             8            0      |
> 1            20         34      |
> 0             7            2      |
>
> 0             5            3      |      set 2
> 0             3            5      |
> 1           25          18      |
> 0            4             9      |
>
> 1          30           12      |      set 3
> 0            2            4       |



From wl at eimb.ru  Wed Jul 23 14:17:09 2003
From: wl at eimb.ru (Wladimir Eremeev)
Date: Wed, 23 Jul 2003 16:17:09 +0400
Subject: [R] lattice: how to format axis labels?
In-Reply-To: <OF3A14F9E2.AD9DB635-ON85256D6C.00415115@convergys.com>
References: <OF3A14F9E2.AD9DB635-ON85256D6C.00415115@convergys.com>
Message-ID: <12678.030723@eimb.ru>

Dear james,

jhcc> check out 'sprintf' for formating in a specific way.

This will not solve the problem.
I will have to specify the argument like labels=(...).
I would like to avoid it.

I wonder if there a key or option to make automatically appearing
labels be formatted in the mentioned way.
I haven't found it in the documentation.

=====================================================
jhcc>   I draw graphics with xyplot() function.
jhcc>   Labels on the y axis are appearing as follows: "1.5, 1, 0.5, 0"

jhcc>   I'd like to have them to be "1.5, 1.0, 0.5, 0.0", i.e. with fixed
jhcc>   number of digits after the dot (one in this case).

jhcc>   Is there any way to do this without implicit specifying labels?


-- 
Best regards
Wladimir Eremeev                                     mailto:wl at eimb.ru

==========================================================================
Research Scientist                                Leninsky Prospect 33,
Space Monitoring & Ecoinformation Systems Sector, Moscow, Russia, 119071,
Institute of Ecology,                             Phone: (095) 135-9972;
Russian Academy of Sciences                       Fax: (095) 954-5534



From Soren.Hojsgaard at agrsci.dk  Wed Jul 23 14:44:16 2003
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 23 Jul 2003 14:44:16 +0200
Subject: [R] Strange behaviour when running R from within Emacs on Winddows
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC105A32@DJFPOST01.djf.agrsci.dk>

Dear R-experts,
I run R in a shell under Emacs on Win2k using ESS. I get the following strange error

> shell("copy c:\\file.txt c:\\newfile.txt")
warning: extra args ignored after 'copy'
Forkert syntaks for kommandoen.
Warning message: 
cmd execution failed with error code 1 in: shell("copy c:\\file.txt c:\\newfile.txt") 

The same problem emerges independently of whether I use a dos or bash as shell! 

However, if I run the shell() thing in the Gui, things work fine and so do they in Rterm

Can anyone help me?
Thanks
S?ren



From laurent.faisnel at ariase.com  Wed Jul 23 14:53:56 2003
From: laurent.faisnel at ariase.com (Laurent Faisnel)
Date: Wed, 23 Jul 2003 14:53:56 +0200
Subject: [R] S3 and S4 classes
Message-ID: <3F1E8564.5070300@ariase.com>

Hi helpers,

I've been programming in R for a few months now but I still have doubts 
about my code - I would like it to be completely S4-compatible. The 
current code works fine but is probably 'unclean'.
I read the interesting article in the last R News and it helped me 
understand the difference on the whole between S3 and S4 classes, but I 
need a practical example. Could anyone point me out what's S3-like in 
the following sample and why it is not fully S4-compatible ? (any other 
comment welcome).

Thanks in advance.
Laurent

-------------------------------------------------------------------------------------------------

# I define a class
setClass("MyClass", representation(mynumber="numeric"));

# the initilization method
setMethod("initialize","MyClass",
          function(.Object)
          {
            .Object at mynumber <- 10;
            return(.Object);
          }
);

# a function that objects of this class have
perform <- function(.Object) UseMethod("perform", .Object);

# the associated method
setMethod("perform","MyClass",
          function(.Object)
          {
            .Object at mynumber <- .Object at mynumber + 10;
            return(.Object);
           }
);



From bates at stat.wisc.edu  Wed Jul 23 16:01:53 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Jul 2003 14:01:53 -0000
Subject: [R] animal models and lme
In-Reply-To: <Pine.GSO.4.33.0307231037030.7256-100000@holyrood.ed.ac.uk>
References: <Pine.GSO.4.33.0307231037030.7256-100000@holyrood.ed.ac.uk>
Message-ID: <6r7k69e3z6.fsf@bates4.stat.wisc.edu>

<iwhite at staffmail.ed.ac.uk> writes:

> Not convinced that responses so far have addressed the problem. The
> model is
> 
> y = mu + U + e
> 
> where e is a vector of independendent errors with variance ve, and U
> is a vector of random effects with covariance matrix va*A, where A is a
> known matrix (which we can assume is a correlation matrix). If we know the
> ratio (va/ve), this reduces to a GLS problem, but not otherwise. Usually
> we have to estimate both ve and va.

Sorry to say that I don't think lme will handle that problem gracefully.



From bates at stat.wisc.edu  Wed Jul 23 16:12:03 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 23 Jul 2003 14:12:03 -0000
Subject: [R] lattice: how to format axis labels?
In-Reply-To: <12678.030723@eimb.ru>
References: <OF3A14F9E2.AD9DB635-ON85256D6C.00415115@convergys.com>
	<12678.030723@eimb.ru>
Message-ID: <6r3cgxe3ig.fsf@bates4.stat.wisc.edu>

High-level control of axes in xyplot is implemented by the scales
argument to xyplot.  You can include components 'at' and 'labels' in
a list given as the scales argument.  See ?xyplot.

Wladimir Eremeev <wl at eimb.ru> writes:

> jhcc> check out 'sprintf' for formating in a specific way.
> 
> This will not solve the problem.
> I will have to specify the argument like labels=(...).
> I would like to avoid it.
> 
> I wonder if there a key or option to make automatically appearing
> labels be formatted in the mentioned way.
> I haven't found it in the documentation.
> 
> =====================================================
> jhcc>   I draw graphics with xyplot() function.
> jhcc>   Labels on the y axis are appearing as follows: "1.5, 1, 0.5, 0"
> 
> jhcc>   I'd like to have them to be "1.5, 1.0, 0.5, 0.0", i.e. with fixed
> jhcc>   number of digits after the dot (one in this case).
> 
> jhcc>   Is there any way to do this without implicit specifying labels?

I don't think so.



From ripley at stats.ox.ac.uk  Wed Jul 23 16:25:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 15:25:26 +0100 (BST)
Subject: [R] Read trajectory file into R
In-Reply-To: <3F1E65C9.1D57A9C1@ysbl.york.ac.uk>
Message-ID: <Pine.LNX.4.44.0307231523540.27992-100000@gannet.stats>

On Wed, 23 Jul 2003, Karim Elsawy wrote:

> 
> Prof Brian Ripley wrote:
> > 
> > On Wed, 23 Jul 2003, Karim Elsawy wrote:
> > 
> > > I wonder if there is a way to read a molecular dynamic trajectory file (
> > > binary file) produced by CHARMM into R. Something like that in matlab.
> > > Actually this will save tremendous effort in post processing.
> > 
> > If you know the file format, yes.  That's a main aim of connections and
> > function readBin().   Function read.S (in package foreign) is an example.
> > 
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> Thanks a lot for your help, actually I do not know the exact file format
> at the moment
> all what I know is :
> "The DCD files (the trajectory files) are single precision binary
> FORTRAN files, so are transportable
> between computer architectures. They are not, unfortunately,
> transportable between big-endian (most workstations) and little endian
> (Intel) architectures "
> 
> is this enough????

Possibly.  readBin can read those (at least on Unix-like OSes), and if you
can look at them some other way you can probably sort out the structure
of the values.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Wed Jul 23 16:25:44 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 23 Jul 2003 07:25:44 -0700 (PDT)
Subject: [R] Read trajectory file into R
In-Reply-To: <3F1E65C9.1D57A9C1@ysbl.york.ac.uk>
Message-ID: <Pine.A41.4.44.0307230720260.205258-100000@homer04.u.washington.edu>

On Wed, 23 Jul 2003, Karim Elsawy wrote:

> Thanks a lot for your help, actually I do not know the exact file format
> at the moment
> all what I know is :
> "The DCD files (the trajectory files) are single precision binary
> FORTRAN files, so are transportable
> between computer architectures. They are not, unfortunately,
> transportable between big-endian (most workstations) and little endian
> (Intel) architectures "
>
> is this enough????

Well, that's enough to get the numbers into R.  You then will have to work
out what they mean.

readBin(connection, numeric(), size=4, n=whatever)

will read `whatever' Fortran single precision numbers from `connection'.
If you are doing this on the machine where the file was generated then you
don't need to worry about endianness.  On a different machine (eg moving
from a Sparc to a PC) you may need to add endian="swap".

Look at readBin for more information.

	-thomas



From spencer.graves at pdf.com  Wed Jul 23 16:50:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 23 Jul 2003 07:50:26 -0700
Subject: [R] animal models and lme
References: <Pine.GSO.4.33.0307231037030.7256-100000@holyrood.ed.ac.uk>
	<6r7k69e3z6.fsf@bates4.stat.wisc.edu>
Message-ID: <3F1EA0B2.6090907@pdf.com>

	  If you can solve the problem for fixed rho = (va/ve) using gls, then 
you can call gls for many values of rho, plot the log(likelihood) 
contours vs. rho, construct confidence intervals, etc.  You may even be 
able to write a function to return (-2)*log(likelihood) for a fixed rho 
and then  use "optim" to minimize that "deviance".  [I would suspect 
that the log(likelihood) might look more parabolic in terms of log(rho) 
that in terms of rho itself.  In addition, "optim" might work better 
with the minimum for log.rho = (-Inf) than with a lower bound for rho at 0.]

hope this helps.  spencer graves

Douglas Bates wrote:
> <iwhite at staffmail.ed.ac.uk> writes:
> 
> 
>>Not convinced that responses so far have addressed the problem. The
>>model is
>>
>>y = mu + U + e
>>
>>where e is a vector of independendent errors with variance ve, and U
>>is a vector of random effects with covariance matrix va*A, where A is a
>>known matrix (which we can assume is a correlation matrix). If we know the
>>ratio (va/ve), this reduces to a GLS problem, but not otherwise. Usually
>>we have to estimate both ve and va.
> 
> 
> Sorry to say that I don't think lme will handle that problem gracefully.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dmurdoch at pair.com  Wed Jul 23 16:55:59 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 23 Jul 2003 10:55:59 -0400
Subject: [R] S3 and S4 classes
In-Reply-To: <3F1E8564.5070300@ariase.com>
References: <3F1E8564.5070300@ariase.com>
Message-ID: <858thvgk9b6u4s8d4t0g86rlutj9h9bpfj@4ax.com>

On Wed, 23 Jul 2003 14:53:56 +0200, Laurent Faisnel
<laurent.faisnel at ariase.com> wrote :
>Could anyone point me out what's S3-like in 
>the following sample and why it is not fully S4-compatible ?

># a function that objects of this class have
>perform <- function(.Object) UseMethod("perform", .Object);

It think this is unnecessary, and somewhat S3-like.  A more S4-looking
way to do the same (?) thing is

setGeneric("perform", function(.Object) standardGeneric("perform"))

but I think this will be generated automatically when you define your
methods.

Duncan Murdoch



From flom at ndri.org  Wed Jul 23 16:54:49 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 23 Jul 2003 10:54:49 -0400
Subject: [R] Condition indexes and variance inflation factors
Message-ID: <sf1e6993.052@MAIL.NDRI.ORG>

Has anyone programmed condition indexes in R?

I know that there is a function for variance inflation factors
available in the car package; however, Belsley (1991) Conditioning
Diagnostics (Wiley) notes that there are several weaknesses of VIFs:
e.g. 1) High VIFs are sufficient but not necessary conditions for
collinearity  2) VIFs don't diagnose the number of collinearities and 3)
No one has determined how high a VIF has to be for the collinearity to
be damaging.

He then develops and suggests using condition indexes instead, so I was
wondering if anyone had programmed them.

Thanks

Peter



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ligges at statistik.uni-dortmund.de  Wed Jul 23 17:35:17 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jul 2003 17:35:17 +0200
Subject: [R] Condition indexes and variance inflation factors
In-Reply-To: <sf1e6993.052@MAIL.NDRI.ORG>
References: <sf1e6993.052@MAIL.NDRI.ORG>
Message-ID: <3F1EAB35.3050601@statistik.uni-dortmund.de>

Peter Flom wrote:

> Has anyone programmed condition indexes in R?
> 
> I know that there is a function for variance inflation factors
> available in the car package; however, Belsley (1991) Conditioning
> Diagnostics (Wiley) notes that there are several weaknesses of VIFs:
> e.g. 1) High VIFs are sufficient but not necessary conditions for
> collinearity  2) VIFs don't diagnose the number of collinearities and 3)
> No one has determined how high a VIF has to be for the collinearity to
> be damaging.
> 
> He then develops and suggests using condition indexes instead, so I was
> wondering if anyone had programmed them.
> 
> Thanks
> 
> Peter


I think Juergen Gross has something like that in his new book
Gross, J. (2003): Linear Regression, Springer (in press - OK, not very 
helpful here).

You might want to contact him privately (in CC).

Uwe Ligges


> 
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Jul 23 17:56:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 16:56:47 +0100 (BST)
Subject: [R] Strange behaviour when running R from within Emacs on Winddows
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC105A32@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.44.0307231654270.27992-100000@gannet.stats>

On Wed, 23 Jul 2003, S?ren H?jsgaard wrote:

> Dear R-experts,
> I run R in a shell under Emacs on Win2k using ESS. I get the following strange error
> 
> > shell("copy c:\\file.txt c:\\newfile.txt")
> warning: extra args ignored after 'copy'
> Forkert syntaks for kommandoen.
> Warning message: 
> cmd execution failed with error code 1 in: shell("copy c:\\file.txt c:\\newfile.txt") 
> 
> The same problem emerges independently of whether I use a dos or bash as shell! 
> 
> However, if I run the shell() thing in the Gui, things work fine and so do they in Rterm
> 
> Can anyone help me?

I suspect Emacs/ESS has set the SHELL variable: you can ask shell to use a 
specific shell via its second argument or by setting R_SHELL.

bash won't work, as `copy' is a DOS internal command.

Why don't you just use file.copy()?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Wed Jul 23 18:06:09 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jul 2003 12:06:09 -0400
Subject: [R] Condition indexes and variance inflation factors
Message-ID: <3A822319EB35174CA3714066D590DCD50205C903@usrymx25.merck.com>

I'm under the impression that condition index is just the ratio of the
maximum singular value to the minimum singular value.  So just by doing
eigen() or svd() on the design matrix (which you can get via, e.g.,
model.matrix) ought to be sufficient.  The number of near-zero eigen values
(and their corresponding eigenvectors) tell you the number (and nature) of
the collinearity.

HTH,
Andy

> -----Original Message-----
> From: Peter Flom [mailto:flom at ndri.org] 
> Sent: Wednesday, July 23, 2003 10:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Condition indexes and variance inflation factors
> 
> 
> Has anyone programmed condition indexes in R?
> 
> I know that there is a function for variance inflation 
> factors available in the car package; however, Belsley (1991) 
> Conditioning Diagnostics (Wiley) notes that there are several 
> weaknesses of VIFs: e.g. 1) High VIFs are sufficient but not 
> necessary conditions for collinearity  2) VIFs don't diagnose 
> the number of collinearities and 3) No one has determined how 
> high a VIF has to be for the collinearity to be damaging.
> 
> He then develops and suggests using condition indexes 
> instead, so I was wondering if anyone had programmed them.
> 
> Thanks
> 
> Peter
> 
> 
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From zhuw at mail.smu.edu  Wed Jul 23 18:10:18 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Wed, 23 Jul 2003 11:10:18 -0500
Subject: [R] Spectral Analysis
Message-ID: <3F1EB36A.4000507@mail.smu.edu>

Take a look at
http://onb.ent.psu.edu/onb1/
Look at the bottom of Software R/S-code.

I believe spec.lomb is not a default package/function in Splus, that 
explains why you can not use spec.lomb unless you install the package "nlt".

Hope this helps.



From dave at evocapital.com  Wed Jul 23 18:17:41 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 23 Jul 2003 17:17:41 +0100
Subject: [R] Passing references to data objects into R functions
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063E1F@mail.internal.net>

Hi. 

I have the following question about reading from large data objects from
within R functions; I have tried to simplify my problem as much as
possible in what follows.

Imagine I have various large data objects sitting in my global
environment (call them "data1", "data2", ...).  I want to write a
function "extract" that extracts some of the rows of a particular data
object, does some further manipulations on the extract and then returns
the result. The function takes the data object's name and an index
vector -- for example the following call would return the first 3 rows
of object data1. 

ans = extract("data1", 1:3)

I could write a simple function like this:

extract1 = function(object.name, index) {

    temp = get(object.name, envir = .GlobalEnv)
    temp = temp[index, , drop=FALSE]

    # do some further manipulations here ....

    return(temp)

}

The problem is that the function makes a copy "temp" of the object in
the function frame, which (in my application) is very memory inefficient
as the data objects are very large. It is especially inefficient when
the length of the "index" vector is much smaller than the number of rows
in the data object. What I really would like to do is to be able to read
from the underlying data object directly (in other programming languages
this would be achieved by passing a pointer to the object instead),
without making a copy.

Given the rules of variable name scoping in R, I could avoid making a
copy with the following call:

extract2 = function(object.name, index) {

    eval(parse(text = "temp = ", object.name, "[index, , drop=FALSE]",
sep=""))
    # do some further manipulations here ....

    return(temp)
}

But this seems very messy. Is there a better way?

Thanks for your help

David Khabie-Zeitoune



From dfs at research.att.com  Wed Jul 23 19:56:22 2003
From: dfs at research.att.com (Deborah Swayne)
Date: Wed, 23 Jul 2003 13:56:22 -0400 (EDT)
Subject: [R] odd behavior with the maps package
Message-ID: <200307231756.NAA95663@fry.research.att.com>


Has anyone else seen this behavior from the "maps" package?

map('state', fill=TRUE)

results in a lively mix of overlapping polygons inside a
map of the US, but they have no obvious relationship to
state boundaries.  (See attached jpeg.)

-------------- next part --------------

I reinstalled the maps and mapdata packages from
   ftp://ftp.mcs.vuw.ac.nz/pub/statistics/map/
to see it that would help, but it doesn't.

Since our platform is an SGI (running R-1.8.0, development
version, June 25), I made sure to use mapget.c.notlinux,
and that didn't help either.

Any thoughts?  Is there something I'm missing?

Debby

From dfs at research.att.com  Wed Jul 23 20:03:26 2003
From: dfs at research.att.com (Deborah Swayne)
Date: Wed, 23 Jul 2003 14:03:26 -0400
Subject: [R] trouble with maps
Message-ID: <16158.52718.544322.106058@fry.research.att.com>

Has anyone else seen this behavior from the "maps" package?
    
map('state', fill=TRUE)

results in a lively mix of overlapping polygons inside a
map of the US, but they have no obvious relationship to
state boundaries.  (See attached jpeg.)

-------------- next part --------------

I reinstalled the maps and mapdata packages from
   ftp://ftp.mcs.vuw.ac.nz/pub/statistics/map/  
to see it that would help, but it doesn't.

Since our platform is an SGI (running R-1.8.0, development
version, June 25), I made sure to use mapget.c.notlinux,  
and that didn't help either.

Any thoughts?  Is there something I'm missing?

Debby

[Apologies if r-help receives this twice; I'm learning to
use a new mailer.]

From bbraumoeller at wcfia.harvard.edu  Wed Jul 23 20:05:08 2003
From: bbraumoeller at wcfia.harvard.edu (Bear F. Braumoeller)
Date: Wed, 23 Jul 2003 14:05:08 -0400
Subject: [R] Curious warning in R for OS X w/Xwindows
In-Reply-To: <x2llup39a0.fsf@biostat.ku.dk>
Message-ID: <31E17524-BD38-11D7-A881-003065F6CEFA@wcfia.harvard.edu>


On Wednesday, July 23, 2003, at 05:03 AM, Peter Dalgaard BSA wrote:

> "Bear F. Braumoeller" <bbraumoeller at wcfia.harvard.edu> writes:
>
>> As to the other questions,
>>
>>> Sys.getenv("TERM")
>>     TERM
>> "xterm"
>>> Sys.getenv("PAGER")
>>            PAGER
>> "/usr/bin/less"
>>> options("pager")
>> $pager
>> [1] "/usr/local/lib/R/bin/pager"
>
> And less itself works OK in an xterm?  The above looks perfectly
> normal to me. The whole procedure is external to R: R writes a file,
> then fires up the pager on it, so it is difficult to imagine that
> something in R itself should cause the problem. You said that "man"
> works; does that use less as its pager too? "man whatever | less"
> might be illuminating.


I thought of that after I wrote, and I ran it through its paces -- less 
works like a charm.  I also piped the output specifically through 
/usr/bin/less in case (for some odd reason) the terminal was defaulting 
to a different copy of less than R was.  Still works just fine.

The only thing that I can see that might (??) be causing trouble is in 
/usr/local/lib/R/bin/pager, which reads

#!/bin/sh
## For the curious: "pager $1" doesn't work in batch, because "more" 
will
## eat the rest of stdin.  The no-argument version is intended for use 
at
## the end of a pipeline.
##
## PAGER is determined at configure time and recorded in `etc/Renviron'.
if test -n "${1}"; then
   exec ${PAGER} < ${1}
else
   exec ${PAGER}
fi

### Local Variables: ***
### mode: sh ***
### sh-indentation: 2 ***
### End: ***


-- but I don't know enough about how R is calling the pager to know 
what this is doing.


Bear F. Braumoeller
Assistant Professor
Department of Government
Harvard University
http://www.people.fas.harvard.edu/~bfbraum



From Benjamin.STABLER at odot.state.or.us  Wed Jul 23 20:09:17 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 23 Jul 2003 11:09:17 -0700
Subject: [R] paste and NAs
Message-ID: <76A000A82289D411952F001083F9DD06047FE189@exsalem4-bu.odot.state.or.us>

I understand how R treats NAs but in the situation below it would be nice to
have a na.skip argument to paste so it does not convert the NAs to "NA"s and
simply skips those elements of the vector for pasting.  

> x
[1] "2.13" "2.3"  NA     NA     "2.83" NA    

> paste(x, "0", sep="")
[1] "2.130" "2.30"  "NA0"   "NA0"   "2.830" "NA0"  

With na.skip argument:

paste(x, "0", sep="", na.skip=T)
[1] "2.130" "2.30"  NA   NA   "2.830" NA  

Otherwise I will use:

y <- paste(x, "0", sep="")
gsub(paste("NA","0",sep=""),"", y)

"" is not the same as NA, but for my purposes "" is close enough.  Why can't
I use NA as a replacement in gsub, even though c("a", NA) works?

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From Jo.Hardin at pomona.edu  Wed Jul 23 20:42:16 2003
From: Jo.Hardin at pomona.edu (Johanna Hardin)
Date: Wed, 23 Jul 2003 11:42:16 -0700
Subject: [R] .ps files in R
Message-ID: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030723/f5f9be11/attachment.pl

From tlumley at u.washington.edu  Wed Jul 23 20:49:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 23 Jul 2003 11:49:47 -0700 (PDT)
Subject: [R] paste and NAs
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE189@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.43.0307231149470.16927@hymn12.u.washington.edu>

On Wed, 23 Jul 2003 Benjamin.STABLER at odot.state.or.us wrote:

> I understand how R treats NAs but in the situation below it would be nice to
> have a na.skip argument to paste so it does not convert the NAs to "NA"s and
> simply skips those elements of the vector for pasting.  
> 
> > x
> [1] "2.13" "2.3"  NA     NA     "2.83" NA    
> 
> > paste(x, "0", sep="")
> [1] "2.130" "2.30"  "NA0"   "NA0"   "2.830" "NA0"  
> 
> With na.skip argument:
> 
> paste(x, "0", sep="", na.skip=T)
> [1] "2.130" "2.30"  NA   NA   "2.830" NA  

How about
   ifelse(is.na(x), NA, paste(x,"0",sep=""))

> Otherwise I will use:
> 
> y <- paste(x, "0", sep="")
> gsub(paste("NA","0",sep=""),"", y)
> 
> "" is not the same as NA, but for my purposes "" is close enough.  Why can't
> I use NA as a replacement in gsub, even though c("a", NA) works?
> 


You need to use a character NA. Plain NA is a logical. SO
gsub("NA0",as.character(NA), y)
will work.  This is arguably a bug.

      -thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jerome at hivnet.ubc.ca  Wed Jul 23 20:55:01 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 23 Jul 2003 11:55:01 -0700
Subject: [R] .ps files in R
In-Reply-To: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
References: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
Message-ID: <200307231901.MAA11148@hivnet.ubc.ca>


You can specify the postscript() option horizontal=F.
See help on "horizontal" in ?postscript.

HTH,
Jerome

On July 23, 2003 11:42 am, Johanna Hardin wrote:
> I have recently "printed" in R to a postscript file.  I'm working on a
> SSH
>
> without an X terminal.  It was fairly automatic:
> > plot(x,y)
> > dev.off()
>
> And then the default creates a file called Rplots.ps which I can ftp to
> my laptop and open in Ghostscript.  I can see the file, and nothing
> looks odd. However, when I import it into LaTeX, it refuses to configure
> right side up. (It stays 90 degrees.)  I've tried saving it as .eps with
> different options in ghostscript.  I've also tried many different
> rotating commands in LaTeX (angle in \includegraphics, \rotate,
> \sideways,...) But, the picture seems to be unaffected by any of these
> commands.
>
> Does anyone know a trick to getting R postscript files into LaTeX?
>
> Thanks, Jo
>
> Johanna Hardin
> Department of Mathematics & Computer Science
> 610 N. College Way
> Pomona College
> Claremont, CA 91711
> (909) 607-8717
> jo.hardin at pomona.edu
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Jul 23 20:56:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 19:56:36 +0100 (BST)
Subject: [R] .ps files in R
In-Reply-To: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
Message-ID: <Pine.LNX.4.44.0307231950110.1347-100000@gannet.stats>

Did you read ?postscript ?  You want an encapsulated PS (EPS) file to
include in LaTeX, and that page tells you the options required.

On Wed, 23 Jul 2003, Johanna Hardin wrote:

> I have recently "printed" in R to a postscript file.  I'm working on a SSH
> without an X terminal.  It was fairly automatic:
> 
> > plot(x,y)
> > dev.off()
> 
> And then the default creates a file called Rplots.ps which I can ftp to my
> laptop and open in Ghostscript.  I can see the file, and nothing looks odd.
> However, when I import it into LaTeX, it refuses to configure right side up.
> (It stays 90 degrees.)  I've tried saving it as .eps with different options
> in ghostscript.  I've also tried many different rotating commands in LaTeX
> (angle in \includegraphics, \rotate, \sideways,...) But, the picture seems
> to be unaffected by any of these commands.  
> 
> Does anyone know a trick to getting R postscript files into LaTeX?
> 
> Thanks, Jo
> 
> Johanna Hardin
> Department of Mathematics & Computer Science
> 610 N. College Way
> Pomona College
> Claremont, CA 91711
> (909) 607-8717
> jo.hardin at pomona.edu
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kwan022 at stat.auckland.ac.nz  Wed Jul 23 21:03:18 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 24 Jul 2003 07:03:18 +1200 (NZST)
Subject: [R] .ps files in R
In-Reply-To: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
Message-ID: <Pine.LNX.4.44.0307240658240.31234-100000@stat55.stat.auckland.ac.nz>

On Wed, 23 Jul 2003, Johanna Hardin wrote:

> in ghostscript.  I've also tried many different rotating commands in LaTeX
> (angle in \includegraphics, \rotate, \sideways,...) But, the picture seems
> to be unaffected by any of these commands.  

I find it the rotation won't show up in the DVI file, but will once you 
convert the DVI file into a PS file.  But that's another story.

> Does anyone know a trick to getting R postscript files into LaTeX?

When I need to generate a PS file in R I always do something like:
  postscript("foo.eps", height = 6.9, width = 6.6,
             horizontal = FALSE, onefile = FALSE, print.it = FALSE)
  plot(1:10)
  dev.off()
then in my LaTeX file do something like:
  \begin{figure}[h!]
    \centering
    \begin{center}
      \includegraphics[width = .8\textwidth]{foo.eps}
    \end{center}
    \caption{My Caption}
    \label{fig:foo}
  \end{figure}

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Roger.Bivand at nhh.no  Wed Jul 23 21:09:05 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 23 Jul 2003 21:09:05 +0200 (CEST)
Subject: [R] trouble with maps
In-Reply-To: <16158.52718.544322.106058@fry.research.att.com>
Message-ID: <Pine.LNX.4.44.0307232042450.24717-100000@reclus.nhh.no>

On Wed, 23 Jul 2003, Deborah Swayne wrote:

> Has anyone else seen this behavior from the "maps" package?
>     
> map('state', fill=TRUE)
> 
> results in a lively mix of overlapping polygons inside a
> map of the US, but they have no obvious relationship to
> state boundaries.  (See attached jpeg.)

Yes, this replicates. I think the trouble is in the part of map() where it 
makes a polygon of the lines:

    if (fill) {
        gonsize <- line$size
        color <- rep(color, length = length(gonsize))
        keep <- !is.na(color)
        coord[c("x", "y")] <- makepoly(coord, gonsize, keep)
        color <- color[keep]
    }

and I think makepoly needs gon, not gonsize, as an argument, to stitch the 
boundary lines together in the correct order - the "interesting" effect 
seems to come from some lines not being reversed. Unfortunately, I don't 
have archival copies of earlier map() functions to check this - it could 
also be in mapgetl():

        if (fill) 
            coord <- mapgetl(database, line$number, xlim, ylim)

although this is less likely, because the same function is used to 
retrieve the same data when fill=FALSE too. Something has got lost in 
building the polygons, it seems! As far as I can establish, fill=TRUE did 
work in earlier versions.

> p <- map('state', region=c('penn'), resolution=0)
> plot(p, type="l")

gives the boundaries in both cases,

> pf <- map('state', region=c('penn'), fill=TRUE, resolution=0)

gives black/white "interesting" polygons, and

> plot(pf, type="l")

draws the boundary lines with wrong links to next line segents.

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From mmiller3 at iupui.edu  Wed Jul 23 21:36:20 2003
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Wed, 23 Jul 2003 14:36:20 -0500
Subject: [R] .ps files in R
In-Reply-To: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
	(Johanna Hardin's message of "Wed, 23 Jul 2003 11:42:16 -0700")
References: <5B526011BAC3D511BFBA00B0D020615A0327399D@excsrv-acd1.pomona.edu>
Message-ID: <87smoxxcgb.fsf@lumen.indyrad.iupui.edu>

You can include in your latex with \includegraphics[angle=90]{foo.eps}



From jmc at research.bell-labs.com  Wed Jul 23 21:45:36 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Wed, 23 Jul 2003 15:45:36 -0400
Subject: [R] S3 and S4 classes
References: <3F1E8564.5070300@ariase.com>
	<858thvgk9b6u4s8d4t0g86rlutj9h9bpfj@4ax.com>
Message-ID: <3F1EE5E0.6F3F5AE3@research.bell-labs.com>

Duncan Murdoch wrote:
> 
> On Wed, 23 Jul 2003 14:53:56 +0200, Laurent Faisnel
> <laurent.faisnel at ariase.com> wrote :
> >Could anyone point me out what's S3-like in
> >the following sample and why it is not fully S4-compatible ?
> 
> ># a function that objects of this class have
> >perform <- function(.Object) UseMethod("perform", .Object);
> 
> It think this is unnecessary, and somewhat S3-like.  A more S4-looking
> way to do the same (?) thing is
> 
> setGeneric("perform", function(.Object) standardGeneric("perform"))
> 
> but I think this will be generated automatically when you define your
> methods.

As Duncan says, this is the S3-style portion of the example.  It's not
wrong, but there are advantages to NOT going this route.

The UseMethod() call says that this is a function with S3-style
methods.  Is that true?  It might well be--you could have a function
perform.default, for example, that was the default method to use.

The disadvantage of hanging on to S3 methods is that they're hidden;
unlike S4 methods, you can't easily find out what methods are defined
(by calling showMethods()).

If you don't  have any existing definition of perform(), you will need
to call setGeneric() as Duncan showed.  The implication is that
perform() doesn't have a default method--unless the argument inherits
from one of the classes in a setMethod() call, the result is an error. 
(If there is a non-generic version of perform, that becomes the default
method, as it would in your example.)

If you DID have a perform.default, you might want to make that
explicitly the S4 default method
  setMethod("perform", "ANY", perform.default)
after the setGeneric call.  Similarly, you could make other S3 methods
into S4 methods.  Then all the methods are visible.

Also, a point of good style, unrelated to methods.  It's not generally a
good idea to have function arguments starting with ".".  Names of this
form are intended for behind-the-scenes manipulations.  By sticking to
names that start with a letter, you avoid the chance of conflicting with
some such manipulation.  So, "Object" rather than ".Object".  (The
reason intialize() uses .Object is exactly BECAUSE it expects
user-defined arguments, in the "...", to start with a letter, and so
chooses .Object to minimize the chance of conflicting.)

Regards,
 John Chambers
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From jfox at mcmaster.ca  Wed Jul 23 21:52:54 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 23 Jul 2003 15:52:54 -0400
Subject: [R] Condition indexes and variance inflation factors
In-Reply-To: <3F1EAB35.3050601@statistik.uni-dortmund.de>
References: <sf1e6993.052@MAIL.NDRI.ORG>
 <sf1e6993.052@MAIL.NDRI.ORG>
Message-ID: <5.1.0.14.2.20030723153355.01f894e0@127.0.0.1>

Dear Peter and Uwe,

I don't have a copy of Belsley's 1991 book here, but I do have Belsley, 
Kuh, and Welsch, Regression Diagnostics (Wiley, 1980). If my memory is 
right, the approach is the same: Belsley's collinearity diagnostics are 
based on a singular-value decomposition of the scaled but uncentred model 
matrix. A straightforward, if inelegant, rendition is

belsley <- function(model){
     X <- model.matrix(model)
     X <- scale(X, center=FALSE)/sqrt(nrow(X) - 1)
     svd.X <- svd(X)
     result <- list(singular.values = svd.X$d, condition.indices = 
max(svd.X$d)/svd.X$d)
     phi <- sweep(svd.X$v^2, 2, svd.X$d^2, "/")
     Pi <- t(sweep(phi, 1, rowSums(phi), "/"))
     colnames(Pi) <- names(coef(model))
     rownames(Pi) <- 1:nrow(Pi)
     result$pi <- Pi
     class(result) <- "belsley"
     result
     }

print.belsley <- function(x, digits = 3, ...){
     cat("\nSingular values: ", x$singular.values)
     cat("\nCondition indices: ", x$condition.indices)
     cat("\n\nVariance-decomposition proportions\n")
     print(round(x$pi, digits))
     invisible(x)
     }

This gives the singular values, condition indices, and 
variance-decomposition proportions. (I'm pretty sure that you can get the 
same thing more elegantly from the qr decomposition, but I don't know how 
off the top of my head -- someone else on the list doubtless can supply the 
details.)

For example, for the illustration on p. 161 of BKW,

 > X
    V1  V2  V3     V4     V5
1 -74  80  18    -56   -112
2  14 -69  21     52    104
3  66 -72  -5    764   1528
4 -12  66 -30   4096   8192
5   3   8  -7 -13276 -26552
6   4 -12   4   8421  16842
 > mod <- lm(y ~ X - 1)  # nb., y was just randomly generated
 > belsley(mod)

Singular values:  1.414214 1.361734 1.066707 0.08840437 3.614479e-17
Condition indices:  1 1.038538 1.325775 15.9971 3.912635e+16

Variance-decomposition proportions
     XV1   XV2   XV3 XV4 XV5
1 0.000 0.000 0.000   0   0
2 0.005 0.005 0.000   0   0
3 0.001 0.001 0.047   0   0
4 0.994 0.994 0.953   0   0
5 0.000 0.000 0.000   1   1

which is in good agreement with the values given in the text.

Now some comments:

(1) I've never liked this approach for a model with a constant, where it 
makes more sense to me to centre the data. I realize that opinions differ 
here, but it seems to me that failing to centre the data conflates 
collinearity with numerical instability.

(2) I also disagree with the comment that condition indices are easier to 
interpret than variance-inflation factors. In either case, since 
collinearity is a continuous phenomenon, cutoffs for large values are 
necessarily arbitrary.

(3) If you're interested in figuring out which variables are involved in 
each collinear relationship, then (for centred and scaled data) you can 
equivalently (and to me, more intuitively) work with the 
principal-components analysis of the predictors.

(4) I have doubts about the whole enterprise. Collinearity is one source of 
imprecision -- others are small sample size, homogeneous predictors, and 
large error variance. Aren't the coefficient standard errors the bottom 
line? If these are sufficiently small, why worry?

I hope that this helps.

John

At 05:35 PM 7/23/2003 +0200, Uwe Ligges wrote:
>Peter Flom wrote:
>
>>Has anyone programmed condition indexes in R?
>>I know that there is a function for variance inflation factors
>>available in the car package; however, Belsley (1991) Conditioning
>>Diagnostics (Wiley) notes that there are several weaknesses of VIFs:
>>e.g. 1) High VIFs are sufficient but not necessary conditions for
>>collinearity  2) VIFs don't diagnose the number of collinearities and 3)
>>No one has determined how high a VIF has to be for the collinearity to
>>be damaging.
>>He then develops and suggests using condition indexes instead, so I was
>>wondering if anyone had programmed them.
>>Thanks
>>Peter
>
>
>I think Juergen Gross has something like that in his new book
>Gross, J. (2003): Linear Regression, Springer (in press - OK, not very 
>helpful here).
>
>You might want to contact him privately (in CC).
>
>Uwe Ligges
>

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From Benjamin.STABLER at odot.state.or.us  Wed Jul 23 22:00:45 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 23 Jul 2003 13:00:45 -0700
Subject: [R] paste and NAs
Message-ID: <76A000A82289D411952F001083F9DD06047FE18A@exsalem4-bu.odot.state.or.us>

I didn't realize that ifelse is vectorized.  Thanks for the suggestion.

>-----Original Message-----
>From: Thomas Lumley [mailto:tlumley at u.washington.edu]
>Sent: Wednesday, July 23, 2003 11:50 AM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch; GREGOR Brian J
>Subject: Re: [R] paste and NAs
>
>
>On Wed, 23 Jul 2003 Benjamin.STABLER at odot.state.or.us wrote:
>
>> I understand how R treats NAs but in the situation below it 
>would be nice to
>> have a na.skip argument to paste so it does not convert the 
>NAs to "NA"s and
>> simply skips those elements of the vector for pasting.  
>> 
>> > x
>> [1] "2.13" "2.3"  NA     NA     "2.83" NA    
>> 
>> > paste(x, "0", sep="")
>> [1] "2.130" "2.30"  "NA0"   "NA0"   "2.830" "NA0"  
>> 
>> With na.skip argument:
>> 
>> paste(x, "0", sep="", na.skip=T)
>> [1] "2.130" "2.30"  NA   NA   "2.830" NA  
>
>How about
>   ifelse(is.na(x), NA, paste(x,"0",sep=""))
>
>> Otherwise I will use:
>> 
>> y <- paste(x, "0", sep="")
>> gsub(paste("NA","0",sep=""),"", y)
>> 
>> "" is not the same as NA, but for my purposes "" is close 
>enough.  Why can't
>> I use NA as a replacement in gsub, even though c("a", NA) works?
>> 
>
>
>You need to use a character NA. Plain NA is a logical. SO
>gsub("NA0",as.character(NA), y)
>will work.  This is arguably a bug.
>
>      -thomas
>
>
>Thomas Lumley			Assoc. Professor, Biostatistics
>tlumley at u.washington.edu	University of Washington, Seattle
>
>



From jg_liao at yahoo.com  Wed Jul 23 22:44:11 2003
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 23 Jul 2003 13:44:11 -0700 (PDT)
Subject: [R] Dismal R performance of Athlon moble CPU?
Message-ID: <20030723204411.84725.qmail@web10505.mail.yahoo.com>

I have been using a laptop computer of Pentium III 1.13 Ghz. I heard
that AMD's Athlon has excellent floating point capacity. So I bought a
Athlon 2200+ laptop yesterday. I expected that new Athlon 2200+ will be
twice as fast as the P III 1.13 GB. I ran a R simulation program and
the new computer is only 30% faster, in fact slightly slower than a
Celeron 1.50 GB laptop. I am very disappointed by this. What is your
experience with Athlon? Should I stick to Intel in the future? Thanks.

By the way, the OS is Windows XP home edtion.

Jason

=====
Jason G. Liao, Ph.D.
Division of Biometrics
University of Medicine and Dentistry of New Jersey
335 George Street, Suite 2200
New Brunswick, NJ 08903-2688
phone (732) 235-8611, fax (732) 235-9777
http://www.geocities.com/jg_liao



From ripley at stats.ox.ac.uk  Wed Jul 23 22:54:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jul 2003 21:54:44 +0100 (BST)
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <20030723204411.84725.qmail@web10505.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0307232147490.1576-100000@gannet.stats>

On Wed, 23 Jul 2003, Jason Liao wrote:

> I have been using a laptop computer of Pentium III 1.13 Ghz. I heard
> that AMD's Athlon has excellent floating point capacity. So I bought a
> Athlon 2200+ laptop yesterday. I expected that new Athlon 2200+ will be
> twice as fast as the P III 1.13 GB. I ran a R simulation program and
> the new computer is only 30% faster, in fact slightly slower than a
> Celeron 1.50 GB laptop. I am very disappointed by this. What is your
> experience with Athlon? Should I stick to Intel in the future? Thanks.

So I expect you think a P4M 1.4GHz (on which I am writing this) should be
a lot faster than a PIII 1GHz?  It is often slower.  Don't compare laptop
chips with desktop ones, nor different chip families (an Athlon 2200 is
not 2.2GHz, BTW).  PIIIs seem the fastest per GHz, but they don't do many
GHz.

I am rather pleased with my dual Athlon 2600, but then P4's don't allow 
multiprocessors and the machine with dual Athlons was cheaper than a 
comparable one with a single 2.4GHz P4.

You have tuned an ATLAS implementation to your CPU, I take it?  If not, 
that's the first step to optimal R performance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Wed Jul 23 23:15:01 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jul 2003 17:15:01 -0400
Subject: [R] Dismal R performance of Athlon moble CPU?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C912@usrymx25.merck.com>

Overall performance depends on a few other things besides CPU clock speed
(e.g., RAM speed and size, cache size, disk speed, etc.)  Unless your code
is spending great majority of the time in the CPU, you should not expect
speed-up to be equal to ratio of clock speeds.  (Also, as Prof. Ripley
pointed out, a P4 does less than a PIII at the same clock speed, and the
number AMD attach to Athlon is not clock speed.)

We do have a dual P4 Xeon 2.4 GHz with 8GB RAM, and jobs run more than twice
as fast as my PIII 933MHz laptop.

Andy

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Wednesday, July 23, 2003 4:55 PM
> To: Jason Liao
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Dismal R performance of Athlon moble CPU?
> 
> 
> On Wed, 23 Jul 2003, Jason Liao wrote:
> 
> > I have been using a laptop computer of Pentium III 1.13 
> Ghz. I heard 
> > that AMD's Athlon has excellent floating point capacity. So 
> I bought a 
> > Athlon 2200+ laptop yesterday. I expected that new Athlon 
> 2200+ will 
> > be twice as fast as the P III 1.13 GB. I ran a R simulation program 
> > and the new computer is only 30% faster, in fact slightly 
> slower than 
> > a Celeron 1.50 GB laptop. I am very disappointed by this. 
> What is your 
> > experience with Athlon? Should I stick to Intel in the 
> future? Thanks.
> 
> So I expect you think a P4M 1.4GHz (on which I am writing 
> this) should be a lot faster than a PIII 1GHz?  It is often 
> slower.  Don't compare laptop chips with desktop ones, nor 
> different chip families (an Athlon 2200 is not 2.2GHz, BTW).  
> PIIIs seem the fastest per GHz, but they don't do many GHz.
> 
> I am rather pleased with my dual Athlon 2600, but then P4's 
> don't allow 
> multiprocessors and the machine with dual Athlons was cheaper than a 
> comparable one with a single 2.4GHz P4.
> 
> You have tuned an ATLAS implementation to your CPU, I take 
> it?  If not, 
> that's the first step to optimal R performance.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From jg_liao at yahoo.com  Wed Jul 23 23:23:15 2003
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 23 Jul 2003 14:23:15 -0700 (PDT)
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C912@usrymx25.merck.com>
Message-ID: <20030723212315.91307.qmail@web10504.mail.yahoo.com>

Thanks for Prof. Ripley and Andy for your technical explantion. It
seems that that the real CPU speed has not advanced as fast as these
Ghz or other performance indicator suggest. 

Yes, my program is totally CPU intensive.

"> We do have a dual P4 Xeon 2.4 GHz with 8GB RAM, and jobs run more
> than twice
> as fast as my PIII 933MHz laptop."

R can not really use dual CPU for one R session if I understand
correctly


Jason

--- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> Overall performance depends on a few other things besides CPU clock
> speed
> (e.g., RAM speed and size, cache size, disk speed, etc.)  Unless your
> code
> is spending great majority of the time in the CPU, you should not
> expect
> speed-up to be equal to ratio of clock speeds.  (Also, as Prof.
> Ripley
> pointed out, a P4 does less than a PIII at the same clock speed, and
> the
> number AMD attach to Athlon is not clock speed.)
> 
> We do have a dual P4 Xeon 2.4 GHz with 8GB RAM, and jobs run more
> than twice
> as fast as my PIII 933MHz laptop.
> 
> Andy
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: Wednesday, July 23, 2003 4:55 PM
> > To: Jason Liao
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Dismal R performance of Athlon moble CPU?
> > 
> > 
> > On Wed, 23 Jul 2003, Jason Liao wrote:
> > 
> > > I have been using a laptop computer of Pentium III 1.13 
> > Ghz. I heard 
> > > that AMD's Athlon has excellent floating point capacity. So 
> > I bought a 
> > > Athlon 2200+ laptop yesterday. I expected that new Athlon 
> > 2200+ will 
> > > be twice as fast as the P III 1.13 GB. I ran a R simulation
> program 
> > > and the new computer is only 30% faster, in fact slightly 
> > slower than 
> > > a Celeron 1.50 GB laptop. I am very disappointed by this. 
> > What is your 
> > > experience with Athlon? Should I stick to Intel in the 
> > future? Thanks.
> > 
> > So I expect you think a P4M 1.4GHz (on which I am writing 
> > this) should be a lot faster than a PIII 1GHz?  It is often 
> > slower.  Don't compare laptop chips with desktop ones, nor 
> > different chip families (an Athlon 2200 is not 2.2GHz, BTW).  
> > PIIIs seem the fastest per GHz, but they don't do many GHz.
> > 
> > I am rather pleased with my dual Athlon 2600, but then P4's 
> > don't allow 
> > multiprocessors and the machine with dual Athlons was cheaper than
> a 
> > comparable one with a single 2.4GHz P4.
> > 
> > You have tuned an ATLAS implementation to your CPU, I take 
> > it?  If not, 
> > that's the first step to optimal R performance.
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 
> 
>
------------------------------------------------------------------------------
> Notice: This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
> USA) that may be confidential, proprietary copyrighted and/or legally
> 
> privileged, and is intended solely for the use of the individual or
> entity
> named on this message. If you are not the intended recipient, and
> have received this message in error, please immediately return this
> by 
> e-mail and then delete it.
>
------------------------------------------------------------------------------


=====
Jason G. Liao, Ph.D.
Division of Biometrics
University of Medicine and Dentistry of New Jersey
335 George Street, Suite 2200
New Brunswick, NJ 08903-2688
phone (732) 235-8611, fax (732) 235-9777
http://www.geocities.com/jg_liao



From jasont at indigoindustrial.co.nz  Wed Jul 23 23:35:23 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 24 Jul 2003 09:35:23 +1200
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <20030723204411.84725.qmail@web10505.mail.yahoo.com>
References: <20030723204411.84725.qmail@web10505.mail.yahoo.com>
Message-ID: <3F1EFF9B.4060107@indigoindustrial.co.nz>

Jason Liao wrote:
> I have been using a laptop computer of Pentium III 1.13 Ghz. I heard
> that AMD's Athlon has excellent floating point capacity. So I bought a
> Athlon 2200+ laptop yesterday. I expected that new Athlon 2200+ will be
> twice as fast as the P III 1.13 GB. I ran a R simulation program and
> the new computer is only 30% faster, in fact slightly slower than a
> Celeron 1.50 GB laptop. I am very disappointed by this. What is your
> experience with Athlon? Should I stick to Intel in the future? Thanks.
> 

I've found a big problem with laptops to be slow hard drive speeds; if 
you're doing any disk read/writes, it's noticably slower.  I believe the 
slower HDs use less battery power, so it makes some design sense to 
stick with slower drives.

For workstations (not laptops) I use Athlon processors whenever 
possible, and pick HDs that run at a minimum ca. 9000 RPM.  I've found 
the Athlons to give very satisfying grunt per dollar, but you need to 
have fast RAM, fast HDs, and decent-sized cache to actually realise the 
full benefits.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From chory at salk.edu  Wed Jul 23 23:54:59 2003
From: chory at salk.edu (Joanne Chory)
Date: Wed, 23 Jul 2003 14:54:59 -0700
Subject: [R] position available
Message-ID: <p05210612bb44b34692d3@[198.202.65.173]>

Bioinformatics Specialist(Full-Time)

Biostatistician  To work closely with Ph.D. level biologists in the area of
quantitative genetics, including array analysis and software development.
Strength in computer programming is required. Experience with
R/bioconductor, perl, and C.  Must be able to execute statistical methods
including linear and nonlinear models, Hidden Markov Models, and Bayesian
methods. Familiarity with multiple testing, permutation, cross-validation,
cluster analysis is needed.  Be able to perform simulations and test
experimental design strategies. Array data includes standard and custom
Affymetrix designs. Extensive opportunity to publish. M.S. or Ph.D. plus at
least two years experience in statistical analysis and programming required.
Position available beginning September 1, 2003.

Please send your CV and salary history to biostat at natural.salk.edu


-- 
*******************************************************

Joanne Chory
Howard Hughes Medical Institute
Plant Biology Laboratory
The Salk Institute
10010 N. Torrey Pines Rd.
La Jolla, CA 92037

tel:  858-552-1148
fax: 858-558-6379

chory at salk.edu



From heinrich.kestler at tnr.at  Thu Jul 24 00:13:23 2003
From: heinrich.kestler at tnr.at (Heinrich Kestler)
Date: Thu, 24 Jul 2003 00:13:23 +0200
Subject: [R] error bars in color
Message-ID: <003101c35167$a1f400f0$0d0210ac@watzmann>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/5143c0ac/attachment.pl

From Benjamin.STABLER at odot.state.or.us  Thu Jul 24 00:28:54 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 23 Jul 2003 15:28:54 -0700
Subject: [R] Shapefiles package upload
Message-ID: <76A000A82289D411952F001083F9DD06047FE18F@exsalem4-bu.odot.state.or.us>

I uploaded version 0.3 of the shapefiles package to CRAN earlier today.
Version 0.2 had a bug that omitted the decimal precision in numeric fields
so some programs (such as ArcGIS) would not parse the fields correctly.  I
also added an argument to write.dbf to swap "." with "_" since ArcGIS does
not permit underscores in field names.  Let me know if you run into any
other problems.  Thanks.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From jerome at hivnet.ubc.ca  Thu Jul 24 00:31:54 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 23 Jul 2003 15:31:54 -0700
Subject: [R] error bars in color
In-Reply-To: <003101c35167$a1f400f0$0d0210ac@watzmann>
References: <003101c35167$a1f400f0$0d0210ac@watzmann>
Message-ID: <200307232238.PAA19247@hivnet.ubc.ca>


Yes. See ?plot, ?segments, ?lines, and in particular see the help on the 
"col" option in ?par.

HTH,
Jerome

On July 23, 2003 03:13 pm, Heinrich Kestler wrote:
> Hi,
> is it possible to generate differently colored error bars in one plot?
>
> Thx in advance,
> Heinrich
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Benjamin.STABLER at odot.state.or.us  Thu Jul 24 00:36:21 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 23 Jul 2003 15:36:21 -0700
Subject: [R] RE: Shapefiles package upload
Message-ID: <76A000A82289D411952F001083F9DD06047FE192@exsalem4-bu.odot.state.or.us>

Sorry, I meant to say ArcGIS does not permit periods in field names.

>-----Original Message-----
>From: STABLER Benjamin 
>Sent: Wednesday, July 23, 2003 3:29 PM
>To: r-help at stat.math.ethz.ch
>Subject: Shapefiles package upload
>
>
>I uploaded version 0.3 of the shapefiles package to CRAN 
>earlier today.  Version 0.2 had a bug that omitted the decimal 
>precision in numeric fields so some programs (such as ArcGIS) 
>would not parse the fields correctly.  I also added an 
>argument to write.dbf to swap "." with "_" since ArcGIS does 
>not permit underscores in field names.  Let me know if you run 
>into any other problems.  Thanks.
>
>Benjamin Stabler
>Transportation Planning Analysis Unit
>Oregon Department of Transportation
>555 13th Street NE, Suite 2
>Salem, OR 97301  Ph: 503-986-4104
>
>



From rossini at blindglobe.net  Wed Jul 23 15:55:26 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 23 Jul 2003 15:55:26 +0200
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <20030723212315.91307.qmail@web10504.mail.yahoo.com> (Jason
	Liao's message of "Wed, 23 Jul 2003 14:23:15 -0700 (PDT)")
References: <20030723212315.91307.qmail@web10504.mail.yahoo.com>
Message-ID: <853cgxl54h.fsf@blindglobe.net>

Jason Liao <jg_liao at yahoo.com> writes:

> R can not really use dual CPU for one R session if I understand
> correctly

It certainly can, using message passing libraries or sockets.  While
it isn't technically "one session", it's awfully similar to that, for
the user.

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From andy_liaw at merck.com  Thu Jul 24 01:04:03 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jul 2003 19:04:03 -0400
Subject: [R] Dismal R performance of Athlon moble CPU?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C913@usrymx25.merck.com>

> From: Jason Liao [mailto:jg_liao at yahoo.com] 
> 
> Thanks for Prof. Ripley and Andy for your technical 
> explantion. It seems that that the real CPU speed has not 
> advanced as fast as these Ghz or other performance indicator suggest. 
> 
> Yes, my program is totally CPU intensive.
> 
> "> We do have a dual P4 Xeon 2.4 GHz with 8GB RAM, and jobs run more
> > than twice
> > as fast as my PIII 933MHz laptop."
> 
> R can not really use dual CPU for one R session if I 
> understand correctly

No, but that machine is being shared by several people.  Even if only one
person uses the box, it helps to have one CPU dedicated to R, and another
taking care of other things.

Having 12k rpm SCSI disks and fast RAM helped, too.

Andy
 
> 
> Jason
> 
> --- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > Overall performance depends on a few other things besides CPU clock 
> > speed (e.g., RAM speed and size, cache size, disk speed, 
> etc.)  Unless 
> > your code
> > is spending great majority of the time in the CPU, you should not
> > expect
> > speed-up to be equal to ratio of clock speeds.  (Also, as Prof.
> > Ripley
> > pointed out, a P4 does less than a PIII at the same clock speed, and
> > the
> > number AMD attach to Athlon is not clock speed.)
> > 
> > We do have a dual P4 Xeon 2.4 GHz with 8GB RAM, and jobs 
> run more than 
> > twice as fast as my PIII 933MHz laptop.
> > 
> > Andy
> > 
> > > -----Original Message-----
> > > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > > Sent: Wednesday, July 23, 2003 4:55 PM
> > > To: Jason Liao
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Dismal R performance of Athlon moble CPU?
> > > 
> > > 
> > > On Wed, 23 Jul 2003, Jason Liao wrote:
> > > 
> > > > I have been using a laptop computer of Pentium III 1.13
> > > Ghz. I heard
> > > > that AMD's Athlon has excellent floating point capacity. So
> > > I bought a
> > > > Athlon 2200+ laptop yesterday. I expected that new Athlon
> > > 2200+ will
> > > > be twice as fast as the P III 1.13 GB. I ran a R simulation
> > program
> > > > and the new computer is only 30% faster, in fact slightly
> > > slower than
> > > > a Celeron 1.50 GB laptop. I am very disappointed by this.
> > > What is your
> > > > experience with Athlon? Should I stick to Intel in the
> > > future? Thanks.
> > > 
> > > So I expect you think a P4M 1.4GHz (on which I am writing
> > > this) should be a lot faster than a PIII 1GHz?  It is often 
> > > slower.  Don't compare laptop chips with desktop ones, nor 
> > > different chip families (an Athlon 2200 is not 2.2GHz, BTW).  
> > > PIIIs seem the fastest per GHz, but they don't do many GHz.
> > > 
> > > I am rather pleased with my dual Athlon 2600, but then P4's
> > > don't allow 
> > > multiprocessors and the machine with dual Athlons was cheaper than
> > a
> > > comparable one with a single 2.4GHz P4.
> > > 
> > > You have tuned an ATLAS implementation to your CPU, I take
> > > it?  If not, 
> > > that's the first step to optimal R performance.
> > > 
> > > -- 
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,
> > http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > > 
> > 
> >
> --------------------------------------------------------------
> ----------------
> > Notice: This e-mail message, together with any attachments, contains
> > information of Merck & Co., Inc. (Whitehouse Station, New Jersey, 
> > USA) that may be confidential, proprietary copyrighted 
> and/or legally
> > 
> > privileged, and is intended solely for the use of the individual or 
> > entity named on this message. If you are not the intended 
> recipient, 
> > and have received this message in error, please immediately return 
> > this by
> > e-mail and then delete it.
> >
> --------------------------------------------------------------
> ----------------
> 
> 
> =====
> Jason G. Liao, Ph.D.
> Division of Biometrics
> University of Medicine and Dentistry of New Jersey
> 335 George Street, Suite 2200
> New Brunswick, NJ 08903-2688
> phone (732) 235-8611, fax (732) 235-9777 
> http://www.geocities.com/jg_liao
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From cberry at tajo.ucsd.edu  Thu Jul 24 02:08:56 2003
From: cberry at tajo.ucsd.edu (cberry@tajo.ucsd.edu)
Date: Wed, 23 Jul 2003 17:08:56 -0700 (PDT)
Subject: [R] unz( "x.zip", "y.csv" ) != pipe( "unzip -p x.zip y.csv" )
Message-ID: <Pine.GSO.4.10.10307231613250.3079-100000@tajo.ucsd.edu>


Not sure this is a bug in R. 

Maybe its a bug in my understanding of unz(). 

The character 'b2' (hexadecimal) is in position 535 of line 1 
of 'naughty.csv'. This character appears as superscript '2' and came to me
in an EXCEL file that I converted to text in a comma separated ( *.csv )
format.

The first line gets truncated by readLines after 534 characters using
unz():

> nchar( readLines( unz( "bad.zip", "naughty.csv" )))
[1] 534  11   9  22
> nchar(readLines( pipe(" unzip -p bad.zip naughty.csv" ) ))
[1] 809  11   9  22


attempting to read the same file using scan( unz( ... ) ) concat's the
rest of the file (including comma separators) to the word that included
'b2', while scan( pipe( "unzip ..." ) ) reads all elements.

>
> options(width = 50 ) # prevent my mailer from line wrapping
>
> nchar(scan(unz( "bad.zip", "naughty.csv") , what="a", sep=",",nlines=1)
)
Read 45 items
 [1]   5   9  12   8  11   4   2   1   1   8   8
[12]   8   9   5  10   8   6  12  10   8  16  16
[23]  12  14  12  20  10   8   6  12  10   8  16
[34]  16  12  14  12  20  20  18  20  18  13  13
[45] 329
> nchar( scan( pipe(" unzip -p bad.zip naughty.csv" ) , what="a",
sep=",",nlines=1) )
Read 62 items
 [1]  5  9 12  8 11  4  2  1  1  8  8  8  9  5 10
[16]  8  6 12 10  8 16 16 12 14 12 20 10  8  6 12
[31] 10  8 16 16 12 14 12 20 20 18 20 18 13 13 10
[46] 13 14 12 12 10 16 14 12 10 16 14 22 20 22 20
[61] 15 15
>
> version    ## LINUX R-1.7.1 gave similar results
         _                   
platform sparc-sun-solaris2.8
arch     sparc               
os       solaris2.8          
system   sparc, solaris2.8   
status                       
major    1                   
minor    7.0                 
year     2003                
month    04                  
day      16                  
language R                   
> 

Chuck


Charles C. Berry                        (858) 534-2098 
                                         Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://hacuna.ucsd.edu/members/ccb.html  La Jolla, San Diego 92093-0717



From pbrunno at yahoo.co.uk  Thu Jul 24 03:28:22 2003
From: pbrunno at yahoo.co.uk (=?iso-8859-1?q?Bruno=20Paul=20Mmbando?=)
Date: Thu, 24 Jul 2003 02:28:22 +0100 (BST)
Subject: [R] negative binomial
Message-ID: <20030724012822.40152.qmail@web10108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/5c511f25/attachment.pl

From ray at mcs.vuw.ac.nz  Thu Jul 24 04:21:19 2003
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 24 Jul 2003 14:21:19 +1200 (NZST)
Subject: [R] trouble with maps
Message-ID: <200307240221.h6O2LJAq008129@tahi.mcs.vuw.ac.nz>

Roger.Bivand at nhh.no wrote:
> On Wed, 23 Jul 2003, Deborah Swayne wrote:
> 
> > Has anyone else seen this behavior from the "maps" package?
> >     
> > map('state', fill=TRUE)
> > 
> > results in a lively mix of overlapping polygons inside a
> > map of the US, but they have no obvious relationship to
> > state boundaries.  (See attached jpeg.)
> 
Ah, this was a 'known problem' :-) first noticed in January this year,
by Ott Toomet <otoomet at econ.dk>.

I suspect it is a feature of the port to R which never worked
properly.  The solution is to reverse the sign of every line number
making up the polygons in the .gon file (or alternately, reverse the
order of the line numbers there).

A 'fixed' maps package (Unix only) is available at:
ftp://ftp.mcs.vuw.ac.nz/pub/statistics/map/maps_1.1-2.tar.gz

Ray Brownrigg



From Arnaud.Dowkiw at dpi.qld.gov.au  Thu Jul 24 05:02:02 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Thu, 24 Jul 2003 13:02:02 +1000
Subject: [R] pls regression - optimal number of LVs
Message-ID: <C2C6EA6C4DADB348BFDF58894B03901201274228@kinsrv001.dpi.qld.gov.au>

Dear R-helpers,

I have performed a PLS regression with the mvr function from the pls.pcr package an I have 2 questions :
1- do you know if mvr automatically centers the data ? It seems to me that it does so...
2- why in  the situation below does the output say that the optimal number of latent variables is 4 ? In my humble opinion, it is 2 because the RMS increases and the R2 decreases when 3 LVs are considered :
> summary(maturityCondor.raw.mvr)
Data:   X dimension: 8 1050 
        Y dimension: 8 1
Method: SIMPLS
Number of latent variables considered: 1-7 


TRAINING:
RMS table:
           [,1]
1 LV's 1.23e+01
2 LV's 6.79e+00
3 LV's 5.00e+00
4 LV's 2.17e+00
5 LV's 1.93e+00
6 LV's 7.79e-01
7 LV's 1.01e-09

Cumulative fraction of variance explained:
           X     Y
1 LV's 0.848 0.499
2 LV's 0.930 0.846
3 LV's 0.979 0.917
4 LV's 0.992 0.984
5 LV's 0.999 0.988
6 LV's 1.000 0.998
7 LV's 1.000 1.000


VALIDATION
Optimal number of latent variables: 4

RMS table (10-fold crossvalidation):
        [,1]
1 LV's 16.21
2 LV's 12.15
3 LV's 13.81
4 LV's  6.68
5 LV's  6.38
6 LV's  5.91
7 LV's 13.38

Coefficient of multiple determination (R2):
       [,1]
1 LV's 0.20
2 LV's 0.51
3 LV's 0.41
4 LV's 0.88
5 LV's 0.87
6 LV's 0.90
7 LV's 0.77

Thanks for your help,

Arnaud


*************************
Arnaud DOWKIW
Department of Primary Industries
J. Bjelke-Petersen Research Station
KINGAROY, QLD 4610
Australia
T : + 61 7 41 600 700
T : + 61 7 41 600 728 (direct)
F : + 61 7 41 600 760
**************************
 

********************************DISCLAIMER******************...{{dropped}}



From znmeb at aracnet.com  Thu Jul 24 05:21:17 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Wed, 23 Jul 2003 20:21:17 -0700
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <20030723204411.84725.qmail@web10505.mail.yahoo.com>
Message-ID: <001c01c35192$a8e5bf90$74c463d8@plaza.ds.adp.com>

I haven't gotten around to assembling the toolset required to build R on
Windows, since most of what I do is smallish interactive problems. However,
another possibility would be to load CygWin/XFree86 on your laptop (which
I've done), then download Atlas 3.5.7 from SourceForge (which I've done),
then build Atlas with CygWin(which I've done) and then build a second
version of R under CygWin using Atlas, and use the CygWin/Atlas R for the
heavy number-crunching jobs. This last I haven't done, so I can't say
whether there are any gotchas, but everything else I've done with
CygWin/XFree86 has worked. My laptop is a Compaq Presario with a 1.67 GHz
Athlon XP. Atlas screams on it; the Atlas folks were grinning when I sent
them the log. Atlas has an assembly language kernel for Athlons (and P4s as
well IIRC).

Oh, yeah ... If you do try my scheme, make sure you don't have spaces in the
paths ... Atlas still isn't immune to that sort of thing under CygWin.

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jason Liao
> Sent: Wednesday, July 23, 2003 1:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Dismal R performance of Athlon moble CPU?
> 
> 
> I have been using a laptop computer of Pentium III 1.13 Ghz. 
> I heard that AMD's Athlon has excellent floating point 
> capacity. So I bought a Athlon 2200+ laptop yesterday. I 
> expected that new Athlon 2200+ will be twice as fast as the P 
> III 1.13 GB. I ran a R simulation program and the new 
> computer is only 30% faster, in fact slightly slower than a 
> Celeron 1.50 GB laptop. I am very disappointed by this. What 
> is your experience with Athlon? Should I stick to Intel in 
> the future? Thanks.
> 
> By the way, the OS is Windows XP home edtion.
> 
> Jason
> 
> =====
> Jason G. Liao, Ph.D.
> Division of Biometrics
> University of Medicine and Dentistry of New Jersey
> 335 George Street, Suite 2200
> New Brunswick, NJ 08903-2688
> phone (732) 235-8611, fax (732) 235-9777 
> http://www.geocities.com/jg_liao
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From krcabrer at epm.net.co  Thu Jul 24 06:20:06 2003
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Wed, 23 Jul 2003 23:20:06 -0500
Subject: [R] Intervention/Impact analysis in time series
Message-ID: <3F1F5E76.7090203@epm.net.co>

Hi R users:
Does any one knows about a R library for deal with
intervention/impact analysis in time series (eg. Box-Tiao et. al. theory?).

Thank you for your help

-- 
Kenneth Roy Cabrera Torres
Celular +57 (315) 405 9339



From ripley at stats.ox.ac.uk  Thu Jul 24 09:14:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jul 2003 08:14:38 +0100 (BST)
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <001c01c35192$a8e5bf90$74c463d8@plaza.ds.adp.com>
Message-ID: <Pine.LNX.4.44.0307240809210.2342-100000@gannet.stats>

R is not supported under Cygwin, so that won't help, and is in any case 
unnecessary.

However, under Windows you can make use of ATLAS, and that is fully
documented: we even provide versions of Rblas.dll for common chips. Part
of my point was that a mobile Athlon will very likely need different
tuning from your Athlon XP.

The R developers have gone to considerable trouble to make ATLAS-tuned 
versions possible under Windows, so why ignore their efforts, Mr Borasky?

On Wed, 23 Jul 2003, M. Edward Borasky wrote:

> I haven't gotten around to assembling the toolset required to build R on
> Windows, since most of what I do is smallish interactive problems. However,
> another possibility would be to load CygWin/XFree86 on your laptop (which
> I've done), then download Atlas 3.5.7 from SourceForge (which I've done),
> then build Atlas with CygWin(which I've done) and then build a second
> version of R under CygWin using Atlas, and use the CygWin/Atlas R for the
> heavy number-crunching jobs. This last I haven't done, so I can't say
> whether there are any gotchas, but everything else I've done with
> CygWin/XFree86 has worked. My laptop is a Compaq Presario with a 1.67 GHz
> Athlon XP. Atlas screams on it; the Atlas folks were grinning when I sent
> them the log. Atlas has an assembly language kernel for Athlons (and P4s as
> well IIRC).
> 
> Oh, yeah ... If you do try my scheme, make sure you don't have spaces in the
> paths ... Atlas still isn't immune to that sort of thing under CygWin.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rwehrens at sci.kun.nl  Thu Jul 24 09:59:34 2003
From: rwehrens at sci.kun.nl (Ron Wehrens)
Date: Thu, 24 Jul 2003 09:59:34 +0200
Subject: [R] Re: pls regression - optimal number of LVs
In-Reply-To: <C2C6EA6C4DADB348BFDF58894B03901201274228@kinsrv001.dpi.qld.gov.au>
References: <C2C6EA6C4DADB348BFDF58894B03901201274228@kinsrv001.dpi.qld.gov.au>
Message-ID: <200307240959.34118.rwehrens@sci.kun.nl>

On Thursday 24 July 2003 05:02, Dowkiw, Arnaud wrote:
> Dear R-helpers,
>
> I have performed a PLS regression with the mvr function from the pls.pcr
> package an I have 2 questions : 1- do you know if mvr automatically centers
> the data ? It seems to me that it does so... 

Yup, it does... common practice.

> 2- why in  the situation below
> does the output say that the optimal number of latent variables is 4 ? In
> my humble opinion, it is 2 because the RMS increases and the R2 decreases
> when 3 LVs are considered :

Many criteria exist and for some data sets they agree, for most they do not. 
The criterion applied here checks whether the decrease in cross-validated 
error is significant; Hastie et al. use it in their book "The elements of 
statistical learning". It is described in the man page, and like all 
criteria, it is not guaranteed to satisfy all users. If you feel better using 
2LVs, you can do that. 

Ron

-- 
Ron Wehrens            
Dept. of Chemometrics  
University of Nijmegen	Email: rwehrens at sci.kun.nl
Toernooiveld 1		http://www-cac.sci.kun.nl/cac/
6525 ED Nijmegen	Tel: +31 24 365 2053
The Netherlands		Fax: +31 24 365 2653



From claus at ekstroem.dk  Thu Jul 24 10:47:15 2003
From: claus at ekstroem.dk (Claus Ekstroem)
Date: Thu, 24 Jul 2003 10:47:15 +0200
Subject: [R] nls.control in gnls
Message-ID: <20030724084715.GA20251@dina.kvl.dk>

Hi,

I've made a selfStart function for use with gnls and the 
following piece of code works nicely:

check1 <- gnls(y ~ spot.shape.fct(xcord, ycord, background, spotintensity, 
                                  rho, sigma, delta, mux, muy),
              start=getInitial(y ~ spot.shape.fct(xcord, ycord, 
                                   background, spotintensity, rho, 
                                   sigma, delta, mux, muy), data=claus),
              data=claus
              )

spot.shape.fct is the selfStart function and I later want to include 
a correlation term which is why I'm using gnls instead of nls. I want to 
get rid of the start= option, but I keep getting the following error 
message: 

Error in nls(formula = y ~ spot.shape.fct(xcord, ycord, background, 
spotintensity,  : 
        number of iterations exceeded maximum of 50

>From the help page and the glns code I can see that the initial start 
values are improved by estimating the parameters in a model without the 
correlation structures that are possible with gnls. This is quite 
reasonable and will quite likely improve the convergence succes and time 
once a correlation is introduced.

Obviously the call to nls does not converge before the standard 50 
iterations specified in nls.control so I was wondering if it is possible 
to pass nls.control information on to nls when calling gnls?

Several work-around exists:

* Write a better initial function to get more precise starting values, 
i.e., fewer iterations needed for nls.

* Make a new function  my.own.gnls  where nls.control(maxiter=100) is 
set in the call to nls.

Maybe it would even be a good idea to pass nlsMaxIter from gnlsControl 
on to the initial call to nls? As far as I can see from the code it is 
only used in maximization after the call to get the starting value.

Happy R'ing

Claus

-- 
*****************************************
Claus Thorn Ekstr?m <ekstrom at dina.kvl.dk>
Dept of Mathematics and Physics, KVL
Thorvaldsensvej 40
DK-1871 Frederiksberg C
Denmark
Phone:[+45] 3528 2341
Fax:  [+45] 3528 2350



From e.corda at oncfs.gouv.fr  Thu Jul 24 11:51:03 2003
From: e.corda at oncfs.gouv.fr (Eve Corda)
Date: Thu, 24 Jul 2003 11:51:03 +0200
Subject: [R]: performing marginal tests to glm objects
Message-ID: <004001c351c9$18a049a0$7a03a8c0@DER6>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/fe63d994/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jul 24 12:11:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jul 2003 11:11:40 +0100 (BST)
Subject: [R]: performing marginal tests to glm objects
In-Reply-To: <004001c351c9$18a049a0$7a03a8c0@DER6>
Message-ID: <Pine.LNX.4.44.0307241107370.7512-100000@gannet.stats>

?drop1 for the valid marginal tests.

If you want `type III' tests (which aren't marginal in the usual sense) 
see Anova in package car, which also does `type II' tests (as performed by 
drop1).

On Thu, 24 Jul 2003, Eve Corda wrote:

> I wonder if it is possible to obtain marginal tests for effects in
> generalized linear models. Indeed, the anova function produces
> sequential tests and it doesn't have any "type" argument to specify that
> we would like marginal tests instead, as in the similar anova function
> for lme objects.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hb at maths.lth.se  Thu Jul 24 12:21:58 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 24 Jul 2003 12:21:58 +0200
Subject: [R] Passing references to data objects into R functions
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063E1F@mail.internal.net>
Message-ID: <000001c351cd$6bd2aba0$2b0040d5@maths.lth.se>

One way is to use an object-oriented design and wrap up the reference
functionality in a common superclass. At
http://www.maths.lth.se/help/R/ImplementingReferences/ I have got some
discussions which are in line what you are trying to achieve and that
you might be able to adopt.

Also, note that passing huge objects as arguments to functions is NOT
expensive (considering memory or time) in R if they are used for
read-only purposes. It only becomes expensive if you assign a new value
to the argument. In such cases R *has to* copy the whole object to make
sure you only modify a local instance of the object. Thus, objects can
be though of being passed by reference to functions as long as they are
not modified, if modified they are passed by value. This is intentional
as R is a (one-threaded) functional language. 

Best wishes

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> Khabie-Zeitoune
> Sent: den 23 juli 2003 18:18
> To: r-help at r-project.org
> Subject: [R] Passing references to data objects into R functions
> 
> 
> Hi. 
> 
> I have the following question about reading from large data 
> objects from within R functions; I have tried to simplify my 
> problem as much as possible in what follows.
> 
> Imagine I have various large data objects sitting in my 
> global environment (call them "data1", "data2", ...).  I want 
> to write a function "extract" that extracts some of the rows 
> of a particular data object, does some further manipulations 
> on the extract and then returns the result. The function 
> takes the data object's name and an index vector -- for 
> example the following call would return the first 3 rows of 
> object data1. 
> 
> ans = extract("data1", 1:3)
> 
> I could write a simple function like this:
> 
> extract1 = function(object.name, index) {
> 
>     temp = get(object.name, envir = .GlobalEnv)
>     temp = temp[index, , drop=FALSE]
> 
>     # do some further manipulations here ....
> 
>     return(temp)
> 
> }
> 
> The problem is that the function makes a copy "temp" of the 
> object in the function frame, which (in my application) is 
> very memory inefficient as the data objects are very large. 
> It is especially inefficient when the length of the "index" 
> vector is much smaller than the number of rows in the data 
> object. What I really would like to do is to be able to read 
> from the underlying data object directly (in other 
> programming languages this would be achieved by passing a 
> pointer to the object instead), without making a copy.
> 
> Given the rules of variable name scoping in R, I could avoid 
> making a copy with the following call:
> 
> extract2 = function(object.name, index) {
> 
>     eval(parse(text = "temp = ", object.name, "[index, , drop=FALSE]",
> sep=""))
>     # do some further manipulations here ....
> 
>     return(temp)
> }
> 
> But this seems very messy. Is there a better way?
> 
> Thanks for your help
> 
> David Khabie-Zeitoune
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help



From th50 at leicester.ac.uk  Thu Jul 24 12:47:47 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 24 Jul 2003 11:47:47 +0100
Subject: [R] Confidence Band for empirical distribution function
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B680@saffron.cfs.le.ac.uk>

Dear Kjetil,

As I already mentioned, it appears that there isn't a function 
available calculating the quantiles directly (at least, it doesn't appear
in the C source of ctest). So as I already suggested, uniroot (or a similar
C routine which calls the corresponding C code directly) is probably the 
best you can do (apart from writing it completely yourself).

I didn't program this using uniroot, but I'd certainly try the following 
for speed-up:

- For symmetry reasons, you only need to compute half of the quantiles.
- The quantiles depend smoothly on the probabilities (of your reference
distribution). Therefore, calculating only a "few" for probabilities 
between 0 and 0.5, and using (e.g. linear) interpolation should be
satisfying.

I am sorry not be of more help.

HTH anyway

Thomas

> -----Original Message-----
> From: kjetil brinchmann halvorsen [mailto:kjetil at entelnet.bo]
> Sent: 23 July 2003 15:46
> To: Hotz, T.
> Subject: RE: [R] Confidence Band for empirical distribution function
> 
> 
> On 22 Jul 2003 at 11:37, Hotz, T. wrote:
> 
> > Dear Leif,
> > 
> > If you look at the definition of ks.test, you'll find the lines
> > 
> > pkstwo <- function(x, tol = 1e-06) {
> >     if (is.numeric(x)) 
> >         x <- as.vector(x)
> >     else stop("Argument x must be numeric")
> >     p <- rep(0, length(x))
> >     p[is.na(x)] <- NA
> >     IND <- which(!is.na(x) & (x > 0))
> >     if (length(IND) > 0) {
> >         p[IND] <- .C("pkstwo", as.integer(length(x)), p = 
> as.double(x[IND]), 
> >             as.double(tol), PACKAGE = "ctest")$p
> >     }
> >     return(p)
> > }
> > 
> > which calls C code to calculate the p-values given the test 
> statistic.
> > You'll find explanations on what this function does in the 
> original C file
> > src/library/ctest/src/ks.c
> > 
> > I haven't tried that but I assume that you could use it to 
> calculate p-values
> > given the test-statistics yourself.
> 
> That could certainly be done, but what was asked for is the inverse, 
> which can be calculated, using for instance uniroot(). I tried that, 
> but it is to slow, .C() will be called repeatedly in a loop. For me 
> it took several minutes.
> 
> Kjetil Halvorsen
> 
> > 
> > Please also note that ks.test() returns the p-value as well.
> > 
> > If you need quantiles, I assume you need to invert the cdf yourself,
> > e.g. using uniroot().
> > 
> > HTH
> > 
> > Thomas
> >       
> > ---
> > 
> > Thomas Hotz
> > Research Associate in Medical Statistics
> > University of Leicester
> > United Kingdom
> > 
> > Department of Epidemiology and Public Health
> > 22-28 Princess Road West
> > Leicester
> > LE1 6TP
> > Tel +44 116 252-5410
> > Fax +44 116 252-5423
> > 
> > Division of Medicine for the Elderly
> > Department of Medicine
> > The Glenfield Hospital
> > Leicester
> > LE3 9QP
> > Tel +44 116 256-3643
> > Fax +44 116 232-2976
> > 
> > 
> > > -----Original Message-----
> > > From: Leif.Boysen [mailto:boysen at math.uni-goettingen.de]
> > > Sent: 21 July 2003 14:42
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Confidence Band for empirical distribution function
> > > 
> > > 
> > > Hi,
> > > 
> > > I was trying to draw an empirical distribution function 
> with uniform
> > > confidence bands. So I tried to find a way to calculate 
> values of the
> > > Kolmogorov-Smirnov Distribution but failed.
> > > I guess it must be hidden somewhere (since the ks-test is 
> > > implemented),
> > > but I was unable to find it. 
> > > 
> > > Is there any way to do this?
> > > 
> > > Thanks
> > > 
> > > Leif Boysen
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From qin.li at unn.ac.uk  Thu Jul 24 13:27:08 2003
From: qin.li at unn.ac.uk (qin.li)
Date: Thu, 24 Jul 2003 12:27:08 +0100
Subject: [R] How can I represent a shape using 1-D discrete wavelet
	descriptor s?
Message-ID: <2792F778DC79D411A86500508BCF83CA021CF8C3@anchorage.unn.ac.uk>

I only concern about the boundary of the shape and I don¡¯t concern gray
level. 
U[m] = x[m] + jy[m] m= 0,1,¡­,N-1 is the N points on the boundary.
It is easy to express U as a discrete Fourier series. 
But it seems like that wd in the wavethresh package cannot deal with complex
values.
And it is not interesting to transfer U to a matrix.
Thanks a lot

Qin



From michael.watson at bbsrc.ac.uk  Thu Jul 24 13:51:38 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 24 Jul 2003 12:51:38 +0100
Subject: [R] R won't connect to the internet on Linux!
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00959@cl-exsrv1.irad.bbsrc.ac.uk>

OK, I really am struggling with this one!  Forgive me if I am being stupid....

I am running R 1.7.1 on Suse Linux 8.1.  I connect to the internet through a proxy so I have:

IAHC-LINUX03:~ # echo $http_proxy
wwwcache.bbsrc.ac.uk:8080
IAHC-LINUX03:~ # echo $HTTP_PROXY
wwwcache.bbsrc.ac.uk:8080

just in case ;-)

SO, i go into R and I get:

> source("http://www.bioconductor.org/getBioC.R")
unable to connect to 'www.bioconductor.org' on port 80.
Error in file(file, "r") : cannot open URL `http://www.bioconductor.org/getBioC.R'

OK so is R just not picking up my proxy setting?  It seems to be trying port 80 on something, and I have specifically set it to port 8080 in my environment variables.  As far as I can see I have followed the reference manual suggestion, so does anyone else have one?

Thanks
Mick



From sbarbar at gwdg.de  Thu Jul 24 13:56:22 2003
From: sbarbar at gwdg.de (Salvatore Barbaro)
Date: Thu, 24 Jul 2003 13:56:22 +0200
Subject: [R] median and joint distribution
Message-ID: <3F1FE586.10428.B9DC13@localhost>

Dear R-"helpers"!

May I kindly ask the pure statistics-experts to help me for a
purpose which first part is not directly concerned with R.
Consider two distribution functions, say f and g. For both, the
median is smaller than a half. Now, the multiplicative or additive
linkage of both distribution leads to a new distribution function,
say h, whereas the median of h is greater than a half. Does
anybody know under which circumstances such a construction of h is
possible (my intuition is that it depends on the correlation of f
and g) or can anybody advice a helpful literature. Furthermore,
does anybody know whether or how such a construction can be done
with R. Thanks in advance.

s.



From laurent.faisnel at ariase.com  Thu Jul 24 14:03:41 2003
From: laurent.faisnel at ariase.com (Laurent Faisnel)
Date: Thu, 24 Jul 2003 14:03:41 +0200
Subject: [R] S3 and S4 classes
References: <3F1E8564.5070300@ariase.com>
	<858thvgk9b6u4s8d4t0g86rlutj9h9bpfj@4ax.com>
	<3F1EE5E0.6F3F5AE3@research.bell-labs.com>
Message-ID: <3F1FCB1D.7080707@ariase.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/d9821e46/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jul 24 14:16:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jul 2003 13:16:40 +0100 (BST)
Subject: [R] R won't connect to the internet on Linux!
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00959@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0307241310000.16725-100000@gannet.stats>

On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:

> OK, I really am struggling with this one!  Forgive me if I am being stupid....

> I am running R 1.7.1 on Suse Linux 8.1.  I connect to the internet
> through a proxy so I have:
> 
> IAHC-LINUX03:~ # echo $http_proxy
> wwwcache.bbsrc.ac.uk:8080
> IAHC-LINUX03:~ # echo $HTTP_PROXY
> wwwcache.bbsrc.ac.uk:8080
> 
> just in case ;-)
> 
> SO, i go into R and I get:
> 
> > source("http://www.bioconductor.org/getBioC.R")
> unable to connect to 'www.bioconductor.org' on port 80.
> Error in file(file, "r") : cannot open URL `http://www.bioconductor.org/getBioC.R'
> 
> OK so is R just not picking up my proxy setting?  

Your setting is wrong, so it is being ignored.  The help page says
quite explicitly

      The form of `"http_proxy"' should be `"http://proxy.dom.com/"' or
     `"http://proxy.dom.com:8080/"' where the port defaults to `80' and
     the trailing slash may be omitted.

> It seems to be trying
> port 80 on something, and I have specifically set it to port 8080 in my
> environment variables.  As far as I can see I have followed the
> reference manual suggestion, so does anyone else have one?

The problem is in your seeing, it seems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mihastaut at hotmail.com  Thu Jul 24 14:17:11 2003
From: mihastaut at hotmail.com (Miha STAUT)
Date: Thu, 24 Jul 2003 12:17:11 +0000
Subject: [R] geoR size limit problem
Message-ID: <BAY2-F101lxw00dSdVu00004176@hotmail.com>

Hi all,

I tried to produce some kriged surfaces with geoR (latest version). The size 
of the grid should be around 900 x 650 cells (what I find is not a very big 
grid), and the number of points is around 2500. The command krige.conv 
stopped after arround 5 min saying it can not allocate a vector with around 
1.5 billion units. Sounds reasonable.

Is there a workaround? How would I partition the map into several smaller 
pieces to krige them separately and retain the smooth transitions from one 
part to another? Is there a possibility to apply a "mask" to the grid in a 
way that it only interpolates the cells of interest?

My machine has 512MB of memory and additional 2000MB of swap.

Thanks in advance, Miha Staut



From flom at ndri.org  Thu Jul 24 14:24:35 2003
From: flom at ndri.org (Peter Flom)
Date: Thu, 24 Jul 2003 08:24:35 -0400
Subject: [R] Condition indexes and variance inflation factors
Message-ID: <sf1f97dd.073@MAIL.NDRI.ORG>

Thanks for all the help.

Juergen Gross supplied a program which does just what Belsley
suggested.

Chuck Cleland, John Fox and Andy Liaw all made useful programming
suggestions.

John Fox asked

<<<
(1) I've never liked this approach for a model with a constant, where
it 
makes more sense to me to centre the data. I realize that opinions
differ 
here, but it seems to me that failing to centre the data conflates 
collinearity with numerical instability.
>>>

Opinions do differ.  A few years ago, I could have given more details
(my dissertation was on this topic, but a lot of the details have
disappeared from memory); I think, though, that Belsley is looking for a
measure that deals not only with collinearity, but with several other
problems, including numerical instability (the subtitle of his later
book is Collinearity and Weak Data in Regression).  I remember being
convinced that centering was generally not a good idea, but there are
lots of people who disagree and who know a lot more statistics than I
do.

<<<
(2) I also disagree with the comment that condition indices are easier
to 
interpret than variance-inflation factors. In either case, since 
collinearity is a continuous phenomenon, cutoffs for large values are 
necessarily arbitrary.
>>>

While any cutoff is arbitrary (and Belsley advises against using a
cutoff rigidly) he does provide some evidence of how regression models
with different condition indices are affected by them.

<<<
(3) If you're interested in figuring out which variables are involved
in 
each collinear relationship, then (for centred and scaled data) you can

equivalently (and to me, more intuitively) work with the 
principal-components analysis of the predictors.
>>>

This would also work.  

<<<
(4) I have doubts about the whole enterprise. Collinearity is one
source of 
imprecision -- others are small sample size, homogeneous predictors,
and 
large error variance. Aren't the coefficient standard errors the bottom

line? If these are sufficiently small, why worry?
>>>

I think (correct me if I am wrong) that the s.e.s and the condition
indices serve very different purposes.  The condition indices are
supposed to determine if small changes in the input data could make big
differences in the results.  Belsley provides some examples where a tiny
change in the data results in completely different results (e.g.,
different standard errors, different coefficients (even reversing sign)
and so on).  



Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From roger at ysidro.econ.uiuc.edu  Thu Jul 24 14:42:05 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 24 Jul 2003 07:42:05 -0500 (CDT)
Subject: [R] median and joint distribution
In-Reply-To: <3F1FE586.10428.B9DC13@localhost>
Message-ID: <Pine.SOL.4.30.0307240729150.19664-100000@ysidro.econ.uiuc.edu>

For distribution functions F and G we have the (Frechet) bounds:

	max{0, F(x)+G(y) -1} <= H(x,y) <= min{F(x),F(y)}

where H is the joint df of (X,Y) having marginals F and G.  If
X and Y are comonotonic (Schmeidler (Econometrica, 1989)), that is
if there is a random variable Z, such that X = f(Z), and Y=g(Z)
for monotone f and g, then the upper Frechet bound holds and the
quantile function of X+Y is the sum of the quantile functions of
X and Y.  For "multiplicative linkage" take logs, presuming, of
course, that I'm not misinterpreting this phrase.  One can think
of comonotone X and Y as "perfectly concordant" in the language
of rank correlation.



url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Thu, 24 Jul 2003, Salvatore Barbaro wrote:

> Dear R-"helpers"!
>
> May I kindly ask the pure statistics-experts to help me for a
> purpose which first part is not directly concerned with R.
> Consider two distribution functions, say f and g. For both, the
> median is smaller than a half. Now, the multiplicative or additive
> linkage of both distribution leads to a new distribution function,
> say h, whereas the median of h is greater than a half. Does
> anybody know under which circumstances such a construction of h is
> possible (my intuition is that it depends on the correlation of f
> and g) or can anybody advice a helpful literature. Furthermore,
> does anybody know whether or how such a construction can be done
> with R. Thanks in advance.
>
> s.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From th50 at leicester.ac.uk  Thu Jul 24 14:42:42 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Thu, 24 Jul 2003 13:42:42 +0100
Subject: [R] median and joint distribution
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B682@saffron.cfs.le.ac.uk>

Dear Salvatore,

Assuming that you mean "convolution" when you write
"additive linkage", the answer is that there is no general 
answer. It will depend heavily on the joint distribution
of the two random variables.

Just to give a simple example, let X~f, Y~g, and
P(X=0.4)=P(Y=0.4)=1. Then, X and Y are independent, their
medians are <0.5, but there sum has a median >0.5.

Its different for "multiplicative", since in general from
X~f, Y~f, P(X>=0.5)<=0.5, and P(Y>=0.5)<=0.5 it follows
that P(XY>=0.5) <= P(X>=p or Y>=q) if p*q=0.5. Thus, if there
are numbers p and q with that property, such that
P(X>=p) + P(Y>=q) <= 0.5, then the median of XY will be <=0.5.

You might argue that there is a relationship between additive
and multiplicative scale through a log-transformation (note that
the median is stable under monotone transformations). However,
I assume there is no obvious formulation of the above statement
on the additive scale.

There is no way of carrying convolutions out in R directly; you'd
need to do numerical integration to do that, e.g. using 
integrate().

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Salvatore Barbaro [mailto:sbarbar at gwdg.de]
> Sent: 24 July 2003 12:56
> To: r-help at stat.math.ethz.ch
> Subject: [R] median and joint distribution
> 
> 
> Dear R-"helpers"!
> 
> May I kindly ask the pure statistics-experts to help me for a
> purpose which first part is not directly concerned with R.
> Consider two distribution functions, say f and g. For both, the
> median is smaller than a half. Now, the multiplicative or additive
> linkage of both distribution leads to a new distribution function,
> say h, whereas the median of h is greater than a half. Does
> anybody know under which circumstances such a construction of h is
> possible (my intuition is that it depends on the correlation of f
> and g) or can anybody advice a helpful literature. Furthermore,
> does anybody know whether or how such a construction can be done
> with R. Thanks in advance.
> 
> s.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From michael.watson at bbsrc.ac.uk  Thu Jul 24 15:16:26 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 24 Jul 2003 14:16:26 +0100
Subject: [R] R won't connect to the internet on Linux!
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0095A@cl-exsrv1.irad.bbsrc.ac.uk>

Hello Professor

If you are suggesting that I am simply missing the "http://" part of my cache URL, or that I am missing a trailing "/", then I pre-empted this response and it still doesn't work.

I have tried setting both http_proxy and HTTP_PROXY to all of:

wwwcache.bbsrc.ac.uk:8080
http://wwwcache.bbsrc.ac.uk:8080
wwwcache.bbsrc.ac.uk:8080/
http://wwwcache.bbsrc.ac.uk:8080/

and I still get the same response - R cannot open the URL.

And yes, that is thw right proxy address, I copied it straight from Netscape on the same computer, and Netscape connects to the internet fine.

Thanks
Mick
 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 24 July 2003 13:17
To: michael watson (IAH-C)
Cc: 'R-help at stat.math.ethz.ch '
Subject: Re: [R] R won't connect to the internet on Linux!


On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:

> OK, I really am struggling with this one!  Forgive me if I am being stupid....

> I am running R 1.7.1 on Suse Linux 8.1.  I connect to the internet
> through a proxy so I have:
> 
> IAHC-LINUX03:~ # echo $http_proxy
> wwwcache.bbsrc.ac.uk:8080
> IAHC-LINUX03:~ # echo $HTTP_PROXY
> wwwcache.bbsrc.ac.uk:8080
> 
> just in case ;-)
> 
> SO, i go into R and I get:
> 
> > source("http://www.bioconductor.org/getBioC.R")
> unable to connect to 'www.bioconductor.org' on port 80.
> Error in file(file, "r") : cannot open URL `http://www.bioconductor.org/getBioC.R'
> 
> OK so is R just not picking up my proxy setting?  

Your setting is wrong, so it is being ignored.  The help page says
quite explicitly

      The form of `"http_proxy"' should be `"http://proxy.dom.com/"' or
     `"http://proxy.dom.com:8080/"' where the port defaults to `80' and
     the trailing slash may be omitted.

> It seems to be trying
> port 80 on something, and I have specifically set it to port 8080 in my
> environment variables.  As far as I can see I have followed the
> reference manual suggestion, so does anyone else have one?

The problem is in your seeing, it seems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hb at maths.lth.se  Thu Jul 24 15:25:04 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 24 Jul 2003 15:25:04 +0200
Subject: [R] S3 and S4 classes
In-Reply-To: <3F1FCB1D.7080707@ariase.com>
Message-ID: <000001c351e7$04150ed0$7a0040d5@maths.lth.se>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Faisnel
> Sent: den 24 juli 2003 14:04
> To: John Chambers
> Cc: r-help at stat.math.ethz.ch; Duncan Murdoch
> Subject: Re: [R] S3 and S4 classes
> 

[snip]

> Thank you for your fast and useful answers. I've found an 
> interesting R 
> help page I had never seen before at 
> http://www.maths.lth.se/help/R/S3toS4 which gives additionnal 
> information about S3 and S4 classes.

Beware that I haven't updated this page since early 2002, it is based on
early versions of the 'methods' package and my early understandings of
it and is likely to be out of date (I'll put a note about this on the
page as soon as I get access to the server). 
 
> Regards,
> Laurent

Have a nice day!

Henrik Bengtsson
Lund University



From adavis at saipan.com  Thu Jul 24 15:27:08 2003
From: adavis at saipan.com (Alan Davis)
Date: Thu, 24 Jul 2003 23:27:08 +1000
Subject: [R] lattice: how to format axis labels?
In-Reply-To: <6r3cgxe3ig.fsf@bates4.stat.wisc.edu>
References: <OF3A14F9E2.AD9DB635-ON85256D6C.00415115@convergys.com>
	<12678.030723@eimb.ru> <6r3cgxe3ig.fsf@bates4.stat.wisc.edu>
Message-ID: <20030724232708.1c256f83.adavis@saipan.com>

On 23 Jul 2003 09:12:07 -0500
Douglas Bates <bates at stat.wisc.edu> wrote:

> High-level control of axes in xyplot is implemented by the scales
> argument to xyplot.  You can include components 'at' and 'labels' in
> a list given as the scales argument.  See ?xyplot.
> 
> Wladimir Eremeev <wl at eimb.ru> writes:
> 
> > jhcc> check out 'sprintf' for formating in a specific way.
> > 
> > This will not solve the problem.
> > I will have to specify the argument like labels=(...).
> > I would like to avoid it.
> > 
> > I wonder if there a key or option to make automatically appearing
> > labels be formatted in the mentioned way.
> > I haven't found it in the documentation.
> > 
> > =====================================================
> > jhcc>   I draw graphics with xyplot() function.
> > jhcc>   Labels on the y axis are appearing as follows: "1.5, 1, 0.5,
> > 0"
> > 
> > jhcc>   I'd like to have them to be "1.5, 1.0, 0.5, 0.0", i.e. with
> > fixed jhcc>   number of digits after the dot (one in this case).
> > 
> > jhcc>   Is there any way to do this without implicit specifying
> > labels?
> 
> I don't think so.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 
adavis at saipan.com                                     1-670-322-6580
    Alan E. Davis,  PMB 30, Box 10006, Saipan, MP 96950-8906, CNMI

I have steadily endeavored to keep my mind free, so as to give up any
hypothesis, however much beloved -- and I cannot resist forming one 
on every subject -- as soon as facts are shown to be opposed to it.  
                                  -- Charles Darwin (1809-1882)

The right to search for truth implies also a duty; one must not
conceal any part of what one has recognized to be true. 
                                  -- Albert Einstein 

As we enjoy great advantages from the inventions of others we should
be glad of an opportunity to serve others by any invention of
ours, and this we should do freely and generously.
                                  -- Benjamin Franklin



From jfox at mcmaster.ca  Thu Jul 24 15:57:24 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 24 Jul 2003 09:57:24 -0400
Subject: [R] Condition indexes and variance inflation factors
In-Reply-To: <sf1f97dd.073@MAIL.NDRI.ORG>
Message-ID: <5.1.0.14.2.20030724093530.01f7a2a0@127.0.0.1>

Dear Peter,

At 08:24 AM 7/24/2003 -0400, Peter Flom wrote:

>(1) I've never liked this approach for a model with a constant, where
>it
>makes more sense to me to centre the data. I realize that opinions
>differ
>here, but it seems to me that failing to centre the data conflates
>collinearity with numerical instability.
> >>>
>
>Opinions do differ.  A few years ago, I could have given more details
>(my dissertation was on this topic, but a lot of the details have
>disappeared from memory); I think, though, that Belsley is looking for a
>measure that deals not only with collinearity, but with several other
>problems, including numerical instability (the subtitle of his later
>book is Collinearity and Weak Data in Regression).  I remember being
>convinced that centering was generally not a good idea, but there are
>lots of people who disagree and who know a lot more statistics than I
>do.

To elaborate my remark slightly, in most problems the intercept is not of 
much interest. When the data are far from the origin, it's natural that the 
intercept isn't well estimated. When data are very far from the origin, 
computations with the uncentred data may be numerically unstable (depending 
upon how the computations are done) because of "collinearity with the 
intercept." If the real interest is in the coefficients other than the 
intercept, this seems to me purely a numerical artefact. The possibly more 
generally interesting sense of "collinearity" is imprecision in estimation 
due to strong relationships among the predictors.

. . .


><<<
>(4) I have doubts about the whole enterprise. Collinearity is one
>source of
>imprecision -- others are small sample size, homogeneous predictors,
>and
>large error variance. Aren't the coefficient standard errors the bottom
>
>line? If these are sufficiently small, why worry?
> >>>
>
>I think (correct me if I am wrong) that the s.e.s and the condition
>indices serve very different purposes.  The condition indices are
>supposed to determine if small changes in the input data could make big
>differences in the results.  Belsley provides some examples where a tiny
>change in the data results in completely different results (e.g.,
>different standard errors, different coefficients (even reversing sign)
>and so on).

Indeed, ill-conditioned data produce unstable numerical solutions (even 
affected by how the data are rounded), but condition indices aren't a 
particularly effective way of looking for instability in a more general 
sense. Consider, for example, Anscombe's famous simple-regression examples, 
which are in the data frame Quartet in the car package. The fourth example 
has a highly influential data point (number 8):


 > Quartet[, c("x4", "y4")]
    x4    y4
1   8  6.58
2   8  5.76
3   8  7.71
4   8  8.84
5   8  8.47
6   8  7.04
7   8  5.25
8  19 12.50
9   8  5.56
10  8  7.91
11  8  6.89

The regression of y4 on x4 isn't especially ill-conditioned (using the 
function I posted yesterday):

 > mod <- lm(y4 ~ x4)
 > belsley(mod)

Singular values:  1.394079 0.2377891
Condition indices:  1 5.86267

Variance-decomposition proportions
   (Intercept)    x4
1       0.028 0.028
2       0.972 0.972


but the 8th observation has an infinite Cook's D:

 > round(cooks.distance(mod), 2)
    1    2    3    4    5    6    7    8    9   10   11
0.01 0.06 0.02 0.14 0.09 0.00 0.12  Inf 0.08 0.03 0.00


Regards,
  John
-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From tlumley at u.washington.edu  Thu Jul 24 16:12:58 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 24 Jul 2003 07:12:58 -0700 (PDT)
Subject: [R] Dismal R performance of Athlon moble CPU?
In-Reply-To: <20030723212315.91307.qmail@web10504.mail.yahoo.com>
Message-ID: <Pine.A41.4.44.0307240706490.77892-100000@homer24.u.washington.edu>

On Wed, 23 Jul 2003, Jason Liao wrote:

> Thanks for Prof. Ripley and Andy for your technical explantion. It
> seems that that the real CPU speed has not advanced as fast as these
> Ghz or other performance indicator suggest.
>
> Yes, my program is totally CPU intensive.
>

It may not be even if you think it is. Note that much of the speed
increase that ATLAS achieves comes from optimal use of various levels of
cache. It's quite possible that your code is actually limited by memory
bandwidth.

In any case, I think R is often limited by integer rather than
floating point peformance.  Back in the days when I timed things in R, its
relative performance across PCs and Sparcs suggested that floating point
wasn't a big deal.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From michael.watson at bbsrc.ac.uk  Thu Jul 24 16:11:27 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 24 Jul 2003 15:11:27 +0100
Subject: [R] R won't connect to the internet on Linux!
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00961@cl-exsrv1.irad.bbsrc.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/4990ebb4/attachment.pl

From dirk.repsilber at gmx.de  Thu Jul 24 16:27:28 2003
From: dirk.repsilber at gmx.de (Dirk Repsilber)
Date: Thu, 24 Jul 2003 16:27:28 +0200
Subject: [R] data.frame subsetting
Message-ID: <3F1FECD0.5020209@gmx.de>

Hi,

is there advice how to subset a data.frame, where until R version 1.7.0 
it was possible to write

 > data[["subset"]]

which meant the same as

 > data$subset

(data is a data.frame). From 1.7.1 on this does not seem to work longer. 
Could there be a bug in downwards compatibility?

sincerely yours

Dirk



From suzette at sdac.harvard.edu  Thu Jul 24 16:39:57 2003
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Thu, 24 Jul 2003 10:39:57 -0400 (EDT)
Subject: [R] trellis plot question
Message-ID: <Pine.GSO.4.05.10307241035390.20715-100000@chaos.harvard.edu>


Greetings,

Does anyone know how to get an id number in the little header 
above each individual plot within a trellis plot?  The default
seems to be to print the word id and add a line indicating on
a linear scale where the current id sits.

Thanks in advance for any help you can send,

Suzette


=================================
Suzette Blanchard, Ph.D.
Research Scientist
Frontier Science Foundation
1244 Boylston St. Suite 303
Chestnut Hill, MA 02467
Email:  suzette at sdac.harvard.edu
Phone:  (617) 632-2007
Fax:    (617) 632-2001



From peterm at andrew.cmu.edu  Thu Jul 24 16:42:05 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Thu, 24 Jul 2003 10:42:05 -0400
Subject: [R] Problem w/ source
Message-ID: <BB45687D.5A95%peterm@andrew.cmu.edu>

I'm trying to use the source command to run commands from a file.  For
instance:  source("do.R"), where do.R is a file in the same directory in
which I am running R.

The contents of do.R are:

ls()
print("hello")
sum(y1)
mean(y1)


After  source("do.R"), all I see is:

> source("do.R")
[1] "hello"


I'm using the X11 version of R for Mac OS X (downloadable binary).  Does
anyone know how to get source to work?

Thanks!

Peter



From jg_liao at yahoo.com  Thu Jul 24 16:50:41 2003
From: jg_liao at yahoo.com (Jason Liao)
Date: Thu, 24 Jul 2003 07:50:41 -0700 (PDT)
Subject: [R] R benchmark, moble Pentium III, 1.13 GHs
In-Reply-To: <Pine.A41.4.44.0307240706490.77892-100000@homer24.u.washington.edu>
Message-ID: <20030724145041.29721.qmail@web10507.mail.yahoo.com>

I got the following using the benchmark program at
http://www.sciviews.org/other/benchmark.htm

under Windows 2000 prof., 256 MB of RAM

 R Benchmark 2
   =============
Number of times each test is run__________________________:  3

   I. Matrix calculation
   ---------------------
Creation, transp., deformation of a 1500x1500 matrix (sec): 
1.97333333333333 
800x800 normal distributed random matrix ^1000______ (sec): 
2.80999999999999 
Sorting of 2,000,000 random values__________________ (sec): 
1.20333333333333 
700x700 cross-product matrix (b = a' * a)___________ (sec): 
1.84000000000000 
Linear regression over a 600x600 matrix (c = a \ b') (sec): 
2.43666666666666 
                      --------------------------------------------
                 Trimmed geom. mean (2 extremes eliminated): 
2.06825841067831 

   II. Matrix functions
   --------------------
FFT over 800,000 random values______________________ (sec): 
1.61666666666666 
Eigenvalues of a 320x320 random matrix______________ (sec): 
1.20333333333333 
Error in eval(expr, envir, enclos) : couldn't find function
"det.Matrix"
Timing stopped at: 0 0 0 NA NA 
> 



=====
Jason G. Liao, Ph.D.
Division of Biometrics
University of Medicine and Dentistry of New Jersey
335 George Street, Suite 2200
New Brunswick, NJ 08903-2688
phone (732) 235-8611, fax (732) 235-9777
http://www.geocities.com/jg_liao



From jonck at vanderkogel.net  Thu Jul 24 16:55:55 2003
From: jonck at vanderkogel.net (Jonck van der Kogel)
Date: Thu, 24 Jul 2003 16:55:55 +0200
Subject: [R] Plotting math functions
Message-ID: <ED544D42-BDE6-11D7-921D-0005026E2B43@vanderkogel.net>

Hi all,
I was wondering whether it is possible to plot math functions, for 
example sin, cos or a Gaussian type function, in R, and if so, how to 
do it. I have been searching through the archives and the R manual but 
had no luck in finding any hints on how to go about this.
Any help is much appreciated!
Thanks, Jonck



From ligges at statistik.uni-dortmund.de  Thu Jul 24 17:04:00 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jul 2003 17:04:00 +0200
Subject: [R] Plotting math functions
In-Reply-To: <ED544D42-BDE6-11D7-921D-0005026E2B43@vanderkogel.net>
References: <ED544D42-BDE6-11D7-921D-0005026E2B43@vanderkogel.net>
Message-ID: <3F1FF560.4070603@statistik.uni-dortmund.de>

Jonck van der Kogel wrote:
> Hi all,
> I was wondering whether it is possible to plot math functions, for 
> example sin, cos or a Gaussian type function, in R, and if so, how to do 
> it. I have been searching through the archives and the R manual but had 
> no luck in finding any hints on how to go about this.
> Any help is much appreciated!
> Thanks, Jonck
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

See ?curve

Uwe Ligges



From flom at ndri.org  Thu Jul 24 17:03:40 2003
From: flom at ndri.org (Peter Flom)
Date: Thu, 24 Jul 2003 11:03:40 -0400
Subject: [R] Condition indexes and variance inflation factors
Message-ID: <sf1fbd37.060@MAIL.NDRI.ORG>

Dear John

An interesting discussion!

I would be the last to suggest ignoring such diagnostics as Cook's D;
as you point out, it diagnoses a problem which condition indices do not:
Whether a point is influential.

OTOH, condition indices diagnose a problem which Cook's D does not:
Would shifting the data slightly change the results.

Consider the data given in Belsley (1991) on p. 5

y <-   c( 3.3979, 1.6094, 3.7131, 1.6767, 0.0419, 3.3768, 1.1661,
0.4701)
x2a <- c(-3.138, -0.297, -4.582, 0.301, 2.729, -4.836, 0.065, 4.102)
x2b <- c(-3.136, -0.296, -4.581, 0.300, 2.730, -4.834, 0.064, 4.103)
x3a <- c(1.286, 0.250, 1.247, 0.498, -0.280, 0.350, 0.208, 1.069)
x3b <- c(1.288, 0.251, 1.246, 0.498, -0.281, 0.349, 0.206, 1.069)
x4a <- c(0.169, 0.044, 0.109, 0.117, 0.035, -0.094, 0.047, 0.375)
x4b <- c(0.170, 0.043, 0.108, 0.118, 0.036, -0.093, 0.048, 0.376)

where x2a , x3a and x4a are very similar to x2b, x3b and x4b,
respecttively, and where both are generated from

y = 1.2I  - 0.4 x2 + 0.6x3 + 0.9x4 + e

e ~ N(0, 0.01)

Then 
modela <- lm(y~ x2a + x3a + x4a)
and 
modelb <- lm(y~x2b + x3b + x4b)

give radically different results, with neither having any significant
parameters other than the intercept.  Admittedly, the standard errors
for a couple of the parameters are large.  But why are they large? I
have certainly dealt with models with large standard errors that have
nothing to do with collinearity.

here, the function PI.lm (supplied by Juergen Gross) gives huge
condition indices, and indicates that the nature of the problem is that
all three of the x variables are highly collinear.

Variance-Decomposition Proportions for
Scaled Condition Indexes:

    (Intercept) x2b x3b x4b
1        0.0494   0   0   0
1        0.0009   0   0   0
3        0.8101   0   0   0
464      0.1396   1   1   1


Regards

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ligges at statistik.uni-dortmund.de  Thu Jul 24 17:08:12 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jul 2003 17:08:12 +0200
Subject: [R] Problem w/ source
In-Reply-To: <BB45687D.5A95%peterm@andrew.cmu.edu>
References: <BB45687D.5A95%peterm@andrew.cmu.edu>
Message-ID: <3F1FF65C.8030901@statistik.uni-dortmund.de>

Peter Muhlberger wrote:

> I'm trying to use the source command to run commands from a file.  For
> instance:  source("do.R"), where do.R is a file in the same directory in
> which I am running R.
> 
> The contents of do.R are:
> 
> ls()
> print("hello")
> sum(y1)
> mean(y1)
> 
> 
> After  source("do.R"), all I see is:
> 
> 
>>source("do.R")
> 
> [1] "hello"
> 
> 
> I'm using the X11 version of R for Mac OS X (downloadable binary).  Does
> anyone know how to get source to work?
> 
> Thanks!
> 
> Peter
> 


Works perfectly, but no automatic print() is executed as the default 
when sourcing. When you want the other objects to be printed either use
   print(ls())
   print(sum(y1))
   ...
or use
   source("do.R", print.eval = TRUE)
as described in ?source.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Jul 24 17:10:07 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jul 2003 17:10:07 +0200
Subject: [R] data.frame subsetting
In-Reply-To: <3F1FECD0.5020209@gmx.de>
References: <3F1FECD0.5020209@gmx.de>
Message-ID: <3F1FF6CF.3070907@statistik.uni-dortmund.de>

Dirk Repsilber wrote:

> Hi,
> 
> is there advice how to subset a data.frame, where until R version 1.7.0 
> it was possible to write
> 
>  > data[["subset"]]
> 
> which meant the same as
> 
>  > data$subset
> 
> (data is a data.frame). From 1.7.1 on this does not seem to work longer. 
> Could there be a bug in downwards compatibility?
> 
> sincerely yours

Works for me. Are you sure the object "data" is a data.frame in your case?

Uwe Ligges



From RBaskin at ahrq.gov  Thu Jul 24 17:09:36 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Thu, 24 Jul 2003 11:09:36 -0400
Subject: [R] Plotting math functions
Message-ID: <3598558AD728D41183350008C7CF291C0F16B8C7@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/f9abd1a9/attachment.pl

From dmurdoch at pair.com  Thu Jul 24 17:21:57 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jul 2003 11:21:57 -0400
Subject: [R] data.frame subsetting
In-Reply-To: <3F1FECD0.5020209@gmx.de>
References: <3F1FECD0.5020209@gmx.de>
Message-ID: <s6uvhvcmc60jut1mig8thit50711b3dvcb@4ax.com>

On Thu, 24 Jul 2003 16:27:28 +0200, Dirk Repsilber
<dirk.repsilber at gmx.de> wrote :

>Hi,
>
>is there advice how to subset a data.frame, where until R version 1.7.0 
>it was possible to write
>
> > data[["subset"]]
>
>which meant the same as
>
> > data$subset
>
>(data is a data.frame). From 1.7.1 on this does not seem to work longer. 
>Could there be a bug in downwards compatibility?

What are you trying to achieve?  The notation you give still works if
"subset" is one of the column names; I don't think it ever worked if
"subset" was a logical vector, like its name would suggest.

Duncan Murdoch



From bates at stat.wisc.edu  Thu Jul 24 17:20:22 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 24 Jul 2003 15:20:22 -0000
Subject: [R] data.frame subsetting
In-Reply-To: <3F1FECD0.5020209@gmx.de>
References: <3F1FECD0.5020209@gmx.de>
Message-ID: <6r4r1cj6io.fsf@bates4.stat.wisc.edu>

Still works for me in both 1.7.1 and 1.8.0 (in development)

> data(women)
> women[["height"]]
 [1] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72
> women$height
 [1] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72

Can you provide an example of how it is failing for you?


Dirk Repsilber <dirk.repsilber at gmx.de> writes:

> Hi,
> 
> is there advice how to subset a data.frame, where until R version
> 1.7.0 it was possible to write
> 
> 
>  > data[["subset"]]
> 
> which meant the same as
> 
>  > data$subset
> 
> (data is a data.frame). From 1.7.1 on this does not seem to work
> longer. Could there be a bug in downwards compatibility?
> 
> 
> sincerely yours
> 
> Dirk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From dmurdoch at pair.com  Thu Jul 24 17:25:16 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jul 2003 11:25:16 -0400
Subject: [R] Problem w/ source
In-Reply-To: <BB45687D.5A95%peterm@andrew.cmu.edu>
References: <BB45687D.5A95%peterm@andrew.cmu.edu>
Message-ID: <eguvhv4la35u4oqem42omph6nobl57q6r4@4ax.com>

On Thu, 24 Jul 2003 10:42:05 -0400, Peter Muhlberger
<peterm at andrew.cmu.edu> wrote :

>I'm trying to use the source command to run commands from a file.  For
>instance:  source("do.R"), where do.R is a file in the same directory in
>which I am running R.
>
>The contents of do.R are:
>
>ls()
>print("hello")
>sum(y1)
>mean(y1)
>
>
>After  source("do.R"), all I see is:
>
>> source("do.R")
>[1] "hello"
>
>
>I'm using the X11 version of R for Mac OS X (downloadable binary).  Does
>anyone know how to get source to work?

You probably want 

source("do.R", echo = TRUE)

or maybe not; see ?source.

Duncan Murdoch



From bates at stat.wisc.edu  Thu Jul 24 17:23:16 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 24 Jul 2003 15:23:16 -0000
Subject: [R] trellis plot question
In-Reply-To: <Pine.GSO.4.05.10307241035390.20715-100000@chaos.harvard.edu>
References: <Pine.GSO.4.05.10307241035390.20715-100000@chaos.harvard.edu>
Message-ID: <6ry8yohrth.fsf@bates4.stat.wisc.edu>

Suzette Blanchard <suzette at sdac.harvard.edu> writes:

> Does anyone know how to get an id number in the little header 
> above each individual plot within a trellis plot?  The default
> seems to be to print the word id and add a line indicating on
> a linear scale where the current id sits.

See the description of the strip argument in ?xyplot and the
documentation for strip.default.  There are currently 6 different
styles available or you can add your own.



From dmurdoch at pair.com  Thu Jul 24 17:27:59 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jul 2003 11:27:59 -0400
Subject: [R] Plotting math functions
In-Reply-To: <ED544D42-BDE6-11D7-921D-0005026E2B43@vanderkogel.net>
References: <ED544D42-BDE6-11D7-921D-0005026E2B43@vanderkogel.net>
Message-ID: <fjuvhvoiivgpd3f4s2paa0erudtkpar9ft@4ax.com>

On Thu, 24 Jul 2003 16:55:55 +0200, Jonck van der Kogel
<jonck at vanderkogel.net> wrote :

>Hi all,
>I was wondering whether it is possible to plot math functions, for 
>example sin, cos or a Gaussian type function, in R, and if so, how to 
>do it. I have been searching through the archives and the R manual but 
>had no luck in finding any hints on how to go about this.
>Any help is much appreciated!

You can use

  plot(sin, from=-10, to=10)

This calls plot.function; look at ?plot.function to read about the
options.

Duncan Murdoch



From bates at stat.wisc.edu  Thu Jul 24 17:27:10 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 24 Jul 2003 15:27:10 -0000
Subject: [R] Problem w/ source
In-Reply-To: <BB45687D.5A95%peterm@andrew.cmu.edu>
References: <BB45687D.5A95%peterm@andrew.cmu.edu>
Message-ID: <6ru19chrmv.fsf@bates4.stat.wisc.edu>

Well, we feel that source does work the way that it is documented to
work.  Please read the documentation and notice that the entire file
is evaluated as one step in the read-eval-print loop.  If you want to
print the results of individual function calls you will need to change
your script to

print(ls())
print("hello")
print(sum(y1))
print(mean(y1))

Peter Muhlberger <peterm at andrew.cmu.edu> writes:

> I'm trying to use the source command to run commands from a file.  For
> instance:  source("do.R"), where do.R is a file in the same directory in
> which I am running R.
> 
> The contents of do.R are:
> 
> ls()
> print("hello")
> sum(y1)
> mean(y1)
> 
> 
> After  source("do.R"), all I see is:
> 
> > source("do.R")
> [1] "hello"
> 
> 
> I'm using the X11 version of R for Mac OS X (downloadable binary).  Does
> anyone know how to get source to work?
> 
> Thanks!
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From rnaidoo at ualberta.ca  Thu Jul 24 19:18:09 2003
From: rnaidoo at ualberta.ca (Robin Naidoo)
Date: Thu, 24 Jul 2003 11:18:09 -0600
Subject: [R] Integer programming in R
Message-ID: <3F1FC071.24495.69904E@localhost>

Dear all,

I am a relative newcomer to the R language, and am sussing out 
the possibilities of using R to do integer programming (which I am 
also new to).  Is there something along the lines of the NUOPT S-
PLUS package that is available, or on the way, for R?  Are there 
other options for integer programming in R?  

Thanks very much in advance,

Robin Naidoo
University of Alberta
Edmonton, AB, Canada



From peterm at andrew.cmu.edu  Thu Jul 24 19:27:35 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Thu, 24 Jul 2003 13:27:35 -0400
Subject: [R] Problem w/ source
Message-ID: <BB458F47.5AA8%peterm@andrew.cmu.edu>

Thanks to everyone for their suggestions on getting source to print!  It
seems not everyone was aware of a couple options that gets source to print
out everything.  I'm now using the following command:

source("do.R", print.eval=TRUE, echo=TRUE)



From tdlong at uci.edu  Thu Jul 24 21:06:21 2003
From: tdlong at uci.edu (Tony Long)
Date: Thu, 24 Jul 2003 12:06:21 -0700
Subject: [R] scatterplot smoothing using gam
Message-ID: <a05210601bb45d8fbfd81@[128.200.28.180]>

All:

	I am trying to use gam in a scatterplot smoothing problem. 
The data being smoothed have greater 1000 observation and have 
multiple "humps".  I can smooth the data fine using a function 
something like:

out <- ksmooth(x,y,"normal",bandwidth=0.25)
plot(x,out$y,type="l")

The problem is when I try to fit the same data using gam

out <- predict.gam(gam(y~s(x)),se=TRUE)
plot(x,out$fit,type="l")

I only seem to get fits that would correspond to "big" bandwidths 
using ksmooth, and straight lines are always fit to the data.  I do 
not appear to appreciate how to "control bandwidth" using gam.  As 
even if I apply something like the gam model above to the smoothed 
"out$y" generated using ksmooth it tends to flatten out the smoothing 
curve.
-- 
###########################

Tony Long

Ecology and Evolutionary Biology
Steinhaus Hall
University of California at Irvine
Irvine, CA
92697-2525

Tel:  (949) 824-2562   (office)
Tel:  (949) 824-5994   (lab)
Fax: (949) 824-2181

email:  tdlong at uci.edu
http://hjmuller.bio.uci.edu/~labhome/



From adiamond at fas.harvard.edu  Thu Jul 24 21:44:34 2003
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Thu, 24 Jul 2003 15:44:34 -0400 (EDT)
Subject: [R] wireframe: how to remove the frame around my plot?
Message-ID: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>

Hi,

I've got a wireframe 3D surface plot, but I don't want a frame around it.
Is there any way to remove the frame, or (worst case)
change the color of the frame to the background color (which looks like
grey).

I'm using ver 1.7.1

I've tried frame.plot = F, but that doesn't seem to work for 'wireframe'.

Many thanks,

Alexis Diamond



From jerome at hivnet.ubc.ca  Thu Jul 24 22:07:00 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 24 Jul 2003 13:07:00 -0700
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
References: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
Message-ID: <200307242013.NAA17437@hivnet.ubc.ca>


You can specify some options in "par.box". Use col=NA to make the frame 
transparent. See example below (which was modified from the help file).
See also the "scales" parameter if you want to remove the arrows as well.

Cheers,
Jerome

     library(lattice)
     x <- seq(-pi, pi, len = 20)
     y <- seq(-pi, pi, len = 20)
     g <- expand.grid(x = x, y = y)
     g$z <- sin(sqrt(g$x^2 + g$y^2))
     wireframe(z ~ x * y, g, drape = TRUE,
               perspective = FALSE,
               aspect = c(3,1), colorkey = FALSE,
               par.box = list(col=NA))



On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> Hi,
>
> I've got a wireframe 3D surface plot, but I don't want a frame around
> it. Is there any way to remove the frame, or (worst case)
> change the color of the frame to the background color (which looks like
> grey).
>
> I'm using ver 1.7.1
>
> I've tried frame.plot = F, but that doesn't seem to work for
> 'wireframe'.
>
> Many thanks,
>
> Alexis Diamond
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Thu Jul 24 22:06:35 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 24 Jul 2003 16:06:35 -0400
Subject: [R] scatterplot smoothing using gam
Message-ID: <3A822319EB35174CA3714066D590DCD50205C929@usrymx25.merck.com>

This is gam() in the mgcv package, I presume?  That does regression splines,
AFAIK.

You don't need gam() to do scatterplot smoothing either.  If you want se
bands, you can get that from locfit(), in the locfit package.  There are
several other packages that does kernel/local polynomial smoothing, but I
don't know if they provide se bands.

HTH,
Andy



> -----Original Message-----
> From: Tony Long [mailto:tdlong at uci.edu] 
> Sent: Thursday, July 24, 2003 3:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] scatterplot smoothing using gam
> 
> 
> All:
> 
> 	I am trying to use gam in a scatterplot smoothing problem. 
> The data being smoothed have greater 1000 observation and have 
> multiple "humps".  I can smooth the data fine using a function 
> something like:
> 
> out <- ksmooth(x,y,"normal",bandwidth=0.25)
> plot(x,out$y,type="l")
> 
> The problem is when I try to fit the same data using gam
> 
> out <- predict.gam(gam(y~s(x)),se=TRUE)
> plot(x,out$fit,type="l")
> 
> I only seem to get fits that would correspond to "big" bandwidths 
> using ksmooth, and straight lines are always fit to the data.  I do 
> not appear to appreciate how to "control bandwidth" using gam.  As 
> even if I apply something like the gam model above to the smoothed 
> "out$y" generated using ksmooth it tends to flatten out the smoothing 
> curve.
> -- 
> ###########################
> 
> Tony Long
> 
> Ecology and Evolutionary Biology
> Steinhaus Hall
> University of California at Irvine
> Irvine, CA
> 92697-2525
> 
> Tel:  (949) 824-2562   (office)
> Tel:  (949) 824-5994   (lab)
> Fax: (949) 824-2181
> 
> email:  tdlong at uci.edu
> http://hjmuller.bio.uci.edu/~labhome/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice: This e-mail message, together with any attachments, ...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Jul 24 22:26:52 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jul 2003 22:26:52 +0200
Subject: [R] wireframe: how to remove the frame around my plot?
References: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
	<200307242013.NAA17437@hivnet.ubc.ca>
Message-ID: <3F20410C.84C822C0@statistik.uni-dortmund.de>

Jerome Asselin wrote:
> 
> You can specify some options in "par.box". Use col=NA to make the frame
> transparent. See example below (which was modified from the help file).
> See also the "scales" parameter if you want to remove the arrows as well.
> 
> Cheers,
> Jerome
> 
>      library(lattice)
>      x <- seq(-pi, pi, len = 20)
>      y <- seq(-pi, pi, len = 20)
>      g <- expand.grid(x = x, y = y)
>      g$z <- sin(sqrt(g$x^2 + g$y^2))
>      wireframe(z ~ x * y, g, drape = TRUE,
>                perspective = FALSE,
>                aspect = c(3,1), colorkey = FALSE,
>                par.box = list(col=NA))

Which doesn't work on device 
 trellis.device(windows)
but works well on, e.g., 
 trellis.device(postscript)

Deepayan, since I don't have the time to check that right now: Is this
bug known? Is this a bug in R, grid or lattice?

Uwe


> On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > Hi,
> >
> > I've got a wireframe 3D surface plot, but I don't want a frame around
> > it. Is there any way to remove the frame, or (worst case)
> > change the color of the frame to the background color (which looks like
> > grey).
> >
> > I'm using ver 1.7.1
> >
> > I've tried frame.plot = F, but that doesn't seem to work for
> > 'wireframe'.
> >
> > Many thanks,
> >
> > Alexis Diamond
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From adiamond at fas.harvard.edu  Thu Jul 24 22:27:51 2003
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Thu, 24 Jul 2003 16:27:51 -0400 (EDT)
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <200307242013.NAA17437@hivnet.ubc.ca>
Message-ID: <Pine.OSF.4.44.0307241619140.5790-100000@is06.fas.harvard.edu>

hi jerome,

thank you for your quick reply.
your advice removes the 3D box in which the 3D plot is generated,
but i like THAT box.  (sorry for being unclear earlier)

what i want to do is remove the 2D frame (a box of thin black lines) that
circumscribes all of my plot area.  any ideas?

thanks again,

alexis

On Thu, 24 Jul 2003, Jerome Asselin wrote:

>
> You can specify some options in "par.box". Use col=NA to make the frame
> transparent. See example below (which was modified from the help file).
> See also the "scales" parameter if you want to remove the arrows as well.
>
> Cheers,
> Jerome
>
>      library(lattice)
>      x <- seq(-pi, pi, len = 20)
>      y <- seq(-pi, pi, len = 20)
>      g <- expand.grid(x = x, y = y)
>      g$z <- sin(sqrt(g$x^2 + g$y^2))
>      wireframe(z ~ x * y, g, drape = TRUE,
>                perspective = FALSE,
>                aspect = c(3,1), colorkey = FALSE,
>                par.box = list(col=NA))
>
>
>
> On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > Hi,
> >
> > I've got a wireframe 3D surface plot, but I don't want a frame around
> > it. Is there any way to remove the frame, or (worst case)
> > change the color of the frame to the background color (which looks like
> > grey).
> >
> > I'm using ver 1.7.1
> >
> > I've tried frame.plot = F, but that doesn't seem to work for
> > 'wireframe'.
> >
> > Many thanks,
> >
> > Alexis Diamond
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jerome at hivnet.ubc.ca  Thu Jul 24 22:37:12 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 24 Jul 2003 13:37:12 -0700
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <200307242013.NAA17437@hivnet.ubc.ca>
References: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
	<200307242013.NAA17437@hivnet.ubc.ca>
Message-ID: <200307242043.NAA18542@hivnet.ubc.ca>


Here is another try... I've added col=NA at the end.

     library(lattice)
     x <- seq(-pi, pi, len = 20)
     y <- seq(-pi, pi, len = 20)
     g <- expand.grid(x = x, y = y)
     g$z <- sin(sqrt(g$x^2 + g$y^2))
     wireframe(z ~ x * y, g, drape = TRUE,
               perspective = FALSE,
               aspect = c(3,1), colorkey = FALSE,
               par.box = list(col=NA),col=NA)

If that still doesn't do it, could you provide us with a toy example (like 
I did) which does what you want (but with only the frame you're talking 
about on it)? Then we could understand better what you want to do rather 
than guess.

Cheers,
Jerome


On July 24, 2003 01:07 pm, Jerome Asselin wrote:
> You can specify some options in "par.box". Use col=NA to make the frame
> transparent. See example below (which was modified from the help file).
> See also the "scales" parameter if you want to remove the arrows as
> well.
>
> Cheers,
> Jerome
>
>      library(lattice)
>      x <- seq(-pi, pi, len = 20)
>      y <- seq(-pi, pi, len = 20)
>      g <- expand.grid(x = x, y = y)
>      g$z <- sin(sqrt(g$x^2 + g$y^2))
>      wireframe(z ~ x * y, g, drape = TRUE,
>                perspective = FALSE,
>                aspect = c(3,1), colorkey = FALSE,
>                par.box = list(col=NA))
>
> On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > Hi,
> >
> > I've got a wireframe 3D surface plot, but I don't want a frame around
> > it. Is there any way to remove the frame, or (worst case)
> > change the color of the frame to the background color (which looks
> > like grey).
> >
> > I'm using ver 1.7.1
> >
> > I've tried frame.plot = F, but that doesn't seem to work for
> > 'wireframe'.
> >
> > Many thanks,
> >
> > Alexis Diamond
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jbowers at csm.berkeley.edu  Thu Jul 24 23:23:17 2003
From: jbowers at csm.berkeley.edu (Jake Bowers)
Date: Thu, 24 Jul 2003 14:23:17 -0700 (PDT)
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <Pine.OSF.4.44.0307241619140.5790-100000@is06.fas.harvard.edu>
References: <Pine.OSF.4.44.0307241619140.5790-100000@is06.fas.harvard.edu>
Message-ID: <Pine.GSO.4.51.0307241421590.26625@csm.Berkeley.EDU>

Hi Y'all,

Alexis, I think Jerome's example works except for one change:

This one has a box but no "wires":

wireframe(z ~ x * y, g, drape = TRUE,
                perspective = FALSE,
                aspect = c(3,1), colorkey = FALSE,
                par.box = list(col=1),col=NA)

versus with no box and no "wires":

wireframe(z ~ x * y, g, drape = TRUE,
               perspective = FALSE,
                aspect = c(3,1), colorkey = FALSE,
                par.box = list(col=NA),col=NA)

Hope this helps!

Jake
-----
Jake Bowers
Dept of Political Science
University of Michigan

On Thu, 24 Jul 2003, Alexis J. Diamond wrote:

> hi jerome,
>
> thank you for your quick reply.
> your advice removes the 3D box in which the 3D plot is generated,
> but i like THAT box.  (sorry for being unclear earlier)
>
> what i want to do is remove the 2D frame (a box of thin black lines) that
> circumscribes all of my plot area.  any ideas?
>
> thanks again,
>
> alexis
>
> On Thu, 24 Jul 2003, Jerome Asselin wrote:
>
> >
> > You can specify some options in "par.box". Use col=NA to make the frame
> > transparent. See example below (which was modified from the help file).
> > See also the "scales" parameter if you want to remove the arrows as well.
> >
> > Cheers,
> > Jerome
> >
> >      library(lattice)
> >      x <- seq(-pi, pi, len = 20)
> >      y <- seq(-pi, pi, len = 20)
> >      g <- expand.grid(x = x, y = y)
> >      g$z <- sin(sqrt(g$x^2 + g$y^2))
> >      wireframe(z ~ x * y, g, drape = TRUE,
> >                perspective = FALSE,
> >                aspect = c(3,1), colorkey = FALSE,
> >                par.box = list(col=NA))
> >
> >
> >
> > On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > > Hi,
> > >
> > > I've got a wireframe 3D surface plot, but I don't want a frame around
> > > it. Is there any way to remove the frame, or (worst case)
> > > change the color of the frame to the background color (which looks like
> > > grey).
> > >
> > > I'm using ver 1.7.1
> > >
> > > I've tried frame.plot = F, but that doesn't seem to work for
> > > 'wireframe'.
> > >
> > > Many thanks,
> > >
> > > Alexis Diamond
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ivo.welch at yale.edu  Thu Jul 24 23:39:00 2003
From: ivo.welch at yale.edu (ivo welch)
Date: Thu, 24 Jul 2003 17:39:00 -0400
Subject: [R] postscript device: pch='.' size?
Message-ID: <3F2051F4.1010901@yale.edu>


hi ladies and gents:

I am using R 1.7.0.  Alas, the pch="." in the postscript device is too 
large for me, but apparently not scaleable via cex.  Maybe a bug, though 
I read a complaing about this in the list archives from 1999, and given 
that it is still around, maybe it is a "feature."

Now, I would be happy with pch="o" and cex=0.05, but for some strange 
reason, the resulting .eps file then becomes 3 times the size (200K, 
rather than 70K).  For web-downloadable files, this can become painful.

Are there any remedies?

Regards,

/iaw



From adiamond at fas.harvard.edu  Thu Jul 24 23:41:39 2003
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Thu, 24 Jul 2003 17:41:39 -0400 (EDT)
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <Pine.GSO.4.51.0307241421590.26625@csm.Berkeley.EDU>
Message-ID: <Pine.OSF.4.44.0307241735280.5790-100000@is06.fas.harvard.edu>

hi jake,

thanks for your e-mail.

what i am actually trying to do is remove the 'picture frame' 2D box (the
super-frame) that circumscribes the entire figure-- for example, the left
vertical side of this box i'm concerned about is to the left of the 'z' axis label.
i hope i'm finally making my query clear.

both of your examples below retain this pictureframe box (jerome's
examples do too), when the output is viewed as a .eps file in ghostcript
viewer, so i am still stuck,
unfortunately.

there's got to be a parameter that modifies this picureframe box's color
and style, right?

thanks again,

alexis


On Thu, 24 Jul 2003, Jake Bowers wrote:

> Hi Y'all,
>
> Alexis, I think Jerome's example works except for one change:
>
> This one has a box but no "wires":
>
> wireframe(z ~ x * y, g, drape = TRUE,
>                 perspective = FALSE,
>                 aspect = c(3,1), colorkey = FALSE,
>                 par.box = list(col=1),col=NA)
>
> versus with no box and no "wires":
>
> wireframe(z ~ x * y, g, drape = TRUE,
>                perspective = FALSE,
>                 aspect = c(3,1), colorkey = FALSE,
>                 par.box = list(col=NA),col=NA)
>
> Hope this helps!
>
> Jake
> -----
> Jake Bowers
> Dept of Political Science
> University of Michigan
>
> On Thu, 24 Jul 2003, Alexis J. Diamond wrote:
>
> > hi jerome,
> >
> > thank you for your quick reply.
> > your advice removes the 3D box in which the 3D plot is generated,
> > but i like THAT box.  (sorry for being unclear earlier)
> >
> > what i want to do is remove the 2D frame (a box of thin black lines) that
> > circumscribes all of my plot area.  any ideas?
> >
> > thanks again,
> >
> > alexis
> >
> > On Thu, 24 Jul 2003, Jerome Asselin wrote:
> >
> > >
> > > You can specify some options in "par.box". Use col=NA to make the frame
> > > transparent. See example below (which was modified from the help file).
> > > See also the "scales" parameter if you want to remove the arrows as well.
> > >
> > > Cheers,
> > > Jerome
> > >
> > >      library(lattice)
> > >      x <- seq(-pi, pi, len = 20)
> > >      y <- seq(-pi, pi, len = 20)
> > >      g <- expand.grid(x = x, y = y)
> > >      g$z <- sin(sqrt(g$x^2 + g$y^2))
> > >      wireframe(z ~ x * y, g, drape = TRUE,
> > >                perspective = FALSE,
> > >                aspect = c(3,1), colorkey = FALSE,
> > >                par.box = list(col=NA))
> > >
> > >
> > >
> > > On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > > > Hi,
> > > >
> > > > I've got a wireframe 3D surface plot, but I don't want a frame around
> > > > it. Is there any way to remove the frame, or (worst case)
> > > > change the color of the frame to the background color (which looks like
> > > > grey).
> > > >
> > > > I'm using ver 1.7.1
> > > >
> > > > I've tried frame.plot = F, but that doesn't seem to work for
> > > > 'wireframe'.
> > > >
> > > > Many thanks,
> > > >
> > > > Alexis Diamond
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>



From jerome at hivnet.ubc.ca  Thu Jul 24 23:54:15 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 24 Jul 2003 14:54:15 -0700
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <Pine.OSF.4.44.0307241735280.5790-100000@is06.fas.harvard.edu>
References: <Pine.OSF.4.44.0307241735280.5790-100000@is06.fas.harvard.edu>
Message-ID: <200307242200.PAA21661@hivnet.ubc.ca>



Finally, I think this works. The function trellis.par.get() contains all 
default values of the parameters. I had to reset the option "axis.line" as 
below. The bounding (square) box is made transparent. See 
trellis.par.get() for the list of all default parameter values.

Cheers,
Jerome

     library(lattice)
     x <- seq(-pi, pi, len = 20)
     y <- seq(-pi, pi, len = 20)
     g <- expand.grid(x = x, y = y)
     g$z <- sin(sqrt(g$x^2 + g$y^2))
     trellis.par.set("axis.line",list(col=NA,lty=1,lwd=1))
     wireframe(z ~ x * y, g, drape = TRUE,
               perspective = FALSE,
               aspect = c(3,1), colorkey = FALSE)


On July 24, 2003 02:41 pm, Alexis J. Diamond wrote:
> hi jake,
>
> thanks for your e-mail.
>
> what i am actually trying to do is remove the 'picture frame' 2D box
> (the super-frame) that circumscribes the entire figure-- for example,
> the left vertical side of this box i'm concerned about is to the left of
> the 'z' axis label. i hope i'm finally making my query clear.
>
> both of your examples below retain this pictureframe box (jerome's
> examples do too), when the output is viewed as a .eps file in ghostcript
> viewer, so i am still stuck,
> unfortunately.
>
> there's got to be a parameter that modifies this picureframe box's color
> and style, right?
>
> thanks again,
>
> alexis
>
> On Thu, 24 Jul 2003, Jake Bowers wrote:
> > Hi Y'all,
> >
> > Alexis, I think Jerome's example works except for one change:
> >
> > This one has a box but no "wires":
> >
> > wireframe(z ~ x * y, g, drape = TRUE,
> >                 perspective = FALSE,
> >                 aspect = c(3,1), colorkey = FALSE,
> >                 par.box = list(col=1),col=NA)
> >
> > versus with no box and no "wires":
> >
> > wireframe(z ~ x * y, g, drape = TRUE,
> >                perspective = FALSE,
> >                 aspect = c(3,1), colorkey = FALSE,
> >                 par.box = list(col=NA),col=NA)
> >
> > Hope this helps!
> >
> > Jake
> > -----
> > Jake Bowers
> > Dept of Political Science
> > University of Michigan
> >
> > On Thu, 24 Jul 2003, Alexis J. Diamond wrote:
> > > hi jerome,
> > >
> > > thank you for your quick reply.
> > > your advice removes the 3D box in which the 3D plot is generated,
> > > but i like THAT box.  (sorry for being unclear earlier)
> > >
> > > what i want to do is remove the 2D frame (a box of thin black lines)
> > > that circumscribes all of my plot area.  any ideas?
> > >
> > > thanks again,
> > >
> > > alexis
> > >
> > > On Thu, 24 Jul 2003, Jerome Asselin wrote:
> > > > You can specify some options in "par.box". Use col=NA to make the
> > > > frame transparent. See example below (which was modified from the
> > > > help file). See also the "scales" parameter if you want to remove
> > > > the arrows as well.
> > > >
> > > > Cheers,
> > > > Jerome
> > > >
> > > >      library(lattice)
> > > >      x <- seq(-pi, pi, len = 20)
> > > >      y <- seq(-pi, pi, len = 20)
> > > >      g <- expand.grid(x = x, y = y)
> > > >      g$z <- sin(sqrt(g$x^2 + g$y^2))
> > > >      wireframe(z ~ x * y, g, drape = TRUE,
> > > >                perspective = FALSE,
> > > >                aspect = c(3,1), colorkey = FALSE,
> > > >                par.box = list(col=NA))
> > > >
> > > > On July 24, 2003 12:44 pm, Alexis J. Diamond wrote:
> > > > > Hi,
> > > > >
> > > > > I've got a wireframe 3D surface plot, but I don't want a frame
> > > > > around it. Is there any way to remove the frame, or (worst case)
> > > > > change the color of the frame to the background color (which
> > > > > looks like grey).
> > > > >
> > > > > I'm using ver 1.7.1
> > > > >
> > > > > I've tried frame.plot = F, but that doesn't seem to work for
> > > > > 'wireframe'.
> > > > >
> > > > > Many thanks,
> > > > >
> > > > > Alexis Diamond
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From d.scott at auckland.ac.nz  Thu Jul 24 23:56:53 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 25 Jul 2003 09:56:53 +1200 (NZST)
Subject: [R] Confidence Band for empirical distribution function
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF714860161B680@saffron.cfs.le.ac.uk>
Message-ID: <Pine.LNX.4.44.0307250953380.18212-100000@hydra.stat.auckland.ac.nz>

On Thu, 24 Jul 2003, Hotz, T. wrote:

> Dear Kjetil,
> 
> As I already mentioned, it appears that there isn't a function 
> available calculating the quantiles directly (at least, it doesn't appear
> in the C source of ctest). So as I already suggested, uniroot (or a similar
> C routine which calls the corresponding C code directly) is probably the 
> best you can do (apart from writing it completely yourself).
> 
> I didn't program this using uniroot, but I'd certainly try the following 
> for speed-up:
> 
> - For symmetry reasons, you only need to compute half of the quantiles.
> - The quantiles depend smoothly on the probabilities (of your reference
> distribution). Therefore, calculating only a "few" for probabilities 
> between 0 and 0.5, and using (e.g. linear) interpolation should be
> satisfying.
> 

There is an example of this in my package HyperbolicDist which has just 
appeared on CRAN. It is a little more sophisticated in that it fits a 
spline in preference to linear interpolation, before using uniroot.

Look at qhyperb if this is of interest.

David Scott



_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From vincent.philion at irda.qc.ca  Fri Jul 25 00:07:52 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Thu, 24 Jul 2003 18:07:52 -0400
Subject: [R] inverse prediction and Poisson regression
Message-ID: <97260c7d6c7dd8055d89d5f2b0402f85@ibookIRDA.local>

Hello to all, I'm a biologist trying to tackle a "fish" (Poisson Regression) which is just too big for my modest understanding of stats!!!

Here goes...

I want to find good literature or proper mathematical procedure to calculate a confidence interval for an inverse prediction of a Poisson regression using R. 

I'm currently trying to analyse a "dose-response" experiment. 

I want to calculate the dose (X) for 50% inhibition of a biological response (Y). My "response" is a "count" data that fits a Poisson distribution perfectly. 

I could make my life easy and calculate: "dose response/control response" = % of total response... and then use logistic regression, but somehow, that doesn't sound right.
 
Should I just stick to logistic regression and go on with my life? Can I be cured of this paranoia?
;-)

I thought a Poisson regression would be more appropriate, but I don't know how to "properly" calculate the dose equivalent to 50% inhibition. i/e confidence intervals, etc on the "X" = dose. Basically an "inverse" prediction problem.

By the way, my data is "graphically" linear for Log(Y) = log(X) where Y is counts and X is dose.

I use a Poisson regression to fit my dose-response experiment by EXCLUDING the response for dose = 0, because of log(0)

Under "R" = 

> 	glm.dose <- glm(response[-1] ~ log(dose[-1]),family=poisson())

(that's why you see the "dose[-1]" term. The "first" dose in the dose vector is 0. 

This is really a nice fit. I can obtain a nice slope (B) and intercept (A):

log(Y) = B log(x) + A

I do have a biological value for dose = 0 from my "control". i/e Ymax = some number with a Poisson error again

So, what I want is EC50x :

Y/Ymax = 0.5 = exp(B log(EC50x) + A) / Ymax

exp((log(0.5) + Log(Ymax)) - A)/B) = EC50x

That's all fine, except I don't have a clue on how to calculate the confidence intervals of EC50x or even if I can model this inverse prediction with a Poisson regression. In OLS linear regression, fitting X based on Y is not a good idea because of the way OLS calculates the slope and intercept. Is the same problem found in GLM/Poisson regression? Moreover, I also have a Poisson error on Ymax that I would have to consider, right?

Help!!!!


-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From jcjorgensen at wisc.edu  Fri Jul 25 00:10:02 2003
From: jcjorgensen at wisc.edu (JEFFREY C JORGENSEN)
Date: Thu, 24 Jul 2003 17:10:02 -0500
Subject: [R] R-WinEdt problems
Message-ID: <13fcf3213fbf0c.13fbf0c13fcf32@wiscmail.wisc.edu>

Hello,


I've done most all the various steps outlined in a
recent posting to the mailing list archives
(and in the help files) to load and run R-WinEdt.  I
can get it to run fine but I am not successful in
getting it to interface with RGui (1.6.2, not
minimized).  I try to use the R-line, R-source,
R-paste buttons with no success.  [All the necessary
*.edt files appear to be in the proper directories.]

I've done everything except modify the rprofile or
the options().  Is that necessary, and if so how
exactly do I do that?  And, if I do modify them how
do I restore them back to their default settings?

Any help would be much appreciated.

Thanks,
Jeff Jorgensen
jcjorgensen at wisc.edu



From adiamond at fas.harvard.edu  Fri Jul 25 01:18:08 2003
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Thu, 24 Jul 2003 19:18:08 -0400 (EDT)
Subject: [R] contourplot: how to ensure all (or certain) 'cuts' get labelled
In-Reply-To: <Pine.LNX.4.44.0307250953380.18212-100000@hydra.stat.auckland.ac.nz>
Message-ID: <Pine.OSF.4.44.0307241903250.5790-100000@is06.fas.harvard.edu>

hi all,

contourplot generates a nice plot, but there are cut lines that don't have
labels in my figure.
i think this is because there's a high ridge stretching
north-west/south-east entirely across the plot, diagonal-to-diagonal,
and contourplot only wants to label each elevation-level only once, which
means only lines in the upper-right quadrant got labelled.

is there some way to *identify* and label a cutpoint that has no label,
or to force contourplot to label each (or a particular) cutpoint?

if i knew what the label should be on a given label-less cutpoint, i could
probably insert it in as text myself.  but i don't even know what it should be.

by the way, does anyone know why i got only 5 cuts, when i specified
"cuts = 10"?

thanks,

alexis
ps-- i am open to using *contour* instead of *contourplot* if this gives
me more control over the output.



From TyagiAnupam at aol.com  Fri Jul 25 01:48:18 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Thu, 24 Jul 2003 19:48:18 EDT
Subject: [R] R-WinEdt problems
Message-ID: <159.220934eb.2c51ca42@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030724/df7afc6f/attachment.pl

From bb2 at duke.edu  Fri Jul 25 02:47:03 2003
From: bb2 at duke.edu (bb2@duke.edu)
Date: Thu, 24 Jul 2003 20:47:03 -0400
Subject: [R] Plotting Vector Fields
Message-ID: <p05210600bb462c774495@[160.36.251.70]>

Dear R List,

Is there a function to plot vector fields in R?

I'm looking for something similar to PlotVectorField in Mathematica 
(see below), e.g. given functions f(x) and f(y), I would like to plot 
the resulting field of vectors (f(x),f(y)) over some range of x and y.


PlotVectorField[{f(x), f(y)}, {x, xmin, xmax}, {y, ymin, ymax}]



Thanks for your help,
Brian



From spencer.graves at pdf.com  Fri Jul 25 03:05:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Jul 2003 18:05:47 -0700
Subject: [R] inverse prediction and Poisson regression
References: <97260c7d6c7dd8055d89d5f2b0402f85@ibookIRDA.local>
Message-ID: <3F20826B.5020106@pdf.com>

	  1.  If you provide a toy data set with, e.g., 5 observations, to 
accompany your example, it would be much easier for people to try out 
ideas and then give you a more solid response.

	  2.  Have you tried something like log(dose+0.5) or I(log(dose+0.5)) 
in your model statement in conjunction with "predict" or "predict.glm" 
on the output from "glm"?

hope this helps.  spencer graves

Vincent Philion wrote:
> Hello to all, I'm a biologist trying to tackle a "fish" 
(Poisson Regression) which is just too big for my modest
understanding of stats!!!
> 
> Here goes...
> 
> I want to find good literature or proper mathematical 
procedure to calculate a confidence interval for an
inverse prediction of a Poisson regression using R.
> 
> I'm currently trying to analyse a "dose-response"
experiment.
> 
> I want to calculate the dose (X) for 50% inhibition 
of a biological response (Y). My "response" is a "count"
data that fits a Poisson distribution perfectly.
> 
> I could make my life easy and calculate: "dose 
response/control response" = % of total response...
and then use logistic regression, but somehow, that
doesn't sound right.
>  
> Should I just stick to logistic regression and go
on with my life? Can I be cured of this paranoia?
> ;-)
> 
> I thought a Poisson regression would be more 
appropriate, but I don't know how to "properly"
calculate the dose equivalent to 50% inhibition.
i/e confidence intervals, etc on the "X" = dose.
Basically an "inverse" prediction problem.
> 
> By the way, my data is "graphically" linear for 
Log(Y) = log(X) where Y is counts and X is dose.
> 
> I use a Poisson regression to fit my dose-response 
experiment by EXCLUDING the response for dose = 0,
because of log(0)
> 
> Under "R" = 
> 
> 
>>	glm.dose <- glm(response[-1] ~ log(dose[-1]),family=poisson())
> 
> 
> (that's why you see the "dose[-1]" term. The 
"first" dose in the dose vector is 0.
> 
> This is really a nice fit. I can obtain a nice 
slope (B) and intercept (A):
> 
> log(Y) = B log(x) + A
> 
> I do have a biological value for dose = 0 from 
my "control". i/e Ymax = some number with a Poisson
error again
> 
> So, what I want is EC50x :
> 
> Y/Ymax = 0.5 = exp(B log(EC50x) + A) / Ymax
> 
> exp((log(0.5) + Log(Ymax)) - A)/B) = EC50x
> 
> That's all fine, except I don't have a clue on how 
to calculate the confidence intervals of EC50x or even
if I can model this inverse prediction with a Poisson
regression. In OLS linear regression, fitting X based
on Y is not a good idea because of the way OLS calculates
the slope and intercept. Is the same problem found in
GLM/Poisson regression? Moreover, I also have a Poisson
error on Ymax that I would have to consider, right?
> 
> Help!!!!
> 
>



From davidD at qimr.edu.au  Fri Jul 25 03:21:25 2003
From: davidD at qimr.edu.au (David Duffy)
Date: Fri, 25 Jul 2003 11:21:25 +1000 (EST)
Subject: [R] animal model in R
Message-ID: <Pine.LNX.4.56.0307251109440.7127@moonboom.qimr.edu.au>

If your datasets are small, then it is not difficult to "roll your own"
using optim().  If you look at
http://www.qimr.edu.au/davidD/sib-pair.R
you will find such a routine at lines 1511-1562.  This calls
kinship.rel() that produces NRM etc.

-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From apjaworski at mmm.com  Fri Jul 25 04:06:02 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 24 Jul 2003 21:06:02 -0500
Subject: [R] Integer programming in R
Message-ID: <OF7D587A81.F83A4C98-ON86256D6E.000AE79D@mmm.com>


Robin,

This is not a direct answer to your question, but there exists a pretty
good linear, mixted and integer programming package called lp_solve.  I
have used it successfully  on a couple of projects.  As far as I know it is
in the public domain and there is source code available.  There is also a
precompiled version available for Windows Visual Basic environment
containing a precompiled DLL and an interface module defining VB function
calls to the DLL.  I think, that having all that it should be possible to
get something going in R, at least in the Windows environment.

Hope this helps,

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "Robin Naidoo"       |
|         |           <rnaidoo at ualberta.ca>|
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           07/24/2003 12:18     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       r-help at stat.math.ethz.ch                                                                                     |
  |      cc:                                                                                                                    |
  |      Subject:  [R] Integer programming in R                                                                                 |
  >-----------------------------------------------------------------------------------------------------------------------------|




Dear all,

I am a relative newcomer to the R language, and am sussing out
the possibilities of using R to do integer programming (which I am
also new to).  Is there something along the lines of the NUOPT S-
PLUS package that is available, or on the way, for R?  Are there
other options for integer programming in R?

Thanks very much in advance,

Robin Naidoo
University of Alberta
Edmonton, AB, Canada

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From nikko at hailmail.net  Fri Jul 25 04:07:35 2003
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Thu, 24 Jul 2003 18:07:35 -0800
Subject: [R] Memory explosion, plotting nmle grouped data object
Message-ID: <20030725020735.6D25A37D63@www.fastmail.fm>

Hi
I am using R 1.7.1 on RH linux 9.0 
> sum(unlist(lapply(ls(),function(x)object.size(get(x)))))/1024^2
[1] 2.424263

so I am not using much memory (I have a gig of ram on my machine)
now in nlme

> gtest<-groupedData(log(X8)~Time|sub,all[,c(names(all)[1:9],"X8")],outer=~A*B)
> object.size(gtest)/1024
[1] 59.98438

> plot(gtest,outer=~Dose*chem,key=FALSE,asp=.5)

Plotting takes forever and from top while it is running

5663 wf00223   15   0 1151M 546M  1780 R     6.1 54.2   3:19   0 R.bin

Any Ideas why R is using so much memory?

Thanks
Nicholas



From keef9490 at uidaho.edu  Fri Jul 25 04:10:27 2003
From: keef9490 at uidaho.edu (Robert Keefe)
Date: Thu, 24 Jul 2003 19:10:27 -0700 (PDT)
Subject: [R] Multiple expressions in system.time()?
Message-ID: <Pine.GHP.4.51.0307241838160.28896@falcon.csrv.uidaho.edu>


Hi All,

Is it possible for system.time() to measure the time
it takes for a machine to evaluate more than one R
expression?

For example,

# This I can do:
> system.time(x <- rnorm(100000))
[1] 0.07 0.00 0.13 0.00 0.00

# But this I can't:
> system.time(x <- rnorm(100000); new <- sample(x, 100000, replace=T))
Error: syntax error

# Nor this:
> system.time(x <- rnorm(100000)
+ new <- sample(x, 100000, replace=T)
Error: syntax error

I'm trying to compare two fairly large code chunks,
so executing system.time() individually for each expression
and then summing the results seems suboptimal.

Have I missed something simple?

Thanks in advance for any suggestions-

Rob


version:

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R
_____________________________________________________

Rob Keefe                 Lab: (208) 885-5165
M.S. student              Home: (208) 882-9749
University of Idaho



From edd at debian.org  Fri Jul 25 04:48:00 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 24 Jul 2003 21:48:00 -0500
Subject: [R] Multiple expressions in system.time()?
In-Reply-To: <Pine.GHP.4.51.0307241838160.28896@falcon.csrv.uidaho.edu>
References: <Pine.GHP.4.51.0307241838160.28896@falcon.csrv.uidaho.edu>
Message-ID: <20030725024800.GA6819@sonny.eddelbuettel.com>

On Thu, Jul 24, 2003 at 07:10:27PM -0700, Robert Keefe wrote:
> # This I can do:
> > system.time(x <- rnorm(100000))
> [1] 0.07 0.00 0.13 0.00 0.00
> 
> # But this I can't:
> > system.time(x <- rnorm(100000); new <- sample(x, 100000, replace=T))
> Error: syntax error

Just use curly braces:

> system.time({x <- rnorm(100000); new <- sample(x, 100000, replace=T)})
[1] 0.11 0.00 0.11 0.00 0.00

> Have I missed something simple?

See above. Another related way is to define a function; you could even use a
'throw-away anonymous' function [1].

Lastly, you probably also want to learn about profiling your code. There is
an introductory article in one of the R News issues.

Regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From vincent.philion at irda.qc.ca  Fri Jul 25 08:01:17 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 02:01:17 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <3F20826B.5020106@pdf.com>
Message-ID: <c11ab20f67f3ffce54bcdbc158df6ce6@ibookIRDA.local>

Hello and thank you for your interest in this problem. 

"real life data" would look like this:

x	y
0		28
0.03		21
0.1		11
0.3		15
1		5
3		4
10		1
30		0
100		0

x	y
0	30
0.0025	30
0.02	25
0.16	25
1.28	10
10.24	0
81.92	0

X	Y
0	35
0.00025	23
0.002	14
0.016	6
0.128	5
1.024	3
8.192	2 

X	Y
0  43
0.00025  35
0.002  20
0.016  16
0.128  11
1.024  6
8.192   0 

Where X is dose and Y is response. 
the relation is linear for log(response) = b log(dose) + intercept

Response for dose 0 is a "control" = Ymax. So, What I want is the dose for 50% response. For instance, in example 1:

Ymax = 28 (this is also an observation with Poisson error)

So I want dose for response = 14 = approx. 0.3

any help would be greatly appreciated!

bye for now,

Vincent

> 	  1.  If you provide a toy data set with, e.g., 5 observations, to accompany 
> your example, it would be much easier for people to try out ideas and then 
> give you a more solid response.
> 
> 	  2.  Have you tried something like log(dose+0.5) or I(log(dose+0.5)) in 
> your model statement in conjunction with "predict" or "predict.glm" on the 
> output from "glm"?
> 
> hope this helps.  spencer graves
> 
> Vincent Philion wrote:
>> Hello to all, I'm a biologist trying to tackle a "fish"
> (Poisson Regression) which is just too big for my modest
> understanding of stats!!!
>> 
>> Here goes...
>> 
>> I want to find good literature or proper mathematical
> procedure to calculate a confidence interval for an
> inverse prediction of a Poisson regression using R.
>> 
>> I'm currently trying to analyse a "dose-response"
> experiment.
>> 
>> I want to calculate the dose (X) for 50% inhibition
> of a biological response (Y). My "response" is a "count"
> data that fits a Poisson distribution perfectly.
>> 
>> I could make my life easy and calculate: "dose
> response/control response" = % of total response...
> and then use logistic regression, but somehow, that
> doesn't sound right.
>>    Should I just stick to logistic regression and go
> on with my life? Can I be cured of this paranoia?
>> ;-)
>> 
>> I thought a Poisson regression would be more
> appropriate, but I don't know how to "properly"
> calculate the dose equivalent to 50% inhibition.
> i/e confidence intervals, etc on the "X" = dose.
> Basically an "inverse" prediction problem.
>> 
>> By the way, my data is "graphically" linear for
> Log(Y) = log(X) where Y is counts and X is dose.
>> 
>> I use a Poisson regression to fit my dose-response
> experiment by EXCLUDING the response for dose = 0,
> because of log(0)
>> 
>> Under "R" =
>> 
>>> 	glm.dose <- glm(response[-1] ~ log(dose[-1]),family=poisson())
>> 
>> 
>> (that's why you see the "dose[-1]" term. The
> "first" dose in the dose vector is 0.
>> 
>> This is really a nice fit. I can obtain a nice
> slope (B) and intercept (A):
>> 
>> log(Y) = B log(x) + A
>> 
>> I do have a biological value for dose = 0 from
> my "control". i/e Ymax = some number with a Poisson
> error again
>> 
>> So, what I want is EC50x :
>> 
>> Y/Ymax = 0.5 = exp(B log(EC50x) + A) / Ymax
>> 
>> exp((log(0.5) + Log(Ymax)) - A)/B) = EC50x
>> 
>> That's all fine, except I don't have a clue on how
> to calculate the confidence intervals of EC50x or even
> if I can model this inverse prediction with a Poisson
> regression. In OLS linear regression, fitting X based
> on Y is not a good idea because of the way OLS calculates
> the slope and intercept. Is the same problem found in
> GLM/Poisson regression? Moreover, I also have a Poisson
> error on Ymax that I would have to consider, right?
>> 
>> Help!!!!
>> 
>> 
> 
> 
> 

-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From ripley at stats.ox.ac.uk  Fri Jul 25 08:43:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 07:43:06 +0100 (BST)
Subject: [R] Multiple expressions in system.time()?
In-Reply-To: <20030725024800.GA6819@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.44.0307250741360.4525-100000@gannet.stats>

On Thu, 24 Jul 2003, Dirk Eddelbuettel wrote:

> On Thu, Jul 24, 2003 at 07:10:27PM -0700, Robert Keefe wrote:
> > # This I can do:
> > > system.time(x <- rnorm(100000))
> > [1] 0.07 0.00 0.13 0.00 0.00
> > 
> > # But this I can't:
> > > system.time(x <- rnorm(100000); new <- sample(x, 100000, replace=T))
> > Error: syntax error
> 
> Just use curly braces:

(which make a single expression out of many)

> 
> > system.time({x <- rnorm(100000); new <- sample(x, 100000, replace=T)})
> [1] 0.11 0.00 0.11 0.00 0.00
> 
> > Have I missed something simple?
> 
> See above. Another related way is to define a function; you could even use a
> 'throw-away anonymous' function [1].
> 
> Lastly, you probably also want to learn about profiling your code. There is
> an introductory article in one of the R News issues.

Later information is in a chapter of the `Writing R Extensions' manual.
(A few things have changed since that R-News article).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Jul 25 08:45:09 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Jul 2003 08:45:09 +0200
Subject: [R] R-WinEdt problems
In-Reply-To: <13fcf3213fbf0c.13fbf0c13fcf32@wiscmail.wisc.edu>
References: <13fcf3213fbf0c.13fbf0c13fcf32@wiscmail.wisc.edu>
Message-ID: <3F20D1F5.1090509@statistik.uni-dortmund.de>

JEFFREY C JORGENSEN wrote:
> Hello,
> 
> 
> I've done most all the various steps outlined in a
> recent posting to the mailing list archives
> (and in the help files) to load and run R-WinEdt.  I
> can get it to run fine but I am not successful in
> getting it to interface with RGui (1.6.2, not
> minimized).  I try to use the R-line, R-source,
> R-paste buttons with no success.  [All the necessary
> *.edt files appear to be in the proper directories.]

Maybe you have changed R-WinEdt's mode?
Use R-WinEdt's menu "R" to "Set R -mdi mode" when you are using R in its 
default configuration.


> I've done everything except modify the rprofile or
> the options().  Is that necessary, 

No.

Uwe Ligges

 > and if so how
> exactly do I do that?  And, if I do modify them how
> do I restore them back to their default settings?
 >
> Any help would be much appreciated.
> 
> Thanks,
> Jeff Jorgensen
> jcjorgensen at wisc.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Jul 25 08:56:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 07:56:17 +0100 (BST)
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <c11ab20f67f3ffce54bcdbc158df6ce6@ibookIRDA.local>
Message-ID: <Pine.LNX.4.44.0307250744220.4525-100000@gannet.stats>

On Fri, 25 Jul 2003, Vincent Philion wrote:

> Hello and thank you for your interest in this problem. 
> 
> "real life data" would look like this:
> 
> x	y
> 0		28
> 0.03		21
> 0.1		11
> 0.3		15
> 1		5
> 3		4
> 10		1
> 30		0
> 100		0
> 
> x	y
> 0	30
> 0.0025	30
> 0.02	25
> 0.16	25
> 1.28	10
> 10.24	0
> 81.92	0
> 
> X	Y
> 0	35
> 0.00025	23
> 0.002	14
> 0.016	6
> 0.128	5
> 1.024	3
> 8.192	2 
> 
> X	Y
> 0  43
> 0.00025  35
> 0.002  20
> 0.016  16
> 0.128  11
> 1.024  6
> 8.192   0 
> 
> Where X is dose and Y is response. 
> the relation is linear for log(response) = b log(dose) + intercept

Is that log(*mean* response), that is a log link and exponential decay 
with dose?

> Response for dose 0 is a "control" = Ymax. So, What I want is the dose
> for 50% response. For instance, in example 1:
> 
> Ymax = 28 (this is also an observation with Poisson error)

Once you observe Ymax, Y is no longer Poisson.

> So I want dose for response = 14 = approx. 0.3

What exactly is Ymax?  Is it the response at dose 0?  The mean response at
dose 0?  The largest response?  About the only thing I can actually
interpret is that you want to fit a curve of mean response vs dose, and
find the dose at which the mean response is half of that at dose 0.
That one is easy.

I think you are confusing response with mean response, and we can't 
disentangle them for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Jul 25 10:06:56 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 25 Jul 2003 10:06:56 +0200
Subject: [R] trellis plot question
In-Reply-To: <Pine.GSO.4.05.10307241035390.20715-100000@chaos.harvard.edu>
Message-ID: <3F210140.20363.7C25FD@localhost>

Hi

You probably use numeric id variable. 
I think you need to make an id variable to become a factor. 

e.g. use 

as.factor(id) 

Cheers
Petr

On 24 Jul 2003 at 10:39, Suzette Blanchard wrote:

> 
> Greetings,
> 
> Does anyone know how to get an id number in the little header 
> above each individual plot within a trellis plot?  The default
> seems to be to print the word id and add a line indicating on
> a linear scale where the current id sits.
> 
> Thanks in advance for any help you can send,
> 
> Suzette
> 
> 
> =================================
> Suzette Blanchard, Ph.D.
> Research Scientist
> Frontier Science Foundation
> 1244 Boylston St. Suite 303
> Chestnut Hill, MA 02467
> Email:  suzette at sdac.harvard.edu
> Phone:  (617) 632-2007
> Fax:    (617) 632-2001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From michael.watson at bbsrc.ac.uk  Fri Jul 25 10:23:59 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 25 Jul 2003 09:23:59 +0100
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00963@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

Thanks once again for your help, I do appreciate it..... however....

Here is what I get with your test.... (under tcsh - i normally use bash, but I will keep everything the same)

users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R

>options(internet.info=0)
>update.packages()
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
unable to connect to 'cran.r-project.org' on port 80
Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"), :
	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'


... and THATS IT!  I don't get any "Using HTTP proxy ... " message at all, which appears to suggest that R, under SUSE Linux 8.1, is NOT PICKING up the http_proxy environment variable - this isn't something thats wrong with my proxy, that works with everything else - internet browsers, ftp clients, wget, instant messenger clients etc etc.  The problem is R, which isn't picking up that it needs to use the http_proxy environment variable.  And I apologise for being blunt, but that is an R problem, not a proxy problem!

Thanks for your help

Mick


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 24 July 2003 16:56
To: michael watson (IAH-C)
Subject: RE: [R] Your proxy seems not to work with R (was R won't
connect to the internet on Linux!)


When I do (under tcsh)

env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> options(internet.info=0)
> update.packages()
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080

it tries to connect to your proxy (as it says) and gets no response, which
is not surprising from my site.  If you get the same, your proxy is
probably not behaving in the standard way (since that has been tested by
many users with standard proxies).

I've changed the emphasis of the subject line to one I feel is more 
equitable: many, many users have counter-evidence to your original 
assertion, which was rather arrogant.

On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:

> Hello Professor
> 
> If you are suggesting that I am simply missing the "http://" part of my
> cache URL, or that I am missing a trailing "/", then I pre-empted this
> response and it still doesn't work.

I was suggesting that `simply' you were not reading the documentation 
correctly. 

> I have tried setting both http_proxy and HTTP_PROXY to all of:

I hope you set to *each* of these.  The first and third are documented to
be incorrect, so using those was perverse.

> wwwcache.bbsrc.ac.uk:8080
> http://wwwcache.bbsrc.ac.uk:8080
> wwwcache.bbsrc.ac.uk:8080/
> http://wwwcache.bbsrc.ac.uk:8080/
> 
> and I still get the same response - R cannot open the URL.
> 
> And yes, that is thw right proxy address, I copied it straight from
> Netscape on the same computer, and Netscape connects to the internet
> fine.
> 
> Thanks
> Mick
>  
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 24 July 2003 13:17
> To: michael watson (IAH-C)
> Cc: 'R-help at stat.math.ethz.ch '
> Subject: Re: [R] R won't connect to the internet on Linux!
> 
> 
> On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> 
> > OK, I really am struggling with this one!  Forgive me if I am being stupid....
> 
> > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the internet
> > through a proxy so I have:
> > 
> > IAHC-LINUX03:~ # echo $http_proxy
> > wwwcache.bbsrc.ac.uk:8080
> > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > wwwcache.bbsrc.ac.uk:8080
> > 
> > just in case ;-)
> > 
> > SO, i go into R and I get:
> > 
> > > source("http://www.bioconductor.org/getBioC.R")
> > unable to connect to 'www.bioconductor.org' on port 80.
> > Error in file(file, "r") : cannot open URL `http://www.bioconductor.org/getBioC.R'
> > 
> > OK so is R just not picking up my proxy setting?  
> 
> Your setting is wrong, so it is being ignored.  The help page says
> quite explicitly
> 
>       The form of `"http_proxy"' should be `"http://proxy.dom.com/"' or
>      `"http://proxy.dom.com:8080/"' where the port defaults to `80' and
>      the trailing slash may be omitted.
> 
> > It seems to be trying
> > port 80 on something, and I have specifically set it to port 8080 in my
> > environment variables.  As far as I can see I have followed the
> > reference manual suggestion, so does anyone else have one?
> 
> The problem is in your seeing, it seems.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mihastaut at hotmail.com  Fri Jul 25 11:04:04 2003
From: mihastaut at hotmail.com (Miha STAUT)
Date: Fri, 25 Jul 2003 09:04:04 +0000
Subject: [R] Re: geoR size limit problem
Message-ID: <BAY2-F15xrCXIjl4OsX0000369c@hotmail.com>

>Dear Miha (cc: Paulo Ribeiro [the developer of geoR, who is not subscribed 
>to R-help])
>
>Some reproducible code would help here.

Unfortunately I do not have an internet connect on my computer, that is why 
I did every thing very descriptive. Anyway I will try:

str(df)
   [#approx 2500]
$ x ...
$ y ...
$ z ...
library(geoR)
g<-as.geodata(df)
expvar<-variog(g, uvec=seq(0,1000,25), option="bin")
matern<-variofit(expvar, ini=c(1600,300), cov.model="matern", fix.nug=T, 
nug=25, kappa=1.5, max.dist=800, weights="npairs")

library(GRASS)
G<-gmeta()
grid<-expand.matrix(G$xseq,G$yseq)
length(G$xseq)
900 #approx
length(G$yseq)
650 #approx
krige.matern<-krige.conv(g, loc=grid, krige=krige.control(obj.m=matern, 
type="OK"))

#5 min processing
can not allocate vector of 1500000000 #approx

Thanks, Miha Staut

>
>Guessing :
>
>* geoR does not implement kriging with local neighbourhoods, but instead 
>conditions on all data. Therefore having 2500 data points would imply 
>having a covariance matrix of size 2500*2500 approx 6million . Not sure if 
>this is too much, but I do not think so.



From deepayansarkar at vsnl.net  Fri Jul 25 15:20:23 2003
From: deepayansarkar at vsnl.net (Deepayan Sarkar)
Date: Fri, 25 Jul 2003 13:20:23 +0000
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <3F20410C.84C822C0@statistik.uni-dortmund.de>
References: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
	<200307242013.NAA17437@hivnet.ubc.ca>
	<3F20410C.84C822C0@statistik.uni-dortmund.de>
Message-ID: <200307251320.23312.deepayansarkar@vsnl.net>

On Thursday 24 July 2003 20:26, Uwe Ligges wrote:
> Jerome Asselin wrote:
> > You can specify some options in "par.box". Use col=NA to make the frame
> > transparent. See example below (which was modified from the help file).
> > See also the "scales" parameter if you want to remove the arrows as well.
> >
> > Cheers,
> > Jerome
> >
> >      library(lattice)
> >      x <- seq(-pi, pi, len = 20)
> >      y <- seq(-pi, pi, len = 20)
> >      g <- expand.grid(x = x, y = y)
> >      g$z <- sin(sqrt(g$x^2 + g$y^2))
> >      wireframe(z ~ x * y, g, drape = TRUE,
> >                perspective = FALSE,
> >                aspect = c(3,1), colorkey = FALSE,
> >                par.box = list(col=NA))
>
> Which doesn't work on device
>  trellis.device(windows)
> but works well on, e.g.,
>  trellis.device(postscript)
>
> Deepayan, since I don't have the time to check that right now: Is this
> bug known? Is this a bug in R, grid or lattice?

Not sure, and I don't have access to R on Windows right now. This is probably 
a grid issue, try doing 

grid.newpage()
grid.lines(gp = gpar(col = NA))

But I think col="transparent" would be a better choice than col=NA here. Does 
that work on windows() ?

Deepayan



From tobias_verbeke at skynet.be  Fri Jul 25 12:16:07 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Fri, 25 Jul 2003 12:16:07 +0200
Subject: [R] variable name of variable in dataframe
Message-ID: <20030725121607.154886fc.tobias_verbeke@skynet.be>

Dear list,

If I have this toy function:

toy <- function(b=.95){
toyframe <- data.frame(lion.95 = c(1, 2))
return(toyframe)
}

How can I obtain that for any value b, 
the name of the column becomes "lionb", 
i.e. lion.95 if b = .95, lion.85 if b = .85 etc.
knowing that .95 (.85 etc.) may also be
given as 0.95 (0.85 etc.) but that the
result should be lion.95 (lion.85 etc.) 


Thanks in advance,

Tobias



From ligges at statistik.uni-dortmund.de  Fri Jul 25 12:21:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 25 Jul 2003 12:21:13 +0200
Subject: [R] wireframe: how to remove the frame around my plot?
In-Reply-To: <200307251320.23312.deepayansarkar@vsnl.net>
References: <Pine.OSF.4.44.0307241540230.5790-100000@is06.fas.harvard.edu>
	<200307242013.NAA17437@hivnet.ubc.ca>
	<3F20410C.84C822C0@statistik.uni-dortmund.de>
	<200307251320.23312.deepayansarkar@vsnl.net>
Message-ID: <3F210499.1070007@statistik.uni-dortmund.de>

Deepayan Sarkar wrote:

> On Thursday 24 July 2003 20:26, Uwe Ligges wrote:
> 
>>Jerome Asselin wrote:
>>
>>>You can specify some options in "par.box". Use col=NA to make the frame
>>>transparent. See example below (which was modified from the help file).
>>>See also the "scales" parameter if you want to remove the arrows as well.
>>>
>>>Cheers,
>>>Jerome
>>>
>>>     library(lattice)
>>>     x <- seq(-pi, pi, len = 20)
>>>     y <- seq(-pi, pi, len = 20)
>>>     g <- expand.grid(x = x, y = y)
>>>     g$z <- sin(sqrt(g$x^2 + g$y^2))
>>>     wireframe(z ~ x * y, g, drape = TRUE,
>>>               perspective = FALSE,
>>>               aspect = c(3,1), colorkey = FALSE,
>>>               par.box = list(col=NA))
>>
>>Which doesn't work on device
>> trellis.device(windows)
>>but works well on, e.g.,
>> trellis.device(postscript)
>>
>>Deepayan, since I don't have the time to check that right now: Is this
>>bug known? Is this a bug in R, grid or lattice?
> 
> 
> Not sure, and I don't have access to R on Windows right now. This is probably 
> a grid issue, try doing 
> 
> grid.newpage()
> grid.lines(gp = gpar(col = NA))
> 
> But I think col="transparent" would be a better choice than col=NA here. Does 
> that work on windows() ?

No, thanks.

I'll take a closer look later ....

Uwe


> Deepayan
> 
>



From hb at maths.lth.se  Fri Jul 25 12:24:58 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 25 Jul 2003 12:24:58 +0200
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00963@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <000101c35297$0039c070$e502eb82@maths.lth.se>

Could it be that you have redefined the command R in your shell such
that the http_proxy environment variable is set in one and R is running
in another? (This is just a wild guess and I am myself only running
WinXP.) What do you get if you do

% env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
% R
> Sys.getenv("http_proxy")

Also, have you considered setting http_proxy in ~/.Renviron (see
?.Renviron).

Cheers

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: den 25 juli 2003 10:24
> To: 'Prof Brian Ripley'
> Cc: 'R-help at stat.math.ethz.ch'
> Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Hi
> 
> Thanks once again for your help, I do appreciate it..... however....
> 
> Here is what I get with your test.... (under tcsh - i 
> normally use bash, but I will keep everything the same)
> 
> users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> 
> >options(internet.info=0)
> >update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> unable to connect to 'cran.r-project.org' on port 80
> Error in download.file(url = paste(contriburl, "PACKAGES", 
> sep = "/"), :
> 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'
> 
> 
> ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> message at all, which appears to suggest that R, under SUSE 
> Linux 8.1, is NOT PICKING up the http_proxy environment 
> variable - this isn't something thats wrong with my proxy, 
> that works with everything else - internet browsers, ftp 
> clients, wget, instant messenger clients etc etc.  The 
> problem is R, which isn't picking up that it needs to use the 
> http_proxy environment variable.  And I apologise for being 
> blunt, but that is an R problem, not a proxy problem!
> 
> Thanks for your help
> 
> Mick
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 24 July 2003 16:56
> To: michael watson (IAH-C)
> Subject: RE: [R] Your proxy seems not to work with R (was R 
> won't connect to the internet on Linux!)
> 
> 
> When I do (under tcsh)
> 
> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > options(internet.info=0)
> > update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080
> 
> it tries to connect to your proxy (as it says) and gets no 
> response, which is not surprising from my site.  If you get 
> the same, your proxy is probably not behaving in the standard 
> way (since that has been tested by many users with standard proxies).
> 
> I've changed the emphasis of the subject line to one I feel is more 
> equitable: many, many users have counter-evidence to your original 
> assertion, which was rather arrogant.
> 
> On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> 
> > Hello Professor
> > 
> > If you are suggesting that I am simply missing the 
> "http://" part of 
> > my cache URL, or that I am missing a trailing "/", then I 
> pre-empted 
> > this response and it still doesn't work.
> 
> I was suggesting that `simply' you were not reading the documentation 
> correctly. 
> 
> > I have tried setting both http_proxy and HTTP_PROXY to all of:
> 
> I hope you set to *each* of these.  The first and third are 
> documented to be incorrect, so using those was perverse.
> 
> > wwwcache.bbsrc.ac.uk:8080
> > http://wwwcache.bbsrc.ac.uk:8080
> > wwwcache.bbsrc.ac.uk:8080/
> > http://wwwcache.bbsrc.ac.uk:8080/
> > 
> > and I still get the same response - R cannot open the URL.
> > 
> > And yes, that is thw right proxy address, I copied it straight from 
> > Netscape on the same computer, and Netscape connects to the 
> internet 
> > fine.
> > 
> > Thanks
> > Mick
> >  
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: 24 July 2003 13:17
> > To: michael watson (IAH-C)
> > Cc: 'R-help at stat.math.ethz.ch '
> > Subject: Re: [R] R won't connect to the internet on Linux!
> > 
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > OK, I really am struggling with this one!  Forgive me if 
> I am being 
> > > stupid....
> > 
> > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> internet 
> > > through a proxy so I have:
> > > 
> > > IAHC-LINUX03:~ # echo $http_proxy
> > > wwwcache.bbsrc.ac.uk:8080
> > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > wwwcache.bbsrc.ac.uk:8080
> > > 
> > > just in case ;-)
> > > 
> > > SO, i go into R and I get:
> > > 
> > > > source("http://www.bioconductor.org/getBioC.R")
> > > unable to connect to 'www.bioconductor.org' on port 80. Error in 
> > > file(file, "r") : cannot open URL 
> > > `http://www.bioconductor.org/getBioC.R'
> > > 
> > > OK so is R just not picking up my proxy setting?
> > 
> > Your setting is wrong, so it is being ignored.  The help page says 
> > quite explicitly
> > 
> >       The form of `"http_proxy"' should be 
> `"http://proxy.dom.com/"' or
> >      `"http://proxy.dom.com:8080/"' where the port defaults 
> to `80' and
> >      the trailing slash may be omitted.
> > 
> > > It seems to be trying
> > > port 80 on something, and I have specifically set it to 
> port 8080 in 
> > > my environment variables.  As far as I can see I have 
> followed the 
> > > reference manual suggestion, so does anyone else have one?
> > 
> > The problem is in your seeing, it seems.
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>



From spencer.graves at pdf.com  Fri Jul 25 12:29:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 03:29:57 -0700
Subject: [R] inverse prediction and Poisson regression
References: <97260c7d6c7dd8055d89d5f2b0402f85@ibookIRDA.local>
Message-ID: <3F2106A5.3060809@pdf.com>

Dear Prof. Ripley & M. Philion:

First some commentary then questions for Prof. Ripley and M. Philion.

COMMENTARY

	  Prof. Ripley said, "to fit a curve of mean response vs dose,
and find the dose at which the mean response is half of that at dose 0.
That one is easy."  Unfortunately, it is not obvious to me at the 
moment.  From "www.r-project.org" -> search -> "R site search" -> 
"LD50", I found "dose.p", described on p. 193, sec. 7.2, of Venables and 
Ripley (2002) Modern Applied Statistics with S, 4th ed. (Springer).

	  Then I cut the data set down to a size that I could easily play with, 
and fit Poisson regression:

Phytopath <- data.frame(x=c(0, 0.03, 0.1), 		
	y=c(28, 21, 11))
(fitP100 <- glm(y~log(x+0.015), data=Phytopath[rep(1:3, 100),],
   family="poisson"))

Call:  glm(formula = y ~ log(x + 0.015), family = "poisson", data = 
Phytopath[rep(1:3,      100), ])

Coefficients:
    (Intercept)  log(x + 0.015)
         1.6088         -0.4203

(LD50P100 <- dose.p(fitP100, p=14))
              Dose         SE
p = 14: -2.451018 0.04858572

	  To get a 95% confidence interval from this, in S-Plus 6.1, I did:

   exp(LD50 + c(-2,0, 2)*LD50 at SE)-0.015
[1] 0.01762321 0.07120579 0.21279601

	  R 1.7.1 seemed to require a different syntax, which I couldn't parse 
in my present insomniac state (3:20 AM in California).

QUESTIONS:

PROF. RIPLEY:  Is this what you said was easy?

M. PHILION:  Does this provide sufficient information for you to now 
solve your problem?

hope this helps.  spencer graves	

Prof Brian Ripley wrote:
 > On Fri, 25 Jul 2003, Vincent Philion wrote:
 >
 >
 >>Hello and thank you for your interest in this problem.
 >>
 >>"real life data" would look like this:
 >>
 >>x	y
 >>0		28
 >>0.03		21
 >>0.1		11
 >>0.3		15
 >>1		5
 >>3		4
 >>10		1
 >>30		0
 >>100		0
 >>
 >>x	y
 >>0	30
 >>0.0025	30
 >>0.02	25
 >>0.16	25
 >>1.28	10
 >>10.24	0
 >>81.92	0
 >>
 >>X	Y
 >>0	35
 >>0.00025	23
 >>0.002	14
 >>0.016	6
 >>0.128	5
 >>1.024	3
 >>8.192	2
 >>
 >>X	Y
 >>0  43
 >>0.00025  35
 >>0.002  20
 >>0.016  16
 >>0.128  11
 >>1.024  6
 >>8.192   0
 >>
 >>Where X is dose and Y is response.
 >>the relation is linear for log(response) = b log(dose) + intercept
 >
 >
 > Is that log(*mean* response), that is a log link and exponential decay
 > with dose?
 >
 >
 >>Response for dose 0 is a "control" = Ymax. So, What I want is the dose
 >>for 50% response. For instance, in example 1:
 >>
 >>Ymax = 28 (this is also an observation with Poisson error)
 >
 >
 > Once you observe Ymax, Y is no longer Poisson.
 >
 >
 >>So I want dose for response = 14 = approx. 0.3
 >
 >
 > What exactly is Ymax?  Is it the response at dose 0?  The mean 
response at
 > dose 0?  The largest response?  About the only thing I can actually
 > interpret is that you want to fit a curve of mean response vs dose, and
 > find the dose at which the mean response is half of that at dose 0.
 > That one is easy.
 >
 > I think you are confusing response with mean response, and we can't
 > disentangle them for you.
 >



From p.dalgaard at biostat.ku.dk  Fri Jul 25 12:41:54 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 25 Jul 2003 10:41:54 -0000
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <Pine.LNX.4.44.0307250744220.4525-100000@gannet.stats>
References: <Pine.LNX.4.44.0307250744220.4525-100000@gannet.stats>
Message-ID: <x2vftqzxx0.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:


> > x	y
> > 0		28
> > 0.03		21
> > 0.1		11
> > 0.3		15
> > 1		5
> > 3		4
> > 10		1
> > 30		0
> > 100		0
....
> > Where X is dose and Y is response. 
> > the relation is linear for log(response) = b log(dose) + intercept
> 
> Is that log(*mean* response), that is a log link and exponential decay 
> with dose?
> 
> > Response for dose 0 is a "control" = Ymax. So, What I want is the dose
> > for 50% response. For instance, in example 1:
> > 
> > Ymax = 28 (this is also an observation with Poisson error)
> 
> Once you observe Ymax, Y is no longer Poisson.
> 
> > So I want dose for response = 14 = approx. 0.3
> 
> What exactly is Ymax?  Is it the response at dose 0?  The mean response at
> dose 0?  The largest response?  About the only thing I can actually
> interpret is that you want to fit a curve of mean response vs dose, and
> find the dose at which the mean response is half of that at dose 0.
> That one is easy.
> 
> I think you are confusing response with mean response, and we can't 
> disentangle them for you.

I don't feel all that confused. Y is Poisson distributed with some
mean depending on x. Ymax is a value at X=0, i.e. Poisson distr.
with a mean as large as it can be. 

I think the main confusion here is trying to fit a functional
relationship which doesn't extend to X=0. If you extrapolate a
log-loglinear relation back to X=0, you get an infinite maximal
response if b is negative, so this is going to be inconsistent with a
finite Ymax. In some of the data sets I believe you actually do see a
leveling off for very small doses.

If you insist on this peculiar model, you'd end up with estimating the
mean of Ymax by its observed value. Then you can get b and the
intercept from the observations with X>0, and find your estimate of
halving dose by solving

 log(Ymax/2) = b * log(dose50) + intercept

i.e. dose50 = (log(Ymax/2)-intercept)/b. That's a nonlinear function
of the estimates, so you'd need (at least) to employ the Delta method
to find the approximate variance of the estimate.

However, I'd suggest that you should look for a more realistic
functional form of the relation, e.g. a logistic curve in log(x) or a
Michalis-Menten style inhibition (mean(Y) = ymax/(1+dose/dose50) or
variants thereof). These models are not (necessarily) GLMs, but I
think you can fit them quite well with gnls() and a suitable variance
function.

[In fact the ymax/(1+dose/dose50) model is a GLM if you use an inverse
link and reparametrize with 1/ymax, 1/(ymax*dose50), but inverse links
are not built-in for the poisson() family, so you'd have to modify the
code yourself.]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From th50 at leicester.ac.uk  Fri Jul 25 12:44:15 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 25 Jul 2003 11:44:15 +0100
Subject: [R] variable name of variable in dataframe
Message-ID: <1F2CE8D4B0195E488213E8B8CCF71486025012E2@saffron.cfs.le.ac.uk>

Dear Tobias,

The trick is "Programming on the Language", see e.g. the "R Language Manual". 
Construct the expression you want, and have it explicitly parsed and evaluated.

toy <- function(b=.95){
  toyframe <- eval(parse(text=paste("data.frame(lion", b, " = c(1, 2))", sep="")))
  return(toyframe)
z}
toy()
toy(0)

HTH

Thomas


---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Tobias Verbeke [mailto:tobias_verbeke at skynet.be]
> Sent: 25 July 2003 11:16
> To: R-help
> Subject: [R] variable name of variable in dataframe
> 
> 
> Dear list,
> 
> If I have this toy function:
> 
> toy <- function(b=.95){
> toyframe <- data.frame(lion.95 = c(1, 2))
> return(toyframe)
> }
> 
> How can I obtain that for any value b, 
> the name of the column becomes "lionb", 
> i.e. lion.95 if b = .95, lion.85 if b = .85 etc.
> knowing that .95 (.85 etc.) may also be
> given as 0.95 (0.85 etc.) but that the
> result should be lion.95 (lion.85 etc.) 
> 
> 
> Thanks in advance,
> 
> Tobias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From suzette at sdac.harvard.edu  Fri Jul 25 13:00:47 2003
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Fri, 25 Jul 2003 07:00:47 -0400 (EDT)
Subject: [R] trellis plot question
In-Reply-To: <3F210140.20363.7C25FD@localhost>
Message-ID: <Pine.GSO.4.40.0307250657480.11569-100000@sdac.harvard.edu>


Yes, that solved the problem.
Thank you,
Suzette

On Fri, 25 Jul 2003, Petr Pikal wrote:

> Hi
>
> You probably use numeric id variable.
> I think you need to make an id variable to become a factor.
>
> e.g. use
>
> as.factor(id)
>
> Cheers
> Petr
>
> On 24 Jul 2003 at 10:39, Suzette Blanchard wrote:
>
> >
> > Greetings,
> >
> > Does anyone know how to get an id number in the little header
> > above each individual plot within a trellis plot?  The default
> > seems to be to print the word id and add a line indicating on
> > a linear scale where the current id sits.
> >
> > Thanks in advance for any help you can send,
> >
> > Suzette
> >
> >
> > =================================
> > Suzette Blanchard, Ph.D.
> > Research Scientist
> > Frontier Science Foundation
> > 1244 Boylston St. Suite 303
> > Chestnut Hill, MA 02467
> > Email:  suzette at sdac.harvard.edu
> > Phone:  (617) 632-2007
> > Fax:    (617) 632-2001
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> Petr Pikal
> petr.pikal at precheza.cz
> p.pik at volny.cz
>
>


=================================
Suzette Blanchard, Ph.D.
Research Scientist
Frontier Science Foundation
1244 Boylston St. Suite 303
Chestnut Hill, MA 02467
Email:  suzette at sdac.harvard.edu
Phone:  (617) 632-2007
Fax:    (617) 632-2001



From hb at maths.lth.se  Fri Jul 25 12:57:25 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 25 Jul 2003 12:57:25 +0200
Subject: [R] variable name of variable in dataframe
In-Reply-To: <20030725121607.154886fc.tobias_verbeke@skynet.be>
Message-ID: <000001c3529b$8830bd90$e502eb82@maths.lth.se>

Assuming 0 <= b < 1 here is one solution:

toy <- function(b=.95){
  toyframe <- data.frame(dummy = c(1, 2))

  # Rename column/field #1
  colnames(toyframe)[1] <- sprintf("lion.%02d", as.integer(100*b)); 

  return(toyframe)
}

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tobias Verbeke
> Sent: den 25 juli 2003 12:16
> To: R-help
> Subject: [R] variable name of variable in dataframe
> 
> 
> Dear list,
> 
> If I have this toy function:
> 
> toy <- function(b=.95){
> toyframe <- data.frame(lion.95 = c(1, 2))
> return(toyframe)
> }
> 
> How can I obtain that for any value b, 
> the name of the column becomes "lionb", 
> i.e. lion.95 if b = .95, lion.85 if b = .85 etc.
> knowing that .95 (.85 etc.) may also be
> given as 0.95 (0.85 etc.) but that the
> result should be lion.95 (lion.85 etc.) 
> 
> 
> Thanks in advance,
> 
> Tobias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>



From ripley at stats.ox.ac.uk  Fri Jul 25 13:00:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 12:00:19 +0100 (BST)
Subject: [R] variable name of variable in dataframe
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF71486025012E2@saffron.cfs.le.ac.uk>
Message-ID: <Pine.LNX.4.44.0307251156070.8446-100000@gannet.stats>

There are easier (and more readable) ways: the simplest is perhaps to use

names(toyframe) <- paste("lion", b, sep=".")

after creating the data frame.  Others are to use  something like

arg <- list(a=c(1,2))
names(args) <- paste("lion", b, sep=".")
toyframe <- do.call("data.frame", args)

or to make use of substitute().  (Those really are `programming on the 
langauge' rather than using R as a text processor.)

On Fri, 25 Jul 2003, Hotz, T. wrote:

> Dear Tobias,
> 
> The trick is "Programming on the Language", see e.g. the "R Language Manual". 
> Construct the expression you want, and have it explicitly parsed and evaluated.
> 
> toy <- function(b=.95){
>   toyframe <- eval(parse(text=paste("data.frame(lion", b, " = c(1, 2))", sep="")))
>   return(toyframe)
> z}
> toy()
> toy(0)
> 
> HTH
> 
> Thomas
> 
> 
> ---
> 
> Thomas Hotz
> Research Associate in Medical Statistics
> University of Leicester
> United Kingdom
> 
> Department of Epidemiology and Public Health
> 22-28 Princess Road West
> Leicester
> LE1 6TP
> Tel +44 116 252-5410
> Fax +44 116 252-5423
> 
> Division of Medicine for the Elderly
> Department of Medicine
> The Glenfield Hospital
> Leicester
> LE3 9QP
> Tel +44 116 256-3643
> Fax +44 116 232-2976
> 
> 
> > -----Original Message-----
> > From: Tobias Verbeke [mailto:tobias_verbeke at skynet.be]
> > Sent: 25 July 2003 11:16
> > To: R-help
> > Subject: [R] variable name of variable in dataframe
> > 
> > 
> > Dear list,
> > 
> > If I have this toy function:
> > 
> > toy <- function(b=.95){
> > toyframe <- data.frame(lion.95 = c(1, 2))
> > return(toyframe)
> > }
> > 
> > How can I obtain that for any value b, 
> > the name of the column becomes "lionb", 
> > i.e. lion.95 if b = .95, lion.85 if b = .85 etc.
> > knowing that .95 (.85 etc.) may also be
> > given as 0.95 (0.85 etc.) but that the
> > result should be lion.95 (lion.85 etc.) 
> > 
> > 
> > Thanks in advance,
> > 
> > Tobias
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Jul 25 13:04:06 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 25 Jul 2003 11:04:06 -0000
Subject: [R] variable name of variable in dataframe
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF71486025012E2@saffron.cfs.le.ac.uk>
References: <1F2CE8D4B0195E488213E8B8CCF71486025012E2@saffron.cfs.le.ac.uk>
Message-ID: <x2r84ezww3.fsf@biostat.ku.dk>

"Hotz, T." <th50 at leicester.ac.uk> writes:

> Dear Tobias,
> 
> The trick is "Programming on the Language", see e.g. the "R Language Manual". 
> Construct the expression you want, and have it explicitly parsed and evaluated.
> 
> toy <- function(b=.95){
>   toyframe <- eval(parse(text=paste("data.frame(lion", b, " = c(1, 2))", sep="")))
>   return(toyframe)
> z}
> toy()
> toy(0)

A bit of an overkill in this case:

toy <- function(b=.95){
    toyframe <- data.frame(x=c(1,2))
    names(toyframe) <- paste("lion",b,sep="")
    toyframe
}

(Getting rid of the leading 0 is left as an exercise...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From olefc at daimi.au.dk  Fri Jul 25 13:11:28 2003
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Fri, 25 Jul 2003 13:11:28 +0200
Subject: [R] Re: geoR size limit problem
In-Reply-To: <BAY2-F15xrCXIjl4OsX0000369c@hotmail.com>
References: <BAY2-F15xrCXIjl4OsX0000369c@hotmail.com>
Message-ID: <3F211060.3080500@daimi.au.dk>

Dear Miha

I can reproduce your problem. The cause of the problem is not the size 
of the data (2500), but a combination of the number data locations and 
prediction location [could be that the covariance matrix of the data and 
the prediction variables is the computational bottleneck].
A workaround by splitting the set of prediction locations into smaller 
blocks is given below.

As I wrote in my previous e-mail, I would consider trying the kriging 
functions in the geostatistical packages.


## Example

library(geoR)
g.data <-as.geodata(cbind(rnorm(2500),rnorm(2500), rnorm(2500)))

expvar<-variog(g.data, uvec=seq(0,1000,25), option="bin")
maternfit<-variofit(expvar, ini=c(1600,300), cov.model="matern", 
fix.nug=TRUE, nug=25, kappa=1.5, max.dist=800, weights="npairs")


p.grid<-expand.grid((1:900)/900,(1:650)/650)

## this doesn't work
krige.matern<-krige.conv(g.data, loc=p.grid, 
krige=krige.control(obj.m=maternfit, type="OK"))


## workaround (be patient, it is slow):
nm.grid <- nrow(p.grid)
result <- list(predict=rep(0,nm.grid), krige.var=rep(0,nm.grid))
claas(result) <- "kriging"
##for(ii in 1:585){
for(ii in 1:5){
temp <- krige.matern<-krige.conv(g.data, 
loc=p.grid[(ii-1)*1000+(1:1000),], krige=krige.control(obj.m=maternfit, 
type="OK"))[c("predict", "krige.var")]
result$predict[(ii-1)*1000+(1:1000)] <- temp$predict
result$krige.var[(ii-1)*1000+(1:1000)] <- temp$krige.var
print(ii)
}


Best Regards Ole



Miha STAUT wrote:

>> Dear Miha (cc: Paulo Ribeiro [the developer of geoR, who is not 
>> subscribed to R-help])
>>
>> Some reproducible code would help here.
>
>
> Unfortunately I do not have an internet connect on my computer, that 
> is why I did every thing very descriptive. Anyway I will try:
>
> str(df)
>   [#approx 2500]
> $ x ...
> $ y ...
> $ z ...
> library(geoR)
> g<-as.geodata(df)
> expvar<-variog(g, uvec=seq(0,1000,25), option="bin")
> matern<-variofit(expvar, ini=c(1600,300), cov.model="matern", 
> fix.nug=T, nug=25, kappa=1.5, max.dist=800, weights="npairs")
>
> library(GRASS)
> G<-gmeta()
> grid<-expand.matrix(G$xseq,G$yseq)
> length(G$xseq)
> 900 #approx
> length(G$yseq)
> 650 #approx
> krige.matern<-krige.conv(g, loc=grid, 
> krige=krige.control(obj.m=matern, type="OK"))
>
> #5 min processing
> can not allocate vector of 1500000000 #approx
>
> Thanks, Miha Staut
>
>>
>> Guessing :
>>
>> * geoR does not implement kriging with local neighbourhoods, but 
>> instead conditions on all data. Therefore having 2500 data points 
>> would imply having a covariance matrix of size 2500*2500 approx 
>> 6million . Not sure if this is too much, but I do not think so.
>
>
> _________________________________________________________________

> http://join.msn.com/?page=features/virus
>
>

-- 
Ole F. Christensen
Center for Bioinformatik
Datalogisk Institut
Aarhus Universitet
Ny Munkegade, Bygning 540
8000 Aarhus C
Denmark



From tobias_verbeke at skynet.be  Fri Jul 25 13:23:41 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Fri, 25 Jul 2003 13:23:41 +0200
Subject: [R] variable name of variable in dataframe
In-Reply-To: <x2r84ezww3.fsf@biostat.ku.dk>
References: <1F2CE8D4B0195E488213E8B8CCF71486025012E2@saffron.cfs.le.ac.uk>
	<x2r84ezww3.fsf@biostat.ku.dk>
Message-ID: <20030725132341.52fa08a5.tobias_verbeke@skynet.be>

Sir Hotz, 
Sir Bengtsson, 
Sir Ripley and
Sir Dalgaard,

Thank you __very__ much for
your help.

Tobias



From michael.watson at bbsrc.ac.uk  Fri Jul 25 13:18:03 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 25 Jul 2003 12:18:03 +0100
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00968@cl-exsrv1.irad.bbsrc.ac.uk>

Hi Henrik

Thanks for your help, I really do appreciate it.

If I follow your instructions, R returns the value http://wwwcache.bbsrc.ac.uk:8080.  That is good and it means that indeed my http_proxy environment variable is set.

I have also added the lines

http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
HTTP_PROXY=http://wwwcache.bbsrc.ac.uk:8080/

both to .Renviron in my home directory, and to /usr/lib/R/etc/Renviron and /usr/lib/R/etc/Renviron.site.

All to no avail... R still doesn't try to connect through my proxy server.  

Please, I genuinely think this is a bug in R 1.7.1 on Suse Linux 8.1.

NOW, here is a little detail I have just discovered that PROVES my proxy is working.  

If I do:

update.packages(method="wget")

then everything works fine.... hmmmm, but I still have a problem as the command I really want to run is :

source("http://wwwbioconductor.org/getBioC.R")

and source() does not accept an option 'method="wget"'....

SO... is there a way in R that I can set it up such that ALL internet connections from within R use method="wget" ??

Thanks
Mick

-----Original Message-----
From: Henrik Bengtsson [mailto:hb at maths.lth.se]
Sent: 25 July 2003 11:25
To: 'michael watson (IAH-C)'
Cc: R-help at stat.math.ethz.ch
Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1


Could it be that you have redefined the command R in your shell such
that the http_proxy environment variable is set in one and R is running
in another? (This is just a wild guess and I am myself only running
WinXP.) What do you get if you do

% env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
% R
> Sys.getenv("http_proxy")

Also, have you considered setting http_proxy in ~/.Renviron (see
?.Renviron).

Cheers

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: den 25 juli 2003 10:24
> To: 'Prof Brian Ripley'
> Cc: 'R-help at stat.math.ethz.ch'
> Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Hi
> 
> Thanks once again for your help, I do appreciate it..... however....
> 
> Here is what I get with your test.... (under tcsh - i 
> normally use bash, but I will keep everything the same)
> 
> users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> 
> >options(internet.info=0)
> >update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> unable to connect to 'cran.r-project.org' on port 80
> Error in download.file(url = paste(contriburl, "PACKAGES", 
> sep = "/"), :
> 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'
> 
> 
> ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> message at all, which appears to suggest that R, under SUSE 
> Linux 8.1, is NOT PICKING up the http_proxy environment 
> variable - this isn't something thats wrong with my proxy, 
> that works with everything else - internet browsers, ftp 
> clients, wget, instant messenger clients etc etc.  The 
> problem is R, which isn't picking up that it needs to use the 
> http_proxy environment variable.  And I apologise for being 
> blunt, but that is an R problem, not a proxy problem!
> 
> Thanks for your help
> 
> Mick
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 24 July 2003 16:56
> To: michael watson (IAH-C)
> Subject: RE: [R] Your proxy seems not to work with R (was R 
> won't connect to the internet on Linux!)
> 
> 
> When I do (under tcsh)
> 
> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > options(internet.info=0)
> > update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080
> 
> it tries to connect to your proxy (as it says) and gets no 
> response, which is not surprising from my site.  If you get 
> the same, your proxy is probably not behaving in the standard 
> way (since that has been tested by many users with standard proxies).
> 
> I've changed the emphasis of the subject line to one I feel is more 
> equitable: many, many users have counter-evidence to your original 
> assertion, which was rather arrogant.
> 
> On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> 
> > Hello Professor
> > 
> > If you are suggesting that I am simply missing the 
> "http://" part of 
> > my cache URL, or that I am missing a trailing "/", then I 
> pre-empted 
> > this response and it still doesn't work.
> 
> I was suggesting that `simply' you were not reading the documentation 
> correctly. 
> 
> > I have tried setting both http_proxy and HTTP_PROXY to all of:
> 
> I hope you set to *each* of these.  The first and third are 
> documented to be incorrect, so using those was perverse.
> 
> > wwwcache.bbsrc.ac.uk:8080
> > http://wwwcache.bbsrc.ac.uk:8080
> > wwwcache.bbsrc.ac.uk:8080/
> > http://wwwcache.bbsrc.ac.uk:8080/
> > 
> > and I still get the same response - R cannot open the URL.
> > 
> > And yes, that is thw right proxy address, I copied it straight from 
> > Netscape on the same computer, and Netscape connects to the 
> internet 
> > fine.
> > 
> > Thanks
> > Mick
> >  
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: 24 July 2003 13:17
> > To: michael watson (IAH-C)
> > Cc: 'R-help at stat.math.ethz.ch '
> > Subject: Re: [R] R won't connect to the internet on Linux!
> > 
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > OK, I really am struggling with this one!  Forgive me if 
> I am being 
> > > stupid....
> > 
> > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> internet 
> > > through a proxy so I have:
> > > 
> > > IAHC-LINUX03:~ # echo $http_proxy
> > > wwwcache.bbsrc.ac.uk:8080
> > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > wwwcache.bbsrc.ac.uk:8080
> > > 
> > > just in case ;-)
> > > 
> > > SO, i go into R and I get:
> > > 
> > > > source("http://www.bioconductor.org/getBioC.R")
> > > unable to connect to 'www.bioconductor.org' on port 80. Error in 
> > > file(file, "r") : cannot open URL 
> > > `http://www.bioconductor.org/getBioC.R'
> > > 
> > > OK so is R just not picking up my proxy setting?
> > 
> > Your setting is wrong, so it is being ignored.  The help page says 
> > quite explicitly
> > 
> >       The form of `"http_proxy"' should be 
> `"http://proxy.dom.com/"' or
> >      `"http://proxy.dom.com:8080/"' where the port defaults 
> to `80' and
> >      the trailing slash may be omitted.
> > 
> > > It seems to be trying
> > > port 80 on something, and I have specifically set it to 
> port 8080 in 
> > > my environment variables.  As far as I can see I have 
> followed the 
> > > reference manual suggestion, so does anyone else have one?
> > 
> > The problem is in your seeing, it seems.
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>



From gblevins at mn.rr.com  Fri Jul 25 13:28:20 2003
From: gblevins at mn.rr.com (Greg Blevins)
Date: Fri, 25 Jul 2003 06:28:20 -0500
Subject: [R] Difficulty replacing NAs using Hmisc aregImpute and Impute
Message-ID: <010e01c3529f$da383420$1c361d41@mn.rr.com>

Hello R experts

I am using Hmisc aregImpute and Impute (following example on page 105 of The
Hmisc and Design Libraries).

*My end goal is to have NAs physically replaced in my dataframe.  I have
read the help pages and example in above sited pdf file, but to no avail.

Here is example of what I did.

Ph, my data frame, is attached.

> xt <-  aregImpute (~ q5 + q22rev02 + q28a, n.impute=10, x=T, data=Ph)
Iteration:1 2 3 4 5 6 7 8 9 10 11 12 13
> impute(xt)

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~q5 + q22rev02 + q28a, data = Ph, n.impute = 10,
    x = T)
Method: ace     n= 406  p= 3    Imputations: 10

Number of NAs:
      q5 q22rev02     q28a
       0       88       51

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
q22rev02     q28a
   0.348    0.170
> mean(q28a)
[1] NA

The q28a that the system is looking at still has NAs.

Much appreciate any help!

Greg Blevins
The Market Solutions Group, Inc.



From jgentry at jimmy.harvard.edu  Fri Jul 25 13:39:20 2003
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Fri, 25 Jul 2003 07:39:20 -0400 (EDT)
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00968@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.SOL.4.20.0307250738500.5581-100000@santiam.dfci.harvard.edu>

> then everything works fine.... hmmmm, but I still have a problem as the
> command I really want to run is :
> source("http://wwwbioconductor.org/getBioC.R")
> and source() does not accept an option 'method="wget"'....

getBioC() accepts the 'method' parameter as per its documentation.



From ripley at stats.ox.ac.uk  Fri Jul 25 13:39:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 12:39:59 +0100 (BST)
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00968@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0307251238220.8505-100000@gannet.stats>

On Fri, 25 Jul 2003, michael watson (IAH-C) wrote:

> SO... is there a way in R that I can set it up such that ALL internet connections from within R use method="wget" ??

[...]

RTFM once again!

options(download.file.method="wget")

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vincent.philion at irda.qc.ca  Fri Jul 25 14:07:03 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 08:07:03 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <Pine.LNX.4.44.0307250744220.4525-100000@gannet.stats>
Message-ID: <d857938f3e69c88417e2e4aa0d8ea2a1@ibookIRDA.local>

Hello sir, answers follow...

... Where X is dose and Y is response. the relation is linear for log(response) 
 = b log(dose) + intercept
 
 *** Is that log(*mean* response), that is a log link and exponential decay with 
 dose?
 
I'm not sure I understand what you mean by "mean", (no pun intended!) but Y is a biologicial "growth". Only one "observation" for each X. But this observation is from the growth contribution of about 500 individuals, so I guess it is a "mean" response by design.

the log link is for the Poisson regression, so the GLM is "response ~ log(dose), (family=poisson)"

 ...Response for dose 0 is a "control" = Ymax. So, What I want is the dose for 50% response. 

*** Once you observe Ymax, Y is no longer Poisson.

I don't understand this? What do you mean? Please explain.
 
 ***What exactly is Ymax?  Is it the response at dose 0? 
Correct. it is measured the same way as for any other Y. (It is also the largest response because the "dose" is always detrimental to growth)

***About the only thing I can actually  interpret is that you want to fit a curve of mean response vs dose, and
 find the dose at which the mean response is half of that at dose 0.

That's it. that sounds right! How? (Confidence interval on log scale and on real scale, etc) Given that the error on Y is Poisson and not "normal"

***That one is easy.
 
OK...?

*** I think you are confusing response with mean response, and we can't 
 disentangle them for you.
 
What else is needed?

bye for now,

-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From vincent.philion at irda.qc.ca  Fri Jul 25 14:20:09 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 08:20:09 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <3F2106A5.3060809@pdf.com>
Message-ID: <2ff899ebe6efa8a92ed82236824bd7a8@ibookIRDA.local>

Hello, and thanks for this.

>  From "www.r-project.org" -> search -> "R site search" -> "LD50", I found 
> "dose.p", described on p. 193, sec. 7.2, of Venables and Ripley (2002) Modern 
> Applied Statistics with S, 4th ed. (Springer).

I found the same, but this is for logistic regression I think, not Poisson. This is used to calculate the dose which causes 50% "death". I could do it this way by "pretending" my data is binomial.
i/e Y/Ymax = some value between 0 and 1. I could "pretend" my response data is "dead" or "alive", but I'm not sure this is "proper". but maybe it is? Any hints on this?

Y and Ymax follows a Poisson distribution. Can I just use Y/Ymax, run a logistic regression and go on with my life???
;-)


(LD50P100 <- dose.p(fitP100, p=14))
>              Dose         SE
> p = 14: -2.451018 0.04858572

Wow, you got the dose.p function to work with poisson regression? but under SPlus only? is it giving the right results?

... my 
> present insomniac state (3:20 AM in California).

The same for me!!!

;-)
I worked on this 2:00AM EDT
thanks and have a nice day, possibly starting with a good coffee!!!

> M. PHILION:  Does this provide sufficient information for you to now solve 
> your problem?

If this works under R and is giving the right solution, I owe you BIG TIME !

bye for now,

-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From baron at psych.upenn.edu  Fri Jul 25 14:22:47 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 25 Jul 2003 08:22:47 -0400
Subject: [R] Difficulty replacing NAs using Hmisc aregImpute and Impute
In-Reply-To: <010e01c3529f$da383420$1c361d41@mn.rr.com>
References: <010e01c3529f$da383420$1c361d41@mn.rr.com>
Message-ID: <20030725122247.GA21626@mail1.sas.upenn.edu>

On 07/25/03 06:28, Greg Blevins wrote:
>Hello R experts
>*My end goal is to have NAs physically replaced in my dataframe.  I have
>read the help pages and example in above sited pdf file, but to no avail.

aregImpute does not do what you want.  It creates n.impute
different values for each missing datum.  You san see them with
"imputed" in what it returns.

You might be able to get what you want with some other package
such as EMV, mix, norm, or the impute function in Hmisc.

An excellent and discussion of imputation for the novice (which I
just read yesterday - being a novice myself!) is by Shafer and
Graham, in Psychological Methods, 2002, 7, 147-177.  What
aregImput does is shown in Fig. 4 of that paper, as I understand
it.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From ripley at stats.ox.ac.uk  Fri Jul 25 14:38:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 13:38:00 +0100 (BST)
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <d857938f3e69c88417e2e4aa0d8ea2a1@ibookIRDA.local>
Message-ID: <Pine.LNX.4.44.0307251324500.8635-100000@gannet.stats>

Ymax is the maximum observation in your example, and also the observation 
at zero.  I was asking which you meant: if you meant Y at 0 (and I think 
you do) then it is somewhat misleading notation.

You have a set of Poisson random variables Y_x at different values of x.
Poisson random variables have a mean (I am using standard statistical 
terminilogy), so let's call that mu(x).  Then you seem to want the value 
of x such that  mu(x) = mu(0)/2 *or* mu(x) = Y_0/2, and I don't know 
which, except that in your model mu(0) would be infinity, and so the
model cannot fit your data (finite values of Y_0 have zero probability).

On Fri, 25 Jul 2003, Vincent Philion wrote:

> Hello sir, answers follow...
> 
> ... Where X is dose and Y is response. the relation is linear for log(response) 
>  = b log(dose) + intercept
>  
>  *** Is that log(*mean* response), that is a log link and exponential decay with 
>  dose?

>  I'm not sure I understand what you mean by "mean", (no pun intended!)
> but Y is a biologicial "growth". Only one "observation" for each X. But
> this observation is from the growth contribution of about 500
> individuals, so I guess it is a "mean" response by design.
> 
> the log link is for the Poisson regression, so the GLM is "response ~
> log(dose), (family=poisson)"

So you have -Inf as the explanatory variable at zero dose?

>  ...Response for dose 0 is a "control" = Ymax. So, What I want is the
> dose for 50% response.
> 
> *** Once you observe Ymax, Y is no longer Poisson.
> 
> I don't understand this? What do you mean? Please explain.

That was understanding Ymax to be the maximum Y, which is what it looks 
like.

>  ***What exactly is Ymax?  Is it the response at dose 0? 
> Correct. it is measured the same way as for any other Y. (It is also 
the largest response because the "dose" is always detrimental to growth)

The last is not true, given your assumptions,  It could have the largest 
mean response, but 0 is a possible value for Y_0.

> ***About the only thing I can actually  interpret is that you want to fit a curve of mean response vs dose, and
>  find the dose at which the mean response is half of that at dose 0.
> 
> That's it. that sounds right! How? (Confidence interval on log scale and on real scale, etc) Given that the error on Y is Poisson and not "normal"
> 
> ***That one is easy.
>  
> OK...?

Fit a model for the mean response (one that actually can fit your data), 
and solve the estimated mu(x) = mu()/2 or Y_0/2.  That gives you an 
estimate, and the delta method will give your standard errors.

> *** I think you are confusing response with mean response, and we can't 
>  disentangle them for you.
>  
> What else is needed?
> 
> bye for now,
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Jul 25 14:43:35 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 05:43:35 -0700
Subject: [R] inverse prediction and Poisson regression
References: <d857938f3e69c88417e2e4aa0d8ea2a1@ibookIRDA.local>
Message-ID: <3F2125F7.7030407@pdf.com>

	  The Poisson assumption means that Y is a number of independent events 
from a theoretically infinite population occurring in a specific time or 
place.  The function "glm" with 'family="poisson"' with the default link 
= "log" assumes that the logarithm of the mean of Y is a linear model in 
the explanatory variable.

	  How is Y measured?  Is it the number out of 500 who exceed a certain 
threshold, or is it the average percentage increase in weight of the 500 
or what?  If it the number out N, with N approximately 500 (and you know 
N), then you have a logistic regression situation.  In that case, 
section 7.2 in Venables and Ripley (2002) should do what you want.  If Y 
is a percentage increase

	  When dose = 0, log(dose) = (-Inf).  Since 0 is a legitimate dose, 
log(dose) is not acceptable in a model like this.  You need a model like 
Peter suggested.  Depending on you purpose, log(dose+0.015) might be 
sufficiently close to a model like what Peter suggested to answer your 
question.  If not, perhaps this solution will help you find a better 
solution.

	  I previously was able to get dose.p to work in R, and I just now was 
able to compute from its output.  The following worked in both S-Plus 
6.1 and R 1.7.1:

 > LD50P100p <- print(LD50P100)
              Dose         SE
p = 14: -2.451018 0.04858572
 > exp(LD50P100p[1,1]+c(-2,0,2)*LD50P100p[1,2])-0.015
[1] 0.06322317 0.07120579 0.08000303

hope this helps.  spencer graves

Vincent Philion wrote:
> Hello sir, answers follow...
> 
> ... Where X is dose and Y is response. the relation is linear for log(response) 
>  = b log(dose) + intercept
>  
>  *** Is that log(*mean* response), that is a log link and exponential decay with 
>  dose?
>  
> I'm not sure I understand what you mean by "mean", (no pun intended!) 
but Y is a biologicial "growth". Only one "observation" for each X. But
this observation is from the growth contribution of about 500 individuals,
so I guess it is a "mean" response by design.
> 
> the log link is for the Poisson regression, so the GLM is "response ~ log(dose), (family=poisson)"
> 
>  ...Response for dose 0 is a "control" = Ymax. So, What I want is the dose for 50% response. 
> 
> *** Once you observe Ymax, Y is no longer Poisson.
> 
> I don't understand this? What do you mean? Please explain.
>  
>  ***What exactly is Ymax?  Is it the response at dose 0? 
> Correct. it is measured the same way as for any other Y. (It is also the largest response because the "dose" is always detrimental to growth)
> 
> ***About the only thing I can actually  interpret is that you want to fit a curve of mean response vs dose, and
>  find the dose at which the mean response is half of that at dose 0.
> 
> That's it. that sounds right! How? (Confidence interval on log scale and on real scale, etc) Given that the error on Y is Poisson and not "normal"
> 
> ***That one is easy.
>  
> OK...?
> 
> *** I think you are confusing response with mean response, and we can't 
>  disentangle them for you.
>  
> What else is needed?
> 
> bye for now,
>



From vincent.philion at irda.qc.ca  Fri Jul 25 15:10:35 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 09:10:35 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <Pine.LNX.4.44.0307251324500.8635-100000@gannet.stats>
Message-ID: <8930170500d876e1fd0a83b6e3191b73@ibookIRDA.local>

Hello again, sorry for the notation. Again, I'm just a biologist!!!

;-)

But I'm enjoying this problem quite a bit! I'm very grateful for all the input. This is great. 


On 2003-07-25 08:38:00 -0400 Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

Answers:

> Ymax is the maximum observation in your example, and also the observation at 
> zero.  I was asking which you meant: if you meant Y at 0 (and I think you do) 
> then it is somewhat misleading notation.

I will clean up my notation!

> 
> You have a set of Poisson random variables Y_x at different values of x.
> Poisson random variables have a mean (I am using standard statistical 
> terminilogy), so let's call that mu(x).  Then you seem to want the value of x 
> such that  mu(x) = mu(0)/2 *or* mu(x) = Y_0/2, 

OK, I want x for mu(x) = mu(0)/2. 

> that in your model mu(0) would be infinity, and so the
> model cannot fit your data (finite values of Y_0 have zero probability).

Correct, This is part of the problem! The model does not "hold" for X = 0.

> the largest response because the "dose" is always detrimental to growth)
> 
> The last is not true, given your assumptions,  It could have the largest mean 
> response, but 0 is a possible value for Y_0.

Yes, you are right, but then there is no growth, nad no LD50 value, so we reject this sample...

> 
> Fit a model for the mean response (one that actually can fit your data), and 
> solve the estimated mu(x) = mu()/2 or Y_0/2.  That gives you an estimate, and 
> the delta method will give your standard errors.

Then you suggest using another model that will account for zero dose, OK. I think I saw something similar in another reply. I need to read it more carefully.

-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From vincent.philion at irda.qc.ca  Fri Jul 25 15:25:13 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 09:25:13 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <3F2125F7.7030407@pdf.com>
Message-ID: <6d33728d3d44803d6d232a7d2a171322@ibookIRDA.local>

Hi, ... and good morning!

;-)

On 2003-07-25 08:43:35 -0400 Spencer Graves <spencer.graves at PDF.COM> wrote:

> 	  The Poisson assumption means that Y is a number of independent events from 
> a theoretically infinite population occurring in a specific time or place.  
> The function "glm" with 'family="poisson"' with the default link = "log" 
> assumes that the logarithm of the mean of Y is a linear model in the 
> explanatory variable.

OK, I think my data can fit that description.

> 
> 	  How is Y measured?  

Y is the number of line intercepts which encounters mycelial growth. i/e if mycelia intercepts the line twice, 2 is reported. This follows poisson. 

If it the number out N, with N approximately 500 (and you know N), 
> then you have a logistic regression situation.

No, 500 spores can grow, but there is no "real" limit on the amount of growth possible, and so no limit on the number of intercepts. So this is why I adopted Poisson, not knowing how complicated my life would become!!!
;-)

  In that case, section 7.2 in 
> Venables and Ripley (2002) should do what you want.  If Y is a percentage 
> increase

... But you may be right, that I'm making this just too complicated and that I should simply look at percentage... Any comments on that?

 
> 	  When dose = 0, log(dose) = (-Inf).  Since 0 is a legitimate dose, 
> log(dose) is not acceptable in a model like this.  You need a model like 
> Peter suggested. 

OK, I see I will need stronger coffee to tackle this, but I will read this in depth today.

 Depending on you purpose, log(dose+0.015) might be 
> sufficiently close to a model like what Peter suggested to answer your 
> question.  If not, perhaps this solution will help you find a better 
> solution.

In other words, "cheat" and model Y_0 with a "small" value = log(0.015) ? How would this affect the LD50 value calculated and the confidence intervals? I guess I could try several methods, but how would I go about choosing the right one? Criteria?

> 	  I previously was able to get dose.p to work in R, and I just now was able 
> to compute from its output.  The following worked in both S-Plus 6.1 and R 
> 1.7.1:
> 
>> LD50P100p <- print(LD50P100)
>              Dose         SE
> p = 14: -2.451018 0.04858572
>> exp(LD50P100p[1,1]+c(-2,0,2)*LD50P100p[1,2])-0.015
> [1] 0.06322317 0.07120579 0.08000303

OK, I will need to try this (later today). I don't see "dose.p" in this?

again, many thanks,

-- 
Vincent Philion, M.Sc. agr.
Phytopathologiste
Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
3300 Sicotte, St-Hyacinthe
Qu?bec
J2S 7B8

t?l?phone: 450-778-6522 poste 233
courriel: vincent.philion at irda.qc.ca
Site internet : www.irda.qc.ca



From vincent.stoliaroff at sgcib.com  Fri Jul 25 16:50:14 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Fri, 25 Jul 2003 16:50:14 +0200
Subject: [R] named list 'start' in fitdistr
Message-ID: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>

Hi R lovers!

I'd like to know how to use the parameter 'start' in the function
fitdistr()
obviously I have to provide the initial value of the parameter to optimize
except in the case of a certain set of given distribution


Indeed according to the help file for fitdistr
"     For the following named distributions, reasonable starting values
     will be computed if `start' is omitted or only partially
     specified: `cauchy', `gamma', `logistic', `negative binomial' (R
     only, parametrized by `mu' and `size'), `t', `uniform', `weibull'.   "


However I cannot fit an univariate distribution named 'test'

I get this:

>fitdistr(test,"lognormal")
Error in fitdistr(test, "lognormal") : `start' must be a named list

> fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
Error: syntax error

How I am supposed to type a value for start depending on the distribution
on which I am fitting my set

thanks




******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From jcjorgensen at wisc.edu  Fri Jul 25 16:56:24 2003
From: jcjorgensen at wisc.edu (JEFFREY C JORGENSEN)
Date: Fri, 25 Jul 2003 09:56:24 -0500
Subject: [R] R-WinEdt problems
Message-ID: <141433414153e4.14153e41414334@wiscmail.wisc.edu>

Thanks, that seems to have worked.  I created a new
R-WinEdt shortcut and specified the path you suggest
below with quotes around R-WinEdt, such that the end
becomes:

-C="R-WinEdt" -e=r.ini

I also verified that the R mode was set to mdi.

Thanks so much.

Jeff Jorgensen
jcjorgensen at wisc.edu

----- Original Message -----
From: TyagiAnupam at aol.com
Date: Thursday, July 24, 2003 6:48 pm
Subject: Re: [R] R-WinEdt problems

> Right click on your WinEdt icon and go to
"Properties" menu-item. 
> In the 
> "Target" box look for something like, 
> "[PathToWinEdt]\WinEdt\WinEdt.exe -C="R-WinEdt"
-e=r.ini"
> 
> If you don't find something like this, make the
change and try it.
> 
> * It may be better to have a new icon for R-WinEdt
on your desktop 
> with the 
> above modified "Target", and use the existing icon
for other work. 
> Not sure how 
> to solve the switching problem, given that there
is a R specific 
> initialization file being used.
> 
> ---Anupam.
> 
> In a message dated 7/24/03 6:15:46 PM Eastern
Daylight Time, 
> jcjorgensen at wisc.edu writes:
> 
> > Hello,
> > 
> > 
> > I've done most all the various steps outlined in a
> > recent posting to the mailing list archives
> > (and in the help files) to load and run R-WinEdt.  I
> > can get it to run fine but I am not successful in
> > getting it to interface with RGui (1.6.2, not
> > minimized).  I try to use the R-line, R-source,
> > R-paste buttons with no success.  [All the necessary
> > *.edt files appear to be in the proper directories.]
> > 
> > I've done everything except modify the rprofile or
> > the options().  Is that necessary, and if so how
> > exactly do I do that?  And, if I do modify them how
> > do I restore them back to their default settings?
> > 
> > Any help would be much appreciated.
> > 
> > Thanks,
> > Jeff Jorgensen
> > jcjorgensen at wisc.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
>



From spencer.graves at pdf.com  Fri Jul 25 17:02:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 08:02:48 -0700
Subject: [R] named list 'start' in fitdistr
References: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>
Message-ID: <3F214698.8060407@pdf.com>

An example in help(fitdistr) (R 1.7.1 for Windows) is "fitdistr(x, 
dgamma, list(shape = 1, rate = 0.1), lower = 0.01)".  Based on this, 
have you tried the following:

 >>fitdistr(test,"lognormal", start=list(meanlog=0, sdlog=1))

hope this helps.  spencer graves

vincent.stoliaroff at sgcib.com wrote:
> Hi R lovers!
> 
> I'd like to know how to use the parameter 'start' in the function
> fitdistr()
> obviously I have to provide the initial value of the parameter to optimize
> except in the case of a certain set of given distribution
> 
> 
> Indeed according to the help file for fitdistr
> "     For the following named distributions, reasonable starting values
>      will be computed if `start' is omitted or only partially
>      specified: `cauchy', `gamma', `logistic', `negative binomial' (R
>      only, parametrized by `mu' and `size'), `t', `uniform', `weibull'.   "
> 
> 
> However I cannot fit an univariate distribution named 'test'
> 
> I get this:
> 
> 
>>fitdistr(test,"lognormal")
> 
> Error in fitdistr(test, "lognormal") : `start' must be a named list
> 
> 
>>fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
> 
> Error: syntax error
> 
> How I am supposed to type a value for start depending on the distribution
> on which I am fitting my set
> 
> thanks
> 
> 
> 
> 
> ******************************************************************
> The sender's email address has changed to 
> firstname.lastname@ sgcib.com. You may want to update your 
> personal address book. Please see http://www.sgcib.com for more 
> information.
>                                **
> This message and any attachments (the "message") are confide...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From vincent.stoliaroff at sgcib.com  Fri Jul 25 17:05:12 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Fri, 25 Jul 2003 17:05:12 +0200
Subject: [R] named list 'start' in fitdistr
Message-ID: <OF5DCB2467.A990BDDB-ONC1256D6E.0052D1F5@ges.marc.societe-generale.fr>


Oups! I should have read further the help file...

Thank you very much



|---------+---------------------------->
|         |           spencer.graves at PD|
|         |           F.COM            |
|         |                            |
|         |           07/25/03 05:02 PM|
|         |                            |
|---------+---------------------------->
  >------------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                              |
  |       To:       Vincent STOLIAROFF/fr/socgen at socgen                                                                          |
  |       cc:       r-help at stat.math.ethz.ch                                                                                     |
  |       Subject:  Re: [R] named list 'start' in fitdistr                                                                       |
  >------------------------------------------------------------------------------------------------------------------------------|




An example in help(fitdistr) (R 1.7.1 for Windows) is "fitdistr(x,
dgamma, list(shape = 1, rate = 0.1), lower = 0.01)".  Based on this,
have you tried the following:

 >>fitdistr(test,"lognormal", start=list(meanlog=0, sdlog=1))

hope this helps.  spencer graves

vincent.stoliaroff at sgcib.com wrote:
> Hi R lovers!
>
> I'd like to know how to use the parameter 'start' in the function
> fitdistr()
> obviously I have to provide the initial value of the parameter to
optimize
> except in the case of a certain set of given distribution
>
>
> Indeed according to the help file for fitdistr
> "     For the following named distributions, reasonable starting values
>      will be computed if `start' is omitted or only partially
>      specified: `cauchy', `gamma', `logistic', `negative binomial' (R
>      only, parametrized by `mu' and `size'), `t', `uniform', `weibull'.
"
>
>
> However I cannot fit an univariate distribution named 'test'
>
> I get this:
>
>
>>fitdistr(test,"lognormal")
>
> Error in fitdistr(test, "lognormal") : `start' must be a named list
>
>
>>fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
>
> Error: syntax error
>
> How I am supposed to type a value for start depending on the distribution
> on which I am fitting my set
>
> thanks
>
>
>
>
> ******************************************************************
> The sender's email address has changed to
> firstname.lastname@ sgcib.com. You may want to update your
> personal address book. Please see http://www.sgcib.com for more
> information.
>                                **
> This message and any attachments (the "message") are confide...
{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help









******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Jul 25 17:06:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jul 2003 16:06:33 +0100 (BST)
Subject: [R] named list 'start' in fitdistr
In-Reply-To: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>
Message-ID: <Pine.LNX.4.44.0307251601230.9107-100000@gannet.stats>

On Fri, 25 Jul 2003 vincent.stoliaroff at sgcib.com wrote:

> Hi R lovers!
> 
> I'd like to know how to use the parameter 'start' in the function
> fitdistr()

kindly provided for you in package MASS, but churlishly unattributed

> obviously I have to provide the initial value of the parameter to optimize
> except in the case of a certain set of given distribution
> 
> 
> Indeed according to the help file for fitdistr
> "     For the following named distributions, reasonable starting values
>      will be computed if `start' is omitted or only partially
>      specified: `cauchy', `gamma', `logistic', `negative binomial' (R
>      only, parametrized by `mu' and `size'), `t', `uniform', `weibull'.   "
> 
> 
> However I cannot fit an univariate distribution named 'test'

??? did you mean a distribution named `lognormal'?

> I get this:
> 
> >fitdistr(test,"lognormal")
> Error in fitdistr(test, "lognormal") : `start' must be a named list
> 
> > fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
> Error: syntax error
> 
> How I am supposed to type a value for start depending on the distribution
> on which I am fitting my set

On reading the help page:

   start: A named list giving the parameters to be optimized with
          initial values. 

so 

> test <- rlnorm(100, meanlog=0, sdlog=1)
> fitdistr(test, "lognormal", start=list(meanlog=0, sdlog=1))

works, just like it says it would.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From info at rhkoning.com  Fri Jul 25 17:09:30 2003
From: info at rhkoning.com (Ruud H. Koning)
Date: Fri, 25 Jul 2003 17:09:30 +0200
Subject: [R] named list 'start' in fitdistr
In-Reply-To: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>
References: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>
Message-ID: <200307251709300781.01DDA940@192.168.1.66>

 x <- exp(rnorm(100))
> fitdistr(x,"lognormal",start=list(meanlog=0,sdlog=1))
     meanlog        sdlog   
  -0.09974374    0.92352880 
 ( 0.09235288) ( 0.06530569)
> 


Ruud

*********** REPLY SEPARATOR  ***********

On 7/25/2003 at 4:50  vincent.stoliaroff at sgcib.com wrote:

>Hi R lovers!
>
>I'd like to know how to use the parameter 'start' in the function
>fitdistr()
>obviously I have to provide the initial value of the parameter to optimize
>except in the case of a certain set of given distribution
>
>
>Indeed according to the help file for fitdistr
>"     For the following named distributions, reasonable starting values
>     will be computed if `start' is omitted or only partially
>     specified: `cauchy', `gamma', `logistic', `negative binomial' (R
>     only, parametrized by `mu' and `size'), `t', `uniform', `weibull'.
"
>
>
>However I cannot fit an univariate distribution named 'test'
>
>I get this:
>
>>fitdistr(test,"lognormal")
>Error in fitdistr(test, "lognormal") : `start' must be a named list
>
>> fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
>Error: syntax error
>
>How I am supposed to type a value for start depending on the distribution
>on which I am fitting my set
>
>thanks
>
>
>
>
>******************************************************************
>The sender's email address has changed to 
>firstname.lastname@ sgcib.com. You may want to update your 
>personal address book. Please see http://www.sgcib.com for more 
>information.
>                               **
>This message and any attachments (the "message") are confide...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Fri Jul 25 17:10:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 25 Jul 2003 08:10:41 -0700 (PDT)
Subject: [R] named list 'start' in fitdistr
In-Reply-To: <OF26DBD14C.1C9857A4-ONC1256D6E.00503100@ges.marc.societe-generale.fr>
Message-ID: <Pine.A41.4.44.0307250809400.131280-100000@homer36.u.washington.edu>

On Fri, 25 Jul 2003 vincent.stoliaroff at sgcib.com wrote:

>
> >fitdistr(test,"lognormal")
> Error in fitdistr(test, "lognormal") : `start' must be a named list
>
> > fitdistr(test,"lognormal",start$meanlog=0,start$sdlog=1)
> Error: syntax error
>
> How I am supposed to type a value for start depending on the distribution
> on which I am fitting my set

Well, if start is supposed to be a named list I would try that:

fitdistr(test,"lognormal",start=list(meanlog=0, sdlog=1))

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From emma042 at yahoo.com  Fri Jul 25 17:24:26 2003
From: emma042 at yahoo.com (=?iso-8859-1?q?Emma=20Tan?=)
Date: Fri, 25 Jul 2003 16:24:26 +0100 (BST)
Subject: [R] glmmPQL using REML instead of ML
Message-ID: <20030725152426.6244.qmail@web14207.mail.yahoo.com>

Hi,

In glmmPQL in the MASS library, the function uses
repeated calls to the function lme(), using ML.  Does
anyone know how you can change this to REML?  I know
that in lme(), the default is actually set to REML and
you can also specify this as 'method=REML' or
'method'ML' but this isn't applicable to glmmPQL().  

I'd appreciate any help or advice!
Thanks,

Emma



From spencer.graves at pdf.com  Fri Jul 25 17:33:35 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 08:33:35 -0700
Subject: [R] inverse prediction and Poisson regression
References: <6d33728d3d44803d6d232a7d2a171322@ibookIRDA.local>
Message-ID: <3F214DCF.7060309@pdf.com>

	  Peter Dalgaard suggested using "a Michalis-Menten style inhibition 
(mean(Y) = ymax/(1+dose/dose50) or variants thereof)."  If you are not 
familiar with the literature on "Michalis-Menten inhibition", I suggest 
you research that and cite it in your research report.  Even if you 
don't use it, the knowledge of it might help persuade others that you at 
least considered it.

	  If we adopt Ripley's notation, this is mu(dose) = 
ymax/(1+dose/dose50), where ymax and dose50 are to be estimated.  We can 
get this, sort of, using "glm", as follows:  This model is the same as 
log(mu(dose)) = ymax - log(1+dose/dose50).  I have not tried this, but I 
believe that for fixed dose50 = 0.3, this model can be written for glm 
with the toy example I used before as follows:

Phytopath <- data.frame(x=c(0, 0.03, 0.1), 		
	y=c(28, 21, 11))
fitP.3 <- glm(y~offset(-log(1+x/0.3)), data=Phytopath[rep(1:3, 100),],
   family="poisson")
fitP.3$deviance
[1] 359.5893

	  Now, replace 0.3 by other possible values for dose50 to find the 
minimum "deviance".  Then get a 95% confidence interval using "profile 
likelihood" to find the points above and below the minimium where the 
"deviance" exceeds the minimum by  qchisq(0.95, 1) = 3.841459.  There 
are better ways to do this, but none that I can make work in the next 2 
minutes.  When I did this, I get the following:

 > fitP.06 <- glm(y~offset(-log(1+x/0.06)), data=Phytopath[rep(1:3, 100),],
+  family="poisson")
 > fitP.06$deviance
[1] 16.54977
 > fitP.065 <- glm(y~offset(-log(1+x/0.065)), data=Phytopath[rep(1:3, 
100),],
+  family="poisson")
 > fitP.065$deviance
[1] 11.59525
 > fitP.07 <- glm(y~offset(-log(1+x/0.07)), data=Phytopath[rep(1:3, 100),],
+  family="poisson")
 > fitP.07$deviance
[1] 10.96356
 > fitP.075 <- glm(y~offset(-log(1+x/0.075)), data=Phytopath[rep(1:3, 
100),],
+  family="poisson")
 > fitP.075$deviance
[1] 13.49366
 > fitP.08 <- glm(y~offset(-log(1+x/0.08)), data=Phytopath[rep(1:3, 100),],
+  family="poisson")
 > fitP.08$deviance
[1] 18.34463

 From this, I get an approximate maximum likelihood estimate for dose50 
of 0.07 with a 95% confidence interval by rough interpolation of 0.061 
to 0.076.

hope this helps.  spencer graves
p.s.  Have you plotted y vs. x with appropriate transformations?  If 
not, I suggest you do that before anything else.  Doug Bates says that 
students in his classes who don't plot the data get instant F's.  When I 
don't bother plotting the data several different ways, I often do stupid 
things.

Vincent Philion wrote:
> Hi, ... and good morning!
> 
> ;-)
> 
> On 2003-07-25 08:43:35 -0400 Spencer Graves <spencer.graves at PDF.COM> wrote:
> 
> 
>>	  The Poisson assumption means that Y is a number of independent events from 
>>a theoretically infinite population occurring in a specific time or place.  
>>The function "glm" with 'family="poisson"' with the default link = "log" 
>>assumes that the logarithm of the mean of Y is a linear model in the 
>>explanatory variable.
> 
> 
> OK, I think my data can fit that description.
> 
> 
>>	  How is Y measured?  
> 
> 
> Y is the number of line intercepts which encounters mycelial growth. 
i/e if mycelia intercepts the line twice, 2 is reported. This follows
poisson.
> 
> If it the number out N, with N approximately 500 (and you know N), 
> 
>>then you have a logistic regression situation.
> 
> 
> No, 500 spores can grow, but there is no "real" limit on the amount of 
growth possible, and so no limit on the number of intercepts. So this is
why I adopted Poisson, not knowing how complicated my life would become!!!
> ;-)
> 
>   In that case, section 7.2 in 
> 
>>Venables and Ripley (2002) should do what you want.  If Y is a percentage 
>>increase
> 
> 
> ... But you may be right, that I'm making this just too complicated 
and that I should simply look at percentage... Any comments on that?
> 
>  
> 
>>	  When dose = 0, log(dose) = (-Inf).  Since 0 is a legitimate dose, 
>>log(dose) is not acceptable in a model like this.  You need a model like 
>>Peter suggested. 
> 
> 
> OK, I see I will need stronger coffee to tackle this, but I will read this in depth today.
> 
>  Depending on you purpose, log(dose+0.015) might be 
> 
>>sufficiently close to a model like what Peter suggested to answer your 
>>question.  If not, perhaps this solution will help you find a better 
>>solution.
> 
> 
> In other words, "cheat" and model Y_0 with a "small" value = log(0.015) ? 
How would this affect the LD50 value calculated and the confidence 
intervals?
I guess I could try several methods, but how would I go about choosing the
right one? Criteria?
> 
> 
>>	  I previously was able to get dose.p to work in R, and I just now was able 
>>to compute from its output.  The following worked in both S-Plus 6.1 and R 
>>1.7.1:
>>
>>
>>>LD50P100p <- print(LD50P100)
>>
>>             Dose         SE
>>p = 14: -2.451018 0.04858572
>>
>>>exp(LD50P100p[1,1]+c(-2,0,2)*LD50P100p[1,2])-0.015
>>
>>[1] 0.06322317 0.07120579 0.08000303
> 
> 
> OK, I will need to try this (later today). I don't see "dose.p" in this?
> 
> again, many thanks,
>



From M.Mamin at intershop.de  Fri Jul 25 17:35:22 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Fri, 25 Jul 2003 17:35:22 +0200
Subject: [R] RODBC.sqlSave and Date format on Oracle
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF06C@jena03.net.j.ad.intershop.net>


Hi,

I try to use sqlSave to fill a date column in Oracle.

value example: '05-JUL-03 13:35:09'

sqlSave does not throw any error, but my table remain empty


does anyone have experience with this ?


Thanks,

Marc Mamin



From rvaradha at jhsph.edu  Fri Jul 25 17:55:22 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 25 Jul 2003 11:55:22 -0400
Subject: [R] inverse prediction and Poisson regression
Message-ID: <568a7056f77a.56f77a568a70@jhsph.edu>

Vincent:

Here is a simple solution using Prof. Bates' non-linear least squares 
algorithm:

Best,
Ravi.

> Phytopath <- data.frame(x=c(0, 0.03, 0.1), y=c(28, 21, 11))

> Phyto.nls <- nls(y ~ Ymax/(1 + x/x50),data=Phytopath,start=list
(Ymax=20.0,x50=0.01),trace=T)
404.3058 :  20.00  0.01 
15.76932 :  27.96313636  0.04960484 
2.043625 :  28.2145584  0.0694645 
1.851401 :  28.33886844  0.07198951 
1.851231 :  28.34892493  0.07185953 
1.851230 :  28.34843670  0.07186804 
1.851230 :  28.3484688  0.0718675 
> summary(Phyto.nls)

Formula: y ~ Ymax/(1 + x/x50)

Parameters:
     Estimate Std. Error t value Pr(>|t|)  
Ymax 28.34847    1.31522  21.554   0.0295 *
x50   0.07187    0.01348   5.332   0.1180  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 1.361 on 1 degrees of freedom

Correlation of Parameter Estimates:
       Ymax
x50 -0.6001



----- Original Message -----
From: Vincent Philion <vincent.philion at irda.qc.ca>
Date: Friday, July 25, 2003 9:25 am
Subject: Re: [R] inverse prediction and Poisson regression

> Hi, ... and good morning!
> 
> ;-)
> 
> On 2003-07-25 08:43:35 -0400 Spencer Graves 
> <spencer.graves at PDF.COM> wrote:
> 
> > 	  The Poisson assumption means that Y is a number of 
> independent events from 
> > a theoretically infinite population occurring in a specific time 
> or place.  
> > The function "glm" with 'family="poisson"' with the default link 
> = "log" 
> > assumes that the logarithm of the mean of Y is a linear model in 
> the 
> > explanatory variable.
> 
> OK, I think my data can fit that description.
> 
> > 
> > 	  How is Y measured?  
> 
> Y is the number of line intercepts which encounters mycelial 
> growth. i/e if mycelia intercepts the line twice, 2 is reported. 
> This follows poisson. 
> 
> If it the number out N, with N approximately 500 (and you know N), 
> > then you have a logistic regression situation.
> 
> No, 500 spores can grow, but there is no "real" limit on the 
> amount of growth possible, and so no limit on the number of 
> intercepts. So this is why I adopted Poisson, not knowing how 
> complicated my life would become!!!
> ;-)
> 
>  In that case, section 7.2 in 
> > Venables and Ripley (2002) should do what you want.  If Y is a 
> percentage 
> > increase
> 
> ... But you may be right, that I'm making this just too 
> complicated and that I should simply look at percentage... Any 
> comments on that?
> 
> 
> > 	  When dose = 0, log(dose) = (-Inf).  Since 0 is a legitimate 
> dose, 
> > log(dose) is not acceptable in a model like this.  You need a 
> model like 
> > Peter suggested. 
> 
> OK, I see I will need stronger coffee to tackle this, but I will 
> read this in depth today.
> 
> Depending on you purpose, log(dose+0.015) might be 
> > sufficiently close to a model like what Peter suggested to 
> answer your 
> > question.  If not, perhaps this solution will help you find a 
> better 
> > solution.
> 
> In other words, "cheat" and model Y_0 with a "small" value = 
> log(0.015) ? How would this affect the LD50 value calculated and 
> the confidence intervals? I guess I could try several methods, but 
> how would I go about choosing the right one? Criteria?
> 
> > 	  I previously was able to get dose.p to work in R, and I just 
> now was able 
> > to compute from its output.  The following worked in both S-Plus 
> 6.1 and R 
> > 1.7.1:
> > 
> >> LD50P100p <- print(LD50P100)
> >              Dose         SE
> > p = 14: -2.451018 0.04858572
> >> exp(LD50P100p[1,1]+c(-2,0,2)*LD50P100p[1,2])-0.015
> > [1] 0.06322317 0.07120579 0.08000303
> 
> OK, I will need to try this (later today). I don't see "dose.p" in 
> this?
> again, many thanks,
> 
> -- 
> Vincent Philion, M.Sc. agr.
> Phytopathologiste
> Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
> 3300 Sicotte, St-Hyacinthe
> Qu?bec
> J2S 7B8
> 
> t?l?phone: 450-778-6522 poste 233
> courriel: vincent.philion at irda.qc.ca
> Site internet : www.irda.qc.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jmacdon at med.umich.edu  Fri Jul 25 17:58:13 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 25 Jul 2003 11:58:13 -0400
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
Message-ID: <sf211b5f.083@mail-02.med.umich.edu>

HTTP_PROXY issues aside, if all you want to do is install Bioconductor,
simply download the latest bioconductor_xx.tar.gz and use R CMD
INSTALL.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> 07/25/03
07:18AM >>>
Hi Henrik

Thanks for your help, I really do appreciate it.

If I follow your instructions, R returns the value
http://wwwcache.bbsrc.ac.uk:8080.  That is good and it means that
indeed my http_proxy environment variable is set.

I have also added the lines

http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
HTTP_PROXY=http://wwwcache.bbsrc.ac.uk:8080/

both to .Renviron in my home directory, and to /usr/lib/R/etc/Renviron
and /usr/lib/R/etc/Renviron.site.

All to no avail... R still doesn't try to connect through my proxy
server.  

Please, I genuinely think this is a bug in R 1.7.1 on Suse Linux 8.1.

NOW, here is a little detail I have just discovered that PROVES my
proxy is working.  

If I do:

update.packages(method="wget")

then everything works fine.... hmmmm, but I still have a problem as the
command I really want to run is :

source("http://wwwbioconductor.org/getBioC.R")

and source() does not accept an option 'method="wget"'....

SO... is there a way in R that I can set it up such that ALL internet
connections from within R use method="wget" ??

Thanks
Mick

-----Original Message-----
From: Henrik Bengtsson [mailto:hb at maths.lth.se] 
Sent: 25 July 2003 11:25
To: 'michael watson (IAH-C)'
Cc: R-help at stat.math.ethz.ch 
Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1


Could it be that you have redefined the command R in your shell such
that the http_proxy environment variable is set in one and R is
running
in another? (This is just a wild guess and I am myself only running
WinXP.) What do you get if you do

% env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
% R
> Sys.getenv("http_proxy")

Also, have you considered setting http_proxy in ~/.Renviron (see
?.Renviron).

Cheers

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: den 25 juli 2003 10:24
> To: 'Prof Brian Ripley'
> Cc: 'R-help at stat.math.ethz.ch' 
> Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Hi
> 
> Thanks once again for your help, I do appreciate it..... however....
> 
> Here is what I get with your test.... (under tcsh - i 
> normally use bash, but I will keep everything the same)
> 
> users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> 
> >options(internet.info=0)
> >update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> unable to connect to 'cran.r-project.org' on port 80
> Error in download.file(url = paste(contriburl, "PACKAGES", 
> sep = "/"), :
> 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'

> 
> 
> ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> message at all, which appears to suggest that R, under SUSE 
> Linux 8.1, is NOT PICKING up the http_proxy environment 
> variable - this isn't something thats wrong with my proxy, 
> that works with everything else - internet browsers, ftp 
> clients, wget, instant messenger clients etc etc.  The 
> problem is R, which isn't picking up that it needs to use the 
> http_proxy environment variable.  And I apologise for being 
> blunt, but that is an R problem, not a proxy problem!
> 
> Thanks for your help
> 
> Mick
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: 24 July 2003 16:56
> To: michael watson (IAH-C)
> Subject: RE: [R] Your proxy seems not to work with R (was R 
> won't connect to the internet on Linux!)
> 
> 
> When I do (under tcsh)
> 
> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > options(internet.info=0)
> > update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080 
> 
> it tries to connect to your proxy (as it says) and gets no 
> response, which is not surprising from my site.  If you get 
> the same, your proxy is probably not behaving in the standard 
> way (since that has been tested by many users with standard
proxies).
> 
> I've changed the emphasis of the subject line to one I feel is more 
> equitable: many, many users have counter-evidence to your original 
> assertion, which was rather arrogant.
> 
> On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> 
> > Hello Professor
> > 
> > If you are suggesting that I am simply missing the 
> "http://" part of 
> > my cache URL, or that I am missing a trailing "/", then I 
> pre-empted 
> > this response and it still doesn't work.
> 
> I was suggesting that `simply' you were not reading the documentation

> correctly. 
> 
> > I have tried setting both http_proxy and HTTP_PROXY to all of:
> 
> I hope you set to *each* of these.  The first and third are 
> documented to be incorrect, so using those was perverse.
> 
> > wwwcache.bbsrc.ac.uk:8080
> > http://wwwcache.bbsrc.ac.uk:8080 
> > wwwcache.bbsrc.ac.uk:8080/
> > http://wwwcache.bbsrc.ac.uk:8080/ 
> > 
> > and I still get the same response - R cannot open the URL.
> > 
> > And yes, that is thw right proxy address, I copied it straight from

> > Netscape on the same computer, and Netscape connects to the 
> internet 
> > fine.
> > 
> > Thanks
> > Mick
> >  
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: 24 July 2003 13:17
> > To: michael watson (IAH-C)
> > Cc: 'R-help at stat.math.ethz.ch '
> > Subject: Re: [R] R won't connect to the internet on Linux!
> > 
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > OK, I really am struggling with this one!  Forgive me if 
> I am being 
> > > stupid....
> > 
> > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> internet 
> > > through a proxy so I have:
> > > 
> > > IAHC-LINUX03:~ # echo $http_proxy
> > > wwwcache.bbsrc.ac.uk:8080
> > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > wwwcache.bbsrc.ac.uk:8080
> > > 
> > > just in case ;-)
> > > 
> > > SO, i go into R and I get:
> > > 
> > > > source("http://www.bioconductor.org/getBioC.R")
> > > unable to connect to 'www.bioconductor.org' on port 80. Error in

> > > file(file, "r") : cannot open URL 
> > > `http://www.bioconductor.org/getBioC.R' 
> > > 
> > > OK so is R just not picking up my proxy setting?
> > 
> > Your setting is wrong, so it is being ignored.  The help page says

> > quite explicitly
> > 
> >       The form of `"http_proxy"' should be 
> `"http://proxy.dom.com/"' or
> >      `"http://proxy.dom.com:8080/"' where the port defaults 
> to `80' and
> >      the trailing slash may be omitted.
> > 
> > > It seems to be trying
> > > port 80 on something, and I have specifically set it to 
> port 8080 in 
> > > my environment variables.  As far as I can see I have 
> followed the 
> > > reference manual suggestion, so does anyone else have one?
> > 
> > The problem is in your seeing, it seems.
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk 
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/

> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Fri Jul 25 18:14:02 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 09:14:02 -0700
Subject: [R] inverse prediction and Poisson regression
References: <568a7056f77a.56f77a568a70@jhsph.edu>
Message-ID: <3F21574A.1080704@pdf.com>

The problem with nls is that it is NOT maximum likelihood for the 
Poisson distribution.  For the Poisson, the standard deviation is the 
square root of the mean, while nls assumes constant standard deviation. 
  That's why I stayed with glm.  The answers should be comparable, but I 
would prefer the glm answers.  spencer graves

Ravi Varadhan wrote:
> Vincent:
> 
> Here is a simple solution using Prof. Bates' non-linear least squares 
> algorithm:
> 
> Best,
> Ravi.
> 
> 
>>Phytopath <- data.frame(x=c(0, 0.03, 0.1), y=c(28, 21, 11))
> 
> 
>>Phyto.nls <- nls(y ~ Ymax/(1 + x/x50),data=Phytopath,start=list
> 
> (Ymax=20.0,x50=0.01),trace=T)
> 404.3058 :  20.00  0.01 
> 15.76932 :  27.96313636  0.04960484 
> 2.043625 :  28.2145584  0.0694645 
> 1.851401 :  28.33886844  0.07198951 
> 1.851231 :  28.34892493  0.07185953 
> 1.851230 :  28.34843670  0.07186804 
> 1.851230 :  28.3484688  0.0718675 
> 
>>summary(Phyto.nls)
> 
> 
> Formula: y ~ Ymax/(1 + x/x50)
> 
> Parameters:
>      Estimate Std. Error t value Pr(>|t|)  
> Ymax 28.34847    1.31522  21.554   0.0295 *
> x50   0.07187    0.01348   5.332   0.1180  
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Residual standard error: 1.361 on 1 degrees of freedom
> 
> Correlation of Parameter Estimates:
>        Ymax
> x50 -0.6001
> 
> 
> 
> ----- Original Message -----
> From: Vincent Philion <vincent.philion at irda.qc.ca>
> Date: Friday, July 25, 2003 9:25 am
> Subject: Re: [R] inverse prediction and Poisson regression
> 
> 
>>Hi, ... and good morning!
>>
>>;-)
>>
>>On 2003-07-25 08:43:35 -0400 Spencer Graves 
>><spencer.graves at PDF.COM> wrote:
>>
>>
>>>	  The Poisson assumption means that Y is a number of 
>>
>>independent events from 
>>
>>>a theoretically infinite population occurring in a specific time 
>>
>>or place.  
>>
>>>The function "glm" with 'family="poisson"' with the default link 
>>
>>= "log" 
>>
>>>assumes that the logarithm of the mean of Y is a linear model in 
>>
>>the 
>>
>>>explanatory variable.
>>
>>OK, I think my data can fit that description.
>>
>>
>>>	  How is Y measured?  
>>
>>Y is the number of line intercepts which encounters mycelial 
>>growth. i/e if mycelia intercepts the line twice, 2 is reported. 
>>This follows poisson. 
>>
>>If it the number out N, with N approximately 500 (and you know N), 
>>
>>>then you have a logistic regression situation.
>>
>>No, 500 spores can grow, but there is no "real" limit on the 
>>amount of growth possible, and so no limit on the number of 
>>intercepts. So this is why I adopted Poisson, not knowing how 
>>complicated my life would become!!!
>>;-)
>>
>> In that case, section 7.2 in 
>>
>>>Venables and Ripley (2002) should do what you want.  If Y is a 
>>
>>percentage 
>>
>>>increase
>>
>>... But you may be right, that I'm making this just too 
>>complicated and that I should simply look at percentage... Any 
>>comments on that?
>>
>>
>>
>>>	  When dose = 0, log(dose) = (-Inf).  Since 0 is a legitimate 
>>
>>dose, 
>>
>>>log(dose) is not acceptable in a model like this.  You need a 
>>
>>model like 
>>
>>>Peter suggested. 
>>
>>OK, I see I will need stronger coffee to tackle this, but I will 
>>read this in depth today.
>>
>>Depending on you purpose, log(dose+0.015) might be 
>>
>>>sufficiently close to a model like what Peter suggested to 
>>
>>answer your 
>>
>>>question.  If not, perhaps this solution will help you find a 
>>
>>better 
>>
>>>solution.
>>
>>In other words, "cheat" and model Y_0 with a "small" value = 
>>log(0.015) ? How would this affect the LD50 value calculated and 
>>the confidence intervals? I guess I could try several methods, but 
>>how would I go about choosing the right one? Criteria?
>>
>>
>>>	  I previously was able to get dose.p to work in R, and I just 
>>
>>now was able 
>>
>>>to compute from its output.  The following worked in both S-Plus 
>>
>>6.1 and R 
>>
>>>1.7.1:
>>>
>>>
>>>>LD50P100p <- print(LD50P100)
>>>
>>>             Dose         SE
>>>p = 14: -2.451018 0.04858572
>>>
>>>>exp(LD50P100p[1,1]+c(-2,0,2)*LD50P100p[1,2])-0.015
>>>
>>>[1] 0.06322317 0.07120579 0.08000303
>>
>>OK, I will need to try this (later today). I don't see "dose.p" in 
>>this?
>>again, many thanks,
>>
>>-- 
>>Vincent Philion, M.Sc. agr.
>>Phytopathologiste
>>Institut de Recherche et de D?veloppement en Agroenvironnement (IRDA)
>>3300 Sicotte, St-Hyacinthe
>>Qu?bec
>>J2S 7B8
>>
>>t?l?phone: 450-778-6522 poste 233
>>courriel: vincent.philion at irda.qc.ca
>>Site internet : www.irda.qc.ca
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
>



From vincent.philion at irda.qc.ca  Fri Jul 25 21:16:31 2003
From: vincent.philion at irda.qc.ca (Vincent Philion)
Date: Fri, 25 Jul 2003 15:16:31 -0400
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <3F21574A.1080704@pdf.com>
Message-ID: <bb6213bd869494ac965db688eec69207@ibookIRDA.local>

Hello to all: first and foremost: thank you for all this input. I only discovered about "R" last week (!) and I think I will dump my SAS license!!!

;-)

This is a very dynamic listserve!
You "R" all great! Thank you!

I just hope some day I can help out a student the way you did today.

I will spend part of the weekend studying the different suggestions in detail. Again, I'm not a stats person, so I will need some time and good coffee to digest all this correctly. (I'm most worried about understanding the nonlin suggestion.) Early next week, I will post a "summary" of the suggestions and the path I chose to follow. (with proper syntax Professor Ripley, I promise)

;-)

Have a nice weekend

Best regards,

Vincent Philion



From p.dalgaard at biostat.ku.dk  Fri Jul 25 21:43:46 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 25 Jul 2003 19:43:46 -0000
Subject: [R] inverse prediction and Poisson regression
In-Reply-To: <3F21574A.1080704@pdf.com>
References: <568a7056f77a.56f77a568a70@jhsph.edu> <3F21574A.1080704@pdf.com>
Message-ID: <x2n0f2z8u6.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> The problem with nls is that it is NOT maximum likelihood for the
> Poisson distribution.  For the Poisson, the standard deviation is the
> square root of the mean, while nls assumes constant standard
> deviation. That's why I stayed with glm.  The answers should be
> comparable, but I would prefer the glm answers.  spencer graves

That's why I was suggesting either a variance stabilizing
transformation or using gnls() with a weights function. In both cases
you need to watch out for scaling of SE's stemming from the fact that
the variance is supposed to be known in the Poisson distribution, but
the fitting routines estimate a sigma from the residuals anyway. 

I.e. for instance:

> Phyto.nls2 <- nls(sqrt(y+.5) ~ sqrt(Ymax/(1 +
> x/x50)+.5),data=Phytopath,start=list(Ymax=20.0,x50=0.01),trace=T)
9.400602 :  20.00  0.01
0.9064723 :  27.21511020  0.03593643
0.06338235 :  28.21654101  0.05995647
0.02616589 :  28.50221759  0.06815302
0.02608587 :  28.54871243  0.06835046
0.02608586 :  28.54960242  0.06834083
0.02608586 :  28.5495605  0.0683413
> summary(Phyto.nls2)

Formula: sqrt(y + 0.5) ~ sqrt(Ymax/(1 + x/x50) + 0.5)

Parameters:
     Estimate Std. Error t value Pr(>|t|)
Ymax 28.54956    1.65113  17.291   0.0368 *
x50   0.06834    0.01264   5.407   0.1164
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.1615 on 1 degrees of freedom

Correlation of Parameter Estimates:
       Ymax
x50 -0.6833

but here you need to know that the theoretical sd is 0.5, so the Std.
errors all need multiplication by 0.5/0.1615.

Using gnls() we'd get (if I got the calling sequence right...)

> Phyto.gnls <- gnls(y ~ Ymax/(1 + x/x50),
+  data=Phytopath,weights=varPower(fixed=.5),start=list(Ymax=20.0,x50=0.01))
> summary(Phyto.gnls)
Generalized nonlinear least squares fit
  Model: y ~ Ymax/(1 + x/x50)
  Data: Phytopath
       AIC      BIC    logLik
  13.71292 11.00875 -3.856458

Variance function:
 Structure: Power of variance covariate
 Formula: ~fitted(.)
 Parameter estimates:
power
  0.5

Coefficients:
         Value Std.Error   t-value p-value
Ymax 29.393796 2.4828723 11.838626  0.0536
x50   0.063028 0.0125244  5.032376  0.1249

 Correlation:
    Ymax
x50 -0.84

Standardized residuals:
[1] -0.4894266  0.7621749 -0.4237346
attr(,"std")
[1] 2.8478142 1.4239071 0.8586483
attr(,"label")
[1] "Standardized residuals"

Residual standard error: 0.6367906
Degrees of freedom: 3 total; 1 residual

and again, it is necessary to adjust the SE's by multiplying with
1/0.6368

It is in fact rather easy to do direct MLE for this kind of model:

> with(Phytopath,
> optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
+ -sum(dpois(y,lambda=Ymax/(1
+ + x/x50),log=TRUE))}, method="BFGS", hessian=T))
$par
[1] 28.55963487  0.06833524

$value
[1] 7.21251

$counts
function gradient
      42       13

$convergence
[1] 0

$message
NULL

$hessian
           [,1]        [,2]
[1,] 0.07356072    6.631868
[2,] 6.63186764 1294.792539

Warning message:
NaNs produced in: dpois(x, lambda, log)

and we can get SE's from the inverse hessian with

> sqrt(diag(solve(with(Phytopath,
+ optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
+ -sum(dpois(y,lambda=Ymax/(1
+ + x/x50),log=TRUE))}, method="BFGS", hessian=T))$hessian)))
[1] 5.02565893 0.03788052

Notice that the variance stabilizing transform seems to do rather
better than gnls() compared to true MLE. I'm slightly puzzled by this,
but presumably it has to do with the fact that MLE for a Gaussian
model with a mean/variance relationship is *not* identical to the
iteratively reweighted least squares that glm() et al. are doing. (I
wouldn't quite rule out that I've simply made a mistake somewhere,
though...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Fri Jul 25 21:56:32 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 12:56:32 -0700
Subject: [R] inverse prediction and Poisson regression
References: <568a7056f77a.56f77a568a70@jhsph.edu> <3F21574A.1080704@pdf.com>
	<x2n0f2z8u6.fsf@biostat.ku.dk>
Message-ID: <3F218B70.2010209@pdf.com>

Dear Peter:

	  Thanks very much.  I said I knew there were better ways, but none 
that I could develop in the time available this morning.  You've helped 
introduce me to other options.

Best Wishes,
Spencer Graves

Peter Dalgaard BSA wrote:
> Spencer Graves <spencer.graves at pdf.com> writes:
> 
> 
>>The problem with nls is that it is NOT maximum likelihood for the
>>Poisson distribution.  For the Poisson, the standard deviation is the
>>square root of the mean, while nls assumes constant standard
>>deviation. That's why I stayed with glm.  The answers should be
>>comparable, but I would prefer the glm answers.  spencer graves
> 
> 
> That's why I was suggesting either a variance stabilizing
> transformation or using gnls() with a weights function. In both cases
> you need to watch out for scaling of SE's stemming from the fact that
> the variance is supposed to be known in the Poisson distribution, but
> the fitting routines estimate a sigma from the residuals anyway. 
> 
> I.e. for instance:
> 
> 
>>Phyto.nls2 <- nls(sqrt(y+.5) ~ sqrt(Ymax/(1 +
>>x/x50)+.5),data=Phytopath,start=list(Ymax=20.0,x50=0.01),trace=T)
> 
> 9.400602 :  20.00  0.01
> 0.9064723 :  27.21511020  0.03593643
> 0.06338235 :  28.21654101  0.05995647
> 0.02616589 :  28.50221759  0.06815302
> 0.02608587 :  28.54871243  0.06835046
> 0.02608586 :  28.54960242  0.06834083
> 0.02608586 :  28.5495605  0.0683413
> 
>>summary(Phyto.nls2)
> 
> 
> Formula: sqrt(y + 0.5) ~ sqrt(Ymax/(1 + x/x50) + 0.5)
> 
> Parameters:
>      Estimate Std. Error t value Pr(>|t|)
> Ymax 28.54956    1.65113  17.291   0.0368 *
> x50   0.06834    0.01264   5.407   0.1164
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.1615 on 1 degrees of freedom
> 
> Correlation of Parameter Estimates:
>        Ymax
> x50 -0.6833
> 
> but here you need to know that the theoretical sd is 0.5, so the Std.
> errors all need multiplication by 0.5/0.1615.
> 
> Using gnls() we'd get (if I got the calling sequence right...)
> 
> 
>>Phyto.gnls <- gnls(y ~ Ymax/(1 + x/x50),
> 
> +  data=Phytopath,weights=varPower(fixed=.5),start=list(Ymax=20.0,x50=0.01))
> 
>>summary(Phyto.gnls)
> 
> Generalized nonlinear least squares fit
>   Model: y ~ Ymax/(1 + x/x50)
>   Data: Phytopath
>        AIC      BIC    logLik
>   13.71292 11.00875 -3.856458
> 
> Variance function:
>  Structure: Power of variance covariate
>  Formula: ~fitted(.)
>  Parameter estimates:
> power
>   0.5
> 
> Coefficients:
>          Value Std.Error   t-value p-value
> Ymax 29.393796 2.4828723 11.838626  0.0536
> x50   0.063028 0.0125244  5.032376  0.1249
> 
>  Correlation:
>     Ymax
> x50 -0.84
> 
> Standardized residuals:
> [1] -0.4894266  0.7621749 -0.4237346
> attr(,"std")
> [1] 2.8478142 1.4239071 0.8586483
> attr(,"label")
> [1] "Standardized residuals"
> 
> Residual standard error: 0.6367906
> Degrees of freedom: 3 total; 1 residual
> 
> and again, it is necessary to adjust the SE's by multiplying with
> 1/0.6368
> 
> It is in fact rather easy to do direct MLE for this kind of model:
> 
> 
>>with(Phytopath,
>>optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
> 
> + -sum(dpois(y,lambda=Ymax/(1
> + + x/x50),log=TRUE))}, method="BFGS", hessian=T))
> $par
> [1] 28.55963487  0.06833524
> 
> $value
> [1] 7.21251
> 
> $counts
> function gradient
>       42       13
> 
> $convergence
> [1] 0
> 
> $message
> NULL
> 
> $hessian
>            [,1]        [,2]
> [1,] 0.07356072    6.631868
> [2,] 6.63186764 1294.792539
> 
> Warning message:
> NaNs produced in: dpois(x, lambda, log)
> 
> and we can get SE's from the inverse hessian with
> 
> 
>>sqrt(diag(solve(with(Phytopath,
> 
> + optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
> + -sum(dpois(y,lambda=Ymax/(1
> + + x/x50),log=TRUE))}, method="BFGS", hessian=T))$hessian)))
> [1] 5.02565893 0.03788052
> 
> Notice that the variance stabilizing transform seems to do rather
> better than gnls() compared to true MLE. I'm slightly puzzled by this,
> but presumably it has to do with the fact that MLE for a Gaussian
> model with a mean/variance relationship is *not* identical to the
> iteratively reweighted least squares that glm() et al. are doing. (I
> wouldn't quite rule out that I've simply made a mistake somewhere,
> though...)
>



From rvaradha at jhsph.edu  Fri Jul 25 22:33:12 2003
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 25 Jul 2003 16:33:12 -0400
Subject: [R] inverse prediction and Poisson regression
Message-ID: <5bab765ba826.5ba8265bab76@jhsph.edu>

Thanks Peter, for the wonderful illustration of various model-fitting 
options for the non-linear models. 

Thinking about your comment that the "variance stabilizing" transform 
does better than the weighted non-linear least-squars - I am 
interpreting this to mean that the residual sum of squares is smaller, 
0.16 versus 0.64.  A possible explanation may be that in the former 
case the responses are actually smaller because of the square-rooting 
(range about 3-5), so the residuals are smaller, whereas in the "gnls" 
case the response is the original variable (range about 10-30). Does 
this seem reasonable?

Ravi.

----- Original Message -----
From: Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk>
Date: Friday, July 25, 2003 3:48 pm
Subject: Re: [R] inverse prediction and Poisson regression

> Spencer Graves <spencer.graves at pdf.com> writes:
> 
> > The problem with nls is that it is NOT maximum likelihood for the
> > Poisson distribution.  For the Poisson, the standard deviation 
> is the
> > square root of the mean, while nls assumes constant standard
> > deviation. That's why I stayed with glm.  The answers should be
> > comparable, but I would prefer the glm answers.  spencer graves
> 
> That's why I was suggesting either a variance stabilizing
> transformation or using gnls() with a weights function. In both cases
> you need to watch out for scaling of SE's stemming from the fact that
> the variance is supposed to be known in the Poisson distribution, but
> the fitting routines estimate a sigma from the residuals anyway. 
> 
> I.e. for instance:
> 
> > Phyto.nls2 <- nls(sqrt(y+.5) ~ sqrt(Ymax/(1 +
> > x/x50)+.5),data=Phytopath,start=list(Ymax=20.0,x50=0.01),trace=T)
> 9.400602 :  20.00  0.01
> 0.9064723 :  27.21511020  0.03593643
> 0.06338235 :  28.21654101  0.05995647
> 0.02616589 :  28.50221759  0.06815302
> 0.02608587 :  28.54871243  0.06835046
> 0.02608586 :  28.54960242  0.06834083
> 0.02608586 :  28.5495605  0.0683413
> > summary(Phyto.nls2)
> 
> Formula: sqrt(y + 0.5) ~ sqrt(Ymax/(1 + x/x50) + 0.5)
> 
> Parameters:
>     Estimate Std. Error t value Pr(>|t|)
> Ymax 28.54956    1.65113  17.291   0.0368 *
> x50   0.06834    0.01264   5.407   0.1164
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.1615 on 1 degrees of freedom
> 
> Correlation of Parameter Estimates:
>       Ymax
> x50 -0.6833
> 
> but here you need to know that the theoretical sd is 0.5, so the Std.
> errors all need multiplication by 0.5/0.1615.
> 
> Using gnls() we'd get (if I got the calling sequence right...)
> 
> > Phyto.gnls <- gnls(y ~ Ymax/(1 + x/x50),
> +  
> data=Phytopath,weights=varPower(fixed=.5),start=list
(Ymax=20.0,x50=0.01))> summary(Phyto.gnls)
> Generalized nonlinear least squares fit
>  Model: y ~ Ymax/(1 + x/x50)
>  Data: Phytopath
>       AIC      BIC    logLik
>  13.71292 11.00875 -3.856458
> 
> Variance function:
> Structure: Power of variance covariate
> Formula: ~fitted(.)
> Parameter estimates:
> power
>  0.5
> 
> Coefficients:
>         Value Std.Error   t-value p-value
> Ymax 29.393796 2.4828723 11.838626  0.0536
> x50   0.063028 0.0125244  5.032376  0.1249
> 
> Correlation:
>    Ymax
> x50 -0.84
> 
> Standardized residuals:
> [1] -0.4894266  0.7621749 -0.4237346
> attr(,"std")
> [1] 2.8478142 1.4239071 0.8586483
> attr(,"label")
> [1] "Standardized residuals"
> 
> Residual standard error: 0.6367906
> Degrees of freedom: 3 total; 1 residual
> 
> and again, it is necessary to adjust the SE's by multiplying with
> 1/0.6368
> 
> It is in fact rather easy to do direct MLE for this kind of model:
> 
> > with(Phytopath,
> > optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
> + -sum(dpois(y,lambda=Ymax/(1
> + + x/x50),log=TRUE))}, method="BFGS", hessian=T))
> $par
> [1] 28.55963487  0.06833524
> 
> $value
> [1] 7.21251
> 
> $counts
> function gradient
>      42       13
> 
> $convergence
> [1] 0
> 
> $message
> NULL
> 
> $hessian
>           [,1]        [,2]
> [1,] 0.07356072    6.631868
> [2,] 6.63186764 1294.792539
> 
> Warning message:
> NaNs produced in: dpois(x, lambda, log)
> 
> and we can get SE's from the inverse hessian with
> 
> > sqrt(diag(solve(with(Phytopath,
> + optim(c(28,.7),fn=function(p){Ymax<-p[1];x50<-p[2];
> + -sum(dpois(y,lambda=Ymax/(1
> + + x/x50),log=TRUE))}, method="BFGS", hessian=T))$hessian)))
> [1] 5.02565893 0.03788052
> 
> Notice that the variance stabilizing transform seems to do rather
> better than gnls() compared to true MLE. I'm slightly puzzled by this,
> but presumably it has to do with the fact that MLE for a Gaussian
> model with a mean/variance relationship is *not* identical to the
> iteratively reweighted least squares that glm() et al. are doing. (I
> wouldn't quite rule out that I've simply made a mistake somewhere,
> though...)
> 
> -- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 
> 35327918~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
>



From jfkincaidsu at netscape.net  Fri Jul 25 22:52:44 2003
From: jfkincaidsu at netscape.net (Joel Kincaid)
Date: Fri, 25 Jul 2003 16:52:44 -0400
Subject: [R] Sweave and Printing Lattice Figures From Loop
Message-ID: <3F21989C.7010608@netscape.net>

Community,

(Windows Xp , 1.7.1, too little memory)

This may have the flavor of an answer in search of a question, but I'll 
pose this question: Is their a better way to do the following than what 
I'll show below?

Problem:
In Sweave I have multiple figures from lattice, some of which have more 
that one page. I can easily figure out the total number of figures I 
have created, but its not convient to figure out the number of pages 
that would be produced. I'm lazy and do not want to type documentation 
chunks for every figure i've created. If all were single page figures, 
not a problem, just loop the printing and use some cat() lines to insert 
the stuff I dont want to type. However, if some figures are multipage, 
this solution breaks down.

Solution:
(note this uses 'list.files()' so may be system dependent..)

Pure R first then
.Snw file after the %%%%%%

######R Code with comments
library(lattice)
#  produce two figures, first is one page, second is two pages
#   use a naming standard for latter use in eval(parse(..stuff..))
     Cond1 <- c("One","Two")
Fig1.1  <- xyplot(rnorm(1:100) ~ rnorm(1:100) | Cond1 )
     Cond2 <- c("One","Two","Three","Four")
Fig1.2  <- xyplot(rnorm(1:100) ~ rnorm(1:100) | Cond2 , layout = c(2,1,2))

# loop to construct list of figure names i max * j max = total number of 
figures
z <- 1
figureList <- rep("A",2)
for(i in 1:1){
     for(j in 1:2){
         figureList[z] <- paste("Fig",i,".",j,sep="")
         z <- z + 1}}
     rm(z)
#   get a list of the files that exist
#   before creating postscripts of figures
#   Note: delete files from previous runs...
fileListOld <-  list.files(getwd())
#   wd needs to be where files are saved!

# number of known figures
NumberFigures   <-  2

for( i in 1:NumberFigures) {

# file=paste("myfile8", i, ".ps", sep="")
#   Note the use of onefile=FALSE--- this allows multiple pages
eval( parse(
     file="",
     text=paste(
     "postscript(file = 'FIG%",
     "0",
     10-i,       # Note (A)
     "d.ps'",
     ",onefile=FALSE,paper='special',width=11.5,height=8,horizontal=T)",
     sep=""
        )
    )
)
lset(col.whitebg())
eval(parse(file="",text=paste("print(",figureList[i],")")))
#   force print due to lattice
dev.off()
}
#   Now I need to get unknown page counts and file
#   names to output the LaTeX stuff....
fileListNew <-  list.files(getwd())
created <-  fileListNew[!fileListNew%in%fileListOld]
#   use of matching to get list of files not in
#   orginal list
# Note (A)::
#   the usage 10 - i gives files with shorter and shorter
#   file names as you print figures. Thus the
#   list called 'created' is correctly ordered
#   I hve nine known figures in my 'real' Sweave
#   file --- can on anything
#   probably NumberFigures+1 is good


#   The following is used to discove how many
#   pages were produced from the known number of
#   of figures
NumberPages    <-  length(created)
FileNameLength  <-  unlist(lapply(created,nchar))
#   Use this to take advantage of that fact
#   that file names with the same number of
#   characters are pages to a given
UniqueLength    <-  unique(FileNameLength)
#   unique length = base figure count
PagesPerFigure <-  unlist(
         lapply(
             UniqueLength, function(x) sum(as.numeric(FileNameLength%in%x))
             )
)
#   Now that i've produced the
#   figures and indexes
#   need to put in the LaTeX
#   code and proper file names
#   as well as any thing else
z<-1
for(i in 1:NumberFigures){
     FigureName <-
     for(j in 1:PagesPerFigure[i]){
# this is whats not easy to know before runnning
#   and maybe you want to change condition order...
cat("\\begin{figure}\n")
cat("\\centering\n\n")
cat("\\includegraphics{", created[z], "}\n\n", sep="")
cat("\\caption{ Something Prepended ", figureList[i],"}\n",sep="")
cat("\\end{figure}\n\n")
cat("\\clearpage\n\n") #force floatprocessing in
# LaTeX to  eliminate too many unprocessed floats..
#   see UK FAQ -- 233, 26* and 'FLOATS'
z <- z + 1
     }
}
rm(z)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%   .SNW file
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%I've set it up so you can just cut and paste in to a latex file
%---replace path to your sweave.sty....

\documentclass{article}
\usepackage{times}
\usepackage{c://mytex//styles//Sweave}
\begin{document}
<<echo=FALSE>>=
library(lattice)
     Cond1 <- c("One","Two")
Fig1.1  <- xyplot(rnorm(1:100) ~ rnorm(1:100) | Cond1 )
     Cond2 <- c("One","Two","Three","Four")
Fig1.2  <- xyplot(rnorm(1:100) ~ rnorm(1:100) | Cond2 , layout = c(2,1,2))
z <- 1
figureList <- rep("A",2)
for(i in 1:1){
     for(j in 1:2){
         figureList[z] <- paste("Fig",i,".",j,sep="")
         z <- z + 1}}
     rm(z)
fileListOld <-  list.files(getwd())
NumberFigures   <-  2
for( i in 1:NumberFigures) {
eval( parse(
     file="",
     text=paste(
     "postscript(file = 'FIG%",
     "0",
     10-i,       # Note (A)
     "d.ps'",
     ",onefile=FALSE,paper='special',width=11.5,height=8,horizontal=T)",
     sep=""
        )
    )
)
lset(col.whitebg())
eval(parse(file="",text=paste("print(",figureList[i],")")))
dev.off()
}
fileListNew <-  list.files(getwd())
created <-  fileListNew[!fileListNew%in%fileListOld]

NumberPages    <-  length(created)
FileNameLength  <-  unlist(lapply(created,nchar))
UniqueLength    <-  unique(FileNameLength)
PagesPerFigure <-  unlist(
         lapply(
             UniqueLength, function(x) sum(as.numeric(FileNameLength%in%x))
             )
)
@
\section{The Graphs}
<<echo=FALSE,results=tex>>=
z<-1
for(i in 1:NumberFigures){
     for(j in 1:PagesPerFigure[i]){
cat("\\begin{figure}\n")
cat("\\centering\n\n")
cat("\\includegraphics{", created[z], "}\n\n", sep="")
cat("\\caption{ Something Prepended ", figureList[i],"}\n",sep="")
cat("\\end{figure}\n\n")
cat("\\clearpage\n\n") #force floatprocessing in
z <- z + 1
     }
}
rm(z)
@
\end{document}



From peterm at andrew.cmu.edu  Fri Jul 25 23:38:53 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Fri, 25 Jul 2003 17:38:53 -0400
Subject: [R] Programs stopped working--.print (newbie question)
Message-ID: <BB471BAD.5B5E%peterm@andrew.cmu.edu>

I must have messed up my R environment, but don't know how or how to undo
it.  The problem is this:

I paste the following into R:

test<-function()
{
    print("hello")
}

And I see this:

> test<-function()
+ {
+ .print("hello")
+ }
> test()
Error in test() : couldn't find function ".print"
> 

When I do the same in a fresh environment, I see this:

> test <-function()
+ {
+ print("hello")
+ }

> test2()
[1] "hello"

Peter



From kjetil at entelnet.bo  Fri Jul 25 23:47:08 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 25 Jul 2003 17:47:08 -0400
Subject: [R] glmmPQL using REML instead of ML
In-Reply-To: <20030725152426.6244.qmail@web14207.mail.yahoo.com>
Message-ID: <3F216D1C.25099.84D9F2@localhost>

On 25 Jul 2003 at 16:24, Emma Tan wrote:

> Hi,
> 
> In glmmPQL in the MASS library, the function uses
> repeated calls to the function lme(), using ML.  Does
> anyone know how you can change this to REML?  I know
> that in lme(), the default is actually set to REML and
> you can also specify this as 'method=REML' or
> 'method'ML' but this isn't applicable to glmmPQL().  

R (and MASS) is free software, you have the source. glmmPQL
contains the line

mcall$method <- "ML"

change that as appropiate. Since glmmPQL is in a namespace,
it might be neccesary to rename your changed function, or maybe use
fixInNamespace(). I don't know enough about namespaces, others will 
know more about this last part. Renaming is maybe safest. 

Why do you want to use REML with glmmPQL?

Kjetil Halvorsen

> 
> I'd appreciate any help or advice!
> Thanks,
> 
> Emma
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Fri Jul 25 23:53:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jul 2003 14:53:57 -0700
Subject: [R] Programs stopped working--.print (newbie question)
References: <BB471BAD.5B5E%peterm@andrew.cmu.edu>
Message-ID: <3F21A6F5.1050307@pdf.com>

I see a period "." before 'print' in your function definition.  Might 
this be the problem?

spencer graves

Peter Muhlberger wrote:
> I must have messed up my R environment, but don't know how or how to undo
> it.  The problem is this:
> 
> I paste the following into R:
> 
> test<-function()
> {
>     print("hello")
> }
> 
> And I see this:
> 
> 
>>test<-function()
> 
> + {
> + .print("hello")
> + }
> 
>>test()
> 
> Error in test() : couldn't find function ".print"
> 
> 
> When I do the same in a fresh environment, I see this:
> 
> 
>>test <-function()
> 
> + {
> + print("hello")
> + }
> 
> 
>>test2()
> 
> [1] "hello"
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Fri Jul 25 23:47:08 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 25 Jul 2003 17:47:08 -0400
Subject: [R] glmmPQL using REML instead of ML
In-Reply-To: <20030725152426.6244.qmail@web14207.mail.yahoo.com>
Message-ID: <3F216D1C.25099.84D9F2@localhost>

On 25 Jul 2003 at 16:24, Emma Tan wrote:

> Hi,
> 
> In glmmPQL in the MASS library, the function uses
> repeated calls to the function lme(), using ML.  Does
> anyone know how you can change this to REML?  I know
> that in lme(), the default is actually set to REML and
> you can also specify this as 'method=REML' or
> 'method'ML' but this isn't applicable to glmmPQL().  

R (and MASS) is free software, you have the source. glmmPQL
contains the line

mcall$method <- "ML"

change that as appropiate. Since glmmPQL is in a namespace,
it might be neccesary to rename your changed function, or maybe use
fixInNamespace(). I don't know enough about namespaces, others will 
know more about this last part. Renaming is maybe safest. 

Why do you want to use REML with glmmPQL?

Kjetil Halvorsen

> 
> I'd appreciate any help or advice!
> Thanks,
> 
> Emma
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From peterm at andrew.cmu.edu  Sat Jul 26 00:28:51 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Fri, 25 Jul 2003 18:28:51 -0400
Subject: [R] Programs stopped working--.print (newbie question)
In-Reply-To: <3F21A6F5.1050307@pdf.com>
Message-ID: <BB472763.5B62%peterm@andrew.cmu.edu>

On 7/25/03 5:53 PM, "Spencer Graves" <spencer.graves at PDF.COM> wrote:

> I see a period "." before 'print' in your function definition.  Might
> this be the problem?
> 
> spencer graves

The code I paste in has no "." in front of 'print' .  But when the code
displays after I put it in, R puts a "." in front of it.  I don't know why.

Thx,

Peter



From MSchwartz at medanalytics.com  Sat Jul 26 01:10:45 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 25 Jul 2003 23:10:45 -0000
Subject: [R] Programs stopped working--.print (newbie question)
In-Reply-To: <BB472763.5B62%peterm@andrew.cmu.edu>
References: <BB472763.5B62%peterm@andrew.cmu.edu>
Message-ID: <1059174652.5080.122.camel@localhost>

On Fri, 2003-07-25 at 17:28, Peter Muhlberger wrote:
> On 7/25/03 5:53 PM, "Spencer Graves" <spencer.graves at PDF.COM> wrote:
> 
> > I see a period "." before 'print' in your function definition.  Might
> > this be the problem?
> > 
> > spencer graves
> 
> The code I paste in has no "." in front of 'print' .  But when the code
> displays after I put it in, R puts a "." in front of it.  I don't know why.
> 
> Thx,
> 
> Peter


Peter,

This may be a long shot, but I noted that there are four spaces before
the word 'print' in your initial code. Those four spaces are replaced by
the period after pasting. Four spaces would be a default TAB indentation
setting for many editors.

You don't indicate the particular OS environment you are using (or R
version), but I am presuming a *nix-alike, since I do not see typical
Windows based e-mail headers on your post. 

The period ('.') that appears could be an indication of a TAB character
being pasted into the console, which could cause the problems you are
seeing.

When I paste from an editor (using TAB characters to indent) into the R
console under RH 9, I get:

> test<-function()
+ {
+ print("hello")
+ }

Note that the indentation of 'print("hello")' goes away.

You indicated in your initial post that you can paste into a "fresh
environment" without problem.  What is the fresh environment?  A new R
console? A new machine?

It is possible that something in your existing open terminal/console
application within which R is running, has been altered (ie. change in
character encoding) which in turn is causing the TAB character to be
interpreted/replaced by the period.

As I said, a long shot...but possible.

HTH,

Marc Schwartz



From thomasio at cs.tu-berlin.de  Sat Jul 26 11:47:33 2003
From: thomasio at cs.tu-berlin.de (Thomas Koenig)
Date: Sat, 26 Jul 2003 10:47:33 +0100
Subject: [R] How to make "<-" generic?
Message-ID: <200307261047.33153.thomasio@cs.tu-berlin.de>

Hi,

perhaps a little bit unusual: is it possible to use "<- " as generic function 
with a new signature?
The following example doesn't work:
> isGeneric("<-")
[1] FALSE
> setClass("A",representation(x = "numeric"))
[1] "A"
> setClass("B",representation(x = "numeric"))
[1] "B"
> myAssign.A <- function(x,value)
+  { return(x); }
> setReplaceMethod("",c("A","B"),myAssign.A)
[1] "<-"
> ## because
> ##> setReplaceMethod
> ##function (f, ...)
> ##setMethod(paste(f, "<-", sep = ""), ...)
> ## and
> isGeneric("<-")
[1] TRUE
> a <- new("A")
> b <- new("B")
> a <- b
> a
An object of class "B"
Slot "x":
numeric(0)
> ## should be a ?
> ## but selectMethod(...) works correct
> selectMethod("<-",c("A","B"))
Method Definition (Class "MethodDefinition"):

function(x,value)
{
print("myAssign.default");
return(x);
}

Signatures:
        x   value
target  "A" "B"
defined "A" "B"

What happens with the setReplaceMethod(...) call?

My version is
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R

Thank you.

Thomas K?nig



From ripley at stats.ox.ac.uk  Sat Jul 26 11:03:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Jul 2003 10:03:07 +0100 (BST)
Subject: [R] How to make "<-" generic?
In-Reply-To: <200307261047.33153.thomasio@cs.tu-berlin.de>
Message-ID: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>

What are you trying to do with this?  Assignment (<-) is not a function,
and the language grammar does not convert a <- b into "<-"(a, b) (as it
would with the binary operator functions).  You could call it that way,
and then it will probably work.

On Sat, 26 Jul 2003, Thomas Koenig wrote:

> Hi,
> 
> perhaps a little bit unusual: is it possible to use "<- " as generic function 
> with a new signature?
> The following example doesn't work:
> > isGeneric("<-")
> [1] FALSE
> > setClass("A",representation(x = "numeric"))
> [1] "A"
> > setClass("B",representation(x = "numeric"))
> [1] "B"
> > myAssign.A <- function(x,value)
> +  { return(x); }
> > setReplaceMethod("",c("A","B"),myAssign.A)
> [1] "<-"
> > ## because
> > ##> setReplaceMethod
> > ##function (f, ...)
> > ##setMethod(paste(f, "<-", sep = ""), ...)
> > ## and
> > isGeneric("<-")
> [1] TRUE
> > a <- new("A")
> > b <- new("B")
> > a <- b
> > a
> An object of class "B"
> Slot "x":
> numeric(0)
> > ## should be a ?
> > ## but selectMethod(...) works correct
> > selectMethod("<-",c("A","B"))
> Method Definition (Class "MethodDefinition"):
> 
> function(x,value)
> {
> print("myAssign.default");
> return(x);
> }
> 
> Signatures:
>         x   value
> target  "A" "B"
> defined "A" "B"
> 
> What happens with the setReplaceMethod(...) call?
> 
> My version is
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
> 
> Thank you.
> 
> Thomas K?nig
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.pagel at gsf.de  Sat Jul 26 11:04:35 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Sat, 26 Jul 2003 11:04:35 +0200
Subject: [R] bug plotting dates?
Message-ID: <20030726090435.GA1927@porcupine.gsf.de>


	Hello R-experts!

I am using R Version 1.7.1  (2003-06-16) on a Debian Linux box and I
have discovered an odd result when plotting data involving dates. Please
try this minimal example:

a = seq(ISOdate(2000,1,1), ISOdate(2001,1,1), "months")
b = 1:13
plot(a,b, col="red")

What I get is a plot that looks as expected except the x-axis is mostly
red. Can anyone reproduce this behaviour? I have tried the same thing on
R 1.6.0 on a machine in the institute and everything looked fine there
(i.e. axis is black). 

Also, if I set the ylim in the plot command:

plot(a,b, col="red", ylim=c(0,20))

R gives me a warning 'parameter "ylim" couldn't be set in high-level
plot() function' but sets the limit as requested, anyway.

Any ideas what I'm doing wrong? Is this a bug?

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From ripley at stats.ox.ac.uk  Sat Jul 26 11:10:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Jul 2003 10:10:49 +0100 (BST)
Subject: [R] bug plotting dates?
In-Reply-To: <20030726090435.GA1927@porcupine.gsf.de>
Message-ID: <Pine.LNX.4.44.0307261007540.24675-100000@gannet.stats>

It's an infelicity already fixed in R-patched (the first) and R-devel 
(both).

On Sat, 26 Jul 2003, Philipp Pagel wrote:

> 
> 	Hello R-experts!
> 
> I am using R Version 1.7.1  (2003-06-16) on a Debian Linux box and I
> have discovered an odd result when plotting data involving dates. Please
> try this minimal example:
> 
> a = seq(ISOdate(2000,1,1), ISOdate(2001,1,1), "months")
> b = 1:13
> plot(a,b, col="red")
> 
> What I get is a plot that looks as expected except the x-axis is mostly
> red. Can anyone reproduce this behaviour? I have tried the same thing on
> R 1.6.0 on a machine in the institute and everything looked fine there
> (i.e. axis is black). 
> 
> Also, if I set the ylim in the plot command:
> 
> plot(a,b, col="red", ylim=c(0,20))
> 
> R gives me a warning 'parameter "ylim" couldn't be set in high-level
> plot() function' but sets the limit as requested, anyway.

> Any ideas what I'm doing wrong? Is this a bug?
> 
> cu
> 	Philipp
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Sat Jul 26 11:55:16 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Sat, 26 Jul 2003 10:55:16 +0100
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00971@cl-exsrv1.irad.bbsrc.ac.uk>

:-))

Installing Bioconductor was how it all began, so I ended up doing what you suggested (in fact I downloaded just the packages I needed as in the full tar ball, rhdf5 wouldn't compile, probably as I don't have the right hdf5 installed but thats not a problem).

BUT it just bugged me that I couldn't get R to work, and unfortunately in my frustration I appear to have upset a few people - for this I deeply apologise.  IT still remains the case that, even with http_proxy set (and HTTP_PROXY set) i get the following results:

> update.packages()
doesn't work :-(

> update.packages(method = "wget")
works! :-D

> options(download.file.method = "wget")
> update.packages()
works! :-D

> options(download.file.method = "wget")
> source("http://www.bioconductor.org/getBioC.R")
doesn't work :-(

> download.file("http://www.bioconductor.org/getBioC.R", destfile = getBioC.R, method = "wget")
> source("getBioC.R")
> getBioC(method = "wget")
the first two commands work fine, the last doesn't and bombs out with the usual "can't connect to the internet"

OK I now have Bioconductor installed by the traditional method of downloading via Netscape and installing .tar.gz's, so I am happy, and for the record I think R is a miraculous piece of software and a testament to the wonders of free, open-source development, as is Bioconductor, both of which make my job, and life, a WHOLE lot easier.  So I thank everyone on this list who has contributed to either or both :-D  But I can't be the only one who wouldn't be curious if the above sequence of events occurred on their system.... ;-)

Have a good weekend one and all :-D

M


-----Original Message-----
From: James MacDonald
To: michael.watson at bbsrc.ac.uk; hb at maths.lth.se
Cc: R-help at stat.math.ethz.ch
Sent: 25/07/03 16:58
Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1

HTTP_PROXY issues aside, if all you want to do is install Bioconductor,
simply download the latest bioconductor_xx.tar.gz and use R CMD
INSTALL.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> 07/25/03
07:18AM >>>
Hi Henrik

Thanks for your help, I really do appreciate it.

If I follow your instructions, R returns the value
http://wwwcache.bbsrc.ac.uk:8080.  That is good and it means that
indeed my http_proxy environment variable is set.

I have also added the lines

http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
HTTP_PROXY=http://wwwcache.bbsrc.ac.uk:8080/

both to .Renviron in my home directory, and to /usr/lib/R/etc/Renviron
and /usr/lib/R/etc/Renviron.site.

All to no avail... R still doesn't try to connect through my proxy
server.  

Please, I genuinely think this is a bug in R 1.7.1 on Suse Linux 8.1.

NOW, here is a little detail I have just discovered that PROVES my
proxy is working.  

If I do:

update.packages(method="wget")

then everything works fine.... hmmmm, but I still have a problem as the
command I really want to run is :

source("http://wwwbioconductor.org/getBioC.R")

and source() does not accept an option 'method="wget"'....

SO... is there a way in R that I can set it up such that ALL internet
connections from within R use method="wget" ??

Thanks
Mick

-----Original Message-----
From: Henrik Bengtsson [mailto:hb at maths.lth.se] 
Sent: 25 July 2003 11:25
To: 'michael watson (IAH-C)'
Cc: R-help at stat.math.ethz.ch 
Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1


Could it be that you have redefined the command R in your shell such
that the http_proxy environment variable is set in one and R is
running
in another? (This is just a wild guess and I am myself only running
WinXP.) What do you get if you do

% env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
% R
> Sys.getenv("http_proxy")

Also, have you considered setting http_proxy in ~/.Renviron (see
?.Renviron).

Cheers

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: den 25 juli 2003 10:24
> To: 'Prof Brian Ripley'
> Cc: 'R-help at stat.math.ethz.ch' 
> Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Hi
> 
> Thanks once again for your help, I do appreciate it..... however....
> 
> Here is what I get with your test.... (under tcsh - i 
> normally use bash, but I will keep everything the same)
> 
> users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> 
> >options(internet.info=0)
> >update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> unable to connect to 'cran.r-project.org' on port 80
> Error in download.file(url = paste(contriburl, "PACKAGES", 
> sep = "/"), :
> 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'

> 
> 
> ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> message at all, which appears to suggest that R, under SUSE 
> Linux 8.1, is NOT PICKING up the http_proxy environment 
> variable - this isn't something thats wrong with my proxy, 
> that works with everything else - internet browsers, ftp 
> clients, wget, instant messenger clients etc etc.  The 
> problem is R, which isn't picking up that it needs to use the 
> http_proxy environment variable.  And I apologise for being 
> blunt, but that is an R problem, not a proxy problem!
> 
> Thanks for your help
> 
> Mick
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: 24 July 2003 16:56
> To: michael watson (IAH-C)
> Subject: RE: [R] Your proxy seems not to work with R (was R 
> won't connect to the internet on Linux!)
> 
> 
> When I do (under tcsh)
> 
> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > options(internet.info=0)
> > update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080 
> 
> it tries to connect to your proxy (as it says) and gets no 
> response, which is not surprising from my site.  If you get 
> the same, your proxy is probably not behaving in the standard 
> way (since that has been tested by many users with standard
proxies).
> 
> I've changed the emphasis of the subject line to one I feel is more 
> equitable: many, many users have counter-evidence to your original 
> assertion, which was rather arrogant.
> 
> On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> 
> > Hello Professor
> > 
> > If you are suggesting that I am simply missing the 
> "http://" part of 
> > my cache URL, or that I am missing a trailing "/", then I 
> pre-empted 
> > this response and it still doesn't work.
> 
> I was suggesting that `simply' you were not reading the documentation

> correctly. 
> 
> > I have tried setting both http_proxy and HTTP_PROXY to all of:
> 
> I hope you set to *each* of these.  The first and third are 
> documented to be incorrect, so using those was perverse.
> 
> > wwwcache.bbsrc.ac.uk:8080
> > http://wwwcache.bbsrc.ac.uk:8080 
> > wwwcache.bbsrc.ac.uk:8080/
> > http://wwwcache.bbsrc.ac.uk:8080/ 
> > 
> > and I still get the same response - R cannot open the URL.
> > 
> > And yes, that is thw right proxy address, I copied it straight from

> > Netscape on the same computer, and Netscape connects to the 
> internet 
> > fine.
> > 
> > Thanks
> > Mick
> >  
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: 24 July 2003 13:17
> > To: michael watson (IAH-C)
> > Cc: 'R-help at stat.math.ethz.ch '
> > Subject: Re: [R] R won't connect to the internet on Linux!
> > 
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > OK, I really am struggling with this one!  Forgive me if 
> I am being 
> > > stupid....
> > 
> > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> internet 
> > > through a proxy so I have:
> > > 
> > > IAHC-LINUX03:~ # echo $http_proxy
> > > wwwcache.bbsrc.ac.uk:8080
> > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > wwwcache.bbsrc.ac.uk:8080
> > > 
> > > just in case ;-)
> > > 
> > > SO, i go into R and I get:
> > > 
> > > > source("http://www.bioconductor.org/getBioC.R")
> > > unable to connect to 'www.bioconductor.org' on port 80. Error in

> > > file(file, "r") : cannot open URL 
> > > `http://www.bioconductor.org/getBioC.R' 
> > > 
> > > OK so is R just not picking up my proxy setting?
> > 
> > Your setting is wrong, so it is being ignored.  The help page says

> > quite explicitly
> > 
> >       The form of `"http_proxy"' should be 
> `"http://proxy.dom.com/"' or
> >      `"http://proxy.dom.com:8080/"' where the port defaults 
> to `80' and
> >      the trailing slash may be omitted.
> > 
> > > It seems to be trying
> > > port 80 on something, and I have specifically set it to 
> port 8080 in 
> > > my environment variables.  As far as I can see I have 
> followed the 
> > > reference manual suggestion, so does anyone else have one?
> > 
> > The problem is in your seeing, it seems.
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk 
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/

> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Sat Jul 26 12:02:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 26 Jul 2003 10:02:37 -0000
Subject: [R] How to make "<-" generic?
In-Reply-To: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
Message-ID: <x2isppzjn5.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> What are you trying to do with this?  Assignment (<-) is not a function,
> and the language grammar does not convert a <- b into "<-"(a, b) (as it
> would with the binary operator functions).  You could call it that way,
> and then it will probably work.

Eh? Are you sure about that???

> quote("<-"(a,b))
a <- b
> e <- quote(a<-b)
> e[[1]]
<-
> e[[2]]
a
> e[[3]]
b
> "<-"<-get("+") # Don't try this at home, kids...
> 2<-2
[1] 4
> rm("<-")
> 2<-2
Error in 2 <- 2 : invalid (do_set) left-hand side to assignment


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Sat Jul 26 12:33:24 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 26 Jul 2003 10:33:24 -0000
Subject: [R] How to make "<-" generic?
In-Reply-To: <x2isppzjn5.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
	<x2isppzjn5.fsf@biostat.ku.dk>
Message-ID: <x2el0dzi7d.fsf@biostat.ku.dk>

Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > What are you trying to do with this?  Assignment (<-) is not a function,
> > and the language grammar does not convert a <- b into "<-"(a, b) (as it
> > would with the binary operator functions).  You could call it that way,
> > and then it will probably work.
> 
> Eh? Are you sure about that???
> 
> > quote("<-"(a,b))
> a <- b

Adding on to this, I think the point is that assignment bypasses the
usual *evaluation* rules, even though it is syntactically a binop.

I think it basically has to be so: For one thing, it is kind of
difficult to check for a signature match without evaluating the
arguments and the left hand side of an assignment will not in general
exist at that point.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From phgrosjean at sciviews.org  Sat Jul 26 12:54:13 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 26 Jul 2003 06:54:13 -0400
Subject: [R] R benchmark, moble Pentium III, 1.13 GHs
Message-ID: <200307260654.AA1080754778@sciviews.org>

Hi Jason,

I suppose you installed the Matrix library, and it is working on your computer? If yes, may be det.Matrix() was removed, or renamed in the Matrix library you have (I cannot check this for the latest version, because I am away of the office until August 1st), but I will do that next week.

In the meantime, you can replace 'det.Matrix' by 'det.default', and it should run. Do not compare the timing for this particular test, with other results if you mqke this change. Anyway, as pointed out by Prof. Brian Ripley some times ago (and I think, also, in the online help), the determinant of a matrix is seldom used in the various algorithms in R, and it is thus a poor indicator of the potentials of the software for data analysis. As always, take the results of this benchmark with care: only few functions tested, not a "real work" comparison, etc... The best being probably to supplement this benchmark with some of your favorite scripts that you run on the various machines you want to compare. This way, you get a better idea for your pqrticulqr work.

Best,

Philippe Grosjean

---------- Original Message ----------------------------------
From: Jason Liao <jg_liao at yahoo.com>
Date:  Thu, 24 Jul 2003 07:50:41 -0700 (PDT)

>I got the following using the benchmark program at
>http://www.sciviews.org/other/benchmark.htm
>
>under Windows 2000 prof., 256 MB of RAM
>
> R Benchmark 2
>   =============
>Number of times each test is run__________________________:  3
>
>   I. Matrix calculation
>   ---------------------
>Creation, transp., deformation of a 1500x1500 matrix (sec): 
>1.97333333333333 
>800x800 normal distributed random matrix ^1000______ (sec): 
>2.80999999999999 
>Sorting of 2,000,000 random values__________________ (sec): 
>1.20333333333333 
>700x700 cross-product matrix (b = a' * a)___________ (sec): 
>1.84000000000000 
>Linear regression over a 600x600 matrix (c = a \ b') (sec): 
>2.43666666666666 
>                      --------------------------------------------
>                 Trimmed geom. mean (2 extremes eliminated): 
>2.06825841067831 
>
>   II. Matrix functions
>   --------------------
>FFT over 800,000 random values______________________ (sec): 
>1.61666666666666 
>Eigenvalues of a 320x320 random matrix______________ (sec): 
>1.20333333333333 
>Error in eval(expr, envir, enclos) : couldn't find function
>"det.Matrix"
>Timing stopped at: 0 0 0 NA NA 
>> 
>
>
>
>=====
>Jason G. Liao, Ph.D.
>Division of Biometrics
>University of Medicine and Dentistry of New Jersey
>335 George Street, Suite 2200
>New Brunswick, NJ 08903-2688
>phone (732) 235-8611, fax (732) 235-9777
>http://www.geocities.com/jg_liao
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

--
..............................................................
 ) ) ) ) )      
( ( ( ( (       Philippe Grosjean 
( ( ( ( ( 	Marine Biol. Lab., ULB, Belgium 
 ) ) ) ) )	Mariculture & Biostatistic  
( ( ( ( (               	           		     
 ) ) ) ) )	SciViews project coordinator
( ( ( ( ( 	(http://www.sciviews.org)	    
 ) ) ) ) )	email:phgrosje at ulb.ac.be 
( ( ( ( (                     	        
..............................................................


--



From thomasio at cs.tu-berlin.de  Sat Jul 26 15:24:48 2003
From: thomasio at cs.tu-berlin.de (Thomas Koenig)
Date: Sat, 26 Jul 2003 14:24:48 +0100
Subject: [R] How to make "<-" generic?
In-Reply-To: <x2el0dzi7d.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
	<x2isppzjn5.fsf@biostat.ku.dk> <x2el0dzi7d.fsf@biostat.ku.dk>
Message-ID: <200307261424.48444.thomasio@cs.tu-berlin.de>

Am Samstag, 26. Juli 2003 11:38 schrieb Peter Dalgaard BSA:
> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:
> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> > > What are you trying to do with this?  Assignment (<-) is not a
> > > function,

But what the difference between <- and e.g. the function length or "[<-"? As I 
understood in "methods" everything has a class. And R says me with is(...) 
(hope that the results are correct):
< is(get("<-"))
[1] "function"         "OptionalFunction" "PossibleMethod"
< is(get("length"))
[1] "function"         "OptionalFunction" "PossibleMethod"
> is(get("[<-"))
[1] "function"         "OptionalFunction" "PossibleMethod"
> ## test for the correct result of get(...) ?
> x <- 10
> is(get("x"))
[1] "numeric" "vector"
> ## and
> setGeneric("<-")
[1] "<-"
Warning message:
"<-" is a primitive function; its generic definition is built in and 
automatically included. in: setGeneric("<-")

> > > and the language grammar does not convert a <- b into "<-"(a,
> > > b) (as it would with the binary operator functions).  You could call it
> > > that way, and then it will probably work.
> >
> > Eh? Are you sure about that???
> >
> > > quote("<-"(a,b))
> >
> > a <- b
>
> Adding on to this, I think the point is that assignment bypasses the
> usual *evaluation* rules, even though it is syntactically a binop.
>
> I think it basically has to be so: For one thing, it is kind of
> difficult to check for a signature match without evaluating the
> arguments and the left hand side of an assignment will not in general
> exist at that point.

In the methods is the class "missing". Could that help?
for one thing : signature=c("missing","A") (only allowed for <- ?)
for both things: signature=c("A","B")
for nothing ;-) : signature=c("missing","missing")

Thanks and Best Regards

Thomas K?nig



From Ted.Harding at nessie.mcc.ac.uk  Sat Jul 26 13:52:17 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 26 Jul 2003 12:52:17 +0100 (BST)
Subject: [R] How to make "<-" generic?
In-Reply-To: <x2el0dzi7d.fsf@biostat.ku.dk>
Message-ID: <XFMail.030726125217.Ted.Harding@nessie.mcc.ac.uk>

On 26-Jul-03 Peter Dalgaard BSA wrote:
> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:
>> > quote("<-"(a,b))
>> a <- b
> 
> Adding on to this, I think the point is that assignment bypasses the
> usual *evaluation* rules, even though it is syntactically a binop.
> 
> I think it basically has to be so: For one thing, it is kind of
> difficult to check for a signature match without evaluating the
> arguments and the left hand side of an assignment will not in general
> exist at that point.

Need this really be so? (Maybe it has to be in R unless R is unpicked)

For example: suppose the "deep syntax" of <- is "<-"(A,B)

Then:

  If A is not there then create it equal to B (surely no problem here).
  Otherwise: assign the value of B to A provided this is legal
               given what A and B happen to be;
             otherwise announce an error.

E.g. if A is a matrix then "<-"(A,2) is OK (bye bye old A),
but if A is a matrix then "<-"(A[2,3],list(one=1,two=2)) is not.

Likewise "<-"(2*A,B) would be illegal. Testing for this would be the
analogue of "lvalues" in C: lvalue is not defined in the formal grammar
of C but is tested for "semantically". Basically an lvalue is an
expression which refers to an "object" (region of memory) in such a
way that it may be altered as well as examined.

But isn't this much like what happens already? E.g.

> "<-"(b,2)
> b
[1] 2
> 
> "<-"(2*b,2)
Error: Target of assignment expands to non-language object
> 

(Actually you could get away with this in C, because of "pointer
arithmetic" whereby the location of the object pointed to by 2*b has
memory address twice the value of the memory address pointed to by b.
And the best of luck ... ).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Jul-03                                       Time: 12:52:17
------------------------------ XFMail ------------------------------



From dmurdoch at pair.com  Sat Jul 26 14:54:29 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 26 Jul 2003 08:54:29 -0400
Subject: [R] How to make "<-" generic?
In-Reply-To: <200307261424.48444.thomasio@cs.tu-berlin.de>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
	<x2isppzjn5.fsf@biostat.ku.dk> <x2el0dzi7d.fsf@biostat.ku.dk>
	<200307261424.48444.thomasio@cs.tu-berlin.de>
Message-ID: <m5u4ivsh54l0239lmrild7hpn6jsv1rbhb@4ax.com>

On Sat, 26 Jul 2003 14:24:48 +0100, you wrote:

>Am Samstag, 26. Juli 2003 11:38 schrieb Peter Dalgaard BSA:

>> I think it basically has to be so: For one thing, it is kind of
>> difficult to check for a signature match without evaluating the
>> arguments and the left hand side of an assignment will not in general
>> exist at that point.
>
>In the methods is the class "missing". Could that help?
>for one thing : signature=c("missing","A") (only allowed for <- ?)
>for both things: signature=c("A","B")
>for nothing ;-) : signature=c("missing","missing")

But A isn't missing in "A <- B"; I can see it right there!

It might make sense to require that "<-" methods have "ANY" as the
type of the first argument.  It's part of the semantics of the
language that the result of "A <- B" does not depend on the value of A
in any way, and that shouldn't be changed.

Duncan Murdoch



From pburns at pburns.seanet.com  Sat Jul 26 15:16:10 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 26 Jul 2003 14:16:10 +0100
Subject: [R] How to make "<-" generic?
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>	<x2isppzjn5.fsf@biostat.ku.dk>
	<x2el0dzi7d.fsf@biostat.ku.dk>
	<200307261424.48444.thomasio@cs.tu-berlin.de>
Message-ID: <3F227F1A.8090402@pburns.seanet.com>

I think Brian's question --- what are you trying to do? -- should
be the first order of business.

Someone can make R do just about anything, but it is better to
do a few things very well than lots of things in a muddled way.

So.  What is the advantage of using assignment as a generic?
I'm open-minded about there being such an advantage, but I
don't see any right off.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Thomas Koenig wrote:

>Am Samstag, 26. Juli 2003 11:38 schrieb Peter Dalgaard BSA:
>  
>
>>Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:
>>    
>>
>>>Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>>>      
>>>
>>>>What are you trying to do with this?  Assignment (<-) is not a
>>>>function,
>>>>        
>>>>
>
>But what the difference between <- and e.g. the function length or "[<-"? As I 
>understood in "methods" everything has a class. And R says me with is(...) 
>(hope that the results are correct):
>< is(get("<-"))
>[1] "function"         "OptionalFunction" "PossibleMethod"
>< is(get("length"))
>[1] "function"         "OptionalFunction" "PossibleMethod"
>  
>
>>is(get("[<-"))
>>    
>>
>[1] "function"         "OptionalFunction" "PossibleMethod"
>  
>
>>## test for the correct result of get(...) ?
>>x <- 10
>>is(get("x"))
>>    
>>
>[1] "numeric" "vector"
>  
>
>>## and
>>setGeneric("<-")
>>    
>>
>[1] "<-"
>Warning message:
>"<-" is a primitive function; its generic definition is built in and 
>automatically included. in: setGeneric("<-")
>
>  
>
>>>>and the language grammar does not convert a <- b into "<-"(a,
>>>>b) (as it would with the binary operator functions).  You could call it
>>>>that way, and then it will probably work.
>>>>        
>>>>
>>>Eh? Are you sure about that???
>>>
>>>      
>>>
>>>>quote("<-"(a,b))
>>>>        
>>>>
>>>a <- b
>>>      
>>>
>>Adding on to this, I think the point is that assignment bypasses the
>>usual *evaluation* rules, even though it is syntactically a binop.
>>
>>I think it basically has to be so: For one thing, it is kind of
>>difficult to check for a signature match without evaluating the
>>arguments and the left hand side of an assignment will not in general
>>exist at that point.
>>    
>>
>
>In the methods is the class "missing". Could that help?
>for one thing : signature=c("missing","A") (only allowed for <- ?)
>for both things: signature=c("A","B")
>for nothing ;-) : signature=c("missing","missing")
>
>Thanks and Best Regards
>
>Thomas K?nig
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From luke at stat.uiowa.edu  Sat Jul 26 17:39:34 2003
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sat, 26 Jul 2003 10:39:34 -0500 (CDT)
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00971@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0307261025360.3238-100000@itasca2.stat.uiowa.edu>

On the SuSE setup I have access to it seems that for whatever reason
the environment variable no_proxy is defined as 'localhost' in the
shell initialization files /etc/SuSEconfig/csh.cshrc and
/etc/SuSEconfig/profile.  This turns use of proxy's off in the R
internals.  See if you have it defined, and if so see if something
like

	(unset no_proxy; R)

in bash or

	(unsetenv no_proxy; R)

in tcsh handles proxies properly; it seems to for me.  When no_proxy
is not defined I get

	> Sys.putenv("http_proxy"="http://foo.bar.com")
	> download.file("http://blah.blah.net/foo.R", "bar.R")
	trying URL `http://blah.blah.net/foo.R'
	unable to resolve 'foo.bar.com'.
	Error in download.file("http://blah.blah.net/foo.R", "bar.R") : 
		cannot open URL `http://blah.blah.net/foo.R'

when proxy's are disabled by no_proxy being defined the "unable to
resolve..."  line is missing.

Best,

luke

On Sat, 26 Jul 2003, michael watson (IAH-C) wrote:

> :-))
> 
> Installing Bioconductor was how it all began, so I ended up doing what you suggested (in fact I downloaded just the packages I needed as in the full tar ball, rhdf5 wouldn't compile, probably as I don't have the right hdf5 installed but thats not a problem).
> 
> BUT it just bugged me that I couldn't get R to work, and unfortunately in my frustration I appear to have upset a few people - for this I deeply apologise.  IT still remains the case that, even with http_proxy set (and HTTP_PROXY set) i get the following results:
> 
> > update.packages()
> doesn't work :-(
> 
> > update.packages(method = "wget")
> works! :-D
> 
> > options(download.file.method = "wget")
> > update.packages()
> works! :-D
> 
> > options(download.file.method = "wget")
> > source("http://www.bioconductor.org/getBioC.R")
> doesn't work :-(
> 
> > download.file("http://www.bioconductor.org/getBioC.R", destfile = getBioC.R, method = "wget")
> > source("getBioC.R")
> > getBioC(method = "wget")
> the first two commands work fine, the last doesn't and bombs out with the usual "can't connect to the internet"
> 
> OK I now have Bioconductor installed by the traditional method of downloading via Netscape and installing .tar.gz's, so I am happy, and for the record I think R is a miraculous piece of software and a testament to the wonders of free, open-source development, as is Bioconductor, both of which make my job, and life, a WHOLE lot easier.  So I thank everyone on this list who has contributed to either or both :-D  But I can't be the only one who wouldn't be curious if the above sequence of events occurred on their system.... ;-)
> 
> Have a good weekend one and all :-D
> 
> M
> 
> 
> -----Original Message-----
> From: James MacDonald
> To: michael.watson at bbsrc.ac.uk; hb at maths.lth.se
> Cc: R-help at stat.math.ethz.ch
> Sent: 25/07/03 16:58
> Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> HTTP_PROXY issues aside, if all you want to do is install Bioconductor,
> simply download the latest bioconductor_xx.tar.gz and use R CMD
> INSTALL.
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> >>> "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> 07/25/03
> 07:18AM >>>
> Hi Henrik
> 
> Thanks for your help, I really do appreciate it.
> 
> If I follow your instructions, R returns the value
> http://wwwcache.bbsrc.ac.uk:8080.  That is good and it means that
> indeed my http_proxy environment variable is set.
> 
> I have also added the lines
> 
> http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
> HTTP_PROXY=http://wwwcache.bbsrc.ac.uk:8080/
> 
> both to .Renviron in my home directory, and to /usr/lib/R/etc/Renviron
> and /usr/lib/R/etc/Renviron.site.
> 
> All to no avail... R still doesn't try to connect through my proxy
> server.  
> 
> Please, I genuinely think this is a bug in R 1.7.1 on Suse Linux 8.1.
> 
> NOW, here is a little detail I have just discovered that PROVES my
> proxy is working.  
> 
> If I do:
> 
> update.packages(method="wget")
> 
> then everything works fine.... hmmmm, but I still have a problem as the
> command I really want to run is :
> 
> source("http://wwwbioconductor.org/getBioC.R")
> 
> and source() does not accept an option 'method="wget"'....
> 
> SO... is there a way in R that I can set it up such that ALL internet
> connections from within R use method="wget" ??
> 
> Thanks
> Mick
> 
> -----Original Message-----
> From: Henrik Bengtsson [mailto:hb at maths.lth.se] 
> Sent: 25 July 2003 11:25
> To: 'michael watson (IAH-C)'
> Cc: R-help at stat.math.ethz.ch 
> Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Could it be that you have redefined the command R in your shell such
> that the http_proxy environment variable is set in one and R is
> running
> in another? (This is just a wild guess and I am myself only running
> WinXP.) What do you get if you do
> 
> % env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
> % R
> > Sys.getenv("http_proxy")
> 
> Also, have you considered setting http_proxy in ~/.Renviron (see
> ?.Renviron).
> 
> Cheers
> 
> Henrik Bengtsson
> Lund University
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > michael watson (IAH-C)
> > Sent: den 25 juli 2003 10:24
> > To: 'Prof Brian Ripley'
> > Cc: 'R-help at stat.math.ethz.ch' 
> > Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> > 
> > 
> > Hi
> > 
> > Thanks once again for your help, I do appreciate it..... however....
> > 
> > Here is what I get with your test.... (under tcsh - i 
> > normally use bash, but I will keep everything the same)
> > 
> > users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > 
> > >options(internet.info=0)
> > >update.packages()
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> > unable to connect to 'cran.r-project.org' on port 80
> > Error in download.file(url = paste(contriburl, "PACKAGES", 
> > sep = "/"), :
> > 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'
> 
> > 
> > 
> > ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> > message at all, which appears to suggest that R, under SUSE 
> > Linux 8.1, is NOT PICKING up the http_proxy environment 
> > variable - this isn't something thats wrong with my proxy, 
> > that works with everything else - internet browsers, ftp 
> > clients, wget, instant messenger clients etc etc.  The 
> > problem is R, which isn't picking up that it needs to use the 
> > http_proxy environment variable.  And I apologise for being 
> > blunt, but that is an R problem, not a proxy problem!
> > 
> > Thanks for your help
> > 
> > Mick
> > 
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: 24 July 2003 16:56
> > To: michael watson (IAH-C)
> > Subject: RE: [R] Your proxy seems not to work with R (was R 
> > won't connect to the internet on Linux!)
> > 
> > 
> > When I do (under tcsh)
> > 
> > env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > > options(internet.info=0)
> > > update.packages()
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> > Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080 
> > 
> > it tries to connect to your proxy (as it says) and gets no 
> > response, which is not surprising from my site.  If you get 
> > the same, your proxy is probably not behaving in the standard 
> > way (since that has been tested by many users with standard
> proxies).
> > 
> > I've changed the emphasis of the subject line to one I feel is more 
> > equitable: many, many users have counter-evidence to your original 
> > assertion, which was rather arrogant.
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > Hello Professor
> > > 
> > > If you are suggesting that I am simply missing the 
> > "http://" part of 
> > > my cache URL, or that I am missing a trailing "/", then I 
> > pre-empted 
> > > this response and it still doesn't work.
> > 
> > I was suggesting that `simply' you were not reading the documentation
> 
> > correctly. 
> > 
> > > I have tried setting both http_proxy and HTTP_PROXY to all of:
> > 
> > I hope you set to *each* of these.  The first and third are 
> > documented to be incorrect, so using those was perverse.
> > 
> > > wwwcache.bbsrc.ac.uk:8080
> > > http://wwwcache.bbsrc.ac.uk:8080 
> > > wwwcache.bbsrc.ac.uk:8080/
> > > http://wwwcache.bbsrc.ac.uk:8080/ 
> > > 
> > > and I still get the same response - R cannot open the URL.
> > > 
> > > And yes, that is thw right proxy address, I copied it straight from
> 
> > > Netscape on the same computer, and Netscape connects to the 
> > internet 
> > > fine.
> > > 
> > > Thanks
> > > Mick
> > >  
> > > 
> > > -----Original Message-----
> > > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > > Sent: 24 July 2003 13:17
> > > To: michael watson (IAH-C)
> > > Cc: 'R-help at stat.math.ethz.ch '
> > > Subject: Re: [R] R won't connect to the internet on Linux!
> > > 
> > > 
> > > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > > 
> > > > OK, I really am struggling with this one!  Forgive me if 
> > I am being 
> > > > stupid....
> > > 
> > > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> > internet 
> > > > through a proxy so I have:
> > > > 
> > > > IAHC-LINUX03:~ # echo $http_proxy
> > > > wwwcache.bbsrc.ac.uk:8080
> > > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > > wwwcache.bbsrc.ac.uk:8080
> > > > 
> > > > just in case ;-)
> > > > 
> > > > SO, i go into R and I get:
> > > > 
> > > > > source("http://www.bioconductor.org/getBioC.R")
> > > > unable to connect to 'www.bioconductor.org' on port 80. Error in
> 
> > > > file(file, "r") : cannot open URL 
> > > > `http://www.bioconductor.org/getBioC.R' 
> > > > 
> > > > OK so is R just not picking up my proxy setting?
> > > 
> > > Your setting is wrong, so it is being ignored.  The help page says
> 
> > > quite explicitly
> > > 
> > >       The form of `"http_proxy"' should be 
> > `"http://proxy.dom.com/"' or
> > >      `"http://proxy.dom.com:8080/"' where the port defaults 
> > to `80' and
> > >      the trailing slash may be omitted.
> > > 
> > > > It seems to be trying
> > > > port 80 on something, and I have specifically set it to 
> > port 8080 in 
> > > > my environment variables.  As far as I can see I have 
> > followed the 
> > > > reference manual suggestion, so does anyone else have one?
> > > 
> > > The problem is in your seeing, it seems.
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk 
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> 
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From fatman at oregonsbest.com  Sat Jul 26 18:18:33 2003
From: fatman at oregonsbest.com (John Jaynes)
Date: Sat, 26 Jul 2003 09:18:33 -0700
Subject: [R] Time Plot Question.
Message-ID: <200307260918.33911.fatman@oregonsbest.com>

Hello,

I have some data (sar -A -f sar_data.file > mydata) that span 24 hours, more or less, 00:00:00 - 23:59:59, or so. 
I would like to retain and plot data for all 24 x 60 x 60 = 86,400 seconds. I am able to set the x coordinates equal 
to  ISOdate( year, month, day, hour, minute, second) calls. However, ISOdate calls such as 

"ISOdate(2003, 7, 13, 7, 0:59, 0:59)"

 do not return all the seconds contained in hour 7. These function calls works:

R> x <- ISOdate(2003, 7, 13, 7)
R> for(j in 0:59) for(i in 0:59) x <- c(x, ISOdate(2003, 7, 13, 7, j, i )),

assigning all the seconds contained in hour 7, to x.

When I attempt this

for(k in 7:23) for(j in 0:59) for(i in 0:59) x <- c(x, ISOdate(2003, 7, 13, k, j, i )),

my computer hangs, as, I am guessing, this call requires too much memory of some sort. I could break up these
data and plot 1 Hour blocks, but would like to also view this data in a 24 Hour plot. The raw data appears as:

Time	              user      nice    system      idle
08:42:07             79.00      0.00     21.00      0.00
08:42:08             87.00      0.00     13.00      0.00
08:42:11             92.24      0.00      7.76      0.00
08:42:13             41.09      0.00      3.27     55.64
08:42:15             95.67      0.00      4.33      0.00
08:42:16             98.00      0.00      2.00      0.00

...

Is there an easier way to do this? I have read the help pages, but have not found one that is applicable. Any help
would be greatly appreciated. Thanks for making this Very useful program freely available.

John



From ripley at stats.ox.ac.uk  Sat Jul 26 18:30:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Jul 2003 17:30:56 +0100 (BST)
Subject: [R] Time Plot Question.
In-Reply-To: <200307260918.33911.fatman@oregonsbest.com>
Message-ID: <Pine.LNX.4.44.0307261724460.26252-100000@gannet.stats>

x0 <- ISOdate(2003, 7, 13, 0, 0, 0)
x <- x0 + seq(0, 86400)

is what you are asking for, or at least it is in the GMT time zone.

The reason your code is so slow is that expanding vectors by c() is very 
wasteful.  If you have pre-allocated x and assigned values by indexing I 
think it would have worked (but the above is the optimal way to do this).

On Sat, 26 Jul 2003, John Jaynes wrote:

> Hello,
> 
> I have some data (sar -A -f sar_data.file > mydata) that span 24 hours,
> more or less, 00:00:00 - 23:59:59, or so.  I would like to retain and
> plot data for all 24 x 60 x 60 = 86,400 seconds. I am able to set the x
> coordinates equal to ISOdate( year, month, day, hour, minute, second)
> calls. However, ISOdate calls such as
> 
> "ISOdate(2003, 7, 13, 7, 0:59, 0:59)"

No, as the recycling rules apply.  You could use expand.grid to create 
suitable arguments.

>  do not return all the seconds contained in hour 7. These function calls works:
> 
> R> x <- ISOdate(2003, 7, 13, 7)
> R> for(j in 0:59) for(i in 0:59) x <- c(x, ISOdate(2003, 7, 13, 7, j, i )),
> 
> assigning all the seconds contained in hour 7, to x.
> 
> When I attempt this
> 
> for(k in 7:23) for(j in 0:59) for(i in 0:59) x <- c(x, ISOdate(2003, 7, 13, k, j, i )),
> 
> my computer hangs, as, I am guessing, this call requires too much memory
> of some sort. I could break up these data and plot 1 Hour blocks, but
> would like to also view this data in a 24 Hour plot. The raw data
> appears as:
> 
> Time	              user      nice    system      idle
> 08:42:07             79.00      0.00     21.00      0.00
> 08:42:08             87.00      0.00     13.00      0.00
> 08:42:11             92.24      0.00      7.76      0.00
> 08:42:13             41.09      0.00      3.27     55.64
> 08:42:15             95.67      0.00      4.33      0.00
> 08:42:16             98.00      0.00      2.00      0.00
> 
> ...
> 
> Is there an easier way to do this? I have read the help pages, but have
> not found one that is applicable. Any help would be greatly appreciated.
> Thanks for making this Very useful program freely available.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thomasio at cs.tu-berlin.de  Sat Jul 26 21:22:28 2003
From: thomasio at cs.tu-berlin.de (Thomas Koenig)
Date: Sat, 26 Jul 2003 20:22:28 +0100
Subject: [R] How to make "<-" generic?
In-Reply-To: <3F227F1A.8090402@pburns.seanet.com>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
	<200307261424.48444.thomasio@cs.tu-berlin.de>
	<3F227F1A.8090402@pburns.seanet.com>
Message-ID: <200307262022.28328.thomasio@cs.tu-berlin.de>

Am Samstag, 26. Juli 2003 14:16 schrieb Patrick Burns:
> I think Brian's question --- what are you trying to do? -- should
> be the first order of business.

I don't want to loose the class (for me, some kind of meaning) of my 
variables, so in the first line as ("a little") insurance that my code does 
that what I want.

A first example
setClass("A",...)
setClass("B",...)
setMethod("doSomething",signature="A",function(x) return(1))
setMethod("doSomething",signature="B",function(x) return(2)) ; ## perhaps 
defined in a complete different library, file, etc.
setReplaceMethod("<-",c("A","A"),myAssign.A) ## only A <- A is now allowed
a <- new("A"); ## this is my a ;-)

... lines of code
a <- b ## loosing my type by an error or something else 
           ## should give the error : Error in myAssign(a,b) : No direct or
           ## inherited method for function "myAssign" for this call

## or (perhaps more "ugly")
f <- function(x) {
	if(runif(1) < 0.5) return(new("A"));
	else return(new("B"));	
}
a <- f(1)

.. lines of code
res <- doSomething(a); ## Function call is random. This is not what I want, 
and it is very hard to find the error. All things works fine, but the meaning 
is completly different

A second example with "length<-" (seems to have the same "problem"?), ensuring 
correct lengths:
setClass("A",representation(x = "numeric",y="numeric");
setReplaceMethod("length<-",c("A","integer"),setting x and y to the same 
lengths)
a <- new("A");
length(a) <- 10
ensures always the same lengths.

A third example, ensuring a consistent model
setClass("Cox",representation(coef = "numeric", otherData))
setClass("CoxControl",representation(method ="character",it = "integer", other 
Data))
setClass("CensoredData",representation(time = "numeric", cens = "integer", 
other Data));

## could be useful
setReplaceMethod("<-",c("Cox","CensoredData"),function(x) {do some checks for 
Censored Data  etc. and do perhaps (depends from the author) recalculation or 
delete old results})
setReplaceMethod("<-",c("Cox","CoxControl"),function(x) {setting only the 
"control slots" and do perhaps (depends from the author) recalculation or 
delete old results })

cox <- new("Cox"); ## my cox ;-)
censData <- new("CensoredData");
coxCtrl <- new("CoxControl")
## prepare censData and ControlData
## feeding the model with data
cox <- censData;
## feeding the model with controls
cox <- CoxControl;
## There is a possibility to have a consistent model

I hope that makes my intentions clearer, and is not completely nonsense.

Best regards and thanks for the discussion and your efforts!

Thomas K?nig



>
> Someone can make R do just about anything, but it is better to
> do a few things very well than lots of things in a muddled way.
>
> So.  What is the advantage of using assignment as a generic?
> I'm open-minded about there being such an advantage, but I
> don't see any right off.
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Thomas Koenig wrote:
> >Am Samstag, 26. Juli 2003 11:38 schrieb Peter Dalgaard BSA:
> >>Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:
> >>>Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> >>>>What are you trying to do with this?  Assignment (<-) is not a
> >>>>function,
> >
> >But what the difference between <- and e.g. the function length or "[<-"?
> > As I understood in "methods" everything has a class. And R says me with
> > is(...) (hope that the results are correct):
> >< is(get("<-"))
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >< is(get("length"))
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >
> >>is(get("[<-"))
> >
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >
> >>## test for the correct result of get(...) ?
> >>x <- 10
> >>is(get("x"))
> >
> >[1] "numeric" "vector"
> >
> >>## and
> >>setGeneric("<-")
> >
> >[1] "<-"
> >Warning message:
> >"<-" is a primitive function; its generic definition is built in and
> >automatically included. in: setGeneric("<-")
> >
> >>>>and the language grammar does not convert a <- b into "<-"(a,
> >>>>b) (as it would with the binary operator functions).  You could call it
> >>>>that way, and then it will probably work.
> >>>
> >>>Eh? Are you sure about that???
> >>>
> >>>>quote("<-"(a,b))
> >>>
> >>>a <- b
> >>
> >>Adding on to this, I think the point is that assignment bypasses the
> >>usual *evaluation* rules, even though it is syntactically a binop.
> >>
> >>I think it basically has to be so: For one thing, it is kind of
> >>difficult to check for a signature match without evaluating the
> >>arguments and the left hand side of an assignment will not in general
> >>exist at that point.
> >
> >In the methods is the class "missing". Could that help?
> >for one thing : signature=c("missing","A") (only allowed for <- ?)
> >for both things: signature=c("A","B")
> >for nothing ;-) : signature=c("missing","missing")
> >
> >Thanks and Best Regards
> >
> >Thomas K?nig
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jrogers at cantatapharm.com  Sat Jul 26 19:33:17 2003
From: jrogers at cantatapharm.com (Jim Rogers)
Date: Sat, 26 Jul 2003 13:33:17 -0400
Subject: [R] inverse prediction and Poisson regression
Message-ID: <99A12772DCDEEB458B996332957B0D5301180C@mercury.cantatapharm.com>

Vincent, 

As long as you are going to digest all those suggestions, let me offer
one more:

If you want to avoid making an assumption about the functional form of
the dose-response relationship (you don't even need to assume
monotonicity!), and also avoid having to trust the asymptotic
distributions of MLE's, a completely different approach is described in 

Stepwise Confidence Intervals without Mulitplicity Adjustment for Dose
Response and Toxicity Studies
Jason C. Hsu and Roger L. Berger 
June 99 issue of JASA

In your case, this approach would involve a pairwise test for each
positive dose population with the zero dose population (but without
multiplicity adjustment, even though you are doing multiple tests!). So
if you can pick a good test for comparing the means of two Poisson
distributions (I'm sure there are plenty, though I'm not sure what I
would recommend), then you can apply this method very easily. You lose
the power of "pooling" that comes with the assumption of a functional
relationship, but you may make up for this by getting more exact (not
asymptotic) confidence bounds (and it's always comforting to make fewer
assumptions). 

Just one more thing for you to stew on...

Happy thinking,
Jim Rogers 


> Hello to all: first and foremost: thank you for all this input. I only
discovered about "R" last week (!) and I think I will dump my SAS
license!!! 
> 
> 
> ;-) 
> 
> 
> This is a very dynamic listserve! 
> You "R" all great! Thank you! 
> 
> 
> I just hope some day I can help out a student the way you did today. 
> 
> 
> I will spend part of the weekend studying the different suggestions in
detail. Again, I'm not a stats person, so I will need some time and good
coffee to digest all this correctly. (I'm most worried about
understanding the nonlin suggestion.) Early next week, I will post a
"summary" of the suggestions and the path I chose to follow. (with
proper syntax Professor Ripley, I promise) 
> 
> 
> ;-) 
> 
> 
> Have a nice weekend 
> 
> 
> Best regards, 
> 
> 
> Vincent Philion 

James A. Rogers, Ph.D. <rogers at cantatapharm.com>
Statistical Scientist
Cantata Pharmaceuticals
300 Technology Square, 5th floor
Cambridge, MA  02139
617.225.9009 x312
Fax 617.225.9010



From peterm at andrew.cmu.edu  Sat Jul 26 19:32:31 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Sat, 26 Jul 2003 13:32:31 -0400
Subject: [R] Re: Programs stopped working--.print
Message-ID: <BB48336F.5B79%peterm@andrew.cmu.edu>

Thank you to everyone who replied to my curious problem, which just got more
curious.  Today I closed my copy of R, opened up a different copy of .RData
(in another directory), one that didn't have the ".print" problem.  Worked
w/ that for a few minutes.  Then closed R again & restarted from the copy of
.RData that was giving me the ".print" problem in all my programs (and
consistently gave me that problem, even when I reloaded it yesterday).  All
those problems disappeared, even running exactly the same code I couldn't
get to work yesterday.  The "."'s disappeared from the code as well.  Go
figure!  I guess I'll always keep a copy of my current .RData file, just in
case this happens again.  For those who asked, I'm working w/ R 1.7.0 on an
OS X 10.2.6 system.

Thanks again,

Peter



From djw1005 at cam.ac.uk  Sat Jul 26 20:14:56 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Sat, 26 Jul 2003 19:14:56 +0100 (BST)
Subject: [R] A model for disease progression
Message-ID: <Pine.SOL.3.96.1030726190308.17427A-100000@libra.cus.cam.ac.uk>


I would be grateful for advice about the following problem. It's not
directly R-related, but I'm hoping that R will help me analyse the
following data.

I have a table which indicates the progression of a certain age-related
disease.  At a certain point in time, a population was sampled; and I have
measurements for the age of each individual, and their disease stage.
(Disease stage is an ordered factor, none<A<B<C<D.)

I hypothesize that the rate of disease-onset depends on age, but that
(once the disease has started) its rate of progression depends on the
current disease stage and not on age.

I would like to plot the data in a sensible way, to let me see if this
hypothesis is reasonable, and to let me explore possible functional forms
for the two rates.

I can see how to plot the rate of disease onset. I could, for example,
plot for each age the fraction of people who have acquired the disease by
that age. But I don't know how to visualize the rate of disease
progression. 

All suggestions (especially pointers to relevant literature or R packages) 
gratefully received!

Damon.



From ozric at web.de  Sat Jul 26 21:41:09 2003
From: ozric at web.de (Christian Schulz)
Date: Sat, 26 Jul 2003 21:41:09 +0200
Subject: [R] A model for disease progression
References: <Pine.SOL.3.96.1030726190308.17427A-100000@libra.cus.cam.ac.uk>
Message-ID: <000b01c353ad$dd3fcf20$780aebd9@pc>

Perhaps the package msm (Multi-state Markov models in continuous time ) is
interesting for you?

christian




----- Original Message -----
From: "Damon Wischik" <djw1005 at cam.ac.uk>
To: "R-Help" <R-help at stat.math.ethz.ch>
Sent: Saturday, July 26, 2003 8:14 PM
Subject: [R] A model for disease progression


>
> I would be grateful for advice about the following problem. It's not
> directly R-related, but I'm hoping that R will help me analyse the
> following data.
>
> I have a table which indicates the progression of a certain age-related
> disease.  At a certain point in time, a population was sampled; and I have
> measurements for the age of each individual, and their disease stage.
> (Disease stage is an ordered factor, none<A<B<C<D.)
>
> I hypothesize that the rate of disease-onset depends on age, but that
> (once the disease has started) its rate of progression depends on the
> current disease stage and not on age.
>
> I would like to plot the data in a sensible way, to let me see if this
> hypothesis is reasonable, and to let me explore possible functional forms
> for the two rates.
>
> I can see how to plot the rate of disease onset. I could, for example,
> plot for each age the fraction of people who have acquired the disease by
> that age. But I don't know how to visualize the rate of disease
> progression.
>
> All suggestions (especially pointers to relevant literature or R packages)
> gratefully received!
>
> Damon.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mhayashi at rmail.plala.or.jp  Sun Jul 27 04:45:55 2003
From: mhayashi at rmail.plala.or.jp (Masayoshi Hayashi)
Date: Sun, 27 Jul 2003 11:45:55 +0900
Subject: [R] Drawing boxplots in pairs function
Message-ID: <FHEKKDCEIDIPFELPNCBDOEDFCBAA.mhayashi@rmail.plala.or.jp>

I tried the following command to produce boxplot of diagonals in pairs
function:

> pairs(t1.5,diag.panel=boxplot)
Error in pairs.default(t1.5, diag.panel = boxplot) :
        The panel function made a new plot

The graphics device draws two graphs, one with a rectangle at (1,1) in 7x7
layout and the other with boxplot(t1.5$wind) at (1,2).

pairs function help file has a similar example with hist function but I am
stuck with producing boxplot version.

Thanks for your time and reply in advance.

The below is t1.5 data from Applied Multivariate Statistical Analysis 5th,
Johnson and Wichern, p40 Table 1.5.

wind radiation co no no2 o3 hc
  8  98  7  2  12  8  2
  7  107  4  3  9  5  3
  7  103  4  3  5  6  3
  10  88  5  2  8  15  4
  6  91  4  2  8  10  3
  8  90  5  2  12  12  4
  9  84  7  4  12  15  5
  5  72  6  4  21  14  4
  7  82  5  1  11  11  3
  8  64  5  2  13  9  4
  6  71  5  4  10  3  3
  6  91  4  2  12  7  3
  7  72  7  4  18  10  3
  10  70  4  2  11  7  3
  10  72  4  1  8  10  3
  9  77  4  1  9  10  3
  8  76  4  1  7  7  3
  8  71  5  3  16  4  4
  9  67  4  2  13  2  3
  9  69  3  3  9  5  3
  10  62  5  3  14  4  4
  9  88  4  2  7  6  3
  8  80  4  2  13  11  4
  5  30  3  3  5  2  3
  6  83  5  1  10  23  4
  8  84  3  2  7  6  3
  6  78  4  2  11  11  3
  8  79  2  1  7  10  3
  6  62  4  3  9  8  3
  10  37  3  1  7  2  3
  8  71  4  1  10  7  3
  7  52  4  1  12  8  4
  5  48  6  5  8  4  3
  6  75  4  1  10  24  3
  10  35  4  1  6  9  2
  8  85  4  1  9  10  2
  5  86  3  1  6  12  2
  5  86  7  2  13  18  2
  7  79  7  4  9  25  3
  7  79  5  2  8  6  2
  6  68  6  2  11  14  3
  8  40  4  3  6  5  2



From adiamond at fas.harvard.edu  Sun Jul 27 08:50:25 2003
From: adiamond at fas.harvard.edu (adiamond@fas.harvard.edu)
Date: Sun, 27 Jul 2003 02:50:25 -0400
Subject: [R] contourplot:how to get it to label all contours like 'contour'
Message-ID: <1059288625.3f2376312bc03@webmail.fas.harvard.edu>

Hi,

one of the nice things about contour is that it labels all contour lines.
contourplot only labels each particular elevation a single time.
i.e., if there are two contour lines corresponding to z = 45, it will only 
label one of them.

is there a way to get contourplot to automatically label all the contour lines, 
even those that are repeated?

i ask because i want to plot contour plots *w/ all contours labelled*, next to  
wireframe plots-- i can plot two lattice plots (wireframe and contourplot)
next to each other easily enough, but contourplot doesn't label all contours.

AND, i can get contour to label all of 'em, but then i don't know how to put 
an 'Old Style' contour plot alongside the lattice wireframe.
mfrow doesn't work for wireframe, & i can't "print" plots generated by contour.

any help is much appreciated.

best,

alexis



From ripley at stats.ox.ac.uk  Sun Jul 27 09:21:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Jul 2003 08:21:50 +0100 (BST)
Subject: [R] Drawing boxplots in pairs function
In-Reply-To: <FHEKKDCEIDIPFELPNCBDOEDFCBAA.mhayashi@rmail.plala.or.jp>
Message-ID: <Pine.LNX.4.44.0307270810410.3341-100000@gannet.stats>

Here's an example

data(swiss)
panel.bxp <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 2, usr[3:4]))
    boxplot(x, add=TRUE)
}
pairs(swiss, diag.panel = panel.bxp, text.panel = function(...){})

You overlooked the `add=TRUE' argument, it seems.

On Sun, 27 Jul 2003, Masayoshi Hayashi wrote:

> I tried the following command to produce boxplot of diagonals in pairs
> function:
> 
> > pairs(t1.5,diag.panel=boxplot)
> Error in pairs.default(t1.5, diag.panel = boxplot) :
>         The panel function made a new plot
> 
> The graphics device draws two graphs, one with a rectangle at (1,1) in 7x7
> layout and the other with boxplot(t1.5$wind) at (1,2).
> 
> pairs function help file has a similar example with hist function but I am
> stuck with producing boxplot version.
> 
> Thanks for your time and reply in advance.
> 
> The below is t1.5 data from Applied Multivariate Statistical Analysis 5th,
> Johnson and Wichern, p40 Table 1.5.
> 
> wind radiation co no no2 o3 hc
>   8  98  7  2  12  8  2
>   7  107  4  3  9  5  3
>   7  103  4  3  5  6  3
>   10  88  5  2  8  15  4
>   6  91  4  2  8  10  3
>   8  90  5  2  12  12  4
>   9  84  7  4  12  15  5
>   5  72  6  4  21  14  4
>   7  82  5  1  11  11  3
>   8  64  5  2  13  9  4
>   6  71  5  4  10  3  3
>   6  91  4  2  12  7  3
>   7  72  7  4  18  10  3
>   10  70  4  2  11  7  3
>   10  72  4  1  8  10  3
>   9  77  4  1  9  10  3
>   8  76  4  1  7  7  3
>   8  71  5  3  16  4  4
>   9  67  4  2  13  2  3
>   9  69  3  3  9  5  3
>   10  62  5  3  14  4  4
>   9  88  4  2  7  6  3
>   8  80  4  2  13  11  4
>   5  30  3  3  5  2  3
>   6  83  5  1  10  23  4
>   8  84  3  2  7  6  3
>   6  78  4  2  11  11  3
>   8  79  2  1  7  10  3
>   6  62  4  3  9  8  3
>   10  37  3  1  7  2  3
>   8  71  4  1  10  7  3
>   7  52  4  1  12  8  4
>   5  48  6  5  8  4  3
>   6  75  4  1  10  24  3
>   10  35  4  1  6  9  2
>   8  85  4  1  9  10  2
>   5  86  3  1  6  12  2
>   5  86  7  2  13  18  2
>   7  79  7  4  9  25  3
>   7  79  5  2  8  6  2
>   6  68  6  2  11  14  3
>   8  40  4  3  6  5  2
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Jul 27 09:30:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Jul 2003 08:30:10 +0100 (BST)
Subject: [R] contourplot:how to get it to label all contours like 'contour'
In-Reply-To: <1059288625.3f2376312bc03@webmail.fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0307270823411.3341-100000@gannet.stats>

On Sun, 27 Jul 2003 adiamond at fas.harvard.edu wrote:

> Hi,
> 
> one of the nice things about contour is that it labels all contour lines.
> contourplot only labels each particular elevation a single time.
> i.e., if there are two contour lines corresponding to z = 45, it will only 
> label one of them.
> 
> is there a way to get contourplot to automatically label all the contour lines, 
> even those that are repeated?
> 
> i ask because i want to plot contour plots *w/ all contours labelled*, next to  
> wireframe plots-- i can plot two lattice plots (wireframe and contourplot)
> next to each other easily enough, but contourplot doesn't label all contours.
> 
> AND, i can get contour to label all of 'em, but then i don't know how to put 
> an 'Old Style' contour plot alongside the lattice wireframe.
> mfrow doesn't work for wireframe, & i can't "print" plots generated by contour.

What are you doing with the plot?  If including in a document, it is
should be easy to include them as two subfigures (it is LaTeX).  If you
want to do this on one on-screen device it isn't actually that hard: use
print.trellis() to put the wireframe() one where you want it, and use
par(fin) to select a figure region and call contour() within that.  I've
done that sort of thing several times when I wanted overlapping figure
regions, usually because wireframe() gave too large margins.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.calboli at ucl.ac.uk  Sun Jul 27 14:35:40 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Sun, 27 Jul 2003 13:35:40 +0100
Subject: [R] continuous independent variable in lme
Message-ID: <3.0.6.32.20030727133540.028cf008@pop-server.ucl.ac.uk>

Dear All,

I am writing to ask a clarification on what R, and in particular lme, is
doing.

I have an experiment where fly wing area was measured in 4 selection lines,
measured at 18 and 25 degrees. I am using a lme model because I have three
replicated per line (coded 1:12 so I need not use getGroups to creat an
orederd factor).

The lines are called: "18"; "25"; "l"; "s".  My data looks like:

  temp line replicate   area  
1   25    l    3  92693 
2   25    l    3 100092 
3   25    l    3 100039 
4   25    l    3  97558 
5   25    l    3  95603 
6   25    l    3 100482 
.....

"18" and "25" are controls for the two other lines so I have set the
following contrasts for lines:

   [,1] [,2] [,3]
18    1    0    1
25   -1    0    1
l     0    1   -1
s     0   -1   -1

If I do the following:

mod1<-lme(area ~ line * temp, random = ~1|replicate/temp, mydata)
anova(mod1)

I get:

            numDF denDF  F-value p-value
(Intercept)     1   336 41817.83  <.0001
line            3     8    14.38  0.0014
temp            1     8   338.21  <.0001
line:temp       3     8     0.62  0.6211


I have a significant effect of selection line. Eyeballing the
interction.plot, it is clear the the line called "25" is smaller at both
temperatures than the other lines.

but when I check the contrasts with summary(mod1) I get:

Fixed effects: area ~ line * temp 
                Value Std.Error  DF   t-value p-value
(Intercept) 165417.32  3102.751 336  53.31312  <.0001
line1         2631.71  4387.952   8   0.59976  0.5653
line2        -2603.27  4387.952   8  -0.59328  0.5694
line3        -4667.61  3102.751   8  -1.50435  0.1709
temp         -2614.39   142.160   8 -18.39045  <.0001
line1:temp      96.39   201.045   8   0.47946  0.6444
line2:temp      95.74   201.045   8   0.47623  0.6466
line3:temp     168.55   142.160   8   1.18561  0.2698

There seems to be no difference in my lines, according to the contrasts I set!

I tried to do the same analysis using temperature as an orderd factor:

mod2<-lme(area ~ line * ordered(temp), random = ~1|replicate/ordered(temp),
mydata)
anova(mod2)

                   numDF denDF  F-value p-value
(Intercept)            1   336 41817.83  <.0001
line                   3     8    14.38  0.0014
ordered(temp)          1     8   338.21  <.0001
line:ordered(temp)     3     8     0.62  0.6211
 
the same anova, but the contrasts are :

Fixed effects: area ~ line * ordered(temp) 
                          Value Std.Error  DF   t-value p-value
(Intercept)           109207.89  534.0393 336 204.49409  <.0001
line1                   4704.14  755.2457   8   6.22862  0.0003
line2                   -544.76  755.2457   8  -0.72130  0.4913
line3                  -1043.87  534.0393   8  -1.95467  0.0864
ordered(temp).L       -12940.58  703.6576   8 -18.39045  <.0001
line1:ordered(temp).L    477.12  995.1221   8   0.47946  0.6444
line2:ordered(temp).L    473.91  995.1221   8   0.47623  0.6466
line3:ordered(temp).L    834.26  703.6576   8   1.18561  0.2698

way different from the previous model. This time the "18" and "25" lines
are different!

As I do not want to specify a model thinking I am doing something when I am
not, I would like to ask you why the difference in the results? 

the fact that my continuous variable, temperature, is rapresented by two
integers, 18 or 25, can cause the difference? assuming I were interested in
the interaction between my contrasts and temperature, to asses differences
in the slope between treatments, what should I do?

Again, I ask this in order to properly understand what lme is doing so I
can go back to work and specify the model I want rather than something else
altogether.

Regards,
Federico Calboli





=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From jfox at mcmaster.ca  Sun Jul 27 14:48:46 2003
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 27 Jul 2003 08:48:46 -0400
Subject: [R] Condition indexes and variance inflation factors
In-Reply-To: <sf1fbd37.059@MAIL.NDRI.ORG>
Message-ID: <5.1.0.14.2.20030727083023.01f97e88@127.0.0.1>

Dear Peter,

I'm sorry that I've taken a while to get back to you -- I was away for a 
few days.

In the example that you give from Belsley (1991), the predictors are 
essentially perfectly linearly related; for example

     > summary(lm(x2a ~ x3a + x4a))

     Call:
     lm(formula = x2a ~ x3a + x4a)

     Residuals:
             1          2          3          4          5          6 
    7
     -0.0195624 -0.0152938  0.0078068  0.0323025 
-0.0087845  0.0025448  0.0014472
             8
     -0.0004606

     Coefficients:
                 Estimate Std. Error  t value Pr(>|t|)
     (Intercept)  0.007943   0.010025    0.792    0.464
     x3a         -6.181811   0.016069 -384.716 2.25e-12 ***
     x4a         28.540996   0.066907  426.580 1.34e-12 ***
     ---
     Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

     Residual standard error: 0.01901 on 5 degrees of freedom
     Multiple R-Squared:     1,      Adjusted R-squared:     1
     F-statistic: 1.033e+05 on 2 and 5 DF,  p-value: 2.879e-12

In a case like this, the variance-inflation factors will also be very large:

 > vif(lm(y~ x2a + x3a + x4a))
      x2a      x3a      x4a
41333.34 47141.19 57958.62

Any of several methods of discovering the linear relationship among the x's 
will work -- including the first regression above, a principal-components 
analysis, and Belsley's approach.

I'm not arguing that discovering the source of large standard errors in a 
regression is completely uninteresting, although in most circumstances 
there isn't much that one can do about it short of collecting new data -- 
but this probably isn't a proper forum to have a detailed discussion about 
collinearity (my fault for broaching the issue in the first place).

Except with respect to centering the data, I suspect that we largely agree 
about these matters.

Regards,
  John


At 11:03 AM 7/24/2003 -0400, Peter Flom wrote:
>Dear John
>
>An interesting discussion!
>
>I would be the last to suggest ignoring such diagnostics as Cook's D;
>as you point out, it diagnoses a problem which condition indices do not:
>Whether a point is influential.
>
>OTOH, condition indices diagnose a problem which Cook's D does not:
>Would shifting the data slightly change the results.
>
>Consider the data given in Belsley (1991) on p. 5
>
>y <-   c( 3.3979, 1.6094, 3.7131, 1.6767, 0.0419, 3.3768, 1.1661,
>0.4701)
>x2a <- c(-3.138, -0.297, -4.582, 0.301, 2.729, -4.836, 0.065, 4.102)
>x2b <- c(-3.136, -0.296, -4.581, 0.300, 2.730, -4.834, 0.064, 4.103)
>x3a <- c(1.286, 0.250, 1.247, 0.498, -0.280, 0.350, 0.208, 1.069)
>x3b <- c(1.288, 0.251, 1.246, 0.498, -0.281, 0.349, 0.206, 1.069)
>x4a <- c(0.169, 0.044, 0.109, 0.117, 0.035, -0.094, 0.047, 0.375)
>x4b <- c(0.170, 0.043, 0.108, 0.118, 0.036, -0.093, 0.048, 0.376)
>
>where x2a , x3a and x4a are very similar to x2b, x3b and x4b,
>respecttively, and where both are generated from
>
>y = 1.2I  - 0.4 x2 + 0.6x3 + 0.9x4 + e
>
>e ~ N(0, 0.01)
>
>Then
>modela <- lm(y~ x2a + x3a + x4a)
>and
>modelb <- lm(y~x2b + x3b + x4b)
>
>give radically different results, with neither having any significant
>parameters other than the intercept.  Admittedly, the standard errors
>for a couple of the parameters are large.  But why are they large? I
>have certainly dealt with models with large standard errors that have
>nothing to do with collinearity.
>
>here, the function PI.lm (supplied by Juergen Gross) gives huge
>condition indices, and indicates that the nature of the problem is that
>all three of the x variables are highly collinear.
>
>Variance-Decomposition Proportions for
>Scaled Condition Indexes:
>
>     (Intercept) x2b x3b x4b
>1        0.0494   0   0   0
>1        0.0009   0   0   0
>3        0.8101   0   0   0
>464      0.1396   1   1   1
>




-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From ripley at stats.ox.ac.uk  Sun Jul 27 15:14:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Jul 2003 14:14:56 +0100 (BST)
Subject: [R] continuous independent variable in lme
In-Reply-To: <3.0.6.32.20030727133540.028cf008@pop-server.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0307271400060.4247-100000@gannet.stats>

Your anova call is a sequential anova, which you are misinterpreting.
You can't conclude terms are significant or not if later terms are.
You need to use type="marginal" to interpret things the way you do (except 
that I hope that does not drop the main effect and keep the interaction).

You also seem to be interpreting main effects in the presence of 
interactions incorrectly.  In your first model the coefs for `line' are
intercepts at 0 temp (probably uninteresting) whereas in the second they 
are at intercepts at temp=21.5 (probably also uninteresting).  It makes 
perfect sense to have lines of different slopes with similar intercepts at 
0 but different ones at 21.5.

Perhaps it is `temp' you want to think hard about how to code?

On Sun, 27 Jul 2003, Federico Calboli wrote:

> I am writing to ask a clarification on what R, and in particular lme, is
> doing.

Actually, it is the user who tells R what to do, and it does as it is
told.  What you are asking is what the language you used means.

> I have an experiment where fly wing area was measured in 4 selection lines,
> measured at 18 and 25 degrees. I am using a lme model because I have three
> replicated per line (coded 1:12 so I need not use getGroups to creat an
> orederd factor).
> 
> The lines are called: "18"; "25"; "l"; "s".  My data looks like:
> 
>   temp line replicate   area  
> 1   25    l    3  92693 
> 2   25    l    3 100092 
> 3   25    l    3 100039 
> 4   25    l    3  97558 
> 5   25    l    3  95603 
> 6   25    l    3 100482 
> .....
> 
> "18" and "25" are controls for the two other lines so I have set the
> following contrasts for lines:
> 
>    [,1] [,2] [,3]
> 18    1    0    1
> 25   -1    0    1
> l     0    1   -1
> s     0   -1   -1
> 
> If I do the following:
> 
> mod1<-lme(area ~ line * temp, random = ~1|replicate/temp, mydata)
> anova(mod1)
> 
> I get:
> 
>             numDF denDF  F-value p-value
> (Intercept)     1   336 41817.83  <.0001
> line            3     8    14.38  0.0014
> temp            1     8   338.21  <.0001
> line:temp       3     8     0.62  0.6211
> 
> 
> I have a significant effect of selection line. Eyeballing the
> interction.plot, it is clear the the line called "25" is smaller at both
> temperatures than the other lines.
> 
> but when I check the contrasts with summary(mod1) I get:
> 
> Fixed effects: area ~ line * temp 
>                 Value Std.Error  DF   t-value p-value
> (Intercept) 165417.32  3102.751 336  53.31312  <.0001
> line1         2631.71  4387.952   8   0.59976  0.5653
> line2        -2603.27  4387.952   8  -0.59328  0.5694
> line3        -4667.61  3102.751   8  -1.50435  0.1709
> temp         -2614.39   142.160   8 -18.39045  <.0001
> line1:temp      96.39   201.045   8   0.47946  0.6444
> line2:temp      95.74   201.045   8   0.47623  0.6466
> line3:temp     168.55   142.160   8   1.18561  0.2698
> 
> There seems to be no difference in my lines, according to the contrasts I set!
> 
> I tried to do the same analysis using temperature as an orderd factor:
> 
> mod2<-lme(area ~ line * ordered(temp), random = ~1|replicate/ordered(temp),
> mydata)
> anova(mod2)
> 
>                    numDF denDF  F-value p-value
> (Intercept)            1   336 41817.83  <.0001
> line                   3     8    14.38  0.0014
> ordered(temp)          1     8   338.21  <.0001
> line:ordered(temp)     3     8     0.62  0.6211
>  
> the same anova, but the contrasts are :
> 
> Fixed effects: area ~ line * ordered(temp) 
>                           Value Std.Error  DF   t-value p-value
> (Intercept)           109207.89  534.0393 336 204.49409  <.0001
> line1                   4704.14  755.2457   8   6.22862  0.0003
> line2                   -544.76  755.2457   8  -0.72130  0.4913
> line3                  -1043.87  534.0393   8  -1.95467  0.0864
> ordered(temp).L       -12940.58  703.6576   8 -18.39045  <.0001
> line1:ordered(temp).L    477.12  995.1221   8   0.47946  0.6444
> line2:ordered(temp).L    473.91  995.1221   8   0.47623  0.6466
> line3:ordered(temp).L    834.26  703.6576   8   1.18561  0.2698
> 
> way different from the previous model. This time the "18" and "25" lines
> are different!

Yes, as the interpretation is different.

> As I do not want to specify a model thinking I am doing something when I am
> not, I would like to ask you why the difference in the results? 
> 
> the fact that my continuous variable, temperature, is rapresented by two
> integers, 18 or 25, can cause the difference? assuming I were interested in
> the interaction between my contrasts and temperature, to asses differences
> in the slope between treatments, what should I do?

Code treatment sensibly, as a factor.  What is sensible depends on your 
subject's context.

> Again, I ask this in order to properly understand what lme is doing so I
> can go back to work and specify the model I want rather than something else
> altogether.

Almost nothing to do with lme, only with linear-model coding.  Please
read thoroughly e.g. the appropriate chapters of MASS so you understand 
coding.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stephen at inf.ed.ac.uk  Sun Jul 27 16:21:56 2003
From: stephen at inf.ed.ac.uk (Stephen Eglen)
Date: Sun, 27 Jul 2003 15:21:56 +0100
Subject: [R] Default title for hist assumes name is short
Message-ID: <16163.57348.71264.672444@bushmills.inf.ed.ac.uk>

On the following plot:

hist(apply(cbind( runif(1000), runif(1000)), 1,
  function(x) {sqrt(sum(x^2))}))

the title is three lines long and so has "Histogram of " at the start
of each line of the title.  This is because the definition of main in
hist.default is main = paste("Histogram of", xname).  Is there an easy
way of changing the default for main so that it only writes "Histogram
of" once?

Thanks, Stepehn



From ligges at statistik.uni-dortmund.de  Sun Jul 27 16:35:54 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Jul 2003 16:35:54 +0200
Subject: [R] Default title for hist assumes name is short
In-Reply-To: <16163.57348.71264.672444@bushmills.inf.ed.ac.uk>
References: <16163.57348.71264.672444@bushmills.inf.ed.ac.uk>
Message-ID: <3F23E34A.3060902@statistik.uni-dortmund.de>

Stephen Eglen wrote:

> On the following plot:
> 
> hist(apply(cbind( runif(1000), runif(1000)), 1,
>   function(x) {sqrt(sum(x^2))}))
> 
> the title is three lines long and so has "Histogram of " at the start
> of each line of the title.  This is because the definition of main in
> hist.default is main = paste("Histogram of", xname).  Is there an easy
> way of changing the default for main so that it only writes "Histogram
> of" once?
> 
> Thanks, Stepehn

By specifying argument "main", as mentioned in ?hist ?

  hist(apply(cbind( runif(1000), runif(1000)), 1,
    function(x) {sqrt(sum(x^2))}), main="Anything")


Uwe Ligges



From spencer.graves at pdf.com  Sun Jul 27 16:36:46 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 27 Jul 2003 07:36:46 -0700
Subject: [R] Default title for hist assumes name is short
References: <16163.57348.71264.672444@bushmills.inf.ed.ac.uk>
Message-ID: <3F23E37E.8030909@pdf.com>

Did you try:

hist(apply(cbind( runif(1000), runif(1000)), 1,
   function(x) {sqrt(sum(x^2))}), main="adsf")

This worked for me in both S-Plus 6.1 and R 1.7.1.
hope this helps.  spencer graves

Stephen Eglen wrote:
> On the following plot:
> 
> hist(apply(cbind( runif(1000), runif(1000)), 1,
>   function(x) {sqrt(sum(x^2))}))
> 
> the title is three lines long and so has "Histogram of " at the start
> of each line of the title.  This is because the definition of main in
> hist.default is main = paste("Histogram of", xname).  Is there an easy
> way of changing the default for main so that it only writes "Histogram
> of" once?
> 
> Thanks, Stepehn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From baron at psych.upenn.edu  Sun Jul 27 20:47:30 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 27 Jul 2003 14:47:30 -0400
Subject: [R] multiple imputation with fit.mult.impute in Hmisc
Message-ID: <20030727184730.GA3255@mail1.sas.upenn.edu>

I have always avoided missing data by keeping my distance from
the real world.  But I have a student who is doing a study of
real patients.  We're trying to test regression models using
multiple imputation.  We did the following (roughly):

f <- aregImpute(~ [list of 32 variables, separated by + signs],
 n.impute=20, defaultLinear=T, data=t1)
# I read that 20 is better than the default of 5.
# defaultLinear makes sense for our data.

fmp <- fit.mult.impute(Y ~ X1 + X2 ... [for the model of interest],
 xtrans=f, fitter=lm, data=t1)

and all goes well (usually) except that we get the following
message at the end of the last step:

 Warning message: Not using a Design fitting function;
 summary(fit) will use standard errors, t, P from last imputation
 only.  Use Varcov(fit) to get the correct covariance matrix,
 sqrt(diag(Varcov(fit))) to get s.e.

I did try using sqrt(diag(Varcov(fmp))), as it suggested, and it
didn't seem to change anything from when I did summary(fmp).

But this Warning message sounds scary.  It sounds like the whole
process of multiple imputation is being ignored, if only the last
one is being used.

So I discovered I could get rid of this warning by loading the
Design library and then using ols instead of lm as the fitter in
fit.mult.imput.  It seems that ols provides a variance/covariance
matrix (or something) that fit.mult.impute can use.

But here I am beyond my (very recently acquired) understanding of
what this is all about.

Should I worry about that warning message?  Or am I maybe off the
track in some larger way?

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From f.calboli at ucl.ac.uk  Mon Jul 28 00:58:52 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Sun, 27 Jul 2003 23:58:52 +0100
Subject: [R] continuous independent variable in lme
In-Reply-To: <Pine.LNX.4.44.0307271400060.4247-100000@gannet.stats>
References: <3.0.6.32.20030727133540.028cf008@pop-server.ucl.ac.uk>
Message-ID: <3.0.6.32.20030727235852.00a5ed50@pop-server.ucl.ac.uk>

At 14:14 27/07/2003 +0100, you wrote:
>Your anova call is a sequential anova, which you are misinterpreting.
>You can't conclude terms are significant or not if later terms are.
>You need to use type="marginal" to interpret things the way you do (except 
>that I hope that does not drop the main effect and keep the interaction).
>
>You also seem to be interpreting main effects in the presence of 
>interactions incorrectly.  In your first model the coefs for `line' are
>intercepts at 0 temp (probably uninteresting) whereas in the second they 
>are at intercepts at temp=21.5 (probably also uninteresting).  It makes 
>perfect sense to have lines of different slopes with similar intercepts at 
>0 but different ones at 21.5.
>
>Perhaps it is `temp' you want to think hard about how to code?

Prof. Ripley,

many thanks for your reply. I coded temperature as a factor and imposed the
contrasts:

   [,1]
18    1
25   -1

After doing this, the results of anova() are the following:

 anova(lme(area  ~line*temp, random= ~ 1|replicate/temp, mydata), type="m")
            numDF denDF  F-value p-value
(Intercept)     1   336 41817.83  <.0001
line            3     8    14.38  0.0014
temp            1     8   338.21  <.0001
line:temp       3     8     0.62  0.6211

which, incidentally, are identical to the call:

anova(lme(area  ~line*temp, random= ~ 1|replicate/temp, mydata))
            numDF denDF  F-value p-value
(Intercept)     1   336 41817.83  <.0001
line            3     8    14.38  0.0014
temp            1     8   338.21  <.0001
line:temp       3     8     0.62  0.6211

as my data is perfectly balanced (at lest I think this is the most
plausible explanation). 

The contrasts read:

                Value Std.Error  DF   t-value p-value
(Intercept) 109207.89  534.0393 336 204.49409  <.0001
line1         4704.14  755.2457   8   6.22862  0.0003
line2         -544.76  755.2457   8  -0.72130  0.4913
line3        -1043.87  534.0393   8  -1.95467  0.0864
temp1         9150.37  497.5610   8  18.39045  <.0001
line1:temp1   -337.37  703.6576   8  -0.47946  0.6444
line2:temp1   -335.11  703.6576   8  -0.47623  0.6466
line3:temp1   -589.91  497.5610   8  -1.18561  0.2698

which is what I would expect from eyeballing the interaction.plot. I
imagine I could come up with a better model, but I still need more
pondering over chpt 6 of MASS. Nonetheless I think I have a better grasp of
what I am doing now.

Regards,

Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From hodgess at gator.dt.uh.edu  Mon Jul 28 01:28:37 2003
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sun, 27 Jul 2003 18:28:37 -0500
Subject: [R] citing an R help file
Message-ID: <200307272328.h6RNSbJ31685@gator.dt.uh.edu>

Dear R People:

What is the correct way to cite help files for a function
in R, please?

(I'm referring to the online HTML documentation.)

Thank you in advance!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From hodgess at gator.dt.uh.edu  Mon Jul 28 01:33:31 2003
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Sun, 27 Jul 2003 18:33:31 -0500
Subject: [R] another citation question
Message-ID: <200307272333.h6RNXVO31763@gator.dt.uh.edu>

Dear R People:

What is the correct way to cite the manuals available online, please, 
such as "An Introduction to R"?

Thanks again,
Erin
hodgess at gator.uhd.edu



From news at chasset.net  Mon Jul 28 01:41:00 2003
From: news at chasset.net (Pierre-Olivier Chasset)
Date: Mon, 28 Jul 2003 01:41:00 +0200
Subject: [R] Perl
Message-ID: <200307280141.00194.news@chasset.net>

Dear Helpers,

I would like to know if I can call other use R-method within Perl scripts.
I have not found any module in CPAN (Perl Archive).
Thanks,

Pierre-Olivier



From ok at cs.otago.ac.nz  Mon Jul 28 02:45:52 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 28 Jul 2003 12:45:52 +1200 (NZST)
Subject: [R] How to make "<-" generic?
Message-ID: <200307280045.h6S0jq3p369171@atlas.otago.ac.nz>

The model of assignment in R is pretty simple:

    (variable) <- (expression)
	Evaluate expression, store result in variable.
	This is core syntax.  A compiled implementation of this would
	not involve any function call.  The behaviour does not in any
	way depend on the prior value of (variable), indeed, there is
	no requirement that (variable) _have_ any value.

    func(arg, args) <- (expression)
	This is handled by macro-expansion to
	arg <- "func<-"(arg, args, value=(expression))
	(If arg is not a variable, the expansion is applied recursively.)

	This involves a perfectly ordinary function call (which happens
	to have a funny name) and a perfectly ordinary assignment.
	The behaviour of "func<-" may depend on the value of arg; it
	need not, and in that case arg need not have a value, thanks to
	lazy evaluation.  Note that "func<-" itself does not, as a rule,
	change any variable (other than its own).  The change is done
	by the ordinary assignment to arg.

	Example:
	"+<-" <- function(lhs, rhs, value) value - rhs
	Now
	x + 1 <- 3
	results in x being 2.

"<-" already *is* as generic as it can possibly be; it does the right
thing whatever the value of the right hand side and whatever the value
(including none) of the left hand side.

If you want a kind of assignment which does something different,
then there is no compelling reason to (ab)use "<-" to do it; you
could follow the example of ?assign and use a readable intention-revealing
function name to do whatever it is that you want, and it could exploit
lazy evaluation to determine the name of the variable(s) it is to affect.

If anyone succeeded in making "<-" act like a normal operator and then
made its behaviour depend on the value of its first argument, we would
be left with no simple way to *initialise* a variable.

Please, DON'T try to 'make "<-" generic'.



From glaziou at pasteur-kh.org  Mon Jul 28 03:10:03 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Mon, 28 Jul 2003 08:10:03 +0700
Subject: [R] another citation question
In-Reply-To: <200307272333.h6RNXVO31763@gator.dt.uh.edu>
References: <200307272333.h6RNXVO31763@gator.dt.uh.edu>
Message-ID: <20030728011003.GA580@pasteur-kh.org>

Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> What is the correct way to cite the manuals available
> online, please, such as "An Introduction to R"?

See the R-FAQ, page 7.

-- 
Philippe



From fharrell at virginia.edu  Mon Jul 28 04:20:10 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Sun, 27 Jul 2003 22:20:10 -0400
Subject: [R] multiple imputation with fit.mult.impute in Hmisc
In-Reply-To: <20030727184730.GA3255@mail1.sas.upenn.edu>
References: <20030727184730.GA3255@mail1.sas.upenn.edu>
Message-ID: <20030727222010.02eeced6.fharrell@virginia.edu>

On Sun, 27 Jul 2003 14:47:30 -0400
Jonathan Baron <baron at psych.upenn.edu> wrote:

> I have always avoided missing data by keeping my distance from
> the real world.  But I have a student who is doing a study of
> real patients.  We're trying to test regression models using
> multiple imputation.  We did the following (roughly):
> 
> f <- aregImpute(~ [list of 32 variables, separated by + signs],
>  n.impute=20, defaultLinear=T, data=t1)
> # I read that 20 is better than the default of 5.
> # defaultLinear makes sense for our data.
> 
> fmp <- fit.mult.impute(Y ~ X1 + X2 ... [for the model of interest],
>  xtrans=f, fitter=lm, data=t1)
> 
> and all goes well (usually) except that we get the following
> message at the end of the last step:
> 
>  Warning message: Not using a Design fitting function;
>  summary(fit) will use standard errors, t, P from last imputation
>  only.  Use Varcov(fit) to get the correct covariance matrix,
>  sqrt(diag(Varcov(fit))) to get s.e.
> 
> I did try using sqrt(diag(Varcov(fmp))), as it suggested, and it
> didn't seem to change anything from when I did summary(fmp).
> 
> But this Warning message sounds scary.  It sounds like the whole
> process of multiple imputation is being ignored, if only the last
> one is being used.

The warning message may be ignored.  But the advice to use Varcov(fmp) is faulty for lm fits - I will fix that in the next release of Hmisc.  You may get the imputation-corrected covariance matrix for now using fmp$var


> 
> So I discovered I could get rid of this warning by loading the
> Design library and then using ols instead of lm as the fitter in
> fit.mult.imput.  It seems that ols provides a variance/covariance
> matrix (or something) that fit.mult.impute can use.

That works too.

Frank

> 
> But here I am beyond my (very recently acquired) understanding of
> what this is all about.
> 
> Should I worry about that warning message?  Or am I maybe off the
> track in some larger way?
> 
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page:            http://www.sas.upenn.edu/~baron
> R page:               http://finzi.psych.upenn.edu/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From jillirwin at optusnet.com.au  Mon Jul 28 08:08:44 2003
From: jillirwin at optusnet.com.au (jill irwin)
Date: Mon, 28 Jul 2003 16:08:44 +1000
Subject: [R] hi
Message-ID: <000a01c354ce$b63db780$711da4cb@q1g8b8>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030728/c2b5f429/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Jul 28 08:49:00 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Jul 2003 08:49:00 +0200
Subject: [R] citing an R help file
In-Reply-To: <200307272328.h6RNSbJ31685@gator.dt.uh.edu>
References: <200307272328.h6RNSbJ31685@gator.dt.uh.edu>
Message-ID: <3F24C75C.2020305@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> What is the correct way to cite help files for a function
> in R, please?

The R Reference Index manual "The R Environment for Statistical 
Computing and Graphics" seems to be the appropriate document.

I'm using

@MANUAL{r-ref,
   author =      {{R Development Core Team}},
   title =       {{The R Environment for Statistical Computing and 
Graphics,} {\upshape Version 1.7.1}},
   YEAR =        "2003",
   ORGANIZATION ={{R--Project}},
   ISBN =        {3-901167-50-1},
   URL =         {http://CRAN.R-project.org/manuals.html}
}


Uwe Ligges

> (I'm referring to the online HTML documentation.)
> 
> Thank you in advance!
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From s195404 at student.uq.edu.au  Mon Jul 28 08:56:56 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon, 28 Jul 2003 06:56:56 +0000
Subject: [R] hi
In-Reply-To: <000a01c354ce$b63db780$711da4cb@q1g8b8>
References: <000a01c354ce$b63db780$711da4cb@q1g8b8>
Message-ID: <1059375416.3f24c93858794@my.uq.edu.au>

Dear Jill,

Assuming you have installed R correctly, several manuals in
PDF will have been installed. Perhaps the most useful for
you is "An Introduction to R" which answers all of your
questions (and more). If you've installed the Windows
version, you can view this by following the menus
Help->Manuals->An Introduction to R. Please work through
this tutorial there before you go much further.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting jill irwin <jillirwin at optusnet.com.au>:

> Guess what
> I have no clue what i am doing/ meant to do with R. i
> have just begun statistics for uni and all commands don't
> seem to work, they say syntax error, we are meant to put
> a hist.r in but ? please help, step by step, to use this
> program i would turn it on (it comes up rgui) is this
> wrong, have i put the wrong bit on it?
> Please help, I am so glad there is an R help 
> Thankyou 
> jill jijipowers at netscape.net
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Mon Jul 28 08:59:50 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Jul 2003 08:59:50 +0200
Subject: [R] hi
In-Reply-To: <000a01c354ce$b63db780$711da4cb@q1g8b8>
References: <000a01c354ce$b63db780$711da4cb@q1g8b8>
Message-ID: <3F24C9E6.7020908@statistik.uni-dortmund.de>

jill irwin wrote:

> Guess what
> I have no clue what i am doing/ meant to do with R.

Ask your teacher (he/she certainly already told you!), we don't know 
either....

Please use a relevant subject line!

Please tell your mailtool to break lines!


> i have just begun statistics for uni and all commands don't seem to work,

So you made a mistake for all of them!

 > they say syntax error, we are meant to put a hist.r in but ?

So why don't you ask your teacher, again (I guess you are looking for 
source(), but you are too unspecific)?
Or in order not to frustrate your teacher as you already have frustrated 
the mailing list: Why not reading at least a couple of pages of the 
manuals, beginning with "An Introduction to R" (please, do read it!)?

Uwe Ligges

 > please help, step by step, to use this program i would turn it on
 > (it comes up rgui) is this wrong, have i put the wrong bit on it?
> Please help, I am so glad there is an R help 
> Thankyou 
> jill jijipowers at netscape.net
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dunn at usq.edu.au  Mon Jul 28 09:00:32 2003
From: dunn at usq.edu.au (Peter Dunn)
Date: Mon, 28 Jul 2003 07:00:32 -0000
Subject: [R] Plotting (mixed) line types and legends
Message-ID: <1059375699.25763.53.camel@grover.sci.usq.edu.au>

Hi all

I have a question about plotting line types and
legends.  Here's a short piece of code to
demonstrate:

x <- y <- seq(1,10)
plot(x,y, type ="l", lty="33")
lines(x,y+1,  lty=1)
legend( 8,2,legend=c("lty=1","lty=\"33\""), lty=c(1,"33") )

On my system (see below), the line types in the legend
are not the same as in the plot--in particular, lty=1.
(In the plot, the  lty=1  is solid as expected; in the
legend, it is dots close together.)

Is this because I shouldn't mix different ways of defining
line types in the legend?  (ie stick to lty=c(1,2) only, 
for example)?  If so, I couldn't find it documented.

Any explanation appreciated.  If anyone know hows to
do such a thing (ie define different line types in the
legend), I'd love to hear.  (And yes, I need to do it--the
example above was a short snippet that just demos the problem.)

P.

My system:

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.0
year     2003
month    04
day      16
language R



-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
  Web:    http://www.sci.usq.edu.au/staff/dunn
  Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From jasont at indigoindustrial.co.nz  Mon Jul 28 09:11:03 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Mon, 28 Jul 2003 19:11:03 +1200
Subject: [R] Perl
In-Reply-To: <200307280141.00194.news@chasset.net>
References: <200307280141.00194.news@chasset.net>
Message-ID: <3F24CC87.6070303@indigoindustrial.co.nz>

Pierre-Olivier Chasset wrote:
> Dear Helpers,
> 
> I would like to know if I can call other use R-method within Perl scripts.
> I have not found any module in CPAN (Perl Archive).
> Thanks,
> 

The server's not responding right now, but check out RSPerl on Omegahat:
http://www.omegahat.org

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From Richard.Rowe at jcu.edu.au  Mon Jul 28 09:01:01 2003
From: Richard.Rowe at jcu.edu.au (Richard Rowe)
Date: Mon, 28 Jul 2003 17:01:01 +1000
Subject: [R] hi [should be 'getting started']
In-Reply-To: <000a01c354ce$b63db780$711da4cb@q1g8b8>
Message-ID: <5.0.0.25.1.20030728164222.02bb8d70@pop.jcu.edu.au>

At 16:08 28/07/03 +1000, you wrote:
>Guess what
>I have no clue what i am doing/ meant to do with R. i have just begun 
>statistics for uni and all commands don't seem to work, they say syntax 
>error, we are meant to put a hist.r in but ? please help, step by step, to 
>use this program i would turn it on (it comes up rgui) is this wrong, have 
>i put the wrong bit on it?
>Please help, I am so glad there is an R help
>Thankyou
>jill jijipowers at netscape.net

Go to the R project home site
http://www.r-project.org/

In the left hand column find the nearest CRAN mirror and go there (in your 
case probably http://mirror.aarnet.edu.au/pub/CRAN/ )

In the left hand column find Documentation:  Manual

now download and read the file 'An introduction to R'
(This should also be in your distributed copy ... but)

then again under 'Documentation' go to 'Contributed'

Download and read the various documentations.  I suggest you start with 
John Maindonald's 'course'

That should be a couple of evening's reading and put you well on your way.

You may get a blasting for your rather uninformative subject line ... and a 
little lurking or a visit to the archives would have got you moving.  This 
is actually a very friendly and supportive list, but the most valuable 
sources of information are very busy and sometimes somewhat brusque.

This is information your lecturer/class coordinator should have provided 
you with ...





Richard Rowe
Senior Lecturer
Department of Zoology and Tropical Ecology, James Cook University
Townsville, Queensland 4811, Australia
fax (61)7 47 25 1570
phone (61)7 47 81 4851
e-mail: Richard.Rowe at jcu.edu.au
http://www.jcu.edu.au/school/tbiol/zoology/homepage.html



From ripley at stats.ox.ac.uk  Mon Jul 28 09:26:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 08:26:40 +0100 (BST)
Subject: [R] Plotting (mixed) line types and legends
In-Reply-To: <1059375699.25763.53.camel@grover.sci.usq.edu.au>
Message-ID: <Pine.LNX.4.44.0307280815130.6786-100000@gannet.stats>

On 28 Jul 2003, Peter Dunn wrote:

> I have a question about plotting line types and
> legends.  Here's a short piece of code to
> demonstrate:
> 
> x <- y <- seq(1,10)
> plot(x,y, type ="l", lty="33")
> lines(x,y+1,  lty=1)
> legend( 8,2,legend=c("lty=1","lty=\"33\""), lty=c(1,"33") )
> 
> On my system (see below), the line types in the legend
> are not the same as in the plot--in particular, lty=1.
> (In the plot, the  lty=1  is solid as expected; in the
> legend, it is dots close together.)
> 
> Is this because I shouldn't mix different ways of defining
> line types in the legend?  (ie stick to lty=c(1,2) only, 
> for example)?  If so, I couldn't find it documented.

Yes.  You have actually use lty=c("1", "33") by the coercion rules: the
argument you passed has to be one mode and that mode is character.
lty=c("solid", "33") works.

Whether lty="1" should work is a moot point: it seems device-dependent.
It appears to work on postscript() and windows(), so looks like a bug in 
the X11() device (which I presume you used, although you did not say so).
On the other hand, one could argue it is a bug that it works anywhere
as it is not as specified in ?par.

> Any explanation appreciated.  If anyone know hows to
> do such a thing (ie define different line types in the
> legend), I'd love to hear.  (And yes, I need to do it--the
> example above was a short snippet that just demos the problem.)

You have not demonstrated that, and I believe you could always use the 
character-string form of lty.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 28 09:29:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 08:29:43 +0100 (BST)
Subject: [R] Perl
In-Reply-To: <3F24CC87.6070303@indigoindustrial.co.nz>
Message-ID: <Pine.LNX.4.44.0307280827150.6786-100000@gannet.stats>

On Mon, 28 Jul 2003, Jason Turner wrote:

> Pierre-Olivier Chasset wrote:
> > Dear Helpers,
> > 
> > I would like to know if I can call other use R-method within Perl scripts.
> > I have not found any module in CPAN (Perl Archive).
> > Thanks,
> > 
> 
> The server's not responding right now, but check out RSPerl on Omegahat:
> http://www.omegahat.org

[We know: it's hosted on cvs.r-project.org.]

There's a mirror of the software in the Omegahat area on CRAN. (but
cran.us.r-project.org is the same machine as cvs.r-project.org).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.pagel at gsf.de  Mon Jul 28 09:31:59 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Mon, 28 Jul 2003 09:31:59 +0200
Subject: [R] hi
In-Reply-To: <000a01c354ce$b63db780$711da4cb@q1g8b8>
References: <000a01c354ce$b63db780$711da4cb@q1g8b8>
Message-ID: <20030728073159.GA2196@porcupine.gsf.de>

	
	Hi!

> I have no clue what i am doing/ meant to do with R. i have just begun
> statistics for uni and all commands don't seem to work, they say
> syntax error, we are meant to put a hist.r in but ?

I guess you should have a look at the manuals. You can find them at the
R-Project web site:

http://www.r-project.org

In addition to the official manuals I found the "Simple R" manual very
helpful. It's under contributed documentation.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From pburns at pburns.seanet.com  Mon Jul 28 11:26:22 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 28 Jul 2003 10:26:22 +0100
Subject: [R] hi
References: <000a01c354ce$b63db780$711da4cb@q1g8b8>
Message-ID: <3F24EC3E.5030805@pburns.seanet.com>

If you are the impatient type, you might want to have a
look at "A Guide for the Unwilling S User".  It is in the
contributed documentation on the R website and on my
company website.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

jill irwin wrote:

>Guess what
>I have no clue what i am doing/ meant to do with R. i have just begun statistics for uni and all commands don't seem to work, they say syntax error, we are meant to put a hist.r in but ? please help, step by step, to use this program i would turn it on (it comes up rgui) is this wrong, have i put the wrong bit on it?
>Please help, I am so glad there is an R help 
>Thankyou 
>jill jijipowers at netscape.net
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From baron at psych.upenn.edu  Mon Jul 28 14:18:09 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 28 Jul 2003 08:18:09 -0400
Subject: [R] multiple imputation with fit.mult.impute in Hmisc
In-Reply-To: <20030727222010.02eeced6.fharrell@virginia.edu>
References: <20030727184730.GA3255@mail1.sas.upenn.edu>
	<20030727222010.02eeced6.fharrell@virginia.edu>
Message-ID: <20030728121809.GA3543@mail1.sas.upenn.edu>

Thanks for the quick reply!  One more question, below.

On 07/27/03 22:20, Frank E Harrell Jr wrote:
>On Sun, 27 Jul 2003 14:47:30 -0400
>Jonathan Baron <baron at psych.upenn.edu> wrote:
>
>> I have always avoided missing data by keeping my distance from
>> the real world.  But I have a student who is doing a study of
>> real patients.  We're trying to test regression models using
>> multiple imputation.  We did the following (roughly):
>> 
>> f <- aregImpute(~ [list of 32 variables, separated by + signs],
>>  n.impute=20, defaultLinear=T, data=t1)
>> # I read that 20 is better than the default of 5.
>> # defaultLinear makes sense for our data.
>> 
>> fmp <- fit.mult.impute(Y ~ X1 + X2 ... [for the model of interest],
>>  xtrans=f, fitter=lm, data=t1)
>> 
>> and all goes well (usually) except that we get the following
>> message at the end of the last step:
>> 
>>  Warning message: Not using a Design fitting function;
>>  summary(fit) will use standard errors, t, P from last imputation
>>  only.  Use Varcov(fit) to get the correct covariance matrix,
>>  sqrt(diag(Varcov(fit))) to get s.e.
>> 
>> I did try using sqrt(diag(Varcov(fmp))), as it suggested, and it
>> didn't seem to change anything from when I did summary(fmp).
>> 
>> But this Warning message sounds scary.  It sounds like the whole
>> process of multiple imputation is being ignored, if only the last
>> one is being used.
>
>The warning message may be ignored.  But the advice to use Varcov(fmp) is faulty for 
>lm fits - I will fix that in the next release of Hmisc.  You may get the 
>imputation-corrected covariance matrix for now using fmp$var

Then it seems to me that summary(fmp) is also giving incorrect
std err.r, t, and p.  Right?  It seems to use Varcof(fmp) and not
fmp$var.

>> So I discovered I could get rid of this warning by loading the
>> Design library and then using ols instead of lm as the fitter in
>> fit.mult.imput.  It seems that ols provides a variance/covariance
>> matrix (or something) that fit.mult.impute can use.
>
>That works too.

That gives me what I get if I use lm and then recalculate the t
values "by hand" from fmp$var.  Thus, ols seems like the way to
go for now, if only to avoid additional calculations.

Jon



From kjetil at entelnet.bo  Mon Jul 28 14:44:08 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 28 Jul 2003 08:44:08 -0400
Subject: [R] Rcmd nuisance (windows XP)
Message-ID: <3F24E258.19410.571FB@localhost>

Hola!

When running 
Rcmd build --binary <path-to-package>, 

the run always end with
  
     	zip warning: name not matched: DESCRIPTION

The package seems to work fine. This happens whith all packages, mine 
as well as CRAN ones. 

Whats happening?

Kjetil Halvorsen



From adiamond at fas.harvard.edu  Mon Jul 28 14:46:53 2003
From: adiamond at fas.harvard.edu (adiamond@fas.harvard.edu)
Date: Mon, 28 Jul 2003 08:46:53 -0400
Subject: [R] aregImpute: warning message re: acepack and mace
Message-ID: <1059396413.3f251b3dde563@webmail.fas.harvard.edu>

hi,

i'm trying to learn how to use aregImpute by doing the examples provided with 
the package, and after installing Hmisc.1.6-1.zip (for Windows),
and running the very first example on R 1.7.1, i get an error message warning 
me about "mace" (see below) and acepack.  

i found the acepack package, but its filename ends in tar.gz
and i'm finding it difficult to open (because its designed for Unix, i guess).
is there an acepack.zip file?  is there any way that i can play
with aregImpute on my Windows system?

thank you for your help,

alexis

> # Multiple imputation and estimation of variances and covariances of
> # regression coefficient estimates accounting for imputation
> # Example 1: large sample size, much missing data, no overlap in
.
.
.
> # Use 100 imputations to better check against individual true values
> f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)

Loading required package: acepack 
Iteration:1 Error in .Fortran("mace", p = as.integer(ncol(x)), n = as.integer
(nrow(x)),  : 
        C/Fortran function name not in load table
In addition: Warning message: 
There is no package called 'acepack' in: library(package, character.only = 
TRUE, logical = TRUE, warn.conflicts = warn.conflicts,



From M.Mamin at intershop.de  Mon Jul 28 15:01:18 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Mon, 28 Jul 2003 15:01:18 +0200
Subject: [R] http://www.omegahat.org/RSXML
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF071@jena03.net.j.ad.intershop.net>


Hi,

As the server www.omegahat.org seems to be down since quite a while, 
could someone send me the RSXML library for Windows2000 ?

Many thanks,

Marc Mamin



From ripley at stats.ox.ac.uk  Mon Jul 28 15:11:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 14:11:58 +0100 (BST)
Subject: [R] Rcmd nuisance (windows XP)
In-Reply-To: <3F24E258.19410.571FB@localhost>
Message-ID: <Pine.LNX.4.44.0307281406240.9126-100000@gannet.stats>

On Mon, 28 Jul 2003, kjetil brinchmann halvorsen wrote:

> Hola!
> 
> When running 
> Rcmd build --binary <path-to-package>, 
> 
> the run always end with
>   
>      	zip warning: name not matched: DESCRIPTION
> 
> The package seems to work fine. This happens whith all packages, mine 
> as well as CRAN ones. 
> 
> Whats happening?

Adding versioned installs broke many things, and this is a symptom.
Which version of R is this?  I think you will find it is fixed in R-devel
and R-patched, and that you can ignore the warning in 1.7.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vincent.stoliaroff at sgcib.com  Mon Jul 28 15:16:28 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Mon, 28 Jul 2003 15:16:28 +0200
Subject: [R] defining and plotting functions thanks to equation
Message-ID: <OF132A93DD.2EB5F5FA-ONC1256D71.0048549C@ges.marc.societe-generale.fr>

Hi R lovers!

Are there any means to define and plot a function given the equation that
specifies the function?

For example I'd like to plot and work with the Gumbel Distribution density
defined by
Lambda(x)=exp(-exp(-x))

My question may appear very simple but I haven't got an idea yet about how
to do that. I could plot something with x a vector/set of value but I don't
know how to proceed with x belonging to a continuous interval

Thanks for any help.




******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Jul 28 15:16:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 14:16:53 +0100 (BST)
Subject: [R] aregImpute: warning message re: acepack and mace
In-Reply-To: <1059396413.3f251b3dde563@webmail.fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0307281412140.9126-100000@gannet.stats>

On Mon, 28 Jul 2003 adiamond at fas.harvard.edu wrote:

> i'm trying to learn how to use aregImpute by doing the examples provided with 
> the package, and after installing Hmisc.1.6-1.zip (for Windows),
> and running the very first example on R 1.7.1, i get an error message warning 
> me about "mace" (see below) and acepack.  
> 
> i found the acepack package, but its filename ends in tar.gz
> and i'm finding it difficult to open (because its designed for Unix, i guess).

No, because those are the sources for all platforms.

> is there an acepack.zip file?  is there any way that i can play
> with aregImpute on my Windows system?

You didn't look in the right place: it's described in rw-FAQ.  (Hint: it 
can all be done from the menus.  Hint2: Hmisc should be installed in the 
same way.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 28 15:22:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 14:22:43 +0100 (BST)
Subject: [R] http://www.omegahat.org/RSXML
In-Reply-To: <770E451830D96B4D84747B54665DA1B202DAF071@jena03.net.j.ad.intershop.net>
Message-ID: <Pine.LNX.4.44.0307281420110.9126-100000@gannet.stats>

On Mon, 28 Jul 2003, Marc Mamin wrote:

> As the server www.omegahat.org seems to be down since quite a while, 

only since Sat morning: don't be impatient.  (And this is not the omegahat
list.)

> could someone send me the RSXML library for Windows2000 ?

It's not there anyway.  If you want it for R, look in the ReadMe in the 
appropriate contributed packages section on CRAN, which tells you where to 
find it.  Or get the sources (which are mirrored on CRAN) and compile it 
yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jul 28 15:28:39 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Jul 2003 15:28:39 +0200
Subject: [R] aregImpute: warning message re: acepack and mace
In-Reply-To: <1059396413.3f251b3dde563@webmail.fas.harvard.edu>
References: <1059396413.3f251b3dde563@webmail.fas.harvard.edu>
Message-ID: <3F252507.7070003@statistik.uni-dortmund.de>

adiamond at fas.harvard.edu wrote:
> hi,
> 
> i'm trying to learn how to use aregImpute by doing the examples provided with 
> the package, and after installing Hmisc.1.6-1.zip (for Windows),

Hmisc_2.0-0 is current.

use e.g. update.packages() to update to the recent version.

> and running the very first example on R 1.7.1, i get an error message warning 
> me about "mace" (see below) and acepack.  
> 
> i found the acepack package, but its filename ends in tar.gz
> and i'm finding it difficult to open (because its designed for Unix, i guess).
> is there an acepack.zip file?  is there any way that i can play
> with aregImpute on my Windows system?

So also update those packages. If they are not already installed, use 
install.packages() to install them.

The binary packages for R-1.7.x can be found at 
CRAN/bin/windows/contrib/1.7 , if you tend to install manually.

Uwe Ligges


> thank you for your help,
> 
> alexis
> 
> 
>># Multiple imputation and estimation of variances and covariances of
>># regression coefficient estimates accounting for imputation
>># Example 1: large sample size, much missing data, no overlap in
> 
> .
> .
> .
> 
>># Use 100 imputations to better check against individual true values
>>f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)
> 
> 
> Loading required package: acepack 
> Iteration:1 Error in .Fortran("mace", p = as.integer(ncol(x)), n = as.integer
> (nrow(x)),  : 
>         C/Fortran function name not in load table
> In addition: Warning message: 
> There is no package called 'acepack' in: library(package, character.only = 
> TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From a.stephenson at lancaster.ac.uk  Mon Jul 28 15:41:13 2003
From: a.stephenson at lancaster.ac.uk (Alec Stephenson)
Date: Mon, 28 Jul 2003 14:41:13 +0100 (BST)
Subject: [R] defining and plotting functions thanks to equation
In-Reply-To: <OF132A93DD.2EB5F5FA-ONC1256D71.0048549C@ges.marc.societe-generale.fr>
Message-ID: <Pine.GSO.4.21.0307281433450.9786-100000@cent1.lancs.ac.uk>



The cdf of the Gumbel distribution is exp(-exp(-x)), not the density.
The easiest way to plot the density is probably to download package evd
and use


library(evd)
plot(dgumbel, -2, 5)


Alec Stephenson                            tel +44 (0) 1524 593950
Department of Mathematics and Statistics   fax +44 (0) 1524 592681
Lancaster University                  a.stephenson at lancaster.ac.uk
Lancaster LA1 4YF          http://www.maths.lancs.ac.uk/~stephena/


On Mon, 28 Jul 2003 vincent.stoliaroff at sgcib.com wrote:

> Hi R lovers!
> 
> Are there any means to define and plot a function given the equation that
> specifies the function?
> 
> For example I'd like to plot and work with the Gumbel Distribution density
> defined by
> Lambda(x)=exp(-exp(-x))
> 
> My question may appear very simple but I haven't got an idea yet about how
> to do that. I could plot something with x a vector/set of value but I don't
> know how to proceed with x belonging to a continuous interval
> 
> Thanks for any help.
> 
> 
> 
> 
> ******************************************************************
> The sender's email address has changed to 
> firstname.lastname@ sgcib.com. You may want to update your 
> personal address book. Please see http://www.sgcib.com for more 
> information.
>                                **
> This message and any attachments (the "message") are confide...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From M.Mamin at intershop.de  Mon Jul 28 15:45:01 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Mon, 28 Jul 2003 15:45:01 +0200
Subject: [R] http://www.omegahat.org/RSXML
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF072@jena03.net.j.ad.intershop.net>

Thank you for your information 
(R is very new for me...)

I've got the package installed by now.


Marc Mamin

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Monday, July 28, 2003 3:23 PM
To: Marc Mamin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] http://www.omegahat.org/RSXML


On Mon, 28 Jul 2003, Marc Mamin wrote:

> As the server www.omegahat.org seems to be down since quite a while, 

only since Sat morning: don't be impatient.  (And this is not the omegahat
list.)

> could someone send me the RSXML library for Windows2000 ?

It's not there anyway.  If you want it for R, look in the ReadMe in the 
appropriate contributed packages section on CRAN, which tells you where to 
find it.  Or get the sources (which are mirrored on CRAN) and compile it 
yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jasont at indigoindustrial.co.nz  Mon Jul 28 16:02:09 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 29 Jul 2003 02:02:09 +1200
Subject: [R] http://www.omegahat.org/RSXML
In-Reply-To: <770E451830D96B4D84747B54665DA1B202DAF071@jena03.net.j.ad.intershop.net>
References: <770E451830D96B4D84747B54665DA1B202DAF071@jena03.net.j.ad.intershop.net>
Message-ID: <3F252CE1.1080007@indigoindustrial.co.nz>

Marc Mamin wrote:
> Hi,
> 
> As the server www.omegahat.org seems to be down since quite a while, 
> could someone send me the RSXML library for Windows2000 ?
> 
> Many thanks,
> 
> Marc Mamin
> 

According to Prof. Brian D. Ripley, earlier today...
 > There's a mirror of the software in the Omegahat area on CRAN.

I just checked

ftp://stat.ethz.ch/pub/Software/CRAN/contrib/main/Omegahat

and it seemed fine.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From jmc at research.bell-labs.com  Mon Jul 28 16:08:40 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Mon, 28 Jul 2003 10:08:40 -0400
Subject: [R] How to make "<-" generic?
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>	<x2isppzjn5.fsf@biostat.ku.dk>
	<x2el0dzi7d.fsf@biostat.ku.dk>
	<200307261424.48444.thomasio@cs.tu-berlin.de>
	<3F227F1A.8090402@pburns.seanet.com>
Message-ID: <3F252E68.D5C4D01E@research.bell-labs.com>

The problem with having methods for "<-" is a practical one, not one of
principle.

1.  The current code does not evaluate the <- operator as a function 
(This is less serious than it sounds, since certain "functions" are
intercepted specially in the C code for method dispatch already.)

2.  Assignments take place everywhere, in virtually every S language
function definition.  It's hard to imagine a version of method dispatch
efficient enough not to slow computation dramatically if every local
assignment had to check for methods.

Point 2 almost certainly rules out methods for <- now and for the
imaginable future.  And in fact, it's unlikely one really wants to
interfere with every single local assignment.

Aside from reality in this sense, none of the other objections are
crippling.

- why would  you want them?  Because of a need to do some checking
and/or adjustment of the object being assigned, perhaps.  For example, a
class can have a "validity" method, but this is not invoked
automatically when the object is assigned, for the same practical
efficiency reason.  One might want to force validity checking for some
class of objects.  But, again, would you really want to do this every
time the object is assigned in any function?

- not evaluating the left side of the operator.  This is no problem in
principle, since generic functions can have a signature (see
?setGeneric); if the signature omits the first argument, methods can be
dispatched without evaluating that argument.

- As Richard O'Keefe points out, replacement expressions (dim(x) <-
NULL, e.g.) are handled as simple assignments of the result of
"replacement functions" (x <- "dim<-"(x, NULL)).  In fact this means
that defining methods for "<-" would catch ALL assignments.  Replacement
functions are not methods but essentially a syntactic shorthand and a
way of defining replacement operations in a language that doesn't allow
pointers.

But given practical constraints, the methods will have to be methods for
some other operator or function, or the solution found some other way.

Whatever the solution, it will involve the software calling some
function or operator other than "<-", or using "<-" in a special way
(e.g., x <- validObject(x) if validity checking was the goal).

Regards,
 John Chambers

Patrick Burns wrote:
> 
> I think Brian's question --- what are you trying to do? -- should
> be the first order of business.
> 
> Someone can make R do just about anything, but it is better to
> do a few things very well than lots of things in a muddled way.
> 
> So.  What is the advantage of using assignment as a generic?
> I'm open-minded about there being such an advantage, but I
> don't see any right off.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Thomas Koenig wrote:
> 
> >Am Samstag, 26. Juli 2003 11:38 schrieb Peter Dalgaard BSA:
> >
> >
> >>Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> writes:
> >>
> >>
> >>>Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> >>>
> >>>
> >>>>What are you trying to do with this?  Assignment (<-) is not a
> >>>>function,
> >>>>
> >>>>
> >
> >But what the difference between <- and e.g. the function length or "[<-"? As I
> >understood in "methods" everything has a class. And R says me with is(...)
> >(hope that the results are correct):
> >< is(get("<-"))
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >< is(get("length"))
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >
> >
> >>is(get("[<-"))
> >>
> >>
> >[1] "function"         "OptionalFunction" "PossibleMethod"
> >
> >
> >>## test for the correct result of get(...) ?
> >>x <- 10
> >>is(get("x"))
> >>
> >>
> >[1] "numeric" "vector"
> >
> >
> >>## and
> >>setGeneric("<-")
> >>
> >>
> >[1] "<-"
> >Warning message:
> >"<-" is a primitive function; its generic definition is built in and
> >automatically included. in: setGeneric("<-")
> >
> >
> >
> >>>>and the language grammar does not convert a <- b into "<-"(a,
> >>>>b) (as it would with the binary operator functions).  You could call it
> >>>>that way, and then it will probably work.
> >>>>
> >>>>
> >>>Eh? Are you sure about that???
> >>>
> >>>
> >>>
> >>>>quote("<-"(a,b))
> >>>>
> >>>>
> >>>a <- b
> >>>
> >>>
> >>Adding on to this, I think the point is that assignment bypasses the
> >>usual *evaluation* rules, even though it is syntactically a binop.
> >>
> >>I think it basically has to be so: For one thing, it is kind of
> >>difficult to check for a signature match without evaluating the
> >>arguments and the left hand side of an assignment will not in general
> >>exist at that point.
> >>
> >>
> >
> >In the methods is the class "missing". Could that help?
> >for one thing : signature=c("missing","A") (only allowed for <- ?)
> >for both things: signature=c("A","B")
> >for nothing ;-) : signature=c("missing","missing")
> >
> >Thanks and Best Regards
> >
> >Thomas K?nig
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From p.dalgaard at biostat.ku.dk  Mon Jul 28 16:29:19 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 28 Jul 2003 14:29:19 -0000
Subject: [R] How to make "<-" generic?
In-Reply-To: <3F252E68.D5C4D01E@research.bell-labs.com>
References: <Pine.LNX.4.44.0307260959311.24675-100000@gannet.stats>
	<x2isppzjn5.fsf@biostat.ku.dk> <x2el0dzi7d.fsf@biostat.ku.dk>
	<200307261424.48444.thomasio@cs.tu-berlin.de>
	<3F227F1A.8090402@pburns.seanet.com>
	<3F252E68.D5C4D01E@research.bell-labs.com>
Message-ID: <x2r84abtzr.fsf@biostat.ku.dk>

John Chambers <jmc at research.bell-labs.com> writes:

> - not evaluating the left side of the operator.  This is no problem in
> principle, since generic functions can have a signature (see
> ?setGeneric); if the signature omits the first argument, methods can be
> dispatched without evaluating that argument.

Yes, but if you want to dispatch *on* the first argument, you're in
trouble. AFAI understood, that was what the orig.poster wanted to do:
If a has class "A", ensure that a <- b leaves a as class "A" even
though b has class "B".
 
> - As Richard O'Keefe points out, replacement expressions (dim(x) <-
> NULL, e.g.) are handled as simple assignments of the result of
> "replacement functions" (x <- "dim<-"(x, NULL)).  In fact this means
> that defining methods for "<-" would catch ALL assignments.  Replacement
> functions are not methods but essentially a syntactic shorthand and a
> way of defining replacement operations in a language that doesn't allow
> pointers.

[Except, as you all of course know, that this is internally optimized
so as not to be quite true. E.g. x[225000] <- 1 does not create an
extra vector of 225000 or more cells, whereas (I think) x <-
"[<-"(x,225000,1) will do so. Luke knows much more about these sorts of
madness...]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Jul 28 02:16:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 27 Jul 2003 17:16:50 -0700 (PDT)
Subject: [R] survey package
Message-ID: <Pine.A41.4.44.0307271703170.159126-100000@homer06.u.washington.edu>


Version 1.9 of the survey package, now percolating through CRAN, adds a
beta implementation of replication weights.  These can either be created
from a survey design (using BRR, JK1, or JKn schemes) or provided by the
user.  These have been tested on only a few examples so far: there seem to
be relatively few published datasets with suitable analyses.

As with earlier versions of the package, I particularly welcome feedback
on the user interface from practising survey statisticians.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From bates at stat.wisc.edu  Mon Jul 28 16:57:17 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 28 Jul 2003 14:57:17 -0000
Subject: [R] cran.us.r-project.org and www.omegahat.org back on line
Message-ID: <6rhe563di6.fsf@bates4.stat.wisc.edu>

We had two power failures Saturday morning that brought down the
machine that serves as cran.us.r-project.org and as www.omegahat.org.
The automatic reboot was unsuccessful because one of the file systems
required a manual file system check.

I was blissfully unaware of this until this morning.  Those sites are
now back online.



From dyang at nrcan.gc.ca  Mon Jul 28 17:12:53 2003
From: dyang at nrcan.gc.ca (Yang, Richard)
Date: Mon, 28 Jul 2003 11:12:53 -0400
Subject: [R] Optimization failed in fitting mixture 3-parameter Weibull
	distri bution using fitdistr()
Message-ID: <F0E0B899CB43D5118D220002A55113CF02F92762@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear All;

	I tried to use fitdistr() in the MASS library to fit a mixture
distribution of the 3-parameter Weibull, but the optimization failed.
Looking at the source code, it seems to indicate the error occurs at 
                if (res$convergence > 0) 
        stop("optimization failed").

            The procedures I tested are as following:

>w3den <- function(x, a,b,c) {c/b*((x -a)/b)^(c-1)*exp(-((x-a)/b)^c)}    #
define 3-parameter Weibull density
>w3den <- function(x, a,b,c) {c/b*((x -a)/b)^(c-1)*exp(-((x-a)/b)^c)}
>set.seed(123)
> x3 <- rweibull(100, shape = 4, scale = 100)                             #
Distribution 1
> fitdistr(x3, w3den, start= list(a = 8.8, b = 90.77, c = 3.678))      #
Fitting 3-parameter

       a            b            c     
   8.9487415   90.6667712    3.6722124 
 (15.1462445) (16.0657103) ( 0.7582376)
Warning messages: 
1: NaNs produced in: log(x) 
2: NaNs produced in: log(x)
>      x4 <- rweibull(50, shape = 3, scale = 90)                           #
Distribution 2
> fitdistr(x4, w3den, start=list(a = 5.0, b = 90, c = 3))
       a            b            c     
  -67.572244   159.020786     5.835588 
 (105.975870) (107.842846) (  4.288019)
Warning messages: 
1: NaNs produced in: log(x) 
2: NaNs produced in: log(x) 

> mweib <- function(x, p, a,b,c,a1, b1, c1) {p*(c/b*((x -
a)/b)^(c-1)*exp(-((x-a)/b)^c))+
+             +(1-p)*(c1/b1*((x -a1)/b1)^(c1-1)*exp(-((x-a1)/b1)^c1))}
# define mixture distribution
> x5 <- c(x3, x4)
> fitdistr(x5, mweib, start=list(p = 0.7, a = 8.9, b=90.77, c = 3.68, a1 =
-67.57, b1 = 159.02, c1 = 5.83))
Error in fitdistr(x5, mweib, start = list(p = 0.7, a = 8.9, b = 90.77,  : 
        optimization failed
In addition: There were 14 warnings (use warnings() to see them).

	I tested the same procedures with a mixture 2-parameter Weibull
distribution without problems. With a limited experience with the
fitdistr(), it seems to me, similar to all nonlinear or optimization
procedures,  the starting values are critical for convergence. Any
suggestions for solving the optimization problem?

	TIA

Richard Yang


Northern Forestry Centre   /	Centre de foresterie du Nord
Canadian Forest Service	   /	Service canadien des for?ts
Natural Resources Canada   /	Ressources naturelles Canada
5320-122 Street     	   /	5320, rue 122

Edmonton (Alberta) Canada
T6H 3S5



From ripley at stats.ox.ac.uk  Mon Jul 28 17:21:40 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 16:21:40 +0100 (BST)
Subject: [R] Optimization failed in fitting mixture 3-parameter Weibull
	distri bution using fitdistr()
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF02F92762@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
Message-ID: <Pine.LNX.4.44.0307281617400.18503-100000@gannet.stats>

I don't think that is the right density: haven't you forgotten I(x > a)?
So you need a constraint on a in the optimization, or at least to return 
density 0 if a >= min(x_i) (but I suspect the mle may well occur at the 
boundary).

Without that constraint you don't have a valid optimization problem.

On Mon, 28 Jul 2003, Yang, Richard wrote:

> Dear All;
> 
> 	I tried to use fitdistr() in the MASS library to fit a mixture
> distribution of the 3-parameter Weibull, but the optimization failed.
> Looking at the source code, it seems to indicate the error occurs at 
>                 if (res$convergence > 0) 
>         stop("optimization failed").
> 
>             The procedures I tested are as following:
> 
> >w3den <- function(x, a,b,c) {c/b*((x -a)/b)^(c-1)*exp(-((x-a)/b)^c)}    #
> define 3-parameter Weibull density
> >w3den <- function(x, a,b,c) {c/b*((x -a)/b)^(c-1)*exp(-((x-a)/b)^c)}
> >set.seed(123)
> > x3 <- rweibull(100, shape = 4, scale = 100)                             #
> Distribution 1
> > fitdistr(x3, w3den, start= list(a = 8.8, b = 90.77, c = 3.678))      #
> Fitting 3-parameter
> 
>        a            b            c     
>    8.9487415   90.6667712    3.6722124 
>  (15.1462445) (16.0657103) ( 0.7582376)
> Warning messages: 
> 1: NaNs produced in: log(x) 
> 2: NaNs produced in: log(x)
> >      x4 <- rweibull(50, shape = 3, scale = 90)                           #
> Distribution 2
> > fitdistr(x4, w3den, start=list(a = 5.0, b = 90, c = 3))
>        a            b            c     
>   -67.572244   159.020786     5.835588 
>  (105.975870) (107.842846) (  4.288019)
> Warning messages: 
> 1: NaNs produced in: log(x) 
> 2: NaNs produced in: log(x) 
> 
> > mweib <- function(x, p, a,b,c,a1, b1, c1) {p*(c/b*((x -
> a)/b)^(c-1)*exp(-((x-a)/b)^c))+
> +             +(1-p)*(c1/b1*((x -a1)/b1)^(c1-1)*exp(-((x-a1)/b1)^c1))}
> # define mixture distribution
> > x5 <- c(x3, x4)
> > fitdistr(x5, mweib, start=list(p = 0.7, a = 8.9, b=90.77, c = 3.68, a1 =
> -67.57, b1 = 159.02, c1 = 5.83))
> Error in fitdistr(x5, mweib, start = list(p = 0.7, a = 8.9, b = 90.77,  : 
>         optimization failed
> In addition: There were 14 warnings (use warnings() to see them).
> 
> 	I tested the same procedures with a mixture 2-parameter Weibull
> distribution without problems. With a limited experience with the
> fitdistr(), it seems to me, similar to all nonlinear or optimization
> procedures,  the starting values are critical for convergence. Any
> suggestions for solving the optimization problem?
> 
> 	TIA
> 
> Richard Yang
> 
> 
> Northern Forestry Centre   /	Centre de foresterie du Nord
> Canadian Forest Service	   /	Service canadien des for?ts
> Natural Resources Canada   /	Ressources naturelles Canada
> 5320-122 Street     	   /	5320, rue 122
> 
> Edmonton (Alberta) Canada
> T6H 3S5
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sen at biostat.ucsf.edu  Mon Jul 28 17:26:22 2003
From: sen at biostat.ucsf.edu (Saunak Sen)
Date: Mon, 28 Jul 2003 08:26:22 -0700
Subject: [R] R compilation error on Solaris
Message-ID: <20030728152622.GA353@howrahbridge.ucsf.edu>

Hello...

I tried to compile an R addon package, R/qtl on a Solaris machine and
I get this error.  I know that R/qtl complies correctly on other
platforms (Windows, Mac, Linuxes, etc), so I am wondering what the
problem might be. 

Any ideas?

Thanks,
Saunak

                -----------------------------------
/export/home/sen/tmp <howrahbridge> <pts/5> R INSTALL --library=/export/home/sen/Rlibs qtl_0.96-5.tar.gz
* Installing *source* package 'qtl' ...
** libs
gcc -I/usr/local/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c discan.c -o discan.o
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: invalid character (0x40)
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: quoted-string operand required
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: statement syntax
*** Error code 1
make: Fatal error: Command failed for target `discan.o'
ERROR: compilation failed for package 'qtl'
                -----------------------------------


-- 
Saunak Sen
Assistant Professor
Department of Epidemiology and Biostatistics
University of California San Francisco
http://www.biostat.ucsf.edu/sen



From sen at biostat.ucsf.edu  Mon Jul 28 17:26:42 2003
From: sen at biostat.ucsf.edu (Saunak Sen)
Date: Mon, 28 Jul 2003 08:26:42 -0700
Subject: [R] R compilation error on Solaris
Message-ID: <20030728152622.GA353@howrahbridge.ucsf.edu>

Hello...

I tried to compile an R addon package, R/qtl on a Solaris machine and
I get this error.  I know that R/qtl complies correctly on other
platforms (Windows, Mac, Linuxes, etc), so I am wondering what the
problem might be. 

Any ideas?

Thanks,
Saunak

                -----------------------------------
/export/home/sen/tmp <howrahbridge> <pts/5> R INSTALL --library=/export/home/sen/Rlibs qtl_0.96-5.tar.gz
* Installing *source* package 'qtl' ...
** libs
gcc -I/usr/local/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c discan.c -o discan.o
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: invalid character (0x40)
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: quoted-string operand required
/usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: statement syntax
*** Error code 1
make: Fatal error: Command failed for target `discan.o'
ERROR: compilation failed for package 'qtl'
                -----------------------------------


-- 
Saunak Sen
Assistant Professor
Department of Epidemiology and Biostatistics
University of California San Francisco
http://www.biostat.ucsf.edu/sen



From p.dalgaard at biostat.ku.dk  Mon Jul 28 17:35:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 28 Jul 2003 15:35:20 -0000
Subject: [R] R compilation error on Solaris
In-Reply-To: <20030728152622.GA353@howrahbridge.ucsf.edu>
References: <20030728152622.GA353@howrahbridge.ucsf.edu>
Message-ID: <x2n0eybqyh.fsf@biostat.ku.dk>

Saunak Sen <sen at biostat.ucsf.edu> writes:

> Hello...
> 
> I tried to compile an R addon package, R/qtl on a Solaris machine and
> I get this error.  I know that R/qtl complies correctly on other
> platforms (Windows, Mac, Linuxes, etc), so I am wondering what the
> problem might be. 
> 
> Any ideas?

The message is coming from the assembler, so this is much more likely
to be a GCC installation issue than an R one. Recent versions of GCC
(or just some of them?) want to use the GNU assembler rather than the
Solaris one.

> /export/home/sen/tmp <howrahbridge> <pts/5> R INSTALL --library=/export/home/sen/Rlibs qtl_0.96-5.tar.gz
> * Installing *source* package 'qtl' ...
> ** libs
> gcc -I/usr/local/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c discan.c -o discan.o
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: invalid character (0x40)
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: quoted-string operand required
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: statement syntax
> *** Error code 1
> make: Fatal error: Command failed for target `discan.o'
> ERROR: compilation failed for package 'qtl'
>                 -----------------------------------
> 
> 
> -- 
> Saunak Sen
> Assistant Professor
> Department of Epidemiology and Biostatistics
> University of California San Francisco
> http://www.biostat.ucsf.edu/sen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Joerg.Schaber at uv.es  Mon Jul 28 17:36:12 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Mon, 28 Jul 2003 17:36:12 +0200
Subject: [R] linear model coefficients
Message-ID: <3F2542EC.8040707@uv.es>

Hi,

I wonder if there is a possibility to avoid that R sets one level of a 
factor equal zero in a model fit.

More precisely, I want to fit a two-way unbalanced linear model: o ~ 0 + 
x + y
x is a factor with 10 levels, y is a factor with 9 levels. In order to 
get a unique solution
i set the intercept =0 and impose that sum(y)=0 i.e.
res <- lm(o ~ 0 + x + y, contrasts=list(y=("contr.sum")))

However, R keeps returning only 8 coefficients for y. I assume that 
internally one coefficient is always set to zero. But when I impose 
"contr.sum", setting one coef to zero is no longer really nessasary, 
isn't it?

So my questions is, if there is a way yo avoid that, or am I completely 
mistaken?

greetings,

joerg



From ripley at stats.ox.ac.uk  Mon Jul 28 17:39:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 16:39:19 +0100 (BST)
Subject: [R] R compilation error on Solaris
In-Reply-To: <20030728152622.GA353@howrahbridge.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0307281636180.18503-100000@gannet.stats>

Looks like a gcc/as mismatch.  You are using a version of gcc compiled on 
that machine with those exact paths (and not one using GNU as or a 
different version of Solaris)?

BTW, qtl does compile on a properly installed Solaris 8 system using gcc 
3.3: I've just checked.

On Mon, 28 Jul 2003, Saunak Sen wrote:

> Hello...
> 
> I tried to compile an R addon package, R/qtl on a Solaris machine and
> I get this error.  I know that R/qtl complies correctly on other
> platforms (Windows, Mac, Linuxes, etc), so I am wondering what the
> problem might be. 
> 
> Any ideas?
> 
> Thanks,
> Saunak
> 
>                 -----------------------------------
> /export/home/sen/tmp <howrahbridge> <pts/5> R INSTALL --library=/export/home/sen/Rlibs qtl_0.96-5.tar.gz
> * Installing *source* package 'qtl' ...
> ** libs
> gcc -I/usr/local/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c discan.c -o discan.o
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: invalid character (0x40)
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: quoted-string operand required
> /usr/ccs/bin/as: "/var/tmp//ccyMYLD1.s", line 641: error: statement syntax
> *** Error code 1
> make: Fatal error: Command failed for target `discan.o'
> ERROR: compilation failed for package 'qtl'
>                 -----------------------------------
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jul 28 17:50:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jul 2003 16:50:39 +0100 (BST)
Subject: [R] linear model coefficients
In-Reply-To: <3F2542EC.8040707@uv.es>
Message-ID: <Pine.LNX.4.44.0307281647370.18709-100000@gannet.stats>

On Mon, 28 Jul 2003, Joerg Schaber wrote:

> I wonder if there is a possibility to avoid that R sets one level of a 
> factor equal zero in a model fit.
> 
> More precisely, I want to fit a two-way unbalanced linear model: o ~ 0 + 
> x + y
> x is a factor with 10 levels, y is a factor with 9 levels. In order to 
> get a unique solution
> i set the intercept =0 and impose that sum(y)=0 i.e.
> res <- lm(o ~ 0 + x + y, contrasts=list(y=("contr.sum")))
> 
> However, R keeps returning only 8 coefficients for y. I assume that 
> internally one coefficient is always set to zero. But when I impose 
> "contr.sum", setting one coef to zero is no longer really nessasary, 
> isn't it?

No coefficient is set to 0: only 8 are defined: see the help for contr.sum 
and the exposition in chapter 6 of MASS.

> So my questions is, if there is a way yo avoid that, or am I completely 
> mistaken?

The second.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fharrell at virginia.edu  Mon Jul 28 17:56:00 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 28 Jul 2003 11:56:00 -0400
Subject: [R] multiple imputation with fit.mult.impute in Hmisc
In-Reply-To: <20030728121809.GA3543@mail1.sas.upenn.edu>
References: <20030727184730.GA3255@mail1.sas.upenn.edu>
	<20030727222010.02eeced6.fharrell@virginia.edu>
	<20030728121809.GA3543@mail1.sas.upenn.edu>
Message-ID: <20030728115600.4b7d29f2.fharrell@virginia.edu>

On Mon, 28 Jul 2003 08:18:09 -0400
Jonathan Baron <baron at psych.upenn.edu> wrote:

> Thanks for the quick reply!  One more question, below.
> 
> On 07/27/03 22:20, Frank E Harrell Jr wrote:
> >On Sun, 27 Jul 2003 14:47:30 -0400
> >Jonathan Baron <baron at psych.upenn.edu> wrote:
> >
> >> I have always avoided missing data by keeping my distance from
> >> the real world.  But I have a student who is doing a study of
> >> real patients.  We're trying to test regression models using
> >> multiple imputation.  We did the following (roughly):
> >> 
> >> f <- aregImpute(~ [list of 32 variables, separated by + signs],
> >>  n.impute=20, defaultLinear=T, data=t1)
> >> # I read that 20 is better than the default of 5.
> >> # defaultLinear makes sense for our data.
> >> 
> >> fmp <- fit.mult.impute(Y ~ X1 + X2 ... [for the model of interest],
> >>  xtrans=f, fitter=lm, data=t1)
> >> 
> >> and all goes well (usually) except that we get the following
> >> message at the end of the last step:
> >> 
> >>  Warning message: Not using a Design fitting function;
> >>  summary(fit) will use standard errors, t, P from last imputation
> >>  only.  Use Varcov(fit) to get the correct covariance matrix,
> >>  sqrt(diag(Varcov(fit))) to get s.e.
> >> 
> >> I did try using sqrt(diag(Varcov(fmp))), as it suggested, and it
> >> didn't seem to change anything from when I did summary(fmp).
> >> 
> >> But this Warning message sounds scary.  It sounds like the whole
> >> process of multiple imputation is being ignored, if only the last
> >> one is being used.
> >
> >The warning message may be ignored.  But the advice to use Varcov(fmp) is faulty for 
> >lm fits - I will fix that in the next release of Hmisc.  You may get the 
> >imputation-corrected covariance matrix for now using fmp$var
> 
> Then it seems to me that summary(fmp) is also giving incorrect
> std err.r, t, and p.  Right?  It seems to use Varcof(fmp) and not
> fmp$var.

summary is using the usual lm output, for the last fit, so it is not adjusted for multiple imputation.  Varcov(fmp) is using what summary uses because I forgot to tell Varcov.lm to look for fmp$var first.

Frank

> 
> >> So I discovered I could get rid of this warning by loading the
> >> Design library and then using ols instead of lm as the fitter in
> >> fit.mult.imput.  It seems that ols provides a variance/covariance
> >> matrix (or something) that fit.mult.impute can use.
> >
> >That works too.
> 
> That gives me what I get if I use lm and then recalculate the t
> values "by hand" from fmp$var.  Thus, ols seems like the way to
> go for now, if only to avoid additional calculations.
> 
> Jon
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From fharrell at virginia.edu  Mon Jul 28 18:12:30 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 28 Jul 2003 12:12:30 -0400
Subject: [R] Ideas for remote collaboration on statistical analyses
Message-ID: <20030728121230.115c4191.fharrell@virginia.edu>

I plan to use Sweave for most of the statistical reports I write for general statistical consulting projects.  I would like to also have a way to collaborate with remote clients using a phone and a web server.  This might involve a more incremental output process than Sweave uses.  One could, for example, use a Wiki like that from twiki.org to upload pieces of an analysis as it proceeds, having the client click the refresh button to get more graphs, tables, etc.  But this would be somewhat of a manual process that involves multiple edits or uploads.  Has anyone thought of a more automated incremental web-based collaborative analysis model?  The best solution might allow Sweave to easily be run once at the conclusion, to get a better packaged and cross-indexed final analysis report in pdf format.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From baron at psych.upenn.edu  Mon Jul 28 18:42:55 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 28 Jul 2003 12:42:55 -0400
Subject: [R] Ideas for remote collaboration on statistical analyses
In-Reply-To: <20030728121230.115c4191.fharrell@virginia.edu>
References: <20030728121230.115c4191.fharrell@virginia.edu>
Message-ID: <20030728164255.GA24087@mail1.sas.upenn.edu>

On 07/28/03 12:12, Frank E Harrell Jr wrote:
>I would like to also have a way to collaborate 
>with remote clients using a phone and a web server.

One idea - which I have yet to convince any of my many remote
collaborators to try - is to use Vnc to allow you and the
client/collaborator to view the same R session at the same time.
It isn't a web server, but it is almost as easy to use.  You can
talk on the phone and discuss what you're doing.  Vnc can, in
principle, allow both of you to enter commands on the same screen
(although I've never tried this because I've never gotten that
far).  It is cross platform.  It requires a high-speed connection
to be useful.

See http://www.realvnc.com.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From rossini at blindglobe.net  Mon Jul 28 19:04:32 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 28 Jul 2003 10:04:32 -0700
Subject: [R] Ideas for remote collaboration on statistical analyses
In-Reply-To: <20030728164255.GA24087@mail1.sas.upenn.edu> (Jonathan Baron's
	message of "Mon, 28 Jul 2003 12:42:55 -0400")
References: <20030728121230.115c4191.fharrell@virginia.edu>
	<20030728164255.GA24087@mail1.sas.upenn.edu>
Message-ID: <85k7a260rj.fsf@blindglobe.net>

Jonathan Baron <baron at psych.upenn.edu> writes:

> On 07/28/03 12:12, Frank E Harrell Jr wrote:
>>I would like to also have a way to collaborate 
>>with remote clients using a phone and a web server.
>
> One idea - which I have yet to convince any of my many remote
> collaborators to try - is to use Vnc to allow you and the
> client/collaborator to view the same R session at the same time.
> It isn't a web server, but it is almost as easy to use.  You can
> talk on the phone and discuss what you're doing.  Vnc can, in
> principle, allow both of you to enter commands on the same screen
> (although I've never tried this because I've never gotten that
> far).  It is cross platform.  It requires a high-speed connection
> to be useful.

I've used the netmeeting approach, very similar, but it suffered from
"screen drag" and slow connections.

I would think that a Wiki that would let you evaluate R code might be
a bit hazardous.  I'm assuming standard/high security precautions so
that you don't have to worry about external attacks or DOS, but the
concern would be typing things unintentionally or that get partially
hidden (html comments that the R code sees, for example), with
unintended circumstances resulting.

best,
-tony

-- 
A.J. Rossini  /  rossini at u.washington.edu  /  rossini at scharp.org
http://software.biostat.washington.edu/ UNTIL IT MOVES IN JULY.
Biomedical and Health Informatics, University of Washington
Biostatistics, HVTN/SCHARP, Fred Hutchinson Cancer Research Center.
FHCRC: 206-667-7025 (fax=4812)|Voicemail is pretty sketchy/use Email 

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From marlon at lscp.pqi.ep.usp.br  Mon Jul 28 20:08:10 2003
From: marlon at lscp.pqi.ep.usp.br (Marlon Martins dos Reis)
Date: Mon, 28 Jul 2003 15:08:10 -0300
Subject: [R] Reading a file every t seg.
Message-ID: <3F25668A.369CC2C0@lscp.pqi.ep.usp.br>

Dear all,
I'm writing a small program to read a file that receives data every time
(e.g. every 1second ). During the execution of my program I get the
message:
" Error in read.table("file.txt", header = T, sep = ";") :
        no lines available in input "
I suppose that it happens when I try to read the "file.txt" while it is
being written. To overcome this problem I have tried the following:

while(!is.numeric(try(read.table('file.txt',header =T, sep =
";")$tempo))){Sys.sleep(2)}

It seems to work, although after three times the message
" Error in read.table("file.txt", header = T, sep = ";") :
        no lines available in input "
is shown, the program stops.
Is there another way of checking if I can try to read a file?
Thanks in advance,
Reagrds,
Marlon !!!
marlon at lscp.pqi.ep.usp.br



From ggrothendieck at volcanomail.com  Mon Jul 28 20:15:23 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Mon, 28 Jul 2003 11:15:23 -0700 (PDT)
Subject: [R] cut.POSIXt - bug?
Message-ID: <20030728181523.0760F47F0@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030728/a2545c4b/attachment.pl

From liping66 at hotmail.com  Mon Jul 28 21:22:26 2003
From: liping66 at hotmail.com (liping)
Date: Mon, 28 Jul 2003 15:22:26 -0400
Subject: [R] RE: how to use R to model Reliability problem
Message-ID: <BAY1-DAV16KvHurVEib0000aa1b@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030728/d21f3e44/attachment.pl

From carlhusa at hotmail.com  Mon Jul 28 22:06:16 2003
From: carlhusa at hotmail.com (Carl Husa)
Date: Mon, 28 Jul 2003 20:06:16 +0000
Subject: [R] Eclipse
Message-ID: <BAY2-F27edPfmsLCqPV00009c1c@hotmail.com>

Is anyone aware of an R plug-in for the Eclipse IDE?



From spencer.graves at pdf.com  Mon Jul 28 22:18:08 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Jul 2003 13:18:08 -0700
Subject: [R] RE: how to use R to model Reliability problem
References: <BAY1-DAV16KvHurVEib0000aa1b@hotmail.com>
Message-ID: <3F258500.6010308@pdf.com>

Did you try "www.r-project.org" -> search -> "R site search" for 
"varcomp"?  If yes, could you please explain why that does not answer 
your question?  Please include in your reply a toy example including a 
"data.frame" statement, preferably with something closer to 6 numbers 
than 40 AND some R commands you have tried that didn't work.  This will 
make it vastly easier for someone to understand what you want and try to 
help.

I hope this helps.
Spencer Graves

liping wrote:
> Dear R users:
> 
> I am having problem with following datasets:
> 
> 4 true measurement:         2     10   15   20
> 
> 10 repeated measurements for each true measurement
> 
>                                    1.9   9.8 14.5 20.2
>                                    2.1  10   15    20.1
>                                    ....
>                                    2.1  9.9  15.1 20.3
> 
> how can I use R to model a mixed model and estimate mean/variance of measurement error and get a 95%CI for each group mean. 
>                           
> any suggestion will be appreciated.
> 
> L.P  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From macq at llnl.gov  Mon Jul 28 22:34:24 2003
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 28 Jul 2003 13:34:24 -0700
Subject: [R] Reading a file every t seg.
In-Reply-To: <3F25668A.369CC2C0@lscp.pqi.ep.usp.br>
References: <3F25668A.369CC2C0@lscp.pqi.ep.usp.br>
Message-ID: <p05210602bb4b37ccfbb9@[128.115.153.6]>

Is data being appended to the file, or is the file being overwritten 
with the new data?

For the former case, I use the $size element returned by file.info() 
to watch for changes in size, and when the size has changed, use 
pipe() and the unix tail command to get the latest data. My data, 
however, is arriving once per minute, so I can use a 5 second pause 
to make sure the data has all arrived.

For the latter case, you might be able to use one of the 'time' 
elements from file.info(). Or perhaps check how many lines the file 
has before trying to read it. If it has zero lines, don't try to 
read, keep waiting instead.

-Don

At 3:08 PM -0300 7/28/03, Marlon Martins dos Reis wrote:
>Dear all,
>I'm writing a small program to read a file that receives data every time
>(e.g. every 1second ). During the execution of my program I get the
>message:
>" Error in read.table("file.txt", header = T, sep = ";") :
>         no lines available in input "
>I suppose that it happens when I try to read the "file.txt" while it is
>being written. To overcome this problem I have tried the following:
>
>while(!is.numeric(try(read.table('file.txt',header =T, sep =
>";")$tempo))){Sys.sleep(2)}
>
>It seems to work, although after three times the message
>" Error in read.table("file.txt", header = T, sep = ";") :
>         no lines available in input "
>is shown, the program stops.
>Is there another way of checking if I can try to read a file?
>Thanks in advance,
>Reagrds,
>Marlon !!!
>marlon at lscp.pqi.ep.usp.br
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From f.calboli at ucl.ac.uk  Mon Jul 28 23:47:16 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Mon, 28 Jul 2003 22:47:16 +0100
Subject: [R] data manipulation: getting mean value every 5 rows
Message-ID: <3.0.6.32.20030728224716.028b86d0@pop-server.ucl.ac.uk>

Dear All,

I would like to ask you how to accomplish a little tricky data
manipulation. I have a large dataset, looking something like:

temp	line	cage	number
18	18	1	6678.63
18	18	1	7774.458
18	18	1	7845.902
18	18	1	9483.578
18	18	1	8983.555
18	18	1	9181.052
18	18	1	9458.696
18	18	1	8138.616
18	18	1	7981.994
18	18	1	7556.491
18	18	1	7672.137
18	18	1	6607.776
18	18	1	8383.65
18	18	1	7129.852
18	18	1	8536.667
18	18	2	8287.8
18	18	2	7924.47
18	18	2	7928.474
18	18	2	7363.157
18	18	2	7952.593
.....

I would like to create a dataframe where I get the mean values, 5 rows at a
time, of columns "number", while keeping the value in the other columns
fixed to the vaules found in the first of the 5 rows (or whatever, it's the
same for the 5 rows) so that the above would be "shrunk" to:

temp	line	cage	number	
18	18	1	8153.2246
18	18	1	8463.3698
18	18	1	7666.0164
18	18	2	7891.2988
 
Any hints?

Regards,

Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From spencer.graves at pdf.com  Mon Jul 28 23:59:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Jul 2003 14:59:04 -0700
Subject: [R] data manipulation: getting mean value every 5 rows
References: <3.0.6.32.20030728224716.028b86d0@pop-server.ucl.ac.uk>
Message-ID: <3F259CA8.1000004@pdf.com>

Have you considered "aggregate" [documented in help(aggregate) or 
"www.r-project.org" -> search -> "R site search" or Venables and Ripley, 
Modern Applied Statistics with S]?

hope this helps.  spencer graves

Federico Calboli wrote:
> Dear All,
> 
> I would like to ask you how to accomplish a little tricky data
> manipulation. I have a large dataset, looking something like:
> 
> temp	line	cage	number
> 18	18	1	6678.63
> 18	18	1	7774.458
> 18	18	1	7845.902
> 18	18	1	9483.578
> 18	18	1	8983.555
> 18	18	1	9181.052
> 18	18	1	9458.696
> 18	18	1	8138.616
> 18	18	1	7981.994
> 18	18	1	7556.491
> 18	18	1	7672.137
> 18	18	1	6607.776
> 18	18	1	8383.65
> 18	18	1	7129.852
> 18	18	1	8536.667
> 18	18	2	8287.8
> 18	18	2	7924.47
> 18	18	2	7928.474
> 18	18	2	7363.157
> 18	18	2	7952.593
> .....
> 
> I would like to create a dataframe where I get the mean values, 5 rows at a
> time, of columns "number", while keeping the value in the other columns
> fixed to the vaules found in the first of the 5 rows (or whatever, it's the
> same for the 5 rows) so that the above would be "shrunk" to:
> 
> temp	line	cage	number	
> 18	18	1	8153.2246
> 18	18	1	8463.3698
> 18	18	1	7666.0164
> 18	18	2	7891.2988
>  
> Any hints?
> 
> Regards,
> 
> Federico Calboli
> 
> =========================
> 
> Federico C.F. Calboli
> 
> Department of Biology
> University College London
> Room 327
> Darwin Building
> Gower Street
> London
> WClE 6BT
> 
> Tel: (+44) 020 7679 4395 
> Fax (+44) 020 7679 7096
> f.calboli at ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tplate at blackmesacapital.com  Tue Jul 29 00:12:31 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 28 Jul 2003 16:12:31 -0600
Subject: [R] data manipulation: getting mean value every 5 rows
In-Reply-To: <3.0.6.32.20030728224716.028b86d0@pop-server.ucl.ac.uk>
Message-ID: <5.2.1.1.2.20030728161023.04689b48@mailhost.blackmesacapital.com>

 > x <- read.table(file("clipboard"), header=T)
 > # add an extra field to define groups of 5 sequential rows
 > x[,"code"] <- rep(seq(len=nrow(x)/5), each=5)
 > x
    temp line cage   number code
1    18   18    1 6678.630    1
2    18   18    1 7774.458    1
3    18   18    1 7845.902    1
4    18   18    1 9483.578    1
5    18   18    1 8983.555    1
6    18   18    1 9181.052    2
7    18   18    1 9458.696    2
8    18   18    1 8138.616    2
9    18   18    1 7981.994    2
10   18   18    1 7556.491    2
11   18   18    1 7672.137    3
12   18   18    1 6607.776    3
13   18   18    1 8383.650    3
14   18   18    1 7129.852    3
15   18   18    1 8536.667    3
16   18   18    2 8287.800    4
17   18   18    2 7924.470    4
18   18   18    2 7928.474    4
19   18   18    2 7363.157    4
20   18   18    2 7952.593    4
 > aggregate(x[,"number",drop=F], x[,c("temp", "line", "cage", "code")], mean)
   temp line cage code   number
1   18   18    1    1 8153.225
2   18   18    1    2 8463.370
3   18   18    1    3 7666.016
4   18   18    2    4 7891.299
 > # result has an additional column named "code" -- easily eliminated

At Monday 10:47 PM 7/28/2003 +0100, you wrote:
>Dear All,
>
>I would like to ask you how to accomplish a little tricky data
>manipulation. I have a large dataset, looking something like:
>
>temp    line    cage    number
>18      18      1       6678.63
>18      18      1       7774.458
>18      18      1       7845.902
>18      18      1       9483.578
>18      18      1       8983.555
>18      18      1       9181.052
>18      18      1       9458.696
>18      18      1       8138.616
>18      18      1       7981.994
>18      18      1       7556.491
>18      18      1       7672.137
>18      18      1       6607.776
>18      18      1       8383.65
>18      18      1       7129.852
>18      18      1       8536.667
>18      18      2       8287.8
>18      18      2       7924.47
>18      18      2       7928.474
>18      18      2       7363.157
>18      18      2       7952.593
>.....
>
>I would like to create a dataframe where I get the mean values, 5 rows at a
>time, of columns "number", while keeping the value in the other columns
>fixed to the vaules found in the first of the 5 rows (or whatever, it's the
>same for the 5 rows) so that the above would be "shrunk" to:
>
>temp    line    cage    number
>18      18      1       8153.2246
>18      18      1       8463.3698
>18      18      1       7666.0164
>18      18      2       7891.2988
>
>Any hints?
>
>Regards,
>
>Federico Calboli
>
>=========================
>
>Federico C.F. Calboli
>
>Department of Biology
>University College London
>Room 327
>Darwin Building
>Gower Street
>London
>WClE 6BT
>
>Tel: (+44) 020 7679 4395
>Fax (+44) 020 7679 7096
>f.calboli at ucl.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Tony Plate   tplate at acm.org



From f.calboli at ucl.ac.uk  Tue Jul 29 01:04:06 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Tue, 29 Jul 2003 00:04:06 +0100
Subject: [R] data manipulation: getting mean value every 5 rows
In-Reply-To: <5.2.1.1.2.20030728161023.04689b48@mailhost.blackmesacapita l.com>
References: <3.0.6.32.20030728224716.028b86d0@pop-server.ucl.ac.uk>
Message-ID: <3.0.6.32.20030729000406.028b86d0@pop-server.ucl.ac.uk>

Dear All, 

thanks for exceptional and speedy help. In particular, thanks to J. R.
Lockwood, Sue Paul, Spencer Graves, Dennis J. Murphy and Tony Plate. 

regards,

Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From wettenhall at wehi.edu.au  Tue Jul 29 02:47:21 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 29 Jul 2003 10:47:21 +1000 (EST)
Subject: [R] Tktable active cell
Message-ID: <Pine.LNX.4.44.0307291042440.11601-100000@unix28.alpha.wehi.edu.au>

Thomas Sudler <TSudler at ch.imshealth.com> wrote:
> I want to start an action when i click into a cell. Example: 
When I click
> into a cell, a message box should open with the information of 
the
> location of the cell where I clicked in. <SNIP> So I only
> need to know how to get the possition of the active cell.

Thomas,

Getting the active cell is easy, just use :
 
row <- tclvalue(tkindex(table1,"active","row"))
col <- tclvalue(tkindex(table1,"active","col"))  

But if you try to catch the left-button-click event and then 
display the cell coordinates in a message box, the problem is 
that your event handler will be run BEFORE the default event 
handler which updates the active cell, so you will get the 
PREVIOUS active cell, (and the first time you will get an 
error because there is no active cell).

The Tcl help for bind :
http://aspn.activestate.com/ASPN/docs/ActiveTcl/ActiveTcl8.4.1.0-html/tcl/TkCmd/bind.htm
suggests that you may be able to use a "+" in front of your 
event handler name so that it is "appended to any existing 
binding", but this feature may not be implemented in R Tcl/Tk 
yet, e.g. this didn't seem to work:

tkbind(table1,"<Button 1>",paste("+",.Tcl.callback(onLeftClick),sep=""))


Regards,
James



From wettenhall at wehi.edu.au  Tue Jul 29 02:49:17 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 29 Jul 2003 10:49:17 +1000 (EST)
Subject: [R] Tktable cell colors
In-Reply-To: <Pine.LNX.4.44.0307291042440.11601-100000@unix28.alpha.wehi.edu.au>
Message-ID: <Pine.LNX.4.44.0307291047330.11601-100000@unix28.alpha.wehi.edu.au>

Thomas Sudler <TSudler at ch.imshealth.com> wrote:
> color of all the cells, that's no problem. But is it also 
possible to define
> another color for different cells? (example: Cell at position 
[1,1]:red,

Thomas,

Try this:

require(tcltk)
tclRequire("Tktable")
tt <- tktoplevel()
table1 <- tkwidget(tt,"table")
tkpack(table1)
tkcmd(.Tk.ID(table1),"tag","celltag","ZeroZero","0,0")
tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
tkcmd(.Tk.ID(table1),"tag","configure","ZeroZero",bg="red")
tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",bg="blue")

Regards,
James

> Thank you very much for your answer. It works great.
> Thomas.



From wettenhall at wehi.edu.au  Tue Jul 29 02:52:28 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 29 Jul 2003 10:52:28 +1000 (EST)
Subject: [R] Tktable disable cell
In-Reply-To: <Pine.LNX.4.44.0307291047330.11601-100000@unix28.alpha.wehi.edu.au>
Message-ID: <Pine.LNX.4.44.0307291049280.11601-100000@unix28.alpha.wehi.edu.au>

Thomas Sudler <TSudler at ch.imshealth.com> wrote:
> Is it possible to "block" a cell (I mean, that you can't make 
an entry)?
> How can I program this? And how can I deactivate a cell when 
the cell is
> activated? (command?)

Thomas,

Try this.  (Note that pasting in Tcl commands to set arrays is 
not very nice.  See my email reply to your question about changing 
cell values.)

Regards,
James

require(tcltk)
tclRequire("Tktable")
tt <- tktoplevel()
table1 <- tkwidget(tt,"table",bg="white")
tkpack(table1)
tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
tkcmd(.Tk.ID(table1),"tag","celltag","ZeroTwo","0,2")
tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",state="disabled",bg="gray")
tkcmd(.Tk.ID(table1),"tag","configure","ZeroTwo",state="normal",bg="white")

.Tcl("set tclarray(0,0) Normal")
.Tcl("set tclarray(0,1) Disabled")
.Tcl("set tclarray(0,2) Normal")
tkconfigure(table1,variable="tclarray")



From wettenhall at wehi.edu.au  Tue Jul 29 02:57:05 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 29 Jul 2003 10:57:05 +1000 (EST)
Subject: [R] Tktable change cell values
In-Reply-To: <Pine.LNX.4.44.0307291049280.11601-100000@unix28.alpha.wehi.edu.au>
Message-ID: <Pine.LNX.4.44.0307291052400.11601-100000@unix28.alpha.wehi.edu.au>

Thomas Sudler <TSudler at ch.imshealth.com> wrote:
> And my last question: How can I change the value of a cell? (I 
mean not by
> clicking into the cell... I mean how I can change the value 
with a
> command).

Thomas,

Currently there is no official interface between R arrays and 
Tcl arrays, so you have to write your own.

I've sketched some ideas on :
http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html

Here's a summary.

Regards,
James

The tclArrayVar is a modified version of Peter Dalgaard's 
tclVar function.

require(tcltk)
tclRequire("Tktable")

tclArrayVar <- function()
{
    n <- evalq(TclVarCount <- TclVarCount + 1, .TkRoot$env)
    name <- paste("::RTcl", n,sep = "")
    l <- list(env = new.env())
    assign(name, NULL, envir = l$env)
    reg.finalizer(l$env, function(env) tkcmd("unset", ls(env)))
    class(l) <- "tclArrayVar"
    .Tcl(paste("set ",name,"(0,0) \"\"",sep=""))
    l
}

assign("[.tclArrayVar",
function(object, i, j) {
  tclArrayName <- ls(object$env)
  tclvalue(paste(tclArrayName,"(",i,",",j,")",sep=""))
})

assign("[<-.tclArrayVar",
function(object, i,j,value) {
  tclArrayName <- ls(object$env)
  .Tcl(paste("set ",tclArrayName,"(",i,",",j,") ",value,sep=""))  
  return(object)
})

tt <- tktoplevel()
tclArray <- tclArrayVar()
tclArrayName <- ls(tclArray$env)
table1 <- tkwidget(tt,"table",variable=tclArrayName)
tkpack(table1)

# Set/Modify the values of some cells.
tclArray[0,0] <- "Zero"
tclArray[0,1] <- "One"
tclArray[1,0] <- "Two"
tclArray[1,1] <- "Three"

# Now get the value of a cell.
tclArray[0,1]



From kjetil at entelnet.bo  Tue Jul 29 06:40:24 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Tue, 29 Jul 2003 00:40:24 -0400
Subject: [R] Problems with Rcmd build --- windows
Message-ID: <3F25C278.4018.280A00@localhost>

Hola!

I am using Rcmd build --binary ... on a windows XP system, and am 
encountering a strange problem. Rcmd claims there are syntax errors 
in one of mine .Rd files

******* Syntax error: mismatched or missing brackets in

I take "brackets" to mean [], and I have none of them in the file, 
and don't think they are needed. Rcmd prints out the offending file, 
and in the printout I can find:

 \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
normal{a factor, did you ever in life use alucinogenos,  with levels 
                \code{No} \code{Si}}             

Which is of course not what I have in my file, I have

 \item{VALU}{a factor, did you ever in life use alucinogenos,  with 
levels 
                \code{No} \code{Si}}    

This strange expansion of the braces {} does not occur any other 
place. I am running Rcmd from a shell window in Xemacs.

Any explanation of this strange happening?

Kjetil Halvorsen



From dunn at usq.edu.au  Tue Jul 29 08:23:08 2003
From: dunn at usq.edu.au (Peter Dunn)
Date: Tue, 29 Jul 2003 06:23:08 -0000
Subject: [R] Sweave: pass  scale  parameter to  includegraphics?
Message-ID: <1059459860.27633.442.camel@grover.sci.usq.edu.au>

Hi all

I'm using Sweave and find it a treat.  But one
question:

I use Sweave to create my pictures which are
automatically included into LaTeX.  For example, 
in the file  test.Snw,  I may have:

% LaTeX stuff
\begin{figure}
<<fig=true,width=5,height=5>>=
x1 <- seq(1,5, length=10)
x2 <- sin(x1)
plot(x1,x2)
@
\caption{Plot}
\end{figure}
% More LaTeX

This produces the LaTeX file test.tex with the
(example) chuck:

% LaTeX stuff
\begin{figure}
\includegraphics{test-001}
\caption{Plot}
\end{figure}
% More LaTeX stuff

But what if I want the chunk to be, for example,
   \includegraphics[scale=0.6]{test-001}
rather than the standard, no frills \includegraphics 
command?  (Why?  Because I create a picture that I
want small!  Changing  width  and  height within the
<<>>= only changes the appearance of the picture--it
alters the x11 window the plot is produced in, not
the final size in my document.)

How can I inform Sweave to create this in the
LaTeX code (not universally, but in the occasional
picture)?  More generally, how can I inform Sweave
to pass these types of parameters to the 
\includegraphics command?

I have searched archives and all the (obvious to me) 
places, but haven't found the solution.

Thanks again,

P.

-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
  Web:    http://www.sci.usq.edu.au/staff/dunn
  Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From ripley at stats.ox.ac.uk  Tue Jul 29 08:53:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jul 2003 07:53:01 +0100 (BST)
Subject: [R] Problems with Rcmd build --- windows
In-Reply-To: <3F25C278.4018.280A00@localhost>
Message-ID: <Pine.LNX.4.44.0307290746200.25234-100000@gannet.stats>

Please note (because you keep saying it): this is not Rcmd.

Rcmd runs Perl to run the Perl script build.

The perl script build --binary runs INSTALL to install your package.

INSTALL runs Rdconv, and it is Rdconv which is complaining.

So the problem is installing your package: did you try installing it from 
the sources?

On Tue, 29 Jul 2003, kjetil brinchmann halvorsen wrote:

> Hola!
> 
> I am using Rcmd build --binary ... on a windows XP system, and am 
> encountering a strange problem. Rcmd claims there are syntax errors 
> in one of mine .Rd files
> 
> ******* Syntax error: mismatched or missing brackets in
> 
> I take "brackets" to mean [], and I have none of them in the file, 
> and don't think they are needed. 

It means braces, { }.

> Rcmd prints out the offending file, 
> and in the printout I can find:
> 
>  \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
> normal{a factor, did you ever in life use alucinogenos,  with levels 
>                 \code{No} \code{Si}}             
> 
> Which is of course not what I have in my file, I have
> 
>  \item{VALU}{a factor, did you ever in life use alucinogenos,  with 
> levels 
>                 \code{No} \code{Si}}    
> 
> This strange expansion of the braces {} does not occur any other 
> place. I am running Rcmd from a shell window in Xemacs.

Yes it does!  It's a normal part of Rd conversion.

> Any explanation of this strange happening?

Yes, there are syntax errors in your .Rd file, or a bug in Rdconv (but 
unlikely as it has successfully processed many files -- is this one 
particularly large?).  Try removing bits of the .Rd file to narrow the 
problem down.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cdeclercq at nordnet.fr  Tue Jul 29 10:05:06 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Tue, 29 Jul 2003 09:05:06 +0100
Subject: [R] Sweave: pass  scale  parameter to  includegraphics?
In-Reply-To: <1059459860.27633.442.camel@grover.sci.usq.edu.au>
Message-ID: <NGBBKLJCOLPAFMJIEMHCKEJMCJAA.cdeclercq@nordnet.fr>

Peter,

To set the width of included graphics in your LaTeX document, see the Sweave
manual: http://www.ci.tuwien.ac.at/~leisch/Sweave/Sweave-manual-20030624.pdf

'It also sets the default LATEX figure width (which is
independent of the size of the generated EPS and PDF files).
The current default is \setkeys{Gin}{width=0.8\textwidth} if
you want to use another width for the figures that are
automatically generated and included by Sweave, simply add a
line similar to the one above after \begin{document}.'

Hope it helps.

Christophe
--
Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org


> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Peter Dunn
> Envoy? : mardi 29 juillet 2003 07:24
> ? : R mailing list
> Objet : [R] Sweave: pass scale parameter to includegraphics?
>
>
> Hi all
>
> I'm using Sweave and find it a treat.  But one
> question:
>
> I use Sweave to create my pictures which are
> automatically included into LaTeX.  For example,
> in the file  test.Snw,  I may have:
>
> % LaTeX stuff
> \begin{figure}
> <<fig=true,width=5,height=5>>=
> x1 <- seq(1,5, length=10)
> x2 <- sin(x1)
> plot(x1,x2)
> @
> \caption{Plot}
> \end{figure}
> % More LaTeX
>
> This produces the LaTeX file test.tex with the
> (example) chuck:
>
> % LaTeX stuff
> \begin{figure}
> \includegraphics{test-001}
> \caption{Plot}
> \end{figure}
> % More LaTeX stuff
>
> But what if I want the chunk to be, for example,
>    \includegraphics[scale=0.6]{test-001}
> rather than the standard, no frills \includegraphics
> command?  (Why?  Because I create a picture that I
> want small!  Changing  width  and  height within the
> <<>>= only changes the appearance of the picture--it
> alters the x11 window the plot is produced in, not
> the final size in my document.)
>
> How can I inform Sweave to create this in the
> LaTeX code (not universally, but in the occasional
> picture)?  More generally, how can I inform Sweave
> to pass these types of parameters to the
> \includegraphics command?
>
> I have searched archives and all the (obvious to me)
> places, but haven't found the solution.
>
> Thanks again,
>
> P.
>
> --
> Dr Peter Dunn          (USQ CRICOS No. 00244B)
>   Web:    http://www.sci.usq.edu.au/staff/dunn
>   Email:  dunn @ usq.edu.au
> Opinions expressed are mine, not those of USQ.  Obviously...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kris.nackaerts at agr.kuleuven.ac.be  Tue Jul 29 10:03:30 2003
From: kris.nackaerts at agr.kuleuven.ac.be (Kris Nackaerts)
Date: Tue, 29 Jul 2003 10:03:30 +0200
Subject: [R] Analysis of AVHRR/VGT timeseries
Message-ID: <3F262A52.5050901@agr.kuleuven.ac.be>

Dear,

Is anyone using R for the analysis of AVHRR or VGT timeseries?

We've just started to explore the capabilities of the ts package and are 
exploring FFT. Any extra resources would be handy.

Regards,

Kris

-- 
------------------------------------------------------------------------
 
 http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=krisn

 http://gloveg.kuleuven.ac.be
 
------------------------------------------------------------------------
 Minds are like parachutes, they only work when open



From p.dalgaard at biostat.ku.dk  Tue Jul 29 10:50:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 29 Jul 2003 08:50:37 -0000
Subject: [R] Tktable change cell values
In-Reply-To: <Pine.LNX.4.44.0307291052400.11601-100000@unix28.alpha.wehi.edu.au>
References: <Pine.LNX.4.44.0307291052400.11601-100000@unix28.alpha.wehi.edu.au>
Message-ID: <x2adax90fk.fsf@biostat.ku.dk>

James Wettenhall <wettenhall at wehi.edu.au> writes:

> Currently there is no official interface between R arrays and 
> Tcl arrays, so you have to write your own.
> 
> I've sketched some ideas on :
> http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html
> 
> Here's a summary.

[implementation skipped]

> tt <- tktoplevel()
> tclArray <- tclArrayVar()
> tclArrayName <- ls(tclArray$env)
> table1 <- tkwidget(tt,"table",variable=tclArrayName)
> tkpack(table1)
> 
> # Set/Modify the values of some cells.
> tclArray[0,0] <- "Zero"
> tclArray[0,1] <- "One"
> tclArray[1,0] <- "Two"
> tclArray[1,1] <- "Three"
> 
> # Now get the value of a cell.
> tclArray[0,1]

I've been working on some array stuff for 1.8.0. It doesn't quite pan
out like the above because Tktable really uses arrays in a rather
special way.

A general Tcl array is more like a Perl hash, or an R environment (or
list with non-fixed length), so you want to be able to do A[["foo"]]
<- "bar", i.e. the index can be an arbitrary string. Tktable uses the
special indices "0,0" "0,1" ... to simulate a matrix.

Since it is probably useful to interface to the full capabilities of
Tcl arrays (every time I skip something, someone comes up with a case
where it is needed...), the thing that I have done works like

 A <- tclArray()
 A[["1,1"]] <- "blah" # autoconverts to tclObj
 A[["1,1"]] # a tclObj

 A$"1,1" # also works 

The matrix-like items could be implememnted on top of this, but I'm a
little unsure precisely how, whether it would require a special class
or not, and whether one should implement the various forms of smart
indexing. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.b.pynsent at bham.ac.uk  Tue Jul 29 11:04:28 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Tue, 29 Jul 2003 10:04:28 +0100
Subject: [R] Alignment when rotating text and symbols
Message-ID: <A87CF8DC-C1A3-11D7-B2EC-003065F42152@bham.ac.uk>

I wanted to annotate some points on lines, above and below the lines.  
I thought the easiest way would be to use text() and two pos values.
However I found when the text was rotated the space and alignment 
between the point and the text did not remain constant. The following 
code illustrates the problem:
x <- c(0,10.)
y <- c(0,10.)
offset <- 3
centre <- 5
plot(x,y, xlim=range(x), ylim=range(y),type="n", xlab="",ylab="", 
main="",xaxt="n",yaxt="n")
for (i in (seq(0, 340, by=45))	)
	{
	px <- centre + cos((i*pi)/180) * offset
	py <- centre + sin((i*pi)/180) * offset
	text(px, py,
	labels=substitute(that%->%phantom(1),list(that=i)), pos=1, col="blue", 
cex=0.7, srt=i)
	text(px, py,
	labels=substitute(that%<-%phantom(1),list(that=i)), pos=3, 
col="blue",cex=0.7,srt=i)
	lines(px, py, type="p", col="black") #just for reference
	}
What have I done wrong?
In addition, on my screen, the arrow symbols do not rotate at all 
although they do on the pdf image.
(R version 1.71,  MacOS X 10.2.6)

Many thanks.
Paul

Dr. P. B. Pynsent,
Research and Teaching Centre,
Royal Orthopaedic Hospital,
Birmingham, B31 2AP, U.K.



From wettenhall at wehi.edu.au  Tue Jul 29 11:50:30 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 29 Jul 2003 19:50:30 +1000 (EST)
Subject: [R] R/Tcl arrays
In-Reply-To: <x2adax90fk.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0307291949200.20120-100000@unix28.alpha.wehi.edu.au>

Peter,

Yes.  I have a pretty limited understanding of hash-arrays
in Tcl.  I was amused to read "The Tcl War":
http://www.vanderburg.org/Tcl/war/

Certainly your more-general array interface will be much better
in the long run.

What I'm trying to provide at the moment is just a few
R/Tktable examples to help me to get my Bioinformatics
application working and to possibly help others.

When your awesome tcltk package evolves some more, I'll gladly
tidy up my code using the new R/TclTk interfaces.

Regards,
James



From d_stone711 at juno.com  Tue Jul 29 12:05:41 2003
From: d_stone711 at juno.com (David Stone)
Date: Tue, 29 Jul 2003 12:05:41 +0200
Subject: [R] CONFIDENTIAL AND URGENT
Message-ID: <200307291005.h6TA56eE024726@stat.math.ethz.ch>

From: Prince David Stone
E-mail: d_stone711 at juno.com
E-mail:d_stone71 at hotmail.com
Number:+228-902-88-44

Dear ,

I got your address from the internet Website.I am
prince David Stone the only survived son to the former
Chairman South Delta Petroleum Development Committee
Dr Douglas Stone.Some Moslem extremist murdered my
father immediately he publicly condemned the 11th
September terrorist attack on World Trade Center in
USA.

But before he finally gave up the ghost he revealed
the sum of $10 million USD he deposited with an online
bank in Spain to me.Now my life is in danger because
this moslems extremists they are still after my life
and i am now seeking refuge in the Republic of Togo,a
neigbouring country!!.They have burnt our family house
and all our known properties. So what I want from you
is to assist me in receiving this fund in your account
overseas. i will give you 15% of the total sum for
your assistance so that you will also send Letter of
Invitation for me to join you in your country, i have
gone through so many traumatic experiences. i am
urgently ! ! waiting for your response.

Please contact me on my private
number:+228-902-88-44.for more information. Thanks.

Yours faithfully,

Prince David Stone



From d_stone711 at juno.com  Tue Jul 29 12:13:18 2003
From: d_stone711 at juno.com (David Stone)
Date: Tue, 29 Jul 2003 12:13:18 +0200
Subject: [R] CONFIDENTIAL AND URGENT
Message-ID: <200307291012.h6TACgeE025312@stat.math.ethz.ch>

From: Prince David Stone
E-mail: d_stone711 at juno.com
E-mail:d_stone71 at hotmail.com
Number:+228-902-88-44

Dear ,

I got your address from the internet Website.I am
prince David Stone the only survived son to the former
Chairman South Delta Petroleum Development Committee
Dr Douglas Stone.Some Moslem extremist murdered my
father immediately he publicly condemned the 11th
September terrorist attack on World Trade Center in
USA.

But before he finally gave up the ghost he revealed
the sum of $10 million USD he deposited with an online
bank in Spain to me.Now my life is in danger because
this moslems extremists they are still after my life
and i am now seeking refuge in the Republic of Togo,a
neigbouring country!!.They have burnt our family house
and all our known properties. So what I want from you
is to assist me in receiving this fund in your account
overseas. i will give you 15% of the total sum for
your assistance so that you will also send Letter of
Invitation for me to join you in your country, i have
gone through so many traumatic experiences. i am
urgently ! ! waiting for your response.

Please contact me on my private
number:+228-902-88-44.for more information. Thanks.

Yours faithfully,

Prince David Stone



From ripley at stats.ox.ac.uk  Tue Jul 29 12:26:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jul 2003 11:26:56 +0100 (BST)
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <A87CF8DC-C1A3-11D7-B2EC-003065F42152@bham.ac.uk>
Message-ID: <Pine.LNX.4.44.0307291116060.32516-100000@gannet.stats>

On Tue, 29 Jul 2003, p.b.pynsent wrote:

> I wanted to annotate some points on lines, above and below the lines.  
> I thought the easiest way would be to use text() and two pos values.
> However I found when the text was rotated the space and alignment 
> between the point and the text did not remain constant. The following 
> code illustrates the problem:

You are rotating the text about its centre, but pos is not in the 
rotated frame, as I read it, so why should this be rotation-invariant?

Try

> plot(1:10, type="n")
> text(5,5, "a bit of text")
> text(5,5, "a bit of text", pos=1)
> text(5,5, "a bit of text", pos=1, srt=45)

to see the effect of pos and srt.  You go down then rotate, not rotate and 
then go down.

> x <- c(0,10.)
> y <- c(0,10.)
> offset <- 3
> centre <- 5
> plot(x,y, xlim=range(x), ylim=range(y),type="n", xlab="",ylab="", 
> main="",xaxt="n",yaxt="n")
> for (i in (seq(0, 340, by=45))	)
> 	{
> 	px <- centre + cos((i*pi)/180) * offset
> 	py <- centre + sin((i*pi)/180) * offset
> 	text(px, py,
> 	labels=substitute(that%->%phantom(1),list(that=i)), pos=1, col="blue", 
> cex=0.7, srt=i)
> 	text(px, py,
> 	labels=substitute(that%<-%phantom(1),list(that=i)), pos=3, 
> col="blue",cex=0.7,srt=i)
> 	lines(px, py, type="p", col="black") #just for reference
> 	}
> What have I done wrong?
> In addition, on my screen, the arrow symbols do not rotate at all 
> although they do on the pdf image.
> (R version 1.71,  MacOS X 10.2.6)

*WHICH* Mac port of R?  There are two!

The lack of rotation is a limitation of the (unspecified) graphics device
of your (unspecified) port, whichever it is: it works on Linux and
Windows, for example, so I would expect this to work on an X11 device on
the Darwin port.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jul 29 14:04:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jul 2003 13:04:00 +0100 (BST)
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <26CF0B7C-C1B9-11D7-B2EC-003065F42152@bham.ac.uk>
Message-ID: <Pine.LNX.4.44.0307291303310.3555-100000@gannet.stats>

On Tue, 29 Jul 2003, p.b.pynsent wrote:

> Dear Prof Ripley,
> Many thanks for this response. I do not understand how your example 
> addresses my problem. Basically what concerns me is the relationship 
> between pos=1 and pos=3 upon rotation, if I take your example and 
> modify it slightly to be at srt=180 I would expect to get an inverted 
> view of srt=0.
> This is not the case, the spacing between the two lines of text becomes 
> so different that at 180 degrees the text now fits within the non 
> rotated text. At intermediate values the text remains aligned to the 
> vertical of the plot rather than to the angle of rotation and so 
> becomes offset.
> 
> What I had hoped for was what I get with srt=0, rotated as whole so it 
> looked the same but was just rotated to the specified angle.

But you got what is documented, and my example addessed that.

> 
> plot(1:10, type="n")
> text(5,5, "a bit of text", pos=1)
> text(5,5, "a bit of text", pos=3)
> text(5,5, "a bit of text", pos=1, srt=180)
> text(5,5, "a bit of text", pos=3, srt=180)
> 
> Paul
> 
> On Tuesday, July 29, 2003, at 11:26  am, Prof Brian Ripley wrote:
> 
> > On Tue, 29 Jul 2003, p.b.pynsent wrote:
> >
> >> I wanted to annotate some points on lines, above and below the lines.
> >> I thought the easiest way would be to use text() and two pos values.
> >> However I found when the text was rotated the space and alignment
> >> between the point and the text did not remain constant. The following
> >> code illustrates the problem:
> >
> > You are rotating the text about its centre, but pos is not in the
> > rotated frame, as I read it, so why should this be rotation-invariant?
> >
> > Try
> >
> >> plot(1:10, type="n")
> >> text(5,5, "a bit of text")
> >> text(5,5, "a bit of text", pos=1)
> >> text(5,5, "a bit of text", pos=1, srt=45)
> >
> > to see the effect of pos and srt.  You go down then rotate, not rotate 
> > and
> > then go down.
> >
> >> x <- c(0,10.)
> >> y <- c(0,10.)
> >> offset <- 3
> >> centre <- 5
> >> plot(x,y, xlim=range(x), ylim=range(y),type="n", xlab="",ylab="",
> >> main="",xaxt="n",yaxt="n")
> >> for (i in (seq(0, 340, by=45))	)
> >> 	{
> >> 	px <- centre + cos((i*pi)/180) * offset
> >> 	py <- centre + sin((i*pi)/180) * offset
> >> 	text(px, py,
> >> 	labels=substitute(that%->%phantom(1),list(that=i)), pos=1, 
> >> col="blue",
> >> cex=0.7, srt=i)
> >> 	text(px, py,
> >> 	labels=substitute(that%<-%phantom(1),list(that=i)), pos=3,
> >> col="blue",cex=0.7,srt=i)
> >> 	lines(px, py, type="p", col="black") #just for reference
> >> 	}
> >> What have I done wrong?
> >> In addition, on my screen, the arrow symbols do not rotate at all
> >> although they do on the pdf image.
> >> (R version 1.71,  MacOS X 10.2.6)
> >
> > *WHICH* Mac port of R?  There are two!
> >
> > The lack of rotation is a limitation of the (unspecified) graphics 
> > device
> > of your (unspecified) port, whichever it is: it works on Linux and
> > Windows, for example, so I would expect this to work on an X11 device 
> > on
> > the Darwin port.
> >
> Yes this is ambiguous,sorry.  Not the Darwin port.
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From djw1005 at cam.ac.uk  Tue Jul 29 15:53:51 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Tue, 29 Jul 2003 14:53:51 +0100 (BST)
Subject: [R] A model for disease progression
In-Reply-To: <000b01c353ad$dd3fcf20$780aebd9@pc>
Message-ID: <Pine.SOL.3.96.1030729143046.10889A-100000@draco.cus.cam.ac.uk>


Thank you to the various people who have made suggestions. In particular,
reading the documentation of the addreg package has prompted me to try to
put the question differently. I would be grateful for any comments on the
following.

As I described before, I have a snapshot of a population taken at a
certain time. I am interested in an age-related disease, which progresses
healthy->A->B. (There is no recovery.) For each individual, I know their
age (in years) and the stage of the disease. There are roughly 800 cases,
with ages spanning 40 years.

Suppose I don't distinguish between stages A and B, and all I am
interested in is whether someone has the disease or not. For each
individual, I therefore have a censored observation of a "lifetime" 
random variable:
  if the individual is age t and is diseased, lifetime is in (0,t].
  if the individual is age t and is healthy, lifetime is in (t,inf)

I would like to plot a survival function for this "lifetime" random
variable. According to the documentation (for R1.7.0), the Surv function
does not let me enter left-censored intervals for non-parametric plots. 
Are there ways around this? I could simply estimate

  Prob(lifetime>t) = fraction of cases of age t who are healthy

and take this as my survival curve, but it produces a noisy plot (in
particular, the curve is not monotone). Is there a good way to get a
better estimate of the survival function?

Once I have a good way to estimate survival functions for this sort of
data, I could estimate the distribution of T1 (the time to reach stage A
or B) and of T2 (the time to reach stage B), and thereby estimate the
distribution of T2-T1 (the time to progress from stage A to stage B) by
some sort of convolution, assuming independence.

Damon.



From kjetil at entelnet.bo  Tue Jul 29 16:27:03 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Tue, 29 Jul 2003 10:27:03 -0400
Subject: [R] Problem with INSTALL packages (was: Problems with Rcmd build
	--- windows)
In-Reply-To: <Pine.LNX.4.44.0307290746200.25234-100000@gannet.stats>
References: <3F25C278.4018.280A00@localhost>
Message-ID: <3F264BF7.3924.533C3A@localhost>

On 29 Jul 2003 at 7:53, Prof Brian Ripley wrote:

Uwe Ligges thought it might be an Xemacs problem, it is nit, see 
below. I have run the scripts in different manners from different 
places, same problem. 

Also, I don't think there is a syntax problem in the file. It is 
thouroghly checked. What seems more probable, is there is a problem 
with size. The complete file have 706 lines. When I omit (almost) all 
the content from the 
\format{ ...
   \describe{ ...
     ...

part, and so reduce the file to 35 lines, the problems disappear. I 
did this reduction in parts, and each time the output of the form

\itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-

appears at about the same line number, until the file is small enough 
it disappears.

Kjetil Halvorsen

> Please note (because you keep saying it): this is not Rcmd.
> 
> Rcmd runs Perl to run the Perl script build.
> 
> The perl script build --binary runs INSTALL to install your package.
> 
> INSTALL runs Rdconv, and it is Rdconv which is complaining.
> 
> So the problem is installing your package: did you try installing it from 
> the sources?
> 
> On Tue, 29 Jul 2003, kjetil brinchmann halvorsen wrote:
> 
> > Hola!
> > 
> > I am using Rcmd build --binary ... on a windows XP system, and am 
> > encountering a strange problem. Rcmd claims there are syntax errors 
> > in one of mine .Rd files
> > 
> > ******* Syntax error: mismatched or missing brackets in
> > 
> > I take "brackets" to mean [], and I have none of them in the file, 
> > and don't think they are needed. 
> 
> It means braces, { }.
> 
> > Rcmd prints out the offending file, 
> > and in the printout I can find:
> > 
> >  \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
> > normal{a factor, did you ever in life use alucinogenos,  with levels 
> >                 \code{No} \code{Si}}             
> > 
> > Which is of course not what I have in my file, I have
> > 
> >  \item{VALU}{a factor, did you ever in life use alucinogenos,  with 
> > levels 
> >                 \code{No} \code{Si}}    
> > 
> > This strange expansion of the braces {} does not occur any other 
> > place. I am running Rcmd from a shell window in Xemacs.
> 
> Yes it does!  It's a normal part of Rd conversion.
> 
> > Any explanation of this strange happening?
> 
> Yes, there are syntax errors in your .Rd file, or a bug in Rdconv (but 
> unlikely as it has successfully processed many files -- is this one 
> particularly large?).  Try removing bits of the .Rd file to narrow the 
> problem down.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Tue Jul 29 16:35:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jul 2003 15:35:12 +0100 (BST)
Subject: [R] Problem with INSTALL packages (was: Problems with Rcmd build
	--- windows)
In-Reply-To: <3F264BF7.3924.533C3A@localhost>
Message-ID: <Pine.LNX.4.44.0307291530450.5596-100000@gannet.stats>

On Tue, 29 Jul 2003, kjetil brinchmann halvorsen wrote:

> On 29 Jul 2003 at 7:53, Prof Brian Ripley wrote:
> 
> Uwe Ligges thought it might be an Xemacs problem, it is nit, see 
> below. I have run the scripts in different manners from different 
> places, same problem. 
> 
> Also, I don't think there is a syntax problem in the file. It is 
> thouroghly checked. What seems more probable, is there is a problem 
> with size. The complete file have 706 lines. When I omit (almost) all 
> the content from the 
> \format{ ...
>    \describe{ ...
>      ...
> 
> part, and so reduce the file to 35 lines, the problems disappear. I 
> did this reduction in parts, and each time the output of the form
> 
> \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
> 
> appears at about the same line number, until the file is small enough 
> it disappears.

We've seen a similar bug before, PR#3400 (and I have seen similar problems 
with large .Rd files)  which is why I asked about the size.  Here's the 
answer:

share/perl/R/Rdconv.pm contains

$MAXLOOPS = 1000;


Just increase it.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rdiaz at cnio.es  Tue Jul 29 16:51:52 2003
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Tue, 29 Jul 2003 16:51:52 +0200
Subject: [R] Eclipse
In-Reply-To: <BAY2-F27edPfmsLCqPV00009c1c@hotmail.com>
References: <BAY2-F27edPfmsLCqPV00009c1c@hotmail.com>
Message-ID: <200307291651.52287.rdiaz@cnio.es>

Dear Carl,

I think Daniel Rosner (Daniel Rosner <rosner at cascadepg.com>), whom I met at 
DSC 2003,  was working on it, or planning to work on it.

Hope it helps,

Ram?n

On Monday 28 July 2003 22:06, Carl Husa wrote:
> Is anyone aware of an R plug-in for the Eclipse IDE?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz



From ripley at stats.ox.ac.uk  Tue Jul 29 16:35:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jul 2003 15:35:12 +0100 (BST)
Subject: [R] Problem with INSTALL packages (was: Problems with Rcmd build
	--- windows)
In-Reply-To: <3F264BF7.3924.533C3A@localhost>
Message-ID: <Pine.LNX.4.44.0307291530450.5596-100000@gannet.stats>

On Tue, 29 Jul 2003, kjetil brinchmann halvorsen wrote:

> On 29 Jul 2003 at 7:53, Prof Brian Ripley wrote:
> 
> Uwe Ligges thought it might be an Xemacs problem, it is nit, see 
> below. I have run the scripts in different manners from different 
> places, same problem. 
> 
> Also, I don't think there is a syntax problem in the file. It is 
> thouroghly checked. What seems more probable, is there is a problem 
> with size. The complete file have 706 lines. When I omit (almost) all 
> the content from the 
> \format{ ...
>    \describe{ ...
>      ...
> 
> part, and so reduce the file to 35 lines, the problems disappear. I 
> did this reduction in parts, and each time the output of the form
> 
> \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
> 
> appears at about the same line number, until the file is small enough 
> it disappears.

We've seen a similar bug before, PR#3400 (and I have seen similar problems 
with large .Rd files)  which is why I asked about the size.  Here's the 
answer:

share/perl/R/Rdconv.pm contains

$MAXLOOPS = 1000;


Just increase it.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Tue Jul 29 17:12:20 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 29 Jul 2003 16:12:20 +0100
Subject: [R] R won't connect to the internet on SUSE Linux 8.1
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C0098C@cl-exsrv1.irad.bbsrc.ac.uk>

Luke, you are an absolute genius.

For anyone who is still interested (and I doubt anyone is ;-) for some reason in SUSE an environment variable "no_proxy" is set to localhost, as described below, and that prevents R from connecting to the internet.

Thank you everyone for your responses

Regards
Mick

-----Original Message-----
From: Luke Tierney [mailto:luke at stat.uiowa.edu]
Sent: 26 July 2003 16:40
To: michael watson (IAH-C)
Cc: 'James MacDonald '; 'R-help at stat.math.ethz.ch '
Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1


On the SuSE setup I have access to it seems that for whatever reason
the environment variable no_proxy is defined as 'localhost' in the
shell initialization files /etc/SuSEconfig/csh.cshrc and
/etc/SuSEconfig/profile.  This turns use of proxy's off in the R
internals.  See if you have it defined, and if so see if something
like

	(unset no_proxy; R)

in bash or

	(unsetenv no_proxy; R)

in tcsh handles proxies properly; it seems to for me.  When no_proxy
is not defined I get

	> Sys.putenv("http_proxy"="http://foo.bar.com")
	> download.file("http://blah.blah.net/foo.R", "bar.R")
	trying URL `http://blah.blah.net/foo.R'
	unable to resolve 'foo.bar.com'.
	Error in download.file("http://blah.blah.net/foo.R", "bar.R") : 
		cannot open URL `http://blah.blah.net/foo.R'

when proxy's are disabled by no_proxy being defined the "unable to
resolve..."  line is missing.

Best,

luke

On Sat, 26 Jul 2003, michael watson (IAH-C) wrote:

> :-))
> 
> Installing Bioconductor was how it all began, so I ended up doing what you suggested (in fact I downloaded just the packages I needed as in the full tar ball, rhdf5 wouldn't compile, probably as I don't have the right hdf5 installed but thats not a problem).
> 
> BUT it just bugged me that I couldn't get R to work, and unfortunately in my frustration I appear to have upset a few people - for this I deeply apologise.  IT still remains the case that, even with http_proxy set (and HTTP_PROXY set) i get the following results:
> 
> > update.packages()
> doesn't work :-(
> 
> > update.packages(method = "wget")
> works! :-D
> 
> > options(download.file.method = "wget")
> > update.packages()
> works! :-D
> 
> > options(download.file.method = "wget")
> > source("http://www.bioconductor.org/getBioC.R")
> doesn't work :-(
> 
> > download.file("http://www.bioconductor.org/getBioC.R", destfile = getBioC.R, method = "wget")
> > source("getBioC.R")
> > getBioC(method = "wget")
> the first two commands work fine, the last doesn't and bombs out with the usual "can't connect to the internet"
> 
> OK I now have Bioconductor installed by the traditional method of downloading via Netscape and installing .tar.gz's, so I am happy, and for the record I think R is a miraculous piece of software and a testament to the wonders of free, open-source development, as is Bioconductor, both of which make my job, and life, a WHOLE lot easier.  So I thank everyone on this list who has contributed to either or both :-D  But I can't be the only one who wouldn't be curious if the above sequence of events occurred on their system.... ;-)
> 
> Have a good weekend one and all :-D
> 
> M
> 
> 
> -----Original Message-----
> From: James MacDonald
> To: michael.watson at bbsrc.ac.uk; hb at maths.lth.se
> Cc: R-help at stat.math.ethz.ch
> Sent: 25/07/03 16:58
> Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> HTTP_PROXY issues aside, if all you want to do is install Bioconductor,
> simply download the latest bioconductor_xx.tar.gz and use R CMD
> INSTALL.
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> >>> "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> 07/25/03
> 07:18AM >>>
> Hi Henrik
> 
> Thanks for your help, I really do appreciate it.
> 
> If I follow your instructions, R returns the value
> http://wwwcache.bbsrc.ac.uk:8080.  That is good and it means that
> indeed my http_proxy environment variable is set.
> 
> I have also added the lines
> 
> http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
> HTTP_PROXY=http://wwwcache.bbsrc.ac.uk:8080/
> 
> both to .Renviron in my home directory, and to /usr/lib/R/etc/Renviron
> and /usr/lib/R/etc/Renviron.site.
> 
> All to no avail... R still doesn't try to connect through my proxy
> server.  
> 
> Please, I genuinely think this is a bug in R 1.7.1 on Suse Linux 8.1.
> 
> NOW, here is a little detail I have just discovered that PROVES my
> proxy is working.  
> 
> If I do:
> 
> update.packages(method="wget")
> 
> then everything works fine.... hmmmm, but I still have a problem as the
> command I really want to run is :
> 
> source("http://wwwbioconductor.org/getBioC.R")
> 
> and source() does not accept an option 'method="wget"'....
> 
> SO... is there a way in R that I can set it up such that ALL internet
> connections from within R use method="wget" ??
> 
> Thanks
> Mick
> 
> -----Original Message-----
> From: Henrik Bengtsson [mailto:hb at maths.lth.se] 
> Sent: 25 July 2003 11:25
> To: 'michael watson (IAH-C)'
> Cc: R-help at stat.math.ethz.ch 
> Subject: RE: [R] R won't connect to the internet on SUSE Linux 8.1
> 
> 
> Could it be that you have redefined the command R in your shell such
> that the http_proxy environment variable is set in one and R is
> running
> in another? (This is just a wild guess and I am myself only running
> WinXP.) What do you get if you do
> 
> % env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/
> % R
> > Sys.getenv("http_proxy")
> 
> Also, have you considered setting http_proxy in ~/.Renviron (see
> ?.Renviron).
> 
> Cheers
> 
> Henrik Bengtsson
> Lund University
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > michael watson (IAH-C)
> > Sent: den 25 juli 2003 10:24
> > To: 'Prof Brian Ripley'
> > Cc: 'R-help at stat.math.ethz.ch' 
> > Subject: [R] R won't connect to the internet on SUSE Linux 8.1
> > 
> > 
> > Hi
> > 
> > Thanks once again for your help, I do appreciate it..... however....
> > 
> > Here is what I get with your test.... (under tcsh - i 
> > normally use bash, but I will keep everything the same)
> > 
> > users/mwatson> env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > 
> > >options(internet.info=0)
> > >update.packages()
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> > unable to connect to 'cran.r-project.org' on port 80
> > Error in download.file(url = paste(contriburl, "PACKAGES", 
> > sep = "/"), :
> > 	cannot open URL 'http://cran.r-project.org/src/contrib/PACKAGES'
> 
> > 
> > 
> > ... and THATS IT!  I don't get any "Using HTTP proxy ... " 
> > message at all, which appears to suggest that R, under SUSE 
> > Linux 8.1, is NOT PICKING up the http_proxy environment 
> > variable - this isn't something thats wrong with my proxy, 
> > that works with everything else - internet browsers, ftp 
> > clients, wget, instant messenger clients etc etc.  The 
> > problem is R, which isn't picking up that it needs to use the 
> > http_proxy environment variable.  And I apologise for being 
> > blunt, but that is an R problem, not a proxy problem!
> > 
> > Thanks for your help
> > 
> > Mick
> > 
> > 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: 24 July 2003 16:56
> > To: michael watson (IAH-C)
> > Subject: RE: [R] Your proxy seems not to work with R (was R 
> > won't connect to the internet on Linux!)
> > 
> > 
> > When I do (under tcsh)
> > 
> > env http_proxy=http://wwwcache.bbsrc.ac.uk:8080/ R
> > > options(internet.info=0)
> > > update.packages()
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES' 
> > Using HTTP proxy http://wwwcache.bbsrc.ac.uk:8080 
> > 
> > it tries to connect to your proxy (as it says) and gets no 
> > response, which is not surprising from my site.  If you get 
> > the same, your proxy is probably not behaving in the standard 
> > way (since that has been tested by many users with standard
> proxies).
> > 
> > I've changed the emphasis of the subject line to one I feel is more 
> > equitable: many, many users have counter-evidence to your original 
> > assertion, which was rather arrogant.
> > 
> > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > 
> > > Hello Professor
> > > 
> > > If you are suggesting that I am simply missing the 
> > "http://" part of 
> > > my cache URL, or that I am missing a trailing "/", then I 
> > pre-empted 
> > > this response and it still doesn't work.
> > 
> > I was suggesting that `simply' you were not reading the documentation
> 
> > correctly. 
> > 
> > > I have tried setting both http_proxy and HTTP_PROXY to all of:
> > 
> > I hope you set to *each* of these.  The first and third are 
> > documented to be incorrect, so using those was perverse.
> > 
> > > wwwcache.bbsrc.ac.uk:8080
> > > http://wwwcache.bbsrc.ac.uk:8080 
> > > wwwcache.bbsrc.ac.uk:8080/
> > > http://wwwcache.bbsrc.ac.uk:8080/ 
> > > 
> > > and I still get the same response - R cannot open the URL.
> > > 
> > > And yes, that is thw right proxy address, I copied it straight from
> 
> > > Netscape on the same computer, and Netscape connects to the 
> > internet 
> > > fine.
> > > 
> > > Thanks
> > > Mick
> > >  
> > > 
> > > -----Original Message-----
> > > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > > Sent: 24 July 2003 13:17
> > > To: michael watson (IAH-C)
> > > Cc: 'R-help at stat.math.ethz.ch '
> > > Subject: Re: [R] R won't connect to the internet on Linux!
> > > 
> > > 
> > > On Thu, 24 Jul 2003, michael watson (IAH-C) wrote:
> > > 
> > > > OK, I really am struggling with this one!  Forgive me if 
> > I am being 
> > > > stupid....
> > > 
> > > > I am running R 1.7.1 on Suse Linux 8.1.  I connect to the 
> > internet 
> > > > through a proxy so I have:
> > > > 
> > > > IAHC-LINUX03:~ # echo $http_proxy
> > > > wwwcache.bbsrc.ac.uk:8080
> > > > IAHC-LINUX03:~ # echo $HTTP_PROXY
> > > > wwwcache.bbsrc.ac.uk:8080
> > > > 
> > > > just in case ;-)
> > > > 
> > > > SO, i go into R and I get:
> > > > 
> > > > > source("http://www.bioconductor.org/getBioC.R")
> > > > unable to connect to 'www.bioconductor.org' on port 80. Error in
> 
> > > > file(file, "r") : cannot open URL 
> > > > `http://www.bioconductor.org/getBioC.R' 
> > > > 
> > > > OK so is R just not picking up my proxy setting?
> > > 
> > > Your setting is wrong, so it is being ignored.  The help page says
> 
> > > quite explicitly
> > > 
> > >       The form of `"http_proxy"' should be 
> > `"http://proxy.dom.com/"' or
> > >      `"http://proxy.dom.com:8080/"' where the port defaults 
> > to `80' and
> > >      the trailing slash may be omitted.
> > > 
> > > > It seems to be trying
> > > > port 80 on something, and I have specifically set it to 
> > port 8080 in 
> > > > my environment variables.  As far as I can see I have 
> > followed the 
> > > > reference manual suggestion, so does anyone else have one?
> > > 
> > > The problem is in your seeing, it seems.
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk 
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> 
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From kjetil at entelnet.bo  Tue Jul 29 17:50:28 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Tue, 29 Jul 2003 11:50:28 -0400
Subject: [R] Problem with INSTALL packages (was: Problems with Rcmd build
	--- windows)
In-Reply-To: <Pine.LNX.4.44.0307291530450.5596-100000@gannet.stats>
References: <3F264BF7.3924.533C3A@localhost>
Message-ID: <3F265F84.31267.9F99BA@localhost>

On 29 Jul 2003 at 15:35, Prof Brian Ripley wrote:

Thanks!

That made the trick.

Kjetil Halvorsen

> On Tue, 29 Jul 2003, kjetil brinchmann halvorsen wrote:
> 
> > On 29 Jul 2003 at 7:53, Prof Brian Ripley wrote:
> > 
> > Uwe Ligges thought it might be an Xemacs problem, it is nit, see 
> > below. I have run the scripts in different manners from different 
> > places, same problem. 
> > 
> > Also, I don't think there is a syntax problem in the file. It is 
> > thouroghly checked. What seems more probable, is there is a problem 
> > with size. The complete file have 706 lines. When I omit (almost) all 
> > the content from the 
> > \format{ ...
> >    \describe{ ...
> >      ...
> > 
> > part, and so reduce the file to 35 lines, the problems disappear. I 
> > did this reduction in parts, and each time the output of the form
> > 
> > \itemnormal-bracket1001bracket-normalVALUnormal-bracket1001bracket-
> > 
> > appears at about the same line number, until the file is small enough 
> > it disappears.
> 
> We've seen a similar bug before, PR#3400 (and I have seen similar problems 
> with large .Rd files)  which is why I asked about the size.  Here's the 
> answer:
> 
> share/perl/R/Rdconv.pm contains
> 
> $MAXLOOPS = 1000;
> 
> 
> Just increase it.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From dave at evocapital.com  Tue Jul 29 17:58:42 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 29 Jul 2003 16:58:42 +0100
Subject: [R] Sending emails from R under Windows
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063E6E@sqlsrvr.evocapital.com>

Hi

Does anyone know of any R routines to send emails from R, under Windows?
I thought about writing such a facility using the R(D)COM package to
drive e.g. MS Outlook, but I don't want to reinvent the wheel. I have
found a function Sys.mail in the library syskern, but this only works
under Unix by shelling out a mail command. 

Thanks,

David



From jasont at indigoindustrial.co.nz  Wed Jul 30 10:09:11 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 30 Jul 2003 20:09:11 +1200
Subject: [R] defining and plotting functions thanks to equation
In-Reply-To: <OF132A93DD.2EB5F5FA-ONC1256D71.0048549C@ges.marc.societe-generale.fr>
References: <OF132A93DD.2EB5F5FA-ONC1256D71.0048549C@ges.marc.societe-generale.fr>
Message-ID: <3F277D27.4020408@indigoindustrial.co.nz>

vincent.stoliaroff at sgcib.com wrote:
> Are there any means to define and plot a function given the equation that
> specifies the function?
> 
> For example I'd like to plot and work with the Gumbel Distribution density
> defined by
> Lambda(x)=exp(-exp(-x))

Like this?

curve(exp(-exp(-x)),0,10,101)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From Jan.Verbesselt at agr.kuleuven.ac.be  Wed Jul 30 10:30:35 2003
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Wed, 30 Jul 2003 10:30:35 +0200
Subject: [R] STL- TimeSeries Decomposition
Message-ID: <000101c35674$d9260b10$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear R Helpers,

Currently I'm working with the ts package of R and created a TimeSerie
from pixels extracted from satellite imagery(S10 NDVI data, 10 daily
composites). I'm trying to decompose this signal in different signals
(seasonal and trend).

When testing out the STL method is says => Only univariate timeseries
are allowed, but the current Timeserie I'm using is univariate! => The
problem is probably that this time series has to much noise so that it
consequently gives the following error.
> plot(stl(Timeserie))
Error in stl(Timeserie) : only univariate series are allowed.  I also
import the data as an ts object.

A solution would be to eliminate the noise (sensor and atmospheric) with
a filter (kalman/ holt-Winters/TsSmooth? Or FFT.) or the BISE method in
R? 

Is the BISE (Best index slope extraction) function already programmed in
R I couldn't find it?

Much appreciated,
Jan

*******************************************************
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750 
Fax: +32-16-329760
http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv



From John.Marsland at CommerzbankIB.com  Wed Jul 30 12:11:26 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Wed, 30 Jul 2003 11:11:26 +0100
Subject: [R] building packages using S4 methods
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E8A4@xmx8lonib.lonib.commerzbank.com>

I have been building a package around a sequence of S4 classes which I have
coded in separate *.R files in the "./R" subdirectory of the package.
The package builds without error, but when I load it in R I get:

Error in reconcilePropertiesAndPrototype(name, slots, prototype,
superClasses) :
	Class "xxxx" extends an undefined class ("yyyyyy"

I guess R is trying to source the *.R files in the wrong order? Since both
classes are defined in the package and work fine at the command prompt. 
Is it just that this has not been an issue before owing to R's lazy
evaluation?
Is there anything I can do about this... maybe by putting something in the
"./install.R"?

Regards,

John Marsland

using: R v1.7.1 on Windows NT 4


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From jasont at indigoindustrial.co.nz  Wed Jul 30 12:32:53 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 30 Jul 2003 22:32:53 +1200
Subject: [R] Sending emails from R under Windows
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063E6E@sqlsrvr.evocapital.com>
References: <8D0F30FE2EB3314182D4A33F738BB19D063E6E@sqlsrvr.evocapital.com>
Message-ID: <3F279ED5.2040202@indigoindustrial.co.nz>

David Khabie-Zeitoune wrote:
> Does anyone know of any R routines to send emails from R, under Windows?
> I thought about writing such a facility using the R(D)COM package to
> drive e.g. MS Outlook, but I don't want to reinvent the wheel. I have
> found a function Sys.mail in the library syskern, but this only works
> under Unix by shelling out a mail command. 

I've used R to call a perl script to handle the mailing; both the raw 
mail commands, plus the compression of attachments, mime encoding of 
plots or data files, etc.  Haven't got it handy, but (being perl) it 
should be very platform independant doing things that way.

You could run the script either as a system() command, or possibly using 
the RSperl library from Omegahat (haven't used the latter - just 
speculating).

Check out the MIME::Parser and Mail::Sender perl modules.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Wed Jul 30 12:39:29 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 30 Jul 2003 22:39:29 +1200
Subject: [R] STL- TimeSeries Decomposition
In-Reply-To: <000101c35674$d9260b10$1145210a@agr.ad10.intern.kuleuven.ac.be>
References: <000101c35674$d9260b10$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <3F27A061.3050008@indigoindustrial.co.nz>

Jan Verbesselt wrote:
> Dear R Helpers,
> 
> Currently I'm working with the ts package of R and created a TimeSerie
> from pixels extracted from satellite imagery(S10 NDVI data, 10 daily
> composites). I'm trying to decompose this signal in different signals
> (seasonal and trend).
> 
> When testing out the STL method is says => Only univariate timeseries
> are allowed, but the current Timeserie I'm using is univariate! 

stl() doesn't think so.  What is the output of dim(Timeserie) or 
ncol(Timeserie)  ?

=> The
> problem is probably that this time series has to much noise so that it
> consequently gives the following error.

If the error message is correct, that's pretty unlikely.

Have you run traceback() after the error message to find out where the 
problem occurred?

> Error in stl(Timeserie) : only univariate series are allowed.  I also
> import the data as an ts object.

How, exactly, did you import the data and convert to time series?

> A solution would be to eliminate the noise (sensor and atmospheric) with
> a filter (kalman/ holt-Winters/TsSmooth? Or FFT.) or the BISE method in
> R? 

This might be good practice, but doesn't sound like the solution to the 
problem.

> Is the BISE (Best index slope extraction) function already programmed in
> R I couldn't find it?

Never heard of it.

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Wed Jul 30 12:21:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 11:21:34 +0100 (BST)
Subject: [R] STL- TimeSeries Decomposition
In-Reply-To: <000101c35674$d9260b10$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.44.0307301118430.18454-100000@gannet.stats>

On Wed, 30 Jul 2003, Jan Verbesselt wrote:

> Dear R Helpers,
> 
> Currently I'm working with the ts package of R and created a TimeSerie
> from pixels extracted from satellite imagery(S10 NDVI data, 10 daily
> composites). I'm trying to decompose this signal in different signals
> (seasonal and trend).
> 
> When testing out the STL method is says => Only univariate timeseries
> are allowed, but the current Timeserie I'm using is univariate! => The
> problem is probably that this time series has to much noise so that it
> consequently gives the following error.
> > plot(stl(Timeserie))
> Error in stl(Timeserie) : only univariate series are allowed.  I also
> import the data as an ts object.

No, the problem *is* that the time series is a matrix, and so not 
univariate.  Try dim(Timeserie) to see.  If it has one column (as I 
suspect), you need to remove that (dim(Timeserie) <- NULL).

> A solution would be to eliminate the noise (sensor and atmospheric) with
> a filter (kalman/ holt-Winters/TsSmooth? Or FFT.) or the BISE method in
> R? 
> 
> Is the BISE (Best index slope extraction) function already programmed in
> R I couldn't find it?

I've never even heard of it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fharrell at virginia.edu  Wed Jul 30 12:48:28 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 30 Jul 2003 06:48:28 -0400
Subject: [R] Hmisc and Design on CRAN
Message-ID: <20030730064828.325db5ac.fharrell@virginia.edu>

I am pleased to announce that new versions of the Hmisc and Design packages, version 2.0-0, are on CRAN.  Thanks to Kurt Hornik (whose patience with my help files knows no bounds) and Uwe Ligges and others for making this possible.  Thanks also to Xiao Gang Fan who has generously ported Hmisc and Design to Windows multiple times before.  Hmisc is available on CRAN for Windows as well as for Unix/Linux/Mac; Design is available for Unix/Linux/Mac now and will soon be available for Windows also when Uwe has time.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From spencer.graves at pdf.com  Wed Jul 30 13:47:58 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Jul 2003 04:47:58 -0700
Subject: [R] defining and plotting functions thanks to equation
References: <OF132A93DD.2EB5F5FA-ONC1256D71.0048549C@ges.marc.societe-generale.fr>
	<3F277D27.4020408@indigoindustrial.co.nz>
Message-ID: <3F27B06E.2010206@pdf.com>

Also:

	 plot(function(x)exp(-exp(-x)),0,10)

Since I also use S-Plus 6.1, I tried both Jason's solution and the above 
there;  both failed.  The following worked

	 x <- seq(0, 10, length=101)
	 plot(x, exp(-exp(-x)), type="l")

hope this helps.  spencer graves

Jason Turner wrote:
> vincent.stoliaroff at sgcib.com wrote:
> 
>> Are there any means to define and plot a function given the equation that
>> specifies the function?
>>
>> For example I'd like to plot and work with the Gumbel Distribution 
>> density
>> defined by
>> Lambda(x)=exp(-exp(-x))
> 
> 
> Like this?
> 
> curve(exp(-exp(-x)),0,10,101)
> 
> Cheers
> 
> Jason



From v_bill_pikounis at merck.com  Wed Jul 30 13:51:09 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Wed, 30 Jul 2003 07:51:09 -0400
Subject: [R] Sending emails from R under Windows
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0197D521@usrymx18.merck.com>

David,
I second Jason's good suggestion, with the additional comment that if you
have Outlook on the same machine, a short VBScript through the Windows
Cscript / Wscript facility should be another sufficient alternative to
consider. An example of this can be found at
http://www.mindspring.com/~tflynn/excelvba2.html#Send_Mail  I use something
similar to this, and even though it is written for Excel VBA, the logic
should transfer over to VBScript. And as Jason specifically says, system()
provides the interface you would need to launch the script.

Hope that helps.

Bill
----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

v_bill_pikounis at merck.com

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: Jason Turner [mailto:jasont at indigoindustrial.co.nz] 
> Sent: Wednesday, July 30, 2003 6:33 AM
> To: David Khabie-Zeitoune
> Cc: R-Help
> Subject: Re: [R] Sending emails from R under Windows
> 
> 
> David Khabie-Zeitoune wrote:
> > Does anyone know of any R routines to send emails from R, 
> under Windows?
> > I thought about writing such a facility using the R(D)COM package to
> > drive e.g. MS Outlook, but I don't want to reinvent the 
> wheel. I have
> > found a function Sys.mail in the library syskern, but this 
> only works
> > under Unix by shelling out a mail command. 
> 
> I've used R to call a perl script to handle the mailing; both the raw 
> mail commands, plus the compression of attachments, mime encoding of 
> plots or data files, etc.  Haven't got it handy, but (being perl) it 
> should be very platform independant doing things that way.
> 
> You could run the script either as a system() command, or 
> possibly using 
> the RSperl library from Omegahat (haven't used the latter - just 
> speculating).
> 
> Check out the MIME::Parser and Mail::Sender perl modules.
> 
> Cheers
> 
> Jason
> -- 
> Indigo Industrial Controls Ltd.
> 64-21-343-545
> jasont at indigoindustrial.co.nz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From ripley at stats.ox.ac.uk  Wed Jul 30 13:59:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 12:59:59 +0100 (BST)
Subject: [R] building packages using S4 methods
In-Reply-To: <8CBAA121CEB4D5118CB200508BB2BBEF0317E8A4@xmx8lonib.lonib.commerzbank.com>
Message-ID: <Pine.LNX.4.44.0307301250160.18536-100000@gannet.stats>

On Wed, 30 Jul 2003, Marsland, John wrote:

> I have been building a package around a sequence of S4 classes which I have
> coded in separate *.R files in the "./R" subdirectory of the package.
> The package builds without error, but when I load it in R I get:

[Incidentally, packages with R syntax errors `build without errors': that
is not a useful test.]

> Error in reconcilePropertiesAndPrototype(name, slots, prototype,
> superClasses) :
> 	Class "xxxx" extends an undefined class ("yyyyyy"
> 
> I guess R is trying to source the *.R files in the wrong order? Since both

Or perhaps you didn't give them names in the right order?  The files are
not in fact source-d, but they are concatenated in alphabetical order and 
the concatenated file is parsed and evaluated (at load time unless you 
used --save: see below).

`alphabetical order' is potentially locale-dependent, although we try to 
ensure that the C locale is used, I don't think this is bound to work on 
Windows.

> classes are defined in the package and work fine at the command prompt. 
> Is it just that this has not been an issue before owing to R's lazy
> evaluation?
> Is there anything I can do about this... maybe by putting something in the
> "./install.R"?

How are you building the package?  Are you using --save?  If not, that is 
probably the solution as it is highly recommended for packages using S4 
classes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From John.Marsland at CommerzbankIB.com  Wed Jul 30 14:15:11 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Wed, 30 Jul 2003 13:15:11 +0100
Subject: [R] building packages using S4 methods
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E8A5@xmx8lonib.lonib.commerzbank.com>

I can only find the option "--save" under install.
I'm using "Rcmd build --binary pkgName" on a Windows NT platform is there
anyway I can pass a similar directive?

Is there any more detailed documentation for Rcmd beyond the "Writing R
Extensions" manual or should I look at the R source code?

Thanks,

John Marsland



> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 30 July 2003 13:00
> To: Marsland, John
> Cc: 'r-help at lists.R-project.org'
> Subject: Re: [R] building packages using S4 methods
> 
> 
> On Wed, 30 Jul 2003, Marsland, John wrote:
> 
> > I have been building a package around a sequence of S4 
> classes which I have
> > coded in separate *.R files in the "./R" subdirectory of 
> the package.
> > The package builds without error, but when I load it in R I get:
> 
> [Incidentally, packages with R syntax errors `build without 
> errors': that
> is not a useful test.]
> 
> > Error in reconcilePropertiesAndPrototype(name, slots, prototype,
> > superClasses) :
> > 	Class "xxxx" extends an undefined class ("yyyyyy"
> > 
> > I guess R is trying to source the *.R files in the wrong 
> order? Since both
> 
> Or perhaps you didn't give them names in the right order?  
> The files are
> not in fact source-d, but they are concatenated in 
> alphabetical order and 
> the concatenated file is parsed and evaluated (at load time 
> unless you 
> used --save: see below).
> 
> `alphabetical order' is potentially locale-dependent, 
> although we try to 
> ensure that the C locale is used, I don't think this is bound 
> to work on 
> Windows.
> 
> > classes are defined in the package and work fine at the 
> command prompt. 
> > Is it just that this has not been an issue before owing to R's lazy
> > evaluation?
> > Is there anything I can do about this... maybe by putting 
> something in the
> > "./install.R"?
> 
> How are you building the package?  Are you using --save?  If 
> not, that is 
> probably the solution as it is highly recommended for 
> packages using S4 
> classes.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From wettenhall at wehi.edu.au  Wed Jul 30 14:38:56 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Wed, 30 Jul 2003 22:38:56 +1000 (EST)
Subject: [R] Tktable White column when WIDTH>13
In-Reply-To: <7132C4D532AAD21180C30000F6C219E301D5DDA5@IMSCHHERCX1>
Message-ID: <Pine.LNX.4.44.0307302230150.3299-100000@unix28.alpha.wehi.edu.au>

On Wed, 30 Jul 2003 TSudler at ch.imshealth.com wrote:
<SNIP>
table1 <- tkwidget(tt,"table",variable="tclArray",
rows=as.character(dim(datifram)[1]+1),
cols=as.character(dim(datifram)[2]),titlerows="1",
titlecols="3",selectmode="extended",height="27",
width="13",bg="white",state="disabled",cursor="arrow",
drawmode="single",selectmode="single",invertselected=TRUE,
xscrollcommand=function(...)tkset(xscr,...),
yscrollcommand=function(...) tkset(yscr,...))
<SNIP>
>
> When I go to the right border with the scrollbar, I see a 
> wihte column without cells and without titlerow. I could 
> determine where the problem is.
> But I don't know why this problem is. When I set the "WIDTH" 
> in tkwidget() lower than 13, this white column doesn't appear. 
> If the WIDTH is equal to 13 or higher, than the white column 
> appears. Can you help me how to eliminate
> this white column when I want to have more than 12 columns in 
> the window?
>

Thomas,

I think the Tktable geometry manager is having trouble because
you have asked it to display 13 columns on the screen and the
default column width is 10 characters in the default font.
Can you fit 130 characters across your screen easily?

Perhaps you could specify colwidth="5" or something smaller than
10 when you create the table, then later on, you could widen
some columns with :

tkcmd(.Tk.ID(table1),"width","0","10")
tkcmd(.Tk.ID(table1),"width","1","10")
...
(to widen the first 2 columns to 10 characters)

Or don't specify the width option (number of columns) in 
tkwidget(tt,"table",width = ...
Just leave it to Tktable.

Regards,
James



From ripley at stats.ox.ac.uk  Wed Jul 30 14:40:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 13:40:12 +0100 (BST)
Subject: [R] building packages using S4 methods
In-Reply-To: <8CBAA121CEB4D5118CB200508BB2BBEF0317E8A5@xmx8lonib.lonib.commerzbank.com>
Message-ID: <Pine.LNX.4.44.0307301332000.18679-100000@gannet.stats>

On Wed, 30 Jul 2003, Marsland, John wrote:

> I can only find the option "--save" under install.
> I'm using "Rcmd build --binary pkgName" on a Windows NT platform is there
> anyway I can pass a similar directive?
> 
> Is there any more detailed documentation for Rcmd beyond the "Writing R
> Extensions" manual or should I look at the R source code?

First point: Rcmd is JUST A FRONT END like R CMD on Unix: what do you want
detailed documentation on that for?  It's *build* you want to be looking
at.

Second point: build --binary is one way to distribute a fully-debugged
package, but INSTALL --build is a better way (as it allows more options).  
In either case you don't want to package up a non-functional package, do
you?

Third point: you can use install.R to provoke saving: that's described
in the help for INSTALL.

The moral is:  use R CMD/Rcmd INSTALL to install a package, and once it is
working, use R CMD/Rcmd build to make a source distribution or (on
Windows) Rcmd INSTALL --build to make a binary distribution

Unsurprisingly it is ?INSTALL that has help on the installation options.


[I presume the answer to my question was that you were not using --save,
but do please try to answer questions put to you by the helpers.]

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpugh at mango-solutions.com  Wed Jul 30 14:49:50 2003
From: rpugh at mango-solutions.com (rpugh)
Date: Wed, 30 Jul 2003 13:49:50 +0100
Subject: [R] S & R Public Training Courses in the UK
Message-ID: <000001c35699$128e5a50$9704fc3e@vsn.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030730/21601a94/attachment.pl

From rpugh at mango-solutions.com  Wed Jul 30 15:02:08 2003
From: rpugh at mango-solutions.com (Richard Pugh)
Date: Wed, 30 Jul 2003 14:02:08 +0100
Subject: [R] S & R Public Training Courses in the UK
Message-ID: <001501c3569a$ca00fac0$9704fc3e@vsn.local>

(Sorry - my poor attempts at formatting made that last email illegible!
Here's a version that makes more sense ...)

Mango Solutions are pleased to announce the following public courses in
the UK.

Course	Advanced S-PLUS Programming 
Date		29th - 31st October
Location	Oxford
URL		www.mango-solutions.com/pubadvanceds.htm

Course	R Programming 
Date		11th - 13th November
Location	London
URL		www.mango-solutions.com/pubrprogramming.htm

Course	R for S-PLUS Users
Date		14th November
Location	London
URL		www.mango-solutions.com/pubstor.htm

For more information, visit the Mango Solutions home page at
www.mango-solutions.com

Many thanks,
Rich.

Mango Solutions
Independent providers of S-PLUS and R services

Tel: (01628) 418134
Mob: (07967) 808091



From djw1005 at cam.ac.uk  Wed Jul 30 16:11:10 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Wed, 30 Jul 2003 15:11:10 +0100 (BST)
Subject: [R] Plotting a function with curve()
Message-ID: <Pine.SOL.3.96.1030730150607.12769A-100000@libra.cus.cam.ac.uk>


Why does
> curve(function(y) y^2, from=0,to=1)
not work, whereas
> myf <- function(y) y^2
> curve(myf, from=0,to=1)
work?

For the former, I get the error message
Error in curve(function(y) y^2, from = 0, to = 1) : 
	'expr' must be a function or an expression containing 'x'

I'm using R1.7.0 under Windows XP.

Damon Wischik.



From dj at research.bell-labs.com  Wed Jul 30 16:13:18 2003
From: dj at research.bell-labs.com (David James)
Date: Wed, 30 Jul 2003 10:13:18 -0400
Subject: [R] Sending emails from R under Windows
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063E6E@sqlsrvr.evocapital.com>;
	from dave@evocapital.com on Tue, Jul 29, 2003 at 04:58:42PM +0100
References: <8D0F30FE2EB3314182D4A33F738BB19D063E6E@sqlsrvr.evocapital.com>
Message-ID: <20030730101318.B23385@jessie.research.bell-labs.com>

David Khabie-Zeitoune wrote:
> Hi
> 
> Does anyone know of any R routines to send emails from R, under Windows?
> I thought about writing such a facility using the R(D)COM package to
> drive e.g. MS Outlook, but I don't want to reinvent the wheel. I have
> found a function Sys.mail in the library syskern, but this only works
> under Unix by shelling out a mail command. 
> 
> Thanks,
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

The R function sendEmail() below uses the RDCOMClient package
to control Outlook or Echange -- more precisely, it interfaces to
Microsoft's Collaboration Data Objects (CDO).  (Please note that 
I don't use Windows for email, so I couldn't test the function.)

> sendEmail(ema = "r-help at r-project.org", 
            name = "R-help mailing list",
            subject = "How to send Email from R using the RDCOMClient"
            msgBody = "here is the body of the message")

The package RDCOMClient is available at http://www.omegahat.org/RDCOMClient.

"sendEmail" <-
function(ema, name, subject, msgBody, deliverNow = TRUE)
{
   require(RDCOMClient)

   ema <- paste("SMPT:", ema, sep="")   ## prepend protocol to address

   ## create an e-mail session 
   session <- COMCreate("Mapi.Session") 
   session$Logon()

   ## add a message to the outbox collection of messages
   outbox <- session[["Outbox"]]
   msg <- outbox[["Messages"]]$Add(subject, msgBody)

   ## add recipient's name  (TODO: addMultiple() or loop, if many recipients)
   msg[["Recipients"]]$Add(name, ema) 
   msg$Send()
   if(deliverNow)
      msg$DeliverNow()

   session$Logoff()   ## wrap up
}

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636



From emb7 at st-andrews.ac.uk  Wed Jul 30 16:23:46 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Wed, 30 Jul 2003 15:23:46 +0100
Subject: [R] Comparing two regression slopes
Message-ID: <oprs4nlwy0b66ldk@localhost>

Hello,
I've written a simple (although probably overly roundabout) function to 
test whether two regression slope coefficients from two linear models on 
independent data sets are significantly different. I'm a bit concerned, 
because when I test it on simulated data with different sample sizes and 
variances, the function seems to be extremely sensitive both of these. I am 
wondering if I've missed something in my function? I'd be very grateful for 
any tips.


Thanks!

Martin


TwoSlope <-function(lm1, lm2) {

## lm1 and lm2 are two linear models on independent data sets

coef1 <-summary(lm1)$coef
coef2 <-summary(lm2)$coef

sigma <-(sum(lm1$residuals^2)+sum(lm2$residuals^2))/(lm1$df.residual + 
lm2$df.residual-4)
SSall <-sum(lm1$model[,2]^2) + sum(lm2$model[,2]^2)
SSprod <-sum(lm1$model[,2]^2) * sum(lm2$model[,2]^2)

F.val <-(as.numeric(coefficients(lm1)[2]) - as.numeric(coefficients(lm2) 
[2]))^2/((SSall/SSprod)*sigma)

p.val <-1-pf(F.val, 1, (lm1$df.residual + lm2$df.residual-4))

cat("\n\nTest for equality between two regression slopes\n\n")
cat("\nCoefficients model 1:\n\n")
print(coef1)

cat("\nCoefficients model 2:\n\n")
print(coef2)

cat("\nF-value on 1 and", lm1$df.residual + lm2$df.residual-4, "degrees of 
freedom:" ,F.val, "\n")
cat("p =", ifelse(p.val>=0.0001, p.val, "< 0.0001"), "\n")

}



From Simon.Fear at synequanon.com  Wed Jul 30 16:56:44 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 30 Jul 2003 15:56:44 +0100
Subject: [R] Plotting a function with curve()
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AB6041@synequanon01>

Clue is in the error message ... "an expression containing x". Not y.
This
works:

curve(x^2, from=0,to=1)

Think of plotting f(x), not calling it y.

SF


-----Original Message-----
From: Damon Wischik [mailto:djw1005 at cam.ac.uk]
Sent: 30 July 2003 15:11
To: R-Help
Subject: [R] Plotting a function with curve()

Why does
> curve(function(y) y^2, from=0,to=1)
not work, whereas
> myf <- function(y) y^2
> curve(myf, from=0,to=1)
work?

For the former, I get the error message
Error in curve(function(y) y^2, from = 0, to = 1) : 
	'expr' must be a function or an expression containing 'x'

I'm using R1.7.0 under Windows XP.

Damon Wischik.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From Joerg.Schaber at uv.es  Wed Jul 30 17:11:12 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 30 Jul 2003 17:11:12 +0200
Subject: [R] robust regression
Message-ID: <3F27E010.4050802@uv.es>

Hi,

trying to do a robudt regression of a two-way linear model, I keep 
getting the following error:

 > lqs(obs ~ y + s -1,method="lms", contrasts=list(s=("contr.sum")))
Error: lqs failed: all the samples were singular

Robust regression with M-estimators works (also regular least square 
fits, of course):
rlm.formula(formula = obs ~ y + s - 1, method = "M", contrasts = list(s 
= ("contr.sum")))

I tried an exact sampling (psamp="exact"), but I keep getting syntax 
errors. Any idea how I can make the first one work?

Thanks,

joerg

-- 
----------------------------------------------------------
J?rg Schaber
Instituto Cavanilles de Biodiversidad y Biologia Evolutiva
Universidad de Valencia               Tel.: ++34 96 354 3666
A.C. 22085                            Fax.: ++34 96 354 3670
46071 Valencia, Espa?a                email : jos at uv.es



From ripley at stats.ox.ac.uk  Wed Jul 30 17:22:11 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 16:22:11 +0100 (BST)
Subject: [R] robust regression
In-Reply-To: <3F27E010.4050802@uv.es>
Message-ID: <Pine.LNX.4.44.0307301616440.25746-100000@gannet.stats>

On Wed, 30 Jul 2003, Joerg Schaber wrote:

> Hi,
> 
> trying to do a robudt regression of a two-way linear model, I keep 
> getting the following error:
> 
>  > lqs(obs ~ y + s -1,method="lms", contrasts=list(s=("contr.sum")))
> Error: lqs failed: all the samples were singular
> 
> Robust regression with M-estimators works (also regular least square 
> fits, of course):
> rlm.formula(formula = obs ~ y + s - 1, method = "M", contrasts = list(s 
> = ("contr.sum")))
> 
> I tried an exact sampling (psamp="exact"), but I keep getting syntax 
> errors. Any idea how I can make the first one work?

You may well not be able to.  lms relies on being able to find a good fit 
to (just over) 50% of the data.  Sounds like you can't fit to 50%.
Try increasing `quantile'.

Generally resistant methods should be employed with caution for designed 
experiments (with factor covariates), and your specification of contrasts 
suggest y is a factor.  So this may not make statistical sense either.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kevin_xie at hotmail.com  Wed Jul 30 17:24:54 2003
From: kevin_xie at hotmail.com (kevin xie)
Date: Wed, 30 Jul 2003 15:24:54 +0000
Subject: [R] Number of components in a mixture model
Message-ID: <BAY1-F28sERtuLcWZd000011411@hotmail.com>

Dear all,

I'm fitting a set of length-of-stay data by a model of mixture of 
exponentials. I've been following the example on page 436 in MASS (5th Ed.). 
However, I have a couple of questions while following this example.

What if we don't know how many components there are in the model in advance. 
Is there any established method to determine the number of components from a 
set of data? I'm aware that the usual likelihood ratio test is not 
appropriate in this case due to the possibility that the ML could occur at 
the boundry of the parameter space.

Secondly, the example in MASS uses a Q-Q plot to informally assess GOF. I 
was wondering if there are some more formal statistical tests for this 
purpose.

I appologise for asking questions that are slightly out-of-topic.

Many thanks.

Kevin



From fharrell at virginia.edu  Wed Jul 30 17:24:19 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 30 Jul 2003 11:24:19 -0400
Subject: [R] Sweave
Message-ID: <20030730112419.42737ce5.fharrell@virginia.edu>

Many of you are using Sweave for making statistical reports.  I thought it might be helpful to some to see an example of the setup I use (in Linux).  For those of you who have not yet discovered the power and productivity gains from using Fritz Leisch's wonderful package, I encourage you to give Sweave a try.

%File: model.nw
%Usage:
% Put library(Hmisc;Design;tools} in .First
% Sweave model   - runs model.nw to produce model.tex and
%                  graphics files in graphics/
% Sweave shell script defined as echo "Sweave(\"$1.nw\")" | R --no-save
% latex or pdflatex model; bibtex model; latex or pdflatex model
% To get .R file: Stangle model
\documentclass{article}
\usepackage{relsize,setspace}  % used by latex(describe( ))
\newcommand{\co}[1]{\texttt{\smaller #1}}

\title{Analysis}
\author{Frank E Harrell Jr}
\begin{document}
\SweaveOpts{prefix.string=graphics/plot}
\setkeys{Gin}{width=1.0\textwidth}
\maketitle
<<echo=F>>=
Hmisc.version  <- package.description('Hmisc')['Version']
Design.version <- package.description('Design')['Version']
@
All calculations were done using \Sexpr{R.version.string} \cite{Roriginal} on
RedHat Linux 9.0, using version \Sexpr{Hmisc.version} of the \texttt{Hmisc}
package and version \Sexpr{Design.version} of the \texttt{Design} package.
\section{Descriptive Statistics}
<<results=tex>>=
load('mydata.sav')
latex(describe(mydata), file='')  # generate LaTeX code in place
@ 
The variable clustering diagram below shows which variables in the
dataset are interrelated.  The $y$-axis is the Spearman $\rho^2$
(squared rank correlation coefficient).  \co{addon} and
\co{mica} cannot be considered together because of an undefined
correlation between them.  \co{addon} correlates weakly with
\co{sodm} (only).  \co{usedlabd} is always unity when it is not
missing, so it is also omitted from variable clustering.
\begin{center}
<<fig=T>>=
plot(varclus(~.-addon-usedlabd, data=mydata))
@
\end{center}
\section{Analysis of Dose Effects}
. . . .
\bibliography{/home/feh/bib/feh.bib}
\bibliographystyle{abbrv}
\end{document}

For more information about reproducible statistical reporting, literate programming, and Sweave see http://hesweb1.med.virginia.edu/biostat/s/LiveDoc.html

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From jrogers at cantatapharm.com  Wed Jul 30 17:39:12 2003
From: jrogers at cantatapharm.com (Jim Rogers)
Date: Wed, 30 Jul 2003 11:39:12 -0400
Subject: [R] Write XML according to ggobi DTD
Message-ID: <99A12772DCDEEB458B996332957B0D53011817@mercury.cantatapharm.com>

Hi, 

Has anyone out there written a function to take a data.frame as input
and generate XML that conforms to the DTD for ggobi ("ggobi.dtd")? In
other words, like a simple version of the writeSDML function in the
StatDataML package, but using ggobi.dtd instead of StatDataML.dtd. 

It looks easy to write such a function to handle data.frames with only
numeric data, but a bit of work with character and factor data. 

Thanks,
Jim Rogers



From fm3a004 at math.uni-hamburg.de  Wed Jul 30 17:43:50 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Wed, 30 Jul 2003 17:43:50 +0200 (MET DST)
Subject: [R] Number of components in a mixture model
In-Reply-To: <BAY1-F28sERtuLcWZd000011411@hotmail.com>
Message-ID: <Pine.GSO.3.95q.1030730173816.6278B-100000@sun28.math.uni-hamburg.de>

Hi Kevin,

a more or less established method (at least for normal mixtures) is the use of
the Bayesian information criterion BIC defined as maximization of 
2* max loglikelihood (s) -log(n)*number of fitted parameters for model s,
s being the number of components, n number of points, over s. 
However I have no experience with it in connection with exponential mixtures.

Christian

On Wed, 30 Jul 2003, kevin xie wrote:

> Dear all,
> 
> I'm fitting a set of length-of-stay data by a model of mixture of 
> exponentials. I've been following the example on page 436 in MASS (5th Ed.). 
> However, I have a couple of questions while following this example.
> 
> What if we don't know how many components there are in the model in advance. 
> Is there any established method to determine the number of components from a 
> set of data? I'm aware that the usual likelihood ratio test is not 
> appropriate in this case due to the possibility that the ML could occur at 
> the boundry of the parameter space.
> 
> Secondly, the example in MASS uses a Q-Q plot to informally assess GOF. I 
> was wondering if there are some more formal statistical tests for this 
> purpose.
> 
> I appologise for asking questions that are slightly out-of-topic.
> 
> Many thanks.
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (current)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From Evrim-Didem.GUNES at insead.edu  Wed Jul 30 17:39:28 2003
From: Evrim-Didem.GUNES at insead.edu (GUNES Evrim-Didem)
Date: Wed, 30 Jul 2003 17:39:28 +0200
Subject: [R] convergence diagnostic test in R
Message-ID: <B0BD7878D2E4664B91ABADF510E5A0470153969B@EUFM007>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030730/f670a611/attachment.pl

From ssullivan at qedgroupllc.com  Wed Jul 30 17:52:27 2003
From: ssullivan at qedgroupllc.com (Steve Sullivan)
Date: Wed, 30 Jul 2003 11:52:27 -0400
Subject: [R] nested for() loops for returning a nearest point
Message-ID: <D4C203B93FEDF04CA2B493EB08F2E431137929@qeds001.hq.wash.qedgroupllc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030730/28e59954/attachment.pl

From tlumley at u.washington.edu  Wed Jul 30 18:14:25 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 30 Jul 2003 09:14:25 -0700 (PDT)
Subject: [R] Plotting a function with curve()
In-Reply-To: <Pine.SOL.3.96.1030730150607.12769A-100000@libra.cus.cam.ac.uk>
Message-ID: <Pine.A41.4.44.0307300908150.120504-100000@homer08.u.washington.edu>

On Wed, 30 Jul 2003, Damon Wischik wrote:

>
> Why does
> > curve(function(y) y^2, from=0,to=1)
> not work, whereas
> > myf <- function(y) y^2
> > curve(myf, from=0,to=1)
> work?
>

Because someone was trying to be too clever in writing curve().  The first
argument must either be an expression with the free variable `x' or the
*name* of a function. You can't specify a literal function or the name of
an expression.

The reason is that it is hard to tell whether an argument is a function or
an expression without evaluating it, and if it's an expression you may not
be able to evaluate it.

You could always use
  plot(function(y) y^2)


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From Roger.Bivand at nhh.no  Wed Jul 30 18:47:45 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 30 Jul 2003 18:47:45 +0200 (CEST)
Subject: [R] nested for() loops for returning a nearest point
In-Reply-To: <D4C203B93FEDF04CA2B493EB08F2E431137929@qeds001.hq.wash.qedgroupllc>
Message-ID: <Pine.LNX.4.44.0307301829100.15275-100000@reclus.nhh.no>

On Wed, 30 Jul 2003, Steve Sullivan wrote:

> I'm trying to do the following:
> 
>  
> 
> For each ordered pair of a data frame (D1) containing longitudes and
> latitudes and unique point IDs, calculate the distance to every point in
> another data frame (D2) also containing longitudes, latitudes and point
> IDs, and return to a new variable in D1 the point ID of the nearest
> element of D2.

I think you can get quite a long way with the function rdist.earth() in 
the fields package:

> loc1 <- expand.grid(long=seq(-150,150,5), lat=seq(-70,70,5))
> dim(loc1)
[1] 1769    2
> loc2 <- expand.grid(long=seq(-150,150,7.5), lat=seq(-70,70,7.5))
> dim(loc2)
[1] 779   2
> dists <- rdist.earth(loc1, loc2)
> id12 <- apply(dists, 1, which.min)
> length(id12)
[1] 1769
> id21 <- apply(dists, 2, which.min)
> length(id21)
[1] 779

using id12 and id21 to choose the point.ids if need be

> loc2$point.id[id12]

Roger

> 
> Dramatis personae (mostly self-explanatory):
> 
> D1$long
> 
> D1$lat
> 
> D1$point.id
> 
> neighbor.id (to be created; for each ordered pair in D1 the point ID of
> the nearest ordered pair in D2)
> 
> D2$long
> 
> D2$lat
> 
> D2$point.id
> 
> dist.geo (to be created)
> 
>  
> 
> I've been attempting this with nested for() loops that step through each
> ordered pair in D1, and for each ordered pair [i] in D1 create a vector
> (dist.geo) the length of D2$lat (say) that contains the distance
> calculated from every ordered pair in D2 to the current ordered pair [i]
> of D1, assign a value for D1$neighbor.id[i] based on
> D2$point.id[(which.min(dist.geo)], and move on to the next ordered pair
> of D1 to create another dist.geo, assign another neighbor.id, etc.
> 
>  
> 
> There are no missings/NAs in any of the longs, lats or point.ids,
> although advice on generalizing this to deal with them would be
> appreciated.
> 
>  
> 
> What I've been trying:
> 
>  
> 
> neighbor.id <- vector(length=length(D1$lat))
> dist.geo <- vector(length=length(D2$lat))
> for(i in 1:length(neighbor.id)){
> for(j in 1:length(dist.geo)){
> dist.geo[j] <- D1$lat[i]-D2$lat[j]}  
> 
> # Yes, I know that isn't the right formula, this is just a test
> 
> neighbor.id[i] <- D2$point.id[which.min(dist.geo)]}
> 
>  
> 
> What I get is a neighbor.id of the appropriate length, but which
> consists only of the same value repeated.  Should I instead pass the
> which.min(dist.geo) to a variable before exiting the inner (j) loop, and
> reference that variable in place of which.min(dist.geo) in the last
> line?  Or is this whole approach wrongheaded?
> 
>  
> 
> This should be elementary, I know, so I appreciate everyone's
> forbearance.
> 
>  
> 
> Steven Sullivan, Ph.D.
> 
> Senior Associate
> 
> The QED Group, LLC
> 
> 1250 Eye St. NW, Suite 802
> 
> Washington, DC  20005
> 
> ssullivan at qedgroupllc.com
> 
> 202.898.1910.x15 (v)
> 
> 202.898.0887 (f)
> 
> 202.421.8161 (m)
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Wed Jul 30 18:58:55 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 30 Jul 2003 16:58:55 -0000
Subject: [R] nested for() loops for returning a nearest point
In-Reply-To: <D4C203B93FEDF04CA2B493EB08F2E431137929@qeds001.hq.wash.qedgroupllc>
References: <D4C203B93FEDF04CA2B493EB08F2E431137929@qeds001.hq.wash.qedgroupllc>
Message-ID: <x2llug54lp.fsf@biostat.ku.dk>

"Steve Sullivan" <ssullivan at qedgroupllc.com> writes:

> neighbor.id <- vector(length=length(D1$lat))
> dist.geo <- vector(length=length(D2$lat))
> for(i in 1:length(neighbor.id)){
> for(j in 1:length(dist.geo)){
> dist.geo[j] <- D1$lat[i]-D2$lat[j]}  
> 
> # Yes, I know that isn't the right formula, this is just a test
> 
> neighbor.id[i] <- D2$point.id[which.min(dist.geo)]}
> 
>  
> 
> What I get is a neighbor.id of the appropriate length, but which
> consists only of the same value repeated.  Should I instead pass the
> which.min(dist.geo) to a variable before exiting the inner (j) loop, and
> reference that variable in place of which.min(dist.geo) in the last
> line?  Or is this whole approach wrongheaded?

Wouldn't you want to define dist.geo with an abs() ? Otherwise, the
North Pole might have the largest negative difference every time...

Apart from that, things look sane to me (but the heat is killing me
today...). You can vectorize things as in 

dist.geo <- abs(D1$lat[i]-D2$lat)

and get rid of the inner loop, but the basic idea looks correct.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.calboli at ucl.ac.uk  Wed Jul 30 19:00:46 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Wed, 30 Jul 2003 18:00:46 +0100
Subject: [R] anova(mymodel.lme, type = "marginal")
Message-ID: <3.0.6.32.20030730180046.028ca218@pop-server.ucl.ac.uk>

Dear All,

recently, while setting me on the straight and narrow about linear
contrasts for a linear mixed effect model, Prof Ripley pointed out that my
interpertation of the call 

anova(mymodel.lme) 

was not correct, because I was meant to add 

type = "marginal", as in anova(mymodel.lme, type = "marginal")

I tried to look deeper in the issue, asking people, checking on the R-list
archives and googling around, but the best I could find was another old
post reiterating that the anova call for fixed terms in a mixed model
requires *type = "marginal"*. I also checked MASS IV and Pinheiro & Bates,
but I am not sure how much I got from my read.

As I am not a statistician, I'd simply like to ask whether is this the
*standard* way of generation an anova table for a mixed effect model, or,
if it is not, what are the general rules underlying the application of a
marginal approach for a mixed model effect.

Regards,
Federico Calboli




=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From p.dalgaard at biostat.ku.dk  Wed Jul 30 19:06:31 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 30 Jul 2003 17:06:31 -0000
Subject: [R] Plotting a function with curve()
In-Reply-To: <Pine.A41.4.44.0307300908150.120504-100000@homer08.u.washington.edu>
References: <Pine.A41.4.44.0307300908150.120504-100000@homer08.u.washington.edu>
Message-ID: <x2he545481.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Wed, 30 Jul 2003, Damon Wischik wrote:
> 
> >
> > Why does
> > > curve(function(y) y^2, from=0,to=1)
> > not work, whereas
> > > myf <- function(y) y^2
> > > curve(myf, from=0,to=1)
> > work?
> >
> 
> Because someone was trying to be too clever in writing curve().  The first
> argument must either be an expression with the free variable `x' or the
> *name* of a function. You can't specify a literal function or the name of
> an expression.
> 
> The reason is that it is hard to tell whether an argument is a function or
> an expression without evaluating it, and if it's an expression you may not
> be able to evaluate it.
> 
> You could always use
>   plot(function(y) y^2)

Actually, it wouldn't be too hard to use identical(sexpr[[1]],
as.name("function")) and detect an explicit function definition, so
someone could have been even more clever...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Wed Jul 30 19:09:46 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 30 Jul 2003 13:09:46 -0400
Subject: [R] nested for() loops for returning a nearest point
Message-ID: <3A822319EB35174CA3714066D590DCD50205C966@usrymx25.merck.com>

> From: Steve Sullivan [mailto:ssullivan at qedgroupllc.com] 
> 
> I'm trying to do the following:
> 
> For each ordered pair of a data frame (D1) containing 
> longitudes and latitudes and unique point IDs, calculate the 
> distance to every point in another data frame (D2) also 
> containing longitudes, latitudes and point IDs, and return to 
> a new variable in D1 the point ID of the nearest element of D2.
> 
> Dramatis personae (mostly self-explanatory):
> 
> D1$long
> 
> D1$lat
> 
> D1$point.id
> 
> neighbor.id (to be created; for each ordered pair in D1 the 
> point ID of the nearest ordered pair in D2)
> 
> D2$long
> 
> D2$lat
> 
> D2$point.id
> 
> dist.geo (to be created)
> 
>  
> 
> I've been attempting this with nested for() loops that step 
> through each ordered pair in D1, and for each ordered pair 
> [i] in D1 create a vector
> (dist.geo) the length of D2$lat (say) that contains the 
> distance calculated from every ordered pair in D2 to the 
> current ordered pair [i] of D1, assign a value for 
> D1$neighbor.id[i] based on D2$point.id[(which.min(dist.geo)], 
> and move on to the next ordered pair of D1 to create another 
> dist.geo, assign another neighbor.id, etc.
> 
>  
> 
> There are no missings/NAs in any of the longs, lats or 
> point.ids, although advice on generalizing this to deal with 
> them would be appreciated.
> 
>  
> 
> What I've been trying:
> 
>  
> 
> neighbor.id <- vector(length=length(D1$lat))
> dist.geo <- vector(length=length(D2$lat))
> for(i in 1:length(neighbor.id)){
> for(j in 1:length(dist.geo)){
> dist.geo[j] <- D1$lat[i]-D2$lat[j]}  
> 
> # Yes, I know that isn't the right formula, this is just a test
> 
> neighbor.id[i] <- D2$point.id[which.min(dist.geo)]}
> 
>  
> 
> What I get is a neighbor.id of the appropriate length, but 
> which consists only of the same value repeated.  Should I 
> instead pass the
> which.min(dist.geo) to a variable before exiting the inner 
> (j) loop, and reference that variable in place of 
> which.min(dist.geo) in the last line?  Or is this whole 
> approach wrongheaded?
> 

For finding nearest neighbors, try the following:

set.seed(1)
d1 <- data.frame(long=rnorm(10), lat=rnorm(10), point.id=factor(1:10))
d2 <- data.frame(long=rnorm(5), lat=rnorm(5), point.id=factor(1:5))

## For each point in D1, find nearest neighbor in D2.
library(class)
d1$neighbor.id <- knn1(as.matrix(d2[,1:2]), as.matrix(d1[,1:2]),
d2$point.id)

If you really want do, you could modify knn1() (and the C code it calls) so
the distance is also returned.  Otherwise, you can just compute the distance
"by hand" in R once the nearest neighbors are found.

HTH,
Andy

>  
> 
> This should be elementary, I know, so I appreciate everyone's 
> forbearance.
> 
>  
> 
> Steven Sullivan, Ph.D.
> 
> Senior Associate
> 
> The QED Group, LLC
> 
> 1250 Eye St. NW, Suite 802
> 
> Washington, DC  20005
> 
> ssullivan at qedgroupllc.com
> 
> 202.898.1910.x15 (v)
> 
> 202.898.0887 (f)
> 
> 202.421.8161 (m)
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From ripley at stats.ox.ac.uk  Wed Jul 30 19:10:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 18:10:53 +0100 (BST)
Subject: [R] nested for() loops for returning a nearest point
In-Reply-To: <Pine.LNX.4.44.0307301829100.15275-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0307301807120.28727-100000@gannet.stats>

For largish datasets, knn1 in package class (in the recommended VR bundle) 
is probably the quickest way to do this.  Something like

knn1(D1[. 1:2], D2[, 1:2], D2$ID)

On Wed, 30 Jul 2003, Roger Bivand wrote:

> On Wed, 30 Jul 2003, Steve Sullivan wrote:
> 
> > I'm trying to do the following:
> > 
> >  
> > 
> > For each ordered pair of a data frame (D1) containing longitudes and
> > latitudes and unique point IDs, calculate the distance to every point in
> > another data frame (D2) also containing longitudes, latitudes and point
> > IDs, and return to a new variable in D1 the point ID of the nearest
> > element of D2.
> 
> I think you can get quite a long way with the function rdist.earth() in 
> the fields package:
> 
> > loc1 <- expand.grid(long=seq(-150,150,5), lat=seq(-70,70,5))
> > dim(loc1)
> [1] 1769    2
> > loc2 <- expand.grid(long=seq(-150,150,7.5), lat=seq(-70,70,7.5))
> > dim(loc2)
> [1] 779   2
> > dists <- rdist.earth(loc1, loc2)
> > id12 <- apply(dists, 1, which.min)
> > length(id12)
> [1] 1769
> > id21 <- apply(dists, 2, which.min)
> > length(id21)
> [1] 779
> 
> using id12 and id21 to choose the point.ids if need be
> 
> > loc2$point.id[id12]
> 
> Roger
> 
> > 
> > Dramatis personae (mostly self-explanatory):
> > 
> > D1$long
> > 
> > D1$lat
> > 
> > D1$point.id
> > 
> > neighbor.id (to be created; for each ordered pair in D1 the point ID of
> > the nearest ordered pair in D2)
> > 
> > D2$long
> > 
> > D2$lat
> > 
> > D2$point.id
> > 
> > dist.geo (to be created)
> > 
> >  
> > 
> > I've been attempting this with nested for() loops that step through each
> > ordered pair in D1, and for each ordered pair [i] in D1 create a vector
> > (dist.geo) the length of D2$lat (say) that contains the distance
> > calculated from every ordered pair in D2 to the current ordered pair [i]
> > of D1, assign a value for D1$neighbor.id[i] based on
> > D2$point.id[(which.min(dist.geo)], and move on to the next ordered pair
> > of D1 to create another dist.geo, assign another neighbor.id, etc.
> > 
> >  
> > 
> > There are no missings/NAs in any of the longs, lats or point.ids,
> > although advice on generalizing this to deal with them would be
> > appreciated.
> > 
> >  
> > 
> > What I've been trying:
> > 
> >  
> > 
> > neighbor.id <- vector(length=length(D1$lat))
> > dist.geo <- vector(length=length(D2$lat))
> > for(i in 1:length(neighbor.id)){
> > for(j in 1:length(dist.geo)){
> > dist.geo[j] <- D1$lat[i]-D2$lat[j]}  
> > 
> > # Yes, I know that isn't the right formula, this is just a test
> > 
> > neighbor.id[i] <- D2$point.id[which.min(dist.geo)]}
> > 
> >  
> > 
> > What I get is a neighbor.id of the appropriate length, but which
> > consists only of the same value repeated.  Should I instead pass the
> > which.min(dist.geo) to a variable before exiting the inner (j) loop, and
> > reference that variable in place of which.min(dist.geo) in the last
> > line?  Or is this whole approach wrongheaded?
> > 
> >  
> > 
> > This should be elementary, I know, so I appreciate everyone's
> > forbearance.
> > 
> >  
> > 
> > Steven Sullivan, Ph.D.
> > 
> > Senior Associate
> > 
> > The QED Group, LLC
> > 
> > 1250 Eye St. NW, Suite 802
> > 
> > Washington, DC  20005
> > 
> > ssullivan at qedgroupllc.com
> > 
> > 202.898.1910.x15 (v)
> > 
> > 202.898.0887 (f)
> > 
> > 202.421.8161 (m)
> > 
> >  
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jul 30 19:18:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 30 Jul 2003 17:18:30 -0000
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <Pine.LNX.4.44.0307291303310.3555-100000@gannet.stats>
References: <Pine.LNX.4.44.0307291303310.3555-100000@gannet.stats>
Message-ID: <x2d6fs53p1.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Tue, 29 Jul 2003, p.b.pynsent wrote:

> > What I had hoped for was what I get with srt=0, rotated as whole so it 
> > looked the same but was just rotated to the specified angle.
> 
> But you got what is documented, and my example addessed that.

The docs are arguably a bit unclear when it comes to combining 'srt'
and 'pos'. You might get more easily predictable results by using the
'adj' argument instead of 'pos'.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Jul 30 19:22:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Jul 2003 10:22:28 -0700
Subject: [R] Comparing two regression slopes
References: <oprs4nlwy0b66ldk@localhost>
Message-ID: <3F27FED4.9020004@pdf.com>

I'm afraid of your manual ANOVA.  It may be correct, but it won't easily 
generalize.  Instead, how about the following:

 > df1 <- data.frame(x=1:3, y=1:3+rnorm(3))
 > df2 <- data.frame(x=1:3, y=1:3+rnorm(3))
 >
 > fit1 <- lm(y~x, df1)
 > s1 <- summary(fit1)$coefficients
 > fit2 <- lm(y~x, df2)
 > s2 <- summary(fit2)$coefficients
 >
 > db <- (s2[2,1]-s1[2,1])
 > sd <- sqrt(s2[2,2]^2+s1[2,2]^2)
 > df <- (fit1$df.residual+fit2$df.residual)
 > td <- db/sd
 > 2*pt(-abs(td), df)
[1] 0.9510506

The function "attributes" helped me figure this out.

hope this helps.  spencer graves

Martin Biuw wrote:
> Hello,
> I've written a simple (although probably overly roundabout) function to 
> test whether two regression slope coefficients from two linear models on 
> independent data sets are significantly different. I'm a bit concerned, 
> because when I test it on simulated data with different sample sizes and 
> variances, the function seems to be extremely sensitive both of these. I 
> am wondering if I've missed something in my function? I'd be very 
> grateful for any tips.
> 
> 
> Thanks!
> 
> Martin
> 
> 
> TwoSlope <-function(lm1, lm2) {
> 
> ## lm1 and lm2 are two linear models on independent data sets
> 
> coef1 <-summary(lm1)$coef
> coef2 <-summary(lm2)$coef
> 
> sigma <-(sum(lm1$residuals^2)+sum(lm2$residuals^2))/(lm1$df.residual + 
> lm2$df.residual-4)
> SSall <-sum(lm1$model[,2]^2) + sum(lm2$model[,2]^2)
> SSprod <-sum(lm1$model[,2]^2) * sum(lm2$model[,2]^2)
> 
> F.val <-(as.numeric(coefficients(lm1)[2]) - as.numeric(coefficients(lm2) 
> [2]))^2/((SSall/SSprod)*sigma)
> 
> p.val <-1-pf(F.val, 1, (lm1$df.residual + lm2$df.residual-4))
> 
> cat("\n\nTest for equality between two regression slopes\n\n")
> cat("\nCoefficients model 1:\n\n")
> print(coef1)
> 
> cat("\nCoefficients model 2:\n\n")
> print(coef2)
> 
> cat("\nF-value on 1 and", lm1$df.residual + lm2$df.residual-4, "degrees 
> of freedom:" ,F.val, "\n")
> cat("p =", ifelse(p.val>=0.0001, p.val, "< 0.0001"), "\n")
> 
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jahumada at usgs.gov  Wed Jul 30 19:24:58 2003
From: jahumada at usgs.gov (Jorge A Ahumada)
Date: Wed, 30 Jul 2003 12:24:58 -0500
Subject: [R] put a label on right y axis?
Message-ID: <BE4B332A-C2B2-11D7-817E-000393DEDD9C@usgs.gov>

If you have a graph with two y-axes like:

plot(...) #1st graph
par(new=T)
plot(.....,axes=F,xlab="",ylab="") #plot second
axis(4) # put axes on the right side

How do you put a ylab to that second axis?

Thanks,

Jorge



From Laurie.Sindlinger at noaa.gov  Wed Jul 30 19:36:28 2003
From: Laurie.Sindlinger at noaa.gov (Laurie Sindlinger)
Date: Wed, 30 Jul 2003 13:36:28 -0400
Subject: [R] Should garbage collection be automatic in R sessions?
Message-ID: <3F28021C.1050506@noaa.gov>

Hello all,

   I am having problems with memory when running R on my PC.  I do not 
have many large objects in my workspace, and yet when trying to create a 
new vector I often encounter this error message:

> lat <- header$lat[match(profile$id, header$id)]
 Error: cannot allocate vector of size 4575 Kb

Since it seems like this may indicate that I don't have enough memory available, I tried increasing nsize using mem.limits, but I still get this error message. Is there a function in R where you can see the size of all objects in the workspace? 
  When I tried using gc(), I was then able to create the vector. It seems to indicate in the help pages that the garbage collection should happen automatically. Does anyone know how often this might take place or how I can check if it is occurring during my R sessions? Thanks for you help, and I have included info about my system below (I have 1 gbyte of memory).

Sincerely,
Laurie Sindlinger

Windows 2000

Intel(R) Pentium
(R) 4CPU 2.40 GHz
AT/AT COMPATIBLE
1,047,536 KB RAM



From jerome at hivnet.ubc.ca  Wed Jul 30 19:41:43 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 30 Jul 2003 10:41:43 -0700
Subject: [R] put a label on right y axis?
In-Reply-To: <BE4B332A-C2B2-11D7-817E-000393DEDD9C@usgs.gov>
References: <BE4B332A-C2B2-11D7-817E-000393DEDD9C@usgs.gov>
Message-ID: <200307301748.KAA05494@hivnet.ubc.ca>


See ?mtext

On July 30, 2003 10:24 am, Jorge A Ahumada wrote:
> If you have a graph with two y-axes like:
>
> plot(...) #1st graph
> par(new=T)
> plot(.....,axes=F,xlab="",ylab="") #plot second
> axis(4) # put axes on the right side
>
> How do you put a ylab to that second axis?
>
> Thanks,
>
> Jorge
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Jul 30 19:52:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 30 Jul 2003 10:52:13 -0700
Subject: [R] put a label on right y axis?
References: <BE4B332A-C2B2-11D7-817E-000393DEDD9C@usgs.gov>
Message-ID: <3F2805CD.9050803@pdf.com>

Both of the following work for me:

plot(1:2)
axis(4)
mtext("right y axis", side=4, line=-1.5)

par(mar=c(5,4,4,5)+.1)
plot(1:2)
axis(4)
mtext("right y axis", side=4, line=3)

hope this helps.  spencer graves

Jorge A Ahumada wrote:
> If you have a graph with two y-axes like:
> 
> plot(...) #1st graph
> par(new=T)
> plot(.....,axes=F,xlab="",ylab="") #plot second
> axis(4) # put axes on the right side
> 
> How do you put a ylab to that second axis?
> 
> Thanks,
> 
> Jorge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From chris1 at psyctc.org  Wed Jul 30 20:01:00 2003
From: chris1 at psyctc.org (Chris Evans)
Date: Wed, 30 Jul 2003 19:01:00 +0100
Subject: [R] Rcgi
Message-ID: <3F2815EC.8003.5674F7D@localhost>

I am keen to look at Rcgi as I want to put up some simple bits of R 
to do prescribed tasks on HTML form input.  Rweb is overkill and 
worryingly flexible for what I want and it sounds as if Rcgi is more 
what I need.  However, I can't get any of the URLs I've found for it 
to work over the last few days.  

Does anyone have a recent copy they could Email me or a working URL 
for it?

TIA,

Chris

PSYCTC: Psychotherapy, Psychology, Psychiatry, Counselling
   and Therapeutic Communities; practice, research, 
   teaching and consultancy.
Chris Evans & Jo-anne Carlyle
http://psyctc.org/ Email: chris at psyctc.org



From ripley at stats.ox.ac.uk  Wed Jul 30 20:06:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jul 2003 19:06:38 +0100 (BST)
Subject: [R] Should garbage collection be automatic in R sessions?
In-Reply-To: <3F28021C.1050506@noaa.gov>
Message-ID: <Pine.LNX.4.44.0307301901060.28841-100000@gannet.stats>

On Wed, 30 Jul 2003, Laurie Sindlinger wrote:

>    I am having problems with memory when running R on my PC.  I do not 
> have many large objects in my workspace, and yet when trying to create a 
> new vector I often encounter this error message:
> 
> > lat <- header$lat[match(profile$id, header$id)]
>  Error: cannot allocate vector of size 4575 Kb
> 
> Since it seems like this may indicate that I don't have enough memory
> available, I tried increasing nsize using mem.limits, but I still get
> this error message. 

See what ?mem.limits says about Windows ...  ?memory.size (and the 
rw-FAQ) may be more use to you.

> Is there a function in R where you can see the size
> of all objects in the workspace?

object.size(ls()), but not all objects using memory are in the workspace.

>   When I tried using gc(), I was then able to create the vector. It
> seems to indicate in the help pages that the garbage collection should
> happen automatically. Does anyone know how often this might take place
> or how I can check if it is occurring during my R sessions? Thanks for
> you help, and I have included info about my system below (I have 1 gbyte
> of memory).

?gcinfo, mentioned on ?mem.limits.

> Windows 2000

which is not the most efficient OS at using large amounts of memory 
(although better than Windows 9x).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jul 30 20:12:12 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 30 Jul 2003 20:12:12 +0200
Subject: [R] Should garbage collection be automatic in R sessions?
References: <3F28021C.1050506@noaa.gov>
Message-ID: <3F280A7C.49067F4E@statistik.uni-dortmund.de>



Laurie Sindlinger wrote:
> 
> Hello all,
> 
>    I am having problems with memory when running R on my PC.  I do not
> have many large objects in my workspace, and yet when trying to create a
> new vector I often encounter this error message:
> 
> > lat <- header$lat[match(profile$id, header$id)]
>  Error: cannot allocate vector of size 4575 Kb
> 
> Since it seems like this may indicate that I don't have enough memory available, I tried increasing nsize using mem.limits, but I still get this error message. Is there a function in R where you can see the size of all objects in the workspace?

See ?memory.size and ?memory.limit

>   When I tried using gc(), I was then able to create the vector. It seems to indicate in the help pages that the garbage collection should happen automatically. Does anyone know how often this might take place or how I can check if it is occurring during my R sessions? Thanks for you help, and I have included info about my system below (I have 1 gbyte of memory).

See ?gcinfo


Uwe Ligges

 
> Sincerely,
> Laurie Sindlinger
> 
> Windows 2000
> 
> Intel(R) Pentium
> (R) 4CPU 2.40 GHz
> AT/AT COMPATIBLE
> 1,047,536 KB RAM
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dave at evocapital.com  Wed Jul 30 20:15:05 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 30 Jul 2003 19:15:05 +0100
Subject: [R] Sending emails from R under Windows
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063E8D@sqlsrvr.evocapital.com>

Thanks to everyone that responded on this subject -- 4 responses with 4
separate solutions to the problem! Thanks especially to Gabor
Grothendieck for introducing me to "blat", the freeware emailer
executable which was the solution I implemented in the end, as it
avoided problems caused by Outlook's security features getting in the
way (my COM instance of Outlook got very paranoid when R tried to send
mails and kept popping up with warning messages which meant I could not
really use this non-interactively).

Anyway, thanks again.

-----Original Message-----
From: David Khabie-Zeitoune 
Sent: 29 July 2003 16:59
To: r-help at r-project.org
Subject: [R] Sending emails from R under Windows


Hi

Does anyone know of any R routines to send emails from R, under Windows?
I thought about writing such a facility using the R(D)COM package to
drive e.g. MS Outlook, but I don't want to reinvent the wheel. I have
found a function Sys.mail in the library syskern, but this only works
under Unix by shelling out a mail command. 

Thanks,

David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From xiwu at uoguelph.ca  Wed Jul 30 21:11:05 2003
From: xiwu at uoguelph.ca (Ximing Wu)
Date: Wed, 30 Jul 2003 15:11:05 -0400 (EDT)
Subject: [R] compile c code from R
Message-ID: <Pine.GHP.4.21.0307301507480.6199-100000@ccshst01.cs.uoguelph.ca>

Hi,
Can I turn my R code automatically into C? If yes, how can I do that? Do I
benefit by doing it?
(I know I can do this in matlab and it increases the speed when there is a
lot of looping...)

thanks a lot.
x.w

------------------------
Ximing Wu
Department of Economics
University of Guelph
Guelph, ON, N1G 2W1
Canada

Tel: (519) 824-4120 ext. 53014
Fax: (519) 763-8497
email: xiwu at uoguelph.ca



From jasont at indigoindustrial.co.nz  Wed Jul 30 21:47:09 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 31 Jul 2003 07:47:09 +1200
Subject: [R] compile c code from R
In-Reply-To: <Pine.GHP.4.21.0307301507480.6199-100000@ccshst01.cs.uoguelph.ca>
References: <Pine.GHP.4.21.0307301507480.6199-100000@ccshst01.cs.uoguelph.ca>
Message-ID: <3F2820BD.1090001@indigoindustrial.co.nz>

Ximing Wu wrote:
> Hi,
> Can I turn my R code automatically into C? If yes, how can I do that? Do I
> benefit by doing it?

Nope. But the R language libraries can be compiled as a dynamic library, 
and linked to from C.  This still means writing your own C code to run 
the R functions, however.

The extra effort *may* be worth it for some applications that are fairly 
static in their development/application (ie few ongoing tweaks).  I 
haven't found any advantage for my own work, however.

> (I know I can do this in matlab and it increases the speed when there is a
> lot of looping...)

There are others on this list who've tried this sort of benchmarking.

For me, saving hours of development time is well worth the extra seconds 
or minutes of code execution time.  And it's far from clear it's costing 
me that much in execution time.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From BuchsbaB at intra.nimh.nih.gov  Wed Jul 30 22:42:59 2003
From: BuchsbaB at intra.nimh.nih.gov (Buchsbaum, Brad (NIH/NIMH))
Date: Wed, 30 Jul 2003 16:42:59 -0400
Subject: [R] reverse array indexing 
Message-ID: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>



Hi,

Suppose I have a multidimensional array:

tmp <- array(1:8, c(2,2,2))

is there a function out there that, given a one-dimensional array index,
will
return the separate indices for each array dimension?

for instance, tmp[8] is equivalent to tmp[2,2,2]. I'd like to derive the
vector (2,2,2)
from the index 8. 

thanks,

Brad Buchsbaum



From pburns at pburns.seanet.com  Wed Jul 30 23:50:10 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 30 Jul 2003 22:50:10 +0100
Subject: [R] reverse array indexing
References: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>
Message-ID: <3F283D92.1010007@pburns.seanet.com>

I believe that the function below satisfies the request:

 rev.index <-
function (x, dims)
{
        ld <- length(dims)
        cumdim <- cumprod(dims)
        ans <- array(NA, c(length(x), ld))

        ans[, ld] <- (x-1) %/% cumdim[ld - 1] + 1
        ans[, ld-1] <- (x-1) %% cumdim[ld - 1] + 1

        if(ld > 2) {
                for(i in (ld-1):2) {
                        y <- ans[, i]
                        ans[, i] <- (y-1) %/% cumdim[i - 1] + 1
                        ans[, i-1] <- (y-1) %% cumdim[i - 1] + 1
                }
        }
        ans
}


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Buchsbaum, Brad (NIH/NIMH) wrote:

>Hi,
>
>Suppose I have a multidimensional array:
>
>tmp <- array(1:8, c(2,2,2))
>
>is there a function out there that, given a one-dimensional array index,
>will
>return the separate indices for each array dimension?
>
>for instance, tmp[8] is equivalent to tmp[2,2,2]. I'd like to derive the
>vector (2,2,2)
>from the index 8. 
>
>thanks,
>
>Brad Buchsbaum
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From siewlengteng at yahoo.com  Thu Jul 31 00:03:06 2003
From: siewlengteng at yahoo.com (Siew Leng TENG)
Date: Wed, 30 Jul 2003 15:03:06 -0700 (PDT)
Subject: [R] stepAIC()
Message-ID: <20030730220306.35875.qmail@web14911.mail.yahoo.com>

Hi,

I am experiencing a baffling behaviour of stepAIC(),
and I hope to get any advice/help on this. Greatly
appreciate any kind advice given.

I am using stepAIC() to, say, select a model via
stepwise selection method.

R Version : 1.7.1
Windows ME

Many thanks!


***Issue :

When stepAIC() is placed within a function, it seems
that stepAIC() cannot detect the data matrix, and the
program is halted as a result. However, when the same
codes are copied, paste and run in R workspace,
stepAIC can execute and R is able to produce the
desired output.

***Actions taken :
I had tried to look into and manipulate environments
and formals(f), but with not much luck.


***Code snippets (an example):

  library(MASS)
  library(nls)

  Data<-data.frame(matrix(rnorm(120),ncol=6))
  colnames(Data)<-c("Y",paste("X",1:5,sep=""))

  f<-function(A)
  {
  a <-glm(Y~X1+X2+X3, data=A)
  b
<-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
  b
  }

 f(Data) 


R gives the following error :

> f(Data)
Error in model.frame.default(formula = Y ~ X1 + X3,
data = A, drop.unused.levels = TRUE) : 
        Object "A" not found


However, when the same codes are copied, paste and run
in the workspace, stepAIC() can run, and R is able to
produce the desired output :

A <- Data
a <-glm(Y~X1+X2+X3, data=A)
b
<-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
b

> b

Call:  glm(formula = Y ~ X4, data = A) 

Coefficients:
(Intercept)           X4  
     0.1942      -0.3314  

Degrees of Freedom: 19 Total (i.e. Null);  18 Residual
Null Deviance:      20.51 
Residual Deviance: 17.95        AIC: 60.6 
> 


__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From jerome at hivnet.ubc.ca  Thu Jul 31 00:07:32 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 30 Jul 2003 15:07:32 -0700
Subject: [R] reverse array indexing
In-Reply-To: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>
References: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>
Message-ID: <200307302214.PAA03709@hivnet.ubc.ca>


Try this... Perhaps this will help you do what you want. The key idea is 
to use which() with option arr.ind = TRUE.

arr <- array(rnorm(27),c(3,3,3))
dimarr <- dim(arr)
tmparr <- array(1:prod(dimarr),dimarr)
sapply(c(3),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)
sapply(c(3,17,13,5),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)

Jerome

On July 30, 2003 01:42 pm, Buchsbaum, Brad (NIH/NIMH) wrote:
> Hi,
>
> Suppose I have a multidimensional array:
>
> tmp <- array(1:8, c(2,2,2))
>
> is there a function out there that, given a one-dimensional array index,
> will
> return the separate indices for each array dimension?
>
> for instance, tmp[8] is equivalent to tmp[2,2,2]. I'd like to derive the
> vector (2,2,2)
> from the index 8.
>
> thanks,
>
> Brad Buchsbaum
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From yanyu at cs.ucla.edu  Thu Jul 31 01:08:03 2003
From: yanyu at cs.ucla.edu (Yan Yu)
Date: Wed, 30 Jul 2003 16:08:03 -0700 (PDT)
Subject: [R] 
	a quick Q:  a function in R, equivalent to "atoi" function in C?
Message-ID: <Pine.SOL.4.33.0307301602570.19493-100000@panther.cs.ucla.edu>

Hello,
   I wonder is there a function in R, which can achieve the functionality
of atoi in C, i.e., convert from a character string to a number?

I use Sys.getenv(), it returns a character string, e.g., "12", but i need
a number, e.g., 12, to fit into a function.

thanks a lot,
yan



From jerome at hivnet.ubc.ca  Thu Jul 31 01:18:53 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 30 Jul 2003 16:18:53 -0700
Subject: [R]  a quick Q:  a function in R,
	equivalent to "atoi" function in C?
In-Reply-To: <Pine.SOL.4.33.0307301602570.19493-100000@panther.cs.ucla.edu>
References: <Pine.SOL.4.33.0307301602570.19493-100000@panther.cs.ucla.edu>
Message-ID: <200307302325.QAA06354@hivnet.ubc.ca>


?as.numeric

On July 30, 2003 04:08 pm, Yan Yu wrote:
> Hello,
>    I wonder is there a function in R, which can achieve the
> functionality of atoi in C, i.e., convert from a character string to a
> number?
>
> I use Sys.getenv(), it returns a character string, e.g., "12", but i
> need a number, e.g., 12, to fit into a function.
>
> thanks a lot,
> yan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Thu Jul 31 01:20:33 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 30 Jul 2003 19:20:33 -0400 (EDT)
Subject: [R]  a quick Q:  a function in R, equivalent to "atoi" function
	in C?
In-Reply-To: <Pine.SOL.4.33.0307301602570.19493-100000@panther.cs.ucla.edu>
Message-ID: <Pine.SOL.4.44.0307301919310.27028-100000@timepilot.gpcc.itd.umich.edu>

On Wed, 30 Jul 2003, Yan Yu wrote:

>    I wonder is there a function in R, which can achieve the functionality
> of atoi in C, i.e., convert from a character string to a number?

as.numeric()

> I use Sys.getenv(), it returns a character string, e.g., "12", but i need
> a number, e.g., 12, to fit into a function.  thanks a lot,
> yan



From p.dalgaard at biostat.ku.dk  Thu Jul 31 01:28:01 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 30 Jul 2003 23:28:01 -0000
Subject: [R] reverse array indexing
In-Reply-To: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>
References: <5F9DE1C25708B04EAD634A1AE3D911300252AC@nihexchange20.nih.gov>
Message-ID: <x2u193bneg.fsf@biostat.ku.dk>

"Buchsbaum, Brad (NIH/NIMH)" <BuchsbaB at intra.nimh.nih.gov> writes:

> Hi,
> 
> Suppose I have a multidimensional array:
> 
> tmp <- array(1:8, c(2,2,2))
> 
> is there a function out there that, given a one-dimensional array index,
> will
> return the separate indices for each array dimension?
> 
> for instance, tmp[8] is equivalent to tmp[2,2,2]. I'd like to derive the
> vector (2,2,2)
> from the index 8. 

One not very efficient way is this:

> foo <- seq(along=tmp)
> dim(foo)<-dim(tmp)
> which(foo==8,arr.ind=TRUE)
     dim1 dim2 dim3
[1,]    2    2    2

For more efficient solutions peek inside the code of which() and see
how it gets from ordinary indices to array indices using %% and %/%
operations. (Not very easy to grasp, I know).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wettenhall at wehi.edu.au  Thu Jul 31 03:22:53 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Thu, 31 Jul 2003 11:22:53 +1000 (EST)
Subject: [R] Tktable White column when WIDTH>13
In-Reply-To: <Pine.LNX.4.44.0307302230150.3299-100000@unix28.alpha.wehi.edu.au>
Message-ID: <Pine.LNX.4.44.0307311113160.17868-100000@unix28.alpha.wehi.edu.au>

Sorry to those not interested in R/TclTk for yet another email!

Thomas found possibly a better way to fix the "white column on 
the right" problem :
tkconfigure(table1,colstretchmode="unset")

Also, he asked how to justify text to the left in cells, noting 
that justify="left" didn't work and that the help for justify 
only mentioned multi-line cells.

His solution was :
tkconfigure(table1,anchor="w")

Or for an individual tagged cell:
tkcmd(.Tk.ID(table1),"tag","cell","ZeroOne","0,1")
tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",anchor="e")

James



From ggrothendieck at volcanomail.com  Thu Jul 31 06:19:36 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Wed, 30 Jul 2003 21:19:36 -0700 (PDT)
Subject: [R] timezones
Message-ID: <20030731041936.A6DFC11F32@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030730/d130d863/attachment.pl

From ok at cs.otago.ac.nz  Thu Jul 31 07:08:21 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 31 Jul 2003 17:08:21 +1200 (NZST)
Subject: [R] reverse array indexing
Message-ID: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>

Jerome Asselin <jerome at hivnet.ubc.ca> suggests this:
	arr <- array(rnorm(27),c(3,3,3))
	dimarr <- dim(arr)
	tmparr <- array(1:prod(dimarr),dimarr)
	sapply(c(3),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)
	sapply(c(3,17,13,5),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)
	
Of course, in R we can simplify the last two lines to
    sapply(<<argument goes here>>, function(x) which(tmparr==x,T))

However, wearing my "computer scientist" hat, I have to wonder about costs.
This is basically the equivalent of the APL "decode" operator.

Let's define

    index.decode <- function (index, array) {
	dimarr <- dim(arr)
	tmparr <- array(1:prod(dimarr), dimarr)
	sapply(index, function(x) which(tmparr == x, T))
    }

The result is a matrix with C=length(index) columns
and R=length(dim(array)) rows.  This has to take time O(R*C), because
the result occupies O(R*C) space and all of it has to be defined.

Now it is possible to implement index.decode so that it does take O(R*C)
time.   Here's an outline, which I shan't bother to finish.  (I'd do
ndims==4 and the general case if I were going to finish it.  I'd also
have a drop= argument to handle the case where length(index)==1.)

    index.decode <- function (index, array) {
	jndex <- index - 1
	dimarr <- dim(arr)
	ndims <- length(dimarr)
	if (ndims == 1) {
	    rbind(index)
	} else
	if (ndims == 2) {
	    rbind(jndex %% dimarr[1] + 1, jndex %/% dimarr[1] + 1)
	} else
	if (ndims == 3) {
	    rbind(jndex %% dimarr[1] + 1,
	          (jndex %/% dimarr[1]) %% dimarr[2] + 1,
	          jndex %/% (dimarr[1]*dimarr[2]) + 1)
        } else {
            stop("length(dims(array)) > 3 not yet implemented")
	}
    }

This is clearly O(R*C).  What about the
sapply(index, function(x) which(tmparr==x, T))
approach?

tmparr is of size prod(dimarr); call that P.  The expression tmparr==x
has to examine each element of tmparr, so that's O(P).  This is done
for each element of index (C times), so the total is O(P*C).

Consider

    mega <- array(1:1000000, c(100,100,100))
    inxs <- as.integer(runif(10000, min=1, max=1000000))

Here C = length(inxs) = 10000, R = length(dim(mega)) = 3,
P = prod(dim(mega)) = 1000000.  O(R*C) is looking *really* good
compared with O(P*C).

> system.time(index.decode(inxs, mega))
[1] 0.03 0.00 0.03 0.00 0.00
> system.time(slow.decode(inxs, mega))
[1] 3.51 0.79 4.33 0.00 0.00

Mind you, on a 500MHz UltraSPARC, I had to use big arrays to get any
measurable time at all...



From tord.snall at ebc.uu.se  Thu Jul 31 10:03:25 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 31 Jul 2003 10:03:25 +0200
Subject: [R] how as.numeric() !-> factor
Message-ID: <3.0.6.32.20030731100325.00cbef28@mail.anst.uu.se>

Dear all,

I have divided two vectors:

Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site), 
         Np.occup97.98 = sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98))

> Np.occup97.98
      site     Np.occup97.98
1  erken97 0.342592592592593
2  erken98 0.333333333333333
3 rormyran  0.48471615720524
4  valkror 0.286026200873362

However, at a later stage of the analysis I want
> round(Np.occup97.98[,2], 2)
Error in Math.factor(x, digits) : "round" not meaningful for factors

neither did this work:

> round(Np.occup97.98[,2], 2)
Error in Math.factor(x, digits) : "round" not meaningful for factors

or this:

> round(as.numeric(Np.occup97.98[,2]), 2)
[1] 3 2 4 1
> 

because, as clearly written in the help file:
"as.numeric for factors yields the codes underlying the factor levels, not
the numeric representation of the labels."

I've discovered this solution:

> Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site), 
+          Np.occup97.98 =
round(sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98,2)))
> 
> Np.occup97.98
      site Np.occup97.98
1  erken97          0.34
2  erken98          0.33
3 rormyran          0.48
4  valkror          0.29


However, I would like to do this rounding later.

Could someone give a tip. I think that I would have been helped by a
sentence in help(as.numeric).


Thanks in advance.


Sincerely,
Tord




-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From ripley at stats.ox.ac.uk  Thu Jul 31 10:06:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 Jul 2003 09:06:55 +0100 (BST)
Subject: [R] how as.numeric() !-> factor
In-Reply-To: <3.0.6.32.20030731100325.00cbef28@mail.anst.uu.se>
Message-ID: <Pine.LNX.4.44.0307310904120.3752-100000@gannet.stats>

You would have been helped by looking in the FAQ: please do so now.

Also, ?factor tells you not to use as.numeric.

On Thu, 31 Jul 2003, Tord Snall wrote:

> Dear all,
> 
> I have divided two vectors:
> 
> Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site), 
>          Np.occup97.98 = sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98))
> 
> > Np.occup97.98
>       site     Np.occup97.98
> 1  erken97 0.342592592592593
> 2  erken98 0.333333333333333
> 3 rormyran  0.48471615720524
> 4  valkror 0.286026200873362
> 
> However, at a later stage of the analysis I want
> > round(Np.occup97.98[,2], 2)
> Error in Math.factor(x, digits) : "round" not meaningful for factors
> 
> neither did this work:
> 
> > round(Np.occup97.98[,2], 2)
> Error in Math.factor(x, digits) : "round" not meaningful for factors
> 
> or this:
> 
> > round(as.numeric(Np.occup97.98[,2]), 2)
> [1] 3 2 4 1
> > 
> 
> because, as clearly written in the help file:
> "as.numeric for factors yields the codes underlying the factor levels, not
> the numeric representation of the labels."
> 
> I've discovered this solution:
> 
> > Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site), 
> +          Np.occup97.98 =
> round(sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98,2)))
> > 
> > Np.occup97.98
>       site Np.occup97.98
> 1  erken97          0.34
> 2  erken98          0.33
> 3 rormyran          0.48
> 4  valkror          0.29
> 
> 
> However, I would like to do this rounding later.
> 
> Could someone give a tip. I think that I would have been helped by a
> sentence in help(as.numeric).
> 
> 
> Thanks in advance.
> 
> 
> Sincerely,
> Tord
> 
> 
> 
> 
> -----------------------------------------------------------------------
> Tord Sn?ll
> Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
> Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
> Villav?gen 14			
> SE-752 36 Uppsala, Sweden
> Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
> Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
> Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
> E-mail: Tord.Snall at ebc.uu.se
> Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From luke at inpharmatica.co.uk  Thu Jul 31 11:01:09 2003
From: luke at inpharmatica.co.uk (Luke Whitaker)
Date: Thu, 31 Jul 2003 10:01:09 +0100 (BST)
Subject: [R] A model for disease progression
In-Reply-To: <Pine.SOL.3.96.1030729143046.10889A-100000@draco.cus.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0307310949330.28028-100000@dollis-hill.inpharmatica.co.uk>

On Tue, 29 Jul 2003, Damon Wischik wrote:
> 
> As I described before, I have a snapshot of a population taken at a
> certain time. I am interested in an age-related disease, which progresses
> healthy->A->B. (There is no recovery.) For each individual, I know their
> age (in years) and the stage of the disease. There are roughly 800 cases,
> with ages spanning 40 years.
> 

I haven't followed this thread so I don't know if it has already been
mentioned, but if all you have is a snapshot, then any inference you
make on disease progression will have to be based on the assumption
that both the disease and the population demographics are in steady
state. I would think that in many real world disease/population
cases this will be a very poor assumption. I'm afraid I can't see
any obvious way around this other than to look for some sort of
cohort/followup data.

Regards,

Luke Whitaker.



From s-plus at wiwi.uni-bielefeld.de  Thu Jul 31 12:00:05 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 31 Jul 2003 12:00:05 +0200
Subject: [R] reverse array indexing
References: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>
Message-ID: <3F28E8A5.4070905@wiwi.uni-bielefeld.de>

Richard A. O'Keefe wrote:

>Jerome Asselin <jerome at hivnet.ubc.ca> suggests this:
>	arr <- array(rnorm(27),c(3,3,3))
>	dimarr <- dim(arr)
>	tmparr <- array(1:prod(dimarr),dimarr)
>	sapply(c(3),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)
>	sapply(c(3,17,13,5),function(x,tmparr) which(tmparr==x,T),tmparr=tmparr)
>	
>Of course, in R we can simplify the last two lines to
>    sapply(<<argument goes here>>, function(x) which(tmparr==x,T))
>
>However, wearing my "computer scientist" hat, I have to wonder about costs.
>This is basically the equivalent of the APL "decode" operator.
>
>Let's define
>
>    index.decode <- function (index, array) {
>	dimarr <- dim(arr)
>	tmparr <- array(1:prod(dimarr), dimarr)
>	sapply(index, function(x) which(tmparr == x, T))
>    }
>
>The result is a matrix with C=length(index) columns
>and R=length(dim(array)) rows.  ...
>  
>
I think you mean the APL encode operator?

 > index<-1:8
 > encode(index-1,c(2,2,2))+1
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    1    1    1    2    2    2    2
[2,]    1    1    2    2    1    1    2    2
[3,]    1    2    1    2    1    2    1    2

download code of encode from:  
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/decodeencode.rev

Peter Wolf



From borgulya at gyer2.sote.hu  Thu Jul 31 12:42:15 2003
From: borgulya at gyer2.sote.hu (=?ISO-8859-1?Q?BORGULYA_G=E1bor?=)
Date: Thu, 31 Jul 2003 12:42:15 +0200
Subject: [R] Sorting a data frame
In-Reply-To: <1058366471.2399.108.camel@localhost>
References: <6B5A9304046AD411BD0200508BDFB6CB021F0DAB@gimli.middleearth.kssg.com>
	<1058366471.2399.108.camel@localhost>
Message-ID: <3F28F287.8070801@gyer2.sote.hu>

Hi!

I think this is an important example! Is there a way to make it included 
in the help of order?

Maybe a shortened version:

# sorting a data frame
df <- data.frame(V1 = c("W","A", "A", "B", ""), V2 = c("E", "M", "B", 
"O", "Q"))
sorted <- df[order(df$V1, df$V2), ]


And a question: what to do to have the sorted data frame with the row 
labels 1:n?

G?bor

----------------------------------------------------------------
Marc Schwartz ?rta:
> On Wed, 2003-07-16 at 08:42, Wayne Jones wrote:
>>Does anyone know if it is possible to sort a dataframe?

> Example:

# Create two column df
df <- data.frame(V1 = c("W","A", "A", "B", ""), V2 = c("E", "M", "B", 
"O", "Q"))
# show df unsorted
df

   V1 V2
1  W  E
2  A  M
3  A  B
4  B  O
5     Q

# now sort df, by V1, then V2
df[order(df$V1, df$V2), ]

   V1 V2
5     Q
3  A  B
2  A  M
4  B  O
1  W  E



From pcovelli at tin.it  Thu Jul 31 15:19:06 2003
From: pcovelli at tin.it (Paolo Covelli)
Date: Thu, 31 Jul 2003 15:19:06 +0200
Subject: [R] clear screen
Message-ID: <000701c35766$5269c510$e90f6850@paolo>

Hi, 
with CTRL + L, I obtain a manual "clear screen", 
is there also an analogous command to insert in a script?

thanks in advance.

Paolo



From erwan.barret at wanadoo.fr  Thu Jul 31 14:27:41 2003
From: erwan.barret at wanadoo.fr (Erwan BARRET)
Date: Thu, 31 Jul 2003 14:27:41 +0200 (CEST)
Subject: [R] history for graphics
Message-ID: <32343159.1059654461696.JavaMail.www@wwinf0203>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/2572e882/attachment.pl

From erwan.barret at wanadoo.fr  Thu Jul 31 14:26:49 2003
From: erwan.barret at wanadoo.fr (Erwan BARRET)
Date: Thu, 31 Jul 2003 14:26:49 +0200 (CEST)
Subject: [R] help with graphics
Message-ID: <32235510.1059654409566.JavaMail.www@wwinf0203>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/34e71549/attachment.pl

From lottobv at netscape.net  Thu Jul 31 13:22:03 2003
From: lottobv at netscape.net (ANN ROSS)
Date: Thu, 31 Jul 2003 13:22:03 +0200
Subject: [R] AWARD NOTIFICATION
Message-ID: <E19iBdM-00042Z-00@bernie.ethz.ch>

                INTERNATIONAL LOTTO BV
                  BURDENSTRAAT 21B,
                  2053 DS AMSTERDAM,
                  THE NETHERLANDS.

FROM: THE DESK OF THE PROMOTIONS MANAGER,
INTERNATIONAL PROMOTIONS/PRIZE AWARD DEPARTMENT,
REF: EGS/2551256003/04
BATCH: 14/0017/IPD

ATTENTION:
        RE/ AWARD NOTIFICATION; FINAL NOTICE

We are pleased to inform you of the announcement
today, 31TH JULY 2003, of winners of the DELOTTO
NETHERLANDS SWEEPSTAKE LOTTERY/ INTERNATIONAL PROGRAMS
held on 31ST OCTOBER 2002, as part of this year
bonanza.

Your company, attached to ticket number
025-1146-1992-750, with serial number 2113-05 drew the
lucky numbers 13-15-22-37-39-43, and consequently won
the lottery in the 3rd category.

You have therefore been approved for a lump sum pay
out of Euro 1,000,000.00 in cash credited to file REF
NO. EGS/2551256003/04. This is from total prize money
of Euro 20,400,000.00 shared among the seventeen
international winners in this category. All
participants were selected through a computer ballot
system drawn form 25,000 names from Middle East, Asia,
Africa, America, Europe and North America as part our
International Promotions Program, which is conducted
annually.

CONGRATULATIONS!

Your fund is now deposited with a Finance and Security
House insured in your name. Due to the mix up of some
numbers and names, we ask that you keep this award
strictly from public notice until your claim has been
processed and your money remitted to your account.
This is part of our security protocol to avoid double
claiming or unscrupulous acts by participants of this
program.

We hope with a part of you prize, you will participate
in our end of year high stakes Euro 1.3 billion
International Lottery.

To begin your claim, please contact your claim agent;
MRS ANN ROSS,
FOREIGN SERVICE MANAGER,
SPRINT SECURITY BV,
AMSTERDAM, n.l
TEL/FAX:31 647556091 
EMAIL:annross200 at netscape.net

For due processing and remittance of your prize money
to a designated account of your choice.

Remember, you must contact your claim agent not later
than 19TH AUGUST 2003. After this date, all funds
will be returned as unclaimed.

NOTE: In order to avoid unnecessary delays and
complications, please remember to quote your reference
and batch numbers in every one of your correspondences
with your agent. Furthermore, should there be any
change of your address, do inform your claims agent as
soon as possible.

Congratulations again from all our staff and thank you
for being part of our promotions program.


Sincerely,

THE PROMOTIONS MANAGER,
DELOTTO NETHERLANDS SWEEPSTAKES LOTTERY.

N.B. Any breach of confidentiality on the part of the
winners will result to disqualification. Please do not
reply to this mail. Contact your claim agent. 


  

From jrogers at cantatapharm.com  Thu Jul 31 15:04:34 2003
From: jrogers at cantatapharm.com (Jim Rogers)
Date: Thu, 31 Jul 2003 09:04:34 -0400
Subject: [R] Write XML according to ggobi DTD
Message-ID: <99A12772DCDEEB458B996332957B0D5301181B@mercury.cantatapharm.com>

If case anyone else is interested, Duncan Temple Lang answered my
question on the ggobi-help mailing list. The code he provided is pasted
into the body of this email, below. It requires the the XML package
(from http://www.stats.ox.ac.uk/pub/RWin/ for Windows and
www.omegahat.org/RSXML/ for everything else). It works with data.frames
with factor and numeric columns. Character columns need to be coverted
to factor.

It sounds like this code will be available in the next build of Rggobi
for Windows, which Duncan informs me is in the not too distant future. 

Thanks, Duncan!

Jim 

> Jim Rogers wrote:
> > 
> > Hi,
> > 
> > Has anyone out there written a function to take an R data.frame as
> > input and generate XML that conforms to the DTD for ggobi 
> > ("ggobi.dtd")? In other words, like a simple version of the 
> writeSDML
> > function in the StatDataML package, but using ggobi.dtd instead of
> > StatDataML.dtd.
> > 
> > It looks easy to write such a function to handle
> data.frames with only
> > numeric data, but a bit of work with character and factor data.
> > 
> > Thanks,
> > Jim Rogers


#
# This can be used to write out an XML description of one or more
# data frames into GGobi's data format.
#


  # Could use another output tree mechanism such as xmlTree(),
xmlOutputBuffer(), etc.
  # e.g.
  #  dom <- xmlTree("ggobidata", attrs = c(count = length(args)))

writeDataXML <-
function(..., dom = xmlOutputDOM("ggobidata", attrs = c(count =
length(args))))  
{
  library(XML)

  args <- list(...)

  for(i in 1:length(args)) {
    name <- names(args)[i]
    # if this is "", use the deparse() version
    addXMLDataset(args[[i]], name, dom)
  }

  dom
}  

addXMLDataset <-
function(data, name, dom, description = NULL, asElements = TRUE)
{

  dom$addTag("data", attrs=c(name=name), close = FALSE)
  dom$addTag("description", description)

  dom$addTag("variables", attrs=c(count = ncol(data)), close=FALSE)
  for(i in names(data)) {
    if(inherits(data[[i]], "factor")) {
      dom$addTag("categorical", attrs = c(name = i), close = FALSE)
        levs <- levels(data[[i]])
        dom$addTag("levels", attrs = c(count=length(levs)), close =
FALSE)
        for(j in 1:length(levs)) {
           dom$addTag("level", levs[j], attrs= c(value=j))
        }
      dom$closeTag("levels")
      dom$closeTag("categorical")
    } else
      dom$addTag("realvariable", attrs = c(name = i))
  }

  dom$addTag("records", attrs =c(count = nrow(data)), close = FALSE)

  rownames <- dimnames(data)[[1]]
  for(i in 1:nrow(data)) {
      # If we want to put <el>value</el><el>value</el> within the
<record>
      # we'll have to do it one at a time!
   if(asElements) {
     dom$addTag("record", close = FALSE)
     for(r in data[i,]) {
        tag <- switch(typeof(r), double="real", integer="int")
        dom$addTag(tag, r)
     }
     dom$closeTag("record")
   } else
     dom$addTag("record", paste(data[i,], collapse=" "), attrs = c(label
= rownames[i]), close = TRUE)
  }
  dom$closeTag("records")
  
  dom$closeTag("data")
}  

###
# Example:
###

data(mtcars)
b = writeDataXML(mtcars)
sink("mtcars.xml")
cat("<?xml version='1.0'?>\n")
b$value()
sink()



From andreas.eckner at soundinvest.net  Thu Jul 31 13:39:55 2003
From: andreas.eckner at soundinvest.net (Andreas Eckner)
Date: Thu, 31 Jul 2003 13:39:55 +0200
Subject: [R] Problem with data.frames
Message-ID: <BB4ECCAB.DA%andreas.eckner@soundinvest.net>

Hi,

I just encountered a problem in R that may easily be fixed: If one uses
attach for a data.frame e.g. 10000 times and forgets detach, then R gets
incredibly slow (less then 10% of the original speed).

My system:

platform powerpc-apple-darwin6.0
arch     powerpc   
os       darwin6.0 
system   powerpc, darwin6.0
status             
major    1         
minor    6.1       
year     2002      
month    11        
day      01        
language R    


Kind regards,
Andreas Eckner



From dapayne at purdue.edu  Thu Jul 31 15:44:09 2003
From: dapayne at purdue.edu (Donnie Payne)
Date: Thu, 31 Jul 2003 08:44:09 -0500
Subject: [R] R on Solaris 9
Message-ID: <000b01c35769$d1553230$d5a5d280@genomics.purdue.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/be4eb5d0/attachment.pl

From djw1005 at cam.ac.uk  Thu Jul 31 15:46:07 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Thu, 31 Jul 2003 14:46:07 +0100 (BST)
Subject: [R] A model for disease progression
In-Reply-To: <Pine.SOL.3.96.1030729143046.10889A-100000@draco.cus.cam.ac.uk>
Message-ID: <Pine.SOL.3.96.1030731143549.15677C-100000@virgo.cus.cam.ac.uk>


Thank you for the continued comments on this problem. I think I have a
solution, which I thought I'd share here, in the hope that any obvious
errors or inefficiencies can be pointed out.

As I described before, I have a snapshot of a population taken at a
certain time. I am interested in an age-related disease, which progresses
healthy->A->B. (There is no recovery.) For each individual, I know their
age (in years) and the stage of the disease.

Suppose I'm interested in the transition healthy->A. For each individual I
have a censored observation of the "lifetime" random variable:
   if the individual is age t and is diseased, lifetime is in (0,t].
   if the individual is age t and is healthy, lifetime is in (t,inf)

The Surv function in R does not deal with this sort of censored data. 
Happily, Mai Zhou has written a package called dblcens, available on CRAN,
which will estimate the survival curve. 

So I can work out the lifetime T0A for the transition healthy->A, and the
lifetime T0B for the transition healthy->B. I would like to know about the
time for the transition A->B, where
  T0B = T0A + TAB.
This is a deconvolution problem. It cannot be solved in general, because
knowing T0A and T0B is insufficient to determine the joint distribution of
(T0A,TAB). If I assume that T0A and TAB are independent, it can be solved.

I used an ad-hoc solution: find the estimated distribution TABe which
minimizes the mean-square-error distance between the survival function for
T0B and that for T0A+TABe (i.e. the convolution of T0A+TABe, for which R
has a handy function convolve). I did this by optimizing over positive
measures for TABe with optim, and tweaking a Lagrange multiplier to make
the the optimum be a distribution. 

I can then plot survival curves for T0A, T0B and T0A+TABe. This lets me
visualize whether the assumption of independence is good.

Damon.



From m.mader at gsf.de  Thu Jul 31 15:57:43 2003
From: m.mader at gsf.de (Michael Mader)
Date: Thu, 31 Jul 2003 15:57:43 +0200
Subject: [R] RFE implemented?
Message-ID: <3F292057.E89BCF10@gsf.de>

Hi,

is there an implementation of RFE (recursive feature elimination; see
e.g. Guyon et al. 2002, Furlanello et al. 2003) existing in R?

Thanks for your help!

Regards

Michael
-- 
Michael T. Mader
Institute for Bioinformatics/MIPS, GSF
Ingolstaedter Landstrasse 1
D-80937 Neuherberg
0049-89-3187-3576
 
In statistics, some people worry about not seeing the forest for the
trees.
I like to look at the bark. (C. R. Blyth, 1967)



From abigail.montgomeryrg at mail4her.com  Thu Jul 31 17:59:07 2003
From: abigail.montgomeryrg at mail4her.com (Abigail Montgomery)
Date: Thu, 31 Jul 2003 15:59:07 +0000
Subject: [R] Another Medical breakthrough
Message-ID: <0eff01c3577c$2334c33c$baff4b6b@q5z607l>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/aa989455/attachment.pl

From tplate at blackmesacapital.com  Thu Jul 31 16:48:57 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 31 Jul 2003 08:48:57 -0600
Subject: [R] how as.numeric() !-> factor
In-Reply-To: <3.0.6.32.20030731100325.00cbef28@mail.anst.uu.se>
Message-ID: <5.2.1.1.2.20030731084042.04675438@mailhost.blackmesacapital.com>

The problem is that the 2nd column in your data frame has been converted 
into a factor.  This happened because you used cbind() with mixed character 
and numeric vectors.  cbind() with these types of arguments will construct 
a character matrix.  Then when you passed that character matrix to 
as.data.frame() it converted both columns to factors.

Here's a simpler example of what happened:

 > cbind(letters[1:2], c(1,3))
      [,1] [,2]
[1,] "a"  "1"
[2,] "b"  "3"
 > x <- as.data.frame(cbind(letters[1:2], c(1,3)))
 > x
   V1 V2
1  a  1
2  b  3
 > as.numeric(x[,2])
[1] 1 2
 > as.numeric(as.character(x[,2]))
[1] 1 3
 >

With the data frame as you constructed it, you need an expression like 
round(as.numeric(as.character(Np.occup97.98[,2])), 2) to accomplish what 
you want.  It would probably be better to construct a more felicitous data 
frame in the first place:

 > df <- data.frame(site = levels(sums$site), Np.occup97.98 = 
sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98)

(unless of course you had some unstated reason for constructing the data 
frame the way you did)

-- Tony Plate

At Thursday 10:03 AM 7/31/2003 +0200, Tord Snall wrote:
>Dear all,
>
>I have divided two vectors:
>
>Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site),
>          Np.occup97.98 = sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98))
>
> > Np.occup97.98
>       site     Np.occup97.98
>1  erken97 0.342592592592593
>2  erken98 0.333333333333333
>3 rormyran  0.48471615720524
>4  valkror 0.286026200873362
>
>However, at a later stage of the analysis I want
> > round(Np.occup97.98[,2], 2)
>Error in Math.factor(x, digits) : "round" not meaningful for factors
>
>neither did this work:
>
> > round(Np.occup97.98[,2], 2)
>Error in Math.factor(x, digits) : "round" not meaningful for factors
>
>or this:
>
> > round(as.numeric(Np.occup97.98[,2]), 2)
>[1] 3 2 4 1
> >
>
>because, as clearly written in the help file:
>"as.numeric for factors yields the codes underlying the factor levels, not
>the numeric representation of the labels."
>
>I've discovered this solution:
>
> > Np.occup97.98<- as.data.frame(cbind(site = levels(sums$site),
>+          Np.occup97.98 =
>round(sums$Ant.Nptrad97.98/Ant.trad$Ant.trad97.98,2)))
> >
> > Np.occup97.98
>       site Np.occup97.98
>1  erken97          0.34
>2  erken98          0.33
>3 rormyran          0.48
>4  valkror          0.29
>
>
>However, I would like to do this rounding later.
>
>Could someone give a tip. I think that I would have been helped by a
>sentence in help(as.numeric).
>
>
>Thanks in advance.
>
>
>Sincerely,
>Tord
>
>
>
>
>-----------------------------------------------------------------------
>Tord Sn?ll
>Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
>Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
>Villav?gen 14
>SE-752 36 Uppsala, Sweden
>Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
>Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
>Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
>E-mail: Tord.Snall at ebc.uu.se
>Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From michel.germain at remvie.fr  Thu Jul 31 17:02:04 2003
From: michel.germain at remvie.fr (GERMAIN Michel)
Date: Thu, 31 Jul 2003 17:02:04 +0200
Subject: [R] problem with summary in survival analysis
Message-ID: <000101c35774$b44e5080$120e0ec6@XP018>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/c5fc8adc/attachment.pl

From dyang at nrcan.gc.ca  Thu Jul 31 17:09:10 2003
From: dyang at nrcan.gc.ca (Yang, Richard)
Date: Thu, 31 Jul 2003 11:09:10 -0400
Subject: [R] Trouble with optim
Message-ID: <F0E0B899CB43D5118D220002A55113CF02F9276A@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear All;

	Searching on the achieve, many questions on optim() have been asked,
but I haven't seen the following.

	The question began with my original inquiry on "Optimization failed
in fitting mixture 3-parameter Weibul l distribution using fitdistr()" which
I posted on Jul. 28, Prof. Ripley kindly advised me to look into options of
optim() for the answer. Following his advice and constraining the location
parameters (a, and a1 in the mixture distribution of 3-parameter Weibull
density) in the objective function, my first attempt fitting the mixture
distribution seemed successful:

> mwb.test3 <- optim(c(0.6,8.87, 100, 4.0, 0.001, 89.1, 2.99), mwb3b.obj, x
= x5, hessian = T,
+          control = list(maxit = 2000))
Warning messages: 
1: NaNs produced in: log(x) 
2: NaNs produced in: log(x) 
3: NaNs produced in: log(x) 
4: NaNs produced in: log(x) 
5: NaNs produced in: log(x) 
6: NaNs produced in: log(x) 
7: NaNs produced in: log(x) 
8: NaNs produced in: log(x) 
9: NaNs produced in: log(x) 
10: NaNs produced in: log(x) 
> rbind(mwb.test3$par, se = sqrt(diag(solve(mwb.test3$hessian))))
        [,1]      [,2]      [,3]     [,4]    [,5]     [,6]     [,7]
   0.5367797  12.70957  91.05743 5.269290 3.26119 81.64733 2.506190
se       NaN 138.88411 135.21737 6.749831     NaN      NaN      NaN
Warning message: 
NaNs produced in: sqrt(diag(solve(mwb.test3$hessian))) 

	In an attempt to obtain se for the estimated parameters, I resorted
to the "L-BFGS-B" method:

> mixwb3.nl1 <- optim(p0, mwb3b.obj, wbmix.gr, method = "L-BFGS-B",
+              hessian = T,
+              lower = c(0, rep(0, 6)), upper = c(1, rep(Inf, 6)), x = x5)
> rbind(est = mixwb3.nl1$par, se = sqrt(diag(solve(mixwb3.nl1$hessian))))
             p        a        b         c        a1       b1        c1
est 0.50843845 4.519553 90.11219 3.0334775 0.6799724 88.06625 3.5105745
se  0.05461006 9.646768 11.02222 0.3451899 9.8708639 10.15148 0.1797819

	Everything looks fine and dandy,  but ...

> mixwb3.nl1$convergence
[1] 52
> mixwb3.nl1$message
[1] "ERROR: ABNORMAL_TERMINATION_IN_LNSRCH"

	So, my questions are:

	1) How to obtain se for estimated parameters from the Nelder-Mead
method?

	2) Is the error message in the L-BFGS-B method serious? How to
circumvent it?

	Any thoughts and suggestions?

	TIA,

Richard Yang

Northern Forestry Centre   /	Centre de foresterie du Nord
Canadian Forest Service	   /	Service canadien des for?ts
Natural Resources Canada   /	Ressources naturelles Canada
5320-122 Street     	   /	5320, rue 122

Edmonton (Alberta) Canada
T6H 3S5



From p.b.pynsent at bham.ac.uk  Thu Jul 31 18:01:50 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Thu, 31 Jul 2003 17:01:50 +0100
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <x2d6fs53p1.fsf@biostat.ku.dk>
Message-ID: <4BAC6C52-C370-11D7-B2EC-003065F42152@bham.ac.uk>

Thank you for this, infact I have just discovered that adj is much more 
predictable and does what  I want.
I still remain surprised that when I rotate some text 180 degrees I do 
not get an upside down version of  the unrotated text relative to the 
point. As the reasons for this are clear to Prof. Ripley and yourself, 
it just demonstrates my ignorance of R.

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> On Tue, 29 Jul 2003, p.b.pynsent wrote:
>
>>> What I had hoped for was what I get with srt=0, rotated as whole so 
>>> it
>>> looked the same but was just rotated to the specified angle.
>>
>> But you got what is documented, and my example addessed that.
>
> The docs are arguably a bit unclear when it comes to combining 'srt'
> and 'pos'. You might get more easily predictable results by using the
> 'adj' argument instead of 'pos'.
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>


Prof. P. B. Pynsent,
Research and Teaching Centre,
Royal Orthopaedic Hospital,
Birmingham, B31 2AP, U.K.



From p.b.pynsent at bham.ac.uk  Thu Jul 31 18:02:09 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Thu, 31 Jul 2003 17:02:09 +0100
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <Pine.LNX.4.44.0307291303310.3555-100000@gannet.stats>
Message-ID: <5742B4FE-C370-11D7-B2EC-003065F42152@bham.ac.uk>


On Tuesday, July 29, 2003, at 01:04  pm, Prof Brian Ripley wrote:
>>>> What have I done wrong?
>>>> In addition, on my screen, the arrow symbols do not rotate at all
>>>> although they do on the pdf image.
>>>> (R version 1.71,  MacOS X 10.2.6)
>>>
>>> *WHICH* Mac port of R?  There are two!
>>>
>>> The lack of rotation is a limitation of the (unspecified) graphics
>>> device
>>> of your (unspecified) port, whichever it is: it works on Linux and
>>> Windows, for example, so I would expect this to work on an X11 device
>>> on
>>> the Darwin port.
>>>
>> Yes this is ambiguous,sorry.  Not the Darwin port.
>

I have now tested the Darwin and OS X versions on my Apple. As you 
predicted, the arrows are correctly rotated on the X11 screen so this 
is some error  on the OS X port.


Dr. P. B. Pynsent,
Research and Teaching Centre,
Royal Orthopaedic Hospital,
Birmingham, B31 2AP, U.K.



From anna at ptolemy.arc.nasa.gov  Thu Jul 31 18:18:31 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Thu, 31 Jul 2003 09:18:31 -0700
Subject: [R] anova
Message-ID: <200307310918.31710.anna@ptolemy.arc.nasa.gov>

I am totally confused as to how to use anova.  I have three vectors and would 
like to use anova on them but I don't understand how lm or glm comes into 
play.  In matlab, you just give the three vectors.  Why isn't it the same in 
R?


Any help would be greatly appreciated.


Anna



From jerome at hivnet.ubc.ca  Thu Jul 31 18:30:26 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 31 Jul 2003 09:30:26 -0700
Subject: [R] timezones
In-Reply-To: <20030731041936.A6DFC11F32@sitemail.everyone.net>
References: <20030731041936.A6DFC11F32@sitemail.everyone.net>
Message-ID: <200307311636.JAA18338@hivnet.ubc.ca>


I share your concerns regarding Problems 1 and 2. However, I am unable to 
provide help on those at this moment.

As for Problem 3, an alternative for the time being would be to use 
another package such as chron or date, although it would be preferable to 
use the classes of the base package if possible.

Sorry I can't be more helpful.

Jerome

On July 30, 2003 09:19 pm, Gabor Grothendieck wrote:
> I have some questions and comments on timezones.
>
> Problem 1.
>
> # get current time in current time zone
>
> > (now <- Sys.time())
>
> [1] "2003-07-29 18:23:58 Eastern Daylight Time"
>
> # convert this to GMT
>
> > (now.gmt <- as.POSIXlt(now,tz="GMT"))
>
> [1] "2003-07-29 22:23:58 GMT"
>
> # take difference
>
> > now-now.gmt
>
> Time difference of -5 hours
>
> Note that the difference between the times displayed by the first two
> R expressions is -4 hours.  Why does the last expression return
> -5 hours?
>
>
> Problem 2.  Why do the two expressions below give different answers?
> I take the difference between two dates in GMT and then repeat it in the
> current time zone (EDT).
>
> # days since origin in GMT
>
> > julian(as.POSIXct("2003-06-29",tz="GMT"),origin=as.POSIXct("1899-12-30
> >",tz="GMT"))
>
> Time difference of 37801 days
>
> # days since origin in current timezone
>
> > julian(as.POSIXct("2003-06-29"),origin=as.POSIXct("1899-12-30"))
>
> Time difference of 37800.96 days
>
>
> I thought this might be daylight savings time related but even with
>
> standard time I get:
> > julian(as.POSIXct("2003-06-29",tz="EST"),origin=as.POSIXct("1899-12-30
> >",tz="EST"))
>
> Time difference of 37800.96 days
>
>
> Problem 3. What is the general strategy of dealing with dates, as
> opposed to datetimes, in R?
>
> I have had so many problems and a great deal of frustration, mostly
> related to timezones.
>
> The basic problem is that various aspects of the date such as the year,
> the month, the day of the month, the day of the week can be different
> depending on the timezone you use.  This is highly undesirable since
> I am not dealing with anything more granular than a day yet timezones,
> which are completely extraneous to dates and by all rights should not
> have to enter into my problems, keep fowling me up.
>
> A lesser problem is that I find myself using irrelevant constants such
> as the number of seconds in a day which you would think would be
> something I would not have to deal with since I am concerned with daily
> data.
>
> With R have nifty object oriented features I think a good project would
> be to implement a class in the base that handled dates without times
> (or timezones!)
>
> P.S. I have sent an earlier version of this but did not see it posted
> so if both get posted please ignore the prior one since this one has
> more info in it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mroberts at ers.usda.gov  Thu Jul 31 18:41:15 2003
From: mroberts at ers.usda.gov (Michael Roberts)
Date: Thu, 31 Jul 2003 12:41:15 -0400
Subject: [R] spatial statistics vs. spatial econometrics
Message-ID: <sf290f2d.026@ers.usda.gov>

Dear R users,

I am putting together reading and resources lists for spatial statistics and spatial econometrics and am looking for some pointers from more experienced practitioners.

In particular, I find two "camps" in spatial modelling, and am wondering which approach is better suitied to which situation.  

The first camp is along the lines of Venables and Ripley's Chapter 14 (and presumably Ripley's book, but I don't have that yet)--spatial trends and kriging (e.g., the geoR package);  the second along the lines of Anselin's book--spatial lag and spatial-autocorrelation models (e.g., the spdep package).

As far as I can tell, these amount to the same thing (in princple).  The first camp likes to use row-standardized "weight matricies" in building covariance structures (to ensure there isn't too much dependence?).  I find this very unappealing to many models.  This camp doesn't seem to look at variograms or correlegrams as often--they just fit the model, which I also find unappealing.  The covariance structures also tend to be very simple.  It looks like there is more flexibility in the second camp.

Mixed model procedures also seem to have spatial covariance structures.

Is there a reason why there appears to be so few cross references between these camps?  What makes each approach best for different kinds of problems?

I'd greatly appreciate your insights.

Many thanks,


Michael J. Roberts

Resource Economics Division
Production, Management, and Technology
USDA-ERS
(202) 694-5557 (phone)
(202) 694-5775 (fax)



From Kosenkov.Kirill at nac.spb.ru  Thu Jul 31 20:00:13 2003
From: Kosenkov.Kirill at nac.spb.ru (Kosenkov Kirill)
Date: Thu, 31 Jul 2003 22:00:13 +0400
Subject: [R] help with tapply and weighted.mean
Message-ID: <3F29592D.6040200@nac.spb.ru>

Hello!

I have data frame with 'weights' in one of the columns. I need to 
compute weighted mean on another column other factor variable and 
i am trying to:

res<-tapply(data$k,list(data$model),weighted.mean,w=data$w,na.rm=T)

and i get:

Warning messages:
1: longer object length
	is not a multiple of shorter object length in: x * w
2: longer object length
	is not a multiple of shorter object length in: x * w
3: longer object length
	is not a multiple of shorter object length in: x * w
4: longer object length
	is not a multiple of shorter object length in: x * w
5: longer object length
	is not a multiple of shorter object length in: x * w
6: longer object length
	is not a multiple of shorter object length in: x * w
7: longer object length
	is not a multiple of shorter object length in: x * w
8: longer object length
....

What i am doing wrong? How i can pass vector of weights to the 
'weighted.mean' when using 'tapply'?

Thanks!



From mavaillant at saiinc.qc.ca  Thu Jul 31 20:53:09 2003
From: mavaillant at saiinc.qc.ca (Marc-Antoine Vaillant)
Date: Thu, 31 Jul 2003 14:53:09 -0400
Subject: [R] (no subject)
Message-ID: <A9938C35F8ABAF4C86C4C8753021AC8B0343BB@mtl00.saiinc.qc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030731/c2322125/attachment.pl

From journey_245 at nashvill.freeserve.co.uk  Wed Jul 30 20:34:07 2003
From: journey_245 at nashvill.freeserve.co.uk (Protection Alert)
Date: Thu, 31 Jul 2003 14:37:07 2003 14:37:07 +0000 EST
Subject: [R] You are not protected
Message-ID: <200307312127.h6VLROiv005859@stat.math.ethz.ch>

A non-text attachment was scrubbed...
Name: not available
Type: text
Size: 828 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20030731/cd9c1b4d/attachment.pl

From chrysopa at insecta.ufv.br  Thu Jul 31 16:21:30 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Thu, 31 Jul 2003 11:21:30 -0300
Subject: [R] Error in par
Message-ID: <200307311121.30717.chrysopa@insecta.ufv.br>

Hi,

I make a function and it work, but I get a error message in the follow step:

  image.plot(zlim = c(0,216),legend.width=0.2,legend.only=T,
             horizontal=T,col=palette(),nlevel=nivcor,offset=0.7,
             legend.shrink=1)

The error is:

Error in par(old.par) : invalid value specified for graphics parameter "fig".

The error appear not important, but i like to correct to eliminate de message.

Any idea

Thanks
Ronaldo
-- 
It now costs more to amuse a child than it once did to educate his father.
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From coupal at uwyo.edu  Tue Jul 22 16:37:06 2003
From: coupal at uwyo.edu (Roger Coupal)
Date: Tue, 22 Jul 2003 14:37:06 -0000
Subject: [R] packages on R on a Mac
Message-ID: <8B5EDD79-BC51-11D7-AF7F-000A956878F6@uwyo.edu>

Hello, i am new to R and have it on my Mac with OSX 10.2. I downloaded 
the systemfit package and tried to run it and it didn't work. I think i 
need to install the package, or update R to let it know that I have 
that in the library. (i simply placed the systemfit folder in the R 
library.) When i run update.packages(systemfit) or 
install.packages(systemfit) i get the following error:

Error: couldn't find function "install.packages"

So apparently just placing it in the library folder is not the right 
way to do it.  How do i add contributed packages? thanks.

Roger Coupal
Dept. of Agricultural and Applied Economics
University of Wyoming
Laramie, Wyoming
307-766-5246
http://agecon.uwyo.edu/


From p.b.pynsent at bham.ac.uk  Tue Jul 29 13:38:19 2003
From: p.b.pynsent at bham.ac.uk (p.b.pynsent)
Date: Tue, 29 Jul 2003 12:38:19 +0100
Subject: [R] Alignment when rotating text and symbols
In-Reply-To: <Pine.LNX.4.44.0307291116060.32516-100000@gannet.stats>
Message-ID: <26CF0B7C-C1B9-11D7-B2EC-003065F42152@bham.ac.uk>

Dear Prof Ripley,
Many thanks for this response. I do not understand how your example 
addresses my problem. Basically what concerns me is the relationship 
between pos=1 and pos=3 upon rotation, if I take your example and 
modify it slightly to be at srt=180 I would expect to get an inverted 
view of srt=0.
This is not the case, the spacing between the two lines of text becomes 
so different that at 180 degrees the text now fits within the non 
rotated text. At intermediate values the text remains aligned to the 
vertical of the plot rather than to the angle of rotation and so 
becomes offset.

What I had hoped for was what I get with srt=0, rotated as whole so it 
looked the same but was just rotated to the specified angle.

plot(1:10, type="n")
text(5,5, "a bit of text", pos=1)
text(5,5, "a bit of text", pos=3)
text(5,5, "a bit of text", pos=1, srt=180)
text(5,5, "a bit of text", pos=3, srt=180)

Paul

On Tuesday, July 29, 2003, at 11:26  am, Prof Brian Ripley wrote:

> On Tue, 29 Jul 2003, p.b.pynsent wrote:
>
>> I wanted to annotate some points on lines, above and below the lines.
>> I thought the easiest way would be to use text() and two pos values.
>> However I found when the text was rotated the space and alignment
>> between the point and the text did not remain constant. The following
>> code illustrates the problem:
>
> You are rotating the text about its centre, but pos is not in the
> rotated frame, as I read it, so why should this be rotation-invariant?
>
> Try
>
>> plot(1:10, type="n")
>> text(5,5, "a bit of text")
>> text(5,5, "a bit of text", pos=1)
>> text(5,5, "a bit of text", pos=1, srt=45)
>
> to see the effect of pos and srt.  You go down then rotate, not rotate 
> and
> then go down.
>
>> x <- c(0,10.)
>> y <- c(0,10.)
>> offset <- 3
>> centre <- 5
>> plot(x,y, xlim=range(x), ylim=range(y),type="n", xlab="",ylab="",
>> main="",xaxt="n",yaxt="n")
>> for (i in (seq(0, 340, by=45))	)
>> 	{
>> 	px <- centre + cos((i*pi)/180) * offset
>> 	py <- centre + sin((i*pi)/180) * offset
>> 	text(px, py,
>> 	labels=substitute(that%->%phantom(1),list(that=i)), pos=1, 
>> col="blue",
>> cex=0.7, srt=i)
>> 	text(px, py,
>> 	labels=substitute(that%<-%phantom(1),list(that=i)), pos=3,
>> col="blue",cex=0.7,srt=i)
>> 	lines(px, py, type="p", col="black") #just for reference
>> 	}
>> What have I done wrong?
>> In addition, on my screen, the arrow symbols do not rotate at all
>> although they do on the pdf image.
>> (R version 1.71,  MacOS X 10.2.6)
>
> *WHICH* Mac port of R?  There are two!
>
> The lack of rotation is a limitation of the (unspecified) graphics 
> device
> of your (unspecified) port, whichever it is: it works on Linux and
> Windows, for example, so I would expect this to work on an X11 device 
> on
> the Darwin port.
>
Yes this is ambiguous,sorry.  Not the Darwin port.
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From fisher at plessthan.com  Tue Jul 29 18:56:20 2003
From: fisher at plessthan.com (Dennis Fisher)
Date: Tue, 29 Jul 2003 09:56:20 -0700
Subject: [R] Manipulating text
Message-ID: <93FF6EDA-C1E5-11D7-B04C-003065500988@plessthan.com>

In labeling axes, I want to combine symbols and text/superscripts.  
Examples include:

m2 (m, followed by a superscripted 2)
?g (micrograms)

How can I accomplish this in R?


Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-8864)
Fax: 1-415-564-2220
www.PLessThan.com


